file_path,api_count,code
setup.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport subprocess\nimport sys\n\nfrom setuptools import find_packages, setup\n\nPROJECT_PATH = os.path.dirname(os.path.abspath(__file__))\nVERSION = """"""\n# This file is auto-generated with the version information during setup.py installation.\n\n__version__ = \'{}\'\n""""""\n\n# Find pyro version.\nfor line in open(os.path.join(PROJECT_PATH, \'pyro\', \'__init__.py\')):\n    if line.startswith(\'version_prefix = \'):\n        version = line.strip().split()[2][1:-1]\n\n# Append current commit sha to version\ncommit_sha = \'\'\ntry:\n    current_tag = subprocess.check_output([\'git\', \'tag\', \'--points-at\', \'HEAD\'],\n                                          cwd=PROJECT_PATH).decode(\'ascii\').strip()\n    # only add sha if HEAD does not point to the release tag\n    if not current_tag == version:\n        commit_sha = subprocess.check_output([\'git\', \'rev-parse\', \'--short\', \'HEAD\'],\n                                             cwd=PROJECT_PATH).decode(\'ascii\').strip()\n# catch all exception to be safe\nexcept Exception:\n    pass  # probably not a git repo\n\n# Write version to _version.py\nif commit_sha:\n    version += \'+{}\'.format(commit_sha)\nwith open(os.path.join(PROJECT_PATH, \'pyro\', \'_version.py\'), \'w\') as f:\n    f.write(VERSION.format(version))\n\n\n# READ README.md for long description on PyPi.\n# This requires uploading via twine, e.g.:\n# $ python setup.py sdist bdist_wheel\n# $ twine upload --repository-url https://test.pypi.org/legacy/ dist/*  # test version\n# $ twine upload dist/*\ntry:\n    long_description = open(\'README.md\', encoding=\'utf-8\').read()\nexcept Exception as e:\n    sys.stderr.write(\'Failed to read README.md: {}\\n\'.format(e))\n    sys.stderr.flush()\n    long_description = \'\'\n\n# Remove badges since they will always be obsolete.\n# This assumes the first 12 lines contain badge info.\nlong_description = \'\\n\'.join([str(line) for line in long_description.split(\'\\n\')[12:]])\n\n# examples/tutorials\nEXTRAS_REQUIRE = [\n    \'jupyter>=1.0.0\',\n    \'graphviz>=0.8\',\n    \'matplotlib>=1.3\',\n    \'torchvision>=0.6.0\',\n    \'visdom>=0.1.4\',\n    \'pandas\',\n    \'seaborn\',\n    \'wget\',\n]\n\nsetup(\n    name=\'pyro-ppl\',\n    version=version,\n    description=\'A Python library for probabilistic modeling and inference\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    packages=find_packages(include=[\'pyro\', \'pyro.*\']),\n    package_data={""pyro.distributions"": [""*.cpp""]},\n    url=\'http://pyro.ai\',\n    author=\'Uber AI Labs\',\n    author_email=\'pyro@uber.com\',\n    install_requires=[\n        # if you add any additional libraries, please also\n        # add them to `docs/requirements.txt`\n        # numpy is necessary for some functionality of PyTorch\n        \'numpy>=1.7\',\n        \'opt_einsum>=2.3.2\',\n        \'pyro-api>=0.1.1\',\n        \'torch>=1.5.0\',\n        \'tqdm>=4.36\',\n    ],\n    extras_require={\n        \'extras\': EXTRAS_REQUIRE,\n        \'test\': EXTRAS_REQUIRE + [\n            \'nbval\',\n            \'pytest>=4.1\',\n            \'pytest-cov\',\n            \'scipy>=1.1\',\n        ],\n        \'profile\': [\'prettytable\', \'pytest-benchmark\', \'snakeviz\'],\n        \'dev\': EXTRAS_REQUIRE + [\n            \'flake8\',\n            \'isort\',\n            \'nbformat\',\n            \'nbsphinx>=0.3.2\',\n            \'nbstripout\',\n            \'nbval\',\n            \'ninja\',\n            \'pypandoc\',\n            \'pytest>=4.1\',\n            \'pytest-xdist\',\n            \'scipy>=1.1\',\n            \'sphinx\',\n            \'sphinx_rtd_theme\',\n            \'yapf\',\n        ],\n    },\n    python_requires=\'>=3.5\',\n    keywords=\'machine learning statistics probabilistic programming bayesian modeling pytorch\',\n    license=\'Apache 2.0\',\n    classifiers=[\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'Operating System :: POSIX :: Linux\',\n        \'Operating System :: MacOS :: MacOS X\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n    ],\n    # yapf\n)\n'"
examples/baseball.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport logging\nimport math\n\nimport pandas as pd\nimport torch\n\nimport pyro\nfrom pyro.distributions import Beta, Binomial, HalfCauchy, Normal, Pareto, Uniform\nfrom pyro.distributions.util import scalar_like\nfrom pyro.infer import MCMC, NUTS, Predictive\nfrom pyro.infer.mcmc.util import initialize_model, summary\nfrom pyro.util import ignore_experimental_warning\n\n""""""\nExample has been adapted from [1]. It demonstrates how to do Bayesian inference using\nNUTS (or, HMC) in Pyro, and use of some common inference utilities.\n\nAs in the Stan tutorial, this uses the small baseball dataset of Efron and Morris [2]\nto estimate players\' batting average which is the fraction of times a player got a\nbase hit out of the number of times they went up at bat.\n\nThe dataset separates the initial 45 at-bats statistics from the remaining season.\nWe use the hits data from the initial 45 at-bats to estimate the batting average\nfor each player. We then use the remaining season\'s data to validate the predictions\nfrom our models.\n\nThree models are evaluated:\n - Complete pooling model: The success probability of scoring a hit is shared\n     amongst all players.\n - No pooling model: Each individual player\'s success probability is distinct and\n     there is no data sharing amongst players.\n - Partial pooling model: A hierarchical model with partial data sharing.\n\n\nWe recommend Radford Neal\'s tutorial on HMC ([3]) to users who would like to get a\nmore comprehensive understanding of HMC and its variants, and to [4] for details on\nthe No U-Turn Sampler, which provides an efficient and automated way (i.e. limited\nhyper-parameters) of running HMC on different problems.\n\n[1] Carpenter B. (2016), [""Hierarchical Partial Pooling for Repeated Binary Trials""]\n    (http://mc-stan.org/users/documentation/case-studies/pool-binary-trials.html).\n[2] Efron B., Morris C. (1975), ""Data analysis using Stein\'s estimator and its\n    generalizations"", J. Amer. Statist. Assoc., 70, 311-319.\n[3] Neal, R. (2012), ""MCMC using Hamiltonian Dynamics"",\n    (https://arxiv.org/pdf/1206.1901.pdf)\n[4] Hoffman, M. D. and Gelman, A. (2014), ""The No-U-turn sampler: Adaptively setting\n    path lengths in Hamiltonian Monte Carlo"", (https://arxiv.org/abs/1111.4246)\n""""""\n\nlogging.basicConfig(format=\'%(message)s\', level=logging.INFO)\nDATA_URL = ""https://d2hg8soec8ck9v.cloudfront.net/datasets/EfronMorrisBB.txt""\n\n\n# ===================================\n#               MODELS\n# ===================================\n\n\ndef fully_pooled(at_bats, hits):\n    r""""""\n    Number of hits in $K$ at bats for each player has a Binomial\n    distribution with a common probability of success, $\\phi$.\n\n    :param (torch.Tensor) at_bats: Number of at bats for each player.\n    :param (torch.Tensor) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    """"""\n    phi_prior = Uniform(scalar_like(at_bats, 0), scalar_like(at_bats, 1))\n    phi = pyro.sample(""phi"", phi_prior)\n    num_players = at_bats.shape[0]\n    with pyro.plate(""num_players"", num_players):\n        return pyro.sample(""obs"", Binomial(at_bats, phi), obs=hits)\n\n\ndef not_pooled(at_bats, hits):\n    r""""""\n    Number of hits in $K$ at bats for each player has a Binomial\n    distribution with independent probability of success, $\\phi_i$.\n\n    :param (torch.Tensor) at_bats: Number of at bats for each player.\n    :param (torch.Tensor) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    """"""\n    num_players = at_bats.shape[0]\n    with pyro.plate(""num_players"", num_players):\n        phi_prior = Uniform(scalar_like(at_bats, 0), scalar_like(at_bats, 1))\n        phi = pyro.sample(""phi"", phi_prior)\n        return pyro.sample(""obs"", Binomial(at_bats, phi), obs=hits)\n\n\ndef partially_pooled(at_bats, hits):\n    r""""""\n    Number of hits has a Binomial distribution with independent\n    probability of success, $\\phi_i$. Each $\\phi_i$ follows a Beta\n    distribution with concentration parameters $c_1$ and $c_2$, where\n    $c_1 = m * kappa$, $c_2 = (1 - m) * kappa$, $m ~ Uniform(0, 1)$,\n    and $kappa ~ Pareto(1, 1.5)$.\n\n    :param (torch.Tensor) at_bats: Number of at bats for each player.\n    :param (torch.Tensor) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    """"""\n    num_players = at_bats.shape[0]\n    m = pyro.sample(""m"", Uniform(scalar_like(at_bats, 0), scalar_like(at_bats, 1)))\n    kappa = pyro.sample(""kappa"", Pareto(scalar_like(at_bats, 1), scalar_like(at_bats, 1.5)))\n    with pyro.plate(""num_players"", num_players):\n        phi_prior = Beta(m * kappa, (1 - m) * kappa)\n        phi = pyro.sample(""phi"", phi_prior)\n        return pyro.sample(""obs"", Binomial(at_bats, phi), obs=hits)\n\n\ndef partially_pooled_with_logit(at_bats, hits):\n    r""""""\n    Number of hits has a Binomial distribution with a logit link function.\n    The logits $\\alpha$ for each player is normally distributed with the\n    mean and scale parameters sharing a common prior.\n\n    :param (torch.Tensor) at_bats: Number of at bats for each player.\n    :param (torch.Tensor) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    """"""\n    num_players = at_bats.shape[0]\n    loc = pyro.sample(""loc"", Normal(scalar_like(at_bats, -1), scalar_like(at_bats, 1)))\n    scale = pyro.sample(""scale"", HalfCauchy(scale=scalar_like(at_bats, 1)))\n    with pyro.plate(""num_players"", num_players):\n        alpha = pyro.sample(""alpha"", Normal(loc, scale))\n        return pyro.sample(""obs"", Binomial(at_bats, logits=alpha), obs=hits)\n\n\n# ===================================\n#        DATA SUMMARIZE UTILS\n# ===================================\n\n\ndef get_summary_table(posterior, sites, player_names, transforms={}, diagnostics=False, group_by_chain=False):\n    """"""\n    Return summarized statistics for each of the ``sites`` in the\n    traces corresponding to the approximate posterior.\n    """"""\n    site_stats = {}\n\n    for site_name in sites:\n        marginal_site = posterior[site_name].cpu()\n\n        if site_name in transforms:\n            marginal_site = transforms[site_name](marginal_site)\n\n        site_summary = summary({site_name: marginal_site}, prob=0.5, group_by_chain=group_by_chain)[site_name]\n        if site_summary[""mean""].shape:\n            site_df = pd.DataFrame(site_summary, index=player_names)\n        else:\n            site_df = pd.DataFrame(site_summary, index=[0])\n        if not diagnostics:\n            site_df = site_df.drop([""n_eff"", ""r_hat""], axis=1)\n        site_stats[site_name] = site_df.astype(float).round(2)\n\n    return site_stats\n\n\ndef train_test_split(pd_dataframe):\n    """"""\n    Training data - 45 initial at-bats and hits for each player.\n    Validation data - Full season at-bats and hits for each player.\n    """"""\n    device = torch.Tensor().device\n    train_data = torch.tensor(pd_dataframe[[""At-Bats"", ""Hits""]].values, dtype=torch.float, device=device)\n    test_data = torch.tensor(pd_dataframe[[""SeasonAt-Bats"", ""SeasonHits""]].values, dtype=torch.float, device=device)\n    first_name = pd_dataframe[""FirstName""].values\n    last_name = pd_dataframe[""LastName""].values\n    player_names = ["" "".join([first, last]) for first, last in zip(first_name, last_name)]\n    return train_data, test_data, player_names\n\n\n# ===================================\n#       MODEL EVALUATION UTILS\n# ===================================\n\n\ndef sample_posterior_predictive(model, posterior_samples, baseball_dataset):\n    """"""\n    Generate samples from posterior predictive distribution.\n    """"""\n    train, test, player_names = train_test_split(baseball_dataset)\n    at_bats = train[:, 0]\n    at_bats_season = test[:, 0]\n    logging.Formatter(""%(message)s"")\n    logging.info(""\\nPosterior Predictive:"")\n    logging.info(""Hit Rate - Initial 45 At Bats"")\n    logging.info(""-----------------------------"")\n    # set hits=None to convert it from observation node to sample node\n    train_predict = Predictive(model, posterior_samples)(at_bats, None)\n    train_summary = get_summary_table(train_predict,\n                                      sites=[""obs""],\n                                      player_names=player_names)[""obs""]\n    train_summary = train_summary.assign(ActualHits=baseball_dataset[[""Hits""]].values)\n    logging.info(train_summary)\n    logging.info(""\\nHit Rate - Season Predictions"")\n    logging.info(""-----------------------------"")\n    with ignore_experimental_warning():\n        test_predict = Predictive(model, posterior_samples)(at_bats_season, None)\n    test_summary = get_summary_table(test_predict,\n                                     sites=[""obs""],\n                                     player_names=player_names)[""obs""]\n    test_summary = test_summary.assign(ActualHits=baseball_dataset[[""SeasonHits""]].values)\n    logging.info(test_summary)\n\n\ndef evaluate_pointwise_pred_density(model, posterior_samples, baseball_dataset):\n    """"""\n    Evaluate the log probability density of observing the unseen data (season hits)\n    given a model and posterior distribution over the parameters.\n    """"""\n    _, test, player_names = train_test_split(baseball_dataset)\n    at_bats_season, hits_season = test[:, 0], test[:, 1]\n    trace = Predictive(model, posterior_samples).get_vectorized_trace(at_bats_season, hits_season)\n    # Use LogSumExp trick to evaluate $log(1/num_samples \\sum_i p(new_data | \\theta^{i})) $,\n    # where $\\theta^{i}$ are parameter samples from the model\'s posterior.\n    trace.compute_log_prob()\n    post_loglik = trace.nodes[""obs""][""log_prob""]\n    # computes expected log predictive density at each data point\n    exp_log_density = (post_loglik.logsumexp(0) - math.log(post_loglik.shape[0])).sum()\n    logging.info(""\\nLog pointwise predictive density"")\n    logging.info(""--------------------------------"")\n    logging.info(""{:.4f}\\n"".format(exp_log_density))\n\n\ndef main(args):\n    baseball_dataset = pd.read_csv(DATA_URL, ""\\t"")\n    train, _, player_names = train_test_split(baseball_dataset)\n    at_bats, hits = train[:, 0], train[:, 1]\n    logging.info(""Original Dataset:"")\n    logging.info(baseball_dataset)\n\n    # (1) Full Pooling Model\n    # In this model, we illustrate how to use MCMC with general potential_fn.\n    init_params, potential_fn, transforms, _ = initialize_model(\n        fully_pooled, model_args=(at_bats, hits), num_chains=args.num_chains,\n        jit_compile=args.jit, skip_jit_warnings=True)\n    nuts_kernel = NUTS(potential_fn=potential_fn)\n    mcmc = MCMC(nuts_kernel,\n                num_samples=args.num_samples,\n                warmup_steps=args.warmup_steps,\n                num_chains=args.num_chains,\n                initial_params=init_params,\n                transforms=transforms)\n    mcmc.run(at_bats, hits)\n    samples_fully_pooled = mcmc.get_samples()\n    logging.info(""\\nModel: Fully Pooled"")\n    logging.info(""==================="")\n    logging.info(""\\nphi:"")\n    logging.info(get_summary_table(mcmc.get_samples(group_by_chain=True),\n                                   sites=[""phi""],\n                                   player_names=player_names,\n                                   diagnostics=True,\n                                   group_by_chain=True)[""phi""])\n    num_divergences = sum(map(len, mcmc.diagnostics()[""divergences""].values()))\n    logging.info(""\\nNumber of divergent transitions: {}\\n"".format(num_divergences))\n    sample_posterior_predictive(fully_pooled, samples_fully_pooled, baseball_dataset)\n    evaluate_pointwise_pred_density(fully_pooled, samples_fully_pooled, baseball_dataset)\n\n    # (2) No Pooling Model\n    nuts_kernel = NUTS(not_pooled, jit_compile=args.jit, ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel,\n                num_samples=args.num_samples,\n                warmup_steps=args.warmup_steps,\n                num_chains=args.num_chains)\n    mcmc.run(at_bats, hits)\n    samples_not_pooled = mcmc.get_samples()\n    logging.info(""\\nModel: Not Pooled"")\n    logging.info(""================="")\n    logging.info(""\\nphi:"")\n    logging.info(get_summary_table(mcmc.get_samples(group_by_chain=True),\n                                   sites=[""phi""],\n                                   player_names=player_names,\n                                   diagnostics=True,\n                                   group_by_chain=True)[""phi""])\n    num_divergences = sum(map(len, mcmc.diagnostics()[""divergences""].values()))\n    logging.info(""\\nNumber of divergent transitions: {}\\n"".format(num_divergences))\n    sample_posterior_predictive(not_pooled, samples_not_pooled, baseball_dataset)\n    evaluate_pointwise_pred_density(not_pooled, samples_not_pooled, baseball_dataset)\n\n    # (3) Partially Pooled Model\n    nuts_kernel = NUTS(partially_pooled, jit_compile=args.jit, ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel,\n                num_samples=args.num_samples,\n                warmup_steps=args.warmup_steps,\n                num_chains=args.num_chains)\n    mcmc.run(at_bats, hits)\n    samples_partially_pooled = mcmc.get_samples()\n    logging.info(""\\nModel: Partially Pooled"")\n    logging.info(""======================="")\n    logging.info(""\\nphi:"")\n    logging.info(get_summary_table(mcmc.get_samples(group_by_chain=True),\n                                   sites=[""phi""],\n                                   player_names=player_names,\n                                   diagnostics=True,\n                                   group_by_chain=True)[""phi""])\n    num_divergences = sum(map(len, mcmc.diagnostics()[""divergences""].values()))\n    logging.info(""\\nNumber of divergent transitions: {}\\n"".format(num_divergences))\n    sample_posterior_predictive(partially_pooled, samples_partially_pooled, baseball_dataset)\n    evaluate_pointwise_pred_density(partially_pooled, samples_partially_pooled, baseball_dataset)\n\n    # (4) Partially Pooled with Logit Model\n    nuts_kernel = NUTS(partially_pooled_with_logit, jit_compile=args.jit, ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel,\n                num_samples=args.num_samples,\n                warmup_steps=args.warmup_steps,\n                num_chains=args.num_chains)\n    mcmc.run(at_bats, hits)\n    samples_partially_pooled_logit = mcmc.get_samples()\n    logging.info(""\\nModel: Partially Pooled with Logit"")\n    logging.info(""=================================="")\n    logging.info(""\\nSigmoid(alpha):"")\n    logging.info(get_summary_table(mcmc.get_samples(group_by_chain=True),\n                                   sites=[""alpha""],\n                                   player_names=player_names,\n                                   transforms={""alpha"": torch.sigmoid},\n                                   diagnostics=True,\n                                   group_by_chain=True)[""alpha""])\n    num_divergences = sum(map(len, mcmc.diagnostics()[""divergences""].values()))\n    logging.info(""\\nNumber of divergent transitions: {}\\n"".format(num_divergences))\n    sample_posterior_predictive(partially_pooled_with_logit, samples_partially_pooled_logit,\n                                baseball_dataset)\n    evaluate_pointwise_pred_density(partially_pooled_with_logit, samples_partially_pooled_logit,\n                                    baseball_dataset)\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""Baseball batting average using HMC"")\n    parser.add_argument(""-n"", ""--num-samples"", nargs=""?"", default=200, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=4, type=int)\n    parser.add_argument(""--warmup-steps"", nargs=\'?\', default=100, type=int)\n    parser.add_argument(""--rng_seed"", nargs=\'?\', default=0, type=int)\n    parser.add_argument(""--jit"", action=""store_true"", default=False,\n                        help=""use PyTorch jit"")\n    parser.add_argument(""--cuda"", action=""store_true"", default=False,\n                        help=""run this example in GPU"")\n    args = parser.parse_args()\n\n    # work around the error ""CUDA error: initialization error""\n    # see https://github.com/pytorch/pytorch/issues/2517\n    torch.multiprocessing.set_start_method(""spawn"")\n\n    pyro.set_rng_seed(args.rng_seed)\n    # Enable validation checks\n    pyro.enable_validation(__debug__)\n\n    # work around with the error ""RuntimeError: received 0 items of ancdata""\n    # see https://discuss.pytorch.org/t/received-0-items-of-ancdata-pytorch-0-4-0/19823\n    torch.multiprocessing.set_sharing_strategy(""file_system"")\n\n    if args.cuda:\n        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n    main(args)\n'"
examples/einsum.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis example demonstrates how to use plated ``einsum`` with different backends\nto compute logprob, gradients, MAP estimates, posterior samples, and marginals.\n\nThe interface for adjoint algorithms requires four steps:\n\n1. Call ``require_backward()`` on all inputs.\n2. Call ``x, = einsum(..., backend=...)`` with a nonstandard backend.\n3. Call ``x._pyro_backward()` on the einsum output.\n4. Retrieve results from ``._pyro_backward_result`` attributes of the inputs.\n\nThe results of these computations are returned, but this script does not\nmake use of them; instead we simply time the operations for profiling.\nAll profiling is done on jit-compiled functions. We exclude jit compilation\ntime from profiling results, assuming this can be done once.\n\nYou can measure complexity of different einsum problems by specifying\n``--equation`` and ``--plates``.\n""""""\n\nimport argparse\nimport timeit\n\nimport torch\nfrom torch.autograd import grad\n\nfrom pyro.ops.contract import einsum\nfrom pyro.ops.einsum.adjoint import require_backward\nfrom pyro.util import ignore_jit_warnings\n\n# We will cache jit-compiled versions of each function.\n_CACHE = {}\n\n\ndef jit_prob(equation, *operands, **kwargs):\n    """"""\n    Runs einsum to compute the partition function.\n\n    This is cheap but less numerically stable than using the torch_log backend.\n    """"""\n    key = \'prob\', equation, kwargs[\'plates\']\n    if key not in _CACHE:\n\n        # This simply wraps einsum for jit compilation.\n        def _einsum(*operands):\n            return einsum(equation, *operands, **kwargs)\n\n        _CACHE[key] = torch.jit.trace(_einsum, operands, check_trace=False)\n\n    return _CACHE[key](*operands)\n\n\ndef jit_logprob(equation, *operands, **kwargs):\n    """"""\n    Runs einsum to compute the log partition function.\n\n    This simulates evaluating an undirected graphical model.\n    """"""\n    key = \'logprob\', equation, kwargs[\'plates\']\n    if key not in _CACHE:\n\n        # This simply wraps einsum for jit compilation.\n        def _einsum(*operands):\n            return einsum(equation, *operands, backend=\'pyro.ops.einsum.torch_log\', **kwargs)\n\n        _CACHE[key] = torch.jit.trace(_einsum, operands, check_trace=False)\n\n    return _CACHE[key](*operands)\n\n\ndef jit_gradient(equation, *operands, **kwargs):\n    """"""\n    Runs einsum and calls backward on the partition function.\n\n    This is simulates training an undirected graphical model.\n    """"""\n    key = \'gradient\', equation, kwargs[\'plates\']\n    if key not in _CACHE:\n\n        # This wraps einsum for jit compilation, but we will call backward on the result.\n        def _einsum(*operands):\n            return einsum(equation, *operands, backend=\'pyro.ops.einsum.torch_log\', **kwargs)\n\n        _CACHE[key] = torch.jit.trace(_einsum, operands, check_trace=False)\n\n    # Run forward pass.\n    losses = _CACHE[key](*operands)\n\n    # Work around PyTorch 1.0.0 bug https://github.com/pytorch/pytorch/issues/14875\n    # whereby tuples of length 1 are unwrapped by the jit.\n    if not isinstance(losses, tuple):\n        losses = (losses,)\n\n    # Run backward pass.\n    grads = tuple(grad(loss, operands, retain_graph=True, allow_unused=True)\n                  for loss in losses)\n    return grads\n\n\ndef _jit_adjoint(equation, *operands, **kwargs):\n    """"""\n    Runs einsum in forward-backward mode using ``pyro.ops.adjoint``.\n\n    This simulates serving predictions from an undirected graphical model.\n    """"""\n    backend = kwargs.pop(\'backend\', \'pyro.ops.einsum.torch_sample\')\n    key = backend, equation, tuple(x.shape for x in operands), kwargs[\'plates\']\n    if key not in _CACHE:\n\n        # This wraps a complete adjoint algorithm call.\n        @ignore_jit_warnings()\n        def _forward_backward(*operands):\n            # First we request backward results on each input operand.\n            # This is the pyro.ops.adjoint equivalent of torch\'s .requires_grad_().\n            for operand in operands:\n                require_backward(operand)\n\n            # Next we run the forward pass.\n            results = einsum(equation, *operands, backend=backend, **kwargs)\n\n            # The we run a backward pass.\n            for result in results:\n                result._pyro_backward()\n\n            # Finally we retrieve results from the ._pyro_backward_result attribute\n            # that has been set on each input operand. If you only want results on a\n            # subset of operands, you can call require_backward() on only those.\n            results = []\n            for x in operands:\n                results.append(x._pyro_backward_result)\n                x._pyro_backward_result = None\n\n            return tuple(results)\n\n        _CACHE[key] = torch.jit.trace(_forward_backward, operands, check_trace=False)\n\n    return _CACHE[key](*operands)\n\n\ndef jit_map(equation, *operands, **kwargs):\n    return _jit_adjoint(equation, *operands, backend=\'pyro.ops.einsum.torch_map\', **kwargs)\n\n\ndef jit_sample(equation, *operands, **kwargs):\n    return _jit_adjoint(equation, *operands, backend=\'pyro.ops.einsum.torch_sample\', **kwargs)\n\n\ndef jit_marginal(equation, *operands, **kwargs):\n    return _jit_adjoint(equation, *operands, backend=\'pyro.ops.einsum.torch_marginal\', **kwargs)\n\n\ndef time_fn(fn, equation, *operands, **kwargs):\n    iters = kwargs.pop(\'iters\')\n    _CACHE.clear()  # Avoid memory leaks.\n    fn(equation, *operands, **kwargs)\n\n    time_start = timeit.default_timer()\n    for i in range(iters):\n        fn(equation, *operands, **kwargs)\n    time_end = timeit.default_timer()\n\n    return (time_end - time_start) / iters\n\n\ndef main(args):\n    if args.cuda:\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n    else:\n        torch.set_default_tensor_type(\'torch.FloatTensor\')\n\n    if args.method == \'all\':\n        for method in [\'prob\', \'logprob\', \'gradient\', \'marginal\', \'map\', \'sample\']:\n            args.method = method\n            main(args)\n        return\n\n    print(\'Plate size  Time per iteration of {} (ms)\'.format(args.method))\n    fn = globals()[\'jit_{}\'.format(args.method)]\n    equation = args.equation\n    plates = args.plates\n    inputs, outputs = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n\n    # Vary all plate sizes at the same time.\n    for plate_size in range(8, 1 + args.max_plate_size, 8):\n        operands = []\n        for dims in inputs:\n            shape = torch.Size([plate_size if d in plates else args.dim_size\n                                for d in dims])\n            operands.append((torch.empty(shape).uniform_() + 0.5).requires_grad_())\n\n        time = time_fn(fn, equation, *operands, plates=plates, modulo_total=True,\n                       iters=args.iters)\n        print(\'{: <11s} {:0.4g}\'.format(\'{} ** {}\'.format(plate_size, len(args.plates)), time * 1e3))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'plated einsum profiler\')\n    parser.add_argument(\'-e\', \'--equation\', default=\'a,abi,bcij,adj,deij->\')\n    parser.add_argument(\'-p\', \'--plates\', default=\'ij\')\n    parser.add_argument(\'-d\', \'--dim-size\', default=32, type=int)\n    parser.add_argument(\'-s\', \'--max-plate-size\', default=32, type=int)\n    parser.add_argument(\'-n\', \'--iters\', default=10, type=int)\n    parser.add_argument(\'--cuda\', action=\'store_true\')\n    parser.add_argument(\'-m\', \'--method\', default=\'all\',\n                        help=\'one of: prob, logprob, gradient, marginal, map, sample\')\n    args = parser.parse_args()\n    main(args)\n'"
examples/hmm.py,25,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis example shows how to marginalize out discrete model variables in Pyro.\n\nThis combines Stochastic Variational Inference (SVI) with a\nvariable elimination algorithm, where we use enumeration to exactly\nmarginalize out some variables from the ELBO computation. We might\ncall the resulting algorithm collapsed SVI or collapsed SGVB (i.e\ncollapsed Stochastic Gradient Variational Bayes). In the case where\nwe exactly sum out all the latent variables (as is the case here),\nthis algorithm reduces to a form of gradient-based Maximum\nLikelihood Estimation.\n\nTo marginalize out discrete variables ``x`` in Pyro\'s SVI:\n\n1. Verify that the variable dependency structure in your model\n    admits tractable inference, i.e. the dependency graph among\n    enumerated variables should have narrow treewidth.\n2. Annotate each target each such sample site in the model\n    with ``infer={""enumerate"": ""parallel""}``\n3. Ensure your model can handle broadcasting of the sample values\n    of those variables\n4. Use the ``TraceEnum_ELBO`` loss inside Pyro\'s ``SVI``.\n\nNote that empirical results for the models defined here can be found in\nreference [1]. This paper also includes a description of the ""tensor\nvariable elimination"" algorithm that Pyro uses under the hood to\nmarginalize out discrete latent variables.\n\nReferences\n\n1. ""Tensor Variable Elimination for Plated Factor Graphs"",\nFritz Obermeyer, Eli Bingham, Martin Jankowiak, Justin Chiu,\nNeeraj Pradhan, Alexander Rush, Noah Goodman. https://arxiv.org/abs/1902.03210\n""""""\nimport argparse\nimport logging\nimport sys\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import constraints\n\nimport dmm.polyphonic_data_loader as poly\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.autoguide import AutoDelta\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\nfrom pyro.ops.indexing import Vindex\nfrom pyro.optim import Adam\nfrom pyro.util import ignore_jit_warnings\n\nlogging.basicConfig(format=\'%(relativeCreated) 9d %(message)s\', level=logging.DEBUG)\n\n# Add another handler for logging debugging events (e.g. for profiling)\n# in a separate stream that can be captured.\nlog = logging.getLogger()\ndebug_handler = logging.StreamHandler(sys.stdout)\ndebug_handler.setLevel(logging.DEBUG)\ndebug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\nlog.addHandler(debug_handler)\n\n\n# Let\'s start with a simple Hidden Markov Model.\n#\n#     x[t-1] --> x[t] --> x[t+1]\n#        |        |         |\n#        V        V         V\n#     y[t-1]     y[t]     y[t+1]\n#\n# This model includes a plate for the data_dim = 88 keys on the piano. This\n# model has two ""style"" parameters probs_x and probs_y that we\'ll draw from a\n# prior. The latent state is x, and the observed state is y. We\'ll drive\n# probs_* with the guide, enumerate over x, and condition on y.\n#\n# Importantly, the dependency structure of the enumerated variables has\n# narrow treewidth, therefore admitting efficient inference by message passing.\n# Pyro\'s TraceEnum_ELBO will find an efficient message passing scheme if one\n# exists.\ndef model_0(sequences, lengths, args, batch_size=None, include_prior=True):\n    assert not torch._C._get_tracing_state()\n    num_sequences, max_length, data_dim = sequences.shape\n    with poutine.mask(mask=include_prior):\n        # Our prior on transition probabilities will be:\n        # stay in the same state with 90% probability; uniformly jump to another\n        # state with 10% probability.\n        probs_x = pyro.sample(""probs_x"",\n                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n                                  .to_event(1))\n        # We put a weak prior on the conditional probability of a tone sounding.\n        # We know that on average about 4 of 88 tones are active, so we\'ll set a\n        # rough weak prior of 10% of the notes being active at any one time.\n        probs_y = pyro.sample(""probs_y"",\n                              dist.Beta(0.1, 0.9)\n                                  .expand([args.hidden_dim, data_dim])\n                                  .to_event(2))\n    # In this first model we\'ll sequentially iterate over sequences in a\n    # minibatch; this will make it easy to reason about tensor shapes.\n    tones_plate = pyro.plate(""tones"", data_dim, dim=-1)\n    for i in pyro.plate(""sequences"", len(sequences), batch_size):\n        length = lengths[i]\n        sequence = sequences[i, :length]\n        x = 0\n        for t in pyro.markov(range(length)):\n            # On the next line, we\'ll overwrite the value of x with an updated\n            # value. If we wanted to record all x values, we could instead\n            # write x[t] = pyro.sample(...x[t-1]...).\n            x = pyro.sample(""x_{}_{}"".format(i, t), dist.Categorical(probs_x[x]),\n                            infer={""enumerate"": ""parallel""})\n            with tones_plate:\n                pyro.sample(""y_{}_{}"".format(i, t), dist.Bernoulli(probs_y[x.squeeze(-1)]),\n                            obs=sequence[t])\n# To see how enumeration changes the shapes of these sample sites, we can use\n# the Trace.format_shapes() to print shapes at each site:\n# $ python examples/hmm.py -m 0 -n 1 -b 1 -t 5 --print-shapes\n# ...\n#  Sample Sites:\n#   probs_x dist          | 16 16\n#          value          | 16 16\n#   probs_y dist          | 16 88\n#          value          | 16 88\n#     tones dist          |\n#          value       88 |\n# sequences dist          |\n#          value        1 |\n#   x_178_0 dist          |\n#          value    16  1 |\n#   y_178_0 dist    16 88 |\n#          value       88 |\n#   x_178_1 dist    16  1 |\n#          value 16  1  1 |\n#   y_178_1 dist 16  1 88 |\n#          value       88 |\n#   x_178_2 dist 16  1  1 |\n#          value    16  1 |\n#   y_178_2 dist    16 88 |\n#          value       88 |\n#   x_178_3 dist    16  1 |\n#          value 16  1  1 |\n#   y_178_3 dist 16  1 88 |\n#          value       88 |\n#   x_178_4 dist 16  1  1 |\n#          value    16  1 |\n#   y_178_4 dist    16 88 |\n#          value       88 |\n#\n# Notice that enumeration (over 16 states) alternates between two dimensions:\n# -2 and -3.  If we had not used pyro.markov above, each enumerated variable\n# would need its own enumeration dimension.\n\n\n# Next let\'s make our simple model faster in two ways: first we\'ll support\n# vectorized minibatches of data, and second we\'ll support the PyTorch jit\n# compiler.  To add batch support, we\'ll introduce a second plate ""sequences""\n# and randomly subsample data to size batch_size.  To add jit support we\n# silence some warnings and try to avoid dynamic program structure.\n\n# Note that this is the ""HMM"" model in reference [1] (with the difference that\n# in [1] the probabilities probs_x and probs_y are not MAP-regularized with\n# Dirichlet and Beta distributions for any of the models)\ndef model_1(sequences, lengths, args, batch_size=None, include_prior=True):\n    # Sometimes it is safe to ignore jit warnings. Here we use the\n    # pyro.util.ignore_jit_warnings context manager to silence warnings about\n    # conversion to integer, since we know all three numbers will be the same\n    # across all invocations to the model.\n    with ignore_jit_warnings():\n        num_sequences, max_length, data_dim = map(int, sequences.shape)\n        assert lengths.shape == (num_sequences,)\n        assert lengths.max() <= max_length\n    with poutine.mask(mask=include_prior):\n        probs_x = pyro.sample(""probs_x"",\n                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n                                  .to_event(1))\n        probs_y = pyro.sample(""probs_y"",\n                              dist.Beta(0.1, 0.9)\n                                  .expand([args.hidden_dim, data_dim])\n                                  .to_event(2))\n    tones_plate = pyro.plate(""tones"", data_dim, dim=-1)\n    # We subsample batch_size items out of num_sequences items. Note that since\n    # we\'re using dim=-1 for the notes plate, we need to batch over a different\n    # dimension, here dim=-2.\n    with pyro.plate(""sequences"", num_sequences, batch_size, dim=-2) as batch:\n        lengths = lengths[batch]\n        x = 0\n        # If we are not using the jit, then we can vary the program structure\n        # each call by running for a dynamically determined number of time\n        # steps, lengths.max(). However if we are using the jit, then we try to\n        # keep a single program structure for all minibatches; the fixed\n        # structure ends up being faster since each program structure would\n        # need to trigger a new jit compile stage.\n        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n                x = pyro.sample(""x_{}"".format(t), dist.Categorical(probs_x[x]),\n                                infer={""enumerate"": ""parallel""})\n                with tones_plate:\n                    pyro.sample(""y_{}"".format(t), dist.Bernoulli(probs_y[x.squeeze(-1)]),\n                                obs=sequences[batch, t])\n# Let\'s see how batching changes the shapes of sample sites:\n# $ python examples/hmm.py -m 1 -n 1 -t 5 --batch-size=10 --print-shapes\n# ...\n#  Sample Sites:\n#   probs_x dist             | 16 16\n#          value             | 16 16\n#   probs_y dist             | 16 88\n#          value             | 16 88\n#     tones dist             |\n#          value          88 |\n# sequences dist             |\n#          value          10 |\n#       x_0 dist       10  1 |\n#          value    16  1  1 |\n#       y_0 dist    16 10 88 |\n#          value       10 88 |\n#       x_1 dist    16 10  1 |\n#          value 16  1  1  1 |\n#       y_1 dist 16  1 10 88 |\n#          value       10 88 |\n#       x_2 dist 16  1 10  1 |\n#          value    16  1  1 |\n#       y_2 dist    16 10 88 |\n#          value       10 88 |\n#       x_3 dist    16 10  1 |\n#          value 16  1  1  1 |\n#       y_3 dist 16  1 10 88 |\n#          value       10 88 |\n#       x_4 dist 16  1 10  1 |\n#          value    16  1  1 |\n#       y_4 dist    16 10 88 |\n#          value       10 88 |\n#\n# Notice that we\'re now using dim=-2 as a batch dimension (of size 10),\n# and that the enumeration dimensions are now dims -3 and -4.\n\n\n# Next let\'s add a dependency of y[t] on y[t-1].\n#\n#     x[t-1] --> x[t] --> x[t+1]\n#        |        |         |\n#        V        V         V\n#     y[t-1] --> y[t] --> y[t+1]\n#\n# Note that this is the ""arHMM"" model in reference [1].\ndef model_2(sequences, lengths, args, batch_size=None, include_prior=True):\n    with ignore_jit_warnings():\n        num_sequences, max_length, data_dim = map(int, sequences.shape)\n        assert lengths.shape == (num_sequences,)\n        assert lengths.max() <= max_length\n    with poutine.mask(mask=include_prior):\n        probs_x = pyro.sample(""probs_x"",\n                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n                                  .to_event(1))\n        probs_y = pyro.sample(""probs_y"",\n                              dist.Beta(0.1, 0.9)\n                                  .expand([args.hidden_dim, 2, data_dim])\n                                  .to_event(3))\n    tones_plate = pyro.plate(""tones"", data_dim, dim=-1)\n    with pyro.plate(""sequences"", num_sequences, batch_size, dim=-2) as batch:\n        lengths = lengths[batch]\n        x, y = 0, 0\n        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n                x = pyro.sample(""x_{}"".format(t), dist.Categorical(probs_x[x]),\n                                infer={""enumerate"": ""parallel""})\n                # Note the broadcasting tricks here: to index probs_y on tensors x and y,\n                # we also need a final tensor for the tones dimension. This is conveniently\n                # provided by the plate associated with that dimension.\n                with tones_plate as tones:\n                    y = pyro.sample(""y_{}"".format(t), dist.Bernoulli(probs_y[x, y, tones]),\n                                    obs=sequences[batch, t]).long()\n\n\n# Next consider a Factorial HMM with two hidden states.\n#\n#    w[t-1] ----> w[t] ---> w[t+1]\n#        \\ x[t-1] --\\-> x[t] --\\-> x[t+1]\n#         \\  /       \\  /       \\  /\n#          \\/         \\/         \\/\n#        y[t-1]      y[t]      y[t+1]\n#\n# Note that since the joint distribution of each y[t] depends on two variables,\n# those two variables become dependent. Therefore during enumeration, the\n# entire joint space of these variables w[t],x[t] needs to be enumerated.\n# For that reason, we set the dimension of each to the square root of the\n# target hidden dimension.\n#\n# Note that this is the ""FHMM"" model in reference [1].\ndef model_3(sequences, lengths, args, batch_size=None, include_prior=True):\n    with ignore_jit_warnings():\n        num_sequences, max_length, data_dim = map(int, sequences.shape)\n        assert lengths.shape == (num_sequences,)\n        assert lengths.max() <= max_length\n    hidden_dim = int(args.hidden_dim ** 0.5)  # split between w and x\n    with poutine.mask(mask=include_prior):\n        probs_w = pyro.sample(""probs_w"",\n                              dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n                                  .to_event(1))\n        probs_x = pyro.sample(""probs_x"",\n                              dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n                                  .to_event(1))\n        probs_y = pyro.sample(""probs_y"",\n                              dist.Beta(0.1, 0.9)\n                                  .expand([hidden_dim, hidden_dim, data_dim])\n                                  .to_event(3))\n    tones_plate = pyro.plate(""tones"", data_dim, dim=-1)\n    with pyro.plate(""sequences"", num_sequences, batch_size, dim=-2) as batch:\n        lengths = lengths[batch]\n        w, x = 0, 0\n        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n                w = pyro.sample(""w_{}"".format(t), dist.Categorical(probs_w[w]),\n                                infer={""enumerate"": ""parallel""})\n                x = pyro.sample(""x_{}"".format(t), dist.Categorical(probs_x[x]),\n                                infer={""enumerate"": ""parallel""})\n                with tones_plate as tones:\n                    pyro.sample(""y_{}"".format(t), dist.Bernoulli(probs_y[w, x, tones]),\n                                obs=sequences[batch, t])\n\n\n# By adding a dependency of x on w, we generalize to a\n# Dynamic Bayesian Network.\n#\n#     w[t-1] ----> w[t] ---> w[t+1]\n#        |  \\       |  \\       |   \\\n#        | x[t-1] ----> x[t] ----> x[t+1]\n#        |   /      |   /      |   /\n#        V  /       V  /       V  /\n#     y[t-1]       y[t]      y[t+1]\n#\n# Note that message passing here has roughly the same cost as with the\n# Factorial HMM, but this model has more parameters.\n#\n# Note that this is the ""PFHMM"" model in reference [1].\ndef model_4(sequences, lengths, args, batch_size=None, include_prior=True):\n    with ignore_jit_warnings():\n        num_sequences, max_length, data_dim = map(int, sequences.shape)\n        assert lengths.shape == (num_sequences,)\n        assert lengths.max() <= max_length\n    hidden_dim = int(args.hidden_dim ** 0.5)  # split between w and x\n    with poutine.mask(mask=include_prior):\n        probs_w = pyro.sample(""probs_w"",\n                              dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n                                  .to_event(1))\n        probs_x = pyro.sample(""probs_x"",\n                              dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n                                  .expand_by([hidden_dim])\n                                  .to_event(2))\n        probs_y = pyro.sample(""probs_y"",\n                              dist.Beta(0.1, 0.9)\n                                  .expand([hidden_dim, hidden_dim, data_dim])\n                                  .to_event(3))\n    tones_plate = pyro.plate(""tones"", data_dim, dim=-1)\n    with pyro.plate(""sequences"", num_sequences, batch_size, dim=-2) as batch:\n        lengths = lengths[batch]\n        # Note the broadcasting tricks here: we declare a hidden torch.arange and\n        # ensure that w and x are always tensors so we can unsqueeze them below,\n        # thus ensuring that the x sample sites have correct distribution shape.\n        w = x = torch.tensor(0, dtype=torch.long)\n        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n                w = pyro.sample(""w_{}"".format(t), dist.Categorical(probs_w[w]),\n                                infer={""enumerate"": ""parallel""})\n                x = pyro.sample(""x_{}"".format(t),\n                                dist.Categorical(Vindex(probs_x)[w, x]),\n                                infer={""enumerate"": ""parallel""})\n                with tones_plate as tones:\n                    pyro.sample(""y_{}"".format(t), dist.Bernoulli(probs_y[w, x, tones]),\n                                obs=sequences[batch, t])\n\n\n# Next let\'s consider a neural HMM model.\n#\n#     x[t-1] --> x[t] --> x[t+1]   } standard HMM +\n#        |        |         |\n#        V        V         V\n#     y[t-1] --> y[t] --> y[t+1]   } neural likelihood\n#\n# First let\'s define a neural net to generate y logits.\nclass TonesGenerator(nn.Module):\n    def __init__(self, args, data_dim):\n        self.args = args\n        self.data_dim = data_dim\n        super().__init__()\n        self.x_to_hidden = nn.Linear(args.hidden_dim, args.nn_dim)\n        self.y_to_hidden = nn.Linear(args.nn_channels * data_dim, args.nn_dim)\n        self.conv = nn.Conv1d(1, args.nn_channels, 3, padding=1)\n        self.hidden_to_logits = nn.Linear(args.nn_dim, data_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x, y):\n        # Hidden units depend on two inputs: a one-hot encoded categorical variable x, and\n        # a bernoulli variable y. Whereas x will typically be enumerated, y will be observed.\n        # We apply x_to_hidden independently from y_to_hidden, then broadcast the non-enumerated\n        # y part up to the enumerated x part in the + operation.\n        x_onehot = y.new_zeros(x.shape[:-1] + (self.args.hidden_dim,)).scatter_(-1, x, 1)\n        y_conv = self.relu(self.conv(y.reshape(-1, 1, self.data_dim))).reshape(y.shape[:-1] + (-1,))\n        h = self.relu(self.x_to_hidden(x_onehot) + self.y_to_hidden(y_conv))\n        return self.hidden_to_logits(h)\n\n\n# We will create a single global instance later.\ntones_generator = None\n\n\n# The neural HMM model now uses tones_generator at each time step.\n#\n# Note that this is the ""nnHMM"" model in reference [1].\ndef model_5(sequences, lengths, args, batch_size=None, include_prior=True):\n    with ignore_jit_warnings():\n        num_sequences, max_length, data_dim = map(int, sequences.shape)\n        assert lengths.shape == (num_sequences,)\n        assert lengths.max() <= max_length\n\n    # Initialize a global module instance if needed.\n    global tones_generator\n    if tones_generator is None:\n        tones_generator = TonesGenerator(args, data_dim)\n    pyro.module(""tones_generator"", tones_generator)\n\n    with poutine.mask(mask=include_prior):\n        probs_x = pyro.sample(""probs_x"",\n                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n                                  .to_event(1))\n    with pyro.plate(""sequences"", num_sequences, batch_size, dim=-2) as batch:\n        lengths = lengths[batch]\n        x = 0\n        y = torch.zeros(data_dim)\n        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n                x = pyro.sample(""x_{}"".format(t), dist.Categorical(probs_x[x]),\n                                infer={""enumerate"": ""parallel""})\n                # Note that since each tone depends on all tones at a previous time step\n                # the tones at different time steps now need to live in separate plates.\n                with pyro.plate(""tones_{}"".format(t), data_dim, dim=-1):\n                    y = pyro.sample(""y_{}"".format(t),\n                                    dist.Bernoulli(logits=tones_generator(x, y)),\n                                    obs=sequences[batch, t])\n\n\n# Next let\'s consider a second-order HMM model\n# in which x[t+1] depends on both x[t] and x[t-1].\n#\n#                     _______>______\n#         _____>_____/______        \\\n#        /          /       \\        \\\n#     x[t-1] --> x[t] --> x[t+1] --> x[t+2]\n#        |        |          |          |\n#        V        V          V          V\n#     y[t-1]     y[t]     y[t+1]     y[t+2]\n#\n#  Note that in this model (in contrast to the previous model) we treat\n#  the transition and emission probabilities as parameters (so they have no prior).\n#\n# Note that this is the ""2HMM"" model in reference [1].\ndef model_6(sequences, lengths, args, batch_size=None, include_prior=False):\n    num_sequences, max_length, data_dim = sequences.shape\n    assert lengths.shape == (num_sequences,)\n    assert lengths.max() <= max_length\n    hidden_dim = args.hidden_dim\n\n    if not args.raftery_parameterization:\n        # Explicitly parameterize the full tensor of transition probabilities, which\n        # has hidden_dim cubed entries.\n        probs_x = pyro.param(""probs_x"", torch.rand(hidden_dim, hidden_dim, hidden_dim),\n                             constraint=constraints.simplex)\n    else:\n        # Use the more parsimonious ""Raftery"" parameterization of\n        # the tensor of transition probabilities. See reference:\n        # Raftery, A. E. A model for high-order markov chains.\n        # Journal of the Royal Statistical Society. 1985.\n        probs_x1 = pyro.param(""probs_x1"", torch.rand(hidden_dim, hidden_dim),\n                              constraint=constraints.simplex)\n        probs_x2 = pyro.param(""probs_x2"", torch.rand(hidden_dim, hidden_dim),\n                              constraint=constraints.simplex)\n        mix_lambda = pyro.param(""mix_lambda"", torch.tensor(0.5), constraint=constraints.unit_interval)\n        # we use broadcasting to combine two tensors of shape (hidden_dim, hidden_dim) and\n        # (hidden_dim, 1, hidden_dim) to obtain a tensor of shape (hidden_dim, hidden_dim, hidden_dim)\n        probs_x = mix_lambda * probs_x1 + (1.0 - mix_lambda) * probs_x2.unsqueeze(-2)\n\n    probs_y = pyro.param(""probs_y"", torch.rand(hidden_dim, data_dim),\n                         constraint=constraints.unit_interval)\n    tones_plate = pyro.plate(""tones"", data_dim, dim=-1)\n    with pyro.plate(""sequences"", num_sequences, batch_size, dim=-2) as batch:\n        lengths = lengths[batch]\n        x_curr, x_prev = torch.tensor(0), torch.tensor(0)\n        # we need to pass the argument `history=2\' to `pyro.markov()`\n        # since our model is now 2-markov\n        for t in pyro.markov(range(lengths.max()), history=2):\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n                probs_x_t = Vindex(probs_x)[x_prev, x_curr]\n                x_prev, x_curr = x_curr, pyro.sample(""x_{}"".format(t), dist.Categorical(probs_x_t),\n                                                     infer={""enumerate"": ""parallel""})\n                with tones_plate:\n                    probs_y_t = probs_y[x_curr.squeeze(-1)]\n                    pyro.sample(""y_{}"".format(t), dist.Bernoulli(probs_y_t),\n                                obs=sequences[batch, t])\n\n\n# Next we demonstrate how to parallelize the neural HMM above using Pyro\'s\n# DiscreteHMM distribution. This model is equivalent to model_5 above, but we\n# manually unroll loops and fuse ops, leading to a single sample statement.\n# DiscreteHMM can lead to over 10x speedup in models where it is applicable.\ndef model_7(sequences, lengths, args, batch_size=None, include_prior=True):\n    with ignore_jit_warnings():\n        num_sequences, max_length, data_dim = map(int, sequences.shape)\n        assert lengths.shape == (num_sequences,)\n        assert lengths.max() <= max_length\n\n    # Initialize a global module instance if needed.\n    global tones_generator\n    if tones_generator is None:\n        tones_generator = TonesGenerator(args, data_dim)\n    pyro.module(""tones_generator"", tones_generator)\n\n    with poutine.mask(mask=include_prior):\n        probs_x = pyro.sample(""probs_x"",\n                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n                                  .to_event(1))\n    with pyro.plate(""sequences"", num_sequences, batch_size, dim=-1) as batch:\n        lengths = lengths[batch]\n        y = sequences[batch] if args.jit else sequences[batch, :lengths.max()]\n        x = torch.arange(args.hidden_dim)\n        t = torch.arange(y.size(1))\n        init_logits = torch.full((args.hidden_dim,), -float(\'inf\'))\n        init_logits[0] = 0\n        trans_logits = probs_x.log()\n        with ignore_jit_warnings():\n            obs_dist = dist.Bernoulli(logits=tones_generator(x, y.unsqueeze(-2))).to_event(1)\n            obs_dist = obs_dist.mask((t < lengths.unsqueeze(-1)).unsqueeze(-1))\n            hmm_dist = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n        pyro.sample(""y"", hmm_dist, obs=y)\n\n\nmodels = {name[len(\'model_\'):]: model\n          for name, model in globals().items()\n          if name.startswith(\'model_\')}\n\n\ndef main(args):\n    if args.cuda:\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n    logging.info(\'Loading data\')\n    data = poly.load_data(poly.JSB_CHORALES)\n\n    logging.info(\'-\' * 40)\n    model = models[args.model]\n    logging.info(\'Training {} on {} sequences\'.format(\n        model.__name__, len(data[\'train\'][\'sequences\'])))\n    sequences = data[\'train\'][\'sequences\']\n    lengths = data[\'train\'][\'sequence_lengths\']\n\n    # find all the notes that are present at least once in the training set\n    present_notes = ((sequences == 1).sum(0).sum(0) > 0)\n    # remove notes that are never played (we remove 37/88 notes)\n    sequences = sequences[..., present_notes]\n\n    if args.truncate:\n        lengths = lengths.clamp(max=args.truncate)\n        sequences = sequences[:, :args.truncate]\n    num_observations = float(lengths.sum())\n    pyro.set_rng_seed(args.seed)\n    pyro.clear_param_store()\n    pyro.enable_validation(__debug__)\n\n    # We\'ll train using MAP Baum-Welch, i.e. MAP estimation while marginalizing\n    # out the hidden state x. This is accomplished via an automatic guide that\n    # learns point estimates of all of our conditional probability tables,\n    # named probs_*.\n    guide = AutoDelta(poutine.block(model, expose_fn=lambda msg: msg[""name""].startswith(""probs_"")))\n\n    # To help debug our tensor shapes, let\'s print the shape of each site\'s\n    # distribution, value, and log_prob tensor. Note this information is\n    # automatically printed on most errors inside SVI.\n    if args.print_shapes:\n        first_available_dim = -2 if model is model_0 else -3\n        guide_trace = poutine.trace(guide).get_trace(\n            sequences, lengths, args=args, batch_size=args.batch_size)\n        model_trace = poutine.trace(\n            poutine.replay(poutine.enum(model, first_available_dim), guide_trace)).get_trace(\n            sequences, lengths, args=args, batch_size=args.batch_size)\n        logging.info(model_trace.format_shapes())\n\n    # Enumeration requires a TraceEnum elbo and declaring the max_plate_nesting.\n    # All of our models have two plates: ""data"" and ""tones"".\n    optim = Adam({\'lr\': args.learning_rate})\n    if args.tmc:\n        if args.jit:\n            raise NotImplementedError(""jit support not yet added for TraceTMC_ELBO"")\n        elbo = TraceTMC_ELBO(max_plate_nesting=1 if model is model_0 else 2)\n        tmc_model = poutine.infer_config(\n            model,\n            lambda msg: {""num_samples"": args.tmc_num_samples, ""expand"": False} if msg[""infer""].get(""enumerate"", None) == ""parallel"" else {})  # noqa: E501\n        svi = SVI(tmc_model, guide, optim, elbo)\n    else:\n        Elbo = JitTraceEnum_ELBO if args.jit else TraceEnum_ELBO\n        elbo = Elbo(max_plate_nesting=1 if model is model_0 else 2,\n                    strict_enumeration_warning=(model is not model_7),\n                    jit_options={""time_compilation"": args.time_compilation})\n        svi = SVI(model, guide, optim, elbo)\n\n    # We\'ll train on small minibatches.\n    logging.info(\'Step\\tLoss\')\n    for step in range(args.num_steps):\n        loss = svi.step(sequences, lengths, args=args, batch_size=args.batch_size)\n        logging.info(\'{: >5d}\\t{}\'.format(step, loss / num_observations))\n\n    if args.jit and args.time_compilation:\n        logging.debug(\'time to compile: {} s.\'.format(elbo._differentiable_loss.compile_time))\n\n    # We evaluate on the entire training dataset,\n    # excluding the prior term so our results are comparable across models.\n    train_loss = elbo.loss(model, guide, sequences, lengths, args, include_prior=False)\n    logging.info(\'training loss = {}\'.format(train_loss / num_observations))\n\n    # Finally we evaluate on the test dataset.\n    logging.info(\'-\' * 40)\n    logging.info(\'Evaluating on {} test sequences\'.format(len(data[\'test\'][\'sequences\'])))\n    sequences = data[\'test\'][\'sequences\'][..., present_notes]\n    lengths = data[\'test\'][\'sequence_lengths\']\n    if args.truncate:\n        lengths = lengths.clamp(max=args.truncate)\n    num_observations = float(lengths.sum())\n\n    # note that since we removed unseen notes above (to make the problem a bit easier and for\n    # numerical stability) this test loss may not be directly comparable to numbers\n    # reported on this dataset elsewhere.\n    test_loss = elbo.loss(model, guide, sequences, lengths, args=args, include_prior=False)\n    logging.info(\'test loss = {}\'.format(test_loss / num_observations))\n\n    # We expect models with higher capacity to perform better,\n    # but eventually overfit to the training set.\n    capacity = sum(value.reshape(-1).size(0)\n                   for value in pyro.get_param_store().values())\n    logging.info(\'{} capacity = {} parameters\'.format(model.__name__, capacity))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""MAP Baum-Welch learning Bach Chorales"")\n    parser.add_argument(""-m"", ""--model"", default=""1"", type=str,\n                        help=""one of: {}"".format("", "".join(sorted(models.keys()))))\n    parser.add_argument(""-n"", ""--num-steps"", default=50, type=int)\n    parser.add_argument(""-b"", ""--batch-size"", default=8, type=int)\n    parser.add_argument(""-d"", ""--hidden-dim"", default=16, type=int)\n    parser.add_argument(""-nn"", ""--nn-dim"", default=48, type=int)\n    parser.add_argument(""-nc"", ""--nn-channels"", default=2, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.05, type=float)\n    parser.add_argument(""-t"", ""--truncate"", type=int)\n    parser.add_argument(""-p"", ""--print-shapes"", action=""store_true"")\n    parser.add_argument(""--seed"", default=0, type=int)\n    parser.add_argument(\'--cuda\', action=\'store_true\')\n    parser.add_argument(\'--jit\', action=\'store_true\')\n    parser.add_argument(\'--time-compilation\', action=\'store_true\')\n    parser.add_argument(\'-rp\', \'--raftery-parameterization\', action=\'store_true\')\n    parser.add_argument(\'--tmc\', action=\'store_true\',\n                        help=""Use Tensor Monte Carlo instead of exact enumeration ""\n                             ""to estimate the marginal likelihood. You probably don\'t want to do this, ""\n                             ""except to see that TMC makes Monte Carlo gradient estimation feasible ""\n                             ""even with very large numbers of non-reparametrized variables."")\n    parser.add_argument(\'--tmc-num-samples\', default=10, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/inclined_plane.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n\nimport argparse\n\nimport numpy as np\nimport torch\n\nimport pyro\nfrom pyro.distributions import Normal, Uniform\nfrom pyro.infer import EmpiricalMarginal, Importance\n\n""""""\nSamantha really likes physics---but she likes Pyro even more. Instead of using\ncalculus to do her physics lab homework (which she could easily do), she\'s going\nto use bayesian inference. The problem setup is as follows. In lab she observed\na little box slide down an inclined plane (length of 2 meters and with an incline of\n30 degrees) 20 times. Each time she measured and recorded the descent time. The timing\ndevice she used has a known measurement error of 20 milliseconds. Using the observed\ndata, she wants to infer the coefficient of friction mu between the box and the inclined\nplane. She already has (deterministic) python code that can simulate the amount of time\nthat it takes the little box to slide down the inclined plane as a function of mu. Using\nPyro, she can reverse the simulator and infer mu from the observed descent times.\n""""""\n\nlittle_g = 9.8  # m/s/s\nmu0 = 0.12  # actual coefficient of friction in the experiment\ntime_measurement_sigma = 0.02  # observation noise in seconds (known quantity)\n\n\n# the forward simulator, which does numerical integration of the equations of motion\n# in steps of size dt, and optionally includes measurement noise\n\ndef simulate(mu, length=2.0, phi=np.pi / 6.0, dt=0.005, noise_sigma=None):\n    T = torch.zeros(())\n    velocity = torch.zeros(())\n    displacement = torch.zeros(())\n    acceleration = torch.tensor(little_g * np.sin(phi)) - \\\n        torch.tensor(little_g * np.cos(phi)) * mu\n\n    if acceleration.numpy() <= 0.0:  # the box doesn\'t slide if the friction is too large\n        return torch.tensor(1.0e5)   # return a very large time instead of infinity\n\n    while displacement.numpy() < length:  # otherwise slide to the end of the inclined plane\n        displacement += velocity * dt\n        velocity += acceleration * dt\n        T += dt\n\n    if noise_sigma is None:\n        return T\n    else:\n        return T + noise_sigma * torch.randn(())\n\n\n# analytic formula that the simulator above is computing via\n# numerical integration (no measurement noise)\n\ndef analytic_T(mu, length=2.0, phi=np.pi / 6.0):\n    numerator = 2.0 * length\n    denominator = little_g * (np.sin(phi) - mu * np.cos(phi))\n    return np.sqrt(numerator / denominator)\n\n\n# generate N_obs observations using simulator and the true coefficient of friction mu0\nprint(""generating simulated data using the true coefficient of friction %.3f"" % mu0)\nN_obs = 20\ntorch.manual_seed(2)\nobserved_data = torch.tensor([simulate(torch.tensor(mu0), noise_sigma=time_measurement_sigma)\n                              for _ in range(N_obs)])\nobserved_mean = np.mean([T.item() for T in observed_data])\n\n\n# define model with uniform prior on mu and gaussian noise on the descent time\ndef model(observed_data):\n    mu_prior = Uniform(0.0, 1.0)\n    mu = pyro.sample(""mu"", mu_prior)\n\n    def observe_T(T_obs, obs_name):\n        T_simulated = simulate(mu)\n        T_obs_dist = Normal(T_simulated, torch.tensor(time_measurement_sigma))\n        pyro.sample(obs_name, T_obs_dist, obs=T_obs)\n\n    for i, T_obs in enumerate(observed_data):\n        observe_T(T_obs, ""obs_%d"" % i)\n\n    return mu\n\n\ndef main(args):\n    # create an importance sampler (the prior is used as the proposal distribution)\n    importance = Importance(model, guide=None, num_samples=args.num_samples)\n    # get posterior samples of mu (which is the return value of model)\n    # from the raw execution traces provided by the importance sampler.\n    print(""doing importance sampling..."")\n    emp_marginal = EmpiricalMarginal(importance.run(observed_data))\n\n    # calculate statistics over posterior samples\n    posterior_mean = emp_marginal.mean\n    posterior_std_dev = emp_marginal.variance.sqrt()\n\n    # report results\n    inferred_mu = posterior_mean.item()\n    inferred_mu_uncertainty = posterior_std_dev.item()\n    print(""the coefficient of friction inferred by pyro is %.3f +- %.3f"" %\n          (inferred_mu, inferred_mu_uncertainty))\n\n    # note that, given the finite step size in the simulator, the simulated descent times will\n    # not precisely match the numbers from the analytic result.\n    # in particular the first two numbers reported below should match each other pretty closely\n    # but will be systematically off from the third number\n    print(""the mean observed descent time in the dataset is: %.4f seconds"" % observed_mean)\n    print(""the (forward) simulated descent time for the inferred (mean) mu is: %.4f seconds"" %\n          simulate(posterior_mean).item())\n    print((""disregarding measurement noise, elementary calculus gives the descent time\\n"" +\n           ""for the inferred (mean) mu as: %.4f seconds"") % analytic_T(posterior_mean.item()))\n\n    """"""\n    ################## EXERCISE ###################\n    # vectorize the computations in this example! #\n    ###############################################\n    """"""\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-samples\', default=500, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/lda.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis example implements amortized Latent Dirichlet Allocation [1],\ndemonstrating how to marginalize out discrete assignment variables in a Pyro\nmodel. This model and inference algorithm treat documents as vectors of\ncategorical variables (vectors of word ids), and collapses word-topic\nassignments using Pyro\'s enumeration. We use PyTorch\'s reparametrized Gamma and\nDirichlet distributions [2], avoiding the need for Laplace approximations as in\n[1]. Following [1] we use the Adam optimizer and clip gradients.\n\n**References:**\n\n[1] Akash Srivastava, Charles Sutton. ICLR 2017.\n    ""Autoencoding Variational Inference for Topic Models""\n    https://arxiv.org/pdf/1703.01488.pdf\n[2] Martin Jankowiak, Fritz Obermeyer. ICML 2018.\n    ""Pathwise gradients beyond the reparametrization trick""\n    https://arxiv.org/pdf/1806.01851.pdf\n""""""\nimport argparse\nimport functools\nimport logging\n\nimport torch\nfrom torch import nn\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO\nfrom pyro.optim import ClippedAdam\n\nlogging.basicConfig(format=\'%(relativeCreated) 9d %(message)s\', level=logging.INFO)\n\n\n# This is a fully generative model of a batch of documents.\n# data is a [num_words_per_doc, num_documents] shaped array of word ids\n# (specifically it is not a histogram). We assume in this simple example\n# that all documents have the same number of words.\ndef model(data=None, args=None, batch_size=None):\n    # Globals.\n    with pyro.plate(""topics"", args.num_topics):\n        topic_weights = pyro.sample(""topic_weights"", dist.Gamma(1. / args.num_topics, 1.))\n        topic_words = pyro.sample(""topic_words"",\n                                  dist.Dirichlet(torch.ones(args.num_words) / args.num_words))\n\n    # Locals.\n    with pyro.plate(""documents"", args.num_docs) as ind:\n        if data is not None:\n            with pyro.util.ignore_jit_warnings():\n                assert data.shape == (args.num_words_per_doc, args.num_docs)\n            data = data[:, ind]\n        doc_topics = pyro.sample(""doc_topics"", dist.Dirichlet(topic_weights))\n        with pyro.plate(""words"", args.num_words_per_doc):\n            # The word_topics variable is marginalized out during inference,\n            # achieved by specifying infer={""enumerate"": ""parallel""} and using\n            # TraceEnum_ELBO for inference. Thus we can ignore this variable in\n            # the guide.\n            word_topics = pyro.sample(""word_topics"", dist.Categorical(doc_topics),\n                                      infer={""enumerate"": ""parallel""})\n            data = pyro.sample(""doc_words"", dist.Categorical(topic_words[word_topics]),\n                               obs=data)\n\n    return topic_weights, topic_words, data\n\n\n# We will use amortized inference of the local topic variables, achieved by a\n# multi-layer perceptron. We\'ll wrap the guide in an nn.Module.\ndef make_predictor(args):\n    layer_sizes = ([args.num_words] +\n                   [int(s) for s in args.layer_sizes.split(\'-\')] +\n                   [args.num_topics])\n    logging.info(\'Creating MLP with sizes {}\'.format(layer_sizes))\n    layers = []\n    for in_size, out_size in zip(layer_sizes, layer_sizes[1:]):\n        layer = nn.Linear(in_size, out_size)\n        layer.weight.data.normal_(0, 0.001)\n        layer.bias.data.normal_(0, 0.001)\n        layers.append(layer)\n        layers.append(nn.Sigmoid())\n    layers.append(nn.Softmax(dim=-1))\n    return nn.Sequential(*layers)\n\n\ndef parametrized_guide(predictor, data, args, batch_size=None):\n    # Use a conjugate guide for global variables.\n    topic_weights_posterior = pyro.param(\n            ""topic_weights_posterior"",\n            lambda: torch.ones(args.num_topics),\n            constraint=constraints.positive)\n    topic_words_posterior = pyro.param(\n            ""topic_words_posterior"",\n            lambda: torch.ones(args.num_topics, args.num_words),\n            constraint=constraints.greater_than(0.5))\n    with pyro.plate(""topics"", args.num_topics):\n        pyro.sample(""topic_weights"", dist.Gamma(topic_weights_posterior, 1.))\n        pyro.sample(""topic_words"", dist.Dirichlet(topic_words_posterior))\n\n    # Use an amortized guide for local variables.\n    pyro.module(""predictor"", predictor)\n    with pyro.plate(""documents"", args.num_docs, batch_size) as ind:\n        data = data[:, ind]\n        # The neural network will operate on histograms rather than word\n        # index vectors, so we\'ll convert the raw data to a histogram.\n        counts = (torch.zeros(args.num_words, ind.size(0))\n                       .scatter_add(0, data, torch.ones(data.shape)))\n        doc_topics = predictor(counts.transpose(0, 1))\n        pyro.sample(""doc_topics"", dist.Delta(doc_topics, event_dim=1))\n\n\ndef main(args):\n    logging.info(\'Generating data\')\n    pyro.set_rng_seed(0)\n    pyro.clear_param_store()\n    pyro.enable_validation(__debug__)\n\n    # We can generate synthetic data directly by calling the model.\n    true_topic_weights, true_topic_words, data = model(args=args)\n\n    # We\'ll train using SVI.\n    logging.info(\'-\' * 40)\n    logging.info(\'Training on {} documents\'.format(args.num_docs))\n    predictor = make_predictor(args)\n    guide = functools.partial(parametrized_guide, predictor)\n    Elbo = JitTraceEnum_ELBO if args.jit else TraceEnum_ELBO\n    elbo = Elbo(max_plate_nesting=2)\n    optim = ClippedAdam({\'lr\': args.learning_rate})\n    svi = SVI(model, guide, optim, elbo)\n    logging.info(\'Step\\tLoss\')\n    for step in range(args.num_steps):\n        loss = svi.step(data, args=args, batch_size=args.batch_size)\n        if step % 10 == 0:\n            logging.info(\'{: >5d}\\t{}\'.format(step, loss))\n    loss = elbo.loss(model, guide, data, args=args)\n    logging.info(\'final loss = {}\'.format(loss))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""Amortized Latent Dirichlet Allocation"")\n    parser.add_argument(""-t"", ""--num-topics"", default=8, type=int)\n    parser.add_argument(""-w"", ""--num-words"", default=1024, type=int)\n    parser.add_argument(""-d"", ""--num-docs"", default=1000, type=int)\n    parser.add_argument(""-wd"", ""--num-words-per-doc"", default=64, type=int)\n    parser.add_argument(""-n"", ""--num-steps"", default=1000, type=int)\n    parser.add_argument(""-l"", ""--layer-sizes"", default=""100-100"")\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.01, type=float)\n    parser.add_argument(""-b"", ""--batch-size"", default=32, type=int)\n    parser.add_argument(\'--jit\', action=\'store_true\')\n    args = parser.parse_args()\n    main(args)\n'"
examples/lkj.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.infer.mcmc import NUTS\n\n""""""\nThis simple example is intended to demonstrate how to use an LKJ prior with\na multivariate distribution.\n\nIt generates entirely random, uncorrelated data, and then attempts to fit a correlation matrix\nand vector of variances.\n""""""\n\n\ndef model(y):\n    d = y.shape[1]\n    N = y.shape[0]\n    options = dict(dtype=y.dtype, device=y.device)\n    # Vector of variances for each of the d variables\n    theta = pyro.sample(""theta"", dist.HalfCauchy(torch.ones(d, **options)))\n    # Lower cholesky factor of a correlation matrix\n    eta = torch.ones(1, **options)  # Implies a uniform distribution over correlation matrices\n    L_omega = pyro.sample(""L_omega"", dist.LKJCorrCholesky(d, eta))\n    # Lower cholesky factor of the covariance matrix\n    L_Omega = torch.mm(torch.diag(theta.sqrt()), L_omega)\n    # For inference with SVI, one might prefer to use torch.bmm(theta.sqrt().diag_embed(), L_omega)\n\n    # Vector of expectations\n    mu = torch.zeros(d, **options)\n\n    with pyro.plate(""observations"", N):\n        obs = pyro.sample(""obs"", dist.MultivariateNormal(mu, scale_tril=L_Omega), obs=y)\n    return obs\n\n\ndef main(args):\n    y = torch.randn(args.n, args.num_variables).to(dtype=torch.double)\n    if args.cuda:\n        y = y.cuda()\n    nuts_kernel = NUTS(model, jit_compile=False, step_size=1e-5)\n    MCMC(nuts_kernel, num_samples=args.num_samples,\n         warmup_steps=args.warmup_steps, num_chains=args.num_chains).run(y)\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""Demonstrate the use of an LKJ Prior"")\n    parser.add_argument(""--num-samples"", nargs=""?"", default=200, type=int)\n    parser.add_argument(""--n"", nargs=""?"", default=500, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=4, type=int)\n    parser.add_argument(""--num-variables"", nargs=\'?\', default=5, type=int)\n    parser.add_argument(""--warmup-steps"", nargs=\'?\', default=100, type=int)\n    parser.add_argument(""--rng_seed"", nargs=\'?\', default=0, type=int)\n    parser.add_argument(""--cuda"", action=""store_true"", default=False)\n    args = parser.parse_args()\n\n    pyro.set_rng_seed(args.rng_seed)\n    # Enable validation checks\n    pyro.enable_validation(__debug__)\n\n    # work around with the error ""RuntimeError: received 0 items of ancdata""\n    # see https://discuss.pytorch.org/t/received-0-items-of-ancdata-pytorch-0-4-0/19823\n    torch.multiprocessing.set_sharing_strategy(""file_system"")\n\n    main(args)\n'"
examples/minipyro.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis example demonstrates the functionality of `pyro.contrib.minipyro`,\nwhich is a minimal implementation of the Pyro Probabilistic Programming\nLanguage that was created for didactic purposes.\n""""""\n\nimport argparse\n\nimport torch\n\nfrom pyro.generic import distributions as dist\n# We use the pyro.generic interface to support dynamic choice of backend.\nfrom pyro.generic import infer, ops, optim, pyro, pyro_backend\n\n\ndef main(args):\n    # Define a basic model with a single Normal latent random variable `loc`\n    # and a batch of Normally distributed observations.\n    def model(data):\n        loc = pyro.sample(""loc"", dist.Normal(0., 1.))\n        with pyro.plate(""data"", len(data), dim=-1):\n            pyro.sample(""obs"", dist.Normal(loc, 1.), obs=data)\n\n    # Define a guide (i.e. variational distribution) with a Normal\n    # distribution over the latent random variable `loc`.\n    def guide(data):\n        guide_loc = pyro.param(""guide_loc"", torch.tensor(0.))\n        guide_scale = ops.exp(pyro.param(""guide_scale_log"", torch.tensor(0.)))\n        pyro.sample(""loc"", dist.Normal(guide_loc, guide_scale))\n\n    # Generate some data.\n    torch.manual_seed(0)\n    data = torch.randn(100) + 3.0\n\n    # Because the API in minipyro matches that of Pyro proper,\n    # training code works with generic Pyro implementations.\n    with pyro_backend(args.backend):\n        # Construct an SVI object so we can do variational inference on our\n        # model/guide pair.\n        Elbo = infer.JitTrace_ELBO if args.jit else infer.Trace_ELBO\n        elbo = Elbo()\n        adam = optim.Adam({""lr"": args.learning_rate})\n        svi = infer.SVI(model, guide, adam, elbo)\n\n        # Basic training loop\n        pyro.get_param_store().clear()\n        for step in range(args.num_steps):\n            loss = svi.step(data)\n            if step % 100 == 0:\n                print(""step {} loss = {}"".format(step, loss))\n\n        # Report the final values of the variational parameters\n        # in the guide after training.\n        for name in pyro.get_param_store():\n            value = pyro.param(name)\n            print(""{} = {}"".format(name, value.detach().cpu().numpy()))\n\n        # For this simple (conjugate) model we know the exact posterior. In\n        # particular we know that the variational distribution should be\n        # centered near 3.0. So let\'s check this explicitly.\n        assert (pyro.param(""guide_loc"") - 3.0).abs() < 0.1\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""Mini Pyro demo"")\n    parser.add_argument(""-b"", ""--backend"", default=""minipyro"")\n    parser.add_argument(""-n"", ""--num-steps"", default=1001, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.02, type=float)\n    parser.add_argument(""--jit"", action=""store_true"")\n    args = parser.parse_args()\n    main(args)\n'"
examples/neutra.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis example illustrates the use of `NeuTraReparam` to run neural transport HMC [1]\non a toy model that draws from a banana-shaped bivariate distribution [2]. We first\ntrain an autoguide by using `AutoNormalizingFlow` that learns a transformation from\na simple latent space (isotropic gaussian) to the more complex geometry of the\nposterior. Subsequently, we use `NeuTraReparam` to run HMC and draw samples from this\nsimplified ""warped"" posterior. Finally, we use our learnt transformation to transform\nthese samples back to the original space. For comparison, we also draw samples from\na NeuTra-reparametrized model that uses a much simpler `AutoDiagonalNormal` guide.\n\nReferences:\n----------\n[1] Hoffman, M., Sountsov, P., Dillon, J. V., Langmore, I., Tran, D., and Vasudevan,\n S. Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport.\n arXiv preprint arXiv:1903.03704, 2019.\n[2] Wang Z., Broccardo M., and Song J. Hamiltonian Monte Carlo Methods for Subset\n Simulation in Reliability Analysis. arXiv preprint arXiv:1706.01435, 2018.\n""""""\n\nimport argparse\nimport logging\nimport os\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom matplotlib.gridspec import GridSpec\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import optim, poutine\nfrom pyro.distributions import constraints\nfrom pyro.distributions.transforms import block_autoregressive, iterated\nfrom pyro.infer import MCMC, NUTS, SVI, Trace_ELBO\nfrom pyro.infer.autoguide import AutoDiagonalNormal, AutoNormalizingFlow\nfrom pyro.infer.reparam import NeuTraReparam\n\nlogging.basicConfig(format=\'%(message)s\', level=logging.INFO)\n\n\nclass BananaShaped(dist.TorchDistribution):\n    support = constraints.real_vector\n\n    def __init__(self, a, b, rho=0.9):\n        self.a = a\n        self.b = b\n        self.rho = rho\n        self.mvn = dist.MultivariateNormal(torch.tensor([0., 0.]),\n                                           covariance_matrix=torch.tensor([[1., self.rho], [self.rho, 1.]]))\n        super().__init__(event_shape=(2,))\n\n    def sample(self, sample_shape=()):\n        u = self.mvn.sample(sample_shape)\n        u0, u1 = u[..., 0], u[..., 1]\n        a, b = self.a, self.b\n        x = a * u0\n        y = (u1 / a) + b * (u0 ** 2 + a ** 2)\n        return torch.stack([x, y], -1)\n\n    def log_prob(self, x):\n        x, y = x[..., 0], x[..., 1]\n        a, b = self.a, self.b\n        u0 = x / a\n        u1 = (y - b * (u0 ** 2 + a ** 2)) * a\n        return self.mvn.log_prob(torch.stack([u0, u1], dim=-1))\n\n\ndef model(a, b, rho=0.9):\n    pyro.sample(\'x\', BananaShaped(a, b, rho))\n\n\ndef fit_guide(guide, args):\n    pyro.clear_param_store()\n    adam = optim.Adam({\'lr\': args.learning_rate})\n    svi = SVI(model, guide, adam, Trace_ELBO())\n    for i in range(args.num_steps):\n        loss = svi.step(args.param_a, args.param_b)\n        if i % 500 == 0:\n            logging.info(""[{}]Elbo loss = {:.2f}"".format(i, loss))\n\n\ndef run_hmc(args, model):\n    nuts_kernel = NUTS(model)\n    mcmc = MCMC(nuts_kernel, warmup_steps=args.num_warmup, num_samples=args.num_samples)\n    mcmc.run(args.param_a, args.param_b)\n    mcmc.summary()\n    return mcmc\n\n\ndef main(args):\n    pyro.set_rng_seed(args.rng_seed)\n    fig = plt.figure(figsize=(8, 16), constrained_layout=True)\n    gs = GridSpec(4, 2, figure=fig)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax3 = fig.add_subplot(gs[1, 0])\n    ax4 = fig.add_subplot(gs[1, 1])\n    ax5 = fig.add_subplot(gs[2, 0])\n    ax6 = fig.add_subplot(gs[2, 1])\n    ax7 = fig.add_subplot(gs[3, 0])\n    ax8 = fig.add_subplot(gs[3, 1])\n    xlim = tuple(int(x) for x in args.x_lim.strip().split(\',\'))\n    ylim = tuple(int(x) for x in args.y_lim.strip().split(\',\'))\n    assert len(xlim) == 2\n    assert len(ylim) == 2\n\n    # 1. Plot samples drawn from BananaShaped distribution\n    x1, x2 = torch.meshgrid([torch.linspace(*xlim, 100), torch.linspace(*ylim, 100)])\n    d = BananaShaped(args.param_a, args.param_b)\n    p = torch.exp(d.log_prob(torch.stack([x1, x2], dim=-1)))\n    ax1.contourf(x1, x2, p, cmap=\'OrRd\',)\n    ax1.set(xlabel=\'x0\', ylabel=\'x1\', xlim=xlim, ylim=ylim,\n            title=\'BananaShaped distribution: \\nlog density\')\n\n    # 2. Run vanilla HMC\n    logging.info(\'\\nDrawing samples using vanilla HMC ...\')\n    mcmc = run_hmc(args, model)\n    vanilla_samples = mcmc.get_samples()[\'x\'].cpu().numpy()\n    ax2.contourf(x1, x2, p, cmap=\'OrRd\')\n    ax2.set(xlabel=\'x0\', ylabel=\'x1\', xlim=xlim, ylim=ylim,\n            title=\'Posterior \\n(vanilla HMC)\')\n    sns.kdeplot(vanilla_samples[:, 0], vanilla_samples[:, 1], ax=ax2)\n\n    # 3(a). Fit a diagonal normal autoguide\n    logging.info(\'\\nFitting a DiagNormal autoguide ...\')\n    guide = AutoDiagonalNormal(model, init_scale=0.05)\n    fit_guide(guide, args)\n    with pyro.plate(\'N\', args.num_samples):\n        guide_samples = guide()[\'x\'].detach().cpu().numpy()\n\n    ax3.contourf(x1, x2, p, cmap=\'OrRd\')\n    ax3.set(xlabel=\'x0\', ylabel=\'x1\', xlim=xlim, ylim=ylim,\n            title=\'Posterior \\n(DiagNormal autoguide)\')\n    sns.kdeplot(guide_samples[:, 0], guide_samples[:, 1], ax=ax3)\n\n    # 3(b). Draw samples using NeuTra HMC\n    logging.info(\'\\nDrawing samples using DiagNormal autoguide + NeuTra HMC ...\')\n    neutra = NeuTraReparam(guide.requires_grad_(False))\n    neutra_model = poutine.reparam(model, config=lambda _: neutra)\n    mcmc = run_hmc(args, neutra_model)\n    zs = mcmc.get_samples()[\'x_shared_latent\']\n    sns.scatterplot(zs[:, 0], zs[:, 1], alpha=0.2, ax=ax4)\n    ax4.set(xlabel=\'x0\', ylabel=\'x1\',\n            title=\'Posterior (warped) samples \\n(DiagNormal + NeuTra HMC)\')\n\n    samples = neutra.transform_sample(zs)\n    samples = samples[\'x\'].cpu().numpy()\n    ax5.contourf(x1, x2, p, cmap=\'OrRd\')\n    ax5.set(xlabel=\'x0\', ylabel=\'x1\', xlim=xlim, ylim=ylim,\n            title=\'Posterior (transformed) \\n(DiagNormal + NeuTra HMC)\')\n    sns.kdeplot(samples[:, 0], samples[:, 1], ax=ax5)\n\n    # 4(a). Fit a BNAF autoguide\n    logging.info(\'\\nFitting a BNAF autoguide ...\')\n    guide = AutoNormalizingFlow(model, partial(iterated, args.num_flows, block_autoregressive))\n    fit_guide(guide, args)\n    with pyro.plate(\'N\', args.num_samples):\n        guide_samples = guide()[\'x\'].detach().cpu().numpy()\n\n    ax6.contourf(x1, x2, p, cmap=\'OrRd\')\n    ax6.set(xlabel=\'x0\', ylabel=\'x1\', xlim=xlim, ylim=ylim,\n            title=\'Posterior \\n(BNAF autoguide)\')\n    sns.kdeplot(guide_samples[:, 0], guide_samples[:, 1], ax=ax6)\n\n    # 4(b). Draw samples using NeuTra HMC\n    logging.info(\'\\nDrawing samples using BNAF autoguide + NeuTra HMC ...\')\n    neutra = NeuTraReparam(guide.requires_grad_(False))\n    neutra_model = poutine.reparam(model, config=lambda _: neutra)\n    mcmc = run_hmc(args, neutra_model)\n    zs = mcmc.get_samples()[\'x_shared_latent\']\n    sns.scatterplot(zs[:, 0], zs[:, 1], alpha=0.2, ax=ax7)\n    ax7.set(xlabel=\'x0\', ylabel=\'x1\', title=\'Posterior (warped) samples \\n(BNAF + NeuTra HMC)\')\n\n    samples = neutra.transform_sample(zs)\n    samples = samples[\'x\'].cpu().numpy()\n    ax8.contourf(x1, x2, p, cmap=\'OrRd\')\n    ax8.set(xlabel=\'x0\', ylabel=\'x1\', xlim=xlim, ylim=ylim,\n            title=\'Posterior (transformed) \\n(BNAF + NeuTra HMC)\')\n    sns.kdeplot(samples[:, 0], samples[:, 1], ax=ax8)\n\n    plt.savefig(os.path.join(os.path.dirname(__file__), \'neutra.pdf\'))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=\'Example illustrating NeuTra Reparametrizer\')\n    parser.add_argument(\'-n\', \'--num-steps\', default=10000, type=int,\n                        help=\'number of SVI steps\')\n    parser.add_argument(\'-lr\', \'--learning-rate\', default=1e-2, type=float,\n                        help=\'learning rate for the Adam optimizer\')\n    parser.add_argument(\'--rng-seed\', default=1, type=int,\n                        help=\'RNG seed\')\n    parser.add_argument(\'--num-warmup\', default=500, type=int,\n                        help=\'number of warmup steps for NUTS\')\n    parser.add_argument(\'--num-samples\', default=1000, type=int,\n                        help=\'number of samples to be drawn from NUTS\')\n    parser.add_argument(\'--param-a\', default=1.15, type=float,\n                        help=\'parameter `a` of BananaShaped distribution\')\n    parser.add_argument(\'--param-b\', default=1., type=float,\n                        help=\'parameter `b` of BananaShaped distribution\')\n    parser.add_argument(\'--num-flows\', default=1, type=int,\n                        help=\'number of flows in the BNAF autoguide\')\n    parser.add_argument(\'--x-lim\', default=\'-3,3\', type=str,\n                        help=\'x limits for the plots\')\n    parser.add_argument(\'--y-lim\', default=\'0,8\', type=str,\n                        help=\'y limits for the plots\')\n\n    args = parser.parse_args()\n    main(args)\n'"
examples/sir_hmc.py,28,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# Introduction\n# ============\n#\n# This advanced Pyro tutorial demonstrates a number of inference and prediction\n# tricks in the context of epidemiological models, specifically stochastic\n# discrete time compartmental models with large discrete state spaces. This\n# tutorial assumes the reader has completed all introductory tutorials and\n# additionally the tutorials on enumeration and effect handlers (poutines):\n# http://pyro.ai/examples/enumeration.html\n# http://pyro.ai/examples/effect_handlers.html\n\nimport argparse\nimport logging\nimport math\nimport re\nfrom collections import OrderedDict\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.distributions.hmm\nimport pyro.poutine as poutine\nfrom pyro.infer import MCMC, NUTS, config_enumerate, infer_discrete\nfrom pyro.infer.autoguide import init_to_value\nfrom pyro.ops.special import safe_log\nfrom pyro.ops.tensor_utils import convolve\nfrom pyro.util import warn_if_nan\n\nlogging.basicConfig(format=\'%(message)s\', level=logging.INFO)\n\n\n# A Discrete SIR Model\n# ====================\n#\n# Let\'s consider one of the simplest compartmental models: an SIR model\n# https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model\n# This models the dynamics of three groups within a population:\n#\n#   population = Susceptible + Infected + Recovered\n#\n# At each discrete time step, each infected person infects a random number of\n# susceptible people, and then randomly may recover. We noisily observe the\n# number of people newly infected at each time step, assuming an unknown false\n# negative rate, but no false positives. Our eventual objective is to estimate\n# global model parameters R0 (the basic reproduction number), tau (the expected\n# recovery time), and rho (the mean response rate = 1 - false negative rate).\n# Having estimated these we will then estimate latent time series and forecast\n# forward.\n#\n# We\'ll start by defining a discrete_model that uses a helper global_model to\n# sample global parameters.\n#\n# Note we need to use ExtendedBinomial rather than Binomial because the data\n# may lie outside of the predicted support. For these values,\n# Binomial.log_prob() will error, whereas ExtendedBinomial.log_prob() will\n# return -inf.\n\ndef global_model(population):\n    tau = args.recovery_time  # Assume this can be measured exactly.\n    R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n    rho = pyro.sample(""rho"", dist.Uniform(0, 1))\n\n    # Convert interpretable parameters to distribution parameters.\n    rate_s = -R0 / (tau * population)\n    prob_i = 1 / (1 + tau)\n\n    return rate_s, prob_i, rho\n\n\ndef discrete_model(args, data):\n    # Sample global parameters.\n    rate_s, prob_i, rho = global_model(args.population)\n\n    # Sequentially sample time-local variables.\n    S = torch.tensor(args.population - 1.)\n    I = torch.tensor(1.)\n    for t, datum in enumerate(data):\n        S2I = pyro.sample(""S2I_{}"".format(t),\n                          dist.Binomial(S, -(rate_s * I).expm1()))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          dist.Binomial(I, prob_i))\n        S = pyro.deterministic(""S_{}"".format(t), S - S2I)\n        I = pyro.deterministic(""I_{}"".format(t), I + S2I - I2R)\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2I, rho),\n                    obs=datum)\n\n\n# We can use this model to simulate data. We\'ll use poutine.condition to pin\n# parameter values and poutine.trace to record sample observations.\n\ndef generate_data(args):\n    logging.info(""Generating data..."")\n    params = {""R0"": torch.tensor(args.basic_reproduction_number),\n              ""rho"": torch.tensor(args.response_rate)}\n    empty_data = [None] * (args.duration + args.forecast)\n\n    # We\'ll retry until we get an actual outbreak.\n    for attempt in range(100):\n        with poutine.trace() as tr:\n            with poutine.condition(data=params):\n                discrete_model(args, empty_data)\n\n        # Concatenate sequential time series into tensors.\n        obs = torch.stack([site[""value""]\n                           for name, site in tr.trace.nodes.items()\n                           if re.match(""obs_[0-9]+"", name)])\n        S2I = torch.stack([site[""value""]\n                          for name, site in tr.trace.nodes.items()\n                          if re.match(""S2I_[0-9]+"", name)])\n        assert len(obs) == len(empty_data)\n\n        obs_sum = int(obs[:args.duration].sum())\n        S2I_sum = int(S2I[:args.duration].sum())\n        if obs_sum >= args.min_observations:\n            logging.info(""Observed {:d}/{:d} infections:\\n{}"".format(\n                obs_sum, S2I_sum, "" "".join([str(int(x)) for x in obs[:args.duration]])))\n            return {""S2I"": S2I, ""obs"": obs}\n\n    raise ValueError(""Failed to generate {} observations. Try increasing ""\n                     ""--population or decreasing --min-observations""\n                     .format(args.min_observations))\n\n\n# Inference\n# =========\n#\n# While the above discrete_model is easy to understand, its discrete latent\n# variables pose a challenge for inference. One of the most popular inference\n# strategies for such models is Sequential Monte Carlo. However since Pyro and\n# PyTorch are stronger in gradient based vectorizable inference algorithms, we\n# will instead pursue inference based on Hamiltonian Monte Carlo (HMC).\n#\n# Our general inference strategy will be to:\n# 1. Introduce auxiliary variables to make the model Markov.\n# 2. Introduce more auxiliary variables to create a discrete parameterization.\n# 3. Marginalize out all remaining discrete latent variables.\n# 4. Vectorize to enable parallel-scan temporal filtering.\n#\n# Let\'s consider reparameterizing in terms of the variables (S, I) rather than\n# (S2I, I2R). Since these may lead to inconsistent states, we need to replace\n# the Binomial transition factors (S2I, I2R) with ExtendedBinomial.\n#\n# The following model is equivalent to the discrete_model:\n\n@config_enumerate\ndef reparameterized_discrete_model(args, data):\n    # Sample global parameters.\n    rate_s, prob_i, rho = global_model(args.population)\n\n    # Sequentially sample time-local variables.\n    S_curr = torch.tensor(args.population - 1.)\n    I_curr = torch.tensor(1.)\n    for t, datum in enumerate(data):\n        # Sample reparameterizing variables.\n        # When reparameterizing to a factor graph, we ignored density via\n        # .mask(False). Thus distributions are used only for initialization.\n        S_prev, I_prev = S_curr, I_curr\n        S_curr = pyro.sample(""S_{}"".format(t),\n                             dist.Binomial(args.population, 0.5).mask(False))\n        I_curr = pyro.sample(""I_{}"".format(t),\n                             dist.Binomial(args.population, 0.5).mask(False))\n\n        # Now we reverse the computation.\n        S2I = S_prev - S_curr\n        I2R = I_prev - I_curr + S2I\n        pyro.sample(""S2I_{}"".format(t),\n                    dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()),\n                    obs=S2I)\n        pyro.sample(""I2R_{}"".format(t),\n                    dist.ExtendedBinomial(I_prev, prob_i),\n                    obs=I2R)\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2I, rho),\n                    obs=datum)\n\n\n# By reparameterizing, we have converted to coordinates that make the model\n# Markov. We have also replaced dynamic integer_interval constraints with\n# easier static integer_interval constraints (although we\'ll still need good\n# initialization to avoid NANs). Since the discrete latent variables are\n# bounded (by population size), we can enumerate out discrete latent variables\n# and perform HMC inference over the global latents. However enumeration\n# complexity is O(population^4), so this is only feasible for very small\n# populations.\n#\n# Here is an inference approach using an MCMC sampler.\n\ndef infer_hmc_enum(args, data):\n    model = reparameterized_discrete_model\n    return _infer_hmc(args, data, model)\n\n\ndef _infer_hmc(args, data, model, init_values={}):\n    logging.info(""Running inference..."")\n    kernel = NUTS(model,\n                  full_mass=[(""R0"", ""rho"")],\n                  max_tree_depth=args.max_tree_depth,\n                  init_strategy=init_to_value(values=init_values),\n                  jit_compile=args.jit, ignore_jit_warnings=True)\n\n    # We\'ll define a hook_fn to log potential energy values during inference.\n    # This is helpful to diagnose whether the chain is mixing.\n    energies = []\n\n    def hook_fn(kernel, *unused):\n        e = float(kernel._potential_energy_last)\n        energies.append(e)\n        if args.verbose:\n            logging.info(""potential = {:0.6g}"".format(e))\n\n    mcmc = MCMC(kernel, hook_fn=hook_fn,\n                num_samples=args.num_samples,\n                warmup_steps=args.warmup_steps)\n    mcmc.run(args, data)\n    mcmc.summary()\n    if args.plot:\n        import matplotlib.pyplot as plt\n        plt.figure(figsize=(6, 3))\n        plt.plot(energies)\n        plt.xlabel(""MCMC step"")\n        plt.ylabel(""potential energy"")\n        plt.title(""MCMC energy trace"")\n        plt.tight_layout()\n\n    samples = mcmc.get_samples()\n    return samples\n\n\n# To scale to large populations, we\'ll continue to reparameterize, this time\n# replacing each of (S_aux,I_aux) with a combination of a bounded real\n# variable and a Categorical variable with only four values.\n#\n# This is the crux: we can now perform HMC over the real variable and\n# marginalize out the Categorical variables using variable elimination.\n#\n# We first define a helper to create enumerated Categorical sites.\n\ndef quantize(name, x_real, min, max):\n    """"""\n    Randomly quantize in a way that preserves probability mass.\n    We use a piecewise polynomial spline of order 3.\n    """"""\n    assert min < max\n    lb = x_real.detach().floor()\n\n    # This cubic spline interpolates over the nearest four integers, ensuring\n    # piecewise quadratic gradients.\n    s = x_real - lb\n    ss = s * s\n    t = 1 - s\n    tt = t * t\n    probs = torch.stack([\n        t * tt,\n        4 + ss * (3 * s - 6),\n        4 + tt * (3 * t - 6),\n        s * ss,\n    ], dim=-1) * (1/6)\n    q = pyro.sample(""Q_"" + name, dist.Categorical(probs)).type_as(x_real)\n\n    x = lb + q - 1\n    x = torch.max(x, 2 * min - 1 - x)\n    x = torch.min(x, 2 * max + 1 - x)\n\n    return pyro.deterministic(name, x)\n\n\n# Now we can define another equivalent model.\n\n@config_enumerate\ndef continuous_model(args, data):\n    # Sample global parameters.\n    rate_s, prob_i, rho = global_model(args.population)\n\n    # Sample reparameterizing variables.\n    S_aux = pyro.sample(""S_aux"",\n                        dist.Uniform(-0.5, args.population + 0.5)\n                            .mask(False).expand(data.shape).to_event(1))\n    I_aux = pyro.sample(""I_aux"",\n                        dist.Uniform(-0.5, args.population + 0.5)\n                            .mask(False).expand(data.shape).to_event(1))\n\n    # Sequentially sample time-local variables.\n    S_curr = torch.tensor(args.population - 1.)\n    I_curr = torch.tensor(1.)\n    for t, datum in poutine.markov(enumerate(data)):\n        S_prev, I_prev = S_curr, I_curr\n        S_curr = quantize(""S_{}"".format(t), S_aux[..., t], min=0, max=args.population)\n        I_curr = quantize(""I_{}"".format(t), I_aux[..., t], min=0, max=args.population)\n\n        # Now we reverse the computation.\n        S2I = S_prev - S_curr\n        I2R = I_prev - I_curr + S2I\n        pyro.sample(""S2I_{}"".format(t),\n                    dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()),\n                    obs=S2I)\n        pyro.sample(""I2R_{}"".format(t),\n                    dist.ExtendedBinomial(I_prev, prob_i),\n                    obs=I2R)\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2I, rho),\n                    obs=datum)\n\n\n# Now all latent variables in the continuous_model are either continuous or\n# enumerated, so we can use HMC. However we need to take special care with\n# constraints because the above Markov reparameterization covers regions of\n# hypothesis space that are infeasible (i.e. whose log_prob is -infinity). We\n# thus heuristically initialize to a feasible point.\n\ndef heuristic_init(args, data):\n    """"""Heuristically initialize to a feasible point.""""""\n    # Start with a single infection.\n    S0 = args.population - 1\n    # Assume 50% <= response rate <= 100%.\n    S2I = data * min(2., (S0 / data.sum()).sqrt())\n    S_aux = (S0 - S2I.cumsum(-1)).clamp(min=0.5)\n    # Account for the single initial infection.\n    S2I[0] += 1\n    # Assume infection lasts less than a month.\n    recovery = torch.arange(30.).div(args.recovery_time).neg().exp()\n    I_aux = convolve(S2I, recovery)[:len(data)].clamp(min=0.5)\n\n    return {\n        ""R0"": torch.tensor(2.0),\n        ""rho"": torch.tensor(0.5),\n        ""S_aux"": S_aux,\n        ""I_aux"": I_aux,\n    }\n\n\ndef infer_hmc_cont(model, args, data):\n    init_values = heuristic_init(args, data)\n    return _infer_hmc(args, data, model, init_values=init_values)\n\n\n# Our final inference trick is to vectorize. We can repurpose DiscreteHMM\'s\n# implementation here, but we\'ll need to manually represent a Markov\n# neighborhood of multiple Categorical of size 4 as single joint Categorical\n# with 4 * 4 = 16 states, and then manually perform variable elimination (the\n# factors here don\'t quite conform to DiscreteHMM\'s interface).\n\ndef quantize_enumerate(x_real, min, max):\n    """"""\n    Randomly quantize in a way that preserves probability mass.\n    We use a piecewise polynomial spline of order 3.\n    """"""\n    assert min < max\n    lb = x_real.detach().floor()\n\n    # This cubic spline interpolates over the nearest four integers, ensuring\n    # piecewise quadratic gradients.\n    s = x_real - lb\n    ss = s * s\n    t = 1 - s\n    tt = t * t\n    probs = torch.stack([\n        t * tt,\n        4 + ss * (3 * s - 6),\n        4 + tt * (3 * t - 6),\n        s * ss,\n    ], dim=-1) * (1/6)\n    logits = safe_log(probs)\n    q = torch.arange(-1., 3.)\n\n    x = lb.unsqueeze(-1) + q\n    x = torch.max(x, 2 * min - 1 - x)\n    x = torch.min(x, 2 * max + 1 - x)\n    return x, logits\n\n\ndef vectorized_model(args, data):\n    # Sample global parameters.\n    rate_s, prob_i, rho = global_model(args.population)\n\n    # Sample reparameterizing variables.\n    S_aux = pyro.sample(""S_aux"",\n                        dist.Uniform(-0.5, args.population + 0.5)\n                            .mask(False).expand(data.shape).to_event(1))\n    I_aux = pyro.sample(""I_aux"",\n                        dist.Uniform(-0.5, args.population + 0.5)\n                            .mask(False).expand(data.shape).to_event(1))\n\n    # Manually enumerate.\n    S_curr, S_logp = quantize_enumerate(S_aux, min=0, max=args.population)\n    I_curr, I_logp = quantize_enumerate(I_aux, min=0, max=args.population)\n    # Truncate final value from the right then pad initial value onto the left.\n    S_prev = torch.nn.functional.pad(S_curr[:-1], (0, 0, 1, 0), value=args.population - 1)\n    I_prev = torch.nn.functional.pad(I_curr[:-1], (0, 0, 1, 0), value=1)\n    # Reshape to support broadcasting, similar to EnumMessenger.\n    T = len(data)\n    Q = 4\n    S_prev = S_prev.reshape(T, Q, 1, 1, 1)\n    I_prev = I_prev.reshape(T, 1, Q, 1, 1)\n    S_curr = S_curr.reshape(T, 1, 1, Q, 1)\n    S_logp = S_logp.reshape(T, 1, 1, Q, 1)\n    I_curr = I_curr.reshape(T, 1, 1, 1, Q)\n    I_logp = I_logp.reshape(T, 1, 1, 1, Q)\n    data = data.reshape(T, 1, 1, 1, 1)\n\n    # Reverse the S2I,I2R computation.\n    S2I = S_prev - S_curr\n    I2R = I_prev - I_curr + S2I\n\n    # Compute probability factors.\n    S2I_logp = dist.ExtendedBinomial(S_prev, -(rate_s * I_prev).expm1()).log_prob(S2I)\n    I2R_logp = dist.ExtendedBinomial(I_prev, prob_i).log_prob(I2R)\n    obs_logp = dist.ExtendedBinomial(S2I, rho).log_prob(data)\n\n    # Manually perform variable elimination.\n    logp = S_logp + (I_logp + obs_logp) + S2I_logp + I2R_logp\n    logp = logp.reshape(-1, Q * Q, Q * Q)\n    logp = pyro.distributions.hmm._sequential_logmatmulexp(logp)\n    logp = logp.reshape(-1).logsumexp(0)\n    logp = logp - math.log(4)  # Account for S,I initial distributions.\n    warn_if_nan(logp)\n    pyro.factor(""obs"", logp)\n\n\n# We can fit vectorized_model exactly as we fit the original continuous_model,\n# using our infer_hmc_cont helper. The vectorized model is more than an order\n# of magnitude faster than the sequential version, and scales logarithmically\n# in time (up to your machine\'s parallelism).\n#\n# After inference we have samples of all latent variables. Let\'s define a\n# helper to examine the inferred posterior distributions.\n\ndef evaluate(args, samples):\n    # Print estimated values.\n    names = {""basic_reproduction_number"": ""R0"",\n             ""response_rate"": ""rho""}\n    for name, key in names.items():\n        mean = samples[key].mean().item()\n        std = samples[key].std().item()\n        logging.info(""{}: truth = {:0.3g}, estimate = {:0.3g} \\u00B1 {:0.3g}""\n                     .format(key, getattr(args, name), mean, std))\n\n    # Optionally plot histograms.\n    if args.plot:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n        fig, axes = plt.subplots(2, 1, figsize=(5, 5))\n        axes[0].set_title(""Posterior parameter estimates"")\n        for ax, (name, key) in zip(axes, names.items()):\n            truth = getattr(args, name)\n            sns.distplot(samples[key], ax=ax, label=""posterior"")\n            ax.axvline(truth, color=""k"", label=""truth"")\n            ax.set_xlabel(key + "" = "" + name.replace(""_"", "" ""))\n            ax.set_yticks(())\n            ax.legend(loc=""best"")\n        plt.tight_layout()\n\n\n# Prediction and Forecasting\n# ==========================\n#\n# So far we\'ve written four models that each describe the same probability\n# distribution. Each successive model made inference cheaper. Next let\'s move\n# beyond inference and consider predicting latent infection rate and\n# forecasting future infections.\n#\n# We\'ll use Pyro\'s effect handlers to combine multiple of the above models,\n# leveraging the vectorized_model for inference, then the continuous_model to\n# compute local latent variables, and finally the original discrete_model to\n# forecast forward in time. Let\'s assume posterior samples have already been\n# generated via infer_hmc_cont(vectorized_model, ...).\n\n@torch.no_grad()\ndef predict(args, data, samples, truth=None):\n    logging.info(""Forecasting {} steps ahead..."".format(args.forecast))\n    particle_plate = pyro.plate(""particles"", args.num_samples, dim=-1)\n\n    # First we sample discrete auxiliary variables from the continuous\n    # variables sampled in vectorized_model. This samples only time steps\n    # [0:duration]. Here infer_discrete runs a forward-filter backward-sample\n    # algorithm. We\'ll add these new samples to the existing dict of samples.\n    model = poutine.condition(continuous_model, samples)\n    model = particle_plate(model)\n    model = infer_discrete(model, first_available_dim=-2)\n    with poutine.trace() as tr:\n        model(args, data)\n    samples = OrderedDict((name, site[""value""])\n                          for name, site in tr.trace.nodes.items()\n                          if site[""type""] == ""sample"")\n\n    # Next we\'ll run the forward generative process in discrete_model. This\n    # samples time steps [duration:duration+forecast]. Again we\'ll update the\n    # dict of samples.\n    extended_data = list(data) + [None] * args.forecast\n    model = poutine.condition(discrete_model, samples)\n    model = particle_plate(model)\n    with poutine.trace() as tr:\n        model(args, extended_data)\n    samples = OrderedDict((name, site[""value""])\n                          for name, site in tr.trace.nodes.items()\n                          if site[""type""] == ""sample"")\n\n    # Finally we\'ll concatenate the sequentially sampled values into contiguous\n    # tensors. This operates on the entire time interval [0:duration+forecast].\n    for key in (""S"", ""I"", ""S2I"", ""I2R""):\n        pattern = key + ""_[0-9]+""\n        series = [value\n                  for name, value in samples.items()\n                  if re.match(pattern, name)]\n        assert len(series) == args.duration + args.forecast\n        series[0] = series[0].expand(series[1].shape)\n        samples[key] = torch.stack(series, dim=-1)\n    S2I = samples[""S2I""]\n    median = S2I.median(dim=0).values\n    logging.info(""Median prediction of new infections (starting on day 0):\\n{}""\n                 .format("" "".join(map(str, map(int, median)))))\n\n    # Optionally plot the latent and forecasted series of new infections.\n    if args.plot:\n        import matplotlib.pyplot as plt\n        plt.figure()\n        time = torch.arange(args.duration + args.forecast)\n        p05 = S2I.kthvalue(int(round(0.5 + 0.05 * args.num_samples)), dim=0).values\n        p95 = S2I.kthvalue(int(round(0.5 + 0.95 * args.num_samples)), dim=0).values\n        plt.fill_between(time, p05, p95, color=""red"", alpha=0.3, label=""90% CI"")\n        plt.plot(time, median, ""r-"", label=""median"")\n        plt.plot(time[:args.duration], data, ""k."", label=""observed"")\n        if truth is not None:\n            plt.plot(time, truth, ""k--"", label=""truth"")\n        plt.axvline(args.duration - 0.5, color=""gray"", lw=1)\n        plt.xlim(0, len(time) - 1)\n        plt.ylim(0, None)\n        plt.xlabel(""day after first infection"")\n        plt.ylabel(""new infections per day"")\n        plt.title(""New infections in population of {}"".format(args.population))\n        plt.legend(loc=""upper left"")\n        plt.tight_layout()\n\n    return samples\n\n\n# Experiments\n# ===========\n#\n# Finally we\'ll define an experiment runner. For example we can simulate 60\n# days of infection on a population of 10000 and forecast forward another 30\n# days, and plot the results as follows (takes about 3 minutes on my laptop):\n#\n# python sir_hmc.py -p 10000 -d 60 -f 30 --plot\n\ndef main(args):\n    pyro.enable_validation(__debug__)\n    pyro.set_rng_seed(args.rng_seed)\n\n    dataset = generate_data(args)\n    obs = dataset[""obs""][:args.duration]\n\n    # Choose among inference methods.\n    if args.enum:\n        samples = infer_hmc_enum(args, obs)\n    elif args.sequential:\n        samples = infer_hmc_cont(continuous_model, args, obs)\n    else:\n        samples = infer_hmc_cont(vectorized_model, args, obs)\n\n    # Evaluate fit.\n    evaluate(args, samples)\n\n    # Predict latent time series.\n    if args.forecast:\n        samples = predict(args, obs, samples, truth=dataset[""S2I""])\n\n    return samples\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""SIR epidemiology modeling using HMC"")\n    parser.add_argument(""-p"", ""--population"", default=10, type=int)\n    parser.add_argument(""-m"", ""--min-observations"", default=3, type=int)\n    parser.add_argument(""-d"", ""--duration"", default=10, type=int)\n    parser.add_argument(""-f"", ""--forecast"", default=0, type=int)\n    parser.add_argument(""-R0"", ""--basic-reproduction-number"", default=1.5, type=float)\n    parser.add_argument(""-tau"", ""--recovery-time"", default=7.0, type=float)\n    parser.add_argument(""-rho"", ""--response-rate"", default=0.5, type=float)\n    parser.add_argument(""-e"", ""--enum"", action=""store_true"",\n                        help=""use the full enumeration model"")\n    parser.add_argument(""-s"", ""--sequential"", action=""store_true"",\n                        help=""use the sequential continuous model"")\n    parser.add_argument(""-n"", ""--num-samples"", default=200, type=int)\n    parser.add_argument(""-w"", ""--warmup-steps"", default=100, type=int)\n    parser.add_argument(""-t"", ""--max-tree-depth"", default=5, type=int)\n    parser.add_argument(""-r"", ""--rng-seed"", default=0, type=int)\n    parser.add_argument(""--double"", action=""store_true"")\n    parser.add_argument(""--jit"", action=""store_true"")\n    parser.add_argument(""--cuda"", action=""store_true"")\n    parser.add_argument(""--verbose"", action=""store_true"")\n    parser.add_argument(""--plot"", action=""store_true"")\n    args = parser.parse_args()\n\n    if args.double:\n        if args.cuda:\n            torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n        else:\n            torch.set_default_tensor_type(torch.DoubleTensor)\n    elif args.cuda:\n        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n    main(args)\n\n    if args.plot:\n        import matplotlib.pyplot as plt\n        plt.show()\n'"
examples/smcfilter.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport logging\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SMCFilter\n\nlogging.basicConfig(format=""%(relativeCreated) 9d %(message)s"", level=logging.INFO)\n\n""""""\nThis file demonstrates how to use the SMCFilter algorithm with\na simple model of a noisy harmonic oscillator of the form:\n\n    z[t] ~ N(A*z[t-1], B*sigma_z)\n    y[t] ~ N(z[t][0], sigma_y)\n\n""""""\n\n\nclass SimpleHarmonicModel:\n\n    def __init__(self, process_noise, measurement_noise):\n        self.A = torch.tensor([[0., 1.],\n                               [-1., 0.]])\n        self.B = torch.tensor([3., 3.])\n        self.sigma_z = torch.tensor(process_noise)\n        self.sigma_y = torch.tensor(measurement_noise)\n\n    def init(self, state, initial):\n        self.t = 0\n        state[""z""] = pyro.sample(""z_init"", dist.Delta(initial, event_dim=1))\n\n    def step(self, state, y=None):\n        self.t += 1\n        state[""z""] = pyro.sample(\n            ""z_{}"".format(self.t),\n            dist.Normal(state[""z""].matmul(self.A), self.B*self.sigma_z).to_event(1))\n        y = pyro.sample(""y_{}"".format(self.t),\n                        dist.Normal(state[""z""][..., 0], self.sigma_y),\n                        obs=y)\n        return state[""z""], y\n\n\nclass SimpleHarmonicModel_Guide:\n\n    def __init__(self, model):\n        self.model = model\n\n    def init(self, state, initial):\n        self.t = 0\n        pyro.sample(""z_init"", dist.Delta(initial, event_dim=1))\n\n    def step(self, state, y=None):\n        self.t += 1\n\n        # Proposal distribution\n        pyro.sample(\n            ""z_{}"".format(self.t),\n            dist.Normal(state[""z""].matmul(self.model.A), torch.tensor([1., 1.])).to_event(1))\n\n\ndef generate_data(args):\n    model = SimpleHarmonicModel(args.process_noise, args.measurement_noise)\n\n    state = {}\n    initial = torch.tensor([1., 0.])\n    model.init(state, initial=initial)\n    zs = [initial]\n    ys = [None]\n    for t in range(args.num_timesteps):\n        z, y = model.step(state)\n        zs.append(z)\n        ys.append(y)\n\n    return zs, ys\n\n\ndef main(args):\n    pyro.set_rng_seed(args.seed)\n    pyro.enable_validation(__debug__)\n\n    model = SimpleHarmonicModel(args.process_noise, args.measurement_noise)\n    guide = SimpleHarmonicModel_Guide(model)\n\n    smc = SMCFilter(model, guide, num_particles=args.num_particles, max_plate_nesting=0)\n\n    logging.info(""Generating data"")\n    zs, ys = generate_data(args)\n\n    logging.info(""Filtering"")\n\n    smc.init(initial=torch.tensor([1., 0.]))\n    for y in ys[1:]:\n        smc.step(y)\n\n    logging.info(""At final time step:"")\n    z = smc.get_empirical()[""z""]\n    logging.info(""truth: {}"".format(zs[-1]))\n    logging.info(""mean: {}"".format(z.mean))\n    logging.info(""std: {}"".format(z.variance ** 0.5))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Simple Harmonic Oscillator w/ SMC Filtering Inference"")\n    parser.add_argument(""-n"", ""--num-timesteps"", default=500, type=int)\n    parser.add_argument(""-p"", ""--num-particles"", default=100, type=int)\n    parser.add_argument(""--process-noise"", default=1., type=float)\n    parser.add_argument(""--measurement-noise"", default=1., type=float)\n    parser.add_argument(""--seed"", default=0, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/sparse_gamma_def.py,11,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n# This is an implementation of the sparse gamma deep exponential family model described in\n# Ranganath, Rajesh, Tang, Linpeng, Charlin, Laurent, and Blei, David. Deep exponential families.\n#\n# To do inference we use one of the following guides:\n# i)   a custom guide (i.e. a hand-designed variational family) or\n# ii)  an \'auto\' guide that is automatically constructed using pyro.infer.autoguide or\n# iii) an \'easy\' guide whose construction is facilitated using pyro.contrib.easyguide.\n#\n# The Olivetti faces dataset is originally from http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n#\n# Compare to Christian Naesseth\'s implementation here:\n# https://github.com/blei-lab/ars-reparameterization/tree/master/sparse%20gamma%20def\n\nimport argparse\nimport errno\nimport os\n\nimport numpy as np\nimport torch\nfrom torch.nn.functional import softplus\n\nimport pyro\nimport pyro.optim as optim\nimport wget\n\nfrom pyro.contrib.examples.util import get_data_directory\nfrom pyro.distributions import Gamma, Poisson, Normal\nfrom pyro.infer import SVI, TraceMeanField_ELBO\nfrom pyro.infer.autoguide import AutoDiagonalNormal\nfrom pyro.infer.autoguide import init_to_feasible\nfrom pyro.contrib.easyguide import EasyGuide\n\n\ntorch.set_default_tensor_type(\'torch.FloatTensor\')\npyro.enable_validation(__debug__)\npyro.util.set_rng_seed(0)\n\n\n# helper for initializing variational parameters\ndef rand_tensor(shape, mean, sigma):\n    return mean * torch.ones(shape) + sigma * torch.randn(shape)\n\n\nclass SparseGammaDEF:\n    def __init__(self):\n        # define the sizes of the layers in the deep exponential family\n        self.top_width = 100\n        self.mid_width = 40\n        self.bottom_width = 15\n        self.image_size = 64 * 64\n        # define hyperparameters that control the prior\n        self.alpha_z = torch.tensor(0.1)\n        self.beta_z = torch.tensor(0.1)\n        self.alpha_w = torch.tensor(0.1)\n        self.beta_w = torch.tensor(0.3)\n        # define parameters used to initialize variational parameters\n        self.alpha_init = 0.5\n        self.mean_init = 0.0\n        self.sigma_init = 0.1\n\n    # define the model\n    def model(self, x):\n        x_size = x.size(0)\n\n        # sample the global weights\n        with pyro.plate(""w_top_plate"", self.top_width * self.mid_width):\n            w_top = pyro.sample(""w_top"", Gamma(self.alpha_w, self.beta_w))\n        with pyro.plate(""w_mid_plate"", self.mid_width * self.bottom_width):\n            w_mid = pyro.sample(""w_mid"", Gamma(self.alpha_w, self.beta_w))\n        with pyro.plate(""w_bottom_plate"", self.bottom_width * self.image_size):\n            w_bottom = pyro.sample(""w_bottom"", Gamma(self.alpha_w, self.beta_w))\n\n        # sample the local latent random variables\n        # (the plate encodes the fact that the z\'s for different datapoints are conditionally independent)\n        with pyro.plate(""data"", x_size):\n            z_top = pyro.sample(""z_top"", Gamma(self.alpha_z, self.beta_z).expand([self.top_width]).to_event(1))\n            # note that we need to use matmul (batch matrix multiplication) as well as appropriate reshaping\n            # to make sure our code is fully vectorized\n            w_top = w_top.reshape(self.top_width, self.mid_width) if w_top.dim() == 1 else \\\n                w_top.reshape(-1, self.top_width, self.mid_width)\n            mean_mid = torch.matmul(z_top, w_top)\n            z_mid = pyro.sample(""z_mid"", Gamma(self.alpha_z, self.beta_z / mean_mid).to_event(1))\n\n            w_mid = w_mid.reshape(self.mid_width, self.bottom_width) if w_mid.dim() == 1 else \\\n                w_mid.reshape(-1, self.mid_width, self.bottom_width)\n            mean_bottom = torch.matmul(z_mid, w_mid)\n            z_bottom = pyro.sample(""z_bottom"", Gamma(self.alpha_z, self.beta_z / mean_bottom).to_event(1))\n\n            w_bottom = w_bottom.reshape(self.bottom_width, self.image_size) if w_bottom.dim() == 1 else \\\n                w_bottom.reshape(-1, self.bottom_width, self.image_size)\n            mean_obs = torch.matmul(z_bottom, w_bottom)\n\n            # observe the data using a poisson likelihood\n            pyro.sample(\'obs\', Poisson(mean_obs).to_event(1), obs=x)\n\n    # define our custom guide a.k.a. variational distribution.\n    # (note the guide is mean field gamma)\n    def guide(self, x):\n        x_size = x.size(0)\n\n        # define a helper function to sample z\'s for a single layer\n        def sample_zs(name, width):\n            alpha_z_q = pyro.param(""alpha_z_q_%s"" % name,\n                                   lambda: rand_tensor((x_size, width), self.alpha_init, self.sigma_init))\n            mean_z_q = pyro.param(""mean_z_q_%s"" % name,\n                                  lambda: rand_tensor((x_size, width), self.mean_init, self.sigma_init))\n            alpha_z_q, mean_z_q = softplus(alpha_z_q), softplus(mean_z_q)\n            pyro.sample(""z_%s"" % name, Gamma(alpha_z_q, alpha_z_q / mean_z_q).to_event(1))\n\n        # define a helper function to sample w\'s for a single layer\n        def sample_ws(name, width):\n            alpha_w_q = pyro.param(""alpha_w_q_%s"" % name,\n                                   lambda: rand_tensor((width), self.alpha_init, self.sigma_init))\n            mean_w_q = pyro.param(""mean_w_q_%s"" % name,\n                                  lambda: rand_tensor((width), self.mean_init, self.sigma_init))\n            alpha_w_q, mean_w_q = softplus(alpha_w_q), softplus(mean_w_q)\n            pyro.sample(""w_%s"" % name, Gamma(alpha_w_q, alpha_w_q / mean_w_q))\n\n        # sample the global weights\n        with pyro.plate(""w_top_plate"", self.top_width * self.mid_width):\n            sample_ws(""top"", self.top_width * self.mid_width)\n        with pyro.plate(""w_mid_plate"", self.mid_width * self.bottom_width):\n            sample_ws(""mid"", self.mid_width * self.bottom_width)\n        with pyro.plate(""w_bottom_plate"", self.bottom_width * self.image_size):\n            sample_ws(""bottom"", self.bottom_width * self.image_size)\n\n        # sample the local latent random variables\n        with pyro.plate(""data"", x_size):\n            sample_zs(""top"", self.top_width)\n            sample_zs(""mid"", self.mid_width)\n            sample_zs(""bottom"", self.bottom_width)\n\n\n# define a helper function to clip parameters defining the custom guide.\n# (this is to avoid regions of the gamma distributions with extremely small means)\ndef clip_params():\n    for param, clip in zip((""alpha"", ""mean""), (-2.5, -4.5)):\n        for layer in [""_q_top"", ""_q_mid"", ""_q_bottom""]:\n            for wz in [""_w"", ""_z""]:\n                pyro.param(param + wz + layer).data.clamp_(min=clip)\n\n\n# Define a guide using the EasyGuide class.\n# Unlike the \'auto\' guide, this guide supports data subsampling.\n# This is the best performing of the three guides.\n#\n# This guide is functionally similar to the auto guide, but performs\n# somewhat better. The reason seems to be some combination of: i) the better\n# numerical stability of the softplus; and ii) the custom initialization.\n# Note however that for both the easy guide and auto guide KL divergences\n# are not computed analytically in the ELBO because the ELBO thinks the\n# mean-field condition is not satisfied, which leads to higher variance gradients.\nclass MyEasyGuide(EasyGuide):\n    def guide(self, x):\n        # group all the latent weights into one large latent variable\n        global_group = self.group(match=""w_.*"")\n        global_mean = pyro.param(""w_mean"",\n                                 lambda: rand_tensor(global_group.event_shape, 0.5, 0.1))\n        global_scale = softplus(pyro.param(""w_scale"",\n                                lambda: rand_tensor(global_group.event_shape, 0.0, 0.1)))\n        # use a mean field Normal distribution on all the ws\n        global_group.sample(""ws"", Normal(global_mean, global_scale).to_event(1))\n\n        # group all the latent zs into one large latent variable\n        local_group = self.group(match=""z_.*"")\n        x_shape = x.shape[:1] + local_group.event_shape\n\n        with self.plate(""data"", x.size(0)):\n            local_mean = pyro.param(""z_mean"",\n                                    lambda: rand_tensor(x_shape, 0.5, 0.1))\n            local_scale = softplus(pyro.param(""z_scale"",\n                                   lambda: rand_tensor(x_shape, 0.0, 0.1)))\n            # use a mean field Normal distribution on all the zs\n            local_group.sample(""zs"", Normal(local_mean, local_scale).to_event(1))\n\n\ndef main(args):\n    # load data\n    print(\'loading training data...\')\n    dataset_directory = get_data_directory(__file__)\n    dataset_path = os.path.join(dataset_directory, \'faces_training.csv\')\n    if not os.path.exists(dataset_path):\n        try:\n            os.makedirs(dataset_directory)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n            pass\n        wget.download(\'https://d2hg8soec8ck9v.cloudfront.net/datasets/faces_training.csv\', dataset_path)\n    data = torch.tensor(np.loadtxt(dataset_path, delimiter=\',\')).float()\n\n    sparse_gamma_def = SparseGammaDEF()\n\n    # Due to the special logic in the custom guide (e.g. parameter clipping), the custom guide\n    # seems to be more amenable to higher learning rates.\n    # Nevertheless, the easy guide performs the best (presumably because of numerical instabilities\n    # related to the gamma distribution in the custom guide).\n    learning_rate = 0.2 if args.guide in [\'auto\', \'easy\'] else 4.5\n    momentum = 0.05 if args.guide in [\'auto\', \'easy\'] else 0.1\n    opt = optim.AdagradRMSProp({""eta"": learning_rate, ""t"": momentum})\n\n    # use one of our three different guide types\n    if args.guide == \'auto\':\n        guide = AutoDiagonalNormal(sparse_gamma_def.model, init_loc_fn=init_to_feasible)\n    elif args.guide == \'easy\':\n        guide = MyEasyGuide(sparse_gamma_def.model)\n    else:\n        guide = sparse_gamma_def.guide\n\n    # this is the svi object we use during training; we use TraceMeanField_ELBO to\n    # get analytic KL divergences\n    svi = SVI(sparse_gamma_def.model, guide, opt, loss=TraceMeanField_ELBO())\n\n    # we use svi_eval during evaluation; since we took care to write down our model in\n    # a fully vectorized way, this computation can be done efficiently with large tensor ops\n    svi_eval = SVI(sparse_gamma_def.model, guide, opt,\n                   loss=TraceMeanField_ELBO(num_particles=args.eval_particles, vectorize_particles=True))\n\n    print(\'\\nbeginning training with %s guide...\' % args.guide)\n\n    # the training loop\n    for k in range(args.num_epochs):\n        loss = svi.step(data)\n        # for the custom guide we clip parameters after each gradient step\n        if args.guide == \'custom\':\n            clip_params()\n\n        if k % args.eval_frequency == 0 and k > 0 or k == args.num_epochs - 1:\n            loss = svi_eval.evaluate_loss(data)\n            print(""[epoch %04d] training elbo: %.4g"" % (k, -loss))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    # parse command line arguments\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-epochs\', default=1500, type=int, help=\'number of training epochs\')\n    parser.add_argument(\'-ef\', \'--eval-frequency\', default=25, type=int,\n                        help=\'how often to evaluate elbo (number of epochs)\')\n    parser.add_argument(\'-ep\', \'--eval-particles\', default=20, type=int,\n                        help=\'number of samples/particles to use during evaluation\')\n    parser.add_argument(\'--guide\', default=\'custom\', type=str,\n                        help=\'use a custom, auto, or easy guide\')\n    args = parser.parse_args()\n    assert args.guide in [\'custom\', \'auto\', \'easy\']\n    main(args)\n'"
examples/sparse_regression.py,29,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport numpy as np\nimport torch\nimport math\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.autoguide import AutoDelta\nfrom pyro.infer import Trace_ELBO\nfrom pyro.infer.autoguide import init_to_median\n\nfrom torch.optim import Adam\n\n\n""""""\nWe demonstrate how to do sparse linear regression using a variant of the\napproach described in [1]. This approach is particularly suitable for situations\nwith many feature dimensions (large P) but not too many datapoints (small N).\nIn particular we consider a quadratic regressor of the form:\n\nf(X) = constant + sum_i theta_i X_i + sum_{i<j} theta_ij X_i X_j + observation noise\n\nNote that in order to keep the set of identified non-negligible weights theta_i\nand theta_ij sparse, the model assumes the weights satisfy a \'strong hierarchy\'\ncondition. See reference [1] for details.\n\nNote that in contrast to [1] we do MAP estimation for the kernel hyperparameters\ninstead of HMC. This is not expected to be as robust as doing full Bayesian inference,\nbut in some regimes this works surprisingly well. For the latter HMC approach see\nthe NumPyro version:\n\nhttps://github.com/pyro-ppl/numpyro/blob/master/examples/sparse_regression.py\n\nReferences\n[1] The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise\n    Interactions in High Dimensions.\n    Raj Agrawal, Jonathan H. Huggins, Brian Trippe, Tamara Broderick\n    https://arxiv.org/abs/1905.06501\n""""""\n\n\npyro.enable_validation(True)\ntorch.set_default_tensor_type(\'torch.FloatTensor\')\n\n\ndef dot(X, Z):\n    return torch.mm(X, Z.t())\n\n\n# The kernel that corresponds to our quadratic regressor.\ndef kernel(X, Z, eta1, eta2, c):\n    eta1sq, eta2sq = eta1.pow(2.0), eta2.pow(2.0)\n    k1 = 0.5 * eta2sq * (1.0 + dot(X, Z)).pow(2.0)\n    k2 = -0.5 * eta2sq * dot(X.pow(2.0), Z.pow(2.0))\n    k3 = (eta1sq - eta2sq) * dot(X, Z)\n    k4 = c ** 2 - 0.5 * eta2sq\n    return k1 + k2 + k3 + k4\n\n\n# Most of the model code is concerned with constructing the sparsity inducing prior.\ndef model(X, Y, hypers, jitter=1.0e-4):\n    S, P, N = hypers[\'expected_sparsity\'], X.size(1), X.size(0)\n\n    sigma = pyro.sample(""sigma"", dist.HalfNormal(hypers[\'alpha3\']))\n    phi = sigma * (S / math.sqrt(N)) / (P - S)\n    eta1 = pyro.sample(""eta1"", dist.HalfCauchy(phi))\n\n    msq = pyro.sample(""msq"", dist.InverseGamma(hypers[\'alpha1\'], hypers[\'beta1\']))\n    xisq = pyro.sample(""xisq"", dist.InverseGamma(hypers[\'alpha2\'], hypers[\'beta2\']))\n\n    eta2 = eta1.pow(2.0) * xisq.sqrt() / msq\n\n    lam = pyro.sample(""lambda"", dist.HalfCauchy(torch.ones(P, device=X.device)).to_event(1))\n    kappa = msq.sqrt() * lam / (msq + (eta1 * lam).pow(2.0)).sqrt()\n    kX = kappa * X\n\n    # sample the observation noise\n    var_obs = pyro.sample(""var_obs"", dist.InverseGamma(hypers[\'alpha_obs\'], hypers[\'beta_obs\']))\n\n    # compute the kernel for the given hyperparameters\n    k = kernel(kX, kX, eta1, eta2, hypers[\'c\']) + (var_obs + jitter) * torch.eye(N, device=X.device)\n\n    # observe the outputs Y\n    pyro.sample(""Y"", dist.MultivariateNormal(torch.zeros(N, device=X.device), covariance_matrix=k),\n                obs=Y)\n\n\n""""""\nHere we compute the mean and variance of coefficients theta_i (where i = dimension) as well\nas for quadratic coefficients theta_ij for a given (in our case MAP) estimate of the kernel\nhyperparameters (eta1, xisq, ...).\nCompare to theorem 5.1 in reference [1].\n""""""\n\n\n@torch.no_grad()\ndef compute_posterior_stats(X, Y, msq, lam, eta1, xisq, c, var_obs, jitter=1.0e-4):\n    N, P = X.shape\n\n    # prepare for computation of posterior statistics for singleton weights\n    probe = torch.zeros((P, 2, P), dtype=X.dtype, device=X.device)\n    probe[:, 0, :] = torch.eye(P, dtype=X.dtype, device=X.device)\n    probe[:, 1, :] = -torch.eye(P, dtype=X.dtype, device=X.device)\n\n    eta2 = eta1.pow(2.0) * xisq.sqrt() / msq\n    kappa = msq.sqrt() * lam / (msq + (eta1 * lam).pow(2.0)).sqrt()\n\n    kX = kappa * X\n    kprobe = kappa * probe\n    kprobe = kprobe.reshape(-1, P)\n\n    # compute various kernels\n    k_xx = kernel(kX, kX, eta1, eta2, c) + (jitter + var_obs) * torch.eye(N, dtype=X.dtype, device=X.device)\n    k_xx_inv = torch.inverse(k_xx)\n    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n\n    # compute mean and variance for singleton weights\n    vec = torch.tensor([0.50, -0.50], dtype=X.dtype, device=X.device)\n    mu = torch.matmul(k_probeX, torch.matmul(k_xx_inv, Y).unsqueeze(-1)).squeeze(-1).reshape(P, 2)\n    mu = (mu * vec).sum(-1)\n\n    var = k_prbprb - torch.matmul(k_probeX, torch.matmul(k_xx_inv, k_probeX.t()))\n    var = var.reshape(P, 2, P, 2).diagonal(dim1=-4, dim2=-2)  # 2 2 P\n    std = ((var * vec.unsqueeze(-1)).sum(-2) * vec.unsqueeze(-1)).sum(-2).clamp(min=0.0).sqrt()\n\n    # select active dimensions (those that are non-zero with sufficient statistical significance)\n    active_dims = (((mu - 4.0 * std) > 0.0) | ((mu + 4.0 * std) < 0.0)).bool()\n    active_dims = active_dims.nonzero().squeeze(-1)\n\n    print(""Identified the following active dimensions:"", active_dims.data.numpy().flatten())\n    print(""Mean estimate for active singleton weights:\\n"", mu[active_dims].data.numpy())\n\n    # if there are 0 or 1 active dimensions there are no quadratic weights to be found\n    M = len(active_dims)\n    if M < 2:\n        return active_dims.data.numpy(), []\n\n    # prep for computation of posterior statistics for quadratic weights\n    left_dims, right_dims = torch.ones(M, M).triu(1).nonzero().t()\n    left_dims, right_dims = active_dims[left_dims], active_dims[right_dims]\n\n    probe = torch.zeros(left_dims.size(0), 4, P, dtype=X.dtype, device=X.device)\n    left_dims_expand = left_dims.unsqueeze(-1).expand(left_dims.size(0), P)\n    right_dims_expand = right_dims.unsqueeze(-1).expand(right_dims.size(0), P)\n    for dim, value in zip(range(4), [1.0, 1.0, -1.0, -1.0]):\n        probe[:, dim, :].scatter_(-1, left_dims_expand, value)\n    for dim, value in zip(range(4), [1.0, -1.0, 1.0, -1.0]):\n        probe[:, dim, :].scatter_(-1, right_dims_expand, value)\n\n    kprobe = kappa * probe\n    kprobe = kprobe.reshape(-1, P)\n    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n\n    # compute mean and covariance for a subset of weights theta_ij (namely those with\n    # \'active\' dimensions i and j)\n    vec = torch.tensor([0.25, -0.25, -0.25, 0.25], dtype=X.dtype, device=X.device)\n    mu = torch.matmul(k_probeX, torch.matmul(k_xx_inv, Y).unsqueeze(-1)).squeeze(-1).reshape(left_dims.size(0), 4)\n    mu = (mu * vec).sum(-1)\n\n    var = k_prbprb - torch.matmul(k_probeX, torch.matmul(k_xx_inv, k_probeX.t()))\n    var = var.reshape(left_dims.size(0), 4, left_dims.size(0), 4).diagonal(dim1=-4, dim2=-2)\n    std = ((var * vec.unsqueeze(-1)).sum(-2) * vec.unsqueeze(-1)).sum(-2).clamp(min=0.0).sqrt()\n\n    active_quad_dims = (((mu - 4.0 * std) > 0.0) | ((mu + 4.0 * std) < 0.0)) & (mu.abs() > 1.0e-4).bool()\n    active_quad_dims = active_quad_dims.nonzero()\n\n    active_quadratic_dims = np.stack([left_dims[active_quad_dims].data.numpy().flatten(),\n                                      right_dims[active_quad_dims].data.numpy().flatten()], axis=1)\n    active_quadratic_dims = np.split(active_quadratic_dims, active_quadratic_dims.shape[0])\n    active_quadratic_dims = [tuple(a.tolist()[0]) for a in active_quadratic_dims]\n\n    return active_dims.data.numpy(), active_quadratic_dims\n\n\n# Create an artifical dataset with N datapoints and P feature dimensions. Of the P\n# dimensions S will have non-zero singleton weights and Q(Q-1)/2 pairs of feature dimensions\n# will have non-zero quadratic weights.\ndef get_data(N=20, P=10, S=2, Q=2, sigma_obs=0.15):\n    assert S < P and P > 3 and S > 2 and Q > 1 and Q <= S\n    torch.manual_seed(1)\n\n    X = torch.randn(N, P)\n\n    singleton_weights = 2.0 * torch.rand(S) - 1.0\n    Y_mean = torch.einsum(""ni,i->n"", X[:, 0:S], singleton_weights)\n\n    quadratic_weights = []\n    expected_quad_dims = []\n    for dim1 in range(Q):\n        for dim2 in range(Q):\n            if dim1 >= dim2:\n                continue\n            expected_quad_dims.append((dim1, dim2))\n            quadratic_weights.append(2.0 * torch.rand(1) - 1.0)\n            Y_mean += quadratic_weights[-1] * X[:, dim1] * X[:, dim2]\n    quadratic_weights = torch.tensor(quadratic_weights)\n\n    # we standardize the outputs Y\n    Y = Y_mean\n    Y -= Y.mean()\n    Y_std1 = Y.std()\n    Y /= Y_std1\n    Y += sigma_obs * torch.randn(N)\n    Y -= Y.mean()\n    Y_std2 = Y.std()\n    Y /= Y_std2\n\n    assert X.shape == (N, P)\n    assert Y.shape == (N,)\n\n    return X, Y, singleton_weights / (Y_std1 * Y_std2), expected_quad_dims\n\n\ndef init_loc_fn(site):\n    value = init_to_median(site, num_samples=50)\n    # we also make sure the initial observation noise is not too large\n    # (otherwise we run the danger of getting stuck in bad local optima during optimization).\n    if site[""name""] == ""var_obs"":\n        value = 0.2 * value\n    return value\n\n\ndef main(args):\n    # setup hyperparameters for the model\n    hypers = {\'expected_sparsity\': max(1.0, args.num_dimensions / 10),\n              \'alpha1\': 3.0, \'beta1\': 1.0, \'alpha2\': 3.0, \'beta2\': 1.0, \'alpha3\': 1.0,\n              \'c\': 1.0, \'alpha_obs\': 3.0, \'beta_obs\': 1.0}\n\n    P = args.num_dimensions\n    S = args.active_dimensions\n    Q = args.quadratic_dimensions\n\n    # generate artificial dataset\n    X, Y, expected_thetas, expected_quad_dims = get_data(N=args.num_data, P=P, S=S,\n                                                         Q=Q, sigma_obs=args.sigma)\n\n    loss_fn = Trace_ELBO().differentiable_loss\n\n    # We initialize the AutoDelta guide (for MAP estimation) with args.num_trials many\n    # initial parameters sampled from the vicinity of the median of the prior distribution\n    # and then continue optimizing with the best performing initialization.\n    init_losses = []\n    for restart in range(args.num_restarts):\n        pyro.clear_param_store()\n        pyro.set_rng_seed(restart)\n        guide = AutoDelta(model, init_loc_fn=init_loc_fn)\n        with torch.no_grad():\n            init_losses.append(loss_fn(model, guide, X, Y, hypers).item())\n\n    pyro.set_rng_seed(np.argmin(init_losses))\n    pyro.clear_param_store()\n    guide = AutoDelta(model, init_loc_fn=init_loc_fn)\n\n    # Instead of using pyro.infer.SVI and pyro.optim we instead construct our own PyTorch\n    # optimizer and take charge of gradient-based optimization ourselves.\n    with poutine.block(), poutine.trace(param_only=True) as param_capture:\n        guide(X, Y, hypers)\n    params = list([pyro.param(name).unconstrained() for name in param_capture.trace])\n    adam = Adam(params, lr=args.lr)\n\n    report_frequency = 50\n    print(""Beginning MAP optimization..."")\n\n    # the optimization loop\n    for step in range(args.num_steps):\n        loss = loss_fn(model, guide, X, Y, hypers) / args.num_data\n        loss.backward()\n        adam.step()\n        adam.zero_grad()\n\n        # we manually reduce the learning rate according to this schedule\n        if step in [100, 300, 700, 900]:\n            adam.param_groups[0][\'lr\'] *= 0.2\n\n        if step % report_frequency == 0 or step == args.num_steps - 1:\n            print(""[step %04d]  loss: %.5f"" % (step, loss))\n\n    print(""Expected singleton thetas:\\n"", expected_thetas.data.numpy())\n\n    # we do the final computation using double precision\n    median = guide.median()  # == mode for MAP inference\n    active_dims, active_quad_dims = \\\n        compute_posterior_stats(X.double(), Y.double(), median[\'msq\'].double(),\n                                median[\'lambda\'].double(), median[\'eta1\'].double(),\n                                median[\'xisq\'].double(), torch.tensor(hypers[\'c\']).double(),\n                                median[\'var_obs\'].double())\n\n    expected_active_dims = np.arange(S).tolist()\n\n    tp_singletons = len(set(active_dims) & set(expected_active_dims))\n    fp_singletons = len(set(active_dims) - set(expected_active_dims))\n    fn_singletons = len(set(expected_active_dims) - set(active_dims))\n    singleton_stats = (tp_singletons, fp_singletons, fn_singletons)\n\n    tp_quads = len(set(active_quad_dims) & set(expected_quad_dims))\n    fp_quads = len(set(active_quad_dims) - set(expected_quad_dims))\n    fn_quads = len(set(expected_quad_dims) - set(active_quad_dims))\n    quad_stats = (tp_quads, fp_quads, fn_quads)\n\n    # We report how well we did, i.e. did we recover the sparse set of coefficients\n    # that we expected for our artificial dataset?\n    print(""[SUMMARY STATS]"")\n    print(""Singletons (true positive, false positive, false negative): "" +\n          ""(%d, %d, %d)"" % singleton_stats)\n    print(""Quadratic  (true positive, false positive, false negative): "" +\n          ""(%d, %d, %d)"" % quad_stats)\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=\'Krylov KIT\')\n    parser.add_argument(\'--num-data\', type=int, default=750)\n    parser.add_argument(\'--num-steps\', type=int, default=1000)\n    parser.add_argument(\'--num-dimensions\', type=int, default=100)\n    parser.add_argument(\'--num-restarts\', type=int, default=10)\n    parser.add_argument(\'--sigma\', type=float, default=0.05)\n    parser.add_argument(\'--active-dimensions\', type=int, default=10)\n    parser.add_argument(\'--quadratic-dimensions\', type=int, default=5)\n    parser.add_argument(\'--lr\', type=float, default=0.3)\n    args = parser.parse_args()\n\n    main(args)\n'"
profiler/__init__.py,0,b''
profiler/distributions.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport torch\nfrom torch.autograd import Variable\n\nfrom profiler.profiling_utils import Profile, profile_print\nfrom pyro.distributions import (Bernoulli, Beta, Categorical, Cauchy, Dirichlet, Exponential, Gamma, LogNormal, Normal,\n                                OneHotCategorical, Poisson, Uniform)\n\n\ndef T(arr):\n    return Variable(torch.DoubleTensor(arr))\n\n\nTOOL = \'timeit\'\nTOOL_CFG = {}\nDISTRIBUTIONS = {\n    \'Bernoulli\': (Bernoulli, {\n        \'probs\': T([0.3, 0.3, 0.3, 0.3])\n    }),\n    \'Beta\': (Beta, {\n        \'concentration1\': T([2.4, 2.4, 2.4, 2.4]),\n        \'concentration0\': T([3.2, 3.2, 3.2, 3.2])\n    }),\n    \'Categorical\': (Categorical, {\n        \'probs\': T([0.1, 0.3, 0.4, 0.2])\n    }),\n    \'OneHotCategorical\': (OneHotCategorical, {\n        \'probs\': T([0.1, 0.3, 0.4, 0.2])\n    }),\n    \'Dirichlet\': (Dirichlet, {\n        \'concentration\': T([2.4, 3, 6, 6])\n    }),\n    \'Normal\': (Normal, {\n        \'loc\': T([0.5, 0.5, 0.5, 0.5]),\n        \'scale\': T([1.2, 1.2, 1.2, 1.2])\n    }),\n    \'LogNormal\': (LogNormal, {\n        \'loc\': T([0.5, 0.5, 0.5, 0.5]),\n        \'scale\': T([1.2, 1.2, 1.2, 1.2])\n    }),\n    \'Cauchy\': (Cauchy, {\n        \'loc\': T([0.5, 0.5, 0.5, 0.5]),\n        \'scale\': T([1.2, 1.2, 1.2, 1.2])\n    }),\n    \'Exponential\': (Exponential, {\n        \'rate\': T([5.5, 3.2, 4.1, 5.6])\n    }),\n    \'Poisson\': (Poisson, {\n        \'rate\': T([5.5, 3.2, 4.1, 5.6])\n    }),\n    \'Gamma\': (Gamma, {\n        \'concentration\': T([2.4, 2.4, 2.4, 2.4]),\n        \'rate\': T([3.2, 3.2, 3.2, 3.2])\n    }),\n    \'Uniform\': (Uniform, {\n        \'low\': T([0, 0, 0, 0]),\n        \'high\': T([4, 4, 4, 4])\n    })\n}\n\n\ndef get_tool():\n    return TOOL\n\n\ndef get_tool_cfg():\n    return TOOL_CFG\n\n\n@Profile(\n    tool=get_tool,\n    tool_cfg=get_tool_cfg,\n    fn_id=lambda dist, batch_size, *args, **kwargs: \'sample_\' + dist.dist_class.__name__ + \'_N=\' + str(batch_size))\ndef sample(dist, batch_size):\n    return dist.sample(sample_shape=(batch_size,))\n\n\n@Profile(\n    tool=get_tool,\n    tool_cfg=get_tool_cfg,\n    fn_id=lambda dist, batch, *args, **kwargs:  #\n    \'log_prob_\' + dist.dist_class.__name__ + \'_N=\' + str(batch.size()[0]))\ndef log_prob(dist, batch):\n    return dist.log_prob(batch)\n\n\ndef run_with_tool(tool, dists, batch_sizes):\n    column_widths, field_format, template = None, None, None\n    if tool == \'timeit\':\n        profile_cols = 2 * len(batch_sizes)\n        column_widths = [14] * (profile_cols + 1)\n        field_format = [None] + [\'{:.6f}\'] * profile_cols\n        template = \'column\'\n    elif tool == \'cprofile\':\n        column_widths = [14, 80]\n        template = \'row\'\n    with profile_print(column_widths, field_format, template) as out:\n        column_headers = []\n        for size in batch_sizes:\n            column_headers += [\'SAMPLE (N=\' + str(size) + \')\', \'LOG_PROB (N=\' + str(size) + \')\']\n        out.header([\'DISTRIBUTION\'] + column_headers)\n        for dist_name in dists:\n            Dist, params = DISTRIBUTIONS[dist_name]\n            result_row = [dist_name]\n            dist = Dist(**params)\n            for size in batch_sizes:\n                sample_result, sample_prof = sample(dist, batch_size=size)\n                _, logpdf_prof = log_prob(dist, sample_result)\n                result_row += [sample_prof, logpdf_prof]\n            out.push(result_row)\n\n\ndef set_tool_cfg(args):\n    global TOOL, TOOL_CFG\n    TOOL = args.tool\n    tool_cfg = {}\n    if args.tool == \'timeit\':\n        repeat = 5\n        if args.repeat is not None:\n            repeat = args.repeat\n        tool_cfg = {\'repeat\': repeat}\n    TOOL_CFG = tool_cfg\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Profiling distributions library using various\' \'tools.\')\n    parser.add_argument(\n        \'--tool\',\n        nargs=\'?\',\n        default=\'timeit\',\n        help=\'Profile using tool. One of following should be specified:\'\n        \' [""timeit"", ""cprofile""]\')\n    parser.add_argument(\n        \'--batch_sizes\',\n        nargs=\'*\',\n        type=int,\n        help=\'Batch size of tensor - max of 4 values allowed. \'\n        \'Default = [10000, 100000]\')\n    parser.add_argument(\n        \'--dists\',\n        nargs=\'*\',\n        type=str,\n        help=\'Run tests on distributions. One or more of following distributions \'\n        \'are supported: [""bernoulli, ""beta"", ""categorical"", ""dirichlet"", \'\n        \'""normal"", ""lognormal"", ""halfcauchy"", ""cauchy"", ""exponential"", \'\n        \'""poisson"", ""one_hot_categorical"", ""gamma"", ""uniform""] \'\n        \'Default - Run profiling on all distributions\')\n    parser.add_argument(\n        \'--repeat\',\n        nargs=\'?\',\n        default=5,\n        type=int,\n        help=\'When profiling using ""timeit"", the number of repetitions to \'\n        \'use for the profiled function. default=5. The minimum value \'\n        \'is reported.\')\n    args = parser.parse_args()\n    set_tool_cfg(args)\n    dists = args.dists\n    batch_sizes = args.batch_sizes\n    if not args.batch_sizes:\n        batch_sizes = [10000, 100000]\n    if len(batch_sizes) >= 4:\n        raise ValueError(""Max of 4 batch sizes can be specified."")\n    if not dists:\n        dists = sorted(DISTRIBUTIONS.keys())\n    run_with_tool(args.tool, dists, batch_sizes)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
profiler/hmm.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport os\nimport pickle\nimport re\nimport subprocess\nimport sys\nfrom collections import defaultdict\nfrom os.path import join, abspath\n\nfrom numpy import median\n\nfrom pyro.util import timed\n\n\nEXAMPLES_DIR = join(abspath(__file__), os.pardir, os.pardir, ""examples"")\n\n\ndef main(args):\n    # Decide what experiments to run.\n    configs = []\n    for model in args.model.split("",""):\n        for seed in args.seed.split("",""):\n            config = [""--seed={}"".format(seed), ""--model={}"".format(model),\n                      ""--num-steps={}"".format(args.num_steps)]\n            if args.cuda:\n                config.append(""--cuda"")\n            if args.jit:\n                config.append(""--jit"")\n                config.append(""--time-compilation"")\n            configs.append(tuple(config))\n\n    # Run timing experiments serially.\n    results = {}\n    if os.path.exists(args.filename):\n        try:\n            with open(args.filename, ""rb"") as f:\n                results = pickle.load(f)\n        except Exception:\n            pass\n    for config in configs:\n        with timed() as t:\n            out = subprocess.check_output((sys.executable, ""-O"", abspath(join(EXAMPLES_DIR, ""hmm.py""))) + config,\n                                          encoding=""utf-8"")\n        results[config] = t.elapsed\n        if ""--jit"" in config:\n            matched = re.search(r""time to compile: (\\d+\\.\\d+)"", out)\n            if matched:\n                compilation_time = float(matched.group(1))\n                results[config + (""(compilation time)"",)] = compilation_time\n        with open(args.filename, ""wb"") as f:\n            pickle.dump(results, f)\n\n    # Group by seed.\n    grouped = defaultdict(list)\n    for config, elapsed in results.items():\n        grouped[config[1:]].append(elapsed)\n\n    # Print a table in github markdown format.\n    print(""| Min (sec) | Mean (sec) | Max (sec) | python -O examples/hmm.py ... |"")\n    print(""| -: | -: | -: | - |"")\n    for config, times in sorted(grouped.items()):\n        print(""| {:0.1f} | {:0.1f} | {:0.1f} | {} |"".format(\n            min(times), median(times), max(times), "" "".join(config)))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Profiler for examples/hmm.py"")\n    parser.add_argument(""-f"", ""--filename"", default=""hmm_profile.pkl"")\n    parser.add_argument(""-n"", ""--num-steps"", default=50, type=int)\n    parser.add_argument(""-s"", ""--seed"", default=""0,1,2,3,4"")\n    parser.add_argument(""-m"", ""--model"", default=""1,2,3,4,5,6,7"")\n    parser.add_argument(""--cuda"", action=""store_true"")\n    parser.add_argument(""--jit"", action=""store_true"")\n    args = parser.parse_args()\n    main(args)\n'"
profiler/profiling_utils.py,0,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport cProfile\nfrom io import StringIO\nimport functools\nimport os\nimport pstats\nimport timeit\nfrom contextlib import contextmanager\n\nfrom prettytable import ALL, PrettyTable\n\nFILE = os.path.abspath(__file__)\nPROF_DIR = os.path.join(os.path.dirname(FILE), 'data')\nif not os.path.exists(PROF_DIR):\n    os.makedirs(PROF_DIR)\n\n\nclass ProfilePrinter:\n\n    def __init__(self, column_widths=None, field_format=None, template='column'):\n        assert template in ('column', 'row')\n        self._template = template\n        self._column_widths = column_widths\n        self._field_format = field_format\n        self._header = None\n        if template == 'column':\n            self.table = PrettyTable(header=False, hrules=ALL)\n        else:\n            self.table = PrettyTable(header=False, hrules=ALL)\n\n    def _formatted_values(self, values):\n        if self._field_format is not None:\n            assert len(self._field_format) == len(values)\n            return [f.format(val) if f else str(val) for f, val in zip(self._field_format, values)]\n        return values\n\n    def _add_using_row_format(self, values):\n        assert len(self._header) == len(values)\n        formatted_vals = self._formatted_values(values)\n        for i in range(len(self._header)):\n            self.table.add_row([self._header[i], formatted_vals[i]])\n\n    def _add_using_column_format(self, values):\n        formatted_vals = self._formatted_values(values)\n        self.table.add_row(formatted_vals)\n\n    def push(self, values):\n        if self._template == 'column':\n            self._add_using_column_format(values)\n        else:\n            self._add_using_row_format(values)\n\n    def header(self, values):\n        self._header = values\n        if self._template == 'column':\n            field_names = values\n            self.table.add_row(values)\n        else:\n            field_names = ['KEY', 'VALUE']\n        self.table.field_names = field_names\n        for i in range(len(field_names)):\n            self.table.align[field_names[i]] = 'l'\n            if self._column_widths:\n                self.table.max_width[field_names[i]] = self._column_widths[i]\n\n    def print(self):\n        print(self.table)\n\n\n@contextmanager\ndef profile_print(column_widths=None, field_format=None, template='column'):\n    out_buffer = ProfilePrinter(column_widths, field_format, template)\n    try:\n        yield out_buffer\n    finally:\n        out_buffer.print()\n\n\ndef profile_timeit(fn_callable, repeat=1):\n    ret = fn_callable()\n    return ret, min(timeit.repeat(fn_callable, repeat=repeat, number=1))\n\n\ndef profile_cprofile(fn_callable, prof_file):\n    prof = cProfile.Profile()\n    ret = prof.runcall(fn_callable)\n    prof.dump_stats(prof_file)\n    prof_stats = StringIO()\n    p = pstats.Stats(prof_file, stream=prof_stats)\n    p.strip_dirs().sort_stats('cumulative').print_stats(0.5)\n    return ret, prof_stats.getvalue()\n\n\nclass Profile:\n\n    def __init__(self, tool, tool_cfg, fn_id):\n        self.tool = tool\n        self.tool_cfg = tool_cfg\n        self.fn_id = fn_id\n\n    def _set_decorator_params(self):\n        if callable(self.tool):\n            self.tool = self.tool()\n        if callable(self.tool_cfg):\n            self.tool_cfg = self.tool_cfg()\n\n    def __call__(self, fn):\n\n        def wrapped_fn(*args, **kwargs):\n            self._set_decorator_params()\n            fn_callable = functools.partial(fn, *args, **kwargs)\n            if self.tool == 'timeit':\n                return profile_timeit(fn_callable, **self.tool_cfg)\n            elif self.tool == 'cprofile':\n                prof_file = os.path.join(PROF_DIR, self.fn_id(*args, **kwargs))\n                return profile_cprofile(fn_callable, prof_file=prof_file)\n            else:\n                raise ValueError('Invalid profiling tool specified: {}.'.format(self.tool))\n\n        return wrapped_fn\n"""
pyro/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro.poutine as poutine\nfrom pyro.logger import log\nfrom pyro.poutine import condition, do, markov\nfrom pyro.primitives import (clear_param_store, deterministic, enable_validation, factor, get_param_store, iarange,\n                             irange, module, param, plate, plate_stack, random_module, sample, subsample,\n                             validation_enabled)\nfrom pyro.util import set_rng_seed\n\nversion_prefix = \'1.3.1\'\n\n# Get the __version__ string from the auto-generated _version.py file, if exists.\ntry:\n    from pyro._version import __version__\nexcept ImportError:\n    __version__ = version_prefix\n\n__all__ = [\n    ""__version__"",\n    ""clear_param_store"",\n    ""condition"",\n    ""deterministic"",\n    ""do"",\n    ""enable_validation"",\n    ""factor"",\n    ""get_param_store"",\n    ""iarange"",\n    ""irange"",\n    ""log"",\n    ""markov"",\n    ""module"",\n    ""param"",\n    ""plate"",\n    ""plate"",\n    ""plate_stack"",\n    ""poutine"",\n    ""random_module"",\n    ""sample"",\n    ""set_rng_seed"",\n    ""subsample"",\n    ""validation_enabled"",\n]\n'"
pyro/generic.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nfrom pyroapi import *  # noqa F401\nfrom pyroapi import __all__  # noqa F401\n\nwarnings.warn(""pyro.generic has moved to the pyroapi package"", DeprecationWarning)\n'"
pyro/logger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\n\ndefault_format = \'%(levelname)s \\t %(message)s\'\nlog = logging.getLogger(""pyro"")\nlog.setLevel(logging.INFO)\n\n\nif not logging.root.handlers:\n    default_handler = logging.StreamHandler()\n    default_handler.setLevel(logging.INFO)\n    default_handler.setFormatter(logging.Formatter(default_format))\n    log.addHandler(default_handler)\n    log.propagate = False\n'"
pyro/primitives.py,22,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport copy\nimport warnings\nfrom collections import OrderedDict\nfrom contextlib import contextmanager, ExitStack\nfrom inspect import isclass\n\nimport pyro.distributions as dist\nimport pyro.infer as infer\nimport pyro.poutine as poutine\nfrom pyro.params import param_with_module_name\nfrom pyro.poutine.plate_messenger import PlateMessenger\nfrom pyro.poutine.runtime import _MODULE_NAMESPACE_DIVIDER, _PYRO_PARAM_STORE, am_i_wrapped, apply_stack, effectful\nfrom pyro.poutine.subsample_messenger import SubsampleMessenger\nfrom pyro.util import deep_getattr, set_rng_seed  # noqa: F401\n\n\ndef get_param_store():\n    """"""\n    Returns the ParamStore\n    """"""\n    return _PYRO_PARAM_STORE\n\n\ndef clear_param_store():\n    """"""\n    Clears the ParamStore. This is especially useful if you\'re working in a REPL.\n    """"""\n    return _PYRO_PARAM_STORE.clear()\n\n\n_param = effectful(_PYRO_PARAM_STORE.get_param, type=""param"")\n\n\ndef param(name, *args, **kwargs):\n    """"""\n    Saves the variable as a parameter in the param store.\n    To interact with the param store or write to disk,\n    see `Parameters <parameters.html>`_.\n\n    :param str name: name of parameter\n    :param init_tensor: initial tensor or lazy callable that returns a tensor.\n        For large tensors, it may be cheaper to write e.g.\n        ``lambda: torch.randn(100000)``, which will only be evaluated on the\n        initial statement.\n    :type init_tensor: torch.Tensor or callable\n    :param constraint: torch constraint, defaults to ``constraints.real``.\n    :type constraint: torch.distributions.constraints.Constraint\n    :param int event_dim: (optional) number of rightmost dimensions unrelated\n        to baching. Dimension to the left of this will be considered batch\n        dimensions; if the param statement is inside a subsampled plate, then\n        corresponding batch dimensions of the parameter will be correspondingly\n        subsampled. If unspecified, all dimensions will be considered event\n        dims and no subsampling will be performed.\n    :returns: parameter\n    :rtype: torch.Tensor\n    """"""\n    kwargs[""name""] = name\n    return _param(name, *args, **kwargs)\n\n\ndef sample(name, fn, *args, **kwargs):\n    """"""\n    Calls the stochastic function `fn` with additional side-effects depending\n    on `name` and the enclosing context (e.g. an inference algorithm).\n    See `Intro I <http://pyro.ai/examples/intro_part_i.html>`_ and\n    `Intro II <http://pyro.ai/examples/intro_part_ii.html>`_ for a discussion.\n\n    :param name: name of sample\n    :param fn: distribution class or function\n    :param obs: observed datum (optional; should only be used in context of\n        inference) optionally specified in kwargs\n    :param dict infer: Optional dictionary of inference parameters specified\n        in kwargs. See inference documentation for details.\n    :returns: sample\n    """"""\n    obs = kwargs.pop(""obs"", None)\n    infer = kwargs.pop(""infer"", {}).copy()\n    # check if stack is empty\n    # if stack empty, default behavior (defined here)\n    if not am_i_wrapped():\n        if obs is not None and not infer.get(""_deterministic""):\n            warnings.warn(""trying to observe a value outside of inference at "" + name,\n                          RuntimeWarning)\n            return obs\n        return fn(*args, **kwargs)\n    # if stack not empty, apply everything in the stack?\n    else:\n        # initialize data structure to pass up/down the stack\n        msg = {\n            ""type"": ""sample"",\n            ""name"": name,\n            ""fn"": fn,\n            ""is_observed"": False,\n            ""args"": args,\n            ""kwargs"": kwargs,\n            ""value"": None,\n            ""infer"": infer,\n            ""scale"": 1.0,\n            ""mask"": None,\n            ""cond_indep_stack"": (),\n            ""done"": False,\n            ""stop"": False,\n            ""continuation"": None\n        }\n        # handle observation\n        if obs is not None:\n            msg[""value""] = obs\n            msg[""is_observed""] = True\n        # apply the stack and return its return value\n        apply_stack(msg)\n        return msg[""value""]\n\n\ndef factor(name, log_factor):\n    """"""\n    Factor statement to add arbitrary log probability factor to a\n    probabilisitic model.\n\n    :param str name: Name of the trivial sample\n    :param torch.Tensor log_factor: A possibly batched log probability factor.\n    """"""\n    unit_dist = dist.Unit(log_factor)\n    unit_value = unit_dist.sample()\n    sample(name, unit_dist, obs=unit_value)\n\n\ndef deterministic(name, value, event_dim=None):\n    """"""\n    EXPERIMENTAL Deterministic statement to add a :class:`~pyro.distributions.Delta`\n    site with name `name` and value `value` to the trace. This is useful when\n    we want to record values which are completely determined by their parents.\n    For example::\n\n        x = sample(""x"", dist.Normal(0, 1))\n        x2 = deterministic(""x2"", x ** 2)\n\n    .. note:: The site does not affect the model density. This currently converts\n        to a :func:`sample` statement, but may change in the future.\n\n    :param str name: Name of the site.\n    :param torch.Tensor value: Value of the site.\n    :param int event_dim: Optional event dimension, defaults to `value.ndim`.\n    """"""\n    event_dim = value.ndim if event_dim is None else event_dim\n    return sample(name, dist.Delta(value, event_dim=event_dim).mask(False),\n                  obs=value, infer={""_deterministic"": True})\n\n\n@effectful(type=""subsample"")\ndef subsample(data, event_dim):\n    """"""\n    EXPERIMENTAL Subsampling statement to subsample data based on enclosing\n    :class:`~pyro.primitives.plate` s.\n\n    This is typically called on arguments to ``model()`` when subsampling is\n    performed automatically by :class:`~pyro.primitives.plate` s by passing\n    either the ``subsample`` or ``subsample_size`` kwarg. For example the\n    following are equivalent::\n\n        # Version 1. using pyro.subsample()\n        def model(data):\n            with pyro.plate(""data"", len(data), subsample_size=10, dim=-data.dim()) as ind:\n                data = data[ind]\n                # ...\n\n        # Version 2. using indexing\n        def model(data):\n            with pyro.plate(""data"", len(data), subsample_size=10, dim=-data.dim()):\n                data = pyro.subsample(data, event_dim=0)\n                # ...\n\n    :param data: A tensor of batched data.\n    :type data: ~torch.Tensor\n    :param int event_dim: The event dimension of the data tensor. Dimensions to\n        the left are considered batch dimensions.\n    :returns: A subsampled version of ``data``\n    :rtype: ~torch.Tensor\n    """"""\n    assert isinstance(event_dim, int) and event_dim >= 0\n    return data  # May be intercepted by SubsampleMessenger.\n\n\nclass plate(PlateMessenger):\n    """"""\n    Construct for conditionally independent sequences of variables.\n\n    ``plate`` can be used either sequentially as a generator or in parallel as\n    a context manager (formerly ``irange`` and ``iarange``, respectively).\n\n    Sequential :class:`plate` is similar to :py:func:`range` in that it generates\n    a sequence of values.\n\n    Vectorized :class:`plate` is similar to :func:`torch.arange` in that it\n    yields an array of indices by which other tensors can be indexed.\n    :class:`plate` differs from :func:`torch.arange` in that it also informs\n    inference algorithms that the variables being indexed are conditionally\n    independent.  To do this, :class:`plate` is a provided as context manager\n    rather than a function, and users must guarantee that all computation\n    within an :class:`plate` context is conditionally independent::\n\n        with plate(""name"", size) as ind:\n            # ...do conditionally independent stuff with ind...\n\n    Additionally, :class:`plate` can take advantage of the conditional\n    independence assumptions by subsampling the indices and informing inference\n    algorithms to scale various computed values. This is typically used to\n    subsample minibatches of data::\n\n        with plate(""data"", len(data), subsample_size=100) as ind:\n            batch = data[ind]\n            assert len(batch) == 100\n\n    By default ``subsample_size=False`` and this simply yields a\n    ``torch.arange(0, size)``. If ``0 < subsample_size <= size`` this yields a\n    single random batch of indices of size ``subsample_size`` and scales all\n    log likelihood terms by ``size/batch_size``, within this context.\n\n    .. warning::  This is only correct if all computation is conditionally\n        independent within the context.\n\n    :param str name: A unique name to help inference algorithms match\n        :class:`plate` sites between models and guides.\n    :param int size: Optional size of the collection being subsampled\n        (like `stop` in builtin `range`).\n    :param int subsample_size: Size of minibatches used in subsampling.\n        Defaults to `size`.\n    :param subsample: Optional custom subsample for user-defined subsampling\n        schemes. If specified, then `subsample_size` will be set to\n        `len(subsample)`.\n    :type subsample: Anything supporting `len()`.\n    :param int dim: An optional dimension to use for this independence index.\n        If specified, ``dim`` should be negative, i.e. should index from the\n        right. If not specified, ``dim`` is set to the rightmost dim that is\n        left of all enclosing ``plate`` contexts.\n    :param bool use_cuda: DEPRECATED, use the `device` arg instead.\n        Optional bool specifying whether to use cuda tensors for `subsample`\n        and `log_prob`. Defaults to ``torch.Tensor.is_cuda``.\n    :param str device: Optional keyword specifying which device to place\n        the results of `subsample` and `log_prob` on. By default, results\n        are placed on the same device as the default tensor.\n    :return: A reusabe context manager yielding a single 1-dimensional\n        :class:`torch.Tensor` of indices.\n\n    Examples:\n\n        .. doctest::\n           :hide:\n\n           >>> loc, scale = torch.tensor(0.), torch.tensor(1.)\n           >>> data = torch.randn(100)\n           >>> z = dist.Bernoulli(0.5).sample((100,))\n\n        >>> # This version declares sequential independence and subsamples data:\n        >>> for i in plate(\'data\', 100, subsample_size=10):\n        ...     if z[i]:  # Control flow in this example prevents vectorization.\n        ...         obs = sample(\'obs_{}\'.format(i), dist.Normal(loc, scale), obs=data[i])\n\n        >>> # This version declares vectorized independence:\n        >>> with plate(\'data\'):\n        ...     obs = sample(\'obs\', dist.Normal(loc, scale), obs=data)\n\n        >>> # This version subsamples data in vectorized way:\n        >>> with plate(\'data\', 100, subsample_size=10) as ind:\n        ...     obs = sample(\'obs\', dist.Normal(loc, scale), obs=data[ind])\n\n        >>> # This wraps a user-defined subsampling method for use in pyro:\n        >>> ind = torch.randint(0, 100, (10,)).long() # custom subsample\n        >>> with plate(\'data\', 100, subsample=ind):\n        ...     obs = sample(\'obs\', dist.Normal(loc, scale), obs=data[ind])\n\n        >>> # This reuses two different independence contexts.\n        >>> x_axis = plate(\'outer\', 320, dim=-1)\n        >>> y_axis = plate(\'inner\', 200, dim=-2)\n        >>> with x_axis:\n        ...     x_noise = sample(""x_noise"", dist.Normal(loc, scale))\n        ...     assert x_noise.shape == (320,)\n        >>> with y_axis:\n        ...     y_noise = sample(""y_noise"", dist.Normal(loc, scale))\n        ...     assert y_noise.shape == (200, 1)\n        >>> with x_axis, y_axis:\n        ...     xy_noise = sample(""xy_noise"", dist.Normal(loc, scale))\n        ...     assert xy_noise.shape == (200, 320)\n\n    See `SVI Part II <http://pyro.ai/examples/svi_part_ii.html>`_ for an\n    extended discussion.\n    """"""\n    pass\n\n\nclass iarange(plate):\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""pyro.iarange is deprecated; use pyro.plate instead"", DeprecationWarning)\n        super().__init__(*args, **kwargs)\n\n\nclass irange(SubsampleMessenger):\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""pyro.irange is deprecated; use pyro.plate instead"", DeprecationWarning)\n        super().__init__(*args, **kwargs)\n\n\n@contextmanager\ndef plate_stack(prefix, sizes, rightmost_dim=-1):\n    """"""\n    Create a contiguous stack of :class:`plate` s with dimensions::\n\n        rightmost_dim - len(sizes), ..., rightmost_dim\n\n    :param str prefix: Name prefix for plates.\n    :param iterable sizes: An iterable of plate sizes.\n    :param int rightmost_dim: The rightmost dim, counting from the right.\n    """"""\n    assert rightmost_dim < 0\n    with ExitStack() as stack:\n        for i, size in enumerate(reversed(sizes)):\n            plate_i = plate(""{}_{}"".format(prefix, i), size, dim=rightmost_dim - i)\n            stack.enter_context(plate_i)\n        yield\n\n\ndef module(name, nn_module, update_module_params=False):\n    """"""\n    Takes a torch.nn.Module and registers its parameters with the ParamStore.\n    In conjunction with the ParamStore save() and load() functionality, this\n    allows the user to save and load modules.\n\n    :param name: name of module\n    :type name: str\n    :param nn_module: the module to be registered with Pyro\n    :type nn_module: torch.nn.Module\n    :param update_module_params: determines whether Parameters\n                                 in the PyTorch module get overridden with the values found in the\n                                 ParamStore (if any). Defaults to `False`\n    :type load_from_param_store: bool\n    :returns: torch.nn.Module\n    """"""\n    assert hasattr(nn_module, ""parameters""), ""module has no parameters""\n    assert _MODULE_NAMESPACE_DIVIDER not in name, ""improper module name, since contains %s"" %\\\n        _MODULE_NAMESPACE_DIVIDER\n\n    if isclass(nn_module):\n        raise NotImplementedError(""pyro.module does not support class constructors for "" +\n                                  ""the argument nn_module"")\n\n    target_state_dict = OrderedDict()\n\n    for param_name, param_value in nn_module.named_parameters():\n        if param_value.requires_grad:\n            # register the parameter in the module with pyro\n            # this only does something substantive if the parameter hasn\'t been seen before\n            full_param_name = param_with_module_name(name, param_name)\n            returned_param = param(full_param_name, param_value)\n\n            if param_value._cdata != returned_param._cdata:\n                target_state_dict[param_name] = returned_param\n        else:\n            warnings.warn(""{} was not registered in the param store because"".format(param_name) +\n                          "" requires_grad=False"")\n\n    if target_state_dict and update_module_params:\n        # WARNING: this is very dangerous. better method?\n        for _name, _param in nn_module.named_parameters():\n            is_param = False\n            name_arr = _name.rsplit(\'.\', 1)\n            if len(name_arr) > 1:\n                mod_name, param_name = name_arr[0], name_arr[1]\n            else:\n                is_param = True\n                mod_name = _name\n            if _name in target_state_dict.keys():\n                if not is_param:\n                    deep_getattr(nn_module, mod_name)._parameters[param_name] = target_state_dict[_name]\n                else:\n                    nn_module._parameters[mod_name] = target_state_dict[_name]\n\n    return nn_module\n\n\ndef random_module(name, nn_module, prior, *args, **kwargs):\n    r""""""\n    .. warning::\n        The `random_module` primitive is deprecated, and will be removed\n        in a future release. Use :class:`~pyro.nn.module.PyroModule` instead to\n        to create Bayesian modules from :class:`torch.nn.Module` instances.\n        See the `Bayesian Regression tutorial <http://pyro.ai/examples/bayesian_regression.html>`_\n        for an example.\n\n\n    Places a prior over the parameters of the module `nn_module`.\n    Returns a distribution (callable) over `nn.Module`\\s, which\n    upon calling returns a sampled `nn.Module`.\n\n    :param name: name of pyro module\n    :type name: str\n    :param nn_module: the module to be registered with pyro\n    :type nn_module: torch.nn.Module\n    :param prior: pyro distribution, stochastic function, or python dict with parameter names\n                  as keys and respective distributions/stochastic functions as values.\n    :returns: a callable which returns a sampled module\n    """"""\n    warnings.warn(""The `random_module` primitive is deprecated, and will be removed ""\n                  ""in a future release. Use `pyro.nn.Module` to create Bayesian ""\n                  ""modules from `torch.nn.Module` instances."", FutureWarning)\n\n    assert hasattr(nn_module, ""parameters""), ""Module is not a NN module.""\n    # register params in param store\n    lifted_fn = poutine.lift(module, prior=prior)\n\n    def _fn():\n        nn_copy = copy.deepcopy(nn_module)\n        # update_module_params must be True or the lifted module will not update local params\n        return lifted_fn(name, nn_copy, update_module_params=True, *args, **kwargs)\n    return _fn\n\n\ndef enable_validation(is_validate=True):\n    """"""\n    Enable or disable validation checks in Pyro. Validation checks provide\n    useful warnings and errors, e.g. NaN checks, validating distribution\n    arguments and support values, etc. which is useful for debugging.\n    Since some of these checks may be expensive, we recommend turning\n    this off for mature models.\n\n    :param bool is_validate: (optional; defaults to True) whether to\n        enable validation checks.\n    """"""\n    dist.enable_validation(is_validate)\n    infer.enable_validation(is_validate)\n    poutine.enable_validation(is_validate)\n\n\n@contextmanager\ndef validation_enabled(is_validate=True):\n    """"""\n    Context manager that is useful when temporarily enabling/disabling\n    validation checks.\n\n    :param bool is_validate: (optional; defaults to True) temporary\n        validation check override.\n    """"""\n    infer_validation_status = infer.is_validation_enabled()\n    distribution_validation_status = dist.is_validation_enabled()\n    poutine_validation_status = poutine.is_validation_enabled()\n    try:\n        enable_validation(is_validate)\n        yield\n    finally:\n        dist.enable_validation(distribution_validation_status)\n        infer.enable_validation(infer_validation_status)\n        poutine.enable_validation(poutine_validation_status)\n'"
pyro/util.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport math\nimport numbers\nimport random\nimport sys\nimport timeit\nimport warnings\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nfrom itertools import zip_longest\n\nimport torch\n\nfrom pyro.poutine.util import site_is_subsample\n\n\ndef set_rng_seed(rng_seed):\n    """"""\n    Sets seeds of `torch` and `torch.cuda` (if available).\n\n    :param int rng_seed: The seed value.\n    """"""\n    torch.manual_seed(rng_seed)\n    random.seed(rng_seed)\n    try:\n        import numpy as np\n        np.random.seed(rng_seed)\n    except ImportError:\n        pass\n\n\ndef get_rng_state():\n    state = {\'torch\': torch.get_rng_state(), \'random\': random.getstate()}\n    try:\n        import numpy as np\n        state[\'numpy\'] = np.random.get_state()\n    except ImportError:\n        pass\n    return state\n\n\ndef set_rng_state(state):\n    torch.set_rng_state(state[\'torch\'])\n    random.setstate(state[\'random\'])\n    if \'numpy\' in state:\n        import numpy as np\n        np.random.set_state(state[\'numpy\'])\n\n\ndef torch_isnan(x):\n    """"""\n    A convenient function to check if a Tensor contains any nan; also works with numbers\n    """"""\n    if isinstance(x, numbers.Number):\n        return x != x\n    return torch.isnan(x).any()\n\n\ndef torch_isinf(x):\n    """"""\n    A convenient function to check if a Tensor contains any +inf; also works with numbers\n    """"""\n    if isinstance(x, numbers.Number):\n        return x == math.inf or x == -math.inf\n    return (x == math.inf).any() or (x == -math.inf).any()\n\n\ndef warn_if_nan(value, msg="""", *, filename=None, lineno=None):\n    """"""\n    A convenient function to warn if a Tensor or its grad contains any nan,\n    also works with numbers.\n    """"""\n    if filename is None:\n        try:\n            frame = sys._getframe(1)\n        except ValueError:\n            filename = ""sys""\n            lineno = 1\n        else:\n            filename = frame.f_code.co_filename\n            lineno = frame.f_lineno\n\n    if torch.is_tensor(value) and value.requires_grad:\n        value.register_hook(lambda x: warn_if_nan(x, ""backward "" + msg, filename=filename, lineno=lineno))\n\n    if torch_isnan(value):\n        warnings.warn_explicit(""Encountered NaN{}"".format(\': \' + msg if msg else \'.\'),\n                               UserWarning, filename, lineno)\n\n    return value\n\n\ndef warn_if_inf(value, msg="""", allow_posinf=False, allow_neginf=False, *,\n                filename=None, lineno=None):\n    """"""\n    A convenient function to warn if a Tensor or its grad contains any inf,\n    also works with numbers.\n    """"""\n    if filename is None:\n        try:\n            frame = sys._getframe(1)\n        except ValueError:\n            filename = ""sys""\n            lineno = 1\n        else:\n            filename = frame.f_code.co_filename\n            lineno = frame.f_lineno\n\n    if torch.is_tensor(value) and value.requires_grad:\n        value.register_hook(lambda x: warn_if_inf(x, ""backward "" + msg,\n                                                  allow_posinf, allow_neginf,\n                                                  filename=filename, lineno=lineno))\n\n    if (not allow_posinf) and (value == math.inf if isinstance(value, numbers.Number)\n                               else (value == math.inf).any()):\n        warnings.warn_explicit(""Encountered +inf{}"".format(\': \' + msg if msg else \'.\'),\n                               UserWarning, filename, lineno)\n    if (not allow_neginf) and (value == -math.inf if isinstance(value, numbers.Number)\n                               else (value == -math.inf).any()):\n        warnings.warn_explicit(""Encountered -inf{}"".format(\': \' + msg if msg else \'.\'),\n                               UserWarning, filename, lineno)\n\n    return value\n\n\ndef save_visualization(trace, graph_output):\n    """"""\n    (DEPRECATED) Take a trace generated by poutine.trace with `graph_type=\'dense\'`\n    and render the graph with the output saved to file.\n\n    - non-reparameterized stochastic nodes are salmon\n    - reparameterized stochastic nodes are half salmon, half grey\n    - observation nodes are green\n\n    :param pyro.poutine.Trace trace: a trace to be visualized\n    :param graph_output: the graph will be saved to graph_output.pdf\n    :type graph_output: str\n\n\n    Example:\n\n    trace = pyro.poutine.trace(model, graph_type=""dense"").get_trace()\n    save_visualization(trace, \'output\')\n    """"""\n    warnings.warn(""`save_visualization` function is deprecated and will be removed in ""\n                  ""a future version."")\n\n    import graphviz\n\n    g = graphviz.Digraph()\n\n    for label, node in trace.nodes.items():\n        if site_is_subsample(node):\n            continue\n        shape = \'ellipse\'\n        if label in trace.stochastic_nodes and label not in trace.reparameterized_nodes:\n            fillcolor = \'salmon\'\n        elif label in trace.reparameterized_nodes:\n            fillcolor = \'lightgrey;.5:salmon\'\n        elif label in trace.observation_nodes:\n            fillcolor = \'darkolivegreen3\'\n        else:\n            # only visualize RVs\n            continue\n        g.node(label, label=label, shape=shape, style=\'filled\', fillcolor=fillcolor)\n\n    for label1, label2 in trace.edges:\n        if site_is_subsample(trace.nodes[label1]):\n            continue\n        if site_is_subsample(trace.nodes[label2]):\n            continue\n        g.edge(label1, label2)\n\n    g.render(graph_output, view=False, cleanup=True)\n\n\ndef check_traces_match(trace1, trace2):\n    """"""\n    :param pyro.poutine.Trace trace1: Trace object of the model\n    :param pyro.poutine.Trace trace2: Trace object of the guide\n    :raises: RuntimeWarning, ValueError\n\n    Checks that (1) there is a bijection between the samples in the two traces\n    and (2) at each sample site two traces agree on sample shape.\n    """"""\n    # Check ordinary sample sites.\n    vars1 = set(name for name, site in trace1.nodes.items() if site[""type""] == ""sample"")\n    vars2 = set(name for name, site in trace2.nodes.items() if site[""type""] == ""sample"")\n    if vars1 != vars2:\n        warnings.warn(""Model vars changed: {} vs {}"".format(vars1, vars2))\n\n    # Check shapes agree.\n    for name in vars1:\n        site1 = trace1.nodes[name]\n        site2 = trace2.nodes[name]\n        if hasattr(site1[""fn""], ""shape"") and hasattr(site2[""fn""], ""shape""):\n            shape1 = site1[""fn""].shape(*site1[""args""], **site1[""kwargs""])\n            shape2 = site2[""fn""].shape(*site2[""args""], **site2[""kwargs""])\n            if shape1 != shape2:\n                raise ValueError(""Site dims disagree at site \'{}\': {} vs {}"".format(name, shape1, shape2))\n\n\ndef check_model_guide_match(model_trace, guide_trace, max_plate_nesting=math.inf):\n    """"""\n    :param pyro.poutine.Trace model_trace: Trace object of the model\n    :param pyro.poutine.Trace guide_trace: Trace object of the guide\n    :raises: RuntimeWarning, ValueError\n\n    Checks the following assumptions:\n    1. Each sample site in the model also appears in the guide and is not\n        marked auxiliary.\n    2. Each sample site in the guide either appears in the model or is marked,\n        auxiliary via ``infer={\'is_auxiliary\': True}``.\n    3. Each :class:``~pyro.plate`` statement in the guide also appears in the\n        model.\n    4. At each sample site that appears in both the model and guide, the model\n        and guide agree on sample shape.\n    """"""\n    # Check ordinary sample sites.\n    guide_vars = set(name for name, site in guide_trace.nodes.items()\n                     if site[""type""] == ""sample""\n                     if type(site[""fn""]).__name__ != ""_Subsample"")\n    aux_vars = set(name for name, site in guide_trace.nodes.items()\n                   if site[""type""] == ""sample""\n                   if site[""infer""].get(""is_auxiliary""))\n    model_vars = set(name for name, site in model_trace.nodes.items()\n                     if site[""type""] == ""sample"" and not site[""is_observed""]\n                     if type(site[""fn""]).__name__ != ""_Subsample"")\n    enum_vars = set(name for name, site in model_trace.nodes.items()\n                    if site[""type""] == ""sample"" and not site[""is_observed""]\n                    if type(site[""fn""]).__name__ != ""_Subsample""\n                    if site[""infer""].get(""_enumerate_dim"") is not None\n                    if name not in guide_vars)\n    if aux_vars & model_vars:\n        warnings.warn(""Found auxiliary vars in the model: {}"".format(aux_vars & model_vars))\n    if not (guide_vars <= model_vars | aux_vars):\n        warnings.warn(""Found non-auxiliary vars in guide but not model, ""\n                      ""consider marking these infer={{\'is_auxiliary\': True}}:\\n{}"".format(\n                          guide_vars - aux_vars - model_vars))\n    if not (model_vars <= guide_vars | enum_vars):\n        warnings.warn(""Found vars in model but not guide: {}"".format(model_vars - guide_vars - enum_vars))\n\n    # Check shapes agree.\n    for name in model_vars & guide_vars:\n        model_site = model_trace.nodes[name]\n        guide_site = guide_trace.nodes[name]\n\n        if hasattr(model_site[""fn""], ""event_dim"") and hasattr(guide_site[""fn""], ""event_dim""):\n            if model_site[""fn""].event_dim != guide_site[""fn""].event_dim:\n                raise ValueError(""Model and guide event_dims disagree at site \'{}\': {} vs {}"".format(\n                    name, model_site[""fn""].event_dim, guide_site[""fn""].event_dim))\n\n        if hasattr(model_site[""fn""], ""shape"") and hasattr(guide_site[""fn""], ""shape""):\n            model_shape = model_site[""fn""].shape(*model_site[""args""], **model_site[""kwargs""])\n            guide_shape = guide_site[""fn""].shape(*guide_site[""args""], **guide_site[""kwargs""])\n            if model_shape == guide_shape:\n                continue\n\n            # Allow broadcasting outside of max_plate_nesting.\n            if len(model_shape) > max_plate_nesting:\n                model_shape = model_shape[len(model_shape) - max_plate_nesting - model_site[""fn""].event_dim:]\n            if len(guide_shape) > max_plate_nesting:\n                guide_shape = guide_shape[len(guide_shape) - max_plate_nesting - guide_site[""fn""].event_dim:]\n            if model_shape == guide_shape:\n                continue\n            for model_size, guide_size in zip_longest(reversed(model_shape), reversed(guide_shape), fillvalue=1):\n                if model_size != guide_size:\n                    raise ValueError(""Model and guide shapes disagree at site \'{}\': {} vs {}"".format(\n                        name, model_shape, guide_shape))\n\n    # Check subsample sites introduced by plate.\n    model_vars = set(name for name, site in model_trace.nodes.items()\n                     if site[""type""] == ""sample"" and not site[""is_observed""]\n                     if type(site[""fn""]).__name__ == ""_Subsample"")\n    guide_vars = set(name for name, site in guide_trace.nodes.items()\n                     if site[""type""] == ""sample""\n                     if type(site[""fn""]).__name__ == ""_Subsample"")\n    if not (guide_vars <= model_vars):\n        warnings.warn(""Found plate statements in guide but not model: {}"".format(guide_vars - model_vars))\n\n\ndef check_site_shape(site, max_plate_nesting):\n    actual_shape = list(site[""log_prob""].shape)\n\n    # Compute expected shape.\n    expected_shape = []\n    for f in site[""cond_indep_stack""]:\n        if f.dim is not None:\n            # Use the specified plate dimension, which counts from the right.\n            assert f.dim < 0\n            if len(expected_shape) < -f.dim:\n                expected_shape = [None] * (-f.dim - len(expected_shape)) + expected_shape\n            if expected_shape[f.dim] is not None:\n                raise ValueError(\'\\n  \'.join([\n                    \'at site ""{}"" within plate(""{}"", dim={}), dim collision\'.format(site[""name""], f.name, f.dim),\n                    \'Try setting dim arg in other plates.\']))\n            expected_shape[f.dim] = f.size\n    expected_shape = [-1 if e is None else e for e in expected_shape]\n\n    # Check for plate stack overflow.\n    if len(expected_shape) > max_plate_nesting:\n        raise ValueError(\'\\n  \'.join([\n            \'at site ""{}"", plate stack overflow\'.format(site[""name""]),\n            \'Try increasing max_plate_nesting to at least {}\'.format(len(expected_shape))]))\n\n    # Ignore dimensions left of max_plate_nesting.\n    if max_plate_nesting < len(actual_shape):\n        actual_shape = actual_shape[len(actual_shape) - max_plate_nesting:]\n\n    # Check for incorrect plate placement on the right of max_plate_nesting.\n    for actual_size, expected_size in zip_longest(reversed(actual_shape), reversed(expected_shape), fillvalue=1):\n        if expected_size != -1 and expected_size != actual_size:\n            raise ValueError(\'\\n  \'.join([\n                \'at site ""{}"", invalid log_prob shape\'.format(site[""name""]),\n                \'Expected {}, actual {}\'.format(expected_shape, actual_shape),\n                \'Try one of the following fixes:\',\n                \'- enclose the batched tensor in a with plate(...): context\',\n                \'- .to_event(...) the distribution being sampled\',\n                \'- .permute() data dimensions\']))\n\n    # Check parallel dimensions on the left of max_plate_nesting.\n    enum_dim = site[""infer""].get(""_enumerate_dim"")\n    if enum_dim is not None:\n        if len(site[""fn""].batch_shape) >= -enum_dim and site[""fn""].batch_shape[enum_dim] != 1:\n            raise ValueError(\'\\n  \'.join([\n                \'Enumeration dim conflict at site ""{}""\'.format(site[""name""]),\n                \'Try increasing pyro.markov history size\']))\n\n\ndef _are_independent(counters1, counters2):\n    for name, counter1 in counters1.items():\n        if name in counters2:\n            if counters2[name] != counter1:\n                return True\n    return False\n\n\ndef check_traceenum_requirements(model_trace, guide_trace):\n    """"""\n    Warn if user could easily rewrite the model or guide in a way that would\n    clearly avoid invalid dependencies on enumerated variables.\n\n    :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO` enumerates over\n    synchronized products rather than full cartesian products. Therefore models\n    must ensure that no variable outside of an plate depends on an enumerated\n    variable inside that plate. Since full dependency checking is impossible,\n    this function aims to warn only in cases where models can be easily\n    rewitten to be obviously correct.\n    """"""\n    enumerated_sites = set(name for name, site in guide_trace.nodes.items()\n                           if site[""type""] == ""sample"" and site[""infer""].get(""enumerate""))\n    for role, trace in [(\'model\', model_trace), (\'guide\', guide_trace)]:\n        plate_counters = {}  # for sequential plates only\n        enumerated_contexts = defaultdict(set)\n        for name, site in trace.nodes.items():\n            if site[""type""] != ""sample"":\n                continue\n            plate_counter = {f.name: f.counter for f in site[""cond_indep_stack""] if not f.vectorized}\n            context = frozenset(f for f in site[""cond_indep_stack""] if f.vectorized)\n\n            # Check that sites outside each independence context precede enumerated sites inside that context.\n            for enumerated_context, names in enumerated_contexts.items():\n                if not (context < enumerated_context):\n                    continue\n                names = sorted(n for n in names if not _are_independent(plate_counter, plate_counters[n]))\n                if not names:\n                    continue\n                diff = sorted(f.name for f in enumerated_context - context)\n                warnings.warn(\'\\n  \'.join([\n                    \'at {} site ""{}"", possibly invalid dependency.\'.format(role, name),\n                    \'Expected site ""{}"" to precede sites ""{}""\'.format(name, \'"", ""\'.join(sorted(names))),\n                    \'to avoid breaking independence of plates ""{}""\'.format(\'"", ""\'.join(diff)),\n                ]), RuntimeWarning)\n\n            plate_counters[name] = plate_counter\n            if name in enumerated_sites:\n                enumerated_contexts[context].add(name)\n\n\ndef check_if_enumerated(guide_trace):\n    enumerated_sites = [name for name, site in guide_trace.nodes.items()\n                        if site[""type""] == ""sample"" and site[""infer""].get(""enumerate"")]\n    if enumerated_sites:\n        warnings.warn(\'\\n\'.join([\n            \'Found sample sites configured for enumeration:\'\n            \', \'.join(enumerated_sites),\n            \'If you want to enumerate sites, you need to use TraceEnum_ELBO instead.\']))\n\n\n@contextmanager\ndef ignore_jit_warnings(filter=None):\n    """"""\n    Ignore JIT tracer warnings with messages that match `filter`. If\n    `filter` is not specified all tracer warnings are ignored.\n\n    Note this only installs warning filters if executed within traced code.\n\n    :param filter: A list containing either warning message (str),\n        or tuple consisting of (warning message (str), Warning class).\n    """"""\n    if not torch._C._get_tracing_state():\n        yield\n        return\n\n    with warnings.catch_warnings():\n        if filter is None:\n            warnings.filterwarnings(""ignore"",\n                                    category=torch.jit.TracerWarning)\n        else:\n            for msg in filter:\n                category = torch.jit.TracerWarning\n                if isinstance(msg, tuple):\n                    msg, category = msg\n                warnings.filterwarnings(""ignore"",\n                                        category=category,\n                                        message=msg)\n        yield\n\n\ndef jit_iter(tensor):\n    """"""\n    Iterate over a tensor, ignoring jit warnings.\n    """"""\n    # The ""Iterating over a tensor"" warning is erroneously a RuntimeWarning\n    # so we use a custom filter here.\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", ""Iterating over a tensor"")\n        warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n        return list(tensor)\n\n\nclass optional:\n    """"""\n    Optionally wrap inside `context_manager` if condition is `True`.\n    """"""\n    def __init__(self, context_manager, condition):\n        self.context_manager = context_manager\n        self.condition = condition\n\n    def __enter__(self):\n        if self.condition:\n            return self.context_manager.__enter__()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.condition:\n            return self.context_manager.__exit__(exc_type, exc_val, exc_tb)\n\n\nclass ExperimentalWarning(UserWarning):\n    pass\n\n\n@contextmanager\ndef ignore_experimental_warning():\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\'ignore\', category=ExperimentalWarning)\n        yield\n\n\ndef deep_getattr(obj, name):\n    """"""\n    Python getattr() for arbitrarily deep attributes\n    Throws an AttributeError if bad attribute\n    """"""\n    return functools.reduce(getattr, name.split("".""), obj)\n\n\nclass timed:\n    def __enter__(self, timer=timeit.default_timer):\n        self.start = timer()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.end = timeit.default_timer()\n        self.elapsed = self.end - self.start\n        return self.elapsed\n\n\ndef torch_float(x):\n    return x.float() if isinstance(x, torch.Tensor) else float(x)\n'"
scripts/update_headers.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport glob\nimport os\nimport sys\n\nroot = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nblacklist = [""/build/"", ""/dist/""]\nfile_types = [\n    (""*.py"", ""# {}""),\n    (""*.cpp"", ""// {}""),\n]\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--check"", action=""store_true"")\nargs = parser.parse_args()\ndirty = []\n\nfor basename, comment in file_types:\n    copyright_line = comment.format(""Copyright Contributors to the Pyro project.\\n"")\n    # See https://spdx.org/ids-how\n    spdx_line = comment.format(""SPDX-License-Identifier: Apache-2.0\\n"")\n\n    filenames = glob.glob(os.path.join(root, ""**"", basename), recursive=True)\n    filenames.sort()\n    filenames = [\n        filename\n        for filename in filenames\n        if not any(word in filename for word in blacklist)\n    ]\n    for filename in filenames:\n        with open(filename) as f:\n            lines = f.readlines()\n\n        # Ignore empty files like __init__.py\n        if all(line.isspace() for line in lines):\n            continue\n\n        # Ensure first few line are copyright notices.\n        changed = False\n        lineno = 0\n        if not lines[lineno].startswith(comment.format(""Copyright"")):\n            lines.insert(lineno, copyright_line)\n            changed = True\n        lineno += 1\n        while lines[lineno].startswith(comment.format(""Copyright"")):\n            lineno += 1\n\n        # Ensure next line is an SPDX short identifier.\n        if not lines[lineno].startswith(comment.format(""SPDX-License-Identifier"")):\n            lines.insert(lineno, spdx_line)\n            changed = True\n        lineno += 1\n\n        # Ensure next line is blank.\n        if not lines[lineno].isspace():\n            lines.insert(lineno, ""\\n"")\n            changed = True\n\n        if not changed:\n            continue\n\n        if args.check:\n            dirty.append(filename)\n            continue\n\n        with open(filename, ""w"") as f:\n            f.write("""".join(lines))\n\n        print(""updated {}"".format(filename[len(root) + 1:]))\n\nif dirty:\n    print(""The following files need license headers:\\n{}""\n          .format(""\\n"".join(dirty)))\n    print(""Please run \'make license\'"")\n    sys.exit(1)\n'"
tests/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\nimport os\n\n# create log handler for tests\nlevel = logging.INFO if ""CI"" in os.environ else logging.DEBUG\nlogging.basicConfig(format=\'%(levelname).1s \\t %(message)s\', level=level)\n'"
tests/common.py,20,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport contextlib\nimport numbers\nimport os\nimport shutil\nimport tempfile\nimport warnings\nfrom itertools import product\n\nimport numpy as np\nimport pytest\nimport torch\nimport torch.cuda\nfrom numpy.testing import assert_allclose\nfrom pytest import approx\n\n""""""\nContains test utilities for assertions, approximate comparison (of tensors and other objects).\n\nCode has been largely adapted from pytorch/test/common.py\nSource: https://github.com/pytorch/pytorch/blob/master/test/common.py\n""""""\n\nTESTS_DIR = os.path.dirname(os.path.abspath(__file__))\nRESOURCE_DIR = os.path.join(TESTS_DIR, \'resources\')\nEXAMPLES_DIR = os.path.join(os.path.dirname(TESTS_DIR), \'examples\')\n\n\ndef xfail_param(*args, **kwargs):\n    return pytest.param(*args, marks=[pytest.mark.xfail(**kwargs)])\n\n\ndef skipif_param(*args, **kwargs):\n    return pytest.param(*args, marks=[pytest.mark.skipif(**kwargs)])\n\n\ndef suppress_warnings(fn):\n    def wrapper(*args, **kwargs):\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            fn(*args, **kwargs)\n\n    return wrapper\n\n\n# backport of Python 3\'s context manager\n@contextlib.contextmanager\ndef TemporaryDirectory():\n    try:\n        path = tempfile.mkdtemp()\n        yield path\n    finally:\n        if os.path.exists(path):\n            shutil.rmtree(path)\n\n\nrequires_cuda = pytest.mark.skipif(not torch.cuda.is_available(),\n                                   reason=""cuda is not available"")\n\n\ndef get_cpu_type(t):\n    assert t.__module__ == \'torch.cuda\'\n    return getattr(torch, t.__class__.__name__)\n\n\ndef get_gpu_type(t):\n    assert t.__module__ == \'torch\'\n    return getattr(torch.cuda, t.__name__)\n\n\n@contextlib.contextmanager\ndef tensors_default_to(host):\n    """"""\n    Context manager to temporarily use Cpu or Cuda tensors in PyTorch.\n\n    :param str host: Either ""cuda"" or ""cpu"".\n    """"""\n    assert host in (\'cpu\', \'cuda\'), host\n    old_module, name = torch.Tensor().type().rsplit(\'.\', 1)\n    new_module = \'torch.cuda\' if host == \'cuda\' else \'torch\'\n    torch.set_default_tensor_type(\'{}.{}\'.format(new_module, name))\n    try:\n        yield\n    finally:\n        torch.set_default_tensor_type(\'{}.{}\'.format(old_module, name))\n\n\n@contextlib.contextmanager\ndef freeze_rng_state():\n    rng_state = torch.get_rng_state()\n    if torch.cuda.is_available():\n        cuda_rng_state = torch.cuda.get_rng_state()\n    yield\n    if torch.cuda.is_available():\n        torch.cuda.set_rng_state(cuda_rng_state)\n    torch.set_rng_state(rng_state)\n\n\n@contextlib.contextmanager\ndef xfail_if_not_implemented(msg=""Not implemented""):\n    try:\n        yield\n    except NotImplementedError as e:\n        pytest.xfail(reason=""{}: {}"".format(msg, e))\n\n\ndef iter_indices(tensor):\n    if tensor.dim() == 0:\n        return range(0)\n    if tensor.dim() == 1:\n        return range(tensor.size(0))\n    return product(*(range(s) for s in tensor.size()))\n\n\ndef is_iterable(obj):\n    try:\n        iter(obj)\n        return True\n    except BaseException:\n        return False\n\n\ndef assert_tensors_equal(a, b, prec=0., msg=\'\'):\n    assert a.size() == b.size(), msg\n    if isinstance(prec, numbers.Number) and prec == 0:\n        assert (a == b).all(), msg\n    if a.numel() == 0 and b.numel() == 0:\n        return\n    b = b.type_as(a)\n    b = b.cuda(device=a.get_device()) if a.is_cuda else b.cpu()\n    # check that NaNs are in the same locations\n    nan_mask = a != a\n    assert torch.equal(nan_mask, b != b), msg\n    diff = a - b\n    diff[a == b] = 0  # handle inf\n    diff[nan_mask] = 0\n    if diff.is_signed():\n        diff = diff.abs()\n    if isinstance(prec, torch.Tensor):\n        assert (diff <= prec).all(), msg\n    else:\n        max_err = diff.max().item()\n        assert (max_err <= prec), msg\n\n\ndef _safe_coalesce(t):\n    tc = t.coalesce()\n    value_map = {}\n    for idx, val in zip(t._indices().t(), t._values()):\n        idx_tup = tuple(idx)\n        if idx_tup in value_map:\n            value_map[idx_tup] += val\n        else:\n            value_map[idx_tup] = val.clone() if torch.is_tensor(val) else val\n\n    new_indices = sorted(list(value_map.keys()))\n    new_values = [value_map[idx] for idx in new_indices]\n    if t._values().dim() < 2:\n        new_values = t._values().new_tensor(new_values)\n    else:\n        new_values = torch.stack(new_values)\n\n    new_indices = t._indices().new_tensor(new_indices).t()\n    tg = t.new(new_indices, new_values, t.size())\n\n    assert (tc._indices() == tg._indices()).all()\n    assert (tc._values() == tg._values()).all()\n    return tg\n\n\ndef assert_close(actual, expected, atol=1e-7, rtol=0, msg=\'\'):\n    if not msg:\n        msg = \'{} vs {}\'.format(actual, expected)\n    if isinstance(actual, numbers.Number) and isinstance(expected, numbers.Number):\n        assert actual == approx(expected, abs=atol, rel=rtol), msg\n    # Placing this as a second check allows for coercing of numeric types above;\n    # this can be moved up to harden type checks.\n    elif type(actual) != type(expected):\n        raise AssertionError(""cannot compare {} and {}"".format(type(actual),\n                                                               type(expected)))\n    elif torch.is_tensor(actual) and torch.is_tensor(expected):\n        prec = atol + rtol * abs(expected) if rtol > 0 else atol\n        assert actual.is_sparse == expected.is_sparse, msg\n        if actual.is_sparse:\n            x = _safe_coalesce(actual)\n            y = _safe_coalesce(expected)\n            assert_tensors_equal(x._indices(), y._indices(), prec, msg)\n            assert_tensors_equal(x._values(), y._values(), prec, msg)\n        else:\n            assert_tensors_equal(actual, expected, prec, msg)\n    elif type(actual) == np.ndarray and type(expected) == np.ndarray:\n        assert_allclose(actual, expected, atol=atol, rtol=rtol, equal_nan=True, err_msg=msg)\n    elif isinstance(actual, numbers.Number) and isinstance(y, numbers.Number):\n        assert actual == approx(expected, abs=atol, rel=rtol), msg\n    elif isinstance(actual, dict):\n        assert set(actual.keys()) == set(expected.keys())\n        for key, x_val in actual.items():\n            assert_close(x_val, expected[key], atol=atol, rtol=rtol,\n                         msg=\'At key{}: {} vs {}\'.format(key, x_val, expected[key]))\n    elif isinstance(actual, str):\n        assert actual == expected, msg\n    elif is_iterable(actual) and is_iterable(expected):\n        assert len(actual) == len(expected), msg\n        for xi, yi in zip(actual, expected):\n            assert_close(xi, yi, atol=atol, rtol=rtol, msg=\'{} vs {}\'.format(xi, yi))\n    else:\n        assert actual == expected, msg\n\n\n# TODO: Remove `prec` arg, and move usages to assert_close\ndef assert_equal(actual, expected, prec=1e-5, msg=\'\'):\n    if prec > 0.:\n        return assert_close(actual, expected, atol=prec, msg=msg)\n    if not msg:\n        msg = \'{} vs {}\'.format(actual, expected)\n    if isinstance(actual, numbers.Number) and isinstance(expected, numbers.Number):\n        assert actual == expected, msg\n    # Placing this as a second check allows for coercing of numeric types above;\n    # this can be moved up to harden type checks.\n    elif type(actual) != type(expected):\n        raise AssertionError(""cannot compare {} and {}"".format(type(actual),\n                                                               type(expected)))\n    elif torch.is_tensor(actual) and torch.is_tensor(expected):\n        assert actual.is_sparse == expected.is_sparse, msg\n        if actual.is_sparse:\n            x = _safe_coalesce(actual)\n            y = _safe_coalesce(expected)\n            assert_tensors_equal(x._indices(), y._indices(), msg=msg)\n            assert_tensors_equal(x._values(), y._values(), msg=msg)\n        else:\n            assert_tensors_equal(actual, expected, msg=msg)\n    elif type(actual) == np.ndarray and type(actual) == np.ndarray:\n        assert (actual == expected).all(), msg\n    elif isinstance(actual, dict):\n        assert set(actual.keys()) == set(expected.keys())\n        for key, x_val in actual.items():\n            assert_equal(x_val, expected[key], prec=0.,\n                         msg=\'At key{}: {} vs {}\'.format(key, x_val, expected[key]))\n    elif isinstance(actual, str):\n        assert actual == expected, msg\n    elif is_iterable(actual) and is_iterable(expected):\n        assert len(actual) == len(expected), msg\n        for xi, yi in zip(actual, expected):\n            assert_equal(xi, yi, prec=0., msg=\'{} vs {}\'.format(xi, yi))\n    else:\n        assert actual == expected, msg\n\n\ndef assert_not_equal(x, y, prec=1e-5, msg=\'\'):\n    try:\n        assert_equal(x, y, prec)\n    except AssertionError:\n        return\n    raise AssertionError(""{} \\nValues are equal: x={}, y={}, prec={}"".format(msg, x, y, prec))\n'"
tests/conftest.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport warnings\n\nimport pytest\nimport torch\n\nimport pyro\n\n\ntorch.set_default_tensor_type(os.environ.get(\'PYRO_TENSOR_TYPE\', \'torch.DoubleTensor\'))\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(""markers"",\n                            ""init(rng_seed): initialize the RNG using the seed provided."")\n    config.addinivalue_line(""markers"",\n                            ""stage(NAME): mark test to run when testing stage matches NAME."")\n    config.addinivalue_line(""markers"",\n                            ""disable_validation: disable all validation on this test."")\n\n\ndef pytest_runtest_setup(item):\n    pyro.clear_param_store()\n    if item.get_closest_marker(""disable_validation""):\n        pyro.enable_validation(False)\n    else:\n        pyro.enable_validation(True)\n    test_initialize_marker = item.get_closest_marker(""init"")\n    if test_initialize_marker:\n        rng_seed = test_initialize_marker.kwargs[""rng_seed""]\n        pyro.set_rng_seed(rng_seed)\n\n\ndef pytest_addoption(parser):\n    parser.addoption(""--stage"",\n                     action=""append"",\n                     metavar=""NAME"",\n                     default=[],\n                     help=""Only run tests matching the stage NAME."")\n\n    parser.addoption(""--lax"",\n                     action=""store_true"",\n                     default=False,\n                     help=""Ignore AssertionError when running tests."")\n\n\ndef _get_highest_specificity_marker(stage_marker):\n    """"""\n    Get the most specific stage marker corresponding to the test. Specificity\n    of test function marker is the highest, followed by test class marker and\n    module marker.\n\n    :return: List of most specific stage markers for the test.\n    """"""\n    is_test_collected = False\n    selected_stages = []\n    try:\n        for marker in stage_marker:\n            selected_stages = list(marker.args)\n            is_test_collected = True\n            break\n    except TypeError:\n        selected_stages = list(stage_marker.args)\n        is_test_collected = True\n    if not is_test_collected:\n        raise RuntimeError(""stage marker needs at least one stage to be specified."")\n    return selected_stages\n\n\ndef _add_marker(marker, items):\n    for item in items:\n        item.add_marker(marker)\n\n\ndef pytest_collection_modifyitems(config, items):\n    test_stages = set(config.getoption(""--stage""))\n\n    # add dynamic markers\n    lax = config.getoption(""--lax"")\n    if lax:\n        _add_marker(pytest.mark.xfail(raises=AssertionError), items)\n\n    # select / deselect tests based on stage criterion\n    if not test_stages or ""all"" in test_stages:\n        return\n    selected_items = []\n    deselected_items = []\n    for item in items:\n        stage_marker = item.get_closest_marker(""stage"")\n        if not stage_marker:\n            selected_items.append(item)\n            warnings.warn(""No stage associated with the test {}. Will run on each stage invocation."".format(item.name))\n            continue\n        item_stage_markers = _get_highest_specificity_marker(stage_marker)\n        if test_stages.isdisjoint(item_stage_markers):\n            deselected_items.append(item)\n        else:\n            selected_items.append(item)\n    config.hook.pytest_deselected(items=deselected_items)\n    items[:] = selected_items\n'"
tests/doctest_fixtures.py,0,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.contrib.gp as gp\nimport pyro.contrib.autoname.named as named\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer import EmpiricalMarginal\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.infer.mcmc import HMC, NUTS\nfrom pyro.params import param_with_module_name\n\n\n# Fix seed for all doctest runs.\npyro.set_rng_seed(0)\n\n\n@pytest.fixture(autouse=True)\ndef add_imports(doctest_namespace):\n    doctest_namespace['dist'] = dist\n    doctest_namespace['gp'] = gp\n    doctest_namespace['named'] = named\n    doctest_namespace['np'] = numpy\n    doctest_namespace['param_with_module_name'] = param_with_module_name\n    doctest_namespace['poutine'] = poutine\n    doctest_namespace['pyro'] = pyro\n    doctest_namespace['torch'] = torch\n    doctest_namespace['EmpiricalMarginal'] = EmpiricalMarginal\n    doctest_namespace['HMC'] = HMC\n    doctest_namespace['MCMC'] = MCMC\n    doctest_namespace['NUTS'] = NUTS\n"""
tests/test_examples.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport os\nimport sys\nfrom subprocess import check_call\n\nimport pytest\n\nfrom tests.common import EXAMPLES_DIR, requires_cuda, xfail_param\n\nlogger = logging.getLogger(__name__)\npytestmark = pytest.mark.stage(\'test_examples\')\n\n\nCPU_EXAMPLES = [\n    \'air/main.py --num-steps=1\',\n    \'air/main.py --num-steps=1 --no-baseline\',\n    \'baseball.py --num-samples=200 --warmup-steps=100 --num-chains=2\',\n    \'lkj.py --n=50 --num-chains=1 --warmup-steps=100 --num-samples=200\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 1\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 2\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 3\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 4\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 5\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 1 --tmc --tmc-num-samples=2\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 2 --tmc --tmc-num-samples=2\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 3 --tmc --tmc-num-samples=2\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 4 --tmc --tmc-num-samples=2\',\n    \'capture_recapture/cjs.py --num-steps=1 -m 5 --tmc --tmc-num-samples=2\',\n    \'contrib/autoname/scoping_mixture.py --num-epochs=1\',\n    \'contrib/autoname/mixture.py --num-epochs=1\',\n    \'contrib/autoname/tree_data.py --num-epochs=1\',\n    \'contrib/cevae/synthetic.py --num-epochs=1\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 -e=2\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 -k=1\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 -e=2 -k=1\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 --haar\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 -nb=8\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 -hfm=3\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 -a\',\n    \'contrib/epidemiology/sir.py -np=128 -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 -o=0.2\',\n    \'contrib/epidemiology/regional.py -t=2 -w=2 -n=4 -r=3 -d=20 -p=1000 -f 2\',\n    \'contrib/epidemiology/regional.py -t=2 -w=2 -n=4 -r=3 -d=20 -p=1000 -f 2 --haar\',\n    \'contrib/epidemiology/regional.py -t=2 -w=2 -n=4 -r=3 -d=20 -p=1000 -f 2 -hfm=3\',\n    \'contrib/forecast/bart.py --num-steps=2 --stride=99999\',\n    \'contrib/gp/sv-dkl.py --epochs=1 --num-inducing=4 --batch-size=1000\',\n    \'contrib/gp/sv-dkl.py --binary --epochs=1 --num-inducing=4 --batch-size=1000\',\n    \'contrib/oed/ab_test.py --num-vi-steps=10 --num-bo-steps=2\',\n    \'contrib/timeseries/gp_models.py -m imgp --test --num-steps=2\',\n    \'contrib/timeseries/gp_models.py -m lcmgp --test --num-steps=2\',\n    \'dmm/dmm.py --num-epochs=1\',\n    \'dmm/dmm.py --num-epochs=1 --tmcelbo --tmc-num-samples=2\',\n    \'dmm/dmm.py --num-epochs=1 --num-iafs=1\',\n    \'dmm/dmm.py --num-epochs=1 --tmc --tmc-num-samples=2\',\n    \'dmm/dmm.py --num-epochs=1 --tmcelbo --tmc-num-samples=2\',\n    \'eight_schools/mcmc.py --num-samples=500 --warmup-steps=100\',\n    \'eight_schools/svi.py --num-epochs=1\',\n    \'einsum.py\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=0\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=1\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=2\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=3\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=4\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=5\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=6\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=6 --raftery-parameterization\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=7\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=0 --tmc --tmc-num-samples=2\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=1 --tmc --tmc-num-samples=2\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=2 --tmc --tmc-num-samples=2\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=3 --tmc --tmc-num-samples=2\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=4 --tmc --tmc-num-samples=2\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=5 --tmc --tmc-num-samples=2\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=6 --tmc --tmc-num-samples=2\',\n    \'inclined_plane.py --num-samples=1\',\n    \'lda.py --num-steps=2 --num-words=100 --num-docs=100 --num-words-per-doc=8\',\n    \'minipyro.py --backend=pyro\',\n    \'minipyro.py\',\n    \'mixed_hmm/experiment.py --timesteps=1\',\n    \'neutra.py -n 10 --num-warmup 10 --num-samples 10\',\n    \'rsa/generics.py --num-samples=10\',\n    \'rsa/hyperbole.py --price=10000\',\n    \'rsa/schelling.py --num-samples=10\',\n    \'rsa/schelling_false.py --num-samples=10\',\n    \'rsa/semantic_parsing.py --num-samples=10\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -d=2 -m=1 --enum\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -d=2 -p=10000 --sequential\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -d=100 -p=10000 -f 2\',\n    \'smcfilter.py --num-timesteps=3 --num-particles=10\',\n    \'sparse_gamma_def.py --num-epochs=2 --eval-particles=2 --eval-frequency=1 --guide custom\',\n    \'sparse_gamma_def.py --num-epochs=2 --eval-particles=2 --eval-frequency=1 --guide auto\',\n    \'sparse_gamma_def.py --num-epochs=2 --eval-particles=2 --eval-frequency=1 --guide easy\',\n    xfail_param(\'sparse_regression.py --num-steps=2 --num-data=50 --num-dimensions 20\',\n                reason=\'https://github.com/pyro-ppl/pyro/issues/2082\'),\n    \'vae/ss_vae_M2.py --num-epochs=1\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --aux-loss\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --enum-discrete=parallel\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --enum-discrete=sequential\',\n    \'vae/vae.py --num-epochs=1\',\n    \'vae/vae_comparison.py --num-epochs=1\',\n]\n\nCUDA_EXAMPLES = [\n    \'air/main.py --num-steps=1 --cuda\',\n    \'baseball.py --num-samples=200 --warmup-steps=100 --num-chains=2 --cuda\',\n    \'contrib/cevae/synthetic.py --num-epochs=1 --cuda\',\n    \'contrib/epidemiology/sir.py -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 --cuda\',\n    \'contrib/epidemiology/sir.py -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 -nb=16 --cuda\',\n    \'contrib/epidemiology/sir.py -t=2 -w=2 -n=4 -d=20 -p=1000 -f 2 --haar --cuda\',\n    \'contrib/epidemiology/regional.py -t=2 -w=2 -n=4 -r=3 -d=20 -p=1000 -f 2 --cuda\',\n    \'contrib/epidemiology/regional.py -t=2 -w=2 -n=4 -r=3 -d=20 -p=1000 -f 2 --haar --cuda\',\n    \'contrib/gp/sv-dkl.py --epochs=1 --num-inducing=4 --cuda\',\n    \'lkj.py --n=50 --num-chains=1 --warmup-steps=100 --num-samples=200 --cuda\',\n    \'dmm/dmm.py --num-epochs=1 --cuda\',\n    \'dmm/dmm.py --num-epochs=1 --num-iafs=1 --cuda\',\n    \'dmm/dmm.py --num-epochs=1 --tmc --tmc-num-samples=2 --cuda\',\n    \'dmm/dmm.py --num-epochs=1 --tmcelbo --tmc-num-samples=2 --cuda\',\n    \'einsum.py --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=0 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=1 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=2 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=3 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=4 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=5 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=6 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=6 --cuda --raftery-parameterization\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=7 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=0 --tmc --tmc-num-samples=2 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=1 --tmc --tmc-num-samples=2 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=2 --tmc --tmc-num-samples=2 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=3 --tmc --tmc-num-samples=2 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=4 --tmc --tmc-num-samples=2 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=5 --tmc --tmc-num-samples=2 --cuda\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=6 --tmc --tmc-num-samples=2 --cuda\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -d=2 -m=1 --enum --cuda\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -d=2 -p=10000 --sequential --cuda\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -d=100 -p=10000 --cuda\',\n    \'vae/vae.py --num-epochs=1 --cuda\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --cuda\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --aux-loss --cuda\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --enum-discrete=parallel --cuda\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --enum-discrete=sequential --cuda\',\n]\n\n\ndef xfail_jit(*args):\n    return pytest.param(*args, marks=[pytest.mark.xfail(reason=""not jittable""),\n                                      pytest.mark.skipif(\'CI\' in os.environ, reason=\'slow test\')])\n\n\nJIT_EXAMPLES = [\n    \'air/main.py --num-steps=1 --jit\',\n    \'baseball.py --num-samples=200 --warmup-steps=100 --jit\',\n    \'contrib/autoname/mixture.py --num-epochs=1 --jit\',\n    \'contrib/cevae/synthetic.py --num-epochs=1 --jit\',\n    xfail_jit(\'lkj.py --n=50 --num-chains=1 --warmup-steps=100 --num-samples=200 --jit\'),\n    xfail_jit(\'contrib/gp/sv-dkl.py --epochs=1 --num-inducing=4 --jit\'),\n    xfail_jit(\'dmm/dmm.py --num-epochs=1 --jit\'),\n    xfail_jit(\'dmm/dmm.py --num-epochs=1 --num-iafs=1 --jit\'),\n    \'eight_schools/mcmc.py --num-samples=500 --warmup-steps=100 --jit\',\n    \'eight_schools/svi.py --num-epochs=1 --jit\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=1 --jit\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=2 --jit\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=3 --jit\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=4 --jit\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=5 --jit\',\n    \'hmm.py --num-steps=1 --truncate=10 --model=7 --jit\',\n    xfail_jit(\'hmm.py --num-steps=1 --truncate=10 --model=1 --tmc --tmc-num-samples=2 --jit\'),\n    xfail_jit(\'hmm.py --num-steps=1 --truncate=10 --model=2 --tmc --tmc-num-samples=2 --jit\'),\n    xfail_jit(\'hmm.py --num-steps=1 --truncate=10 --model=3 --tmc --tmc-num-samples=2 --jit\'),\n    xfail_jit(\'hmm.py --num-steps=1 --truncate=10 --model=4 --tmc --tmc-num-samples=2 --jit\'),\n    \'lda.py --num-steps=2 --num-words=100 --num-docs=100 --num-words-per-doc=8 --jit\',\n    \'minipyro.py --backend=pyro --jit\',\n    \'minipyro.py --jit\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -d=2 -m=1 --enum --jit\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -d=2 -p=10000 --sequential --jit\',\n    \'sir_hmc.py -t=2 -w=2 -n=4 -p=10000 --jit\',\n    xfail_jit(\'vae/ss_vae_M2.py --num-epochs=1 --aux-loss --jit\'),\n    \'vae/ss_vae_M2.py --num-epochs=1 --enum-discrete=parallel --jit\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --enum-discrete=sequential --jit\',\n    \'vae/ss_vae_M2.py --num-epochs=1 --jit\',\n    \'vae/vae.py --num-epochs=1 --jit\',\n    \'vae/vae_comparison.py --num-epochs=1 --jit\',\n]\n\n\ndef test_coverage():\n    cpu_tests = set((e if isinstance(e, str) else e.values[0]).split()[0] for e in CPU_EXAMPLES)\n    cuda_tests = set((e if isinstance(e, str) else e.values[0]).split()[0] for e in CUDA_EXAMPLES)\n    jit_tests = set((e if isinstance(e, str) else e.values[0]).split()[0] for e in JIT_EXAMPLES)\n    for root, dirs, files in os.walk(EXAMPLES_DIR):\n        for basename in files:\n            if not basename.endswith(\'.py\'):\n                continue\n            path = os.path.join(root, basename)\n            with open(path) as f:\n                text = f.read()\n            example = os.path.relpath(path, EXAMPLES_DIR)\n            if \'__main__\' in text:\n                if example not in cpu_tests:\n                    pytest.fail(\'Example: {} not covered in CPU_EXAMPLES.\'.format(example))\n                if \'--cuda\' in text and example not in cuda_tests:\n                    pytest.fail(\'Example: {} not covered by CUDA_EXAMPLES.\'.format(example))\n                if \'--jit\' in text and example not in jit_tests:\n                    pytest.fail(\'Example: {} not covered by JIT_EXAMPLES.\'.format(example))\n\n\n@pytest.mark.parametrize(\'example\', CPU_EXAMPLES)\ndef test_cpu(example):\n    logger.info(\'Running:\\npython examples/{}\'.format(example))\n    example = example.split()\n    filename, args = example[0], example[1:]\n    filename = os.path.join(EXAMPLES_DIR, filename)\n    check_call([sys.executable, filename] + args)\n\n\n@requires_cuda\n@pytest.mark.parametrize(\'example\', CUDA_EXAMPLES)\ndef test_cuda(example):\n    logger.info(\'Running:\\npython examples/{}\'.format(example))\n    example = example.split()\n    filename, args = example[0], example[1:]\n    filename = os.path.join(EXAMPLES_DIR, filename)\n    check_call([sys.executable, filename] + args)\n\n\n@pytest.mark.parametrize(\'example\', JIT_EXAMPLES)\ndef test_jit(example):\n    logger.info(\'Running:\\npython examples/{}\'.format(example))\n    example = example.split()\n    filename, args = example[0], example[1:]\n    filename = os.path.join(EXAMPLES_DIR, filename)\n    check_call([sys.executable, filename] + args)\n'"
tests/test_generic.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\nfrom pyro.generic import handlers, infer, pyro, pyro_backend, ops\nfrom pyroapi.testing import MODELS\nfrom tests.common import xfail_if_not_implemented\n\npytestmark = pytest.mark.stage(\'unit\')\n\n\n@pytest.mark.filterwarnings(""ignore"", category=UserWarning)\n@pytest.mark.parametrize(\'model\', MODELS)\n@pytest.mark.parametrize(\'backend\', [\'pyro\'])\ndef test_mcmc_interface(model, backend):\n    with pyro_backend(backend), handlers.seed(rng_seed=20):\n        f = MODELS[model]()\n        model, args, kwargs = f[\'model\'], f.get(\'model_args\', ()), f.get(\'model_kwargs\', {})\n        nuts_kernel = infer.NUTS(model=model)\n        mcmc = infer.MCMC(nuts_kernel, num_samples=10, warmup_steps=10)\n        mcmc.run(*args, **kwargs)\n        mcmc.summary()\n\n\n@pytest.mark.parametrize(\'backend\', [\'pyro\', \'minipyro\'])\ndef test_not_implemented(backend):\n    with pyro_backend(backend):\n        pyro.sample  # should be implemented\n        pyro.param  # should be implemented\n        with pytest.raises(NotImplementedError):\n            pyro.nonexistent_primitive\n\n\n@pytest.mark.parametrize(\'model\', MODELS)\n@pytest.mark.parametrize(\'backend\', [\'minipyro\', \'pyro\'])\ndef test_model_sample(model, backend):\n    with pyro_backend(backend), handlers.seed(rng_seed=2), xfail_if_not_implemented():\n        f = MODELS[model]()\n        model, model_args = f[\'model\'], f.get(\'model_args\', ())\n        model(*model_args)\n\n\n@pytest.mark.parametrize(\'model\', MODELS)\n@pytest.mark.parametrize(\'backend\', [\'minipyro\', \'pyro\'])\ndef test_rng_seed(model, backend):\n    with pyro_backend(backend), handlers.seed(rng_seed=2), xfail_if_not_implemented():\n        f = MODELS[model]()\n        model, model_args = f[\'model\'], f.get(\'model_args\', ())\n        with handlers.seed(rng_seed=0):\n            expected = model(*model_args)\n        if expected is None:\n            pytest.skip()\n        with handlers.seed(rng_seed=0):\n            actual = model(*model_args)\n        assert ops.allclose(actual, expected)\n\n\n@pytest.mark.parametrize(\'model\', MODELS)\n@pytest.mark.parametrize(\'backend\', [\'minipyro\', \'pyro\'])\ndef test_rng_state(model, backend):\n    with pyro_backend(backend), handlers.seed(rng_seed=2), xfail_if_not_implemented():\n        f = MODELS[model]()\n        model, model_args = f[\'model\'], f.get(\'model_args\', ())\n        with handlers.seed(rng_seed=0):\n            model(*model_args)\n            expected = model(*model_args)\n        if expected is None:\n            pytest.skip()\n        with handlers.seed(rng_seed=0):\n            model(*model_args)\n            with handlers.seed(rng_seed=0):\n                model(*model_args)\n            actual = model(*model_args)\n        assert ops.allclose(actual, expected)\n\n\n@pytest.mark.parametrize(\'model\', MODELS)\n@pytest.mark.parametrize(\'backend\', [\'minipyro\', \'pyro\'])\ndef test_trace_handler(model, backend):\n    with pyro_backend(backend), handlers.seed(rng_seed=2), xfail_if_not_implemented():\n        f = MODELS[model]()\n        model, model_args, model_kwargs = f[\'model\'], f.get(\'model_args\', ()), f.get(\'model_kwargs\', {})\n        # should be implemented\n        handlers.trace(model).get_trace(*model_args, **model_kwargs)\n'"
tests/test_primitives.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport pyro\nimport pyro.distributions as dist\nimport torch\n\npytestmark = pytest.mark.stage(\'unit\')\n\n\ndef test_sample_ok():\n    x = pyro.sample(""x"", dist.Normal(0, 1))\n    assert isinstance(x, torch.Tensor)\n    assert x.shape == ()\n\n\ndef test_observe_warn():\n    with pytest.warns(RuntimeWarning):\n        pyro.sample(""x"", dist.Normal(0, 1),\n                    obs=torch.tensor(0.))\n\n\ndef test_param_ok():\n    x = pyro.param(""x"", torch.tensor(0.))\n    assert isinstance(x, torch.Tensor)\n    assert x.shape == ()\n\n\ndef test_deterministic_ok():\n    x = pyro.deterministic(""x"", torch.tensor(0.))\n    assert isinstance(x, torch.Tensor)\n    assert x.shape == ()\n'"
tests/test_util.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\nimport pytest\n\nimport torch\nfrom pyro import util\n\npytestmark = pytest.mark.stage(\'unit\')\n\n\ndef test_warn_if_nan():\n    # scalar case\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        x = float(\'inf\')\n        msg = ""example message""\n        y = util.warn_if_nan(x, msg)\n        assert y is x\n        assert len(w) == 0\n\n        x = float(\'nan\')\n        util.warn_if_nan(x, msg)\n        # Verify some things\n        assert len(w) == 1\n        assert msg in str(w[-1].message)\n\n    # tensor case\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        x = torch.ones(2)\n        msg = ""example message""\n        util.warn_if_nan(x, msg)\n        x[1] = float(\'nan\')\n        util.warn_if_nan(x, msg)\n        assert len(w) == 1\n        assert msg in str(w[-1].message)\n\n    # grad case\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        x = torch.ones(2, requires_grad=True)\n        util.warn_if_nan(x, msg)\n        y = x.sum()\n        y.backward([torch.tensor(float(\'nan\'))])\n        assert len(w) == 1\n        assert msg in str(w[-1].message)\n\n\ndef test_warn_if_inf():\n    # scalar case\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        x = 3\n        msg = ""example message""\n        y = util.warn_if_inf(x, msg, allow_posinf=True, allow_neginf=True)\n        assert y is x\n        assert len(w) == 0\n        x = float(\'inf\')\n        util.warn_if_inf(x, msg, allow_posinf=True)\n        assert len(w) == 0\n        util.warn_if_inf(x, msg, allow_neginf=True)\n        assert len(w) == 1\n        assert msg in str(w[-1].message)\n\n    # tensor case\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        x = torch.ones(2)\n        util.warn_if_inf(x, msg, allow_posinf=True, allow_neginf=True)\n        assert len(w) == 0\n        x[0] = float(\'inf\')\n        util.warn_if_inf(x, msg, allow_posinf=True)\n        assert len(w) == 0\n        util.warn_if_inf(x, msg, allow_neginf=True)\n        assert len(w) == 1\n        assert msg in str(w[-1].message)\n\n    # grad case\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        x = torch.ones(2, requires_grad=True)\n        util.warn_if_inf(x, msg, allow_posinf=True)\n        y = x.sum()\n        y.backward([torch.tensor(float(\'inf\'))])\n        assert len(w) == 0\n\n        x.grad = None\n        y.backward([torch.tensor(-float(\'inf\'))])\n        assert len(w) == 1\n        assert msg in str(w[-1].message)\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        z = torch.ones(2, requires_grad=True)\n        y = z.sum()\n        util.warn_if_inf(z, msg, allow_neginf=True)\n        y.backward([torch.tensor(-float(\'inf\'))])\n        assert len(w) == 0\n        z.grad = None\n        y.backward([torch.tensor(float(\'inf\'))])\n        assert len(w) == 1\n        assert msg in str(w[-1].message)\n'"
docs/source/conf.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport sys\n\nimport sphinx_rtd_theme\n\n\n# import pkg_resources\n\n# -*- coding: utf-8 -*-\n#\n# Pyro documentation build configuration file, created by\n# sphinx-quickstart on Thu Jun 15 17:16:14 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.intersphinx\',  #\n    \'sphinx.ext.todo\',  #\n    \'sphinx.ext.mathjax\',  #\n    \'sphinx.ext.ifconfig\',  #\n    \'sphinx.ext.viewcode\',  #\n    \'sphinx.ext.githubpages\',  #\n    \'sphinx.ext.graphviz\',  #\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n]\n\n# Disable documentation inheritance so as to avoid inheriting\n# docstrings in a different format, e.g. when the parent class\n# is a PyTorch class.\n\nautodoc_inherit_docstrings = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Pyro\'\ncopyright = u\'2017-2018, Uber Technologies, Inc\'\nauthor = u\'Uber AI Labs\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n\nversion = \'\'\n\nif \'READTHEDOCS\' not in os.environ:\n    # if developing locally, use pyro.__version__ as version\n    from pyro import __version__  # noqaE402\n    version = __version__\n\n# release version\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n# do not prepend module name to functions\nadd_module_names = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# logo\nhtml_logo = \'_static/img/pyro_logo_wide.png\'\n\n# logo\nhtml_favicon = \'_static/img/favicon/favicon.ico\'\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n\nhtml_theme_options = {\n    \'navigation_depth\': 3,\n    \'logo_only\': True,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\nhtml_style = \'css/pyro.css\'\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Pyrodoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'Pyro.tex\', u\'Pyro Documentation\', u\'Uber AI Labs\', \'manual\'),\n]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \'pyro\', u\'Pyro Documentation\', [author], 1)]\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Pyro\', u\'Pyro Documentation\', author, \'Pyro\',\n     \'Deep Universal Probabilistic Programming.\', \'Miscellaneous\'),\n]\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3/\', None),\n    \'torch\': (\'https://pytorch.org/docs/master/\', None),\n    \'funsor\': (\'http://funsor.pyro.ai/en/stable/\', None),\n    \'opt_einsum\': (\'https://optimized-einsum.readthedocs.io/en/stable/\', None),\n    \'scipy\': (\'https://docs.scipy.org/doc/scipy/reference/\', None),\n}\n\n# document class constructors (__init__ methods):\n"""""" comment out this functionality for now;\ndef skip(app, what, name, obj, skip, options):\n    if name == ""__init__"":\n        return False\n    return skip\n""""""\n\n\ndef setup(app):\n    app.add_stylesheet(\'css/pyro.css\')\n#     app.connect(""autodoc-skip-member"", skip)\n\n\n# @jpchen\'s hack to get rtd builder to install latest pytorch\n# See similar line in the install section of .travis.yml\nif \'READTHEDOCS\' in os.environ:\n    os.system(\'pip install torch==1.5.0+cpu torchvision==0.6.0+cpu \'\n              \'-f https://download.pytorch.org/whl/torch_stable.html\')\n'"
examples/air/air.py,29,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nAn implementation of the model described in [1].\n\n[1] Eslami, SM Ali, et al. ""Attend, infer, repeat: Fast scene\nunderstanding with generative models."" Advances in Neural Information\nProcessing Systems. 2016.\n""""""\n\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pyro\nimport pyro.distributions as dist\nfrom modules import MLP, Decoder, Encoder, Identity, Predict\n\n\n# Default prior success probability for z_pres.\ndef default_z_pres_prior_p(t):\n    return 0.5\n\n\nModelState = namedtuple(\'ModelState\', [\'x\', \'z_pres\', \'z_where\'])\nGuideState = namedtuple(\'GuideState\', [\'h\', \'c\', \'bl_h\', \'bl_c\', \'z_pres\', \'z_where\', \'z_what\'])\n\n\nclass AIR(nn.Module):\n    def __init__(self,\n                 num_steps,\n                 x_size,\n                 window_size,\n                 z_what_size,\n                 rnn_hidden_size,\n                 encoder_net=[],\n                 decoder_net=[],\n                 predict_net=[],\n                 embed_net=None,\n                 bl_predict_net=[],\n                 non_linearity=\'ReLU\',\n                 decoder_output_bias=None,\n                 decoder_output_use_sigmoid=False,\n                 use_masking=True,\n                 use_baselines=True,\n                 baseline_scalar=None,\n                 scale_prior_mean=3.0,\n                 scale_prior_sd=0.1,\n                 pos_prior_mean=0.0,\n                 pos_prior_sd=1.0,\n                 likelihood_sd=0.3,\n                 use_cuda=False):\n\n        super().__init__()\n\n        self.num_steps = num_steps\n        self.x_size = x_size\n        self.window_size = window_size\n        self.z_what_size = z_what_size\n        self.rnn_hidden_size = rnn_hidden_size\n        self.use_masking = use_masking\n        self.use_baselines = use_baselines\n        self.baseline_scalar = baseline_scalar\n        self.likelihood_sd = likelihood_sd\n        self.use_cuda = use_cuda\n        prototype = torch.tensor(0.).cuda() if use_cuda else torch.tensor(0.)\n        self.options = dict(dtype=prototype.dtype, device=prototype.device)\n\n        self.z_pres_size = 1\n        self.z_where_size = 3\n        # By making these parameters they will be moved to the gpu\n        # when necessary. (They are not registered with pyro for\n        # optimization.)\n        self.z_where_loc_prior = nn.Parameter(\n            torch.FloatTensor([scale_prior_mean, pos_prior_mean, pos_prior_mean]),\n            requires_grad=False)\n        self.z_where_scale_prior = nn.Parameter(\n            torch.FloatTensor([scale_prior_sd, pos_prior_sd, pos_prior_sd]),\n            requires_grad=False)\n\n        # Create nn modules.\n        rnn_input_size = x_size ** 2 if embed_net is None else embed_net[-1]\n        rnn_input_size += self.z_where_size + z_what_size + self.z_pres_size\n        nl = getattr(nn, non_linearity)\n\n        self.rnn = nn.LSTMCell(rnn_input_size, rnn_hidden_size)\n        self.encode = Encoder(window_size ** 2, encoder_net, z_what_size, nl)\n        self.decode = Decoder(window_size ** 2, decoder_net, z_what_size,\n                              decoder_output_bias, decoder_output_use_sigmoid, nl)\n        self.predict = Predict(rnn_hidden_size, predict_net, self.z_pres_size, self.z_where_size, nl)\n        self.embed = Identity() if embed_net is None else MLP(x_size ** 2, embed_net, nl, True)\n\n        self.bl_rnn = nn.LSTMCell(rnn_input_size, rnn_hidden_size)\n        self.bl_predict = MLP(rnn_hidden_size, bl_predict_net + [1], nl)\n        self.bl_embed = Identity() if embed_net is None else MLP(x_size ** 2, embed_net, nl, True)\n\n        # Create parameters.\n        self.h_init = nn.Parameter(torch.zeros(1, rnn_hidden_size))\n        self.c_init = nn.Parameter(torch.zeros(1, rnn_hidden_size))\n        self.bl_h_init = nn.Parameter(torch.zeros(1, rnn_hidden_size))\n        self.bl_c_init = nn.Parameter(torch.zeros(1, rnn_hidden_size))\n        self.z_where_init = nn.Parameter(torch.zeros(1, self.z_where_size))\n        self.z_what_init = nn.Parameter(torch.zeros(1, self.z_what_size))\n\n        if use_cuda:\n            self.cuda()\n\n    def prior(self, n, **kwargs):\n\n        state = ModelState(\n            x=torch.zeros(n, self.x_size, self.x_size, **self.options),\n            z_pres=torch.ones(n, self.z_pres_size, **self.options),\n            z_where=None)\n\n        z_pres = []\n        z_where = []\n\n        for t in range(self.num_steps):\n            state = self.prior_step(t, n, state, **kwargs)\n            z_where.append(state.z_where)\n            z_pres.append(state.z_pres)\n\n        return (z_where, z_pres), state.x\n\n    def prior_step(self, t, n, prev, z_pres_prior_p=default_z_pres_prior_p):\n\n        # Sample presence indicators.\n        z_pres = pyro.sample(\'z_pres_{}\'.format(t),\n                             dist.Bernoulli(z_pres_prior_p(t) * prev.z_pres)\n                                 .to_event(1))\n\n        # If zero is sampled for a data point, then no more objects\n        # will be added to its output image. We can\'t\n        # straight-forwardly avoid generating further objects, so\n        # instead we zero out the log_prob_sum of future choices.\n        sample_mask = z_pres if self.use_masking else torch.tensor(1.0)\n\n        # Sample attention window position.\n        z_where = pyro.sample(\'z_where_{}\'.format(t),\n                              dist.Normal(self.z_where_loc_prior.expand(n, self.z_where_size),\n                                          self.z_where_scale_prior.expand(n, self.z_where_size))\n                                  .mask(sample_mask)\n                                  .to_event(1))\n\n        # Sample latent code for contents of the attention window.\n        z_what = pyro.sample(\'z_what_{}\'.format(t),\n                             dist.Normal(torch.zeros(n, self.z_what_size, **self.options),\n                                         torch.ones(n, self.z_what_size, **self.options))\n                                 .mask(sample_mask)\n                                 .to_event(1))\n\n        # Map latent code to pixel space.\n        y_att = self.decode(z_what)\n\n        # Position/scale attention window within larger image.\n        y = window_to_image(z_where, self.window_size, self.x_size, y_att)\n\n        # Combine the image generated at this step with the image so far.\n        # (Note that there\'s no notion of occlusion here. Overlapping\n        # objects can create pixel intensities > 1.)\n        x = prev.x + (y * z_pres.view(-1, 1, 1))\n\n        return ModelState(x=x, z_pres=z_pres, z_where=z_where)\n\n    def model(self, data, batch_size, **kwargs):\n        pyro.module(""decode"", self.decode)\n        with pyro.plate(\'data\', data.size(0), device=data.device) as ix:\n            batch = data[ix]\n            n = batch.size(0)\n            (z_where, z_pres), x = self.prior(n, **kwargs)\n            pyro.sample(\'obs\',\n                        dist.Normal(x.view(n, -1),\n                                    (self.likelihood_sd * torch.ones(n, self.x_size ** 2, **self.options)))\n                            .to_event(1),\n                        obs=batch.view(n, -1))\n\n    def guide(self, data, batch_size, **kwargs):\n        pyro.module(\'rnn\', self.rnn),\n        pyro.module(\'predict\', self.predict),\n        pyro.module(\'encode\', self.encode),\n        pyro.module(\'embed\', self.embed),\n        pyro.module(\'bl_rnn\', self.bl_rnn),\n        pyro.module(\'bl_predict\', self.bl_predict),\n        pyro.module(\'bl_embed\', self.bl_embed)\n\n        pyro.param(\'h_init\', self.h_init)\n        pyro.param(\'c_init\', self.c_init)\n        pyro.param(\'z_where_init\', self.z_where_init)\n        pyro.param(\'z_what_init\', self.z_what_init)\n        pyro.param(\'bl_h_init\', self.bl_h_init)\n        pyro.param(\'bl_c_init\', self.bl_c_init)\n\n        with pyro.plate(\'data\', data.size(0), subsample_size=batch_size, device=data.device) as ix:\n            batch = data[ix]\n            n = batch.size(0)\n\n            # Embed inputs.\n            flattened_batch = batch.view(n, -1)\n            inputs = {\n                \'raw\': batch,\n                \'embed\': self.embed(flattened_batch),\n                \'bl_embed\': self.bl_embed(flattened_batch)\n            }\n\n            # Initial state.\n            state = GuideState(\n                h=batch_expand(self.h_init, n),\n                c=batch_expand(self.c_init, n),\n                bl_h=batch_expand(self.bl_h_init, n),\n                bl_c=batch_expand(self.bl_c_init, n),\n                z_pres=torch.ones(n, self.z_pres_size, **self.options),\n                z_where=batch_expand(self.z_where_init, n),\n                z_what=batch_expand(self.z_what_init, n))\n\n            z_pres = []\n            z_where = []\n\n            for t in range(self.num_steps):\n                state = self.guide_step(t, n, state, inputs)\n                z_where.append(state.z_where)\n                z_pres.append(state.z_pres)\n\n            return z_where, z_pres\n\n    def guide_step(self, t, n, prev, inputs):\n\n        rnn_input = torch.cat((inputs[\'embed\'], prev.z_where, prev.z_what, prev.z_pres), 1)\n        h, c = self.rnn(rnn_input, (prev.h, prev.c))\n        z_pres_p, z_where_loc, z_where_scale = self.predict(h)\n\n        # Compute baseline estimates for discrete choice z_pres.\n        infer_dict, bl_h, bl_c = self.baseline_step(prev, inputs)\n\n        # Sample presence.\n        z_pres = pyro.sample(\'z_pres_{}\'.format(t),\n                             dist.Bernoulli(z_pres_p * prev.z_pres).to_event(1),\n                             infer=infer_dict)\n\n        sample_mask = z_pres if self.use_masking else torch.tensor(1.0)\n\n        z_where = pyro.sample(\'z_where_{}\'.format(t),\n                              dist.Normal(z_where_loc + self.z_where_loc_prior,\n                                          z_where_scale * self.z_where_scale_prior)\n                                  .mask(sample_mask)\n                                  .to_event(1))\n\n        # Figure 2 of [1] shows x_att depending on z_where and h,\n        # rather than z_where and x as here, but I think this is\n        # correct.\n        x_att = image_to_window(z_where, self.window_size, self.x_size, inputs[\'raw\'])\n\n        # Encode attention windows.\n        z_what_loc, z_what_scale = self.encode(x_att)\n\n        z_what = pyro.sample(\'z_what_{}\'.format(t),\n                             dist.Normal(z_what_loc, z_what_scale)\n                                 .mask(sample_mask)\n                                 .to_event(1))\n        return GuideState(h=h, c=c, bl_h=bl_h, bl_c=bl_c, z_pres=z_pres, z_where=z_where, z_what=z_what)\n\n    def baseline_step(self, prev, inputs):\n        if not self.use_baselines:\n            return dict(), None, None\n\n        # Prevent gradients flowing back from baseline loss to\n        # inference net by detaching from graph here.\n        rnn_input = torch.cat((inputs[\'bl_embed\'],\n                               prev.z_where.detach(),\n                               prev.z_what.detach(),\n                               prev.z_pres.detach()), 1)\n        bl_h, bl_c = self.bl_rnn(rnn_input, (prev.bl_h, prev.bl_c))\n        bl_value = self.bl_predict(bl_h)\n\n        # Zero out values for finished data points. This avoids adding\n        # superfluous terms to the loss.\n        if self.use_masking:\n            bl_value = bl_value * prev.z_pres\n\n        # The value that the baseline net is estimating can be very\n        # large. An option to scale the nets output is provided\n        # to make it easier for the net to output values of this\n        # scale.\n        if self.baseline_scalar is not None:\n            bl_value = bl_value * self.baseline_scalar\n\n        infer_dict = dict(baseline=dict(baseline_value=bl_value.squeeze(-1)))\n        return infer_dict, bl_h, bl_c\n\n\n# Spatial transformer helpers.\n\nexpansion_indices = torch.LongTensor([1, 0, 2, 0, 1, 3])\n\n\ndef expand_z_where(z_where):\n    # Take a batch of three-vectors, and massages them into a batch of\n    # 2x3 matrices with elements like so:\n    # [s,x,y] -> [[s,0,x],\n    #             [0,s,y]]\n    n = z_where.size(0)\n    out = torch.cat((z_where.new_zeros(n, 1), z_where), 1)\n    ix = expansion_indices\n    if z_where.is_cuda:\n        ix = ix.cuda()\n    out = torch.index_select(out, 1, ix)\n    out = out.view(n, 2, 3)\n    return out\n\n\n# Scaling by `1/scale` here is unsatisfactory, as `scale` could be\n# zero.\ndef z_where_inv(z_where):\n    # Take a batch of z_where vectors, and compute their ""inverse"".\n    # That is, for each row compute:\n    # [s,x,y] -> [1/s,-x/s,-y/s]\n    # These are the parameters required to perform the inverse of the\n    # spatial transform performed in the generative model.\n    n = z_where.size(0)\n    out = torch.cat((z_where.new_ones(n, 1), -z_where[:, 1:]), 1)\n    # Divide all entries by the scale.\n    out = out / z_where[:, 0:1]\n    return out\n\n\ndef window_to_image(z_where, window_size, image_size, windows):\n    n = windows.size(0)\n    assert windows.size(1) == window_size ** 2, \'Size mismatch.\'\n    theta = expand_z_where(z_where)\n    grid = F.affine_grid(theta, torch.Size((n, 1, image_size, image_size)))\n    out = F.grid_sample(windows.view(n, 1, window_size, window_size), grid)\n    return out.view(n, image_size, image_size)\n\n\ndef image_to_window(z_where, window_size, image_size, images):\n    n = images.size(0)\n    assert images.size(1) == images.size(2) == image_size, \'Size mismatch.\'\n    theta_inv = expand_z_where(z_where_inv(z_where))\n    grid = F.affine_grid(theta_inv, torch.Size((n, 1, window_size, window_size)))\n    out = F.grid_sample(images.view(n, 1, image_size, image_size), grid)\n    return out.view(n, -1)\n\n\n# Helper to expand parameters to the size of the mini-batch. I would\n# like to remove this and just write `t.expand(n, -1)` inline, but the\n# `-1` argument of `expand` doesn\'t seem to work with PyTorch 0.2.0.\ndef batch_expand(t, n):\n    return t.expand(n, t.size(1))\n\n\n# Combine z_pres and z_where (as returned by the model and guide) into\n# a single tensor, with size:\n# [batch_size, num_steps, z_where_size + z_pres_size]\ndef latents_to_tensor(z):\n    return torch.stack([\n        torch.cat((z_where.cpu().data, z_pres.cpu().data), 1)\n        for z_where, z_pres in zip(*z)]).transpose(0, 1)\n'"
examples/air/main.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nAIR applied to the multi-mnist data set [1].\n\n[1] Eslami, SM Ali, et al. ""Attend, infer, repeat: Fast scene\nunderstanding with generative models."" Advances in Neural Information\nProcessing Systems. 2016.\n""""""\n\n\nimport argparse\nimport math\nimport os\nimport time\nfrom functools import partial\n\nimport numpy as np\nimport torch\nimport visdom\n\nimport pyro\nimport pyro.contrib.examples.multi_mnist as multi_mnist\nimport pyro.optim as optim\nimport pyro.poutine as poutine\nfrom air import AIR, latents_to_tensor\nfrom pyro.contrib.examples.util import get_data_directory\nfrom pyro.infer import SVI, JitTraceGraph_ELBO, TraceGraph_ELBO\nfrom viz import draw_many, tensor_to_objs\n\n\ndef count_accuracy(X, true_counts, air, batch_size):\n    assert X.size(0) == true_counts.size(0), \'Size mismatch.\'\n    assert X.size(0) % batch_size == 0, \'Input size must be multiple of batch_size.\'\n    counts = torch.LongTensor(3, 4).zero_()\n    error_latents = []\n    error_indicators = []\n\n    def count_vec_to_mat(vec, max_index):\n        out = torch.LongTensor(vec.size(0), max_index + 1).zero_()\n        out.scatter_(1, vec.type(torch.LongTensor).view(vec.size(0), 1), 1)\n        return out\n\n    for i in range(X.size(0) // batch_size):\n        X_batch = X[i * batch_size:(i + 1) * batch_size]\n        true_counts_batch = true_counts[i * batch_size:(i + 1) * batch_size]\n        z_where, z_pres = air.guide(X_batch, batch_size)\n        inferred_counts = sum(z.cpu() for z in z_pres).squeeze().data\n        true_counts_m = count_vec_to_mat(true_counts_batch, 2)\n        inferred_counts_m = count_vec_to_mat(inferred_counts, 3)\n        counts += torch.mm(true_counts_m.t(), inferred_counts_m)\n        error_ind = 1 - (true_counts_batch == inferred_counts)\n        error_ix = error_ind.nonzero().squeeze()\n        error_latents.append(latents_to_tensor((z_where, z_pres)).index_select(0, error_ix))\n        error_indicators.append(error_ind)\n\n    acc = counts.diag().sum().float() / X.size(0)\n    error_indices = torch.cat(error_indicators).nonzero().squeeze()\n    if X.is_cuda:\n        error_indices = error_indices.cuda()\n    return acc, counts, torch.cat(error_latents), error_indices\n\n\n# Defines something like a truncated geometric. Like the geometric,\n# this has the property that there\'s a constant difference in log prob\n# between p(steps=n) and p(steps=n+1).\ndef make_prior(k):\n    assert 0 < k <= 1\n    u = 1 / (1 + k + k**2 + k**3)\n    p0 = 1 - u\n    p1 = 1 - (k * u) / p0\n    p2 = 1 - (k**2 * u) / (p0 * p1)\n    trial_probs = [p0, p1, p2]\n    # dist = [1 - p0, p0 * (1 - p1), p0 * p1 * (1 - p2), p0 * p1 * p2]\n    # print(dist)\n    return lambda t: trial_probs[t]\n\n\n# Implements ""prior annealing"" as described in this blog post:\n# http://akosiorek.github.io/ml/2017/09/03/implementing-air.html\n\n# That implementation does something very close to the following:\n# --z-pres-prior (1 - 1e-15)\n# --z-pres-prior-raw\n# --anneal-prior exp\n# --anneal-prior-to 1e-7\n# --anneal-prior-begin 1000\n# --anneal-prior-duration 1e6\n\n# e.g. After 200K steps z_pres_p will have decayed to ~0.04\n\n# These compute the value of a decaying value at time t.\n# initial: initial value\n# final: final value, reached after begin + duration steps\n# begin: number of steps before decay begins\n# duration: number of steps over which decay occurs\n# t: current time step\n\n\ndef lin_decay(initial, final, begin, duration, t):\n    assert duration > 0\n    x = (final - initial) * (t - begin) / duration + initial\n    return max(min(x, initial), final)\n\n\ndef exp_decay(initial, final, begin, duration, t):\n    assert final > 0\n    assert duration > 0\n    # half_life = math.log(2) / math.log(initial / final) * duration\n    decay_rate = math.log(initial / final) / duration\n    x = initial * math.exp(-decay_rate * (t - begin))\n    return max(min(x, initial), final)\n\n\ndef load_data():\n    inpath = get_data_directory(__file__)\n    X_np, Y = multi_mnist.load(inpath)\n    X_np = X_np.astype(np.float32)\n    X_np /= 255.0\n    X = torch.from_numpy(X_np)\n    # Using FloatTensor to allow comparison with values sampled from\n    # Bernoulli.\n    counts = torch.FloatTensor([len(objs) for objs in Y])\n    return X, counts\n\n\ndef main(**kwargs):\n\n    args = argparse.Namespace(**kwargs)\n\n    if \'save\' in args:\n        if os.path.exists(args.save):\n            raise RuntimeError(\'Output file ""{}"" already exists.\'.format(args.save))\n\n    if args.seed is not None:\n        pyro.set_rng_seed(args.seed)\n\n    X, true_counts = load_data()\n    X_size = X.size(0)\n    if args.cuda:\n        X = X.cuda()\n\n    # Build a function to compute z_pres prior probabilities.\n    if args.z_pres_prior_raw:\n        def base_z_pres_prior_p(t):\n            return args.z_pres_prior\n    else:\n        base_z_pres_prior_p = make_prior(args.z_pres_prior)\n\n    # Wrap with logic to apply any annealing.\n    def z_pres_prior_p(opt_step, time_step):\n        p = base_z_pres_prior_p(time_step)\n        if args.anneal_prior == \'none\':\n            return p\n        else:\n            decay = dict(lin=lin_decay, exp=exp_decay)[args.anneal_prior]\n            return decay(p, args.anneal_prior_to, args.anneal_prior_begin,\n                         args.anneal_prior_duration, opt_step)\n\n    model_arg_keys = [\'window_size\',\n                      \'rnn_hidden_size\',\n                      \'decoder_output_bias\',\n                      \'decoder_output_use_sigmoid\',\n                      \'baseline_scalar\',\n                      \'encoder_net\',\n                      \'decoder_net\',\n                      \'predict_net\',\n                      \'embed_net\',\n                      \'bl_predict_net\',\n                      \'non_linearity\',\n                      \'pos_prior_mean\',\n                      \'pos_prior_sd\',\n                      \'scale_prior_mean\',\n                      \'scale_prior_sd\']\n    model_args = {key: getattr(args, key) for key in model_arg_keys if key in args}\n    air = AIR(\n        num_steps=args.model_steps,\n        x_size=50,\n        use_masking=not args.no_masking,\n        use_baselines=not args.no_baselines,\n        z_what_size=args.encoder_latent_size,\n        use_cuda=args.cuda,\n        **model_args\n    )\n\n    if args.verbose:\n        print(air)\n        print(args)\n\n    if \'load\' in args:\n        print(\'Loading parameters...\')\n        air.load_state_dict(torch.load(args.load))\n\n    # Viz sample from prior.\n    if args.viz:\n        vis = visdom.Visdom(env=args.visdom_env)\n        z, x = air.prior(5, z_pres_prior_p=partial(z_pres_prior_p, 0))\n        vis.images(draw_many(x, tensor_to_objs(latents_to_tensor(z))))\n\n    def isBaselineParam(module_name, param_name):\n        return \'bl_\' in module_name or \'bl_\' in param_name\n\n    def per_param_optim_args(module_name, param_name):\n        lr = args.baseline_learning_rate if isBaselineParam(module_name, param_name) else args.learning_rate\n        return {\'lr\': lr}\n\n    adam = optim.Adam(per_param_optim_args)\n    elbo = JitTraceGraph_ELBO() if args.jit else TraceGraph_ELBO()\n    svi = SVI(air.model, air.guide, adam, loss=elbo)\n\n    # Do inference.\n    t0 = time.time()\n    examples_to_viz = X[5:10]\n\n    for i in range(1, args.num_steps + 1):\n\n        loss = svi.step(X, batch_size=args.batch_size, z_pres_prior_p=partial(z_pres_prior_p, i))\n\n        if args.progress_every > 0 and i % args.progress_every == 0:\n            print(\'i={}, epochs={:.2f}, elapsed={:.2f}, elbo={:.2f}\'.format(\n                i,\n                (i * args.batch_size) / X_size,\n                (time.time() - t0) / 3600,\n                loss / X_size))\n\n        if args.viz and i % args.viz_every == 0:\n            trace = poutine.trace(air.guide).get_trace(examples_to_viz, None)\n            z, recons = poutine.replay(air.prior, trace=trace)(examples_to_viz.size(0))\n            z_wheres = tensor_to_objs(latents_to_tensor(z))\n\n            # Show data with inferred objection positions.\n            vis.images(draw_many(examples_to_viz, z_wheres))\n            # Show reconstructions of data.\n            vis.images(draw_many(recons, z_wheres))\n\n        if args.eval_every > 0 and i % args.eval_every == 0:\n            # Measure accuracy on subset of training data.\n            acc, counts, error_z, error_ix = count_accuracy(X, true_counts, air, 1000)\n            print(\'i={}, accuracy={}, counts={}\'.format(i, acc, counts.numpy().tolist()))\n            if args.viz and error_ix.size(0) > 0:\n                vis.images(draw_many(X[error_ix[0:5]], tensor_to_objs(error_z[0:5])),\n                           opts=dict(caption=\'errors ({})\'.format(i)))\n\n        if \'save\' in args and i % args.save_every == 0:\n            print(\'Saving parameters...\')\n            torch.save(air.state_dict(), args.save)\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""Pyro AIR example"", argument_default=argparse.SUPPRESS)\n    parser.add_argument(\'-n\', \'--num-steps\', type=int, default=int(1e8),\n                        help=\'number of optimization steps to take\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=int, default=64,\n                        help=\'batch size\')\n    parser.add_argument(\'-lr\', \'--learning-rate\', type=float, default=1e-4,\n                        help=\'learning rate\')\n    parser.add_argument(\'-blr\', \'--baseline-learning-rate\', type=float, default=1e-3,\n                        help=\'baseline learning rate\')\n    parser.add_argument(\'--progress-every\', type=int, default=1,\n                        help=\'number of steps between writing progress to stdout\')\n    parser.add_argument(\'--eval-every\', type=int, default=0,\n                        help=\'number of steps between evaluations\')\n    parser.add_argument(\'--baseline-scalar\', type=float,\n                        help=\'scale the output of the baseline nets by this value\')\n    parser.add_argument(\'--no-baselines\', action=\'store_true\', default=False,\n                        help=\'do not use data dependent baselines\')\n    parser.add_argument(\'--encoder-net\', type=int, nargs=\'+\', default=[200],\n                        help=\'encoder net hidden layer sizes\')\n    parser.add_argument(\'--decoder-net\', type=int, nargs=\'+\', default=[200],\n                        help=\'decoder net hidden layer sizes\')\n    parser.add_argument(\'--predict-net\', type=int, nargs=\'+\',\n                        help=\'predict net hidden layer sizes\')\n    parser.add_argument(\'--embed-net\', type=int, nargs=\'+\',\n                        help=\'embed net architecture\')\n    parser.add_argument(\'--bl-predict-net\', type=int, nargs=\'+\',\n                        help=\'baseline predict net hidden layer sizes\')\n    parser.add_argument(\'--non-linearity\', type=str,\n                        help=\'non linearity to use throughout\')\n    parser.add_argument(\'--viz\', action=\'store_true\', default=False,\n                        help=\'generate vizualizations during optimization\')\n    parser.add_argument(\'--viz-every\', type=int, default=100,\n                        help=\'number of steps between vizualizations\')\n    parser.add_argument(\'--visdom-env\', default=\'main\',\n                        help=\'visdom enviroment name\')\n    parser.add_argument(\'--load\', type=str,\n                        help=\'load previously saved parameters\')\n    parser.add_argument(\'--save\', type=str,\n                        help=\'save parameters to specified file\')\n    parser.add_argument(\'--save-every\', type=int, default=1e4,\n                        help=\'number of steps between parameter saves\')\n    parser.add_argument(\'--cuda\', action=\'store_true\', default=False,\n                        help=\'use cuda\')\n    parser.add_argument(\'--jit\', action=\'store_true\', default=False,\n                        help=\'use PyTorch jit\')\n    parser.add_argument(\'-t\', \'--model-steps\', type=int, default=3,\n                        help=\'number of time steps\')\n    parser.add_argument(\'--rnn-hidden-size\', type=int, default=256,\n                        help=\'rnn hidden size\')\n    parser.add_argument(\'--encoder-latent-size\', type=int, default=50,\n                        help=\'attention window encoder/decoder latent space size\')\n    parser.add_argument(\'--decoder-output-bias\', type=float,\n                        help=\'bias added to decoder output (prior to applying non-linearity)\')\n    parser.add_argument(\'--decoder-output-use-sigmoid\', action=\'store_true\',\n                        help=\'apply sigmoid function to output of decoder network\')\n    parser.add_argument(\'--window-size\', type=int, default=28,\n                        help=\'attention window size\')\n    parser.add_argument(\'--z-pres-prior\', type=float, default=0.5,\n                        help=\'prior success probability for z_pres\')\n    parser.add_argument(\'--z-pres-prior-raw\', action=\'store_true\', default=False,\n                        help=\'use --z-pres-prior directly as success prob instead of a geometric like prior\')\n    parser.add_argument(\'--anneal-prior\', choices=\'none lin exp\'.split(), default=\'none\',\n                        help=\'anneal z_pres prior during optimization\')\n    parser.add_argument(\'--anneal-prior-to\', type=float, default=1e-7,\n                        help=\'target z_pres prior prob\')\n    parser.add_argument(\'--anneal-prior-begin\', type=int, default=0,\n                        help=\'number of steps to wait before beginning to anneal the prior\')\n    parser.add_argument(\'--anneal-prior-duration\', type=int, default=100000,\n                        help=\'number of steps over which to anneal the prior\')\n    parser.add_argument(\'--pos-prior-mean\', type=float,\n                        help=\'mean of the window position prior\')\n    parser.add_argument(\'--pos-prior-sd\', type=float,\n                        help=\'std. dev. of the window position prior\')\n    parser.add_argument(\'--scale-prior-mean\', type=float,\n                        help=\'mean of the window scale prior\')\n    parser.add_argument(\'--scale-prior-sd\', type=float,\n                        help=\'std. dev. of the window scale prior\')\n    parser.add_argument(\'--no-masking\', action=\'store_true\', default=False,\n                        help=\'do not mask out the costs of unused choices\')\n    parser.add_argument(\'--seed\', type=int, help=\'random seed\', default=None)\n    parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', default=False,\n                        help=\'write hyper parameters and network architecture to stdout\')\n    main(**vars(parser.parse_args()))\n'"
examples/air/modules.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import softplus\n\n\n# Takes pixel intensities of the attention window to parameters (mean,\n# standard deviation) of the distribution over the latent code,\n# z_what.\nclass Encoder(nn.Module):\n    def __init__(self, x_size, h_sizes, z_size, non_linear_layer):\n        super().__init__()\n        self.z_size = z_size\n        output_size = 2 * z_size\n        self.mlp = MLP(x_size, h_sizes + [output_size], non_linear_layer)\n\n    def forward(self, x):\n        a = self.mlp(x)\n        return a[:, 0:self.z_size], softplus(a[:, self.z_size:])\n\n\n# Takes a latent code, z_what, to pixel intensities.\nclass Decoder(nn.Module):\n    def __init__(self, x_size, h_sizes, z_size, bias, use_sigmoid, non_linear_layer):\n        super().__init__()\n        self.bias = bias\n        self.use_sigmoid = use_sigmoid\n        self.mlp = MLP(z_size, h_sizes + [x_size], non_linear_layer)\n\n    def forward(self, z):\n        a = self.mlp(z)\n        if self.bias is not None:\n            a = a + self.bias\n        return torch.sigmoid(a) if self.use_sigmoid else a\n\n\n# A general purpose module to construct networks that look like:\n# [Linear (256 -> 1)]\n# [Linear (256 -> 256), ReLU (), Linear (256 -> 1)]\n# [Linear (256 -> 256), ReLU (), Linear (256 -> 1), ReLU ()]\n# etc.\nclass MLP(nn.Module):\n    def __init__(self, in_size, out_sizes, non_linear_layer, output_non_linearity=False):\n        super().__init__()\n        assert len(out_sizes) >= 1\n        layers = []\n        in_sizes = [in_size] + out_sizes[0:-1]\n        sizes = list(zip(in_sizes, out_sizes))\n        for (i, o) in sizes[0:-1]:\n            layers.append(nn.Linear(i, o))\n            layers.append(non_linear_layer())\n        layers.append(nn.Linear(sizes[-1][0], sizes[-1][1]))\n        if output_non_linearity:\n            layers.append(non_linear_layer())\n        self.seq = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.seq(x)\n\n\n# Takes the guide RNN hidden state to parameters of the guide\n# distributions over z_where and z_pres.\nclass Predict(nn.Module):\n    def __init__(self, input_size, h_sizes, z_pres_size, z_where_size, non_linear_layer):\n        super().__init__()\n        self.z_pres_size = z_pres_size\n        self.z_where_size = z_where_size\n        output_size = z_pres_size + 2 * z_where_size\n        self.mlp = MLP(input_size, h_sizes + [output_size], non_linear_layer)\n\n    def forward(self, h):\n        out = self.mlp(h)\n        z_pres_p = torch.sigmoid(out[:, 0:self.z_pres_size])\n        z_where_loc = out[:, self.z_pres_size:self.z_pres_size + self.z_where_size]\n        z_where_scale = softplus(out[:, (self.z_pres_size + self.z_where_size):])\n        return z_pres_p, z_where_loc, z_where_scale\n\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n'"
examples/air/viz.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nfrom collections import namedtuple\n\nimport numpy as np\nfrom PIL import Image, ImageDraw\n\n\ndef bounding_box(z_where, x_size):\n    """"""This doesn\'t take into account interpolation, but it\'s close\n    enough to be usable.""""""\n    w = x_size / z_where.s\n    h = x_size / z_where.s\n    xtrans = -z_where.x / z_where.s * x_size / 2.\n    ytrans = -z_where.y / z_where.s * x_size / 2.\n    x = (x_size - w) / 2 + xtrans  # origin is top left\n    y = (x_size - h) / 2 + ytrans\n    return (x, y), w, h\n\n\ndef arr2img(arr):\n    # arr is expected to be a 2d array of floats in [0,1]\n    return Image.frombuffer(\'L\', arr.shape, (arr * 255).astype(np.uint8).tostring(), \'raw\', \'L\', 0, 1)\n\n\ndef img2arr(img):\n    # assumes color image\n    # returns an array suitable for sending to visdom\n    return np.array(img.getdata(), np.uint8).reshape(img.size + (3,)).transpose((2, 0, 1))\n\n\ndef colors(k):\n    return [(255, 0, 0), (0, 255, 0), (0, 0, 255)][k % 3]\n\n\ndef draw_one(imgarr, z_arr):\n    # Note that this clipping makes the visualisation somewhat\n    # misleading, as it incorrectly suggests objects occlude one\n    # another.\n    clipped = np.clip(imgarr.detach().cpu().numpy(), 0, 1)\n    img = arr2img(clipped).convert(\'RGB\')\n    draw = ImageDraw.Draw(img)\n    for k, z in enumerate(z_arr):\n        # It would be better to use z_pres to change the opacity of\n        # the bounding boxes, but I couldn\'t make that work with PIL.\n        # Instead this darkens the color, and skips boxes altogether\n        # when z_pres==0.\n        if z.pres > 0:\n            (x, y), w, h = bounding_box(z, imgarr.size(0))\n            color = tuple(map(lambda c: int(c * z.pres), colors(k)))\n            draw.rectangle([x, y, x + w, y + h], outline=color)\n    is_relaxed = any(z.pres != math.floor(z.pres) for z in z_arr)\n    fmtstr = \'{:.1f}\' if is_relaxed else \'{:.0f}\'\n    draw.text((0, 0), fmtstr.format(sum(z.pres for z in z_arr)), fill=\'white\')\n    return img2arr(img)\n\n\ndef draw_many(imgarrs, z_arr):\n    # canvases is expected to be a (n,w,h) numpy array\n    # z_where_arr is expected to be a list of length n\n    return [draw_one(imgarr, z) for (imgarr, z) in zip(imgarrs.cpu(), z_arr)]\n\n\nz_obj = namedtuple(\'z\', \'s,x,y,pres\')\n\n\n# Map a tensor of latents (as produced by latents_to_tensor) to a list\n# of z_obj named tuples.\ndef tensor_to_objs(latents):\n    return [[z_obj._make(step) for step in z] for z in latents]\n'"
examples/capture_recapture/cjs.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nWe show how to implement several variants of the Cormack-Jolly-Seber (CJS)\n[4, 5, 6] model used in ecology to analyze animal capture-recapture data.\nFor a discussion of these models see reference [1].\n\nWe make use of two datasets:\n-- the European Dipper (Cinclus cinclus) data from reference [2]\n   (this is Norway\'s national bird).\n-- the meadow voles data from reference [3].\n\nCompare to the Stan implementations in [7].\n\nReferences\n[1] Kery, M., & Schaub, M. (2011). Bayesian population analysis using\n    WinBUGS: a hierarchical perspective. Academic Press.\n[2] Lebreton, J.D., Burnham, K.P., Clobert, J., & Anderson, D.R. (1992).\n    Modeling survival and testing biological hypotheses using marked animals:\n    a unified approach with case studies. Ecological monographs, 62(1), 67-118.\n[3] Nichols, Pollock, Hines (1984) The use of a robust capture-recapture design\n    in small mammal population studies: A field example with Microtus pennsylvanicus.\n    Acta Theriologica 29:357-365.\n[4] Cormack, R.M., 1964. Estimates of survival from the sighting of marked animals.\n    Biometrika 51, 429-438.\n[5] Jolly, G.M., 1965. Explicit estimates from capture-recapture data with both death\n    and immigration-stochastic model. Biometrika 52, 225-247.\n[6] Seber, G.A.F., 1965. A note on the multiple recapture census. Biometrika 52, 249-259.\n[7] https://github.com/stan-dev/example-models/tree/master/BPA/Ch.07\n""""""\n\nimport argparse\nimport os\n\nimport numpy as np\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer.autoguide import AutoDiagonalNormal\nfrom pyro.infer import SVI, TraceEnum_ELBO, TraceTMC_ELBO\nfrom pyro.optim import Adam\n\n\n""""""\nOur first and simplest CJS model variant only has two continuous\n(scalar) latent random variables: i) the survival probability phi;\nand ii) the recapture probability rho. These are treated as fixed\neffects with no temporal or individual/group variation.\n""""""\n\n\ndef model_1(capture_history, sex):\n    N, T = capture_history.shape\n    phi = pyro.sample(""phi"", dist.Uniform(0.0, 1.0))  # survival probability\n    rho = pyro.sample(""rho"", dist.Uniform(0.0, 1.0))  # recapture probability\n\n    with pyro.plate(""animals"", N, dim=-1):\n        z = torch.ones(N)\n        # we use this mask to eliminate extraneous log probabilities\n        # that arise for a given individual before its first capture.\n        first_capture_mask = torch.zeros(N).bool()\n        for t in pyro.markov(range(T)):\n            with poutine.mask(mask=first_capture_mask):\n                mu_z_t = first_capture_mask.float() * phi * z + (1 - first_capture_mask.float())\n                # we use parallel enumeration to exactly sum out\n                # the discrete states z_t.\n                z = pyro.sample(""z_{}"".format(t), dist.Bernoulli(mu_z_t),\n                                infer={""enumerate"": ""parallel""})\n                mu_y_t = rho * z\n                pyro.sample(""y_{}"".format(t), dist.Bernoulli(mu_y_t),\n                            obs=capture_history[:, t])\n            first_capture_mask |= capture_history[:, t].bool()\n\n\n""""""\nIn our second model variant there is a time-varying survival probability phi_t for\nT-1 of the T time periods of the capture data; each phi_t is treated as a fixed effect.\n""""""\n\n\ndef model_2(capture_history, sex):\n    N, T = capture_history.shape\n    rho = pyro.sample(""rho"", dist.Uniform(0.0, 1.0))  # recapture probability\n\n    z = torch.ones(N)\n    first_capture_mask = torch.zeros(N).bool()\n    # we create the plate once, outside of the loop over t\n    animals_plate = pyro.plate(""animals"", N, dim=-1)\n    for t in pyro.markov(range(T)):\n        # note that phi_t needs to be outside the plate, since\n        # phi_t is shared across all N individuals\n        phi_t = pyro.sample(""phi_{}"".format(t), dist.Uniform(0.0, 1.0)) if t > 0 \\\n                else 1.0\n        with animals_plate, poutine.mask(mask=first_capture_mask):\n            mu_z_t = first_capture_mask.float() * phi_t * z + (1 - first_capture_mask.float())\n            # we use parallel enumeration to exactly sum out\n            # the discrete states z_t.\n            z = pyro.sample(""z_{}"".format(t), dist.Bernoulli(mu_z_t),\n                            infer={""enumerate"": ""parallel""})\n            mu_y_t = rho * z\n            pyro.sample(""y_{}"".format(t), dist.Bernoulli(mu_y_t),\n                        obs=capture_history[:, t])\n        first_capture_mask |= capture_history[:, t].bool()\n\n\n""""""\nIn our third model variant there is a survival probability phi_t for T-1\nof the T time periods of the capture data (just like in model_2), but here\neach phi_t is treated as a random effect.\n""""""\n\n\ndef model_3(capture_history, sex):\n    def logit(p):\n        return torch.log(p) - torch.log1p(-p)\n    N, T = capture_history.shape\n    phi_mean = pyro.sample(""phi_mean"", dist.Uniform(0.0, 1.0))  # mean survival probability\n    phi_logit_mean = logit(phi_mean)\n    # controls temporal variability of survival probability\n    phi_sigma = pyro.sample(""phi_sigma"", dist.Uniform(0.0, 10.0))\n    rho = pyro.sample(""rho"", dist.Uniform(0.0, 1.0))  # recapture probability\n\n    z = torch.ones(N)\n    first_capture_mask = torch.zeros(N).bool()\n    # we create the plate once, outside of the loop over t\n    animals_plate = pyro.plate(""animals"", N, dim=-1)\n    for t in pyro.markov(range(T)):\n        phi_logit_t = pyro.sample(""phi_logit_{}"".format(t),\n                                  dist.Normal(phi_logit_mean, phi_sigma)) if t > 0 \\\n                      else torch.tensor(0.0)\n        phi_t = torch.sigmoid(phi_logit_t)\n        with animals_plate, poutine.mask(mask=first_capture_mask):\n            mu_z_t = first_capture_mask.float() * phi_t * z + (1 - first_capture_mask.float())\n            # we use parallel enumeration to exactly sum out\n            # the discrete states z_t.\n            z = pyro.sample(""z_{}"".format(t), dist.Bernoulli(mu_z_t),\n                            infer={""enumerate"": ""parallel""})\n            mu_y_t = rho * z\n            pyro.sample(""y_{}"".format(t), dist.Bernoulli(mu_y_t),\n                        obs=capture_history[:, t])\n        first_capture_mask |= capture_history[:, t].bool()\n\n\n""""""\nIn our fourth model variant we include group-level fixed effects\nfor sex (male, female).\n""""""\n\n\ndef model_4(capture_history, sex):\n    N, T = capture_history.shape\n    # survival probabilities for males/females\n    phi_male = pyro.sample(""phi_male"", dist.Uniform(0.0, 1.0))\n    phi_female = pyro.sample(""phi_female"", dist.Uniform(0.0, 1.0))\n    # we construct a N-dimensional vector that contains the appropriate\n    # phi for each individual given its sex (female = 0, male = 1)\n    phi = sex * phi_male + (1.0 - sex) * phi_female\n    rho = pyro.sample(""rho"", dist.Uniform(0.0, 1.0))  # recapture probability\n\n    with pyro.plate(""animals"", N, dim=-1):\n        z = torch.ones(N)\n        # we use this mask to eliminate extraneous log probabilities\n        # that arise for a given individual before its first capture.\n        first_capture_mask = torch.zeros(N).bool()\n        for t in pyro.markov(range(T)):\n            with poutine.mask(mask=first_capture_mask):\n                mu_z_t = first_capture_mask.float() * phi * z + (1 - first_capture_mask.float())\n                # we use parallel enumeration to exactly sum out\n                # the discrete states z_t.\n                z = pyro.sample(""z_{}"".format(t), dist.Bernoulli(mu_z_t),\n                                infer={""enumerate"": ""parallel""})\n                mu_y_t = rho * z\n                pyro.sample(""y_{}"".format(t), dist.Bernoulli(mu_y_t),\n                            obs=capture_history[:, t])\n            first_capture_mask |= capture_history[:, t].bool()\n\n\n""""""\nIn our final model variant we include both fixed group effects and fixed\ntime effects for the survival probability phi:\n\nlogit(phi_t) = beta_group + gamma_t\n\nWe need to take care that the model is not overparameterized; to do this\nwe effectively let a single scalar beta encode the difference in male\nand female survival probabilities.\n""""""\n\n\ndef model_5(capture_history, sex):\n    N, T = capture_history.shape\n\n    # phi_beta controls the survival probability differential\n    # for males versus females (in logit space)\n    phi_beta = pyro.sample(""phi_beta"", dist.Normal(0.0, 10.0))\n    phi_beta = sex * phi_beta\n    rho = pyro.sample(""rho"", dist.Uniform(0.0, 1.0))  # recapture probability\n\n    z = torch.ones(N)\n    first_capture_mask = torch.zeros(N).bool()\n    # we create the plate once, outside of the loop over t\n    animals_plate = pyro.plate(""animals"", N, dim=-1)\n    for t in pyro.markov(range(T)):\n        phi_gamma_t = pyro.sample(""phi_gamma_{}"".format(t), dist.Normal(0.0, 10.0)) if t > 0 \\\n                      else 0.0\n        phi_t = torch.sigmoid(phi_beta + phi_gamma_t)\n        with animals_plate, poutine.mask(mask=first_capture_mask):\n            mu_z_t = first_capture_mask.float() * phi_t * z + (1 - first_capture_mask.float())\n            # we use parallel enumeration to exactly sum out\n            # the discrete states z_t.\n            z = pyro.sample(""z_{}"".format(t), dist.Bernoulli(mu_z_t),\n                            infer={""enumerate"": ""parallel""})\n            mu_y_t = rho * z\n            pyro.sample(""y_{}"".format(t), dist.Bernoulli(mu_y_t),\n                        obs=capture_history[:, t])\n        first_capture_mask |= capture_history[:, t].bool()\n\n\nmodels = {name[len(\'model_\'):]: model\n          for name, model in globals().items()\n          if name.startswith(\'model_\')}\n\n\ndef main(args):\n    pyro.set_rng_seed(0)\n    pyro.clear_param_store()\n    pyro.enable_validation(__debug__)\n\n    # load data\n    if args.dataset == ""dipper"":\n        capture_history_file = os.path.dirname(os.path.abspath(__file__)) + \'/dipper_capture_history.csv\'\n    elif args.dataset == ""vole"":\n        capture_history_file = os.path.dirname(os.path.abspath(__file__)) + \'/meadow_voles_capture_history.csv\'\n    else:\n        raise ValueError(""Available datasets are \\\'dipper\\\' and \\\'vole\\\'."")\n\n    capture_history = torch.tensor(np.genfromtxt(capture_history_file, delimiter=\',\')).float()[:, 1:]\n    N, T = capture_history.shape\n    print(""Loaded {} capture history for {} individuals collected over {} time periods."".format(\n          args.dataset, N, T))\n\n    if args.dataset == ""dipper"" and args.model in [""4"", ""5""]:\n        sex_file = os.path.dirname(os.path.abspath(__file__)) + \'/dipper_sex.csv\'\n        sex = torch.tensor(np.genfromtxt(sex_file, delimiter=\',\')).float()[:, 1]\n        print(""Loaded dipper sex data."")\n    elif args.dataset == ""vole"" and args.model in [""4"", ""5""]:\n        raise ValueError(""Cannot run model_{} on meadow voles data, since we lack sex ""\n                         ""information for these animals."".format(args.model))\n    else:\n        sex = None\n\n    model = models[args.model]\n\n    # we use poutine.block to only expose the continuous latent variables\n    # in the models to AutoDiagonalNormal (all of which begin with \'phi\'\n    # or \'rho\')\n    def expose_fn(msg):\n        return msg[""name""][0:3] in [\'phi\', \'rho\']\n\n    # we use a mean field diagonal normal variational distributions (i.e. guide)\n    # for the continuous latent variables.\n    guide = AutoDiagonalNormal(poutine.block(model, expose_fn=expose_fn))\n\n    # since we enumerate the discrete random variables,\n    # we need to use TraceEnum_ELBO or TraceTMC_ELBO.\n    optim = Adam({\'lr\': args.learning_rate})\n    if args.tmc:\n        elbo = TraceTMC_ELBO(max_plate_nesting=1)\n        tmc_model = poutine.infer_config(\n            model,\n            lambda msg: {""num_samples"": args.tmc_num_samples, ""expand"": False} if msg[""infer""].get(""enumerate"", None) == ""parallel"" else {})  # noqa: E501\n        svi = SVI(tmc_model, guide, optim, elbo)\n    else:\n        elbo = TraceEnum_ELBO(max_plate_nesting=1, num_particles=20, vectorize_particles=True)\n        svi = SVI(model, guide, optim, elbo)\n\n    losses = []\n\n    print(""Beginning training of model_{} with Stochastic Variational Inference."".format(args.model))\n\n    for step in range(args.num_steps):\n        loss = svi.step(capture_history, sex)\n        losses.append(loss)\n        if step % 20 == 0 and step > 0 or step == args.num_steps - 1:\n            print(""[iteration %03d] loss: %.3f"" % (step, np.mean(losses[-20:])))\n\n    # evaluate final trained model\n    elbo_eval = TraceEnum_ELBO(max_plate_nesting=1, num_particles=2000, vectorize_particles=True)\n    svi_eval = SVI(model, guide, optim, elbo_eval)\n    print(""Final loss: %.4f"" % svi_eval.evaluate_loss(capture_history, sex))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""CJS capture-recapture model for ecological data"")\n    parser.add_argument(""-m"", ""--model"", default=""1"", type=str,\n                        help=""one of: {}"".format("", "".join(sorted(models.keys()))))\n    parser.add_argument(""-d"", ""--dataset"", default=""dipper"", type=str)\n    parser.add_argument(""-n"", ""--num-steps"", default=400, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.002, type=float)\n    parser.add_argument(""--tmc"", action=\'store_true\',\n                        help=""Use Tensor Monte Carlo instead of exact enumeration ""\n                             ""to estimate the marginal likelihood. You probably don\'t want to do this, ""\n                             ""except to see that TMC makes Monte Carlo gradient estimation feasible ""\n                             ""even with very large numbers of non-reparametrized variables."")\n    parser.add_argument(""--tmc-num-samples"", default=10, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/dmm/__init__.py,0,b''
examples/dmm/dmm.py,17,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nAn implementation of a Deep Markov Model in Pyro based on reference [1].\nThis is essentially the DKS variant outlined in the paper. The primary difference\nbetween this implementation and theirs is that in our version any KL divergence terms\nin the ELBO are estimated via sampling, while they make use of the analytic formulae.\nWe also illustrate the use of normalizing flows in the variational distribution (in which\ncase analytic formulae for the KL divergences are in any case unavailable).\n\nReference:\n\n[1] Structured Inference Networks for Nonlinear State Space Models [arXiv:1609.09869]\n    Rahul G. Krishnan, Uri Shalit, David Sontag\n""""""\n\nimport argparse\nimport time\nfrom os.path import exists\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport polyphonic_data_loader as poly\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.distributions import TransformedDistribution\nfrom pyro.distributions.transforms import affine_autoregressive\nfrom pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO, TraceEnum_ELBO, TraceTMC_ELBO, config_enumerate\nfrom pyro.optim import ClippedAdam\nfrom util import get_logger\n\n\nclass Emitter(nn.Module):\n    """"""\n    Parameterizes the bernoulli observation likelihood `p(x_t | z_t)`\n    """"""\n\n    def __init__(self, input_dim, z_dim, emission_dim):\n        super().__init__()\n        # initialize the three linear transformations used in the neural network\n        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n        # initialize the two non-linearities used in the neural network\n        self.relu = nn.ReLU()\n\n    def forward(self, z_t):\n        """"""\n        Given the latent z at a particular time step t we return the vector of\n        probabilities `ps` that parameterizes the bernoulli distribution `p(x_t|z_t)`\n        """"""\n        h1 = self.relu(self.lin_z_to_hidden(z_t))\n        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n        ps = torch.sigmoid(self.lin_hidden_to_input(h2))\n        return ps\n\n\nclass GatedTransition(nn.Module):\n    """"""\n    Parameterizes the gaussian latent transition probability `p(z_t | z_{t-1})`\n    See section 5 in the reference for comparison.\n    """"""\n\n    def __init__(self, z_dim, transition_dim):\n        super().__init__()\n        # initialize the six linear transformations used in the neural network\n        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n        self.lin_sig = nn.Linear(z_dim, z_dim)\n        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n        # modify the default initialization of lin_z_to_loc\n        # so that it\'s starts out as the identity function\n        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n        # initialize the three non-linearities used in the neural network\n        self.relu = nn.ReLU()\n        self.softplus = nn.Softplus()\n\n    def forward(self, z_t_1):\n        """"""\n        Given the latent `z_{t-1}` corresponding to the time step t-1\n        we return the mean and scale vectors that parameterize the\n        (diagonal) gaussian distribution `p(z_t | z_{t-1})`\n        """"""\n        # compute the gating function\n        _gate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n        gate = torch.sigmoid(self.lin_gate_hidden_to_z(_gate))\n        # compute the \'proposed mean\'\n        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n        # assemble the actual mean used to sample z_t, which mixes a linear transformation\n        # of z_{t-1} with the proposed mean modulated by the gating function\n        loc = (1 - gate) * self.lin_z_to_loc(z_t_1) + gate * proposed_mean\n        # compute the scale used to sample z_t, using the proposed mean from\n        # above as input the softplus ensures that scale is positive\n        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n        # return loc, scale which can be fed into Normal\n        return loc, scale\n\n\nclass Combiner(nn.Module):\n    """"""\n    Parameterizes `q(z_t | z_{t-1}, x_{t:T})`, which is the basic building block\n    of the guide (i.e. the variational distribution). The dependence on `x_{t:T}` is\n    through the hidden state of the RNN (see the PyTorch module `rnn` below)\n    """"""\n\n    def __init__(self, z_dim, rnn_dim):\n        super().__init__()\n        # initialize the three linear transformations used in the neural network\n        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n        # initialize the two non-linearities used in the neural network\n        self.tanh = nn.Tanh()\n        self.softplus = nn.Softplus()\n\n    def forward(self, z_t_1, h_rnn):\n        """"""\n        Given the latent z at at a particular time step t-1 as well as the hidden\n        state of the RNN `h(x_{t:T})` we return the mean and scale vectors that\n        parameterize the (diagonal) gaussian distribution `q(z_t | z_{t-1}, x_{t:T})`\n        """"""\n        # combine the rnn hidden state with a transformed version of z_t_1\n        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n        # use the combined hidden state to compute the mean used to sample z_t\n        loc = self.lin_hidden_to_loc(h_combined)\n        # use the combined hidden state to compute the scale used to sample z_t\n        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n        # return loc, scale which can be fed into Normal\n        return loc, scale\n\n\nclass DMM(nn.Module):\n    """"""\n    This PyTorch Module encapsulates the model as well as the\n    variational distribution (the guide) for the Deep Markov Model\n    """"""\n\n    def __init__(self, input_dim=88, z_dim=100, emission_dim=100,\n                 transition_dim=200, rnn_dim=600, num_layers=1, rnn_dropout_rate=0.0,\n                 num_iafs=0, iaf_dim=50, use_cuda=False):\n        super().__init__()\n        # instantiate PyTorch modules used in the model and guide below\n        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n        self.trans = GatedTransition(z_dim, transition_dim)\n        self.combiner = Combiner(z_dim, rnn_dim)\n        # dropout just takes effect on inner layers of rnn\n        rnn_dropout_rate = 0. if num_layers == 1 else rnn_dropout_rate\n        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim, nonlinearity=\'relu\',\n                          batch_first=True, bidirectional=False, num_layers=num_layers,\n                          dropout=rnn_dropout_rate)\n\n        # if we\'re using normalizing flows, instantiate those too\n        self.iafs = [affine_autoregressive(z_dim, hidden_dims=[iaf_dim]) for _ in range(num_iafs)]\n        self.iafs_modules = nn.ModuleList(self.iafs)\n\n        # define a (trainable) parameters z_0 and z_q_0 that help define the probability\n        # distributions p(z_1) and q(z_1)\n        # (since for t = 1 there are no previous latents to condition on)\n        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n        # define a (trainable) parameter for the initial hidden state of the rnn\n        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n\n        self.use_cuda = use_cuda\n        # if on gpu cuda-ize all PyTorch (sub)modules\n        if use_cuda:\n            self.cuda()\n\n    # the model p(x_{1:T} | z_{1:T}) p(z_{1:T})\n    def model(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n              mini_batch_seq_lengths, annealing_factor=1.0):\n\n        # this is the number of time steps we need to process in the mini-batch\n        T_max = mini_batch.size(1)\n\n        # register all PyTorch (sub)modules with pyro\n        # this needs to happen in both the model and guide\n        pyro.module(""dmm"", self)\n\n        # set z_prev = z_0 to setup the recursive conditioning in p(z_t | z_{t-1})\n        z_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))\n\n        # we enclose all the sample statements in the model in a plate.\n        # this marks that each datapoint is conditionally independent of the others\n        with pyro.plate(""z_minibatch"", len(mini_batch)):\n            # sample the latents z and observed x\'s one time step at a time\n            # we wrap this loop in pyro.markov so that TraceEnum_ELBO can use multiple samples from the guide at each z\n            for t in pyro.markov(range(1, T_max + 1)):\n                # the next chunk of code samples z_t ~ p(z_t | z_{t-1})\n                # note that (both here and elsewhere) we use poutine.scale to take care\n                # of KL annealing. we use the mask() method to deal with raggedness\n                # in the observed data (i.e. different sequences in the mini-batch\n                # have different lengths)\n\n                # first compute the parameters of the diagonal gaussian distribution p(z_t | z_{t-1})\n                z_loc, z_scale = self.trans(z_prev)\n\n                # then sample z_t according to dist.Normal(z_loc, z_scale)\n                # note that we use the reshape method so that the univariate Normal distribution\n                # is treated as a multivariate Normal distribution with a diagonal covariance.\n                with poutine.scale(scale=annealing_factor):\n                    z_t = pyro.sample(""z_%d"" % t,\n                                      dist.Normal(z_loc, z_scale)\n                                          .mask(mini_batch_mask[:, t - 1:t])\n                                          .to_event(1))\n\n                # compute the probabilities that parameterize the bernoulli likelihood\n                emission_probs_t = self.emitter(z_t)\n                # the next statement instructs pyro to observe x_t according to the\n                # bernoulli distribution p(x_t|z_t)\n                pyro.sample(""obs_x_%d"" % t,\n                            dist.Bernoulli(emission_probs_t)\n                                .mask(mini_batch_mask[:, t - 1:t])\n                                .to_event(1),\n                            obs=mini_batch[:, t - 1, :])\n                # the latent sampled at this time step will be conditioned upon\n                # in the next time step so keep track of it\n                z_prev = z_t\n\n    # the guide q(z_{1:T} | x_{1:T}) (i.e. the variational distribution)\n    def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n              mini_batch_seq_lengths, annealing_factor=1.0):\n\n        # this is the number of time steps we need to process in the mini-batch\n        T_max = mini_batch.size(1)\n        # register all PyTorch (sub)modules with pyro\n        pyro.module(""dmm"", self)\n\n        # if on gpu we need the fully broadcast view of the rnn initial state\n        # to be in contiguous gpu memory\n        h_0_contig = self.h_0.expand(1, mini_batch.size(0), self.rnn.hidden_size).contiguous()\n        # push the observed x\'s through the rnn;\n        # rnn_output contains the hidden state at each time step\n        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n        # reverse the time-ordering in the hidden state and un-pack it\n        rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n        # set z_prev = z_q_0 to setup the recursive conditioning in q(z_t |...)\n        z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))\n\n        # we enclose all the sample statements in the guide in a plate.\n        # this marks that each datapoint is conditionally independent of the others.\n        with pyro.plate(""z_minibatch"", len(mini_batch)):\n            # sample the latents z one time step at a time\n            # we wrap this loop in pyro.markov so that TraceEnum_ELBO can use multiple samples from the guide at each z\n            for t in pyro.markov(range(1, T_max + 1)):\n                # the next two lines assemble the distribution q(z_t | z_{t-1}, x_{t:T})\n                z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n\n                # if we are using normalizing flows, we apply the sequence of transformations\n                # parameterized by self.iafs to the base distribution defined in the previous line\n                # to yield a transformed distribution that we use for q(z_t|...)\n                if len(self.iafs) > 0:\n                    z_dist = TransformedDistribution(dist.Normal(z_loc, z_scale), self.iafs)\n                    assert z_dist.event_shape == (self.z_q_0.size(0),)\n                    assert z_dist.batch_shape[-1:] == (len(mini_batch),)\n                else:\n                    z_dist = dist.Normal(z_loc, z_scale)\n                    assert z_dist.event_shape == ()\n                    assert z_dist.batch_shape[-2:] == (len(mini_batch), self.z_q_0.size(0))\n\n                # sample z_t from the distribution z_dist\n                with pyro.poutine.scale(scale=annealing_factor):\n                    if len(self.iafs) > 0:\n                        # in output of normalizing flow, all dimensions are correlated (event shape is not empty)\n                        z_t = pyro.sample(""z_%d"" % t,\n                                          z_dist.mask(mini_batch_mask[:, t - 1]))\n                    else:\n                        # when no normalizing flow used, "".to_event(1)"" indicates latent dimensions are independent\n                        z_t = pyro.sample(""z_%d"" % t,\n                                          z_dist.mask(mini_batch_mask[:, t - 1:t])\n                                          .to_event(1))\n                # the latent sampled at this time step will be conditioned upon in the next time step\n                # so keep track of it\n                z_prev = z_t\n\n\n# setup, training, and evaluation\ndef main(args):\n    # setup logging\n    log = get_logger(args.log)\n    log(args)\n\n    data = poly.load_data(poly.JSB_CHORALES)\n    training_seq_lengths = data[\'train\'][\'sequence_lengths\']\n    training_data_sequences = data[\'train\'][\'sequences\']\n    test_seq_lengths = data[\'test\'][\'sequence_lengths\']\n    test_data_sequences = data[\'test\'][\'sequences\']\n    val_seq_lengths = data[\'valid\'][\'sequence_lengths\']\n    val_data_sequences = data[\'valid\'][\'sequences\']\n    N_train_data = len(training_seq_lengths)\n    N_train_time_slices = float(torch.sum(training_seq_lengths))\n    N_mini_batches = int(N_train_data / args.mini_batch_size +\n                         int(N_train_data % args.mini_batch_size > 0))\n\n    log(""N_train_data: %d     avg. training seq. length: %.2f    N_mini_batches: %d"" %\n        (N_train_data, training_seq_lengths.float().mean(), N_mini_batches))\n\n    # how often we do validation/test evaluation during training\n    val_test_frequency = 50\n    # the number of samples we use to do the evaluation\n    n_eval_samples = 1\n\n    # package repeated copies of val/test data for faster evaluation\n    # (i.e. set us up for vectorization)\n    def rep(x):\n        rep_shape = torch.Size([x.size(0) * n_eval_samples]) + x.size()[1:]\n        repeat_dims = [1] * len(x.size())\n        repeat_dims[0] = n_eval_samples\n        return x.repeat(repeat_dims).reshape(n_eval_samples, -1).transpose(1, 0).reshape(rep_shape)\n\n    # get the validation/test data ready for the dmm: pack into sequences, etc.\n    val_seq_lengths = rep(val_seq_lengths)\n    test_seq_lengths = rep(test_seq_lengths)\n    val_batch, val_batch_reversed, val_batch_mask, val_seq_lengths = poly.get_mini_batch(\n        torch.arange(n_eval_samples * val_data_sequences.shape[0]), rep(val_data_sequences),\n        val_seq_lengths, cuda=args.cuda)\n    test_batch, test_batch_reversed, test_batch_mask, test_seq_lengths = poly.get_mini_batch(\n        torch.arange(n_eval_samples * test_data_sequences.shape[0]), rep(test_data_sequences),\n        test_seq_lengths, cuda=args.cuda)\n\n    # instantiate the dmm\n    dmm = DMM(rnn_dropout_rate=args.rnn_dropout_rate, num_iafs=args.num_iafs,\n              iaf_dim=args.iaf_dim, use_cuda=args.cuda)\n\n    # setup optimizer\n    adam_params = {""lr"": args.learning_rate, ""betas"": (args.beta1, args.beta2),\n                   ""clip_norm"": args.clip_norm, ""lrd"": args.lr_decay,\n                   ""weight_decay"": args.weight_decay}\n    adam = ClippedAdam(adam_params)\n\n    # setup inference algorithm\n    if args.tmc:\n        if args.jit:\n            raise NotImplementedError(""no JIT support yet for TMC"")\n        tmc_loss = TraceTMC_ELBO()\n        dmm_guide = config_enumerate(dmm.guide, default=""parallel"", num_samples=args.tmc_num_samples, expand=False)\n        svi = SVI(dmm.model, dmm_guide, adam, loss=tmc_loss)\n    elif args.tmcelbo:\n        if args.jit:\n            raise NotImplementedError(""no JIT support yet for TMC ELBO"")\n        elbo = TraceEnum_ELBO()\n        dmm_guide = config_enumerate(dmm.guide, default=""parallel"", num_samples=args.tmc_num_samples, expand=False)\n        svi = SVI(dmm.model, dmm_guide, adam, loss=elbo)\n    else:\n        elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()\n        svi = SVI(dmm.model, dmm.guide, adam, loss=elbo)\n\n    # now we\'re going to define some functions we need to form the main training loop\n\n    # saves the model and optimizer states to disk\n    def save_checkpoint():\n        log(""saving model to %s..."" % args.save_model)\n        torch.save(dmm.state_dict(), args.save_model)\n        log(""saving optimizer states to %s..."" % args.save_opt)\n        adam.save(args.save_opt)\n        log(""done saving model and optimizer checkpoints to disk."")\n\n    # loads the model and optimizer states from disk\n    def load_checkpoint():\n        assert exists(args.load_opt) and exists(args.load_model), \\\n            ""--load-model and/or --load-opt misspecified""\n        log(""loading model from %s..."" % args.load_model)\n        dmm.load_state_dict(torch.load(args.load_model))\n        log(""loading optimizer states from %s..."" % args.load_opt)\n        adam.load(args.load_opt)\n        log(""done loading model and optimizer states."")\n\n    # prepare a mini-batch and take a gradient step to minimize -elbo\n    def process_minibatch(epoch, which_mini_batch, shuffled_indices):\n        if args.annealing_epochs > 0 and epoch < args.annealing_epochs:\n            # compute the KL annealing factor approriate for the current mini-batch in the current epoch\n            min_af = args.minimum_annealing_factor\n            annealing_factor = min_af + (1.0 - min_af) * \\\n                (float(which_mini_batch + epoch * N_mini_batches + 1) /\n                 float(args.annealing_epochs * N_mini_batches))\n        else:\n            # by default the KL annealing factor is unity\n            annealing_factor = 1.0\n\n        # compute which sequences in the training set we should grab\n        mini_batch_start = (which_mini_batch * args.mini_batch_size)\n        mini_batch_end = np.min([(which_mini_batch + 1) * args.mini_batch_size, N_train_data])\n        mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n        # grab a fully prepped mini-batch using the helper function in the data loader\n        mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths \\\n            = poly.get_mini_batch(mini_batch_indices, training_data_sequences,\n                                  training_seq_lengths, cuda=args.cuda)\n        # do an actual gradient step\n        loss = svi.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n                        mini_batch_seq_lengths, annealing_factor)\n        # keep track of the training loss\n        return loss\n\n    # helper function for doing evaluation\n    def do_evaluation():\n        # put the RNN into evaluation mode (i.e. turn off drop-out if applicable)\n        dmm.rnn.eval()\n\n        # compute the validation and test loss n_samples many times\n        val_nll = svi.evaluate_loss(val_batch, val_batch_reversed, val_batch_mask,\n                                    val_seq_lengths) / float(torch.sum(val_seq_lengths))\n        test_nll = svi.evaluate_loss(test_batch, test_batch_reversed, test_batch_mask,\n                                     test_seq_lengths) / float(torch.sum(test_seq_lengths))\n\n        # put the RNN back into training mode (i.e. turn on drop-out if applicable)\n        dmm.rnn.train()\n        return val_nll, test_nll\n\n    # if checkpoint files provided, load model and optimizer states from disk before we start training\n    if args.load_opt != \'\' and args.load_model != \'\':\n        load_checkpoint()\n\n    #################\n    # TRAINING LOOP #\n    #################\n    times = [time.time()]\n    for epoch in range(args.num_epochs):\n        # if specified, save model and optimizer states to disk every checkpoint_freq epochs\n        if args.checkpoint_freq > 0 and epoch > 0 and epoch % args.checkpoint_freq == 0:\n            save_checkpoint()\n\n        # accumulator for our estimate of the negative log likelihood (or rather -elbo) for this epoch\n        epoch_nll = 0.0\n        # prepare mini-batch subsampling indices for this epoch\n        shuffled_indices = torch.randperm(N_train_data)\n\n        # process each mini-batch; this is where we take gradient steps\n        for which_mini_batch in range(N_mini_batches):\n            epoch_nll += process_minibatch(epoch, which_mini_batch, shuffled_indices)\n\n        # report training diagnostics\n        times.append(time.time())\n        epoch_time = times[-1] - times[-2]\n        log(""[training epoch %04d]  %.4f \\t\\t\\t\\t(dt = %.3f sec)"" %\n            (epoch, epoch_nll / N_train_time_slices, epoch_time))\n\n        # do evaluation on test and validation data and report results\n        if val_test_frequency > 0 and epoch > 0 and epoch % val_test_frequency == 0:\n            val_nll, test_nll = do_evaluation()\n            log(""[val/test epoch %04d]  %.4f  %.4f"" % (epoch, val_nll, test_nll))\n\n\n# parse command-line arguments and execute the main method\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-epochs\', type=int, default=5000)\n    parser.add_argument(\'-lr\', \'--learning-rate\', type=float, default=0.0003)\n    parser.add_argument(\'-b1\', \'--beta1\', type=float, default=0.96)\n    parser.add_argument(\'-b2\', \'--beta2\', type=float, default=0.999)\n    parser.add_argument(\'-cn\', \'--clip-norm\', type=float, default=10.0)\n    parser.add_argument(\'-lrd\', \'--lr-decay\', type=float, default=0.99996)\n    parser.add_argument(\'-wd\', \'--weight-decay\', type=float, default=2.0)\n    parser.add_argument(\'-mbs\', \'--mini-batch-size\', type=int, default=20)\n    parser.add_argument(\'-ae\', \'--annealing-epochs\', type=int, default=1000)\n    parser.add_argument(\'-maf\', \'--minimum-annealing-factor\', type=float, default=0.2)\n    parser.add_argument(\'-rdr\', \'--rnn-dropout-rate\', type=float, default=0.1)\n    parser.add_argument(\'-iafs\', \'--num-iafs\', type=int, default=0)\n    parser.add_argument(\'-id\', \'--iaf-dim\', type=int, default=100)\n    parser.add_argument(\'-cf\', \'--checkpoint-freq\', type=int, default=0)\n    parser.add_argument(\'-lopt\', \'--load-opt\', type=str, default=\'\')\n    parser.add_argument(\'-lmod\', \'--load-model\', type=str, default=\'\')\n    parser.add_argument(\'-sopt\', \'--save-opt\', type=str, default=\'\')\n    parser.add_argument(\'-smod\', \'--save-model\', type=str, default=\'\')\n    parser.add_argument(\'--cuda\', action=\'store_true\')\n    parser.add_argument(\'--jit\', action=\'store_true\')\n    parser.add_argument(\'--tmc\', action=\'store_true\')\n    parser.add_argument(\'--tmcelbo\', action=\'store_true\')\n    parser.add_argument(\'--tmc-num-samples\', default=10, type=int)\n    parser.add_argument(\'-l\', \'--log\', type=str, default=\'dmm.log\')\n    args = parser.parse_args()\n\n    main(args)\n'"
examples/dmm/polyphonic_data_loader.py,15,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nData loader logic with two main responsibilities:\n(i)  download raw data and process; this logic is initiated upon import\n(ii) helper functions for dealing with mini-batches, sequence packing, etc.\n\nData are taken from\n\nBoulanger-Lewandowski, N., Bengio, Y. and Vincent, P.,\n""Modeling Temporal Dependencies in High-Dimensional Sequences: Application to\nPolyphonic Music Generation and Transcription""\n\nhowever, the original source of the data seems to be the Institut fuer Algorithmen\nund Kognitive Systeme at Universitaet Karlsruhe.\n""""""\n\nimport os\nfrom collections import namedtuple\nfrom urllib.request import urlopen\nimport pickle\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom pyro.contrib.examples.util import get_data_directory\n\n\ndset = namedtuple(""dset"", [""name"", ""url"", ""filename""])\n\nJSB_CHORALES = dset(""jsb_chorales"",\n                    ""https://d2hg8soec8ck9v.cloudfront.net/datasets/polyphonic/jsb_chorales.pickle"",\n                    ""jsb_chorales.pkl"")\n\nPIANO_MIDI = dset(""piano_midi"",\n                  ""https://d2hg8soec8ck9v.cloudfront.net/datasets/polyphonic/piano_midi.pickle"",\n                  ""piano_midi.pkl"")\n\nMUSE_DATA = dset(""muse_data"",\n                 ""https://d2hg8soec8ck9v.cloudfront.net/datasets/polyphonic/muse_data.pickle"",\n                 ""muse_data.pkl"")\n\nNOTTINGHAM = dset(""nottingham"",\n                  ""https://d2hg8soec8ck9v.cloudfront.net/datasets/polyphonic/nottingham.pickle"",\n                  ""nottingham.pkl"")\n\n\n# this function processes the raw data; in particular it unsparsifies it\ndef process_data(base_path, dataset, min_note=21, note_range=88):\n    output = os.path.join(base_path, dataset.filename)\n    if os.path.exists(output):\n        try:\n            with open(output, ""rb"") as f:\n                return pickle.load(f)\n        except (ValueError, UnicodeDecodeError):\n            # Assume python env has changed.\n            # Recreate pickle file in this env\'s format.\n            os.remove(output)\n\n    print(""processing raw data - {} ..."".format(dataset.name))\n    data = pickle.load(urlopen(dataset.url))\n    processed_dataset = {}\n    for split, data_split in data.items():\n        processed_dataset[split] = {}\n        n_seqs = len(data_split)\n        processed_dataset[split][\'sequence_lengths\'] = torch.zeros(n_seqs, dtype=torch.long)\n        processed_dataset[split][\'sequences\'] = []\n        for seq in range(n_seqs):\n            seq_length = len(data_split[seq])\n            processed_dataset[split][\'sequence_lengths\'][seq] = seq_length\n            processed_sequence = torch.zeros((seq_length, note_range))\n            for t in range(seq_length):\n                note_slice = torch.tensor(list(data_split[seq][t])) - min_note\n                slice_length = len(note_slice)\n                if slice_length > 0:\n                    processed_sequence[t, note_slice] = torch.ones(slice_length)\n            processed_dataset[split][\'sequences\'].append(processed_sequence)\n    pickle.dump(processed_dataset, open(output, ""wb""), pickle.HIGHEST_PROTOCOL)\n    print(""dumped processed data to %s"" % output)\n\n\n# this logic will be initiated upon import\nbase_path = get_data_directory(__file__)\nif not os.path.exists(base_path):\n    os.mkdir(base_path)\n\n\n# ingest training/validation/test data from disk\ndef load_data(dataset):\n    # download and process dataset if it does not exist\n    process_data(base_path, dataset)\n    file_loc = os.path.join(base_path, dataset.filename)\n    with open(file_loc, ""rb"") as f:\n        dset = pickle.load(f)\n        for k, v in dset.items():\n            sequences = v[""sequences""]\n            dset[k][""sequences""] = pad_sequence(sequences, batch_first=True).type(torch.Tensor)\n            dset[k][""sequence_lengths""] = v[""sequence_lengths""].to(device=torch.Tensor().device)\n    return dset\n\n\n# this function takes a torch mini-batch and reverses each sequence\n# (w.r.t. the temporal axis, i.e. axis=1).\ndef reverse_sequences(mini_batch, seq_lengths):\n    reversed_mini_batch = torch.zeros_like(mini_batch)\n    for b in range(mini_batch.size(0)):\n        T = seq_lengths[b]\n        time_slice = torch.arange(T - 1, -1, -1, device=mini_batch.device)\n        reversed_sequence = torch.index_select(mini_batch[b, :, :], 0, time_slice)\n        reversed_mini_batch[b, 0:T, :] = reversed_sequence\n    return reversed_mini_batch\n\n\n# this function takes the hidden state as output by the PyTorch rnn and\n# unpacks it it; it also reverses each sequence temporally\ndef pad_and_reverse(rnn_output, seq_lengths):\n    rnn_output, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n    reversed_output = reverse_sequences(rnn_output, seq_lengths)\n    return reversed_output\n\n\n# this function returns a 0/1 mask that can be used to mask out a mini-batch\n# composed of sequences of length `seq_lengths`\ndef get_mini_batch_mask(mini_batch, seq_lengths):\n    mask = torch.zeros(mini_batch.shape[0:2])\n    for b in range(mini_batch.shape[0]):\n        mask[b, 0:seq_lengths[b]] = torch.ones(seq_lengths[b])\n    return mask\n\n\n# this function prepares a mini-batch for training or evaluation.\n# it returns a mini-batch in forward temporal order (`mini_batch`) as\n# well as a mini-batch in reverse temporal order (`mini_batch_reversed`).\n# it also deals with the fact that packed sequences (which are what what we\n# feed to the PyTorch rnn) need to be sorted by sequence length.\ndef get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):\n    # get the sequence lengths of the mini-batch\n    seq_lengths = seq_lengths[mini_batch_indices]\n    # sort the sequence lengths\n    _, sorted_seq_length_indices = torch.sort(seq_lengths)\n    sorted_seq_length_indices = sorted_seq_length_indices.flip(0)\n    sorted_seq_lengths = seq_lengths[sorted_seq_length_indices]\n    sorted_mini_batch_indices = mini_batch_indices[sorted_seq_length_indices]\n\n    # compute the length of the longest sequence in the mini-batch\n    T_max = torch.max(seq_lengths)\n    # this is the sorted mini-batch\n    mini_batch = sequences[sorted_mini_batch_indices, 0:T_max, :]\n    # this is the sorted mini-batch in reverse temporal order\n    mini_batch_reversed = reverse_sequences(mini_batch, sorted_seq_lengths)\n    # get mask for mini-batch\n    mini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)\n\n    # cuda() here because need to cuda() before packing\n    if cuda:\n        mini_batch = mini_batch.cuda()\n        mini_batch_mask = mini_batch_mask.cuda()\n        mini_batch_reversed = mini_batch_reversed.cuda()\n\n    # do sequence packing\n    mini_batch_reversed = nn.utils.rnn.pack_padded_sequence(mini_batch_reversed,\n                                                            sorted_seq_lengths,\n                                                            batch_first=True)\n\n    return mini_batch, mini_batch_reversed, mini_batch_mask, sorted_seq_lengths\n'"
examples/dmm/util.py,0,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\n\ndef get_logger(log_file):\n    logging.basicConfig(level=logging.DEBUG, format='%(message)s', filename=log_file, filemode='w')\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    logging.getLogger('').addHandler(console)\n\n    def log(s):\n        logging.info(s)\n\n    return log\n"""
examples/eight_schools/data.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\n\nJ = 8\ny = torch.tensor([28,  8, -3,  7, -1,  1, 18, 12]).type(torch.Tensor)\nsigma = torch.tensor([15, 10, 16, 11,  9, 11, 10, 18]).type(torch.Tensor)\n'"
examples/eight_schools/mcmc.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport logging\n\nimport torch\n\nimport data\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer import MCMC, NUTS\n\nlogging.basicConfig(format=\'%(message)s\', level=logging.INFO)\npyro.enable_validation(__debug__)\npyro.set_rng_seed(0)\n\n\ndef model(sigma):\n    eta = pyro.sample(\'eta\', dist.Normal(torch.zeros(data.J), torch.ones(data.J)))\n    mu = pyro.sample(\'mu\', dist.Normal(torch.zeros(1), 10 * torch.ones(1)))\n    tau = pyro.sample(\'tau\', dist.HalfCauchy(scale=25 * torch.ones(1)))\n\n    theta = mu + tau * eta\n\n    return pyro.sample(""obs"", dist.Normal(theta, sigma))\n\n\ndef conditioned_model(model, sigma, y):\n    return poutine.condition(model, data={""obs"": y})(sigma)\n\n\ndef main(args):\n    nuts_kernel = NUTS(conditioned_model, jit_compile=args.jit)\n    mcmc = MCMC(nuts_kernel,\n                num_samples=args.num_samples,\n                warmup_steps=args.warmup_steps,\n                num_chains=args.num_chains)\n    mcmc.run(model, data.sigma, data.y)\n    mcmc.summary(prob=0.5)\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=\'Eight Schools MCMC\')\n    parser.add_argument(\'--num-samples\', type=int, default=1000,\n                        help=\'number of MCMC samples (default: 1000)\')\n    parser.add_argument(\'--num-chains\', type=int, default=1,\n                        help=\'number of parallel MCMC chains (default: 1)\')\n    parser.add_argument(\'--warmup-steps\', type=int, default=1000,\n                        help=\'number of MCMC samples for warmup (default: 1000)\')\n    parser.add_argument(\'--jit\', action=\'store_true\', default=False)\n    args = parser.parse_args()\n\n    main(args)\n'"
examples/eight_schools/svi.py,11,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport logging\n\nimport torch\nfrom torch.distributions import constraints, transforms\n\nimport pyro\nimport pyro.distributions as dist\nfrom data import J, sigma, y\nfrom pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\nfrom pyro.optim import Adam\n\nlogging.basicConfig(format=\'%(message)s\', level=logging.INFO)\ndata = torch.stack([y, sigma], dim=1)\n\n\ndef model(data):\n    y = data[:, 0]\n    sigma = data[:, 1]\n\n    eta = pyro.sample(\'eta\', dist.Normal(torch.zeros(J), torch.ones(J)))\n    mu = pyro.sample(\'mu\', dist.Normal(torch.zeros(1), 10 * torch.ones(1)))\n    tau = pyro.sample(\'tau\', dist.HalfCauchy(scale=25 * torch.ones(1)))\n\n    theta = mu + tau * eta\n\n    pyro.sample(""obs"", dist.Normal(theta, sigma), obs=y)\n\n\ndef guide(data):\n    loc_eta = torch.randn(J, 1)\n    # note that we initialize our scales to be pretty narrow\n    scale_eta = 0.1 * torch.rand(J, 1)\n    loc_mu = torch.randn(1)\n    scale_mu = 0.1 * torch.rand(1)\n    loc_logtau = torch.randn(1)\n    scale_logtau = 0.1 * torch.rand(1)\n\n    # register learnable params in the param store\n    m_eta_param = pyro.param(""loc_eta"", loc_eta)\n    s_eta_param = pyro.param(""scale_eta"", scale_eta, constraint=constraints.positive)\n    m_mu_param = pyro.param(""loc_mu"", loc_mu)\n    s_mu_param = pyro.param(""scale_mu"", scale_mu, constraint=constraints.positive)\n    m_logtau_param = pyro.param(""loc_logtau"", loc_logtau)\n    s_logtau_param = pyro.param(""scale_logtau"", scale_logtau, constraint=constraints.positive)\n\n    # guide distributions\n    dist_eta = dist.Normal(m_eta_param, s_eta_param)\n    dist_mu = dist.Normal(m_mu_param, s_mu_param)\n    dist_tau = dist.TransformedDistribution(dist.Normal(m_logtau_param, s_logtau_param),\n                                            transforms=transforms.ExpTransform())\n\n    pyro.sample(\'eta\', dist_eta)\n    pyro.sample(\'mu\', dist_mu)\n    pyro.sample(\'tau\', dist_tau)\n\n\ndef main(args):\n    optim = Adam({\'lr\': args.lr})\n    elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()\n    svi = SVI(model, guide, optim, loss=elbo)\n\n    pyro.clear_param_store()\n    for j in range(args.num_epochs):\n        loss = svi.step(data)\n        if j % 100 == 0:\n            logging.info(""[epoch %04d] loss: %.4f"" % (j + 1, loss))\n\n    for name, value in pyro.get_param_store().items():\n        logging.info(name)\n        logging.info(value.detach().cpu().numpy())\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=\'Eight Schools SVI\')\n    parser.add_argument(\'--lr\', type=float, default=0.01,\n                        help=\'learning rate (default: 0.01)\')\n    parser.add_argument(\'--num-epochs\', type=int, default=1000,\n                        help=\'number of epochs (default: 1000)\')\n    parser.add_argument(\'--jit\', action=\'store_true\', default=False)\n    args = parser.parse_args()\n\n    main(args)\n'"
examples/mixed_hmm/__init__.py,0,b''
examples/mixed_hmm/experiment.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport os\nimport json\nimport uuid\nimport functools\n\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.infer import TraceEnum_ELBO\n\nfrom model import model_generic, guide_generic\nfrom seal_data import prepare_seal\n\n\ndef aic_num_parameters(model, guide=None):\n    """"""\n    hacky AIC param count that includes all parameters in the model and guide\n    """"""\n\n    def _size(tensor):\n        """"""product of shape""""""\n        s = 1\n        for d in tensor.shape:\n            s = s * d\n        return s\n\n    with poutine.block(), poutine.trace(param_only=True) as param_capture:\n        TraceEnum_ELBO(max_plate_nesting=2).differentiable_loss(model, guide)\n\n    return sum(_size(node[""value""]) for node in param_capture.trace.nodes.values())\n\n\ndef run_expt(args):\n\n    data_dir = args[""folder""]\n    dataset = ""seal""  # args[""dataset""]\n    seed = args[""seed""]\n    optim = args[""optim""]\n    lr = args[""learnrate""]\n    timesteps = args[""timesteps""]\n    schedule = [] if not args[""schedule""] else [int(i) for i in args[""schedule""].split("","")]\n    random_effects = {""group"": args[""group""], ""individual"": args[""individual""]}\n\n    pyro.enable_validation(args[""validation""])\n    pyro.set_rng_seed(seed)  # reproducible random effect parameter init\n\n    filename = os.path.join(data_dir, ""prep_seal_data.csv"")\n    config = prepare_seal(filename, random_effects)\n\n    model = functools.partial(model_generic, config)  # for JITing\n    guide = functools.partial(guide_generic, config)\n\n    # count the number of parameters once\n    num_parameters = aic_num_parameters(model, guide)\n\n    losses = []\n    # SGD\n    if optim == ""sgd"":\n        loss_fn = TraceEnum_ELBO(max_plate_nesting=2).differentiable_loss\n        with pyro.poutine.trace(param_only=True) as param_capture:\n            loss_fn(model, guide)\n        params = [site[""value""].unconstrained() for site in param_capture.trace.nodes.values()]\n        optimizer = torch.optim.Adam(params, lr=lr)\n\n        if schedule:\n            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=schedule, gamma=0.5)\n            schedule_step_loss = False\n        else:\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \'min\')\n            schedule_step_loss = True\n\n        for t in range(timesteps):\n\n            optimizer.zero_grad()\n            loss = loss_fn(model, guide)\n            loss.backward()\n            optimizer.step()\n            scheduler.step(loss.item() if schedule_step_loss else t)\n            losses.append(loss.item())\n\n            print(""Loss: {}, AIC[{}]: "".format(loss.item(), t),\n                  2. * loss + 2. * num_parameters)\n\n    # LBFGS\n    elif optim == ""lbfgs"":\n        loss_fn = TraceEnum_ELBO(max_plate_nesting=2).differentiable_loss\n        with pyro.poutine.trace(param_only=True) as param_capture:\n            loss_fn(model, guide)\n        params = [site[""value""].unconstrained() for site in param_capture.trace.nodes.values()]\n        optimizer = torch.optim.LBFGS(params, lr=lr)\n\n        if schedule:\n            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=schedule, gamma=0.5)\n            schedule_step_loss = False\n        else:\n            schedule_step_loss = True\n            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \'min\')\n\n        for t in range(timesteps):\n            def closure():\n                optimizer.zero_grad()\n                loss = loss_fn(model, guide)\n                loss.backward()\n                return loss\n            loss = optimizer.step(closure)\n            scheduler.step(loss.item() if schedule_step_loss else t)\n            losses.append(loss.item())\n            print(""Loss: {}, AIC[{}]: "".format(loss.item(), t),\n                  2. * loss + 2. * num_parameters)\n\n    else:\n        raise ValueError(""{} not supported optimizer"".format(optim))\n\n    aic_final = 2. * losses[-1] + 2. * num_parameters\n    print(""AIC final: {}"".format(aic_final))\n\n    results = {}\n    results[""args""] = args\n    results[""sizes""] = config[""sizes""]\n    results[""likelihoods""] = losses\n    results[""likelihood_final""] = losses[-1]\n    results[""aic_final""] = aic_final\n    results[""aic_num_parameters""] = num_parameters\n\n    if args[""resultsdir""] is not None and os.path.exists(args[""resultsdir""]):\n        re_str = ""g"" + (""n"" if args[""group""] is None else ""d"" if args[""group""] == ""discrete"" else ""c"")\n        re_str += ""i"" + (""n"" if args[""individual""] is None else ""d"" if args[""individual""] == ""discrete"" else ""c"")\n        results_filename = ""expt_{}_{}_{}.json"".format(dataset, re_str, str(uuid.uuid4().hex)[0:5])\n        with open(os.path.join(args[""resultsdir""], results_filename), ""w"") as f:\n            json.dump(results, f)\n\n    return results\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-g"", ""--group"", default=""none"", type=str)\n    parser.add_argument(""-i"", ""--individual"", default=""none"", type=str)\n    parser.add_argument(""-f"", ""--folder"", default=""./"", type=str)\n    parser.add_argument(""-o"", ""--optim"", default=""sgd"", type=str)\n    parser.add_argument(""-lr"", ""--learnrate"", default=0.05, type=float)\n    parser.add_argument(""-t"", ""--timesteps"", default=1000, type=int)\n    parser.add_argument(""-r"", ""--resultsdir"", default=""./results"", type=str)\n    parser.add_argument(""-s"", ""--seed"", default=101, type=int)\n    parser.add_argument(""--schedule"", default="""", type=str)\n    parser.add_argument(\'--validation\', action=\'store_true\')\n    args = parser.parse_args()\n\n    if args.group == ""none"":\n        args.group = None\n    if args.individual == ""none"":\n        args.individual = None\n\n    run_expt(vars(args))\n'"
examples/mixed_hmm/model.py,25,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer import config_enumerate\nfrom pyro.ops.indexing import Vindex\n\n\ndef guide_generic(config):\n    """"""generic mean-field guide for continuous random effects""""""\n    N_state = config[""sizes""][""state""]\n\n    if config[""group""][""random""] == ""continuous"":\n        loc_g = pyro.param(""loc_group"", lambda: torch.zeros((N_state ** 2,)))\n        scale_g = pyro.param(""scale_group"", lambda: torch.ones((N_state ** 2,)),\n                             constraint=constraints.positive)\n\n    # initialize individual-level random effect parameters\n    N_c = config[""sizes""][""group""]\n    if config[""individual""][""random""] == ""continuous"":\n        loc_i = pyro.param(""loc_individual"", lambda: torch.zeros((N_c, N_state ** 2,)))\n        scale_i = pyro.param(""scale_individual"", lambda: torch.ones((N_c, N_state ** 2,)),\n                             constraint=constraints.positive)\n\n    N_c = config[""sizes""][""group""]\n    with pyro.plate(""group"", N_c, dim=-1):\n\n        if config[""group""][""random""] == ""continuous"":\n            pyro.sample(""eps_g"", dist.Normal(loc_g, scale_g).to_event(1),\n                        )  # infer={""num_samples"": 10})\n\n        N_s = config[""sizes""][""individual""]\n        with pyro.plate(""individual"", N_s, dim=-2), poutine.mask(mask=config[""individual""][""mask""]):\n\n            # individual-level random effects\n            if config[""individual""][""random""] == ""continuous"":\n                pyro.sample(""eps_i"", dist.Normal(loc_i, scale_i).to_event(1),\n                            )  # infer={""num_samples"": 10})\n\n\n@config_enumerate\ndef model_generic(config):\n    """"""Hierarchical mixed-effects hidden markov model""""""\n\n    MISSING = config[""MISSING""]\n    N_v = config[""sizes""][""random""]\n    N_state = config[""sizes""][""state""]\n\n    # initialize group-level random effect parameterss\n    if config[""group""][""random""] == ""discrete"":\n        probs_e_g = pyro.param(""probs_e_group"", lambda: torch.randn((N_v,)).abs(), constraint=constraints.simplex)\n        theta_g = pyro.param(""theta_group"", lambda: torch.randn((N_v, N_state ** 2)))\n    elif config[""group""][""random""] == ""continuous"":\n        loc_g = torch.zeros((N_state ** 2,))\n        scale_g = torch.ones((N_state ** 2,))\n\n    # initialize individual-level random effect parameters\n    N_c = config[""sizes""][""group""]\n    if config[""individual""][""random""] == ""discrete"":\n        probs_e_i = pyro.param(""probs_e_individual"",\n                               lambda: torch.randn((N_c, N_v,)).abs(),\n                               constraint=constraints.simplex)\n        theta_i = pyro.param(""theta_individual"",\n                             lambda: torch.randn((N_c, N_v, N_state ** 2)))\n    elif config[""individual""][""random""] == ""continuous"":\n        loc_i = torch.zeros((N_c, N_state ** 2,))\n        scale_i = torch.ones((N_c, N_state ** 2,))\n\n    # initialize likelihood parameters\n    # observation 1: step size (step ~ Gamma)\n    step_zi_param = pyro.param(""step_zi_param"", lambda: torch.ones((N_state, 2)))\n    step_concentration = pyro.param(""step_param_concentration"",\n                                    lambda: torch.randn((N_state,)).abs(),\n                                    constraint=constraints.positive)\n    step_rate = pyro.param(""step_param_rate"",\n                           lambda: torch.randn((N_state,)).abs(),\n                           constraint=constraints.positive)\n\n    # observation 2: step angle (angle ~ VonMises)\n    angle_concentration = pyro.param(""angle_param_concentration"",\n                                     lambda: torch.randn((N_state,)).abs(),\n                                     constraint=constraints.positive)\n    angle_loc = pyro.param(""angle_param_loc"", lambda: torch.randn((N_state,)).abs())\n\n    # observation 3: dive activity (omega ~ Beta)\n    omega_zi_param = pyro.param(""omega_zi_param"", lambda: torch.ones((N_state, 2)))\n    omega_concentration0 = pyro.param(""omega_param_concentration0"",\n                                      lambda: torch.randn((N_state,)).abs(),\n                                      constraint=constraints.positive)\n    omega_concentration1 = pyro.param(""omega_param_concentration1"",\n                                      lambda: torch.randn((N_state,)).abs(),\n                                      constraint=constraints.positive)\n\n    # initialize gamma to uniform\n    gamma = torch.zeros((N_state ** 2,))\n\n    N_c = config[""sizes""][""group""]\n    with pyro.plate(""group"", N_c, dim=-1):\n\n        # group-level random effects\n        if config[""group""][""random""] == ""discrete"":\n            # group-level discrete effect\n            e_g = pyro.sample(""e_g"", dist.Categorical(probs_e_g))\n            eps_g = Vindex(theta_g)[..., e_g, :]\n        elif config[""group""][""random""] == ""continuous"":\n            eps_g = pyro.sample(""eps_g"", dist.Normal(loc_g, scale_g).to_event(1),\n                                )  # infer={""num_samples"": 10})\n        else:\n            eps_g = 0.\n\n        # add group-level random effect to gamma\n        gamma = gamma + eps_g\n\n        N_s = config[""sizes""][""individual""]\n        with pyro.plate(""individual"", N_s, dim=-2), poutine.mask(mask=config[""individual""][""mask""]):\n\n            # individual-level random effects\n            if config[""individual""][""random""] == ""discrete"":\n                # individual-level discrete effect\n                e_i = pyro.sample(""e_i"", dist.Categorical(probs_e_i))\n                eps_i = Vindex(theta_i)[..., e_i, :]\n                # assert eps_i.shape[-3:] == (1, N_c, N_state ** 2) and eps_i.shape[0] == N_v\n            elif config[""individual""][""random""] == ""continuous"":\n                eps_i = pyro.sample(""eps_i"", dist.Normal(loc_i, scale_i).to_event(1),\n                                    )  # infer={""num_samples"": 10})\n            else:\n                eps_i = 0.\n\n            # add individual-level random effect to gamma\n            gamma = gamma + eps_i\n\n            y = torch.tensor(0).long()\n\n            N_t = config[""sizes""][""timesteps""]\n            for t in pyro.markov(range(N_t)):\n                with poutine.mask(mask=config[""timestep""][""mask""][..., t]):\n                    gamma_t = gamma  # per-timestep variable\n\n                    # finally, reshape gamma as batch of transition matrices\n                    gamma_t = gamma_t.reshape(tuple(gamma_t.shape[:-1]) + (N_state, N_state))\n\n                    # we\'ve accounted for all effects, now actually compute gamma_y\n                    gamma_y = Vindex(gamma_t)[..., y, :]\n                    y = pyro.sample(""y_{}"".format(t), dist.Categorical(logits=gamma_y))\n\n                    # observation 1: step size\n                    step_dist = dist.Gamma(\n                        concentration=Vindex(step_concentration)[..., y],\n                        rate=Vindex(step_rate)[..., y]\n                    )\n\n                    # zero-inflation with MaskedMixture\n                    step_zi = Vindex(step_zi_param)[..., y, :]\n                    step_zi_mask = pyro.sample(""step_zi_{}"".format(t),\n                                               dist.Categorical(logits=step_zi),\n                                               obs=(config[""observations""][""step""][..., t] == MISSING))\n                    step_zi_zero_dist = dist.Delta(v=torch.tensor(MISSING))\n                    step_zi_dist = dist.MaskedMixture(step_zi_mask, step_dist, step_zi_zero_dist)\n\n                    pyro.sample(""step_{}"".format(t),\n                                step_zi_dist,\n                                obs=config[""observations""][""step""][..., t])\n\n                    # observation 2: step angle\n                    angle_dist = dist.VonMises(\n                        concentration=Vindex(angle_concentration)[..., y],\n                        loc=Vindex(angle_loc)[..., y]\n                    )\n                    pyro.sample(""angle_{}"".format(t),\n                                angle_dist,\n                                obs=config[""observations""][""angle""][..., t])\n\n                    # observation 3: dive activity\n                    omega_dist = dist.Beta(\n                        concentration0=Vindex(omega_concentration0)[..., y],\n                        concentration1=Vindex(omega_concentration1)[..., y]\n                    )\n\n                    # zero-inflation with MaskedMixture\n                    omega_zi = Vindex(omega_zi_param)[..., y, :]\n                    omega_zi_mask = pyro.sample(\n                        ""omega_zi_{}"".format(t),\n                        dist.Categorical(logits=omega_zi),\n                        obs=(config[""observations""][""omega""][..., t] == MISSING))\n\n                    omega_zi_zero_dist = dist.Delta(v=torch.tensor(MISSING))\n                    omega_zi_dist = dist.MaskedMixture(omega_zi_mask, omega_dist, omega_zi_zero_dist)\n\n                    pyro.sample(""omega_{}"".format(t),\n                                omega_zi_dist,\n                                obs=config[""observations""][""omega""][..., t])\n'"
examples/mixed_hmm/seal_data.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nfrom urllib.request import urlopen\n\nimport pandas as pd\n\nimport torch\n\n\nMISSING = 1e-6\n\n\ndef download_seal_data(filename):\n    """"""download the preprocessed seal data and save it to filename""""""\n    url = ""https://d2hg8soec8ck9v.cloudfront.net/datasets/prep_seal_data.csv""\n    with open(filename, ""wb"") as f:\n        f.write(urlopen(url).read())\n\n\ndef prepare_seal(filename, random_effects):\n\n    if not os.path.exists(filename):\n        download_seal_data(filename)\n\n    seal_df = pd.read_csv(filename)\n    obs_keys = [""step"", ""angle"", ""omega""]\n    # data format for z1, z2:\n    # single tensor with shape (individual, group, time, coords)\n    observations = torch.zeros((20, 2, 1800, len(obs_keys))).fill_(float(""-inf""))\n    for g, (group, group_df) in enumerate(seal_df.groupby(""sex"")):\n        for i, (ind, ind_df) in enumerate(group_df.groupby(""ID"")):\n            for o, obs_key in enumerate(obs_keys):\n                observations[i, g, 0:len(ind_df), o] = torch.tensor(ind_df[obs_key].values)\n\n    observations[torch.isnan(observations)] = float(""-inf"")\n\n    # make masks\n    # mask_i should mask out individuals, it applies at all timesteps\n    mask_i = (observations > float(""-inf"")).any(dim=-1).any(dim=-1)  # time nonempty\n\n    # mask_t handles padding for time series of different length\n    mask_t = (observations > float(""-inf"")).all(dim=-1)   # include non-inf\n\n    # temporary hack to avoid zero-inflation issues\n    # observations[observations == 0.] = MISSING\n    observations[(observations == 0.) | (observations == float(""-inf""))] = MISSING\n    assert not torch.isnan(observations).any()\n\n    # observations = observations[..., 5:11, :]  # truncate for testing\n\n    config = {\n        ""MISSING"": MISSING,\n        ""sizes"": {\n            ""state"": 3,\n            ""random"": 4,\n            ""group"": observations.shape[1],\n            ""individual"": observations.shape[0],\n            ""timesteps"": observations.shape[2],\n        },\n        ""group"": {""random"": random_effects[""group""], ""fixed"": None},\n        ""individual"": {""random"": random_effects[""individual""], ""fixed"": None, ""mask"": mask_i},\n        ""timestep"": {""random"": None, ""fixed"": None, ""mask"": mask_t},\n        ""observations"": {\n            ""step"": observations[..., 0],\n            ""angle"": observations[..., 1],\n            ""omega"": observations[..., 2],\n        },\n    }\n\n    return config\n'"
examples/rsa/generics.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nInterpreting generic statements with RSA models of pragmatics.\n\nTaken from:\n[0] http://forestdb.org/models/generics.html\n[1] https://gscontras.github.io/probLang/chapters/07-generics.html\n""""""\n\nimport torch\n\nimport argparse\nimport numbers\nimport collections\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\n\nfrom search_inference import HashingMarginal, memoize, Search\n\ntorch.set_default_dtype(torch.float64)  # double precision for numerical stability\n\n\ndef Marginal(fn):\n    return memoize(lambda *args: HashingMarginal(Search(fn).run(*args)))\n\n\n#######################\n# models\n#######################\n\n# hashable params\nParams = collections.namedtuple(""Params"", [""theta"", ""gamma"", ""delta""])\n\n\ndef discretize_beta_pdf(bins, gamma, delta):\n    """"""\n    discretized version of the Beta pdf used for approximately integrating via Search\n    """"""\n    shape_alpha = gamma * delta\n    shape_beta = (1.-gamma) * delta\n    return torch.tensor(\n        list(map(lambda x: (x ** (shape_alpha-1)) * ((1.-x)**(shape_beta-1)), bins)))\n\n\n@Marginal\ndef structured_prior_model(params):\n    propertyIsPresent = pyro.sample(""propertyIsPresent"",\n                                    dist.Bernoulli(params.theta)).item() == 1\n    if propertyIsPresent:\n        # approximately integrate over a beta by enumerating over bins\n        beta_bins = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n        ix = pyro.sample(""bin"", dist.Categorical(\n            probs=discretize_beta_pdf(beta_bins, params.gamma, params.delta)))\n        return beta_bins[ix]\n    else:\n        return 0\n\n\ndef threshold_prior():\n    threshold_bins = [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n    ix = pyro.sample(""threshold"", dist.Categorical(logits=torch.zeros(len(threshold_bins))))\n    return threshold_bins[ix]\n\n\ndef utterance_prior():\n    utterances = [""generic is true"", ""mu""]\n    ix = pyro.sample(""utterance"", dist.Categorical(logits=torch.zeros(len(utterances))))\n    return utterances[ix]\n\n\ndef meaning(utterance, state, threshold):\n    if isinstance(utterance, numbers.Number):\n        return state == utterance\n    if utterance == ""generic is true"":\n        return state > threshold\n    if utterance == ""generic is false"":\n        return state <= threshold\n    if utterance == ""mu"":\n        return True\n    if utterance == ""some"":\n        return state > 0\n    if utterance == ""most"":\n        return state >= 0.5\n    if utterance == ""all"":\n        return state >= 0.99\n    return True\n\n\n@Marginal\ndef listener0(utterance, threshold, prior):\n    state = pyro.sample(""state"", prior)\n    m = meaning(utterance, state, threshold)\n    pyro.factor(""listener0_true"", 0. if m else -99999.)\n    return state\n\n\n@Marginal\ndef speaker1(state, threshold, prior):\n    s1Optimality = 5.\n    utterance = utterance_prior()\n    L0 = listener0(utterance, threshold, prior)\n    with poutine.scale(scale=torch.tensor(s1Optimality)):\n        pyro.sample(""L0_score"", L0, obs=state)\n    return utterance\n\n\n@Marginal\ndef listener1(utterance, prior):\n    state = pyro.sample(""state"", prior)\n    threshold = threshold_prior()\n    S1 = speaker1(state, threshold, prior)\n    pyro.sample(""S1_score"", S1, obs=utterance)\n    return state\n\n\n@Marginal\ndef speaker2(prevalence, prior):\n    utterance = utterance_prior()\n    wL1 = listener1(utterance, prior)\n    pyro.sample(""wL1_score"", wL1, obs=prevalence)\n    return utterance\n\n\ndef main(args):\n    hasWingsERP = structured_prior_model(Params(theta=0.5, gamma=0.99, delta=10.))\n    laysEggsERP = structured_prior_model(Params(theta=0.5, gamma=0.5, delta=10.))\n    carriesMalariaERP = structured_prior_model(Params(theta=0.1, gamma=0.01, delta=2.))\n    areFemaleERP = structured_prior_model(Params(theta=0.99, gamma=0.5, delta=50.))\n\n    # listener interpretation of generics\n    wingsPosterior = listener1(""generic is true"", hasWingsERP)\n    malariaPosterior = listener1(""generic is true"", carriesMalariaERP)\n    eggsPosterior = listener1(""generic is true"", laysEggsERP)\n    femalePosterior = listener1(""generic is true"", areFemaleERP)\n    listeners = {""wings"": wingsPosterior, ""malaria"": malariaPosterior,\n                 ""eggs"": eggsPosterior, ""female"": femalePosterior}\n\n    for name, listener in listeners.items():\n        for elt in listener.enumerate_support():\n            print(name, elt, listener.log_prob(elt).exp().item())\n\n    # truth judgments\n    malariaSpeaker = speaker2(0.1, carriesMalariaERP)\n    eggSpeaker = speaker2(0.6, laysEggsERP)\n    femaleSpeaker = speaker2(0.5, areFemaleERP)\n    lionSpeaker = speaker2(0.01, laysEggsERP)\n    speakers = {""malaria"": malariaSpeaker, ""egg"": eggSpeaker,\n                ""female"": femaleSpeaker, ""lion"": lionSpeaker}\n\n    for name, speaker in speakers.items():\n        for elt in speaker.enumerate_support():\n            print(name, elt, speaker.log_prob(elt).exp().item())\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-samples\', default=10, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/rsa/hyperbole.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nInterpreting hyperbole with RSA models of pragmatics.\n\nTaken from: https://gscontras.github.io/probLang/chapters/03-nonliteral.html\n""""""\n\nimport torch\n\nimport collections\nimport argparse\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\n\nfrom search_inference import HashingMarginal, memoize, Search\n\ntorch.set_default_dtype(torch.float64)  # double precision for numerical stability\n\n\ndef Marginal(fn):\n    return memoize(lambda *args: HashingMarginal(Search(fn).run(*args)))\n\n\n######################################\n# models\n######################################\n\n# hashable state\nState = collections.namedtuple(""State"", [""price"", ""valence""])\n\n\ndef approx(x, b=None):\n    if b is None:\n        b = 10.\n    div = float(x)/b\n    rounded = int(div) + 1 if div - float(int(div)) >= 0.5 else int(div)\n    return int(b) * rounded\n\n\ndef price_prior():\n    values = [50, 51, 500, 501, 1000, 1001, 5000, 5001, 10000, 10001]\n    probs = torch.tensor([0.4205, 0.3865, 0.0533, 0.0538, 0.0223, 0.0211, 0.0112, 0.0111, 0.0083, 0.0120])\n    ix = pyro.sample(""price"", dist.Categorical(probs=probs))\n    return values[ix]\n\n\ndef valence_prior(price):\n    probs = {\n        50: 0.3173,\n        51: 0.3173,\n        500: 0.7920,\n        501: 0.7920,\n        1000: 0.8933,\n        1001: 0.8933,\n        5000: 0.9524,\n        5001: 0.9524,\n        10000: 0.9864,\n        10001: 0.9864\n    }\n    return pyro.sample(""valence"", dist.Bernoulli(probs=probs[price])).item() == 1\n\n\ndef meaning(utterance, price):\n    return utterance == price\n\n\nqud_fns = {\n    ""price"": lambda state: State(price=state.price, valence=None),\n    ""valence"": lambda state: State(price=None, valence=state.valence),\n    ""priceValence"": lambda state: State(price=state.price, valence=state.valence),\n    ""approxPrice"": lambda state: State(price=approx(state.price), valence=None),\n    ""approxPriceValence"": lambda state: State(price=approx(state.price), valence=state.valence),\n}\n\n\ndef qud_prior():\n    values = [""price"", ""valence"", ""priceValence"", ""approxPrice"", ""approxPriceValence""]\n    ix = pyro.sample(""qud"", dist.Categorical(probs=torch.ones(len(values)) / len(values)))\n    return values[ix]\n\n\ndef utterance_cost(numberUtt):\n    preciseNumberCost = 1.\n    return 0. if approx(numberUtt) == numberUtt else preciseNumberCost\n\n\ndef utterance_prior():\n    utterances = [50, 51, 500, 501, 1000, 1001, 5000, 5001, 10000, 10001]\n    utteranceLogits = -torch.tensor(list(map(utterance_cost, utterances)),\n                                    dtype=torch.float64)\n    ix = pyro.sample(""utterance"", dist.Categorical(logits=utteranceLogits))\n    return utterances[ix]\n\n\n@Marginal\ndef literal_listener(utterance, qud):\n    price = price_prior()\n    state = State(price=price, valence=valence_prior(price))\n    pyro.factor(""literal_meaning"", 0. if meaning(utterance, price) else -999999.)\n    return qud_fns[qud](state)\n\n\n@Marginal\ndef speaker(qudValue, qud):\n    alpha = 1.\n    utterance = utterance_prior()\n    literal_marginal = literal_listener(utterance, qud)\n    with poutine.scale(scale=torch.tensor(alpha)):\n        pyro.sample(""listener"", literal_marginal, obs=qudValue)\n    return utterance\n\n\n@Marginal\ndef pragmatic_listener(utterance):\n    # priors\n    price = price_prior()\n    valence = valence_prior(price)\n    qud = qud_prior()\n\n    # model\n    state = State(price=price, valence=valence)\n    qudValue = qud_fns[qud](state)\n    speaker_marginal = speaker(qudValue, qud)\n    pyro.sample(""speaker"", speaker_marginal, obs=utterance)\n    return state\n\n\ndef test_truth():\n    true_vals = {\n        ""probs"": torch.tensor([0.0018655171404222354,0.1512643329444101,0.0030440475496016296,0.23182161303428897,0.00003854830096338984,0.01502495595927897,0.00003889558295405101,0.015160315922876075,0.00016425635615857924,0.026788637869123822,0.00017359794987375924,0.028312162297699582,0.0008164336950199063,0.060558944822420434,0.0008088460212743665,0.05999612935009309,0.01925106279557206,0.17429720083660782,0.02094455861717477,0.18962994295418778]),  # noqa: E231,E501\n        ""support"": list(map(lambda d: State(**d), [{""price"":10001,""valence"":False},{""price"":10001,""valence"":True},{""price"":10000,""valence"":False},{""price"":10000,""valence"":True},{""price"":5001,""valence"":False},{""price"":5001,""valence"":True},{""price"":5000,""valence"":False},{""price"":5000,""valence"":True},{""price"":1001,""valence"":False},{""price"":1001,""valence"":True},{""price"":1000,""valence"":False},{""price"":1000,""valence"":True},{""price"":501,""valence"":False},{""price"":501,""valence"":True},{""price"":500,""valence"":False},{""price"":500,""valence"":True},{""price"":51,""valence"":False},{""price"":51,""valence"":True},{""price"":50,""valence"":False},{""price"":50,""valence"":True}]))  # noqa: E231,E501\n    }\n\n    pragmatic_marginal = pragmatic_listener(10000)\n    for i, elt in enumerate(true_vals[""support""]):\n        print(""{}: true prob {} pyro prob {}"".format(\n            elt, true_vals[""probs""][i].item(),\n            pragmatic_marginal.log_prob(elt).exp().item()))\n\n\ndef main(args):\n\n    # test_truth()\n\n    pragmatic_marginal = pragmatic_listener(args.price)\n\n    pd, pv = pragmatic_marginal._dist_and_values()\n    print([(s, pragmatic_marginal.log_prob(s).exp().item())\n           for s in pragmatic_marginal.enumerate_support()])\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-samples\', default=10, type=int)\n    parser.add_argument(\'--price\', default=10000, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/rsa/schelling.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nSchelling coordination game:\nTwo spies, Alice and Bob, want to meet.\n\nThey must choose between two locations without communicating\nby recursively reasoning about one another.\n\nTaken from: http://forestdb.org/models/schelling.html\n""""""\nimport argparse\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.distributions import Bernoulli\nfrom search_inference import HashingMarginal, Search\n\n\ndef location(preference):\n    """"""\n    Flips a weighted coin to decide between two locations to meet\n    In this example, we assume that Alice and Bob share a prior preference\n    for one location over another, reflected in the value of preference below.\n    """"""\n    return pyro.sample(""loc"", Bernoulli(preference))\n\n\ndef alice(preference, depth):\n    """"""\n    Alice decides where to go by reasoning about Bob\'s choice\n    """"""\n    alice_prior = location(preference)\n    with poutine.block():\n        bob_marginal = HashingMarginal(Search(bob).run(preference, depth - 1))\n    return pyro.sample(""bob_choice"", bob_marginal, obs=alice_prior)\n\n\ndef bob(preference, depth):\n    """"""\n    Bob decides where to go by reasoning about Alice\'s choice\n    """"""\n    bob_prior = location(preference)\n    if depth > 0:\n        with poutine.block():\n            alice_marginal = HashingMarginal(Search(alice).run(preference, depth))\n        return pyro.sample(""alice_choice"", alice_marginal, obs=bob_prior)\n    else:\n        return bob_prior\n\n\ndef main(args):\n    # Here Alice and Bob slightly prefer one location over the other a priori\n    shared_preference = torch.tensor([args.preference])\n\n    bob_depth = args.depth\n    num_samples = args.num_samples\n\n    # We sample Bob\'s choice of location by marginalizing\n    # over his decision process.\n    bob_decision = HashingMarginal(Search(bob).run(shared_preference, bob_depth))\n    bob_prob = bob_decision._dist_and_values()[0].probs\n    print(""bob prob"", bob_prob)\n\n    # draw num_samples samples from Bob\'s decision process\n    # and use those to estimate the marginal probability\n    # that Bob chooses their preferred location\n    bob_prob = sum([bob_decision()\n                    for i in range(num_samples)]) / float(num_samples)\n\n    print(""Empirical frequency of Bob choosing their favored location "" +\n          ""given preference {} and recursion depth {}: {}""\n          .format(shared_preference, bob_depth, bob_prob))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-samples\', default=10, type=int)\n    parser.add_argument(\'--depth\', default=2, type=int)\n    parser.add_argument(\'--preference\', default=0.6, type=float)\n    args = parser.parse_args()\n    main(args)\n'"
examples/rsa/schelling_false.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nSchelling coordination game with false belief:\nTwo spies, Alice and Bob, claim to want to meet.\nBob wants to meet Alice, but Alice actually wants to avoid Bob.\n\nThey must choose between two locations without communicating\nby recursively reasoning about one another.\n\nTaken from: http://forestdb.org/models/schelling-falsebelief.html\n""""""\nimport argparse\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.distributions import Bernoulli\nfrom search_inference import HashingMarginal, Search\n\n\ndef location(preference):\n    """"""\n    Flips a weighted coin to decide between two locations to meet.\n    In this example, we assume that Alice and Bob share a prior preference\n    for one location over another, reflected in the value of preference below.\n    """"""\n    return pyro.sample(""loc"", Bernoulli(preference))\n\n\ndef alice_fb(preference, depth):\n    """"""\n    Alice\'s actual decision process:\n    Alice decides where to go by reasoning about Bob\'s choice\n    and choosing the other location.\n    """"""\n    alice_prior = location(preference)\n    with poutine.block():\n        bob_marginal = HashingMarginal(Search(bob).run(preference, depth-1))\n    pyro.sample(""bob_choice"", bob_marginal, obs=alice_prior)\n    return 1 - alice_prior\n\n\ndef alice(preference, depth):\n    """"""\n    Alice decides where to go by reasoning about Bob\'s choice\n    """"""\n    alice_prior = location(preference)\n    with poutine.block():\n        bob_marginal = HashingMarginal(Search(bob).run(preference, depth - 1))\n    return pyro.sample(""bob_choice"", bob_marginal, obs=alice_prior)\n\n\ndef bob(preference, depth):\n    """"""\n    Bob decides where to go by reasoning about Alice\'s choice\n    """"""\n    bob_prior = location(preference)\n    if depth > 0:\n        with poutine.block():\n            alice_marginal = HashingMarginal(Search(alice).run(preference, depth))\n        return pyro.sample(""alice_choice"", alice_marginal, obs=bob_prior)\n    else:\n        return bob_prior\n\n\ndef main(args):\n\n    # Here Alice and Bob slightly prefer one location over the other a priori\n    shared_preference = torch.tensor([args.preference])\n\n    alice_depth = args.depth\n    num_samples = args.num_samples\n\n    # We sample Alice\'s true choice of location\n    # by marginalizing over her decision process\n    alice_decision = HashingMarginal(Search(alice_fb).run(shared_preference, alice_depth))\n\n    # draw num_samples samples from Alice\'s decision process\n    # and use those to estimate the marginal probability\n    # that Alice chooses their preferred location\n    alice_prob = sum([alice_decision()\n                      for i in range(num_samples)]) / float(num_samples)\n\n    print(""Empirical frequency of Alice choosing their favored location "" +\n          ""given preference {} and recursion depth {}: {}""\n          .format(shared_preference, alice_depth, alice_prob))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-samples\', default=10, type=int)\n    parser.add_argument(\'--depth\', default=3, type=int)\n    parser.add_argument(\'--preference\', default=0.55, type=float)\n    args = parser.parse_args()\n    main(args)\n'"
examples/rsa/search_inference.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nInference algorithms and utilities used in the RSA example models.\n\nAdapted from: http://dippl.org/chapters/03-enumeration.html\n""""""\n\nimport collections\n\nimport torch\nimport queue\nimport functools\n\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer.abstract_infer import TracePosterior\nfrom pyro.poutine.runtime import NonlocalExit\n\n\ndef memoize(fn=None, **kwargs):\n    if fn is None:\n        return lambda _fn: memoize(_fn, **kwargs)\n    return functools.lru_cache(**kwargs)(fn)\n\n\nclass HashingMarginal(dist.Distribution):\n    """"""\n    :param trace_dist: a TracePosterior instance representing a Monte Carlo posterior\n\n    Marginal histogram distribution.\n    Turns a TracePosterior object into a Distribution\n    over the return values of the TracePosterior\'s model.\n    """"""\n    def __init__(self, trace_dist, sites=None):\n        assert isinstance(trace_dist, TracePosterior), \\\n            ""trace_dist must be trace posterior distribution object""\n\n        if sites is None:\n            sites = ""_RETURN""\n\n        assert isinstance(sites, (str, list)), \\\n            ""sites must be either \'_RETURN\' or list""\n\n        self.sites = sites\n        super().__init__()\n        self.trace_dist = trace_dist\n\n    has_enumerate_support = True\n\n    @memoize(maxsize=10)\n    def _dist_and_values(self):\n        # XXX currently this whole object is very inefficient\n        values_map, logits = collections.OrderedDict(), collections.OrderedDict()\n        for tr, logit in zip(self.trace_dist.exec_traces,\n                             self.trace_dist.log_weights):\n            if isinstance(self.sites, str):\n                value = tr.nodes[self.sites][""value""]\n            else:\n                value = {site: tr.nodes[site][""value""] for site in self.sites}\n            if not torch.is_tensor(logit):\n                logit = torch.tensor(logit)\n\n            if torch.is_tensor(value):\n                value_hash = hash(value.cpu().contiguous().numpy().tobytes())\n            elif isinstance(value, dict):\n                value_hash = hash(self._dict_to_tuple(value))\n            else:\n                value_hash = hash(value)\n            if value_hash in logits:\n                # Value has already been seen.\n                logits[value_hash] = dist.util.logsumexp(torch.stack([logits[value_hash], logit]), dim=-1)\n            else:\n                logits[value_hash] = logit\n                values_map[value_hash] = value\n\n        logits = torch.stack(list(logits.values())).contiguous().view(-1)\n        logits = logits - dist.util.logsumexp(logits, dim=-1)\n        d = dist.Categorical(logits=logits)\n        return d, values_map\n\n    def sample(self):\n        d, values_map = self._dist_and_values()\n        ix = d.sample()\n        return list(values_map.values())[ix]\n\n    def log_prob(self, val):\n        d, values_map = self._dist_and_values()\n        if torch.is_tensor(val):\n            value_hash = hash(val.cpu().contiguous().numpy().tobytes())\n        elif isinstance(val, dict):\n            value_hash = hash(self._dict_to_tuple(val))\n        else:\n            value_hash = hash(val)\n        return d.log_prob(torch.tensor([list(values_map.keys()).index(value_hash)]))\n\n    def enumerate_support(self):\n        d, values_map = self._dist_and_values()\n        return list(values_map.values())[:]\n\n    def _dict_to_tuple(self, d):\n        """"""\n        Recursively converts a dictionary to a list of key-value tuples\n        Only intended for use as a helper function inside HashingMarginal!!\n        May break when keys cant be sorted, but that is not an expected use-case\n        """"""\n        if isinstance(d, dict):\n            return tuple([(k, self._dict_to_tuple(d[k])) for k in sorted(d.keys())])\n        else:\n            return d\n\n    def _weighted_mean(self, value, dim=0):\n        weights = self._log_weights.reshape([-1] + (value.dim() - 1) * [1])\n        max_weight = weights.max(dim=dim)[0]\n        relative_probs = (weights - max_weight).exp()\n        return (value * relative_probs).sum(dim=dim) / relative_probs.sum(dim=dim)\n\n    @property\n    def mean(self):\n        samples = torch.stack(list(self._dist_and_values()[1].values()))\n        return self._weighted_mean(samples)\n\n    @property\n    def variance(self):\n        samples = torch.stack(list(self._dist_and_values()[1].values()))\n        deviation_squared = torch.pow(samples - self.mean, 2)\n        return self._weighted_mean(deviation_squared)\n\n\n########################\n# Exact Search inference\n########################\n\nclass Search(TracePosterior):\n    """"""\n    Exact inference by enumerating over all possible executions\n    """"""\n    def __init__(self, model, max_tries=int(1e6), **kwargs):\n        self.model = model\n        self.max_tries = max_tries\n        super().__init__(**kwargs)\n\n    def _traces(self, *args, **kwargs):\n        q = queue.Queue()\n        q.put(poutine.Trace())\n        p = poutine.trace(\n            poutine.queue(self.model, queue=q, max_tries=self.max_tries))\n        while not q.empty():\n            tr = p.get_trace(*args, **kwargs)\n            yield tr, tr.log_prob_sum()\n\n\n###############################################\n# Best-first Search Inference\n###############################################\n\n\ndef pqueue(fn, queue):\n\n    def sample_escape(tr, site):\n        return (site[""name""] not in tr) and \\\n            (site[""type""] == ""sample"") and \\\n            (not site[""is_observed""])\n\n    def _fn(*args, **kwargs):\n\n        for i in range(int(1e6)):\n            assert not queue.empty(), \\\n                ""trying to get() from an empty queue will deadlock""\n\n            priority, next_trace = queue.get()\n            try:\n                ftr = poutine.trace(poutine.escape(poutine.replay(fn, next_trace),\n                                                   functools.partial(sample_escape,\n                                                                     next_trace)))\n                return ftr(*args, **kwargs)\n            except NonlocalExit as site_container:\n                site_container.reset_stack()\n                for tr in poutine.util.enum_extend(ftr.trace.copy(),\n                                                   site_container.site):\n                    # add a little bit of noise to the priority to break ties...\n                    queue.put((tr.log_prob_sum().item() - torch.rand(1).item() * 1e-2, tr))\n\n        raise ValueError(""max tries ({}) exceeded"".format(str(1e6)))\n\n    return _fn\n\n\nclass BestFirstSearch(TracePosterior):\n    """"""\n    Inference by enumerating executions ordered by their probabilities.\n    Exact (and results equivalent to Search) if all executions are enumerated.\n    """"""\n    def __init__(self, model, num_samples=None, **kwargs):\n        if num_samples is None:\n            num_samples = 100\n        self.num_samples = num_samples\n        self.model = model\n        super().__init__(**kwargs)\n\n    def _traces(self, *args, **kwargs):\n        q = queue.PriorityQueue()\n        # add a little bit of noise to the priority to break ties...\n        q.put((torch.zeros(1).item() - torch.rand(1).item() * 1e-2, poutine.Trace()))\n        q_fn = pqueue(self.model, queue=q)\n        for i in range(self.num_samples):\n            if q.empty():\n                # num_samples was too large!\n                break\n            tr = poutine.trace(q_fn).get_trace(*args, **kwargs)  # XXX should block\n            yield tr, tr.log_prob_sum()\n'"
examples/rsa/semantic_parsing.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nCombining models of RSA pragmatics and CCG-based compositional semantics.\n\nTaken from: http://dippl.org/examples/zSemanticPragmaticMashup.html\n""""""\n\nimport torch\n\nimport argparse\nimport collections\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom search_inference import HashingMarginal, BestFirstSearch, memoize\n\ntorch.set_default_dtype(torch.float64)\n\n\ndef Marginal(fn=None, **kwargs):\n    if fn is None:\n        return lambda _fn: Marginal(_fn, **kwargs)\n    return memoize(lambda *args: HashingMarginal(BestFirstSearch(fn, **kwargs).run(*args)))\n\n\n###################################################################\n# Lexical semantics\n###################################################################\n\ndef flip(name, p):\n    return pyro.sample(name, dist.Bernoulli(p)).item() == 1\n\n\n# hashable state\nobj = collections.namedtuple(""Obj"", [""name"", ""blond"", ""nice"", ""tall""])\n\n\ndef Obj(name):\n    return obj(name=name,\n               blond=flip(name + ""_blond"", 0.5),\n               nice=flip(name + ""_nice"", 0.5),\n               tall=flip(name + ""_tall"", 0.5))\n\n\nclass Meaning:\n    def sem(self, world):\n        raise NotImplementedError\n\n    __call__ = sem\n\n    def syn(self):\n        raise NotImplementedError\n\n\nclass UndefinedMeaning(Meaning):\n    def sem(self, world):\n        return None\n\n    def syn(self):\n        return """"\n\n\nclass BlondMeaning(Meaning):\n    def sem(self, world):\n        return lambda obj: obj.blond\n\n    def syn(self):\n        return {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""}\n\n\nclass NiceMeaning(Meaning):\n    def sem(self, world):\n        return lambda obj: obj.nice\n\n    def syn(self):\n        return {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""}\n\n\nclass TallMeaning(Meaning):\n    def sem(self, world):\n        return lambda obj: obj.tall\n\n    def syn(self):\n        return {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""}\n\n\nclass BobMeaning(Meaning):\n    def sem(self, world):\n        return list(filter(lambda obj: obj.name == ""Bob"", world))[0]\n\n    def syn(self):\n        return ""NP""\n\n\nclass SomeMeaning(Meaning):\n    def sem(self, world):\n        def f1(P):\n            def f2(Q):\n                return len(list(filter(Q, filter(P, world)))) > 0\n            return f2\n\n        return f1\n\n    def syn(self):\n        return {\n            ""dir"": ""R"",\n            ""int"": {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""},\n            ""out"": {\n                ""dir"": ""R"",\n                ""int"": {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""},\n                ""out"": ""S""\n            }\n        }\n\n\nclass AllMeaning(Meaning):\n    def sem(self, world):\n        def f1(P):\n            def f2(Q):\n                return len(list(filter(lambda *args: not Q(*args),\n                                       filter(P, world)))) == 0\n            return f2\n\n        return f1\n\n    def syn(self):\n        return {\n            ""dir"": ""R"",\n            ""int"": {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""},\n            ""out"": {\n                ""dir"": ""R"",\n                ""int"": {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""},\n                ""out"": ""S""\n            }\n        }\n\n\nclass NoneMeaning(Meaning):\n    def sem(self, world):\n        def f1(P):\n            def f2(Q):\n                return len(list(filter(Q, filter(P, world)))) == 0\n            return f2\n\n        return f1\n\n    def syn(self):\n        return {\n            ""dir"": ""R"",\n            ""int"": {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""},\n            ""out"": {\n                ""dir"": ""R"",\n                ""int"": {""dir"": ""L"", ""int"": ""NP"", ""out"": ""S""},\n                ""out"": ""S""\n            }\n        }\n\n\nclass CompoundMeaning(Meaning):\n    def __init__(self, sem, syn):\n        self._sem = sem\n        self._syn = syn\n\n    def sem(self, world):\n        return self._sem(world)\n\n    def syn(self):\n        return self._syn\n\n\n###################################################################\n# Compositional semantics\n###################################################################\n\ndef heuristic(is_good):\n    if is_good:\n        return torch.tensor(0.)\n    return torch.tensor(-100.0)\n\n\ndef world_prior(num_objs, meaning_fn):\n    prev_factor = torch.tensor(0.)\n    world = []\n    for i in range(num_objs):\n        world.append(Obj(""obj_{}"".format(i)))\n        new_factor = heuristic(meaning_fn(world))\n        pyro.factor(""factor_{}"".format(i), new_factor - prev_factor)\n        prev_factor = new_factor\n\n    pyro.factor(""factor_{}"".format(num_objs), prev_factor * -1)\n    return tuple(world)\n\n\ndef lexical_meaning(word):\n    meanings = {\n        ""blond"": BlondMeaning,\n        ""nice"": NiceMeaning,\n        ""Bob"": BobMeaning,\n        ""some"": SomeMeaning,\n        ""none"": NoneMeaning,\n        ""all"": AllMeaning\n    }\n    if word in meanings:\n        return meanings[word]()\n    else:\n        return UndefinedMeaning()\n\n\ndef apply_world_passing(f, a):\n    return lambda w: f(w)(a(w))\n\n\ndef syntax_match(s, t):\n    if ""dir"" in s and ""dir"" in t:\n        return (s[""dir""] and t[""dir""]) and \\\n            syntax_match(s[""int""], t[""int""]) and \\\n            syntax_match(s[""out""], t[""out""])\n    else:\n        return s == t\n\n\ndef can_apply(meanings):\n    inds = []\n    for i, meaning in enumerate(meanings):\n        applies = False\n        s = meaning.syn()\n        if ""dir"" in s:\n            if s[""dir""] == ""L"":\n                applies = syntax_match(s[""int""], meanings[i-1].syn())\n            elif s[""dir""] == ""R"":\n                applies = syntax_match(s[""int""], meanings[i+1].syn())\n            else:\n                applies = False\n\n        if applies:\n            inds.append(i)\n\n    return inds\n\n\ndef combine_meaning(meanings, c):\n    possible_combos = can_apply(meanings)\n    N = len(possible_combos)\n    ix = pyro.sample(""ix_{}"".format(c),\n                     dist.Categorical(torch.ones(N) / N))\n    i = possible_combos[ix]\n    s = meanings[i].syn()\n    if s[""dir""] == ""L"":\n        f = meanings[i].sem\n        a = meanings[i-1].sem\n        new_meaning = CompoundMeaning(sem=apply_world_passing(f, a),\n                                      syn=s[""out""])\n        return meanings[0:i-1] + [new_meaning] + meanings[i+1:]\n    if s[""dir""] == ""R"":\n        f = meanings[i].sem\n        a = meanings[i+1].sem\n        new_meaning = CompoundMeaning(sem=apply_world_passing(f, a),\n                                      syn=s[""out""])\n        return meanings[0:i] + [new_meaning] + meanings[i+2:]\n\n\ndef combine_meanings(meanings, c=0):\n    if len(meanings) == 1:\n        return meanings[0].sem\n    else:\n        return combine_meanings(combine_meaning(meanings, c), c=c+1)\n\n\ndef meaning(utterance):\n    defined = filter(lambda w: """" != w.syn(),\n                     list(map(lexical_meaning, utterance.split("" ""))))\n    return combine_meanings(list(defined))\n\n\n@Marginal(num_samples=100)\ndef literal_listener(utterance):\n    m = meaning(utterance)\n    world = world_prior(2, m)\n    pyro.factor(""world_constraint"", heuristic(m(world)) * 1000)\n    return world\n\n\ndef utterance_prior():\n    utterances = [""some of the blond people are nice"",\n                  ""all of the blond people are nice"",\n                  ""none of the blond people are nice""]\n    ix = pyro.sample(""utterance"", dist.Categorical(torch.ones(3) / 3.0))\n    return utterances[ix]\n\n\n@Marginal(num_samples=100)\ndef speaker(world):\n    utterance = utterance_prior()\n    L = literal_listener(utterance)\n    pyro.sample(""speaker_constraint"", L, obs=world)\n    return utterance\n\n\ndef rsa_listener(utterance, qud):\n    world = world_prior(2, meaning(utterance))\n    S = speaker(world)\n    pyro.sample(""listener_constraint"", S, obs=utterance)\n    return qud(world)\n\n\ndef literal_listener_raw(utterance, qud):\n    m = meaning(utterance)\n    world = world_prior(3, m)\n    pyro.factor(""world_constraint"", heuristic(m(world)) * 1000)\n    return qud(world)\n\n\ndef main(args):\n\n    mll = Marginal(literal_listener_raw, num_samples=args.num_samples)\n\n    def is_any_qud(world):\n        return any(map(lambda obj: obj.nice, world))\n\n    print(mll(""all blond people are nice"", is_any_qud)())\n\n    def is_all_qud(world):\n        m = True\n        for obj in world:\n            if obj.blond:\n                if obj.nice:\n                    m = m and True\n                else:\n                    m = m and False\n            else:\n                m = m and True\n        return m\n\n    rsa = Marginal(rsa_listener, num_samples=args.num_samples)\n\n    print(rsa(""some of the blond people are nice"", is_all_qud)())\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-samples\', default=10, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/vae/ss_vae_M2.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport torch\nimport torch.nn as nn\nfrom visdom import Visdom\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.examples.util import print_and_log\nfrom pyro.infer import SVI, JitTrace_ELBO, JitTraceEnum_ELBO, Trace_ELBO, TraceEnum_ELBO, config_enumerate\nfrom pyro.optim import Adam\nfrom utils.custom_mlp import MLP, Exp\nfrom utils.mnist_cached import MNISTCached, mkdir_p, setup_data_loaders\nfrom utils.vae_plots import mnist_test_tsne_ssvae, plot_conditional_samples_ssvae\n\n\nclass SSVAE(nn.Module):\n    """"""\n    This class encapsulates the parameters (neural networks) and models & guides needed to train a\n    semi-supervised variational auto-encoder on the MNIST image dataset\n\n    :param output_size: size of the tensor representing the class label (10 for MNIST since\n                        we represent the class labels as a one-hot vector with 10 components)\n    :param input_size: size of the tensor representing the image (28*28 = 784 for our MNIST dataset\n                       since we flatten the images and scale the pixels to be in [0,1])\n    :param z_dim: size of the tensor representing the latent random variable z\n                  (handwriting style for our MNIST dataset)\n    :param hidden_layers: a tuple (or list) of MLP layers to be used in the neural networks\n                          representing the parameters of the distributions in our model\n    :param use_cuda: use GPUs for faster training\n    :param aux_loss_multiplier: the multiplier to use with the auxiliary loss\n    """"""\n    def __init__(self, output_size=10, input_size=784, z_dim=50, hidden_layers=(500,),\n                 config_enum=None, use_cuda=False, aux_loss_multiplier=None):\n\n        super().__init__()\n\n        # initialize the class with all arguments provided to the constructor\n        self.output_size = output_size\n        self.input_size = input_size\n        self.z_dim = z_dim\n        self.hidden_layers = hidden_layers\n        self.allow_broadcast = config_enum == \'parallel\'\n        self.use_cuda = use_cuda\n        self.aux_loss_multiplier = aux_loss_multiplier\n\n        # define and instantiate the neural networks representing\n        # the paramters of various distributions in the model\n        self.setup_networks()\n\n    def setup_networks(self):\n\n        z_dim = self.z_dim\n        hidden_sizes = self.hidden_layers\n\n        # define the neural networks used later in the model and the guide.\n        # these networks are MLPs (multi-layered perceptrons or simple feed-forward networks)\n        # where the provided activation parameter is used on every linear layer except\n        # for the output layer where we use the provided output_activation parameter\n        self.encoder_y = MLP([self.input_size] + hidden_sizes + [self.output_size],\n                             activation=nn.Softplus,\n                             output_activation=nn.Softmax,\n                             allow_broadcast=self.allow_broadcast,\n                             use_cuda=self.use_cuda)\n\n        # a split in the final layer\'s size is used for multiple outputs\n        # and potentially applying separate activation functions on them\n        # e.g. in this network the final output is of size [z_dim,z_dim]\n        # to produce loc and scale, and apply different activations [None,Exp] on them\n        self.encoder_z = MLP([self.input_size + self.output_size] +\n                             hidden_sizes + [[z_dim, z_dim]],\n                             activation=nn.Softplus,\n                             output_activation=[None, Exp],\n                             allow_broadcast=self.allow_broadcast,\n                             use_cuda=self.use_cuda)\n\n        self.decoder = MLP([z_dim + self.output_size] +\n                           hidden_sizes + [self.input_size],\n                           activation=nn.Softplus,\n                           output_activation=nn.Sigmoid,\n                           allow_broadcast=self.allow_broadcast,\n                           use_cuda=self.use_cuda)\n\n        # using GPUs for faster training of the networks\n        if self.use_cuda:\n            self.cuda()\n\n    def model(self, xs, ys=None):\n        """"""\n        The model corresponds to the following generative process:\n        p(z) = normal(0,I)              # handwriting style (latent)\n        p(y|x) = categorical(I/10.)     # which digit (semi-supervised)\n        p(x|y,z) = bernoulli(loc(y,z))   # an image\n        loc is given by a neural network  `decoder`\n\n        :param xs: a batch of scaled vectors of pixels from an image\n        :param ys: (optional) a batch of the class labels i.e.\n                   the digit corresponding to the image(s)\n        :return: None\n        """"""\n        # register this pytorch module and all of its sub-modules with pyro\n        pyro.module(""ss_vae"", self)\n\n        batch_size = xs.size(0)\n        options = dict(dtype=xs.dtype, device=xs.device)\n        with pyro.plate(""data""):\n\n            # sample the handwriting style from the constant prior distribution\n            prior_loc = torch.zeros(batch_size, self.z_dim, **options)\n            prior_scale = torch.ones(batch_size, self.z_dim, **options)\n            zs = pyro.sample(""z"", dist.Normal(prior_loc, prior_scale).to_event(1))\n\n            # if the label y (which digit to write) is supervised, sample from the\n            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n            alpha_prior = torch.ones(batch_size, self.output_size, **options) / (1.0 * self.output_size)\n            ys = pyro.sample(""y"", dist.OneHotCategorical(alpha_prior), obs=ys)\n\n            # finally, score the image (x) using the handwriting style (z) and\n            # the class label y (which digit to write) against the\n            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n            # where `decoder` is a neural network\n            loc = self.decoder.forward([zs, ys])\n            pyro.sample(""x"", dist.Bernoulli(loc).to_event(1), obs=xs)\n            # return the loc so we can visualize it later\n            return loc\n\n    def guide(self, xs, ys=None):\n        """"""\n        The guide corresponds to the following:\n        q(y|x) = categorical(alpha(x))              # infer digit from an image\n        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer handwriting style from an image and the digit\n        loc, scale are given by a neural network `encoder_z`\n        alpha is given by a neural network `encoder_y`\n\n        :param xs: a batch of scaled vectors of pixels from an image\n        :param ys: (optional) a batch of the class labels i.e.\n                   the digit corresponding to the image(s)\n        :return: None\n        """"""\n        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n        with pyro.plate(""data""):\n\n            # if the class label (the digit) is not supervised, sample\n            # (and score) the digit with the variational distribution\n            # q(y|x) = categorical(alpha(x))\n            if ys is None:\n                alpha = self.encoder_y.forward(xs)\n                ys = pyro.sample(""y"", dist.OneHotCategorical(alpha))\n\n            # sample (and score) the latent handwriting-style with the variational\n            # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n            loc, scale = self.encoder_z.forward([xs, ys])\n            pyro.sample(""z"", dist.Normal(loc, scale).to_event(1))\n\n    def classifier(self, xs):\n        """"""\n        classify an image (or a batch of images)\n\n        :param xs: a batch of scaled vectors of pixels from an image\n        :return: a batch of the corresponding class labels (as one-hots)\n        """"""\n        # use the trained model q(y|x) = categorical(alpha(x))\n        # compute all class probabilities for the image(s)\n        alpha = self.encoder_y.forward(xs)\n\n        # get the index (digit) that corresponds to\n        # the maximum predicted class probability\n        res, ind = torch.topk(alpha, 1)\n\n        # convert the digit(s) to one-hot tensor(s)\n        ys = torch.zeros_like(alpha).scatter_(1, ind, 1.0)\n        return ys\n\n    def model_classify(self, xs, ys=None):\n        """"""\n        this model is used to add an auxiliary (supervised) loss as described in the\n        Kingma et al., ""Semi-Supervised Learning with Deep Generative Models"".\n        """"""\n        # register all pytorch (sub)modules with pyro\n        pyro.module(""ss_vae"", self)\n\n        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n        with pyro.plate(""data""):\n            # this here is the extra term to yield an auxiliary loss that we do gradient descent on\n            if ys is not None:\n                alpha = self.encoder_y.forward(xs)\n                with pyro.poutine.scale(scale=self.aux_loss_multiplier):\n                    pyro.sample(""y_aux"", dist.OneHotCategorical(alpha), obs=ys)\n\n    def guide_classify(self, xs, ys=None):\n        """"""\n        dummy guide function to accompany model_classify in inference\n        """"""\n        pass\n\n\ndef run_inference_for_epoch(data_loaders, losses, periodic_interval_batches):\n    """"""\n    runs the inference algorithm for an epoch\n    returns the values of all losses separately on supervised and unsupervised parts\n    """"""\n    num_losses = len(losses)\n\n    # compute number of batches for an epoch\n    sup_batches = len(data_loaders[""sup""])\n    unsup_batches = len(data_loaders[""unsup""])\n    batches_per_epoch = sup_batches + unsup_batches\n\n    # initialize variables to store loss values\n    epoch_losses_sup = [0.] * num_losses\n    epoch_losses_unsup = [0.] * num_losses\n\n    # setup the iterators for training data loaders\n    sup_iter = iter(data_loaders[""sup""])\n    unsup_iter = iter(data_loaders[""unsup""])\n\n    # count the number of supervised batches seen in this epoch\n    ctr_sup = 0\n    for i in range(batches_per_epoch):\n\n        # whether this batch is supervised or not\n        is_supervised = (i % periodic_interval_batches == 1) and ctr_sup < sup_batches\n\n        # extract the corresponding batch\n        if is_supervised:\n            (xs, ys) = next(sup_iter)\n            ctr_sup += 1\n        else:\n            (xs, ys) = next(unsup_iter)\n\n        # run the inference for each loss with supervised or un-supervised\n        # data as arguments\n        for loss_id in range(num_losses):\n            if is_supervised:\n                new_loss = losses[loss_id].step(xs, ys)\n                epoch_losses_sup[loss_id] += new_loss\n            else:\n                new_loss = losses[loss_id].step(xs)\n                epoch_losses_unsup[loss_id] += new_loss\n\n    # return the values of all losses\n    return epoch_losses_sup, epoch_losses_unsup\n\n\ndef get_accuracy(data_loader, classifier_fn, batch_size):\n    """"""\n    compute the accuracy over the supervised training set or the testing set\n    """"""\n    predictions, actuals = [], []\n\n    # use the appropriate data loader\n    for (xs, ys) in data_loader:\n        # use classification function to compute all predictions for each batch\n        predictions.append(classifier_fn(xs))\n        actuals.append(ys)\n\n    # compute the number of accurate predictions\n    accurate_preds = 0\n    for pred, act in zip(predictions, actuals):\n        for i in range(pred.size(0)):\n            v = torch.sum(pred[i] == act[i])\n            accurate_preds += (v.item() == 10)\n\n    # calculate the accuracy between 0 and 1\n    accuracy = (accurate_preds * 1.0) / (len(predictions) * batch_size)\n    return accuracy\n\n\ndef visualize(ss_vae, viz, test_loader):\n    if viz:\n        plot_conditional_samples_ssvae(ss_vae, viz)\n        mnist_test_tsne_ssvae(ssvae=ss_vae, test_loader=test_loader)\n\n\ndef main(args):\n    """"""\n    run inference for SS-VAE\n    :param args: arguments for SS-VAE\n    :return: None\n    """"""\n    if args.seed is not None:\n        pyro.set_rng_seed(args.seed)\n\n    viz = None\n    if args.visualize:\n        viz = Visdom()\n        mkdir_p(""./vae_results"")\n\n    # batch_size: number of images (and labels) to be considered in a batch\n    ss_vae = SSVAE(z_dim=args.z_dim,\n                   hidden_layers=args.hidden_layers,\n                   use_cuda=args.cuda,\n                   config_enum=args.enum_discrete,\n                   aux_loss_multiplier=args.aux_loss_multiplier)\n\n    # setup the optimizer\n    adam_params = {""lr"": args.learning_rate, ""betas"": (args.beta_1, 0.999)}\n    optimizer = Adam(adam_params)\n\n    # set up the loss(es) for inference. wrapping the guide in config_enumerate builds the loss as a sum\n    # by enumerating each class label for the sampled discrete categorical distribution in the model\n    guide = config_enumerate(ss_vae.guide, args.enum_discrete, expand=True)\n    elbo = (JitTraceEnum_ELBO if args.jit else TraceEnum_ELBO)(max_plate_nesting=1)\n    loss_basic = SVI(ss_vae.model, guide, optimizer, loss=elbo)\n\n    # build a list of all losses considered\n    losses = [loss_basic]\n\n    # aux_loss: whether to use the auxiliary loss from NIPS 14 paper (Kingma et al)\n    if args.aux_loss:\n        elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()\n        loss_aux = SVI(ss_vae.model_classify, ss_vae.guide_classify, optimizer, loss=elbo)\n        losses.append(loss_aux)\n\n    try:\n        # setup the logger if a filename is provided\n        logger = open(args.logfile, ""w"") if args.logfile else None\n\n        data_loaders = setup_data_loaders(MNISTCached, args.cuda, args.batch_size, sup_num=args.sup_num)\n\n        # how often would a supervised batch be encountered during inference\n        # e.g. if sup_num is 3000, we would have every 16th = int(50000/3000) batch supervised\n        # until we have traversed through the all supervised batches\n        periodic_interval_batches = int(MNISTCached.train_data_size / (1.0 * args.sup_num))\n\n        # number of unsupervised examples\n        unsup_num = MNISTCached.train_data_size - args.sup_num\n\n        # initializing local variables to maintain the best validation accuracy\n        # seen across epochs over the supervised training set\n        # and the corresponding testing set and the state of the networks\n        best_valid_acc, corresponding_test_acc = 0.0, 0.0\n\n        # run inference for a certain number of epochs\n        for i in range(0, args.num_epochs):\n\n            # get the losses for an epoch\n            epoch_losses_sup, epoch_losses_unsup = \\\n                run_inference_for_epoch(data_loaders, losses, periodic_interval_batches)\n\n            # compute average epoch losses i.e. losses per example\n            avg_epoch_losses_sup = map(lambda v: v / args.sup_num, epoch_losses_sup)\n            avg_epoch_losses_unsup = map(lambda v: v / unsup_num, epoch_losses_unsup)\n\n            # store the loss and validation/testing accuracies in the logfile\n            str_loss_sup = "" "".join(map(str, avg_epoch_losses_sup))\n            str_loss_unsup = "" "".join(map(str, avg_epoch_losses_unsup))\n\n            str_print = ""{} epoch: avg losses {}"".format(i, ""{} {}"".format(str_loss_sup, str_loss_unsup))\n\n            validation_accuracy = get_accuracy(data_loaders[""valid""], ss_vae.classifier, args.batch_size)\n            str_print += "" validation accuracy {}"".format(validation_accuracy)\n\n            # this test accuracy is only for logging, this is not used\n            # to make any decisions during training\n            test_accuracy = get_accuracy(data_loaders[""test""], ss_vae.classifier, args.batch_size)\n            str_print += "" test accuracy {}"".format(test_accuracy)\n\n            # update the best validation accuracy and the corresponding\n            # testing accuracy and the state of the parent module (including the networks)\n            if best_valid_acc < validation_accuracy:\n                best_valid_acc = validation_accuracy\n                corresponding_test_acc = test_accuracy\n\n            print_and_log(logger, str_print)\n\n        final_test_accuracy = get_accuracy(data_loaders[""test""], ss_vae.classifier, args.batch_size)\n        print_and_log(logger, ""best validation accuracy {} corresponding testing accuracy {} ""\n                      ""last testing accuracy {}"".format(best_valid_acc, corresponding_test_acc, final_test_accuracy))\n\n        # visualize the conditional samples\n        visualize(ss_vae, viz, data_loaders[""test""])\n    finally:\n        # close the logger file object if we opened it earlier\n        if args.logfile:\n            logger.close()\n\n\nEXAMPLE_RUN = ""example run: python ss_vae_M2.py --seed 0 --cuda -n 2 --aux-loss -alm 46 -enum parallel "" \\\n              ""-sup 3000 -zd 50 -hl 500 -lr 0.00042 -b1 0.95 -bs 200 -log ./tmp.log""\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n\n    parser = argparse.ArgumentParser(description=""SS-VAE\\n{}"".format(EXAMPLE_RUN))\n\n    parser.add_argument(\'--cuda\', action=\'store_true\',\n                        help=""use GPU(s) to speed up training"")\n    parser.add_argument(\'--jit\', action=\'store_true\',\n                        help=""use PyTorch jit to speed up training"")\n    parser.add_argument(\'-n\', \'--num-epochs\', default=50, type=int,\n                        help=""number of epochs to run"")\n    parser.add_argument(\'--aux-loss\', action=""store_true"",\n                        help=""whether to use the auxiliary loss from NIPS 14 paper ""\n                             ""(Kingma et al). It is not used by default "")\n    parser.add_argument(\'-alm\', \'--aux-loss-multiplier\', default=46, type=float,\n                        help=""the multiplier to use with the auxiliary loss"")\n    parser.add_argument(\'-enum\', \'--enum-discrete\', default=""parallel"",\n                        help=""parallel, sequential or none. uses parallel enumeration by default"")\n    parser.add_argument(\'-sup\', \'--sup-num\', default=3000,\n                        type=float, help=""supervised amount of the data i.e. ""\n                                         ""how many of the images have supervised labels"")\n    parser.add_argument(\'-zd\', \'--z-dim\', default=50, type=int,\n                        help=""size of the tensor representing the latent variable z ""\n                             ""variable (handwriting style for our MNIST dataset)"")\n    parser.add_argument(\'-hl\', \'--hidden-layers\', nargs=\'+\', default=[500], type=int,\n                        help=""a tuple (or list) of MLP layers to be used in the neural networks ""\n                             ""representing the parameters of the distributions in our model"")\n    parser.add_argument(\'-lr\', \'--learning-rate\', default=0.00042, type=float,\n                        help=""learning rate for Adam optimizer"")\n    parser.add_argument(\'-b1\', \'--beta-1\', default=0.9, type=float,\n                        help=""beta-1 parameter for Adam optimizer"")\n    parser.add_argument(\'-bs\', \'--batch-size\', default=200, type=int,\n                        help=""number of images (and labels) to be considered in a batch"")\n    parser.add_argument(\'-log\', \'--logfile\', default=""./tmp.log"", type=str,\n                        help=""filename for logging the outputs"")\n    parser.add_argument(\'--seed\', default=None, type=int,\n                        help=""seed for controlling randomness in this example"")\n    parser.add_argument(\'--visualize\', action=""store_true"",\n                        help=""use a visdom server to visualize the embeddings"")\n    args = parser.parse_args()\n\n    # some assertions to make sure that batching math assumptions are met\n    assert args.sup_num % args.batch_size == 0, ""assuming simplicity of batching math""\n    assert MNISTCached.validation_size % args.batch_size == 0, \\\n        ""batch size should divide the number of validation examples""\n    assert MNISTCached.train_data_size % args.batch_size == 0, \\\n        ""batch size doesn\'t divide total number of training data examples""\n    assert MNISTCached.test_size % args.batch_size == 0, ""batch size should divide the number of test examples""\n\n    main(args)\n'"
examples/vae/vae.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport visdom\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\nfrom pyro.optim import Adam\nfrom utils.mnist_cached import MNISTCached as MNIST\nfrom utils.mnist_cached import setup_data_loaders\nfrom utils.vae_plots import mnist_test_tsne, plot_llk, plot_vae_samples\n\n\n# define the PyTorch module that parameterizes the\n# diagonal gaussian distribution q(z|x)\nclass Encoder(nn.Module):\n    def __init__(self, z_dim, hidden_dim):\n        super().__init__()\n        # setup the three linear transformations used\n        self.fc1 = nn.Linear(784, hidden_dim)\n        self.fc21 = nn.Linear(hidden_dim, z_dim)\n        self.fc22 = nn.Linear(hidden_dim, z_dim)\n        # setup the non-linearities\n        self.softplus = nn.Softplus()\n\n    def forward(self, x):\n        # define the forward computation on the image x\n        # first shape the mini-batch to have pixels in the rightmost dimension\n        x = x.reshape(-1, 784)\n        # then compute the hidden units\n        hidden = self.softplus(self.fc1(x))\n        # then return a mean vector and a (positive) square root covariance\n        # each of size batch_size x z_dim\n        z_loc = self.fc21(hidden)\n        z_scale = torch.exp(self.fc22(hidden))\n        return z_loc, z_scale\n\n\n# define the PyTorch module that parameterizes the\n# observation likelihood p(x|z)\nclass Decoder(nn.Module):\n    def __init__(self, z_dim, hidden_dim):\n        super().__init__()\n        # setup the two linear transformations used\n        self.fc1 = nn.Linear(z_dim, hidden_dim)\n        self.fc21 = nn.Linear(hidden_dim, 784)\n        # setup the non-linearities\n        self.softplus = nn.Softplus()\n\n    def forward(self, z):\n        # define the forward computation on the latent z\n        # first compute the hidden units\n        hidden = self.softplus(self.fc1(z))\n        # return the parameter for the output Bernoulli\n        # each is of size batch_size x 784\n        loc_img = torch.sigmoid(self.fc21(hidden))\n        return loc_img\n\n\n# define a PyTorch module for the VAE\nclass VAE(nn.Module):\n    # by default our latent space is 50-dimensional\n    # and we use 400 hidden units\n    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n        super().__init__()\n        # create the encoder and decoder networks\n        self.encoder = Encoder(z_dim, hidden_dim)\n        self.decoder = Decoder(z_dim, hidden_dim)\n\n        if use_cuda:\n            # calling cuda() here will put all the parameters of\n            # the encoder and decoder networks into gpu memory\n            self.cuda()\n        self.use_cuda = use_cuda\n        self.z_dim = z_dim\n\n    # define the model p(x|z)p(z)\n    def model(self, x):\n        # register PyTorch module `decoder` with Pyro\n        pyro.module(""decoder"", self.decoder)\n        with pyro.plate(""data"", x.shape[0]):\n            # setup hyperparameters for prior p(z)\n            z_loc = torch.zeros(x.shape[0], self.z_dim, dtype=x.dtype, device=x.device)\n            z_scale = torch.ones(x.shape[0], self.z_dim, dtype=x.dtype, device=x.device)\n            # sample from prior (value will be sampled by guide when computing the ELBO)\n            z = pyro.sample(""latent"", dist.Normal(z_loc, z_scale).to_event(1))\n            # decode the latent code z\n            loc_img = self.decoder.forward(z)\n            # score against actual images\n            pyro.sample(""obs"", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n            # return the loc so we can visualize it later\n            return loc_img\n\n    # define the guide (i.e. variational distribution) q(z|x)\n    def guide(self, x):\n        # register PyTorch module `encoder` with Pyro\n        pyro.module(""encoder"", self.encoder)\n        with pyro.plate(""data"", x.shape[0]):\n            # use the encoder to get the parameters used to define q(z|x)\n            z_loc, z_scale = self.encoder.forward(x)\n            # sample the latent code z\n            pyro.sample(""latent"", dist.Normal(z_loc, z_scale).to_event(1))\n\n    # define a helper function for reconstructing images\n    def reconstruct_img(self, x):\n        # encode image x\n        z_loc, z_scale = self.encoder(x)\n        # sample in latent space\n        z = dist.Normal(z_loc, z_scale).sample()\n        # decode the image (note we don\'t sample in image space)\n        loc_img = self.decoder(z)\n        return loc_img\n\n\ndef main(args):\n    # clear param store\n    pyro.clear_param_store()\n\n    # setup MNIST data loaders\n    # train_loader, test_loader\n    train_loader, test_loader = setup_data_loaders(MNIST, use_cuda=args.cuda, batch_size=256)\n\n    # setup the VAE\n    vae = VAE(use_cuda=args.cuda)\n\n    # setup the optimizer\n    adam_args = {""lr"": args.learning_rate}\n    optimizer = Adam(adam_args)\n\n    # setup the inference algorithm\n    elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()\n    svi = SVI(vae.model, vae.guide, optimizer, loss=elbo)\n\n    # setup visdom for visualization\n    if args.visdom_flag:\n        vis = visdom.Visdom()\n\n    train_elbo = []\n    test_elbo = []\n    # training loop\n    for epoch in range(args.num_epochs):\n        # initialize loss accumulator\n        epoch_loss = 0.\n        # do a training epoch over each mini-batch x returned\n        # by the data loader\n        for x, _ in train_loader:\n            # if on GPU put mini-batch into CUDA memory\n            if args.cuda:\n                x = x.cuda()\n            # do ELBO gradient and accumulate loss\n            epoch_loss += svi.step(x)\n\n        # report training diagnostics\n        normalizer_train = len(train_loader.dataset)\n        total_epoch_loss_train = epoch_loss / normalizer_train\n        train_elbo.append(total_epoch_loss_train)\n        print(""[epoch %03d]  average training loss: %.4f"" % (epoch, total_epoch_loss_train))\n\n        if epoch % args.test_frequency == 0:\n            # initialize loss accumulator\n            test_loss = 0.\n            # compute the loss over the entire test set\n            for i, (x, _) in enumerate(test_loader):\n                # if on GPU put mini-batch into CUDA memory\n                if args.cuda:\n                    x = x.cuda()\n                # compute ELBO estimate and accumulate loss\n                test_loss += svi.evaluate_loss(x)\n\n                # pick three random test images from the first mini-batch and\n                # visualize how well we\'re reconstructing them\n                if i == 0:\n                    if args.visdom_flag:\n                        plot_vae_samples(vae, vis)\n                        reco_indices = np.random.randint(0, x.shape[0], 3)\n                        for index in reco_indices:\n                            test_img = x[index, :]\n                            reco_img = vae.reconstruct_img(test_img)\n                            vis.image(test_img.reshape(28, 28).detach().cpu().numpy(),\n                                      opts={\'caption\': \'test image\'})\n                            vis.image(reco_img.reshape(28, 28).detach().cpu().numpy(),\n                                      opts={\'caption\': \'reconstructed image\'})\n\n            # report test diagnostics\n            normalizer_test = len(test_loader.dataset)\n            total_epoch_loss_test = test_loss / normalizer_test\n            test_elbo.append(total_epoch_loss_test)\n            print(""[epoch %03d]  average test loss: %.4f"" % (epoch, total_epoch_loss_test))\n\n        if epoch == args.tsne_iter:\n            mnist_test_tsne(vae=vae, test_loader=test_loader)\n            plot_llk(np.array(train_elbo), np.array(test_elbo))\n\n    return vae\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    # parse command line arguments\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-epochs\', default=101, type=int, help=\'number of training epochs\')\n    parser.add_argument(\'-tf\', \'--test-frequency\', default=5, type=int, help=\'how often we evaluate the test set\')\n    parser.add_argument(\'-lr\', \'--learning-rate\', default=1.0e-3, type=float, help=\'learning rate\')\n    parser.add_argument(\'--cuda\', action=\'store_true\', default=False, help=\'whether to use cuda\')\n    parser.add_argument(\'--jit\', action=\'store_true\', default=False, help=\'whether to use PyTorch jit\')\n    parser.add_argument(\'-visdom\', \'--visdom_flag\', action=""store_true"", help=\'Whether plotting in visdom is desired\')\n    parser.add_argument(\'-i-tsne\', \'--tsne_iter\', default=100, type=int, help=\'epoch when tsne visualization runs\')\n    args = parser.parse_args()\n\n    model = main(args)\n'"
examples/vae/vae_comparison.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport itertools\nimport os\nfrom abc import ABCMeta, abstractmethod\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional\nfrom torchvision.utils import save_image\n\nimport pyro\nfrom pyro.contrib.examples import util\nfrom pyro.distributions import Bernoulli, Normal\nfrom pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\nfrom pyro.optim import Adam\nfrom utils.mnist_cached import DATA_DIR, RESULTS_DIR\n\n""""""\nComparison of VAE implementation in PyTorch and Pyro. This example can be\nused for profiling purposes.\n\nThe PyTorch VAE example is taken (with minor modification) from pytorch/examples.\nSource: https://github.com/pytorch/examples/tree/master/vae\n""""""\n\nTRAIN = \'train\'\nTEST = \'test\'\nOUTPUT_DIR = RESULTS_DIR\n\n\n# VAE encoder network\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 400)\n        self.fc21 = nn.Linear(400, 20)\n        self.fc22 = nn.Linear(400, 20)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = x.reshape(-1, 784)\n        h1 = self.relu(self.fc1(x))\n        return self.fc21(h1), torch.exp(self.fc22(h1))\n\n\n# VAE Decoder network\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc3 = nn.Linear(20, 400)\n        self.fc4 = nn.Linear(400, 784)\n        self.relu = nn.ReLU()\n\n    def forward(self, z):\n        h3 = self.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h3))\n\n\nclass VAE(object, metaclass=ABCMeta):\n    """"""\n    Abstract class for the variational auto-encoder. The abstract method\n    for training the network is implemented by subclasses.\n    """"""\n\n    def __init__(self, args, train_loader, test_loader):\n        self.args = args\n        self.vae_encoder = Encoder()\n        self.vae_decoder = Decoder()\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n        self.mode = TRAIN\n\n    def set_train(self, is_train=True):\n        if is_train:\n            self.mode = TRAIN\n            self.vae_encoder.train()\n            self.vae_decoder.train()\n        else:\n            self.mode = TEST\n            self.vae_encoder.eval()\n            self.vae_decoder.eval()\n\n    @abstractmethod\n    def compute_loss_and_gradient(self, x):\n        """"""\n        Given a batch of data `x`, run the optimizer (backpropagate the gradient),\n        and return the computed loss.\n\n        :param x: batch of data or a single datum (MNIST image).\n        :return: loss computed on the data batch.\n        """"""\n        return\n\n    def model_eval(self, x):\n        """"""\n        Given a batch of data `x`, run it through the trained VAE network to get\n        the reconstructed image.\n\n        :param x: batch of data or a single datum (MNIST image).\n        :return: reconstructed image, and the latent z\'s mean and variance.\n        """"""\n        z_mean, z_var = self.vae_encoder(x)\n        if self.mode == TRAIN:\n            z = Normal(z_mean, z_var.sqrt()).rsample()\n        else:\n            z = z_mean\n        return self.vae_decoder(z), z_mean, z_var\n\n    def train(self, epoch):\n        self.set_train(is_train=True)\n        train_loss = 0\n        for batch_idx, (x, _) in enumerate(self.train_loader):\n            loss = self.compute_loss_and_gradient(x)\n            train_loss += loss\n        print(\'====> Epoch: {} \\nTraining loss: {:.4f}\'.format(\n            epoch, train_loss / len(self.train_loader.dataset)))\n\n    def test(self, epoch):\n        self.set_train(is_train=False)\n        test_loss = 0\n        for i, (x, _) in enumerate(self.test_loader):\n            with torch.no_grad():\n                recon_x = self.model_eval(x)[0]\n                test_loss += self.compute_loss_and_gradient(x)\n            if i == 0:\n                n = min(x.size(0), 8)\n                comparison = torch.cat([x[:n],\n                                        recon_x.reshape(self.args.batch_size, 1, 28, 28)[:n]])\n                save_image(comparison.detach().cpu(),\n                           os.path.join(OUTPUT_DIR, \'reconstruction_\' + str(epoch) + \'.png\'),\n                           nrow=n)\n\n        test_loss /= len(self.test_loader.dataset)\n        print(\'Test set loss: {:.4f}\'.format(test_loss))\n\n\nclass PyTorchVAEImpl(VAE):\n    """"""\n    Adapted from pytorch/examples.\n    Source: https://github.com/pytorch/examples/tree/master/vae\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.optimizer = self.initialize_optimizer(lr=1e-3)\n\n    def compute_loss_and_gradient(self, x):\n        self.optimizer.zero_grad()\n        recon_x, z_mean, z_var = self.model_eval(x)\n        binary_cross_entropy = functional.binary_cross_entropy(recon_x, x.reshape(-1, 784))\n        # Uses analytical KL divergence expression for D_kl(q(z|x) || p(z))\n        # Refer to Appendix B from VAE paper:\n        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n        # (https://arxiv.org/abs/1312.6114)\n        kl_div = -0.5 * torch.sum(1 + z_var.log() - z_mean.pow(2) - z_var)\n        kl_div /= self.args.batch_size * 784\n        loss = binary_cross_entropy + kl_div\n        if self.mode == TRAIN:\n            loss.backward()\n            self.optimizer.step()\n        return loss.item()\n\n    def initialize_optimizer(self, lr=1e-3):\n        model_params = itertools.chain(self.vae_encoder.parameters(), self.vae_decoder.parameters())\n        return torch.optim.Adam(model_params, lr)\n\n\nclass PyroVAEImpl(VAE):\n    """"""\n    Implementation of VAE using Pyro. Only the model and the guide specification\n    is needed to run the optimizer (the objective function does not need to be\n    specified as in the PyTorch implementation).\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.optimizer = self.initialize_optimizer(lr=1e-3)\n\n    def model(self, data):\n        decoder = pyro.module(\'decoder\', self.vae_decoder)\n        z_mean, z_std = torch.zeros([data.size(0), 20]), torch.ones([data.size(0), 20])\n        with pyro.plate(\'data\', data.size(0)):\n            z = pyro.sample(\'latent\', Normal(z_mean, z_std).to_event(1))\n            img = decoder.forward(z)\n            pyro.sample(\'obs\',\n                        Bernoulli(img).to_event(1),\n                        obs=data.reshape(-1, 784))\n\n    def guide(self, data):\n        encoder = pyro.module(\'encoder\', self.vae_encoder)\n        with pyro.plate(\'data\', data.size(0)):\n            z_mean, z_var = encoder.forward(data)\n            pyro.sample(\'latent\', Normal(z_mean, z_var.sqrt()).to_event(1))\n\n    def compute_loss_and_gradient(self, x):\n        if self.mode == TRAIN:\n            loss = self.optimizer.step(x)\n        else:\n            loss = self.optimizer.evaluate_loss(x)\n        loss /= self.args.batch_size * 784\n        return loss\n\n    def initialize_optimizer(self, lr):\n        optimizer = Adam({\'lr\': lr})\n        elbo = JitTrace_ELBO() if self.args.jit else Trace_ELBO()\n        return SVI(self.model, self.guide, optimizer, loss=elbo)\n\n\ndef setup(args):\n    pyro.set_rng_seed(args.rng_seed)\n    train_loader = util.get_data_loader(dataset_name=\'MNIST\',\n                                        data_dir=DATA_DIR,\n                                        batch_size=args.batch_size,\n                                        is_training_set=True,\n                                        shuffle=True)\n    test_loader = util.get_data_loader(dataset_name=\'MNIST\',\n                                       data_dir=DATA_DIR,\n                                       batch_size=args.batch_size,\n                                       is_training_set=False,\n                                       shuffle=True)\n    global OUTPUT_DIR\n    OUTPUT_DIR = os.path.join(RESULTS_DIR, args.impl)\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n    pyro.clear_param_store()\n    return train_loader, test_loader\n\n\ndef main(args):\n    train_loader, test_loader = setup(args)\n    if args.impl == \'pyro\':\n        vae = PyroVAEImpl(args, train_loader, test_loader)\n        print(\'Running Pyro VAE implementation\')\n    elif args.impl == \'pytorch\':\n        vae = PyTorchVAEImpl(args, train_loader, test_loader)\n        print(\'Running PyTorch VAE implementation\')\n    else:\n        raise ValueError(\'Incorrect implementation specified: {}\'.format(args.impl))\n    for i in range(args.num_epochs):\n        vae.train(i)\n        if not args.skip_eval:\n            vae.test(i)\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=\'VAE using MNIST dataset\')\n    parser.add_argument(\'-n\', \'--num-epochs\', nargs=\'?\', default=10, type=int)\n    parser.add_argument(\'--batch_size\', nargs=\'?\', default=128, type=int)\n    parser.add_argument(\'--rng_seed\', nargs=\'?\', default=0, type=int)\n    parser.add_argument(\'--impl\', nargs=\'?\', default=\'pyro\', type=str)\n    parser.add_argument(\'--skip_eval\', action=\'store_true\')\n    parser.add_argument(\'--jit\', action=\'store_true\')\n    parser.set_defaults(skip_eval=False)\n    args = parser.parse_args()\n    main(args)\n'"
pyro/contrib/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nr""""""\nContributed Code\n================\n\n.. warning:: Code in ``pyro.contrib`` is under various stages of development.\n    This code makes no guarantee about maintaining backwards compatibility.\n""""""\n\nfrom pyro.contrib import autoname, bnn, easyguide, epidemiology, forecast, gp, oed, tracking\n\n__all__ = [\n    ""autoname"",\n    ""bnn"",\n    ""easyguide"",\n    ""epidemiology"",\n    ""forecast"",\n    ""gp"",\n    ""oed"",\n    ""tracking"",\n]\n'"
pyro/contrib/autoguide.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nfrom pyro.infer.autoguide import *  # noqa F403\n\nwarnings.warn(""pyro.contrib.autoguide has moved to pyro.infer.autoguide. ""\n              ""The contrib alias will stop working in Pyro 0.5."",\n              DeprecationWarning)\n'"
pyro/contrib/minipyro.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nMini Pyro\n---------\n\nThis file contains a minimal implementation of the Pyro Probabilistic\nProgramming Language. The API (method signatures, etc.) match that of\nthe full implementation as closely as possible. This file is independent\nof the rest of Pyro, with the exception of the :mod:`pyro.distributions`\nmodule.\n\nAn accompanying example that makes use of this implementation can be\nfound at examples/minipyro.py.\n""""""\nimport random\nimport warnings\nimport weakref\nfrom collections import OrderedDict\n\nimport torch\n\nfrom pyro.distributions import validation_enabled\n\n# Pyro keeps track of two kinds of global state:\n# i)  The effect handler stack, which enables non-standard interpretations of\n#     Pyro primitives like sample();\n#     See http://docs.pyro.ai/en/stable/poutine.html\n# ii) Trainable parameters in the Pyro ParamStore;\n#     See http://docs.pyro.ai/en/stable/parameters.html\n\nPYRO_STACK = []\nPARAM_STORE = {}  # maps name -> (unconstrained_value, constraint)\n\n\ndef get_param_store():\n    return PARAM_STORE\n\n\n# The base effect handler class (called Messenger here for consistency with Pyro).\nclass Messenger:\n    def __init__(self, fn=None):\n        self.fn = fn\n\n    # Effect handlers push themselves onto the PYRO_STACK.\n    # Handlers earlier in the PYRO_STACK are applied first.\n    def __enter__(self):\n        PYRO_STACK.append(self)\n\n    def __exit__(self, *args, **kwargs):\n        assert PYRO_STACK[-1] is self\n        PYRO_STACK.pop()\n\n    def process_message(self, msg):\n        pass\n\n    def postprocess_message(self, msg):\n        pass\n\n    def __call__(self, *args, **kwargs):\n        with self:\n            return self.fn(*args, **kwargs)\n\n\n# A first useful example of an effect handler.\n# trace records the inputs and outputs of any primitive site it encloses,\n# and returns a dictionary containing that data to the user.\nclass trace(Messenger):\n    def __enter__(self):\n        super().__enter__()\n        self.trace = OrderedDict()\n        return self.trace\n\n    # trace illustrates why we need postprocess_message in addition to process_message:\n    # We only want to record a value after all other effects have been applied\n    def postprocess_message(self, msg):\n        assert msg[""type""] != ""sample"" or msg[""name""] not in self.trace, \\\n            ""sample sites must have unique names""\n        self.trace[msg[""name""]] = msg.copy()\n\n    def get_trace(self, *args, **kwargs):\n        self(*args, **kwargs)\n        return self.trace\n\n\n# A second example of an effect handler for setting the value at a sample site.\n# This illustrates why effect handlers are a useful PPL implementation technique:\n# We can compose trace and replay to replace values but preserve distributions,\n# allowing us to compute the joint probability density of samples under a model.\n# See the definition of elbo(...) below for an example of this pattern.\nclass replay(Messenger):\n    def __init__(self, fn, guide_trace):\n        self.guide_trace = guide_trace\n        super().__init__(fn)\n\n    def process_message(self, msg):\n        if msg[""name""] in self.guide_trace:\n            msg[""value""] = self.guide_trace[msg[""name""]][""value""]\n\n\n# block allows the selective application of effect handlers to different parts of a model.\n# Sites hidden by block will only have the handlers below block on the PYRO_STACK applied,\n# allowing inference or other effectful computations to be nested inside models.\nclass block(Messenger):\n    def __init__(self, fn=None, hide_fn=lambda msg: True):\n        self.hide_fn = hide_fn\n        super().__init__(fn)\n\n    def process_message(self, msg):\n        if self.hide_fn(msg):\n            msg[""stop""] = True\n\n\n# seed is used to fix the RNG state when calling a model.\nclass seed(Messenger):\n    def __init__(self, fn=None, rng_seed=None):\n        self.rng_seed = rng_seed\n        super().__init__(fn)\n\n    def __enter__(self):\n        self.old_state = {\'torch\': torch.get_rng_state(), \'random\': random.getstate()}\n        torch.manual_seed(self.rng_seed)\n        random.seed(self.rng_seed)\n        try:\n            import numpy as np\n            np.random.seed(self.rng_seed)\n            self.old_state[\'numpy\'] = np.random.get_state()\n        except ImportError:\n            pass\n\n    def __exit__(self, type, value, traceback):\n        torch.set_rng_state(self.old_state[\'torch\'])\n        random.setstate(self.old_state[\'random\'])\n        if \'numpy\' in self.old_state:\n            import numpy as np\n            np.random.set_state(self.old_state[\'numpy\'])\n\n\n# This limited implementation of PlateMessenger only implements broadcasting.\nclass PlateMessenger(Messenger):\n    def __init__(self, fn, size, dim):\n        assert dim < 0\n        self.size = size\n        self.dim = dim\n        super().__init__(fn)\n\n    def process_message(self, msg):\n        if msg[""type""] == ""sample"":\n            batch_shape = msg[""fn""].batch_shape\n            if len(batch_shape) < -self.dim or batch_shape[self.dim] != self.size:\n                batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape)\n                batch_shape[self.dim] = self.size\n                msg[""fn""] = msg[""fn""].expand(torch.Size(batch_shape))\n\n    def __iter__(self):\n        return range(self.size)\n\n\n# apply_stack is called by pyro.sample and pyro.param.\n# It is responsible for applying each Messenger to each effectful operation.\ndef apply_stack(msg):\n    for pointer, handler in enumerate(reversed(PYRO_STACK)):\n        handler.process_message(msg)\n        # When a Messenger sets the ""stop"" field of a message,\n        # it prevents any Messengers above it on the stack from being applied.\n        if msg.get(""stop""):\n            break\n    if msg[""value""] is None:\n        msg[""value""] = msg[""fn""](*msg[""args""])\n\n    # A Messenger that sets msg[""stop""] == True also prevents application\n    # of postprocess_message by Messengers above it on the stack\n    # via the pointer variable from the process_message loop\n    for handler in PYRO_STACK[-pointer-1:]:\n        handler.postprocess_message(msg)\n    return msg\n\n\n# sample is an effectful version of Distribution.sample(...)\n# When any effect handlers are active, it constructs an initial message and calls apply_stack.\ndef sample(name, fn, *args, **kwargs):\n    obs = kwargs.pop(\'obs\', None)\n\n    # if there are no active Messengers, we just draw a sample and return it as expected:\n    if not PYRO_STACK:\n        return fn(*args, **kwargs)\n\n    # Otherwise, we initialize a message...\n    initial_msg = {\n        ""type"": ""sample"",\n        ""name"": name,\n        ""fn"": fn,\n        ""args"": args,\n        ""kwargs"": kwargs,\n        ""value"": obs,\n    }\n\n    # ...and use apply_stack to send it to the Messengers\n    msg = apply_stack(initial_msg)\n    return msg[""value""]\n\n\n# param is an effectful version of PARAM_STORE.setdefault that also handles constraints.\n# When any effect handlers are active, it constructs an initial message and calls apply_stack.\ndef param(name, init_value=None, constraint=torch.distributions.constraints.real, event_dim=None):\n    if event_dim is not None:\n        raise NotImplementedError(""minipyro.plate does not support the event_dim arg"")\n\n    def fn(init_value, constraint):\n        if name in PARAM_STORE:\n            unconstrained_value, constraint = PARAM_STORE[name]\n        else:\n            # Initialize with a constrained value.\n            assert init_value is not None\n            with torch.no_grad():\n                constrained_value = init_value.detach()\n                unconstrained_value = torch.distributions.transform_to(constraint).inv(constrained_value)\n            unconstrained_value.requires_grad_()\n            PARAM_STORE[name] = unconstrained_value, constraint\n\n        # Transform from unconstrained space to constrained space.\n        constrained_value = torch.distributions.transform_to(constraint)(unconstrained_value)\n        constrained_value.unconstrained = weakref.ref(unconstrained_value)\n        return constrained_value\n\n    # if there are no active Messengers, we just draw a sample and return it as expected:\n    if not PYRO_STACK:\n        return fn(init_value, constraint)\n\n    # Otherwise, we initialize a message...\n    initial_msg = {\n        ""type"": ""param"",\n        ""name"": name,\n        ""fn"": fn,\n        ""args"": (init_value, constraint),\n        ""value"": None,\n    }\n\n    # ...and use apply_stack to send it to the Messengers\n    msg = apply_stack(initial_msg)\n    return msg[""value""]\n\n\n# boilerplate to match the syntax of actual pyro.plate:\ndef plate(name, size, dim=None):\n    if dim is None:\n        raise NotImplementedError(""minipyro.plate requires a dim arg"")\n    return PlateMessenger(fn=None, size=size, dim=dim)\n\n\n# This is a thin wrapper around the `torch.optim.Adam` class that\n# dynamically generates optimizers for dynamically generated parameters.\n# See http://docs.pyro.ai/en/stable/optimization.html\nclass Adam:\n    def __init__(self, optim_args):\n        self.optim_args = optim_args\n        # Each parameter will get its own optimizer, which we keep track\n        # of using this dictionary keyed on parameters.\n        self.optim_objs = {}\n\n    def __call__(self, params):\n        for param in params:\n            # If we\'ve seen this parameter before, use the previously\n            # constructed optimizer.\n            if param in self.optim_objs:\n                optim = self.optim_objs[param]\n            # If we\'ve never seen this parameter before, construct\n            # an Adam optimizer and keep track of it.\n            else:\n                optim = torch.optim.Adam([param], **self.optim_args)\n                self.optim_objs[param] = optim\n            # Take a gradient step for the parameter param.\n            optim.step()\n\n\n# This is a unified interface for stochastic variational inference in Pyro.\n# The actual construction of the loss is taken care of by `loss`.\n# See http://docs.pyro.ai/en/stable/inference_algos.html\nclass SVI:\n    def __init__(self, model, guide, optim, loss):\n        self.model = model\n        self.guide = guide\n        self.optim = optim\n        self.loss = loss\n\n    # This method handles running the model and guide, constructing the loss\n    # function, and taking a gradient step.\n    def step(self, *args, **kwargs):\n        # This wraps both the call to `model` and `guide` in a `trace` so that\n        # we can record all the parameters that are encountered. Note that\n        # further tracing occurs inside of `loss`.\n        with trace() as param_capture:\n            # We use block here to allow tracing to record parameters only.\n            with block(hide_fn=lambda msg: msg[""type""] == ""sample""):\n                loss = self.loss(self.model, self.guide, *args, **kwargs)\n        # Differentiate the loss.\n        loss.backward()\n        # Grab all the parameters from the trace.\n        params = [site[""value""].unconstrained()\n                  for site in param_capture.values()]\n        # Take a step w.r.t. each parameter in params.\n        self.optim(params)\n        # Zero out the gradients so that they don\'t accumulate.\n        for p in params:\n            p.grad = torch.zeros_like(p)\n        return loss.item()\n\n\n# This is a basic implementation of the Evidence Lower Bound, which is the\n# fundamental objective in Variational Inference.\n# See http://pyro.ai/examples/svi_part_i.html for details.\n# This implementation has various limitations (for example it only supports\n# random variables with reparameterized samplers), but all the ELBO\n# implementations in Pyro share the same basic logic.\ndef elbo(model, guide, *args, **kwargs):\n    # Run the guide with the arguments passed to SVI.step() and trace the execution,\n    # i.e. record all the calls to Pyro primitives like sample() and param().\n    guide_trace = trace(guide).get_trace(*args, **kwargs)\n    # Now run the model with the same arguments and trace the execution. Because\n    # model is being run with replay, whenever we encounter a sample site in the\n    # model, instead of sampling from the corresponding distribution in the model,\n    # we instead reuse the corresponding sample from the guide. In probabilistic\n    # terms, this means our loss is constructed as an expectation w.r.t. the joint\n    # distribution defined by the guide.\n    model_trace = trace(replay(model, guide_trace)).get_trace(*args, **kwargs)\n    # We will accumulate the various terms of the ELBO in `elbo`.\n    elbo = 0.\n    # Loop over all the sample sites in the model and add the corresponding\n    # log p(z) term to the ELBO. Note that this will also include any observed\n    # data, i.e. sample sites with the keyword `obs=...`.\n    for site in model_trace.values():\n        if site[""type""] == ""sample"":\n            elbo = elbo + site[""fn""].log_prob(site[""value""]).sum()\n    # Loop over all the sample sites in the guide and add the corresponding\n    # -log q(z) term to the ELBO.\n    for site in guide_trace.values():\n        if site[""type""] == ""sample"":\n            elbo = elbo - site[""fn""].log_prob(site[""value""]).sum()\n    # Return (-elbo) since by convention we do gradient descent on a loss and\n    # the ELBO is a lower bound that needs to be maximized.\n    return -elbo\n\n\n# This is a wrapper for compatibility with full Pyro.\ndef Trace_ELBO(**kwargs):\n    return elbo\n\n\n# This is a Jit wrapper around elbo() that (1) delays tracing until the first\n# invocation, and (2) registers pyro.param() statements with torch.jit.trace.\n# This version does not support variable number of args or non-tensor kwargs.\nclass JitTrace_ELBO:\n    def __init__(self, **kwargs):\n        self.ignore_jit_warnings = kwargs.pop(""ignore_jit_warnings"", False)\n        self._compiled = None\n        self._param_trace = None\n\n    def __call__(self, model, guide, *args):\n        # On first call, initialize params and save their names.\n        if self._param_trace is None:\n            with block(), trace() as tr, block(hide_fn=lambda m: m[""type""] != ""param""):\n                elbo(model, guide, *args)\n            self._param_trace = tr\n\n        # Augment args with reads from the global param store.\n        unconstrained_params = tuple(param(name).unconstrained()\n                                     for name in self._param_trace)\n        params_and_args = unconstrained_params + args\n\n        # On first call, create a compiled elbo.\n        if self._compiled is None:\n\n            def compiled(*params_and_args):\n                unconstrained_params = params_and_args[:len(self._param_trace)]\n                args = params_and_args[len(self._param_trace):]\n                for name, unconstrained_param in zip(self._param_trace, unconstrained_params):\n                    constrained_param = param(name)  # assume param has been initialized\n                    assert constrained_param.unconstrained() is unconstrained_param\n                    self._param_trace[name][""value""] = constrained_param\n                return replay(elbo, guide_trace=self._param_trace)(model, guide, *args)\n\n            with validation_enabled(False), warnings.catch_warnings():\n                if self.ignore_jit_warnings:\n                    warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n                self._compiled = torch.jit.trace(compiled, params_and_args, check_trace=False)\n\n        return self._compiled(*params_and_args)\n'"
pyro/contrib/util.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\nimport torch\nimport pyro\n\n\ndef get_indices(labels, sizes=None, tensors=None):\n    indices = []\n    start = 0\n    if sizes is None:\n        sizes = OrderedDict([(l, t.shape[0]) for l, t in tensors.items()])\n    for label in sizes:\n        end = start+sizes[label]\n        if label in labels:\n            indices.extend(range(start, end))\n        start = end\n    return torch.tensor(indices)\n\n\ndef tensor_to_dict(sizes, tensor, subset=None):\n    if subset is None:\n        subset = sizes.keys()\n    start = 0\n    out = {}\n    for label, size in sizes.items():\n        end = start + size\n        if label in subset:\n            out[label] = tensor[..., start:end]\n        start = end\n    return out\n\n\ndef rmm(A, B):\n    """"""Shorthand for `matmul`.""""""\n    return torch.matmul(A, B)\n\n\ndef rmv(A, b):\n    """"""Tensorized matrix vector multiplication of rightmost dimensions.""""""\n    return torch.matmul(A, b.unsqueeze(-1)).squeeze(-1)\n\n\ndef rvv(a, b):\n    """"""Tensorized vector vector multiplication of rightmost dimensions.""""""\n    return torch.matmul(a.unsqueeze(-2), b.unsqueeze(-1)).squeeze(-2).squeeze(-1)\n\n\ndef lexpand(A, *dimensions):\n    """"""Expand tensor, adding new dimensions on left.""""""\n    return A.expand(tuple(dimensions) + A.shape)\n\n\ndef rexpand(A, *dimensions):\n    """"""Expand tensor, adding new dimensions on right.""""""\n    return A.view(A.shape + (1,)*len(dimensions)).expand(A.shape + tuple(dimensions))\n\n\ndef rdiag(v):\n    """"""Converts the rightmost dimension to a diagonal matrix.""""""\n    return rexpand(v, v.shape[-1])*torch.eye(v.shape[-1])\n\n\ndef rtril(M, diagonal=0, upper=False):\n    """"""Takes the lower-triangular of the rightmost 2 dimensions.""""""\n    if upper:\n        return rtril(M, diagonal=diagonal, upper=False).transpose(-1, -2)\n    return M*torch.tril(torch.ones(M.shape[-2], M.shape[-1]), diagonal=diagonal)\n\n\ndef iter_plates_to_shape(shape):\n    # Go backwards (right to left)\n    for i, s in enumerate(shape[::-1]):\n        yield pyro.plate(""plate_"" + str(i), s)\n'"
pyro/distributions/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro.distributions.torch_patch  # noqa F403\nfrom pyro.distributions.avf_mvn import AVFMultivariateNormal\nfrom pyro.distributions.coalescent import CoalescentRateLikelihood, CoalescentTimes, CoalescentTimesWithRate\nfrom pyro.distributions.conditional import (ConditionalDistribution, ConditionalTransform,\n                                            ConditionalTransformedDistribution, ConditionalTransformModule)\nfrom pyro.distributions.conjugate import BetaBinomial, DirichletMultinomial, GammaPoisson\nfrom pyro.distributions.delta import Delta\nfrom pyro.distributions.diag_normal_mixture import MixtureOfDiagNormals\nfrom pyro.distributions.diag_normal_mixture_shared_cov import MixtureOfDiagNormalsSharedCovariance\nfrom pyro.distributions.distribution import Distribution\nfrom pyro.distributions.empirical import Empirical\nfrom pyro.distributions.extended import ExtendedBetaBinomial, ExtendedBinomial\nfrom pyro.distributions.folded import FoldedDistribution\nfrom pyro.distributions.gaussian_scale_mixture import GaussianScaleMixture\nfrom pyro.distributions.hmm import DiscreteHMM, GammaGaussianHMM, GaussianHMM, GaussianMRF, IndependentHMM, LinearHMM\nfrom pyro.distributions.improper_uniform import ImproperUniform\nfrom pyro.distributions.inverse_gamma import InverseGamma\nfrom pyro.distributions.lkj import LKJCorrCholesky\nfrom pyro.distributions.mixture import MaskedMixture\nfrom pyro.distributions.multivariate_studentt import MultivariateStudentT\nfrom pyro.distributions.omt_mvn import OMTMultivariateNormal\nfrom pyro.distributions.polya_gamma import TruncatedPolyaGamma\nfrom pyro.distributions.rejector import Rejector\nfrom pyro.distributions.relaxed_straight_through import (RelaxedBernoulliStraightThrough,\n                                                         RelaxedOneHotCategoricalStraightThrough)\nfrom pyro.distributions.spanning_tree import SpanningTree\nfrom pyro.distributions.stable import Stable\nfrom pyro.distributions.torch import *  # noqa F403\nfrom pyro.distributions.torch import __all__ as torch_dists\nfrom pyro.distributions.torch_distribution import MaskedDistribution, TorchDistribution\nfrom pyro.distributions.torch_transform import ComposeTransformModule, TransformModule\nfrom pyro.distributions.unit import Unit\nfrom pyro.distributions.util import enable_validation, is_validation_enabled, validation_enabled\nfrom pyro.distributions.von_mises_3d import VonMises3D\nfrom pyro.distributions.zero_inflated import ZeroInflatedDistribution, ZeroInflatedNegativeBinomial, ZeroInflatedPoisson\n\nfrom . import constraints, kl, transforms\n\n__all__ = [\n    ""AVFMultivariateNormal"",\n    ""BetaBinomial"",\n    ""CoalescentRateLikelihood"",\n    ""CoalescentTimes"",\n    ""CoalescentTimesWithRate"",\n    ""ComposeTransformModule"",\n    ""ConditionalDistribution"",\n    ""ConditionalTransform"",\n    ""ConditionalTransformModule"",\n    ""ConditionalTransformedDistribution"",\n    ""Delta"",\n    ""DirichletMultinomial"",\n    ""DiscreteHMM"",\n    ""Distribution"",\n    ""Empirical"",\n    ""ExtendedBetaBinomial"",\n    ""ExtendedBinomial"",\n    ""FoldedDistribution"",\n    ""GammaGaussianHMM"",\n    ""GammaPoisson"",\n    ""GaussianHMM"",\n    ""GaussianMRF"",\n    ""GaussianScaleMixture"",\n    ""ImproperUniform"",\n    ""IndependentHMM"",\n    ""InverseGamma"",\n    ""LinearHMM"",\n    ""LKJCorrCholesky"",\n    ""MaskedDistribution"",\n    ""MaskedMixture"",\n    ""MixtureOfDiagNormals"",\n    ""MixtureOfDiagNormalsSharedCovariance"",\n    ""MultivariateStudentT"",\n    ""OMTMultivariateNormal"",\n    ""Rejector"",\n    ""RelaxedBernoulliStraightThrough"",\n    ""RelaxedOneHotCategoricalStraightThrough"",\n    ""SpanningTree"",\n    ""Stable"",\n    ""TorchDistribution"",\n    ""TransformModule"",\n    ""TruncatedPolyaGamma"",\n    ""Unit"",\n    ""VonMises3D"",\n    ""ZeroInflatedPoisson"",\n    ""ZeroInflatedNegativeBinomial"",\n    ""ZeroInflatedDistribution"",\n    ""constraints"",\n    ""enable_validation"",\n    ""is_validation_enabled"",\n    ""kl"",\n    ""transforms"",\n    ""validation_enabled"",\n]\n\n# Import all torch distributions from `pyro.distributions.torch_distribution`\n__all__.extend(torch_dists)\ndel torch_dists\n'"
pyro/distributions/avf_mvn.py,15,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch import MultivariateNormal\nfrom pyro.distributions.util import sum_leftmost\n\n\nclass AVFMultivariateNormal(MultivariateNormal):\n    """"""Multivariate normal (Gaussian) distribution with transport equation inspired control\n    variates (adaptive velocity fields).\n\n    A distribution over vectors in which all the elements have a joint Gaussian density.\n\n    :param torch.Tensor loc: D-dimensional mean vector.\n    :param torch.Tensor scale_tril: Cholesky of Covariance matrix; D x D matrix.\n    :param torch.Tensor control_var: 2 x L x D tensor that parameterizes the control variate;\n        L is an arbitrary positive integer.  This parameter needs to be learned (i.e. adapted) to\n        achieve lower variance gradients. In a typical use case this parameter will be adapted\n        concurrently with the `loc` and `scale_tril` that define the distribution.\n\n\n    Example usage::\n\n        control_var = torch.tensor(0.1 * torch.ones(2, 1, D), requires_grad=True)\n        opt_cv = torch.optim.Adam([control_var], lr=0.1, betas=(0.5, 0.999))\n\n        for _ in range(1000):\n            d = AVFMultivariateNormal(loc, scale_tril, control_var)\n            z = d.rsample()\n            cost = torch.pow(z, 2.0).sum()\n            cost.backward()\n            opt_cv.step()\n            opt_cv.zero_grad()\n\n    """"""\n    arg_constraints = {""loc"": constraints.real, ""scale_tril"": constraints.lower_triangular,\n                       ""control_var"": constraints.real}\n\n    def __init__(self, loc, scale_tril, control_var):\n        if loc.dim() != 1:\n            raise ValueError(""AVFMultivariateNormal loc must be 1-dimensional"")\n        if scale_tril.dim() != 2:\n            raise ValueError(""AVFMultivariateNormal scale_tril must be 2-dimensional"")\n        if control_var.dim() != 3 or control_var.size(0) != 2 or control_var.size(2) != loc.size(0):\n            raise ValueError(""control_var should be of size 2 x L x D, where D is the dimension of the location parameter loc"")  # noqa: E501\n        self.control_var = control_var\n        super().__init__(loc, scale_tril=scale_tril)\n\n    def rsample(self, sample_shape=torch.Size()):\n        return _AVFMVNSample.apply(self.loc, self.scale_tril, self.control_var, sample_shape + self.loc.shape)\n\n\nclass _AVFMVNSample(Function):\n    @staticmethod\n    def forward(ctx, loc, scale_tril, control_var, shape):\n        white = torch.randn(shape, dtype=loc.dtype, device=loc.device)\n        z = torch.matmul(white, scale_tril.t())\n        ctx.save_for_backward(scale_tril, control_var, white)\n        return loc + z\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        L, control_var, epsilon = ctx.saved_tensors\n        B, C = control_var\n        g = grad_output\n        loc_grad = sum_leftmost(grad_output, -1)\n\n        # compute the rep trick gradient\n        epsilon_jb = epsilon.unsqueeze(-2)\n        g_ja = g.unsqueeze(-1)\n        diff_L_ab = sum_leftmost(g_ja * epsilon_jb, -2)\n\n        # modulate the velocity fields with infinitesimal rotations, i.e. apply the control variate\n        gL = torch.matmul(g, L)\n        eps_gL_ab = sum_leftmost(gL.unsqueeze(-1) * epsilon.unsqueeze(-2), -2)\n        xi_ab = eps_gL_ab - eps_gL_ab.t()\n        BC_lab = B.unsqueeze(-1) * C.unsqueeze(-2)\n        diff_L_ab += (xi_ab.unsqueeze(0) * BC_lab).sum(0)\n        L_grad = torch.tril(diff_L_ab)\n\n        # compute control_var grads\n        diff_B = (L_grad.unsqueeze(0) * C.unsqueeze(-2) * xi_ab.unsqueeze(0)).sum(2)\n        diff_C = (L_grad.t().unsqueeze(0) * B.unsqueeze(-2) * xi_ab.t().unsqueeze(0)).sum(2)\n        diff_CV = torch.stack([diff_B, diff_C])\n\n        return loc_grad, L_grad, diff_CV, None\n'"
pyro/distributions/coalescent.py,26,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport weakref\nfrom collections import namedtuple\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.util import broadcast_shape, is_validation_enabled\nfrom pyro.ops.special import safe_log\n\nfrom .torch_distribution import TorchDistribution\n\n\nclass CoalescentTimesConstraint(constraints.Constraint):\n    def __init__(self, leaf_times, *, ordered=True):\n        self.leaf_times = leaf_times\n        self.ordered = ordered\n\n    def check(self, value):\n        # There must always at least one lineage.\n        coal_times = value\n        phylogeny = _make_phylogeny(self.leaf_times, coal_times)\n        at_least_one_lineage = (phylogeny.lineages > 0).all(dim=-1)\n        if not self.ordered:\n            return at_least_one_lineage\n\n        # Inputs must be ordered.\n        ordered = (value[..., :-1] <= value[..., 1:]).all(dim=-1)\n        return ordered & at_least_one_lineage\n\n\nclass CoalescentTimes(TorchDistribution):\n    """"""\n    Distribution over coalescent times given irregular sampled ``leaf_times``.\n\n    Sample values will be sorted sets of binary coalescent times. Each sample\n    ``value`` will have cardinality ``value.size(-1) = leaf_times.size(-1) -\n    1``, so that phylogenies are complete binary trees.  This distribution can\n    thus be batched over multiple samples of phylogenies given fixed (number\n    of) leaf times, e.g. over phylogeny samples from BEAST or MrBayes.\n\n    **References**\n\n    [1] J.F.C. Kingman (1982)\n        ""On the Genealogy of Large Populations""\n        Journal of Applied Probability\n    [2] J.F.C. Kingman (1982)\n        ""The Coalescent""\n        Stochastic Processes and their Applications\n\n    :param torch.Tensor leaf_times: Vector of times of sampling events, i.e.\n        leaf nodes in the phylogeny. These can be arbitrary real numbers with\n        arbitrary order and duplicates.\n    """"""\n    arg_constraints = {""leaf_times"": constraints.real}\n\n    def __init__(self, leaf_times, *, validate_args=None):\n        event_shape = (leaf_times.size(-1) - 1,)\n        batch_shape = leaf_times.shape[:-1]\n        self.leaf_times = leaf_times\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    @constraints.dependent_property\n    def support(self):\n        return CoalescentTimesConstraint(self.leaf_times)\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        coal_times = value\n        phylogeny = _make_phylogeny(self.leaf_times, coal_times)\n\n        # The coalescent process is like a Poisson process with rate binomial\n        # in the number of lineages, which changes at each event.\n        binomial = phylogeny.binomial[..., :-1]\n        interval = phylogeny.times[..., :-1] - phylogeny.times[..., 1:]\n        log_prob = -(binomial * interval).sum(-1)\n\n        # Scaling by those rates and accounting for log|jacobian|, the density\n        # is that of a collection of independent Exponential intervals.\n        log_abs_det_jacobian = phylogeny.coal_binomial.log().sum(-1).neg()\n        return log_prob - log_abs_det_jacobian\n\n    def sample(self, sample_shape=torch.Size()):\n        shape = self._extended_shape(sample_shape)[:-1]\n        leaf_times = self.leaf_times.expand(shape + (-1,))\n        return _sample_coalescent_times(leaf_times)\n\n\nclass CoalescentTimesWithRate(TorchDistribution):\n    """"""\n    Distribution over coalescent times given irregular sampled ``leaf_times``\n    and piecewise constant coalescent rates defined on a regular time grid.\n\n    This assumes a piecewise constant base coalescent rate specified on time\n    intervals ``(-inf,1]``, ``[1,2]``, ..., ``[T-1,inf)``,  where ``T =\n    rate_grid.size(-1)``. Leaves may be sampled at arbitrary real times, but\n    are commonly sampled in the interval ``[0, T]``.\n\n    Sample values will be sorted sets of binary coalescent times. Each sample\n    ``value`` will have cardinality ``value.size(-1) = leaf_times.size(-1) -\n    1``, so that phylogenies are complete binary trees.  This distribution can\n    thus be batched over multiple samples of phylogenies given fixed (number\n    of) leaf times, e.g. over phylogeny samples from BEAST or MrBayes.\n\n    This distribution implements :meth:`log_prob` but not ``.sample()``.\n\n    See also :class:`~pyro.distributions.CoalescentRateLikelihood`.\n\n    **References**\n\n    [1] J.F.C. Kingman (1982)\n        ""On the Genealogy of Large Populations""\n        Journal of Applied Probability\n    [2] J.F.C. Kingman (1982)\n        ""The Coalescent""\n        Stochastic Processes and their Applications\n    [3] A. Popinga, T. Vaughan, T. Statler, A.J. Drummond (2014)\n        ""Inferring epidemiological dynamics with Bayesian coalescent inference:\n        The merits of deterministic and stochastic models""\n        https://arxiv.org/pdf/1407.1792.pdf\n\n    :param torch.Tensor leaf_times: Tensor of times of sampling events, i.e.\n        leaf nodes in the phylogeny. These can be arbitrary real numbers with\n        arbitrary order and duplicates.\n    :param torch.Tensor rate_grid: Tensor of base coalescent rates (pairwise\n        rate of coalescence). For example in a simple SIR model this might be\n        ``beta S / I``. The rightmost dimension is time, and this tensor\n        represents a (batch of) rates that are piecwise constant in time.\n    """"""\n    arg_constraints = {""leaf_times"": constraints.real,\n                       ""rate_grid"": constraints.positive}\n\n    def __init__(self, leaf_times, rate_grid, *, validate_args=None):\n        batch_shape = broadcast_shape(leaf_times.shape[:-1], rate_grid.shape[:-1])\n        event_shape = (leaf_times.size(-1) - 1,)\n        self.leaf_times = leaf_times\n        self.rate_grid = rate_grid\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    @constraints.dependent_property\n    def support(self):\n        return CoalescentTimesConstraint(self.leaf_times)\n\n    @property\n    def duration(self):\n        return self.rate_grid.size(-1)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(CoalescentTimesWithRate, _instance)\n        new.leaf_times = self.leaf_times\n        new.rate_grid = self.rate_grid\n        super(CoalescentTimesWithRate, new).__init__(\n            batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(""_validate_args"")\n        return new\n\n    def log_prob(self, value):\n        """"""\n        Computes likelihood as in equations 7-8 of [3].\n\n        This has time complexity ``O(T + S N log(N))`` where ``T`` is the\n        number of time steps, ``N`` is the number of leaves, and ``S =\n        sample_shape.numel()`` is the number of samples of ``value``.\n\n        This is differentiable wrt ``rate_grid`` but neither ``leaf_times`` nor\n        ``value = coal_times``.\n\n        :param torch.Tensor value: A tensor of coalescent times. These denote\n            sets of size ``leaf_times.size(-1) - 1`` along the trailing\n            dimension and should be sorted along that dimension.\n        :returns: Likelihood ``p(coal_times | leaf_times, rate_grid)``\n        :rtype: torch.Tensor\n        """"""\n        if self._validate_args:\n            self._validate_sample(value)\n        coal_times = value\n        phylogeny = _make_phylogeny(self.leaf_times, coal_times)\n\n        # Compute survival factors for closed intervals.\n        cumsum = self.rate_grid.cumsum(-1)\n        cumsum = torch.nn.functional.pad(cumsum, (1, 0), value=0)\n        integral = _interpolate_gather(cumsum, phylogeny.times[..., 1:])  # ignore the final lonely leaf\n        integral = integral[..., :-1] - integral[..., 1:]\n        integral = integral.clamp(min=torch.finfo(integral.dtype).tiny)  # avoid nan\n        log_prob = -(phylogeny.binomial[..., 1:-1] * integral).sum(-1)\n\n        # Compute density of coalescent events.\n        i = coal_times.floor().clamp(min=0, max=self.duration - 1).long()\n        rates = phylogeny.coal_binomial * _gather(self.rate_grid, -1, i)\n        log_prob = log_prob + safe_log(rates).sum(-1)\n\n        batch_shape = broadcast_shape(self.batch_shape, value.shape[:-1])\n        log_prob = log_prob.expand(batch_shape)\n        return log_prob\n\n\nclass CoalescentRateLikelihood:\n    """"""\n    EXPERIMENTAL This is not a :class:`~pyro.distributions.Distribution`, but\n    acts as a transposed version of :class:`CoalescentTimesWithRate` making the\n    elements of ``rate_grid`` independent and thus compatible with ``plate``\n    and ``poutine.markov``. For non-batched inputs the following are all\n    equivalent likelihoods::\n\n        # Version 1.\n        pyro.sample(""coalescent"",\n                    CoalescentTimesWithRate(leaf_times, rate_grid),\n                    obs=coal_times)\n\n        # Version 2. using pyro.plate\n        likelihood = CoalescentRateLikelihood(leaf_times, coal_times, len(rate_grid))\n        with pyro.plate(""time"", len(rate_grid)):\n            pyro.factor(""coalescent"", likelihood(rate_grid))\n\n        # Version 3. using pyro.markov\n        likelihood = CoalescentRateLikelihood(leaf_times, coal_times, len(rate_grid))\n        for t in pyro.markov(range(len(rate_grid))):\n            pyro.factor(""coalescent_{}"".format(t), likelihood(rate_grid[t], t))\n\n    The third version is useful for e.g.\n    :class:`~pyro.infer.smcfilter.SMCFilter` where ``rate_grid`` might be\n    computed sequentially.\n\n    :param torch.Tensor leaf_times: Tensor of times of sampling events, i.e.\n        leaf nodes in the phylogeny. These can be arbitrary real numbers with\n        arbitrary order and duplicates.\n    :param torch.Tensor coal_times: A tensor of coalescent times. These denote\n        sets of size ``leaf_times.size(-1) - 1`` along the trailing dimension\n        and should be sorted along that dimension.\n    :param int duration: Size of the rate grid, ``rate_grid.size(-1)``.\n    """"""\n    def __init__(self, leaf_times, coal_times, duration, *, validate_args=None):\n        assert leaf_times.size(-1) == 1 + coal_times.size(-1)\n        assert isinstance(duration, int) and duration >= 2\n        if validate_args is True or validate_args is None and is_validation_enabled:\n            constraint = CoalescentTimesConstraint(leaf_times, ordered=False)\n            if not constraint.check(coal_times).all():\n                raise ValueError(""Invalid (leaf_times, coal_times)"")\n\n        phylogeny = _make_phylogeny(leaf_times, coal_times)\n        batch_shape = phylogeny.times.shape[:-1]\n        new_zeros = leaf_times.new_zeros\n        new_ones = leaf_times.new_ones\n\n        # Construct linear part from intervals of survival outside of [0,duration].\n        times = phylogeny.times.clamp(max=0)\n        intervals = times[..., 1:] - times[..., :-1]\n        pre_linear = (phylogeny.binomial[..., :-1] * intervals).sum(-1, keepdim=True)\n        times = phylogeny.times.clamp(min=duration)\n        intervals = times[..., 1:] - times[..., :-1]\n        post_linear = (phylogeny.binomial[..., :-1] * intervals).sum(-1, keepdim=True)\n        self._linear = torch.cat([pre_linear,\n                                  new_zeros(pre_linear.shape[:-1] + (duration - 2,)),\n                                  post_linear], dim=-1)\n\n        # Construct linear part from intervals of survival within [0, duration].\n        times = phylogeny.times.clamp(min=0, max=duration)\n        sparse_diff = phylogeny.binomial[..., :-1] - phylogeny.binomial[..., 1:]\n        dense_diff = new_zeros(batch_shape + (1 + duration,))\n        _interpolate_scatter_add_(dense_diff, times[..., 1:], sparse_diff)\n        self._linear += dense_diff.flip([-1]).cumsum(-1)[..., :-1].flip([-1])\n\n        # Construct const and log part from coalescent events.\n        coal_index = coal_times.floor().clamp(min=0, max=duration - 1).long()\n        self._const = new_zeros(batch_shape + (duration,))\n        self._const.scatter_add_(-1, coal_index, phylogeny.coal_binomial.log())\n        self._log = new_zeros(batch_shape + (duration,))\n        self._log.scatter_add_(-1, coal_index, new_ones(coal_index.shape))\n\n    def __call__(self, rate_grid, t=slice(None)):\n        """"""\n        Computes the likelihood of [1] equations 7-9 for one or all time\n        points.\n\n        **References**\n\n        [1] A. Popinga, T. Vaughan, T. Statler, A.J. Drummond (2014)\n            ""Inferring epidemiological dynamics with Bayesian coalescent\n            inference: The merits of deterministic and stochastic models""\n            https://arxiv.org/pdf/1407.1792.pdf\n\n        :param torch.Tensor rate_grid: Tensor of base coalescent rates\n            (pairwise rate of coalescence). For example in a simple SIR model\n            this might be ``beta S / I``. The rightmost dimension is time, and\n            this tensor represents a (batch of) rates that are piecwise\n            constant in time.\n        :param time: Optional time index by which the input was sliced, as in\n            ``rate_grid[..., t]`` This can be an integer for sequential models\n            or ``slice(None)`` for vectorized models.\n        :type time: int or slice\n        :returns: Likelihood ``p(coal_times | leaf_times, rate_grid)``,\n            or a part of that likelihood corresponding to a single time step.\n        :rtype: torch.Tensor\n        """"""\n        const = self._const[..., t]\n        linear = self._linear[..., t] * rate_grid\n        log = self._log[..., t] * rate_grid.clamp(min=torch.finfo(rate_grid.dtype).tiny).log()\n        return const + linear + log\n\n\ndef _gather(tensor, dim, index):\n    """"""\n    Like :func:`torch.gather` but broadcasts.\n    """"""\n    if dim != -1:\n        raise NotImplementedError\n    shape = broadcast_shape(tensor.shape[:-1], index.shape[:-1]) + (-1,)\n    tensor = tensor.expand(shape)\n    index = index.expand(shape)\n    return tensor.gather(dim, index)\n\n\ndef _interpolate_gather(array, x):\n    """"""\n    Like ``torch.gather(-1, array, x)`` but continuously indexes into the\n    rightmost dim of an array, linearly interpolating between array values.\n    """"""\n    with torch.no_grad():\n        x0 = x.floor().clamp(min=0, max=array.size(-1) - 2)\n        x1 = x0 + 1\n    f0 = _gather(array, -1, x0.long())\n    f1 = _gather(array, -1, x1.long())\n    return f0 * (x1 - x) + f1 * (x - x0)\n\n\ndef _interpolate_scatter_add_(dst, x, src):\n    """"""\n    Like ``dst.scatter_add_(-1, x, src)`` but continuously index into the\n    rightmost dim of an array, linearly interpolating between array values.\n    """"""\n    with torch.no_grad():\n        x0 = x.floor().clamp(min=0, max=dst.size(-1) - 2)\n        x1 = x0 + 1\n    dst.scatter_add_(-1, x0.long(), src * (x1 - x))\n    dst.scatter_add_(-1, x1.long(), src * (x - x0))\n    return dst\n\n\ndef _weak_memoize(fn):\n    cache = {}\n\n    @functools.wraps(fn)\n    def memoized_fn(*args):\n        key = tuple(map(id, args))\n        if key not in cache:\n            cache[key] = fn(*args)\n            for arg in args:\n                weakref.finalize(arg, cache.pop, key, None)\n        return cache[key]\n\n    return memoized_fn\n\n\n# This helper data structure has only timing information.\n_Phylogeny = namedtuple(""_Phylogeny"", (\n    ""times"", ""signs"", ""lineages"", ""binomial"", ""coal_binomial"",\n))\n\n\n@_weak_memoize\n@torch.no_grad()\ndef _make_phylogeny(leaf_times, coal_times):\n    assert leaf_times.size(-1) == 1 + coal_times.size(-1)\n    assert not leaf_times.requires_grad\n    assert not coal_times.requires_grad\n\n    # Expand shapes to match.\n    N = leaf_times.size(-1)\n    batch_shape = broadcast_shape(leaf_times.shape[:-1], coal_times.shape[:-1])\n    if leaf_times.shape[:-1] != batch_shape:\n        leaf_times = leaf_times.expand(batch_shape + (N,))\n    if coal_times.shape[:-1] != batch_shape:\n        coal_times = coal_times.expand(batch_shape + (N - 1,))\n\n    # Combine N sampling events (leaf_times) plus N-1 coalescent events\n    # (coal_times) into a pair (times, signs) of arrays of length 2N-1, where\n    # leaf sample sign is +1 and coalescent sign is -1.\n    times = torch.cat([coal_times, leaf_times], dim=-1)\n    signs = torch.linspace(1.5 - N, N - 0.5, 2 * N - 1).sign()  # e.g. [-1, -1, +1, +1, +1]\n\n    # Sort the events reverse-ordered in time, i.e. latest to earliest.\n    times, index = times.sort(dim=-1, descending=True)\n    signs = signs[index]\n    inv_index = index.new_empty(index.shape)\n    inv_index.scatter_(-1, index, torch.arange(2 * N - 1).expand_as(index))\n\n    # Compute the number n of lineages preceding each event, then the binomial\n    # coefficients that will multiply the base coalescence rate.\n    lineages = signs.cumsum(-1)\n    binomial = lineages * (lineages - 1) / 2\n\n    # Compute the binomial coefficient following each coalescent event.\n    coal_index = inv_index[..., :N - 1]\n    coal_binomial = binomial.gather(-1, coal_index - 1)\n\n    return _Phylogeny(times, signs, lineages, binomial, coal_binomial)\n\n\ndef _sample_coalescent_times(leaf_times):\n    leaf_times = leaf_times.detach()\n    N = leaf_times.size(-1)\n    batch_shape = leaf_times.shape[:-1]\n\n    # We don\'t bother to implement a version that vectorizes over batches;\n    # instead we simply sequentially sample and stack.\n    if batch_shape:\n        flat_leaf_times = leaf_times.reshape(-1, N)\n        flat_coal_times = torch.stack(list(map(_sample_coalescent_times, flat_leaf_times)))\n        return flat_coal_times.reshape(batch_shape + (N - 1,))\n    assert leaf_times.shape == (N,)\n\n    # Sequentially sample coalescent events from latest to earliest.\n    leaf_times = leaf_times.sort(dim=-1, descending=True).values.tolist()\n    coal_times = []\n    # Start with the minimum of two active leaves.\n    leaf = 1\n    t = leaf_times[leaf]\n    active = 2\n    binomial = active * (active - 1) / 2\n    for u in torch.empty(N - 1).exponential_().tolist():\n        while leaf + 1 < N and u > (t - leaf_times[leaf + 1]) * binomial:\n            # Move past the next leaf.\n            leaf += 1\n            u -= (t - leaf_times[leaf]) * binomial\n            t = leaf_times[leaf]\n            active += 1\n            binomial = active * (active - 1) / 2\n        # Add a coalescent event.\n        t -= u / binomial\n        active -= 1\n        binomial = active * (active - 1) / 2\n        coal_times.append(t)\n    coal_times.reverse()\n\n    return torch.tensor(coal_times)\n'"
pyro/distributions/conditional.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABC, abstractmethod\n\nimport torch\nimport torch.nn\n\nfrom .torch import TransformedDistribution\n\n\nclass ConditionalDistribution(ABC):\n    @abstractmethod\n    def condition(self, context):\n        """""":rtype: torch.distributions.Distribution""""""\n        raise NotImplementedError\n\n\nclass ConditionalTransform(ABC):\n    @abstractmethod\n    def condition(self, context):\n        """""":rtype: torch.distributions.Transform""""""\n        raise NotImplementedError\n\n\nclass ConditionalTransformModule(ConditionalTransform, torch.nn.Module):\n    """"""\n    Conditional transforms with learnable parameters such as normalizing flows should inherit from this class rather\n    than :class:`~pyro.distributions.conditional.ConditionalTransform` so they are also a subclass of\n    :class:`~torch.nn.Module` and inherit all the useful methods of that class.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def __hash__(self):\n        return super().__hash__()\n\n\nclass ConstantConditionalDistribution(ConditionalDistribution):\n    def __init__(self, base_dist):\n        assert isinstance(base_dist, torch.distributions.Distribution)\n        self.base_dist = base_dist\n\n    def condition(self, context):\n        return self.base_dist\n\n\nclass ConstantConditionalTransform(ConditionalTransform):\n    def __init__(self, transform):\n        assert isinstance(transform, torch.distributions.Transform)\n        self.transform = transform\n\n    def condition(self, context):\n        return self.transform\n\n\nclass ConditionalTransformedDistribution(ConditionalDistribution):\n    def __init__(self, base_dist, transforms):\n        self.base_dist = base_dist if isinstance(\n            base_dist, ConditionalDistribution) else ConstantConditionalDistribution(base_dist)\n        self.transforms = [t if isinstance(t, ConditionalTransform)\n                           else ConstantConditionalTransform(t) for t in transforms]\n\n    def condition(self, context):\n        base_dist = self.base_dist.condition(context)\n        transforms = [t.condition(context) for t in self.transforms]\n        return TransformedDistribution(base_dist, transforms)\n'"
pyro/distributions/conjugate.py,19,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numbers\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.utils import broadcast_all\n\nfrom pyro.distributions.torch import Beta, Binomial, Dirichlet, Gamma, Multinomial, Poisson\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.ops.special import log_beta, log_binomial\n\n\ndef _log_beta_1(alpha, value, is_sparse):\n    if is_sparse:\n        mask = (value != 0)\n        value, alpha, mask = torch.broadcast_tensors(value, alpha, mask)\n        result = torch.zeros_like(value)\n        value = value[mask]\n        alpha = alpha[mask]\n        result[mask] = torch.lgamma(1 + value) + torch.lgamma(alpha) - torch.lgamma(value + alpha)\n        return result\n    else:\n        return torch.lgamma(1 + value) + torch.lgamma(alpha) - torch.lgamma(value + alpha)\n\n\nclass BetaBinomial(TorchDistribution):\n    r""""""\n    Compound distribution comprising of a beta-binomial pair. The probability of\n    success (``probs`` for the :class:`~pyro.distributions.Binomial` distribution)\n    is unknown and randomly drawn from a :class:`~pyro.distributions.Beta` distribution\n    prior to a certain number of Bernoulli trials given by ``total_count``.\n\n    :param concentration1: 1st concentration parameter (alpha) for the\n        Beta distribution.\n    :type concentration1: float or torch.Tensor\n    :param concentration0: 2nd concentration parameter (beta) for the\n        Beta distribution.\n    :type concentration0: float or torch.Tensor\n    :param total_count: Number of Bernoulli trials.\n    :type total_count: float or torch.Tensor\n    """"""\n    arg_constraints = {\'concentration1\': constraints.positive, \'concentration0\': constraints.positive,\n                       \'total_count\': constraints.nonnegative_integer}\n    has_enumerate_support = True\n    support = Binomial.support\n\n    # EXPERIMENTAL If set to a positive value, the .log_prob() method will use\n    # a shifted Sterling\'s approximation to the Beta function, reducing\n    # computational cost from 9 lgamma() evaluations to 12 log() evaluations\n    # plus arithmetic. Recommended values are between 0.1 and 0.01.\n    approx_log_prob_tol = 0.\n\n    def __init__(self, concentration1, concentration0, total_count=1, validate_args=None):\n        concentration1, concentration0, total_count = broadcast_all(\n            concentration1, concentration0, total_count)\n        self._beta = Beta(concentration1, concentration0)\n        self.total_count = total_count\n        super().__init__(self._beta._batch_shape, validate_args=validate_args)\n\n    @property\n    def concentration1(self):\n        return self._beta.concentration1\n\n    @property\n    def concentration0(self):\n        return self._beta.concentration0\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(BetaBinomial, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new._beta = self._beta.expand(batch_shape)\n        new.total_count = self.total_count.expand_as(new._beta.concentration0)\n        super(BetaBinomial, new).__init__(batch_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def sample(self, sample_shape=()):\n        probs = self._beta.sample(sample_shape)\n        return Binomial(self.total_count, probs, validate_args=False).sample()\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n\n        n = self.total_count\n        k = value\n        a = self.concentration1\n        b = self.concentration0\n        tol = self.approx_log_prob_tol\n        return log_binomial(n, k, tol) + log_beta(k + a, n - k + b, tol) - log_beta(a, b, tol)\n\n    @property\n    def mean(self):\n        return self._beta.mean * self.total_count\n\n    @property\n    def variance(self):\n        return self._beta.variance * self.total_count * (self.concentration0 + self.concentration1 + self.total_count)\n\n    def enumerate_support(self, expand=True):\n        total_count = int(self.total_count.max())\n        if not self.total_count.min() == total_count:\n            raise NotImplementedError(""Inhomogeneous total count not supported by `enumerate_support`."")\n        values = torch.arange(1 + total_count, dtype=self.concentration1.dtype, device=self.concentration1.device)\n        values = values.view((-1,) + (1,) * len(self._batch_shape))\n        if expand:\n            values = values.expand((-1,) + self._batch_shape)\n        return values\n\n\nclass DirichletMultinomial(TorchDistribution):\n    r""""""\n    Compound distribution comprising of a dirichlet-multinomial pair. The probability of\n    classes (``probs`` for the :class:`~pyro.distributions.Multinomial` distribution)\n    is unknown and randomly drawn from a :class:`~pyro.distributions.Dirichlet`\n    distribution prior to a certain number of Categorical trials given by\n    ``total_count``.\n\n    :param float or torch.Tensor concentration: concentration parameter (alpha) for the\n        Dirichlet distribution.\n    :param int or torch.Tensor total_count: number of Categorical trials.\n    :param bool is_sparse: Whether to assume value is mostly zero when computing\n        :meth:`log_prob`, which can speed up computation when data is sparse.\n    """"""\n    arg_constraints = {\'concentration\': constraints.positive, \'total_count\': constraints.nonnegative_integer}\n    support = Multinomial.support\n\n    def __init__(self, concentration, total_count=1, is_sparse=False, validate_args=None):\n        if isinstance(total_count, numbers.Number):\n            total_count = torch.tensor(total_count, dtype=concentration.dtype, device=concentration.device)\n        total_count_1 = total_count.unsqueeze(-1)\n        concentration, total_count = torch.broadcast_tensors(concentration, total_count_1)\n        total_count = total_count_1.squeeze(-1)\n        self._dirichlet = Dirichlet(concentration)\n        self.total_count = total_count\n        self.is_sparse = is_sparse\n        super().__init__(\n            self._dirichlet._batch_shape, self._dirichlet.event_shape, validate_args=validate_args)\n\n    @property\n    def concentration(self):\n        return self._dirichlet.concentration\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(DirichletMultinomial, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new._dirichlet = self._dirichlet.expand(batch_shape)\n        new.total_count = self.total_count.expand(batch_shape)\n        new.is_sparse = self.is_sparse\n        super(DirichletMultinomial, new).__init__(\n            new._dirichlet.batch_shape, new._dirichlet.event_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def sample(self, sample_shape=()):\n        probs = self._dirichlet.sample(sample_shape)\n        total_count = int(self.total_count.max())\n        if not self.total_count.min() == total_count:\n            raise NotImplementedError(""Inhomogeneous total count not supported by `sample`."")\n        return Multinomial(total_count, probs).sample()\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        alpha = self.concentration\n        return (_log_beta_1(alpha.sum(-1), value.sum(-1), self.is_sparse) -\n                _log_beta_1(alpha, value, self.is_sparse).sum(-1))\n\n    @property\n    def mean(self):\n        return self._dirichlet.mean * self.total_count.unsqueeze(-1)\n\n    @property\n    def variance(self):\n        n = self.total_count.unsqueeze(-1)\n        alpha = self.concentration\n        alpha_sum = self.concentration.sum(-1, keepdim=True)\n        alpha_ratio = alpha / alpha_sum\n        return n * alpha_ratio * (1 - alpha_ratio) * (n + alpha_sum) / (1 + alpha_sum)\n\n\nclass GammaPoisson(TorchDistribution):\n    r""""""\n    Compound distribution comprising of a gamma-poisson pair, also referred to as\n    a gamma-poisson mixture. The ``rate`` parameter for the\n    :class:`~pyro.distributions.Poisson` distribution is unknown and randomly\n    drawn from a :class:`~pyro.distributions.Gamma` distribution.\n\n    .. note:: This can be treated as an alternate parametrization of the\n        :class:`~pyro.distributions.NegativeBinomial` (``total_count``, ``probs``)\n        distribution, with `concentration = total_count` and `rate = (1 - probs) / probs`.\n\n    :param float or torch.Tensor concentration: shape parameter (alpha) of the Gamma\n        distribution.\n    :param float or torch.Tensor rate: rate parameter (beta) for the Gamma\n        distribution.\n    """"""\n\n    arg_constraints = {\'concentration\': constraints.positive, \'rate\': constraints.positive}\n    support = Poisson.support\n\n    def __init__(self, concentration, rate, validate_args=None):\n        concentration, rate = broadcast_all(concentration, rate)\n        self._gamma = Gamma(concentration, rate)\n        super().__init__(self._gamma._batch_shape, validate_args=validate_args)\n\n    @property\n    def concentration(self):\n        return self._gamma.concentration\n\n    @property\n    def rate(self):\n        return self._gamma.rate\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(GammaPoisson, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new._gamma = self._gamma.expand(batch_shape)\n        super(GammaPoisson, new).__init__(batch_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def sample(self, sample_shape=()):\n        rate = self._gamma.sample(sample_shape)\n        return Poisson(rate).sample()\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        post_value = self.concentration + value\n        return -log_beta(self.concentration, value + 1) - post_value.log() + \\\n            self.concentration * self.rate.log() - post_value * (1 + self.rate).log()\n\n    @property\n    def mean(self):\n        return self.concentration / self.rate\n\n    @property\n    def variance(self):\n        return self.concentration / self.rate.pow(2) * (1 + self.rate)\n'"
pyro/distributions/constraints.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch.distributions.constraints import *  # noqa F403\nfrom torch.distributions.constraints import Constraint\nfrom torch.distributions.constraints import __all__ as torch_constraints\nfrom torch.distributions.constraints import lower_cholesky\n\n\n# TODO move this upstream to torch.distributions\nclass IndependentConstraint(Constraint):\n    """"""\n    Wraps a constraint by aggregating over ``reinterpreted_batch_ndims``-many\n    dims in :meth:`check`, so that an event is valid only if all its\n    independent entries are valid.\n\n    :param torch.distributions.constraints.Constraint base_constraint: A base\n        constraint whose entries are incidentally independent.\n    :param int reinterpreted_batch_ndims: The number of extra event dimensions that will\n        be considered dependent.\n    """"""\n    def __init__(self, base_constraint, reinterpreted_batch_ndims):\n        self.base_constraint = base_constraint\n        self.reinterpreted_batch_ndims = reinterpreted_batch_ndims\n\n    def check(self, value):\n        result = self.base_constraint.check(value)\n        result = result.reshape(result.shape[:result.dim() - self.reinterpreted_batch_ndims] + (-1,))\n        result = result.min(-1)[0]\n        return result\n\n\n# TODO move this upstream to torch.distributions\nclass _Integer(Constraint):\n    """"""\n    Constrain to integers.\n    """"""\n    def check(self, value):\n        return value % 1 == 0\n\n    def __repr__(self):\n        return self.__class__.__name__[1:]\n\n\nclass _CorrCholesky(Constraint):\n    """"""\n    Constrains to lower-triangular square matrices with positive diagonals and\n    Euclidean norm of each row is 1, such that `torch.mm(omega, omega.t())` will\n    have unit diagonal.\n    """"""\n\n    def check(self, value):\n        unit_norm_row = (value.norm(dim=-1).sub(1) < 1e-4).min(-1)[0]\n        return lower_cholesky.check(value) & unit_norm_row\n\n\ncorr_cholesky_constraint = _CorrCholesky()\ninteger = _Integer()\n\n__all__ = [\n    \'IndependentConstraint\',\n    \'corr_cholesky_constraint\',\n    \'integer\',\n]\n\n__all__.extend(torch_constraints)\ndel torch_constraints\n'"
pyro/distributions/delta.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numbers\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import sum_rightmost\n\n\nclass Delta(TorchDistribution):\n    """"""\n    Degenerate discrete distribution (a single point).\n\n    Discrete distribution that assigns probability one to the single element in\n    its support. Delta distribution parameterized by a random choice should not\n    be used with MCMC based inference, as doing so produces incorrect results.\n\n    :param torch.Tensor v: The single support element.\n    :param torch.Tensor log_density: An optional density for this Delta. This\n        is useful to keep the class of :class:`Delta` distributions closed\n        under differentiable transformation.\n    :param int event_dim: Optional event dimension, defaults to zero.\n    """"""\n    has_rsample = True\n    arg_constraints = {\'v\': constraints.real, \'log_density\': constraints.real}\n    support = constraints.real\n\n    def __init__(self, v, log_density=0.0, event_dim=0, validate_args=None):\n        if event_dim > v.dim():\n            raise ValueError(\'Expected event_dim <= v.dim(), actual {} vs {}\'.format(event_dim, v.dim()))\n        batch_dim = v.dim() - event_dim\n        batch_shape = v.shape[:batch_dim]\n        event_shape = v.shape[batch_dim:]\n        if isinstance(log_density, numbers.Number):\n            log_density = torch.full(batch_shape, log_density, dtype=v.dtype, device=v.device)\n        elif validate_args and log_density.shape != batch_shape:\n            raise ValueError(\'Expected log_density.shape = {}, actual {}\'.format(\n                log_density.shape, batch_shape))\n        self.v = v\n        self.log_density = log_density\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(Delta, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new.v = self.v.expand(batch_shape + self.event_shape)\n        new.log_density = self.log_density.expand(batch_shape)\n        super(Delta, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def rsample(self, sample_shape=torch.Size()):\n        shape = sample_shape + self.v.shape\n        return self.v.expand(shape)\n\n    def log_prob(self, x):\n        v = self.v.expand(self.shape())\n        log_prob = (x == v).type(x.dtype).log()\n        log_prob = sum_rightmost(log_prob, self.event_dim)\n        return log_prob + self.log_density\n\n    @property\n    def mean(self):\n        return self.v\n\n    @property\n    def variance(self):\n        return torch.zeros_like(self.v)\n'"
pyro/distributions/diag_normal_mixture.py,37,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.distributions import constraints, Categorical\n\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import sum_leftmost\n\n\nclass MixtureOfDiagNormals(TorchDistribution):\n    """"""\n    Mixture of Normal distributions with arbitrary means and arbitrary\n    diagonal covariance matrices.\n\n    That is, this distribution is a mixture with K components, where each\n    component distribution is a D-dimensional Normal distribution with a\n    D-dimensional mean parameter and a D-dimensional diagonal covariance\n    matrix. The K different component means are gathered into the K x D\n    dimensional parameter `locs` and the K different scale parameters are\n    gathered into the K x D dimensional parameter `coord_scale`. The mixture\n    weights are controlled by a K-dimensional vector of softmax logits,\n    `component_logits`. This distribution implements pathwise derivatives\n    for samples from the distribution.\n\n    See reference [1] for details on the implementations of the pathwise\n    derivative. Please consider citing this reference if you use the pathwise\n    derivative in your research. Note that this distribution does not support\n    dimension D = 1.\n\n    [1] Pathwise Derivatives for Multivariate Distributions, Martin Jankowiak &\n    Theofanis Karaletsos. arXiv:1806.01856\n\n    :param torch.Tensor locs: K x D mean matrix\n    :param torch.Tensor coord_scale: K x D scale matrix\n    :param torch.Tensor component_logits: K-dimensional vector of softmax logits\n    """"""\n    has_rsample = True\n    arg_constraints = {""locs"": constraints.real, ""coord_scale"": constraints.positive,\n                       ""component_logits"": constraints.real}\n\n    def __init__(self, locs, coord_scale, component_logits):\n        self.batch_mode = (locs.dim() > 2)\n        assert(coord_scale.shape == locs.shape)\n        assert(self.batch_mode or locs.dim() == 2), \\\n            ""The locs parameter in MixtureOfDiagNormals should be K x D dimensional (or B x K x D if doing batches)""\n        if not self.batch_mode:\n            assert(coord_scale.dim() == 2), \\\n                ""The coord_scale parameter in MixtureOfDiagNormals should be K x D dimensional""\n            assert(component_logits.dim() == 1), \\\n                ""The component_logits parameter in MixtureOfDiagNormals should be K dimensional""\n            assert(component_logits.size(-1) == locs.size(-2))\n            batch_shape = ()\n        else:\n            assert(coord_scale.dim() > 2), \\\n                ""The coord_scale parameter in MixtureOfDiagNormals should be B x K x D dimensional""\n            assert(component_logits.dim() > 1), \\\n                ""The component_logits parameter in MixtureOfDiagNormals should be B x K dimensional""\n            assert(component_logits.size(-1) == locs.size(-2))\n            batch_shape = tuple(locs.shape[:-2])\n\n        self.locs = locs\n        self.coord_scale = coord_scale\n        self.component_logits = component_logits\n        self.dim = locs.size(-1)\n        self.categorical = Categorical(logits=component_logits)\n        self.probs = self.categorical.probs\n        super().__init__(batch_shape=torch.Size(batch_shape),\n                         event_shape=torch.Size((self.dim,)))\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(MixtureOfDiagNormals, _instance)\n        new.batch_mode = True\n        batch_shape = torch.Size(batch_shape)\n        new.dim = self.dim\n        new.locs = self.locs.expand(batch_shape + self.locs.shape[-2:])\n        new.coord_scale = self.coord_scale.expand(batch_shape + self.coord_scale.shape[-2:])\n        new.component_logits = self.component_logits.expand(batch_shape + self.component_logits.shape[-1:])\n        new.categorical = self.categorical.expand(batch_shape)\n        new.probs = self.probs.expand(batch_shape + self.probs.shape[-1:])\n        super(MixtureOfDiagNormals, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def log_prob(self, value):\n        epsilon = (value.unsqueeze(-2) - self.locs) / self.coord_scale  # L B K D\n        eps_sqr = 0.5 * torch.pow(epsilon, 2.0).sum(-1)  # L B K\n        eps_sqr_min = torch.min(eps_sqr, -1)[0]  # L B K\n        coord_scale_prod_log_sum = self.coord_scale.log().sum(-1)  # B K\n        result = self.categorical.logits + (-eps_sqr + eps_sqr_min.unsqueeze(-1)) - coord_scale_prod_log_sum  # L B K\n        result = torch.logsumexp(result, dim=-1)  # L B\n        result = result - 0.5 * math.log(2.0 * math.pi) * float(self.dim)\n        result = result - eps_sqr_min\n        return result\n\n    def rsample(self, sample_shape=torch.Size()):\n        which = self.categorical.sample(sample_shape)\n        return _MixDiagNormalSample.apply(self.locs, self.coord_scale,\n                                          self.component_logits, self.categorical.probs, which,\n                                          sample_shape + self.locs.shape[:-2] + (self.dim,))\n\n\nclass _MixDiagNormalSample(Function):\n    @staticmethod\n    def forward(ctx, locs, scales, component_logits, pis, which, noise_shape):\n        dim = scales.size(-1)\n        white = locs.new(noise_shape).normal_()\n        n_unsqueezes = locs.dim() - which.dim()\n        for _ in range(n_unsqueezes):\n            which = which.unsqueeze(-1)\n        which_expand = which.expand(tuple(which.shape[:-1] + (dim,)))\n        loc = torch.gather(locs, -2, which_expand).squeeze(-2)\n        sigma = torch.gather(scales, -2, which_expand).squeeze(-2)\n        z = loc + sigma * white\n        ctx.save_for_backward(z, scales, locs, component_logits, pis)\n        return z\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n\n        z, scales, locs, logits, pis = ctx.saved_tensors\n        dim = scales.size(-1)\n        K = logits.size(-1)\n        g = grad_output  # l b i\n        g = g.unsqueeze(-2)  # l b 1 i\n        batch_dims = locs.dim() - 2\n\n        locs_tilde = locs / scales  # b j i\n        sigma_0 = torch.min(scales, -2, keepdim=True)[0]  # b 1 i\n        z_shift = (z.unsqueeze(-2) - locs) / sigma_0  # l b j i\n        z_tilde = z.unsqueeze(-2) / scales - locs_tilde  # l b j i\n\n        mu_cd = locs.unsqueeze(-2) - locs.unsqueeze(-3)  # b c d i\n        mu_cd_norm = torch.pow(mu_cd, 2.0).sum(-1).sqrt()  # b c d\n        mu_cd /= mu_cd_norm.unsqueeze(-1)  # b c d i\n        diagonals = torch.empty((K,), dtype=torch.long, device=z.device)\n        torch.arange(K, out=diagonals)\n        mu_cd[..., diagonals, diagonals, :] = 0.0\n\n        mu_ll_cd = (locs.unsqueeze(-2) * mu_cd).sum(-1)  # b c d\n        z_ll_cd = (z.unsqueeze(-2).unsqueeze(-2) * mu_cd).sum(-1)  # l b c d\n        z_perp_cd = z.unsqueeze(-2).unsqueeze(-2) - z_ll_cd.unsqueeze(-1) * mu_cd  # l b c d i\n        z_perp_cd_sqr = torch.pow(z_perp_cd, 2.0).sum(-1)  # l b c d\n\n        shift_indices = torch.empty((dim,), dtype=torch.long, device=z.device)\n        torch.arange(dim, out=shift_indices)\n        shift_indices = shift_indices - 1\n        shift_indices[0] = 0\n\n        z_shift_cumsum = torch.pow(z_shift, 2.0)\n        z_shift_cumsum = z_shift_cumsum.sum(-1, keepdim=True) - torch.cumsum(z_shift_cumsum, dim=-1)  # l b j i\n        z_tilde_cumsum = torch.cumsum(torch.pow(z_tilde, 2.0), dim=-1)  # l b j i\n        z_tilde_cumsum = torch.index_select(z_tilde_cumsum, -1, shift_indices)\n        z_tilde_cumsum[..., 0] = 0.0\n        r_sqr_ji = z_shift_cumsum + z_tilde_cumsum  # l b j i\n\n        log_scales = torch.log(scales)  # b j i\n        epsilons_sqr = torch.pow(z_tilde, 2.0)  # l b j i\n        log_qs = -0.5 * epsilons_sqr - 0.5 * math.log(2.0 * math.pi) - log_scales  # l b j i\n        log_q_j = log_qs.sum(-1, keepdim=True)  # l b j 1\n        q_j = torch.exp(log_q_j)  # l b j 1\n        q_tot = (pis * q_j.squeeze(-1)).sum(-1)  # l b\n        q_tot = q_tot.unsqueeze(-1)  # l b 1\n\n        root_two = math.sqrt(2.0)\n        shift_log_scales = log_scales[..., shift_indices]\n        shift_log_scales[..., 0] = 0.0\n        sigma_products = torch.cumsum(shift_log_scales, dim=-1).exp()  # b j i\n\n        reverse_indices = torch.tensor(range(dim - 1, -1, -1), dtype=torch.long, device=z.device)\n        reverse_log_sigma_0 = sigma_0.log()[..., reverse_indices]  # b 1 i\n        sigma_0_products = torch.cumsum(reverse_log_sigma_0, dim=-1).exp()[..., reverse_indices - 1]  # b 1 i\n        sigma_0_products[..., -1] = 1.0\n        sigma_products *= sigma_0_products\n\n        logits_grad = torch.erf(z_tilde / root_two) - torch.erf(z_shift / root_two)  # l b j i\n        logits_grad *= torch.exp(-0.5 * r_sqr_ji)  # l b j i\n        logits_grad = (logits_grad * g / sigma_products).sum(-1)  # l b j\n        logits_grad = sum_leftmost(logits_grad / q_tot, -1 - batch_dims)  # b j\n        logits_grad *= 0.5 * math.pow(2.0 * math.pi, -0.5 * (dim - 1))\n        logits_grad = -pis * logits_grad\n        logits_grad = logits_grad - logits_grad.sum(-1, keepdim=True) * pis\n\n        mu_ll_dc = torch.transpose(mu_ll_cd, -1, -2)\n        v_cd = torch.erf((z_ll_cd - mu_ll_cd) / root_two) - torch.erf((z_ll_cd + mu_ll_dc) / root_two)\n        v_cd *= torch.exp(-0.5 * z_perp_cd_sqr)  # l b c d\n        mu_cd_g = (g.unsqueeze(-2) * mu_cd).sum(-1)  # l b c d\n        v_cd *= -mu_cd_g * pis.unsqueeze(-2) * 0.5 * math.pow(2.0 * math.pi, -0.5 * (dim - 1))  # l b c d\n        v_cd = pis * sum_leftmost(v_cd.sum(-1) / q_tot, -1 - batch_dims)\n        logits_grad += v_cd\n\n        prefactor = pis.unsqueeze(-1) * q_j * g / q_tot.unsqueeze(-1)\n        locs_grad = sum_leftmost(prefactor, -2 - batch_dims)\n        scales_grad = sum_leftmost(prefactor * z_tilde, -2 - batch_dims)\n\n        return locs_grad, scales_grad, logits_grad, None, None, None\n'"
pyro/distributions/diag_normal_mixture_shared_cov.py,25,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.distributions import Categorical, constraints\n\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import sum_leftmost\n\n\nclass MixtureOfDiagNormalsSharedCovariance(TorchDistribution):\n    """"""\n    Mixture of Normal distributions with diagonal covariance matrices.\n\n    That is, this distribution is a mixture with K components, where each\n    component distribution is a D-dimensional Normal distribution with a\n    D-dimensional mean parameter loc and a D-dimensional diagonal covariance\n    matrix specified by a scale parameter `coord_scale`. The K different\n    component means are gathered into the parameter `locs` and the scale\n    parameter is shared between all K components. The mixture weights are\n    controlled by a K-dimensional vector of softmax logits, `component_logits`.\n    This distribution implements pathwise derivatives for samples from the\n    distribution.\n\n    See reference [1] for details on the implementations of the pathwise\n    derivative. Please consider citing this reference if you use the pathwise\n    derivative in your research. Note that this distribution does not support\n    dimension D = 1.\n\n    [1] Pathwise Derivatives for Multivariate Distributions, Martin Jankowiak &\n    Theofanis Karaletsos. arXiv:1806.01856\n\n    :param torch.Tensor locs: K x D mean matrix\n    :param torch.Tensor coord_scale: shared D-dimensional scale vector\n    :param torch.Tensor component_logits: K-dimensional vector of softmax logits\n    """"""\n    has_rsample = True\n    arg_constraints = {""locs"": constraints.real, ""coord_scale"": constraints.positive,\n                       ""component_logits"": constraints.real}\n\n    def __init__(self, locs, coord_scale, component_logits):\n        self.batch_mode = (locs.dim() > 2)\n        assert(self.batch_mode or locs.dim() == 2), \\\n            ""The locs parameter in MixtureOfDiagNormals should be K x D dimensional (or ... x B x K x D in batch mode)""\n        if not self.batch_mode:\n            assert(coord_scale.dim() == 1), ""The coord_scale parameter in MixtureOfDiagNormals should be D dimensional""\n            assert(component_logits.dim() == 1), \\\n                ""The component_logits parameter in MixtureOfDiagNormals should be K dimensional""\n            assert(component_logits.size(0) == locs.size(0))\n            batch_shape = ()\n        else:\n            assert(coord_scale.dim() > 1), \\\n                ""The coord_scale parameter in MixtureOfDiagNormals should be ... x B x D dimensional""\n            assert(component_logits.dim() > 1), \\\n                ""The component_logits parameter in MixtureOfDiagNormals should be ... x B x K dimensional""\n            assert(component_logits.size(-1) == locs.size(-2))\n            batch_shape = tuple(locs.shape[:-2])\n        self.locs = locs\n        self.coord_scale = coord_scale\n        self.component_logits = component_logits\n        self.dim = locs.size(-1)\n        if self.dim < 2:\n            raise NotImplementedError(\'This distribution does not support D = 1\')\n        self.categorical = Categorical(logits=component_logits)\n        self.probs = self.categorical.probs\n        super().__init__(batch_shape=batch_shape, event_shape=(self.dim,))\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(MixtureOfDiagNormalsSharedCovariance, _instance)\n        new.batch_mode = True\n        batch_shape = torch.Size(batch_shape)\n        new.dim = self.dim\n        new.locs = self.locs.expand(batch_shape + self.locs.shape[-2:])\n        coord_scale_shape = -1 if self.batch_mode else -2\n        new.coord_scale = self.coord_scale.expand(batch_shape + self.coord_scale.shape[coord_scale_shape:])\n        new.component_logits = self.component_logits.expand(batch_shape + self.component_logits.shape[-1:])\n        new.categorical = self.categorical.expand(batch_shape)\n        new.probs = self.probs.expand(batch_shape + self.probs.shape[-1:])\n        super(MixtureOfDiagNormalsSharedCovariance, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def log_prob(self, value):\n        coord_scale = self.coord_scale.unsqueeze(-2) if self.batch_mode else self.coord_scale\n        epsilon = (value.unsqueeze(-2) - self.locs) / coord_scale  # L B K D\n        eps_sqr = 0.5 * torch.pow(epsilon, 2.0).sum(-1)  # L B K\n        eps_sqr_min = torch.min(eps_sqr, -1)[0]  # L B\n        result = self.categorical.logits + (-eps_sqr + eps_sqr_min.unsqueeze(-1))  # L B K\n        result = torch.logsumexp(result, dim=-1)  # L B\n        result = result - (0.5 * math.log(2.0 * math.pi) * float(self.dim))\n        result = result - (torch.log(self.coord_scale).sum(-1))\n        result = result - eps_sqr_min\n        return result\n\n    def rsample(self, sample_shape=torch.Size()):\n        which = self.categorical.sample(sample_shape)\n        return _MixDiagNormalSharedCovarianceSample.apply(self.locs, self.coord_scale, self.component_logits,\n                                                          self.probs, which, sample_shape + self.coord_scale.shape)\n\n\nclass _MixDiagNormalSharedCovarianceSample(Function):\n    @staticmethod\n    def forward(ctx, locs, coord_scale, component_logits, pis, which, noise_shape):\n        dim = coord_scale.size(-1)\n        white = torch.randn(noise_shape, dtype=locs.dtype, device=locs.device)\n        n_unsqueezes = locs.dim() - which.dim()\n        for _ in range(n_unsqueezes):\n            which = which.unsqueeze(-1)\n        expand_tuple = tuple(which.shape[:-1] + (dim,))\n        loc = torch.gather(locs, -2, which.expand(expand_tuple)).squeeze(-2)\n        z = loc + coord_scale * white\n        ctx.save_for_backward(z, coord_scale, locs, component_logits, pis)\n        return z\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n\n        z, coord_scale, locs, component_logits, pis = ctx.saved_tensors\n        K = component_logits.size(-1)\n        batch_dims = coord_scale.dim() - 1\n        g = grad_output  # l b i\n\n        z_tilde = z / coord_scale  # l b i\n        locs_tilde = locs / coord_scale.unsqueeze(-2)  # b j i\n        mu_ab = locs_tilde.unsqueeze(-2) - locs_tilde.unsqueeze(-3)  # b k j i\n        mu_ab_norm = torch.pow(mu_ab, 2.0).sum(-1).sqrt()  # b k j\n        mu_ab /= mu_ab_norm.unsqueeze(-1)  # b k j i\n        diagonals = torch.empty((K,), dtype=torch.long, device=z.device)\n        torch.arange(K, out=diagonals)\n        mu_ab[..., diagonals, diagonals, :] = 0.0\n\n        mu_ll_ab = (locs_tilde.unsqueeze(-2) * mu_ab).sum(-1)  # b k j\n        z_ll_ab = (z_tilde.unsqueeze(-2).unsqueeze(-2) * mu_ab).sum(-1)  # l b k j\n        z_perp_ab = z_tilde.unsqueeze(-2).unsqueeze(-2) - z_ll_ab.unsqueeze(-1) * mu_ab  # l b k j i\n        z_perp_ab_sqr = torch.pow(z_perp_ab, 2.0).sum(-1)  # l b k j\n\n        epsilons = z_tilde.unsqueeze(-2) - locs_tilde  # l b j i\n        log_qs = -0.5 * torch.pow(epsilons, 2.0)   # l b j i\n        log_q_j = log_qs.sum(-1, keepdim=True)     # l b j 1\n        log_q_j_max = torch.max(log_q_j, -2, keepdim=True)[0]\n        q_j_prime = torch.exp(log_q_j - log_q_j_max)  # l b j 1\n        q_j = torch.exp(log_q_j)  # l b j 1\n\n        q_tot = (pis.unsqueeze(-1) * q_j).sum(-2)  # l b 1\n        q_tot_prime = (pis.unsqueeze(-1) * q_j_prime).sum(-2).unsqueeze(-1)  # l b 1 1\n\n        root_two = math.sqrt(2.0)\n        mu_ll_ba = torch.transpose(mu_ll_ab, -1, -2)\n        logits_grad = torch.erf((z_ll_ab - mu_ll_ab) / root_two) - torch.erf((z_ll_ab + mu_ll_ba) / root_two)\n        logits_grad *= torch.exp(-0.5 * z_perp_ab_sqr)  # l b k j\n\n        #                 bi      lbi                               bkji\n        mu_ab_sigma_g = ((coord_scale * g).unsqueeze(-2).unsqueeze(-2) * mu_ab).sum(-1)  # l b k j\n        logits_grad *= -mu_ab_sigma_g * pis.unsqueeze(-2)  # l b k j\n        logits_grad = pis * sum_leftmost(logits_grad.sum(-1) / q_tot, -(1 + batch_dims))  # b k\n        logits_grad *= math.sqrt(0.5 * math.pi)\n\n        #           b j                 l b j 1   l b i             l b 1 1\n        prefactor = pis.unsqueeze(-1) * q_j_prime * g.unsqueeze(-2) / q_tot_prime  # l b j i\n        locs_grad = sum_leftmost(prefactor, -(2 + batch_dims))  # b j i\n        coord_scale_grad = sum_leftmost(prefactor * epsilons, -(2 + batch_dims)).sum(-2)  # b i\n\n        return locs_grad, coord_scale_grad, logits_grad, None, None, None\n'"
pyro/distributions/distribution.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABCMeta, abstractmethod\n\nfrom pyro.distributions.score_parts import ScoreParts\n\n\nclass Distribution(object, metaclass=ABCMeta):\n    """"""\n    Base class for parameterized probability distributions.\n\n    Distributions in Pyro are stochastic function objects with :meth:`sample` and\n    :meth:`log_prob` methods. Distribution are stochastic functions with fixed\n    parameters::\n\n      d = dist.Bernoulli(param)\n      x = d()                                # Draws a random sample.\n      p = d.log_prob(x)                      # Evaluates log probability of x.\n\n    **Implementing New Distributions**:\n\n    Derived classes must implement the methods: :meth:`sample`,\n    :meth:`log_prob`.\n\n    **Examples**:\n\n    Take a look at the `examples <http://pyro.ai/examples>`_ to see how they interact\n    with inference algorithms.\n    """"""\n    has_rsample = False\n    has_enumerate_support = False\n\n    def __call__(self, *args, **kwargs):\n        """"""\n        Samples a random value (just an alias for ``.sample(*args, **kwargs)``).\n\n        For tensor distributions, the returned tensor should have the same ``.shape`` as the\n        parameters.\n\n        :return: A random value.\n        :rtype: torch.Tensor\n        """"""\n        return self.sample(*args, **kwargs)\n\n    @abstractmethod\n    def sample(self, *args, **kwargs):\n        """"""\n        Samples a random value.\n\n        For tensor distributions, the returned tensor should have the same ``.shape`` as the\n        parameters, unless otherwise noted.\n\n        :param sample_shape: the size of the iid batch to be drawn from the\n            distribution.\n        :type sample_shape: torch.Size\n        :return: A random value or batch of random values (if parameters are\n            batched). The shape of the result should be ``self.shape()``.\n        :rtype: torch.Tensor\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def log_prob(self, x, *args, **kwargs):\n        """"""\n        Evaluates log probability densities for each of a batch of samples.\n\n        :param torch.Tensor x: A single value or a batch of values\n            batched along axis 0.\n        :return: log probability densities as a one-dimensional\n            :class:`~torch.Tensor` with same batch size as value and\n            params. The shape of the result should be ``self.batch_size``.\n        :rtype: torch.Tensor\n        """"""\n        raise NotImplementedError\n\n    def score_parts(self, x, *args, **kwargs):\n        """"""\n        Computes ingredients for stochastic gradient estimators of ELBO.\n\n        The default implementation is correct both for non-reparameterized and\n        for fully reparameterized distributions. Partially reparameterized\n        distributions should override this method to compute correct\n        `.score_function` and `.entropy_term` parts.\n\n        Setting ``.has_rsample`` on a distribution instance will determine\n        whether inference engines like :class:`~pyro.infer.svi.SVI` use\n        reparameterized samplers or the score function estimator.\n\n        :param torch.Tensor x: A single value or batch of values.\n        :return: A `ScoreParts` object containing parts of the ELBO estimator.\n        :rtype: ScoreParts\n        """"""\n        log_prob = self.log_prob(x, *args, **kwargs)\n        if self.has_rsample:\n            return ScoreParts(log_prob=log_prob, score_function=0, entropy_term=log_prob)\n        else:\n            # XXX should the user be able to control inclusion of the entropy term?\n            # See Roeder, Wu, Duvenaud (2017) ""Sticking the Landing"" https://arxiv.org/abs/1703.09194\n            return ScoreParts(log_prob=log_prob, score_function=log_prob, entropy_term=0)\n\n    def enumerate_support(self, expand=True):\n        """"""\n        Returns a representation of the parametrized distribution\'s support,\n        along the first dimension. This is implemented only by discrete\n        distributions.\n\n        Note that this returns support values of all the batched RVs in\n        lock-step, rather than the full cartesian product.\n\n        :param bool expand: whether to expand the result to a tensor of shape\n            ``(n,) + batch_shape + event_shape``. If false, the return value\n            has unexpanded shape ``(n,) + (1,)*len(batch_shape) + event_shape``\n            which can be broadcasted to the full shape.\n        :return: An iterator over the distribution\'s discrete support.\n        :rtype: iterator\n        """"""\n        raise NotImplementedError(""Support not implemented for {}"".format(type(self).__name__))\n\n    def conjugate_update(self, other):\n        """"""\n        EXPERIMENTAL Creates an updated distribution fusing information from\n        another compatible distribution. This is supported by only a few\n        conjugate distributions.\n\n        This should satisfy the equation::\n\n            fg, log_normalizer = f.conjugate_update(g)\n            assert f.log_prob(x) + g.log_prob(x) == fg.log_prob(x) + log_normalizer\n\n        Note this is equivalent to :obj:`funsor.ops.add` on\n        :class:`~funsor.terms.Funsor` distributions, but we return a lazy sum\n        ``(updated, log_normalizer)`` because PyTorch distributions must be\n        normalized.  Thus :meth:`conjugate_update` should commute with\n        :func:`~funsor.pyro.convert.dist_to_funsor` and\n        :func:`~funsor.pyro.convert.tensor_to_funsor` ::\n\n            dist_to_funsor(f) + dist_to_funsor(g)\n              == dist_to_funsor(fg) + tensor_to_funsor(log_normalizer)\n\n        :param other: A distribution representing ``p(data|latent)`` but\n            normalized over ``latent`` rather than ``data``. Here ``latent``\n            is a candidate sample from ``self`` and ``data`` is a ground\n            observation of unrelated type.\n        :return: a pair ``(updated,log_normalizer)`` where ``updated`` is an\n            updated distribution of type ``type(self)``, and ``log_normalizer``\n            is a :class:`~torch.Tensor` representing the normalization factor.\n        """"""\n        raise NotImplementedError(""{} does not support .conjugate_update()""\n                                  .format(type(self).__name__))\n\n    def has_rsample_(self, value):\n        """"""\n        Force reparameterized or detached sampling on a single distribution\n        instance. This sets the ``.has_rsample`` attribute in-place.\n\n        This is useful to instruct inference algorithms to avoid\n        reparameterized gradients for variables that discontinuously determine\n        downstream control flow.\n\n        :param bool value: Whether samples will be pathwise differentiable.\n        :return: self\n        :rtype: Distribution\n        """"""\n        if not (value is True or value is False):\n            raise ValueError(""Expected value in [False,True], actual {}"".format(value))\n        self.has_rsample = value\n        return self\n\n    @property\n    def rv(self):\n        """"""\n        EXPERIMENTAL Switch to the Random Variable DSL for applying transformations\n        to random variables. Supports either chaining operations or arithmetic\n        operator overloading.\n\n        Example usage::\n\n            # This should be equivalent to an Exponential distribution.\n            Uniform(0, 1).rv.log().neg().dist\n\n            # These two distributions Y1, Y2 should be the same\n            X = Uniform(0, 1).rv\n            Y1 = X.mul(4).pow(0.5).sub(1).abs().neg().dist\n            Y2 = (-abs((4*X)**(0.5) - 1)).dist\n\n\n        :return: A :class: `~pyro.contrib.randomvariable.random_variable.RandomVariable`\n            object wrapping this distribution.\n        :rtype: ~pyro.contrib.randomvariable.random_variable.RandomVariable\n        """"""\n        from pyro.contrib.randomvariable import RandomVariable\n        return RandomVariable(self)\n'"
pyro/distributions/empirical.py,20,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch import Categorical\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import copy_docs_from\n\n\n@copy_docs_from(TorchDistribution)\nclass Empirical(TorchDistribution):\n    r""""""\n    Empirical distribution associated with the sampled data. Note that the shape\n    requirement for `log_weights` is that its shape must match the leftmost shape\n    of `samples`. Samples are aggregated along the ``aggregation_dim``, which is\n    the rightmost dim of `log_weights`.\n\n    Example:\n\n    >>> emp_dist = Empirical(torch.randn(2, 3, 10), torch.ones(2, 3))\n    >>> emp_dist.batch_shape\n    torch.Size([2])\n    >>> emp_dist.event_shape\n    torch.Size([10])\n\n    >>> single_sample = emp_dist.sample()\n    >>> single_sample.shape\n    torch.Size([2, 10])\n    >>> batch_sample = emp_dist.sample((100,))\n    >>> batch_sample.shape\n    torch.Size([100, 2, 10])\n\n    >>> emp_dist.log_prob(single_sample).shape\n    torch.Size([2])\n    >>> # Vectorized samples cannot be scored by log_prob.\n    >>> with pyro.validation_enabled():\n    ...     emp_dist.log_prob(batch_sample).shape\n    Traceback (most recent call last):\n    ...\n    ValueError: ``value.shape`` must be torch.Size([2, 10])\n\n    :param torch.Tensor samples: samples from the empirical distribution.\n    :param torch.Tensor log_weights: log weights (optional) corresponding\n        to the samples.\n    """"""\n\n    arg_constraints = {}\n    support = constraints.real\n    has_enumerate_support = True\n\n    def __init__(self, samples, log_weights, validate_args=None):\n        self._samples = samples\n        self._log_weights = log_weights\n        sample_shape, weight_shape = samples.size(), log_weights.size()\n        if weight_shape > sample_shape or weight_shape != sample_shape[:len(weight_shape)]:\n            raise ValueError(""The shape of ``log_weights`` ({}) must match ""\n                             ""the leftmost shape of ``samples`` ({})"".format(weight_shape, sample_shape))\n        self._aggregation_dim = log_weights.dim() - 1\n        event_shape = sample_shape[len(weight_shape):]\n        self._categorical = Categorical(logits=self._log_weights)\n        super().__init__(batch_shape=weight_shape[:-1],\n                         event_shape=event_shape,\n                         validate_args=validate_args)\n\n    @property\n    def sample_size(self):\n        """"""\n        Number of samples that constitute the empirical distribution.\n\n        :return int: number of samples collected.\n        """"""\n        return self._log_weights.numel()\n\n    def sample(self, sample_shape=torch.Size()):\n        sample_idx = self._categorical.sample(sample_shape)  # sample_shape x batch_shape\n        # reorder samples to bring aggregation_dim to the front:\n        # batch_shape x num_samples x event_shape -> num_samples x batch_shape x event_shape\n        samples = self._samples.unsqueeze(0).transpose(0, self._aggregation_dim + 1).squeeze(self._aggregation_dim + 1)\n        # make sample_idx.shape compatible with samples.shape: sample_shape_numel x batch_shape x event_shape\n        sample_idx = sample_idx.reshape((-1,) + self.batch_shape + (1,) * len(self.event_shape))\n        sample_idx = sample_idx.expand((-1,) + samples.shape[1:])\n        return samples.gather(0, sample_idx).reshape(sample_shape + samples.shape[1:])\n\n    def log_prob(self, value):\n        """"""\n        Returns the log of the probability mass function evaluated at ``value``.\n        Note that this currently only supports scoring values with empty\n        ``sample_shape``.\n\n        :param torch.Tensor value: scalar or tensor value to be scored.\n        """"""\n        if self._validate_args:\n            if value.shape != self.batch_shape + self.event_shape:\n                raise ValueError(""``value.shape`` must be {}"".format(self.batch_shape + self.event_shape))\n        if self.batch_shape:\n            value = value.unsqueeze(self._aggregation_dim)\n        selection_mask = self._samples.eq(value)\n        # Get a mask for all entries in the ``weights`` tensor\n        # that correspond to ``value``.\n        for _ in range(len(self.event_shape)):\n            selection_mask = selection_mask.min(dim=-1)[0]\n        selection_mask = selection_mask.type(self._categorical.probs.type())\n        return (self._categorical.probs * selection_mask).sum(dim=-1).log()\n\n    def _weighted_mean(self, value, keepdim=False):\n        weights = self._log_weights.reshape(self._log_weights.size() +\n                                            torch.Size([1] * (value.dim() - self._log_weights.dim())))\n        dim = self._aggregation_dim\n        max_weight = weights.max(dim=dim, keepdim=True)[0]\n        relative_probs = (weights - max_weight).exp()\n        return (value * relative_probs).sum(dim=dim, keepdim=keepdim) / relative_probs.sum(dim=dim, keepdim=keepdim)\n\n    @property\n    def event_shape(self):\n        return self._event_shape\n\n    @property\n    def mean(self):\n        if self._samples.dtype in (torch.int32, torch.int64):\n            raise ValueError(""Mean for discrete empirical distribution undefined. "" +\n                             ""Consider converting samples to ``torch.float32`` "" +\n                             ""or ``torch.float64``. If these are samples from a "" +\n                             ""`Categorical` distribution, consider converting to a "" +\n                             ""`OneHotCategorical` distribution."")\n        return self._weighted_mean(self._samples)\n\n    @property\n    def variance(self):\n        if self._samples.dtype in (torch.int32, torch.int64):\n            raise ValueError(""Variance for discrete empirical distribution undefined. "" +\n                             ""Consider converting samples to ``torch.float32`` "" +\n                             ""or ``torch.float64``. If these are samples from a "" +\n                             ""`Categorical` distribution, consider converting to a "" +\n                             ""`OneHotCategorical` distribution."")\n        mean = self.mean.unsqueeze(self._aggregation_dim)\n        deviation_squared = torch.pow(self._samples - mean, 2)\n        return self._weighted_mean(deviation_squared)\n\n    @property\n    def log_weights(self):\n        return self._log_weights\n\n    def enumerate_support(self, expand=True):\n        # Empirical does not support batching, so expanding is a no-op.\n        return self._samples\n'"
pyro/distributions/extended.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nfrom pyro.distributions import constraints\n\nfrom .conjugate import BetaBinomial\nfrom .torch import Binomial\n\n\nclass ExtendedBinomial(Binomial):\n    """"""\n    EXPERIMENTAL :class:`~pyro.distributions.Binomial` distribution extended to\n    have logical support the entire integers and to allow arbitrary integer\n    ``total_count``. Numerical support is still the integer interval ``[0,\n    total_count]``.\n    """"""\n    arg_constraints = {""total_count"": constraints.integer,\n                       ""probs"": constraints.unit_interval,\n                       ""logits"": constraints.real}\n    support = constraints.integer\n\n    def log_prob(self, value):\n        result = super().log_prob(value)\n        invalid = (value < 0) | (value > self.total_count)\n        return result.masked_fill(invalid, -math.inf)\n\n\nclass ExtendedBetaBinomial(BetaBinomial):\n    """"""\n    EXPERIMENTAL :class:`~pyro.distributions.BetaBinomial` distribution\n    extended to have logical support the entire integers and to allow arbitrary\n    integer ``total_count``. Numerical support is still the integer interval\n    ``[0, total_count]``.\n    """"""\n    arg_constraints = {""concentration1"": constraints.positive,\n                       ""concentration0"": constraints.positive,\n                       ""total_count"": constraints.integer}\n    support = constraints.integer\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n\n        total_count = self.total_count\n        invalid = (value < 0) | (value > total_count)\n        n = total_count.clamp(min=0)\n        k = value.masked_fill(invalid, 0)\n\n        try:\n            self.total_count = n\n            result = super().log_prob(k)\n        finally:\n            self.total_count = total_count\n\n        return result.masked_fill(invalid, -math.inf)\n'"
pyro/distributions/folded.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import AbsTransform\n\nfrom pyro.distributions.torch import TransformedDistribution\n\n\nclass FoldedDistribution(TransformedDistribution):\n    """"""\n    Equivalent to ``TransformedDistribution(base_dist, AbsTransform())``,\n    but additionally supports :meth:`log_prob` .\n\n    :param ~torch.distributions.Distribution base_dist: The distribution to\n        reflect.\n    """"""\n    support = constraints.positive\n\n    def __init__(self, base_dist, validate_args=None):\n        if base_dist.event_shape:\n            raise ValueError(""Only univariate distributions can be folded."")\n        super().__init__(base_dist, AbsTransform(), validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(type(self), _instance)\n        return super().expand(batch_shape, _instance=new)\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        dim = max(len(self.batch_shape), value.dim())\n        plus_minus = value.new_tensor([1., -1.]).reshape((2,) + (1,) * dim)\n        return self.base_dist.log_prob(plus_minus * value).logsumexp(0)\n'"
pyro/distributions/gaussian_scale_mixture.py,21,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.distributions import constraints, Categorical\n\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import sum_leftmost\n\n\nclass GaussianScaleMixture(TorchDistribution):\n    """"""\n    Mixture of Normal distributions with zero mean and diagonal covariance\n    matrices.\n\n    That is, this distribution is a mixture with K components, where each\n    component distribution is a D-dimensional Normal distribution with zero\n    mean and a D-dimensional diagonal covariance matrix. The K different\n    covariance matrices are controlled by the parameters `coord_scale` and\n    `component_scale`.  That is, the covariance matrix of the k\'th component is\n    given by\n\n    Sigma_ii = (component_scale_k * coord_scale_i) ** 2   (i = 1, ..., D)\n\n    where `component_scale_k` is a positive scale factor and `coord_scale_i`\n    are positive scale parameters shared between all K components. The mixture\n    weights are controlled by a K-dimensional vector of softmax logits,\n    `component_logits`. This distribution implements pathwise derivatives for\n    samples from the distribution. This distribution does not currently\n    support batched parameters.\n\n    See reference [1] for details on the implementations of the pathwise\n    derivative. Please consider citing this reference if you use the pathwise\n    derivative in your research.\n\n    [1] Pathwise Derivatives for Multivariate Distributions, Martin Jankowiak &\n    Theofanis Karaletsos. arXiv:1806.01856\n\n    Note that this distribution supports both even and odd dimensions, but the\n    former should be more a bit higher precision, since it doesn\'t use any erfs\n    in the backward call. Also note that this distribution does not support\n    D = 1.\n\n    :param torch.tensor coord_scale: D-dimensional vector of scales\n    :param torch.tensor component_logits: K-dimensional vector of logits\n    :param torch.tensor component_scale: K-dimensional vector of scale multipliers\n    """"""\n    has_rsample = True\n    arg_constraints = {""component_scale"": constraints.positive, ""coord_scale"": constraints.positive,\n                       ""component_logits"": constraints.real}\n\n    def __init__(self, coord_scale, component_logits, component_scale):\n        self.dim = coord_scale.size(0)\n        if self.dim < 2:\n            raise NotImplementedError(\'This distribution does not support D = 1\')\n        assert(coord_scale.dim() == 1), ""The coord_scale parameter in GaussianScaleMixture should be D dimensional""\n        assert(component_scale.dim() == 1), \\\n            ""The component_scale parameter in GaussianScaleMixture should be K dimensional""\n        assert(component_logits.dim() == 1), \\\n            ""The component_logits parameter in GaussianScaleMixture should be K dimensional""\n        assert(component_logits.shape == component_scale.shape), \\\n            ""The component_logits and component_scale parameters in GaussianScaleMixture should be K dimensional""\n        self.coord_scale = coord_scale\n        self.component_logits = component_logits\n        self.component_scale = component_scale\n        self.coeffs = self._compute_coeffs()\n        self.categorical = Categorical(logits=component_logits)\n        super().__init__(event_shape=(self.dim,))\n\n    def _compute_coeffs(self):\n        """"""\n        These coefficients are used internally in the backward call.\n        """"""\n        dimov2 = int(self.dim / 2)  # this is correct for both even and odd dimensions\n        coeffs = torch.ones(dimov2)\n        for k in range(dimov2 - 1):\n            coeffs[k + 1:] *= self.dim - 2 * (k + 1)\n        return coeffs\n\n    def log_prob(self, value):\n        assert value.dim() == 1 and value.size(0) == self.dim\n        epsilon_sqr = torch.pow(value / self.coord_scale, 2.0).sum()\n        component_scale_log_power = self.component_scale.log() * -self.dim\n        # logits in Categorical is already normalized\n        result = torch.logsumexp(\n            component_scale_log_power + self.categorical.logits +\n            -0.5 * epsilon_sqr / torch.pow(self.component_scale, 2.0), dim=-1)  # K\n        result -= 0.5 * math.log(2.0 * math.pi) * float(self.dim)\n        result -= self.coord_scale.log().sum()\n        return result\n\n    def rsample(self, sample_shape=torch.Size()):\n        which = self.categorical.sample(sample_shape)\n        return _GSMSample.apply(self.coord_scale, self.component_logits, self.component_scale, self.categorical.probs,\n                                which, sample_shape + torch.Size((self.dim,)), self.coeffs)\n\n\nclass _GSMSample(Function):\n    @staticmethod\n    def forward(ctx, coord_scale, component_logits, component_scale, pis, which, shape, coeffs):\n        white = coord_scale.new(shape).normal_()\n        which_component_scale = component_scale[which].unsqueeze(-1)\n        z = coord_scale * which_component_scale * white\n        ctx.save_for_backward(z, coord_scale, component_logits, component_scale, pis, coeffs)\n        return z\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        z, coord_scale, component_logits, component_scale, pis, coeffs = ctx.saved_tensors\n        dim = coord_scale.size(0)\n        g = grad_output  # l i\n        g = g.unsqueeze(-2)  # l 1 i\n\n        component_scale_sqr = torch.pow(component_scale, 2.0)  # j\n        epsilons = z / coord_scale  # l i\n        epsilons_sqr = torch.pow(epsilons, 2.0)  # l i\n        r_sqr = epsilons_sqr.sum(-1, keepdim=True)  # l\n        r_sqr_j = r_sqr / component_scale_sqr  # l j\n        coord_scale_product = coord_scale.prod()\n        component_scale_power = torch.pow(component_scale, float(dim))\n\n        q_j = torch.exp(-0.5 * r_sqr_j) / math.pow(2.0 * math.pi, 0.5 * float(dim))  # l j\n        q_j /= coord_scale_product * component_scale_power  # l j\n        q_tot = (pis * q_j).sum(-1, keepdim=True)  # l\n\n        Phi_j = torch.exp(-0.5 * r_sqr_j)  # l j\n        exponents = - torch.arange(1., int(dim/2) + 1., 1.)\n        if z.dim() > 1:\n            r_j_poly = r_sqr_j.unsqueeze(-1).expand(-1, -1, int(dim/2))  # l j d/2\n        else:\n            r_j_poly = r_sqr_j.unsqueeze(-1).expand(-1, int(dim/2))  # l j d/2\n        r_j_poly = coeffs * torch.pow(r_j_poly, exponents)\n        Phi_j *= r_j_poly.sum(-1)\n        if dim % 2 == 1:\n            root_two = math.sqrt(2.0)\n            extra_term = coeffs[-1] * math.sqrt(0.5 * math.pi) * (1.0 - torch.erf(r_sqr_j.sqrt() / root_two))  # l j\n            Phi_j += extra_term * torch.pow(r_sqr_j, -0.5 * float(dim))\n\n        logits_grad = (z.unsqueeze(-2) * Phi_j.unsqueeze(-1) * g).sum(-1)  # l j\n        logits_grad /= q_tot\n        logits_grad = sum_leftmost(logits_grad, -1) * math.pow(2.0 * math.pi, -0.5 * float(dim))\n        logits_grad = pis * logits_grad / (component_scale_power * coord_scale_product)\n        logits_grad = logits_grad - logits_grad.sum() * pis\n\n        prefactor = pis.unsqueeze(-1) * q_j.unsqueeze(-1) * g / q_tot.unsqueeze(-1)  # l j i\n        coord_scale_grad = sum_leftmost(prefactor * epsilons.unsqueeze(-2), -1)\n        component_scale_grad = sum_leftmost((prefactor * z.unsqueeze(-2)).sum(-1) / component_scale, -1)\n\n        return coord_scale_grad, logits_grad, component_scale_grad, None, None, None, None\n'"
pyro/distributions/hmm.py,75,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch import Categorical, Gamma, Independent, MultivariateNormal\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.ops.gamma_gaussian import (GammaGaussian, gamma_and_mvn_to_gamma_gaussian, gamma_gaussian_tensordot,\n                                     matrix_and_mvn_to_gamma_gaussian)\nfrom pyro.ops.gaussian import Gaussian, gaussian_tensordot, matrix_and_mvn_to_gaussian, mvn_to_gaussian\nfrom pyro.ops.special import safe_log\nfrom pyro.ops.tensor_utils import cholesky, cholesky_solve\n\n\n@torch.jit.script\ndef _linear_integrate(init, trans, shift):\n    """"""\n    Integrate the inhomogeneous linear shifterence equation::\n\n        x[0] = init\n        x[t] = x[t-1] @ trans[t] + shift[t]\n\n    :return: An integrated tensor ``x[:, :]``.\n    """"""\n    # xs: List[Tensor]\n    xs = []\n    x = init.unsqueeze(-2)\n    shift = shift.unsqueeze(-3)\n    for t in range(trans.size(-3)):\n        x = x @ trans[..., t, :, :] + shift[..., t, :]\n        xs.append(x)\n    return torch.cat(xs, dim=-2)\n\n\ndef _logmatmulexp(x, y):\n    """"""\n    Numerically stable version of ``(x.log() @ y.log()).exp()``.\n    """"""\n    finfo = torch.finfo(x.dtype)  # avoid nan due to -inf - -inf\n    x_shift = x.max(-1, keepdim=True).values.clamp(min=finfo.min)\n    y_shift = y.max(-2, keepdim=True).values.clamp(min=finfo.min)\n    xy = safe_log(torch.matmul((x - x_shift).exp(), (y - y_shift).exp()))\n    return xy + x_shift + y_shift\n\n\n# TODO re-enable jitting once _SafeLog is supported by the jit.\n# See https://discuss.pytorch.org/t/does-torch-jit-script-support-custom-operators/65759/4\n# @torch.jit.script\ndef _sequential_logmatmulexp(logits):\n    """"""\n    For a tensor ``x`` whose time dimension is -3, computes::\n\n        x[..., 0, :, :] @ x[..., 1, :, :] @ ... @ x[..., T-1, :, :]\n\n    but does so numerically stably in log space.\n    """"""\n    batch_shape = logits.shape[:-3]\n    state_dim = logits.size(-1)\n    while logits.size(-3) > 1:\n        time = logits.size(-3)\n        even_time = time // 2 * 2\n        even_part = logits[..., :even_time, :, :]\n        x_y = even_part.reshape(batch_shape + (even_time // 2, 2, state_dim, state_dim))\n        x, y = x_y.unbind(-3)\n        contracted = _logmatmulexp(x, y)\n        if time > even_time:\n            contracted = torch.cat((contracted, logits[..., -1:, :, :]), dim=-3)\n        logits = contracted\n    return logits.squeeze(-3)\n\n\ndef _sequential_gaussian_tensordot(gaussian):\n    """"""\n    Integrates a Gaussian ``x`` whose rightmost batch dimension is time, computes::\n\n        x[..., 0] @ x[..., 1] @ ... @ x[..., T-1]\n    """"""\n    assert isinstance(gaussian, Gaussian)\n    assert gaussian.dim() % 2 == 0, ""dim is not even""\n    batch_shape = gaussian.batch_shape[:-1]\n    state_dim = gaussian.dim() // 2\n    while gaussian.batch_shape[-1] > 1:\n        time = gaussian.batch_shape[-1]\n        even_time = time // 2 * 2\n        even_part = gaussian[..., :even_time]\n        x_y = even_part.reshape(batch_shape + (even_time // 2, 2))\n        x, y = x_y[..., 0], x_y[..., 1]\n        contracted = gaussian_tensordot(x, y, state_dim)\n        if time > even_time:\n            contracted = Gaussian.cat((contracted, gaussian[..., -1:]), dim=-1)\n        gaussian = contracted\n    return gaussian[..., 0]\n\n\ndef _is_subshape(x, y):\n    return broadcast_shape(x, y) == y\n\n\ndef _sequential_gaussian_filter_sample(init, trans, sample_shape):\n    """"""\n    Draws a reparameterized sample from a Markov product of Gaussians via\n    parallel-scan forward-filter backward-sample.\n    """"""\n    assert isinstance(init, Gaussian)\n    assert isinstance(trans, Gaussian)\n    assert trans.dim() == 2 * init.dim()\n    assert _is_subshape(trans.batch_shape[:-1], init.batch_shape)\n    state_dim = trans.dim() // 2\n    device = trans.precision.device\n    perm = torch.cat([torch.arange(1 * state_dim, 2 * state_dim, device=device),\n                      torch.arange(0 * state_dim, 1 * state_dim, device=device),\n                      torch.arange(2 * state_dim, 3 * state_dim, device=device)])\n\n    # Forward filter, similar to _sequential_gaussian_tensordot().\n    tape = []\n    shape = trans.batch_shape[:-1]  # Note trans may be unbroadcasted.\n    gaussian = trans\n    while gaussian.batch_shape[-1] > 1:\n        time = gaussian.batch_shape[-1]\n        even_time = time // 2 * 2\n        even_part = gaussian[..., :even_time]\n        x_y = even_part.reshape(shape + (even_time // 2, 2))\n        x, y = x_y[..., 0], x_y[..., 1]\n        x = x.event_pad(right=state_dim)\n        y = y.event_pad(left=state_dim)\n        joint = (x + y).event_permute(perm)\n        tape.append(joint)\n        contracted = joint.marginalize(left=state_dim)\n        if time > even_time:\n            contracted = Gaussian.cat((contracted, gaussian[..., -1:]), dim=-1)\n        gaussian = contracted\n    gaussian = gaussian[..., 0] + init.event_pad(right=state_dim)\n\n    # Backward sample.\n    shape = sample_shape + init.batch_shape\n    result = gaussian.rsample(sample_shape).reshape(shape + (2, state_dim))\n    for joint in reversed(tape):\n        # The following comments demonstrate two example computations, one\n        # EVEN, one ODD.  Ignoring sample_shape and batch_shape, let each zn be\n        # a single sampled event of shape (state_dim,).\n        if joint.batch_shape[-1] == result.size(-2) - 1:  # EVEN case.\n            # Suppose e.g. result = [z0, z2, z4]\n            cond = result.repeat_interleave(2, dim=-2)  # [z0, z0, z2, z2, z4, z4]\n            cond = cond[..., 1:-1, :]  # [z0, z2, z2, z4]\n            cond = cond.reshape(shape + (-1, 2 * state_dim))  # [z0z2, z2z4]\n            sample = joint.condition(cond).rsample()  # [z1, z3]\n            sample = torch.nn.functional.pad(sample, (0, 0, 0, 1))  # [z1, z3, 0]\n            result = torch.stack([\n                result,  # [z0, z2, z4]\n                sample,  # [z1, z3, 0]\n            ], dim=-2)  # [[z0, z1], [z2, z3], [z4, 0]]\n            result = result.reshape(shape + (-1, state_dim))  # [z0, z1, z2, z3, z4, 0]\n            result = result[..., :-1, :]  # [z0, z1, z2, z3, z4]\n        else:  # ODD case.\n            assert joint.batch_shape[-1] == result.size(-2) - 2\n            # Suppose e.g. result = [z0, z2, z3]\n            cond = result[..., :-1, :].repeat_interleave(2, dim=-2)  # [z0, z0, z2, z2]\n            cond = cond[..., 1:-1, :]  # [z0, z2]\n            cond = cond.reshape(shape + (-1, 2 * state_dim))  # [z0z2]\n            sample = joint.condition(cond).rsample()  # [z1]\n            sample = torch.cat([sample, result[..., -1:, :]], dim=-2)  # [z1, z3]\n            result = torch.stack([\n                result[..., :-1, :],  # [z0, z2]\n                sample,  # [z1, z3]\n            ], dim=-2)  # [[z0, z1], [z2, z3]]\n            result = result.reshape(shape + (-1, state_dim))  # [z0, z1, z2, z3]\n\n    return result[..., 1:, :]  # [z1, z2, z3, ...]\n\n\ndef _sequential_gamma_gaussian_tensordot(gamma_gaussian):\n    """"""\n    Integrates a GammaGaussian ``x`` whose rightmost batch dimension is time, computes::\n\n        x[..., 0] @ x[..., 1] @ ... @ x[..., T-1]\n    """"""\n    assert isinstance(gamma_gaussian, GammaGaussian)\n    assert gamma_gaussian.dim() % 2 == 0, ""dim is not even""\n    batch_shape = gamma_gaussian.batch_shape[:-1]\n    state_dim = gamma_gaussian.dim() // 2\n    while gamma_gaussian.batch_shape[-1] > 1:\n        time = gamma_gaussian.batch_shape[-1]\n        even_time = time // 2 * 2\n        even_part = gamma_gaussian[..., :even_time]\n        x_y = even_part.reshape(batch_shape + (even_time // 2, 2))\n        x, y = x_y[..., 0], x_y[..., 1]\n        contracted = gamma_gaussian_tensordot(x, y, state_dim)\n        if time > even_time:\n            contracted = GammaGaussian.cat((contracted, gamma_gaussian[..., -1:]), dim=-1)\n        gamma_gaussian = contracted\n    return gamma_gaussian[..., 0]\n\n\nclass HiddenMarkovModel(TorchDistribution):\n    """"""\n    Abstract base class for Hidden Markov Models.\n\n    The purpose of this class is to handle duration logic for homogeneous HMMs.\n\n    :param int duration: Optional size of the time axis ``event_shape[0]``.\n        This is required when sampling from homogeneous HMMs whose parameters\n        are not expanded along the time axis.\n    """"""\n    def __init__(self, duration, batch_shape, event_shape, validate_args=None):\n        if duration is None:\n            if event_shape[0] != 1:\n                # Infer duration from event_shape.\n                duration = event_shape[0]\n        elif duration != event_shape[0]:\n            if event_shape[0] != 1:\n                raise ValueError(""duration, event_shape mismatch: {} vs {}""\n                                 .format(duration, event_shape))\n            # Infer event_shape from duration.\n            event_shape = torch.Size((duration,) + event_shape[1:])\n        self._duration = duration\n        super().__init__(batch_shape, event_shape, validate_args)\n\n    @property\n    def duration(self):\n        """"""\n        Returns the size of the time axis, or None if unknown.\n        """"""\n        return self._duration\n\n    def _validate_sample(self, value):\n        if value.dim() < self.event_dim:\n            raise ValueError(""value has too few dimensions: {}"".format(value.shape))\n\n        if self.duration is not None:\n            super()._validate_sample(value)\n            return\n\n        # Temporarily infer duration from value.shape.\n        duration = value.size(-self.event_dim)\n        old = self._event_shape\n        new = torch.Size((duration,)) + self._event_shape[1:]\n        try:\n            self._event_shape = new\n            super()._validate_sample(value)\n        finally:\n            self._event_shape = old\n\n\nclass DiscreteHMM(HiddenMarkovModel):\n    """"""\n    Hidden Markov Model with discrete latent state and arbitrary observation\n    distribution. This uses [1] to parallelize over time, achieving\n    O(log(time)) parallel complexity.\n\n    The event_shape of this distribution includes time on the left::\n\n        event_shape = (num_steps,) + observation_dist.event_shape\n\n    This distribution supports any combination of homogeneous/heterogeneous\n    time dependency of ``transition_logits`` and ``observation_dist``. However,\n    because time is included in this distribution\'s event_shape, the\n    homogeneous+homogeneous case will have a broadcastable event_shape with\n    ``num_steps = 1``, allowing :meth:`log_prob` to work with arbitrary length\n    data::\n\n        # homogeneous + homogeneous case:\n        event_shape = (1,) + observation_dist.event_shape\n\n    **References:**\n\n    [1] Simo Sarkka, Angel F. Garcia-Fernandez (2019)\n        ""Temporal Parallelization of Bayesian Filters and Smoothers""\n        https://arxiv.org/pdf/1905.13002.pdf\n\n    :param ~torch.Tensor initial_logits: A logits tensor for an initial\n        categorical distribution over latent states. Should have rightmost size\n        ``state_dim`` and be broadcastable to ``batch_shape + (state_dim,)``.\n    :param ~torch.Tensor transition_logits: A logits tensor for transition\n        conditional distributions between latent states. Should have rightmost\n        shape ``(state_dim, state_dim)`` (old, new), and be broadcastable to\n        ``batch_shape + (num_steps, state_dim, state_dim)``.\n    :param ~torch.distributions.Distribution observation_dist: A conditional\n        distribution of observed data conditioned on latent state. The\n        ``.batch_shape`` should have rightmost size ``state_dim`` and be\n        broadcastable to ``batch_shape + (num_steps, state_dim)``. The\n        ``.event_shape`` may be arbitrary.\n    :param int duration: Optional size of the time axis ``event_shape[0]``.\n        This is required when sampling from homogeneous HMMs whose parameters\n        are not expanded along the time axis.\n    """"""\n    arg_constraints = {""initial_logits"": constraints.real,\n                       ""transition_logits"": constraints.real}\n\n    def __init__(self, initial_logits, transition_logits, observation_dist,\n                 validate_args=None, duration=None):\n        if initial_logits.dim() < 1:\n            raise ValueError(""expected initial_logits to have at least one dim, ""\n                             ""actual shape = {}"".format(initial_logits.shape))\n        if transition_logits.dim() < 2:\n            raise ValueError(""expected transition_logits to have at least two dims, ""\n                             ""actual shape = {}"".format(transition_logits.shape))\n        if len(observation_dist.batch_shape) < 1:\n            raise ValueError(""expected observation_dist to have at least one batch dim, ""\n                             ""actual .batch_shape = {}"".format(observation_dist.batch_shape))\n        shape = broadcast_shape(initial_logits.shape[:-1] + (1,),\n                                transition_logits.shape[:-2],\n                                observation_dist.batch_shape[:-1])\n        batch_shape, time_shape = shape[:-1], shape[-1:]\n        event_shape = time_shape + observation_dist.event_shape\n        self.initial_logits = initial_logits - initial_logits.logsumexp(-1, True)\n        self.transition_logits = transition_logits - transition_logits.logsumexp(-1, True)\n        self.observation_dist = observation_dist\n        super().__init__(duration, batch_shape, event_shape, validate_args=validate_args)\n\n    @property\n    def support(self):\n        return self.observation_dist.support\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(DiscreteHMM, _instance)\n        batch_shape = torch.Size(broadcast_shape(self.batch_shape, batch_shape))\n        # We only need to expand one of the inputs, since batch_shape is determined\n        # by broadcasting all three. To save computation in _sequential_logmatmulexp(),\n        # we expand only initial_logits, which is applied only after the logmatmulexp.\n        # This is similar to the ._unbroadcasted_* pattern used elsewhere in distributions.\n        new.initial_logits = self.initial_logits.expand(batch_shape + (-1,))\n        new.transition_logits = self.transition_logits\n        new.observation_dist = self.observation_dist\n        super(DiscreteHMM, new).__init__(self.duration, batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n\n        # Combine observation and transition factors.\n        value = value.unsqueeze(-1 - self.observation_dist.event_dim)\n        observation_logits = self.observation_dist.log_prob(value)\n        result = self.transition_logits + observation_logits.unsqueeze(-2)\n\n        # Eliminate time dimension.\n        result = _sequential_logmatmulexp(result)\n\n        # Combine initial factor.\n        result = self.initial_logits + result.logsumexp(-1)\n\n        # Marginalize out final state.\n        result = result.logsumexp(-1)\n        return result\n\n    def filter(self, value):\n        """"""\n        Compute posterior over final state given a sequence of observations.\n\n        :param ~torch.Tensor value: A sequence of observations.\n        :return: A posterior distribution\n            over latent states at the final time step. ``result.logits`` can\n            then be used as ``initial_logits`` in a sequential Pyro model for\n            prediction.\n        :rtype: ~pyro.distributions.Categorical\n        """"""\n        if self._validate_args:\n            self._validate_sample(value)\n\n        # Combine observation and transition factors.\n        value = value.unsqueeze(-1 - self.observation_dist.event_dim)\n        observation_logits = self.observation_dist.log_prob(value)\n        logp = self.transition_logits + observation_logits.unsqueeze(-2)\n\n        # Eliminate time dimension.\n        logp = _sequential_logmatmulexp(logp)\n\n        # Combine initial factor.\n        logp = (self.initial_logits.unsqueeze(-1) + logp).logsumexp(-2)\n\n        # Convert to a distribution.\n        return Categorical(logits=logp, validate_args=self._validate_args)\n\n\nclass GaussianHMM(HiddenMarkovModel):\n    """"""\n    Hidden Markov Model with Gaussians for initial, transition, and observation\n    distributions. This adapts [1] to parallelize over time to achieve\n    O(log(time)) parallel complexity, however it differs in that it tracks the\n    log normalizer to ensure :meth:`log_prob` is differentiable.\n\n    This corresponds to the generative model::\n\n        z = initial_distribution.sample()\n        x = []\n        for t in range(num_events):\n            z = z @ transition_matrix + transition_dist.sample()\n            x.append(z @ observation_matrix + observation_dist.sample())\n\n    The event_shape of this distribution includes time on the left::\n\n        event_shape = (num_steps,) + observation_dist.event_shape\n\n    This distribution supports any combination of homogeneous/heterogeneous\n    time dependency of ``transition_dist`` and ``observation_dist``. However,\n    because time is included in this distribution\'s event_shape, the\n    homogeneous+homogeneous case will have a broadcastable event_shape with\n    ``num_steps = 1``, allowing :meth:`log_prob` to work with arbitrary length\n    data::\n\n        event_shape = (1, obs_dim)  # homogeneous + homogeneous case\n\n    **References:**\n\n    [1] Simo Sarkka, Angel F. Garcia-Fernandez (2019)\n        ""Temporal Parallelization of Bayesian Filters and Smoothers""\n        https://arxiv.org/pdf/1905.13002.pdf\n\n    :ivar int hidden_dim: The dimension of the hidden state.\n    :ivar int obs_dim: The dimension of the observed state.\n    :param ~torch.distributions.MultivariateNormal initial_dist: A distribution\n        over initial states. This should have batch_shape broadcastable to\n        ``self.batch_shape``.  This should have event_shape ``(hidden_dim,)``.\n    :param ~torch.Tensor transition_matrix: A linear transformation of hidden\n        state. This should have shape broadcastable to\n        ``self.batch_shape + (num_steps, hidden_dim, hidden_dim)`` where the\n        rightmost dims are ordered ``(old, new)``.\n    :param ~torch.distributions.MultivariateNormal transition_dist: A process\n        noise distribution. This should have batch_shape broadcastable to\n        ``self.batch_shape + (num_steps,)``.  This should have event_shape\n        ``(hidden_dim,)``.\n    :param ~torch.Tensor observation_matrix: A linear transformation from hidden\n        to observed state. This should have shape broadcastable to\n        ``self.batch_shape + (num_steps, hidden_dim, obs_dim)``.\n    :param observation_dist: An observation noise distribution. This should\n        have batch_shape broadcastable to ``self.batch_shape + (num_steps,)``.\n        This should have event_shape ``(obs_dim,)``.\n    :type observation_dist: ~torch.distributions.MultivariateNormal or\n        ~torch.distributions.Independent of ~torch.distributions.Normal\n    :param int duration: Optional size of the time axis ``event_shape[0]``.\n        This is required when sampling from homogeneous HMMs whose parameters\n        are not expanded along the time axis.\n    """"""\n    has_rsample = True\n    arg_constraints = {}\n    support = constraints.real\n\n    def __init__(self, initial_dist, transition_matrix, transition_dist,\n                 observation_matrix, observation_dist, validate_args=None, duration=None):\n        assert (isinstance(initial_dist, torch.distributions.MultivariateNormal) or\n                (isinstance(initial_dist, torch.distributions.Independent) and\n                 isinstance(initial_dist.base_dist, torch.distributions.Normal)))\n        assert isinstance(transition_matrix, torch.Tensor)\n        assert (isinstance(transition_dist, torch.distributions.MultivariateNormal) or\n                (isinstance(transition_dist, torch.distributions.Independent) and\n                 isinstance(transition_dist.base_dist, torch.distributions.Normal)))\n        assert isinstance(observation_matrix, torch.Tensor)\n        assert (isinstance(observation_dist, torch.distributions.MultivariateNormal) or\n                (isinstance(observation_dist, torch.distributions.Independent) and\n                 isinstance(observation_dist.base_dist, torch.distributions.Normal)))\n        hidden_dim, obs_dim = observation_matrix.shape[-2:]\n        assert initial_dist.event_shape == (hidden_dim,)\n        assert transition_matrix.shape[-2:] == (hidden_dim, hidden_dim)\n        assert transition_dist.event_shape == (hidden_dim,)\n        assert observation_dist.event_shape == (obs_dim,)\n        shape = broadcast_shape(initial_dist.batch_shape + (1,),\n                                transition_matrix.shape[:-2],\n                                transition_dist.batch_shape,\n                                observation_matrix.shape[:-2],\n                                observation_dist.batch_shape)\n        batch_shape, time_shape = shape[:-1], shape[-1:]\n        event_shape = time_shape + (obs_dim,)\n        super().__init__(duration, batch_shape, event_shape, validate_args=validate_args)\n\n        self.hidden_dim = hidden_dim\n        self.obs_dim = obs_dim\n        self._init = mvn_to_gaussian(initial_dist).expand(self.batch_shape)\n        self._trans = matrix_and_mvn_to_gaussian(transition_matrix, transition_dist)\n        self._obs = matrix_and_mvn_to_gaussian(observation_matrix, observation_dist)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(GaussianHMM, _instance)\n        new.hidden_dim = self.hidden_dim\n        new.obs_dim = self.obs_dim\n        new._obs = self._obs\n        new._trans = self._trans\n\n        # To save computation in _sequential_gaussian_tensordot(), we expand\n        # only _init, which is applied only after\n        # _sequential_gaussian_tensordot().\n        batch_shape = torch.Size(broadcast_shape(self.batch_shape, batch_shape))\n        new._init = self._init.expand(batch_shape)\n\n        super(GaussianHMM, new).__init__(self.duration, batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n\n        # Combine observation and transition factors.\n        result = self._trans + self._obs.condition(value).event_pad(left=self.hidden_dim)\n\n        # Eliminate time dimension.\n        result = _sequential_gaussian_tensordot(result.expand(result.batch_shape))\n\n        # Combine initial factor.\n        result = gaussian_tensordot(self._init, result, dims=self.hidden_dim)\n\n        # Marginalize out final state.\n        result = result.event_logsumexp()\n        return result\n\n    def rsample(self, sample_shape=torch.Size()):\n        assert self.duration is not None\n        sample_shape = torch.Size(sample_shape)\n        trans = self._trans + self._obs.marginalize(right=self.obs_dim).event_pad(left=self.hidden_dim)\n        trans = trans.expand(trans.batch_shape[:-1] + (self.duration,))\n        z = _sequential_gaussian_filter_sample(self._init, trans, sample_shape)\n        x = self._obs.left_condition(z).rsample()\n        return x\n\n    def rsample_posterior(self, value, sample_shape=torch.Size()):\n        """"""\n        EXPERIMENTAL Sample from the latent state conditioned on observation.\n        """"""\n        trans = self._trans + self._obs.condition(value).event_pad(left=self.hidden_dim)\n        trans = trans.expand(trans.batch_shape)\n        z = _sequential_gaussian_filter_sample(self._init, trans, sample_shape)\n        return z\n\n    def filter(self, value):\n        """"""\n        Compute posterior over final state given a sequence of observations.\n\n        :param ~torch.Tensor value: A sequence of observations.\n        :return: A posterior\n            distribution over latent states at the final time step. ``result``\n            can then be used as ``initial_dist`` in a sequential Pyro model for\n            prediction.\n        :rtype: ~pyro.distributions.MultivariateNormal\n        """"""\n        if self._validate_args:\n            self._validate_sample(value)\n\n        # Combine observation and transition factors.\n        logp = self._trans + self._obs.condition(value).event_pad(left=self.hidden_dim)\n\n        # Eliminate time dimension.\n        logp = _sequential_gaussian_tensordot(logp.expand(logp.batch_shape))\n\n        # Combine initial factor.\n        logp = gaussian_tensordot(self._init, logp, dims=self.hidden_dim)\n\n        # Convert to a distribution\n        precision = logp.precision\n        loc = cholesky_solve(logp.info_vec.unsqueeze(-1), cholesky(precision)).squeeze(-1)\n        return MultivariateNormal(loc, precision_matrix=precision,\n                                  validate_args=self._validate_args)\n\n    def conjugate_update(self, other):\n        """"""\n        EXPERIMENTAL Creates an updated :class:`GaussianHMM` fusing information\n        from another compatible distribution.\n\n        This should satisfy::\n\n            fg, log_normalizer = f.conjugate_update(g)\n            assert f.log_prob(x) + g.log_prob(x) == fg.log_prob(x) + log_normalizer\n\n        :param other: A distribution representing ``p(data|self.probs)`` but\n            normalized over ``self.probs`` rather than ``data``.\n        :type other: ~torch.distributions.Independent of\n            ~torch.distributions.MultivariateNormal or ~torch.distributions.Normal\n        :return: a pair ``(updated,log_normalizer)`` where ``updated`` is an\n            updated :class:`GaussianHMM` , and ``log_normalizer`` is a\n            :class:`~torch.Tensor` representing the normalization factor.\n        """"""\n        assert (isinstance(other, torch.distributions.Independent) and\n                (isinstance(other.base_dist, torch.distributions.Normal) or\n                 isinstance(other.base_dist, torch.distributions.MultivariateNormal)))\n        duration = other.event_shape[0] if self.duration is None else self.duration\n        event_shape = torch.Size((duration, self.obs_dim))\n        assert other.event_shape == event_shape\n\n        new = self._get_checked_instance(GaussianHMM)\n        new.hidden_dim = self.hidden_dim\n        new.obs_dim = self.obs_dim\n        new._init = self._init\n        new._trans = self._trans\n        new._obs = self._obs + mvn_to_gaussian(other.to_event(-1)).event_pad(left=self.hidden_dim)\n\n        # Normalize.\n        # TODO cache this computation for the forward pass of .rsample().\n        logp = new._trans + new._obs.marginalize(right=new.obs_dim).event_pad(left=new.hidden_dim)\n        logp = _sequential_gaussian_tensordot(logp.expand(logp.batch_shape))\n        logp = gaussian_tensordot(new._init, logp, dims=new.hidden_dim)\n        log_normalizer = logp.event_logsumexp()\n        new._init = new._init - log_normalizer\n\n        batch_shape = log_normalizer.shape\n        super(GaussianHMM, new).__init__(duration, batch_shape, event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(\'_validate_args\')\n        return new, log_normalizer\n\n    def prefix_condition(self, data):\n        """"""\n        EXPERIMENTAL Given self has ``event_shape == (t+f, d)`` and data ``x``\n        of shape ``batch_shape + (t, d)``, compute a conditional distribution\n        of event_shape ``(f, d)``. Typically ``t`` is the number of training\n        time steps, ``f`` is the number of forecast time steps, and ``d`` is\n        the data dimension.\n\n        :param data: data of dimension at least 2.\n        :type data: ~torch.Tensor\n        """"""\n        assert data.dim() >= 2\n        assert data.size(-1) == self.event_shape[-1]\n        assert data.size(-2) < self.duration\n        t = data.size(-2)\n        f = self.duration - t\n\n        left = self._get_checked_instance(GaussianHMM)\n        left.hidden_dim = self.hidden_dim\n        left.obs_dim = self.obs_dim\n        left._init = self._init\n\n        right = self._get_checked_instance(GaussianHMM)\n        right.hidden_dim = self.hidden_dim\n        right.obs_dim = self.obs_dim\n\n        if self._obs.batch_shape == () or self._obs.batch_shape[-1] == 1:  # homogeneous\n            left._obs = self._obs\n            right._obs = self._obs\n        else:  # heterogeneous\n            left._obs = self._obs[..., :t]\n            right._obs = self._obs[..., t:]\n\n        if self._trans.batch_shape == () or self._trans.batch_shape[-1] == 1:  # homogeneous\n            left._trans = self._trans\n            right._trans = self._trans\n        else:  # heterogeneous\n            left._trans = self._trans[..., :t]\n            right._trans = self._trans[..., t:]\n\n        super(GaussianHMM, left).__init__(t, self.batch_shape, (t, self.obs_dim),\n                                          validate_args=self._validate_args)\n        initial_dist = left.filter(data)\n        right._init = mvn_to_gaussian(initial_dist)\n        batch_shape = broadcast_shape(right._init.batch_shape, self.batch_shape)\n        super(GaussianHMM, right).__init__(f, batch_shape, (f, self.obs_dim),\n                                           validate_args=self._validate_args)\n        return right\n\n\nclass GammaGaussianHMM(HiddenMarkovModel):\n    """"""\n    Hidden Markov Model with the joint distribution of initial state, hidden\n    state, and observed state is a :class:`~pyro.distributions.MultivariateStudentT`\n    distribution along the line of references [2] and [3]. This adapts [1]\n    to parallelize over time to achieve O(log(time)) parallel complexity.\n\n    This GammaGaussianHMM class corresponds to the generative model::\n\n        s = Gamma(df/2, df/2).sample()\n        z = scale(initial_dist, s).sample()\n        x = []\n        for t in range(num_events):\n            z = z @ transition_matrix + scale(transition_dist, s).sample()\n            x.append(z @ observation_matrix + scale(observation_dist, s).sample())\n\n    where `scale(mvn(loc, precision), s) := mvn(loc, s * precision)`.\n\n    The event_shape of this distribution includes time on the left::\n\n        event_shape = (num_steps,) + observation_dist.event_shape\n\n    This distribution supports any combination of homogeneous/heterogeneous\n    time dependency of ``transition_dist`` and ``observation_dist``. However,\n    because time is included in this distribution\'s event_shape, the\n    homogeneous+homogeneous case will have a broadcastable event_shape with\n    ``num_steps = 1``, allowing :meth:`log_prob` to work with arbitrary length\n    data::\n\n        event_shape = (1, obs_dim)  # homogeneous + homogeneous case\n\n    **References:**\n\n    [1] Simo Sarkka, Angel F. Garcia-Fernandez (2019)\n        ""Temporal Parallelization of Bayesian Filters and Smoothers""\n        https://arxiv.org/pdf/1905.13002.pdf\n\n    [2] F. J. Giron and J. C. Rojano (1994)\n        ""Bayesian Kalman filtering with elliptically contoured errors""\n\n    [3] Filip Tronarp, Toni Karvonen, and Simo Sarkka (2019)\n        ""Student\'s t-filters for noise scale estimation""\n        https://users.aalto.fi/~ssarkka/pub/SPL2019.pdf\n\n    :ivar int hidden_dim: The dimension of the hidden state.\n    :ivar int obs_dim: The dimension of the observed state.\n    :param Gamma scale_dist: Prior of the mixing distribution.\n    :param MultivariateNormal initial_dist: A distribution with unit scale mixing\n        over initial states. This should have batch_shape broadcastable to\n        ``self.batch_shape``.  This should have event_shape ``(hidden_dim,)``.\n    :param ~torch.Tensor transition_matrix: A linear transformation of hidden\n        state. This should have shape broadcastable to\n        ``self.batch_shape + (num_steps, hidden_dim, hidden_dim)`` where the\n        rightmost dims are ordered ``(old, new)``.\n    :param MultivariateNormal transition_dist: A process noise distribution\n        with unit scale mixing. This should have batch_shape broadcastable to\n        ``self.batch_shape + (num_steps,)``. This should have event_shape\n        ``(hidden_dim,)``.\n    :param ~torch.Tensor observation_matrix: A linear transformation from hidden\n        to observed state. This should have shape broadcastable to\n        ``self.batch_shape + (num_steps, hidden_dim, obs_dim)``.\n    :param MultivariateNormal observation_dist: An observation noise distribution\n        with unit scale mixing. This should have batch_shape broadcastable to\n        ``self.batch_shape + (num_steps,)``.\n        This should have event_shape ``(obs_dim,)``.\n    :param int duration: Optional size of the time axis ``event_shape[0]``.\n        This is required when sampling from homogeneous HMMs whose parameters\n        are not expanded along the time axis.\n    """"""\n    arg_constraints = {}\n    support = constraints.real\n\n    def __init__(self, scale_dist, initial_dist, transition_matrix, transition_dist,\n                 observation_matrix, observation_dist, validate_args=None, duration=None):\n        assert isinstance(scale_dist, Gamma)\n        assert isinstance(initial_dist, MultivariateNormal)\n        assert isinstance(transition_matrix, torch.Tensor)\n        assert isinstance(transition_dist, MultivariateNormal)\n        assert isinstance(observation_matrix, torch.Tensor)\n        assert isinstance(observation_dist, MultivariateNormal)\n        hidden_dim, obs_dim = observation_matrix.shape[-2:]\n        assert initial_dist.event_shape == (hidden_dim,)\n        assert transition_matrix.shape[-2:] == (hidden_dim, hidden_dim)\n        assert transition_dist.event_shape == (hidden_dim,)\n        assert observation_dist.event_shape == (obs_dim,)\n        shape = broadcast_shape(scale_dist.batch_shape + (1,),\n                                initial_dist.batch_shape + (1,),\n                                transition_matrix.shape[:-2],\n                                transition_dist.batch_shape,\n                                observation_matrix.shape[:-2],\n                                observation_dist.batch_shape)\n        batch_shape, time_shape = shape[:-1], shape[-1:]\n        event_shape = time_shape + (obs_dim,)\n        super().__init__(duration, batch_shape, event_shape, validate_args=validate_args)\n        self.hidden_dim = hidden_dim\n        self.obs_dim = obs_dim\n        self._init = gamma_and_mvn_to_gamma_gaussian(scale_dist, initial_dist)\n        self._trans = matrix_and_mvn_to_gamma_gaussian(transition_matrix, transition_dist)\n        self._obs = matrix_and_mvn_to_gamma_gaussian(observation_matrix, observation_dist)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(GammaGaussianHMM, _instance)\n        batch_shape = torch.Size(broadcast_shape(self.batch_shape, batch_shape))\n        new.hidden_dim = self.hidden_dim\n        new.obs_dim = self.obs_dim\n        # We only need to expand one of the inputs, since batch_shape is determined\n        # by broadcasting all three. To save computation in _sequential_gaussian_tensordot(),\n        # we expand only _init, which is applied only after _sequential_gaussian_tensordot().\n        new._init = self._init.expand(batch_shape)\n        new._trans = self._trans\n        new._obs = self._obs\n        super(GammaGaussianHMM, new).__init__(self.duration, batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n\n        # Combine observation and transition factors.\n        result = self._trans + self._obs.condition(value).event_pad(left=self.hidden_dim)\n\n        # Eliminate time dimension.\n        result = _sequential_gamma_gaussian_tensordot(result.expand(result.batch_shape))\n\n        # Combine initial factor.\n        result = gamma_gaussian_tensordot(self._init, result, dims=self.hidden_dim)\n\n        # Marginalize out final state.\n        result = result.event_logsumexp()\n\n        # Marginalize out multiplier.\n        result = result.logsumexp()\n        return result\n\n    def filter(self, value):\n        """"""\n        Compute posteriors over the multiplier and the final state\n        given a sequence of observations. The posterior is a pair of\n        Gamma and MultivariateNormal distributions (i.e. a GammaGaussian\n        instance).\n\n        :param ~torch.Tensor value: A sequence of observations.\n        :return: A pair of posterior distributions over the mixing and the latent\n            state at the final time step.\n        :rtype: a tuple of ~pyro.distributions.Gamma and ~pyro.distributions.MultivariateNormal\n        """"""\n        if self._validate_args:\n            self._validate_sample(value)\n\n        # Combine observation and transition factors.\n        logp = self._trans + self._obs.condition(value).event_pad(left=self.hidden_dim)\n\n        # Eliminate time dimension.\n        logp = _sequential_gamma_gaussian_tensordot(logp.expand(logp.batch_shape))\n\n        # Combine initial factor.\n        logp = gamma_gaussian_tensordot(self._init, logp, dims=self.hidden_dim)\n\n        # Posterior of the scale\n        gamma_dist = logp.event_logsumexp()\n        scale_post = Gamma(gamma_dist.concentration, gamma_dist.rate,\n                           validate_args=self._validate_args)\n        # Conditional of last state on unit scale\n        scale_tril = cholesky(logp.precision)\n        loc = cholesky_solve(logp.info_vec.unsqueeze(-1), scale_tril).squeeze(-1)\n        mvn = MultivariateNormal(loc, scale_tril=scale_tril,\n                                 validate_args=self._validate_args)\n        return scale_post, mvn\n\n\nclass LinearHMM(HiddenMarkovModel):\n    r""""""\n    Hidden Markov Model with linear dynamics and observations and arbitrary\n    noise for initial, transition, and observation distributions.  Each of\n    those distributions can be e.g.\n    :class:`~pyro.distributions.MultivariateNormal` or\n    :class:`~pyro.distributions.Independent` of\n    :class:`~pyro.distributions.Normal`,\n    :class:`~pyro.distributions.StudentT`, or :class:`~pyro.distributions.Stable` .\n    Additionally the observation distribution may be constrained, e.g.\n    :class:`~pyro.distributions.LogNormal`\n\n    This corresponds to the generative model::\n\n        z = initial_distribution.sample()\n        x = []\n        for t in range(num_events):\n            z = z @ transition_matrix + transition_dist.sample()\n            y = z @ observation_matrix + obs_base_dist.sample()\n            x.append(obs_transform(y))\n\n    where ``observation_dist`` is split into ``obs_base_dist`` and an optional\n    ``obs_transform`` (defaulting to the identity).\n\n    This implements a reparameterized :meth:`rsample` method but does not\n    implement a :meth:`log_prob` method. Derived classes may implement\n    :meth:`log_prob` .\n\n    Inference without :meth:`log_prob` can be performed using either\n    reparameterization with :class:`~pyro.infer.reparam.hmm.LinearHMMReparam`\n    or likelihood-free algorithms such as\n    :class:`~pyro.infer.energy_distance.EnergyDistance` .  Note that while\n    stable processes generally require a common shared stability parameter\n    :math:`\\alpha` , this distribution and the above inference algorithms allow\n    heterogeneous stability parameters.\n\n    The event_shape of this distribution includes time on the left::\n\n        event_shape = (num_steps,) + observation_dist.event_shape\n\n    This distribution supports any combination of homogeneous/heterogeneous\n    time dependency of ``transition_dist`` and ``observation_dist``. However at\n    least one of the distributions or matrices must be expanded to contain the\n    time dimension.\n\n    :ivar int hidden_dim: The dimension of the hidden state.\n    :ivar int obs_dim: The dimension of the observed state.\n    :param initial_dist: A distribution over initial states. This should have\n        batch_shape broadcastable to ``self.batch_shape``.  This should have\n        event_shape ``(hidden_dim,)``.\n    :param ~torch.Tensor transition_matrix: A linear transformation of hidden\n        state. This should have shape broadcastable to ``self.batch_shape +\n        (num_steps, hidden_dim, hidden_dim)`` where the rightmost dims are\n        ordered ``(old, new)``.\n    :param transition_dist: A distribution over process noise. This should have\n        batch_shape broadcastable to ``self.batch_shape + (num_steps,)``.  This\n        should have event_shape ``(hidden_dim,)``.\n    :param ~torch.Tensor observation_matrix: A linear transformation from hidden\n        to observed state. This should have shape broadcastable to\n        ``self.batch_shape + (num_steps, hidden_dim, obs_dim)``.\n    :param observation_dist: A observation noise distribution. This should have\n        batch_shape broadcastable to ``self.batch_shape + (num_steps,)``.  This\n        should have event_shape ``(obs_dim,)``.\n    :param int duration: Optional size of the time axis ``event_shape[0]``.\n        This is required when sampling from homogeneous HMMs whose parameters\n        are not expanded along the time axis.\n    """"""\n    arg_constraints = {}\n    support = constraints.real\n    has_rsample = True\n\n    def __init__(self, initial_dist, transition_matrix, transition_dist,\n                 observation_matrix, observation_dist,\n                 validate_args=None, duration=None):\n        assert initial_dist.has_rsample\n        assert initial_dist.event_dim == 1\n        assert (isinstance(transition_matrix, torch.Tensor) and\n                transition_matrix.dim() >= 2)\n        assert transition_dist.has_rsample\n        assert transition_dist.event_dim == 1\n        assert (isinstance(observation_matrix, torch.Tensor) and\n                observation_matrix.dim() >= 2)\n        assert observation_dist.has_rsample\n        assert observation_dist.event_dim == 1\n\n        hidden_dim, obs_dim = observation_matrix.shape[-2:]\n        assert initial_dist.event_shape == (hidden_dim,)\n        assert transition_matrix.shape[-2:] == (hidden_dim, hidden_dim)\n        assert transition_dist.event_shape == (hidden_dim,)\n        assert observation_dist.event_shape == (obs_dim,)\n        shape = broadcast_shape(initial_dist.batch_shape + (1,),\n                                transition_matrix.shape[:-2],\n                                transition_dist.batch_shape,\n                                observation_matrix.shape[:-2],\n                                observation_dist.batch_shape)\n        batch_shape, time_shape = shape[:-1], shape[-1:]\n        event_shape = time_shape + (obs_dim,)\n        super().__init__(duration, batch_shape, event_shape, validate_args=validate_args)\n\n        # Expand eagerly.\n        if initial_dist.batch_shape != batch_shape:\n            initial_dist = initial_dist.expand(batch_shape)\n        if transition_matrix.shape[:-2] != batch_shape + time_shape:\n            transition_matrix = transition_matrix.expand(\n                batch_shape + time_shape + (hidden_dim, hidden_dim))\n        if transition_dist.batch_shape != batch_shape + time_shape:\n            transition_dist = transition_dist.expand(batch_shape + time_shape)\n        if observation_matrix.shape[:-2] != batch_shape + time_shape:\n            observation_matrix = observation_matrix.expand(\n                batch_shape + time_shape + (hidden_dim, obs_dim))\n        if observation_dist.batch_shape != batch_shape + time_shape:\n            observation_dist = observation_dist.expand(batch_shape + time_shape)\n\n        # Extract observation transforms.\n        transforms = []\n        while True:\n            if isinstance(observation_dist, torch.distributions.Independent):\n                observation_dist = observation_dist.base_dist\n            elif isinstance(observation_dist, torch.distributions.TransformedDistribution):\n                transforms = observation_dist.transforms + transforms\n                observation_dist = observation_dist.base_dist\n            else:\n                break\n        if not observation_dist.event_shape:\n            observation_dist = Independent(observation_dist, 1)\n\n        self.hidden_dim = hidden_dim\n        self.obs_dim = obs_dim\n        self.initial_dist = initial_dist\n        self.transition_matrix = transition_matrix\n        self.transition_dist = transition_dist\n        self.observation_matrix = observation_matrix\n        self.observation_dist = observation_dist\n        self.transforms = transforms\n\n    @property\n    def support(self):\n        return self.observation_dist.support\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(LinearHMM, _instance)\n        batch_shape = torch.Size(batch_shape)\n        time_shape = self.transition_dist.batch_shape[-1:]\n        new.hidden_dim = self.hidden_dim\n        new.obs_dim = self.obs_dim\n        new.initial_dist = self.initial_dist.expand(batch_shape)\n        new.transition_matrix = self.transition_matrix.expand(\n            batch_shape + time_shape + (self.hidden_dim, self.hidden_dim))\n        new.transition_dist = self.transition_dist.expand(batch_shape + time_shape)\n        new.observation_matrix = self.observation_matrix.expand(\n            batch_shape + time_shape + (self.hidden_dim, self.obs_dim))\n        new.observation_dist = self.observation_dist.expand(batch_shape + time_shape)\n        new.transforms = self.transforms\n        super(LinearHMM, new).__init__(self.duration, batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n    def log_prob(self, value):\n        raise NotImplementedError(""LinearHMM.log_prob() is not implemented"")\n\n    def rsample(self, sample_shape=torch.Size()):\n        assert self.duration is not None\n        init = self.initial_dist.rsample(sample_shape)\n        trans = self.transition_dist.expand(self.batch_shape + (self.duration,)).rsample(sample_shape)\n        obs = self.observation_dist.expand(self.batch_shape + (self.duration,)).rsample(sample_shape)\n        trans_matrix = self.transition_matrix.expand(self.batch_shape + (self.duration, -1, -1))\n        z = _linear_integrate(init, trans_matrix, trans)\n        x = (z.unsqueeze(-2) @ self.observation_matrix).squeeze(-2) + obs\n        for t in self.transforms:\n            x = t(x)\n        return x\n\n\nclass IndependentHMM(TorchDistribution):\n    """"""\n    Wrapper class to treat a batch of independent univariate HMMs as a single\n    multivariate distribution. This converts distribution shapes as follows:\n\n    +-----------+--------------------+---------------------+\n    |           |       .batch_shape | .event_shape        |\n    +===========+====================+=====================+\n    | base_dist | shape + (obs_dim,) | (duration, 1)       |\n    +-----------+--------------------+---------------------+\n    |    result |              shape | (duration, obs_dim) |\n    +-----------+--------------------+---------------------+\n\n    :param HiddenMarkovModel base_dist: A base hidden Markov model instance.\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, base_dist):\n        assert base_dist.batch_shape\n        assert base_dist.event_dim == 2\n        assert base_dist.event_shape[-1] == 1\n        batch_shape = base_dist.batch_shape[:-1]\n        event_shape = base_dist.event_shape[:-1] + base_dist.batch_shape[-1:]\n        super().__init__(batch_shape, event_shape)\n        self.base_dist = base_dist\n\n    @constraints.dependent_property\n    def support(self):\n        return self.base_dist.support\n\n    @property\n    def has_rsample(self):\n        return self.base_dist.has_rsample\n\n    @property\n    def duration(self):\n        return self.base_dist.duration\n\n    def expand(self, batch_shape, _instance=None):\n        batch_shape = torch.Size(batch_shape)\n        new = self._get_checked_instance(IndependentHMM, _instance)\n        new.base_dist = self.base_dist.expand(batch_shape + self.base_dist.batch_shape[-1:])\n        super(IndependentHMM, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n    def rsample(self, sample_shape=torch.Size()):\n        base_value = self.base_dist.rsample(sample_shape)\n        return base_value.squeeze(-1).transpose(-1, -2)\n\n    def log_prob(self, value):\n        base_value = value.transpose(-1, -2).unsqueeze(-1)\n        return self.base_dist.log_prob(base_value).sum(-1)\n\n\nclass GaussianMRF(TorchDistribution):\n    """"""\n    Temporal Markov Random Field with Gaussian factors for initial, transition,\n    and observation distributions. This adapts [1] to parallelize over time to\n    achieve O(log(time)) parallel complexity, however it differs in that it\n    tracks the log normalizer to ensure :meth:`log_prob` is differentiable.\n\n    The event_shape of this distribution includes time on the left::\n\n        event_shape = (num_steps,) + observation_dist.event_shape\n\n    This distribution supports any combination of homogeneous/heterogeneous\n    time dependency of ``transition_dist`` and ``observation_dist``. However,\n    because time is included in this distribution\'s event_shape, the\n    homogeneous+homogeneous case will have a broadcastable event_shape with\n    ``num_steps = 1``, allowing :meth:`log_prob` to work with arbitrary length\n    data::\n\n        event_shape = (1, obs_dim)  # homogeneous + homogeneous case\n\n    **References:**\n\n    [1] Simo Sarkka, Angel F. Garcia-Fernandez (2019)\n        ""Temporal Parallelization of Bayesian Filters and Smoothers""\n        https://arxiv.org/pdf/1905.13002.pdf\n\n    :ivar int hidden_dim: The dimension of the hidden state.\n    :ivar int obs_dim: The dimension of the observed state.\n    :param ~torch.distributions.MultivariateNormal initial_dist: A distribution\n        over initial states. This should have batch_shape broadcastable to\n        ``self.batch_shape``.  This should have event_shape ``(hidden_dim,)``.\n    :param ~torch.distributions.MultivariateNormal transition_dist: A joint\n        distribution factor over a pair of successive time steps. This should\n        have batch_shape broadcastable to ``self.batch_shape + (num_steps,)``.\n        This should have event_shape ``(hidden_dim + hidden_dim,)`` (old+new).\n    :param ~torch.distributions.MultivariateNormal observation_dist: A joint\n        distribution factor over a hidden and an observed state. This should\n        have batch_shape broadcastable to ``self.batch_shape + (num_steps,)``.\n        This should have event_shape ``(hidden_dim + obs_dim,)``.\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, initial_dist, transition_dist, observation_dist, validate_args=None):\n        assert isinstance(initial_dist, torch.distributions.MultivariateNormal)\n        assert isinstance(transition_dist, torch.distributions.MultivariateNormal)\n        assert isinstance(observation_dist, torch.distributions.MultivariateNormal)\n        hidden_dim = initial_dist.event_shape[0]\n        assert transition_dist.event_shape[0] == hidden_dim + hidden_dim\n        obs_dim = observation_dist.event_shape[0] - hidden_dim\n\n        shape = broadcast_shape(initial_dist.batch_shape + (1,),\n                                transition_dist.batch_shape,\n                                observation_dist.batch_shape)\n        batch_shape, time_shape = shape[:-1], shape[-1:]\n        event_shape = time_shape + (obs_dim,)\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n        self.hidden_dim = hidden_dim\n        self.obs_dim = obs_dim\n        self._init = mvn_to_gaussian(initial_dist)\n        self._trans = mvn_to_gaussian(transition_dist)\n        self._obs = mvn_to_gaussian(observation_dist)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(GaussianMRF, _instance)\n        batch_shape = torch.Size(broadcast_shape(self.batch_shape, batch_shape))\n        new.hidden_dim = self.hidden_dim\n        new.obs_dim = self.obs_dim\n        # We only need to expand one of the inputs, since batch_shape is determined\n        # by broadcasting all three. To save computation in _sequential_gaussian_tensordot(),\n        # we expand only _init, which is applied only after _sequential_gaussian_tensordot().\n        new._init = self._init.expand(batch_shape)\n        new._trans = self._trans\n        new._obs = self._obs\n        super(GaussianMRF, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n    def log_prob(self, value):\n        # We compute a normalized distribution as p(obs,hidden) / p(hidden).\n        logp_oh = self._trans\n        logp_h = self._trans\n\n        # Combine observation and transition factors.\n        logp_oh += self._obs.condition(value).event_pad(left=self.hidden_dim)\n        logp_h += self._obs.marginalize(right=self.obs_dim).event_pad(left=self.hidden_dim)\n\n        # Concatenate p(obs,hidden) and p(hidden) into a single Gaussian.\n        batch_dim = 1 + max(len(self._init.batch_shape) + 1, len(logp_oh.batch_shape))\n        batch_shape = (1,) * (batch_dim - len(logp_oh.batch_shape)) + logp_oh.batch_shape\n        logp = Gaussian.cat([logp_oh.expand(batch_shape),\n                             logp_h.expand(batch_shape)])\n\n        # Eliminate time dimension.\n        logp = _sequential_gaussian_tensordot(logp)\n\n        # Combine initial factor.\n        logp = gaussian_tensordot(self._init, logp, dims=self.hidden_dim)\n\n        # Marginalize out final state.\n        logp_oh, logp_h = logp.event_logsumexp()\n        return logp_oh - logp_h  # = log( p(obs,hidden) / p(hidden) )\n'"
pyro/distributions/improper_uniform.py,7,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom .torch_distribution import TorchDistribution\nfrom .util import broadcast_shape\n\n\nclass ImproperUniform(TorchDistribution):\n    """"""\n    Improper distribution with zero :meth:`log_prob` and undefined\n    :meth:`sample`.\n\n    This is useful for transforming a model from generative dag form to factor\n    graph form for use in HMC. For example the following are equal in\n    distribution::\n\n        # Version 1. a generative dag\n        x = pyro.sample(""x"", Normal(0, 1))\n        y = pyro.sample(""y"", Normal(x, 1))\n        z = pyro.sample(""z"", Normal(y, 1))\n\n        # Version 2. a factor graph\n        xyz = pyro.sample(""xyz"", ImproperUniform(constraints.real, (), (3,)))\n        x, y, z = xyz.unbind(-1)\n        pyro.sample(""x"", Normal(0, 1), obs=x)\n        pyro.sample(""y"", Normal(x, 1), obs=y)\n        pyro.sample(""z"", Normal(y, 1), obs=z)\n\n    Note this distribution errors when :meth:`sample` is called. To create a\n    similar distribution that instead samples from a specified distribution\n    consider using ``.mask(False)`` as in::\n\n        xyz = dist.Normal(0, 1).expand([3]).to_event(1).mask(False)\n\n    :param support: The support of the distribution.\n    :type support: ~torch.distributions.constraints.Constraint\n    :param torch.Size batch_shape: The batch shape.\n    :param torch.Size event_shape: The event shape.\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, support, batch_shape, event_shape):\n        assert isinstance(support, constraints.Constraint)\n        self._support = support\n        super().__init__(batch_shape, event_shape)\n\n    @constraints.dependent_property\n    def support(self):\n        return self._support\n\n    def expand(self, batch_shape, _instance=None):\n        batch_shape = torch.Size(batch_shape)\n        new = self._get_checked_instance(ImproperUniform, _instance)\n        new._support = self._support\n        super(ImproperUniform, new).__init__(batch_shape, self.event_shape)\n        return new\n\n    def log_prob(self, value):\n        batch_shape = value.shape[:value.dim() - self.event_dim]\n        batch_shape = broadcast_shape(batch_shape, self.batch_shape)\n        return torch.zeros(()).expand(batch_shape)\n\n    def sample(self, sample_shape=torch.Size()):\n        raise NotImplementedError(""ImproperUniform does not support sampling"")\n'"
pyro/distributions/inverse_gamma.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import PowerTransform\nfrom pyro.distributions.torch import Gamma, TransformedDistribution\n\n\nclass InverseGamma(TransformedDistribution):\n    r""""""\n    Creates an inverse-gamma distribution parameterized by\n    `concentration` and `rate`.\n\n        X ~ Gamma(concentration, rate)\n        Y = 1/X ~ InverseGamma(concentration, rate)\n\n    :param torch.Tensor concentration: the concentration parameter (i.e. alpha).\n    :param torch.Tensor rate: the rate parameter (i.e. beta).\n    """"""\n    arg_constraints = {\'concentration\': constraints.positive, \'rate\': constraints.positive}\n    support = constraints.positive\n    has_rsample = True\n\n    def __init__(self, concentration, rate, validate_args=None):\n        base_dist = Gamma(concentration, rate)\n        super().__init__(base_dist, PowerTransform(-base_dist.rate.new_ones(())),\n                         validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(InverseGamma, _instance)\n        return super().expand(batch_shape, _instance=new)\n\n    @property\n    def concentration(self):\n        return self.base_dist.concentration\n\n    @property\n    def rate(self):\n        return self.base_dist.rate\n'"
pyro/distributions/kl.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nfrom torch.distributions import TransformedDistribution, kl_divergence, register_kl\n\nfrom pyro.distributions.delta import Delta\nfrom pyro.distributions.distribution import Distribution\nfrom pyro.distributions.torch import Independent, MultivariateNormal, Normal\nfrom pyro.distributions.util import sum_rightmost\n\n\n@register_kl(Delta, Distribution)\ndef _kl_delta(p, q):\n    return -q.log_prob(p.v)\n\n\n@register_kl(Independent, Independent)\ndef _kl_independent_independent(p, q):\n    shared_ndims = min(p.reinterpreted_batch_ndims, q.reinterpreted_batch_ndims)\n    p_ndims = p.reinterpreted_batch_ndims - shared_ndims\n    q_ndims = q.reinterpreted_batch_ndims - shared_ndims\n    p = Independent(p.base_dist, p_ndims) if p_ndims else p.base_dist\n    q = Independent(q.base_dist, q_ndims) if q_ndims else q.base_dist\n    kl = kl_divergence(p, q)\n    if shared_ndims:\n        kl = sum_rightmost(kl, shared_ndims)\n    return kl\n\n\n@register_kl(Independent, MultivariateNormal)\ndef _kl_independent_mvn(p, q):\n    if isinstance(p.base_dist, Delta) and p.reinterpreted_batch_ndims == 1:\n        return -q.log_prob(p.base_dist.v)\n\n    if isinstance(p.base_dist, Normal) and p.reinterpreted_batch_ndims == 1:\n        dim = q.event_shape[0]\n        p_cov = p.base_dist.scale ** 2\n        q_precision = q.precision_matrix.diagonal(dim1=-2, dim2=-1)\n        return (0.5 * (p_cov * q_precision).sum(-1)\n                - 0.5 * dim * (1 + math.log(2 * math.pi))\n                - q.log_prob(p.base_dist.loc)\n                - p.base_dist.scale.log().sum(-1))\n\n    raise NotImplementedError\n\n\n# TODO: move upstream\n@register_kl(TransformedDistribution, TransformedDistribution)\ndef _kl_transformed_transformed(p, q):\n    if p.transforms != q.transforms:\n        raise NotImplementedError\n    if p.event_shape != q.event_shape:\n        raise NotImplementedError\n    extra_event_dim = len(p.base_dist.batch_shape) - len(p.batch_shape)\n    base_kl_divergence = kl_divergence(p.base_dist, q.base_dist)\n    return sum_rightmost(base_kl_divergence, extra_event_dim)\n\n\n__all__ = []\n'"
pyro/distributions/lkj.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.constraints import corr_cholesky_constraint\nfrom pyro.distributions.torch import Beta\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.transforms.cholesky import _vector_to_l_cholesky\n\n\n# TODO: Modify class to support more than one eta value at a time?\nclass LKJCorrCholesky(TorchDistribution):\n    """"""\n    Generates cholesky factors of correlation matrices using an LKJ prior.\n\n    The expected use is to combine it with a vector of variances and pass it\n    to the scale_tril parameter of a multivariate distribution such as MultivariateNormal.\n\n    E.g., if theta is a (positive) vector of covariances with the same dimensionality\n    as this distribution, and Omega is sampled from this distribution,\n    scale_tril=torch.mm(torch.diag(sqrt(theta)), Omega)\n\n    Note that the `event_shape` of this distribution is `[d, d]`\n\n    .. note::\n\n       When using this distribution with HMC/NUTS, it is important to\n       use a `step_size` such as 1e-4. If not, you are likely to experience LAPACK\n       errors regarding positive-definiteness.\n\n    For example usage, refer to\n    `pyro/examples/lkj.py <https://github.com/pyro-ppl/pyro/blob/dev/examples/lkj.py>`_.\n\n    :param int d: Dimensionality of the matrix\n    :param torch.Tensor eta: A single positive number parameterizing the distribution.\n    """"""\n    arg_constraints = {""eta"": constraints.positive}\n    support = corr_cholesky_constraint\n    has_rsample = False\n\n    def __init__(self, d, eta, validate_args=None):\n        if eta.numel() != 1:\n            raise ValueError(""eta must be a single number; for a larger batch size, call expand"")\n        if d <= 1:\n            raise ValueError(""d must be > 1 in any correlation matrix"")\n        eta = eta.squeeze()\n        vector_size = (d * (d - 1)) // 2\n        alpha = eta.add(0.5 * (d - 1.0))\n\n        concentrations = torch.empty(vector_size, dtype=eta.dtype, device=eta.device)\n        i = 0\n        for k in range(d - 1):\n            alpha -= .5\n            concentrations[..., i:(i + d - k - 1)] = alpha\n            i += d - k - 1\n        self._gen = Beta(concentrations, concentrations)\n        self.eta = eta\n        self._d = d\n        self._lkj_constant = None\n        super().__init__(torch.Size(), torch.Size((d, d)), validate_args=validate_args)\n\n    def sample(self, sample_shape=torch.Size()):\n        y = self._gen.sample(sample_shape=self.batch_shape + sample_shape).detach()\n        z = y.mul(2).add(-1.0)\n        return _vector_to_l_cholesky(z)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(LKJCorrCholesky, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new._gen = self._gen\n        new.eta = self.eta\n        new._d = self._d\n        new._lkj_constant = self._lkj_constant\n        super(LKJCorrCholesky, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def lkj_constant(self, eta, K):\n        if self._lkj_constant is not None:\n            return self._lkj_constant\n\n        Km1 = K - 1\n\n        constant = torch.lgamma(eta.add(0.5 * Km1)).mul(Km1)\n\n        k = torch.linspace(start=1, end=Km1, steps=Km1, dtype=eta.dtype, device=eta.device)\n        constant -= (k.mul(math.log(math.pi) * 0.5) + torch.lgamma(eta.add(0.5 * (Km1 - k)))).sum()\n\n        self._lkj_constant = constant\n        return constant\n\n    def log_prob(self, x):\n        if self._validate_args:\n            self._validate_sample(x)\n\n        eta = self.eta\n\n        lp = self.lkj_constant(eta, self._d)\n\n        Km1 = self._d - 1\n\n        log_diagonals = x.diagonal(offset=0, dim1=-1, dim2=-2)[..., 1:].log()\n        # TODO: Figure out why the `device` kwarg to torch.linspace seems to not work in certain situations,\n        # and a seemingly redundant .to(x.device) is needed below.\n        values = log_diagonals * torch.linspace(start=Km1 - 1, end=0, steps=Km1,\n                                                dtype=x.dtype,\n                                                device=x.device).expand_as(log_diagonals).to(x.device)\n\n        values += log_diagonals.mul(eta.mul(2).add(-2.0))\n        values = values.sum(-1) + lp\n        values, _ = torch.broadcast_tensors(values, torch.empty(self.batch_shape))\n        return values\n'"
pyro/distributions/mixture.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.utils import lazy_property\n\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import broadcast_shape\n\n\nclass MaskedConstraint(constraints.Constraint):\n    """"""\n    Combines two constraints interleaved elementwise by a mask.\n\n    :param torch.Tensor mask: boolean mask tensor (of dtype ``torch.bool``)\n    :param torch.constraints.Constraint constraint0: constraint that holds\n        wherever ``mask == 0``\n    :param torch.constraints.Constraint constraint1: constraint that holds\n        wherever ``mask == 1``\n    """"""\n    def __init__(self, mask, constraint0, constraint1):\n        self.mask = mask\n        self.constraint0 = constraint0\n        self.constraint1 = constraint1\n\n    def check(self, value):\n        result = self.constraint0.check(value)\n        mask = self.mask.expand(result.shape) if result.shape != self.mask.shape else self.mask\n        result[mask] = self.constraint1.check(value)[mask]\n        return result\n\n\nclass MaskedMixture(TorchDistribution):\n    """"""\n    A masked deterministic mixture of two distributions.\n\n    This is useful when the mask is sampled from another distribution,\n    possibly correlated across the batch. Often the mask can be\n    marginalized out via enumeration.\n\n    Example::\n\n        change_point = pyro.sample(""change_point"",\n                                   dist.Categorical(torch.ones(len(data) + 1)),\n                                   infer={\'enumerate\': \'parallel\'})\n        mask = torch.arange(len(data), dtype=torch.long) >= changepoint\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", MaskedMixture(mask, dist1, dist2), obs=data)\n\n    :param torch.Tensor mask: A byte tensor toggling between ``component0``\n        and ``component1``.\n    :param pyro.distributions.TorchDistribution component0: a distribution\n        for batch elements ``mask == 0``.\n    :param pyro.distributions.TorchDistribution component1: a distribution\n        for batch elements ``mask == 1``.\n    """"""\n    arg_constraints = {}  # nothing can be constrained\n\n    def __init__(self, mask, component0, component1, validate_args=None):\n        if not torch.is_tensor(mask) or mask.dtype != torch.bool:\n            raise ValueError(\'Expected mask to be a BoolTensor but got {}\'.format(type(mask)))\n        if component0.event_shape != component1.event_shape:\n            raise ValueError(\'components event_shape disagree: {} vs {}\'\n                             .format(component0.event_shape, component1.event_shape))\n        batch_shape = broadcast_shape(mask.shape, component0.batch_shape, component1.batch_shape)\n        if mask.shape != batch_shape:\n            mask = mask.expand(batch_shape)\n        if component0.batch_shape != batch_shape:\n            component0 = component0.expand(batch_shape)\n        if component1.batch_shape != batch_shape:\n            component1 = component1.expand(batch_shape)\n\n        self.mask = mask\n        self.component0 = component0\n        self.component1 = component1\n        super().__init__(batch_shape, component0.event_shape, validate_args)\n\n        # We need to disable _validate_sample on each component since samples are only valid on the\n        # component from which they are drawn. Instead we perform validation using a MaskedConstraint.\n        self.component0._validate_args = False\n        self.component1._validate_args = False\n\n    @property\n    def has_rsample(self):\n        return self.component0.has_rsample and self.component1.has_rsample\n\n    @constraints.dependent_property\n    def support(self):\n        if self.component0.support is self.component1.support:\n            return self.component0.support\n        return MaskedConstraint(self.mask, self.component0.support, self.component1.support)\n\n    def expand(self, batch_shape):\n        try:\n            return super().expand(batch_shape)\n        except NotImplementedError:\n            mask = self.mask.expand(batch_shape)\n            component0 = self.component0.expand(batch_shape)\n            component1 = self.component1.expand(batch_shape)\n            return type(self)(mask, component0, component1)\n\n    def sample(self, sample_shape=torch.Size()):\n        mask = self.mask.reshape(self.mask.shape + (1,) * self.event_dim)\n        mask = mask.expand(sample_shape + self.shape())\n        result = torch.where(mask,\n                             self.component1.sample(sample_shape),\n                             self.component0.sample(sample_shape))\n        return result\n\n    def rsample(self, sample_shape=torch.Size()):\n        mask = self.mask.reshape(self.mask.shape + (1,) * self.event_dim)\n        mask = mask.expand(sample_shape + self.shape())\n        result = torch.where(mask,\n                             self.component1.rsample(sample_shape),\n                             self.component0.rsample(sample_shape))\n        return result\n\n    def log_prob(self, value):\n        value_shape = broadcast_shape(value.shape, self.batch_shape + self.event_shape)\n        if value.shape != value_shape:\n            value = value.expand(value_shape)\n        if self._validate_args:\n            self._validate_sample(value)\n        mask_shape = value_shape[:len(value_shape) - len(self.event_shape)]\n        mask = self.mask\n        if mask.shape != mask_shape:\n            mask = mask.expand(mask_shape)\n        result = torch.where(mask,\n                             self.component1.log_prob(value),\n                             self.component0.log_prob(value))\n        return result\n\n    @lazy_property\n    def mean(self):\n        result = self.component0.mean.clone()\n        result[self.mask] = self.component1.mean[self.mask]\n        return result\n\n    @lazy_property\n    def variance(self):\n        result = self.component0.variance.clone()\n        result[self.mask] = self.component1.variance[self.mask]\n        return result\n'"
pyro/distributions/multivariate_studentt.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.utils import lazy_property\n\nfrom pyro.distributions.torch import Chi2\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import broadcast_shape\n\n\nclass MultivariateStudentT(TorchDistribution):\n    """"""\n    Creates a multivariate Student\'s t-distribution parameterized by degree of\n    freedom :attr:`df`, mean :attr:`loc` and scale :attr:`scale_tril`.\n\n    :param ~torch.Tensor df: degrees of freedom\n    :param ~torch.Tensor loc: mean of the distribution\n    :param ~torch.Tensor scale_tril: scale of the distribution, which is\n        a lower triangular matrix with positive diagonal entries\n    """"""\n    arg_constraints = {\'df\': constraints.positive,\n                       \'loc\': constraints.real_vector,\n                       \'scale_tril\': constraints.lower_cholesky}\n    support = constraints.real_vector\n    has_rsample = True\n\n    def __init__(self, df, loc, scale_tril, validate_args=None):\n        dim = loc.size(-1)\n        assert scale_tril.shape[-2:] == (dim, dim)\n        if not isinstance(df, torch.Tensor):\n            df = loc.new_tensor(df)\n        batch_shape = broadcast_shape(df.shape, loc.shape[:-1], scale_tril.shape[:-2])\n        event_shape = (dim,)\n        self.df = df.expand(batch_shape)\n        self.loc = loc.expand(batch_shape + event_shape)\n        self._unbroadcasted_scale_tril = scale_tril\n        self._chi2 = Chi2(self.df)\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    @lazy_property\n    def scale_tril(self):\n        return self._unbroadcasted_scale_tril.expand(\n            self._batch_shape + self._event_shape + self._event_shape)\n\n    @lazy_property\n    def covariance_matrix(self):\n        # NB: this is not covariance of this distribution;\n        # the actual covariance is df / (df - 2) * covariance_matrix\n        return (torch.matmul(self._unbroadcasted_scale_tril,\n                             self._unbroadcasted_scale_tril.transpose(-1, -2))\n                .expand(self._batch_shape + self._event_shape + self._event_shape))\n\n    @lazy_property\n    def precision_matrix(self):\n        identity = torch.eye(self.loc.size(-1), device=self.loc.device, dtype=self.loc.dtype)\n        return torch.cholesky_solve(identity, self._unbroadcasted_scale_tril).expand(\n            self._batch_shape + self._event_shape + self._event_shape)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(MultivariateStudentT, _instance)\n        batch_shape = torch.Size(batch_shape)\n        loc_shape = batch_shape + self.event_shape\n        scale_shape = loc_shape + self.event_shape\n        new.df = self.df.expand(batch_shape)\n        new.loc = self.loc.expand(loc_shape)\n        new._unbroadcasted_scale_tril = self._unbroadcasted_scale_tril\n        if \'scale_tril\' in self.__dict__:\n            new.scale_tril = self.scale_tril.expand(scale_shape)\n        if \'covariance_matrix\' in self.__dict__:\n            new.covariance_matrix = self.covariance_matrix.expand(scale_shape)\n        if \'precision_matrix\' in self.__dict__:\n            new.precision_matrix = self.precision_matrix.expand(scale_shape)\n        new._chi2 = self._chi2.expand(batch_shape)\n        super(MultivariateStudentT, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def rsample(self, sample_shape=torch.Size()):\n        shape = self._extended_shape(sample_shape)\n        X = torch.empty(shape, dtype=self.df.dtype, device=self.df.device).normal_()\n        Z = self._chi2.rsample(sample_shape)\n        Y = X * torch.rsqrt(Z / self.df).unsqueeze(-1)\n        return self.loc + self.scale_tril.matmul(Y.unsqueeze(-1)).squeeze(-1)\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        n = self.loc.size(-1)\n        y = (value - self.loc).unsqueeze(-1).triangular_solve(self.scale_tril, upper=False).solution.squeeze(-1)\n        Z = (self.scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1) +\n             0.5 * n * self.df.log() +\n             0.5 * n * math.log(math.pi) +\n             torch.lgamma(0.5 * self.df) -\n             torch.lgamma(0.5 * (self.df + n)))\n        return -0.5 * (self.df + n) * torch.log1p(y.pow(2).sum(-1) / self.df) - Z\n\n    @property\n    def mean(self):\n        m = self.loc.clone()\n        m[self.df <= 1, :] = float(\'nan\')\n        return m\n\n    @property\n    def variance(self):\n        m = self.scale_tril.pow(2).sum(-1) * (self.df / (self.df - 2)).unsqueeze(-1)\n        m[(self.df <= 2) & (self.df > 1), :] = float(\'inf\')\n        m[self.df <= 1, :] = float(\'nan\')\n        return m\n'"
pyro/distributions/omt_mvn.py,18,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch import MultivariateNormal\nfrom pyro.distributions.util import eye_like, sum_leftmost\n\n\nclass OMTMultivariateNormal(MultivariateNormal):\n    """"""Multivariate normal (Gaussian) distribution with OMT gradients w.r.t. both\n    parameters. Note the gradient computation w.r.t. the Cholesky factor has cost\n    O(D^3), although the resulting gradient variance is generally expected to be lower.\n\n    A distribution over vectors in which all the elements have a joint Gaussian\n    density.\n\n    :param torch.Tensor loc: Mean.\n    :param torch.Tensor scale_tril: Cholesky of Covariance matrix.\n    """"""\n    arg_constraints = {""loc"": constraints.real, ""scale_tril"": constraints.lower_triangular}\n\n    def __init__(self, loc, scale_tril):\n        if loc.dim() != 1:\n            raise ValueError(""OMTMultivariateNormal loc must be 1-dimensional"")\n        if scale_tril.dim() != 2:\n            raise ValueError(""OMTMultivariateNormal scale_tril must be 2-dimensional"")\n        super().__init__(loc, scale_tril=scale_tril)\n\n    def rsample(self, sample_shape=torch.Size()):\n        return _OMTMVNSample.apply(self.loc, self.scale_tril, sample_shape + self.loc.shape)\n\n\nclass _OMTMVNSample(Function):\n    @staticmethod\n    def forward(ctx, loc, scale_tril, shape):\n        white = torch.randn(shape, dtype=loc.dtype, device=loc.device)\n        z = torch.matmul(white, scale_tril.t())\n        ctx.save_for_backward(z, white, scale_tril)\n        return loc + z\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        jitter = 1.0e-8  # do i really need this?\n        z, epsilon, L = ctx.saved_tensors\n\n        dim = L.shape[0]\n        g = grad_output\n        loc_grad = sum_leftmost(grad_output, -1)\n\n        identity = eye_like(g, dim)\n        R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0]\n\n        z_ja = z.unsqueeze(-1)\n        g_R_inv = torch.matmul(g, R_inv).unsqueeze(-2)\n        epsilon_jb = epsilon.unsqueeze(-2)\n        g_ja = g.unsqueeze(-1)\n        diff_L_ab = 0.5 * sum_leftmost(g_ja * epsilon_jb + g_R_inv * z_ja, -2)\n\n        Sigma_inv = torch.mm(R_inv, R_inv.t())\n        V, D, _ = torch.svd(Sigma_inv + jitter)\n        D_outer = D.unsqueeze(-1) + D.unsqueeze(0)\n\n        expand_tuple = tuple([-1] * (z.dim() - 1) + [dim, dim])\n        z_tilde = identity * torch.matmul(z, V).unsqueeze(-1).expand(*expand_tuple)\n        g_tilde = identity * torch.matmul(g, V).unsqueeze(-1).expand(*expand_tuple)\n\n        Y = sum_leftmost(torch.matmul(z_tilde, torch.matmul(1.0 / D_outer, g_tilde)), -2)\n        Y = torch.mm(V, torch.mm(Y, V.t()))\n        Y = Y + Y.t()\n\n        Tr_xi_Y = torch.mm(torch.mm(Sigma_inv, Y), R_inv) - torch.mm(Y, torch.mm(Sigma_inv, R_inv))\n        diff_L_ab += 0.5 * Tr_xi_Y\n        L_grad = torch.tril(diff_L_ab)\n\n        return loc_grad, L_grad, None\n'"
pyro/distributions/polya_gamma.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch import Exponential\nfrom pyro.distributions.torch_distribution import TorchDistribution\n\n\nclass TruncatedPolyaGamma(TorchDistribution):\n    """"""\n    This is a PolyaGamma(1, 0) distribution truncated to have finite support in\n    the interval (0, 2.5). See [1] for details. As a consequence of the truncation\n    the `log_prob` method is only accurate to about six decimal places. In\n    addition the provided sampler is a rough approximation that is only meant to\n    be used in contexts where sample accuracy is not important (e.g. in initialization).\n    Broadly, this implementation is only intended for usage in cases where good\n    approximations of the `log_prob` are sufficient, as is the case e.g. in HMC.\n\n    :param tensor prototype: A prototype tensor of arbitrary shape used to determine\n        the `dtype` and `device` returned by `sample` and `log_prob`.\n\n    References\n\n    [1] \'Bayesian inference for logistic models using Polya-Gamma latent variables\'\n        Nicholas G. Polson, James G. Scott, Jesse Windle.\n    """"""\n    truncation_point = 2.5\n    num_log_prob_terms = 7\n    num_gamma_variates = 8\n    assert num_log_prob_terms % 2 == 1\n\n    arg_constraints = {}\n    support = constraints.interval(0.0, truncation_point)\n    has_rsample = False\n\n    def __init__(self, prototype, validate_args=None):\n        self.prototype = prototype\n        super(TruncatedPolyaGamma, self).__init__(batch_shape=(), event_shape=(), validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(TruncatedPolyaGamma, _instance)\n        super(TruncatedPolyaGamma, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self.__dict__.get(""_validate_args"")\n        new.prototype = self.prototype\n        return new\n\n    def sample(self, sample_shape=()):\n        denom = torch.arange(0.5, self.num_gamma_variates, device=self.prototype.device).pow(2.0)\n        ones = self.prototype.new_ones((self.num_gamma_variates))\n        x = Exponential(ones).sample(self.batch_shape + sample_shape)\n        x = (x / denom).sum(-1)\n        return torch.clamp(x * (0.5 / math.pi ** 2), max=self.truncation_point)\n\n    def log_prob(self, value):\n        value = value.unsqueeze(-1)\n        two_n_plus_one = 2.0 * torch.arange(0, self.num_log_prob_terms, device=self.prototype.device) + 1.0\n        log_terms = two_n_plus_one.log() - 1.5 * value.log() - 0.125 * two_n_plus_one.pow(2.0) / value\n        even_terms = log_terms[..., ::2]\n        odd_terms = log_terms[..., 1::2]\n        sum_even = torch.logsumexp(even_terms, dim=-1).exp()\n        sum_odd = torch.logsumexp(odd_terms, dim=-1).exp()\n        return (sum_even - sum_odd).log() - 0.5 * math.log(2.0 * math.pi)\n'"
pyro/distributions/rejector.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.distributions.score_parts import ScoreParts\nfrom pyro.distributions.torch_distribution import TorchDistribution\n\n\nclass Rejector(TorchDistribution):\n    """"""\n    Rejection sampled distribution given an acceptance rate function.\n\n    :param Distribution propose: A proposal distribution that samples batched\n        proposals via ``propose()``. :meth:`rsample` supports a ``sample_shape``\n        arg only if ``propose()`` supports a ``sample_shape`` arg.\n    :param callable log_prob_accept: A callable that inputs a batch of\n        proposals and returns a batch of log acceptance probabilities.\n    :param log_scale: Total log probability of acceptance.\n    """"""\n    arg_constraints = {}\n    has_rsample = True\n\n    def __init__(self, propose, log_prob_accept, log_scale, *,\n                 batch_shape=None, event_shape=None):\n        self.propose = propose\n        self.log_prob_accept = log_prob_accept\n        self._log_scale = log_scale\n        if batch_shape is None:\n            batch_shape = propose.batch_shape\n        if event_shape is None:\n            event_shape = propose.event_shape\n        super().__init__(batch_shape, event_shape)\n\n        # These LRU(1) caches allow work to be shared across different method calls.\n        self._log_prob_accept_cache = None, None\n        self._propose_log_prob_cache = None, None\n\n    def _log_prob_accept(self, x):\n        if x is not self._log_prob_accept_cache[0]:\n            self._log_prob_accept_cache = x, self.log_prob_accept(x) - self._log_scale\n        return self._log_prob_accept_cache[1]\n\n    def _propose_log_prob(self, x):\n        if x is not self._propose_log_prob_cache[0]:\n            self._propose_log_prob_cache = x, self.propose.log_prob(x)\n        return self._propose_log_prob_cache[1]\n\n    def rsample(self, sample_shape=torch.Size()):\n        # Implements parallel batched accept-reject sampling.\n        x = self.propose(sample_shape) if sample_shape else self.propose()\n        log_prob_accept = self.log_prob_accept(x)\n        probs = torch.exp(log_prob_accept).clamp_(0.0, 1.0)\n        done = torch.bernoulli(probs).bool()\n        while not done.all():\n            proposed_x = self.propose(sample_shape) if sample_shape else self.propose()\n            log_prob_accept = self.log_prob_accept(proposed_x)\n            prob_accept = torch.exp(log_prob_accept).clamp_(0.0, 1.0)\n            accept = torch.bernoulli(prob_accept).bool() & ~done\n            if accept.any():\n                x[accept] = proposed_x[accept]\n                done |= accept\n        return x\n\n    def log_prob(self, x):\n        return self._propose_log_prob(x) + self._log_prob_accept(x)\n\n    def score_parts(self, x):\n        score_function = self._log_prob_accept(x)\n        log_prob = self.log_prob(x)\n        return ScoreParts(log_prob, score_function, log_prob)\n'"
pyro/distributions/relaxed_straight_through.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.distributions.torch import RelaxedOneHotCategorical, RelaxedBernoulli\nfrom pyro.distributions.util import copy_docs_from\nfrom torch.distributions.utils import clamp_probs\n\n\n@copy_docs_from(RelaxedOneHotCategorical)\nclass RelaxedOneHotCategoricalStraightThrough(RelaxedOneHotCategorical):\n    """"""\n    An implementation of\n    :class:`~torch.distributions.relaxed_categorical.RelaxedOneHotCategorical`\n    with a straight-through gradient estimator.\n\n    This distribution has the following properties:\n\n    - The samples returned by the :meth:`rsample` method are discrete/quantized.\n    - The :meth:`log_prob` method returns the log probability of the\n      relaxed/unquantized sample using the GumbelSoftmax distribution.\n    - In the backward pass the gradient of the sample with respect to the\n      parameters of the distribution uses the relaxed/unquantized sample.\n\n    References:\n\n    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,\n        Chris J. Maddison, Andriy Mnih, Yee Whye Teh\n    [2] Categorical Reparameterization with Gumbel-Softmax,\n        Eric Jang, Shixiang Gu, Ben Poole\n    """"""\n    def rsample(self, sample_shape=torch.Size()):\n        soft_sample = super().rsample(sample_shape)\n        soft_sample = clamp_probs(soft_sample)\n        hard_sample = QuantizeCategorical.apply(soft_sample)\n        return hard_sample\n\n    def log_prob(self, value):\n        value = getattr(value, \'_unquantize\', value)\n        return super().log_prob(value)\n\n\nclass QuantizeCategorical(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, soft_value):\n        argmax = soft_value.max(-1)[1]\n        hard_value = torch.zeros_like(soft_value)\n        hard_value._unquantize = soft_value\n        if argmax.dim() < hard_value.dim():\n            argmax = argmax.unsqueeze(-1)\n        return hard_value.scatter_(-1, argmax, 1)\n\n    @staticmethod\n    def backward(ctx, grad):\n        return grad\n\n\n@copy_docs_from(RelaxedBernoulli)\nclass RelaxedBernoulliStraightThrough(RelaxedBernoulli):\n    """"""\n    An implementation of\n    :class:`~torch.distributions.relaxed_bernoulli.RelaxedBernoulli`\n    with a straight-through gradient estimator.\n\n    This distribution has the following properties:\n\n    - The samples returned by the :meth:`rsample` method are discrete/quantized.\n    - The :meth:`log_prob` method returns the log probability of the\n      relaxed/unquantized sample using the GumbelSoftmax distribution.\n    - In the backward pass the gradient of the sample with respect to the\n      parameters of the distribution uses the relaxed/unquantized sample.\n\n    References:\n\n    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,\n        Chris J. Maddison, Andriy Mnih, Yee Whye Teh\n    [2] Categorical Reparameterization with Gumbel-Softmax,\n        Eric Jang, Shixiang Gu, Ben Poole\n    """"""\n    def rsample(self, sample_shape=torch.Size()):\n        soft_sample = super().rsample(sample_shape)\n        soft_sample = clamp_probs(soft_sample)\n        hard_sample = QuantizeBernoulli.apply(soft_sample)\n        return hard_sample\n\n    def log_prob(self, value):\n        value = getattr(value, \'_unquantize\', value)\n        return super().log_prob(value)\n\n\nclass QuantizeBernoulli(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, soft_value):\n        hard_value = soft_value.round()\n        hard_value._unquantize = soft_value\n        return hard_value\n\n    @staticmethod\n    def backward(ctx, grad):\n        return grad\n'"
pyro/distributions/score_parts.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\n\nfrom pyro.distributions.util import scale_and_mask\n\n\nclass ScoreParts(namedtuple(\'ScoreParts\', [\'log_prob\', \'score_function\', \'entropy_term\'])):\n    """"""\n    This data structure stores terms used in stochastic gradient estimators that\n    combine the pathwise estimator and the score function estimator.\n    """"""\n    def scale_and_mask(self, scale=1.0, mask=None):\n        """"""\n        Scale and mask appropriate terms of a gradient estimator by a data multiplicity factor.\n        Note that the `score_function` term should not be scaled or masked.\n\n        :param scale: a positive scale\n        :type scale: torch.Tensor or number\n        :param mask: an optional masking tensor\n        :type mask: torch.BoolTensor or None\n        """"""\n        log_prob = scale_and_mask(self.log_prob, scale, mask)\n        score_function = self.score_function  # not scaled\n        entropy_term = scale_and_mask(self.entropy_term, scale, mask)\n        return ScoreParts(log_prob, score_function, entropy_term)\n'"
pyro/distributions/spanning_tree.py,33,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport math\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.utils import lazy_property\n\nfrom pyro.distributions.torch_distribution import TorchDistribution\n\n\nclass SpanningTree(TorchDistribution):\n    """"""\n    Distribution over spanning trees on a fixed number ``V`` of vertices.\n\n    A tree is represented as :class:`torch.LongTensor` ``edges`` of shape\n    ``(V-1,2)`` satisfying the following properties:\n\n    1. The edges constitute a tree, i.e. are connected and cycle free.\n    2. Each edge ``(v1,v2) = edges[e]`` is sorted, i.e. ``v1 < v2``.\n    3. The entire tensor is sorted in colexicographic order.\n\n    Use :func:`validate_edges` to verify `edges` are correctly formed.\n\n    The ``edge_logits`` tensor has one entry for each of the ``V*(V-1)//2``\n    edges in the complete graph on ``V`` vertices, where edges are each sorted\n    and the edge order is colexicographic::\n\n        (0,1), (0,2), (1,2), (0,3), (1,3), (2,3), (0,4), (1,4), (2,4), ...\n\n    This ordering corresponds to the size-independent pairing function::\n\n        k = v1 + v2 * (v2 - 1) // 2\n\n    where ``k`` is the rank of the edge ``(v1,v2)`` in the complete graph.\n    To convert a matrix of edge logits to the linear representation used here::\n\n        assert my_matrix.shape == (V, V)\n        i, j = make_complete_graph(V)\n        edge_logits = my_matrix[i, j]\n\n    :param torch.Tensor edge_logits: A tensor of length ``V*(V-1)//2``\n        containing logits (aka negative energies) of all edges in the complete\n        graph on ``V`` vertices. See above comment for edge ordering.\n    :param dict sampler_options: An optional dict of sampler options including:\n        ``mcmc_steps`` defaulting to a single MCMC step (which is pretty good);\n        ``initial_edges`` defaulting to a cheap approximate sample;\n        ``backend`` one of ""python"" or ""cpp"", defaulting to ""python"".\n    """"""\n    arg_constraints = {\'edge_logits\': constraints.real}\n    support = constraints.nonnegative_integer\n    has_enumerate_support = True\n\n    def __init__(self, edge_logits, sampler_options=None, validate_args=None):\n        if edge_logits.is_cuda:\n            raise NotImplementedError(""SpanningTree does not support cuda tensors"")\n        K = len(edge_logits)\n        V = int(round(0.5 + (0.25 + 2 * K)**0.5))\n        assert K == V * (V - 1) // 2\n        E = V - 1\n        event_shape = (E, 2)\n        batch_shape = ()\n        self.edge_logits = edge_logits\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n        if self._validate_args:\n            if edge_logits.shape != (K,):\n                raise ValueError(""Expected edge_logits of shape ({},), but got shape {}""\n                                 .format(K, edge_logits.shape))\n        self.num_vertices = V\n        self.sampler_options = {} if sampler_options is None else sampler_options\n\n    def validate_edges(self, edges):\n        """"""\n        Validates a batch of ``edges`` tensors, as returned by :meth:`sample` or\n        :meth:`enumerate_support` or as input to :meth:`log_prob()`.\n\n        :param torch.LongTensor edges: A batch of edges.\n        :raises: ValueError\n        :returns: None\n        """"""\n        if edges.shape[-2:] != self.event_shape:\n            raise ValueError(""Invalid edges shape: {}"".format(edges.shape))\n\n        # Verify canonical ordering.\n        if not ((0 <= edges) & (edges < self.num_vertices)).all():\n            raise ValueError(""Invalid vertex ids:\\n{}"".format(edges))\n        if not (edges[..., 0] < edges[..., 1]).all():\n            raise ValueError(""Vertices are not sorted in each edge:\\n{}"".format(edges))\n        if not ((edges[..., :-1, 1] < edges[..., 1:, 1]) |\n                ((edges[..., :-1, 1] == edges[..., 1:, 1]) &\n                 (edges[..., :-1, 0] < edges[..., 1:, 0]))).all():\n            raise ValueError(""Edges are not sorted colexicographically:\\n{}"".format(edges))\n\n        # Verify tree property, i.e. connectivity.\n        V = self.num_vertices\n        for i in itertools.product(*map(range, edges.shape[:-2])):\n            edges_i = edges[i]\n            connected = torch.eye(V, dtype=torch.float)\n            connected[edges_i[:, 0], edges_i[:, 1]] = 1\n            connected[edges_i[:, 1], edges_i[:, 0]] = 1\n            for i in range(int(math.ceil(V ** 0.5))):\n                connected = connected.mm(connected).clamp_(max=1)\n            if not connected.min() > 0:\n                raise ValueError(""Edges do not constitute a tree:\\n{}"".format(edges_i))\n\n    @lazy_property\n    def log_partition_function(self):\n        # By Kirchoff\'s matrix-tree theorem, the partition function is the\n        # determinant of a truncated version of the graph Laplacian matrix. We\n        # use a Cholesky decomposition to compute the log determinant.\n        # See https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem\n        V = self.num_vertices\n        grid = make_complete_graph(V)\n        shift = self.edge_logits.max()\n        edge_probs = (self.edge_logits - shift).exp()\n        adjacency = torch.zeros(V, V, dtype=edge_probs.dtype)\n        adjacency[grid[0], grid[1]] = edge_probs\n        adjacency[grid[1], grid[0]] = edge_probs\n        laplacian = adjacency.sum(-1).diag() - adjacency\n        truncated = laplacian[:-1, :-1]\n        log_det = torch.cholesky(truncated).diag().log().sum() * 2\n        return log_det + shift * (V - 1)\n\n    def log_prob(self, edges):\n        if self._validate_args:\n            self.validate_edges(edges)\n        v1 = edges[..., 0]\n        v2 = edges[..., 1]\n        k = v1 + v2 * (v2 - 1) // 2\n        return self.edge_logits[k].sum(-1) - self.log_partition_function\n\n    def sample(self, sample_shape=torch.Size()):\n        """"""\n        This sampler is implemented using MCMC run for a small number of steps\n        after being initialized by a cheap approximate sampler. This sampler is\n        approximate and cubic time. This is faster than the classic\n        Aldous-Broder sampler [1,2], especially for graphs with large mixing\n        time. Recent research [3,4] proposes samplers that run in\n        sub-matrix-multiply time but are more complex to implement.\n\n        **References**\n\n        [1] `Generating random spanning trees`\n            Andrei Broder (1989)\n        [2] `The Random Walk Construction of Uniform Spanning Trees and Uniform Labelled Trees`,\n            David J. Aldous (1990)\n        [3] `Sampling Random Spanning Trees Faster than Matrix Multiplication`,\n            David Durfee, Rasmus Kyng, John Peebles, Anup B. Rao, Sushant Sachdeva\n            (2017) https://arxiv.org/abs/1611.07451\n        [4] `An almost-linear time algorithm for uniform random spanning tree generation`,\n            Aaron Schild (2017) https://arxiv.org/abs/1711.06455\n        """"""\n        if sample_shape:\n            raise NotImplementedError(""SpanningTree does not support batching"")\n        edges = sample_tree(self.edge_logits, **self.sampler_options)\n        assert edges.dim() >= 2 and edges.shape[-2:] == self.event_shape\n        return edges\n\n    def enumerate_support(self, expand=True):\n        """"""\n        This is implemented for trees with up to 6 vertices (and 5 edges).\n        """"""\n        trees = enumerate_spanning_trees(self.num_vertices)\n        return torch.tensor(trees, dtype=torch.long)\n\n\n################################################################################\n# Sampler implementation.\n################################################################################\n\n_cpp_module = None\n\n\ndef _get_cpp_module():\n    """"""\n    JIT compiles the cpp_spanning_tree module.\n    """"""\n    global _cpp_module\n    if _cpp_module is None:\n        import os\n        from torch.utils.cpp_extension import load\n        path = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""spanning_tree.cpp"")\n        _cpp_module = load(name=""cpp_spanning_tree"",\n                           sources=[path],\n                           extra_cflags=[\'-O2\'],\n                           verbose=True)\n    return _cpp_module\n\n\ndef make_complete_graph(num_vertices, backend=""python""):\n    """"""\n    Constructs a complete graph.\n\n    The pairing function is: ``k = v1 + v2 * (v2 - 1) // 2``\n\n    :param int num_vertices: Number of vertices.\n    :returns: a 2 x K grid of (vertex, vertex) pairs.\n    """"""\n    if backend == ""python"":\n        return _make_complete_graph(num_vertices)\n    elif backend == ""cpp"":\n        return _get_cpp_module().make_complete_graph(num_vertices)\n    else:\n        raise ValueError(""unknown backend: {}"".format(repr(backend)))\n\n\ndef _make_complete_graph(num_vertices):\n    if num_vertices < 2:\n        raise ValueError(\'PyTorch cannot handle zero-sized multidimensional tensors\')\n    V = num_vertices\n    K = V * (V - 1) // 2\n    v1 = torch.arange(V)\n    v2 = torch.arange(V).unsqueeze(-1)\n    v1, v2 = torch.broadcast_tensors(v1, v2)\n    v1 = v1.contiguous().view(-1)\n    v2 = v2.contiguous().view(-1)\n    mask = (v1 < v2)\n    grid = torch.stack((v1[mask], v2[mask]))\n    assert grid.shape == (2, K)\n    return grid\n\n\ndef _remove_edge(grid, edge_ids, neighbors, components, e):\n    """"""\n    Remove an edge from a spanning tree.\n    """"""\n    k = edge_ids[e]\n    v1 = grid[0, k].item()\n    v2 = grid[1, k].item()\n    neighbors[v1].remove(v2)\n    neighbors[v2].remove(v1)\n    components[v1] = 1\n    pending = [v1]\n    while pending:\n        v1 = pending.pop()\n        for v2 in neighbors[v1]:\n            if not components[v2]:\n                components[v2] = 1\n                pending.append(v2)\n    return k\n\n\ndef _add_edge(grid, edge_ids, neighbors, components, e, k):\n    """"""\n    Add an edge connecting two components to create a spanning tree.\n    """"""\n    edge_ids[e] = k\n    v1 = grid[0, k].item()\n    v2 = grid[1, k].item()\n    neighbors[v1].add(v2)\n    neighbors[v2].add(v1)\n    components.fill_(0)\n\n\ndef _find_valid_edges(components, valid_edge_ids):\n    """"""\n    Find all edges between two components in a complete undirected graph.\n\n    :param components: A [V]-shaped array of boolean component ids. This\n        assumes there are exactly two nonemtpy components.\n    :param valid_edge_ids: An uninitialized array where output is written. On\n        return, the subarray valid_edge_ids[:end] will contain edge ids k for all\n        valid edges.\n    :returns: The number of valid edges found.\n    """"""\n    k = 0\n    end = 0\n    for v2, c2 in enumerate(components):\n        for v1 in range(v2):\n            if c2 ^ components[v1]:\n                valid_edge_ids[end] = k\n                end += 1\n            k += 1\n    return end\n\n\n@torch.no_grad()\ndef _sample_tree_mcmc(edge_logits, edges):\n    if len(edges) <= 1:\n        return edges\n\n    E = len(edges)\n    V = E + 1\n    K = V * (V - 1) // 2\n    grid = make_complete_graph(V)\n\n    # Each of E edges in the tree is stored as an id k in [0, K) indexing into\n    # the complete graph. The id of an edge (v1,v2) is k = v1+v2*(v2-1)/2.\n    edge_ids = torch.empty(E, dtype=torch.long)\n    # This maps each vertex to the set of its neighboring vertices.\n    neighbors = {v: set() for v in range(V)}\n    # This maps each vertex to its connected component id (0 or 1).\n    components = torch.zeros(V, dtype=torch.bool)\n    for e in range(E):\n        v1, v2 = map(int, edges[e])\n        assert v1 < v2\n        edge_ids[e] = v1 + v2 * (v2 - 1) // 2\n        neighbors[v1].add(v2)\n        neighbors[v2].add(v1)\n    # This stores ids of edges that are valid candidates for Gibbs moves.\n    valid_edges_buffer = torch.empty(K, dtype=torch.long)\n\n    # Cycle through all edges in a random order.\n    for e in torch.randperm(E):\n        e = int(e)\n\n        # Perform a single-site Gibbs update by moving this edge elsewhere.\n        k = _remove_edge(grid, edge_ids, neighbors, components, e)\n        num_valid_edges = _find_valid_edges(components, valid_edges_buffer)\n        valid_edge_ids = valid_edges_buffer[:num_valid_edges]\n        valid_logits = edge_logits[valid_edge_ids]\n        valid_probs = (valid_logits - valid_logits.max()).exp()\n        total_prob = valid_probs.sum()\n        if total_prob > 0:\n            sample = torch.multinomial(valid_probs, 1)[0]\n            k = valid_edge_ids[sample]\n        _add_edge(grid, edge_ids, neighbors, components, e, k)\n\n    # Convert edge ids to a canonical list of pairs.\n    edge_ids = edge_ids.sort()[0]\n    edges = torch.empty((E, 2), dtype=torch.long)\n    edges[:, 0] = grid[0, edge_ids]\n    edges[:, 1] = grid[1, edge_ids]\n    return edges\n\n\ndef sample_tree_mcmc(edge_logits, edges, backend=""python""):\n    """"""\n    Sample a random spanning tree of a dense weighted graph using MCMC.\n\n    This uses Gibbs sampling on edges. Consider E undirected edges that can\n    move around a graph of ``V=1+E`` vertices. The edges are constrained so\n    that no two edges can span the same pair of vertices and so that the edges\n    must form a spanning tree. To Gibbs sample, chose one of the E edges at\n    random and move it anywhere else in the graph. After we remove the edge,\n    notice that the graph is split into two connected components. The\n    constraints imply that the edge must be replaced so as to connect the two\n    components.  Hence to Gibbs sample, we collect all such bridging\n    (vertex,vertex) pairs and sample from them in proportion to\n    ``exp(edge_logits)``.\n\n    :param torch.Tensor edge_logits: A length-K array of nonnormalized log\n        probabilities.\n    :param torch.Tensor edges: An E x 2 tensor of initial edges in the form\n        of (vertex,vertex) pairs. Each edge should be sorted and the entire\n        tensor should be lexicographically sorted.\n    :returns: An E x 2 tensor of edges in the form of (vertex,vertex) pairs.\n        Each edge should be sorted and the entire tensor should be\n        lexicographically sorted.\n    :rtype: torch.Tensor\n    """"""\n    if backend == ""python"":\n        return _sample_tree_mcmc(edge_logits, edges)\n    elif backend == ""cpp"":\n        return _get_cpp_module().sample_tree_mcmc(edge_logits, edges)\n    else:\n        raise ValueError(""unknown backend: {}"".format(repr(backend)))\n\n\n@torch.no_grad()\ndef _sample_tree_approx(edge_logits):\n    K = len(edge_logits)\n    V = int(round(0.5 + (0.25 + 2 * K)**0.5))\n    assert K == V * (V - 1) // 2\n    E = V - 1\n    grid = make_complete_graph(V)\n\n    # Each of E edges in the tree is stored as an id k in [0, K) indexing into\n    # the complete graph. The id of an edge (v1,v2) is k = v1+v2*(v2-1)/2.\n    edge_ids = torch.empty((E,), dtype=torch.long)\n    # This maps each vertex to whether it is a member of the cumulative tree.\n    components = torch.zeros(V, dtype=torch.bool)\n\n    # Sample the first edge at random.\n    probs = (edge_logits - edge_logits.max()).exp()\n    k = torch.multinomial(probs, 1)[0]\n    components[grid[:, k]] = 1\n    edge_ids[0] = k\n\n    # Sample edges connecting the cumulative tree to a new leaf.\n    for e in range(1, E):\n        c1, c2 = components[grid]\n        mask = (c1 != c2)\n        valid_logits = edge_logits[mask]\n        probs = (valid_logits - valid_logits.max()).exp()\n        k = mask.nonzero()[torch.multinomial(probs, 1)[0]]\n        components[grid[:, k]] = 1\n        edge_ids[e] = k\n\n    # Convert edge ids to a canonical list of pairs.\n    edge_ids = edge_ids.sort()[0]\n    edges = torch.empty((E, 2), dtype=torch.long)\n    edges[:, 0] = grid[0, edge_ids]\n    edges[:, 1] = grid[1, edge_ids]\n    return edges\n\n\ndef sample_tree_approx(edge_logits, backend=""python""):\n    """"""\n    Approximately sample a random spanning tree of a dense weighted graph.\n\n    This is mainly useful for initializing an MCMC sampler.\n\n    :param torch.Tensor edge_logits: A length-K array of nonnormalized log\n        probabilities.\n    :returns: An E x 2 tensor of edges in the form of (vertex,vertex) pairs.\n        Each edge should be sorted and the entire tensor should be\n        lexicographically sorted.\n    :rtype: torch.Tensor\n    """"""\n    if backend == ""python"":\n        return _sample_tree_approx(edge_logits)\n    elif backend == ""cpp"":\n        return _get_cpp_module().sample_tree_approx(edge_logits)\n    else:\n        raise ValueError(""unknown backend: {}"".format(repr(backend)))\n\n\ndef sample_tree(edge_logits, init_edges=None, mcmc_steps=1, backend=""python""):\n    edges = init_edges\n    if edges is None:\n        edges = sample_tree_approx(edge_logits, backend=backend)\n    for step in range(mcmc_steps):\n        edges = sample_tree_mcmc(edge_logits, edges, backend=backend)\n    return edges\n\n\n################################################################################\n# Enumeration implementation.\n################################################################################\n\n# See https://oeis.org/A000272\nNUM_SPANNING_TREES = [\n    1, 1, 1, 3, 16, 125, 1296, 16807, 262144, 4782969, 100000000, 2357947691,\n    61917364224, 1792160394037, 56693912375296, 1946195068359375,\n    72057594037927936, 2862423051509815793, 121439531096594251776,\n    5480386857784802185939,\n]\n\n# These topologically distinct sets of trees generate sets of all trees\n# under permutation of vertices. See https://oeis.org/A000055\n_TREE_GENERATORS = [\n    [[]],\n    [[]],\n    [[(0, 1)]],\n    [[(0, 1), (0, 2)]],\n    [\n        [(0, 1), (0, 2), (0, 3)],\n        [(0, 1), (1, 2), (2, 3)],\n    ],\n    [\n        [(0, 1), (0, 2), (0, 3), (0, 4)],\n        [(0, 1), (0, 2), (0, 3), (1, 4)],\n        [(0, 1), (1, 2), (2, 3), (3, 4)],\n    ],\n    [\n        [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)],\n        [(0, 1), (0, 2), (0, 3), (0, 4), (1, 5)],\n        [(0, 1), (0, 2), (0, 3), (1, 4), (4, 5)],\n        [(0, 1), (0, 2), (0, 3), (2, 4), (3, 5)],\n        [(0, 1), (0, 2), (0, 3), (3, 4), (3, 5)],\n        [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)],\n    ],\n]\n\n\ndef _permute_tree(perm, tree):\n    edges = [tuple(sorted([perm[u], perm[v]])) for (u, v) in tree]\n    edges.sort(key=lambda uv: (uv[1], uv[0]))\n    return tuple(edges)\n\n\ndef _close_under_permutations(V, tree_generators):\n    vertices = list(range(V))\n    trees = []\n    for tree in tree_generators:\n        trees.extend(set(_permute_tree(perm, tree)\n                         for perm in itertools.permutations(vertices)))\n    trees.sort()\n    return trees\n\n\ndef enumerate_spanning_trees(V):\n    """"""\n    Compute the set of spanning trees on V vertices.\n    """"""\n    if V >= len(_TREE_GENERATORS):\n        raise NotImplementedError(\n            ""enumerate_spanning_trees() is implemented only for trees with up to {} vertices""\n            .format(len(_TREE_GENERATORS) - 1))\n    all_trees = _close_under_permutations(V, _TREE_GENERATORS[V])\n    assert len(all_trees) == NUM_SPANNING_TREES[V]\n    return all_trees\n'"
pyro/distributions/stable.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.utils import broadcast_all\n\nfrom pyro.distributions.torch_distribution import TorchDistribution\n\n\ndef _unsafe_standard_stable(alpha, beta, V, W, coords):\n    # Implements a noisily reparametrized version of the sampler\n    # Chambers-Mallows-Stuck method as corrected by Weron [1,3] and simplified\n    # by Nolan [4]. This will fail if alpha is close to 1.\n\n    # Differentiably transform noise via parameters.\n    assert V.shape == W.shape\n    inv_alpha = alpha.reciprocal()\n    half_pi = math.pi / 2\n    eps = torch.finfo(V.dtype).eps\n    # make V belong to the open interval (-pi/2, pi/2)\n    V = V.clamp(min=2 * eps - half_pi, max=half_pi - 2 * eps)\n    ha = half_pi * alpha\n    b = beta * ha.tan()\n    # +/- `ha` term to keep the precision of alpha * (V + half_pi) when V ~ -half_pi\n    v = b.atan() - ha + alpha * (V + half_pi)\n    Z = v.sin() / ((1 + b * b).rsqrt() * V.cos()).pow(inv_alpha) \\\n        * ((v - V).cos().clamp(min=eps) / W).pow(inv_alpha - 1)\n    Z.data[Z.data != Z.data] = 0  # drop occasional NANs\n\n    # Optionally convert to Nolan\'s parametrization S^0 where samples depend\n    # continuously on (alpha,beta), allowing interpolation around the hole at\n    # alpha=1.\n    if coords == ""S0"":\n        return Z - b\n    elif coords == ""S"":\n        return Z\n    else:\n        raise ValueError(""Unknown coords: {}"".format(coords))\n\n\nRADIUS = 0.01\n\n\ndef _standard_stable(alpha, beta, aux_uniform, aux_exponential, coords):\n    """"""\n    Differentiably transform two random variables::\n\n        aux_uniform ~ Uniform(-pi/2, pi/2)\n        aux_exponential ~ Exponential(1)\n\n    to a standard ``Stable(alpha, beta)`` random variable.\n    """"""\n    # Determine whether a hole workaround is needed.\n    with torch.no_grad():\n        hole = 1.\n        near_hole = (alpha - hole).abs() <= RADIUS\n    if not torch._C._get_tracing_state() and not near_hole.any():\n        return _unsafe_standard_stable(alpha, beta, aux_uniform, aux_exponential, coords=coords)\n    if coords == ""S"":\n        # S coords are discontinuous, so interpolate instead in S0 coords.\n        Z = _standard_stable(alpha, beta, aux_uniform, aux_exponential, ""S0"")\n        return torch.where(alpha == 1, Z, Z + beta * (math.pi / 2 * alpha).tan())\n\n    # Avoid the hole at alpha=1 by interpolating between pairs\n    # of points at hole-RADIUS and hole+RADIUS.\n    aux_uniform_ = aux_uniform.unsqueeze(-1)\n    aux_exponential_ = aux_exponential.unsqueeze(-1)\n    beta_ = beta.unsqueeze(-1)\n    alpha_ = alpha.unsqueeze(-1).expand(alpha.shape + (2,)).contiguous()\n    with torch.no_grad():\n        lower, upper = alpha_.unbind(-1)\n        lower.data[near_hole] = hole - RADIUS\n        upper.data[near_hole] = hole + RADIUS\n        # We don\'t need to backprop through weights, since we\'ve pretended\n        # alpha_ is reparametrized, even though we\'ve clamped some values.\n        #               |a - a\'|\n        # weight = 1 - ----------\n        #              2 * RADIUS\n        weights = (alpha_ - alpha.unsqueeze(-1)).abs_().mul_(-1 / (2 * RADIUS)).add_(1)\n        weights[~near_hole] = 0.5\n    pairs = _unsafe_standard_stable(alpha_, beta_, aux_uniform_, aux_exponential_, coords=coords)\n    return (pairs * weights).sum(-1)\n\n\nclass Stable(TorchDistribution):\n    r""""""\n    Levy :math:`\\alpha`-stable distribution. See [1] for a review.\n\n    This uses Nolan\'s parametrization [2] of the ``loc`` parameter, which is\n    required for continuity and differentiability. This corresponds to the\n    notation :math:`S^0_\\alpha(\\beta,\\sigma,\\mu_0)` of [1], where\n    :math:`\\alpha` = stability, :math:`\\beta` = skew, :math:`\\sigma` = scale,\n    and :math:`\\mu_0` = loc. To instead use the S parameterization as in scipy,\n    pass ``coords=""S""``, but BEWARE this is discontinuous at ``stability=1``\n    and has poor geometry for inference.\n\n    This implements a reparametrized sampler :meth:`rsample` , but does not\n    implement :meth:`log_prob` . Inference can be performed using either\n    likelihood-free algorithms such as\n    :class:`~pyro.infer.energy_distance.EnergyDistance`, or reparameterization\n    via the :func:`~pyro.poutine.handlers.reparam` handler with one of the\n    reparameterizers :class:`~pyro.infer.reparam.stable.LatentStableReparam` ,\n    :class:`~pyro.infer.reparam.stable.SymmetricStableReparam` , or\n    :class:`~pyro.infer.reparam.stable.StableReparam` e.g.::\n\n        with poutine.reparam(config={""x"": StableReparam()}):\n            pyro.sample(""x"", Stable(stability, skew, scale, loc))\n\n    [1] S. Borak, W. Hardle, R. Weron (2005).\n        Stable distributions.\n        https://edoc.hu-berlin.de/bitstream/handle/18452/4526/8.pdf\n    [2] J.P. Nolan (1997).\n        Numerical calculation of stable densities and distribution functions.\n    [3] Rafal Weron (1996).\n        On the Chambers-Mallows-Stuck Method for\n        Simulating Skewed Stable Random Variables.\n    [4] J.P. Nolan (2017).\n        Stable Distributions: Models for Heavy Tailed Data.\n        http://fs2.american.edu/jpnolan/www/stable/chap1.pdf\n\n    :param Tensor stability: Levy stability parameter :math:`\\alpha\\in(0,2]` .\n    :param Tensor skew: Skewness :math:`\\beta\\in[-1,1]` .\n    :param Tensor scale: Scale :math:`\\sigma > 0` . Defaults to 1.\n    :param Tensor loc: Location :math:`\\mu_0` when using Nolan\'s S0\n        parametrization [2], or :math:`\\mu` when using the S parameterization.\n        Defaults to 0.\n    :param str coords: Either ""S0"" (default) to use Nolan\'s continuous S0\n        parametrization, or ""S"" to use the discontinuous parameterization.\n    """"""\n    has_rsample = True\n    arg_constraints = {""stability"": constraints.interval(0, 2),  # half-open (0, 2]\n                       ""skew"": constraints.interval(-1, 1),  # closed [-1, 1]\n                       ""scale"": constraints.positive,\n                       ""loc"": constraints.real}\n    support = constraints.real\n\n    def __init__(self, stability, skew, scale=1.0, loc=0.0, coords=""S0"", validate_args=None):\n        assert coords in (""S"", ""S0""), coords\n        self.stability, self.skew, self.scale, self.loc = broadcast_all(\n            stability, skew, scale, loc)\n        self.coords = coords\n        super().__init__(self.loc.shape, validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(Stable, _instance)\n        batch_shape = torch.Size(batch_shape)\n        for name in self.arg_constraints:\n            setattr(new, name, getattr(self, name).expand(batch_shape))\n        new.coords = self.coords\n        super(Stable, new).__init__(batch_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def log_prob(self, value):\n        raise NotImplementedError(""Stable.log_prob() is not implemented"")\n\n    def rsample(self, sample_shape=torch.Size()):\n        # Draw parameter-free noise.\n        with torch.no_grad():\n            shape = self._extended_shape(sample_shape)\n            new_empty = self.stability.new_empty\n            aux_uniform = new_empty(shape).uniform_(-math.pi / 2, math.pi / 2)\n            aux_exponential = new_empty(shape).exponential_()\n\n        # Differentiably transform.\n        x = _standard_stable(self.stability, self.skew, aux_uniform, aux_exponential, coords=self.coords)\n        return self.loc + self.scale * x\n\n    @property\n    def mean(self):\n        result = self.loc\n        if self.coords == ""S0"":\n            result = result - self.scale * self.skew * (math.pi / 2 * self.stability).tan()\n        return result.masked_fill(self.stability <= 1, math.nan)\n\n    @property\n    def variance(self):\n        var = self.scale * self.scale\n        return var.mul(2).masked_fill(self.stability < 2, math.inf)\n'"
pyro/distributions/torch.py,35,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.constraints import IndependentConstraint\nfrom pyro.distributions.torch_distribution import TorchDistributionMixin\nfrom pyro.distributions.util import sum_rightmost\nfrom pyro.ops.special import log_binomial\n\n\ndef _clamp_by_zero(x):\n    # works like clamp(x, min=0) but has grad at 0 is 0.5\n    return (x.clamp(min=0) + x - x.clamp(max=0)) / 2\n\n\nclass Beta(torch.distributions.Beta, TorchDistributionMixin):\n    def conjugate_update(self, other):\n        """"""\n        EXPERIMENTAL.\n        """"""\n        assert isinstance(other, Beta)\n        concentration1 = self.concentration1 + other.concentration1 - 1\n        concentration0 = self.concentration0 + other.concentration0 - 1\n        updated = Beta(concentration1, concentration0)\n\n        def _log_normalizer(d):\n            x = d.concentration1\n            y = d.concentration0\n            return (x + y).lgamma() - x.lgamma() - y.lgamma()\n\n        log_normalizer = _log_normalizer(self) + _log_normalizer(other) - _log_normalizer(updated)\n        return updated, log_normalizer\n\n\nclass Binomial(torch.distributions.Binomial, TorchDistributionMixin):\n    # EXPERIMENTAL threshold on total_count above which sampling will use a\n    # clamped Poisson approximation for Binomial samples. This is useful for\n    # sampling very large populations.\n    approx_sample_thresh = math.inf\n\n    # EXPERIMENTAL If set to a positive value, the .log_prob() method will use\n    # a shifted Sterling\'s approximation to the Beta function, reducing\n    # computational cost from 3 lgamma() evaluations to 4 log() evaluations\n    # plus arithmetic. Recommended values are between 0.1 and 0.01.\n    approx_log_prob_tol = 0.\n\n    def sample(self, sample_shape=torch.Size()):\n        if self.approx_sample_thresh < math.inf:\n            exact = self.total_count <= self.approx_sample_thresh\n            if not exact.all():\n                # Approximate large counts with a moment-matched clamped Poisson.\n                with torch.no_grad():\n                    shape = self._extended_shape(sample_shape)\n                    p = self.probs\n                    q = 1 - self.probs\n                    mean = torch.min(p, q) * self.total_count\n                    variance = p * q * self.total_count\n                    shift = (mean - variance).round()\n                    result = torch.poisson(variance.expand(shape))\n                    result = torch.min(result + shift, self.total_count)\n                    sample = torch.where(p < q, result, self.total_count - result)\n                # Draw exact samples for remaining items.\n                if exact.any():\n                    total_count = torch.where(exact, self.total_count,\n                                              torch.zeros_like(self.total_count))\n                    exact_sample = torch.distributions.Binomial(\n                        total_count, self.probs, validate_args=False).sample(sample_shape)\n                    sample = torch.where(exact, exact_sample, sample)\n                return sample\n        return super().sample(sample_shape)\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n\n        n = self.total_count\n        k = value\n        # k * log(p) + (n - k) * log(1 - p) = k * (log(p) - log(1 - p)) + n * log(1 - p)\n        #     (case logit < 0)              = k * logit - n * log1p(e^logit)\n        #     (case logit > 0)              = k * logit - n * (log(p) - log(1 - p)) + n * log(p)\n        #                                   = k * logit - n * logit - n * log1p(e^-logit)\n        #     (merge two cases)             = k * logit - n * max(logit, 0) - n * log1p(e^-|logit|)\n        normalize_term = n * (_clamp_by_zero(self.logits) + self.logits.abs().neg().exp().log1p())\n        return (k * self.logits - normalize_term\n                + log_binomial(n, k, tol=self.approx_log_prob_tol))\n\n\n# This overloads .log_prob() and .enumerate_support() to speed up evaluating\n# log_prob on the support of this variable: we can completely avoid tensor ops\n# and merely reshape the self.logits tensor. This is especially important for\n# Pyro models that use enumeration.\nclass Categorical(torch.distributions.Categorical, TorchDistributionMixin):\n\n    def log_prob(self, value):\n        if getattr(value, \'_pyro_categorical_support\', None) == id(self):\n            # Assume value is a reshaped torch.arange(event_shape[0]).\n            # In this case we can call .reshape() rather than torch.gather().\n            if not torch._C._get_tracing_state():\n                if self._validate_args:\n                    self._validate_sample(value)\n                assert value.size(0) == self.logits.size(-1)\n            logits = self.logits\n            if logits.dim() <= value.dim():\n                logits = logits.reshape((1,) * (1 + value.dim() - logits.dim()) + logits.shape)\n            if not torch._C._get_tracing_state():\n                assert logits.size(-1 - value.dim()) == 1\n            return logits.transpose(-1 - value.dim(), -1).squeeze(-1)\n        return super().log_prob(value)\n\n    def enumerate_support(self, expand=True):\n        result = super().enumerate_support(expand=expand)\n        if not expand:\n            result._pyro_categorical_support = id(self)\n        return result\n\n\nclass Dirichlet(torch.distributions.Dirichlet, TorchDistributionMixin):\n    def conjugate_update(self, other):\n        """"""\n        EXPERIMENTAL.\n        """"""\n        assert isinstance(other, Dirichlet)\n        concentration = self.concentration + other.concentration - 1\n        updated = Dirichlet(concentration)\n\n        def _log_normalizer(d):\n            c = d.concentration\n            return c.sum(-1).lgamma() - c.lgamma().sum(-1)\n\n        log_normalizer = _log_normalizer(self) + _log_normalizer(other) - _log_normalizer(updated)\n        return updated, log_normalizer\n\n\nclass Gamma(torch.distributions.Gamma, TorchDistributionMixin):\n    def conjugate_update(self, other):\n        """"""\n        EXPERIMENTAL.\n        """"""\n        assert isinstance(other, Gamma)\n        concentration = self.concentration + other.concentration - 1\n        rate = self.rate + other.rate\n        updated = Gamma(concentration, rate)\n\n        def _log_normalizer(d):\n            c = d.concentration\n            return d.rate.log() * c - c.lgamma()\n\n        log_normalizer = _log_normalizer(self) + _log_normalizer(other) - _log_normalizer(updated)\n        return updated, log_normalizer\n\n\nclass Geometric(torch.distributions.Geometric, TorchDistributionMixin):\n    # TODO: move upstream\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        return (-value - 1) * torch.nn.functional.softplus(self.logits) + self.logits\n\n\nclass LogNormal(torch.distributions.LogNormal, TorchDistributionMixin):\n    def __init__(self, loc, scale, validate_args=None):\n        base_dist = Normal(loc, scale)\n        # This differs from torch.distributions.LogNormal only in that base_dist is\n        # a pyro.distributions.Normal rather than a torch.distributions.Normal.\n        super(torch.distributions.LogNormal, self).__init__(\n            base_dist, torch.distributions.transforms.ExpTransform(), validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(LogNormal, _instance)\n        return super(torch.distributions.LogNormal, self).expand(batch_shape, _instance=new)\n\n\nclass MultivariateNormal(torch.distributions.MultivariateNormal, TorchDistributionMixin):\n    support = IndependentConstraint(constraints.real, 1)  # TODO move upstream\n\n\nclass Normal(torch.distributions.Normal, TorchDistributionMixin):\n    pass\n\n\nclass Independent(torch.distributions.Independent, TorchDistributionMixin):\n    @constraints.dependent_property\n    def support(self):\n        return IndependentConstraint(self.base_dist.support, self.reinterpreted_batch_ndims)\n\n    @property\n    def _validate_args(self):\n        return self.base_dist._validate_args\n\n    @_validate_args.setter\n    def _validate_args(self, value):\n        self.base_dist._validate_args = value\n\n    def conjugate_update(self, other):\n        """"""\n        EXPERIMENTAL.\n        """"""\n        n = self.reintepreted_batch_ndims\n        updated, log_normalizer = self.base_dist.conjugate_update(other.to_event(-n))\n        updated = updated.to_event(n)\n        log_normalizer = sum_rightmost(log_normalizer, n)\n        return updated, log_normalizer\n\n\nclass Uniform(torch.distributions.Uniform, TorchDistributionMixin):\n    def __init__(self, low, high, validate_args=None):\n        self._unbroadcasted_low = low\n        self._unbroadcasted_high = high\n        super().__init__(low, high, validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(Uniform, _instance)\n        new = super().expand(batch_shape, _instance=new)\n        new._unbroadcasted_low = self._unbroadcasted_low\n        new._unbroadcasted_high = self._unbroadcasted_high\n        return new\n\n    @constraints.dependent_property\n    def support(self):\n        return constraints.interval(self._unbroadcasted_low, self._unbroadcasted_high)\n\n\n# Programmatically load all distributions from PyTorch.\n__all__ = []\nfor _name, _Dist in torch.distributions.__dict__.items():\n    if not isinstance(_Dist, type):\n        continue\n    if not issubclass(_Dist, torch.distributions.Distribution):\n        continue\n    if _Dist is torch.distributions.Distribution:\n        continue\n\n    try:\n        _PyroDist = locals()[_name]\n    except KeyError:\n        _PyroDist = type(_name, (_Dist, TorchDistributionMixin), {})\n        _PyroDist.__module__ = __name__\n        locals()[_name] = _PyroDist\n\n    _PyroDist.__doc__ = \'\'\'\n    Wraps :class:`{}.{}` with\n    :class:`~pyro.distributions.torch_distribution.TorchDistributionMixin`.\n    \'\'\'.format(_Dist.__module__, _Dist.__name__)\n\n    __all__.append(_name)\n\n\n# Create sphinx documentation.\n__doc__ = \'\\n\\n\'.join([\n\n    \'\'\'\n    {0}\n    ----------------------------------------------------------------\n    .. autoclass:: pyro.distributions.{0}\n    \'\'\'.format(_name)\n    for _name in sorted(__all__)\n])\n'"
pyro/distributions/torch_distribution.py,53,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\nfrom collections import OrderedDict\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.kl import kl_divergence, register_kl\n\nimport pyro.distributions.torch\nfrom pyro.distributions.distribution import Distribution\nfrom pyro.distributions.score_parts import ScoreParts\nfrom pyro.distributions.util import broadcast_shape, scale_and_mask\n\n\nclass TorchDistributionMixin(Distribution):\n    """"""\n    Mixin to provide Pyro compatibility for PyTorch distributions.\n\n    You should instead use `TorchDistribution` for new distribution classes.\n\n    This is mainly useful for wrapping existing PyTorch distributions for\n    use in Pyro.  Derived classes must first inherit from\n    :class:`torch.distributions.distribution.Distribution` and then inherit\n    from :class:`TorchDistributionMixin`.\n    """"""\n    def __call__(self, sample_shape=torch.Size()):\n        """"""\n        Samples a random value.\n\n        This is reparameterized whenever possible, calling\n        :meth:`~torch.distributions.distribution.Distribution.rsample` for\n        reparameterized distributions and\n        :meth:`~torch.distributions.distribution.Distribution.sample` for\n        non-reparameterized distributions.\n\n        :param sample_shape: the size of the iid batch to be drawn from the\n            distribution.\n        :type sample_shape: torch.Size\n        :return: A random value or batch of random values (if parameters are\n            batched). The shape of the result should be `self.shape()`.\n        :rtype: torch.Tensor\n        """"""\n        return self.rsample(sample_shape) if self.has_rsample else self.sample(sample_shape)\n\n    @property\n    def event_dim(self):\n        """"""\n        :return: Number of dimensions of individual events.\n        :rtype: int\n        """"""\n        return len(self.event_shape)\n\n    def shape(self, sample_shape=torch.Size()):\n        """"""\n        The tensor shape of samples from this distribution.\n\n        Samples are of shape::\n\n          d.shape(sample_shape) == sample_shape + d.batch_shape + d.event_shape\n\n        :param sample_shape: the size of the iid batch to be drawn from the\n            distribution.\n        :type sample_shape: torch.Size\n        :return: Tensor shape of samples.\n        :rtype: torch.Size\n        """"""\n        return sample_shape + self.batch_shape + self.event_shape\n\n    def expand(self, batch_shape, _instance=None):\n        """"""\n        Returns a new :class:`ExpandedDistribution` instance with batch\n        dimensions expanded to `batch_shape`.\n\n        :param tuple batch_shape: batch shape to expand to.\n        :param _instance: unused argument for compatibility with\n            :meth:`torch.distributions.Distribution.expand`\n        :return: an instance of `ExpandedDistribution`.\n        :rtype: :class:`ExpandedDistribution`\n        """"""\n        return ExpandedDistribution(self, batch_shape)\n\n    def expand_by(self, sample_shape):\n        """"""\n        Expands a distribution by adding ``sample_shape`` to the left side of\n        its :attr:`~torch.distributions.distribution.Distribution.batch_shape`.\n\n        To expand internal dims of ``self.batch_shape`` from 1 to something\n        larger, use :meth:`expand` instead.\n\n        :param torch.Size sample_shape: The size of the iid batch to be drawn\n            from the distribution.\n        :return: An expanded version of this distribution.\n        :rtype: :class:`ExpandedDistribution`\n        """"""\n        try:\n            expanded_dist = self.expand(torch.Size(sample_shape) + self.batch_shape)\n        except NotImplementedError:\n            expanded_dist = TorchDistributionMixin.expand(self, torch.Size(sample_shape) + self.batch_shape)\n        return expanded_dist\n\n    def reshape(self, sample_shape=None, extra_event_dims=None):\n        raise Exception(\'\'\'\n            .reshape(sample_shape=s, extra_event_dims=n) was renamed and split into\n            .expand_by(sample_shape=s).to_event(reinterpreted_batch_ndims=n).\'\'\')\n\n    def to_event(self, reinterpreted_batch_ndims=None):\n        """"""\n        Reinterprets the ``n`` rightmost dimensions of this distributions\n        :attr:`~torch.distributions.distribution.Distribution.batch_shape`\n        as event dims, adding them to the left side of\n        :attr:`~torch.distributions.distribution.Distribution.event_shape`.\n\n        Example:\n\n            .. doctest::\n               :hide:\n\n               >>> d0 = dist.Normal(torch.zeros(2, 3, 4, 5), torch.ones(2, 3, 4, 5))\n               >>> [d0.batch_shape, d0.event_shape]\n               [torch.Size([2, 3, 4, 5]), torch.Size([])]\n               >>> d1 = d0.to_event(2)\n\n            >>> [d1.batch_shape, d1.event_shape]\n            [torch.Size([2, 3]), torch.Size([4, 5])]\n            >>> d2 = d1.to_event(1)\n            >>> [d2.batch_shape, d2.event_shape]\n            [torch.Size([2]), torch.Size([3, 4, 5])]\n            >>> d3 = d1.to_event(2)\n            >>> [d3.batch_shape, d3.event_shape]\n            [torch.Size([]), torch.Size([2, 3, 4, 5])]\n\n        :param int reinterpreted_batch_ndims: The number of batch dimensions to\n            reinterpret as event dimensions. May be negative to remove\n            dimensions from an :class:`pyro.distributions.torch.Independent` .\n            If None, convert all dimensions to event dimensions.\n        :return: A reshaped version of this distribution.\n        :rtype: :class:`pyro.distributions.torch.Independent`\n        """"""\n        if reinterpreted_batch_ndims is None:\n            reinterpreted_batch_ndims = len(self.batch_shape)\n\n        # Deconstruct Independent distributions.\n        base_dist = self\n        while isinstance(base_dist, torch.distributions.Independent):\n            reinterpreted_batch_ndims += base_dist.reinterpreted_batch_ndims\n            base_dist = base_dist.base_dist\n\n        if reinterpreted_batch_ndims == 0:\n            return base_dist\n        if reinterpreted_batch_ndims < 0:\n            raise ValueError(""Cannot remove event dimensions from {}"".format(type(self)))\n        return pyro.distributions.torch.Independent(base_dist, reinterpreted_batch_ndims)\n\n    def independent(self, reinterpreted_batch_ndims=None):\n        warnings.warn(""independent is deprecated; use to_event instead"", DeprecationWarning)\n        return self.to_event(reinterpreted_batch_ndims=reinterpreted_batch_ndims)\n\n    def mask(self, mask):\n        """"""\n        Masks a distribution by a boolean or boolean-valued tensor that is\n        broadcastable to the distributions\n        :attr:`~torch.distributions.distribution.Distribution.batch_shape` .\n\n        :param mask: A boolean or boolean valued tensor.\n        :type mask: bool or torch.Tensor\n        :return: A masked copy of this distribution.\n        :rtype: :class:`MaskedDistribution`\n        """"""\n        return MaskedDistribution(self, mask)\n\n\nclass TorchDistribution(torch.distributions.Distribution, TorchDistributionMixin):\n    """"""\n    Base class for PyTorch-compatible distributions with Pyro support.\n\n    This should be the base class for almost all new Pyro distributions.\n\n    .. note::\n\n        Parameters and data should be of type :class:`~torch.Tensor`\n        and all methods return type :class:`~torch.Tensor` unless\n        otherwise noted.\n\n    **Tensor Shapes**:\n\n    TorchDistributions provide a method ``.shape()`` for the tensor shape of samples::\n\n      x = d.sample(sample_shape)\n      assert x.shape == d.shape(sample_shape)\n\n    Pyro follows the same distribution shape semantics as PyTorch. It distinguishes\n    between three different roles for tensor shapes of samples:\n\n    - *sample shape* corresponds to the shape of the iid samples drawn from the distribution.\n      This is taken as an argument by the distribution\'s `sample` method.\n    - *batch shape* corresponds to non-identical (independent) parameterizations of\n      the distribution, inferred from the distribution\'s parameter shapes. This is\n      fixed for a distribution instance.\n    - *event shape* corresponds to the event dimensions of the distribution, which\n      is fixed for a distribution class. These are collapsed when we try to score\n      a sample from the distribution via `d.log_prob(x)`.\n\n    These shapes are related by the equation::\n\n      assert d.shape(sample_shape) == sample_shape + d.batch_shape + d.event_shape\n\n    Distributions provide a vectorized\n    :meth:`~torch.distributions.distribution.Distribution.log_prob` method that\n    evaluates the log probability density of each event in a batch\n    independently, returning a tensor of shape\n    ``sample_shape + d.batch_shape``::\n\n      x = d.sample(sample_shape)\n      assert x.shape == d.shape(sample_shape)\n      log_p = d.log_prob(x)\n      assert log_p.shape == sample_shape + d.batch_shape\n\n    **Implementing New Distributions**:\n\n    Derived classes must implement the methods\n    :meth:`~torch.distributions.distribution.Distribution.sample`\n    (or :meth:`~torch.distributions.distribution.Distribution.rsample` if\n    ``.has_rsample == True``) and\n    :meth:`~torch.distributions.distribution.Distribution.log_prob`, and must\n    implement the properties\n    :attr:`~torch.distributions.distribution.Distribution.batch_shape`,\n    and :attr:`~torch.distributions.distribution.Distribution.event_shape`.\n    Discrete classes may also implement the\n    :meth:`~torch.distributions.distribution.Distribution.enumerate_support`\n    method to improve gradient estimates and set\n    ``.has_enumerate_support = True``.\n    """"""\n    # Provides a default `.expand` method for Pyro distributions which overrides\n    # torch.distributions.Distribution.expand (throws a NotImplementedError).\n    expand = TorchDistributionMixin.expand\n\n\nclass MaskedDistribution(TorchDistribution):\n    """"""\n    Masks a distribution by a boolean tensor that is broadcastable to the\n    distribution\'s :attr:`~torch.distributions.distribution.Distribution.batch_shape`.\n\n    In the special case ``mask is False``, computation of :meth:`log_prob` ,\n    :meth:`score_parts` , and ``kl_divergence()`` is skipped, and constant zero\n    values are returned instead.\n\n    :param mask: A boolean or boolean-valued tensor.\n    :type mask: torch.Tensor or bool\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, base_dist, mask):\n        if isinstance(mask, bool):\n            self._mask = mask\n        else:\n            batch_shape = broadcast_shape(mask.shape, base_dist.batch_shape)\n            if mask.shape != batch_shape:\n                mask = mask.expand(batch_shape)\n            if base_dist.batch_shape != batch_shape:\n                base_dist = base_dist.expand(batch_shape)\n            self._mask = mask.bool()\n        self.base_dist = base_dist\n        super().__init__(base_dist.batch_shape, base_dist.event_shape)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(MaskedDistribution, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new.base_dist = self.base_dist.expand(batch_shape)\n        new._mask = self._mask\n        if isinstance(new._mask, torch.Tensor):\n            new._mask = new._mask.expand(batch_shape)\n        super(MaskedDistribution, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    @property\n    def has_rsample(self):\n        return self.base_dist.has_rsample\n\n    @property\n    def has_enumerate_support(self):\n        return self.base_dist.has_enumerate_support\n\n    @constraints.dependent_property\n    def support(self):\n        return self.base_dist.support\n\n    def sample(self, sample_shape=torch.Size()):\n        return self.base_dist.sample(sample_shape)\n\n    def rsample(self, sample_shape=torch.Size()):\n        return self.base_dist.rsample(sample_shape)\n\n    def log_prob(self, value):\n        if self._mask is False:\n            shape = broadcast_shape(self.base_dist.batch_shape,\n                                    value.shape[:value.dim() - self.event_dim])\n            return torch.zeros((), device=value.device).expand(shape)\n        if self._mask is True:\n            return self.base_dist.log_prob(value)\n        return scale_and_mask(self.base_dist.log_prob(value), mask=self._mask)\n\n    def score_parts(self, value):\n        if isinstance(self._mask, bool):\n            return super().score_parts(value)  # calls self.log_prob(value)\n        return self.base_dist.score_parts(value).scale_and_mask(mask=self._mask)\n\n    def enumerate_support(self, expand=True):\n        return self.base_dist.enumerate_support(expand=expand)\n\n    @property\n    def mean(self):\n        return self.base_dist.mean\n\n    @property\n    def variance(self):\n        return self.base_dist.variance\n\n    def conjugate_update(self, other):\n        """"""\n        EXPERIMENTAL.\n        """"""\n        updated, log_normalizer = self.base_dist.conjugate_update(other)\n        updated = updated.mask(self._mask)\n        log_normalizer = torch.where(self._mask, log_normalizer, torch.zeros_like(log_normalizer))\n        return updated, log_normalizer\n\n\nclass ExpandedDistribution(TorchDistribution):\n    arg_constraints = {}\n\n    def __init__(self, base_dist, batch_shape=torch.Size()):\n        self.base_dist = base_dist\n        super().__init__(base_dist.batch_shape, base_dist.event_shape)\n        # adjust batch shape\n        self.expand(batch_shape)\n\n    def expand(self, batch_shape, _instance=None):\n        # Do basic validation. e.g. we should not ""unexpand"" distributions even if that is possible.\n        new_shape, _, _ = self._broadcast_shape(self.batch_shape, batch_shape)\n        # Record interstitial and expanded dims/sizes w.r.t. the base distribution\n        new_shape, expanded_sizes, interstitial_sizes = self._broadcast_shape(self.base_dist.batch_shape,\n                                                                              new_shape)\n        self._batch_shape = new_shape\n        self._expanded_sizes = expanded_sizes\n        self._interstitial_sizes = interstitial_sizes\n        return self\n\n    @staticmethod\n    def _broadcast_shape(existing_shape, new_shape):\n        if len(new_shape) < len(existing_shape):\n            raise ValueError(""Cannot broadcast distribution of shape {} to shape {}""\n                             .format(existing_shape, new_shape))\n        reversed_shape = list(reversed(existing_shape))\n        expanded_sizes, interstitial_sizes = [], []\n        for i, size in enumerate(reversed(new_shape)):\n            if i >= len(reversed_shape):\n                reversed_shape.append(size)\n                expanded_sizes.append((-i - 1, size))\n            elif reversed_shape[i] == 1:\n                if size != 1:\n                    reversed_shape[i] = size\n                    interstitial_sizes.append((-i - 1, size))\n            elif reversed_shape[i] != size:\n                raise ValueError(""Cannot broadcast distribution of shape {} to shape {}""\n                                 .format(existing_shape, new_shape))\n        return tuple(reversed(reversed_shape)), OrderedDict(expanded_sizes), OrderedDict(interstitial_sizes)\n\n    @property\n    def has_rsample(self):\n        return self.base_dist.has_rsample\n\n    @property\n    def has_enumerate_support(self):\n        return self.base_dist.has_enumerate_support\n\n    @constraints.dependent_property\n    def support(self):\n        return self.base_dist.support\n\n    def _sample(self, sample_fn, sample_shape):\n        interstitial_dims = tuple(self._interstitial_sizes.keys())\n        interstitial_dims = tuple(i - self.event_dim for i in interstitial_dims)\n        interstitial_sizes = tuple(self._interstitial_sizes.values())\n        expanded_sizes = tuple(self._expanded_sizes.values())\n        batch_shape = expanded_sizes + interstitial_sizes\n        samples = sample_fn(sample_shape + batch_shape)\n        interstitial_idx = len(sample_shape) + len(expanded_sizes)\n        interstitial_sample_dims = tuple(range(interstitial_idx, interstitial_idx + len(interstitial_sizes)))\n        for dim1, dim2 in zip(interstitial_dims, interstitial_sample_dims):\n            samples = samples.transpose(dim1, dim2)\n        return samples.reshape(sample_shape + self.batch_shape + self.event_shape)\n\n    def sample(self, sample_shape=torch.Size()):\n        return self._sample(self.base_dist.sample, sample_shape)\n\n    def rsample(self, sample_shape=torch.Size()):\n        return self._sample(self.base_dist.rsample, sample_shape)\n\n    def log_prob(self, value):\n        shape = broadcast_shape(self.batch_shape, value.shape[:value.dim() - self.event_dim])\n        log_prob = self.base_dist.log_prob(value)\n        return log_prob.expand(shape)\n\n    def score_parts(self, value):\n        shape = broadcast_shape(self.batch_shape, value.shape[:value.dim() - self.event_dim])\n        log_prob, score_function, entropy_term = self.base_dist.score_parts(value)\n        if self.batch_shape != self.base_dist.batch_shape:\n            log_prob = log_prob.expand(shape)\n            if isinstance(score_function, torch.Tensor):\n                score_function = score_function.expand(shape)\n            if isinstance(score_function, torch.Tensor):\n                entropy_term = entropy_term.expand(shape)\n        return ScoreParts(log_prob, score_function, entropy_term)\n\n    def enumerate_support(self, expand=True):\n        samples = self.base_dist.enumerate_support(expand=False)\n        enum_shape = samples.shape[:1]\n        samples = samples.reshape(enum_shape + (1,) * len(self.batch_shape))\n        if expand:\n            samples = samples.expand(enum_shape + self.batch_shape)\n        return samples\n\n    @property\n    def mean(self):\n        return self.base_dist.mean.expand(self.batch_shape + self.event_shape)\n\n    @property\n    def variance(self):\n        return self.base_dist.variance.expand(self.batch_shape + self.event_shape)\n\n    def conjugate_update(self, other):\n        """"""\n        EXPERIMENTAL.\n        """"""\n        updated, log_normalizer = self.base_dist.conjugate_update(other)\n        updated = updated.expand(self.batch_shape)\n        log_normalizer = log_normalizer.expand(self.batch_shape)\n        return updated, log_normalizer\n\n\n@register_kl(MaskedDistribution, MaskedDistribution)\ndef _kl_masked_masked(p, q):\n    if p._mask is False or q._mask is False:\n        mask = False\n    elif p._mask is True:\n        mask = q._mask\n    elif q._mask is True:\n        mask = p._mask\n    elif p._mask is q._mask:\n        mask = p._mask\n    else:\n        mask = p._mask & q._mask\n\n    if mask is False:\n        return 0.  # Return a float, since we cannot determine device.\n    if mask is True:\n        return kl_divergence(p.base_dist, q.base_dist)\n    kl = kl_divergence(p.base_dist, q.base_dist)\n    return scale_and_mask(kl, mask=mask)\n'"
pyro/distributions/torch_patch.py,7,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport weakref\n\nimport torch\n\nassert torch.__version__.startswith('1.')\n\n\ndef patch_dependency(target, root_module=torch):\n    parts = target.split('.')\n    assert parts[0] == root_module.__name__\n    module = root_module\n    for part in parts[1:-1]:\n        module = getattr(module, part)\n    name = parts[-1]\n    old_fn = getattr(module, name, None)\n    old_fn = getattr(old_fn, '_pyro_unpatched', old_fn)  # ensure patching is idempotent\n\n    def decorator(new_fn):\n        try:\n            functools.update_wrapper(new_fn, old_fn)\n        except Exception:\n            for attr in functools.WRAPPER_ASSIGNMENTS:\n                if hasattr(old_fn, attr):\n                    setattr(new_fn, attr, getattr(old_fn, attr))\n        new_fn._pyro_unpatched = old_fn\n        setattr(module, name, new_fn)\n        return new_fn\n\n    return decorator\n\n\n# TODO: Move upstream to allow for pickle serialization of transforms\n@patch_dependency('torch.distributions.transforms.Transform.__getstate__')\ndef _Transform__getstate__(self):\n    attrs = {}\n    for k, v in self.__dict__.items():\n        if isinstance(v, weakref.ref):\n            attrs[k] = None\n        else:\n            attrs[k] = v\n    return attrs\n\n\n# Fixes a shape error in Multinomial.support with inhomogeneous .total_count\n@patch_dependency('torch.distributions.Multinomial.support')\n@torch.distributions.constraints.dependent_property\ndef _Multinomial_support(self):\n    total_count = self.total_count\n    if isinstance(total_count, torch.Tensor):\n        total_count = total_count.unsqueeze(-1)\n    return torch.distributions.constraints.integer_interval(0, total_count)\n\n\n# This adds a __call__ method to satisfy sphinx.\n@patch_dependency('torch.distributions.utils.lazy_property.__call__')\ndef _lazy_property__call__(self):\n    raise NotImplementedError\n\n\n__all__ = []\n"""
pyro/distributions/torch_transform.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\n\nclass TransformModule(torch.distributions.Transform, torch.nn.Module):\n    """"""\n    Transforms with learnable parameters such as normalizing flows should inherit from this class rather\n    than `Transform` so they are also a subclass of `nn.Module` and inherit all the useful methods of that class.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def __hash__(self):\n        return super(torch.nn.Module, self).__hash__()\n\n\nclass ComposeTransformModule(torch.distributions.ComposeTransform, torch.nn.ModuleList):\n    """"""\n    This allows us to use a list of `TransformModule` in the same way as\n    :class:`~torch.distributions.transform.ComposeTransform`. This is needed\n    so that transform parameters are automatically registered by Pyro\'s param\n    store when used in :class:`~pyro.nn.module.PyroModule` instances.\n    """"""\n    def __init__(self, parts):\n        super().__init__(parts)\n        for part in parts:\n            self.append(part)\n\n    def __hash__(self):\n        return super(torch.nn.Module, self).__hash__()\n'"
pyro/distributions/unit.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.distributions.util import broadcast_shape\n\n\nclass Unit(TorchDistribution):\n    """"""\n    Trivial nonnormalized distribution representing the unit type.\n\n    The unit type has a single value with no data, i.e. ``value.numel() == 0``.\n\n    This is used for :func:`pyro.factor` statements.\n    """"""\n    arg_constraints = {\'log_factor\': constraints.real}\n    support = constraints.real\n\n    def __init__(self, log_factor, validate_args=None):\n        log_factor = torch.as_tensor(log_factor)\n        batch_shape = log_factor.shape\n        event_shape = torch.Size((0,))  # This satisfies .numel() == 0.\n        self.log_factor = log_factor\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(Unit, _instance)\n        new.log_factor = self.log_factor.expand(batch_shape)\n        super(Unit, new).__init__(batch_shape, self.event_shape, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n    def sample(self, sample_shape=torch.Size()):\n        return self.log_factor.new_empty(sample_shape + self.shape())\n\n    def log_prob(self, value):\n        shape = broadcast_shape(self.batch_shape, value.shape[:-1])\n        return self.log_factor.expand(shape)\n'"
pyro/distributions/util.py,19,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport numbers\nimport weakref\nfrom contextlib import contextmanager\n\nimport torch\nimport torch.distributions as torch_dist\nfrom torch import logsumexp\nfrom torch.distributions.utils import broadcast_all\n\nfrom pyro.util import ignore_jit_warnings\n\n_VALIDATION_ENABLED = False\n\nlog_sum_exp = logsumexp  # DEPRECATED\n\n\ndef copy_docs_from(source_class, full_text=False):\n    """"""\n    Decorator to copy class and method docs from source to destin class.\n    """"""\n\n    def decorator(destin_class):\n        # This works only in python 3.3+:\n        # if not destin_class.__doc__:\n        #     destin_class.__doc__ = source_class.__doc__\n        for name in dir(destin_class):\n            if name.startswith(\'_\'):\n                continue\n            destin_attr = getattr(destin_class, name)\n            destin_attr = getattr(destin_attr, \'__func__\', destin_attr)\n            source_attr = getattr(source_class, name, None)\n            source_doc = getattr(source_attr, \'__doc__\', None)\n            if source_doc and not getattr(destin_attr, \'__doc__\', None):\n                if full_text or source_doc.startswith(\'See \'):\n                    destin_doc = source_doc\n                else:\n                    destin_doc = \'See :meth:`{}.{}.{}`\'.format(\n                        source_class.__module__, source_class.__name__, name)\n                if isinstance(destin_attr, property):\n                    # Set docs for object properties.\n                    # Since __doc__ is read-only, we need to reset the property\n                    # with the updated doc.\n                    updated_property = property(destin_attr.fget,\n                                                destin_attr.fset,\n                                                destin_attr.fdel,\n                                                destin_doc)\n                    setattr(destin_class, name, updated_property)\n                else:\n                    destin_attr.__doc__ = destin_doc\n        return destin_class\n\n    return decorator\n\n\ndef weakmethod(fn):\n    """"""\n    Decorator to enforce weak binding of a method, so as to avoid reference\n    cycles when passing a bound method as an argument to other functions.\n\n    In the following example, functional behavior is the same with and without\n    the ``@weakmethod`` decorator, but decoration avoids a reference cycle::\n\n        class Foo:\n            def __init__(self):\n                self.callback = self._callback\n            @weakmethod\n            def _callback(self, result):\n                print(result)\n    """"""\n    def weak_fn(weakself, *args, **kwargs):\n        self = weakself()\n        if self is None:\n            raise AttributeError(""self was garbage collected when calling self.{}""\n                                 .format(fn.__name__))\n        return fn(self, *args, **kwargs)\n\n    @property\n    def weak_binder(self):\n        weakself = weakref.ref(self)\n        return functools.partial(weak_fn, weakself)\n\n    @weak_binder.setter\n    def weak_binder(self, new):\n        if not (isinstance(new, functools.partial) and new.func is weak_fn and\n                len(new.args) == 1 and new.args[0] is weakref.ref(self)):\n            raise AttributeError(""cannot overwrite weakmethod {}"".format(fn.__name__))\n\n    return weak_binder\n\n\ndef is_identically_zero(x):\n    """"""\n    Check if argument is exactly the number zero. True for the number zero;\n    false for other numbers; false for :class:`~torch.Tensor`s.\n    """"""\n    if isinstance(x, numbers.Number):\n        return x == 0\n    if not torch._C._get_tracing_state():\n        if isinstance(x, torch.Tensor) and x.dtype == torch.int64 and not x.shape:\n            return x.item() == 0\n    return False\n\n\ndef is_identically_one(x):\n    """"""\n    Check if argument is exactly the number one. True for the number one;\n    false for other numbers; false for :class:`~torch.Tensor`s.\n    """"""\n    if isinstance(x, numbers.Number):\n        return x == 1\n    if not torch._C._get_tracing_state():\n        if isinstance(x, torch.Tensor) and x.dtype == torch.int64 and not x.shape:\n            return x.item() == 1\n    return False\n\n\ndef broadcast_shape(*shapes, **kwargs):\n    """"""\n    Similar to ``np.broadcast()`` but for shapes.\n    Equivalent to ``np.broadcast(*map(np.empty, shapes)).shape``.\n\n    :param tuple shapes: shapes of tensors.\n    :param bool strict: whether to use extend-but-not-resize broadcasting.\n    :returns: broadcasted shape\n    :rtype: tuple\n    :raises: ValueError\n    """"""\n    strict = kwargs.pop(\'strict\', False)\n    reversed_shape = []\n    for shape in shapes:\n        for i, size in enumerate(reversed(shape)):\n            if i >= len(reversed_shape):\n                reversed_shape.append(size)\n            elif reversed_shape[i] == 1 and not strict:\n                reversed_shape[i] = size\n            elif reversed_shape[i] != size and (size != 1 or strict):\n                raise ValueError(\'shape mismatch: objects cannot be broadcast to a single shape: {}\'.format(\n                    \' vs \'.join(map(str, shapes))))\n    return tuple(reversed(reversed_shape))\n\n\ndef gather(value, index, dim):\n    """"""\n    Broadcasted gather of indexed values along a named dim.\n    """"""\n    value, index = broadcast_all(value, index)\n    with ignore_jit_warnings():\n        zero = torch.zeros(1, dtype=torch.long, device=index.device)\n    index = index.index_select(dim, zero)\n    return value.gather(dim, index)\n\n\ndef sum_rightmost(value, dim):\n    """"""\n    Sum out ``dim`` many rightmost dimensions of a given tensor.\n\n    If ``dim`` is 0, no dimensions are summed out.\n    If ``dim`` is ``float(\'inf\')``, then all dimensions are summed out.\n    If ``dim`` is 1, the rightmost 1 dimension is summed out.\n    If ``dim`` is 2, the rightmost two dimensions are summed out.\n    If ``dim`` is -1, all but the leftmost 1 dimension is summed out.\n    If ``dim`` is -2, all but the leftmost 2 dimensions are summed out.\n    etc.\n\n    :param torch.Tensor value: A tensor of ``.dim()`` at least ``dim``.\n    :param int dim: The number of rightmost dims to sum out.\n    """"""\n    if isinstance(value, numbers.Number):\n        return value\n    if dim < 0:\n        dim += value.dim()\n    if dim == 0:\n        return value\n    if dim >= value.dim():\n        return value.sum()\n    return value.reshape(value.shape[:-dim] + (-1,)).sum(-1)\n\n\ndef sum_leftmost(value, dim):\n    """"""\n    Sum out ``dim`` many leftmost dimensions of a given tensor.\n\n    If ``dim`` is 0, no dimensions are summed out.\n    If ``dim`` is ``float(\'inf\')``, then all dimensions are summed out.\n    If ``dim`` is 1, the leftmost 1 dimension is summed out.\n    If ``dim`` is 2, the leftmost two dimensions are summed out.\n    If ``dim`` is -1, all but the rightmost 1 dimension is summed out.\n    If ``dim`` is -2, all but the rightmost 2 dimensions are summed out.\n    etc.\n\n    Example::\n\n        x = torch.ones(2, 3, 4)\n        assert sum_leftmost(x, 1).shape == (3, 4)\n        assert sum_leftmost(x, -1).shape == (4,)\n\n    :param torch.Tensor value: A tensor\n    :param int dim: Specifies the number of dims to sum out\n    """"""\n    if isinstance(value, numbers.Number):\n        return value\n    if dim < 0:\n        dim += value.dim()\n    if dim == 0:\n        return value\n    if dim >= value.dim():\n        return value.sum()\n    return value.reshape(-1, *value.shape[dim:]).sum(0)\n\n\ndef scale_and_mask(tensor, scale=1.0, mask=None):\n    """"""\n    Scale and mask a tensor, broadcasting and avoiding unnecessary ops.\n\n    :param tensor: an input tensor or zero\n    :type tensor: torch.Tensor or the number zero\n    :param scale: a positive scale\n    :type scale: torch.Tensor or number\n    :param mask: an optional masking tensor\n    :type mask: torch.BoolTensor or None\n    """"""\n    if is_identically_zero(tensor) or (mask is None and is_identically_one(scale)):\n        return tensor\n    if mask is None:\n        return tensor * scale\n    return torch.where(mask, tensor * scale, tensor.new_zeros(()))\n\n\ndef scalar_like(prototype, fill_value):\n    return torch.tensor(fill_value, dtype=prototype.dtype, device=prototype.device)\n\n\n# work around lack of jit support for torch.eye(..., out=value)\ndef eye_like(value, m, n=None):\n    if n is None:\n        n = m\n    eye = torch.zeros(m, n, dtype=value.dtype, device=value.device)\n    eye.view(-1)[:min(m, n) * n:n + 1] = 1\n    return eye\n\n\ndef enable_validation(is_validate):\n    global _VALIDATION_ENABLED\n    _VALIDATION_ENABLED = is_validate\n    torch_dist.Distribution.set_default_validate_args(is_validate)\n\n\ndef is_validation_enabled():\n    return _VALIDATION_ENABLED\n\n\n@contextmanager\ndef validation_enabled(is_validate=True):\n    distribution_validation_status = is_validation_enabled()\n    try:\n        enable_validation(is_validate)\n        yield\n    finally:\n        enable_validation(distribution_validation_status)\n'"
pyro/distributions/von_mises_3d.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.distributions import TorchDistribution\n\n\nclass VonMises3D(TorchDistribution):\n    """"""\n    Spherical von Mises distribution.\n\n    This implementation combines the direction parameter and concentration\n    parameter into a single combined parameter that contains both direction and\n    magnitude. The ``value`` arg is represented in cartesian coordinates: it\n    must be a normalized 3-vector that lies on the 2-sphere.\n\n    See :class:`~pyro.distributions.VonMises` for a 2D polar coordinate cousin\n    of this distribution.\n\n    Currently only :meth:`log_prob` is implemented.\n\n    :param torch.Tensor concentration: A combined location-and-concentration\n        vector. The direction of this vector is the location, and its\n        magnitude is the concentration.\n    """"""\n    arg_constraints = {\'concentration\': constraints.real}\n    support = constraints.real  # TODO implement constraints.sphere or similar\n\n    def __init__(self, concentration, validate_args=None):\n        if concentration.dim() < 1 or concentration.shape[-1] != 3:\n            raise ValueError(\'Expected concentration to have rightmost dim 3, actual shape = {}\'.format(\n                concentration.shape))\n        self.concentration = concentration\n        batch_shape, event_shape = concentration.shape[:-1], concentration.shape[-1:]\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    def log_prob(self, value):\n        if self._validate_args:\n            if value.dim() < 1 or value.shape[-1] != 3:\n                raise ValueError(\'Expected value to have rightmost dim 3, actual shape = {}\'.format(\n                    value.shape))\n            if not (torch.abs(value.norm(2, -1) - 1) < 1e-6).all():\n                raise ValueError(\'direction vectors are not normalized\')\n        scale = self.concentration.norm(2, -1)\n        log_normalizer = scale.log() - scale.sinh().log() - math.log(4 * math.pi)\n        return (self.concentration * value).sum(-1) + log_normalizer\n\n    def expand(self, batch_shape):\n        try:\n            return super().expand(batch_shape)\n        except NotImplementedError:\n            validate_args = self.__dict__.get(\'_validate_args\')\n            concentration = self.concentration.expand(torch.Size(batch_shape) + (3,))\n            return type(self)(concentration, validate_args=validate_args)\n'"
pyro/distributions/zero_inflated.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.utils import broadcast_all, lazy_property\n\nfrom pyro.distributions import TorchDistribution, Poisson, NegativeBinomial\n\n\nclass ZeroInflatedDistribution(TorchDistribution):\n    """"""\n    Base class for a Zero Inflated distribution.\n\n    :param torch.Tensor gate: probability of extra zeros given via a Bernoulli distribution.\n    :param TorchDistribution base_dist: the base distribution.\n    """"""\n    arg_constraints = {""gate"": constraints.unit_interval}\n\n    def __init__(self, gate, base_dist, validate_args=None):\n        self.gate = gate\n        self.base_dist = base_dist\n        batch_shape = self.gate.shape\n        event_shape = torch.Size()\n\n        super().__init__(batch_shape, event_shape, validate_args)\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n\n        gate, value = broadcast_all(self.gate, value)\n        log_prob = (-gate).log1p() + self.base_dist.log_prob(value)\n        log_prob = torch.where(value == 0, (gate + log_prob.exp()).log(), log_prob)\n        return log_prob\n\n    def sample(self, sample_shape=torch.Size()):\n        shape = self._extended_shape(sample_shape)\n        with torch.no_grad():\n            mask = torch.bernoulli(self.gate.expand(shape)).bool()\n            samples = self.base_dist.expand(shape).sample()\n            samples = torch.where(mask, samples.new_zeros(()), samples)\n        return samples\n\n    @lazy_property\n    def mean(self):\n        return (1 - self.gate) * self.base_dist.mean\n\n    @lazy_property\n    def variance(self):\n        return (1 - self.gate) * (\n            self.base_dist.mean ** 2 + self.base_dist.variance\n        ) - (self.mean) ** 2\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(type(self), _instance)\n        batch_shape = torch.Size(batch_shape)\n        gate = self.gate.expand(batch_shape)\n        base_dist = self.base_dist.expand(batch_shape)\n        ZeroInflatedDistribution.__init__(new, gate, base_dist, validate_args=False)\n        new._validate_args = self._validate_args\n        return new\n\n\nclass ZeroInflatedPoisson(ZeroInflatedDistribution):\n    """"""\n    A Zero Inflated Poisson distribution.\n\n    :param torch.Tensor gate: probability of extra zeros.\n    :param torch.Tensor rate: rate of poisson distribution.\n    """"""\n    arg_constraints = {""gate"": constraints.unit_interval,\n                       ""rate"": constraints.positive}\n    support = constraints.nonnegative_integer\n\n    def __init__(self, gate, rate, validate_args=None):\n        base_dist = Poisson(rate=rate, validate_args=False)\n        base_dist._validate_args = validate_args\n\n        super().__init__(\n            gate, base_dist, validate_args=validate_args\n        )\n\n    @property\n    def rate(self):\n        return self.base_dist.rate\n\n\nclass ZeroInflatedNegativeBinomial(ZeroInflatedDistribution):\n    """"""\n    A Zero Inflated Negative Binomial distribution.\n\n    :param torch.Tensor gate: probability of extra zeros.\n    :param total_count: non-negative number of negative Bernoulli trials.\n    :type total_count: float or torch.Tensor\n    :param torch.Tensor probs: Event probabilities of success in the half open interval [0, 1).\n    :param torch.Tensor logits: Event log-odds for probabilities of success.\n    """"""\n    arg_constraints = {""gate"": constraints.unit_interval,\n                       ""total_count"": constraints.greater_than_eq(0),\n                       ""probs"": constraints.half_open_interval(0., 1.),\n                       ""logits"": constraints.real}\n    support = constraints.nonnegative_integer\n\n    def __init__(self, gate, total_count, probs=None, logits=None, validate_args=None):\n        base_dist = NegativeBinomial(\n            total_count=total_count,\n            probs=probs,\n            logits=logits,\n            validate_args=False,\n        )\n        base_dist._validate_args = validate_args\n\n        super().__init__(\n            gate, base_dist, validate_args=validate_args\n        )\n\n    @property\n    def total_count(self):\n        return self.base_dist.total_count\n\n    @property\n    def probs(self):\n        return self.base_dist.probs\n\n    @property\n    def logits(self):\n        return self.base_dist.logits\n'"
pyro/infer/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.infer.abstract_infer import EmpiricalMarginal, TracePosterior, TracePredictive\nfrom pyro.infer.csis import CSIS\nfrom pyro.infer.discrete import infer_discrete\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.energy_distance import EnergyDistance\nfrom pyro.infer.enum import config_enumerate\nfrom pyro.infer.importance import Importance\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.infer.mcmc.hmc import HMC\nfrom pyro.infer.mcmc.nuts import NUTS\nfrom pyro.infer.predictive import Predictive\nfrom pyro.infer.renyi_elbo import RenyiELBO\nfrom pyro.infer.rws import ReweightedWakeSleep\nfrom pyro.infer.smcfilter import SMCFilter\nfrom pyro.infer.svgd import SVGD, IMQSteinKernel, RBFSteinKernel\nfrom pyro.infer.svi import SVI\nfrom pyro.infer.tracetmc_elbo import TraceTMC_ELBO\nfrom pyro.infer.trace_elbo import JitTrace_ELBO, Trace_ELBO\nfrom pyro.infer.trace_mean_field_elbo import JitTraceMeanField_ELBO, TraceMeanField_ELBO\nfrom pyro.infer.trace_mmd import Trace_MMD\nfrom pyro.infer.trace_tail_adaptive_elbo import TraceTailAdaptive_ELBO\nfrom pyro.infer.traceenum_elbo import JitTraceEnum_ELBO, TraceEnum_ELBO\nfrom pyro.infer.tracegraph_elbo import JitTraceGraph_ELBO, TraceGraph_ELBO\nfrom pyro.infer.util import enable_validation, is_validation_enabled\n\n__all__ = [\n    ""config_enumerate"",\n    ""CSIS"",\n    ""enable_validation"",\n    ""is_validation_enabled"",\n    ""ELBO"",\n    ""EmpiricalMarginal"",\n    ""EnergyDistance"",\n    ""HMC"",\n    ""Importance"",\n    ""IMQSteinKernel"",\n    ""infer_discrete"",\n    ""JitTraceEnum_ELBO"",\n    ""JitTraceGraph_ELBO"",\n    ""JitTraceMeanField_ELBO"",\n    ""JitTrace_ELBO"",\n    ""MCMC"",\n    ""NUTS"",\n    ""Predictive"",\n    ""RBFSteinKernel"",\n    ""RenyiELBO"",\n    ""ReweightedWakeSleep"",\n    ""SMCFilter"",\n    ""SVGD"",\n    ""SVI"",\n    ""TraceTMC_ELBO"",\n    ""TraceEnum_ELBO"",\n    ""TraceGraph_ELBO"",\n    ""TraceMeanField_ELBO"",\n    ""TracePosterior"",\n    ""TracePredictive"",\n    ""TraceTailAdaptive_ELBO"",\n    ""Trace_ELBO"",\n    ""Trace_MMD"",\n]\n'"
pyro/infer/abstract_infer.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numbers\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom collections import OrderedDict, defaultdict\n\nimport torch\n\nimport pyro.poutine as poutine\nfrom pyro.distributions import Categorical, Empirical\nfrom pyro.infer.util import site_is_subsample\nfrom pyro.ops.stats import waic\n\n\nclass EmpiricalMarginal(Empirical):\n    """"""\n    Marginal distribution over a single site (or multiple, provided they have the same\n    shape) from the ``TracePosterior``\'s model.\n\n    .. note:: If multiple sites are specified, they must have the same tensor shape.\n        Samples from each site will be stacked and stored within a single tensor. See\n        :class:`~pyro.distributions.Empirical`. To hold the marginal distribution of sites\n        having different shapes, use :class:`~pyro.infer.abstract_infer.Marginals` instead.\n\n    :param TracePosterior trace_posterior: a ``TracePosterior`` instance representing\n        a Monte Carlo posterior.\n    :param list sites: optional list of sites for which we need to generate\n        the marginal distribution.\n    """"""\n\n    def __init__(self, trace_posterior, sites=None, validate_args=None):\n        assert isinstance(trace_posterior, TracePosterior), \\\n            ""trace_dist must be trace posterior distribution object""\n        if sites is None:\n            sites = ""_RETURN""\n        self._num_chains = 1\n        self._samples_buffer = defaultdict(list)\n        self._weights_buffer = defaultdict(list)\n        self._populate_traces(trace_posterior, sites)\n        samples, weights = self._get_samples_and_weights()\n        super().__init__(samples, weights, validate_args=validate_args)\n\n    def _get_samples_and_weights(self):\n        """"""\n        Appends values collected in the samples/weights buffers to their\n        corresponding tensors.\n        """"""\n        num_chains = len(self._samples_buffer)\n        samples_by_chain = []\n        weights_by_chain = []\n        for i in range(num_chains):\n            samples = torch.stack(self._samples_buffer[i], dim=0)\n            samples_by_chain.append(samples)\n            weights_dtype = samples.dtype if samples.dtype.is_floating_point else torch.float32\n            weights = torch.as_tensor(self._weights_buffer[i], device=samples.device, dtype=weights_dtype)\n            weights_by_chain.append(weights)\n        if len(samples_by_chain) == 1:\n            return samples_by_chain[0], weights_by_chain[0]\n        else:\n            return torch.stack(samples_by_chain, dim=0), torch.stack(weights_by_chain, dim=0)\n\n    def _add_sample(self, value, log_weight=None, chain_id=0):\n        """"""\n        Adds a new data point to the sample. The values in successive calls to\n        ``add`` must have the same tensor shape and size. Optionally, an\n        importance weight can be specified via ``log_weight`` or ``weight``\n        (default value of `1` is used if not specified).\n\n        :param torch.Tensor value: tensor to add to the sample.\n        :param torch.Tensor log_weight: log weight (optional) corresponding\n            to the sample.\n        :param int chain_id: chain id that generated the sample (optional).\n            Note that if this argument is provided, ``chain_id`` must lie\n            in ``[0, num_chains - 1]``, and there must be equal number\n            of samples per chain.\n        """"""\n        # Apply default weight of 1.0.\n        if log_weight is None:\n            log_weight = 0.0\n        if self._validate_args and not isinstance(log_weight, numbers.Number) and log_weight.dim() > 0:\n            raise ValueError(""``weight.dim() > 0``, but weight should be a scalar."")\n\n        # Append to the buffer list\n        self._samples_buffer[chain_id].append(value)\n        self._weights_buffer[chain_id].append(log_weight)\n        self._num_chains = max(self._num_chains, chain_id + 1)\n\n    def _populate_traces(self, trace_posterior, sites):\n        assert isinstance(sites, (list, str))\n        for tr, log_weight, chain_id in zip(trace_posterior.exec_traces,\n                                            trace_posterior.log_weights,\n                                            trace_posterior.chain_ids):\n            value = tr.nodes[sites][""value""] if isinstance(sites, str) else \\\n                torch.stack([tr.nodes[site][""value""] for site in sites], 0)\n            self._add_sample(value, log_weight=log_weight, chain_id=chain_id)\n\n\nclass Marginals:\n    """"""\n    Holds the marginal distribution over one or more sites from the ``TracePosterior``\'s\n    model. This is a convenience container class, which can be extended by ``TracePosterior``\n    subclasses. e.g. for implementing diagnostics.\n\n    :param TracePosterior trace_posterior: a TracePosterior instance representing\n        a Monte Carlo posterior.\n    :param list sites: optional list of sites for which we need to generate\n        the marginal distribution.\n    """"""\n    def __init__(self, trace_posterior, sites=None, validate_args=None):\n        assert isinstance(trace_posterior, TracePosterior), \\\n            ""trace_dist must be trace posterior distribution object""\n        if sites is None:\n            sites = [""_RETURN""]\n        elif isinstance(sites, str):\n            sites = [sites]\n        else:\n            assert isinstance(sites, list)\n        self.sites = sites\n        self._marginals = OrderedDict()\n        self._diagnostics = OrderedDict()\n        self._trace_posterior = trace_posterior\n        self._populate_traces(trace_posterior, validate_args)\n\n    def _populate_traces(self, trace_posterior, validate):\n        self._marginals = {site: EmpiricalMarginal(trace_posterior, site, validate)\n                           for site in self.sites}\n\n    def support(self, flatten=False):\n        """"""\n        Gets support of this marginal distribution.\n\n        :param bool flatten: A flag to decide if we want to flatten `batch_shape`\n            when the marginal distribution is collected from the posterior with\n            ``num_chains > 1``. Defaults to False.\n        :returns: a dict with keys are sites\' names and values are sites\' supports.\n        :rtype: :class:`OrderedDict`\n        """"""\n        support = OrderedDict([(site, value.enumerate_support())\n                               for site, value in self._marginals.items()])\n        if self._trace_posterior.num_chains > 1 and flatten:\n            for site, samples in support.items():\n                shape = samples.size()\n                flattened_shape = torch.Size((shape[0] * shape[1],)) + shape[2:]\n                support[site] = samples.reshape(flattened_shape)\n        return support\n\n    @property\n    def empirical(self):\n        """"""\n        A dictionary of sites\' names and their corresponding :class:`EmpiricalMarginal`\n        distribution.\n\n        :type: :class:`OrderedDict`\n        """"""\n        return self._marginals\n\n\nclass TracePosterior(object, metaclass=ABCMeta):\n    """"""\n    Abstract TracePosterior object from which posterior inference algorithms inherit.\n    When run, collects a bag of execution traces from the approximate posterior.\n    This is designed to be used by other utility classes like `EmpiricalMarginal`,\n    that need access to the collected execution traces.\n    """"""\n    def __init__(self, num_chains=1):\n        self.num_chains = num_chains\n        self._reset()\n\n    def _reset(self):\n        self.log_weights = []\n        self.exec_traces = []\n        self.chain_ids = []  # chain id corresponding to the sample\n        self._idx_by_chain = [[] for _ in range(self.num_chains)]  # indexes of samples by chain id\n        self._categorical = None\n\n    def marginal(self, sites=None):\n        """"""\n        Generates the marginal distribution of this posterior.\n\n        :param list sites: optional list of sites for which we need to generate\n            the marginal distribution.\n        :returns: A :class:`Marginals` class instance.\n        :rtype: :class:`Marginals`\n        """"""\n        return Marginals(self, sites)\n\n    @abstractmethod\n    def _traces(self, *args, **kwargs):\n        """"""\n        Abstract method implemented by classes that inherit from `TracePosterior`.\n\n        :return: Generator over ``(exec_trace, weight)`` or\n        ``(exec_trace, weight, chain_id)``.\n        """"""\n        raise NotImplementedError(""Inference algorithm must implement ``_traces``."")\n\n    def __call__(self, *args, **kwargs):\n        # To ensure deterministic sampling in the presence of multiple chains,\n        # we get the index from ``idxs_by_chain`` instead of sampling from\n        # the marginal directly.\n        random_idx = self._categorical.sample().item()\n        chain_idx, sample_idx = random_idx % self.num_chains, random_idx // self.num_chains\n        sample_idx = self._idx_by_chain[chain_idx][sample_idx]\n        trace = self.exec_traces[sample_idx].copy()\n        for name in trace.observation_nodes:\n            trace.remove_node(name)\n        return trace\n\n    def run(self, *args, **kwargs):\n        """"""\n        Calls `self._traces` to populate execution traces from a stochastic\n        Pyro model.\n\n        :param args: optional args taken by `self._traces`.\n        :param kwargs: optional keywords args taken by `self._traces`.\n        """"""\n        self._reset()\n        with poutine.block():\n            for i, vals in enumerate(self._traces(*args, **kwargs)):\n                if len(vals) == 2:\n                    chain_id = 0\n                    tr, logit = vals\n                else:\n                    tr, logit, chain_id = vals\n                    assert chain_id < self.num_chains\n                self.exec_traces.append(tr)\n                self.log_weights.append(logit)\n                self.chain_ids.append(chain_id)\n                self._idx_by_chain[chain_id].append(i)\n        self._categorical = Categorical(logits=torch.tensor(self.log_weights))\n        return self\n\n    def information_criterion(self, pointwise=False):\n        """"""\n        Computes information criterion of the model. Currently, returns only ""Widely\n        Applicable/Watanabe-Akaike Information Criterion"" (WAIC) and the corresponding\n        effective number of parameters.\n\n        Reference:\n\n        [1] `Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC`,\n        Aki Vehtari, Andrew Gelman, and Jonah Gabry\n\n        :param bool pointwise: a flag to decide if we want to get a vectorized WAIC or not. When\n            ``pointwise=False``, returns the sum.\n        :returns: a dictionary containing values of WAIC and its effective number of\n            parameters.\n        :rtype: :class:`OrderedDict`\n        """"""\n        if not self.exec_traces:\n            return {}\n        obs_node = None\n        log_likelihoods = []\n        for trace in self.exec_traces:\n            obs_nodes = trace.observation_nodes\n            if len(obs_nodes) > 1:\n                raise ValueError(""Infomation criterion calculation only works for models ""\n                                 ""with one observation node."")\n            if obs_node is None:\n                obs_node = obs_nodes[0]\n            elif obs_node != obs_nodes[0]:\n                raise ValueError(""Observation node has been changed, expected {} but got {}""\n                                 .format(obs_node, obs_nodes[0]))\n\n            log_likelihoods.append(trace.nodes[obs_node][""fn""]\n                                   .log_prob(trace.nodes[obs_node][""value""]))\n\n        ll = torch.stack(log_likelihoods, dim=0)\n        waic_value, p_waic = waic(ll, torch.tensor(self.log_weights, device=ll.device), pointwise)\n        return OrderedDict([(""waic"", waic_value), (""p_waic"", p_waic)])\n\n\nclass TracePredictive(TracePosterior):\n    """"""\n    .. warning::\n        This class is deprecated and will be removed in a future release.\n        Use the :class:`~pyro.infer.predictive.Predictive` class instead.\n\n    Generates and holds traces from the posterior predictive distribution,\n    given model execution traces from the approximate posterior. This is\n    achieved by constraining latent sites to randomly sampled parameter\n    values from the model execution traces and running the model forward\n    to generate traces with new response (""_RETURN"") sites.\n    :param model: arbitrary Python callable containing Pyro primitives.\n    :param TracePosterior posterior: trace posterior instance holding samples from the model\'s approximate posterior.\n    :param int num_samples: number of samples to generate.\n    :param keep_sites: The sites which should be sampled from posterior distribution (default: all)\n    """"""\n    def __init__(self, model, posterior, num_samples, keep_sites=None):\n        self.model = model\n        self.posterior = posterior\n        self.num_samples = num_samples\n        self.keep_sites = keep_sites\n        super().__init__()\n        warnings.warn(\'The `TracePredictive` class is deprecated and will be removed \'\n                      \'in a future release. Use the `pyro.infer.Predictive` class instead.\',\n                      FutureWarning)\n\n    def _traces(self, *args, **kwargs):\n        if not self.posterior.exec_traces:\n            self.posterior.run(*args, **kwargs)\n        data_trace = poutine.trace(self.model).get_trace(*args, **kwargs)\n        for _ in range(self.num_samples):\n            model_trace = self.posterior().copy()\n            self._remove_dropped_nodes(model_trace)\n            self._adjust_to_data(model_trace, data_trace)\n            resampled_trace = poutine.trace(poutine.replay(self.model, model_trace)).get_trace(*args, **kwargs)\n            yield (resampled_trace, 0., 0)\n\n    def _remove_dropped_nodes(self, trace):\n        if self.keep_sites is None:\n            return\n        for name, site in list(trace.nodes.items()):\n            if name not in self.keep_sites:\n                trace.remove_node(name)\n                continue\n\n    def _adjust_to_data(self, trace, data_trace):\n        subsampled_idxs = dict()\n        for name, site in trace.iter_stochastic_nodes():\n            # Adjust subsample sites\n            if site_is_subsample(site):\n                site[""fn""] = data_trace.nodes[name][""fn""]\n                site[""value""] = data_trace.nodes[name][""value""]\n            # Adjust sites under conditionally independent stacks\n            orig_cis_stack = site[""cond_indep_stack""]\n            site[""cond_indep_stack""] = data_trace.nodes[name][""cond_indep_stack""]\n            assert len(orig_cis_stack) == len(site[""cond_indep_stack""])\n            site[""fn""] = data_trace.nodes[name][""fn""]\n            for ocis, cis in zip(orig_cis_stack, site[""cond_indep_stack""]):\n                # Select random sub-indices to replay values under conditionally independent stacks.\n                # Otherwise, we assume there is an dependence of indexes between training data\n                # and prediction data.\n                assert ocis.name == cis.name\n                assert not site_is_subsample(site)\n                batch_dim = cis.dim - site[""fn""].event_dim\n                subsampled_idxs[cis.name] = subsampled_idxs.get(cis.name,\n                                                                torch.randint(0, ocis.size, (cis.size,),\n                                                                              device=site[""value""].device))\n                site[""value""] = site[""value""].index_select(batch_dim, subsampled_idxs[cis.name])\n\n    def marginal(self, sites=None):\n        """"""\n        Gets marginal distribution for this predictive posterior distribution.\n        """"""\n        return Marginals(self, sites)\n'"
pyro/infer/csis.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\n\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.infer.importance import Importance\nfrom pyro.infer.util import torch_item\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import check_model_guide_match, warn_if_nan\n\n\nclass CSIS(Importance):\n    """"""\n    Compiled Sequential Importance Sampling, allowing compilation of a guide\n    program to minimise KL(model posterior || guide), and inference with\n    importance sampling.\n\n    **Reference**\n    ""Inference Compilation and Universal Probabilistic Programming"" `pdf https://arxiv.org/pdf/1610.09900.pdf`\n\n    :param model: probabilistic model defined as a function. Must accept a\n        keyword argument named `observations`, in which observed values are\n        passed as, with the names of nodes as the keys.\n    :param guide: guide function which is used as an approximate posterior. Must\n        also accept `observations` as keyword argument.\n    :param optim: a Pyro optimizer\n    :type optim: pyro.optim.PyroOptim\n    :param num_inference_samples: The number of importance-weighted samples to\n        draw during inference.\n    :param training_batch_size: Number of samples to use to approximate the loss\n        before each gradient descent step during training.\n    :param validation_batch_size: Number of samples to use for calculating\n        validation loss (will only be used if `.validation_loss` is called).\n    """"""\n    def __init__(self,\n                 model,\n                 guide,\n                 optim,\n                 num_inference_samples=10,\n                 training_batch_size=10,\n                 validation_batch_size=20):\n        super().__init__(model, guide, num_inference_samples)\n        self.model = model\n        self.guide = guide\n        self.optim = optim\n        self.training_batch_size = training_batch_size\n        self.validation_batch_size = validation_batch_size\n        self.validation_batch = None\n\n    def set_validation_batch(self, *args, **kwargs):\n        """"""\n        Samples a batch of model traces and stores it as an object property.\n\n        Arguments are passed directly to model.\n        """"""\n        self.validation_batch = [self._sample_from_joint(*args, **kwargs)\n                                 for _ in range(self.validation_batch_size)]\n\n    def step(self, *args, **kwargs):\n        """"""\n        :returns: estimate of the loss\n        :rtype: float\n\n        Take a gradient step on the loss function. Arguments are passed to the\n        model and guide.\n        """"""\n        with poutine.trace(param_only=True) as param_capture:\n            loss = self.loss_and_grads(True, None, *args, **kwargs)\n\n        params = set(site[""value""].unconstrained()\n                     for site in param_capture.trace.nodes.values()\n                     if site[""value""].grad is not None)\n\n        self.optim(params)\n\n        pyro.infer.util.zero_grads(params)\n\n        return torch_item(loss)\n\n    def loss_and_grads(self, grads, batch, *args, **kwargs):\n        """"""\n        :returns: an estimate of the loss (expectation over p(x, y) of\n            -log q(x, y) ) - where p is the model and q is the guide\n        :rtype: float\n\n        If a batch is provided, the loss is estimated using these traces\n        Otherwise, a fresh batch is generated from the model.\n\n        If grads is True, will also call `backward` on loss.\n\n        `args` and `kwargs` are passed to the model and guide.\n        """"""\n        if batch is None:\n            batch = (self._sample_from_joint(*args, **kwargs)\n                     for _ in range(self.training_batch_size))\n            batch_size = self.training_batch_size\n        else:\n            batch_size = len(batch)\n\n        loss = 0\n        for model_trace in batch:\n            with poutine.trace(param_only=True) as particle_param_capture:\n                guide_trace = self._get_matched_trace(model_trace, *args, **kwargs)\n            particle_loss = self._differentiable_loss_particle(guide_trace)\n            particle_loss /= batch_size\n\n            if grads:\n                guide_params = set(site[""value""].unconstrained()\n                                   for site in particle_param_capture.trace.nodes.values())\n                guide_grads = torch.autograd.grad(particle_loss, guide_params, allow_unused=True)\n                for guide_grad, guide_param in zip(guide_grads, guide_params):\n                    guide_param.grad = guide_grad if guide_param.grad is None else guide_param.grad + guide_grad\n\n            loss += torch_item(particle_loss)\n\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def _differentiable_loss_particle(self, guide_trace):\n        return -guide_trace.log_prob_sum()\n\n    def validation_loss(self, *args, **kwargs):\n        """"""\n        :returns: loss estimated using validation batch\n        :rtype: float\n\n        Calculates loss on validation batch. If no validation batch is set,\n        will set one by calling `set_validation_batch`. Can be used to track\n        the loss in a less noisy way during training.\n\n        Arguments are passed to the model and guide.\n        """"""\n        if self.validation_batch is None:\n            self.set_validation_batch(*args, **kwargs)\n\n        return self.loss_and_grads(False, self.validation_batch, *args, **kwargs)\n\n    def _get_matched_trace(self, model_trace, *args, **kwargs):\n        """"""\n        :param model_trace: a trace from the model\n        :type model_trace: pyro.poutine.trace_struct.Trace\n        :returns: guide trace with sampled values matched to model_trace\n        :rtype: pyro.poutine.trace_struct.Trace\n\n        Returns a guide trace with values at sample and observe statements\n        matched to those in model_trace.\n\n        `args` and `kwargs` are passed to the guide.\n        """"""\n        kwargs[""observations""] = {}\n        for node in itertools.chain(model_trace.stochastic_nodes, model_trace.observation_nodes):\n            if ""was_observed"" in model_trace.nodes[node][""infer""]:\n                model_trace.nodes[node][""is_observed""] = True\n                kwargs[""observations""][node] = model_trace.nodes[node][""value""]\n\n        guide_trace = poutine.trace(poutine.replay(self.guide,\n                                                   model_trace)\n                                    ).get_trace(*args, **kwargs)\n\n        check_model_guide_match(model_trace, guide_trace)\n        guide_trace = prune_subsample_sites(guide_trace)\n\n        return guide_trace\n\n    def _sample_from_joint(self, *args, **kwargs):\n        """"""\n        :returns: a sample from the joint distribution over unobserved and\n            observed variables\n        :rtype: pyro.poutine.trace_struct.Trace\n\n        Returns a trace of the model without conditioning on any observations.\n\n        Arguments are passed directly to the model.\n        """"""\n        unconditioned_model = pyro.poutine.uncondition(self.model)\n        return poutine.trace(unconditioned_model).get_trace(*args, **kwargs)\n'"
pyro/infer/discrete.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nfrom collections import OrderedDict\n\nfrom opt_einsum import shared_intermediates\n\nimport pyro.ops.packed as packed\nfrom pyro import poutine\nfrom pyro.infer.traceenum_elbo import TraceEnum_ELBO\nfrom pyro.ops.contract import contract_tensor_tree\nfrom pyro.ops.einsum.adjoint import require_backward\nfrom pyro.ops.rings import MapRing, SampleRing\nfrom pyro.poutine.enum_messenger import EnumMessenger\nfrom pyro.poutine.replay_messenger import ReplayMessenger\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import jit_iter\n\n_RINGS = {0: MapRing, 1: SampleRing}\n\n\ndef _make_ring(temperature, cache, dim_to_size):\n    try:\n        return _RINGS[temperature](cache=cache, dim_to_size=dim_to_size)\n    except KeyError:\n        raise ValueError(""temperature must be 0 (map) or 1 (sample) for now"")\n\n\nclass SamplePosteriorMessenger(ReplayMessenger):\n    # This acts like ReplayMessenger but additionally replays cond_indep_stack.\n\n    def _pyro_sample(self, msg):\n        if msg[""infer""].get(""enumerate"") == ""parallel"":\n            super()._pyro_sample(msg)\n        if msg[""name""] in self.trace:\n            msg[""cond_indep_stack""] = self.trace.nodes[msg[""name""]][""cond_indep_stack""]\n\n\ndef _sample_posterior(model, first_available_dim, temperature, *args, **kwargs):\n    # For internal use by infer_discrete.\n\n    # Create an enumerated trace.\n    with poutine.block(), EnumMessenger(first_available_dim):\n        enum_trace = poutine.trace(model).get_trace(*args, **kwargs)\n    enum_trace = prune_subsample_sites(enum_trace)\n    enum_trace.compute_log_prob()\n    enum_trace.pack_tensors()\n\n    return _sample_posterior_from_trace(model, enum_trace, temperature, *args, **kwargs)\n\n\ndef _sample_posterior_from_trace(model, enum_trace, temperature, *args, **kwargs):\n    plate_to_symbol = enum_trace.plate_to_symbol\n\n    # Collect a set of query sample sites to which the backward algorithm will propagate.\n    sum_dims = set()\n    queries = []\n    dim_to_size = {}\n    cost_terms = OrderedDict()\n    enum_terms = OrderedDict()\n    for node in enum_trace.nodes.values():\n        if node[""type""] == ""sample"":\n            ordinal = frozenset(plate_to_symbol[f.name]\n                                for f in node[""cond_indep_stack""]\n                                if f.vectorized and f.size > 1)\n            # For sites that depend on an enumerated variable, we need to apply\n            # the mask but not the scale when sampling.\n            if ""masked_log_prob"" not in node[""packed""]:\n                node[""packed""][""masked_log_prob""] = packed.scale_and_mask(\n                    node[""packed""][""unscaled_log_prob""], mask=node[""packed""][""mask""])\n            log_prob = node[""packed""][""masked_log_prob""]\n            sum_dims.update(frozenset(log_prob._pyro_dims) - ordinal)\n            if sum_dims.isdisjoint(log_prob._pyro_dims):\n                continue\n            dim_to_size.update(zip(log_prob._pyro_dims, log_prob.shape))\n            if node[""infer""].get(""_enumerate_dim"") is None:\n                cost_terms.setdefault(ordinal, []).append(log_prob)\n            else:\n                enum_terms.setdefault(ordinal, []).append(log_prob)\n            # Note we mark all sample sites with require_backward to gather\n            # enumerated sites and adjust cond_indep_stack of all sample sites.\n            if not node[""is_observed""]:\n                queries.append(log_prob)\n                require_backward(log_prob)\n\n    # We take special care to match the term ordering in\n    # pyro.infer.traceenum_elbo._compute_model_factors() to allow\n    # contract_tensor_tree() to use shared_intermediates() inside\n    # TraceEnumSample_ELBO. The special ordering is: first all cost terms in\n    # order of model_trace, then all enum_terms in order of model trace.\n    log_probs = cost_terms\n    for ordinal, terms in enum_terms.items():\n        log_probs.setdefault(ordinal, []).extend(terms)\n\n    # Run forward-backward algorithm, collecting the ordinal of each connected component.\n    cache = getattr(enum_trace, ""_sharing_cache"", {})\n    ring = _make_ring(temperature, cache, dim_to_size)\n    with shared_intermediates(cache):\n        log_probs = contract_tensor_tree(log_probs, sum_dims, ring=ring)  # run forward algorithm\n    query_to_ordinal = {}\n    pending = object()  # a constant value for pending queries\n    for query in queries:\n        query._pyro_backward_result = pending\n    for ordinal, terms in log_probs.items():\n        for term in terms:\n            if hasattr(term, ""_pyro_backward""):\n                term._pyro_backward()  # run backward algorithm\n        # Note: this is quadratic in number of ordinals\n        for query in queries:\n            if query not in query_to_ordinal and query._pyro_backward_result is not pending:\n                query_to_ordinal[query] = ordinal\n\n    # Construct a collapsed trace by gathering and adjusting cond_indep_stack.\n    collapsed_trace = poutine.Trace()\n    for node in enum_trace.nodes.values():\n        if node[""type""] == ""sample"" and not node[""is_observed""]:\n            # TODO move this into a Leaf implementation somehow\n            new_node = {\n                ""type"": ""sample"",\n                ""name"": node[""name""],\n                ""is_observed"": False,\n                ""infer"": node[""infer""].copy(),\n                ""cond_indep_stack"": node[""cond_indep_stack""],\n                ""value"": node[""value""],\n            }\n            log_prob = node[""packed""][""masked_log_prob""]\n            if hasattr(log_prob, ""_pyro_backward_result""):\n                # Adjust the cond_indep_stack.\n                ordinal = query_to_ordinal[log_prob]\n                new_node[""cond_indep_stack""] = tuple(\n                    f for f in node[""cond_indep_stack""]\n                    if not (f.vectorized and f.size > 1) or plate_to_symbol[f.name] in ordinal)\n\n                # Gather if node depended on an enumerated value.\n                sample = log_prob._pyro_backward_result\n                if sample is not None:\n                    new_value = packed.pack(node[""value""], node[""infer""][""_dim_to_symbol""])\n                    for index, dim in zip(jit_iter(sample), sample._pyro_sample_dims):\n                        if dim in new_value._pyro_dims:\n                            index._pyro_dims = sample._pyro_dims[1:]\n                            new_value = packed.gather(new_value, index, dim)\n                    new_node[""value""] = packed.unpack(new_value, enum_trace.symbol_to_dim)\n\n            collapsed_trace.add_node(node[""name""], **new_node)\n\n    # Replay the model against the collapsed trace.\n    with SamplePosteriorMessenger(trace=collapsed_trace):\n        return model(*args, **kwargs)\n\n\ndef infer_discrete(fn=None, first_available_dim=None, temperature=1):\n    """"""\n    A poutine that samples discrete sites marked with\n    ``site[""infer""][""enumerate""] = ""parallel""`` from the posterior,\n    conditioned on observations.\n\n    Example::\n\n        @infer_discrete(first_available_dim=-1, temperature=0)\n        @config_enumerate\n        def viterbi_decoder(data, hidden_dim=10):\n            transition = 0.3 / hidden_dim + 0.7 * torch.eye(hidden_dim)\n            means = torch.arange(float(hidden_dim))\n            states = [0]\n            for t in pyro.markov(range(len(data))):\n                states.append(pyro.sample(""states_{}"".format(t),\n                                          dist.Categorical(transition[states[-1]])))\n                pyro.sample(""obs_{}"".format(t),\n                            dist.Normal(means[states[-1]], 1.),\n                            obs=data[t])\n            return states  # returns maximum likelihood states\n\n    .. warning: The ``log_prob``s of the inferred model\'s trace are not\n        meaningful, and may be changed future release.\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param int first_available_dim: The first tensor dimension (counting\n        from the right) that is available for parallel enumeration. This\n        dimension and all dimensions left may be used internally by Pyro.\n        This should be a negative integer.\n    :param int temperature: Either 1 (sample via forward-filter backward-sample)\n        or 0 (optimize via Viterbi-like MAP inference). Defaults to 1 (sample).\n    """"""\n    assert first_available_dim < 0, first_available_dim\n    if fn is None:  # support use as a decorator\n        return functools.partial(infer_discrete,\n                                 first_available_dim=first_available_dim,\n                                 temperature=temperature)\n    return functools.partial(_sample_posterior, fn, first_available_dim, temperature)\n\n\nclass TraceEnumSample_ELBO(TraceEnum_ELBO):\n    """"""\n    This extends :class:`TraceEnum_ELBO` to make it cheaper to sample from\n    discrete latent states during SVI.\n\n    The following are equivalent but the first is cheaper, sharing work\n    between the computations of ``loss`` and ``z``::\n\n        # Version 1.\n        elbo = TraceEnumSample_ELBO(max_plate_nesting=1)\n        loss = elbo.loss(*args, **kwargs)\n        z = elbo.sample_saved()\n\n        # Version 2.\n        elbo = TraceEnum_ELBO(max_plate_nesting=1)\n        loss = elbo.loss(*args, **kwargs)\n        guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n        z = infer_discrete(poutine.replay(model, guide_trace),\n                           first_available_dim=-2)(*args, **kwargs)\n\n    """"""\n    def _get_trace(self, model, guide, args, kwargs):\n        model_trace, guide_trace = super()._get_trace(\n            model, guide, args, kwargs)\n\n        # Mark all sample sites with require_backward to gather enumerated\n        # sites and adjust cond_indep_stack of all sample sites.\n        for node in model_trace.nodes.values():\n            if node[""type""] == ""sample"" and not node[""is_observed""]:\n                log_prob = node[""packed""][""unscaled_log_prob""]\n                require_backward(log_prob)\n\n        self._saved_state = model, model_trace, guide_trace, args, kwargs\n        return model_trace, guide_trace\n\n    def sample_saved(self):\n        """"""\n        Generate latent samples while reusing work from SVI.step().\n        """"""\n        model, model_trace, guide_trace, args, kwargs = self._saved_state\n        model = poutine.replay(model, guide_trace)\n        temperature = 1\n        return _sample_posterior_from_trace(model, model_trace, temperature, *args, **kwargs)\n'"
pyro/infer/elbo.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport warnings\nfrom abc import ABCMeta, abstractmethod\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.infer.util import is_validation_enabled\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import check_site_shape\n\n\nclass ELBO(object, metaclass=ABCMeta):\n    """"""\n    :class:`ELBO` is the top-level interface for stochastic variational\n    inference via optimization of the evidence lower bound.\n\n    Most users will not interact with this base class :class:`ELBO` directly;\n    instead they will create instances of derived classes:\n    :class:`~pyro.infer.trace_elbo.Trace_ELBO`,\n    :class:`~pyro.infer.tracegraph_elbo.TraceGraph_ELBO`, or\n    :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO`.\n\n    :param num_particles: The number of particles/samples used to form the ELBO\n        (gradient) estimators.\n    :param int max_plate_nesting: Optional bound on max number of nested\n        :func:`pyro.plate` contexts. This is only required when enumerating\n        over sample sites in parallel, e.g. if a site sets\n        ``infer={""enumerate"": ""parallel""}``. If omitted, ELBO may guess a valid\n        value by running the (model,guide) pair once, however this guess may\n        be incorrect if model or guide structure is dynamic.\n    :param bool vectorize_particles: Whether to vectorize the ELBO computation\n        over `num_particles`. Defaults to False. This requires static structure\n        in model and guide.\n    :param bool strict_enumeration_warning: Whether to warn about possible\n        misuse of enumeration, i.e. that\n        :class:`pyro.infer.traceenum_elbo.TraceEnum_ELBO` is used iff there\n        are enumerated sample sites.\n    :param bool ignore_jit_warnings: Flag to ignore warnings from the JIT\n        tracer. When this is True, all :class:`torch.jit.TracerWarning` will\n        be ignored. Defaults to False.\n    :param bool jit_options: Optional dict of options to pass to\n        :func:`torch.jit.trace` , e.g. ``{""check_trace"": True}``.\n    :param bool retain_graph: Whether to retain autograd graph during an SVI\n        step. Defaults to None (False).\n    :param float tail_adaptive_beta: Exponent beta with ``-1.0 <= beta < 0.0`` for\n        use with `TraceTailAdaptive_ELBO`.\n\n    References\n\n    [1] `Automated Variational Inference in Probabilistic Programming`\n    David Wingate, Theo Weber\n\n    [2] `Black Box Variational Inference`,\n    Rajesh Ranganath, Sean Gerrish, David M. Blei\n    """"""\n\n    def __init__(self,\n                 num_particles=1,\n                 max_plate_nesting=float(\'inf\'),\n                 max_iarange_nesting=None,  # DEPRECATED\n                 vectorize_particles=False,\n                 strict_enumeration_warning=True,\n                 ignore_jit_warnings=False,\n                 jit_options=None,\n                 retain_graph=None,\n                 tail_adaptive_beta=-1.0):\n        if max_iarange_nesting is not None:\n            warnings.warn(""max_iarange_nesting is deprecated; use max_plate_nesting instead"",\n                          DeprecationWarning)\n            max_plate_nesting = max_iarange_nesting\n        self.max_plate_nesting = max_plate_nesting\n        self.num_particles = num_particles\n        self.vectorize_particles = vectorize_particles\n        self.retain_graph = retain_graph\n        if self.vectorize_particles and self.num_particles > 1:\n            self.max_plate_nesting += 1\n        self.strict_enumeration_warning = strict_enumeration_warning\n        self.ignore_jit_warnings = ignore_jit_warnings\n        self.jit_options = jit_options\n        self.tail_adaptive_beta = tail_adaptive_beta\n\n    def _guess_max_plate_nesting(self, model, guide, args, kwargs):\n        """"""\n        Guesses max_plate_nesting by running the (model,guide) pair once\n        without enumeration. This optimistically assumes static model\n        structure.\n        """"""\n        # Ignore validation to allow model-enumerated sites absent from the guide.\n        with poutine.block():\n            guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)\n            model_trace = poutine.trace(\n                poutine.replay(model, trace=guide_trace)).get_trace(*args, **kwargs)\n        guide_trace = prune_subsample_sites(guide_trace)\n        model_trace = prune_subsample_sites(model_trace)\n        sites = [site\n                 for trace in (model_trace, guide_trace)\n                 for site in trace.nodes.values()\n                 if site[""type""] == ""sample""]\n\n        # Validate shapes now, since shape constraints will be weaker once\n        # max_plate_nesting is changed from float(\'inf\') to some finite value.\n        # Here we know the traces are not enumerated, but later we\'ll need to\n        # allow broadcasting of dims to the left of max_plate_nesting.\n        if is_validation_enabled():\n            guide_trace.compute_log_prob()\n            model_trace.compute_log_prob()\n            for site in sites:\n                check_site_shape(site, max_plate_nesting=float(\'inf\'))\n\n        dims = [frame.dim\n                for site in sites\n                for frame in site[""cond_indep_stack""]\n                if frame.vectorized]\n        self.max_plate_nesting = -min(dims) if dims else 0\n        if self.vectorize_particles and self.num_particles > 1:\n            self.max_plate_nesting += 1\n        logging.info(\'Guessed max_plate_nesting = {}\'.format(self.max_plate_nesting))\n\n    def _vectorized_num_particles(self, fn):\n        """"""\n        Wraps a callable inside an outermost :class:`~pyro.plate` to parallelize\n        ELBO computation over `num_particles`, and to broadcast batch shapes of\n        sample site functions in accordance with the `~pyro.plate` contexts\n        within which they are embedded.\n\n        :param fn: arbitrary callable containing Pyro primitives.\n        :return: wrapped callable.\n        """"""\n\n        def wrapped_fn(*args, **kwargs):\n            if self.num_particles == 1:\n                return fn(*args, **kwargs)\n            with pyro.plate(""num_particles_vectorized"", self.num_particles, dim=-self.max_plate_nesting):\n                return fn(*args, **kwargs)\n\n        return wrapped_fn\n\n    def _get_vectorized_trace(self, model, guide, args, kwargs):\n        """"""\n        Wraps the model and guide to vectorize ELBO computation over\n        ``num_particles``, and returns a single trace from the wrapped model\n        and guide.\n        """"""\n        return self._get_trace(self._vectorized_num_particles(model),\n                               self._vectorized_num_particles(guide),\n                               args, kwargs)\n\n    @abstractmethod\n    def _get_trace(self, model, guide, args, kwargs):\n        """"""\n        Returns a single trace from the guide, and the model that is run\n        against it.\n        """"""\n        raise NotImplementedError\n\n    def _get_traces(self, model, guide, args, kwargs):\n        """"""\n        Runs the guide and runs the model against the guide with\n        the result packaged as a trace generator.\n        """"""\n        if self.vectorize_particles:\n            if self.max_plate_nesting == float(\'inf\'):\n                self._guess_max_plate_nesting(model, guide, args, kwargs)\n            yield self._get_vectorized_trace(model, guide, args, kwargs)\n        else:\n            for i in range(self.num_particles):\n                yield self._get_trace(model, guide, args, kwargs)\n'"
pyro/infer/energy_distance.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport operator\nfrom collections import OrderedDict\nfrom functools import reduce\n\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.distributions.util import scale_and_mask\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.util import is_validation_enabled, validation_enabled\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import check_model_guide_match, check_site_shape, warn_if_nan\n\n\ndef _squared_error(x, y, scale, mask):\n    diff = x - y\n    if getattr(scale, \'shape\', ()) or getattr(mask, \'shape\', ()):\n        error = torch.einsum(""nbe,nbe->nb"", diff, diff)\n        return scale_and_mask(error, scale, mask).sum(-1)\n    else:\n        error = torch.einsum(""nbe,nbe->n"", diff, diff)\n        return scale_and_mask(error, scale, mask)\n\n\nclass EnergyDistance:\n    r""""""\n    Posterior predictive energy distance [1,2] with optional Bayesian\n    regularization by the prior.\n\n    Let `p(x,z)=p(z) p(x|z)` be the model, `q(z|x)` be the guide.  Then given\n    data `x` and drawing an iid pair of samples :math:`(Z,X)` and\n    :math:`(Z\',X\')` (where `Z` is latent and `X` is the posterior predictive),\n\n    .. math ::\n\n        & Z \\sim q(z|x); \\quad X \\sim p(x|Z) \\\\\n        & Z\' \\sim q(z|x); \\quad X\' \\sim p(x|Z\') \\\\\n        & loss = \\mathbb E_X \\|X-x\\|^\\beta\n               - \\frac 1 2 \\mathbb E_{X,X\'}\\|X-X\'\\|^\\beta\n               - \\lambda \\mathbb E_Z \\log p(Z)\n\n    This is a likelihood-free inference algorithm, and can be used for\n    likelihoods without tractable density functions. The :math:`\\beta` energy\n    distance is a robust loss functions, and is well defined for any\n    distribution with finite fractional moment :math:`\\mathbb E[\\|X\\|^\\beta]`.\n\n    This requires static model structure, a fully reparametrized guide, and\n    reparametrized likelihood distributions in the model. Model latent\n    distributions may be non-reparametrized.\n\n    **References**\n\n    [1] Gabor J. Szekely, Maria L. Rizzo (2003)\n        Energy Statistics: A Class of Statistics Based on Distances.\n    [2] Tilmann Gneiting, Adrian E. Raftery (2007)\n        Strictly Proper Scoring Rules, Prediction, and Estimation.\n        https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf\n\n    :param float beta: Exponent :math:`\\beta` from [1,2]. The loss function is\n        strictly proper for distributions with finite :math:`beta`-absolute moment\n        :math:`E[\\|X\\|^\\beta]`. Thus for heavy tailed distributions ``beta`` should\n        be small, e.g. for ``Cauchy`` distributions, :math:`\\beta<1` is strictly\n        proper. Defaults to 1. Must be in the open interval (0,2).\n    :param float prior_scale: Nonnegative scale for prior regularization.\n        Model parameters are trained only if this is positive.\n        If zero (default), then model log densities will not be computed\n        (guide log densities are never computed).\n    :param int num_particles: The number of particles/samples used to form the\n        gradient estimators. Must be at least 2.\n    :param int max_plate_nesting: Optional bound on max number of nested\n        :func:`pyro.plate` contexts. If omitted, this will guess a valid value\n        by running the (model,guide) pair once.\n    """"""\n    def __init__(self,\n                 beta=1.,\n                 prior_scale=0.,\n                 num_particles=2,\n                 max_plate_nesting=float(\'inf\')):\n        if not (isinstance(beta, (float, int)) and 0 < beta and beta < 2):\n            raise ValueError(""Expected beta in (0,2), actual {}"".format(beta))\n        if not (isinstance(prior_scale, (float, int)) and prior_scale >= 0):\n            raise ValueError(""Expected prior_scale >= 0, actual {}"".format(prior_scale))\n        if not (isinstance(num_particles, int) and num_particles >= 2):\n            raise ValueError(""Expected num_particles >= 2, actual {}"".format(num_particles))\n        self.beta = beta\n        self.prior_scale = prior_scale\n        self.num_particles = num_particles\n        self.vectorize_particles = True\n        self.max_plate_nesting = max_plate_nesting\n\n    def _pow(self, x):\n        if self.beta == 1:\n            return x.sqrt()  # cheaper than .pow()\n        return x.pow(self.beta / 2)\n\n    def _get_traces(self, model, guide, args, kwargs):\n        if self.max_plate_nesting == float(""inf""):\n            with validation_enabled(False):  # Avoid calling .log_prob() when undefined.\n                # TODO factor this out as a stand-alone helper.\n                ELBO._guess_max_plate_nesting(self, model, guide, args, kwargs)\n        vectorize = pyro.plate(""num_particles_vectorized"", self.num_particles,\n                               dim=-self.max_plate_nesting)\n\n        # Trace the guide as in ELBO.\n        with poutine.trace() as tr, vectorize:\n            guide(*args, **kwargs)\n        guide_trace = tr.trace\n\n        # Trace the model, drawing posterior predictive samples.\n        with poutine.trace() as tr, poutine.uncondition():\n            with poutine.replay(trace=guide_trace), vectorize:\n                model(*args, **kwargs)\n        model_trace = tr.trace\n        for site in model_trace.nodes.values():\n            if site[""type""] == ""sample"" and site[""infer""].get(""was_observed"", False):\n                site[""is_observed""] = True\n        if is_validation_enabled():\n            check_model_guide_match(model_trace, guide_trace, self.max_plate_nesting)\n\n        guide_trace = prune_subsample_sites(guide_trace)\n        model_trace = prune_subsample_sites(model_trace)\n        if is_validation_enabled():\n            for site in guide_trace.nodes.values():\n                if site[""type""] == ""sample"":\n                    warn_if_nan(site[""value""], site[""name""])\n                    if not getattr(site[""fn""], ""has_rsample"", False):\n                        raise ValueError(""EnergyDistance requires fully reparametrized guides"")\n            for trace in model_trace.nodes.values():\n                if site[""type""] == ""sample"":\n                    if site[""is_observed""]:\n                        warn_if_nan(site[""value""], site[""name""])\n                        if not getattr(site[""fn""], ""has_rsample"", False):\n                            raise ValueError(""EnergyDistance requires reparametrized likelihoods"")\n\n        if self.prior_scale > 0:\n            model_trace.compute_log_prob(site_filter=lambda name, site: not site[""is_observed""])\n            if is_validation_enabled():\n                for site in model_trace.nodes.values():\n                    if site[""type""] == ""sample"":\n                        if not site[""is_observed""]:\n                            check_site_shape(site, self.max_plate_nesting)\n\n        return guide_trace, model_trace\n\n    def __call__(self, model, guide, *args, **kwargs):\n        """"""\n        Computes the surrogate loss that can be differentiated with autograd\n        to produce gradient estimates for the model and guide parameters.\n        """"""\n        guide_trace, model_trace = self._get_traces(model, guide, args, kwargs)\n\n        # Extract observations and posterior predictive samples.\n        data = OrderedDict()\n        samples = OrderedDict()\n        for name, site in model_trace.nodes.items():\n            if site[""type""] == ""sample"" and site[""is_observed""]:\n                data[name] = site[""infer""][""obs""]\n                samples[name] = site[""value""]\n        assert list(data.keys()) == list(samples.keys())\n        if not data:\n            raise ValueError(""Found no observations"")\n\n        # Compute energy distance from mean average error and generalized entropy.\n        squared_error = []  # E[ (X - x)^2 ]\n        squared_entropy = []  # E[ (X - X\')^2 ]\n        prototype = next(iter(data.values()))\n        pairs = prototype.new_ones(self.num_particles, self.num_particles).tril(-1).nonzero()\n        for name, obs in data.items():\n            sample = samples[name]\n            scale = model_trace.nodes[name][""scale""]\n            mask = model_trace.nodes[name][""mask""]\n\n            # Flatten to subshapes of (num_particles, batch_size, event_size).\n            event_dim = model_trace.nodes[name][""fn""].event_dim\n            batch_shape = obs.shape[:obs.dim() - event_dim]\n            event_shape = obs.shape[obs.dim() - event_dim:]\n            if getattr(scale, \'shape\', ()):\n                scale = scale.expand(batch_shape).reshape(-1)\n            if getattr(mask, \'shape\', ()):\n                mask = mask.expand(batch_shape).reshape(-1)\n            obs = obs.reshape(batch_shape.numel(), event_shape.numel())\n            sample = sample.reshape(self.num_particles, batch_shape.numel(), event_shape.numel())\n\n            squared_error.append(_squared_error(sample, obs, scale, mask))\n            squared_entropy.append(_squared_error(*sample[pairs].unbind(1), scale, mask))\n\n        squared_error = reduce(operator.add, squared_error)\n        squared_entropy = reduce(operator.add, squared_entropy)\n        error = self._pow(squared_error).mean()  # E[ ||X-x||^beta ]\n        entropy = self._pow(squared_entropy).mean()  # E[ ||X-X\'||^beta ]\n        energy = error - 0.5 * entropy\n\n        # Compute prior.\n        log_prior = 0\n        if self.prior_scale > 0:\n            for site in model_trace.nodes.values():\n                if site[""type""] == ""sample"" and not site[""is_observed""]:\n                    log_prior = log_prior + site[""log_prob_sum""]\n\n        # Compute final loss.\n        loss = energy - self.prior_scale * log_prior\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def loss(self, *args, **kwargs):\n        """"""\n        Not implemented. Added for compatibility with unit tests only.\n        """"""\n        raise NotImplementedError(""EnergyDistance implements only surrogate loss"")\n'"
pyro/infer/enum.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numbers\nfrom functools import partial\nfrom queue import LifoQueue\n\nfrom pyro import poutine\nfrom pyro.infer.util import is_validation_enabled\nfrom pyro.poutine import Trace\nfrom pyro.poutine.enum_messenger import enumerate_site\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import check_model_guide_match, check_site_shape, ignore_jit_warnings\n\n\ndef iter_discrete_escape(trace, msg):\n    return ((msg[""type""] == ""sample"") and\n            (not msg[""is_observed""]) and\n            (msg[""infer""].get(""enumerate"") == ""sequential"") and  # only sequential\n            (msg[""name""] not in trace))\n\n\ndef iter_discrete_extend(trace, site, **ignored):\n    values = enumerate_site(site)\n    enum_total = values.shape[0]\n    with ignore_jit_warnings([""Converting a tensor to a Python index"",\n                              (""Iterating over a tensor"", RuntimeWarning)]):\n        values = iter(values)\n    for i, value in enumerate(values):\n        extended_site = site.copy()\n        extended_site[""infer""] = site[""infer""].copy()\n        extended_site[""infer""][""_enum_total""] = enum_total\n        extended_site[""value""] = value\n        extended_trace = trace.copy()\n        extended_trace.add_node(site[""name""], **extended_site)\n        yield extended_trace\n\n\ndef get_importance_trace(graph_type, max_plate_nesting, model, guide, args, kwargs, detach=False):\n    """"""\n    Returns a single trace from the guide, which can optionally be detached,\n    and the model that is run against it.\n    """"""\n    guide_trace = poutine.trace(guide, graph_type=graph_type).get_trace(*args, **kwargs)\n    if detach:\n        guide_trace.detach_()\n    model_trace = poutine.trace(poutine.replay(model, trace=guide_trace),\n                                graph_type=graph_type).get_trace(*args, **kwargs)\n    if is_validation_enabled():\n        check_model_guide_match(model_trace, guide_trace, max_plate_nesting)\n\n    guide_trace = prune_subsample_sites(guide_trace)\n    model_trace = prune_subsample_sites(model_trace)\n\n    model_trace.compute_log_prob()\n    guide_trace.compute_score_parts()\n    if is_validation_enabled():\n        for site in model_trace.nodes.values():\n            if site[""type""] == ""sample"":\n                check_site_shape(site, max_plate_nesting)\n        for site in guide_trace.nodes.values():\n            if site[""type""] == ""sample"":\n                check_site_shape(site, max_plate_nesting)\n\n    return model_trace, guide_trace\n\n\ndef iter_discrete_traces(graph_type, fn, *args, **kwargs):\n    """"""\n    Iterate over all discrete choices of a stochastic function.\n\n    When sampling continuous random variables, this behaves like `fn`.\n    When sampling discrete random variables, this iterates over all choices.\n\n    This yields traces scaled by the probability of the discrete choices made\n    in the `trace`.\n\n    :param str graph_type: The type of the graph, e.g. ""flat"" or ""dense"".\n    :param callable fn: A stochastic function.\n    :returns: An iterator over traces pairs.\n    """"""\n    queue = LifoQueue()\n    queue.put(Trace())\n    traced_fn = poutine.trace(\n        poutine.queue(fn, queue, escape_fn=iter_discrete_escape, extend_fn=iter_discrete_extend),\n        graph_type=graph_type)\n    while not queue.empty():\n        yield traced_fn.get_trace(*args, **kwargs)\n\n\ndef _config_fn(default, expand, num_samples, tmc, site):\n    if site[""type""] != ""sample"" or site[""is_observed""]:\n        return {}\n    if type(site[""fn""]).__name__ == ""_Subsample"":\n        return {}\n    if num_samples is not None:\n        return {""enumerate"": site[""infer""].get(""enumerate"", default),\n                ""num_samples"": site[""infer""].get(""num_samples"", num_samples),\n                ""expand"": site[""infer""].get(""expand"", expand),\n                ""tmc"": site[""infer""].get(""tmc"", tmc)}\n    if getattr(site[""fn""], ""has_enumerate_support"", False):\n        return {""enumerate"": site[""infer""].get(""enumerate"", default),\n                ""expand"": site[""infer""].get(""expand"", expand)}\n    return {}\n\n\ndef _config_enumerate(default, expand, num_samples, tmc):\n    return partial(_config_fn, default, expand, num_samples, tmc)\n\n\ndef config_enumerate(guide=None, default=""parallel"", expand=False, num_samples=None, tmc=""diagonal""):\n    """"""\n    Configures enumeration for all relevant sites in a guide. This is mainly\n    used in conjunction with :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO`.\n\n    When configuring for exhaustive enumeration of discrete variables, this\n    configures all sample sites whose distribution satisfies\n    ``.has_enumerate_support == True``.\n    When configuring for local parallel Monte Carlo sampling via\n    ``default=""parallel"", num_samples=n``, this configures all sample sites.\n    This does not overwrite existing annotations ``infer={""enumerate"": ...}``.\n\n    This can be used as either a function::\n\n        guide = config_enumerate(guide)\n\n    or as a decorator::\n\n        @config_enumerate\n        def guide1(*args, **kwargs):\n            ...\n\n        @config_enumerate(default=""sequential"", expand=True)\n        def guide2(*args, **kwargs):\n            ...\n\n    :param callable guide: a pyro model that will be used as a guide in\n        :class:`~pyro.infer.svi.SVI`.\n    :param str default: Which enumerate strategy to use, one of\n        ""sequential"", ""parallel"", or None. Defaults to ""parallel"".\n    :param bool expand: Whether to expand enumerated sample values. See\n        :meth:`~pyro.distributions.Distribution.enumerate_support` for details.\n        This only applies to exhaustive enumeration, where ``num_samples=None``.\n        If ``num_samples`` is not ``None``, then this samples will always be\n        expanded.\n    :param num_samples: if not ``None``, use local Monte Carlo sampling rather\n        than exhaustive enumeration. This makes sense for both continuous and\n        discrete distributions.\n    :type num_samples: int or None\n    :param tmc: ""mixture"" or ""diagonal"" strategies to use in Tensor Monte Carlo\n    :type tmc: string or None\n    :return: an annotated guide\n    :rtype: callable\n    """"""\n    if default not in [""sequential"", ""parallel"", None]:\n        raise ValueError(""Invalid default value. Expected \'sequential\', \'parallel\', or None, but got {}"".format(\n            repr(default)))\n    if expand not in [True, False]:\n        raise ValueError(""Invalid expand value. Expected True or False, but got {}"".format(repr(expand)))\n    if num_samples is not None:\n        if not (isinstance(num_samples, numbers.Number) and num_samples > 0):\n            raise ValueError(""Invalid num_samples, expected None or positive integer, but got {}"".format(\n                repr(num_samples)))\n        if default == ""sequential"":\n            raise ValueError(\'Local sampling does not support ""sequential"" sampling; \'\n                             \'use ""parallel"" sampling instead.\')\n    if tmc is not None:\n        if tmc not in (""mixture"", ""diagonal""):\n            raise ValueError(""{} not a valid TMC strategy"".format(tmc))\n\n    # Support usage as a decorator:\n    if guide is None:\n        return lambda guide: config_enumerate(guide, default=default, expand=expand, num_samples=num_samples, tmc=tmc)\n\n    return poutine.infer_config(guide, config_fn=_config_enumerate(default, expand, num_samples, tmc))\n'"
pyro/infer/importance.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport warnings\n\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.ops.stats import fit_generalized_pareto\n\nfrom .abstract_infer import TracePosterior\nfrom .enum import get_importance_trace\n\n\nclass Importance(TracePosterior):\n    """"""\n    :param model: probabilistic model defined as a function\n    :param guide: guide used for sampling defined as a function\n    :param num_samples: number of samples to draw from the guide (default 10)\n\n    This method performs posterior inference by importance sampling\n    using the guide as the proposal distribution.\n    If no guide is provided, it defaults to proposing from the model\'s prior.\n    """"""\n\n    def __init__(self, model, guide=None, num_samples=None):\n        """"""\n        Constructor. default to num_samples = 10, guide = model\n        """"""\n        super().__init__()\n        if num_samples is None:\n            num_samples = 10\n            warnings.warn(""num_samples not provided, defaulting to {}"".format(num_samples))\n        if guide is None:\n            # propose from the prior by making a guide from the model by hiding observes\n            guide = poutine.block(model, hide_types=[""observe""])\n        self.num_samples = num_samples\n        self.model = model\n        self.guide = guide\n\n    def _traces(self, *args, **kwargs):\n        """"""\n        Generator of weighted samples from the proposal distribution.\n        """"""\n        for i in range(self.num_samples):\n            guide_trace = poutine.trace(self.guide).get_trace(*args, **kwargs)\n            model_trace = poutine.trace(\n                poutine.replay(self.model, trace=guide_trace)).get_trace(*args, **kwargs)\n            log_weight = model_trace.log_prob_sum() - guide_trace.log_prob_sum()\n            yield (model_trace, log_weight)\n\n    def get_log_normalizer(self):\n        """"""\n        Estimator of the normalizing constant of the target distribution.\n        (mean of the unnormalized weights)\n        """"""\n        # ensure list is not empty\n        if self.log_weights:\n            log_w = torch.tensor(self.log_weights)\n            log_num_samples = torch.log(torch.tensor(self.num_samples * 1.))\n            return torch.logsumexp(log_w - log_num_samples, 0)\n        else:\n            warnings.warn(""The log_weights list is empty, can not compute normalizing constant estimate."")\n\n    def get_normalized_weights(self, log_scale=False):\n        """"""\n        Compute the normalized importance weights.\n        """"""\n        if self.log_weights:\n            log_w = torch.tensor(self.log_weights)\n            log_w_norm = log_w - torch.logsumexp(log_w, 0)\n            return log_w_norm if log_scale else torch.exp(log_w_norm)\n        else:\n            warnings.warn(""The log_weights list is empty. There is nothing to normalize."")\n\n    def get_ESS(self):\n        """"""\n        Compute (Importance Sampling) Effective Sample Size (ESS).\n        """"""\n        if self.log_weights:\n            log_w_norm = self.get_normalized_weights(log_scale=True)\n            ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))\n        else:\n            warnings.warn(""The log_weights list is empty, effective sample size is zero."")\n            ess = 0\n        return ess\n\n\ndef vectorized_importance_weights(model, guide, *args, **kwargs):\n    """"""\n    :param model: probabilistic model defined as a function\n    :param guide: guide used for sampling defined as a function\n    :param num_samples: number of samples to draw from the guide (default 1)\n    :param int max_plate_nesting: Bound on max number of nested :func:`pyro.plate` contexts.\n    :param bool normalized: set to True to return self-normalized importance weights\n    :returns: returns a ``(num_samples,)``-shaped tensor of importance weights\n        and the model and guide traces that produced them\n\n    Vectorized computation of importance weights for models with static structure::\n\n        log_weights, model_trace, guide_trace = \\\\\n            vectorized_importance_weights(model, guide, *args,\n                                          num_samples=1000,\n                                          max_plate_nesting=4,\n                                          normalized=False)\n    """"""\n    num_samples = kwargs.pop(""num_samples"", 1)\n    max_plate_nesting = kwargs.pop(""max_plate_nesting"", None)\n    normalized = kwargs.pop(""normalized"", False)\n\n    if max_plate_nesting is None:\n        raise ValueError(""must provide max_plate_nesting"")\n    max_plate_nesting += 1\n\n    def vectorize(fn):\n        def _fn(*args, **kwargs):\n            with pyro.plate(""num_particles_vectorized"", num_samples, dim=-max_plate_nesting):\n                return fn(*args, **kwargs)\n        return _fn\n\n    model_trace, guide_trace = get_importance_trace(\n        ""flat"", max_plate_nesting, vectorize(model), vectorize(guide), args, kwargs)\n\n    guide_trace.pack_tensors()\n    model_trace.pack_tensors(guide_trace.plate_to_symbol)\n\n    if num_samples == 1:\n        log_weights = model_trace.log_prob_sum() - guide_trace.log_prob_sum()\n    else:\n        wd = guide_trace.plate_to_symbol[""num_particles_vectorized""]\n        log_weights = 0.\n        for site in model_trace.nodes.values():\n            if site[""type""] != ""sample"":\n                continue\n            log_weights += torch.einsum(site[""packed""][""log_prob""]._pyro_dims + ""->"" + wd,\n                                        [site[""packed""][""log_prob""]])\n\n        for site in guide_trace.nodes.values():\n            if site[""type""] != ""sample"":\n                continue\n            log_weights -= torch.einsum(site[""packed""][""log_prob""]._pyro_dims + ""->"" + wd,\n                                        [site[""packed""][""log_prob""]])\n\n    if normalized:\n        log_weights = log_weights - torch.logsumexp(log_weights)\n    return log_weights, model_trace, guide_trace\n\n\n@torch.no_grad()\ndef psis_diagnostic(model, guide, *args, **kwargs):\n    """"""\n    Computes the Pareto tail index k for a model/guide pair using the technique\n    described in [1], which builds on previous work in [2]. If :math:`0 < k < 0.5`\n    the guide is a good approximation to the model posterior, in the sense\n    described in [1]. If :math:`0.5 \\\\le k \\\\le 0.7`, the guide provides a suboptimal\n    approximation to the posterior, but may still be useful in practice. If\n    :math:`k > 0.7` the guide program provides a poor approximation to the full\n    posterior, and caution should be used when using the guide. Note, however,\n    that a guide may be a poor fit to the full posterior while still yielding\n    reasonable model predictions. If :math:`k < 0.0` the importance weights\n    corresponding to the model and guide appear to be bounded from above; this\n    would be a bizarre outcome for a guide trained via ELBO maximization. Please\n    see [1] for a more complete discussion of how the tail index k should be\n    interpreted.\n\n    Please be advised that a large number of samples may be required for an\n    accurate estimate of k.\n\n    Note that we assume that the model and guide are both vectorized and have\n    static structure. As is canonical in Pyro, the args and kwargs are passed\n    to the model and guide.\n\n    References\n    [1] \'Yes, but Did It Work?: Evaluating Variational Inference.\'\n    Yuling Yao, Aki Vehtari, Daniel Simpson, Andrew Gelman\n    [2] \'Pareto Smoothed Importance Sampling.\'\n    Aki Vehtari, Andrew Gelman, Jonah Gabry\n\n    :param callable model: the model program.\n    :param callable guide: the guide program.\n    :param int num_particles: the total number of times we run the model and guide in\n        order to compute the diagnostic. defaults to 1000.\n    :param max_simultaneous_particles: the maximum number of simultaneous samples drawn\n        from the model and guide. defaults to `num_particles`. `num_particles` must be\n        divisible by `max_simultaneous_particles`. compute the diagnostic. defaults to 1000.\n    :param int max_plate_nesting: optional bound on max number of nested :func:`pyro.plate`\n        contexts in the model/guide. defaults to 7.\n    :returns float: the PSIS diagnostic k\n    """"""\n\n    num_particles = kwargs.pop(\'num_particles\', 1000)\n    max_simultaneous_particles = kwargs.pop(\'max_simultaneous_particles\', num_particles)\n    max_plate_nesting = kwargs.pop(\'max_plate_nesting\', 7)\n\n    if num_particles % max_simultaneous_particles != 0:\n        raise ValueError(""num_particles must be divisible by max_simultaneous_particles."")\n\n    N = num_particles // max_simultaneous_particles\n    log_weights = [vectorized_importance_weights(model, guide, num_samples=max_simultaneous_particles,\n                                                 max_plate_nesting=max_plate_nesting,\n                                                 *args, **kwargs)[0] for _ in range(N)]\n    log_weights = torch.cat(log_weights)\n    log_weights -= log_weights.max()\n    log_weights = torch.sort(log_weights, descending=False)[0]\n\n    cutoff_index = - int(math.ceil(min(0.2 * num_particles, 3.0 * math.sqrt(num_particles)))) - 1\n    lw_cutoff = max(math.log(1.0e-15), log_weights[cutoff_index])\n    lw_tail = log_weights[log_weights > lw_cutoff]\n\n    if len(lw_tail) < 10:\n        warnings.warn(""Not enough tail samples to compute PSIS diagnostic; increase num_particles."")\n        k = float(\'inf\')\n    else:\n        k, _ = fit_generalized_pareto(lw_tail.exp() - math.exp(lw_cutoff))\n\n    return k\n'"
pyro/infer/predictive.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import reduce\nimport warnings\n\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.poutine.util import prune_subsample_sites\n\n\ndef _guess_max_plate_nesting(model, args, kwargs):\n    """"""\n    Guesses max_plate_nesting by running the model once\n    without enumeration. This optimistically assumes static model\n    structure.\n    """"""\n    with poutine.block():\n        model_trace = poutine.trace(model).get_trace(*args, **kwargs)\n    sites = [site for site in model_trace.nodes.values()\n             if site[""type""] == ""sample""]\n\n    dims = [frame.dim\n            for site in sites\n            for frame in site[""cond_indep_stack""]\n            if frame.vectorized]\n    max_plate_nesting = -min(dims) if dims else 0\n    return max_plate_nesting\n\n\ndef _predictive_sequential(model, posterior_samples, model_args, model_kwargs,\n                           num_samples, return_site_shapes, return_trace=False):\n    collected = []\n    samples = [{k: v[i] for k, v in posterior_samples.items()} for i in range(num_samples)]\n    for i in range(num_samples):\n        trace = poutine.trace(poutine.condition(model, samples[i])).get_trace(*model_args, **model_kwargs)\n        if return_trace:\n            collected.append(trace)\n        else:\n            collected.append({site: trace.nodes[site][\'value\'] for site in return_site_shapes})\n\n    if return_trace:\n        return collected\n    else:\n        return {site: torch.stack([s[site] for s in collected]).reshape(shape)\n                for site, shape in return_site_shapes.items()}\n\n\ndef _predictive(model, posterior_samples, num_samples, return_sites=(),\n                return_trace=False, parallel=False, model_args=(), model_kwargs={}):\n    max_plate_nesting = _guess_max_plate_nesting(model, model_args, model_kwargs)\n    vectorize = pyro.plate(""_num_predictive_samples"", num_samples, dim=-max_plate_nesting-1)\n    model_trace = prune_subsample_sites(poutine.trace(model).get_trace(*model_args, **model_kwargs))\n    reshaped_samples = {}\n\n    for name, sample in posterior_samples.items():\n        sample_shape = sample.shape[1:]\n        sample = sample.reshape((num_samples,) + (1,) * (max_plate_nesting - len(sample_shape)) + sample_shape)\n        reshaped_samples[name] = sample\n\n    if return_trace:\n        trace = poutine.trace(poutine.condition(vectorize(model), reshaped_samples))\\\n            .get_trace(*model_args, **model_kwargs)\n        return trace\n\n    return_site_shapes = {}\n    for site in model_trace.stochastic_nodes + model_trace.observation_nodes:\n        append_ndim = max_plate_nesting - len(model_trace.nodes[site][""fn""].batch_shape)\n        site_shape = (num_samples,) + (1,) * append_ndim + model_trace.nodes[site][\'value\'].shape\n        # non-empty return-sites\n        if return_sites:\n            if site in return_sites:\n                return_site_shapes[site] = site_shape\n        # special case (for guides): include all sites\n        elif return_sites is None:\n            return_site_shapes[site] = site_shape\n        # default case: return sites = ()\n        # include all sites not in posterior samples\n        elif site not in posterior_samples:\n            return_site_shapes[site] = site_shape\n\n    # handle _RETURN site\n    if return_sites is not None and \'_RETURN\' in return_sites:\n        value = model_trace.nodes[\'_RETURN\'][\'value\']\n        shape = (num_samples,) + value.shape if torch.is_tensor(value) else None\n        return_site_shapes[\'_RETURN\'] = shape\n\n    if not parallel:\n        return _predictive_sequential(model, posterior_samples, model_args, model_kwargs, num_samples,\n                                      return_site_shapes, return_trace=False)\n\n    trace = poutine.trace(poutine.condition(vectorize(model), reshaped_samples))\\\n        .get_trace(*model_args, **model_kwargs)\n    predictions = {}\n    for site, shape in return_site_shapes.items():\n        value = trace.nodes[site][\'value\']\n        if site == \'_RETURN\' and shape is None:\n            predictions[site] = value\n            continue\n        if value.numel() < reduce((lambda x, y: x * y), shape):\n            predictions[site] = value.expand(shape)\n        else:\n            predictions[site] = value.reshape(shape)\n\n    return predictions\n\n\nclass Predictive(torch.nn.Module):\n    """"""\n    EXPERIMENTAL class used to construct predictive distribution. The predictive\n    distribution is obtained by running the `model` conditioned on latent samples\n    from `posterior_samples`. If a `guide` is provided, then posterior samples\n    from all the latent sites are also returned.\n\n    .. warning::\n        The interface for the :class:`Predictive` class is experimental, and\n        might change in the future.\n\n    :param model: Python callable containing Pyro primitives.\n    :param dict posterior_samples: dictionary of samples from the posterior.\n    :param callable guide: optional guide to get posterior samples of sites not present\n        in `posterior_samples`.\n    :param int num_samples: number of samples to draw from the predictive distribution.\n        This argument has no effect if ``posterior_samples`` is non-empty, in which case,\n        the leading dimension size of samples in ``posterior_samples`` is used.\n    :param return_sites: sites to return; by default only sample sites not present\n        in `posterior_samples` are returned.\n    :type return_sites: list, tuple, or set\n    :param bool parallel: predict in parallel by wrapping the existing model\n        in an outermost `plate` messenger. Note that this requires that the model has\n        all batch dims correctly annotated via :class:`~pyro.plate`. Default is `False`.\n    """"""\n    def __init__(self, model, posterior_samples=None, guide=None, num_samples=None,\n                 return_sites=(), parallel=False):\n        super().__init__()\n        if posterior_samples is None:\n            if num_samples is None:\n                raise ValueError(""Either posterior_samples or num_samples must be specified."")\n            posterior_samples = {}\n\n        for name, sample in posterior_samples.items():\n            batch_size = sample.shape[0]\n            if num_samples is None:\n                num_samples = batch_size\n            elif num_samples != batch_size:\n                warnings.warn(""Sample\'s leading dimension size {} is different from the ""\n                              ""provided {} num_samples argument. Defaulting to {}.""\n                              .format(batch_size, num_samples, batch_size), UserWarning)\n                num_samples = batch_size\n\n        if num_samples is None:\n            raise ValueError(""No sample sites in posterior samples to infer `num_samples`."")\n\n        if guide is not None and posterior_samples:\n            raise ValueError(""`posterior_samples` cannot be provided with the `guide` argument."")\n\n        if return_sites is not None:\n            assert isinstance(return_sites, (list, tuple, set))\n\n        self.model = model\n        self.posterior_samples = {} if posterior_samples is None else posterior_samples\n        self.num_samples = num_samples\n        self.guide = guide\n        self.return_sites = return_sites\n        self.parallel = parallel\n\n    def call(self, *args, **kwargs):\n        """"""\n        Method that calls :meth:`forward` and returns parameter values of the\n        guide as a `tuple` instead of a `dict`, which is a requirement for\n        JIT tracing. Unlike :meth:`forward`, this method can be traced by\n        :func:`torch.jit.trace_module`.\n\n        .. warning::\n            This method may be removed once PyTorch JIT tracer starts accepting\n            `dict` as valid return types. See\n            `issue <https://github.com/pytorch/pytorch/issues/27743>`_.\n        """"""\n        result = self.forward(*args, **kwargs)\n        return tuple(v for _, v in sorted(result.items()))\n\n    def forward(self, *args, **kwargs):\n        """"""\n        Returns dict of samples from the predictive distribution. By default, only sample sites not\n        contained in `posterior_samples` are returned. This can be modified by changing the\n        `return_sites` keyword argument of this :class:`Predictive` instance.\n\n        :param args: model arguments.\n        :param kwargs: model keyword arguments.\n        """"""\n        posterior_samples = self.posterior_samples\n        return_sites = self.return_sites\n        if self.guide is not None:\n            # return all sites by default if a guide is provided.\n            return_sites = None if not return_sites else return_sites\n            posterior_samples = _predictive(self.guide, posterior_samples, self.num_samples, return_sites=None,\n                                            parallel=self.parallel, model_args=args, model_kwargs=kwargs)\n        return _predictive(self.model, posterior_samples, self.num_samples, return_sites=return_sites,\n                           parallel=self.parallel, model_args=args, model_kwargs=kwargs)\n\n    def get_samples(self, *args, **kwargs):\n        warnings.warn(""The method `.get_samples` has been deprecated in favor of `.forward`."",\n                      DeprecationWarning)\n        return self.forward(*args, **kwargs)\n\n    def get_vectorized_trace(self, *args, **kwargs):\n        """"""\n        Returns a single vectorized `trace` from the predictive distribution. Note that this\n        requires that the model has all batch dims correctly annotated via :class:`~pyro.plate`.\n\n        :param args: model arguments.\n        :param kwargs: model keyword arguments.\n        """"""\n        posterior_samples = self.posterior_samples\n        if self.guide is not None:\n            posterior_samples = _predictive(self.guide, posterior_samples, self.num_samples,\n                                            parallel=self.parallel, model_args=args, model_kwargs=kwargs)\n        return _predictive(self.model, posterior_samples, self.num_samples,\n                           return_trace=True, model_args=args, model_kwargs=kwargs)\n'"
pyro/infer/renyi_elbo.py,11,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport warnings\n\nimport torch\n\nfrom pyro.distributions.util import is_identically_zero\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.enum import get_importance_trace\nfrom pyro.infer.util import get_dependent_plate_dims, is_validation_enabled, torch_sum\nfrom pyro.util import check_if_enumerated, warn_if_nan\n\n\nclass RenyiELBO(ELBO):\n    r""""""\n    An implementation of Renyi\'s :math:`\\alpha`-divergence variational inference\n    following reference [1].\n\n    In order for the objective to be a strict lower bound, we require\n    :math:`\\alpha \\ge 0`. Note, however, that according to reference [1], depending\n    on the dataset :math:`\\alpha < 0` might give better results. In the special case\n    :math:`\\alpha = 0`, the objective function is that of the important weighted\n    autoencoder derived in reference [2].\n\n    .. note:: Setting :math:`\\alpha < 1` gives a better bound than the usual ELBO.\n        For :math:`\\alpha = 1`, it is better to use\n        :class:`~pyro.infer.trace_elbo.Trace_ELBO` class because it helps reduce\n        variances of gradient estimations.\n\n    :param float alpha: The order of :math:`\\alpha`-divergence. Here\n        :math:`\\alpha \\neq 1`. Default is 0.\n    :param num_particles: The number of particles/samples used to form the objective\n        (gradient) estimator. Default is 2.\n    :param int max_plate_nesting: Bound on max number of nested\n        :func:`pyro.plate` contexts. Default is infinity.\n    :param bool strict_enumeration_warning: Whether to warn about possible\n        misuse of enumeration, i.e. that\n        :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO` is used iff there\n        are enumerated sample sites.\n\n    References:\n\n    [1] `Renyi Divergence Variational Inference`,\n        Yingzhen Li, Richard E. Turner\n\n    [2] `Importance Weighted Autoencoders`,\n        Yuri Burda, Roger Grosse, Ruslan Salakhutdinov\n    """"""\n\n    def __init__(self,\n                 alpha=0,\n                 num_particles=2,\n                 max_plate_nesting=float(\'inf\'),\n                 max_iarange_nesting=None,  # DEPRECATED\n                 vectorize_particles=False,\n                 strict_enumeration_warning=True):\n        if max_iarange_nesting is not None:\n            warnings.warn(""max_iarange_nesting is deprecated; use max_plate_nesting instead"",\n                          DeprecationWarning)\n            max_plate_nesting = max_iarange_nesting\n\n        if alpha == 1:\n            raise ValueError(""The order alpha should not be equal to 1. Please use Trace_ELBO class""\n                             ""for the case alpha = 1."")\n        self.alpha = alpha\n        super().__init__(num_particles=num_particles,\n                         max_plate_nesting=max_plate_nesting,\n                         vectorize_particles=vectorize_particles,\n                         strict_enumeration_warning=strict_enumeration_warning)\n\n    def _get_trace(self, model, guide, args, kwargs):\n        """"""\n        Returns a single trace from the guide, and the model that is run\n        against it.\n        """"""\n        model_trace, guide_trace = get_importance_trace(\n            ""flat"", self.max_plate_nesting, model, guide, args, kwargs)\n        if is_validation_enabled():\n            check_if_enumerated(guide_trace)\n        return model_trace, guide_trace\n\n    @torch.no_grad()\n    def loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Evaluates the ELBO with an estimator that uses num_particles many samples/particles.\n        """"""\n        elbo_particles = []\n        is_vectorized = self.vectorize_particles and self.num_particles > 1\n\n        # grab a vectorized trace from the generator\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            elbo_particle = 0.\n            sum_dims = get_dependent_plate_dims(model_trace.nodes.values())\n\n            # compute elbo\n            for name, site in model_trace.nodes.items():\n                if site[""type""] == ""sample"":\n                    log_prob_sum = torch_sum(site[""log_prob""], sum_dims)\n                    elbo_particle = elbo_particle + log_prob_sum\n\n            for name, site in guide_trace.nodes.items():\n                if site[""type""] == ""sample"":\n                    log_prob, score_function_term, entropy_term = site[""score_parts""]\n                    log_prob_sum = torch_sum(site[""log_prob""], sum_dims)\n                    elbo_particle = elbo_particle - log_prob_sum\n\n            elbo_particles.append(elbo_particle)\n\n        if is_vectorized:\n            elbo_particles = elbo_particles[0]\n        else:\n            elbo_particles = torch.stack(elbo_particles)\n\n        log_weights = (1. - self.alpha) * elbo_particles\n        log_mean_weight = torch.logsumexp(log_weights, dim=0) - math.log(self.num_particles)\n        elbo = log_mean_weight.sum().item() / (1. - self.alpha)\n\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator.\n        Performs backward on the latter. Num_particle many samples are used to form the estimators.\n        """"""\n        elbo_particles = []\n        surrogate_elbo_particles = []\n        is_vectorized = self.vectorize_particles and self.num_particles > 1\n        tensor_holder = None\n\n        # grab a vectorized trace from the generator\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            elbo_particle = 0\n            surrogate_elbo_particle = 0\n            sum_dims = get_dependent_plate_dims(model_trace.nodes.values())\n\n            # compute elbo and surrogate elbo\n            for name, site in model_trace.nodes.items():\n                if site[""type""] == ""sample"":\n                    log_prob_sum = torch_sum(site[""log_prob""], sum_dims)\n                    elbo_particle = elbo_particle + log_prob_sum.detach()\n                    surrogate_elbo_particle = surrogate_elbo_particle + log_prob_sum\n\n            for name, site in guide_trace.nodes.items():\n                if site[""type""] == ""sample"":\n                    log_prob, score_function_term, entropy_term = site[""score_parts""]\n                    log_prob_sum = torch_sum(site[""log_prob""], sum_dims)\n\n                    elbo_particle = elbo_particle - log_prob_sum.detach()\n\n                    if not is_identically_zero(entropy_term):\n                        surrogate_elbo_particle = surrogate_elbo_particle - log_prob_sum\n\n                        if not is_identically_zero(score_function_term):\n                            # link to the issue: https://github.com/pyro-ppl/pyro/issues/1222\n                            raise NotImplementedError\n\n                    if not is_identically_zero(score_function_term):\n                        surrogate_elbo_particle = (surrogate_elbo_particle +\n                                                   (self.alpha / (1. - self.alpha)) * log_prob_sum)\n\n            if is_identically_zero(elbo_particle):\n                if tensor_holder is not None:\n                    elbo_particle = torch.zeros_like(tensor_holder)\n                    surrogate_elbo_particle = torch.zeros_like(tensor_holder)\n            else:  # elbo_particle is not None\n                if tensor_holder is None:\n                    tensor_holder = torch.zeros_like(elbo_particle)\n                    # change types of previous `elbo_particle`s\n                    for i in range(len(elbo_particles)):\n                        elbo_particles[i] = torch.zeros_like(tensor_holder)\n                        surrogate_elbo_particles[i] = torch.zeros_like(tensor_holder)\n\n            elbo_particles.append(elbo_particle)\n            surrogate_elbo_particles.append(surrogate_elbo_particle)\n\n        if tensor_holder is None:\n            return 0.\n\n        if is_vectorized:\n            elbo_particles = elbo_particles[0]\n            surrogate_elbo_particles = surrogate_elbo_particles[0]\n        else:\n            elbo_particles = torch.stack(elbo_particles)\n            surrogate_elbo_particles = torch.stack(surrogate_elbo_particles)\n\n        log_weights = (1. - self.alpha) * elbo_particles\n        log_mean_weight = torch.logsumexp(log_weights, dim=0, keepdim=True) - math.log(self.num_particles)\n        elbo = log_mean_weight.sum().item() / (1. - self.alpha)\n\n        # collect parameters to train from model and guide\n        trainable_params = any(site[""type""] == ""param""\n                               for trace in (model_trace, guide_trace)\n                               for site in trace.nodes.values())\n\n        if trainable_params and getattr(surrogate_elbo_particles, \'requires_grad\', False):\n            normalized_weights = (log_weights - log_mean_weight).exp()\n            surrogate_elbo = (normalized_weights * surrogate_elbo_particles).sum() / self.num_particles\n            surrogate_loss = -surrogate_elbo\n            surrogate_loss.backward()\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n'"
pyro/infer/rws.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.enum import get_importance_trace\nfrom pyro.infer.util import is_validation_enabled\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import check_if_enumerated, check_model_guide_match, warn_if_nan\n\n\nclass ReweightedWakeSleep(ELBO):\n    r""""""\n    An implementation of Reweighted Wake Sleep following reference [1].\n\n    .. note:: Sampling and log_prob evaluation asymptotic complexity:\n\n    1) Using wake-theta and/or wake-phi\n        O(`num_particles`) samples from guide,\n        O(`num_particles`) `log_prob` evaluations of model and guide\n\n    2) Using sleep-phi\n        O(`num_sleep_particles`) samples from model,\n        O(`num_sleep_particles`) `log_prob` evaluations of guide\n\n    if 1) and 2) are combined,\n        O(`num_particles`) samples from the guide,\n        O(`num_sleep_particles`) from the model,\n        O(`num_particles` + `num_sleep_particles`) `log_prob` evaluations of the guide, and\n        O(`num_particles`) evaluations of the model\n\n    .. note:: This is particularly useful for models with stochastic branching,\n        as described in [2].\n\n    .. note:: This returns _two_ losses, one each for (a) the model parameters (`theta`), computed using the\n        `iwae` objective, and (b) the guide parameters (`phi`), computed using (a combination of) the `csis`\n        objective and a self-normalized importance-sampled version of the `csis` objective.\n\n    .. note:: In order to enable computing the sleep-phi terms, the guide program must have its observations\n        explicitly passed in through the keyworded argument `observations`. Where the value of the observations\n        is unknown during definition, such as for amortized variational inference, it may be given a default\n        argument as `observations=None`, and the correct value supplied during learning through\n        `svi.step(observations=...)`.\n\n    .. warning:: Mini-batch training is not supported yet.\n\n    :param int num_particles: The number of particles/samples used to form the objective\n        (gradient) estimator. Default is 2.\n    :param insomnia: The scaling between the wake-phi and sleep-phi terms. Default is 1.0 [wake-phi]\n    :param bool model_has_params: Indicate if model has learnable params. Useful in avoiding extra\n        computation when running in pure sleep mode [csis]. Default is True.\n    :param int num_sleep_particles: The number of particles used to form the sleep-phi estimator.\n        Matches `num_particles` by default.\n    :param bool vectorize_particles: Whether the traces should be vectorised\n        across `num_particles`. Default is True.\n    :param int max_plate_nesting: Bound on max number of nested\n        :func:`pyro.plate` contexts. Default is infinity.\n    :param bool strict_enumeration_warning: Whether to warn about possible\n        misuse of enumeration, i.e. that\n        :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO` is used iff there\n        are enumerated sample sites.\n\n    References:\n\n    [1] `Reweighted Wake-Sleep`,\n        J\xc3\xb6rg Bornschein, Yoshua Bengio\n\n    [2] `Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow`,\n        Tuan Anh Le, Adam R. Kosiorek, N. Siddharth, Yee Whye Teh, Frank Wood\n    """"""\n\n    def __init__(self,\n                 num_particles=2,\n                 insomnia=1.,\n                 model_has_params=True,\n                 num_sleep_particles=None,\n                 vectorize_particles=True,\n                 max_plate_nesting=float(\'inf\'),\n                 strict_enumeration_warning=True):\n        # force K > 1 otherwise SNIS not possible\n        assert(num_particles > 1), \\\n            ""Reweighted Wake Sleep needs to be run with more than one particle""\n\n        super().__init__(num_particles=num_particles,\n                         max_plate_nesting=max_plate_nesting,\n                         vectorize_particles=vectorize_particles,\n                         strict_enumeration_warning=strict_enumeration_warning)\n        self.insomnia = insomnia\n        self.model_has_params = model_has_params\n        self.num_sleep_particles = num_particles if num_sleep_particles is None else num_sleep_particles\n\n        assert(insomnia >= 0 and insomnia <= 1), \\\n            ""insomnia should be in [0, 1]""\n\n    def _get_trace(self, model, guide, args, kwargs):\n        """"""\n        Returns a single trace from the guide, and the model that is run against it.\n        """"""\n        model_trace, guide_trace = get_importance_trace(""flat"", self.max_plate_nesting,\n                                                        model, guide, args, kwargs, detach=True)\n        if is_validation_enabled():\n            check_if_enumerated(guide_trace)\n        return model_trace, guide_trace\n\n    def _loss(self, model, guide, args, kwargs):\n        """"""\n        :returns: returns model loss and guide loss\n        :rtype: float, float\n\n        Computes the re-weighted wake-sleep estimators for the model (wake-theta) and the\n          guide (insomnia * wake-phi + (1 - insomnia) * sleep-phi).\n        Performs backward as appropriate on both, over the specified number of particles.\n        """"""\n\n        wake_theta_loss = torch.tensor(100.)\n        if self.model_has_params or self.insomnia > 0.:\n            # compute quantities for wake theta and wake phi\n            log_joints = []\n            log_qs = []\n\n            for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n                log_joint = 0.\n                log_q = 0.\n\n                for _, site in model_trace.nodes.items():\n                    if site[""type""] == ""sample"":\n                        if self.vectorize_particles:\n                            log_p_site = site[""log_prob""].reshape(self.num_particles, -1).sum(-1)\n                        else:\n                            log_p_site = site[""log_prob_sum""]\n                        log_joint = log_joint + log_p_site\n\n                for _, site in guide_trace.nodes.items():\n                    if site[""type""] == ""sample"":\n                        if self.vectorize_particles:\n                            log_q_site = site[""log_prob""].reshape(self.num_particles, -1).sum(-1)\n                        else:\n                            log_q_site = site[""log_prob_sum""]\n                        log_q = log_q + log_q_site\n\n                log_joints.append(log_joint)\n                log_qs.append(log_q)\n\n            log_joints = log_joints[0] if self.vectorize_particles else torch.stack(log_joints)\n            log_qs = log_qs[0] if self.vectorize_particles else torch.stack(log_qs)\n            log_weights = log_joints - log_qs.detach()\n\n            # compute wake theta loss\n            log_sum_weight = torch.logsumexp(log_weights, dim=0)\n            wake_theta_loss = -(log_sum_weight - math.log(self.num_particles)).sum()\n            warn_if_nan(wake_theta_loss, ""wake theta loss"")\n\n        if self.insomnia > 0:\n            # compute wake phi loss\n            normalised_weights = (log_weights - log_sum_weight).exp().detach()\n            wake_phi_loss = -(normalised_weights * log_qs).sum()\n            warn_if_nan(wake_phi_loss, ""wake phi loss"")\n\n        if self.insomnia < 1:\n            # compute sleep phi loss\n            _model = pyro.poutine.uncondition(model)\n            _guide = guide\n            _log_q = 0.\n\n            if self.vectorize_particles:\n                if self.max_plate_nesting == float(\'inf\'):\n                    self._guess_max_plate_nesting(_model, _guide, args, kwargs)\n                _model = self._vectorized_num_sleep_particles(_model)\n                _guide = self._vectorized_num_sleep_particles(guide)\n\n            for _ in range(1 if self.vectorize_particles else self.num_sleep_particles):\n                _model_trace = poutine.trace(_model).get_trace(*args, **kwargs)\n                _model_trace.detach_()\n                _guide_trace = self._get_matched_trace(_model_trace, _guide, args, kwargs)\n                _log_q += _guide_trace.log_prob_sum()\n\n            sleep_phi_loss = -_log_q / self.num_sleep_particles\n            warn_if_nan(sleep_phi_loss, ""sleep phi loss"")\n\n        # compute phi loss\n        phi_loss = sleep_phi_loss if self.insomnia == 0 \\\n            else wake_phi_loss if self.insomnia == 1 \\\n            else self.insomnia * wake_phi_loss + (1. - self.insomnia) * sleep_phi_loss\n\n        return wake_theta_loss, phi_loss\n\n    def loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns model loss and guide loss\n        :rtype: float, float\n\n        Computes the re-weighted wake-sleep estimators for the model (wake-theta) and the\n          guide (insomnia * wake-phi + (1 - insomnia) * sleep-phi).\n        """"""\n        with torch.no_grad():\n            wake_theta_loss, phi_loss = self._loss(model, guide, args, kwargs)\n\n        return wake_theta_loss, phi_loss\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns model loss and guide loss\n        :rtype: float\n\n        Computes the RWS estimators for the model (wake-theta) and the guide (wake-phi).\n        Performs backward as appropriate on both, using num_particle many samples/particles.\n        """"""\n        wake_theta_loss, phi_loss = self._loss(model, guide, args, kwargs)\n        # convenience addition to ensure easier gradients without requiring `retain_graph=True`\n        (wake_theta_loss + phi_loss).backward()\n\n        return wake_theta_loss.detach().item(), phi_loss.detach().item()\n\n    def _vectorized_num_sleep_particles(self, fn):\n        """"""\n        Copy of `_vectorised_num_particles` that uses `num_sleep_particles`.\n        """"""\n        def wrapped_fn(*args, **kwargs):\n            if self.num_sleep_particles == 1:\n                return fn(*args, **kwargs)\n            with pyro.plate(""num_sleep_particles_vectorized"", self.num_sleep_particles, dim=-self.max_plate_nesting):\n                return fn(*args, **kwargs)\n\n        return wrapped_fn\n\n    @staticmethod\n    def _get_matched_trace(model_trace, guide, args, kwargs):\n        kwargs[""observations""] = {}\n        for node in model_trace.stochastic_nodes + model_trace.observation_nodes:\n            if ""was_observed"" in model_trace.nodes[node][""infer""]:\n                model_trace.nodes[node][""is_observed""] = True\n                kwargs[""observations""][node] = model_trace.nodes[node][""value""]\n\n        guide_trace = poutine.trace(poutine.replay(guide, model_trace)).get_trace(*args, **kwargs)\n        check_model_guide_match(model_trace, guide_trace)\n        guide_trace = prune_subsample_sites(guide_trace)\n\n        return guide_trace\n'"
pyro/infer/smcfilter.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport contextlib\nimport math\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer.util import is_validation_enabled\nfrom pyro.poutine.util import prune_subsample_sites\n\n\nclass SMCFailed(ValueError):\n    """"""\n    Exception raised when :class:`SMCFilter` fails to find any hypothesis with\n    nonzero probability.\n    """"""\n    pass\n\n\nclass SMCFilter:\n    """"""\n    :class:`SMCFilter` is the top-level interface for filtering via sequential\n    monte carlo.\n\n    The model and guide should be objects with two methods:\n    ``.init(state, ...)`` and ``.step(state, ...)``, intended to be called\n    first with :meth:`init` , then with :meth:`step` repeatedly.  These two\n    methods should have the same signature as :class:`SMCFilter` \'s\n    :meth:`init` and :meth:`step` of this class, but with an extra first\n    argument ``state`` that should be used to store all tensors that depend on\n    sampled variables. The ``state`` will be a dict-like object,\n    :class:`SMCState` , with arbitrary keys and :class:`torch.Tensor` values.\n    Models can read and write ``state`` but guides can only read from it.\n\n    Inference complexity is ``O(len(state) * num_time_steps)``, so to avoid\n    quadratic complexity in Markov models, ensure that ``state`` has fixed size.\n\n    :param object model: probabilistic model with ``init`` and ``step`` methods\n    :param object guide: guide used for sampling,  with ``init`` and ``step``\n        methods\n    :param int num_particles: The number of particles used to form the\n        distribution.\n    :param int max_plate_nesting: Bound on max number of nested\n        :func:`pyro.plate` contexts.\n    :param float ess_threshold: Effective sample size threshold for deciding\n        when to importance resample: resampling occurs when\n        ``ess < ess_threshold * num_particles``.\n    """"""\n    # TODO: Add window kwarg that defaults to float(""inf"")\n    def __init__(self, model, guide, num_particles, max_plate_nesting, *,\n                 ess_threshold=0.5):\n        assert 0 < ess_threshold <= 1\n        self.model = model\n        self.guide = guide\n        self.num_particles = num_particles\n        self.max_plate_nesting = max_plate_nesting\n        self.ess_threshold = ess_threshold\n\n        # Equivalent to an empirical distribution, but allows a\n        # user-defined dynamic collection of tensors.\n        self.state = SMCState(self.num_particles)\n\n    def init(self, *args, **kwargs):\n        """"""\n        Perform any initialization for sequential importance resampling.\n        Any args or kwargs are passed to the model and guide\n        """"""\n        self.particle_plate = pyro.plate(""particles"", self.num_particles, dim=-1-self.max_plate_nesting)\n        with poutine.block(), self.particle_plate:\n            with self.state._lock():\n                guide_trace = poutine.trace(self.guide.init).get_trace(self.state, *args, **kwargs)\n            model = poutine.replay(self.model.init, guide_trace)\n            model_trace = poutine.trace(model).get_trace(self.state, *args, **kwargs)\n\n        self._update_weights(model_trace, guide_trace)\n        self._maybe_importance_resample()\n\n    def step(self, *args, **kwargs):\n        """"""\n        Take a filtering step using sequential importance resampling updating the\n        particle weights and values while resampling if desired.\n        Any args or kwargs are passed to the model and guide\n        """"""\n        with poutine.block(), self.particle_plate:\n            with self.state._lock():\n                guide_trace = poutine.trace(self.guide.step).get_trace(self.state, *args, **kwargs)\n            model = poutine.replay(self.model.step, guide_trace)\n            model_trace = poutine.trace(model).get_trace(self.state, *args, **kwargs)\n\n        self._update_weights(model_trace, guide_trace)\n        self._maybe_importance_resample()\n\n    def get_empirical(self):\n        """"""\n        :returns: a marginal distribution over all state tensors.\n        :rtype: a dictionary with keys which are latent variables and values\n            which are :class:`~pyro.distributions.Empirical` objects.\n        """"""\n        return {key: dist.Empirical(value, self.state._log_weights)\n                for key, value in self.state.items()}\n\n    @torch.no_grad()\n    def _update_weights(self, model_trace, guide_trace):\n        # w_t <-w_{t-1}*p(y_t|z_t) * p(z_t|z_t-1)/q(z_t)\n\n        model_trace = prune_subsample_sites(model_trace)\n        guide_trace = prune_subsample_sites(guide_trace)\n\n        model_trace.compute_log_prob()\n        guide_trace.compute_log_prob()\n\n        for name, guide_site in guide_trace.nodes.items():\n            if guide_site[""type""] == ""sample"":\n                model_site = model_trace.nodes[name]\n                log_p = model_site[""log_prob""].reshape(self.num_particles, -1).sum(-1)\n                log_q = guide_site[""log_prob""].reshape(self.num_particles, -1).sum(-1)\n                self.state._log_weights += log_p - log_q\n                if not (self.state._log_weights.max() > -math.inf):\n                    raise SMCFailed(""Failed to find feasible hypothesis after site {}""\n                                    .format(name))\n\n        for site in model_trace.nodes.values():\n            if site[""type""] == ""sample"" and site[""is_observed""]:\n                log_p = site[""log_prob""].reshape(self.num_particles, -1).sum(-1)\n                self.state._log_weights += log_p\n                if not (self.state._log_weights.max() > -math.inf):\n                    raise SMCFailed(""Failed to find feasible hypothesis after site {}""\n                                    .format(site[""name""]))\n\n        self.state._log_weights -= self.state._log_weights.max()\n\n    def _maybe_importance_resample(self):\n        if not self.state:\n            return\n        # Decide whether to resample based on ESS.\n        logp = self.state._log_weights\n        logp -= logp.logsumexp(-1)\n        probs = logp.exp()\n        ess = probs.dot(probs).reciprocal()\n        if ess < self.ess_threshold * self.num_particles:\n            self._importance_resample(probs)\n\n    def _importance_resample(self, probs):\n        index = _systematic_sample(probs)\n        self.state._resample(index)\n\n\ndef _systematic_sample(probs):\n    # Systematic sampling preserves diversity better than multinomial sampling\n    # via Categorical(probs).sample().\n    batch_shape, size = probs.shape[:-1], probs.size(-1)\n    n = probs.cumsum(-1).mul_(size).add_(torch.rand(batch_shape + (1,)))\n    n = n.floor_().clamp_(min=0, max=size).long()\n    diff = probs.new_zeros(batch_shape + (size + 1,))\n    diff.scatter_add_(-1, n, torch.ones_like(probs))\n    index = diff[..., :-1].cumsum(-1).long()\n    return index\n\n\nclass SMCState(dict):\n    """"""\n    Dictionary-like object to hold a vectorized collection of tensors to\n    represent all state during inference with :class:`SMCFilter`. During\n    inference, the :class:`SMCFilter` resample these tensors.\n\n    Keys may have arbitrary hashable type.\n    Values must be :class:`torch.Tensor` s.\n\n    :param int num_particles:\n    """"""\n    def __init__(self, num_particles):\n        assert isinstance(num_particles, int) and num_particles > 0\n        super().__init__()\n        self._num_particles = num_particles\n        self._log_weights = torch.zeros(num_particles)\n        self._locked = False\n\n    @contextlib.contextmanager\n    def _lock(self):\n        self._locked = True\n        try:\n            yield\n        finally:\n            self._locked = False\n\n    def __setitem__(self, key, value):\n        if self._locked:\n            raise RuntimeError(""Guide cannot write to SMCState"")\n        if is_validation_enabled():\n            if not isinstance(value, torch.Tensor):\n                raise TypeError(""Only Tensors can be stored in an SMCState, but got {}""\n                                .format(type(value).__name__))\n            if value.dim() == 0 or value.size(0) != self._num_particles:\n                raise ValueError(""Expected leading dim of size {} but got shape {}""\n                                 .format(self._num_particles, value.shape))\n        super().__setitem__(key, value)\n\n    def _resample(self, index):\n        for key, value in self.items():\n            self[key] = value[index].contiguous()\n        self._log_weights.fill_(0.)\n'"
pyro/infer/svgd.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABCMeta, abstractmethod\nimport math\n\nimport torch\nfrom torch.distributions import biject_to\n\nimport pyro\nfrom pyro import poutine\nfrom pyro.distributions import Delta\nfrom pyro.infer.trace_elbo import Trace_ELBO\nfrom pyro.infer.autoguide.guides import AutoContinuous\nfrom pyro.infer.autoguide.initialization import init_to_sample\nfrom pyro.distributions.util import copy_docs_from\n\n\ndef vectorize(fn, num_particles, max_plate_nesting):\n    def _fn(*args, **kwargs):\n        with pyro.plate(""num_particles_vectorized"", num_particles, dim=-max_plate_nesting - 1):\n            return fn(*args, **kwargs)\n    return _fn\n\n\nclass _SVGDGuide(AutoContinuous):\n    """"""\n    This modification of :class:`AutoContinuous` is used internally in the\n    :class:`SVGD` inference algorithm.\n    """"""\n    def __init__(self, model):\n        super().__init__(model, init_loc_fn=init_to_sample)\n\n    def get_posterior(self, *args, **kwargs):\n        svgd_particles = pyro.param(""svgd_particles"", self._init_loc)\n        return Delta(svgd_particles, event_dim=1)\n\n\nclass SteinKernel(object, metaclass=ABCMeta):\n    """"""\n    Abstract class for kernels used in the :class:`SVGD` inference algorithm.\n    """"""\n\n    @abstractmethod\n    def log_kernel_and_grad(self, particles):\n        """"""\n        Compute the component kernels and their gradients.\n\n        :param particles: a tensor with shape (N, D)\n        :returns: A pair (`log_kernel`, `kernel_grad`) where `log_kernel` is a (N, N, D)-shaped\n            tensor equal to the logarithm of the kernel and `kernel_grad` is a (N, N, D)-shaped\n            tensor where the entry (n, m, d) represents the derivative of `log_kernel` w.r.t.\n            x_{m,d}, where x_{m,d} is the d^th dimension of particle m.\n        """"""\n        raise NotImplementedError\n\n\n@copy_docs_from(SteinKernel)\nclass RBFSteinKernel(SteinKernel):\n    """"""\n    A RBF kernel for use in the SVGD inference algorithm. The bandwidth of the kernel is chosen from the\n    particles using a simple heuristic as in reference [1].\n\n    :param float bandwidth_factor: Optional factor by which to scale the bandwidth, defaults to 1.0.\n    :ivar float ~.bandwidth_factor: Property that controls the factor by which to scale the bandwidth\n        at each iteration.\n\n    References\n\n    [1] ""Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm,""\n        Qiang Liu, Dilin Wang\n    """"""\n    def __init__(self, bandwidth_factor=None):\n        """"""\n        :param float bandwidth_factor: Optional factor by which to scale the bandwidth\n        """"""\n        self.bandwidth_factor = bandwidth_factor\n\n    def _bandwidth(self, norm_sq):\n        """"""\n        Compute the bandwidth along each dimension using the median pairwise squared distance between particles.\n        """"""\n        num_particles = norm_sq.size(0)\n        index = torch.arange(num_particles)\n        norm_sq = norm_sq[index > index.unsqueeze(-1), ...]\n        median = norm_sq.median(dim=0)[0]\n        if self.bandwidth_factor is not None:\n            median = self.bandwidth_factor * median\n        assert median.shape == norm_sq.shape[-1:]\n        return median / math.log(num_particles + 1)\n\n    @torch.no_grad()\n    def log_kernel_and_grad(self, particles):\n        delta_x = particles.unsqueeze(0) - particles.unsqueeze(1)  # N N D\n        assert delta_x.dim() == 3\n        norm_sq = delta_x.pow(2.0)  # N N D\n        h = self._bandwidth(norm_sq)  # D\n        log_kernel = -(norm_sq / h)  # N N D\n        grad_term = 2.0 * delta_x / h  # N N D\n        assert log_kernel.shape == grad_term.shape\n        return log_kernel, grad_term\n\n    @property\n    def bandwidth_factor(self):\n        return self._bandwidth_factor\n\n    @bandwidth_factor.setter\n    def bandwidth_factor(self, bandwidth_factor):\n        """"""\n        :param float bandwidth_factor: Optional factor by which to scale the bandwidth\n        """"""\n        if bandwidth_factor is not None:\n            assert bandwidth_factor > 0.0, ""bandwidth_factor must be positive.""\n        self._bandwidth_factor = bandwidth_factor\n\n\n@copy_docs_from(SteinKernel)\nclass IMQSteinKernel(SteinKernel):\n    r""""""\n    An IMQ (inverse multi-quadratic) kernel for use in the SVGD inference algorithm [1]. The bandwidth of the kernel\n    is chosen from the particles using a simple heuristic as in reference [2]. The kernel takes the form\n\n    :math:`K(x, y) = (\\alpha + ||x-y||^2/h)^{\\beta}`\n\n    where :math:`\\alpha` and :math:`\\beta` are user-specified parameters and :math:`h` is the bandwidth.\n\n    :param float alpha: Kernel hyperparameter, defaults to 0.5.\n    :param float beta: Kernel hyperparameter, defaults to -0.5.\n    :param float bandwidth_factor: Optional factor by which to scale the bandwidth, defaults to 1.0.\n    :ivar float ~.bandwidth_factor: Property that controls the factor by which to scale the bandwidth\n        at each iteration.\n\n    References\n\n    [1] ""Stein Points,"" Wilson Ye Chen, Lester Mackey, Jackson Gorham, Francois-Xavier Briol, Chris. J. Oates.\n    [2] ""Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm,"" Qiang Liu, Dilin Wang\n    """"""\n    def __init__(self, alpha=0.5, beta=-0.5, bandwidth_factor=None):\n        """"""\n        :param float alpha: Kernel hyperparameter, defaults to 0.5.\n        :param float beta: Kernel hyperparameter, defaults to -0.5.\n        :param float bandwidth_factor: Optional factor by which to scale the bandwidth\n        """"""\n        assert alpha > 0.0, ""alpha must be positive.""\n        assert beta < 0.0, ""beta must be negative.""\n        self.alpha = alpha\n        self.beta = beta\n        self.bandwidth_factor = bandwidth_factor\n\n    def _bandwidth(self, norm_sq):\n        """"""\n        Compute the bandwidth along each dimension using the median pairwise squared distance between particles.\n        """"""\n        num_particles = norm_sq.size(0)\n        index = torch.arange(num_particles)\n        norm_sq = norm_sq[index > index.unsqueeze(-1), ...]\n        median = norm_sq.median(dim=0)[0]\n        if self.bandwidth_factor is not None:\n            median = self.bandwidth_factor * median\n        assert median.shape == norm_sq.shape[-1:]\n        return median / math.log(num_particles + 1)\n\n    @torch.no_grad()\n    def log_kernel_and_grad(self, particles):\n        delta_x = particles.unsqueeze(0) - particles.unsqueeze(1)  # N N D\n        assert delta_x.dim() == 3\n        norm_sq = delta_x.pow(2.0)  # N N D\n        h = self._bandwidth(norm_sq)  # D\n        base_term = self.alpha + norm_sq / h\n        log_kernel = self.beta * torch.log(base_term)  # N N D\n        grad_term = (-2.0 * self.beta) * delta_x / h  # N N D\n        grad_term = grad_term / base_term\n        assert log_kernel.shape == grad_term.shape\n        return log_kernel, grad_term\n\n    @property\n    def bandwidth_factor(self):\n        return self._bandwidth_factor\n\n    @bandwidth_factor.setter\n    def bandwidth_factor(self, bandwidth_factor):\n        """"""\n        :param float bandwidth_factor: Optional factor by which to scale the bandwidth\n        """"""\n        if bandwidth_factor is not None:\n            assert bandwidth_factor > 0.0, ""bandwidth_factor must be positive.""\n        self._bandwidth_factor = bandwidth_factor\n\n\nclass SVGD:\n    """"""\n    A basic implementation of Stein Variational Gradient Descent as described in reference [1].\n\n    :param model: The model (callable containing Pyro primitives). Model must be fully vectorized\n        and may only contain continuous latent variables.\n    :param kernel: a SVGD compatible kernel like :class:`RBFSteinKernel`.\n    :param optim: A wrapper for a PyTorch optimizer.\n    :type optim: pyro.optim.PyroOptim\n    :param int num_particles: The number of particles used in SVGD.\n    :param int max_plate_nesting: The max number of nested :func:`pyro.plate` contexts in the model.\n    :param str mode: Whether to use a Kernelized Stein Discrepancy that makes use of `multivariate`\n        test functions (as in [1]) or `univariate` test functions (as in [2]). Defaults to `univariate`.\n\n    Example usage:\n\n    .. code-block:: python\n\n        from pyro.infer import SVGD, RBFSteinKernel\n        from pyro.optim import Adam\n\n        kernel = RBFSteinKernel()\n        adam = Adam({""lr"": 0.1})\n        svgd = SVGD(model, kernel, adam, num_particles=50, max_plate_nesting=0)\n\n        for step in range(500):\n            svgd.step(model_arg1, model_arg2)\n\n        final_particles = svgd.get_named_particles()\n\n    References\n\n    [1] ""Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm,""\n        Qiang Liu, Dilin Wang\n    [2] ""Kernelized Complete Conditional Stein Discrepancy,""\n        Raghav Singhal, Saad Lahlou, Rajesh Ranganath\n    """"""\n    def __init__(self, model, kernel, optim, num_particles, max_plate_nesting, mode=""univariate""):\n        assert callable(model)\n        assert isinstance(kernel, SteinKernel), ""Must provide a valid SteinKernel""\n        assert isinstance(optim, pyro.optim.PyroOptim), ""Must provide a valid Pyro optimizer""\n        assert num_particles > 1, ""Must use at least two particles""\n        assert max_plate_nesting >= 0\n        assert mode in [\'univariate\', \'multivariate\'], ""mode must be one of (univariate, multivariate)""\n\n        self.model = vectorize(model, num_particles, max_plate_nesting)\n        self.kernel = kernel\n        self.optim = optim\n        self.num_particles = num_particles\n        self.max_plate_nesting = max_plate_nesting\n        self.mode = mode\n\n        self.loss = Trace_ELBO().differentiable_loss\n        self.guide = _SVGDGuide(self.model)\n\n    def get_named_particles(self):\n        """"""\n        Create a dictionary mapping name to vectorized value, of the form ``{name: tensor}``.\n        The leading dimension of each tensor corresponds to particles, i.e. this creates a struct of arrays.\n        """"""\n        return {site[""name""]: biject_to(site[""fn""].support)(unconstrained_value)\n                for site, unconstrained_value in self.guide._unpack_latent(pyro.param(""svgd_particles""))}\n\n    @torch.no_grad()\n    def step(self, *args, **kwargs):\n        """"""\n        Computes the SVGD gradient, passing args and kwargs to the model,\n        and takes a gradient step.\n\n        :return dict: A dictionary of the form {name: float}, where each float\n            is a mean squared gradient. This can be used to monitor the convergence of SVGD.\n        """"""\n        # compute gradients of log model joint\n        with torch.enable_grad(), poutine.trace(param_only=True) as param_capture:\n            loss = self.loss(self.model, self.guide, *args, **kwargs)\n            loss.backward()\n\n        # get particles used in the _SVGDGuide and reshape to have num_particles leading dimension\n        particles = pyro.param(""svgd_particles"").unconstrained()\n        reshaped_particles = particles.reshape(self.num_particles, -1)\n        reshaped_particles_grad = particles.grad.reshape(self.num_particles, -1)\n\n        # compute kernel ingredients\n        log_kernel, kernel_grad = self.kernel.log_kernel_and_grad(reshaped_particles)\n\n        if self.mode == ""multivariate"":\n            kernel = log_kernel.sum(-1).exp()\n            assert kernel.shape == (self.num_particles, self.num_particles)\n            attractive_grad = torch.mm(kernel, reshaped_particles_grad)\n            repulsive_grad = torch.einsum(""nm,nm...->n..."", kernel, kernel_grad)\n        elif self.mode == ""univariate"":\n            kernel = log_kernel.exp()\n            assert kernel.shape == (self.num_particles, self.num_particles, reshaped_particles.size(-1))\n            attractive_grad = torch.einsum(""nmd,md->nd"", kernel, reshaped_particles_grad)\n            repulsive_grad = torch.einsum(""nmd,nmd->nd"", kernel, kernel_grad)\n\n        # combine the attractive and repulsive terms in the SVGD gradient\n        assert attractive_grad.shape == repulsive_grad.shape\n        particles.grad = (attractive_grad + repulsive_grad).reshape(particles.shape) / self.num_particles\n\n        # compute per-parameter mean squared gradients\n        squared_gradients = {site[""name""]: value.mean().item()\n                             for site, value in self.guide._unpack_latent(particles.grad.pow(2.0))}\n\n        # torch.optim objects gets instantiated for any params that haven\'t been seen yet\n        params = set(site[""value""].unconstrained() for site in param_capture.trace.nodes.values())\n        self.optim(params)\n\n        # zero gradients\n        pyro.infer.util.zero_grads(params)\n\n        # return per-parameter mean squared gradients to user\n        return squared_gradients\n'"
pyro/infer/svi.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nimport torch\n\nimport pyro\nimport pyro.optim\nimport pyro.poutine as poutine\nfrom pyro.infer.abstract_infer import TracePosterior\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.util import torch_item\n\n\nclass SVI(TracePosterior):\n    """"""\n    :param model: the model (callable containing Pyro primitives)\n    :param guide: the guide (callable containing Pyro primitives)\n    :param optim: a wrapper a for a PyTorch optimizer\n    :type optim: ~pyro.optim.optim.PyroOptim\n    :param loss: an instance of a subclass of :class:`~pyro.infer.elbo.ELBO`.\n        Pyro provides three built-in losses:\n        :class:`~pyro.infer.trace_elbo.Trace_ELBO`,\n        :class:`~pyro.infer.tracegraph_elbo.TraceGraph_ELBO`, and\n        :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO`.\n        See the :class:`~pyro.infer.elbo.ELBO` docs to learn how to implement\n        a custom loss.\n    :type loss: pyro.infer.elbo.ELBO\n    :param num_samples: (DEPRECATED) the number of samples for Monte Carlo posterior approximation\n    :param num_steps: (DEPRECATED) the number of optimization steps to take in ``run()``\n\n    A unified interface for stochastic variational inference in Pyro. The most\n    commonly used loss is ``loss=Trace_ELBO()``. See the tutorial\n    `SVI Part I <http://pyro.ai/examples/svi_part_i.html>`_ for a discussion.\n    """"""\n    def __init__(self,\n                 model,\n                 guide,\n                 optim,\n                 loss,\n                 loss_and_grads=None,\n                 num_samples=0,\n                 num_steps=0,\n                 **kwargs):\n        if num_steps:\n            warnings.warn(\'The `num_steps` argument to SVI is deprecated and will be removed in \'\n                          \'a future release. Use `SVI.step` directly to control the \'\n                          \'number of iterations.\', FutureWarning)\n        if num_samples:\n            warnings.warn(\'The `num_samples` argument to SVI is deprecated and will be removed in \'\n                          \'a future release. Use `pyro.infer.Predictive` class to draw \'\n                          \'samples from the posterior.\', FutureWarning)\n\n        self.model = model\n        self.guide = guide\n        self.optim = optim\n        self.num_steps = num_steps\n        self.num_samples = num_samples\n        super().__init__(**kwargs)\n\n        if not isinstance(optim, pyro.optim.PyroOptim):\n            raise ValueError(""Optimizer should be an instance of pyro.optim.PyroOptim class."")\n\n        if isinstance(loss, ELBO):\n            self.loss = loss.loss\n            self.loss_and_grads = loss.loss_and_grads\n        else:\n            if loss_and_grads is None:\n                def _loss_and_grads(*args, **kwargs):\n                    loss_val = loss(*args, **kwargs)\n                    if getattr(loss_val, \'requires_grad\', False):\n                        loss_val.backward(retain_graph=True)\n                    return loss_val\n                loss_and_grads = _loss_and_grads\n            self.loss = loss\n            self.loss_and_grads = loss_and_grads\n\n    def run(self, *args, **kwargs):\n        """"""\n        .. warning::\n            This method is deprecated, and will be removed in a future release.\n            For inference, use :meth:`step` directly, and for predictions,\n            use the :class:`~pyro.infer.predictive.Predictive` class.\n        """"""\n        warnings.warn(\'The `SVI.run` method is deprecated and will be removed in a \'\n                      \'future release. For inference, use `SVI.step` directly, \'\n                      \'and for predictions, use the `pyro.infer.Predictive` class.\',\n                      FutureWarning)\n        if self.num_steps > 0:\n            with poutine.block():\n                for i in range(self.num_steps):\n                    self.step(*args, **kwargs)\n        return super().run(*args, **kwargs)\n\n    def _traces(self, *args, **kwargs):\n        for i in range(self.num_samples):\n            guide_trace = poutine.trace(self.guide).get_trace(*args, **kwargs)\n            model_trace = poutine.trace(poutine.replay(self.model, trace=guide_trace)).get_trace(*args, **kwargs)\n            yield model_trace, 1.0\n\n    def evaluate_loss(self, *args, **kwargs):\n        """"""\n        :returns: estimate of the loss\n        :rtype: float\n\n        Evaluate the loss function. Any args or kwargs are passed to the model and guide.\n        """"""\n        with torch.no_grad():\n            loss = self.loss(self.model, self.guide, *args, **kwargs)\n            if isinstance(loss, tuple):\n                # Support losses that return a tuple, e.g. ReweightedWakeSleep.\n                return type(loss)(map(torch_item, loss))\n            else:\n                return torch_item(loss)\n\n    def step(self, *args, **kwargs):\n        """"""\n        :returns: estimate of the loss\n        :rtype: float\n\n        Take a gradient step on the loss function (and any auxiliary loss functions\n        generated under the hood by `loss_and_grads`).\n        Any args or kwargs are passed to the model and guide\n        """"""\n        # get loss and compute gradients\n        with poutine.trace(param_only=True) as param_capture:\n            loss = self.loss_and_grads(self.model, self.guide, *args, **kwargs)\n\n        params = set(site[""value""].unconstrained()\n                     for site in param_capture.trace.nodes.values())\n\n        # actually perform gradient steps\n        # torch.optim objects gets instantiated for any params that haven\'t been seen yet\n        self.optim(params)\n\n        # zero gradients\n        pyro.infer.util.zero_grads(params)\n\n        if isinstance(loss, tuple):\n            # Support losses that return a tuple, e.g. ReweightedWakeSleep.\n            return type(loss)(map(torch_item, loss))\n        else:\n            return torch_item(loss)\n'"
pyro/infer/trace_elbo.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport weakref\n\nimport pyro\nimport pyro.ops.jit\nfrom pyro.distributions.util import is_identically_zero\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.enum import get_importance_trace\nfrom pyro.infer.util import MultiFrameTensor, get_plate_stacks, is_validation_enabled, torch_item\nfrom pyro.util import check_if_enumerated, warn_if_nan\n\n\ndef _compute_log_r(model_trace, guide_trace):\n    log_r = MultiFrameTensor()\n    stacks = get_plate_stacks(model_trace)\n    for name, model_site in model_trace.nodes.items():\n        if model_site[""type""] == ""sample"":\n            log_r_term = model_site[""log_prob""]\n            if not model_site[""is_observed""]:\n                log_r_term = log_r_term - guide_trace.nodes[name][""log_prob""]\n            log_r.add((stacks[name], log_r_term.detach()))\n    return log_r\n\n\nclass Trace_ELBO(ELBO):\n    """"""\n    A trace implementation of ELBO-based SVI. The estimator is constructed\n    along the lines of references [1] and [2]. There are no restrictions on the\n    dependency structure of the model or the guide. The gradient estimator includes\n    partial Rao-Blackwellization for reducing the variance of the estimator when\n    non-reparameterizable random variables are present. The Rao-Blackwellization is\n    partial in that it only uses conditional independence information that is marked\n    by :class:`~pyro.plate` contexts. For more fine-grained Rao-Blackwellization,\n    see :class:`~pyro.infer.tracegraph_elbo.TraceGraph_ELBO`.\n\n    References\n\n    [1] Automated Variational Inference in Probabilistic Programming,\n        David Wingate, Theo Weber\n\n    [2] Black Box Variational Inference,\n        Rajesh Ranganath, Sean Gerrish, David M. Blei\n    """"""\n\n    def _get_trace(self, model, guide, args, kwargs):\n        """"""\n        Returns a single trace from the guide, and the model that is run\n        against it.\n        """"""\n        model_trace, guide_trace = get_importance_trace(\n            ""flat"", self.max_plate_nesting, model, guide, args, kwargs)\n        if is_validation_enabled():\n            check_if_enumerated(guide_trace)\n        return model_trace, guide_trace\n\n    def loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Evaluates the ELBO with an estimator that uses num_particles many samples/particles.\n        """"""\n        elbo = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            elbo_particle = torch_item(model_trace.log_prob_sum()) - torch_item(guide_trace.log_prob_sum())\n            elbo += elbo_particle / self.num_particles\n\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def _differentiable_loss_particle(self, model_trace, guide_trace):\n        elbo_particle = 0\n        surrogate_elbo_particle = 0\n        log_r = None\n\n        # compute elbo and surrogate elbo\n        for name, site in model_trace.nodes.items():\n            if site[""type""] == ""sample"":\n                elbo_particle = elbo_particle + torch_item(site[""log_prob_sum""])\n                surrogate_elbo_particle = surrogate_elbo_particle + site[""log_prob_sum""]\n\n        for name, site in guide_trace.nodes.items():\n            if site[""type""] == ""sample"":\n                log_prob, score_function_term, entropy_term = site[""score_parts""]\n\n                elbo_particle = elbo_particle - torch_item(site[""log_prob_sum""])\n\n                if not is_identically_zero(entropy_term):\n                    surrogate_elbo_particle = surrogate_elbo_particle - entropy_term.sum()\n\n                if not is_identically_zero(score_function_term):\n                    if log_r is None:\n                        log_r = _compute_log_r(model_trace, guide_trace)\n                    site = log_r.sum_to(site[""cond_indep_stack""])\n                    surrogate_elbo_particle = surrogate_elbo_particle + (site * score_function_term).sum()\n\n        return -elbo_particle, -surrogate_elbo_particle\n\n    def differentiable_loss(self, model, guide, *args, **kwargs):\n        """"""\n        Computes the surrogate loss that can be differentiated with autograd\n        to produce gradient estimates for the model and guide parameters\n        """"""\n        loss = 0.\n        surrogate_loss = 0.\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            loss_particle, surrogate_loss_particle = self._differentiable_loss_particle(model_trace, guide_trace)\n            surrogate_loss += surrogate_loss_particle / self.num_particles\n            loss += loss_particle / self.num_particles\n        warn_if_nan(surrogate_loss, ""loss"")\n        return loss + (surrogate_loss - torch_item(surrogate_loss))\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator.\n        Performs backward on the latter. Num_particle many samples are used to form the estimators.\n        """"""\n        loss = 0.0\n        # grab a trace from the generator\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            loss_particle, surrogate_loss_particle = self._differentiable_loss_particle(model_trace, guide_trace)\n            loss += loss_particle / self.num_particles\n\n            # collect parameters to train from model and guide\n            trainable_params = any(site[""type""] == ""param""\n                                   for trace in (model_trace, guide_trace)\n                                   for site in trace.nodes.values())\n\n            if trainable_params and getattr(surrogate_loss_particle, \'requires_grad\', False):\n                surrogate_loss_particle = surrogate_loss_particle / self.num_particles\n                surrogate_loss_particle.backward(retain_graph=self.retain_graph)\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n\nclass JitTrace_ELBO(Trace_ELBO):\n    """"""\n    Like :class:`Trace_ELBO` but uses :func:`pyro.ops.jit.compile` to compile\n    :meth:`loss_and_grads`.\n\n    This works only for a limited set of models:\n\n    -   Models must have static structure.\n    -   Models must not depend on any global data (except the param store).\n    -   All model inputs that are tensors must be passed in via ``*args``.\n    -   All model inputs that are *not* tensors must be passed in via\n        ``**kwargs``, and compilation will be triggered once per unique\n        ``**kwargs``.\n    """"""\n    def loss_and_surrogate_loss(self, model, guide, *args, **kwargs):\n        kwargs[\'_pyro_model_id\'] = id(model)\n        kwargs[\'_pyro_guide_id\'] = id(guide)\n        if getattr(self, \'_loss_and_surrogate_loss\', None) is None:\n            # build a closure for loss_and_surrogate_loss\n            weakself = weakref.ref(self)\n\n            @pyro.ops.jit.trace(ignore_warnings=self.ignore_jit_warnings,\n                                jit_options=self.jit_options)\n            def loss_and_surrogate_loss(*args, **kwargs):\n                kwargs.pop(\'_pyro_model_id\')\n                kwargs.pop(\'_pyro_guide_id\')\n                self = weakself()\n                loss = 0.0\n                surrogate_loss = 0.0\n                for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n                    elbo_particle = 0\n                    surrogate_elbo_particle = 0\n                    log_r = None\n\n                    # compute elbo and surrogate elbo\n                    for name, site in model_trace.nodes.items():\n                        if site[""type""] == ""sample"":\n                            elbo_particle = elbo_particle + site[""log_prob_sum""]\n                            surrogate_elbo_particle = surrogate_elbo_particle + site[""log_prob_sum""]\n\n                    for name, site in guide_trace.nodes.items():\n                        if site[""type""] == ""sample"":\n                            log_prob, score_function_term, entropy_term = site[""score_parts""]\n\n                            elbo_particle = elbo_particle - site[""log_prob_sum""]\n\n                            if not is_identically_zero(entropy_term):\n                                surrogate_elbo_particle = surrogate_elbo_particle - entropy_term.sum()\n\n                            if not is_identically_zero(score_function_term):\n                                if log_r is None:\n                                    log_r = _compute_log_r(model_trace, guide_trace)\n                                site = log_r.sum_to(site[""cond_indep_stack""])\n                                surrogate_elbo_particle = surrogate_elbo_particle + (site * score_function_term).sum()\n\n                    loss = loss - elbo_particle / self.num_particles\n                    surrogate_loss = surrogate_loss - surrogate_elbo_particle / self.num_particles\n\n                return loss, surrogate_loss\n\n            self._loss_and_surrogate_loss = loss_and_surrogate_loss\n\n        return self._loss_and_surrogate_loss(*args, **kwargs)\n\n    def differentiable_loss(self, model, guide, *args, **kwargs):\n        loss, surrogate_loss = self.loss_and_surrogate_loss(model, guide, *args, **kwargs)\n\n        warn_if_nan(loss, ""loss"")\n        return loss + (surrogate_loss - surrogate_loss.detach())\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        loss, surrogate_loss = self.loss_and_surrogate_loss(model, guide, *args, **kwargs)\n        surrogate_loss.backward()\n        loss = loss.item()\n\n        warn_if_nan(loss, ""loss"")\n        return loss\n'"
pyro/infer/trace_mean_field_elbo.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\nimport weakref\n\nimport torch\nfrom torch.distributions import kl_divergence\n\nimport pyro.ops.jit\nfrom pyro.distributions.util import scale_and_mask\nfrom pyro.infer.trace_elbo import Trace_ELBO\nfrom pyro.infer.util import is_validation_enabled, torch_item, check_fully_reparametrized\nfrom pyro.util import warn_if_nan\n\n\ndef _check_mean_field_requirement(model_trace, guide_trace):\n    """"""\n    Checks that the guide and model sample sites are ordered identically.\n    This is sufficient but not necessary for correctness.\n    """"""\n    model_sites = [name for name, site in model_trace.nodes.items()\n                   if site[""type""] == ""sample"" and name in guide_trace.nodes]\n    guide_sites = [name for name, site in guide_trace.nodes.items()\n                   if site[""type""] == ""sample"" and name in model_trace.nodes]\n    assert set(model_sites) == set(guide_sites)\n    if model_sites != guide_sites:\n        warnings.warn(""Failed to verify mean field restriction on the guide. ""\n                      ""To eliminate this warning, ensure model and guide sites ""\n                      ""occur in the same order.\\n"" +\n                      ""Model sites:\\n  "" + ""\\n  "".join(model_sites) +\n                      ""Guide sites:\\n  "" + ""\\n  "".join(guide_sites))\n\n\nclass TraceMeanField_ELBO(Trace_ELBO):\n    """"""\n    A trace implementation of ELBO-based SVI. This is currently the only\n    ELBO estimator in Pyro that uses analytic KL divergences when those\n    are available.\n\n    In contrast to, e.g.,\n    :class:`~pyro.infer.tracegraph_elbo.TraceGraph_ELBO` and\n    :class:`~pyro.infer.tracegraph_elbo.Trace_ELBO` this estimator places\n    restrictions on the dependency structure of the model and guide.\n    In particular it assumes that the guide has a mean-field structure,\n    i.e. that it factorizes across the different latent variables present\n    in the guide. It also assumes that all of the latent variables in the\n    guide are reparameterized. This latter condition is satisfied for, e.g.,\n    the Normal distribution but is not satisfied for, e.g., the Categorical\n    distribution.\n\n    .. warning:: This estimator may give incorrect results if the mean-field\n      condition is not satisfied.\n\n    Note for advanced users:\n\n    The mean field condition is a sufficient but not necessary condition for\n    this estimator to be correct. The precise condition is that for every\n    latent variable `z` in the guide, its parents in the model must not include\n    any latent variables that are descendants of `z` in the guide. Here\n    \'parents in the model\' and \'descendants in the guide\' is with respect\n    to the corresponding (statistical) dependency structure. For example, this\n    condition is always satisfied if the model and guide have identical\n    dependency structures.\n    """"""\n    def _get_trace(self, model, guide, args, kwargs):\n        model_trace, guide_trace = super()._get_trace(\n            model, guide, args, kwargs)\n        if is_validation_enabled():\n            _check_mean_field_requirement(model_trace, guide_trace)\n        return model_trace, guide_trace\n\n    def loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Evaluates the ELBO with an estimator that uses num_particles many samples/particles.\n        """"""\n        loss = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            loss_particle, _ = self._differentiable_loss_particle(model_trace, guide_trace)\n            loss = loss + loss_particle / self.num_particles\n\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def _differentiable_loss_particle(self, model_trace, guide_trace):\n        elbo_particle = 0\n\n        for name, model_site in model_trace.nodes.items():\n            if model_site[""type""] == ""sample"":\n                if model_site[""is_observed""]:\n                    elbo_particle = elbo_particle + model_site[""log_prob_sum""]\n                else:\n                    guide_site = guide_trace.nodes[name]\n                    if is_validation_enabled():\n                        check_fully_reparametrized(guide_site)\n\n                    # use kl divergence if available, else fall back on sampling\n                    try:\n                        kl_qp = kl_divergence(guide_site[""fn""], model_site[""fn""])\n                        kl_qp = scale_and_mask(kl_qp, scale=guide_site[""scale""], mask=guide_site[""mask""])\n                        if torch.is_tensor(kl_qp):\n                            assert kl_qp.shape == guide_site[""fn""].batch_shape\n                            kl_qp_sum = kl_qp.sum()\n                        else:\n                            kl_qp_sum = kl_qp * torch.Size(guide_site[""fn""].batch_shape).numel()\n                        elbo_particle = elbo_particle - kl_qp_sum\n                    except NotImplementedError:\n                        entropy_term = guide_site[""score_parts""].entropy_term\n                        elbo_particle = elbo_particle + model_site[""log_prob_sum""] - entropy_term.sum()\n\n        # handle auxiliary sites in the guide\n        for name, guide_site in guide_trace.nodes.items():\n            if guide_site[""type""] == ""sample"" and name not in model_trace.nodes:\n                assert guide_site[""infer""].get(""is_auxiliary"")\n                if is_validation_enabled():\n                    check_fully_reparametrized(guide_site)\n                entropy_term = guide_site[""score_parts""].entropy_term\n                elbo_particle = elbo_particle - entropy_term.sum()\n\n        loss = -(elbo_particle.detach() if torch._C._get_tracing_state() else torch_item(elbo_particle))\n        surrogate_loss = -elbo_particle\n        return loss, surrogate_loss\n\n\nclass JitTraceMeanField_ELBO(TraceMeanField_ELBO):\n    """"""\n    Like :class:`TraceMeanField_ELBO` but uses :func:`pyro.ops.jit.trace` to\n    compile :meth:`loss_and_grads`.\n\n    This works only for a limited set of models:\n\n    -   Models must have static structure.\n    -   Models must not depend on any global data (except the param store).\n    -   All model inputs that are tensors must be passed in via ``*args``.\n    -   All model inputs that are *not* tensors must be passed in via\n        ``**kwargs``, and compilation will be triggered once per unique\n        ``**kwargs``.\n    """"""\n    def differentiable_loss(self, model, guide, *args, **kwargs):\n        kwargs[\'_pyro_model_id\'] = id(model)\n        kwargs[\'_pyro_guide_id\'] = id(guide)\n        if getattr(self, \'_loss_and_surrogate_loss\', None) is None:\n            # build a closure for loss_and_surrogate_loss\n            weakself = weakref.ref(self)\n\n            @pyro.ops.jit.trace(ignore_warnings=self.ignore_jit_warnings,\n                                jit_options=self.jit_options)\n            def differentiable_loss(*args, **kwargs):\n                kwargs.pop(\'_pyro_model_id\')\n                kwargs.pop(\'_pyro_guide_id\')\n                self = weakself()\n                loss = 0.0\n                for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n                    _, loss_particle = self._differentiable_loss_particle(model_trace, guide_trace)\n                    loss = loss + loss_particle / self.num_particles\n                return loss\n\n            self._differentiable_loss = differentiable_loss\n\n        return self._differentiable_loss(*args, **kwargs)\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        loss = self.differentiable_loss(model, guide, *args, **kwargs)\n        loss.backward()\n        loss = torch_item(loss)\n\n        warn_if_nan(loss, ""loss"")\n        return loss\n'"
pyro/infer/trace_mmd.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import defaultdict\n\nimport torch\n\nimport pyro\nimport pyro.ops.jit\nfrom pyro import poutine\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.util import torch_item, is_validation_enabled\nfrom pyro.infer.enum import get_importance_trace\nfrom pyro.util import check_if_enumerated, warn_if_nan\n\n\ndef _compute_mmd(X, Z, kernel):\n    mmd = torch.mean(kernel(X)) + torch.mean(kernel(Z)) - torch.mean(kernel(X, Z)) * 2\n    return mmd\n\n\nclass Trace_MMD(ELBO):\n    """"""\n    An objective similar to ELBO, but with Maximum Mean Discrepancy (MMD)\n    between marginal variational posterior and prior distributions\n    instead of KL-divergence between variational posterior and prior distributions\n    as in vanilla ELBO.\n    The simplest example is MMD-VAE model [1]. The corresponding loss function is given as follows:\n\n        :math: `L(\\\\theta, \\\\phi) = -E_{p_{data}(x)} E_{q(z | x; \\\\phi)} \\\\log p(x | z; \\\\theta) +\n        MMD(q(z; \\\\phi) \\\\| p(z))`,\n\n    where z is a latent code. MMD between two distributions is defined as follows:\n\n        :math: `MMD(q(z) \\\\| p(z)) = E_{p(z), p(z\')} k(z,z\') + E_{q(z), q(z\')} k(z,z\') - 2 E_{p(z), q(z\')} k(z,z\')`,\n\n    where k is a kernel.\n\n    DISCLAIMER: this implementation treats only the particle dimension as batch dimension when computing MMD.\n    All other dimensions are treated as event dimensions.\n    For this reason, one needs large `num_particles` in order to have reasonable variance of MMD Monte-Carlo estimate.\n    As a consequence, it is recommended to set `vectorize_particles=True` (default).\n    The general case will be implemented in future versions.\n\n    :param kernel: A kernel used to compute MMD.\n        An instance of :class: `pyro.contrib.gp.kernels.kernel.Kernel`,\n        or a dict that maps latent variable names to instances of :class: `pyro.contrib.gp.kernels.kernel.Kernel`.\n        In the latter case, different kernels are used for different latent variables.\n\n    :param mmd_scale: A scaling factor for MMD terms.\n        Float, or a dict that maps latent variable names to floats.\n        In the latter case, different scaling factors are used for different latent variables.\n\n    References\n\n    [1] `A Tutorial on Information Maximizing Variational Autoencoders (InfoVAE)`\n        Shengjia Zhao\n        https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n\n    [2] `InfoVAE: Balancing Learning and Inference in Variational Autoencoders`\n        Shengjia Zhao, Jiaming Song, Stefano Ermon\n    """"""\n\n    def __init__(self,\n                 kernel, mmd_scale=1,\n                 num_particles=10,\n                 max_plate_nesting=float(\'inf\'),\n                 max_iarange_nesting=None,  # DEPRECATED\n                 vectorize_particles=True,\n                 strict_enumeration_warning=True,\n                 ignore_jit_warnings=False,\n                 retain_graph=None):\n        super().__init__(\n            num_particles, max_plate_nesting, max_iarange_nesting, vectorize_particles,\n            strict_enumeration_warning, ignore_jit_warnings, retain_graph,\n        )\n        self._kernel = None\n        self._mmd_scale = None\n        self.kernel = kernel\n        self.mmd_scale = mmd_scale\n\n    @property\n    def kernel(self):\n        return self._kernel\n\n    @kernel.setter\n    def kernel(self, kernel):\n        if isinstance(kernel, dict):\n            # fix kernel\'s parameters\n            for k in kernel.values():\n                if isinstance(k, pyro.contrib.gp.kernels.kernel.Kernel):\n                    k.requires_grad_(False)\n                else:\n                    raise TypeError(""`kernel` values should be instances of `pyro.contrib.gp.kernels.kernel.Kernel`"")\n            self._kernel = kernel\n        elif isinstance(kernel, pyro.contrib.gp.kernels.kernel.Kernel):\n            kernel.requires_grad_(False)\n            self._kernel = defaultdict(lambda: kernel)\n        else:\n            raise TypeError(""`kernel` should be an instance of `pyro.contrib.gp.kernels.kernel.Kernel`"")\n\n    @property\n    def mmd_scale(self):\n        return self._mmd_scale\n\n    @mmd_scale.setter\n    def mmd_scale(self, mmd_scale):\n        if isinstance(mmd_scale, dict):\n            self._mmd_scale = mmd_scale\n        elif isinstance(mmd_scale, (int, float)):\n            self._mmd_scale = defaultdict(lambda: float(mmd_scale))\n        else:\n            raise TypeError(""`mmd_scale` should be either float, or a dict of floats"")\n\n    def _get_trace(self, model, guide, args, kwargs):\n        """"""\n        Returns a single trace from the guide, and the model that is run\n        against it.\n        """"""\n        model_trace, guide_trace = get_importance_trace(\n            ""flat"", self.max_plate_nesting, model, guide, args, kwargs)\n        if is_validation_enabled():\n            check_if_enumerated(guide_trace)\n        return model_trace, guide_trace\n\n    def _differentiable_loss_parts(self, model, guide, args, kwargs):\n        all_model_samples = defaultdict(list)\n        all_guide_samples = defaultdict(list)\n\n        loglikelihood = 0.0\n        penalty = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            if self.vectorize_particles:\n                model_trace_independent = poutine.trace(\n                    self._vectorized_num_particles(model)\n                ).get_trace(*args, **kwargs)\n            else:\n                model_trace_independent = poutine.trace(model, graph_type=\'flat\').get_trace(*args, **kwargs)\n\n            loglikelihood_particle = 0.0\n            for name, model_site in model_trace.nodes.items():\n                if model_site[\'type\'] == \'sample\':\n                    if name in guide_trace and not model_site[\'is_observed\']:\n                        guide_site = guide_trace.nodes[name]\n                        independent_model_site = model_trace_independent.nodes[name]\n                        if not independent_model_site[""fn""].has_rsample:\n                            raise ValueError(""Model site {} is not reparameterizable"".format(name))\n                        if not guide_site[""fn""].has_rsample:\n                            raise ValueError(""Guide site {} is not reparameterizable"".format(name))\n\n                        particle_dim = -self.max_plate_nesting - independent_model_site[""fn""].event_dim\n\n                        model_samples = independent_model_site[\'value\']\n                        guide_samples = guide_site[\'value\']\n\n                        if self.vectorize_particles:\n                            model_samples = model_samples.transpose(-model_samples.dim(), particle_dim)\n                            model_samples = model_samples.view(model_samples.shape[0], -1)\n\n                            guide_samples = guide_samples.transpose(-guide_samples.dim(), particle_dim)\n                            guide_samples = guide_samples.view(guide_samples.shape[0], -1)\n                        else:\n                            model_samples = model_samples.view(1, -1)\n                            guide_samples = guide_samples.view(1, -1)\n\n                        all_model_samples[name].append(model_samples)\n                        all_guide_samples[name].append(guide_samples)\n                    else:\n                        loglikelihood_particle = loglikelihood_particle + model_site[\'log_prob_sum\']\n\n            loglikelihood = loglikelihood_particle / self.num_particles + loglikelihood\n\n        for name in all_model_samples.keys():\n            all_model_samples[name] = torch.cat(all_model_samples[name])\n            all_guide_samples[name] = torch.cat(all_guide_samples[name])\n            divergence = _compute_mmd(all_model_samples[name], all_guide_samples[name], kernel=self._kernel[name])\n            penalty = self._mmd_scale[name] * divergence + penalty\n\n        warn_if_nan(loglikelihood, ""loglikelihood"")\n        warn_if_nan(penalty, ""penalty"")\n        return loglikelihood, penalty\n\n    def differentiable_loss(self, model, guide, *args, **kwargs):\n        """"""\n        Computes the MMD-VAE-type loss [1]. Calling backward on the latter\n        leads to valid gradient estimates as long as latent variables\n        in both the guide and the model are reparameterizable.\n\n        References\n\n        [1] `A Tutorial on Information Maximizing Variational Autoencoders (InfoVAE)`\n            Shengjia Zhao\n            https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n        """"""\n        loglikelihood, penalty = self._differentiable_loss_parts(model, guide, args, kwargs)\n        loss = -loglikelihood + penalty\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the MMD-VAE-type loss [1]\n        :rtype: float\n\n        Computes the MMD-VAE-type loss with an estimator that uses num_particles many samples/particles.\n\n        References\n\n        [1] `A Tutorial on Information Maximizing Variational Autoencoders (InfoVAE)`\n            Shengjia Zhao\n            https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n        """"""\n        loss = self.differentiable_loss(model, guide, *args, **kwargs)\n        return torch_item(loss)\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the MMD-VAE-type loss [1]\n        :rtype: float\n\n        Computes the MMD-VAE-type loss and performs backward on it.\n        Leads to valid gradient estimates as long as latent variables\n        in both the guide and the model are reparameterizable.\n        Num_particles many samples are used to form the estimators.\n\n        References\n\n        [1] `A Tutorial on Information Maximizing Variational Autoencoders (InfoVAE)`\n            Shengjia Zhao\n            https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n        """"""\n        loss = self.differentiable_loss(model, guide, *args, **kwargs)\n        loss.backward(retain_graph=self.retain_graph)\n        return torch_item(loss)\n'"
pyro/infer/trace_tail_adaptive_elbo.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nimport torch\n\nfrom pyro.infer.trace_elbo import Trace_ELBO\nfrom pyro.infer.util import is_validation_enabled, check_fully_reparametrized\n\n\nclass TraceTailAdaptive_ELBO(Trace_ELBO):\n    """"""\n    Interface for Stochastic Variational Inference with an adaptive\n    f-divergence as described in ref. [1]. Users should specify\n    `num_particles` > 1 and `vectorize_particles==True`. The argument\n    `tail_adaptive_beta` can be specified to modify how the adaptive\n    f-divergence is constructed. See reference for details.\n\n    Note that this interface does not support computing the varational\n    objective itself; rather it only supports computing gradients of the\n    variational objective. Consequently, one might want to use\n    another SVI interface (e.g. `RenyiELBO`) in order to monitor convergence.\n\n    Note that this interface only supports models in which all the latent\n    variables are fully reparameterized. It also does not support data\n    subsampling.\n\n    References\n    [1] ""Variational Inference with Tail-adaptive f-Divergence"", Dilin Wang,\n    Hao Liu, Qiang Liu, NeurIPS 2018\n    https://papers.nips.cc/paper/7816-variational-inference-with-tail-adaptive-f-divergence\n    """"""\n    def loss(self, model, guide, *args, **kwargs):\n        """"""\n        It is not necessary to estimate the tail-adaptive f-divergence itself in order\n        to compute the corresponding gradients. Consequently the loss method is left\n        unimplemented.\n        """"""\n        raise NotImplementedError(""Loss method for TraceTailAdaptive_ELBO not implemented"")\n\n    def _differentiable_loss_particle(self, model_trace, guide_trace):\n        if not self.vectorize_particles:\n            raise NotImplementedError(""TraceTailAdaptive_ELBO only implemented for vectorize_particles==True"")\n\n        if self.num_particles == 1:\n            warnings.warn(""For num_particles==1 TraceTailAdaptive_ELBO uses the same loss function as Trace_ELBO. "" +\n                          ""Increase num_particles to get an adaptive f-divergence."")\n\n        log_p, log_q = 0, 0\n\n        for name, site in model_trace.nodes.items():\n            if site[""type""] == ""sample"":\n                site_log_p = site[""log_prob""].reshape(self.num_particles, -1).sum(-1)\n                log_p = log_p + site_log_p\n\n        for name, site in guide_trace.nodes.items():\n            if site[""type""] == ""sample"":\n                site_log_q = site[""log_prob""].reshape(self.num_particles, -1).sum(-1)\n                log_q = log_q + site_log_q\n                if is_validation_enabled():\n                    check_fully_reparametrized(site)\n\n        # rank the particles according to p/q\n        log_pq = log_p - log_q\n        rank = torch.argsort(log_pq, descending=False)\n        rank = torch.index_select(torch.arange(self.num_particles, device=log_pq.device) + 1, -1, rank).type_as(log_pq)\n\n        # compute the particle-specific weights used to construct the surrogate loss\n        gamma = torch.pow(rank, self.tail_adaptive_beta).detach()\n        surrogate_loss = -(log_pq * gamma).sum() / gamma.sum()\n\n        # we do not compute the loss, so return `inf`\n        return float(\'inf\'), surrogate_loss\n'"
pyro/infer/traceenum_elbo.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\nimport weakref\nfrom collections import OrderedDict\nimport queue\n\nimport torch\nfrom opt_einsum import shared_intermediates\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.ops.jit\nimport pyro.poutine as poutine\nfrom pyro.distributions.util import is_identically_zero\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.enum import get_importance_trace, iter_discrete_escape, iter_discrete_extend\nfrom pyro.infer.util import Dice, is_validation_enabled\nfrom pyro.ops import packed\nfrom pyro.ops.contract import contract_tensor_tree, contract_to_tensor\nfrom pyro.ops.rings import SampleRing\nfrom pyro.poutine.enum_messenger import EnumMessenger\nfrom pyro.util import check_traceenum_requirements, ignore_jit_warnings, warn_if_nan\n\n\n@ignore_jit_warnings()\ndef _get_common_scale(scales):\n    # Check that all enumerated sites share a common subsampling scale.\n    # Note that we use a cheap weak comparison by id rather than tensor value, because\n    # (1) it is expensive to compare tensors by value, and (2) tensors must agree not\n    # only in value but at all derivatives.\n    scales_set = set()\n    for scale in scales:\n        if isinstance(scale, torch.Tensor) and scale.dim():\n            raise ValueError(\'enumeration only supports scalar poutine.scale\')\n        scales_set.add(float(scale))\n    if len(scales_set) != 1:\n        raise ValueError(""Expected all enumerated sample sites to share a common poutine.scale, ""\n                         ""but found {} different scales."".format(len(scales_set)))\n    return scales[0]\n\n\ndef _check_model_guide_enumeration_constraint(model_enum_sites, guide_trace):\n    min_ordinal = frozenset.intersection(*model_enum_sites.keys())\n    for name, site in guide_trace.nodes.items():\n        if site[""type""] == ""sample"" and site[""infer""].get(""_enumerate_dim"") is not None:\n            for f in site[""cond_indep_stack""]:\n                if f.vectorized and guide_trace.plate_to_symbol[f.name] not in min_ordinal:\n                    raise ValueError(""Expected model enumeration to be no more global than guide enumeration, ""\n                                     ""but found model enumeration sites upstream of guide site \'{}\' in plate(\'{}\'). ""\n                                     ""Try converting some model enumeration sites to guide enumeration sites.""\n                                     .format(name, f.name))\n\n\ndef _check_tmc_elbo_constraint(model_trace, guide_trace):\n    num_samples = frozenset(\n        site[""infer""].get(""num_samples"")\n        for site in guide_trace.nodes.values()\n        if site[""type""] == ""sample"" and\n        site[""infer""].get(""enumerate"") == ""parallel"" and\n        site[""infer""].get(""num_samples"") is not None)\n    if len(num_samples) > 1:\n        warnings.warn(\'\\n\'.join([\n            ""Using different numbers of Monte Carlo samples for different guide sites in TraceEnum_ELBO."",\n            ""This may be biased if the guide is not factorized"",\n        ]), UserWarning)\n    for name, site in model_trace.nodes.items():\n        if site[""type""] == ""sample"" and \\\n                site[""infer""].get(""enumerate"", None) == ""parallel"" and \\\n                site[""infer""].get(""num_samples"", None) and \\\n                name not in guide_trace:\n            warnings.warn(\'\\n\'.join([\n                ""Site {} is multiply sampled in model,"".format(site[""name""]),\n                ""expect incorrect gradient estimates from TraceEnum_ELBO."",\n                ""Consider using exact enumeration or guide sampling if possible."",\n            ]), RuntimeWarning)\n\n\ndef _find_ordinal(trace, site):\n    return frozenset(trace.plate_to_symbol[f.name]\n                     for f in site[""cond_indep_stack""]\n                     if f.vectorized)\n\n\n# TODO move this logic into a poutine\ndef _compute_model_factors(model_trace, guide_trace):\n    # y depends on x iff ordering[x] <= ordering[y]\n    # TODO refine this coarse dependency ordering using time.\n    ordering = {name: _find_ordinal(trace, site)\n                for trace in (model_trace, guide_trace)\n                for name, site in trace.nodes.items()\n                if site[""type""] == ""sample""}\n\n    # Collect model sites that may have been enumerated in the model.\n    cost_sites = OrderedDict()\n    enum_sites = OrderedDict()\n    enum_dims = set()\n    non_enum_dims = set().union(*ordering.values())\n    for name, site in model_trace.nodes.items():\n        if site[""type""] == ""sample"":\n            if name in guide_trace.nodes:\n                cost_sites.setdefault(ordering[name], []).append(site)\n                non_enum_dims.update(guide_trace.nodes[name][""packed""][""log_prob""]._pyro_dims)\n            elif site[""infer""].get(""_enumerate_dim"") is None:\n                cost_sites.setdefault(ordering[name], []).append(site)\n            else:\n                enum_sites.setdefault(ordering[name], []).append(site)\n                enum_dims.update(site[""packed""][""log_prob""]._pyro_dims)\n    enum_dims -= non_enum_dims\n    log_factors = OrderedDict()\n    scale = 1\n    if not enum_sites:\n        marginal_costs = OrderedDict((t, [site[""packed""][""log_prob""] for site in sites_t])\n                                     for t, sites_t in cost_sites.items())\n        return marginal_costs, log_factors, ordering, enum_dims, scale\n    _check_model_guide_enumeration_constraint(enum_sites, guide_trace)\n\n    # Marginalize out all variables that have been enumerated in the model.\n    marginal_costs = OrderedDict()\n    scales = []\n    for t, sites_t in cost_sites.items():\n        for site in sites_t:\n            if enum_dims.isdisjoint(site[""packed""][""log_prob""]._pyro_dims):\n                # For sites that do not depend on an enumerated variable, proceed as usual.\n                marginal_costs.setdefault(t, []).append(site[""packed""][""log_prob""])\n            else:\n                # For sites that depend on an enumerated variable, we need to apply\n                # the mask inside- and the scale outside- of the log expectation.\n                if ""masked_log_prob"" not in site[""packed""]:\n                    site[""packed""][""masked_log_prob""] = packed.scale_and_mask(\n                        site[""packed""][""unscaled_log_prob""], mask=site[""packed""][""mask""])\n                cost = site[""packed""][""masked_log_prob""]\n                log_factors.setdefault(t, []).append(cost)\n                scales.append(site[""scale""])\n    if log_factors:\n        for t, sites_t in enum_sites.items():\n            # TODO refine this coarse dependency ordering using time and tensor shapes.\n            if any(t <= u for u in log_factors):\n                for site in sites_t:\n                    logprob = site[""packed""][""unscaled_log_prob""]\n                    log_factors.setdefault(t, []).append(logprob)\n                    scales.append(site[""scale""])\n        scale = _get_common_scale(scales)\n    return marginal_costs, log_factors, ordering, enum_dims, scale\n\n\ndef _compute_dice_elbo(model_trace, guide_trace):\n    # Accumulate marginal model costs.\n    marginal_costs, log_factors, ordering, sum_dims, scale = _compute_model_factors(\n            model_trace, guide_trace)\n    if log_factors:\n        dim_to_size = {}\n        for terms in log_factors.values():\n            for term in terms:\n                dim_to_size.update(zip(term._pyro_dims, term.shape))\n\n        # Note that while most applications of tensor message passing use the\n        # contract_to_tensor() interface and can be easily refactored to use ubersum(),\n        # the application here relies on contract_tensor_tree() to extract the dependency\n        # structure of different log_prob terms, which is used by Dice to eliminate\n        # zero-expectation terms. One possible refactoring would be to replace\n        # contract_to_tensor() with a RaggedTensor -> Tensor contraction operation, but\n        # replace contract_tensor_tree() with a RaggedTensor -> RaggedTensor contraction\n        # that preserves some dependency structure.\n        with shared_intermediates() as cache:\n            ring = SampleRing(cache=cache, dim_to_size=dim_to_size)\n            log_factors = contract_tensor_tree(log_factors, sum_dims, ring=ring)\n            model_trace._sharing_cache = cache  # For TraceEnumSample_ELBO.\n        for t, log_factors_t in log_factors.items():\n            marginal_costs_t = marginal_costs.setdefault(t, [])\n            for term in log_factors_t:\n                term = packed.scale_and_mask(term, scale=scale)\n                marginal_costs_t.append(term)\n    costs = marginal_costs\n\n    # Accumulate negative guide costs.\n    for name, site in guide_trace.nodes.items():\n        if site[""type""] == ""sample"":\n            cost = packed.neg(site[""packed""][""log_prob""])\n            costs.setdefault(ordering[name], []).append(cost)\n\n    return Dice(guide_trace, ordering).compute_expectation(costs)\n\n\ndef _make_dist(dist_, logits):\n    # Reshape for Bernoulli vs Categorical, OneHotCategorical, etc..\n    if isinstance(dist_, dist.Bernoulli):\n        logits = logits[..., 1] - logits[..., 0]\n    return type(dist_)(logits=logits)\n\n\ndef _compute_marginals(model_trace, guide_trace):\n    args = _compute_model_factors(model_trace, guide_trace)\n    marginal_costs, log_factors, ordering, sum_dims, scale = args\n\n    marginal_dists = OrderedDict()\n    with shared_intermediates() as cache:\n        for name, site in model_trace.nodes.items():\n            if (site[""type""] != ""sample"" or\n                    name in guide_trace.nodes or\n                    site[""infer""].get(""_enumerate_dim"") is None):\n                continue\n\n            enum_dim = site[""infer""][""_enumerate_dim""]\n            enum_symbol = site[""infer""][""_enumerate_symbol""]\n            ordinal = _find_ordinal(model_trace, site)\n            logits = contract_to_tensor(log_factors, sum_dims,\n                                        target_ordinal=ordinal, target_dims={enum_symbol},\n                                        cache=cache)\n            logits = packed.unpack(logits, model_trace.symbol_to_dim)\n            logits = logits.unsqueeze(-1).transpose(-1, enum_dim - 1)\n            while logits.shape[0] == 1:\n                logits = logits.squeeze(0)\n            marginal_dists[name] = _make_dist(site[""fn""], logits)\n    return marginal_dists\n\n\nclass BackwardSampleMessenger(pyro.poutine.messenger.Messenger):\n    """"""\n    Implements forward filtering / backward sampling for sampling\n    from the joint posterior distribution\n    """"""\n    def __init__(self, enum_trace, guide_trace):\n        self.enum_trace = enum_trace\n        args = _compute_model_factors(enum_trace, guide_trace)\n        self.log_factors = args[1]\n        self.sum_dims = args[3]\n\n    def __enter__(self):\n        self.cache = {}\n        return super().__enter__()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if exc_type is None:\n            assert not self.sum_dims, self.sum_dims\n        return super().__exit__(exc_type, exc_value, traceback)\n\n    def _pyro_sample(self, msg):\n        enum_msg = self.enum_trace.nodes.get(msg[""name""])\n        if enum_msg is None:\n            return\n        enum_symbol = enum_msg[""infer""].get(""_enumerate_symbol"")\n        if enum_symbol is None:\n            return\n        enum_dim = enum_msg[""infer""][""_enumerate_dim""]\n        with shared_intermediates(self.cache):\n            ordinal = _find_ordinal(self.enum_trace, msg)\n            logits = contract_to_tensor(self.log_factors, self.sum_dims,\n                                        target_ordinal=ordinal, target_dims={enum_symbol},\n                                        cache=self.cache)\n            logits = packed.unpack(logits, self.enum_trace.symbol_to_dim)\n            logits = logits.unsqueeze(-1).transpose(-1, enum_dim - 1)\n            while logits.shape[0] == 1:\n                logits = logits.squeeze(0)\n        msg[""fn""] = _make_dist(msg[""fn""], logits)\n\n    def _pyro_post_sample(self, msg):\n        enum_msg = self.enum_trace.nodes.get(msg[""name""])\n        if enum_msg is None:\n            return\n        enum_symbol = enum_msg[""infer""].get(""_enumerate_symbol"")\n        if enum_symbol is None:\n            return\n        value = packed.pack(msg[""value""].long(), enum_msg[""infer""][""_dim_to_symbol""])\n        assert enum_symbol not in value._pyro_dims\n        for t, terms in self.log_factors.items():\n            for i, term in enumerate(terms):\n                if enum_symbol in term._pyro_dims:\n                    terms[i] = packed.gather(term, value, enum_symbol)\n        self.sum_dims.remove(enum_symbol)\n\n\nclass TraceEnum_ELBO(ELBO):\n    """"""\n    A trace implementation of ELBO-based SVI that supports\n    - exhaustive enumeration over discrete sample sites, and\n    - local parallel sampling over any sample site in the guide.\n\n    To enumerate over a sample site in the ``guide``, mark the site with either\n    ``infer={\'enumerate\': \'sequential\'}`` or\n    ``infer={\'enumerate\': \'parallel\'}``. To configure all guide sites at once,\n    use :func:`~pyro.infer.enum.config_enumerate`. To enumerate over a sample\n    site in the ``model``, mark the site ``infer={\'enumerate\': \'parallel\'}``\n    and ensure the site does not appear in the ``guide``.\n\n    This assumes restricted dependency structure on the model and guide:\n    variables outside of an :class:`~pyro.plate` can never depend on\n    variables inside that :class:`~pyro.plate`.\n    """"""\n\n    def _get_trace(self, model, guide, args, kwargs):\n        """"""\n        Returns a single trace from the guide, and the model that is run\n        against it.\n        """"""\n        model_trace, guide_trace = get_importance_trace(\n            ""flat"", self.max_plate_nesting, model, guide, args, kwargs)\n\n        if is_validation_enabled():\n            check_traceenum_requirements(model_trace, guide_trace)\n            _check_tmc_elbo_constraint(model_trace, guide_trace)\n\n            has_enumerated_sites = any(site[""infer""].get(""enumerate"")\n                                       for trace in (guide_trace, model_trace)\n                                       for name, site in trace.nodes.items()\n                                       if site[""type""] == ""sample"")\n\n            if self.strict_enumeration_warning and not has_enumerated_sites:\n                warnings.warn(\'TraceEnum_ELBO found no sample sites configured for enumeration. \'\n                              \'If you want to enumerate sites, you need to @config_enumerate or set \'\n                              \'infer={""enumerate"": ""sequential""} or infer={""enumerate"": ""parallel""}? \'\n                              \'If you do not want to enumerate, consider using Trace_ELBO instead.\')\n\n        guide_trace.pack_tensors()\n        model_trace.pack_tensors(guide_trace.plate_to_symbol)\n        return model_trace, guide_trace\n\n    def _get_traces(self, model, guide, args, kwargs):\n        """"""\n        Runs the guide and runs the model against the guide with\n        the result packaged as a trace generator.\n        """"""\n        if self.max_plate_nesting == float(\'inf\'):\n            self._guess_max_plate_nesting(model, guide, args, kwargs)\n        if self.vectorize_particles:\n            guide = self._vectorized_num_particles(guide)\n            model = self._vectorized_num_particles(model)\n\n        # Enable parallel enumeration over the vectorized guide and model.\n        # The model allocates enumeration dimensions after (to the left of) the guide,\n        # accomplished by preserving the _ENUM_ALLOCATOR state after the guide call.\n        guide_enum = EnumMessenger(first_available_dim=-1 - self.max_plate_nesting)\n        model_enum = EnumMessenger()  # preserve _ENUM_ALLOCATOR state\n        guide = guide_enum(guide)\n        model = model_enum(model)\n\n        q = queue.LifoQueue()\n        guide = poutine.queue(guide, q,\n                              escape_fn=iter_discrete_escape,\n                              extend_fn=iter_discrete_extend)\n        for i in range(1 if self.vectorize_particles else self.num_particles):\n            q.put(poutine.Trace())\n            while not q.empty():\n                yield self._get_trace(model, guide, args, kwargs)\n\n    def loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: an estimate of the ELBO\n        :rtype: float\n\n        Estimates the ELBO using ``num_particles`` many samples (particles).\n        """"""\n        elbo = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            elbo_particle = _compute_dice_elbo(model_trace, guide_trace)\n            if is_identically_zero(elbo_particle):\n                continue\n\n            elbo += elbo_particle.item() / self.num_particles\n\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def differentiable_loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: a differentiable estimate of the ELBO\n        :rtype: torch.Tensor\n        :raises ValueError: if the ELBO is not differentiable (e.g. is\n            identically zero)\n\n        Estimates a differentiable ELBO using ``num_particles`` many samples\n        (particles).  The result should be infinitely differentiable (as long\n        as underlying derivatives have been implemented).\n        """"""\n        elbo = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            elbo_particle = _compute_dice_elbo(model_trace, guide_trace)\n            if is_identically_zero(elbo_particle):\n                continue\n\n            elbo = elbo + elbo_particle\n        elbo = elbo / self.num_particles\n\n        if not torch.is_tensor(elbo) or not elbo.requires_grad:\n            raise ValueError(\'ELBO is cannot be differentiated: {}\'.format(elbo))\n\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: an estimate of the ELBO\n        :rtype: float\n\n        Estimates the ELBO using ``num_particles`` many samples (particles).\n        Performs backward on the ELBO of each particle.\n        """"""\n        elbo = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            elbo_particle = _compute_dice_elbo(model_trace, guide_trace)\n            if is_identically_zero(elbo_particle):\n                continue\n\n            elbo += elbo_particle.item() / self.num_particles\n\n            # collect parameters to train from model and guide\n            trainable_params = any(site[""type""] == ""param""\n                                   for trace in (model_trace, guide_trace)\n                                   for site in trace.nodes.values())\n\n            if trainable_params and elbo_particle.requires_grad:\n                loss_particle = -elbo_particle\n                (loss_particle / self.num_particles).backward(retain_graph=True)\n\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def compute_marginals(self, model, guide, *args, **kwargs):\n        """"""\n        Computes marginal distributions at each model-enumerated sample site.\n\n        :returns: a dict mapping site name to marginal ``Distribution`` object\n        :rtype: OrderedDict\n        """"""\n        if self.num_particles != 1:\n            raise NotImplementedError(""TraceEnum_ELBO.compute_marginals() is not ""\n                                      ""compatible with multiple particles."")\n        model_trace, guide_trace = next(self._get_traces(model, guide, args, kwargs))\n        for site in guide_trace.nodes.values():\n            if site[""type""] == ""sample"":\n                if ""_enumerate_dim"" in site[""infer""] or ""_enum_total"" in site[""infer""]:\n                    raise NotImplementedError(""TraceEnum_ELBO.compute_marginals() is not ""\n                                              ""compatible with guide enumeration."")\n        return _compute_marginals(model_trace, guide_trace)\n\n    def sample_posterior(self, model, guide, *args, **kwargs):\n        """"""\n        Sample from the joint posterior distribution of all model-enumerated sites given all observations\n        """"""\n        if self.num_particles != 1:\n            raise NotImplementedError(""TraceEnum_ELBO.sample_posterior() is not ""\n                                      ""compatible with multiple particles."")\n        with poutine.block(), warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", ""Found vars in model but not guide"")\n            model_trace, guide_trace = next(self._get_traces(model, guide, args, kwargs))\n\n        for name, site in guide_trace.nodes.items():\n            if site[""type""] == ""sample"":\n                if ""_enumerate_dim"" in site[""infer""] or ""_enum_total"" in site[""infer""]:\n                    raise NotImplementedError(""TraceEnum_ELBO.sample_posterior() is not ""\n                                              ""compatible with guide enumeration."")\n\n        # TODO replace BackwardSample with torch_sample backend to ubersum\n        with BackwardSampleMessenger(model_trace, guide_trace):\n            return poutine.replay(model, trace=guide_trace)(*args, **kwargs)\n\n\nclass JitTraceEnum_ELBO(TraceEnum_ELBO):\n    """"""\n    Like :class:`TraceEnum_ELBO` but uses :func:`pyro.ops.jit.compile` to\n    compile :meth:`loss_and_grads`.\n\n    This works only for a limited set of models:\n\n    -   Models must have static structure.\n    -   Models must not depend on any global data (except the param store).\n    -   All model inputs that are tensors must be passed in via ``*args``.\n    -   All model inputs that are *not* tensors must be passed in via\n        ``**kwargs``, and compilation will be triggered once per unique\n        ``**kwargs``.\n    """"""\n    def differentiable_loss(self, model, guide, *args, **kwargs):\n        kwargs[\'_model_id\'] = id(model)\n        kwargs[\'_guide_id\'] = id(guide)\n        if getattr(self, \'_differentiable_loss\', None) is None:\n            # build a closure for differentiable_loss\n            weakself = weakref.ref(self)\n\n            @pyro.ops.jit.trace(ignore_warnings=self.ignore_jit_warnings,\n                                jit_options=self.jit_options)\n            def differentiable_loss(*args, **kwargs):\n                kwargs.pop(\'_model_id\')\n                kwargs.pop(\'_guide_id\')\n                self = weakself()\n                elbo = 0.0\n                for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n                    elbo = elbo + _compute_dice_elbo(model_trace, guide_trace)\n                return elbo * (-1.0 / self.num_particles)\n\n            self._differentiable_loss = differentiable_loss\n\n        return self._differentiable_loss(*args, **kwargs)\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        differentiable_loss = self.differentiable_loss(model, guide, *args, **kwargs)\n        differentiable_loss.backward()  # this line triggers jit compilation\n        loss = differentiable_loss.item()\n\n        warn_if_nan(loss, ""loss"")\n        return loss\n'"
pyro/infer/tracegraph_elbo.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport weakref\nfrom operator import itemgetter\n\nimport torch\n\nimport pyro\nimport pyro.ops.jit\nfrom pyro.distributions.util import is_identically_zero\nfrom pyro.infer import ELBO\nfrom pyro.infer.enum import get_importance_trace\nfrom pyro.infer.util import (MultiFrameTensor, detach_iterable, get_plate_stacks,\n                             is_validation_enabled, torch_backward, torch_item)\nfrom pyro.util import check_if_enumerated, warn_if_nan\n\n\ndef _get_baseline_options(site):\n    """"""\n    Extracts baseline options from ``site[""infer""][""baseline""]``.\n    """"""\n    # XXX default for baseline_beta currently set here\n    options_dict = site[""infer""].get(""baseline"", {}).copy()\n    options_tuple = (options_dict.pop(\'nn_baseline\', None),\n                     options_dict.pop(\'nn_baseline_input\', None),\n                     options_dict.pop(\'use_decaying_avg_baseline\', False),\n                     options_dict.pop(\'baseline_beta\', 0.90),\n                     options_dict.pop(\'baseline_value\', None))\n    if options_dict:\n        raise ValueError(""Unrecognized baseline options: {}"".format(options_dict.keys()))\n    return options_tuple\n\n\ndef _construct_baseline(node, guide_site, downstream_cost):\n\n    # XXX should the average baseline be in the param store as below?\n\n    baseline = 0.0\n    baseline_loss = 0.0\n\n    (nn_baseline, nn_baseline_input, use_decaying_avg_baseline, baseline_beta,\n        baseline_value) = _get_baseline_options(guide_site)\n\n    use_nn_baseline = nn_baseline is not None\n    use_baseline_value = baseline_value is not None\n\n    use_baseline = use_nn_baseline or use_decaying_avg_baseline or use_baseline_value\n\n    assert(not (use_nn_baseline and use_baseline_value)), \\\n        ""cannot use baseline_value and nn_baseline simultaneously""\n    if use_decaying_avg_baseline:\n        dc_shape = downstream_cost.shape\n        param_name = ""__baseline_avg_downstream_cost_"" + node\n        with torch.no_grad():\n            avg_downstream_cost_old = pyro.param(param_name,\n                                                 torch.zeros(dc_shape, device=guide_site[\'value\'].device))\n            avg_downstream_cost_new = (1 - baseline_beta) * downstream_cost + \\\n                baseline_beta * avg_downstream_cost_old\n        pyro.get_param_store()[param_name] = avg_downstream_cost_new\n        baseline += avg_downstream_cost_old\n    if use_nn_baseline:\n        # block nn_baseline_input gradients except in baseline loss\n        baseline += nn_baseline(detach_iterable(nn_baseline_input))\n    elif use_baseline_value:\n        # it\'s on the user to make sure baseline_value tape only points to baseline params\n        baseline += baseline_value\n    if use_nn_baseline or use_baseline_value:\n        # accumulate baseline loss\n        baseline_loss += torch.pow(downstream_cost.detach() - baseline, 2.0).sum()\n\n    if use_baseline:\n        if downstream_cost.shape != baseline.shape:\n            raise ValueError(""Expected baseline at site {} to be {} instead got {}"".format(\n                node, downstream_cost.shape, baseline.shape))\n\n    return use_baseline, baseline_loss, baseline\n\n\ndef _compute_downstream_costs(model_trace, guide_trace,  #\n                              non_reparam_nodes):\n    # recursively compute downstream cost nodes for all sample sites in model and guide\n    # (even though ultimately just need for non-reparameterizable sample sites)\n    # 1. downstream costs used for rao-blackwellization\n    # 2. model observe sites (as well as terms that arise from the model and guide having different\n    # dependency structures) are taken care of via \'children_in_model\' below\n    topo_sort_guide_nodes = guide_trace.topological_sort(reverse=True)\n    topo_sort_guide_nodes = [x for x in topo_sort_guide_nodes\n                             if guide_trace.nodes[x][""type""] == ""sample""]\n    ordered_guide_nodes_dict = {n: i for i, n in enumerate(topo_sort_guide_nodes)}\n\n    downstream_guide_cost_nodes = {}\n    downstream_costs = {}\n    stacks = get_plate_stacks(model_trace)\n\n    for node in topo_sort_guide_nodes:\n        downstream_costs[node] = MultiFrameTensor((stacks[node],\n                                                   model_trace.nodes[node][\'log_prob\'] -\n                                                   guide_trace.nodes[node][\'log_prob\']))\n        nodes_included_in_sum = set([node])\n        downstream_guide_cost_nodes[node] = set([node])\n        # make more efficient by ordering children appropriately (higher children first)\n        children = [(k, -ordered_guide_nodes_dict[k]) for k in guide_trace.successors(node)]\n        sorted_children = sorted(children, key=itemgetter(1))\n        for child, _ in sorted_children:\n            child_cost_nodes = downstream_guide_cost_nodes[child]\n            downstream_guide_cost_nodes[node].update(child_cost_nodes)\n            if nodes_included_in_sum.isdisjoint(child_cost_nodes):  # avoid duplicates\n                downstream_costs[node].add(*downstream_costs[child].items())\n                # XXX nodes_included_in_sum logic could be more fine-grained, possibly leading\n                # to speed-ups in case there are many duplicates\n                nodes_included_in_sum.update(child_cost_nodes)\n        missing_downstream_costs = downstream_guide_cost_nodes[node] - nodes_included_in_sum\n        # include terms we missed because we had to avoid duplicates\n        for missing_node in missing_downstream_costs:\n            downstream_costs[node].add((stacks[missing_node],\n                                        model_trace.nodes[missing_node][\'log_prob\'] -\n                                        guide_trace.nodes[missing_node][\'log_prob\']))\n\n    # finish assembling complete downstream costs\n    # (the above computation may be missing terms from model)\n    for site in non_reparam_nodes:\n        children_in_model = set()\n        for node in downstream_guide_cost_nodes[site]:\n            children_in_model.update(model_trace.successors(node))\n        # remove terms accounted for above\n        children_in_model.difference_update(downstream_guide_cost_nodes[site])\n        for child in children_in_model:\n            assert (model_trace.nodes[child][""type""] == ""sample"")\n            downstream_costs[site].add((stacks[child],\n                                        model_trace.nodes[child][\'log_prob\']))\n            downstream_guide_cost_nodes[site].update([child])\n\n    for k in non_reparam_nodes:\n        downstream_costs[k] = downstream_costs[k].sum_to(guide_trace.nodes[k][""cond_indep_stack""])\n\n    return downstream_costs, downstream_guide_cost_nodes\n\n\ndef _compute_elbo_reparam(model_trace, guide_trace):\n\n    # In ref [1], section 3.2, the part of the surrogate loss computed here is\n    # \\sum{cost}, which in this case is the ELBO. Instead of using the ELBO,\n    # this implementation uses a surrogate ELBO which modifies some entropy\n    # terms depending on the parameterization. This reduces the variance of the\n    # gradient under some conditions.\n\n    elbo = 0.0\n    surrogate_elbo = 0.0\n\n    # Bring log p(x, z|...) terms into both the ELBO and the surrogate\n    for name, site in model_trace.nodes.items():\n        if site[""type""] == ""sample"":\n            elbo += site[""log_prob_sum""]\n            surrogate_elbo += site[""log_prob_sum""]\n\n    # Bring log q(z|...) terms into the ELBO, and effective terms into the\n    # surrogate. Depending on the parameterization of a site, its log q(z|...)\n    # cost term may not contribute (in expectation) to the gradient. To reduce\n    # the variance under some conditions, the default entropy terms from\n    # site[`score_parts`] are used.\n    for name, site in guide_trace.nodes.items():\n        if site[""type""] == ""sample"":\n            elbo -= site[""log_prob_sum""]\n            entropy_term = site[""score_parts""].entropy_term\n            # For fully reparameterized terms, this entropy_term is log q(z|...)\n            # For fully non-reparameterized terms, it is zero\n            if not is_identically_zero(entropy_term):\n                surrogate_elbo -= entropy_term.sum()\n\n    return elbo, surrogate_elbo\n\n\ndef _compute_elbo_non_reparam(guide_trace, non_reparam_nodes, downstream_costs):\n    # construct all the reinforce-like terms.\n    # we include only downstream costs to reduce variance\n    # optionally include baselines to further reduce variance\n    surrogate_elbo = 0.0\n    baseline_loss = 0.0\n    for node in non_reparam_nodes:\n        guide_site = guide_trace.nodes[node]\n        downstream_cost = downstream_costs[node]\n        score_function = guide_site[""score_parts""].score_function\n\n        use_baseline, baseline_loss_term, baseline = _construct_baseline(node, guide_site, downstream_cost)\n\n        if use_baseline:\n            downstream_cost = downstream_cost - baseline\n            baseline_loss = baseline_loss + baseline_loss_term\n\n        surrogate_elbo += (score_function * downstream_cost.detach()).sum()\n\n    return surrogate_elbo, baseline_loss\n\n\nclass TraceGraph_ELBO(ELBO):\n    """"""\n    A TraceGraph implementation of ELBO-based SVI. The gradient estimator\n    is constructed along the lines of reference [1] specialized to the case\n    of the ELBO. It supports arbitrary dependency structure for the model\n    and guide as well as baselines for non-reparameterizable random variables.\n    Where possible, conditional dependency information as recorded in the\n    :class:`~pyro.poutine.trace.Trace` is used to reduce the variance of the gradient estimator.\n    In particular two kinds of conditional dependency information are\n    used to reduce variance:\n\n    - the sequential order of samples (z is sampled after y => y does not depend on z)\n    - :class:`~pyro.plate` generators\n\n    References\n\n    [1] `Gradient Estimation Using Stochastic Computation Graphs`,\n        John Schulman, Nicolas Heess, Theophane Weber, Pieter Abbeel\n\n    [2] `Neural Variational Inference and Learning in Belief Networks`\n        Andriy Mnih, Karol Gregor\n    """"""\n\n    def _get_trace(self, model, guide, args, kwargs):\n        """"""\n        Returns a single trace from the guide, and the model that is run\n        against it.\n        """"""\n        model_trace, guide_trace = get_importance_trace(\n            ""dense"", self.max_plate_nesting, model, guide, args, kwargs)\n        if is_validation_enabled():\n            check_if_enumerated(guide_trace)\n        return model_trace, guide_trace\n\n    def loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Evaluates the ELBO with an estimator that uses num_particles many samples/particles.\n        """"""\n        elbo = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            elbo_particle = torch_item(model_trace.log_prob_sum()) - torch_item(guide_trace.log_prob_sum())\n            elbo += elbo_particle / float(self.num_particles)\n\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: returns an estimate of the ELBO\n        :rtype: float\n\n        Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator.\n        Performs backward on the latter. Num_particle many samples are used to form the estimators.\n        If baselines are present, a baseline loss is also constructed and differentiated.\n        """"""\n        elbo, surrogate_loss = self._loss_and_surrogate_loss(model, guide, args, kwargs)\n\n        torch_backward(surrogate_loss, retain_graph=self.retain_graph)\n\n        elbo = torch_item(elbo)\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def _loss_and_surrogate_loss(self, model, guide, args, kwargs):\n\n        loss = 0.0\n        surrogate_loss = 0.0\n\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n\n            lp, slp = self._loss_and_surrogate_loss_particle(model_trace, guide_trace)\n            loss += lp\n            surrogate_loss += slp\n\n        loss /= self.num_particles\n        surrogate_loss /= self.num_particles\n\n        return loss, surrogate_loss\n\n    def _loss_and_surrogate_loss_particle(self, model_trace, guide_trace):\n\n        # compute elbo for reparameterized nodes\n        elbo, surrogate_elbo = _compute_elbo_reparam(model_trace, guide_trace)\n        baseline_loss = 0.0\n\n        # the following computations are only necessary if we have non-reparameterizable nodes\n        non_reparam_nodes = set(guide_trace.nonreparam_stochastic_nodes)\n        if non_reparam_nodes:\n            downstream_costs, _ = _compute_downstream_costs(model_trace, guide_trace, non_reparam_nodes)\n            surrogate_elbo_term, baseline_loss = _compute_elbo_non_reparam(guide_trace,\n                                                                           non_reparam_nodes,\n                                                                           downstream_costs)\n            surrogate_elbo += surrogate_elbo_term\n\n        surrogate_loss = -surrogate_elbo + baseline_loss\n\n        return elbo, surrogate_loss\n\n\nclass JitTraceGraph_ELBO(TraceGraph_ELBO):\n    """"""\n    Like :class:`TraceGraph_ELBO` but uses :func:`torch.jit.trace` to\n    compile :meth:`loss_and_grads`.\n\n    This works only for a limited set of models:\n\n    -   Models must have static structure.\n    -   Models must not depend on any global data (except the param store).\n    -   All model inputs that are tensors must be passed in via ``*args``.\n    -   All model inputs that are *not* tensors must be passed in via\n        ``**kwargs``, and compilation will be triggered once per unique\n        ``**kwargs``.\n    """"""\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        kwargs[\'_pyro_model_id\'] = id(model)\n        kwargs[\'_pyro_guide_id\'] = id(guide)\n        if getattr(self, \'_jit_loss_and_surrogate_loss\', None) is None:\n            # build a closure for loss_and_surrogate_loss\n            weakself = weakref.ref(self)\n\n            @pyro.ops.jit.trace(ignore_warnings=self.ignore_jit_warnings,\n                                jit_options=self.jit_options)\n            def jit_loss_and_surrogate_loss(*args, **kwargs):\n                kwargs.pop(\'_pyro_model_id\')\n                kwargs.pop(\'_pyro_guide_id\')\n                self = weakself()\n                return self._loss_and_surrogate_loss(model, guide, args, kwargs)\n\n            self._jit_loss_and_surrogate_loss = jit_loss_and_surrogate_loss\n\n        loss, surrogate_loss = self._jit_loss_and_surrogate_loss(*args, **kwargs)\n\n        surrogate_loss.backward(retain_graph=self.retain_graph)  # triggers jit compilation\n\n        loss = loss.item()\n        warn_if_nan(loss, ""loss"")\n        return loss\n'"
pyro/infer/tracetmc_elbo.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport queue\nimport warnings\n\nimport torch\n\nimport pyro.poutine as poutine\n\nfrom pyro.distributions.util import is_identically_zero\nfrom pyro.infer.elbo import ELBO\nfrom pyro.infer.enum import get_importance_trace, iter_discrete_escape, iter_discrete_extend\nfrom pyro.infer.util import compute_site_dice_factor, is_validation_enabled, torch_item\nfrom pyro.ops import packed\nfrom pyro.ops.contract import einsum\nfrom pyro.poutine.enum_messenger import EnumMessenger\nfrom pyro.util import check_traceenum_requirements, warn_if_nan\n\n\ndef _compute_dice_factors(model_trace, guide_trace):\n    """"""\n    compute per-site DiCE log-factors for non-reparameterized proposal sites\n    this logic is adapted from pyro.infer.util.Dice.__init__\n    """"""\n    log_probs = []\n    for role, trace in zip((""model"", ""guide""), (model_trace, guide_trace)):\n        for name, site in trace.nodes.items():\n            if site[""type""] != ""sample"" or site[""is_observed""]:\n                continue\n            if role == ""model"" and name in guide_trace:\n                continue\n\n            log_prob, log_denom = compute_site_dice_factor(site)\n            if not is_identically_zero(log_denom):\n                dims = log_prob._pyro_dims\n                log_prob = log_prob - log_denom\n                log_prob._pyro_dims = dims\n            if not is_identically_zero(log_prob):\n                log_probs.append(log_prob)\n\n    return log_probs\n\n\ndef _compute_tmc_factors(model_trace, guide_trace):\n    """"""\n    compute per-site log-factors for all observed and unobserved variables\n    log-factors are log(p / q) for unobserved sites and log(p) for observed sites\n    """"""\n    log_factors = []\n    for name, site in guide_trace.nodes.items():\n        if site[""type""] != ""sample"" or site[""is_observed""]:\n            continue\n        log_proposal = site[""packed""][""log_prob""]\n        log_factors.append(packed.neg(log_proposal))\n    for name, site in model_trace.nodes.items():\n        if site[""type""] != ""sample"":\n            continue\n        if site[""name""] not in guide_trace and \\\n                not site[""is_observed""] and \\\n                site[""infer""].get(""enumerate"", None) == ""parallel"" and \\\n                site[""infer""].get(""num_samples"", -1) > 0:\n            # site was sampled from the prior\n            log_proposal = packed.neg(site[""packed""][""log_prob""])\n            log_factors.append(log_proposal)\n        log_factors.append(site[""packed""][""log_prob""])\n    return log_factors\n\n\ndef _compute_tmc_estimate(model_trace, guide_trace):\n    """"""\n    Use :func:`~pyro.ops.contract.einsum` to compute the Tensor Monte Carlo\n    estimate of the marginal likelihood given parallel-sampled traces.\n    """"""\n    # factors\n    log_factors = _compute_tmc_factors(model_trace, guide_trace)\n    log_factors += _compute_dice_factors(model_trace, guide_trace)\n\n    if not log_factors:\n        return 0.\n\n    # loss\n    eqn = "","".join([f._pyro_dims for f in log_factors]) + ""->""\n    plates = """".join(frozenset().union(list(model_trace.plate_to_symbol.values()),\n                                       list(guide_trace.plate_to_symbol.values())))\n    tmc, = einsum(eqn, *log_factors, plates=plates,\n                  backend=""pyro.ops.einsum.torch_log"",\n                  modulo_total=False)\n    return tmc\n\n\nclass TraceTMC_ELBO(ELBO):\n    """"""\n    A trace-based implementation of Tensor Monte Carlo [1]\n    by way of Tensor Variable Elimination [2] that supports:\n    - local parallel sampling over any sample site in the model or guide\n    - exhaustive enumeration over any sample site in the model or guide\n\n    To take multiple samples, mark the site with\n    ``infer={\'enumerate\': \'parallel\', \'num_samples\': N}``.\n    To configure all sites in a model or guide at once,\n    use :func:`~pyro.infer.enum.config_enumerate` .\n    To enumerate or sample a sample site in the ``model``,\n    mark the site and ensure the site does not appear in the ``guide``.\n\n    This assumes restricted dependency structure on the model and guide:\n    variables outside of an :class:`~pyro.plate` can never depend on\n    variables inside that :class:`~pyro.plate` .\n\n    References\n\n    [1] `Tensor Monte Carlo: Particle Methods for the GPU Era`,\n        Laurence Aitchison (2018)\n\n    [2] `Tensor Variable Elimination for Plated Factor Graphs`,\n        Fritz Obermeyer, Eli Bingham, Martin Jankowiak, Justin Chiu, Neeraj Pradhan,\n        Alexander Rush, Noah Goodman (2019)\n    """"""\n\n    def _get_trace(self, model, guide, args, kwargs):\n        """"""\n        Returns a single trace from the guide, and the model that is run\n        against it.\n        """"""\n        model_trace, guide_trace = get_importance_trace(\n            ""flat"", self.max_plate_nesting, model, guide, args, kwargs)\n\n        if is_validation_enabled():\n            check_traceenum_requirements(model_trace, guide_trace)\n\n            has_enumerated_sites = any(site[""infer""].get(""enumerate"")\n                                       for trace in (guide_trace, model_trace)\n                                       for name, site in trace.nodes.items()\n                                       if site[""type""] == ""sample"")\n\n            if self.strict_enumeration_warning and not has_enumerated_sites:\n                warnings.warn(\'Found no sample sites configured for enumeration. \'\n                              \'If you want to enumerate sites, you need to @config_enumerate or set \'\n                              \'infer={""enumerate"": ""sequential""} or infer={""enumerate"": ""parallel""}? \'\n                              \'If you do not want to enumerate, consider using Trace_ELBO instead.\')\n\n        model_trace.compute_score_parts()\n        guide_trace.pack_tensors()\n        model_trace.pack_tensors(guide_trace.plate_to_symbol)\n        return model_trace, guide_trace\n\n    def _get_traces(self, model, guide, args, kwargs):\n        """"""\n        Runs the guide and runs the model against the guide with\n        the result packaged as a trace generator.\n        """"""\n        if self.max_plate_nesting == float(\'inf\'):\n            self._guess_max_plate_nesting(model, guide, args, kwargs)\n        if self.vectorize_particles:\n            guide = self._vectorized_num_particles(guide)\n            model = self._vectorized_num_particles(model)\n\n        # Enable parallel enumeration over the vectorized guide and model.\n        # The model allocates enumeration dimensions after (to the left of) the guide,\n        # accomplished by preserving the _ENUM_ALLOCATOR state after the guide call.\n        guide_enum = EnumMessenger(first_available_dim=-1 - self.max_plate_nesting)\n        model_enum = EnumMessenger()  # preserve _ENUM_ALLOCATOR state\n        guide = guide_enum(guide)\n        model = model_enum(model)\n\n        q = queue.LifoQueue()\n        guide = poutine.queue(guide, q,\n                              escape_fn=iter_discrete_escape,\n                              extend_fn=iter_discrete_extend)\n        for i in range(1 if self.vectorize_particles else self.num_particles):\n            q.put(poutine.Trace())\n            while not q.empty():\n                yield self._get_trace(model, guide, args, kwargs)\n\n    def differentiable_loss(self, model, guide, *args, **kwargs):\n        """"""\n        :returns: a differentiable estimate of the marginal log-likelihood\n        :rtype: torch.Tensor\n        :raises ValueError: if the ELBO is not differentiable (e.g. is\n            identically zero)\n\n        Computes a differentiable TMC estimate using ``num_particles`` many samples\n        (particles).  The result should be infinitely differentiable (as long\n        as underlying derivatives have been implemented).\n        """"""\n        elbo = 0.0\n        for model_trace, guide_trace in self._get_traces(model, guide, args, kwargs):\n            elbo_particle = _compute_tmc_estimate(model_trace, guide_trace)\n            if is_identically_zero(elbo_particle):\n                continue\n\n            elbo = elbo + elbo_particle\n        elbo = elbo / self.num_particles\n\n        loss = -elbo\n        warn_if_nan(loss, ""loss"")\n        return loss\n\n    def loss(self, model, guide, *args, **kwargs):\n        with torch.no_grad():\n            loss = self.differentiable_loss(model, guide, *args, **kwargs)\n            if is_identically_zero(loss) or not loss.requires_grad:\n                return torch_item(loss)\n            return loss.item()\n\n    def loss_and_grads(self, model, guide, *args, **kwargs):\n        loss = self.differentiable_loss(model, guide, *args, **kwargs)\n        if is_identically_zero(loss) or not loss.requires_grad:\n            return torch_item(loss)\n        loss.backward()\n        return loss.item()\n'"
pyro/infer/util.py,18,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport numbers\nfrom collections import Counter, defaultdict\nfrom contextlib import contextmanager\n\nimport torch\nfrom opt_einsum import shared_intermediates\nfrom opt_einsum.sharing import count_cached_ops\n\nfrom pyro.distributions.util import is_identically_zero\nfrom pyro.ops import packed\nfrom pyro.ops.einsum.adjoint import require_backward\nfrom pyro.ops.rings import MarginalRing\nfrom pyro.poutine.util import site_is_subsample\n\n_VALIDATION_ENABLED = False\nLAST_CACHE_SIZE = [Counter()]  # for profiling\n\n\ndef enable_validation(is_validate):\n    global _VALIDATION_ENABLED\n    _VALIDATION_ENABLED = is_validate\n\n\ndef is_validation_enabled():\n    return _VALIDATION_ENABLED\n\n\n@contextmanager\ndef validation_enabled(is_validate=True):\n    old = is_validation_enabled()\n    try:\n        enable_validation(is_validate)\n        yield\n    finally:\n        enable_validation(old)\n\n\ndef torch_item(x):\n    """"""\n    Like ``x.item()`` for a :class:`~torch.Tensor`, but also works with numbers.\n    """"""\n    return x if isinstance(x, numbers.Number) else x.item()\n\n\ndef torch_backward(x, retain_graph=None):\n    """"""\n    Like ``x.backward()`` for a :class:`~torch.Tensor`, but also accepts\n    numbers and tensors without grad_fn (resulting in a no-op)\n    """"""\n    if torch.is_tensor(x) and x.grad_fn:\n        x.backward(retain_graph=retain_graph)\n\n\ndef torch_exp(x):\n    """"""\n    Like ``x.exp()`` for a :class:`~torch.Tensor`, but also accepts\n    numbers.\n    """"""\n    if torch.is_tensor(x):\n        return torch.exp(x)\n    else:\n        return math.exp(x)\n\n\ndef torch_sum(tensor, dims):\n    """"""\n    Like :func:`torch.sum` but sum out dims only if they exist.\n    """"""\n    assert all(d < 0 for d in dims)\n    leftmost = -tensor.dim()\n    dims = [d for d in dims if leftmost <= d]\n    return tensor.sum(dims) if dims else tensor\n\n\ndef detach_iterable(iterable):\n    if torch.is_tensor(iterable):\n        return iterable.detach()\n    else:\n        return [var.detach() for var in iterable]\n\n\ndef zero_grads(tensors):\n    """"""\n    Sets gradients of list of Tensors to zero in place\n    """"""\n    for p in tensors:\n        if p.grad is not None:\n            p.grad = torch.zeros_like(p.grad)\n\n\ndef get_plate_stacks(trace):\n    """"""\n    This builds a dict mapping site name to a set of plate stacks.  Each\n    plate stack is a list of :class:`CondIndepStackFrame`s corresponding to\n    an :class:`plate`.  This information is used by :class:`Trace_ELBO` and\n    :class:`TraceGraph_ELBO`.\n    """"""\n    return {name: [f for f in node[""cond_indep_stack""] if f.vectorized]\n            for name, node in trace.nodes.items()\n            if node[""type""] == ""sample"" and not site_is_subsample(node)}\n\n\ndef get_dependent_plate_dims(sites):\n    """"""\n    Return a list of dims for plates that are not common to all sites.\n    """"""\n    plate_sets = [site[""cond_indep_stack""]\n                  for site in sites if site[""type""] == ""sample""]\n    all_plates = set().union(*plate_sets)\n    common_plates = all_plates.intersection(*plate_sets)\n    sum_plates = all_plates - common_plates\n    sum_dims = list(sorted(f.dim for f in sum_plates))\n    return sum_dims\n\n\nclass MultiFrameTensor(dict):\n    """"""\n    A container for sums of Tensors among different :class:`plate` contexts.\n\n    Used in :class:`~pyro.infer.tracegraph_elbo.TraceGraph_ELBO` to simplify\n    downstream cost computation logic.\n\n    Example::\n\n        downstream_cost = MultiFrameTensor()\n        for site in downstream_nodes:\n            downstream_cost.add((site[""cond_indep_stack""], site[""log_prob""]))\n        downstream_cost.add(*other_costs.items())  # add in bulk\n        summed = downstream_cost.sum_to(target_site[""cond_indep_stack""])\n    """"""\n    def __init__(self, *items):\n        super().__init__()\n        self.add(*items)\n\n    def add(self, *items):\n        """"""\n        Add a collection of (cond_indep_stack, tensor) pairs. Keys are\n        ``cond_indep_stack``s, i.e. tuples of :class:`CondIndepStackFrame`s.\n        Values are :class:`torch.Tensor`s.\n        """"""\n        for cond_indep_stack, value in items:\n            frames = frozenset(f for f in cond_indep_stack if f.vectorized)\n            assert all(f.dim < 0 and -value.dim() <= f.dim for f in frames)\n            if frames in self:\n                self[frames] = self[frames] + value\n            else:\n                self[frames] = value\n\n    def sum_to(self, target_frames):\n        total = None\n        for frames, value in self.items():\n            for f in frames:\n                if f not in target_frames and value.shape[f.dim] != 1:\n                    value = value.sum(f.dim, True)\n            while value.shape and value.shape[0] == 1:\n                value = value.squeeze(0)\n            total = value if total is None else total + value\n        return total\n\n    def __repr__(self):\n        return \'%s(%s)\' % (type(self).__name__, "",\\n\\t"".join([\n            \'({}, ...)\'.format(frames) for frames in self]))\n\n\ndef compute_site_dice_factor(site):\n    log_denom = 0\n    log_prob = site[""packed""][""score_parts""].score_function  # not scaled by subsampling\n    dims = getattr(log_prob, ""_pyro_dims"", """")\n    if site[""infer""].get(""enumerate""):\n        num_samples = site[""infer""].get(""num_samples"")\n        if num_samples is not None:  # site was multiply sampled\n            if not is_identically_zero(log_prob):\n                log_prob = log_prob - log_prob.detach()\n            log_prob = log_prob - math.log(num_samples)\n            if not isinstance(log_prob, torch.Tensor):\n                log_prob = torch.tensor(float(log_prob), device=site[""value""].device)\n            log_prob._pyro_dims = dims\n            # I don\'t know why the following broadcast is needed, but it makes tests pass:\n            log_prob, _ = packed.broadcast_all(log_prob, site[""packed""][""log_prob""])\n        elif site[""infer""][""enumerate""] == ""sequential"":\n            log_denom = math.log(site[""infer""].get(""_enum_total"", num_samples))\n    else:  # site was monte carlo sampled\n        if not is_identically_zero(log_prob):\n            log_prob = log_prob - log_prob.detach()\n            log_prob._pyro_dims = dims\n\n    return log_prob, log_denom\n\n\nclass Dice:\n    """"""\n    An implementation of the DiCE operator compatible with Pyro features.\n\n    This implementation correctly handles:\n    - scaled log-probability due to subsampling\n    - independence in different ordinals due to plate\n    - weights due to parallel and sequential enumeration\n    - weights due to local multiple sampling\n\n    This assumes restricted dependency structure on the model and guide:\n    variables outside of an :class:`~pyro.plate` can never depend on\n    variables inside that :class:`~pyro.plate`.\n\n    References:\n    [1] Jakob Foerster, Greg Farquhar, Maruan Al-Shedivat, Tim Rocktaeschel,\n        Eric P. Xing, Shimon Whiteson (2018)\n        ""DiCE: The Infinitely Differentiable Monte-Carlo Estimator""\n        https://arxiv.org/abs/1802.05098\n    [2] Laurence Aitchison (2018)\n        ""Tensor Monte Carlo: particle methods for the GPU era""\n        https://arxiv.org/abs/1806.08593\n\n    :param pyro.poutine.trace.Trace guide_trace: A guide trace.\n    :param ordering: A dictionary mapping model site names to ordinal values.\n        Ordinal values may be any type that is (1) ``<=`` comparable and (2)\n        hashable; the canonical ordinal is a ``frozenset`` of site names.\n    """"""\n    def __init__(self, guide_trace, ordering):\n        log_denoms = defaultdict(float)  # avoids double-counting when sequentially enumerating\n        log_probs = defaultdict(list)  # accounts for upstream probabilties\n\n        for name, site in guide_trace.nodes.items():\n            if site[""type""] != ""sample"":\n                continue\n\n            ordinal = ordering[name]\n            log_prob, log_denom = compute_site_dice_factor(site)\n            if not is_identically_zero(log_prob):\n                log_probs[ordinal].append(log_prob)\n            if not is_identically_zero(log_denom):\n                log_denoms[ordinal] += log_denom\n\n        self.log_denom = log_denoms\n        self.log_probs = log_probs\n\n    def _get_log_factors(self, target_ordinal):\n        """"""\n        Returns a list of DiCE factors at a given ordinal.\n        """"""\n        log_denom = 0\n        for ordinal, term in self.log_denom.items():\n            if not ordinal <= target_ordinal:  # not downstream\n                log_denom += term  # term = log(# times this ordinal is counted)\n\n        log_factors = [] if is_identically_zero(log_denom) else [-log_denom]\n        for ordinal, terms in self.log_probs.items():\n            if ordinal <= target_ordinal:  # upstream\n                log_factors.extend(terms)  # terms = [log(dice weight of this ordinal)]\n\n        return log_factors\n\n    def compute_expectation(self, costs):\n        """"""\n        Returns a differentiable expected cost, summing over costs at given ordinals.\n\n        :param dict costs: A dict mapping ordinals to lists of cost tensors\n        :returns: a scalar expected cost\n        :rtype: torch.Tensor or float\n        """"""\n        # Share computation across all cost terms.\n        with shared_intermediates() as cache:\n            ring = MarginalRing(cache=cache)\n            expected_cost = 0.\n            for ordinal, cost_terms in costs.items():\n                log_factors = self._get_log_factors(ordinal)\n                scale = math.exp(sum(x for x in log_factors if not isinstance(x, torch.Tensor)))\n                log_factors = [x for x in log_factors if isinstance(x, torch.Tensor)]\n\n                # Collect log_prob terms to query for marginal probability.\n                queries = {frozenset(cost._pyro_dims): None for cost in cost_terms}\n                for log_factor in log_factors:\n                    key = frozenset(log_factor._pyro_dims)\n                    if queries.get(key, False) is None:\n                        queries[key] = log_factor\n                # Ensure a query exists for each cost term.\n                for cost in cost_terms:\n                    key = frozenset(cost._pyro_dims)\n                    if queries[key] is None:\n                        query = torch.zeros_like(cost)\n                        query._pyro_dims = cost._pyro_dims\n                        log_factors.append(query)\n                        queries[key] = query\n\n                # Perform sum-product contraction. Note that plates never need to be\n                # product-contracted due to our plate-based dependency ordering.\n                sum_dims = set().union(*(x._pyro_dims for x in log_factors)) - ordinal\n                for query in queries.values():\n                    require_backward(query)\n                root = ring.sumproduct(log_factors, sum_dims)\n                root._pyro_backward()\n                probs = {key: query._pyro_backward_result.exp() for key, query in queries.items()}\n\n                # Aggregate prob * cost terms.\n                for cost in cost_terms:\n                    key = frozenset(cost._pyro_dims)\n                    prob = probs[key]\n                    prob._pyro_dims = queries[key]._pyro_dims\n                    mask = prob > 0\n                    if torch._C._get_tracing_state() or not mask.all():\n                        mask._pyro_dims = prob._pyro_dims\n                        cost, prob, mask = packed.broadcast_all(cost, prob, mask)\n                        prob = prob.masked_select(mask)\n                        cost = cost.masked_select(mask)\n                    else:\n                        cost, prob = packed.broadcast_all(cost, prob)\n                    expected_cost = expected_cost + scale * torch.tensordot(prob, cost, prob.dim())\n\n        LAST_CACHE_SIZE[0] = count_cached_ops(cache)\n        return expected_cost\n\n\ndef check_fully_reparametrized(guide_site):\n    log_prob, score_function_term, entropy_term = guide_site[""score_parts""]\n    fully_rep = (guide_site[""fn""].has_rsample and not is_identically_zero(entropy_term) and\n                 is_identically_zero(score_function_term))\n    if not fully_rep:\n        raise NotImplementedError(""All distributions in the guide must be fully reparameterized."")\n'"
pyro/nn/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom pyro.nn.auto_reg_nn import AutoRegressiveNN, ConditionalAutoRegressiveNN, MaskedLinear\nfrom pyro.nn.dense_nn import ConditionalDenseNN, DenseNN\nfrom pyro.nn.module import PyroModule, PyroParam, PyroSample, pyro_method\n\n__all__ = [\n    ""AutoRegressiveNN"",\n    ""ConditionalAutoRegressiveNN"",\n    ""ConditionalDenseNN"",\n    ""DenseNN"",\n    ""MaskedLinear"",\n    ""PyroModule"",\n    ""PyroParam"",\n    ""PyroSample"",\n    ""pyro_method"",\n]\n'"
pyro/nn/auto_reg_nn.py,26,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\ndef sample_mask_indices(input_dim, hidden_dim, simple=True):\n    """"""\n    Samples the indices assigned to hidden units during the construction of MADE masks\n\n    :param input_dim: the dimensionality of the input variable\n    :type input_dim: int\n    :param hidden_dim: the dimensionality of the hidden layer\n    :type hidden_dim: int\n    :param simple: True to space fractional indices by rounding to nearest int, false round randomly\n    :type simple: bool\n    """"""\n    indices = torch.linspace(1, input_dim, steps=hidden_dim, device=\'cpu\').to(torch.Tensor().device)\n    if simple:\n        # Simple procedure tries to space fractional indices evenly by rounding to nearest int\n        return torch.round(indices)\n    else:\n        # ""Non-simple"" procedure creates fractional indices evenly then rounds at random\n        ints = indices.floor()\n        ints += torch.bernoulli(indices - ints)\n        return ints\n\n\ndef create_mask(input_dim, context_dim, hidden_dims, permutation, output_dim_multiplier):\n    """"""\n    Creates MADE masks for a conditional distribution\n\n    :param input_dim: the dimensionality of the input variable\n    :type input_dim: int\n    :param context_dim: the dimensionality of the variable that is conditioned on (for conditional densities)\n    :type context_dim: int\n    :param hidden_dims: the dimensionality of the hidden layers(s)\n    :type hidden_dims: list[int]\n    :param permutation: the order of the input variables\n    :type permutation: torch.LongTensor\n    :param output_dim_multiplier: tiles the output (e.g. for when a separate mean and scale parameter are desired)\n    :type output_dim_multiplier: int\n    """"""\n    # Create mask indices for input, hidden layers, and final layer\n    # We use 0 to refer to the elements of the variable being conditioned on,\n    # and range(1:(D_latent+1)) for the input variable\n    var_index = torch.empty(permutation.shape, dtype=torch.get_default_dtype())\n    var_index[permutation] = torch.arange(input_dim, dtype=torch.get_default_dtype())\n\n    # Create the indices that are assigned to the neurons\n    input_indices = torch.cat((torch.zeros(context_dim), 1 + var_index))\n\n    # For conditional MADE, introduce a 0 index that all the conditioned variables are connected to\n    # as per Paige and Wood (2016) (see below)\n    if context_dim > 0:\n        hidden_indices = [sample_mask_indices(input_dim, h) - 1 for h in hidden_dims]\n    else:\n        hidden_indices = [sample_mask_indices(input_dim - 1, h) for h in hidden_dims]\n\n    output_indices = (var_index + 1).repeat(output_dim_multiplier)\n\n    # Create mask from input to output for the skips connections\n    mask_skip = (output_indices.unsqueeze(-1) > input_indices.unsqueeze(0)).type_as(var_index)\n\n    # Create mask from input to first hidden layer, and between subsequent hidden layers\n    masks = [(hidden_indices[0].unsqueeze(-1) >= input_indices.unsqueeze(0)).type_as(var_index)]\n    for i in range(1, len(hidden_dims)):\n        masks.append((hidden_indices[i].unsqueeze(-1) >= hidden_indices[i - 1].unsqueeze(0)).type_as(var_index))\n\n    # Create mask from last hidden layer to output layer\n    masks.append((output_indices.unsqueeze(-1) > hidden_indices[-1].unsqueeze(0)).type_as(var_index))\n\n    return masks, mask_skip\n\n\nclass MaskedLinear(nn.Linear):\n    """"""\n    A linear mapping with a given mask on the weights (arbitrary bias)\n\n    :param in_features: the number of input features\n    :type in_features: int\n    :param out_features: the number of output features\n    :type out_features: int\n    :param mask: the mask to apply to the in_features x out_features weight matrix\n    :type mask: torch.Tensor\n    :param bias: whether or not `MaskedLinear` should include a bias term. defaults to `True`\n    :type bias: bool\n    """"""\n\n    def __init__(self, in_features, out_features, mask, bias=True):\n        super().__init__(in_features, out_features, bias)\n        self.register_buffer(\'mask\', mask.data)\n\n    def forward(self, _input):\n        """"""\n        the forward method that does the masked linear computation and returns the result\n        """"""\n        masked_weight = self.weight * self.mask\n        return F.linear(_input, masked_weight, self.bias)\n\n\nclass ConditionalAutoRegressiveNN(nn.Module):\n    """"""\n    An implementation of a MADE-like auto-regressive neural network that can input an additional context variable.\n    (See Reference [2] Section 3.3 for an explanation of how the conditional MADE architecture works.)\n\n    Example usage:\n\n    >>> x = torch.randn(100, 10)\n    >>> y = torch.randn(100, 5)\n    >>> arn = ConditionalAutoRegressiveNN(10, 5, [50], param_dims=[1])\n    >>> p = arn(x, context=y)  # 1 parameters of size (100, 10)\n    >>> arn = ConditionalAutoRegressiveNN(10, 5, [50], param_dims=[1, 1])\n    >>> m, s = arn(x, context=y) # 2 parameters of size (100, 10)\n    >>> arn = ConditionalAutoRegressiveNN(10, 5, [50], param_dims=[1, 5, 3])\n    >>> a, b, c = arn(x, context=y) # 3 parameters of sizes, (100, 1, 10), (100, 5, 10), (100, 3, 10)\n\n    :param input_dim: the dimensionality of the input variable\n    :type input_dim: int\n    :param context_dim: the dimensionality of the context variable\n    :type context_dim: int\n    :param hidden_dims: the dimensionality of the hidden units per layer\n    :type hidden_dims: list[int]\n    :param param_dims: shape the output into parameters of dimension (p_n, input_dim) for p_n in param_dims\n        when p_n > 1 and dimension (input_dim) when p_n == 1. The default is [1, 1], i.e. output two parameters\n        of dimension (input_dim), which is useful for inverse autoregressive flow.\n    :type param_dims: list[int]\n    :param permutation: an optional permutation that is applied to the inputs and controls the order of the\n        autoregressive factorization. in particular for the identity permutation the autoregressive structure\n        is such that the Jacobian is upper triangular. By default this is chosen at random.\n    :type permutation: torch.LongTensor\n    :param skip_connections: Whether to add skip connections from the input to the output.\n    :type skip_connections: bool\n    :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no\n        nonlinearity is applied to the final network output, so the output is an unbounded real number.\n    :type nonlinearity: torch.nn.module\n\n    Reference:\n\n    1. MADE: Masked Autoencoder for Distribution Estimation [arXiv:1502.03509]\n    Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle\n\n    2. Inference Networks for Sequential Monte Carlo in Graphical Models [arXiv:1602.06701]\n    Brooks Paige, Frank Wood\n\n    """"""\n\n    def __init__(\n            self,\n            input_dim,\n            context_dim,\n            hidden_dims,\n            param_dims=[1, 1],\n            permutation=None,\n            skip_connections=False,\n            nonlinearity=nn.ReLU()):\n        super().__init__()\n        if input_dim == 1:\n            warnings.warn(\'ConditionalAutoRegressiveNN input_dim = 1. Consider using an affine transformation instead.\')\n        self.input_dim = input_dim\n        self.context_dim = context_dim\n        self.hidden_dims = hidden_dims\n        self.param_dims = param_dims\n        self.count_params = len(param_dims)\n        self.output_multiplier = sum(param_dims)\n        self.all_ones = (torch.tensor(param_dims) == 1).all().item()\n\n        # Calculate the indices on the output corresponding to each parameter\n        ends = torch.cumsum(torch.tensor(param_dims), dim=0)\n        starts = torch.cat((torch.zeros(1).type_as(ends), ends[:-1]))\n        self.param_slices = [slice(s.item(), e.item()) for s, e in zip(starts, ends)]\n\n        # Hidden dimension must be not less than the input otherwise it isn\'t\n        # possible to connect to the outputs correctly\n        for h in hidden_dims:\n            if h < input_dim:\n                raise ValueError(\'Hidden dimension must not be less than input dimension.\')\n\n        if permutation is None:\n            # By default set a random permutation of variables, which is important for performance with multiple steps\n            P = torch.randperm(input_dim, device=\'cpu\').to(torch.Tensor().device)\n        else:\n            # The permutation is chosen by the user\n            P = permutation.type(dtype=torch.int64)\n        self.register_buffer(\'permutation\', P)\n\n        # Create masks\n        self.masks, self.mask_skip = create_mask(\n            input_dim=input_dim, context_dim=context_dim, hidden_dims=hidden_dims, permutation=self.permutation,\n            output_dim_multiplier=self.output_multiplier)\n\n        # Create masked layers\n        layers = [MaskedLinear(input_dim + context_dim, hidden_dims[0], self.masks[0])]\n        for i in range(1, len(hidden_dims)):\n            layers.append(MaskedLinear(hidden_dims[i - 1], hidden_dims[i], self.masks[i]))\n        layers.append(MaskedLinear(hidden_dims[-1], input_dim * self.output_multiplier, self.masks[-1]))\n        self.layers = nn.ModuleList(layers)\n\n        if skip_connections:\n            self.skip_layer = MaskedLinear(\n                input_dim +\n                context_dim,\n                input_dim *\n                self.output_multiplier,\n                self.mask_skip,\n                bias=False)\n        else:\n            self.skip_layer = None\n\n        # Save the nonlinearity\n        self.f = nonlinearity\n\n    def get_permutation(self):\n        """"""\n        Get the permutation applied to the inputs (by default this is chosen at random)\n        """"""\n        return self.permutation\n\n    def forward(self, x, context=None):\n        """"""\n        The forward method\n        """"""\n        # We must be able to broadcast the size of the context over the input\n        if context is None:\n            context = self.context\n\n        context = context.expand(x.size()[:-1] + (context.size(-1),))\n        x = torch.cat([context, x], dim=-1)\n        return self._forward(x)\n\n    def _forward(self, x):\n        h = x\n        for layer in self.layers[:-1]:\n            h = self.f(layer(h))\n        h = self.layers[-1](h)\n\n        if self.skip_layer is not None:\n            h = h + self.skip_layer(x)\n\n        # Shape the output, squeezing the parameter dimension if all ones\n        if self.output_multiplier == 1:\n            return h\n        else:\n            h = h.reshape(list(x.size()[:-1]) + [self.output_multiplier, self.input_dim])\n\n            # Squeeze dimension if all parameters are one dimensional\n            if self.count_params == 1:\n                return h\n\n            elif self.all_ones:\n                return torch.unbind(h, dim=-2)\n\n            # If not all ones, then probably don\'t want to squeeze a single dimension parameter\n            else:\n                return tuple([h[..., s, :] for s in self.param_slices])\n\n\nclass AutoRegressiveNN(ConditionalAutoRegressiveNN):\n    """"""\n    An implementation of a MADE-like auto-regressive neural network.\n\n    Example usage:\n\n    >>> x = torch.randn(100, 10)\n    >>> arn = AutoRegressiveNN(10, [50], param_dims=[1])\n    >>> p = arn(x)  # 1 parameters of size (100, 10)\n    >>> arn = AutoRegressiveNN(10, [50], param_dims=[1, 1])\n    >>> m, s = arn(x) # 2 parameters of size (100, 10)\n    >>> arn = AutoRegressiveNN(10, [50], param_dims=[1, 5, 3])\n    >>> a, b, c = arn(x) # 3 parameters of sizes, (100, 1, 10), (100, 5, 10), (100, 3, 10)\n\n    :param input_dim: the dimensionality of the input variable\n    :type input_dim: int\n    :param hidden_dims: the dimensionality of the hidden units per layer\n    :type hidden_dims: list[int]\n    :param param_dims: shape the output into parameters of dimension (p_n, input_dim) for p_n in param_dims\n        when p_n > 1 and dimension (input_dim) when p_n == 1. The default is [1, 1], i.e. output two parameters\n        of dimension (input_dim), which is useful for inverse autoregressive flow.\n    :type param_dims: list[int]\n    :param permutation: an optional permutation that is applied to the inputs and controls the order of the\n        autoregressive factorization. in particular for the identity permutation the autoregressive structure\n        is such that the Jacobian is upper triangular. By default this is chosen at random.\n    :type permutation: torch.LongTensor\n    :param skip_connections: Whether to add skip connections from the input to the output.\n    :type skip_connections: bool\n    :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no\n        nonlinearity is applied to the final network output, so the output is an unbounded real number.\n    :type nonlinearity: torch.nn.module\n\n    Reference:\n\n    MADE: Masked Autoencoder for Distribution Estimation [arXiv:1502.03509]\n    Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle\n\n    """"""\n\n    def __init__(\n            self,\n            input_dim,\n            hidden_dims,\n            param_dims=[1, 1],\n            permutation=None,\n            skip_connections=False,\n            nonlinearity=nn.ReLU()):\n        super(\n            AutoRegressiveNN,\n            self).__init__(\n            input_dim,\n            0,\n            hidden_dims,\n            param_dims=param_dims,\n            permutation=permutation,\n            skip_connections=skip_connections,\n            nonlinearity=nonlinearity)\n\n    def forward(self, x):\n        """"""\n        The forward method\n        """"""\n        return self._forward(x)\n'"
pyro/nn/dense_nn.py,17,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\n\nclass ConditionalDenseNN(torch.nn.Module):\n    """"""\n    An implementation of a simple dense feedforward network taking a context variable, for use in, e.g.,\n    some conditional flows such as :class:`pyro.distributions.transforms.ConditionalAffineCoupling`.\n\n    Example usage:\n\n    >>> input_dim = 10\n    >>> context_dim = 5\n    >>> x = torch.rand(100, input_dim)\n    >>> z = torch.rand(100, context_dim)\n    >>> nn = ConditionalDenseNN(input_dim, context_dim, [50], param_dims=[1, input_dim, input_dim])\n    >>> a, b, c = nn(x, context=z)  # parameters of size (100, 1), (100, 10), (100, 10)\n\n    :param input_dim: the dimensionality of the input\n    :type input_dim: int\n    :param context_dim: the dimensionality of the context variable\n    :type context_dim: int\n    :param hidden_dims: the dimensionality of the hidden units per layer\n    :type hidden_dims: list[int]\n    :param param_dims: shape the output into parameters of dimension (p_n,) for p_n in param_dims\n        when p_n > 1 and dimension () when p_n == 1. The default is [1, 1], i.e. output two parameters of dimension ().\n    :type param_dims: list[int]\n    :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no\n        nonlinearity is applied to the final network output, so the output is an unbounded real number.\n    :type nonlinearity: torch.nn.Module\n\n    """"""\n\n    def __init__(\n            self,\n            input_dim,\n            context_dim,\n            hidden_dims,\n            param_dims=[1, 1],\n            nonlinearity=torch.nn.ReLU()):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.context_dim = context_dim\n        self.hidden_dims = hidden_dims\n        self.param_dims = param_dims\n        self.count_params = len(param_dims)\n        self.output_multiplier = sum(param_dims)\n\n        # Calculate the indices on the output corresponding to each parameter\n        ends = torch.cumsum(torch.tensor(param_dims), dim=0)\n        starts = torch.cat((torch.zeros(1).type_as(ends), ends[:-1]))\n        self.param_slices = [slice(s.item(), e.item()) for s, e in zip(starts, ends)]\n\n        # Create masked layers\n        layers = [torch.nn.Linear(input_dim+context_dim, hidden_dims[0])]\n        for i in range(1, len(hidden_dims)):\n            layers.append(torch.nn.Linear(hidden_dims[i - 1], hidden_dims[i]))\n        layers.append(torch.nn.Linear(hidden_dims[-1], self.output_multiplier))\n        self.layers = torch.nn.ModuleList(layers)\n\n        # Save the nonlinearity\n        self.f = nonlinearity\n\n    def forward(self, x, context):\n        """"""\n        The forward method\n        """"""\n        # We must be able to broadcast the size of the context over the input\n        context = context.expand(x.size()[:-1]+(context.size(-1),))\n\n        x = torch.cat([context, x], dim=-1)\n        return self._forward(x)\n\n    def _forward(self, x):\n        """"""\n        The forward method\n        """"""\n        h = x\n        for layer in self.layers[:-1]:\n            h = self.f(layer(h))\n        h = self.layers[-1](h)\n\n        # Shape the output, squeezing the parameter dimension if all ones\n        if self.output_multiplier == 1:\n            return h\n        else:\n            h = h.reshape(list(x.size()[:-1]) + [self.output_multiplier])\n\n            if self.count_params == 1:\n                return h\n\n            else:\n                return tuple([h[..., s] for s in self.param_slices])\n\n\nclass DenseNN(ConditionalDenseNN):\n    """"""\n    An implementation of a simple dense feedforward network, for use in, e.g., some conditional flows such as\n    :class:`pyro.distributions.transforms.ConditionalPlanarFlow` and other unconditional flows such as\n    :class:`pyro.distributions.transforms.AffineCoupling` that do not require an autoregressive network.\n\n    Example usage:\n\n    >>> input_dim = 10\n    >>> context_dim = 5\n    >>> z = torch.rand(100, context_dim)\n    >>> nn = DenseNN(context_dim, [50], param_dims=[1, input_dim, input_dim])\n    >>> a, b, c = nn(z)  # parameters of size (100, 1), (100, 10), (100, 10)\n\n    :param input_dim: the dimensionality of the input\n    :type input_dim: int\n    :param hidden_dims: the dimensionality of the hidden units per layer\n    :type hidden_dims: list[int]\n    :param param_dims: shape the output into parameters of dimension (p_n,) for p_n in param_dims\n        when p_n > 1 and dimension () when p_n == 1. The default is [1, 1], i.e. output two parameters of dimension ().\n    :type param_dims: list[int]\n    :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no\n        nonlinearity is applied to the final network output, so the output is an unbounded real number.\n    :type nonlinearity: torch.nn.module\n\n    """"""\n\n    def __init__(\n            self,\n            input_dim,\n            hidden_dims,\n            param_dims=[1, 1],\n            nonlinearity=torch.nn.ReLU()):\n        super(DenseNN, self).__init__(\n            input_dim,\n            0,\n            hidden_dims,\n            param_dims=param_dims,\n            nonlinearity=nonlinearity\n        )\n\n    def forward(self, x):\n        """"""\n        The forward method\n        """"""\n        return self._forward(x)\n'"
pyro/nn/module.py,42,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nPyro includes a class :class:`~pyro.nn.module.PyroModule`, a subclass of\n:class:`torch.nn.Module`, whose attributes can be modified by Pyro effects.  To\ncreate a poutine-aware attribute, use either the :class:`PyroParam` struct or\nthe :class:`PyroSample` struct::\n\n    my_module = PyroModule()\n    my_module.x = PyroParam(torch.tensor(1.), constraint=constraints.positive)\n    my_module.y = PyroSample(dist.Normal(0, 1))\n\n""""""\nimport functools\nimport inspect\nfrom collections import OrderedDict, namedtuple\n\nimport torch\nfrom torch.distributions import constraints, transform_to\n\nimport pyro\nfrom pyro.poutine.runtime import _PYRO_PARAM_STORE\n\n\nclass PyroParam(namedtuple(""PyroParam"", (""init_value"", ""constraint"", ""event_dim""))):\n    """"""\n    Declares a Pyro-managed learnable attribute of a :class:`PyroModule`,\n    similar to :func:`pyro.param <pyro.primitives.param>`.\n\n    This can be used either to set attributes of :class:`PyroModule`\n    instances::\n\n        assert isinstance(my_module, PyroModule)\n        my_module.x = PyroParam(torch.zeros(4))                   # eager\n        my_module.y = PyroParam(lambda: torch.randn(4))           # lazy\n        my_module.z = PyroParam(torch.ones(4),                    # eager\n                                constraint=constraints.positive,\n                                event_dim=1)\n\n    or EXPERIMENTALLY as a decorator on lazy initialization properties::\n\n        class MyModule(PyroModule):\n            @PyroParam\n            def x(self):\n                return torch.zeros(4)\n\n            @PyroParam\n            def y(self):\n                return torch.randn(4)\n\n            @PyroParam(constraint=constraints.real, event_dim=1)\n            def z(self):\n                return torch.ones(4)\n\n            def forward(self):\n                return self.x + self.y + self.z  # accessed like a @property\n\n    :param init_value: Either a tensor for eager initialization, a callable for\n        lazy initialization, or None for use as a decorator.\n    :type init_value: torch.Tensor or callable returning a torch.Tensor or None\n    :param constraint: torch constraint, defaults to ``constraints.real``.\n    :type constraint: ~torch.distributions.constraints.Constraint\n    :param int event_dim: (optional) number of rightmost dimensions unrelated\n        to baching. Dimension to the left of this will be considered batch\n        dimensions; if the param statement is inside a subsampled plate, then\n        corresponding batch dimensions of the parameter will be correspondingly\n        subsampled. If unspecified, all dimensions will be considered event\n        dims and no subsampling will be performed.\n    """"""\n    # Support use as a decorator.\n    def __get__(self, obj, obj_type):\n        assert issubclass(obj_type, PyroModule)\n        if obj is None:\n            return self\n\n        name = self.init_value.__name__\n        if name not in obj.__dict__[""_pyro_params""]:\n            init_value, constraint, event_dim = self\n            init_value = functools.partial(init_value, obj)  # bind method\'s self arg\n            setattr(obj, name, PyroParam(init_value, constraint, event_dim))\n        return obj.__getattr__(name)\n\n    # Support decoration with optional kwargs, e.g. @PyroParam(event_dim=0).\n    def __call__(self, init_value):\n        assert self.init_value is None\n        return PyroParam(init_value, self.constraint, self.event_dim)\n\n\nPyroParam.__new__.__defaults__ = (None, constraints.real, None)\n\n\nclass PyroSample(namedtuple(""PyroSample"", (""prior"",))):\n    """"""\n    Declares a Pyro-managed random attribute of a :class:`PyroModule`, similar\n    to :func:`pyro.sample <pyro.primitives.sample>`.\n\n    This can be used either to set attributes of :class:`PyroModule`\n    instances::\n\n        assert isinstance(my_module, PyroModule)\n        my_module.x = PyroSample(Normal(0, 1))                    # independent\n        my_module.y = PyroSample(lambda self: Normal(self.x, 1))  # dependent\n\n    or EXPERIMENTALLY as a decorator on lazy initialization methods::\n\n        class MyModule(PyroModule):\n            @PyroSample\n            def x(self):\n                return Normal(0, 1)       # independent\n\n            @PyroSample\n            def y(self):\n                return Normal(self.x, 1)  # dependent\n\n            def forward(self):\n                return self.y             # accessed like a @property\n\n    :param prior: distribution object or function that inputs the\n        :class:`PyroModule` instance ``self`` and returns a distribution\n        object.\n    """"""\n    def __init__(self, prior):\n        super().__init__()\n        if not hasattr(prior, ""sample""):  # if not a distribution\n            assert 1 == sum(1 for p in inspect.signature(prior).parameters.values()\n                            if p.default is inspect.Parameter.empty), \\\n                ""prior should take the single argument \'self\'""\n            self.name = getattr(prior, ""__name__"", None)\n            if self.name is not None:\n                # Ensure decorated function is accessible for pickling.\n                prior.__name__ = ""_pyro_prior_"" + prior.__name__\n                qualname = prior.__qualname__.rsplit(""."", 1)\n                qualname[-1] = prior.__name__\n                prior.__qualname__ = ""."".join(qualname)\n\n    # Support use as a decorator.\n    def __get__(self, obj, obj_type):\n        assert issubclass(obj_type, PyroModule)\n        if obj is None:\n            return self\n\n        if self.name is None:\n            for name in dir(obj_type):\n                if getattr(obj_type, name) is self:\n                    self.name = name\n                    break\n        else:\n            setattr(obj_type, self.prior.__name__, self.prior)  # for pickling\n\n        obj.__dict__[""_pyro_samples""].setdefault(self.name, self.prior)\n        return obj.__getattr__(self.name)\n\n\ndef _make_name(prefix, name):\n    return ""{}.{}"".format(prefix, name) if prefix else name\n\n\ndef _unconstrain(constrained_value, constraint):\n    with torch.no_grad():\n        if callable(constrained_value):\n            constrained_value = constrained_value()\n        unconstrained_value = transform_to(constraint).inv(constrained_value.detach())\n        return torch.nn.Parameter(unconstrained_value)\n\n\nclass _Context:\n    """"""\n    Sometimes-active cache for ``PyroModule.__call__()`` contexts.\n    """"""\n    def __init__(self):\n        self.active = 0\n        self.cache = {}\n        self.used = False\n\n    def __enter__(self):\n        self.active += 1\n        self.used = True\n\n    def __exit__(self, type, value, traceback):\n        self.active -= 1\n        if not self.active:\n            self.cache.clear()\n\n    def get(self, name):\n        if self.active:\n            return self.cache.get(name)\n\n    def set(self, name, value):\n        if self.active:\n            self.cache[name] = value\n\n\ndef _get_pyro_params(module):\n    for name in module._parameters:\n        if name.endswith(""_unconstrained""):\n            constrained_name = name[:-len(""_unconstrained"")]\n            if isinstance(module, PyroModule) and constrained_name in module._pyro_params:\n                yield constrained_name, getattr(module, constrained_name)\n                continue\n        yield name, module._parameters[name]\n\n\nclass _PyroModuleMeta(type):\n    _pyro_mixin_cache = {}\n\n    # Unpickling helper to create an empty object of type PyroModule[Module].\n    class _New:\n        def __init__(self, Module):\n            self.__class__ = PyroModule[Module]\n\n    def __getitem__(cls, Module):\n        assert isinstance(Module, type)\n        assert issubclass(Module, torch.nn.Module)\n        if issubclass(Module, PyroModule):\n            return Module\n        if Module is torch.nn.Module:\n            return PyroModule\n        if Module in _PyroModuleMeta._pyro_mixin_cache:\n            return _PyroModuleMeta._pyro_mixin_cache[Module]\n        bases = [PyroModule[b] for b in Module.__bases__\n                 if issubclass(b, torch.nn.Module)]\n\n        class result(Module, *bases):\n            # Unpickling helper to load an object of type PyroModule[Module].\n            def __reduce__(self):\n                state = getattr(self, \'__getstate__\', self.__dict__.copy)()\n                return _PyroModuleMeta._New, (Module,), state\n\n        result.__name__ = ""Pyro"" + Module.__name__\n        _PyroModuleMeta._pyro_mixin_cache[Module] = result\n        return result\n\n\nclass PyroModule(torch.nn.Module, metaclass=_PyroModuleMeta):\n    """"""\n    Subclass of :class:`torch.nn.Module` whose attributes can be modified by\n    Pyro effects. Attributes can be set using helpers :class:`PyroParam` and\n    :class:`PyroSample` , and methods can be decorated by :func:`pyro_method` .\n\n    **Parameters**\n\n    To create a Pyro-managed parameter attribute, set that attribute using\n    either :class:`torch.nn.Parameter` (for unconstrained parameters) or\n    :class:`PyroParam` (for constrained parameters). Reading that attribute\n    will then trigger a :func:`pyro.param <pyro.primitives.param>` statement.\n    For example::\n\n        # Create Pyro-managed parameter attributes.\n        my_module = PyroModule()\n        my_module.loc = nn.Parameter(torch.tensor(0.))\n        my_module.scale = PyroParam(torch.tensor(1.),\n                                    constraint=constraints.positive)\n        # Read the attributes.\n        loc = my_module.loc  # Triggers a pyro.param statement.\n        scale = my_module.scale  # Triggers another pyro.param statement.\n\n    Note that, unlike normal :class:`torch.nn.Module` s, :class:`PyroModule` s\n    should not be registered with :func:`pyro.module <pyro.primitives.module>`\n    statements.  :class:`PyroModule` s can contain other :class:`PyroModule` s\n    and normal :class:`torch.nn.Module` s.  Accessing a normal\n    :class:`torch.nn.Module` attribute of a :class:`PyroModule` triggers a\n    :func:`pyro.module <pyro.primitives.module>` statement.  If multiple\n    :class:`PyroModule` s appear in a single Pyro model or guide, they should\n    be included in a single root :class:`PyroModule` for that model.\n\n    :class:`PyroModule` s synchronize data with the param store at each\n    ``setattr``, ``getattr``, and ``delattr`` event, based on the nested name\n    of an attribute:\n\n    -   Setting ``mod.x = x_init`` tries to read ``x`` from the param store. If a\n        value is found in the param store, that value is copied into ``mod``\n        and ``x_init`` is ignored; otherwise ``x_init`` is copied into both\n        ``mod`` and the param store.\n    -   Reading ``mod.x`` tries to read ``x`` from the param store. If a\n        value is found in the param store, that value is copied into ``mod``;\n        otherwise ``mod``\'s value is copied into the param store. Finally\n        ``mod`` and the param store agree on a single value to return.\n    -   Deleting ``del mod.x`` removes a value from both ``mod`` and the param\n        store.\n\n    Note two :class:`PyroModule` of the same name will both synchronize with\n    the global param store and thus contain the same data.  When creating a\n    :class:`PyroModule`, then deleting it, then creating another with the same\n    name, the latter will be populated with the former\'s data from the param\n    store. To avoid this persistence, either ``pyro.clear_param_store()`` or\n    call :func:`clear` before deleting a :class:`PyroModule` .\n\n    :class:`PyroModule` s can be saved and loaded either directly using\n    :func:`torch.save` / :func:`torch.load` or indirectly using the param\n    store\'s :meth:`~pyro.params.param_store.ParamStoreDict.save` /\n    :meth:`~pyro.params.param_store.ParamStoreDict.load` . Note that\n    :func:`torch.load` will be overridden by any values in the param store, so\n    it is safest to ``pyro.clear_param_store()`` before loading.\n\n    **Samples**\n\n    To create a Pyro-managed random attribute, set that attribute using the\n    :class:`PyroSample` helper, specifying a prior distribution. Reading that\n    attribute will then trigger a :func:`pyro.sample <pyro.primitives.sample>`\n    statement. For example::\n\n        # Create Pyro-managed random attributes.\n        my_module.x = PyroSample(dist.Normal(0, 1))\n        my_module.y = PyroSample(lambda self: dist.Normal(self.loc, self.scale))\n\n        # Sample the attributes.\n        x = my_module.x  # Triggers a pyro.sample statement.\n        y = my_module.y  # Triggers one pyro.sample + two pyro.param statements.\n\n    Sampling is cached within each invocation of ``.__call__()`` or method\n    decorated by :func:`pyro_method` .  Because sample statements can appear\n    only once in a Pyro trace, you should ensure that traced access to sample\n    attributes is wrapped in a single invocation of ``.__call__()`` or method\n    decorated by :func:`pyro_method` .\n\n    To make an existing module probabilistic, you can create a subclass and\n    overwrite some parameters with :class:`PyroSample` s::\n\n        class RandomLinear(nn.Linear, PyroModule):  # used as a mixin\n            def __init__(self, in_features, out_features):\n                super().__init__(in_features, out_features)\n                self.weight = PyroSample(\n                    lambda self: dist.Normal(0, 1)\n                                     .expand([self.out_features,\n                                              self.in_features])\n                                     .to_event(2))\n\n    **Mixin classes**\n\n    :class:`PyroModule` can be used as a mixin class, and supports simple\n    syntax for dynamically creating mixins, for example the following are\n    equivalent::\n\n        # Version 1. create a named mixin class\n        class PyroLinear(nn.Linear, PyroModule):\n            pass\n\n        m.linear = PyroLinear(m, n)\n\n        # Version 2. create a dynamic mixin class\n        m.linear = PyroModule[nn.Linear](m, n)\n\n    This notation can be used recursively to create Bayesian modules, e.g.::\n\n        model = PyroModule[nn.Sequential](\n            PyroModule[nn.Linear](28 * 28, 100),\n            PyroModule[nn.Sigmoid](),\n            PyroModule[nn.Linear](100, 100),\n            PyroModule[nn.Sigmoid](),\n            PyroModule[nn.Linear](100, 10),\n        )\n        assert isinstance(model, nn.Sequential)\n        assert isinstance(model, PyroModule)\n\n        # Now we can be Bayesian about weights in the first layer.\n        model[0].weight = PyroSample(\n            prior=dist.Normal(0, 1).expand([28 * 28, 100]).to_event(2))\n        guide = AutoDiagonalNormal(model)\n\n    Note that ``PyroModule[...]`` does not recursively mix in\n    :class:`PyroModule` to submodules of the input ``Module``; hence we needed\n    to wrap each submodule of the ``nn.Sequential`` above.\n\n    :param str name: Optional name for a root PyroModule. This is ignored in\n        sub-PyroModules of another PyroModule.\n    """"""\n    def __init__(self, name=""""):\n        self._pyro_name = name\n        self._pyro_context = _Context()  # shared among sub-PyroModules\n        self._pyro_params = OrderedDict()\n        self._pyro_samples = OrderedDict()\n        super().__init__()\n\n    def add_module(self, name, module):\n        """"""\n        Adds a child module to the current module.\n        """"""\n        if isinstance(module, PyroModule):\n            module._pyro_set_supermodule(_make_name(self._pyro_name, name), self._pyro_context)\n        super().add_module(name, module)\n\n    def named_pyro_params(self, prefix=\'\', recurse=True):\n        """"""\n        Returns an iterator over PyroModule parameters, yielding both the\n        name of the parameter as well as the parameter itself.\n\n        :param str prefix: prefix to prepend to all parameter names.\n        :param bool recurse: if True, then yields parameters of this module\n            and all submodules. Otherwise, yields only parameters that\n            are direct members of this module.\n        :returns: a generator which yields tuples containing the name and parameter\n        """"""\n        gen = self._named_members(_get_pyro_params, prefix=prefix, recurse=recurse)\n        for elem in gen:\n            yield elem\n\n    def _pyro_set_supermodule(self, name, context):\n        self._pyro_name = name\n        self._pyro_context = context\n        for key, value in self._modules.items():\n            if isinstance(value, PyroModule):\n                assert not value._pyro_context.used, \\\n                    ""submodule {} has executed outside of supermodule"".format(name)\n                value._pyro_set_supermodule(_make_name(name, key), context)\n\n    def _pyro_get_fullname(self, name):\n        assert self.__dict__[\'_pyro_context\'].used, ""fullname is not yet defined""\n        return _make_name(self.__dict__[\'_pyro_name\'], name)\n\n    def __call__(self, *args, **kwargs):\n        with self._pyro_context:\n            return super().__call__(*args, **kwargs)\n\n    def __getattr__(self, name):\n        # PyroParams trigger pyro.param statements.\n        if \'_pyro_params\' in self.__dict__:\n            _pyro_params = self.__dict__[\'_pyro_params\']\n            if name in _pyro_params:\n                constraint, event_dim = _pyro_params[name]\n                unconstrained_value = getattr(self, name + ""_unconstrained"")\n                if self._pyro_context.active:\n                    fullname = self._pyro_get_fullname(name)\n                    if fullname in _PYRO_PARAM_STORE:\n                        if _PYRO_PARAM_STORE._params[fullname] is not unconstrained_value:\n                            # Update PyroModule <--- ParamStore.\n                            unconstrained_value = _PYRO_PARAM_STORE._params[fullname]\n                            if not isinstance(unconstrained_value, torch.nn.Parameter):\n                                # Update PyroModule ---> ParamStore (type only; data is preserved).\n                                unconstrained_value = torch.nn.Parameter(unconstrained_value)\n                                _PYRO_PARAM_STORE._params[fullname] = unconstrained_value\n                                _PYRO_PARAM_STORE._param_to_name[unconstrained_value] = fullname\n                            super().__setattr__(name + ""_unconstrained"", unconstrained_value)\n                    else:\n                        # Update PyroModule ---> ParamStore.\n                        _PYRO_PARAM_STORE._constraints[fullname] = constraint\n                        _PYRO_PARAM_STORE._params[fullname] = unconstrained_value\n                        _PYRO_PARAM_STORE._param_to_name[unconstrained_value] = fullname\n                    return pyro.param(fullname, event_dim=event_dim)\n                else:  # Cannot determine supermodule and hence cannot compute fullname.\n                    return transform_to(constraint)(unconstrained_value)\n\n        # PyroSample trigger pyro.sample statements.\n        if \'_pyro_samples\' in self.__dict__:\n            _pyro_samples = self.__dict__[\'_pyro_samples\']\n            if name in _pyro_samples:\n                prior = _pyro_samples[name]\n                context = self._pyro_context\n                if context.active:\n                    fullname = self._pyro_get_fullname(name)\n                    value = context.get(fullname)\n                    if value is None:\n                        if not hasattr(prior, ""sample""):  # if not a distribution\n                            prior = prior(self)\n                        value = pyro.sample(fullname, prior)\n                        context.set(fullname, value)\n                    return value\n                else:  # Cannot determine supermodule and hence cannot compute fullname.\n                    if not hasattr(prior, ""sample""):  # if not a distribution\n                        prior = prior(self)\n                    return prior()\n\n        result = super().__getattr__(name)\n\n        # Regular nn.Parameters trigger pyro.param statements.\n        if isinstance(result, torch.nn.Parameter) and not name.endswith(""_unconstrained""):\n            if self._pyro_context.active:\n                pyro.param(self._pyro_get_fullname(name), result)\n\n        # Regular nn.Modules trigger pyro.module statements.\n        if isinstance(result, torch.nn.Module) and not isinstance(result, PyroModule):\n            if self._pyro_context.active:\n                pyro.module(self._pyro_get_fullname(name), result)\n\n        return result\n\n    def __setattr__(self, name, value):\n        if isinstance(value, PyroModule):\n            # Create a new sub PyroModule, overwriting any old value.\n            try:\n                delattr(self, name)\n            except AttributeError:\n                pass\n            self.add_module(name, value)\n            return\n\n        if isinstance(value, PyroParam):\n            # Create a new PyroParam, overwriting any old value.\n            try:\n                delattr(self, name)\n            except AttributeError:\n                pass\n            constrained_value, constraint, event_dim = value\n            self._pyro_params[name] = constraint, event_dim\n            if self._pyro_context.active:\n                fullname = self._pyro_get_fullname(name)\n                pyro.param(fullname, constrained_value, constraint=constraint, event_dim=event_dim)\n                constrained_value = pyro.param(fullname)\n                unconstrained_value = constrained_value.unconstrained()\n                if not isinstance(unconstrained_value, torch.nn.Parameter):\n                    # Update PyroModule ---> ParamStore (type only; data is preserved).\n                    unconstrained_value = torch.nn.Parameter(unconstrained_value)\n                    _PYRO_PARAM_STORE._params[fullname] = unconstrained_value\n                    _PYRO_PARAM_STORE._param_to_name[unconstrained_value] = fullname\n            else:  # Cannot determine supermodule and hence cannot compute fullname.\n                unconstrained_value = _unconstrain(constrained_value, constraint)\n            super().__setattr__(name + ""_unconstrained"", unconstrained_value)\n            return\n\n        if isinstance(value, torch.nn.Parameter):\n            # Create a new nn.Parameter, overwriting any old value.\n            try:\n                delattr(self, name)\n            except AttributeError:\n                pass\n            if self._pyro_context.active:\n                fullname = self._pyro_get_fullname(name)\n                value = pyro.param(fullname, value)\n                if not isinstance(value, torch.nn.Parameter):\n                    # Update PyroModule ---> ParamStore (type only; data is preserved).\n                    value = torch.nn.Parameter(value)\n                    _PYRO_PARAM_STORE._params[fullname] = value\n                    _PYRO_PARAM_STORE._param_to_name[value] = fullname\n            super().__setattr__(name, value)\n            return\n\n        if isinstance(value, torch.Tensor):\n            if name in self._pyro_params:\n                # Update value of an existing PyroParam.\n                constraint, event_dim = self._pyro_params[name]\n                unconstrained_value = getattr(self, name + ""_unconstrained"")\n                with torch.no_grad():\n                    unconstrained_value.data = transform_to(constraint).inv(value.detach())\n                return\n\n        if isinstance(value, PyroSample):\n            # Create a new PyroSample, overwriting any old value.\n            try:\n                delattr(self, name)\n            except AttributeError:\n                pass\n            _pyro_samples = self.__dict__[\'_pyro_samples\']\n            _pyro_samples[name] = value.prior\n            return\n\n        super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        if name in self._parameters:\n            del self._parameters[name]\n            if self._pyro_context.used:\n                fullname = self._pyro_get_fullname(name)\n                if fullname in _PYRO_PARAM_STORE:\n                    # Update PyroModule ---> ParamStore.\n                    del _PYRO_PARAM_STORE[fullname]\n            return\n\n        if name in self._pyro_params:\n            delattr(self, name + ""_unconstrained"")\n            del self._pyro_params[name]\n            if self._pyro_context.used:\n                fullname = self._pyro_get_fullname(name)\n                if fullname in _PYRO_PARAM_STORE:\n                    # Update PyroModule ---> ParamStore.\n                    del _PYRO_PARAM_STORE[fullname]\n            return\n\n        if name in self._pyro_samples:\n            del self._pyro_samples[name]\n            return\n\n        if name in self._modules:\n            del self._modules[name]\n            if self._pyro_context.used:\n                fullname = self._pyro_get_fullname(name)\n                for p in list(_PYRO_PARAM_STORE.keys()):\n                    if p.startswith(fullname):\n                        del _PYRO_PARAM_STORE[p]\n            return\n\n        super().__delattr__(name)\n\n\ndef pyro_method(fn):\n    """"""\n    Decorator for top-level methods of a :class:`PyroModule` to enable pyro\n    effects and cache ``pyro.sample`` statements.\n\n    This should be applied to all public methods that read Pyro-managed\n    attributes, but is not needed for ``.forward()``.\n    """"""\n\n    @functools.wraps(fn)\n    def cached_fn(self, *args, **kwargs):\n        with self._pyro_context:\n            return fn(self, *args, **kwargs)\n\n    return cached_fn\n\n\ndef clear(mod):\n    """"""\n    Removes data from both a :class:`PyroModule` and the param store.\n\n    :param PyroModule mod: A module to clear.\n    """"""\n    assert isinstance(mod, PyroModule)\n    for name in list(mod._pyro_params):\n        delattr(mod, name)\n    for name in list(mod._parameters):\n        delattr(mod, name)\n    for name in list(mod._modules):\n        delattr(mod, name)\n\n\ndef to_pyro_module_(m, recurse=True):\n    """"""\n    Converts an ordinary :class:`torch.nn.Module` instance to a\n    :class:`PyroModule` **in-place**.\n\n    This is useful for adding Pyro effects to third-party modules: no\n    third-party code needs to be modified. For example::\n\n        model = nn.Sequential(\n            nn.Linear(28 * 28, 100),\n            nn.Sigmoid(),\n            nn.Linear(100, 100),\n            nn.Sigmoid(),\n            nn.Linear(100, 10),\n        )\n        to_pyro_module_(model)\n        assert isinstance(model, PyroModule[nn.Sequential])\n        assert isinstance(model[0], PyroModule[nn.Linear])\n\n        # Now we can attempt to be fully Bayesian:\n        for m in model.modules():\n            for name, value in list(m.named_parameters(recurse=False)):\n                setattr(m, name, PyroSample(prior=dist.Normal(0, 1)\n                                                      .expand(value.shape)\n                                                      .to_event(value.dim())))\n        guide = AutoDiagonalNormal(model)\n\n    :param torch.nn.Module m: A module instance.\n    :param bool recurse: Whether to convert submodules to :class:`PyroModules` .\n    """"""\n    if not isinstance(m, torch.nn.Module):\n        raise TypeError(""Expected an nn.Module instance but got a {}"".format(type(m)))\n\n    if isinstance(m, PyroModule):\n        if recurse:\n            for name, value in list(m._modules.items()):\n                to_pyro_module_(value)\n                setattr(m, name, value)\n        return\n\n    # Change m\'s type in-place.\n    m.__class__ = PyroModule[m.__class__]\n    m._pyro_name = """"\n    m._pyro_context = _Context()\n    m._pyro_params = OrderedDict()\n    m._pyro_samples = OrderedDict()\n\n    # Reregister parameters and submodules.\n    for name, value in list(m._parameters.items()):\n        setattr(m, name, value)\n    for name, value in list(m._modules.items()):\n        if recurse:\n            to_pyro_module_(value)\n        setattr(m, name, value)\n\n\n# The following descriptor disables the ._flat_weights cache of\n# torch.nn.RNNBase, forcing recomputation on each access of the ._flat_weights\n# attribute. This is required if any attribute is set to a PyroParam or\n# PyroSample. For motivation, see https://github.com/pyro-ppl/pyro/issues/2390\nclass _FlatWeightsDescriptor:\n    def __get__(self, obj, obj_type=None):\n        if obj is None:\n            return self\n        return [getattr(obj, name) for name in obj._flat_weights_names]\n\n    def __set__(self, obj, value):\n        pass  # Ignore value.\n\n\nPyroModule[torch.nn.RNNBase]._flat_weights = _FlatWeightsDescriptor()\n'"
pyro/ops/__init__.py,0,b''
pyro/ops/arrowhead.py,8,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\n\nimport torch\n\n\nSymmArrowhead = namedtuple(""SymmArrowhead"", [""top"", ""bottom_diag""])\nTriuArrowhead = namedtuple(""TriuArrowhead"", [""top"", ""bottom_diag""])\n\n\ndef sqrt(x):\n    """"""\n    EXPERIMENTAL Computes the upper triangular square root of an\n    symmetric arrowhead matrix.\n\n    :param SymmArrowhead x: an symmetric arrowhead matrix\n    :return: the square root of `x`\n    :rtype: TriuArrowhead\n    """"""\n    assert isinstance(x, SymmArrowhead)\n    head_size = x.top.size(0)\n    if head_size == 0:\n        return TriuArrowhead(x.top, x.bottom_diag.sqrt())\n\n    A, B = x.top[:, :head_size], x.top[:, head_size:]\n    # NB: the complexity is O(N * head_size^2)\n    # ref: https://en.wikipedia.org/wiki/Schur_complement#Background\n    Dsqrt = x.bottom_diag.sqrt()\n    B_Dsqrt = B / Dsqrt.unsqueeze(-2)  # shape: head_size x N\n    schur_complement = A - B_Dsqrt.matmul(B_Dsqrt.t())  # complexity: head_size^2 x N\n    # we will decompose schur_complement to U @ U.T (so that the sqrt matrix\n    # is upper triangular) using some `flip` operators:\n    #   flip(cholesky(flip(schur_complement)))\n    top_left = torch.flip(torch.flip(schur_complement, (-2, -1)).cholesky(), (-2, -1))\n    top_right = B_Dsqrt\n    top = torch.cat([top_left, top_right], -1)\n    bottom_diag = Dsqrt\n    return TriuArrowhead(top, bottom_diag)\n\n\ndef triu_inverse(x):\n    """"""\n    EXPERIMENTAL Computes the inverse of an upper-triangular arrowhead matrix.\n\n    :param TriuArrowhead x: an upper-triangular arrowhead matrix.\n    :return: the inverse of `x`\n    :rtype: TriuArrowhead\n    """"""\n    assert isinstance(x, TriuArrowhead)\n    head_size = x.top.size(0)\n    if head_size == 0:\n        return TriuArrowhead(x.top, x.bottom_diag.reciprocal())\n\n    A, B = x.top[:, :head_size], x.top[:, head_size:]\n    B_Dinv = B / x.bottom_diag.unsqueeze(-2)\n\n    identity = torch.eye(head_size, dtype=A.dtype, device=A.device)\n    top_left = torch.triangular_solve(identity, A, upper=True)[0]\n    top_right = -top_left.matmul(B_Dinv)  # complexity: head_size^2 x N\n    top = torch.cat([top_left, top_right], -1)\n    bottom_diag = x.bottom_diag.reciprocal()\n    return TriuArrowhead(top, bottom_diag)\n\n\ndef triu_matvecmul(x, y, transpose=False):\n    """"""\n    EXPERIMENTAL Computes matrix-vector product of an upper-triangular\n    arrowhead matrix `x` and a vector `y`.\n\n    :param TriuArrowhead x: an upper-triangular arrowhead matrix.\n    :param torch.Tensor y: a 1D tensor\n    :return: matrix-vector product of `x` and `y`\n    :rtype: TriuArrowhead\n    """"""\n    assert isinstance(x, TriuArrowhead)\n    head_size = x.top.size(0)\n    if transpose:\n        z = x.top.transpose(-2, -1).matmul(y[:head_size])\n        # here we exploit the diagonal structure of the bottom right part\n        # of arrowhead_sqrt matrix; so the complexity is still O(N)\n        top = z[:head_size]\n        bottom = z[head_size:] + x.bottom_diag * y[head_size:]\n    else:\n        top = x.top.matmul(y)\n        bottom = x.bottom_diag * y[head_size:]\n    return torch.cat([top, bottom], 0)\n\n\ndef triu_gram(x):\n    """"""\n    EXPERIMENTAL Computes the gram matrix `x.T @ x` from an upper-triangular\n    arrowhead matrix `x`.\n\n    :param TriuArrowhead x: an upper-triangular arrowhead matrix.\n    :return: the square of `x`\n    :rtype: TriuArrowhead\n    """"""\n    assert isinstance(x, TriuArrowhead)\n    head_size = x.top.size(0)\n    if head_size == 0:\n        return x.bottom_diag.pow(2)\n\n    A, B = x.top[:, :head_size], x.top[:, head_size:]\n    top = A.t().matmul(x.top)\n    bottom_left = top[:, head_size:].t()\n    # the following matmul operator is O(N^2 x head_size)\n    bottom_right = B.t().matmul(B) + x.bottom_diag.pow(2).diag()\n    return torch.cat([top, torch.cat([bottom_left, bottom_right], -1)], 0)\n'"
pyro/ops/contract.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport warnings\nfrom collections import OrderedDict, defaultdict\n\nimport opt_einsum\nimport torch\nfrom opt_einsum import shared_intermediates\n\nfrom pyro.ops.rings import BACKEND_TO_RING, LogRing\nfrom pyro.util import ignore_jit_warnings\n\n\ndef _check_plates_are_sensible(output_dims, nonoutput_ordinal):\n    if output_dims and nonoutput_ordinal:\n        raise ValueError(u""It is nonsensical to preserve a plated dim without preserving ""\n                         u""all of that dim\'s plates, but found \'{}\' without \'{}\'""\n                         .format(output_dims, \',\'.join(nonoutput_ordinal)))\n\n\ndef _check_tree_structure(parent, leaf):\n    if parent == leaf:\n        raise NotImplementedError(\n            ""Expected tree-structured plate nesting, but found ""\n            ""dependencies on independent plates [{}]. ""\n            ""Try converting one of the vectorized plates to a sequential plate (but beware ""\n            ""exponential cost in the size of the sequence)""\n            .format(\', \'.join(getattr(f, \'name\', str(f)) for f in leaf)))\n\n\ndef _partition_terms(ring, terms, dims):\n    """"""\n    Given a list of terms and a set of contraction dims, partitions the terms\n    up into sets that must be contracted together. By separating these\n    components we avoid broadcasting.\n\n    This function should be deterministic and free of side effects.\n    """"""\n    # Construct a bipartite graph between terms and the dims in which they\n    # are enumerated. This conflates terms and dims (tensors and ints).\n    neighbors = OrderedDict([(t, []) for t in terms] + [(d, []) for d in sorted(dims)])\n    for term in terms:\n        for dim in term._pyro_dims:\n            if dim in dims:\n                neighbors[term].append(dim)\n                neighbors[dim].append(term)\n\n    # Partition the bipartite graph into connected components for contraction.\n    components = []\n    while neighbors:\n        v, pending = neighbors.popitem()\n        component = OrderedDict([(v, None)])  # used as an OrderedSet\n        for v in pending:\n            component[v] = None\n        while pending:\n            v = pending.pop()\n            for v in neighbors.pop(v):\n                if v not in component:\n                    component[v] = None\n                    pending.append(v)\n\n        # Split this connected component into tensors and dims.\n        component_terms = [v for v in component if isinstance(v, torch.Tensor)]\n        if component_terms:\n            component_dims = set(v for v in component if not isinstance(v, torch.Tensor))\n            components.append((component_terms, component_dims))\n    return components\n\n\ndef _contract_component(ring, tensor_tree, sum_dims, target_dims):\n    """"""\n    Contract out ``sum_dims - target_dims`` in a tree of tensors in-place, via\n    message passing. This reduces all tensors down to a single tensor in the\n    minimum plate context.\n\n    This function should be deterministic.\n    This function has side-effects: it modifies ``tensor_tree``.\n\n    :param pyro.ops.rings.Ring ring: an algebraic ring defining tensor\n        operations.\n    :param OrderedDict tensor_tree: a dictionary mapping ordinals to lists of\n        tensors. An ordinal is a frozenset of ``CondIndepStack`` frames.\n    :param set sum_dims: the complete set of sum-contractions dimensions\n        (indexed from the right). This is needed to distinguish sum-contraction\n        dimensions from product-contraction dimensions.\n    :param set target_dims: An subset of ``sum_dims`` that should be preserved\n        in the result.\n    :return: a pair ``(ordinal, tensor)``\n    :rtype: tuple of frozenset and torch.Tensor\n    """"""\n    # Group sum dims by ordinal.\n    dim_to_ordinal = {}\n    for t, terms in tensor_tree.items():\n        for term in terms:\n            for dim in sum_dims.intersection(term._pyro_dims):\n                dim_to_ordinal[dim] = dim_to_ordinal.get(dim, t) & t\n    dims_tree = defaultdict(set)\n    for dim, t in dim_to_ordinal.items():\n        dims_tree[t].add(dim)\n\n    # Recursively combine terms in different plate contexts.\n    local_terms = []\n    local_dims = target_dims.copy()\n    local_ordinal = frozenset()\n    min_ordinal = frozenset.intersection(*tensor_tree)\n    while any(dims_tree.values()):\n        # Arbitrarily deterministically choose a leaf.\n        leaf = max(tensor_tree, key=len)\n        leaf_terms = tensor_tree.pop(leaf)\n        leaf_dims = dims_tree.pop(leaf, set())\n\n        # Split terms at the current ordinal into connected components.\n        for terms, dims in _partition_terms(ring, leaf_terms, leaf_dims):\n\n            # Eliminate sum dims via a sumproduct contraction.\n            term = ring.sumproduct(terms, dims - local_dims)\n\n            # Eliminate extra plate dims via product contractions.\n            if leaf == min_ordinal:\n                parent = leaf\n            else:\n                pending_dims = sum_dims.intersection(term._pyro_dims)\n                parent = frozenset.union(*(t for t, d in dims_tree.items() if d & pending_dims))\n                _check_tree_structure(parent, leaf)\n                contract_frames = leaf - parent\n                contract_dims = dims & local_dims\n                if contract_dims:\n                    term, local_term = ring.global_local(term, contract_dims, contract_frames)\n                    local_terms.append(local_term)\n                    local_dims |= sum_dims.intersection(local_term._pyro_dims)\n                    local_ordinal |= leaf\n                else:\n                    term = ring.product(term, contract_frames)\n            tensor_tree.setdefault(parent, []).append(term)\n\n    # Extract single tensor at root ordinal.\n    assert len(tensor_tree) == 1\n    ordinal, (term,) = tensor_tree.popitem()\n    assert ordinal == min_ordinal\n\n    # Perform optional localizing pass.\n    if local_terms:\n        assert target_dims\n        local_terms.append(term)\n        term = ring.sumproduct(local_terms, local_dims - target_dims)\n        ordinal |= local_ordinal\n\n    return ordinal, term\n\n\ndef contract_tensor_tree(tensor_tree, sum_dims, cache=None, ring=None):\n    """"""\n    Contract out ``sum_dims`` in a tree of tensors via message passing.\n    This partially contracts out plate dimensions.\n\n    This function should be deterministic and free of side effects.\n\n    :param OrderedDict tensor_tree: a dictionary mapping ordinals to lists of\n        tensors. An ordinal is a frozenset of ``CondIndepStack`` frames.\n    :param set sum_dims: the complete set of sum-contractions dimensions\n        (indexed from the right). This is needed to distinguish sum-contraction\n        dimensions from product-contraction dimensions.\n    :param dict cache: an optional :func:`~opt_einsum.shared_intermediates`\n        cache.\n    :param pyro.ops.rings.Ring ring: an optional algebraic ring defining tensor\n        operations.\n    :returns: A contracted version of ``tensor_tree``\n    :rtype: OrderedDict\n    """"""\n    assert isinstance(tensor_tree, OrderedDict)\n    assert isinstance(sum_dims, set)\n\n    if ring is None:\n        ring = LogRing(cache)\n\n    ordinals = {term: t for t, terms in tensor_tree.items() for term in terms}\n    all_terms = [term for terms in tensor_tree.values() for term in terms]\n    contracted_tree = OrderedDict()\n\n    # Split this tensor tree into connected components.\n    for terms, dims in _partition_terms(ring, all_terms, sum_dims):\n        component = OrderedDict()\n        for term in terms:\n            component.setdefault(ordinals[term], []).append(term)\n\n        # Contract this connected component down to a single tensor.\n        ordinal, term = _contract_component(ring, component, dims, set())\n        contracted_tree.setdefault(ordinal, []).append(term)\n\n    return contracted_tree\n\n\ndef contract_to_tensor(tensor_tree, sum_dims, target_ordinal=None, target_dims=None,\n                       cache=None, ring=None):\n    """"""\n    Contract out ``sum_dims`` in a tree of tensors, via message\n    passing. This reduces all terms down to a single tensor in the plate\n    context specified by ``target_ordinal``, optionally preserving sum\n    dimensions ``target_dims``.\n\n    This function should be deterministic and free of side effects.\n\n    :param OrderedDict tensor_tree: a dictionary mapping ordinals to lists of\n        tensors. An ordinal is a frozenset of ``CondIndepStack`` frames.\n    :param set sum_dims: the complete set of sum-contractions dimensions\n        (indexed from the right). This is needed to distinguish sum-contraction\n        dimensions from product-contraction dimensions.\n    :param frozenset target_ordinal: An optional ordinal to which the result\n        will be contracted or broadcasted.\n    :param set target_dims: An optional subset of ``sum_dims`` that should be\n        preserved in the result.\n    :param dict cache: an optional :func:`~opt_einsum.shared_intermediates`\n        cache.\n    :param pyro.ops.rings.Ring ring: an optional algebraic ring defining tensor\n        operations.\n    :returns: a single tensor\n    :rtype: torch.Tensor\n    """"""\n    if target_ordinal is None:\n        target_ordinal = frozenset()\n    if target_dims is None:\n        target_dims = set()\n    assert isinstance(tensor_tree, OrderedDict)\n    assert isinstance(sum_dims, set)\n    assert isinstance(target_ordinal, frozenset)\n    assert isinstance(target_dims, set) and target_dims <= sum_dims\n    if ring is None:\n        ring = LogRing(cache)\n\n    ordinals = {term: t for t, terms in tensor_tree.items() for term in terms}\n    all_terms = [term for terms in tensor_tree.values() for term in terms]\n    contracted_terms = []\n\n    # Split this tensor tree into connected components.\n    modulo_total = bool(target_dims)\n    for terms, dims in _partition_terms(ring, all_terms, sum_dims):\n        if modulo_total and dims.isdisjoint(target_dims):\n            continue\n        component = OrderedDict()\n        for term in terms:\n            component.setdefault(ordinals[term], []).append(term)\n\n        # Contract this connected component down to a single tensor.\n        ordinal, term = _contract_component(ring, component, dims, target_dims & dims)\n        _check_plates_are_sensible(target_dims.intersection(term._pyro_dims),\n                                   ordinal - target_ordinal)\n\n        # Eliminate extra plate dims via product contractions.\n        contract_frames = ordinal - target_ordinal\n        if contract_frames:\n            assert not sum_dims.intersection(term._pyro_dims)\n            term = ring.product(term, contract_frames)\n\n        contracted_terms.append(term)\n\n    # Combine contracted tensors via product, then broadcast.\n    term = ring.sumproduct(contracted_terms, set())\n    assert sum_dims.intersection(term._pyro_dims) <= target_dims\n    return ring.broadcast(term, target_ordinal)\n\n\ndef einsum(equation, *operands, **kwargs):\n    """"""\n    Generalized plated sum-product algorithm via tensor variable elimination.\n\n    This generalizes :func:`~pyro.ops.einsum.contract` in two ways:\n\n    1.  Multiple outputs are allowed, and intermediate results can be shared.\n    2.  Inputs and outputs can be plated along symbols given in ``plates``;\n        reductions along ``plates`` are product reductions.\n\n    The best way to understand this function is to try the examples below,\n    which show how :func:`einsum` calls can be implemented as multiple calls\n    to :func:`~pyro.ops.einsum.contract` (which is generally more expensive).\n\n    To illustrate multiple outputs, note that the following are equivalent::\n\n        z1, z2, z3 = einsum(\'ab,bc->a,b,c\', x, y)  # multiple outputs\n\n        z1 = contract(\'ab,bc->a\', x, y)\n        z2 = contract(\'ab,bc->b\', x, y)\n        z3 = contract(\'ab,bc->c\', x, y)\n\n    To illustrate plated inputs, note that the following are equivalent::\n\n        assert len(x) == 3 and len(y) == 3\n        z = einsum(\'ab,ai,bi->b\', w, x, y, plates=\'i\')\n\n        z = contract(\'ab,a,a,a,b,b,b->b\', w, *x, *y)\n\n    When a sum dimension `a` always appears with a plate dimension `i`,\n    then `a` corresponds to a distinct symbol for each slice of `a`. Thus\n    the following are equivalent::\n\n        assert len(x) == 3 and len(y) == 3\n        z = einsum(\'ai,ai->\', x, y, plates=\'i\')\n\n        z = contract(\'a,b,c,a,b,c->\', *x, *y)\n\n    When such a sum dimension appears in the output, it must be\n    accompanied by all of its plate dimensions, e.g. the following are\n    equivalent::\n\n        assert len(x) == 3 and len(y) == 3\n        z = einsum(\'abi,abi->bi\', x, y, plates=\'i\')\n\n        z0 = contract(\'ab,ac,ad,ab,ac,ad->b\', *x, *y)\n        z1 = contract(\'ab,ac,ad,ab,ac,ad->c\', *x, *y)\n        z2 = contract(\'ab,ac,ad,ab,ac,ad->d\', *x, *y)\n        z = torch.stack([z0, z1, z2])\n\n    Note that each plate slice through the output is multilinear in all plate\n    slices through all inptus, thus e.g. batch matrix multiply would be\n    implemented *without* ``plates``, so the following are all equivalent::\n\n        xy = einsum(\'abc,acd->abd\', x, y, plates=\'\')\n        xy = torch.stack([xa.mm(ya) for xa, ya in zip(x, y)])\n        xy = torch.bmm(x, y)\n\n    Among all valid equations, some computations are polynomial in the sizes of\n    the input tensors and other computations are exponential in the sizes of\n    the input tensors. This function raises :py:class:`NotImplementedError`\n    whenever the computation is exponential.\n\n    :param str equation: An einsum equation, optionally with multiple outputs.\n    :param torch.Tensor operands: A collection of tensors.\n    :param str plates: An optional string of plate symbols.\n    :param str backend: An optional einsum backend, defaults to \'torch\'.\n    :param dict cache: An optional :func:`~opt_einsum.shared_intermediates`\n        cache.\n    :param bool modulo_total: Optionally allow einsum to arbitrarily scale\n        each result plate, which can significantly reduce computation. This is\n        safe to set whenever each result plate denotes a nonnormalized\n        probability distribution whose total is not of interest.\n    :return: a tuple of tensors of requested shape, one entry per output.\n    :rtype: tuple\n    :raises ValueError: if tensor sizes mismatch or an output requests a\n        plated dim without that dim\'s plates.\n    :raises NotImplementedError: if contraction would have cost exponential in\n        the size of any input tensor.\n    """"""\n    # Extract kwargs.\n    cache = kwargs.pop(\'cache\', None)\n    plates = kwargs.pop(\'plates\', \'\')\n    backend = kwargs.pop(\'backend\', \'torch\')\n    modulo_total = kwargs.pop(\'modulo_total\', False)\n    try:\n        Ring = BACKEND_TO_RING[backend]\n    except KeyError:\n        raise NotImplementedError(\'\\n\'.join(\n            [\'Only the following pyro backends are currently implemented:\'] +\n            list(BACKEND_TO_RING)))\n\n    # Parse generalized einsum equation.\n    if \'.\' in equation:\n        raise NotImplementedError(\'ubsersum does not yet support ellipsis notation\')\n    inputs, outputs = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n    outputs = outputs.split(\',\')\n    assert len(inputs) == len(operands)\n    assert all(isinstance(x, torch.Tensor) for x in operands)\n    if not modulo_total and any(outputs):\n        raise NotImplementedError(\'Try setting modulo_total=True and ensuring that your use case \'\n                                  \'allows an arbitrary scale factor on each result plate.\')\n    if len(operands) != len(set(operands)):\n        operands = [x[...] for x in operands]  # ensure tensors are unique\n\n    # Check sizes.\n    with ignore_jit_warnings():\n        dim_to_size = {}\n        for dims, term in zip(inputs, operands):\n            for dim, size in zip(dims, map(int, term.shape)):\n                old = dim_to_size.setdefault(dim, size)\n                if old != size:\n                    raise ValueError(u""Dimension size mismatch at dim \'{}\': {} vs {}""\n                                     .format(dim, size, old))\n\n    # Construct a tensor tree shared by all outputs.\n    tensor_tree = OrderedDict()\n    plates = frozenset(plates)\n    for dims, term in zip(inputs, operands):\n        assert len(dims) == term.dim()\n        term._pyro_dims = dims\n        ordinal = plates.intersection(dims)\n        tensor_tree.setdefault(ordinal, []).append(term)\n\n    # Compute outputs, sharing intermediate computations.\n    results = []\n    with shared_intermediates(cache) as cache:\n        ring = Ring(cache, dim_to_size=dim_to_size)\n        for output in outputs:\n            sum_dims = set(output).union(*inputs) - set(plates)\n            term = contract_to_tensor(tensor_tree, sum_dims,\n                                      target_ordinal=plates.intersection(output),\n                                      target_dims=sum_dims.intersection(output),\n                                      ring=ring)\n            if term._pyro_dims != output:\n                term = term.permute(*map(term._pyro_dims.index, output))\n                term._pyro_dims = output\n            results.append(term)\n    return tuple(results)\n\n\ndef ubersum(equation, *operands, **kwargs):\n    """"""\n    Deprecated, use :func:`einsum` instead.\n    """"""\n    warnings.warn(""\'ubersum\' is deprecated, use \'pyro.ops.contract.einsum\' instead"",\n                  DeprecationWarning)\n    if \'batch_dims\' in kwargs:\n        warnings.warn(""\'batch_dims\' is deprecated, use \'plates\' instead"",\n                      DeprecationWarning)\n        kwargs[\'plates\'] = kwargs.pop(\'batch_dims\')\n    kwargs.setdefault(\'backend\', \'pyro.ops.einsum.torch_log\')\n    return einsum(equation, *operands, **kwargs)\n\n\ndef _select(tensor, dims, indices):\n    for dim, index in zip(dims, indices):\n        tensor = tensor.select(dim, index)\n    return tensor\n\n\nclass _DimUnroller:\n    """"""\n    Object to map plated dims to collections of unrolled dims.\n\n    :param dict dim_to_ordinal: a mapping from contraction dim to the set of\n        plates over which the contraction dim is plated.\n    """"""\n    def __init__(self, dim_to_ordinal):\n        self._plates = {d: tuple(sorted(ordinal)) for d, ordinal in dim_to_ordinal.items()}\n        self._symbols = map(opt_einsum.get_symbol, itertools.count())\n        self._map = {}\n\n    def __call__(self, dim, indices):\n        """"""\n        Converts a plate dim + plate indices to a unrolled dim.\n\n        :param str dim: a plate dimension to unroll\n        :param dict indices: a mapping from plate dimension to int\n        :return: a unrolled dim\n        :rtype: str\n        """"""\n        plate = self._plates.get(dim, ())\n        index = tuple(indices[d] for d in plate)\n        key = dim, index\n        if key in self._map:\n            return self._map[key]\n        normal_dim = next(self._symbols)\n        self._map[key] = normal_dim\n        return normal_dim\n\n\ndef naive_ubersum(equation, *operands, **kwargs):\n    """"""\n    Naive reference implementation of :func:`ubersum` via unrolling.\n\n    This implementation should never raise ``NotImplementedError``.\n    This implementation should agree with :func:`ubersum` whenver\n    :func:`ubersum` does not raise ``NotImplementedError``.\n    """"""\n    # Parse equation, without loss of generality assuming a single output.\n    inputs, outputs = equation.split(\'->\')\n    outputs = outputs.split(\',\')\n    if len(outputs) > 1:\n        return tuple(naive_ubersum(inputs + \'->\' + output, *operands, **kwargs)[0]\n                     for output in outputs)\n    output, = outputs\n    inputs = inputs.split(\',\')\n    backend = kwargs.pop(\'backend\', \'pyro.ops.einsum.torch_log\')\n\n    # Split dims into plate dims, contraction dims, and dims to keep.\n    plates = set(kwargs.pop(\'plates\', \'\'))\n    if not plates:\n        result = opt_einsum.contract(equation, *operands, backend=backend)\n        return (result,)\n    output_dims = set(output)\n\n    # Collect sizes of all dimensions.\n    sizes = {}\n    for input_, operand in zip(inputs, operands):\n        for dim, size in zip(input_, operand.shape):\n            old = sizes.setdefault(dim, size)\n            if old != size:\n                raise ValueError(u""Dimension size mismatch at dim \'{}\': {} vs {}""\n                                 .format(dim, size, old))\n\n    # Compute plate context for each non-plate dim, by convention the\n    # intersection over all plate contexts of tensors in which the dim appears.\n    dim_to_ordinal = {}\n    for dims in map(set, inputs):\n        ordinal = dims & plates\n        for dim in dims - plates:\n            dim_to_ordinal[dim] = dim_to_ordinal.get(dim, ordinal) & ordinal\n    for dim in output_dims - plates:\n        _check_plates_are_sensible({dim}, dim_to_ordinal[dim] - output_dims)\n\n    # Unroll by replicating along plate dimensions.\n    unroll_dim = _DimUnroller(dim_to_ordinal)\n    flat_inputs = []\n    flat_operands = []\n    for input_, operand in zip(inputs, operands):\n        local_dims = [d for d in input_ if d in plates]\n        offsets = [input_.index(d) - len(input_) for d in local_dims]\n        for index in itertools.product(*(range(sizes[d]) for d in local_dims)):\n            flat_inputs.append(\'\'.join(unroll_dim(d, dict(zip(local_dims, index)))\n                                       for d in input_ if d not in plates))\n            flat_operands.append(_select(operand, offsets, index))\n\n    # Defer to unplated einsum.\n    result = torch.empty(torch.Size(sizes[d] for d in output),\n                         dtype=operands[0].dtype, device=operands[0].device)\n    local_dims = [d for d in output if d in plates]\n    offsets = [output.index(d) - len(output) for d in local_dims]\n    for index in itertools.product(*(range(sizes[d]) for d in local_dims)):\n        flat_output = \'\'.join(unroll_dim(d, dict(zip(local_dims, index)))\n                              for d in output if d not in plates)\n        flat_equation = \',\'.join(flat_inputs) + \'->\' + flat_output\n        flat_result = opt_einsum.contract(flat_equation, *flat_operands, backend=backend)\n        if not local_dims:\n            result = flat_result\n            break\n        _select(result, offsets, index).copy_(flat_result)\n    return (result,)\n'"
pyro/ops/dual_averaging.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n\nclass DualAveraging:\n    """"""\n    Dual Averaging is a scheme to solve convex optimization problems. It belongs\n    to a class of subgradient methods which uses subgradients to update parameters\n    (in primal space) of a model. Under some conditions, the averages of generated\n    parameters during the scheme are guaranteed to converge to an optimal value.\n    However, a counter-intuitive aspect of traditional subgradient methods is\n    ""new subgradients enter the model with decreasing weights"" (see :math:`[1]`).\n    Dual Averaging scheme solves that phenomenon by updating parameters using\n    weights equally for subgradients (which lie in a dual space), hence we have\n    the name ""dual averaging"".\n\n    This class implements a dual averaging scheme which is adapted for Markov chain\n    Monte Carlo (MCMC) algorithms. To be more precise, we will replace subgradients\n    by some statistics calculated during an MCMC trajectory. In addition,\n    introducing some free parameters such as ``t0`` and ``kappa`` is helpful and\n    still guarantees the convergence of the scheme.\n\n    References\n\n    [1] `Primal-dual subgradient methods for convex problems`,\n    Yurii Nesterov\n\n    [2] `The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo`,\n    Matthew D. Hoffman, Andrew Gelman\n\n    :param float prox_center: A ""prox-center"" parameter introduced in :math:`[1]`\n        which pulls the primal sequence towards it.\n    :param float t0: A free parameter introduced in :math:`[2]`\n        that stabilizes the initial steps of the scheme.\n    :param float kappa: A free parameter introduced in :math:`[2]`\n        that controls the weights of steps of the scheme.\n        For a small ``kappa``, the scheme will quickly forget states\n        from early steps. This should be a number in :math:`(0.5, 1]`.\n    :param float gamma: A free parameter which controls the speed\n        of the convergence of the scheme.\n    """"""\n\n    def __init__(self, prox_center=0, t0=10, kappa=0.75, gamma=0.05):\n        self.prox_center = prox_center\n        self.t0 = t0\n        self.kappa = kappa\n        self.gamma = gamma\n        self.reset()\n\n    def reset(self):\n        self._x_avg = 0  # average of primal sequence\n        self._g_avg = 0  # average of dual sequence\n        self._t = 0\n\n    def step(self, g):\n        """"""\n        Updates states of the scheme given a new statistic/subgradient ``g``.\n\n        :param float g: A statistic calculated during an MCMC trajectory or subgradient.\n        """"""\n        self._t += 1\n        # g_avg = (g_1 + ... + g_t) / t\n        self._g_avg = (1 - 1/(self._t + self.t0)) * self._g_avg + g / (self._t + self.t0)\n        # According to formula (3.4) of [1], we have\n        #     x_t = argmin{ g_avg . x + loc_t . |x - x0|^2 },\n        # where loc_t := beta_t / t, beta_t := (gamma/2) * sqrt(t)\n        self._x_t = self.prox_center - (self._t ** 0.5) / self.gamma * self._g_avg\n        # weight for the new x_t\n        weight_t = self._t ** (-self.kappa)\n        self._x_avg = (1 - weight_t) * self._x_avg + weight_t * self._x_t\n\n    def get_state(self):\n        r""""""\n        Returns the latest :math:`x_t` and average of\n        :math:`\\left\\{x_i\\right\\}_{i=1}^t` in primal space.\n        """"""\n        return self._x_t, self._x_avg\n'"
pyro/ops/gamma_gaussian.py,25,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions.utils import lazy_property\nfrom torch.nn.functional import pad\n\nfrom pyro.distributions.multivariate_studentt import MultivariateStudentT\nfrom pyro.distributions.torch import MultivariateNormal\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.ops.tensor_utils import precision_to_scale_tril\n\n\nclass Gamma:\n    """"""\n    Non-normalized Gamma distribution.\n\n        Gamma(concentration, rate) ~ (concentration - 1) * log(s) - rate * s\n    """"""\n    def __init__(self, log_normalizer, concentration, rate):\n        self.log_normalizer = log_normalizer\n        self.concentration = concentration\n        self.rate = rate\n\n    def log_density(self, s):\n        """"""\n        Non-normalized log probability of Gamma distribution.\n\n        This is mainly used for testing.\n        """"""\n        return self.log_normalizer + (self.concentration - 1) * s.log() - self.rate * s\n\n    def logsumexp(self):\n        """"""\n        Integrates out the latent variable.\n        """"""\n        return self.log_normalizer + torch.lgamma(self.concentration) - \\\n            self.concentration * self.rate.log()\n\n\nclass GammaGaussian:\n    """"""\n    Non-normalized GammaGaussian distribution:\n\n        GammaGaussian(x, s) ~ (concentration + 0.5 * dim - 1) * log(s)\n                              - rate * s - s * 0.5 * info_vec.T @ inv(precision) @ info_vec)\n                              - s * 0.5 * x.T @ precision @ x + s * x.T @ info_vec,\n\n    which will be reparameterized as\n\n        GammaGaussian(x, s) =: alpha * log(s) + s * (-0.5 * x.T @ precision @ x + x.T @ info_vec - beta).\n\n    The `s` variable plays the role of a mixing variable such that\n\n        p(x | s) ~ Gaussian(s * info_vec, s * precision).\n\n    Conditioned on `s`, this represents an arbitrary semidefinite quadratic function,\n    which can be interpreted as a rank-deficient Gaussian distribution.\n    The precision matrix may have zero eigenvalues, thus it may be impossible\n    to work directly with the covariance matrix.\n\n    :param torch.Tensor log_normalizer: a normalization constant, which is mainly used to keep\n        track of normalization terms during contractions.\n    :param torch.Tensor info_vec: information vector, which is a scaled version of the mean\n        ``info_vec = precision @ mean``. We use this represention to make gaussian contraction\n        fast and stable.\n    :param torch.Tensor precision: precision matrix of this gaussian.\n    :param torch.Tensor alpha: reparameterized shape parameter of the marginal Gamma distribution of\n        `s`. The shape parameter Gamma.concentration is reparameterized by:\n\n            alpha = Gamma.concentration + 0.5 * dim - 1\n\n    :param torch.Tensor beta: reparameterized rate parameter of the marginal Gamma distribution of\n        `s`. The rate parameter Gamma.rate is reparameterized by:\n\n            beta = Gamma.rate + 0.5 * info_vec.T @ inv(precision) @ info_vec\n    """"""\n    def __init__(self, log_normalizer, info_vec, precision, alpha, beta):\n        # NB: using info_vec instead of mean to deal with rank-deficient problem\n        assert info_vec.dim() >= 1\n        assert precision.dim() >= 2\n        assert precision.shape[-2:] == info_vec.shape[-1:] * 2\n        self.log_normalizer = log_normalizer\n        self.info_vec = info_vec\n        self.precision = precision\n        self.alpha = alpha\n        self.beta = beta\n\n    def dim(self):\n        return self.info_vec.size(-1)\n\n    @lazy_property\n    def batch_shape(self):\n        return broadcast_shape(self.log_normalizer.shape,\n                               self.info_vec.shape[:-1],\n                               self.precision.shape[:-2],\n                               self.alpha.shape,\n                               self.beta.shape)\n\n    def expand(self, batch_shape):\n        n = self.dim()\n        log_normalizer = self.log_normalizer.expand(batch_shape)\n        info_vec = self.info_vec.expand(batch_shape + (n,))\n        precision = self.precision.expand(batch_shape + (n, n))\n        alpha = self.alpha.expand(batch_shape)\n        beta = self.beta.expand(batch_shape)\n        return GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n\n    def reshape(self, batch_shape):\n        n = self.dim()\n        log_normalizer = self.log_normalizer.reshape(batch_shape)\n        info_vec = self.info_vec.reshape(batch_shape + (n,))\n        precision = self.precision.reshape(batch_shape + (n, n))\n        alpha = self.alpha.reshape(batch_shape)\n        beta = self.beta.reshape(batch_shape)\n        return GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n\n    def __getitem__(self, index):\n        """"""\n        Index into the batch_shape of a GammaGaussian.\n        """"""\n        assert isinstance(index, tuple)\n        log_normalizer = self.log_normalizer[index]\n        info_vec = self.info_vec[index + (slice(None),)]\n        precision = self.precision[index + (slice(None), slice(None))]\n        alpha = self.alpha[index]\n        beta = self.beta[index]\n        return GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n\n    @staticmethod\n    def cat(parts, dim=0):\n        """"""\n        Concatenate a list of GammaGaussians along a given batch dimension.\n        """"""\n        if dim < 0:\n            dim += len(parts[0].batch_shape)\n        args = [torch.cat([getattr(g, attr) for g in parts], dim=dim)\n                for attr in [""log_normalizer"", ""info_vec"", ""precision"", ""alpha"", ""beta""]]\n        return GammaGaussian(*args)\n\n    def event_pad(self, left=0, right=0):\n        """"""\n        Pad along event dimension.\n        """"""\n        lr = (left, right)\n        info_vec = pad(self.info_vec, lr)\n        precision = pad(self.precision, lr + lr)\n        # no change for alpha, beta because we are working with reparameterized version;\n        # otherwise, we need to change alpha (similar for beta) to\n        # keep the term (alpha + 0.5 * dim - 1) * log(s) constant\n        # (note that `dim` has been changed due to padding)\n        return GammaGaussian(self.log_normalizer, info_vec, precision, self.alpha, self.beta)\n\n    def event_permute(self, perm):\n        """"""\n        Permute along event dimension.\n        """"""\n        assert isinstance(perm, torch.Tensor)\n        assert perm.shape == (self.dim(),)\n        info_vec = self.info_vec[..., perm]\n        precision = self.precision[..., perm][..., perm, :]\n        return GammaGaussian(self.log_normalizer, info_vec, precision, self.alpha, self.beta)\n\n    def __add__(self, other):\n        """"""\n        Adds two GammaGaussians in log-density space.\n        """"""\n        assert isinstance(other, GammaGaussian)\n        assert self.dim() == other.dim()\n        return GammaGaussian(self.log_normalizer + other.log_normalizer,\n                             self.info_vec + other.info_vec,\n                             self.precision + other.precision,\n                             self.alpha + other.alpha,\n                             self.beta + other.beta)\n\n    def log_density(self, value, s):\n        """"""\n        Evaluate the log density of this GammaGaussian at a point value::\n\n            alpha * log(s) + s * (-0.5 * value.T @ precision @ value + value.T @ info_vec - beta) + log_normalizer\n\n        This is mainly used for testing.\n        """"""\n        if value.size(-1) == 0:\n            batch_shape = broadcast_shape(value.shape[:-1], s.shape, self.batch_shape)\n            return self.alpha * s.log() - self.beta * s + self.log_normalizer.expand(batch_shape)\n        result = (-0.5) * self.precision.matmul(value.unsqueeze(-1)).squeeze(-1)\n        result = result + self.info_vec\n        result = (value * result).sum(-1)\n        return self.alpha * s.log() + (result - self.beta) * s + self.log_normalizer\n\n    def condition(self, value):\n        """"""\n        Condition the Gaussian component on a trailing subset of its state.\n        This should satisfy::\n\n            g.condition(y).dim() == g.dim() - y.size(-1)\n\n        Note that since this is a non-normalized Gaussian, we include the\n        density of ``y`` in the result. Thus :meth:`condition` is similar to a\n        ``functools.partial`` binding of arguments::\n\n            left = x[..., :n]\n            right = x[..., n:]\n            g.log_density(x, s) == g.condition(right).log_density(left, s)\n        """"""\n        assert isinstance(value, torch.Tensor)\n        assert value.size(-1) <= self.info_vec.size(-1)\n\n        n = self.dim() - value.size(-1)\n        info_a = self.info_vec[..., :n]\n        info_b = self.info_vec[..., n:]\n        P_aa = self.precision[..., :n, :n]\n        P_ab = self.precision[..., :n, n:]\n        P_bb = self.precision[..., n:, n:]\n        b = value\n\n        info_vec = info_a - P_ab.matmul(b.unsqueeze(-1)).squeeze(-1)\n        precision = P_aa\n\n        log_normalizer = self.log_normalizer\n        alpha = self.alpha\n        beta = self.beta + 0.5 * P_bb.matmul(b.unsqueeze(-1)).squeeze(-1).mul(b).sum(-1) - b.mul(info_b).sum(-1)\n        return GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n\n    def marginalize(self, left=0, right=0):\n        """"""\n        Marginalizing out variables on either side of the event dimension::\n\n            g.marginalize(left=n).event_logsumexp() = g.event_logsumexp()\n            g.marginalize(right=n).event_logsumexp() = g.event_logsumexp()\n\n        and for data ``x``:\n\n            g.condition(x).event_logsumexp().log_density(s)\n              = g.marginalize(left=g.dim() - x.size(-1)).log_density(x, s)\n        """"""\n        # NB: the easiest way to think about this process is to consider GammaGaussian\n        # as a Gaussian with precision and info_vec scaled by `s`.\n        if left == 0 and right == 0:\n            return self\n        if left > 0 and right > 0:\n            raise NotImplementedError\n        n = self.dim()\n        n_b = left + right\n        a = slice(left, n - right)  # preserved\n        b = slice(None, left) if left else slice(n - right, None)\n\n        P_aa = self.precision[..., a, a]\n        P_ba = self.precision[..., b, a]\n        P_bb = self.precision[..., b, b]\n        P_b = P_bb.cholesky()\n        P_a = P_ba.triangular_solve(P_b, upper=False).solution\n        P_at = P_a.transpose(-1, -2)\n        precision = P_aa - P_at.matmul(P_a)\n\n        info_a = self.info_vec[..., a]\n        info_b = self.info_vec[..., b]\n        b_tmp = info_b.unsqueeze(-1).triangular_solve(P_b, upper=False).solution\n        info_vec = info_a\n        if n_b < n:\n            info_vec = info_vec - P_at.matmul(b_tmp).squeeze(-1)\n\n        alpha = self.alpha - 0.5 * n_b\n        beta = self.beta - 0.5 * b_tmp.squeeze(-1).pow(2).sum(-1)\n        log_normalizer = (self.log_normalizer +\n                          0.5 * n_b * math.log(2 * math.pi) -\n                          P_b.diagonal(dim1=-2, dim2=-1).log().sum(-1))\n        return GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n\n    def compound(self):\n        """"""\n        Integrates out the latent multiplier `s`. The result will be a\n        Student-T distribution.\n        """"""\n        concentration = self.alpha - 0.5 * self.dim() + 1\n        scale_tril = precision_to_scale_tril(self.precision)\n        scale_tril_t_u = scale_tril.transpose(-1, -2).matmul(self.info_vec.unsqueeze(-1)).squeeze(-1)\n        u_Pinv_u = scale_tril_t_u.pow(2).sum(-1)\n        rate = self.beta - 0.5 * u_Pinv_u\n\n        loc = scale_tril.matmul(scale_tril_t_u.unsqueeze(-1)).squeeze(-1)\n        scale_tril = scale_tril * (rate / concentration).sqrt().unsqueeze(-1).unsqueeze(-1)\n        return MultivariateStudentT(2 * concentration, loc, scale_tril)\n\n    def event_logsumexp(self):\n        """"""\n        Integrates out all latent state (i.e. operating on event dimensions) of Gaussian component.\n        """"""\n        n = self.dim()\n        chol_P = self.precision.cholesky()\n        chol_P_u = self.info_vec.unsqueeze(-1).triangular_solve(chol_P, upper=False).solution.squeeze(-1)\n        u_P_u = chol_P_u.pow(2).sum(-1)\n        # considering GammaGaussian as a Gaussian with precision = s * precision, info_vec = s * info_vec,\n        # marginalize x variable, we get\n        #   logsumexp(s) = alpha * log(s) - s * beta + 0.5 n * log(2 pi) + \\\n        #       0.5 s * uPu - 0.5 * log|P| - 0.5 n * log(s)\n        # use the original parameterization of Gamma, we get\n        #   logsumexp(s) = (concentration - 1) * log(s) - s * rate + 0.5 n * log(2 pi) - 0.5 * |P|\n        # Note that `(concentration - 1) * log(s) - s * rate` is unnormalized log_prob of\n        #   Gamma(concentration, rate)\n        concentration = self.alpha - 0.5 * n + 1\n        rate = self.beta - 0.5 * u_P_u\n        log_normalizer_tmp = 0.5 * n * math.log(2 * math.pi) - chol_P.diagonal(dim1=-2, dim2=-1).log().sum(-1)\n        return Gamma(self.log_normalizer + log_normalizer_tmp, concentration, rate)\n\n\ndef gamma_and_mvn_to_gamma_gaussian(gamma, mvn):\n    """"""\n    Convert a pair of Gamma and Gaussian distributions to a GammaGaussian.\n\n        p(x | s) ~ Gaussian(s * info_vec, s * precision)\n        p(s) ~ Gamma(alpha, beta)\n        p(x, s) ~ GammaGaussian(info_vec, precison, alpha, beta)\n\n    :param ~pyro.distributions.Gamma gamma: the mixing distribution\n    :param ~pyro.distributions.MultivariateNormal mvn: the conditional distribution\n        when mixing is 1.\n    :return: A GammaGaussian object.\n    :rtype: ~pyro.ops.gaussian_gamma.GammaGaussian\n    """"""\n    assert isinstance(gamma, torch.distributions.Gamma)\n    assert isinstance(mvn, torch.distributions.MultivariateNormal)\n    n = mvn.loc.size(-1)\n    precision = mvn.precision_matrix\n    info_vec = precision.matmul(mvn.loc.unsqueeze(-1)).squeeze(-1)\n\n    # reparameterized version of concentration, rate in GaussianGamma\n    alpha = gamma.concentration + (0.5 * n - 1)\n    beta = gamma.rate + 0.5 * (info_vec * mvn.loc).sum(-1)\n    gaussian_logsumexp = 0.5 * n * math.log(2 * math.pi) + \\\n        mvn.scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1)\n    log_normalizer = -Gamma(gaussian_logsumexp, gamma.concentration, gamma.rate).logsumexp()\n    return GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n\n\ndef scale_mvn(mvn, s):\n    """"""\n    Transforms a MVN distribution to another MVN distribution according to\n\n        scale(mvn(loc, precision), s) := mvn(loc, s * precision).\n    """"""\n    assert isinstance(mvn, torch.distributions.MultivariateNormal)\n    assert isinstance(s, torch.Tensor)\n    batch_shape = broadcast_shape(s.shape, mvn.batch_shape)\n    loc = mvn.loc.expand(batch_shape + (-1,))\n    # XXX: we might use mvn._unbroadcasted_scale_tril here\n    scale_tril = mvn.scale_tril / s.sqrt().unsqueeze(-1).unsqueeze(-1)\n    return MultivariateNormal(loc, scale_tril=scale_tril)\n\n\ndef matrix_and_mvn_to_gamma_gaussian(matrix, mvn):\n    """"""\n    Convert a noisy affine function to a GammaGaussian, where the noise precision\n    is scaled by an auxiliary variable `s`. The noisy affine function (conditioned\n    on `s`) is defined as::\n\n        y = x @ matrix + scale(mvn, s).sample()\n\n    :param ~torch.Tensor matrix: A matrix with rightmost shape ``(x_dim, y_dim)``.\n    :param ~pyro.distributions.MultivariateNormal mvn: A multivariate normal distribution.\n    :return: A GammaGaussian with broadcasted batch shape and ``.dim() == x_dim + y_dim``.\n    :rtype: ~pyro.ops.gaussian_gamma.GammaGaussian\n    """"""\n    assert isinstance(mvn, torch.distributions.MultivariateNormal)\n    assert isinstance(matrix, torch.Tensor)\n    x_dim, y_dim = matrix.shape[-2:]\n    assert mvn.event_shape == (y_dim,)\n    batch_shape = broadcast_shape(matrix.shape[:-2], mvn.batch_shape)\n    matrix = matrix.expand(batch_shape + (x_dim, y_dim))\n    mvn = mvn.expand(batch_shape)\n\n    P_yy = mvn.precision_matrix\n    neg_P_xy = matrix.matmul(P_yy)\n    P_xy = -neg_P_xy\n    P_yx = P_xy.transpose(-1, -2)\n    P_xx = neg_P_xy.matmul(matrix.transpose(-1, -2))\n    precision = torch.cat([torch.cat([P_xx, P_xy], -1),\n                           torch.cat([P_yx, P_yy], -1)], -2)\n    info_y = P_yy.matmul(mvn.loc.unsqueeze(-1)).squeeze(-1)\n    info_x = -matrix.matmul(info_y.unsqueeze(-1)).squeeze(-1)\n    info_vec = torch.cat([info_x, info_y], -1)\n    log_normalizer = -0.5 * y_dim * math.log(2 * math.pi) - mvn.scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1)\n    beta = 0.5 * (info_y * mvn.loc).sum(-1)\n    alpha = beta.new_full(beta.shape, 0.5 * y_dim)\n\n    result = GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n    assert result.batch_shape == batch_shape\n    assert result.dim() == x_dim + y_dim\n    return result\n\n\ndef gamma_gaussian_tensordot(x, y, dims=0):\n    """"""\n    Computes the integral over two GammaGaussians:\n\n        `(x @ y)((a,c),s) = log(integral(exp(x((a,b),s) + y((b,c),s)), b))`,\n\n    where `x` is a gaussian over variables (a,b), `y` is a gaussian over variables\n    (b,c), (a,b,c) can each be sets of zero or more variables, and `dims` is the size of b.\n\n    :param x: a GammaGaussian instance\n    :param y: a GammaGaussian instance\n    :param dims: number of variables to contract\n    """"""\n    assert isinstance(x, GammaGaussian)\n    assert isinstance(y, GammaGaussian)\n    na = x.dim() - dims\n    nb = dims\n    nc = y.dim() - dims\n    assert na >= 0\n    assert nb >= 0\n    assert nc >= 0\n\n    device = x.info_vec.device\n    perm = torch.cat([\n        torch.arange(na, device=device),\n        torch.arange(x.dim(), x.dim() + nc, device=device),\n        torch.arange(na, x.dim(), device=device)])\n    return (x.event_pad(right=nc) + y.event_pad(left=na)).event_permute(perm).marginalize(right=nb)\n'"
pyro/ops/gaussian.py,38,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions.utils import lazy_property\nfrom torch.nn.functional import pad\n\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.ops.tensor_utils import cholesky, matmul, matvecmul, triangular_solve\n\n\nclass Gaussian:\n    """"""\n    Non-normalized Gaussian distribution.\n\n    This represents an arbitrary semidefinite quadratic function, which can be\n    interpreted as a rank-deficient scaled Gaussian distribution. The precision\n    matrix may have zero eigenvalues, thus it may be impossible to work\n    directly with the covariance matrix.\n\n    :param torch.Tensor log_normalizer: a normalization constant, which is mainly used to keep\n        track of normalization terms during contractions.\n    :param torch.Tensor info_vec: information vector, which is a scaled version of the mean\n        ``info_vec = precision @ mean``. We use this represention to make gaussian contraction\n        fast and stable.\n    :param torch.Tensor precision: precision matrix of this gaussian.\n    """"""\n    def __init__(self, log_normalizer, info_vec, precision):\n        # NB: using info_vec instead of mean to deal with rank-deficient problem\n        assert info_vec.dim() >= 1\n        assert precision.dim() >= 2\n        assert precision.shape[-2:] == info_vec.shape[-1:] * 2\n        self.log_normalizer = log_normalizer\n        self.info_vec = info_vec\n        self.precision = precision\n\n    def dim(self):\n        return self.info_vec.size(-1)\n\n    @lazy_property\n    def batch_shape(self):\n        return broadcast_shape(self.log_normalizer.shape,\n                               self.info_vec.shape[:-1],\n                               self.precision.shape[:-2])\n\n    def expand(self, batch_shape):\n        n = self.dim()\n        log_normalizer = self.log_normalizer.expand(batch_shape)\n        info_vec = self.info_vec.expand(batch_shape + (n,))\n        precision = self.precision.expand(batch_shape + (n, n))\n        return Gaussian(log_normalizer, info_vec, precision)\n\n    def reshape(self, batch_shape):\n        n = self.dim()\n        log_normalizer = self.log_normalizer.reshape(batch_shape)\n        info_vec = self.info_vec.reshape(batch_shape + (n,))\n        precision = self.precision.reshape(batch_shape + (n, n))\n        return Gaussian(log_normalizer, info_vec, precision)\n\n    def __getitem__(self, index):\n        """"""\n        Index into the batch_shape of a Gaussian.\n        """"""\n        assert isinstance(index, tuple)\n        log_normalizer = self.log_normalizer[index]\n        info_vec = self.info_vec[index + (slice(None),)]\n        precision = self.precision[index + (slice(None), slice(None))]\n        return Gaussian(log_normalizer, info_vec, precision)\n\n    @staticmethod\n    def cat(parts, dim=0):\n        """"""\n        Concatenate a list of Gaussians along a given batch dimension.\n        """"""\n        if dim < 0:\n            dim += len(parts[0].batch_shape)\n        args = [torch.cat([getattr(g, attr) for g in parts], dim=dim)\n                for attr in [""log_normalizer"", ""info_vec"", ""precision""]]\n        return Gaussian(*args)\n\n    def event_pad(self, left=0, right=0):\n        """"""\n        Pad along event dimension.\n        """"""\n        lr = (left, right)\n        log_normalizer = self.log_normalizer\n        info_vec = pad(self.info_vec, lr)\n        precision = pad(self.precision, lr + lr)\n        return Gaussian(log_normalizer, info_vec, precision)\n\n    def event_permute(self, perm):\n        """"""\n        Permute along event dimension.\n        """"""\n        assert isinstance(perm, torch.Tensor)\n        assert perm.shape == (self.dim(),)\n        info_vec = self.info_vec[..., perm]\n        precision = self.precision[..., perm][..., perm, :]\n        return Gaussian(self.log_normalizer, info_vec, precision)\n\n    def __add__(self, other):\n        """"""\n        Adds two Gaussians in log-density space.\n        """"""\n        if isinstance(other, Gaussian):\n            assert self.dim() == other.dim()\n            return Gaussian(self.log_normalizer + other.log_normalizer,\n                            self.info_vec + other.info_vec,\n                            self.precision + other.precision)\n        if isinstance(other, (int, float, torch.Tensor)):\n            return Gaussian(self.log_normalizer + other, self.info_vec, self.precision)\n        raise ValueError(""Unsupported type: {}"".format(type(other)))\n\n    def __sub__(self, other):\n        if isinstance(other, (int, float, torch.Tensor)):\n            return Gaussian(self.log_normalizer - other, self.info_vec, self.precision)\n        raise ValueError(""Unsupported type: {}"".format(type(other)))\n\n    def log_density(self, value):\n        """"""\n        Evaluate the log density of this Gaussian at a point value::\n\n            -0.5 * value.T @ precision @ value + value.T @ info_vec + log_normalizer\n\n        This is mainly used for testing.\n        """"""\n        if value.size(-1) == 0:\n            batch_shape = broadcast_shape(value.shape[:-1], self.batch_shape)\n            return self.log_normalizer.expand(batch_shape)\n        result = (-0.5) * matvecmul(self.precision, value)\n        result = result + self.info_vec\n        result = (value * result).sum(-1)\n        return result + self.log_normalizer\n\n    def rsample(self, sample_shape=torch.Size()):\n        """"""\n        Reparameterized sampler.\n        """"""\n        P_chol = cholesky(self.precision)\n        loc = self.info_vec.unsqueeze(-1).cholesky_solve(P_chol).squeeze(-1)\n        shape = sample_shape + self.batch_shape + (self.dim(), 1)\n        noise = torch.randn(shape, dtype=loc.dtype, device=loc.device)\n        noise = triangular_solve(noise, P_chol, upper=False, transpose=True).squeeze(-1)\n        return loc + noise\n\n    def condition(self, value):\n        """"""\n        Condition this Gaussian on a trailing subset of its state.\n        This should satisfy::\n\n            g.condition(y).dim() == g.dim() - y.size(-1)\n\n        Note that since this is a non-normalized Gaussian, we include the\n        density of ``y`` in the result. Thus :meth:`condition` is similar to a\n        ``functools.partial`` binding of arguments::\n\n            left = x[..., :n]\n            right = x[..., n:]\n            g.log_density(x) == g.condition(right).log_density(left)\n        """"""\n        assert isinstance(value, torch.Tensor)\n        right = value.size(-1)\n        dim = self.dim()\n        assert right <= dim\n\n        n = dim - right\n        info_a = self.info_vec[..., :n]\n        info_b = self.info_vec[..., n:]\n        P_aa = self.precision[..., :n, :n]\n        P_ab = self.precision[..., :n, n:]\n        P_bb = self.precision[..., n:, n:]\n        b = value\n\n        info_vec = info_a - matvecmul(P_ab, b)\n        precision = P_aa\n        log_normalizer = (self.log_normalizer +\n                          -0.5 * matvecmul(P_bb, b).mul(b).sum(-1) +\n                          b.mul(info_b).sum(-1))\n        return Gaussian(log_normalizer, info_vec, precision)\n\n    def left_condition(self, value):\n        """"""\n        Condition this Gaussian on a leading subset of its state.\n        This should satisfy::\n\n            g.condition(y).dim() == g.dim() - y.size(-1)\n\n        Note that since this is a non-normalized Gaussian, we include the\n        density of ``y`` in the result. Thus :meth:`condition` is similar to a\n        ``functools.partial`` binding of arguments::\n\n            left = x[..., :n]\n            right = x[..., n:]\n            g.log_density(x) == g.left_condition(left).log_density(right)\n        """"""\n        assert isinstance(value, torch.Tensor)\n        left = value.size(-1)\n        dim = self.dim()\n        assert left <= dim\n\n        perm = torch.cat([torch.arange(left, dim, device=value.device),\n                          torch.arange(left, device=value.device)])\n        return self.event_permute(perm).condition(value)\n\n    def marginalize(self, left=0, right=0):\n        """"""\n        Marginalizing out variables on either side of the event dimension::\n\n            g.marginalize(left=n).event_logsumexp() = g.logsumexp()\n            g.marginalize(right=n).event_logsumexp() = g.logsumexp()\n\n        and for data ``x``:\n\n            g.condition(x).event_logsumexp()\n              = g.marginalize(left=g.dim() - x.size(-1)).log_density(x)\n        """"""\n        if left == 0 and right == 0:\n            return self\n        if left > 0 and right > 0:\n            raise NotImplementedError\n        n = self.dim()\n        n_b = left + right\n        a = slice(left, n - right)  # preserved\n        b = slice(None, left) if left else slice(n - right, None)\n\n        P_aa = self.precision[..., a, a]\n        P_ba = self.precision[..., b, a]\n        P_bb = self.precision[..., b, b]\n        P_b = cholesky(P_bb)\n        P_a = triangular_solve(P_ba, P_b, upper=False)\n        P_at = P_a.transpose(-1, -2)\n        precision = P_aa - matmul(P_at, P_a)\n\n        info_a = self.info_vec[..., a]\n        info_b = self.info_vec[..., b]\n        b_tmp = triangular_solve(info_b.unsqueeze(-1), P_b, upper=False)\n        info_vec = info_a - matmul(P_at, b_tmp).squeeze(-1)\n\n        log_normalizer = (self.log_normalizer +\n                          0.5 * n_b * math.log(2 * math.pi) -\n                          P_b.diagonal(dim1=-2, dim2=-1).log().sum(-1) +\n                          0.5 * b_tmp.squeeze(-1).pow(2).sum(-1))\n        return Gaussian(log_normalizer, info_vec, precision)\n\n    def event_logsumexp(self):\n        """"""\n        Integrates out all latent state (i.e. operating on event dimensions).\n        """"""\n        n = self.dim()\n        chol_P = cholesky(self.precision)\n        chol_P_u = triangular_solve(self.info_vec.unsqueeze(-1), chol_P, upper=False).squeeze(-1)\n        u_P_u = chol_P_u.pow(2).sum(-1)\n        return (self.log_normalizer + 0.5 * n * math.log(2 * math.pi) + 0.5 * u_P_u -\n                chol_P.diagonal(dim1=-2, dim2=-1).log().sum(-1))\n\n\nclass AffineNormal:\n    """"""\n    Represents a conditional diagonal normal distribution over a random\n    variable ``Y`` whose mean is an affine function of a random variable ``X``.\n    The likelihood of ``X`` is thus::\n\n        AffineNormal(matrix, loc, scale).condition(y).log_density(x)\n\n    which is equivalent to::\n\n        Normal(x @ matrix + loc, scale).to_event(1).log_prob(y)\n\n    :param torch.Tensor matrix: A transformation from ``X`` to ``Y``.\n        Should have rightmost shape ``(x_dim, y_dim)``.\n    :param torch.Tensor loc: A constant offset for ``Y``\'s mean.\n        Should have rightmost shape ``(y_dim,)``.\n    :param torch.Tensor scale: Standard deviation for ``Y``.\n        Should have rightmost shape ``(y_dim,)``.\n    """"""\n    def __init__(self, matrix, loc, scale):\n        assert loc.shape == scale.shape\n        assert matrix.shape[:-2] == loc.shape[:-1]\n        assert matrix.size(-1) == loc.size(-1)\n        self.matrix = matrix\n        self.loc = loc\n        self.scale = scale\n        self._gaussian = None\n\n    @lazy_property\n    def batch_shape(self):\n        return self.matrix.shape[:-2]\n\n    def condition(self, value):\n        if value.size(-1) == self.loc.size(-1):\n            prec_sqrt = self.matrix / self.scale.unsqueeze(-2)\n            precision = matmul(prec_sqrt, prec_sqrt.transpose(-1, -2))\n            delta = (value - self.loc) / self.scale\n            info_vec = matvecmul(prec_sqrt, delta)\n            log_normalizer = (-0.5 * self.loc.size(-1) * math.log(2 * math.pi)\n                              - 0.5 * delta.pow(2).sum(-1) - self.scale.log().sum(-1))\n            return Gaussian(log_normalizer, info_vec, precision)\n        else:\n            return self.to_gaussian().condition(value)\n\n    def left_condition(self, value):\n        """"""\n        If ``value.size(-1) == x_dim``, this returns a Normal distribution with\n        ``event_dim=1``. After applying this method, the cost to draw a sample is\n        ``O(y_dim)`` instead of ``O(y_dim ** 3)``.\n        """"""\n        if value.size(-1) == self.matrix.size(-2):\n            loc = matvecmul(self.matrix.transpose(-1, -2), value) + self.loc\n            matrix = value.new_zeros(loc.shape[:-1] + (0, loc.size(-1)))\n            scale = self.scale.expand(loc.shape)\n            return AffineNormal(matrix, loc, scale)\n        else:\n            return self.to_gaussian().left_condition(value)\n\n    def rsample(self, sample_shape=torch.Size()):\n        """"""\n        Reparameterized sampler.\n        """"""\n        if self.matrix.size(-2) > 0:\n            raise NotImplementedError\n        shape = sample_shape + self.batch_shape + self.loc.shape[-1:]\n        noise = torch.randn(shape, dtype=self.loc.dtype, device=self.loc.device)\n        return self.loc + noise * self.scale\n\n    def to_gaussian(self):\n        if self._gaussian is None:\n            mvn = torch.distributions.Independent(\n                torch.distributions.Normal(self.loc, scale=self.scale), 1)\n            y_gaussian = mvn_to_gaussian(mvn)\n            self._gaussian = _matrix_and_gaussian_to_gaussian(self.matrix, y_gaussian)\n        return self._gaussian\n\n    def expand(self, batch_shape):\n        matrix = self.matrix.expand(batch_shape + self.matrix.shape[-2:])\n        loc = self.loc.expand(batch_shape + self.loc.shape[-1:])\n        scale = self.scale.expand(batch_shape + self.scale.shape[-1:])\n        return AffineNormal(matrix, loc, scale)\n\n    def reshape(self, batch_shape):\n        matrix = self.matrix.reshape(batch_shape + self.matrix.shape[-2:])\n        loc = self.loc.reshape(batch_shape + self.loc.shape[-1:])\n        scale = self.scale.reshape(batch_shape + self.scale.shape[-1:])\n        return AffineNormal(matrix, loc, scale)\n\n    def __getitem__(self, index):\n        assert isinstance(index, tuple)\n        matrix = self.matrix[index + (slice(None), slice(None))]\n        loc = self.loc[index + (slice(None),)]\n        scale = self.scale[index + (slice(None),)]\n        return AffineNormal(matrix, loc, scale)\n\n    def event_permute(self, perm):\n        return self.to_gaussian().event_permute(perm)\n\n    def __add__(self, other):\n        return self.to_gaussian() + other\n\n    def marginalize(self, left=0, right=0):\n        if left == 0 and right == self.loc.size(-1):\n            n = self.matrix.size(-2)\n            precision = self.scale.new_zeros(self.batch_shape + (n, n))\n            info_vec = self.scale.new_zeros(self.batch_shape + (n,))\n            log_normalizer = self.scale.new_zeros(self.batch_shape)\n            return Gaussian(log_normalizer, info_vec, precision)\n        else:\n            return self.to_gaussian().marginalize(left, right)\n\n\ndef mvn_to_gaussian(mvn):\n    """"""\n    Convert a MultivariateNormal distribution to a Gaussian.\n\n    :param ~torch.distributions.MultivariateNormal mvn: A multivariate normal distribution.\n    :return: An equivalent Gaussian object.\n    :rtype: ~pyro.ops.gaussian.Gaussian\n    """"""\n    assert (isinstance(mvn, torch.distributions.MultivariateNormal) or\n            (isinstance(mvn, torch.distributions.Independent) and\n             isinstance(mvn.base_dist, torch.distributions.Normal)))\n    if isinstance(mvn, torch.distributions.Independent):\n        mvn = mvn.base_dist\n        precision_diag = mvn.scale.pow(-2)\n        precision = precision_diag.diag_embed()\n        info_vec = mvn.loc * precision_diag\n        scale_diag = mvn.scale\n    else:\n        precision = mvn.precision_matrix\n        info_vec = matvecmul(precision, mvn.loc)\n        scale_diag = mvn.scale_tril.diagonal(dim1=-2, dim2=-1)\n\n    n = mvn.loc.size(-1)\n    log_normalizer = (-0.5 * n * math.log(2 * math.pi) +\n                      -0.5 * (info_vec * mvn.loc).sum(-1) -\n                      scale_diag.log().sum(-1))\n    return Gaussian(log_normalizer, info_vec, precision)\n\n\ndef _matrix_and_gaussian_to_gaussian(matrix, y_gaussian):\n    P_yy = y_gaussian.precision\n    neg_P_xy = matmul(matrix, P_yy)\n    P_xy = -neg_P_xy\n    P_yx = P_xy.transpose(-1, -2)\n    P_xx = matmul(neg_P_xy, matrix.transpose(-1, -2))\n    precision = torch.cat([torch.cat([P_xx, P_xy], -1),\n                           torch.cat([P_yx, P_yy], -1)], -2)\n    info_y = y_gaussian.info_vec\n    info_x = -matvecmul(matrix, info_y)\n    info_vec = torch.cat([info_x, info_y], -1)\n    log_normalizer = y_gaussian.log_normalizer\n\n    result = Gaussian(log_normalizer, info_vec, precision)\n    return result\n\n\ndef matrix_and_mvn_to_gaussian(matrix, mvn):\n    """"""\n    Convert a noisy affine function to a Gaussian. The noisy affine function is defined as::\n\n        y = x @ matrix + mvn.sample()\n\n    :param ~torch.Tensor matrix: A matrix with rightmost shape ``(x_dim, y_dim)``.\n    :param ~torch.distributions.MultivariateNormal mvn: A multivariate normal distribution.\n    :return: A Gaussian with broadcasted batch shape and ``.dim() == x_dim + y_dim``.\n    :rtype: ~pyro.ops.gaussian.Gaussian\n    """"""\n    assert (isinstance(mvn, torch.distributions.MultivariateNormal) or\n            (isinstance(mvn, torch.distributions.Independent) and\n             isinstance(mvn.base_dist, torch.distributions.Normal)))\n    assert isinstance(matrix, torch.Tensor)\n    x_dim, y_dim = matrix.shape[-2:]\n    assert mvn.event_shape == (y_dim,)\n    batch_shape = broadcast_shape(matrix.shape[:-2], mvn.batch_shape)\n    matrix = matrix.expand(batch_shape + (x_dim, y_dim))\n    mvn = mvn.expand(batch_shape)\n\n    # Handle diagonal normal distributions as an efficient special case.\n    if isinstance(mvn, torch.distributions.Independent):\n        return AffineNormal(matrix, mvn.base_dist.loc, mvn.base_dist.scale)\n\n    y_gaussian = mvn_to_gaussian(mvn)\n    result = _matrix_and_gaussian_to_gaussian(matrix, y_gaussian)\n    assert result.batch_shape == batch_shape\n    assert result.dim() == x_dim + y_dim\n    return result\n\n\ndef gaussian_tensordot(x, y, dims=0):\n    """"""\n    Computes the integral over two gaussians:\n\n        `(x @ y)(a,c) = log(integral(exp(x(a,b) + y(b,c)), b))`,\n\n    where `x` is a gaussian over variables (a,b), `y` is a gaussian over variables\n    (b,c), (a,b,c) can each be sets of zero or more variables, and `dims` is the size of b.\n\n    :param x: a Gaussian instance\n    :param y: a Gaussian instance\n    :param dims: number of variables to contract\n    """"""\n    assert isinstance(x, Gaussian)\n    assert isinstance(y, Gaussian)\n    na = x.dim() - dims\n    nb = dims\n    nc = y.dim() - dims\n    assert na >= 0\n    assert nb >= 0\n    assert nc >= 0\n\n    Paa, Pba, Pbb = x.precision[..., :na, :na], x.precision[..., na:, :na], x.precision[..., na:, na:]\n    Qbb, Qbc, Qcc = y.precision[..., :nb, :nb], y.precision[..., :nb, nb:], y.precision[..., nb:, nb:]\n    xa, xb = x.info_vec[..., :na], x.info_vec[..., na:]  # x.precision @ x.mean\n    yb, yc = y.info_vec[..., :nb], y.info_vec[..., nb:]  # y.precision @ y.mean\n\n    precision = pad(Paa, (0, nc, 0, nc)) + pad(Qcc, (na, 0, na, 0))\n    info_vec = pad(xa, (0, nc)) + pad(yc, (na, 0))\n    log_normalizer = x.log_normalizer + y.log_normalizer\n    if nb > 0:\n        B = pad(Pba, (0, nc)) + pad(Qbc, (na, 0))\n        b = xb + yb\n\n        # Pbb + Qbb needs to be positive definite, so that we can malginalize out `b` (to have a finite integral)\n        L = cholesky(Pbb + Qbb)\n        LinvB = triangular_solve(B, L, upper=False)\n        LinvBt = LinvB.transpose(-2, -1)\n        Linvb = triangular_solve(b.unsqueeze(-1), L, upper=False)\n\n        precision = precision - matmul(LinvBt, LinvB)\n        # NB: precision might not be invertible for getting mean = precision^-1 @ info_vec\n        if na + nc > 0:\n            info_vec = info_vec - matmul(LinvBt, Linvb).squeeze(-1)\n        logdet = torch.diagonal(L, dim1=-2, dim2=-1).log().sum(-1)\n        diff = 0.5 * nb * math.log(2 * math.pi) + 0.5 * Linvb.squeeze(-1).pow(2).sum(-1) - logdet\n        log_normalizer = log_normalizer + diff\n\n    return Gaussian(log_normalizer, info_vec, precision)\n'"
pyro/ops/hessian.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\n\ndef hessian(y, xs):\n    """"""\n    Convenience method for computing hessians. Note that this is slow in high\n    dimensions because computing hessians in a reverse-mode AD library like\n    PyTorch is inherently slow (note the for loop).\n    """"""\n    dys = torch.autograd.grad(y, xs, create_graph=True)\n    flat_dy = torch.cat([dy.reshape(-1) for dy in dys])\n    H = []\n    for dyi in flat_dy:\n        Hi = torch.cat([Hij.reshape(-1) for Hij in torch.autograd.grad(dyi, xs, retain_graph=True)])\n        H.append(Hi)\n    H = torch.stack(H)\n    return H\n'"
pyro/ops/indexing.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\n\ndef _is_batched(arg):\n    return isinstance(arg, torch.Tensor) and arg.dim()\n\n\ndef _flatten(args, out):\n    if isinstance(args, tuple):\n        for arg in args:\n            _flatten(arg, out)\n    else:\n        # Combine consecutive Ellipsis.\n        if args is Ellipsis and out and out[-1] is Ellipsis:\n            return\n        out.append(args)\n\n\ndef index(tensor, args):\n    """"""\n    Indexing with nested tuples.\n\n    See also the convenience wrapper :class:`Index`.\n\n    This is useful for writing indexing code that is compatible with multiple\n    interpretations, e.g. scalar evaluation, vectorized evaluation, or\n    reshaping.\n\n    For example suppose ``x`` is a parameter with ``x.dim() == 2`` and we wish\n    to generalize the expression ``x[..., t]`` where ``t`` can be any of:\n\n    - a scalar ``t=1`` as in ``x[..., 1]``;\n    - a slice ``t=slice(None)`` equivalent to ``x[..., :]``; or\n    - a reshaping operation ``t=(Ellipsis, None)`` equivalent to\n      ``x.unsqueeze(-1)``.\n\n    While naive indexing would work for the first two , the third example would\n    result in a nested tuple ``(Ellipsis, (Ellipsis, None))``. This helper\n    flattens that nested tuple and combines consecutive ``Ellipsis``.\n\n    :param torch.Tensor tensor: A tensor to be indexed.\n    :param tuple args: An index, as args to ``__getitem__``.\n    :returns: A flattened interpetation of ``tensor[args]``.\n    :rtype: torch.Tensor\n    """"""\n    if not isinstance(args, tuple):\n        return tensor[args]\n    if not args:\n        return tensor\n\n    # Flatten.\n    flat = []\n    _flatten(args, flat)\n    args = tuple(flat)\n\n    return tensor[args]\n\n\nclass Index:\n    """"""\n    Convenience wrapper around :func:`index`.\n\n    The following are equivalent::\n\n        Index(x)[..., i, j, :]\n        index(x, (Ellipsis, i, j, slice(None)))\n\n    :param torch.Tensor tensor: A tensor to be indexed.\n    :return: An object with a special :meth:`__getitem__` method.\n    """"""\n    def __init__(self, tensor):\n        self._tensor = tensor\n\n    def __getitem__(self, args):\n        return index(self._tensor, args)\n\n\ndef vindex(tensor, args):\n    """"""\n    Vectorized advanced indexing with broadcasting semantics.\n\n    See also the convenience wrapper :class:`Vindex`.\n\n    This is useful for writing indexing code that is compatible with batching\n    and enumeration, especially for selecting mixture components with discrete\n    random variables.\n\n    For example suppose ``x`` is a parameter with ``x.dim() == 3`` and we wish\n    to generalize the expression ``x[i, :, j]`` from integer ``i,j`` to tensors\n    ``i,j`` with batch dims and enum dims (but no event dims). Then we can\n    write the generalize version using :class:`Vindex` ::\n\n        xij = Vindex(x)[i, :, j]\n\n        batch_shape = broadcast_shape(i.shape, j.shape)\n        event_shape = (x.size(1),)\n        assert xij.shape == batch_shape + event_shape\n\n    To handle the case when ``x`` may also contain batch dimensions (e.g. if\n    ``x`` was sampled in a plated context as when using vectorized particles),\n    :func:`vindex` uses the special convention that ``Ellipsis`` denotes batch\n    dimensions (hence ``...`` can appear only on the left, never in the middle\n    or in the right). Suppose ``x`` has event dim 3. Then we can write::\n\n        old_batch_shape = x.shape[:-3]\n        old_event_shape = x.shape[-3:]\n\n        xij = Vindex(x)[..., i, :, j]   # The ... denotes unknown batch shape.\n\n        new_batch_shape = broadcast_shape(old_batch_shape, i.shape, j.shape)\n        new_event_shape = (x.size(1),)\n        assert xij.shape = new_batch_shape + new_event_shape\n\n    Note that this special handling of ``Ellipsis`` differs from the NEP [1].\n\n    Formally, this function assumes:\n\n    1.  Each arg is either ``Ellipsis``, ``slice(None)``, an integer, or a\n        batched ``torch.LongTensor`` (i.e. with empty event shape). This\n        function does not support Nontrivial slices or ``torch.BoolTensor``\n        masks. ``Ellipsis`` can only appear on the left as ``args[0]``.\n    2.  If ``args[0] is not Ellipsis`` then ``tensor`` is not\n        batched, and its event dim is equal to ``len(args)``.\n    3.  If ``args[0] is Ellipsis`` then ``tensor`` is batched and\n        its event dim is equal to ``len(args[1:])``. Dims of ``tensor``\n        to the left of the event dims are considered batch dims and will be\n        broadcasted with dims of tensor args.\n\n    Note that if none of the args is a tensor with ``.dim() > 0``, then this\n    function behaves like standard indexing::\n\n        if not any(isinstance(a, torch.Tensor) and a.dim() for a in args):\n            assert Vindex(x)[args] == x[args]\n\n    **References**\n\n    [1] https://www.numpy.org/neps/nep-0021-advanced-indexing.html\n        introduces ``vindex`` as a helper for vectorized indexing.\n        The Pyro implementation is similar to the proposed notation\n        ``x.vindex[]`` except for slightly different handling of ``Ellipsis``.\n\n    :param torch.Tensor tensor: A tensor to be indexed.\n    :param tuple args: An index, as args to ``__getitem__``.\n    :returns: A nonstandard interpetation of ``tensor[args]``.\n    :rtype: torch.Tensor\n    """"""\n    if not isinstance(args, tuple):\n        return tensor[args]\n    if not args:\n        return tensor\n\n    # Compute event dim before and after indexing.\n    if args[0] is Ellipsis:\n        args = args[1:]\n        if not args:\n            return tensor\n        old_event_dim = len(args)\n        args = (slice(None),) * (tensor.dim() - len(args)) + args\n    else:\n        args = args + (slice(None),) * (tensor.dim() - len(args))\n        old_event_dim = len(args)\n    assert len(args) == tensor.dim()\n    if any(a is Ellipsis for a in args):\n        raise NotImplementedError(""Non-leading Ellipsis is not supported"")\n\n    # In simple cases, standard advanced indexing broadcasts correctly.\n    is_standard = True\n    if tensor.dim() > old_event_dim and _is_batched(args[0]):\n        is_standard = False\n    elif any(_is_batched(a) for a in args[1:]):\n        is_standard = False\n    if is_standard:\n        return tensor[args]\n\n    # Convert args to use broadcasting semantics.\n    new_event_dim = sum(isinstance(a, slice) for a in args[-old_event_dim:])\n    new_dim = 0\n    args = list(args)\n    for i, arg in reversed(list(enumerate(args))):\n        if isinstance(arg, slice):\n            # Convert slices to torch.arange()s.\n            if arg != slice(None):\n                raise NotImplementedError(""Nontrivial slices are not supported"")\n            arg = torch.arange(tensor.size(i), dtype=torch.long, device=tensor.device)\n            arg = arg.reshape((-1,) + (1,) * new_dim)\n            new_dim += 1\n        elif _is_batched(arg):\n            # Reshape nontrivial tensors.\n            arg = arg.reshape(arg.shape + (1,) * new_event_dim)\n        args[i] = arg\n    args = tuple(args)\n\n    return tensor[args]\n\n\nclass Vindex:\n    """"""\n    Convenience wrapper around :func:`vindex`.\n\n    The following are equivalent::\n\n        Vindex(x)[..., i, j, :]\n        vindex(x, (Ellipsis, i, j, slice(None)))\n\n    :param torch.Tensor tensor: A tensor to be indexed.\n    :return: An object with a special :meth:`__getitem__` method.\n    """"""\n    def __init__(self, tensor):\n        self._tensor = tensor\n\n    def __getitem__(self, args):\n        return vindex(self._tensor, args)\n'"
pyro/ops/integrator.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch.autograd import grad\n\n\ndef velocity_verlet(z, r, potential_fn, kinetic_grad, step_size, num_steps=1, z_grads=None):\n    r""""""\n    Second order symplectic integrator that uses the velocity verlet algorithm.\n\n    :param dict z: dictionary of sample site names and their current values\n        (type :class:`~torch.Tensor`).\n    :param dict r: dictionary of sample site names and corresponding momenta\n        (type :class:`~torch.Tensor`).\n    :param callable potential_fn: function that returns potential energy given z\n        for each sample site. The negative gradient of the function with respect\n        to ``z`` determines the rate of change of the corresponding sites\'\n        momenta ``r``.\n    :param callable kinetic_grad: a function calculating gradient of kinetic energy\n        w.r.t. momentum variable.\n    :param float step_size: step size for each time step iteration.\n    :param int num_steps: number of discrete time steps over which to integrate.\n    :param torch.Tensor z_grads: optional gradients of potential energy at current ``z``.\n    :return tuple (z_next, r_next, z_grads, potential_energy): next position and momenta,\n        together with the potential energy and its gradient w.r.t. ``z_next``.\n    """"""\n    z_next = z.copy()\n    r_next = r.copy()\n    for _ in range(num_steps):\n        z_next, r_next, z_grads, potential_energy = _single_step_verlet(z_next,\n                                                                        r_next,\n                                                                        potential_fn,\n                                                                        kinetic_grad,\n                                                                        step_size,\n                                                                        z_grads)\n    return z_next, r_next, z_grads, potential_energy\n\n\ndef _single_step_verlet(z, r, potential_fn, kinetic_grad, step_size, z_grads=None):\n    r""""""\n    Single step velocity verlet that modifies the `z`, `r` dicts in place.\n    """"""\n\n    z_grads = potential_grad(potential_fn, z)[0] if z_grads is None else z_grads\n\n    for site_name in r:\n        r[site_name] = r[site_name] + 0.5 * step_size * (-z_grads[site_name])  # r(n+1/2)\n\n    r_grads = kinetic_grad(r)\n    for site_name in z:\n        z[site_name] = z[site_name] + step_size * r_grads[site_name]  # z(n+1)\n\n    z_grads, potential_energy = potential_grad(potential_fn, z)\n    for site_name in r:\n        r[site_name] = r[site_name] + 0.5 * step_size * (-z_grads[site_name])  # r(n+1)\n\n    return z, r, z_grads, potential_energy\n\n\ndef potential_grad(potential_fn, z):\n    """"""\n    Gradient of `potential_fn` w.r.t. parameters z.\n\n    :param potential_fn: python callable that takes in a dictionary of parameters\n        and returns the potential energy.\n    :param dict z: dictionary of parameter values keyed by site name.\n    :return: tuple of `(z_grads, potential_energy)`, where `z_grads` is a dictionary\n        with the same keys as `z` containing gradients and potential_energy is a\n        torch scalar.\n    """"""\n    z_keys, z_nodes = zip(*z.items())\n    for node in z_nodes:\n        node.requires_grad_(True)\n    try:\n        potential_energy = potential_fn(z)\n    # deal with singular matrices\n    except RuntimeError as e:\n        if ""singular U"" in str(e):\n            grads = {k: v.new_zeros(v.shape) for k, v in z.items()}\n            return grads, z_nodes[0].new_tensor(float(\'nan\'))\n        else:\n            raise e\n\n    grads = grad(potential_energy, z_nodes)\n    for node in z_nodes:\n        node.requires_grad_(False)\n    return dict(zip(z_keys, grads)), potential_energy.detach()\n'"
pyro/ops/jit.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport warnings\nimport weakref\n\nimport torch\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.util import ignore_jit_warnings, optional, timed\n\n\ndef _hash(value, allow_id):\n    try:\n        hash(value)\n        return value\n    except TypeError as e:\n        if isinstance(value, list):\n            return tuple(_hash(x, allow_id) for x in value)\n        elif isinstance(value, dict):\n            return tuple(sorted((_hash(x, allow_id), _hash(y, allow_id)) for x, y in value.items()))\n        elif isinstance(value, set):\n            return frozenset(_hash(x, allow_id) for x in value)\n        elif isinstance(value, argparse.Namespace):\n            return str(value)\n        elif allow_id:\n            return id(value)\n        raise e\n\n\ndef _hashable_args_kwargs(args, kwargs):\n    items = sorted(kwargs.items())\n    hashable_kwargs = tuple((key, _hash(value, False)) for key, value in items)\n    try:\n        hash(hashable_kwargs)\n    except TypeError:\n        warnings.warn(""Failed to hash kwargs; attempting to hash by id."")\n        hashable_kwargs = tuple((key, _hash(value, True)) for key, value in items)\n    return len(args), hashable_kwargs\n\n\nclass CompiledFunction:\n    """"""\n    Output type of :func:`pyro.ops.jit.trace`.\n\n    Wrapper around the output of :func:`torch.jit.trace`\n    that handles parameter plumbing.\n\n    The actual PyTorch compilation artifact is stored in :attr:`compiled`.\n    Call diagnostic methods on this attribute.\n    """"""\n    def __init__(self, fn, ignore_warnings=False, jit_options=None):\n        self.fn = fn\n        self.compiled = {}  # len(args) -> callable\n        self.ignore_warnings = ignore_warnings\n        self.jit_options = {} if jit_options is None else jit_options\n        self.jit_options.setdefault(\'check_trace\', False)\n        self.compile_time = None\n        self._param_names = None\n\n    def __call__(self, *args, **kwargs):\n        key = _hashable_args_kwargs(args, kwargs)\n\n        # if first time\n        if key not in self.compiled:\n            # param capture\n            with poutine.block():\n                with poutine.trace(param_only=True) as first_param_capture:\n                    self.fn(*args, **kwargs)\n\n            self._param_names = list(set(first_param_capture.trace.nodes.keys()))\n            unconstrained_params = tuple(pyro.param(name).unconstrained()\n                                         for name in self._param_names)\n            params_and_args = unconstrained_params + args\n            weakself = weakref.ref(self)\n\n            def compiled(*params_and_args):\n                self = weakself()\n                unconstrained_params = params_and_args[:len(self._param_names)]\n                args = params_and_args[len(self._param_names):]\n                constrained_params = {}\n                for name, unconstrained_param in zip(self._param_names, unconstrained_params):\n                    constrained_param = pyro.param(name)  # assume param has been initialized\n                    assert constrained_param.unconstrained() is unconstrained_param\n                    constrained_params[name] = constrained_param\n                return poutine.replay(self.fn, params=constrained_params)(*args, **kwargs)\n\n            if self.ignore_warnings:\n                compiled = ignore_jit_warnings()(compiled)\n            with pyro.validation_enabled(False):\n                time_compilation = self.jit_options.pop(""time_compilation"", False)\n                with optional(timed(), time_compilation) as t:\n                    self.compiled[key] = torch.jit.trace(compiled, params_and_args, **self.jit_options)\n                if time_compilation:\n                    self.compile_time = t.elapsed\n        else:\n            unconstrained_params = [pyro.param(name).unconstrained()\n                                    for name in self._param_names]\n            params_and_args = unconstrained_params + list(args)\n\n        with poutine.block(hide=self._param_names):\n            with poutine.trace(param_only=True) as param_capture:\n                ret = self.compiled[key](*params_and_args)\n\n        for name in param_capture.trace.nodes.keys():\n            if name not in self._param_names:\n                raise NotImplementedError(\'pyro.ops.jit.trace assumes all params are created on \'\n                                          \'first invocation, but found new param: {}\'.format(name))\n\n        return ret\n\n\ndef trace(fn=None, ignore_warnings=False, jit_options=None):\n    """"""\n    Lazy replacement for :func:`torch.jit.trace` that works with\n    Pyro functions that call :func:`pyro.param`.\n\n    The actual compilation artifact is stored in the ``compiled`` attribute of\n    the output. Call diagnostic methods on this attribute.\n\n    Example::\n\n        def model(x):\n            scale = pyro.param(""scale"", torch.tensor(0.5), constraint=constraints.positive)\n            return pyro.sample(""y"", dist.Normal(x, scale))\n\n        @pyro.ops.jit.trace\n        def model_log_prob_fn(x, y):\n            cond_model = pyro.condition(model, data={""y"": y})\n            tr = pyro.poutine.trace(cond_model).get_trace(x)\n            return tr.log_prob_sum()\n\n    :param callable fn: The function to be traced.\n    :param bool ignore_warnins: Whether to ignore jit warnings.\n    :param dict jit_options: Optional dict of options to pass to\n        :func:`torch.jit.trace` , e.g. ``{""optimize"": False}``.\n    """"""\n    if fn is None:\n        return lambda fn: trace(fn, ignore_warnings=ignore_warnings, jit_options=jit_options)\n    return CompiledFunction(fn, ignore_warnings=ignore_warnings, jit_options=jit_options)\n'"
pyro/ops/linalg.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\n\n\ndef rinverse(M, sym=False):\n    """"""Matrix inversion of rightmost dimensions (batched).\n\n    For 1, 2, and 3 dimensions this uses the formulae.\n    For larger matrices, it uses blockwise inversion to reduce to\n    smaller matrices.\n    """"""\n    assert M.shape[-1] == M.shape[-2]\n    if M.shape[-1] == 1:\n        return 1./M\n    elif M.shape[-1] == 2:\n        det = M[..., 0, 0]*M[..., 1, 1] - M[..., 1, 0]*M[..., 0, 1]\n        inv = torch.empty_like(M)\n        inv[..., 0, 0] = M[..., 1, 1]\n        inv[..., 1, 1] = M[..., 0, 0]\n        inv[..., 0, 1] = -M[..., 0, 1]\n        inv[..., 1, 0] = -M[..., 1, 0]\n        return inv / det.unsqueeze(-1).unsqueeze(-1)\n    elif M.shape[-1] == 3:\n        return inv3d(M, sym=sym)\n    else:\n        return torch.inverse(M)\n\n\ndef determinant_3d(H):\n    """"""\n    Returns the determinants of a batched 3-D matrix\n    """"""\n    detH = (H[..., 0, 0] * (H[..., 1, 1] * H[..., 2, 2] - H[..., 2, 1] * H[..., 1, 2]) +\n            H[..., 0, 1] * (H[..., 1, 2] * H[..., 2, 0] - H[..., 1, 0] * H[..., 2, 2]) +\n            H[..., 0, 2] * (H[..., 1, 0] * H[..., 2, 1] - H[..., 2, 0] * H[..., 1, 1]))\n    return detH\n\n\ndef eig_3d(H):\n    """"""\n    Returns the eigenvalues of a symmetric batched 3-D matrix\n    """"""\n    p1 = H[..., 0, 1].pow(2) + H[..., 0, 2].pow(2) + H[..., 1, 2].pow(2)\n    q = (H[..., 0, 0] + H[..., 1, 1] + H[..., 2, 2]) / 3\n    p2 = (H[..., 0, 0] - q).pow(2) + (H[..., 1, 1] - q).pow(2) + (H[..., 2, 2] - q).pow(2) + 2 * p1\n    p = torch.sqrt(p2 / 6)\n    B = (1 / p).unsqueeze(-1).unsqueeze(-1) * (H - q.unsqueeze(-1).unsqueeze(-1) * torch.eye(3))\n    r = determinant_3d(B) / 2\n    phi = (r.acos() / 3).unsqueeze(-1).unsqueeze(-1).expand(r.shape + (3, 3))\n    phi[r < -1 + 1e-6] = math.pi / 3\n    phi[r > 1 - 1e-6] = 0.\n\n    eig1 = q + 2 * p * torch.cos(phi[..., 0, 0])\n    eig2 = q + 2 * p * torch.cos(phi[..., 0, 0] + (2 * math.pi/3))\n    eig3 = 3 * q - eig1 - eig2\n    # eig2 <= eig3 <= eig1\n    return eig2, eig3, eig1\n\n\ndef inv3d(H, sym=False):\n    """"""\n    Calculates the inverse of a batched 3-D matrix\n    """"""\n    detH = determinant_3d(H)\n    Hinv = torch.empty_like(H)\n    Hinv[..., 0, 0] = H[..., 1, 1] * H[..., 2, 2] - H[..., 1, 2] * H[..., 2, 1]\n    Hinv[..., 1, 1] = H[..., 0, 0] * H[..., 2, 2] - H[..., 0, 2] * H[..., 2, 0]\n    Hinv[..., 2, 2] = H[..., 0, 0] * H[..., 1, 1] - H[..., 0, 1] * H[..., 1, 0]\n\n    Hinv[..., 0, 1] = H[..., 0, 2] * H[..., 2, 1] - H[..., 0, 1] * H[..., 2, 2]\n    Hinv[..., 0, 2] = H[..., 0, 1] * H[..., 1, 2] - H[..., 0, 2] * H[..., 1, 1]\n    Hinv[..., 1, 2] = H[..., 0, 2] * H[..., 1, 0] - H[..., 0, 0] * H[..., 1, 2]\n\n    if sym:\n        Hinv[..., 1, 0] = Hinv[..., 0, 1]\n        Hinv[..., 2, 0] = Hinv[..., 0, 2]\n        Hinv[..., 2, 1] = Hinv[..., 1, 2]\n    else:\n        Hinv[..., 1, 0] = H[..., 2, 0] * H[..., 1, 2] - H[..., 1, 0] * H[..., 2, 2]\n        Hinv[..., 2, 0] = H[..., 1, 0] * H[..., 2, 1] - H[..., 2, 0] * H[..., 1, 1]\n        Hinv[..., 2, 1] = H[..., 2, 0] * H[..., 0, 1] - H[..., 0, 0] * H[..., 2, 1]\n    Hinv = Hinv / detH.unsqueeze(-1).unsqueeze(-1)\n    return Hinv\n'"
pyro/ops/newton.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.autograd import grad\n\nfrom pyro.util import warn_if_nan\nfrom pyro.ops.linalg import rinverse, eig_3d\n\n\ndef newton_step(loss, x, trust_radius=None):\n    """"""\n    Performs a Newton update step to minimize loss on a batch of variables,\n    optionally constraining to a trust region [1].\n\n    This is especially usful because the final solution of newton iteration\n    is differentiable wrt the inputs, even when all but the final ``x`` is\n    detached, due to this method\'s quadratic convergence [2]. ``loss`` must be\n    twice-differentiable as a function of ``x``. If ``loss`` is ``2+d``-times\n    differentiable, then the return value of this function is ``d``-times\n    differentiable.\n\n    When ``loss`` is interpreted as a negative log probability density, then\n    the return values ``mode,cov`` of this function can be used to construct a\n    Laplace approximation ``MultivariateNormal(mode,cov)``.\n\n    .. warning:: Take care to detach the result of this function when used in\n        an optimization loop. If you forget to detach the result of this\n        function during optimization, then backprop will propagate through\n        the entire iteration process, and worse will compute two extra\n        derivatives for each step.\n\n    Example use inside a loop::\n\n        x = torch.zeros(1000, 2)  # arbitrary initial value\n        for step in range(100):\n            x = x.detach()          # block gradients through previous steps\n            x.requires_grad = True  # ensure loss is differentiable wrt x\n            loss = my_loss_function(x)\n            x = newton_step(loss, x, trust_radius=1.0)\n        # the final x is still differentiable\n\n    [1] Yuan, Ya-xiang. Iciam. Vol. 99. 2000.\n        ""A review of trust region algorithms for optimization.""\n        ftp://ftp.cc.ac.cn/pub/yyx/papers/p995.pdf\n    [2] Christianson, Bruce. Optimization Methods and Software 3.4 (1994)\n        ""Reverse accumulation and attractive fixed points.""\n        http://uhra.herts.ac.uk/bitstream/handle/2299/4338/903839.pdf\n\n    :param torch.Tensor loss: A scalar function of ``x`` to be minimized.\n    :param torch.Tensor x: A dependent variable of shape ``(N, D)``\n        where ``N`` is the batch size and ``D`` is a small number.\n    :param float trust_radius: An optional trust region trust_radius. The\n        updated value ``mode`` of this function will be within\n        ``trust_radius`` of the input ``x``.\n    :return: A pair ``(mode, cov)`` where ``mode`` is an updated tensor\n        of the same shape as the original value ``x``, and ``cov`` is an\n        esitmate of the covariance DxD matrix with\n        ``cov.shape == x.shape[:-1] + (D,D)``.\n    :rtype: tuple\n    """"""\n    if x.dim() < 1:\n        raise ValueError(\'Expected x to have at least one dimension, actual shape {}\'.format(x.shape))\n    dim = x.shape[-1]\n    if dim == 1:\n        return newton_step_1d(loss, x, trust_radius)\n    elif dim == 2:\n        return newton_step_2d(loss, x, trust_radius)\n    elif dim == 3:\n        return newton_step_3d(loss, x, trust_radius)\n    else:\n        raise NotImplementedError(\'newton_step_nd is not implemented\')\n\n\ndef newton_step_1d(loss, x, trust_radius=None):\n    """"""\n    Performs a Newton update step to minimize loss on a batch of 1-dimensional\n    variables, optionally regularizing to constrain to a trust region.\n\n    See :func:`newton_step` for details.\n\n    :param torch.Tensor loss: A scalar function of ``x`` to be minimized.\n    :param torch.Tensor x: A dependent variable with rightmost size of 1.\n    :param float trust_radius: An optional trust region trust_radius. The\n        updated value ``mode`` of this function will be within\n        ``trust_radius`` of the input ``x``.\n    :return: A pair ``(mode, cov)`` where ``mode`` is an updated tensor\n        of the same shape as the original value ``x``, and ``cov`` is an\n        esitmate of the covariance 1x1 matrix with\n        ``cov.shape == x.shape[:-1] + (1,1)``.\n    :rtype: tuple\n    """"""\n    if loss.shape != ():\n        raise ValueError(\'Expected loss to be a scalar, actual shape {}\'.format(loss.shape))\n    if x.dim() < 1 or x.shape[-1] != 1:\n        raise ValueError(\'Expected x to have rightmost size 1, actual shape {}\'.format(x.shape))\n\n    # compute derivatives\n    g = grad(loss, [x], create_graph=True)[0]\n    H = grad(g.sum(), [x], create_graph=True)[0]\n    warn_if_nan(g, \'g\')\n    warn_if_nan(H, \'H\')\n    Hinv = H.clamp(min=1e-8).reciprocal()\n    dx = -g * Hinv\n    dx[~(dx == dx)] = 0\n    if trust_radius is not None:\n        dx.clamp_(min=-trust_radius, max=trust_radius)\n\n    # apply update\n    x_new = x.detach() + dx\n    assert x_new.shape == x.shape\n    return x_new, Hinv.unsqueeze(-1)\n\n\ndef newton_step_2d(loss, x, trust_radius=None):\n    """"""\n    Performs a Newton update step to minimize loss on a batch of 2-dimensional\n    variables, optionally regularizing to constrain to a trust region.\n\n    See :func:`newton_step` for details.\n\n    :param torch.Tensor loss: A scalar function of ``x`` to be minimized.\n    :param torch.Tensor x: A dependent variable with rightmost size of 2.\n    :param float trust_radius: An optional trust region trust_radius. The\n        updated value ``mode`` of this function will be within\n        ``trust_radius`` of the input ``x``.\n    :return: A pair ``(mode, cov)`` where ``mode`` is an updated tensor\n        of the same shape as the original value ``x``, and ``cov`` is an\n        esitmate of the covariance 2x2 matrix with\n        ``cov.shape == x.shape[:-1] + (2,2)``.\n    :rtype: tuple\n    """"""\n    if loss.shape != ():\n        raise ValueError(\'Expected loss to be a scalar, actual shape {}\'.format(loss.shape))\n    if x.dim() < 1 or x.shape[-1] != 2:\n        raise ValueError(\'Expected x to have rightmost size 2, actual shape {}\'.format(x.shape))\n\n    # compute derivatives\n    g = grad(loss, [x], create_graph=True)[0]\n    H = torch.stack([grad(g[..., 0].sum(), [x], create_graph=True)[0],\n                     grad(g[..., 1].sum(), [x], create_graph=True)[0]], -1)\n    assert g.shape[-1:] == (2,)\n    assert H.shape[-2:] == (2, 2)\n    warn_if_nan(g, \'g\')\n    warn_if_nan(H, \'H\')\n\n    if trust_radius is not None:\n        # regularize to keep update within ball of given trust_radius\n        detH = H[..., 0, 0] * H[..., 1, 1] - H[..., 0, 1] * H[..., 1, 0]\n        mean_eig = (H[..., 0, 0] + H[..., 1, 1]) / 2\n        min_eig = mean_eig - (mean_eig ** 2 - detH).clamp(min=0).sqrt()\n        regularizer = (g.pow(2).sum(-1).sqrt() / trust_radius - min_eig).clamp_(min=1e-8)\n        warn_if_nan(regularizer, \'regularizer\')\n        H = H + regularizer.unsqueeze(-1).unsqueeze(-1) * torch.eye(2, dtype=H.dtype, device=H.device)\n\n    # compute newton update\n    Hinv = rinverse(H, sym=True)\n    warn_if_nan(Hinv, \'Hinv\')\n\n    # apply update\n    x_new = x.detach() - (Hinv * g.unsqueeze(-2)).sum(-1)\n    assert x_new.shape == x.shape\n    return x_new, Hinv\n\n\ndef newton_step_3d(loss, x, trust_radius=None):\n    """"""\n    Performs a Newton update step to minimize loss on a batch of 3-dimensional\n    variables, optionally regularizing to constrain to a trust region.\n\n    See :func:`newton_step` for details.\n\n    :param torch.Tensor loss: A scalar function of ``x`` to be minimized.\n    :param torch.Tensor x: A dependent variable with rightmost size of 2.\n    :param float trust_radius: An optional trust region trust_radius. The\n        updated value ``mode`` of this function will be within\n        ``trust_radius`` of the input ``x``.\n    :return: A pair ``(mode, cov)`` where ``mode`` is an updated tensor\n        of the same shape as the original value ``x``, and ``cov`` is an\n        esitmate of the covariance 3x3 matrix with\n        ``cov.shape == x.shape[:-1] + (3,3)``.\n    :rtype: tuple\n    """"""\n    if loss.shape != ():\n        raise ValueError(\'Expected loss to be a scalar, actual shape {}\'.format(loss.shape))\n    if x.dim() < 1 or x.shape[-1] != 3:\n        raise ValueError(\'Expected x to have rightmost size 3, actual shape {}\'.format(x.shape))\n\n    # compute derivatives\n    g = grad(loss, [x], create_graph=True)[0]\n    H = torch.stack([grad(g[..., 0].sum(), [x], create_graph=True)[0],\n                     grad(g[..., 1].sum(), [x], create_graph=True)[0],\n                     grad(g[..., 2].sum(), [x], create_graph=True)[0]], -1)\n    assert g.shape[-1:] == (3,)\n    assert H.shape[-2:] == (3, 3)\n    warn_if_nan(g, \'g\')\n    warn_if_nan(H, \'H\')\n\n    if trust_radius is not None:\n        # regularize to keep update within ball of given trust_radius\n        # calculate eigenvalues of symmetric matrix\n        min_eig, _, _ = eig_3d(H)\n        regularizer = (g.pow(2).sum(-1).sqrt() / trust_radius - min_eig).clamp_(min=1e-8)\n        warn_if_nan(regularizer, \'regularizer\')\n        H = H + regularizer.unsqueeze(-1).unsqueeze(-1) * torch.eye(3, dtype=H.dtype, device=H.device)\n\n    # compute newton update\n    Hinv = rinverse(H, sym=True)\n    warn_if_nan(Hinv, \'Hinv\')\n\n    # apply update\n    x_new = x.detach() - (Hinv * g.unsqueeze(-2)).sum(-1)\n    assert x_new.shape == x.shape, ""{} {}"".format(x_new.shape, x.shape)\n    return x_new, Hinv\n'"
pyro/ops/packed.py,15,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\n\nfrom pyro.distributions.util import is_identically_one\nfrom pyro.util import ignore_jit_warnings\n\n\ndef pack(value, dim_to_symbol):\n    """"""\n    Converts an unpacked tensor to a packed tensor.\n\n    :param value: a number or tensor\n    :param dim_to_symbol: a map from negative integers to characters\n    """"""\n    if isinstance(value, torch.Tensor):\n        assert not hasattr(value, \'_pyro_dims\'), \'tried to pack an already-packed tensor\'\n        shape = value.shape\n        shift = len(shape)\n        try:\n            with ignore_jit_warnings():\n                dims = \'\'.join(dim_to_symbol[dim - shift]\n                               for dim, size in enumerate(shape)\n                               if size > 1)\n        except KeyError:\n            raise ValueError(\'\\n  \'.join([\n                \'Invalid tensor shape.\',\n                \'Allowed dims: {}\'.format(\', \'.join(map(str, sorted(dim_to_symbol)))),\n                \'Actual shape: {}\'.format(tuple(value.shape)),\n                ""Try adding shape assertions for your model\'s sample values and distribution parameters.""]))\n        value = value.squeeze()\n        value._pyro_dims = dims\n        assert value.dim() == len(value._pyro_dims)\n    return value\n\n\ndef unpack(value, symbol_to_dim):\n    """"""\n    Converts a packed tensor to an unpacked tensor.\n\n    :param value: a number or tensor\n    :param symbol_to_dim: a map from characters to negative integers\n    """"""\n    if isinstance(value, torch.Tensor):\n        assert value.dim() == len(value._pyro_dims)\n        if value.dim():\n            unsorted_dims = [symbol_to_dim[dim] for dim in value._pyro_dims]\n            dims = sorted(unsorted_dims)\n            value = value.permute(*(unsorted_dims.index(dim) for dim in dims))\n            shape = [1] * -min(dims)\n            for dim, size in zip(dims, value.shape):\n                shape[dim] = size\n            value = value.reshape(shape)\n        else:\n            value = value[...]  # ensure ._pyro_dims attr is not set\n    return value\n\n\ndef broadcast_all(*values, **kwargs):\n    """"""\n    Packed broadcasting of multiple tensors.\n    """"""\n    dims = kwargs.get(\'dims\')\n    sizes = {dim: size for value in values for dim, size in zip(value._pyro_dims, value.shape)}\n    if dims is None:\n        dims = \'\'.join(sorted(sizes))\n    else:\n        assert set(dims) == set(sizes)\n    shape = torch.Size(sizes[dim] for dim in dims)\n    values = list(values)\n    for i, x in enumerate(values):\n        old_dims = x._pyro_dims\n        if old_dims != dims:\n            x = x.permute(tuple(old_dims.index(dim) for dim in dims if dim in old_dims))\n            x = x.reshape(tuple(sizes[dim] if dim in old_dims else 1 for dim in dims))\n            x = x.expand(shape)\n            x._pyro_dims = dims\n            assert x.dim() == len(x._pyro_dims)\n            values[i] = x\n    return tuple(values)\n\n\ndef gather(value, index, dim):\n    """"""\n    Packed broadcasted gather of indexed values along a named dim.\n    """"""\n    assert dim in value._pyro_dims\n    assert dim not in index._pyro_dims\n    value, index = broadcast_all(value, index)\n    dims = value._pyro_dims.replace(dim, \'\')\n    pos = value._pyro_dims.index(dim)\n    with ignore_jit_warnings():\n        zero = torch.zeros(1, dtype=torch.long, device=index.device)\n    index = index.index_select(pos, zero)\n    value = value.gather(pos, index).squeeze(pos)\n    value._pyro_dims = dims\n    assert value.dim() == len(value._pyro_dims)\n    return value\n\n\ndef mul(lhs, rhs):\n    """"""\n    Packed broadcasted multiplication.\n    """"""\n    if isinstance(lhs, torch.Tensor) and isinstance(rhs, torch.Tensor):\n        dims = \'\'.join(sorted(set(lhs._pyro_dims + rhs._pyro_dims)))\n        equation = lhs._pyro_dims + \',\' + rhs._pyro_dims + \'->\' + dims\n        result = torch.einsum(equation, lhs, rhs, backend=\'torch\')\n        result._pyro_dims = dims\n        return result\n    result = lhs * rhs\n    if isinstance(lhs, torch.Tensor):\n        result._pyro_dims = lhs._pyro_dims\n    elif isinstance(rhs, torch.Tensor):\n        result._pyro_dims = rhs._pyro_dims\n    return result\n\n\ndef scale_and_mask(tensor, scale=1.0, mask=None):\n    """"""\n    Scale and mask a packed tensor, broadcasting and avoiding unnecessary ops.\n\n    :param torch.Tensor tensor: a packed tensor\n    :param scale: a positive scale\n    :type scale: torch.Tensor or number\n    :param mask: an optional packed tensor mask\n    :type mask: torch.BoolTensor or None\n    """"""\n    if isinstance(scale, torch.Tensor) and scale.dim():\n        raise NotImplementedError(\'non-scalar scale is not supported\')\n    if mask is None:\n        if is_identically_one(scale):\n            return tensor\n        result = tensor * scale\n        result._pyro_dims = tensor._pyro_dims\n        return result\n    tensor, mask = broadcast_all(tensor, mask)\n    result = torch.where(mask, tensor, tensor.new_zeros(()))\n    result._pyro_dims = tensor._pyro_dims\n    return result\n\n\ndef neg(value):\n    """"""\n    Packed negation.\n    """"""\n    result = -value\n    if isinstance(value, torch.Tensor):\n        result._pyro_dims = value._pyro_dims\n    return result\n\n\ndef exp(value):\n    """"""\n    Packed pointwise exponential.\n    """"""\n    if isinstance(value, torch.Tensor):\n        result = value.exp()\n        result._pyro_dims = value._pyro_dims\n    else:\n        result = math.exp(value)\n    return result\n\n\ndef rename_equation(equation, *operands):\n    """"""\n    Renames symbols in an einsum/ubersum equation to match the\n    ``.pyro_dims`` attributes of packed ``operands``.\n    """"""\n    inputs, outputs = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n    assert len(inputs) == len(operands)\n    rename = {old: new\n              for input_, operand in zip(inputs, operands)\n              for old, new in zip(input_, operand._pyro_dims)}\n    return \'\'.join(rename.get(s, s) for s in equation)\n'"
pyro/ops/rings.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport weakref\nfrom abc import ABCMeta, abstractmethod\n\nimport torch\n\nfrom pyro.ops.einsum import contract\nfrom pyro.ops.einsum.adjoint import SAMPLE_SYMBOL, Backward\nfrom pyro.util import ignore_jit_warnings\n\n\nclass Ring(object, metaclass=ABCMeta):\n    """"""\n    Abstract tensor ring class.\n\n    Each tensor ring class has a notion of ``dims`` that can be sum-contracted\n    out, and a notion of ``ordinal`` that represents a set of plate dimensions\n    that can be broadcasted-up or product-contracted out.\n    Implementations should cache intermediate results to be compatible with\n    :func:`~opt_einsum.shared_intermediates`.\n\n    Dims are characters (string or unicode).\n    Ordinals are frozensets of characters.\n\n    :param dict cache: an optional :func:`~opt_einsum.shared_intermediates`\n        cache.\n    """"""\n    def __init__(self, cache=None):\n        self._cache = {} if cache is None else cache\n\n    def _hash_by_id(self, tensor):\n        """"""\n        Returns the id of a tensor and saves the tensor so that this id can be\n        used as a key in the cache without risk of the id being recycled.\n        """"""\n        result = id(tensor)\n        assert self._cache.setdefault((\'tensor\', result), tensor) is tensor\n        return result\n\n    @abstractmethod\n    def sumproduct(self, terms, dims):\n        """"""\n        Multiply all ``terms`` together, then sum-contract out all ``dims``\n        from the result.\n\n        :param list terms: a list of tensors\n        :param dims: an iterable of sum dims to contract\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def product(self, term, ordinal):\n        """"""\n        Product-contract the given ``term`` along any plate dimensions\n        present in given ``ordinal``.\n\n        :param torch.Tensor term: the term to contract\n        :param frozenset ordinal: an ordinal specifying plates to contract\n        """"""\n        raise NotImplementedError\n\n    def broadcast(self, term, ordinal):\n        """"""\n        Broadcast the given ``term`` by expanding along any plate dimensions\n        present in ``ordinal`` but not ``term``.\n\n        :param torch.Tensor term: the term to expand\n        :param frozenset ordinal: an ordinal specifying plates\n        """"""\n        dims = term._pyro_dims\n        missing_dims = \'\'.join(sorted(set(ordinal) - set(dims)))\n        if missing_dims:\n            key = \'broadcast\', self._hash_by_id(term), missing_dims\n            if key in self._cache:\n                term = self._cache[key]\n            else:\n                missing_shape = tuple(self._dim_to_size[dim] for dim in missing_dims)\n                term = term.expand(missing_shape + term.shape)\n                dims = missing_dims + dims\n                self._cache[key] = term\n                term._pyro_dims = dims\n        return term\n\n    @abstractmethod\n    def inv(self, term):\n        """"""\n        Computes the reciprocal of a term, for use in inclusion-exclusion.\n\n        :param torch.Tensor term: the term to invert\n        """"""\n        raise NotImplementedError\n\n    def global_local(self, term, dims, ordinal):\n        r""""""\n        Computes global and local terms for tensor message passing\n        using inclusion-exclusion::\n\n            term / sum(term, dims) * product(sum(term, dims), ordinal)\n            \\____________________/   \\_______________________________/\n                  local part                    global part\n\n        :param torch.Tensor term: the term to contract\n        :param dims: an iterable of sum dims to contract\n        :param frozenset ordinal: an ordinal specifying plates to contract\n        :return: a tuple ``(global_part, local_part)`` as defined above\n        :rtype: tuple\n        """"""\n        assert dims, \'dims was empty, use .product() instead\'\n        key = \'global_local\', self._hash_by_id(term), frozenset(dims), ordinal\n        if key in self._cache:\n            return self._cache[key]\n\n        term_sum = self.sumproduct([term], dims)\n        global_part = self.product(term_sum, ordinal)\n        with ignore_jit_warnings():\n            local_part = self.sumproduct([term, self.inv(term_sum)], set())\n        assert sorted(local_part._pyro_dims) == sorted(term._pyro_dims)\n        result = global_part, local_part\n        self._cache[key] = result\n        return result\n\n\nclass LinearRing(Ring):\n    """"""\n    Ring of sum-product operations in linear space.\n\n    Tensor dimensions are packed; to read the name of a tensor, read the\n    ``._pyro_dims`` attribute, which is a string of dimension names aligned\n    with the tensor\'s shape.\n    """"""\n    _backend = \'torch\'\n\n    def __init__(self, cache=None, dim_to_size=None):\n        super().__init__(cache=cache)\n        self._dim_to_size = {} if dim_to_size is None else dim_to_size\n\n    def sumproduct(self, terms, dims):\n        inputs = [term._pyro_dims for term in terms]\n        output = \'\'.join(sorted(set(\'\'.join(inputs)) - set(dims)))\n        equation = \',\'.join(inputs) + \'->\' + output\n        term = contract(equation, *terms, backend=self._backend)\n        term._pyro_dims = output\n        return term\n\n    def product(self, term, ordinal):\n        dims = term._pyro_dims\n        for dim in sorted(ordinal, reverse=True):\n            pos = dims.find(dim)\n            if pos != -1:\n                key = \'product\', self._hash_by_id(term), dim\n                if key in self._cache:\n                    term = self._cache[key]\n                else:\n                    term = term.prod(pos)\n                    dims = dims.replace(dim, \'\')\n                    self._cache[key] = term\n                    term._pyro_dims = dims\n        return term\n\n    def inv(self, term):\n        key = \'inv\', self._hash_by_id(term)\n        if key in self._cache:\n            return self._cache[key]\n\n        result = term.reciprocal()\n        result = result.clamp(max=torch.finfo(result.dtype).max)  # avoid nan due to inf / inf\n        result._pyro_dims = term._pyro_dims\n        self._cache[key] = result\n        return result\n\n\nclass LogRing(Ring):\n    """"""\n    Ring of sum-product operations in log space.\n\n    Tensor values are in log units, so ``sum`` is implemented as ``logsumexp``,\n    and ``product`` is implemented as ``sum``.\n    Tensor dimensions are packed; to read the name of a tensor, read the\n    ``._pyro_dims`` attribute, which is a string of dimension names aligned\n    with the tensor\'s shape.\n    """"""\n    _backend = \'pyro.ops.einsum.torch_log\'\n\n    def __init__(self, cache=None, dim_to_size=None):\n        super().__init__(cache=cache)\n        self._dim_to_size = {} if dim_to_size is None else dim_to_size\n\n    def sumproduct(self, terms, dims):\n        inputs = [term._pyro_dims for term in terms]\n        output = \'\'.join(sorted(set(\'\'.join(inputs)) - set(dims)))\n        equation = \',\'.join(inputs) + \'->\' + output\n        term = contract(equation, *terms, backend=self._backend)\n        term._pyro_dims = output\n        return term\n\n    def product(self, term, ordinal):\n        dims = term._pyro_dims\n        for dim in sorted(ordinal, reverse=True):\n            pos = dims.find(dim)\n            if pos != -1:\n                key = \'product\', self._hash_by_id(term), dim\n                if key in self._cache:\n                    term = self._cache[key]\n                else:\n                    term = term.sum(pos)\n                    dims = dims.replace(dim, \'\')\n                    self._cache[key] = term\n                    term._pyro_dims = dims\n        return term\n\n    def inv(self, term):\n        key = \'inv\', self._hash_by_id(term)\n        if key in self._cache:\n            return self._cache[key]\n\n        result = -term\n        result = result.clamp(max=torch.finfo(result.dtype).max)  # avoid nan due to inf - inf\n        result._pyro_dims = term._pyro_dims\n        self._cache[key] = result\n        return result\n\n\nclass _SampleProductBackward(Backward):\n    """"""\n    Backward-sample implementation of product.\n\n    This is agnostic to sampler implementation, and hence can be used both by\n    :class:`MapRing` (temperature 0 sampling) and :class:`SampleRing`\n    (temperature 1 sampling).\n    """"""\n    def __init__(self, ring, term, ordinal):\n        self.ring = ring\n        self.term = term\n        self.ordinal = ordinal\n\n    def process(self, message):\n        if message is not None:\n            sample_dims = message._pyro_sample_dims\n            message = self.ring.broadcast(message, self.ordinal)\n            if message._pyro_dims.index(SAMPLE_SYMBOL) != 0:\n                dims = SAMPLE_SYMBOL + message._pyro_dims.replace(SAMPLE_SYMBOL, \'\')\n                message = message.permute(tuple(map(message._pyro_dims.find, dims)))\n                message._pyro_dims = dims\n                assert message.dim() == len(message._pyro_dims)\n            message._pyro_sample_dims = sample_dims\n            assert message.size(0) == len(message._pyro_sample_dims)\n        yield self.term._pyro_backward, message\n\n\nclass MapRing(LogRing):\n    """"""\n    Ring of forward-maxsum backward-argmax operations.\n    """"""\n    _backend = \'pyro.ops.einsum.torch_map\'\n\n    def product(self, term, ordinal):\n        result = super().product(term, ordinal)\n        if hasattr(term, \'_pyro_backward\'):\n            result._pyro_backward = _SampleProductBackward(self, term, ordinal)\n        return result\n\n\nclass SampleRing(LogRing):\n    """"""\n    Ring of forward-sumproduct backward-sample operations in log space.\n    """"""\n    _backend = \'pyro.ops.einsum.torch_sample\'\n\n    def product(self, term, ordinal):\n        result = super().product(term, ordinal)\n        if hasattr(term, \'_pyro_backward\'):\n            result._pyro_backward = _SampleProductBackward(self, term, ordinal)\n        return result\n\n\nclass _MarginalProductBackward(Backward):\n    """"""\n    Backward-marginal implementation of product, using inclusion-exclusion.\n    """"""\n    def __init__(self, ring, term, ordinal, result):\n        self.ring = ring\n        self.term = term\n        self.ordinal = ordinal\n        self.result = weakref.ref(result)\n\n    def process(self, message):\n        ring = self.ring\n        term = self.term\n        result = self.result()\n        factors = [result]\n        if message is not None:\n            message._pyro_dims = result._pyro_dims\n            factors.append(message)\n        if term._pyro_backward.is_leaf:\n            product = ring.sumproduct(factors, set())\n            message = ring.broadcast(product, self.ordinal)\n        else:\n            factors.append(ring.inv(term))\n            message = ring.sumproduct(factors, set())\n        yield term._pyro_backward, message\n\n\nclass MarginalRing(LogRing):\n    """"""\n    Ring of forward-sumproduct backward-marginal operations in log space.\n    """"""\n    _backend = \'pyro.ops.einsum.torch_marginal\'\n\n    def product(self, term, ordinal):\n        result = super().product(term, ordinal)\n        if hasattr(term, \'_pyro_backward\'):\n            result._pyro_backward = _MarginalProductBackward(self, term, ordinal, result)\n        return result\n\n\nBACKEND_TO_RING = {\n    \'torch\': LinearRing,\n    \'pyro.ops.einsum.torch_log\': LogRing,\n    \'pyro.ops.einsum.torch_map\': MapRing,\n    \'pyro.ops.einsum.torch_sample\': SampleRing,\n    \'pyro.ops.einsum.torch_marginal\': MarginalRing,\n}\n'"
pyro/ops/special.py,12,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport math\nimport operator\n\nimport torch\n\n\nclass _SafeLog(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return x.log()\n\n    @staticmethod\n    def backward(ctx, grad):\n        x, = ctx.saved_tensors\n        return grad / x.clamp(min=torch.finfo(x.dtype).eps)\n\n\ndef safe_log(x):\n    """"""\n    Like :func:`torch.log` but avoids infinite gradients at log(0)\n    by clamping them to at most ``1 / finfo.eps``.\n    """"""\n    return _SafeLog.apply(x)\n\n\ndef log_beta(x, y, tol=0.):\n    """"""\n    Computes log Beta function.\n\n    When ``tol >= 0.02`` this uses a shifted Stirling\'s approximation to the\n    log Beta function. The approximation adapts Stirling\'s approximation of the\n    log Gamma function::\n\n        lgamma(z) \xe2\x89\x88 (z - 1/2) * log(z) - z + log(2 * pi) / 2\n\n    to approximate the log Beta function::\n\n        log_beta(x, y) \xe2\x89\x88 ((x-1/2) * log(x) + (y-1/2) * log(y)\n                          - (x+y-1/2) * log(x+y) + log(2*pi)/2)\n\n    The approximation additionally improves accuracy near zero by iteratively\n    shifting the log Gamma approximation using the recursion::\n\n        lgamma(x) = lgamma(x + 1) - log(x)\n\n    If this recursion is applied ``n`` times, then absolute error is bounded by\n    ``error < 0.082 / n < tol``, thus we choose ``n`` based on the user\n    provided ``tol``.\n\n    :param torch.Tensor x: A positive tensor.\n    :param torch.Tensor y: A positive tensor.\n    :param float tol: Bound on maximum absolute error. Defaults to 0.1. For\n        very small ``tol``, this function simply defers to :func:`log_beta`.\n    :rtype: torch.Tensor\n    """"""\n    assert isinstance(tol, (float, int)) and tol >= 0\n    if tol < 0.02:\n        # At small tolerance it is cheaper to defer to torch.lgamma().\n        return x.lgamma() + y.lgamma() - (x + y).lgamma()\n\n    # This bound holds for arbitrary x,y. We could do better with large x,y.\n    shift = int(math.ceil(0.082 / tol))\n\n    xy = x + y\n    factors = []\n    for _ in range(shift):\n        factors.append(xy / (x * y))\n        x = x + 1\n        y = y + 1\n        xy = xy + 1\n\n    log_factor = functools.reduce(operator.mul, factors).log()\n\n    return (log_factor + (x - 0.5) * x.log() + (y - 0.5) * y.log()\n            - (xy - 0.5) * xy.log() + (math.log(2 * math.pi) / 2 - shift))\n\n\n@torch.no_grad()\ndef log_binomial(n, k, tol=0.):\n    """"""\n    Computes log binomial coefficient.\n\n    When ``tol >= 0.02`` this uses a shifted Stirling\'s approximation to the\n    log Beta function via :func:`log_beta`.\n\n    :param torch.Tensor n: A nonnegative integer tensor.\n    :param torch.Tensor k: An integer tensor ranging in ``[0, n]``.\n    :rtype: torch.Tensor\n    """"""\n    assert isinstance(tol, (float, int)) and tol >= 0\n    n_plus_1 = n + 1\n    if tol < 0.02:\n        # At small tolerance it is cheaper to defer to torch.lgamma().\n        return n_plus_1.lgamma() - (k + 1).lgamma() - (n_plus_1 - k).lgamma()\n\n    return -n_plus_1.log() - log_beta(k + 1, n_plus_1 - k, tol=tol)\n'"
pyro/ops/ssm_gp.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.nn import PyroModule, pyro_method, PyroParam\n\nroot_three = math.sqrt(3.0)\nroot_five = math.sqrt(5.0)\nfive_thirds = 5.0 / 3.0\n\n\nclass MaternKernel(PyroModule):\n    """"""\n    Provides the building blocks for representing univariate Gaussian Processes (GPs)\n    with Matern kernels as state space models.\n\n    :param float nu: The order of the Matern kernel (one of 0.5, 1.5 or 2.5)\n    :param int num_gps: the number of GPs\n    :param torch.Tensor length_scale_init: optional `num_gps`-dimensional vector of initializers\n        for the length scale\n    :param torch.Tensor kernel_scale_init: optional `num_gps`-dimensional vector of initializers\n        for the kernel scale\n\n    **References**\n\n    [1] `Kalman Filtering and Smoothing Solutions to Temporal Gaussian Process Regression Models`,\n        Jouni Hartikainen and Simo Sarkka.\n    [2] `Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression`,\n        Arno Solin.\n    """"""\n    def __init__(self, nu=1.5, num_gps=1, length_scale_init=None, kernel_scale_init=None):\n        if nu not in [0.5, 1.5, 2.5]:\n            raise NotImplementedError(""The only supported values of nu are 0.5, 1.5 and 2.5"")\n        self.nu = nu\n        self.state_dim = {0.5: 1, 1.5: 2, 2.5: 3}[nu]\n        self.num_gps = num_gps\n\n        if length_scale_init is None:\n            length_scale_init = torch.ones(num_gps)\n        assert length_scale_init.shape == (num_gps,)\n\n        if kernel_scale_init is None:\n            kernel_scale_init = torch.ones(num_gps)\n        assert kernel_scale_init.shape == (num_gps,)\n\n        super().__init__()\n\n        self.length_scale = PyroParam(length_scale_init, constraint=constraints.positive)\n        self.kernel_scale = PyroParam(kernel_scale_init, constraint=constraints.positive)\n\n        if self.state_dim > 1:\n            for x in range(self.state_dim):\n                for y in range(self.state_dim):\n                    mask = torch.zeros(self.state_dim, self.state_dim)\n                    mask[x, y] = 1.0\n                    self.register_buffer(""mask{}{}"".format(x, y), mask)\n\n    @pyro_method\n    def transition_matrix(self, dt):\n        """"""\n        Compute the (exponentiated) transition matrix of the GP latent space.\n        The resulting matrix has layout (num_gps, old_state, new_state), i.e. this\n        matrix multiplies states from the right.\n\n        See section 5 in reference [1] for details.\n\n        :param float dt: the time interval over which the GP latent space evolves.\n        :returns torch.Tensor: a 3-dimensional tensor of transition matrices of shape\n            (num_gps, state_dim, state_dim).\n        """"""\n        if self.nu == 0.5:\n            rho = self.length_scale.unsqueeze(-1).unsqueeze(-1)\n            return torch.exp(-dt / rho)\n        elif self.nu == 1.5:\n            rho = self.length_scale.unsqueeze(-1).unsqueeze(-1)\n            dt_rho = dt / rho\n            trans = (1.0 + root_three * dt_rho) * self.mask00 + \\\n                (-3.0 * dt_rho / rho) * self.mask01 + \\\n                dt * self.mask10 + \\\n                (1.0 - root_three * dt_rho) * self.mask11\n            return torch.exp(-root_three * dt_rho) * trans\n        elif self.nu == 2.5:\n            rho = self.length_scale.unsqueeze(-1).unsqueeze(-1)\n            dt_rho = root_five * dt / rho\n            dt_rho_sq = dt_rho.pow(2.0)\n            dt_rho_cu = dt_rho.pow(3.0)\n            dt_rho_qu = dt_rho.pow(4.0)\n            dt_sq = dt ** 2.0\n            trans = (1.0 + dt_rho + 0.5 * dt_rho_sq) * self.mask00 + \\\n                (-0.5 * dt_rho_cu / dt) * self.mask01 + \\\n                ((0.5 * dt_rho_qu - dt_rho_cu) / dt_sq) * self.mask02 + \\\n                ((dt_rho + 1.0) * dt) * self.mask10 + \\\n                (1.0 + dt_rho - dt_rho_sq) * self.mask11 + \\\n                ((dt_rho_cu - 3.0 * dt_rho_sq) / dt) * self.mask12 + \\\n                (0.5 * dt_sq) * self.mask20 + \\\n                ((1.0 - 0.5 * dt_rho) * dt) * self.mask21 + \\\n                (1.0 - 2.0 * dt_rho + 0.5 * dt_rho_sq) * self.mask22\n            return torch.exp(-dt_rho) * trans\n\n    @pyro_method\n    def stationary_covariance(self):\n        """"""\n        Compute the stationary state covariance. See Eqn. 3.26 in reference [2].\n\n        :returns torch.Tensor: a 3-dimensional tensor of covariance matrices of shape\n            (num_gps, state_dim, state_dim).\n        """"""\n        if self.nu == 0.5:\n            sigmasq = self.kernel_scale.pow(2).unsqueeze(-1).unsqueeze(-1)\n            return sigmasq\n        elif self.nu == 1.5:\n            sigmasq = self.kernel_scale.pow(2).unsqueeze(-1).unsqueeze(-1)\n            rhosq = self.length_scale.pow(2).unsqueeze(-1).unsqueeze(-1)\n            p_infinity = self.mask00 + (3.0 / rhosq) * self.mask11\n            return sigmasq * p_infinity\n        elif self.nu == 2.5:\n            sigmasq = self.kernel_scale.pow(2).unsqueeze(-1).unsqueeze(-1)\n            rhosq = self.length_scale.pow(2).unsqueeze(-1).unsqueeze(-1)\n            p_infinity = 0.0\n            p_infinity = self.mask00 + \\\n                (five_thirds / rhosq) * (self.mask11 - self.mask02 - self.mask20) + \\\n                (25.0 / rhosq.pow(2.0)) * self.mask22\n            return sigmasq * p_infinity\n\n    @pyro_method\n    def process_covariance(self, A):\n        """"""\n        Given a transition matrix `A` computed with `transition_matrix` compute the\n        the process covariance as described in Eqn. 3.11 in reference [2].\n\n        :returns torch.Tensor: a batched covariance matrix of shape (num_gps, state_dim, state_dim)\n        """"""\n        assert A.shape[-2:] == (self.state_dim, self.state_dim)\n        p = self.stationary_covariance()\n        q = p - torch.matmul(A.transpose(-1, -2), torch.matmul(p, A))\n        return q\n\n    @pyro_method\n    def transition_matrix_and_covariance(self, dt):\n        """"""\n        Get the transition matrix and process covariance corresponding to a time interval `dt`.\n\n        :param float dt: the time interval over which the GP latent space evolves.\n        :returns tuple: (`transition_matrix`, `process_covariance`) both 3-dimensional tensors of\n            shape (num_gps, state_dim, state_dim)\n        """"""\n        trans_matrix = self.transition_matrix(dt)\n        process_covar = self.process_covariance(trans_matrix)\n        return trans_matrix, process_covar\n'"
pyro/ops/stats.py,54,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport numbers\n\nimport torch\n\nfrom .tensor_utils import next_fast_len\n\n\ndef _compute_chain_variance_stats(input):\n    # compute within-chain variance and variance estimator\n    # input has shape N x C x sample_shape\n    N = input.size(0)\n    chain_var = input.var(dim=0)\n    var_within = chain_var.mean(dim=0)\n    var_estimator = (N - 1) / N * var_within\n    if input.size(1) > 1:\n        chain_mean = input.mean(dim=0)\n        var_between = chain_mean.var(dim=0)\n        var_estimator = var_estimator + var_between\n    else:\n        # to make rho_k is the same as autocorrelation when num_chains == 1\n        # in computing effective_sample_size\n        var_within = var_estimator\n    return var_within, var_estimator\n\n\ndef gelman_rubin(input, chain_dim=0, sample_dim=1):\n    """"""\n    Computes R-hat over chains of samples. It is required that\n    ``input.size(sample_dim) >= 2`` and ``input.size(chain_dim) >= 2``.\n\n    :param torch.Tensor input: the input tensor.\n    :param int chain_dim: the chain dimension.\n    :param int sample_dim: the sample dimension.\n    :returns torch.Tensor: R-hat of ``input``.\n    """"""\n    assert input.dim() >= 2\n    assert input.size(sample_dim) >= 2\n    assert input.size(chain_dim) >= 2\n    # change input.shape to 1 x 1 x input.shape\n    # then transpose sample_dim with 0, chain_dim with 1\n    sample_dim = input.dim() + sample_dim if sample_dim < 0 else sample_dim\n    chain_dim = input.dim() + chain_dim if chain_dim < 0 else chain_dim\n    assert chain_dim != sample_dim\n    input = input.reshape((1, 1) + input.shape)\n    input = input.transpose(0, sample_dim + 2).transpose(1, chain_dim + 2)\n\n    var_within, var_estimator = _compute_chain_variance_stats(input)\n    rhat = (var_estimator / var_within).sqrt()\n    return rhat.squeeze(max(sample_dim, chain_dim)).squeeze(min(sample_dim, chain_dim))\n\n\ndef split_gelman_rubin(input, chain_dim=0, sample_dim=1):\n    """"""\n    Computes R-hat over chains of samples. It is required that\n    ``input.size(sample_dim) >= 4``.\n\n    :param torch.Tensor input: the input tensor.\n    :param int chain_dim: the chain dimension.\n    :param int sample_dim: the sample dimension.\n    :returns torch.Tensor: split R-hat of ``input``.\n    """"""\n    assert input.dim() >= 2\n    assert input.size(sample_dim) >= 4\n    # change input.shape to 1 x 1 x input.shape\n    # then transpose chain_dim with 0, sample_dim with 1\n    sample_dim = input.dim() + sample_dim if sample_dim < 0 else sample_dim\n    chain_dim = input.dim() + chain_dim if chain_dim < 0 else chain_dim\n    assert chain_dim != sample_dim\n    input = input.reshape((1, 1) + input.shape)\n    input = input.transpose(0, chain_dim + 2).transpose(1, sample_dim + 2)\n\n    N_half = input.size(1) // 2\n    new_input = torch.stack([input[:, :N_half], input[:, -N_half:]], dim=1)\n    new_input = new_input.reshape((-1, N_half) + input.shape[2:])\n    split_rhat = gelman_rubin(new_input)\n    return split_rhat.squeeze(max(sample_dim, chain_dim)).squeeze(min(sample_dim, chain_dim))\n\n\ndef autocorrelation(input, dim=0):\n    """"""\n    Computes the autocorrelation of samples at dimension ``dim``.\n\n    Reference: https://en.wikipedia.org/wiki/Autocorrelation#Efficient_computation\n\n    :param torch.Tensor input: the input tensor.\n    :param int dim: the dimension to calculate autocorrelation.\n    :returns torch.Tensor: autocorrelation of ``input``.\n    """"""\n    if (not input.is_cuda) and (not torch.backends.mkl.is_available()):\n        raise NotImplementedError(""For CPU tensor, this method is only supported ""\n                                  ""with MKL installed."")\n\n    # Adapted from Stan implementation\n    # https://github.com/stan-dev/math/blob/develop/stan/math/prim/mat/fun/autocorrelation.hpp\n    N = input.size(dim)\n    M = next_fast_len(N)\n    M2 = 2 * M\n\n    # transpose dim with -1 for Fourier transform\n    input = input.transpose(dim, -1)\n\n    # centering and padding x\n    centered_signal = input - input.mean(dim=-1, keepdim=True)\n    pad = torch.zeros(input.shape[:-1] + (M2 - N,), dtype=input.dtype, device=input.device)\n    centered_signal = torch.cat([centered_signal, pad], dim=-1)\n\n    # Fourier transform\n    freqvec = torch.rfft(centered_signal, signal_ndim=1, onesided=False)\n    # take square of magnitude of freqvec (or freqvec x freqvec*)\n    freqvec_gram = freqvec.pow(2).sum(-1, keepdim=True)\n    freqvec_gram = torch.cat([freqvec_gram, torch.zeros(freqvec_gram.shape, dtype=input.dtype,\n                                                        device=input.device)], dim=-1)\n    # inverse Fourier transform\n    autocorr = torch.irfft(freqvec_gram, signal_ndim=1, onesided=False)\n\n    # truncate and normalize the result, then transpose back to original shape\n    autocorr = autocorr[..., :N]\n    autocorr = autocorr / torch.tensor(range(N, 0, -1), dtype=input.dtype, device=input.device)\n    autocorr = autocorr / autocorr[..., :1]\n    return autocorr.transpose(dim, -1)\n\n\ndef autocovariance(input, dim=0):\n    """"""\n    Computes the autocovariance of samples at dimension ``dim``.\n\n    :param torch.Tensor input: the input tensor.\n    :param int dim: the dimension to calculate autocorrelation.\n    :returns torch.Tensor: autocorrelation of ``input``.\n    """"""\n    return autocorrelation(input, dim) * input.var(dim, unbiased=False, keepdim=True)\n\n\ndef _cummin(input):\n    """"""\n    Computes cummulative minimum of input at dimension ``dim=0``.\n\n    :param torch.Tensor input: the input tensor.\n    :returns torch.Tensor: accumulate min of `input` at dimension `dim=0`.\n    """"""\n    # FIXME: is there a better trick to find accumulate min of a sequence?\n    N = input.size(0)\n    input_tril = input.unsqueeze(0).repeat((N,) + (1,) * input.dim())\n    triu_mask = (torch.ones(N, N, dtype=input.dtype, device=input.device)\n                 .triu(diagonal=1).reshape((N, N) + (1,) * (input.dim() - 1)))\n    triu_mask = triu_mask.expand((N, N) + input.shape[1:]) > 0.5\n    input_tril.masked_fill_(triu_mask, input.max())\n    return input_tril.min(dim=1)[0]\n\n\ndef effective_sample_size(input, chain_dim=0, sample_dim=1):\n    """"""\n    Computes effective sample size of input.\n\n    Reference:\n\n    [1] `Introduction to Markov Chain Monte Carlo`,\n        Charles J. Geyer\n\n    [2] `Stan Reference Manual version 2.18`,\n        Stan Development Team\n\n    :param torch.Tensor input: the input tensor.\n    :param int chain_dim: the chain dimension.\n    :param int sample_dim: the sample dimension.\n    :returns torch.Tensor: effective sample size of ``input``.\n    """"""\n    assert input.dim() >= 2\n    assert input.size(sample_dim) >= 2\n    # change input.shape to 1 x 1 x input.shape\n    # then transpose sample_dim with 0, chain_dim with 1\n    sample_dim = input.dim() + sample_dim if sample_dim < 0 else sample_dim\n    chain_dim = input.dim() + chain_dim if chain_dim < 0 else chain_dim\n    assert chain_dim != sample_dim\n    input = input.reshape((1, 1) + input.shape)\n    input = input.transpose(0, sample_dim + 2).transpose(1, chain_dim + 2)\n\n    N, C = input.size(0), input.size(1)\n    # find autocovariance for each chain at lag k\n    gamma_k_c = autocovariance(input, dim=0)  # N x C x sample_shape\n\n    # find autocorrelation at lag k (from Stan reference)\n    var_within, var_estimator = _compute_chain_variance_stats(input)\n    rho_k = (var_estimator - var_within + gamma_k_c.mean(dim=1)) / var_estimator\n    rho_k[0] = 1  # correlation at lag 0 is always 1\n\n    # initial positive sequence (formula 1.18 in [1]) applied for autocorrelation\n    Rho_k = rho_k if N % 2 == 0 else rho_k[:-1]\n    Rho_k = Rho_k.reshape((N // 2, 2) + Rho_k.shape[1:]).sum(dim=1)\n\n    # separate the first index\n    Rho_init = Rho_k[0]\n\n    if Rho_k.size(0) > 1:\n        # Theoretically, Rho_k is positive, but due to noise of correlation computation,\n        # Rho_k might not be positive at some point. So we need to truncate (ignore first index).\n        Rho_positive = Rho_k[1:].clamp(min=0)\n\n        # Now we make the initial monotone (decreasing) sequence.\n        Rho_monotone = _cummin(Rho_positive)\n\n        # Formula 1.19 in [1]\n        tau = -1 + 2 * Rho_init + 2 * Rho_monotone.sum(dim=0)\n    else:\n        tau = -1 + 2 * Rho_init\n\n    n_eff = C * N / tau\n    return n_eff.squeeze(max(sample_dim, chain_dim)).squeeze(min(sample_dim, chain_dim))\n\n\ndef resample(input, num_samples, dim=0, replacement=False):\n    """"""\n    Draws ``num_samples`` samples from ``input`` at dimension ``dim``.\n\n    :param torch.Tensor input: the input tensor.\n    :param int num_samples: the number of samples to draw from ``input``.\n    :param int dim: dimension to draw from ``input``.\n    :returns torch.Tensor: samples drawn randomly from ``input``.\n    """"""\n    weights = torch.ones(input.size(dim), dtype=input.dtype, device=input.device)\n    indices = torch.multinomial(weights, num_samples, replacement)\n    return input.index_select(dim, indices)\n\n\ndef quantile(input, probs, dim=0):\n    """"""\n    Computes quantiles of ``input`` at ``probs``. If ``probs`` is a scalar,\n    the output will be squeezed at ``dim``.\n\n    :param torch.Tensor input: the input tensor.\n    :param list probs: quantile positions.\n    :param int dim: dimension to take quantiles from ``input``.\n    :returns torch.Tensor: quantiles of ``input`` at ``probs``.\n    """"""\n    if isinstance(probs, (numbers.Number, list, tuple)):\n        probs = torch.tensor(probs, dtype=input.dtype, device=input.device)\n    sorted_input = input.sort(dim)[0]\n    max_index = input.size(dim) - 1\n    indices = probs * max_index\n    # because indices is float, we interpolate the quantiles linearly from nearby points\n    indices_below = indices.long()\n    indices_above = (indices_below + 1).clamp(max=max_index)\n    quantiles_above = sorted_input.index_select(dim, indices_above)\n    quantiles_below = sorted_input.index_select(dim, indices_below)\n    shape_to_broadcast = [1] * input.dim()\n    shape_to_broadcast[dim] = indices.numel()\n    weights_above = indices - indices_below.type_as(indices)\n    weights_above = weights_above.reshape(shape_to_broadcast)\n    weights_below = 1 - weights_above\n    quantiles = weights_below * quantiles_below + weights_above * quantiles_above\n    return quantiles if probs.shape != torch.Size([]) else quantiles.squeeze(dim)\n\n\ndef pi(input, prob, dim=0):\n    """"""\n    Computes percentile interval which assigns equal probability mass\n    to each tail of the interval.\n\n    :param torch.Tensor input: the input tensor.\n    :param float prob: the probability mass of samples within the interval.\n    :param int dim: dimension to calculate percentile interval from ``input``.\n    :returns torch.Tensor: quantiles of ``input`` at ``probs``.\n    """"""\n    return quantile(input, [(1 - prob) / 2, (1 + prob) / 2], dim)\n\n\ndef hpdi(input, prob, dim=0):\n    """"""\n    Computes ""highest posterior density interval"" which is the narrowest\n    interval with probability mass ``prob``.\n\n    :param torch.Tensor input: the input tensor.\n    :param float prob: the probability mass of samples within the interval.\n    :param int dim: dimension to calculate percentile interval from ``input``.\n    :returns torch.Tensor: quantiles of ``input`` at ``probs``.\n    """"""\n    sorted_input = input.sort(dim)[0]\n    mass = input.size(dim)\n    index_length = int(prob * mass)\n    intervals_left = sorted_input.index_select(\n        dim, torch.tensor(range(mass - index_length), dtype=torch.long, device=input.device))\n    intervals_right = sorted_input.index_select(\n        dim, torch.tensor(range(index_length, mass), dtype=torch.long, device=input.device))\n    intervals_length = intervals_right - intervals_left\n    index_start = intervals_length.argmin(dim)\n    indices = torch.stack([index_start, index_start + index_length], dim)\n    return torch.gather(sorted_input, dim, indices)\n\n\ndef _weighted_mean(input, log_weights, dim=0, keepdim=False):\n    dim = input.dim() + dim if dim < 0 else dim\n    log_weights = log_weights.reshape([-1] + (input.dim() - dim - 1) * [1])\n    max_log_weight = log_weights.max(dim=0)[0]\n    relative_probs = (log_weights - max_log_weight).exp()\n    return (input * relative_probs).sum(dim=dim, keepdim=keepdim) / relative_probs.sum()\n\n\ndef _weighted_variance(input, log_weights, dim=0, keepdim=False, unbiased=True):\n    # Ref: https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Frequency_weights\n    deviation_squared = (input - _weighted_mean(input, log_weights, dim, keepdim=True)).pow(2)\n    correction = log_weights.size(0) / (log_weights.size(0) - 1.) if unbiased else 1.\n    return _weighted_mean(deviation_squared, log_weights, dim, keepdim) * correction\n\n\ndef waic(input, log_weights=None, pointwise=False, dim=0):\n    """"""\n    Computes ""Widely Applicable/Watanabe-Akaike Information Criterion"" (WAIC) and\n    its corresponding effective number of parameters.\n\n    Reference:\n\n    [1] `WAIC and cross-validation in Stan`,\n    Aki Vehtari, Andrew Gelman\n\n    :param torch.Tensor input: the input tensor, which is log likelihood of a model.\n    :param torch.Tensor log_weights: weights of samples along ``dim``.\n    :param int dim: the sample dimension of ``input``.\n    :returns tuple: tuple of WAIC and effective number of parameters.\n    """"""\n    if log_weights is None:\n        log_weights = torch.zeros(input.size(dim), dtype=input.dtype, device=input.device)\n\n    # computes log pointwise predictive density: formula (3) of [1]\n    dim = input.dim() + dim if dim < 0 else dim\n    weighted_input = input + log_weights.reshape([-1] + (input.dim() - dim - 1) * [1])\n    lpd = torch.logsumexp(weighted_input, dim=dim) - torch.logsumexp(log_weights, dim=0)\n\n    # computes the effective number of parameters: formula (6) of [1]\n    p_waic = _weighted_variance(input, log_weights, dim)\n\n    # computes expected log pointwise predictive density: formula (4) of [1]\n    elpd = lpd - p_waic\n    waic = -2 * elpd\n    return (waic, p_waic) if pointwise else (waic.sum(), p_waic.sum())\n\n\ndef fit_generalized_pareto(X):\n    """"""\n    Given a dataset X assumed to be drawn from the Generalized Pareto\n    Distribution, estimate the distributional parameters k, sigma using a\n    variant of the technique described in reference [1], as described in\n    reference [2].\n\n    References\n    [1] \'A new and efficient estimation method for the generalized Pareto distribution.\'\n    Zhang, J. and Stephens, M.A. (2009).\n    [2] \'Pareto Smoothed Importance Sampling.\'\n    Aki Vehtari, Andrew Gelman, Jonah Gabry\n\n    :param torch.Tensor: the input data X\n    :returns tuple: tuple of floats (k, sigma) corresponding to the fit parameters\n    """"""\n    if not isinstance(X, torch.Tensor) or X.dim() != 1:\n        raise ValueError(""Input X must be a 1-dimensional torch tensor"")\n\n    X = X.double()\n    X = torch.sort(X, descending=False)[0]\n\n    N = X.size(0)\n    M = 30 + int(math.sqrt(N))\n\n    # b = k / sigma\n    bs = 1.0 - math.sqrt(M) / (torch.arange(1, M + 1, dtype=torch.double) - 0.5).sqrt()\n    bs /= 3.0 * X[int(N/4 - 0.5)]\n    bs += 1 / X[-1]\n\n    ks = torch.log1p(-bs.unsqueeze(-1) * X).mean(-1)\n    Ls = N * (torch.log(-bs / ks) - (ks + 1.0))\n\n    weights = torch.exp(Ls - Ls.unsqueeze(-1))\n    weights = 1.0 / weights.sum(-1)\n\n    not_small_weights = weights > 1.0e-30\n    weights = weights[not_small_weights]\n    bs = bs[not_small_weights]\n    weights /= weights.sum()\n\n    b = (bs * weights).sum().item()\n    k = torch.log1p(-b * X).mean().item()\n    sigma = -k / b\n    k = k * N / (N + 10.0) + 5.0 / (N + 10.0)\n\n    return k, sigma\n\n\ndef crps_empirical(pred, truth):\n    """"""\n    Computes negative Continuous Ranked Probability Score CRPS* [1] between a\n    set of samples ``pred`` and true data ``truth``. This uses an ``n log(n)``\n    time algorithm to compute a quantity equal that would naively have\n    complexity quadratic in the number of samples ``n``::\n\n        CRPS* = E|pred - truth| - 1/2 E|pred - pred\'|\n              = (pred - truth).abs().mean(0)\n              - (pred - pred.unsqueeze(1)).abs().mean([0, 1]) / 2\n\n    Note that for a single sample this reduces to absolute error.\n\n    **References**\n\n    [1] Tilmann Gneiting, Adrian E. Raftery (2007)\n        `Strictly Proper Scoring Rules, Prediction, and Estimation`\n        https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf\n\n    :param torch.Tensor pred: A set of sample predictions batched on rightmost dim.\n        This should have shape ``(num_samples,) + truth.shape``.\n    :param torch.Tensor truth: A tensor of true observations.\n    :return: A tensor of shape ``truth.shape``.\n    :rtype: torch.Tensor\n    """"""\n    if pred.shape[1:] != (1,) * (pred.dim() - truth.dim() - 1) + truth.shape:\n        raise ValueError(""Expected pred to have one extra sample dim on left. ""\n                         ""Actual shapes: {} versus {}"".format(pred.shape, truth.shape))\n    opts = dict(device=pred.device, dtype=pred.dtype)\n    num_samples = pred.size(0)\n    if num_samples == 1:\n        return (pred[0] - truth).abs()\n\n    pred = pred.sort(dim=0).values\n    diff = pred[1:] - pred[:-1]\n    weight = (torch.arange(1, num_samples, **opts) *\n              torch.arange(num_samples - 1, 0, -1, **opts))\n    weight = weight.reshape(weight.shape + (1,) * (diff.dim() - 1))\n\n    return (pred - truth).abs().mean(0) - (diff * weight).sum(0) / num_samples**2\n'"
pyro/ops/tensor_utils.py,45,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\n\n\n_ROOT_TWO_INVERSE = 1.0 / math.sqrt(2.0)\n\n\ndef block_diag_embed(mat):\n    """"""\n    Takes a tensor of shape (..., B, M, N) and returns a block diagonal tensor\n    of shape (..., B x M, B x N).\n\n    :param torch.Tensor mat: an input tensor with 3 or more dimensions\n    :returns torch.Tensor: a block diagonal tensor with dimension `m.dim() - 1`\n    """"""\n    assert mat.dim() > 2, ""Input to block_diag() must be of dimension 3 or higher""\n    B, M, N = mat.shape[-3:]\n    eye = torch.eye(B, dtype=mat.dtype, device=mat.device).reshape(B, 1, B, 1)\n    return (mat.unsqueeze(-2) * eye).reshape(mat.shape[:-3] + (B * M, B * N))\n\n\ndef block_diagonal(mat, block_size):\n    """"""\n    Takes a block diagonal tensor of shape (..., B x M, B x N) and returns a tensor\n    of shape (..., B, M, N).\n\n    :param torch.Tensor mat: an input tensor with 2 or more dimensions\n    :param int block_size: the number of blocks B.\n    :returns torch.Tensor: a tensor with dimension `mat.dim() + 1`\n    """"""\n    B = block_size\n    M = mat.size(-2) // B\n    N = mat.size(-1) // B\n    assert mat.shape[-2:] == (B * M, B * N)\n    mat = mat.reshape(mat.shape[:-2] + (B, M, B, N))\n    mat = mat.transpose(-2, -3)\n    mat = mat.reshape(mat.shape[:-4] + (B * B, M, N))\n    return mat[..., ::B + 1, :, :]\n\n\ndef periodic_repeat(tensor, size, dim):\n    """"""\n    Repeat a ``period``-sized tensor up to given ``size``. For example::\n\n        >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n        >>> periodic_repeat(x, 4, 0)\n        tensor([[1, 2, 3],\n                [4, 5, 6],\n                [1, 2, 3],\n                [4, 5, 6]])\n        >>> periodic_repeat(x, 4, 1)\n        tensor([[1, 2, 3, 1],\n                [4, 5, 6, 4]])\n\n    This is useful for computing static seasonality in time series models.\n\n    :param torch.Tensor tensor: A tensor of differences.\n    :param int size: Desired size of the result along dimension ``dim``.\n    :param int dim: The tensor dimension along which to repeat.\n    """"""\n    assert isinstance(size, int) and size >= 0\n    assert isinstance(dim, int)\n    if dim >= 0:\n        dim -= tensor.dim()\n\n    period = tensor.size(dim)\n    repeats = [1] * tensor.dim()\n    repeats[dim] = (size + period - 1) // period\n    result = tensor.repeat(*repeats)\n    result = result[(Ellipsis, slice(None, size)) + (slice(None),) * (-1 - dim)]\n    return result\n\n\ndef periodic_cumsum(tensor, period, dim):\n    """"""\n    Compute periodic cumsum along a given dimension. For example if dim=0::\n\n        for t in range(period):\n            assert result[t] == tensor[t]\n        for t in range(period, len(tensor)):\n            assert result[t] == tensor[t] + result[t - period]\n\n    This is useful for computing drifting seasonality in time series models.\n\n    :param torch.Tensor tensor: A tensor of differences.\n    :param int period: The period of repetition.\n    :param int dim: The tensor dimension along which to accumulate.\n    """"""\n    assert isinstance(period, int) and period > 0\n    assert isinstance(dim, int)\n    if dim >= 0:\n        dim -= tensor.dim()\n\n    # Pad to even size.\n    size = tensor.size(dim)\n    repeats = (size + period - 1) // period\n    padding = repeats * period - size\n    if torch._C._get_tracing_state() or padding:\n        tensor = torch.nn.functional.pad(tensor, (0, 0) * (-1 - dim) + (0, padding))\n\n    # Accumulate.\n    shape = tensor.shape[:dim] + (repeats, period) + tensor.shape[tensor.dim() + dim + 1:]\n    result = tensor.reshape(shape).cumsum(dim=dim - 1).reshape(tensor.shape)\n\n    # Truncate to original size.\n    if torch._C._get_tracing_state() or padding:\n        result = result[(Ellipsis, slice(None, size)) + (slice(None),) * (-1 - dim)]\n    return result\n\n\ndef periodic_features(duration, max_period=None, min_period=None, **options):\n    r""""""\n    Create periodic (sin,cos) features from ``max_period`` down to\n    ``min_period``.\n\n    This is useful in time series models where long uneven seasonality can be\n    treated via regression. When only ``max_period`` is specified this\n    generates periodic features at all length scales. When also ``min_period``\n    is specified this generates periodic features at large length scales, but\n    omits high frequency features. This is useful when combining regression for\n    long seasonality with other techniques like :func:`periodic_repeat` and\n    :func:`periodic_cumsum` for short time scales. For example, to combine\n    regress yearly seasonality down to the scale of one week one could set\n    ``max_period=365.25`` and ``min_period=7``.\n\n    :param int duration: Number of discrete time steps.\n    :param float max_period: Optional max period, defaults to ``duration``.\n    :param float min_period: Optional min period (exclusive), defaults to\n        2 = Nyquist cutoff.\n    :param \\*\\*options: Tensor construction options, e.g. ``dtype`` and\n        ``device``.\n    :returns: A ``(duration, 2 * ceil(max_period / min_period) - 2)``-shaped\n        tensor of features normalized to lie in [-1,1].\n    :rtype: ~torch.Tensor\n    """"""\n    assert isinstance(duration, int) and duration >= 0\n    if max_period is None:\n        max_period = duration\n    if min_period is None:\n        min_period = 2\n    assert 2 <= min_period, ""min_period is below Nyquist cutoff""\n    assert min_period <= max_period\n\n    t = torch.arange(float(duration), **options).unsqueeze(-1).unsqueeze(-1)\n    phase = torch.tensor([0, math.pi / 2], **options).unsqueeze(-1)\n    freq = torch.arange(1, max_period / min_period, **options).mul_(2 * math.pi / max_period)\n    result = (freq * t + phase).cos_().reshape(duration, -1).contiguous()\n    return result\n\n\n_NEXT_FAST_LEN = {}\n\n\ndef next_fast_len(size):\n    """"""\n    Returns the next largest number ``n >= size`` whose prime factors are all\n    2, 3, or 5. These sizes are efficient for fast fourier transforms.\n    Equivalent to :func:`scipy.fftpack.next_fast_len`.\n\n    :param int size: A positive number.\n    :returns: A possibly larger number.\n    :rtype int:\n    """"""\n    try:\n        return _NEXT_FAST_LEN[size]\n    except KeyError:\n        pass\n\n    assert isinstance(size, int) and size > 0\n    next_size = size\n    while True:\n        remaining = next_size\n        for n in (2, 3, 5):\n            while remaining % n == 0:\n                remaining //= n\n        if remaining == 1:\n            _NEXT_FAST_LEN[size] = next_size\n            return next_size\n        next_size += 1\n\n\ndef _complex_mul(a, b):\n    ar, ai = a.unbind(-1)\n    br, bi = b.unbind(-1)\n    return torch.stack([ar * br - ai * bi, ar * bi + ai * br], dim=-1)\n\n\ndef convolve(signal, kernel, mode=\'full\'):\n    """"""\n    Computes the 1-d convolution of signal by kernel using FFTs.\n    The two arguments should have the same rightmost dim, but may otherwise be\n    arbitrarily broadcastable.\n\n    :param torch.Tensor signal: A signal to convolve.\n    :param torch.Tensor kernel: A convolution kernel.\n    :param str mode: One of: \'full\', \'valid\', \'same\'.\n    :return: A tensor with broadcasted shape. Letting ``m = signal.size(-1)``\n        and ``n = kernel.size(-1)``, the rightmost size of the result will be:\n        ``m + n - 1`` if mode is \'full\';\n        ``max(m, n) - min(m, n) + 1`` if mode is \'valid\'; or\n        ``max(m, n)`` if mode is \'same\'.\n    :rtype torch.Tensor:\n    """"""\n    m = signal.size(-1)\n    n = kernel.size(-1)\n    if mode == \'full\':\n        truncate = m + n - 1\n    elif mode == \'valid\':\n        truncate = max(m, n) - min(m, n) + 1\n    elif mode == \'same\':\n        truncate = max(m, n)\n    else:\n        raise ValueError(\'Unknown mode: {}\'.format(mode))\n\n    # Compute convolution using fft.\n    padded_size = m + n - 1\n    # Round up for cheaper fft.\n    fast_ftt_size = next_fast_len(padded_size)\n    f_signal = torch.rfft(torch.nn.functional.pad(signal, (0, fast_ftt_size - m)), 1, onesided=False)\n    f_kernel = torch.rfft(torch.nn.functional.pad(kernel, (0, fast_ftt_size - n)), 1, onesided=False)\n    f_result = _complex_mul(f_signal, f_kernel)\n    result = torch.irfft(f_result, 1, onesided=False)\n\n    start_idx = (padded_size - truncate) // 2\n    return result[..., start_idx: start_idx + truncate]\n\n\ndef repeated_matmul(M, n):\n    """"""\n    Takes a batch of matrices `M` as input and returns the stacked result of doing the\n    `n`-many matrix multiplications :math:`M`, :math:`M^2`, ..., :math:`M^n`.\n    Parallel cost is logarithmic in `n`.\n\n    :param torch.Tensor M: A batch of square tensors of shape (..., N, N).\n    :param int n: The order of the largest product :math:`M^n`\n    :returns torch.Tensor: A batch of square tensors of shape (n, ..., N, N)\n    """"""\n    assert M.size(-1) == M.size(-2), ""Input tensors must satisfy M.size(-1) == M.size(-2).""\n    assert n > 0, ""argument n to parallel_scan_repeated_matmul must be 1 or larger""\n\n    doubling_rounds = 0 if n <= 2 else math.ceil(math.log(n, 2)) - 1\n\n    if n == 1:\n        return M.unsqueeze(0)\n\n    result = torch.stack([M, torch.matmul(M, M)])\n\n    for i in range(doubling_rounds):\n        doubled = torch.matmul(result[-1].unsqueeze(0), result)\n        result = torch.stack([result, doubled]).reshape(-1, *result.shape[1:])\n\n    return result[0:n]\n\n\ndef _real_of_complex_mul(a, b):\n    ar, ai = a.unbind(-1)\n    br, bi = b.unbind(-1)\n    return ar * br - ai * bi\n\n\ndef dct(x, dim=-1):\n    """"""\n    Discrete cosine transform of type II, scaled to be orthonormal.\n\n    This is the inverse of :func:`idct_ii` , and is equivalent to\n    :func:`scipy.fftpack.dct` with ``norm=""ortho""``.\n\n    :param Tensor x: The input signal.\n    :param int dim: Dimension along which to compute DCT.\n    :rtype: Tensor\n    """"""\n    if dim >= 0:\n        dim -= x.dim()\n    if dim != -1:\n        y = x.reshape(x.shape[:dim + 1] + (-1,)).transpose(-1, -2)\n        return dct(y).transpose(-1, -2).reshape(x.shape)\n\n    # Ref: http://fourier.eng.hmc.edu/e161/lectures/dct/node2.html\n    N = x.size(-1)\n    # Step 1\n    y = torch.cat([x[..., ::2], x[..., 1::2].flip(-1)], dim=-1)\n    # Step 2\n    Y = torch.rfft(y, 1, onesided=False)\n    # Step 3\n    coef_real = torch.cos(torch.linspace(0, 0.5 * math.pi, N + 1, dtype=x.dtype, device=x.device))\n    coef = torch.stack([coef_real[:-1], -coef_real[1:].flip(-1)], dim=-1)\n    X = _real_of_complex_mul(coef, Y)\n    # orthogonalize\n    scale = torch.cat([x.new_tensor([math.sqrt(N)]), x.new_full((N - 1,), math.sqrt(0.5 * N))])\n    return X / scale\n\n\ndef idct(x, dim=-1):\n    """"""\n    Inverse discrete cosine transform of type II, scaled to be orthonormal.\n\n    This is the inverse of :func:`dct_ii` , and is equivalent to\n    :func:`scipy.fftpack.idct` with ``norm=""ortho""``.\n\n    :param Tensor x: The input signal.\n    :param int dim: Dimension along which to compute DCT.\n    :rtype: Tensor\n    """"""\n    if dim >= 0:\n        dim -= x.dim()\n    if dim != -1:\n        y = x.reshape(x.shape[:dim + 1] + (-1,)).transpose(-1, -2)\n        return idct(y).transpose(-1, -2).reshape(x.shape)\n\n    N = x.size(-1)\n    scale = torch.cat([x.new_tensor([math.sqrt(N)]), x.new_full((N - 1,), math.sqrt(0.5 * N))])\n    x = x * scale\n    # Step 1, solve X = cos(k) * Yr + sin(k) * Yi\n    # We know that Y[1:] is conjugate to Y[:0:-1], hence\n    # X[:0:-1] = sin(k) * Yr[1:] + cos(k) * Yi[1:]\n    # So Yr[1:] = cos(k) * X[1:] + sin(k) * X[:0:-1]\n    # and Yi[1:] = sin(k) * X[1:] - cos(k) * X[:0:-1]\n    # In addition, Yi[0] = 0, Yr[0] = X[0]\n    # In other words, Y = complex_mul(e^ik, X - i[0, X[:0:-1]])\n    xi = torch.nn.functional.pad(-x[..., 1:], (0, 1)).flip(-1)\n    X = torch.stack([x, xi], dim=-1)\n    coef_real = torch.cos(torch.linspace(0, 0.5 * math.pi, N + 1))\n    coef = torch.stack([coef_real[:-1], coef_real[1:].flip(-1)], dim=-1)\n    half_size = N // 2 + 1\n    Y = _complex_mul(coef[..., :half_size, :], X[..., :half_size, :])\n    # Step 2\n    y = torch.irfft(Y, 1, onesided=True, signal_sizes=(N,))\n    # Step 3\n    return torch.stack([y, y.flip(-1)], axis=-1).reshape(x.shape[:-1] + (-1,))[..., :N]\n\n\ndef haar_transform(x):\n    """"""\n    Discrete Haar transform.\n\n    Performs a Haar transform along the final dimension.\n    This is the inverse of :func:`inverse_haar_transform`.\n\n    :param Tensor x: The input signal.\n    :rtype: Tensor\n    """"""\n    n = x.size(-1) // 2\n    even, odd, end = x[..., 0:n+n:2], x[..., 1:n+n:2], x[..., n+n:]\n    hi = _ROOT_TWO_INVERSE * (even - odd)\n    lo = _ROOT_TWO_INVERSE * (even + odd)\n    if n >= 2:\n        lo = haar_transform(lo)\n    x = torch.cat([lo, hi, end], dim=-1)\n    return x\n\n\ndef inverse_haar_transform(x):\n    """"""\n    Performs an inverse Haar transform along the final dimension.\n    This is the inverse of :func:`haar_transform`.\n\n    :param Tensor x: The input signal.\n    :rtype: Tensor\n    """"""\n    n = x.size(-1) // 2\n    lo, hi, end = x[..., :n], x[..., n:n+n], x[..., n+n:]\n    if n >= 2:\n        lo = inverse_haar_transform(lo)\n    even = _ROOT_TWO_INVERSE * (lo + hi)\n    odd = _ROOT_TWO_INVERSE * (lo - hi)\n    even_odd = torch.stack([even, odd], dim=-1).reshape(even.shape[:-1] + (-1,))\n    x = torch.cat([even_odd, end], dim=-1)\n    return x\n\n\ndef cholesky(x):\n    if x.size(-1) == 1:\n        return x.sqrt()\n    return x.cholesky()\n\n\ndef cholesky_solve(x, y):\n    if y.size(-1) == 1:\n        return x / (y * y)\n    return x.cholesky_solve(y)\n\n\ndef matmul(x, y):\n    if x.size(-1) == 1:\n        return x.mul(y)\n    return x.matmul(y)\n\n\ndef matvecmul(x, y):\n    if x.size(-1) == 1:\n        return x.squeeze(-1).mul(y)\n    return x.matmul(y.unsqueeze(-1)).squeeze(-1)\n\n\ndef triangular_solve(x, y, upper=False, transpose=False):\n    if y.size(-1) == 1:\n        return x / y\n    return x.triangular_solve(y, upper=upper, transpose=transpose).solution\n\n\ndef precision_to_scale_tril(P):\n    Lf = torch.cholesky(torch.flip(P, (-2, -1)))\n    L_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)\n    L = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),\n                               L_inv, upper=False)[0]\n    return L\n'"
pyro/ops/welford.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\n\nclass WelfordCovariance:\n    """"""\n    Implements Welford\'s online scheme for estimating (co)variance (see :math:`[1]`).\n    Useful for adapting diagonal and dense mass structures for HMC.\n\n    **References**\n\n    [1] `The Art of Computer Programming`,\n    Donald E. Knuth\n    """"""\n    def __init__(self, diagonal=True):\n        self.diagonal = diagonal\n        self.reset()\n\n    def reset(self):\n        self._mean = 0.\n        self._m2 = 0.\n        self.n_samples = 0\n\n    def update(self, sample):\n        self.n_samples += 1\n        delta_pre = sample - self._mean\n        self._mean = self._mean + delta_pre / self.n_samples\n        delta_post = sample - self._mean\n\n        if self.diagonal:\n            self._m2 += delta_pre * delta_post\n        else:\n            self._m2 += torch.ger(delta_post, delta_pre)\n\n    def get_covariance(self, regularize=True):\n        if self.n_samples < 2:\n            raise RuntimeError(\'Insufficient samples to estimate covariance\')\n        cov = self._m2 / (self.n_samples - 1)\n        if regularize:\n            # Regularization from stan\n            scaled_cov = (self.n_samples / (self.n_samples + 5.)) * cov\n            shrinkage = 1e-3 * (5. / (self.n_samples + 5.0))\n            if self.diagonal:\n                cov = scaled_cov + shrinkage\n            else:\n                scaled_cov.view(-1)[::scaled_cov.size(0) + 1] += shrinkage\n                cov = scaled_cov\n        return cov\n\n\nclass WelfordArrowheadCovariance:\n    """"""\n    Likes :class:`WelfordCovariance` but generalized to the arrowhead structure.\n    """"""\n    def __init__(self, head_size=0):\n        self.head_size = head_size\n        self.reset()\n\n    def reset(self):\n        self._mean = 0.\n        self._m2_top = 0.  # upper part, shape: head_size x matrix_size\n        self._m2_bottom_diag = 0.  # lower right part, shape: (matrix_size - head_size)\n        self.n_samples = 0\n\n    def update(self, sample):\n        self.n_samples += 1\n        delta_pre = sample - self._mean\n        self._mean = self._mean + delta_pre / self.n_samples\n        delta_post = sample - self._mean\n        if self.head_size > 0:\n            self._m2_top = self._m2_top + torch.ger(delta_post[:self.head_size], delta_pre)\n        else:\n            self._m2_top = sample.new_empty(0, sample.size(0))\n        self._m2_bottom_diag = self._m2_bottom_diag + delta_post[self.head_size:] * delta_pre[self.head_size:]\n\n    def get_covariance(self, regularize=True):\n        """"""\n        Gets the covariance in arrowhead form: (top, bottom_diag) where `top = cov[:head_size]`\n        and `bottom_diag = cov.diag()[head_size:]`.\n        """"""\n        if self.n_samples < 2:\n            raise RuntimeError(\'Insufficient samples to estimate covariance\')\n        top = self._m2_top / (self.n_samples - 1)\n        bottom_diag = self._m2_bottom_diag / (self.n_samples - 1)\n        if regularize:\n            top = top * (self.n_samples / (self.n_samples + 5.))\n            bottom_diag = bottom_diag * (self.n_samples / (self.n_samples + 5.))\n            shrinkage = 1e-3 * (5. / (self.n_samples + 5.0))\n            top.view(-1)[::top.size(-1) + 1] += shrinkage\n            bottom_diag = bottom_diag + shrinkage\n\n        return top, bottom_diag\n'"
pyro/optim/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro.optim.multi  # noqa F403\nfrom pyro.optim.lr_scheduler import PyroLRScheduler\nfrom pyro.optim.optim import AdagradRMSProp, ClippedAdam, DCTAdam, PyroOptim\nfrom pyro.optim.pytorch_optimizers import *  # noqa F403\nfrom pyro.optim.pytorch_optimizers import __all__ as pytorch_optims\n\n__all__ = [\n    ""AdagradRMSProp"",\n    ""ClippedAdam"",\n    ""DCTAdam"",\n    ""PyroOptim"",\n    ""PyroLRScheduler"",\n]\n__all__.extend(pytorch_optims)\n'"
pyro/optim/adagrad_rmsprop.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdagradRMSProp(Optimizer):\n    """"""\n    Implements a mash-up of the Adagrad algorithm and RMSProp. For the precise\n    update equation see equations 10 and 11 in reference [1].\n\n    References:\n    [1] \'Automatic Differentiation Variational Inference\', Alp Kucukelbir,\n    Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M. Blei\n    URL: https://arxiv.org/abs/1603.00788\n    [2] \'Lecture 6.5 RmsProp: Divide the gradient by a running average\n    of its recent magnitude\', Tieleman, T. and Hinton, G.,\n    COURSERA: Neural Networks for Machine Learning.\n    [3] \'Adaptive subgradient methods for online learning and stochastic optimization\',\n    Duchi, John, Hazan, E and Singer, Y.\n\n    Arguments:\n\n    :param params: iterable of parameters to optimize or dicts defining parameter groups\n    :param eta: sets the step size scale (optional; default: 1.0)\n    :type eta: float\n    :param t:  t, optional): momentum parameter (optional; default: 0.1)\n    :type t: float\n    :param delta: modulates the exponent that controls how the step size scales (optional: default: 1e-16)\n    :type delta: float\n    """"""\n\n    def __init__(self, params, eta=1.0, delta=1.0e-16, t=0.1):\n        defaults = dict(eta=eta, delta=delta, t=t)\n        super().__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = 0\n                state[\'sum\'] = torch.zeros_like(p.data)\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'sum\'].share_memory_()\n\n    def step(self, closure=None):\n        """"""\n        Performs a single optimization step.\n\n        :param closure: A (optional) closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n\n                grad = p.grad.data\n\n                if grad.is_sparse:\n                    raise NotImplementedError\n\n                state = self.state[p]\n                state[\'step\'] += 1\n                if state[\'step\'] == 1:\n                    # if first step, initialize variance bit to grad^2\n                    state[\'sum\'] = grad * grad\n                else:\n                    state[\'sum\'] *= (1.0 - group[\'t\'])\n                    state[\'sum\'] += group[\'t\'] * grad * grad\n\n                lr = group[\'eta\'] * (state[\'step\'] ** (-0.5 + group[\'delta\']))\n                std = state[\'sum\'].sqrt()\n                p.data.addcdiv_(grad, 1.0 + std, value=-lr)\n\n        return loss\n'"
pyro/optim/clipped_adam.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass ClippedAdam(Optimizer):\n    """"""\n    :param params: iterable of parameters to optimize or dicts defining parameter groups\n    :param lr: learning rate (default: 1e-3)\n    :param Tuple betas: coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    :param eps: term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    :param weight_decay: weight decay (L2 penalty) (default: 0)\n    :param clip_norm: magnitude of norm to which gradients are clipped (default: 10.0)\n    :param lrd: rate at which learning rate decays (default: 1.0)\n\n    Small modification to the Adam algorithm implemented in torch.optim.Adam\n    to include gradient clipping and learning rate decay.\n\n    Reference\n\n    `A Method for Stochastic Optimization`, Diederik P. Kingma, Jimmy Ba\n    https://arxiv.org/abs/1412.6980\n    """"""\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, clip_norm=10.0, lrd=1.0):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay,\n                        clip_norm=clip_norm, lrd=lrd)\n        super().__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""\n        :param closure: An optional closure that reevaluates the model and returns the loss.\n\n        Performs a single optimization step.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            group[\'lr\'] *= group[\'lrd\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                grad.clamp_(-group[\'clip_norm\'], group[\'clip_norm\'])\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(grad)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(grad)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(p.data, alpha=group[\'weight_decay\'])\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss\n'"
pyro/optim/dct_adam.py,9,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.nn.functional import pad\nfrom torch.optim.optimizer import Optimizer\n\nfrom pyro.ops.tensor_utils import dct, idct, next_fast_len\n\n\ndef _transform_forward(x, dim, duration):\n    assert not x.requires_grad\n    assert dim < 0\n    assert duration == x.size(dim)\n    time_domain = x\n    new_size = next_fast_len(duration)\n    if new_size == duration:\n        freq_domain = x\n    else:\n        freq_domain = pad(x, (0, 0) * (-1 - dim) + (0, new_size - duration))\n    freq_domain = dct(freq_domain, dim)\n    return torch.cat([time_domain, freq_domain], dim=dim)\n\n\ndef _transform_inverse(x, dim, duration):\n    assert not x.requires_grad\n    assert dim < 0\n    dots = (slice(None),) * (x.dim() + dim)\n    left = dots + (slice(None, duration),)\n    right = dots + (slice(duration, None),)\n    return idct(x[right], dim)[left].add_(x[left])\n\n\ndef _get_mask(x, indices):\n    # create a boolean mask from subsample indices:\n    #   + start with a zero tensor\n    #   + at a specific `dim`, we increase by 1\n    #     all entries which have index at `dim`\n    #     belong to `indices[dim]`\n    #   + at the end, we will get a tensor whose\n    #     values at `indices` are `len(indices)`\n    mask = x.new_zeros(x.shape, dtype=torch.long, device=x.device)\n    if len(indices) > 0:\n        idx = []\n        for dim in range(-x.dim(), 0):\n            if dim in indices:\n                mask[idx + [indices[dim]]] += 1\n            idx.append(slice(None))\n    return mask == len(indices)\n\n\nclass DCTAdam(Optimizer):\n    """"""\n    EXPERIMENTAL Discrete Cosine Transform-augmented\n    :class:`~pyro.optim.clipped_adam.ClippedAdam` optimizer.\n\n    This acts like :class:`~pyro.optim.clipped_adam.ClippedAdam` on most\n    parameters, but if a parameter has an attribute ``._pyro_dct_dim``\n    indicating a time dimension, this creates a secondary optimize in the\n    frequency domain. This is useful for parameters of time series models.\n\n    :param params: iterable of parameters to optimize or dicts defining parameter groups\n    :param float lr: learning rate (default: 1e-3)\n    :param tuple betas: coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    :param float eps: term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    :param float clip_norm: magnitude of norm to which gradients are clipped (default: 10.0)\n    :param float lrd: rate at which learning rate decays (default: 1.0)\n    :param bool subsample_aware: whether to update gradient statistics only for\n        those elements that appear in a subsample (default: False).\n    """"""\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 clip_norm=10.0, lrd=1.0, subsample_aware=False):\n        defaults = dict(lr=lr, betas=betas, eps=eps, clip_norm=clip_norm, lrd=lrd,\n                        subsample_aware=subsample_aware)\n        super().__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""\n        :param closure: An optional closure that reevaluates the model and returns the loss.\n\n        Performs a single optimization step.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            group[\'lr\'] *= group[\'lrd\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n\n                subsample = getattr(p, ""_pyro_subsample"", {})\n                if subsample and group[\'subsample_aware\']:\n                    self._step_param_subsample(group, p, subsample)\n                else:\n                    self._step_param(group, p)\n\n        return loss\n\n    def _step_param(self, group, p):\n        grad = p.grad.data\n        grad.clamp_(-group[\'clip_norm\'], group[\'clip_norm\'])\n\n        # Transform selected parameters via dct.\n        time_dim = getattr(p, ""_pyro_dct_dim"", None)\n        if time_dim is not None:\n            duration = p.size(time_dim)\n            grad = _transform_forward(grad, time_dim, duration)\n\n        state = self.state[p]\n\n        # State initialization\n        if len(state) == 0:\n            state[\'step\'] = 0\n            # Exponential moving average of gradient values\n            state[\'exp_avg\'] = torch.zeros_like(grad)\n            # Exponential moving average of squared gradient values\n            state[\'exp_avg_sq\'] = torch.zeros_like(grad)\n\n        exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n        beta1, beta2 = group[\'betas\']\n\n        state[\'step\'] += 1\n\n        # Decay the first and second moment running average coefficient\n        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n        denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n        bias_correction1 = 1 - beta1 ** state[\'step\']\n        bias_correction2 = 1 - beta2 ** state[\'step\']\n        step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n        if time_dim is None:\n            p.data.addcdiv_(exp_avg, denom, value=-step_size)\n        else:\n            step = _transform_inverse(exp_avg / denom, time_dim, duration)\n            p.data.add_(step.mul_(-step_size))\n\n    def _step_param_subsample(self, group, p, subsample):\n        mask = _get_mask(p, subsample)\n\n        grad = p.grad.data.masked_select(mask)\n        grad.clamp_(-group[\'clip_norm\'], group[\'clip_norm\'])\n\n        # Transform selected parameters via dct.\n        time_dim = getattr(p, ""_pyro_dct_dim"", None)\n        if time_dim is not None:\n            duration = p.size(time_dim)\n            grad = _transform_forward(grad, time_dim, duration)\n\n        state = self.state[p]\n\n        # State initialization\n        if len(state) == 0:\n            state[\'step\'] = torch.zeros_like(p)\n            # Exponential moving average of gradient values\n            state[\'exp_avg\'] = torch.zeros_like(p)\n            # Exponential moving average of squared gradient values\n            state[\'exp_avg_sq\'] = torch.zeros_like(p)\n\n        beta1, beta2 = group[\'betas\']\n\n        state_step = state[\'step\'].masked_select(mask).add_(1)\n        state[\'step\'].masked_scatter_(mask, state_step)\n\n        # Decay the first and second moment running average coefficient\n        exp_avg = state[\'exp_avg\'].masked_select(mask).mul_(beta1).add_(grad, alpha=1 - beta1)\n        state[\'exp_avg\'].masked_scatter_(mask, exp_avg)\n\n        exp_avg_sq = state[\'exp_avg_sq\'].masked_select(mask).mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n        state[\'exp_avg_sq\'].masked_scatter_(mask, exp_avg_sq)\n\n        denom = exp_avg_sq.sqrt_().add_(group[\'eps\'])\n\n        bias_correction1 = 1 - beta1 ** state_step\n        bias_correction2 = 1 - beta2 ** state_step\n        step_size = bias_correction2.sqrt_().div_(bias_correction1).mul_(group[\'lr\'])\n\n        step = exp_avg.div_(denom)\n        if time_dim is not None:\n            step = _transform_inverse(step, time_dim, duration)\n        p_data = p.data.masked_select(mask).sub_(step.mul_(step_size))\n        p.data.masked_scatter_(mask, p_data)\n'"
pyro/optim/lr_scheduler.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.optim.optim import PyroOptim\n\n\nclass PyroLRScheduler(PyroOptim):\n    """"""\n    A wrapper for :class:`~torch.optim.lr_scheduler` objects that adjusts learning rates\n    for dynamically generated parameters.\n\n    :param scheduler_constructor: a :class:`~torch.optim.lr_scheduler`\n    :param optim_args: a dictionary of learning arguments for the optimizer or a callable that returns\n        such dictionaries. must contain the key \'optimizer\' with pytorch optimizer value\n    :param clip_args: a dictionary of clip_norm and/or clip_value args or a callable that returns\n        such dictionaries.\n\n    Example::\n\n        optimizer = torch.optim.SGD\n        scheduler = pyro.optim.ExponentialLR({\'optimizer\': optimizer, \'optim_args\': {\'lr\': 0.01}, \'gamma\': 0.1})\n        svi = SVI(model, guide, scheduler, loss=TraceGraph_ELBO())\n        for i in range(epochs):\n            for minibatch in DataLoader(dataset, batch_size):\n                svi.step(minibatch)\n            scheduler.step()\n    """"""\n    def __init__(self, scheduler_constructor, optim_args, clip_args=None):\n        # pytorch scheduler\n        self.pt_scheduler_constructor = scheduler_constructor\n        # torch optimizer\n        pt_optim_constructor = optim_args.pop(\'optimizer\')\n        # kwargs for the torch optimizer\n        optim_kwargs = optim_args.pop(\'optim_args\')\n        self.kwargs = optim_args\n        super().__init__(pt_optim_constructor, optim_kwargs, clip_args)\n\n    def __call__(self, params, *args, **kwargs):\n        super().__call__(params, *args, **kwargs)\n\n    def _get_optim(self, params):\n        optim = super()._get_optim(params)\n        return self.pt_scheduler_constructor(optim, **self.kwargs)\n\n    def step(self, *args, **kwargs):\n        """"""\n        Takes the same arguments as the PyTorch scheduler\n        (e.g. optional ``loss`` for ``ReduceLROnPlateau``)\n        """"""\n        for scheduler in self.optim_objs.values():\n            scheduler.step(*args, **kwargs)\n'"
pyro/optim/multi.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.ops.newton import newton_step\nfrom pyro.optim.optim import PyroOptim\n\n\nclass MultiOptimizer:\n    """"""\n    Base class of optimizers that make use of higher-order derivatives.\n\n    Higher-order optimizers generally use :func:`torch.autograd.grad` rather\n    than :meth:`torch.Tensor.backward`, and therefore require a different\n    interface from usual Pyro and PyTorch optimizers. In this interface,\n    the :meth:`step` method inputs a ``loss`` tensor to be differentiated,\n    and backpropagation is triggered one or more times inside the optimizer.\n\n    Derived classes must implement :meth:`step` to compute derivatives and\n    update parameters in-place.\n\n    Example::\n\n        tr = poutine.trace(model).get_trace(*args, **kwargs)\n        loss = -tr.log_prob_sum()\n        params = {name: site[\'value\'].unconstrained()\n                  for name, site in tr.nodes.items()\n                  if site[\'type\'] == \'param\'}\n        optim.step(loss, params)\n    """"""\n    def step(self, loss, params):\n        """"""\n        Performs an in-place optimization step on parameters given a\n        differentiable ``loss`` tensor.\n\n        Note that this detaches the updated tensors.\n\n        :param torch.Tensor loss: A differentiable tensor to be minimized.\n            Some optimizers require this to be differentiable multiple times.\n        :param dict params: A dictionary mapping param name to unconstrained\n            value as stored in the param store.\n        """"""\n        updated_values = self.get_step(loss, params)\n        for name, value in params.items():\n            with torch.no_grad():\n                # we need to detach because updated_value may depend on value\n                value.copy_(updated_values[name].detach())\n\n    def get_step(self, loss, params):\n        """"""\n        Computes an optimization step of parameters given a differentiable\n        ``loss`` tensor, returning the updated values.\n\n        Note that this preserves derivatives on the updated tensors.\n\n        :param torch.Tensor loss: A differentiable tensor to be minimized.\n            Some optimizers require this to be differentiable multiple times.\n        :param dict params: A dictionary mapping param name to unconstrained\n            value as stored in the param store.\n        :return: A dictionary mapping param name to updated unconstrained\n            value.\n        :rtype: dict\n        """"""\n        raise NotImplementedError\n\n\nclass PyroMultiOptimizer(MultiOptimizer):\n    """"""\n    Facade to wrap :class:`~pyro.optim.optim.PyroOptim` objects\n    in a :class:`MultiOptimizer` interface.\n    """"""\n    def __init__(self, optim):\n        if not isinstance(optim, PyroOptim):\n            raise TypeError(\'Expected a PyroOptim object but got a {}\'.format(type(optim)))\n        self.optim = optim\n\n    def step(self, loss, params):\n        values = params.values()\n        grads = torch.autograd.grad(loss, values, create_graph=True)\n        for x, g in zip(values, grads):\n            x.grad = g\n        self.optim(values)\n\n\nclass TorchMultiOptimizer(PyroMultiOptimizer):\n    """"""\n    Facade to wrap :class:`~torch.optim.Optimizer` objects\n    in a :class:`MultiOptimizer` interface.\n    """"""\n    def __init__(self, optim_constructor, optim_args):\n        optim = PyroOptim(optim_constructor, optim_args)\n        super().__init__(optim)\n\n\nclass MixedMultiOptimizer(MultiOptimizer):\n    """"""\n    Container class to combine different :class:`MultiOptimizer` instances for\n    different parameters.\n\n    :param list parts: A list of ``(names, optim)`` pairs, where each\n        ``names`` is a list of parameter names, and each ``optim`` is a\n        :class:`MultiOptimizer` or :class:`~pyro.optim.optim.PyroOptim` object\n        to be used for the named parameters. Together the ``names`` should\n        partition up all desired parameters to optimize.\n    :raises ValueError: if any name is optimized by multiple optimizers.\n    """"""\n    def __init__(self, parts):\n        optim_dict = {}\n        self.parts = []\n        for names_part, optim in parts:\n            if isinstance(optim, PyroOptim):\n                optim = PyroMultiOptimizer(optim)\n            for name in names_part:\n                if name in optim_dict:\n                    raise ValueError(""Attempted to optimize parameter \'{}\' by two different optimizers: ""\n                                     ""{} vs {}"" .format(name, optim_dict[name], optim))\n                optim_dict[name] = optim\n            self.parts.append((names_part, optim))\n\n    def step(self, loss, params):\n        for names_part, optim in self.parts:\n            optim.step(loss, {name: params[name] for name in names_part})\n\n    def get_step(self, loss, params):\n        updated_values = {}\n        for names_part, optim in self.parts:\n            updated_values.update(\n                optim.get_step(loss, {name: params[name] for name in names_part}))\n        return updated_values\n\n\nclass Newton(MultiOptimizer):\n    """"""\n    Implementation of :class:`MultiOptimizer` that performs a Newton update\n    on batched low-dimensional variables, optionally regularizing via a\n    per-parameter ``trust_radius``. See :func:`~pyro.ops.newton.newton_step`\n    for details.\n\n    The result of :meth:`get_step` will be differentiable, however the\n    updated values from :meth:`step` will be detached.\n\n    :param dict trust_radii: a dict mapping parameter name to radius of trust\n        region. Missing names will use unregularized Newton update, equivalent\n        to infinite trust radius.\n    """"""\n    def __init__(self, trust_radii={}):\n        self.trust_radii = trust_radii\n\n    def get_step(self, loss, params):\n        updated_values = {}\n        for name, value in params.items():\n            trust_radius = self.trust_radii.get(name)\n            updated_value, cov = newton_step(loss, value, trust_radius)\n            updated_values[name] = updated_value\n        return updated_values\n'"
pyro/optim/optim.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.nn.utils import clip_grad_norm_, clip_grad_value_\n\nimport pyro\nfrom pyro.optim.adagrad_rmsprop import AdagradRMSProp as pt_AdagradRMSProp\nfrom pyro.optim.clipped_adam import ClippedAdam as pt_ClippedAdam\nfrom pyro.optim.dct_adam import DCTAdam as pt_DCTAdam\nfrom pyro.params import module_from_param_with_module_name, user_param_name\n\n\nclass PyroOptim:\n    """"""\n    A wrapper for torch.optim.Optimizer objects that helps with managing dynamically generated parameters.\n\n    :param optim_constructor: a torch.optim.Optimizer\n    :param optim_args: a dictionary of learning arguments for the optimizer or a callable that returns\n        such dictionaries\n    :param clip_args: a dictionary of clip_norm and/or clip_value args or a callable that returns\n        such dictionaries\n    """"""\n    def __init__(self, optim_constructor, optim_args, clip_args=None):\n        self.pt_optim_constructor = optim_constructor\n\n        # must be callable or dict\n        assert callable(optim_args) or isinstance(\n            optim_args, dict), ""optim_args must be function that returns defaults or a defaults dictionary""\n\n        if clip_args is None:\n            clip_args = {}\n\n        # must be callable or dict\n        assert callable(clip_args) or isinstance(\n            clip_args, dict), ""clip_args must be function that returns defaults or a defaults dictionary""\n\n        # hold our args to be called/used\n        self.pt_optim_args = optim_args\n        self.pt_clip_args = clip_args\n\n        # holds the torch optimizer objects\n        self.optim_objs = {}\n        self.grad_clip = {}\n\n        # any optimizer state that\'s waiting to be consumed (because that parameter hasn\'t been seen before)\n        self._state_waiting_to_be_consumed = {}\n\n    def __call__(self, params,  *args, **kwargs):\n        """"""\n        :param params: a list of parameters\n        :type params: an iterable of strings\n\n        Do an optimization step for each param in params. If a given param has never been seen before,\n        initialize an optimizer for it.\n        """"""\n        for p in params:\n            # if we have not seen this param before, we instantiate an optim object to deal with it\n            if p not in self.optim_objs:\n                # create a single optim object for that param\n                self.optim_objs[p] = self._get_optim(p)\n                # create a gradient clipping function if specified\n                self.grad_clip[p] = self._get_grad_clip(p)\n                # set state from _state_waiting_to_be_consumed if present\n                param_name = pyro.get_param_store().param_name(p)\n                if param_name in self._state_waiting_to_be_consumed:\n                    state = self._state_waiting_to_be_consumed.pop(param_name)\n                    self.optim_objs[p].load_state_dict(state)\n\n            if self.grad_clip[p] is not None:\n                self.grad_clip[p](p)\n\n            if isinstance(self.optim_objs[p], torch.optim.lr_scheduler._LRScheduler) or \\\n                    isinstance(self.optim_objs[p], torch.optim.lr_scheduler.ReduceLROnPlateau):\n                # if optim object was a scheduler, perform an optimizer step\n                self.optim_objs[p].optimizer.step(*args, **kwargs)\n            else:\n                self.optim_objs[p].step(*args, **kwargs)\n\n    def get_state(self):\n        """"""\n        Get state associated with all the optimizers in the form of a dictionary with\n        key-value pairs (parameter name, optim state dicts)\n        """"""\n        state_dict = {}\n        for param in self.optim_objs:\n            param_name = pyro.get_param_store().param_name(param)\n            state_dict[param_name] = self.optim_objs[param].state_dict()\n        return state_dict\n\n    def set_state(self, state_dict):\n        """"""\n        Set the state associated with all the optimizers using the state obtained\n        from a previous call to get_state()\n        """"""\n        self._state_waiting_to_be_consumed = state_dict\n\n    def save(self, filename):\n        """"""\n        :param filename: file name to save to\n        :type filename: str\n\n        Save optimizer state to disk\n        """"""\n        with open(filename, ""wb"") as output_file:\n            torch.save(self.get_state(), output_file)\n\n    def load(self, filename):\n        """"""\n        :param filename: file name to load from\n        :type filename: str\n\n        Load optimizer state from disk\n        """"""\n        with open(filename, ""rb"") as input_file:\n            state = torch.load(input_file)\n        self.set_state(state)\n\n    def _get_optim(self, param):\n        return self.pt_optim_constructor([param], **self._get_optim_args(param))\n\n    # helper to fetch the optim args if callable (only used internally)\n    def _get_optim_args(self, param):\n        # if we were passed a fct, we call fct with param info\n        # arguments are (module name, param name) e.g. (\'mymodule\', \'bias\')\n        if callable(self.pt_optim_args):\n\n            # get param name\n            param_name = pyro.get_param_store().param_name(param)\n            module_name = module_from_param_with_module_name(param_name)\n            stripped_param_name = user_param_name(param_name)\n\n            # invoke the user-provided callable\n            opt_dict = self.pt_optim_args(module_name, stripped_param_name)\n\n            # must be dictionary\n            assert isinstance(opt_dict, dict), ""per-param optim arg must return defaults dictionary""\n            return opt_dict\n        else:\n            return self.pt_optim_args\n\n    def _get_grad_clip(self, param):\n        grad_clip_args = self._get_grad_clip_args(param)\n\n        if not grad_clip_args:\n            return None\n\n        def _clip_grad(params):\n            self._clip_grad(params, **grad_clip_args)\n\n        return _clip_grad\n\n    def _get_grad_clip_args(self, param):\n        # if we were passed a fct, we call fct with param info\n        # arguments are (module name, param name) e.g. (\'mymodule\', \'bias\')\n        if callable(self.pt_clip_args):\n\n            # get param name\n            param_name = pyro.get_param_store().param_name(param)\n            module_name = module_from_param_with_module_name(param_name)\n            stripped_param_name = user_param_name(param_name)\n\n            # invoke the user-provided callable\n            clip_dict = self.pt_clip_args(module_name, stripped_param_name)\n\n            # must be dictionary\n            assert isinstance(clip_dict, dict), ""per-param clip arg must return defaults dictionary""\n            return clip_dict\n        else:\n            return self.pt_clip_args\n\n    @staticmethod\n    def _clip_grad(params, clip_norm=None, clip_value=None):\n        if clip_norm is not None:\n            clip_grad_norm_(params, clip_norm)\n        if clip_value is not None:\n            clip_grad_value_(params, clip_value)\n\n\ndef AdagradRMSProp(optim_args):\n    """"""\n    Wraps :class:`pyro.optim.adagrad_rmsprop.AdagradRMSProp` with :class:`~pyro.optim.optim.PyroOptim`.\n    """"""\n    return PyroOptim(pt_AdagradRMSProp, optim_args)\n\n\ndef ClippedAdam(optim_args):\n    """"""\n    Wraps :class:`pyro.optim.clipped_adam.ClippedAdam` with :class:`~pyro.optim.optim.PyroOptim`.\n    """"""\n    return PyroOptim(pt_ClippedAdam, optim_args)\n\n\ndef DCTAdam(optim_args):\n    """"""\n    Wraps :class:`pyro.optim.dct_adam.DCTAdam` with :class:`~pyro.optim.optim.PyroOptim`.\n    """"""\n    return PyroOptim(pt_DCTAdam, optim_args)\n'"
pyro/optim/pytorch_optimizers.py,9,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.optim import PyroOptim\nfrom pyro.optim.lr_scheduler import PyroLRScheduler\n\n__all__ = []\n# Programmatically load all optimizers from PyTorch.\nfor _name, _Optim in torch.optim.__dict__.items():\n    if not isinstance(_Optim, type):\n        continue\n    if not issubclass(_Optim, torch.optim.Optimizer):\n        continue\n    if _Optim is torch.optim.Optimizer:\n        continue\n    if _Optim is torch.optim.LBFGS:\n        # XXX LBFGS is not supported for SVI yet\n        continue\n\n    _PyroOptim = (lambda _Optim: lambda optim_args, clip_args=None: PyroOptim(_Optim, optim_args, clip_args))(_Optim)\n    _PyroOptim.__name__ = _name\n    _PyroOptim.__doc__ = 'Wraps :class:`torch.optim.{}` with :class:`~pyro.optim.optim.PyroOptim`.'.format(_name)\n\n    locals()[_name] = _PyroOptim\n    __all__.append(_name)\n    del _PyroOptim\n\n# Load all schedulers from PyTorch\nfor _name, _Optim in torch.optim.lr_scheduler.__dict__.items():\n    if not isinstance(_Optim, type):\n        continue\n    if not issubclass(_Optim, torch.optim.lr_scheduler._LRScheduler) and _name != 'ReduceLROnPlateau':\n        continue\n    if _Optim is torch.optim.Optimizer:\n        continue\n\n    _PyroOptim = (\n        lambda _Optim: lambda optim_args, clip_args=None: PyroLRScheduler(_Optim, optim_args, clip_args)\n    )(_Optim)\n    _PyroOptim.__name__ = _name\n    _PyroOptim.__doc__ = 'Wraps :class:`torch.optim.{}` with '.format(_name) +\\\n                         ':class:`~pyro.optim.lr_scheduler.PyroLRScheduler`.'\n\n    locals()[_name] = _PyroOptim\n    __all__.append(_name)\n    del _PyroOptim\n"""
pyro/params/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .param_store import module_from_param_with_module_name, param_with_module_name, user_param_name\n\n__all__ = [\n    ""module_from_param_with_module_name"",\n    ""param_with_module_name"",\n    ""user_param_name"",\n]\n'"
pyro/params/param_store.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport re\nimport warnings\nimport weakref\n\nimport torch\nfrom torch.distributions import constraints, transform_to\n\n\nclass ParamStoreDict:\n    """"""\n    Global store for parameters in Pyro. This is basically a key-value store.\n    The typical user interacts with the ParamStore primarily through the\n    primitive `pyro.param`.\n\n    See `Intro Part II <http://pyro.ai/examples/intro_part_ii.html>`_ for further discussion\n    and `SVI Part I <http://pyro.ai/examples/svi_part_i.html>`_ for some examples.\n\n    Some things to bear in mind when using parameters in Pyro:\n\n    - parameters must be assigned unique names\n    - the `init_tensor` argument to `pyro.param` is only used the first time that a given (named)\n      parameter is registered with Pyro.\n    - for this reason, a user may need to use the `clear()` method if working in a REPL in order to\n      get the desired behavior. this method can also be invoked with `pyro.clear_param_store()`.\n    - the internal name of a parameter within a PyTorch `nn.Module` that has been registered with\n      Pyro is prepended with the Pyro name of the module. so nothing prevents the user from having\n      two different modules each of which contains a parameter named `weight`. by contrast, a user\n      can only have one top-level parameter named `weight` (outside of any module).\n    - parameters can be saved and loaded from disk using `save` and `load`.\n    - in general parameters are associated with both *constrained* and *unconstrained* values. for\n      example, under the hood a parameter that is constrained to be positive is represented as an\n      unconstrained tensor in log space.\n    """"""\n\n    # -------------------------------------------------------------------------------\n    # New dict-like interface\n\n    def __init__(self):\n        """"""\n        initialize ParamStore data structures\n        """"""\n        self._params = {}  # dictionary from param name to param\n        self._param_to_name = {}  # dictionary from unconstrained param to param name\n        self._constraints = {}  # dictionary from param name to constraint object\n\n    def clear(self):\n        """"""\n        Clear the ParamStore\n        """"""\n        self._params = {}\n        self._param_to_name = {}\n        self._constraints = {}\n\n    def items(self):\n        """"""\n        Iterate over ``(name, constrained_param)`` pairs. Note that `constrained_param` is\n        in the constrained (i.e. user-facing) space.\n        """"""\n        for name in self._params:\n            yield name, self[name]\n\n    def keys(self):\n        """"""\n        Iterate over param names.\n        """"""\n        return self._params.keys()\n\n    def values(self):\n        """"""\n        Iterate over constrained parameter values.\n        """"""\n        for name, constrained_param in self.items():\n            yield constrained_param\n\n    def __bool__(self):\n        return bool(self._params)\n\n    def __len__(self):\n        return len(self._params)\n\n    def __contains__(self, name):\n        return name in self._params\n\n    def __iter__(self):\n        """"""\n        Iterate over param names.\n        """"""\n        return iter(self.keys())\n\n    def __delitem__(self, name):\n        """"""\n        Remove a parameter from the param store.\n        """"""\n        unconstrained_value = self._params.pop(name)\n        self._param_to_name.pop(unconstrained_value)\n        self._constraints.pop(name)\n\n    def __getitem__(self, name):\n        """"""\n        Get the *constrained* value of a named parameter.\n        """"""\n        unconstrained_value = self._params[name]\n\n        # compute the constrained value\n        constraint = self._constraints[name]\n        constrained_value = transform_to(constraint)(unconstrained_value)\n        constrained_value.unconstrained = weakref.ref(unconstrained_value)\n\n        return constrained_value\n\n    def __setitem__(self, name, new_constrained_value):\n        """"""\n        Set the constrained value of an existing parameter, or the value of a\n        new *unconstrained* parameter. To declare a new parameter with\n        constraint, use :meth:`setdefault`.\n        """"""\n        # store constraint, defaulting to unconstrained\n        constraint = self._constraints.setdefault(name, constraints.real)\n\n        # compute the unconstrained value\n        with torch.no_grad():\n            # FIXME should we .detach() the new_constrained_value?\n            unconstrained_value = transform_to(constraint).inv(new_constrained_value)\n            unconstrained_value = unconstrained_value.contiguous()\n        unconstrained_value.requires_grad_(True)\n\n        # store a bidirectional mapping between name and unconstrained tensor\n        self._params[name] = unconstrained_value\n        self._param_to_name[unconstrained_value] = name\n\n    def setdefault(self, name, init_constrained_value, constraint=constraints.real):\n        """"""\n        Retrieve a *constrained* parameter value from the if it exists, otherwise\n        set the initial value. Note that this is a little fancier than\n        :meth:`dict.setdefault`.\n\n        If the parameter already exists, ``init_constrained_tensor`` will be ignored. To avoid\n        expensive creation of ``init_constrained_tensor`` you can wrap it in a ``lambda`` that\n        will only be evaluated if the parameter does not already exist::\n\n            param_store.get(""foo"", lambda: (0.001 * torch.randn(1000, 1000)).exp(),\n                            constraint=constraints.positive)\n\n        :param str name: parameter name\n        :param init_constrained_value: initial constrained value\n        :type init_constrained_value: torch.Tensor or callable returning a torch.Tensor\n        :param constraint: torch constraint object\n        :type constraint: ~torch.distributions.constraints.Constraint\n        :returns: constrained parameter value\n        :rtype: torch.Tensor\n        """"""\n        if name not in self._params:\n            # set the constraint\n            self._constraints[name] = constraint\n\n            # evaluate the lazy value\n            if callable(init_constrained_value):\n                init_constrained_value = init_constrained_value()\n\n            # set the initial value\n            self[name] = init_constrained_value\n\n        # get the param, which is guaranteed to exist\n        return self[name]\n\n    # -------------------------------------------------------------------------------\n    # Old non-dict interface\n\n    def named_parameters(self):\n        """"""\n        Returns an iterator over ``(name, unconstrained_value)`` tuples for\n        each parameter in the ParamStore. Note that, in the event the parameter is constrained,\n        `unconstrained_value` is in the unconstrained space implicitly used by the constraint.\n        """"""\n        return self._params.items()\n\n    def get_all_param_names(self):\n        warnings.warn(""ParamStore.get_all_param_names() is deprecated; use .keys() instead."",\n                      DeprecationWarning)\n        return self.keys()\n\n    def replace_param(self, param_name, new_param, old_param):\n        warnings.warn(""ParamStore.replace_param() is deprecated; use .__setitem__() instead."",\n                      DeprecationWarning)\n        assert self._params[param_name] is old_param.unconstrained()\n        self[param_name] = new_param\n\n    def get_param(self, name, init_tensor=None, constraint=constraints.real, event_dim=None):\n        """"""\n        Get parameter from its name. If it does not yet exist in the\n        ParamStore, it will be created and stored.\n        The Pyro primitive `pyro.param` dispatches to this method.\n\n        :param name: parameter name\n        :type name: str\n        :param init_tensor: initial tensor\n        :type init_tensor: torch.Tensor\n        :param constraint: torch constraint\n        :type constraint: torch.distributions.constraints.Constraint\n        :param int event_dim: (ignored)\n        :returns: parameter\n        :rtype: torch.Tensor\n        """"""\n        if init_tensor is None:\n            return self[name]\n        else:\n            return self.setdefault(name, init_tensor, constraint)\n\n    def match(self, name):\n        """"""\n        Get all parameters that match regex. The parameter must exist.\n\n        :param name: regular expression\n        :type name: str\n        :returns: dict with key param name and value torch Tensor\n        """"""\n        pattern = re.compile(name)\n        return {name: self[name] for name in self if pattern.match(name)}\n\n    def param_name(self, p):\n        """"""\n        Get parameter name from parameter\n\n        :param p: parameter\n        :returns: parameter name\n        """"""\n        return self._param_to_name.get(p)\n\n    def get_state(self):\n        """"""\n        Get the ParamStore state.\n        """"""\n        state = {\n            \'params\': self._params,\n            \'constraints\': self._constraints,\n        }\n        return state\n\n    def set_state(self, state):\n        """"""\n        Set the ParamStore state using state from a previous get_state() call\n        """"""\n        assert isinstance(state, dict), ""malformed ParamStore state""\n        assert set(state.keys()) == set([\'params\', \'constraints\']), \\\n            ""malformed ParamStore keys {}"".format(state.keys())\n\n        for param_name, param in state[\'params\'].items():\n            self._params[param_name] = param\n            self._param_to_name[param] = param_name\n\n        for param_name, constraint in state[\'constraints\'].items():\n            if isinstance(constraint, type(constraints.real)):\n                # Work around lack of hash & equality comparison on constraints.\n                constraint = constraints.real\n            self._constraints[param_name] = constraint\n\n    def save(self, filename):\n        """"""\n        Save parameters to disk\n\n        :param filename: file name to save to\n        :type filename: str\n        """"""\n        with open(filename, ""wb"") as output_file:\n            torch.save(self.get_state(), output_file)\n\n    def load(self, filename, map_location=None):\n        """"""\n        Loads parameters from disk\n\n        .. note::\n\n           If using :meth:`pyro.module` on parameters loaded from\n           disk, be sure to set the ``update_module_params`` flag::\n\n               pyro.get_param_store().load(\'saved_params.save\')\n               pyro.module(\'module\', nn, update_module_params=True)\n\n        :param filename: file name to load from\n        :type filename: str\n        :param map_location: specifies how to remap storage locations\n        :type map_location: function, torch.device, string or a dict\n        """"""\n        with open(filename, ""rb"") as input_file:\n            state = torch.load(input_file, map_location)\n        self.set_state(state)\n\n\n# used to create fully-formed param names, e.g. mymodule$$$mysubmodule.weight\n_MODULE_NAMESPACE_DIVIDER = ""$$$""\n\n\ndef param_with_module_name(pyro_name, param_name):\n    return _MODULE_NAMESPACE_DIVIDER.join([pyro_name, param_name])\n\n\ndef module_from_param_with_module_name(param_name):\n    return param_name.split(_MODULE_NAMESPACE_DIVIDER)[0]\n\n\ndef user_param_name(param_name):\n    if _MODULE_NAMESPACE_DIVIDER in param_name:\n        return param_name.split(_MODULE_NAMESPACE_DIVIDER)[1]\n    return param_name\n'"
pyro/poutine/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .handlers import (block, broadcast, condition, do, enum, escape, infer_config, lift, markov, mask, queue, reparam,\n                       replay, scale, seed, trace, uncondition)\nfrom .runtime import NonlocalExit\nfrom .trace_struct import Trace\nfrom .util import enable_validation, is_validation_enabled\n\n__all__ = [\n    ""block"",\n    ""broadcast"",\n    ""condition"",\n    ""do"",\n    ""enable_validation"",\n    ""enum"",\n    ""escape"",\n    ""infer_config"",\n    ""is_validation_enabled"",\n    ""lift"",\n    ""markov"",\n    ""mask"",\n    ""NonlocalExit"",\n    ""replay"",\n    ""reparam"",\n    ""queue"",\n    ""scale"",\n    ""seed"",\n    ""trace"",\n    ""Trace"",\n    ""uncondition"",\n]\n'"
pyro/poutine/block_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nfrom pyro.poutine.messenger import Messenger\n\n\ndef _block_fn(expose, expose_types, hide, hide_types, hide_all, msg):\n    # handle observes\n    if msg[""type""] == ""sample"" and msg[""is_observed""]:\n        msg_type = ""observe""\n    else:\n        msg_type = msg[""type""]\n\n    is_not_exposed = (msg[""name""] not in expose) and \\\n                     (msg_type not in expose_types)\n\n    # decision rule for hiding:\n    if (msg[""name""] in hide) or \\\n            (msg_type in hide_types) or \\\n            (is_not_exposed and hide_all):  # noqa: E129\n\n        return True\n    # otherwise expose\n    else:\n        return False\n\n\ndef _make_default_hide_fn(hide_all, expose_all, hide, expose, hide_types, expose_types):\n    # first, some sanity checks:\n    # hide_all and expose_all intersect?\n    assert (hide_all is False and expose_all is False) or \\\n        (hide_all != expose_all), ""cannot hide and expose a site""\n\n    # hide and expose intersect?\n    if hide is None:\n        hide = []\n    else:\n        hide_all = False\n\n    if expose is None:\n        expose = []\n    else:\n        hide_all = True\n\n    assert set(hide).isdisjoint(set(expose)), \\\n        ""cannot hide and expose a site""\n\n    # hide_types and expose_types intersect?\n    if hide_types is None:\n        hide_types = []\n    else:\n        hide_all = False\n\n    if expose_types is None:\n        expose_types = []\n    else:\n        hide_all = True\n\n    assert set(hide_types).isdisjoint(set(expose_types)), \\\n        ""cannot hide and expose a site type""\n\n    return partial(_block_fn, expose, expose_types, hide, hide_types, hide_all)\n\n\nclass BlockMessenger(Messenger):\n    """"""\n    This handler selectively hides Pyro primitive sites from the outside world.\n    Default behavior: block everything.\n\n    A site is hidden if at least one of the following holds:\n\n        0. ``hide_fn(msg) is True`` or ``(not expose_fn(msg)) is True``\n        1. ``msg[""name""] in hide``\n        2. ``msg[""type""] in hide_types``\n        3. ``msg[""name""] not in expose and msg[""type""] not in expose_types``\n        4. ``hide``, ``hide_types``, and ``expose_types`` are all ``None``\n\n    For example, suppose the stochastic function fn has two sample sites ""a"" and ""b"".\n    Then any effect outside of ``BlockMessenger(fn, hide=[""a""])``\n    will not be applied to site ""a"" and will only see site ""b"":\n\n        >>> def fn():\n        ...     a = pyro.sample(""a"", dist.Normal(0., 1.))\n        ...     return pyro.sample(""b"", dist.Normal(a, 1.))\n        >>> fn_inner = pyro.poutine.trace(fn)\n        >>> fn_outer = pyro.poutine.trace(pyro.poutine.block(fn_inner, hide=[""a""]))\n        >>> trace_inner = fn_inner.get_trace()\n        >>> trace_outer  = fn_outer.get_trace()\n        >>> ""a"" in trace_inner\n        True\n        >>> ""a"" in trace_outer\n        False\n        >>> ""b"" in trace_inner\n        True\n        >>> ""b"" in trace_outer\n        True\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param hide_fn: function that takes a site and returns True to hide the site\n        or False/None to expose it.  If specified, all other parameters are ignored.\n        Only specify one of hide_fn or expose_fn, not both.\n    :param expose_fn: function that takes a site and returns True to expose the site\n        or False/None to hide it.  If specified, all other parameters are ignored.\n        Only specify one of hide_fn or expose_fn, not both.\n    :param bool hide_all: hide all sites\n    :param bool expose_all: expose all sites normally\n    :param list hide: list of site names to hide\n    :param list expose: list of site names to be exposed while all others hidden\n    :param list hide_types: list of site types to be hidden\n    :param lits expose_types: list of site types to be exposed while all others hidden\n    :returns: stochastic function decorated with a :class:`~pyro.poutine.block_messenger.BlockMessenger`\n    """"""\n\n    def __init__(self, hide_fn=None, expose_fn=None,\n                 hide_all=True, expose_all=False,\n                 hide=None, expose=None,\n                 hide_types=None, expose_types=None):\n        super().__init__()\n        if not (hide_fn is None or expose_fn is None):\n            raise ValueError(""Only specify one of hide_fn or expose_fn"")\n        if hide_fn is not None:\n            self.hide_fn = hide_fn\n        elif expose_fn is not None:\n            self.hide_fn = lambda msg: not expose_fn(msg)\n        else:\n            self.hide_fn = _make_default_hide_fn(hide_all, expose_all,\n                                                 hide, expose,\n                                                 hide_types, expose_types)\n\n    def _process_message(self, msg):\n        msg[""stop""] = bool(self.hide_fn(msg))\n        return None\n'"
pyro/poutine/broadcast_messenger.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.util import ignore_jit_warnings\nfrom .messenger import Messenger\n\n\nclass BroadcastMessenger(Messenger):\n    """"""\n    Automatically broadcasts the batch shape of the stochastic function\n    at a sample site when inside a single or nested plate context.\n    The existing `batch_shape` must be broadcastable with the size\n    of the :class:`~pyro.plate` contexts installed in the\n    `cond_indep_stack`.\n\n    Notice how `model_automatic_broadcast` below automates expanding of\n    distribution batch shapes. This makes it easy to modularize a\n    Pyro model as the sub-components are agnostic of the wrapping\n    :class:`~pyro.plate` contexts.\n\n    >>> def model_broadcast_by_hand():\n    ...     with IndepMessenger(""batch"", 100, dim=-2):\n    ...         with IndepMessenger(""components"", 3, dim=-1):\n    ...             sample = pyro.sample(""sample"", dist.Bernoulli(torch.ones(3) * 0.5)\n    ...                                                .expand_by(100))\n    ...             assert sample.shape == torch.Size((100, 3))\n    ...     return sample\n\n    >>> @poutine.broadcast\n    ... def model_automatic_broadcast():\n    ...     with IndepMessenger(""batch"", 100, dim=-2):\n    ...         with IndepMessenger(""components"", 3, dim=-1):\n    ...             sample = pyro.sample(""sample"", dist.Bernoulli(torch.tensor(0.5)))\n    ...             assert sample.shape == torch.Size((100, 3))\n    ...     return sample\n    """"""\n\n    @staticmethod\n    @ignore_jit_warnings([""Converting a tensor to a Python boolean""])\n    def _pyro_sample(msg):\n        """"""\n        :param msg: current message at a trace site.\n        """"""\n        if msg[""done""] or msg[""type""] != ""sample"":\n            return\n\n        dist = msg[""fn""]\n        actual_batch_shape = getattr(dist, ""batch_shape"", None)\n        if actual_batch_shape is not None:\n            target_batch_shape = [None if size == 1 else size\n                                  for size in actual_batch_shape]\n            for f in msg[""cond_indep_stack""]:\n                if f.dim is None or f.size == -1:\n                    continue\n                assert f.dim < 0\n                target_batch_shape = [None] * (-f.dim - len(target_batch_shape)) + target_batch_shape\n                if target_batch_shape[f.dim] is not None and target_batch_shape[f.dim] != f.size:\n                    raise ValueError(""Shape mismatch inside plate(\'{}\') at site {} dim {}, {} vs {}"".format(\n                        f.name, msg[\'name\'], f.dim, f.size, target_batch_shape[f.dim]))\n                target_batch_shape[f.dim] = f.size\n            # Starting from the right, if expected size is None at an index,\n            # set it to the actual size if it exists, else 1.\n            for i in range(-len(target_batch_shape) + 1, 1):\n                if target_batch_shape[i] is None:\n                    target_batch_shape[i] = actual_batch_shape[i] if len(actual_batch_shape) >= -i else 1\n            msg[""fn""] = dist.expand(target_batch_shape)\n            if msg[""fn""].has_rsample != dist.has_rsample:\n                msg[""fn""].has_rsample = dist.has_rsample  # copy custom attribute\n'"
pyro/poutine/condition_messenger.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .messenger import Messenger\nfrom .trace_struct import Trace\n\n\nclass ConditionMessenger(Messenger):\n    """"""\n    Given a stochastic function with some sample statements\n    and a dictionary of observations at names,\n    change the sample statements at those names into observes\n    with those values.\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(""s"", torch.tensor(0.5))\n        ...     z = pyro.sample(""z"", dist.Normal(x, s))\n        ...     return z ** 2\n\n    To observe a value for site `z`, we can write\n\n        >>> conditioned_model = pyro.poutine.condition(model, data={""z"": torch.tensor(1.)})\n\n    This is equivalent to adding `obs=value` as a keyword argument\n    to `pyro.sample(""z"", ...)` in `model`.\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param data: a dict or a :class:`~pyro.poutine.Trace`\n    :returns: stochastic function decorated with a :class:`~pyro.poutine.condition_messenger.ConditionMessenger`\n    """"""\n    def __init__(self, data):\n        """"""\n        :param data: a dict or a Trace\n\n        Constructor. Doesn\'t do much, just stores the stochastic function\n        and the data to condition on.\n        """"""\n        super().__init__()\n        self.data = data\n\n    def _pyro_sample(self, msg):\n        """"""\n        :param msg: current message at a trace site.\n        :returns: a sample from the stochastic function at the site.\n\n        If msg[""name""] appears in self.data,\n        convert the sample site into an observe site\n        whose observed value is the value from self.data[msg[""name""]].\n\n        Otherwise, implements default sampling behavior\n        with no additional effects.\n        """"""\n        name = msg[""name""]\n\n        if name in self.data:\n            if isinstance(self.data, Trace):\n                msg[""value""] = self.data.nodes[name][""value""]\n            else:\n                msg[""value""] = self.data[name]\n            msg[""is_observed""] = msg[""value""] is not None\n        return None\n'"
pyro/poutine/do_messenger.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numbers\nimport warnings\n\nimport torch\n\nfrom .messenger import Messenger\nfrom .runtime import apply_stack\n\n\nclass DoMessenger(Messenger):\n    """"""\n    Given a stochastic function with some sample statements\n    and a dictionary of values at names,\n    set the return values of those sites equal to the values\n    as if they were hard-coded to those values\n    and introduce fresh sample sites with the same names\n    whose values do not propagate.\n\n    Composes freely with :func:`~pyro.poutine.handlers.condition`\n    to represent counterfactual distributions over potential outcomes.\n    See Single World Intervention Graphs [1] for additional details and theory.\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(""s"", torch.tensor(0.5))\n        ...     z = pyro.sample(""z"", dist.Normal(x, s))\n        ...     return z ** 2\n\n    To intervene with a value for site `z`, we can write\n\n        >>> intervened_model = pyro.poutine.do(model, data={""z"": torch.tensor(1.)})\n\n    This is equivalent to replacing `z = pyro.sample(""z"", ...)` with\n    `z = torch.tensor(1.)`\n    and introducing a fresh sample site pyro.sample(""z"", ...) whose value is not used elsewhere.\n\n    References\n\n    [1] `Single World Intervention Graphs: A Primer`,\n        Thomas Richardson, James Robins\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param data: a ``dict`` mapping sample site names to interventions\n    :returns: stochastic function decorated with a :class:`~pyro.poutine.do_messenger.DoMessenger`\n    """"""\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n        self._intervener_id = str(id(self))\n\n    def _pyro_sample(self, msg):\n        if msg.get(\'_intervener_id\', None) != self._intervener_id and \\\n                self.data.get(msg[\'name\']) is not None:\n\n            if msg.get(\'_intervener_id\', None) is not None:\n                warnings.warn(\n                    ""Attempting to intervene on variable {} multiple times,""\n                    ""this is almost certainly incorrect behavior"".format(msg[\'name\']),\n                    RuntimeWarning)\n\n            msg[\'_intervener_id\'] = self._intervener_id\n\n            # split node, avoid reapplying self recursively to new node\n            new_msg = msg.copy()\n            apply_stack(new_msg)\n\n            # apply intervention\n            intervention = self.data[msg[\'name\']]\n            msg[\'name\'] = msg[\'name\'] + ""__CF""  # mangle old name\n\n            if isinstance(intervention, (numbers.Number, torch.Tensor)):\n                msg[\'value\'] = intervention\n                msg[\'is_observed\'] = True\n                msg[\'stop\'] = True\n            else:\n                raise NotImplementedError(\n                    ""Interventions of type {} not implemented (yet)"".format(type(intervention)))\n\n        return None\n'"
pyro/poutine/enum_messenger.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.distributions import Categorical\nfrom pyro.distributions.torch_distribution import TorchDistributionMixin\nfrom pyro.ops.indexing import Vindex\nfrom pyro.util import ignore_jit_warnings\n\nfrom .messenger import Messenger\nfrom .runtime import _ENUM_ALLOCATOR\n\n\ndef _tmc_mixture_sample(msg):\n    dist, num_samples = msg[""fn""], msg[""infer""].get(""num_samples"")\n\n    # find batch dims that aren\'t plate dims\n    batch_shape = [1] * len(dist.batch_shape)\n    for f in msg[""cond_indep_stack""]:\n        if f.vectorized:\n            batch_shape[f.dim] = f.size if f.size > 0 else dist.batch_shape[f.dim]\n    batch_shape = tuple(batch_shape)\n\n    # sample a batch\n    sample_shape = (num_samples,)\n    fat_sample = dist(sample_shape=sample_shape)  # TODO thin before sampling\n    assert fat_sample.shape == sample_shape + dist.batch_shape + dist.event_shape\n    assert any(d > 1 for d in fat_sample.shape)\n\n    target_shape = (num_samples,) + batch_shape + dist.event_shape\n\n    # if this site has any possible ancestors, sample ancestor indices uniformly\n    thin_sample = fat_sample\n    if thin_sample.shape != target_shape:\n\n        index = [Ellipsis] + [slice(None)] * (len(thin_sample.shape) - 1)\n        squashed_dims = []\n        for squashed_dim, squashed_size in zip(range(1, len(thin_sample.shape)), thin_sample.shape[1:]):\n            if squashed_size > 1 and (target_shape[squashed_dim] == 1 or squashed_dim == 0):\n                # uniformly sample one ancestor per upstream particle population\n                ancestor_dist = Categorical(logits=torch.zeros((squashed_size,), device=thin_sample.device))\n                ancestor_index = ancestor_dist.sample(sample_shape=(num_samples,))\n                index[squashed_dim] = ancestor_index\n                squashed_dims.append(squashed_dim)\n\n        thin_sample = Vindex(thin_sample)[tuple(index)]\n        for squashed_dim in squashed_dims:\n            thin_sample = thin_sample.unsqueeze(squashed_dim)\n\n    assert thin_sample.shape == target_shape\n    return thin_sample\n\n\ndef _tmc_diagonal_sample(msg):\n    dist, num_samples = msg[""fn""], msg[""infer""].get(""num_samples"")\n\n    # find batch dims that aren\'t plate dims\n    batch_shape = [1] * len(dist.batch_shape)\n    for f in msg[""cond_indep_stack""]:\n        if f.vectorized:\n            batch_shape[f.dim] = f.size if f.size > 0 else dist.batch_shape[f.dim]\n    batch_shape = tuple(batch_shape)\n\n    # sample a batch\n    sample_shape = (num_samples,)\n    fat_sample = dist(sample_shape=sample_shape)  # TODO thin before sampling\n    assert fat_sample.shape == sample_shape + dist.batch_shape + dist.event_shape\n    assert any(d > 1 for d in fat_sample.shape)\n\n    target_shape = (num_samples,) + batch_shape + dist.event_shape\n\n    # if this site has any ancestors, choose ancestors from diagonal approximation\n    thin_sample = fat_sample\n    if thin_sample.shape != target_shape:\n\n        index = [Ellipsis] + [slice(None)] * (len(thin_sample.shape) - 1)\n        squashed_dims = []\n        for squashed_dim, squashed_size in zip(range(1, len(thin_sample.shape)), thin_sample.shape[1:]):\n            if squashed_size > 1 and (target_shape[squashed_dim] == 1 or squashed_dim == 0):\n                # diagonal approximation: identify particle indices across populations\n                ancestor_index = torch.arange(squashed_size, device=thin_sample.device)\n                index[squashed_dim] = ancestor_index\n                squashed_dims.append(squashed_dim)\n\n        thin_sample = Vindex(thin_sample)[tuple(index)]\n        for squashed_dim in squashed_dims:\n            thin_sample = thin_sample.unsqueeze(squashed_dim)\n\n    assert thin_sample.shape == target_shape\n    return thin_sample\n\n\ndef enumerate_site(msg):\n    dist = msg[""fn""]\n    num_samples = msg[""infer""].get(""num_samples"", None)\n    if num_samples is None:\n        # Enumerate over the support of the distribution.\n        value = dist.enumerate_support(expand=msg[""infer""].get(""expand"", False))\n    elif num_samples > 1 and not msg[""infer""].get(""expand"", False):\n        tmc_strategy = msg[""infer""].get(""tmc"", ""diagonal"")\n        if tmc_strategy == ""mixture"":\n            value = _tmc_mixture_sample(msg)\n        elif tmc_strategy == ""diagonal"":\n            value = _tmc_diagonal_sample(msg)\n        else:\n            raise ValueError(""{} not a valid TMC strategy"".format(tmc_strategy))\n    elif num_samples > 1 and msg[""infer""][""expand""]:\n        # Monte Carlo sample the distribution.\n        value = dist(sample_shape=(num_samples,))\n    assert value.dim() == 1 + len(dist.batch_shape) + len(dist.event_shape)\n    return value\n\n\nclass EnumMessenger(Messenger):\n    """"""\n    Enumerates in parallel over discrete sample sites marked\n    ``infer={""enumerate"": ""parallel""}``.\n\n    :param int first_available_dim: The first tensor dimension (counting\n        from the right) that is available for parallel enumeration. This\n        dimension and all dimensions left may be used internally by Pyro.\n        This should be a negative integer or None.\n    """"""\n    def __init__(self, first_available_dim=None):\n        assert first_available_dim is None or first_available_dim < 0, first_available_dim\n        self.first_available_dim = first_available_dim\n        super().__init__()\n\n    def __enter__(self):\n        if self.first_available_dim is not None:\n            _ENUM_ALLOCATOR.set_first_available_dim(self.first_available_dim)\n        self._markov_depths = {}  # site name -> depth (nonnegative integer)\n        self._param_dims = {}  # site name -> (enum dim -> unique id)\n        self._value_dims = {}  # site name -> (enum dim -> unique id)\n        return super().__enter__()\n\n    @ignore_jit_warnings()\n    def _pyro_sample(self, msg):\n        """"""\n        :param msg: current message at a trace site.\n        :returns: a sample from the stochastic function at the site.\n        """"""\n        if msg[""done""] or not isinstance(msg[""fn""], TorchDistributionMixin):\n            return\n\n        # Compute upstream dims in scope; these are unsafe to use for this site\'s target_dim.\n        scope = msg[""infer""].get(""_markov_scope"")  # site name -> markov depth\n        param_dims = _ENUM_ALLOCATOR.dim_to_id.copy()  # enum dim -> unique id\n        if scope is not None:\n            for name, depth in scope.items():\n                if self._markov_depths[name] == depth:  # hide sites whose markov context has exited\n                    param_dims.update(self._value_dims[name])\n            self._markov_depths[msg[""name""]] = msg[""infer""][""_markov_depth""]\n        self._param_dims[msg[""name""]] = param_dims\n        if msg[""is_observed""] or msg[""infer""].get(""enumerate"") != ""parallel"":\n            return\n\n        # Compute an enumerated value (at an arbitrary dim).\n        value = enumerate_site(msg)\n        actual_dim = -1 - len(msg[""fn""].batch_shape)  # the leftmost dim of log_prob\n\n        # Move actual_dim to a safe target_dim.\n        target_dim, id_ = _ENUM_ALLOCATOR.allocate(None if scope is None else param_dims)\n        event_dim = msg[""fn""].event_dim\n        categorical_support = getattr(value, \'_pyro_categorical_support\', None)\n        if categorical_support is not None:\n            # Preserve categorical supports to speed up Categorical.log_prob().\n            # See pyro/distributions/torch.py for details.\n            assert target_dim < 0\n            value = value.reshape(value.shape[:1] + (1,) * (-1 - target_dim))\n            value._pyro_categorical_support = categorical_support\n        elif actual_dim < target_dim:\n            assert value.size(target_dim - event_dim) == 1, \\\n                \'pyro.markov dim conflict at dim {}\'.format(actual_dim)\n            value = value.transpose(target_dim - event_dim, actual_dim - event_dim)\n            while value.dim() and value.size(0) == 1:\n                value = value.squeeze(0)\n        elif target_dim < actual_dim:\n            diff = actual_dim - target_dim\n            value = value.reshape(value.shape[:1] + (1,) * diff + value.shape[1:])\n\n        # Compute dims passed downstream through the value.\n        value_dims = {dim: param_dims[dim] for dim in range(event_dim - value.dim(), 0)\n                      if value.size(dim - event_dim) > 1 and dim in param_dims}\n        value_dims[target_dim] = id_\n\n        msg[""infer""][""_enumerate_dim""] = target_dim\n        msg[""infer""][""_dim_to_id""] = value_dims\n        msg[""value""] = value\n        msg[""done""] = True\n\n    def _pyro_post_sample(self, msg):\n        # Save all dims exposed in this sample value.\n        # Whereas all of site[""_dim_to_id""] are needed to interpret a\n        # site\'s log_prob tensor, only a filtered subset self._value_dims[msg[""name""]]\n        # are needed to interpret a site\'s value.\n        if not isinstance(msg[""fn""], TorchDistributionMixin):\n            return\n        value = msg[""value""]\n        if value is None:\n            return\n        shape = value.shape[:value.dim() - msg[""fn""].event_dim]\n        dim_to_id = msg[""infer""].setdefault(""_dim_to_id"", {})\n        dim_to_id.update(self._param_dims.get(msg[""name""], {}))\n        with ignore_jit_warnings():\n            self._value_dims[msg[""name""]] = {dim: id_ for dim, id_ in dim_to_id.items()\n                                             if len(shape) >= -dim and shape[dim] > 1}\n'"
pyro/poutine/escape_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .messenger import Messenger\nfrom .runtime import NonlocalExit\n\n\nclass EscapeMessenger(Messenger):\n    """"""\n    Messenger that does a nonlocal exit by raising a util.NonlocalExit exception\n    """"""\n    def __init__(self, escape_fn):\n        """"""\n        :param escape_fn: function that takes a msg as input and returns True\n            if the poutine should perform a nonlocal exit at that site.\n\n        Constructor.  Stores fn and escape_fn.\n        """"""\n        super().__init__()\n        self.escape_fn = escape_fn\n\n    def _pyro_sample(self, msg):\n        """"""\n        :param msg: current message at a trace site\n        :returns: a sample from the stochastic function at the site.\n\n        Evaluates self.escape_fn on the site (self.escape_fn(msg)).\n\n        If this returns True, raises an exception NonlocalExit(msg).\n        Else, implements default _pyro_sample behavior with no additional effects.\n        """"""\n        if self.escape_fn(msg):\n            msg[""done""] = True\n            msg[""stop""] = True\n\n            def cont(m):\n                raise NonlocalExit(m)\n            msg[""continuation""] = cont\n        return None\n'"
pyro/poutine/handlers.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nPoutine is a library of composable effect handlers for recording and modifying the\nbehavior of Pyro programs. These lower-level ingredients simplify the implementation\nof new inference algorithms and behavior.\n\nHandlers can be used as higher-order functions, decorators, or context managers\nto modify the behavior of functions or blocks of code:\n\nFor example, consider the following Pyro program:\n\n    >>> def model(x):\n    ...     s = pyro.param(""s"", torch.tensor(0.5))\n    ...     z = pyro.sample(""z"", dist.Normal(x, s))\n    ...     return z ** 2\n\nWe can mark sample sites as observed using ``condition``,\nwhich returns a callable with the same input and output signatures as ``model``:\n\n    >>> conditioned_model = poutine.condition(model, data={""z"": 1.0})\n\nWe can also use handlers as decorators:\n\n    >>> @pyro.condition(data={""z"": 1.0})\n    ... def model(x):\n    ...     s = pyro.param(""s"", torch.tensor(0.5))\n    ...     z = pyro.sample(""z"", dist.Normal(x, s))\n    ...     return z ** 2\n\nOr as context managers:\n\n    >>> with pyro.condition(data={""z"": 1.0}):\n    ...     s = pyro.param(""s"", torch.tensor(0.5))\n    ...     z = pyro.sample(""z"", dist.Normal(0., s))\n    ...     y = z ** 2\n\nHandlers compose freely:\n\n    >>> conditioned_model = poutine.condition(model, data={""z"": 1.0})\n    >>> traced_model = poutine.trace(conditioned_model)\n\nMany inference algorithms or algorithmic components can be implemented\nin just a few lines of code::\n\n    guide_tr = poutine.trace(guide).get_trace(...)\n    model_tr = poutine.trace(poutine.replay(conditioned_model, trace=guide_tr)).get_trace(...)\n    monte_carlo_elbo = model_tr.log_prob_sum() - guide_tr.log_prob_sum()\n""""""\n\nimport functools\nimport re\n\nfrom pyro.poutine import util\n\nfrom .block_messenger import BlockMessenger\nfrom .broadcast_messenger import BroadcastMessenger\nfrom .condition_messenger import ConditionMessenger\nfrom .do_messenger import DoMessenger\nfrom .enum_messenger import EnumMessenger\nfrom .escape_messenger import EscapeMessenger\nfrom .infer_config_messenger import InferConfigMessenger\nfrom .lift_messenger import LiftMessenger\nfrom .markov_messenger import MarkovMessenger\nfrom .mask_messenger import MaskMessenger\nfrom .plate_messenger import PlateMessenger  # noqa F403\nfrom .reparam_messenger import ReparamMessenger\nfrom .replay_messenger import ReplayMessenger\nfrom .runtime import NonlocalExit\nfrom .scale_messenger import ScaleMessenger\nfrom .seed_messenger import SeedMessenger\nfrom .trace_messenger import TraceMessenger\nfrom .uncondition_messenger import UnconditionMessenger\n\n############################################\n# Begin primitive operations\n############################################\n\n_msngrs = [\n    BlockMessenger,\n    BroadcastMessenger,\n    ConditionMessenger,\n    DoMessenger,\n    EnumMessenger,\n    EscapeMessenger,\n    InferConfigMessenger,\n    LiftMessenger,\n    MarkovMessenger,\n    MaskMessenger,\n    ReparamMessenger,\n    ReplayMessenger,\n    ScaleMessenger,\n    SeedMessenger,\n    TraceMessenger,\n    UnconditionMessenger,\n]\n\n\ndef _make_handler(msngr_cls):\n\n    def handler(fn=None, *args, **kwargs):\n        if fn is not None and not callable(fn):\n            raise ValueError(\n                ""{} is not callable, did you mean to pass it as a keyword arg?"".format(fn))\n        msngr = msngr_cls(*args, **kwargs)\n        return functools.update_wrapper(msngr(fn), fn, updated=()) if fn is not None else msngr\n\n    return handler\n\n\n_re1 = re.compile(\'(.)([A-Z][a-z]+)\')\n_re2 = re.compile(\'([a-z0-9])([A-Z])\')\nfor _msngr_cls in _msngrs:\n    _handler_name = _re2.sub(\n        r\'\\1_\\2\', _re1.sub(r\'\\1_\\2\', _msngr_cls.__name__.split(""Messenger"")[0])).lower()\n    _handler = _make_handler(_msngr_cls)\n    _handler.__module__ = __name__\n    _handler.__doc__ = """"""Convenient wrapper of :class:`~pyro.poutine.{}.{}` \\n\\n"""""".format(\n        _handler_name + ""_messenger"", _msngr_cls.__name__) + _msngr_cls.__doc__\n    _handler.__name__ = _handler_name\n    locals()[_handler_name] = _handler\n\n\n#########################################\n# Begin composite operations\n#########################################\n\ndef queue(fn=None, queue=None, max_tries=None,\n          extend_fn=None, escape_fn=None, num_samples=None):\n    """"""\n    Used in sequential enumeration over discrete variables.\n\n    Given a stochastic function and a queue,\n    return a return value from a complete trace in the queue.\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param queue: a queue data structure like multiprocessing.Queue to hold partial traces\n    :param max_tries: maximum number of attempts to compute a single complete trace\n    :param extend_fn: function (possibly stochastic) that takes a partial trace and a site,\n        and returns a list of extended traces\n    :param escape_fn: function (possibly stochastic) that takes a partial trace and a site,\n        and returns a boolean value to decide whether to exit\n    :param num_samples: optional number of extended traces for extend_fn to return\n    :returns: stochastic function decorated with poutine logic\n    """"""\n\n    if max_tries is None:\n        max_tries = int(1e6)\n\n    if extend_fn is None:\n        extend_fn = util.enum_extend\n\n    if escape_fn is None:\n        escape_fn = util.discrete_escape\n\n    if num_samples is None:\n        num_samples = -1\n\n    def wrapper(wrapped):\n        def _fn(*args, **kwargs):\n\n            for i in range(max_tries):\n                assert not queue.empty(), \\\n                    ""trying to get() from an empty queue will deadlock""\n\n                next_trace = queue.get()\n                try:\n                    ftr = trace(escape(replay(wrapped, trace=next_trace),  # noqa: F821\n                                       escape_fn=functools.partial(escape_fn,\n                                                                   next_trace)))\n                    return ftr(*args, **kwargs)\n                except NonlocalExit as site_container:\n                    site_container.reset_stack()\n                    for tr in extend_fn(ftr.trace.copy(), site_container.site,\n                                        num_samples=num_samples):\n                        queue.put(tr)\n\n            raise ValueError(""max tries ({}) exceeded"".format(str(max_tries)))\n        return _fn\n\n    return wrapper(fn) if fn is not None else wrapper\n\n\ndef markov(fn=None, history=1, keep=False, dim=None, name=None):\n    """"""\n    Markov dependency declaration.\n\n    This can be used in a variety of ways:\n\n        - as a context manager\n        - as a decorator for recursive functions\n        - as an iterator for markov chains\n\n    :param int history: The number of previous contexts visible from the\n        current context. Defaults to 1. If zero, this is similar to\n        :class:`pyro.plate`.\n    :param bool keep: If true, frames are replayable. This is important\n        when branching: if ``keep=True``, neighboring branches at the same\n        level can depend on each other; if ``keep=False``, neighboring branches\n        are independent (conditioned on their share""\n    :param int dim: An optional dimension to use for this independence index.\n        Interface stub, behavior not yet implemented.\n    :param str name: An optional unique name to help inference algorithms match\n        :func:`pyro.markov` sites between models and guides.\n        Interface stub, behavior not yet implemented.\n    """"""\n    if fn is None:\n        # Used as a decorator with bound args\n        return MarkovMessenger(history=history, keep=keep, dim=dim, name=name)\n    if not callable(fn):\n        # Used as a generator\n        return MarkovMessenger(history=history, keep=keep, dim=dim, name=name).generator(iterable=fn)\n    # Used as a decorator with bound args\n    return MarkovMessenger(history=history, keep=keep, dim=dim, name=name)(fn)\n'"
pyro/poutine/indep_messenger.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numbers\nfrom collections import namedtuple\n\nimport torch\n\nfrom pyro.util import ignore_jit_warnings\nfrom .messenger import Messenger\nfrom .runtime import _DIM_ALLOCATOR\n\n\nclass CondIndepStackFrame(namedtuple(""CondIndepStackFrame"", [""name"", ""dim"", ""size"", ""counter""])):\n    @property\n    def vectorized(self):\n        return self.dim is not None\n\n    def _key(self):\n        with ignore_jit_warnings([""Converting a tensor to a Python number""]):\n            size = self.size.item() if isinstance(self.size, torch.Tensor) else self.size\n            return self.name, self.dim, size, self.counter\n\n    def __eq__(self, other):\n        return type(self) == type(other) and self._key() == other._key()\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return hash(self._key())\n\n    def __str__(self):\n        return self.name\n\n\nclass IndepMessenger(Messenger):\n    """"""\n    This messenger keeps track of stack of independence information declared by\n    nested ``plate`` contexts. This information is stored in a\n    ``cond_indep_stack`` at each sample/observe site for consumption by\n    ``TraceMessenger``.\n\n    Example::\n\n        x_axis = IndepMessenger(\'outer\', 320, dim=-1)\n        y_axis = IndepMessenger(\'inner\', 200, dim=-2)\n        with x_axis:\n            x_noise = sample(""x_noise"", dist.Normal(loc, scale).expand_by([320]))\n        with y_axis:\n            y_noise = sample(""y_noise"", dist.Normal(loc, scale).expand_by([200, 1]))\n        with x_axis, y_axis:\n            xy_noise = sample(""xy_noise"", dist.Normal(loc, scale).expand_by([200, 320]))\n\n    """"""\n    def __init__(self, name=None, size=None, dim=None, device=None):\n        if not torch._C._get_tracing_state() and size == 0:\n            raise ZeroDivisionError(""size cannot be zero"")\n\n        super().__init__()\n        self._vectorized = None\n        if dim is not None:\n            self._vectorized = True\n\n        self._indices = None\n        self.name = name\n        self.dim = dim\n        self.size = size\n        self.device = device\n        self.counter = 0\n\n    def next_context(self):\n        """"""\n        Increments the counter.\n        """"""\n        self.counter += 1\n\n    def __enter__(self):\n        if self._vectorized is not False:\n            self._vectorized = True\n\n        if self._vectorized is True:\n            self.dim = _DIM_ALLOCATOR.allocate(self.name, self.dim)\n\n        return super().__enter__()\n\n    def __exit__(self, *args):\n        if self._vectorized is True:\n            _DIM_ALLOCATOR.free(self.name, self.dim)\n        return super().__exit__(*args)\n\n    def __iter__(self):\n        if self._vectorized is True or self.dim is not None:\n            raise ValueError(\n                ""cannot use plate {} as both vectorized and non-vectorized""\n                ""independence context"".format(self.name))\n\n        self._vectorized = False\n        self.dim = None\n        with ignore_jit_warnings([(""Iterating over a tensor"", RuntimeWarning)]):\n            for i in self.indices:\n                self.next_context()\n                with self:\n                    yield i if isinstance(i, numbers.Number) else i.item()\n\n    def _reset(self):\n        if self._vectorized:\n            _DIM_ALLOCATOR.free(self.name, self.dim)\n        self._vectorized = None\n        self.counter = 0\n\n    @property\n    def indices(self):\n        if self._indices is None:\n            self._indices = torch.arange(self.size, dtype=torch.long).to(self.device)\n        return self._indices\n\n    def _process_message(self, msg):\n        frame = CondIndepStackFrame(self.name, self.dim, self.size, self.counter)\n        msg[""cond_indep_stack""] = (frame,) + msg[""cond_indep_stack""]\n'"
pyro/poutine/infer_config_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .messenger import Messenger\n\n\nclass InferConfigMessenger(Messenger):\n    """"""\n    Given a callable `fn` that contains Pyro primitive calls\n    and a callable `config_fn` taking a trace site and returning a dictionary,\n    updates the value of the infer kwarg at a sample site to config_fn(site).\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param config_fn: a callable taking a site and returning an infer dict\n    :returns: stochastic function decorated with :class:`~pyro.poutine.infer_config_messenger.InferConfigMessenger`\n    """"""\n    def __init__(self, config_fn):\n        """"""\n        :param config_fn: a callable taking a site and returning an infer dict\n\n        Constructor. Doesn\'t do much, just stores the stochastic function\n        and the config_fn.\n        """"""\n        super().__init__()\n        self.config_fn = config_fn\n\n    def _pyro_sample(self, msg):\n        """"""\n        :param msg: current message at a trace site.\n\n        If self.config_fn is not None, calls self.config_fn on msg\n        and stores the result in msg[""infer""].\n\n        Otherwise, implements default sampling behavior\n        with no additional effects.\n        """"""\n        msg[""infer""].update(self.config_fn(msg))\n        return None\n\n    def _pyro_param(self, msg):\n        """"""\n        :param msg: current message at a trace site.\n\n        If self.config_fn is not None, calls self.config_fn on msg\n        and stores the result in msg[""infer""].\n\n        Otherwise, implements default param behavior\n        with no additional effects.\n        """"""\n        msg[""infer""].update(self.config_fn(msg))\n        return None\n'"
pyro/poutine/lift_messenger.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nfrom pyro import params\nfrom pyro.distributions.distribution import Distribution\nfrom pyro.poutine.util import is_validation_enabled\n\nfrom .messenger import Messenger\n\n\nclass LiftMessenger(Messenger):\n    """"""\n    Given a stochastic function with param calls and a prior distribution,\n    create a stochastic function where all param calls are replaced by sampling from prior.\n    Prior should be a callable or a dict of names to callables.\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(""s"", torch.tensor(0.5))\n        ...     z = pyro.sample(""z"", dist.Normal(x, s))\n        ...     return z ** 2\n        >>> lifted_model = pyro.poutine.lift(model, prior={""s"": dist.Exponential(0.3)})\n\n    ``lift`` makes ``param`` statements behave like ``sample`` statements\n    using the distributions in ``prior``.  In this example, site `s` will now behave\n    as if it was replaced with ``s = pyro.sample(""s"", dist.Exponential(0.3))``:\n\n        >>> tr = pyro.poutine.trace(lifted_model).get_trace(0.0)\n        >>> tr.nodes[""s""][""type""] == ""sample""\n        True\n        >>> tr2 = pyro.poutine.trace(lifted_model).get_trace(0.0)\n        >>> bool((tr2.nodes[""s""][""value""] == tr.nodes[""s""][""value""]).all())\n        False\n\n    :param fn: function whose parameters will be lifted to random values\n    :param prior: prior function in the form of a Distribution or a dict of stochastic fns\n    :returns: ``fn`` decorated with a :class:`~pyro.poutine.lift_messenger.LiftMessenger`\n    """"""\n\n    def __init__(self, prior):\n        """"""\n        :param prior: prior used to lift parameters. Prior can be of type\n                      dict, pyro.distributions, or a python stochastic fn\n\n        Constructor\n        """"""\n        super().__init__()\n        self.prior = prior\n        self._samples_cache = {}\n\n    def __enter__(self):\n        self._samples_cache = {}\n        if is_validation_enabled() and isinstance(self.prior, dict):\n            self._param_hits = set()\n            self._param_misses = set()\n        return super().__enter__()\n\n    def __exit__(self, *args, **kwargs):\n        self._samples_cache = {}\n        if is_validation_enabled() and isinstance(self.prior, dict):\n            extra = set(self.prior) - self._param_hits\n            if extra:\n                warnings.warn(\n                    ""pyro.module prior did not find params [\'{}\']. ""\n                    ""Did you instead mean one of [\'{}\']?""\n                    .format(""\', \'"".join(extra), ""\', \'"".join(self._param_misses)))\n        return super().__exit__(*args, **kwargs)\n\n    def _pyro_sample(self, msg):\n        return None\n\n    def _pyro_param(self, msg):\n        """"""\n        Overrides the `pyro.param` call with samples sampled from the\n        distribution specified in the prior. The prior can be a\n        pyro.distributions object or a dict of distributions keyed\n        on the param names. If the param name does not match the\n        name the keys in the prior, that param name is unchanged.\n        """"""\n        name = msg[""name""]\n        param_name = params.user_param_name(name)\n        if isinstance(self.prior, dict):\n            # prior is a dict of distributions\n            if param_name in self.prior.keys():\n                msg[""fn""] = self.prior[param_name]\n                msg[""args""] = msg[""args""][1:]\n                if isinstance(msg[\'fn\'], Distribution):\n                    msg[""args""] = ()\n                    msg[""kwargs""] = {}\n                    msg[""infer""] = {}\n                if is_validation_enabled():\n                    self._param_hits.add(param_name)\n            else:\n                if is_validation_enabled():\n                    self._param_misses.add(param_name)\n                return None\n        elif isinstance(self.prior, Distribution):\n            # prior is a distribution\n            msg[""fn""] = self.prior\n            msg[""args""] = ()\n            msg[""kwargs""] = {}\n            msg[""infer""] = {}\n        elif callable(self.prior):\n            if not isinstance(self.prior, Distribution):\n                # prior is a stochastic fn. block sample\n                msg[""stop""] = True\n            msg[""fn""] = self.prior\n            msg[""args""] = msg[""args""][1:]\n        else:\n            # otherwise leave as is\n            return None\n        msg[""type""] = ""sample""\n        if name in self._samples_cache:\n            # Multiple pyro.param statements with the same\n            # name. Block the site and fix the value.\n            msg[\'value\'] = self._samples_cache[name][\'value\']\n            msg[""is_observed""] = True\n            msg[""stop""] = True\n        else:\n            self._samples_cache[name] = msg\n            msg[""is_observed""] = False\n        return self._pyro_sample(msg)\n'"
pyro/poutine/markov_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import Counter\nfrom contextlib import ExitStack  # python 3\n\nfrom .reentrant_messenger import ReentrantMessenger\n\n\nclass MarkovMessenger(ReentrantMessenger):\n    """"""\n    Markov dependency declaration.\n\n    This is a statistical equivalent of a memory management arena.\n\n    :param int history: The number of previous contexts visible from the\n        current context. Defaults to 1. If zero, this is similar to\n        :class:`pyro.plate`.\n    :param bool keep: If true, frames are replayable. This is important\n        when branching: if ``keep=True``, neighboring branches at the same\n        level can depend on each other; if ``keep=False``, neighboring branches\n        are independent (conditioned on their shared ancestors).\n    :param int dim: An optional dimension to use for this independence index.\n        Interface stub, behavior not yet implemented.\n    :param str name: An optional unique name to help inference algorithms match\n        :func:`pyro.markov` sites between models and guides.\n        Interface stub, behavior not yet implemented.\n    """"""\n    def __init__(self, history=1, keep=False, dim=None, name=None):\n        assert history >= 0\n        self.history = history\n        self.keep = keep\n        self.dim = dim\n        self.name = name\n        if dim is not None:\n            raise NotImplementedError(\n                ""vectorized markov not yet implemented, try setting dim to None"")\n        if name is not None:\n            raise NotImplementedError(\n                ""vectorized markov not yet implemented, try setting name to None"")\n        self._iterable = None\n        self._pos = -1\n        self._stack = []\n        super().__init__()\n\n    def generator(self, iterable):\n        self._iterable = iterable\n        return self\n\n    def __iter__(self):\n        with ExitStack() as stack:\n            for value in self._iterable:\n                stack.enter_context(self)\n                yield value\n\n    def __enter__(self):\n        self._pos += 1\n        if len(self._stack) <= self._pos:\n            self._stack.append(set())\n        return super().__enter__()\n\n    def __exit__(self, *args, **kwargs):\n        if not self.keep:\n            self._stack.pop()\n        self._pos -= 1\n        return super().__exit__(*args, **kwargs)\n\n    def _pyro_sample(self, msg):\n        if msg[""done""] or type(msg[""fn""]).__name__ == ""_Subsample"":\n            return\n\n        # We use a Counter rather than a set here so that sites can correctly\n        # go out of scope when any one of their markov contexts exits.\n        # This accounting can be done by users of these fields,\n        # e.g. EnumMessenger.\n        infer = msg[""infer""]\n        scope = infer.setdefault(""_markov_scope"", Counter())  # site name -> markov depth\n        for pos in range(max(0, self._pos - self.history), self._pos + 1):\n            scope.update(self._stack[pos])\n        infer[""_markov_depth""] = 1 + infer.get(""_markov_depth"", 0)\n        self._stack[self._pos].add(msg[""name""])\n'"
pyro/poutine/mask_messenger.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.util import ignore_jit_warnings\n\nfrom .messenger import Messenger\n\n\nclass MaskMessenger(Messenger):\n    """"""\n    Given a stochastic function with some batched sample statements and\n    masking tensor, mask out some of the sample statements elementwise.\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param torch.BoolTensor mask: a ``{0,1}``-valued masking tensor\n        (1 includes a site, 0 excludes a site)\n    :returns: stochastic function decorated with a :class:`~pyro.poutine.scale_messenger.MaskMessenger`\n    """"""\n    def __init__(self, mask):\n        if isinstance(mask, torch.Tensor):\n            if mask.dtype != torch.bool:\n                raise ValueError(\'Expected mask to be a BoolTensor but got {}\'.format(type(mask)))\n        else:\n            if mask not in (True, False):\n                raise ValueError(\'Expected mask to be a boolean but got {}\'.format(type(mask)))\n            with ignore_jit_warnings():\n                mask = torch.tensor(mask)\n        super().__init__()\n        self.mask = mask\n\n    def _process_message(self, msg):\n        msg[""mask""] = self.mask if msg[""mask""] is None else self.mask & msg[""mask""]\n        return None\n'"
pyro/poutine/messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nfrom .runtime import _PYRO_STACK\n\n\ndef _context_wrap(context, fn, *args, **kwargs):\n    with context:\n        return fn(*args, **kwargs)\n\n\nclass _bound_partial(partial):\n    """"""\n    Converts a (possibly) bound method into a partial function to\n    support class methods as arguments to handlers.\n    """"""\n    def __get__(self, instance, owner):\n        if instance is None:\n            return self\n        return partial(self.func, instance)\n\n\nclass Messenger:\n    """"""\n    Context manager class that modifies behavior\n    and adds side effects to stochastic functions\n    i.e. callables containing Pyro primitive statements.\n\n    This is the base Messenger class.\n    It implements the default behavior for all Pyro primitives,\n    so that the joint distribution induced by a stochastic function fn\n    is identical to the joint distribution induced by ``Messenger()(fn)``.\n\n    Class of transformers for messages passed during inference.\n    Most inference operations are implemented in subclasses of this.\n    """"""\n\n    def __init__(self):\n        pass\n\n    def __call__(self, fn):\n        wraps = _bound_partial(partial(_context_wrap, self, fn))\n        return wraps\n\n    def __enter__(self):\n        """"""\n        :returns: self\n        :rtype: pyro.poutine.Messenger\n\n        Installs this messenger at the bottom of the Pyro stack.\n\n        Can be overloaded to add any additional per-call setup functionality,\n        but the derived class must always push itself onto the stack, usually\n        by calling super().__enter__().\n\n        Derived versions cannot be overridden to take arguments\n        and must always return self.\n        """"""\n        if not (self in _PYRO_STACK):\n            # if this poutine is not already installed,\n            # put it on the bottom of the stack.\n            _PYRO_STACK.append(self)\n\n            # necessary to return self because the return value of __enter__\n            # is bound to VAR in with EXPR as VAR.\n            return self\n        else:\n            # note: currently we raise an error if trying to install a poutine twice.\n            # However, this isn\'t strictly necessary,\n            # and blocks recursive poutine execution patterns like\n            # like calling self.__call__ inside of self.__call__\n            # or with Handler(...) as p: with p: <BLOCK>\n            # It\'s hard to imagine use cases for this pattern,\n            # but it could in principle be enabled...\n            raise ValueError(""cannot install a Messenger instance twice"")\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        """"""\n        :param exc_type: exception type, e.g. ValueError\n        :param exc_value: exception instance?\n        :param traceback: traceback for exception handling\n        :returns: None\n        :rtype: None\n\n        Removes this messenger from the bottom of the Pyro stack.\n        If an exception is raised, removes this messenger and everything below it.\n        Always called after every execution of self.fn via self.__call__.\n\n        Can be overloaded by derived classes to add any other per-call teardown functionality,\n        but the stack must always be popped by the derived class,\n        usually by calling super().__exit__(*args).\n\n        Derived versions cannot be overridden to take other arguments,\n        and must always return None or False.\n\n        The arguments are the mandatory arguments used by a with statement.\n        Users should never be specifying these.\n        They are all None unless the body of the with statement raised an exception.\n        """"""\n        if exc_type is None:  # callee or enclosed block returned successfully\n            # if the callee or enclosed block returned successfully,\n            # this poutine should be on the bottom of the stack.\n            # If so, remove it from the stack.\n            # if not, raise a ValueError because something really weird happened.\n            if _PYRO_STACK[-1] == self:\n                _PYRO_STACK.pop()\n            else:\n                # should never get here, but just in case...\n                raise ValueError(""This Messenger is not on the bottom of the stack"")\n        else:  # the wrapped function or block raised an exception\n            # poutine exception handling:\n            # when the callee or enclosed block raises an exception,\n            # find this poutine\'s position in the stack,\n            # then remove it and everything below it in the stack.\n            if self in _PYRO_STACK:\n                loc = _PYRO_STACK.index(self)\n                for i in range(loc, len(_PYRO_STACK)):\n                    _PYRO_STACK.pop()\n\n    def _reset(self):\n        pass\n\n    def _process_message(self, msg):\n        """"""\n        :param msg: current message at a trace site\n        :returns: None\n\n        Process the message by calling appropriate method of itself based\n        on message type. The message is updated in place.\n        """"""\n        method_name = ""_pyro_{}"".format(msg[""type""])\n        if hasattr(self, method_name):\n            return getattr(self, method_name)(msg)\n        return None\n\n    def _postprocess_message(self, msg):\n        method_name = ""_pyro_post_{}"".format(msg[""type""])\n        if hasattr(self, method_name):\n            return getattr(self, method_name)(msg)\n        return None\n\n    @classmethod\n    def register(cls, fn=None, type=None, post=None):\n        """"""\n        :param fn: function implementing operation\n        :param str type: name of the operation\n            (also passed to :func:`~pyro.poutine.runtime.effectful`)\n        :param bool post: if `True`, use this operation as postprocess\n\n        Dynamically add operations to an effect.\n        Useful for generating wrappers for libraries.\n\n        Example::\n\n            @SomeMessengerClass.register\n            def some_function(msg)\n                ...do_something...\n                return msg\n\n        """"""\n        if fn is None:\n            return lambda x: cls.register(x, type=type, post=post)\n\n        if type is None:\n            raise ValueError(""An operation type name must be provided"")\n\n        setattr(cls, ""_pyro_"" + (""post_"" if post else """") + type, staticmethod(fn))\n        return fn\n\n    @classmethod\n    def unregister(cls, fn=None, type=None):\n        """"""\n        :param fn: function implementing operation\n        :param str type: name of the operation\n            (also passed to :func:`~pyro.poutine.runtime.effectful`)\n\n        Dynamically remove operations from an effect.\n        Useful for removing wrappers from libraries.\n\n        Example::\n\n            SomeMessengerClass.unregister(some_function, ""name"")\n        """"""\n        if type is None:\n            raise ValueError(""An operation type name must be provided"")\n\n        try:\n            delattr(cls, ""_pyro_post_"" + type)\n        except AttributeError:\n            pass\n\n        try:\n            delattr(cls, ""_pyro_"" + type)\n        except AttributeError:\n            pass\n\n        return fn\n'"
pyro/poutine/plate_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .broadcast_messenger import BroadcastMessenger\nfrom .subsample_messenger import SubsampleMessenger\n\n\nclass PlateMessenger(SubsampleMessenger):\n    """"""\n    Swiss army knife of broadcasting amazingness:\n    combines shape inference, independence annotation, and subsampling\n    """"""\n    def _process_message(self, msg):\n        super()._process_message(msg)\n        return BroadcastMessenger._pyro_sample(msg)\n\n    def __enter__(self):\n        super().__enter__()\n        if self._vectorized and self._indices is not None:\n            return self.indices\n        return None\n'"
pyro/poutine/reentrant_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\n\nfrom .messenger import Messenger\n\n\nclass ReentrantMessenger(Messenger):\n    def __init__(self):\n        self._ref_count = 0\n        super().__init__()\n\n    def __call__(self, fn):\n        return functools.wraps(fn)(super().__call__(fn))\n\n    def __enter__(self):\n        self._ref_count += 1\n        if self._ref_count == 1:\n            super().__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._ref_count -= 1\n        if self._ref_count == 0:\n            super().__exit__(exc_type, exc_value, traceback)\n'"
pyro/poutine/reparam_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .messenger import Messenger\n\n\nclass ReparamMessenger(Messenger):\n    """"""\n    Reparametrizes each affected sample site into one or more auxiliary sample\n    sites followed by a deterministic transformation [1].\n\n    To specify reparameterizers, pass a ``config`` dict or callable to the\n    constructor.  See the :mod:`pyro.infer.reparam` module for available\n    reparameterizers.\n\n    Note some reparameterizers can examine the ``*args,**kwargs`` inputs of\n    functions they affect; these reparameterizers require using\n    ``poutine.reparam`` as a decorator rather than as a context manager.\n\n    [1] Maria I. Gorinova, Dave Moore, Matthew D. Hoffman (2019)\n        ""Automatic Reparameterisation of Probabilistic Programs""\n        https://arxiv.org/pdf/1906.03028.pdf\n\n    :param config: Configuration, either a dict mapping site name to\n        :class:`~pyro.infer.reparam.reparam.Reparameterizer` ,\n        or a function mapping site to\n        :class:`~pyro.infer.reparam.reparam.Reparameterizer` or None.\n    :type config: dict or callable\n    """"""\n    def __init__(self, config):\n        super().__init__()\n        assert isinstance(config, dict) or callable(config)\n        self.config = config\n        self._args_kwargs = None\n\n    def __call__(self, fn):\n        return ReparamHandler(self, fn)\n\n    def _pyro_sample(self, msg):\n        if isinstance(self.config, dict):\n            reparam = self.config.get(msg[""name""])\n        else:\n            reparam = self.config(msg)\n        if reparam is None:\n            return\n\n        reparam.args_kwargs = self._args_kwargs\n        try:\n            new_fn, value = reparam(msg[""name""], msg[""fn""], msg[""value""])\n        finally:\n            reparam.args_kwargs = None\n\n        if value is not None:\n            if msg[""value""] is None:\n                msg[""is_observed""] = True\n            msg[""value""] = value\n            if getattr(msg[""fn""], ""_validation_enabled"", False):\n                # Validate while the original msg[""fn""] is known.\n                msg[""fn""]._validate_sample(value)\n        msg[""fn""] = new_fn\n\n\nclass ReparamHandler(object):\n    """"""\n    Reparameterization poutine.\n    """"""\n    def __init__(self, msngr, fn):\n        self.msngr = msngr\n        self.fn = fn\n\n    def __call__(self, *args, **kwargs):\n        # This saves args,kwargs for optional use by reparameterizers.\n        self.msngr._args_kwargs = args, kwargs\n        try:\n            with self.msngr:\n                return self.fn(*args, **kwargs)\n        finally:\n            self.msngr._args_kwargs = None\n'"
pyro/poutine/replay_messenger.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .messenger import Messenger\n\n\nclass ReplayMessenger(Messenger):\n    """"""\n    Given a callable that contains Pyro primitive calls,\n    return a callable that runs the original, reusing the values at sites in trace\n    at those sites in the new trace\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(""s"", torch.tensor(0.5))\n        ...     z = pyro.sample(""z"", dist.Normal(x, s))\n        ...     return z ** 2\n\n    ``replay`` makes ``sample`` statements behave as if they had sampled the values\n    at the corresponding sites in the trace:\n\n        >>> old_trace = pyro.poutine.trace(model).get_trace(1.0)\n        >>> replayed_model = pyro.poutine.replay(model, trace=old_trace)\n        >>> bool(replayed_model(0.0) == old_trace.nodes[""_RETURN""][""value""])\n        True\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param trace: a :class:`~pyro.poutine.Trace` data structure to replay against\n    :param params: dict of names of param sites and constrained values\n        in fn to replay against\n    :returns: a stochastic function decorated with a :class:`~pyro.poutine.replay_messenger.ReplayMessenger`\n    """"""\n\n    def __init__(self, trace=None, params=None):\n        """"""\n        :param trace: a trace whose values should be reused\n\n        Constructor.\n        Stores trace in an attribute.\n        """"""\n        super().__init__()\n        if trace is None and params is None:\n            raise ValueError(""must provide trace or params to replay against"")\n        self.trace = trace\n        self.params = params\n\n    def _pyro_sample(self, msg):\n        """"""\n        :param msg: current message at a trace site.\n\n        At a sample site that appears in self.trace,\n        returns the value from self.trace instead of sampling\n        from the stochastic function at the site.\n\n        At a sample site that does not appear in self.trace,\n        reverts to default Messenger._pyro_sample behavior with no additional side effects.\n        """"""\n        name = msg[""name""]\n        if self.trace is not None and name in self.trace:\n            guide_msg = self.trace.nodes[name]\n            if msg[""is_observed""]:\n                return None\n            if guide_msg[""type""] != ""sample"" or \\\n                    guide_msg[""is_observed""]:\n                raise RuntimeError(""site {} must be sample in trace"".format(name))\n            msg[""done""] = True\n            msg[""value""] = guide_msg[""value""]\n            msg[""infer""] = guide_msg[""infer""]\n        return None\n\n    def _pyro_param(self, msg):\n        name = msg[""name""]\n        if self.params is not None and name in self.params:\n            assert hasattr(self.params[name], ""unconstrained""), \\\n                ""param {} must be constrained value"".format(name)\n            msg[""done""] = True\n            msg[""value""] = self.params[name]\n        return None\n'"
pyro/poutine/runtime.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\n\nfrom pyro.params.param_store import _MODULE_NAMESPACE_DIVIDER, ParamStoreDict  # noqa: F401\n\n# the global pyro stack\n_PYRO_STACK = []\n\n# the global ParamStore\n_PYRO_PARAM_STORE = ParamStoreDict()\n\n\nclass _DimAllocator:\n    """"""\n    Dimension allocator for internal use by :class:`plate`.\n    There is a single global instance.\n\n    Note that dimensions are indexed from the right, e.g. -1, -2.\n    """"""\n    def __init__(self):\n        self._stack = []  # in reverse orientation of log_prob.shape\n\n    def allocate(self, name, dim):\n        """"""\n        Allocate a dimension to an :class:`plate` with given name.\n        Dim should be either None for automatic allocation or a negative\n        integer for manual allocation.\n        """"""\n        if name in self._stack:\n            raise ValueError(\'duplicate plate ""{}""\'.format(name))\n        if dim is None:\n            # Automatically designate the rightmost available dim for allocation.\n            dim = -1\n            while -dim <= len(self._stack) and self._stack[-1 - dim] is not None:\n                dim -= 1\n        elif dim >= 0:\n            raise ValueError(\'Expected dim < 0 to index from the right, actual {}\'.format(dim))\n\n        # Allocate the requested dimension.\n        while dim < -len(self._stack):\n            self._stack.append(None)\n        if self._stack[-1 - dim] is not None:\n            raise ValueError(\'\\n\'.join([\n                \'at plates ""{}"" and ""{}"", collide at dim={}\'.format(name, self._stack[-1 - dim], dim),\n                \'\\nTry moving the dim of one plate to the left, e.g. dim={}\'.format(dim - 1)]))\n        self._stack[-1 - dim] = name\n        return dim\n\n    def free(self, name, dim):\n        """"""\n        Free a dimension.\n        """"""\n        free_idx = -1 - dim  # stack index to free\n        assert self._stack[free_idx] == name\n        self._stack[free_idx] = None\n        while self._stack and self._stack[-1] is None:\n            self._stack.pop()\n\n\n# Handles placement of plate dimensions\n_DIM_ALLOCATOR = _DimAllocator()\n\n\nclass _EnumAllocator:\n    """"""\n    Dimension allocator for internal use by :func:`~pyro.poutine.markov`.\n    There is a single global instance.\n\n    Note that dimensions are indexed from the right, e.g. -1, -2.\n    Note that ids are simply nonnegative integers here.\n    """"""\n    def set_first_available_dim(self, first_available_dim):\n        """"""\n        Set the first available dim, which should be to the left of all\n        :class:`plate` dimensions, e.g. ``-1 - max_plate_nesting``. This should\n        be called once per program. In SVI this should be called only once per\n        (guide,model) pair.\n        """"""\n        assert first_available_dim < 0, first_available_dim\n        self.next_available_dim = first_available_dim\n        self.next_available_id = 0\n        self.dim_to_id = {}  # only the global ids\n\n    def allocate(self, scope_dims=None):\n        """"""\n        Allocate a new recyclable dim and a unique id.\n\n        If ``scope_dims`` is None, this allocates a global enumeration dim\n        that will never be recycled. If ``scope_dims`` is specified, this\n        allocates a local enumeration dim that can be reused by at any other\n        local site whose scope excludes this site.\n\n        :param set scope_dims: An optional set of (negative integer)\n            local enumeration dims to avoid when allocating this dim.\n        :return: A pair ``(dim, id)``, where ``dim`` is a negative integer\n            and ``id`` is a nonnegative integer.\n        :rtype: tuple\n        """"""\n        id_ = self.next_available_id\n        self.next_available_id += 1\n\n        dim = self.next_available_dim\n        if dim == -float(\'inf\'):\n            raise ValueError(""max_plate_nesting must be set to a finite value for parallel enumeration"")\n        if scope_dims is None:\n            # allocate a new global dimension\n            self.next_available_dim -= 1\n            self.dim_to_id[dim] = id_\n        else:\n            # allocate a new local dimension\n            while dim in scope_dims:\n                dim -= 1\n\n        return dim, id_\n\n\n# Handles placement of enumeration dimensions\n_ENUM_ALLOCATOR = _EnumAllocator()\n\n\nclass NonlocalExit(Exception):\n    """"""\n    Exception for exiting nonlocally from poutine execution.\n\n    Used by poutine.EscapeMessenger to return site information.\n    """"""\n    def __init__(self, site, *args, **kwargs):\n        """"""\n        :param site: message at a pyro site constructor.\n            Just stores the input site.\n        """"""\n        super().__init__(*args, **kwargs)\n        self.site = site\n\n    def reset_stack(self):\n        """"""\n        Reset the state of the frames remaining in the stack.\n        Necessary for multiple re-executions in poutine.queue.\n        """"""\n        for frame in reversed(_PYRO_STACK):\n            frame._reset()\n            if type(frame).__name__ == ""BlockMessenger"" and frame.hide_fn(self.site):\n                break\n\n\ndef default_process_message(msg):\n    """"""\n    Default method for processing messages in inference.\n\n    :param msg: a message to be processed\n    :returns: None\n    """"""\n    if msg[""done""] or msg[""is_observed""] or msg[""value""] is not None:\n        msg[""done""] = True\n        return msg\n\n    msg[""value""] = msg[""fn""](*msg[""args""], **msg[""kwargs""])\n\n    # after fn has been called, update msg to prevent it from being called again.\n    msg[""done""] = True\n\n\ndef apply_stack(initial_msg):\n    """"""\n    Execute the effect stack at a single site according to the following scheme:\n\n        1. For each ``Messenger`` in the stack from bottom to top,\n           execute ``Messenger._process_message`` with the message;\n           if the message field ""stop"" is True, stop;\n           otherwise, continue\n        2. Apply default behavior (``default_process_message``) to finish remaining site execution\n        3. For each ``Messenger`` in the stack from top to bottom,\n           execute ``_postprocess_message`` to update the message and internal messenger state with the site results\n        4. If the message field ""continuation"" is not ``None``, call it with the message\n\n    :param dict initial_msg: the starting version of the trace site\n    :returns: ``None``\n    """"""\n    stack = _PYRO_STACK\n    # TODO check at runtime if stack is valid\n\n    # msg is used to pass information up and down the stack\n    msg = initial_msg\n\n    pointer = 0\n    # go until time to stop?\n    for frame in reversed(stack):\n\n        pointer = pointer + 1\n\n        frame._process_message(msg)\n\n        if msg[""stop""]:\n            break\n\n    default_process_message(msg)\n\n    for frame in stack[-pointer:]:\n        frame._postprocess_message(msg)\n\n    cont = msg[""continuation""]\n    if cont is not None:\n        cont(msg)\n\n    return None\n\n\ndef am_i_wrapped():\n    """"""\n    Checks whether the current computation is wrapped in a poutine.\n    :returns: bool\n    """"""\n    return len(_PYRO_STACK) > 0\n\n\ndef effectful(fn=None, type=None):\n    """"""\n    :param fn: function or callable that performs an effectful computation\n    :param str type: the type label of the operation, e.g. `""sample""`\n\n    Wrapper for calling :func:`~pyro.poutine.runtime.apply_stack` to apply any active effects.\n    """"""\n    if fn is None:\n        return functools.partial(effectful, type=type)\n\n    if getattr(fn, ""_is_effectful"", None):\n        return fn\n\n    assert type is not None, ""must provide a type label for operation {}"".format(fn)\n    assert type != ""message"", ""cannot use \'message\' as keyword""\n\n    @functools.wraps(fn)\n    def _fn(*args, **kwargs):\n\n        name = kwargs.pop(""name"", None)\n        infer = kwargs.pop(""infer"", {})\n\n        value = kwargs.pop(""obs"", None)\n        is_observed = value is not None\n\n        if not am_i_wrapped():\n            return fn(*args, **kwargs)\n        else:\n            msg = {\n                ""type"": type,\n                ""name"": name,\n                ""fn"": fn,\n                ""is_observed"": is_observed,\n                ""args"": args,\n                ""kwargs"": kwargs,\n                ""value"": value,\n                ""scale"": 1.0,\n                ""mask"": None,\n                ""cond_indep_stack"": (),\n                ""done"": False,\n                ""stop"": False,\n                ""continuation"": None,\n                ""infer"": infer,\n            }\n            # apply the stack and return its return value\n            apply_stack(msg)\n            return msg[""value""]\n    _fn._is_effectful = True\n    return _fn\n'"
pyro/poutine/scale_messenger.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.poutine.util import is_validation_enabled\n\nfrom .messenger import Messenger\n\n\nclass ScaleMessenger(Messenger):\n    """"""\n    Given a stochastic function with some sample statements and a positive\n    scale factor, scale the score of all sample and observe sites in the\n    function.\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(""s"", torch.tensor(0.5))\n        ...     z = pyro.sample(""z"", dist.Normal(x, s), obs=1.0)\n        ...     return z ** 2\n\n    ``scale`` multiplicatively scales the log-probabilities of sample sites:\n\n        >>> scaled_model = pyro.poutine.scale(model, scale=0.5)\n        >>> scaled_tr = pyro.poutine.trace(scaled_model).get_trace(0.0)\n        >>> unscaled_tr = pyro.poutine.trace(model).get_trace(0.0)\n        >>> bool((scaled_tr.log_prob_sum() == 0.5 * unscaled_tr.log_prob_sum()).all())\n        True\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param scale: a positive scaling factor\n    :returns: stochastic function decorated with a :class:`~pyro.poutine.scale_messenger.ScaleMessenger`\n    """"""\n    def __init__(self, scale):\n        if isinstance(scale, torch.Tensor):\n            if is_validation_enabled() and not (scale > 0).all():\n                raise ValueError(""Expected scale > 0 but got {}. "".format(scale) +\n                                 ""Consider using poutine.mask() instead of poutine.scale()."")\n        elif not (scale > 0):\n            raise ValueError(""Expected scale > 0 but got {}"".format(scale))\n        super().__init__()\n        self.scale = scale\n\n    def _process_message(self, msg):\n        msg[""scale""] = self.scale * msg[""scale""]\n        return None\n'"
pyro/poutine/seed_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.util import get_rng_state, set_rng_seed, set_rng_state\n\nfrom .messenger import Messenger\n\n\nclass SeedMessenger(Messenger):\n    """"""\n    Handler to set the random number generator to a pre-defined state by setting its\n    seed. This is the same as calling :func:`pyro.set_rng_seed` before the\n    call to `fn`. This handler has no additional effect on primitive statements on the\n    standard Pyro backend, but it might intercept ``pyro.sample`` calls in other\n    backends. e.g. the NumPy backend.\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls).\n    :param int rng_seed: rng seed.\n    """"""\n    def __init__(self, rng_seed):\n        assert isinstance(rng_seed, int)\n        self.rng_seed = rng_seed\n        super().__init__()\n\n    def __enter__(self):\n        self.old_state = get_rng_state()\n        set_rng_seed(self.rng_seed)\n\n    def __exit__(self, type, value, traceback):\n        set_rng_state(self.old_state)\n'"
pyro/poutine/subsample_messenger.py,11,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.distributions.distribution import Distribution\nfrom pyro.poutine.util import is_validation_enabled\nfrom pyro.util import ignore_jit_warnings\n\nfrom .indep_messenger import CondIndepStackFrame, IndepMessenger\nfrom .runtime import apply_stack\n\n\nclass _Subsample(Distribution):\n    """"""\n    Randomly select a subsample of a range of indices.\n\n    Internal use only. This should only be used by `plate`.\n    """"""\n\n    def __init__(self, size, subsample_size, use_cuda=None, device=None):\n        """"""\n        :param int size: the size of the range to subsample from\n        :param int subsample_size: the size of the returned subsample\n        :param bool use_cuda: DEPRECATED, use the `device` arg instead.\n            Whether to use cuda tensors.\n        :param str device: device to place the `sample` and `log_prob`\n            results on.\n        """"""\n        self.size = size\n        self.subsample_size = subsample_size\n        self.use_cuda = use_cuda\n        if self.use_cuda is not None:\n            if self.use_cuda ^ (device != ""cpu""):\n                raise ValueError(""Incompatible arg values use_cuda={}, device={}.""\n                                 .format(use_cuda, device))\n        with ignore_jit_warnings([""torch.Tensor results are registered as constants""]):\n            self.device = torch.Tensor().device if not device else device\n\n    @ignore_jit_warnings([""Converting a tensor to a Python boolean""])\n    def sample(self, sample_shape=torch.Size()):\n        """"""\n        :returns: a random subsample of `range(size)`\n        :rtype: torch.LongTensor\n        """"""\n        if sample_shape:\n            raise NotImplementedError\n        subsample_size = self.subsample_size\n        if subsample_size is None or subsample_size >= self.size:\n            result = torch.arange(self.size, device=self.device)\n        else:\n            result = torch.randperm(self.size, device=self.device)[:subsample_size].clone()\n        return result.cuda() if self.use_cuda else result\n\n    def log_prob(self, x):\n        # This is zero so that plate can provide an unbiased estimate of\n        # the non-subsampled log_prob.\n        result = torch.tensor(0., device=self.device)\n        return result.cuda() if self.use_cuda else result\n\n\nclass SubsampleMessenger(IndepMessenger):\n    """"""\n    Extension of IndepMessenger that includes subsampling.\n    """"""\n\n    def __init__(self, name, size=None, subsample_size=None, subsample=None, dim=None,\n                 use_cuda=None, device=None):\n        super().__init__(name, size, dim, device)\n        self.subsample_size = subsample_size\n        self._indices = subsample\n        self.use_cuda = use_cuda\n        self.device = device\n\n        self.size, self.subsample_size, self._indices = self._subsample(\n            self.name, self.size, self.subsample_size,\n            self._indices, self.use_cuda, self.device)\n\n    @staticmethod\n    def _subsample(name, size=None, subsample_size=None, subsample=None, use_cuda=None, device=None):\n        """"""\n        Helper function for plate. See its docstrings for details.\n        """"""\n        if size is None:\n            assert subsample_size is None\n            assert subsample is None\n            size = -1  # This is PyTorch convention for ""arbitrary size""\n            subsample_size = -1\n        else:\n            msg = {\n                ""type"": ""sample"",\n                ""name"": name,\n                ""fn"": _Subsample(size, subsample_size, use_cuda, device),\n                ""is_observed"": False,\n                ""args"": (),\n                ""kwargs"": {},\n                ""value"": subsample,\n                ""infer"": {},\n                ""scale"": 1.0,\n                ""mask"": None,\n                ""cond_indep_stack"": (),\n                ""done"": False,\n                ""stop"": False,\n                ""continuation"": None\n            }\n            apply_stack(msg)\n            subsample = msg[""value""]\n\n        with ignore_jit_warnings():\n            if subsample_size is None:\n                subsample_size = subsample.size(0) if isinstance(subsample, torch.Tensor) \\\n                    else len(subsample)\n            elif subsample is not None and subsample_size != len(subsample):\n                raise ValueError(""subsample_size does not match len(subsample), {} vs {}."".format(\n                    subsample_size, len(subsample)) +\n                    "" Did you accidentally use different subsample_size in the model and guide?"")\n\n        return size, subsample_size, subsample\n\n    def _reset(self):\n        self._indices = None\n        super()._reset()\n\n    def _process_message(self, msg):\n        frame = CondIndepStackFrame(self.name, self.dim, self.subsample_size, self.counter)\n        frame.full_size = self.size  # Used for param initialization.\n        msg[""cond_indep_stack""] = (frame,) + msg[""cond_indep_stack""]\n        if isinstance(self.size, torch.Tensor) or isinstance(self.subsample_size, torch.Tensor):\n            if not isinstance(msg[""scale""], torch.Tensor):\n                with ignore_jit_warnings():\n                    msg[""scale""] = torch.tensor(msg[""scale""])\n        msg[""scale""] = msg[""scale""] * self.size / self.subsample_size\n\n    def _postprocess_message(self, msg):\n        if msg[""type""] in (""param"", ""subsample"") and self.dim is not None:\n            event_dim = msg[""kwargs""].get(""event_dim"")\n            if event_dim is not None:\n                assert event_dim >= 0\n                dim = self.dim - event_dim\n                shape = msg[""value""].shape\n                if len(shape) >= -dim and shape[dim] != 1:\n                    if is_validation_enabled() and shape[dim] != self.size:\n                        if msg[""type""] == ""param"":\n                            statement = ""pyro.param({}, ..., event_dim={})"".format(msg[""name""], event_dim)\n                        else:\n                            statement = ""pyro.subsample(..., event_dim={})"".format(event_dim)\n                        raise ValueError(\n                            ""Inside pyro.plate({}, {}, dim={}) invalid shape of {}: {}""\n                            .format(self.name, self.size, self.dim, statement, shape))\n                    # Subsample parameters with known batch semantics.\n                    if self.subsample_size < self.size:\n                        value = msg[""value""]\n                        new_value = value.index_select(dim, self._indices)\n                        if msg[""type""] == ""param"":\n                            if hasattr(value, ""_pyro_unconstrained_param""):\n                                param = value._pyro_unconstrained_param\n                            else:\n                                param = value.unconstrained()\n\n                            if not hasattr(param, ""_pyro_subsample""):\n                                param._pyro_subsample = {}\n\n                            param._pyro_subsample[dim] = self._indices\n                            new_value._pyro_unconstrained_param = param\n                        msg[""value""] = new_value\n'"
pyro/poutine/trace_messenger.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport sys\n\nfrom .messenger import Messenger\nfrom .trace_struct import Trace\nfrom .util import site_is_subsample\n\n\ndef identify_dense_edges(trace):\n    """"""\n    Modifies a trace in-place by adding all edges based on the\n    `cond_indep_stack` information stored at each site.\n    """"""\n    for name, node in trace.nodes.items():\n        if site_is_subsample(node):\n            continue\n        if node[""type""] == ""sample"":\n            for past_name, past_node in trace.nodes.items():\n                if site_is_subsample(node):\n                    continue\n                if past_node[""type""] == ""sample"":\n                    if past_name == name:\n                        break\n                    past_node_independent = False\n                    for query, target in zip(node[""cond_indep_stack""], past_node[""cond_indep_stack""]):\n                        if query.name == target.name and query.counter != target.counter:\n                            past_node_independent = True\n                            break\n                    if not past_node_independent:\n                        trace.add_edge(past_name, name)\n\n\nclass TraceMessenger(Messenger):\n    """"""\n    Return a handler that records the inputs and outputs of primitive calls\n    and their dependencies.\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(""s"", torch.tensor(0.5))\n        ...     z = pyro.sample(""z"", dist.Normal(x, s))\n        ...     return z ** 2\n\n    We can record its execution using ``trace``\n    and use the resulting data structure to compute the log-joint probability\n    of all of the sample sites in the execution or extract all parameters.\n\n        >>> trace = pyro.poutine.trace(model).get_trace(0.0)\n        >>> logp = trace.log_prob_sum()\n        >>> params = [trace.nodes[name][""value""].unconstrained() for name in trace.param_nodes]\n\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param graph_type: string that specifies the kind of graph to construct\n    :param param_only: if true, only records params and not samples\n    :returns: stochastic function decorated with a :class:`~pyro.poutine.trace_messenger.TraceMessenger`\n    """"""\n\n    def __init__(self, graph_type=None, param_only=None):\n        """"""\n        :param string graph_type: string that specifies the type of graph\n            to construct (currently only ""flat"" or ""dense"" supported)\n        :param param_only: boolean that specifies whether to record sample sites\n        """"""\n        super().__init__()\n        if graph_type is None:\n            graph_type = ""flat""\n        if param_only is None:\n            param_only = False\n        assert graph_type in (""flat"", ""dense"")\n        self.graph_type = graph_type\n        self.param_only = param_only\n        self.trace = Trace(graph_type=self.graph_type)\n\n    def __enter__(self):\n        self.trace = Trace(graph_type=self.graph_type)\n        return super().__enter__()\n\n    def __exit__(self, *args, **kwargs):\n        """"""\n        Adds appropriate edges based on cond_indep_stack information\n        upon exiting the context.\n        """"""\n        if self.param_only:\n            for node in list(self.trace.nodes.values()):\n                if node[""type""] != ""param"":\n                    self.trace.remove_node(node[""name""])\n        if self.graph_type == ""dense"":\n            identify_dense_edges(self.trace)\n        return super().__exit__(*args, **kwargs)\n\n    def __call__(self, fn):\n        """"""\n        TODO docs\n        """"""\n        return TraceHandler(self, fn)\n\n    def get_trace(self):\n        """"""\n        :returns: data structure\n        :rtype: pyro.poutine.Trace\n\n        Helper method for a very common use case.\n        Returns a shallow copy of ``self.trace``.\n        """"""\n        return self.trace.copy()\n\n    def _reset(self):\n        tr = Trace(graph_type=self.graph_type)\n        if ""_INPUT"" in self.trace.nodes:\n            tr.add_node(""_INPUT"",\n                        name=""_INPUT"", type=""input"",\n                        args=self.trace.nodes[""_INPUT""][""args""],\n                        kwargs=self.trace.nodes[""_INPUT""][""kwargs""])\n        self.trace = tr\n        super()._reset()\n\n    def _pyro_post_sample(self, msg):\n        if self.param_only:\n            return\n        if msg[""infer""].get(""_do_not_trace""):\n            assert msg[""infer""].get(""is_auxiliary"")\n            assert not msg[""is_observed""]\n            return\n        self.trace.add_node(msg[""name""], **msg.copy())\n\n    def _pyro_post_param(self, msg):\n        self.trace.add_node(msg[""name""], **msg.copy())\n\n\nclass TraceHandler:\n    """"""\n    Execution trace poutine.\n\n    A TraceHandler records the input and output to every Pyro primitive\n    and stores them as a site in a Trace().\n    This should, in theory, be sufficient information for every inference algorithm\n    (along with the implicit computational graph in the Variables?)\n\n    We can also use this for visualization.\n    """"""\n    def __init__(self, msngr, fn):\n        self.fn = fn\n        self.msngr = msngr\n\n    def __call__(self, *args, **kwargs):\n        """"""\n        Runs the stochastic function stored in this poutine,\n        with additional side effects.\n\n        Resets self.trace to an empty trace,\n        installs itself on the global execution stack,\n        runs self.fn with the given arguments,\n        uninstalls itself from the global execution stack,\n        stores the arguments and return value of the function in special sites,\n        and returns self.fn\'s return value\n        """"""\n        with self.msngr:\n            self.msngr.trace.add_node(""_INPUT"",\n                                      name=""_INPUT"", type=""args"",\n                                      args=args, kwargs=kwargs)\n            try:\n                ret = self.fn(*args, **kwargs)\n            except (ValueError, RuntimeError):\n                exc_type, exc_value, traceback = sys.exc_info()\n                shapes = self.msngr.trace.format_shapes()\n                exc = exc_type(u""{}\\n{}"".format(exc_value, shapes))\n                exc = exc.with_traceback(traceback)\n                raise exc from None\n            self.msngr.trace.add_node(""_RETURN"", name=""_RETURN"", type=""return"", value=ret)\n        return ret\n\n    @property\n    def trace(self):\n        return self.msngr.trace\n\n    def get_trace(self, *args, **kwargs):\n        """"""\n        :returns: data structure\n        :rtype: pyro.poutine.Trace\n\n        Helper method for a very common use case.\n        Calls this poutine and returns its trace instead of the function\'s return value.\n        """"""\n        self(*args, **kwargs)\n        return self.msngr.get_trace()\n'"
pyro/poutine/trace_struct.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\nimport sys\n\nimport opt_einsum\n\nfrom pyro.distributions.score_parts import ScoreParts\nfrom pyro.distributions.util import scale_and_mask\nfrom pyro.ops.packed import pack\nfrom pyro.poutine.util import is_validation_enabled\nfrom pyro.util import warn_if_inf, warn_if_nan\n\n\nclass Trace:\n    """"""\n    Graph data structure denoting the relationships amongst different pyro primitives\n    in the execution trace.\n\n    An execution trace of a Pyro program is a record of every call\n    to ``pyro.sample()`` and ``pyro.param()`` in a single execution of that program.\n    Traces are directed graphs whose nodes represent primitive calls or input/output,\n    and whose edges represent conditional dependence relationships\n    between those primitive calls. They are created and populated by ``poutine.trace``.\n\n    Each node (or site) in a trace contains the name, input and output value of the site,\n    as well as additional metadata added by inference algorithms or user annotation.\n    In the case of ``pyro.sample``, the trace also includes the stochastic function\n    at the site, and any observed data added by users.\n\n    Consider the following Pyro program:\n\n        >>> def model(x):\n        ...     s = pyro.param(""s"", torch.tensor(0.5))\n        ...     z = pyro.sample(""z"", dist.Normal(x, s))\n        ...     return z ** 2\n\n    We can record its execution using ``pyro.poutine.trace``\n    and use the resulting data structure to compute the log-joint probability\n    of all of the sample sites in the execution or extract all parameters.\n\n        >>> trace = pyro.poutine.trace(model).get_trace(0.0)\n        >>> logp = trace.log_prob_sum()\n        >>> params = [trace.nodes[name][""value""].unconstrained() for name in trace.param_nodes]\n\n    We can also inspect or manipulate individual nodes in the trace.\n    ``trace.nodes`` contains a ``collections.OrderedDict``\n    of site names and metadata corresponding to ``x``, ``s``, ``z``, and the return value:\n\n        >>> list(name for name in trace.nodes.keys())  # doctest: +SKIP\n        [""_INPUT"", ""s"", ""z"", ""_RETURN""]\n\n    Values of ``trace.nodes`` are dictionaries of node metadata:\n\n        >>> trace.nodes[""z""]  # doctest: +SKIP\n        {\'type\': \'sample\', \'name\': \'z\', \'is_observed\': False,\n         \'fn\': Normal(), \'value\': tensor(0.6480), \'args\': (), \'kwargs\': {},\n         \'infer\': {}, \'scale\': 1.0, \'cond_indep_stack\': (),\n         \'done\': True, \'stop\': False, \'continuation\': None}\n\n    ``\'infer\'`` is a dictionary of user- or algorithm-specified metadata.\n    ``\'args\'`` and ``\'kwargs\'`` are the arguments passed via ``pyro.sample``\n    to ``fn.__call__`` or ``fn.log_prob``.\n    ``\'scale\'`` is used to scale the log-probability of the site when computing the log-joint.\n    ``\'cond_indep_stack\'`` contains data structures corresponding to ``pyro.plate`` contexts\n    appearing in the execution.\n    ``\'done\'``, ``\'stop\'``, and ``\'continuation\'`` are only used by Pyro\'s internals.\n\n    :param string graph_type: string specifying the kind of trace graph to construct\n    """"""\n\n    def __init__(self, graph_type=""flat""):\n        assert graph_type in (""flat"", ""dense""), \\\n            ""{} not a valid graph type"".format(graph_type)\n        self.graph_type = graph_type\n        self.nodes = OrderedDict()\n        self._succ = OrderedDict()\n        self._pred = OrderedDict()\n\n    def __contains__(self, name):\n        return name in self.nodes\n\n    def __iter__(self):\n        return iter(self.nodes.keys())\n\n    def __len__(self):\n        return len(self.nodes)\n\n    @property\n    def edges(self):\n        for site, adj_nodes in self._succ.items():\n            for adj_node in adj_nodes:\n                yield site, adj_node\n\n    def add_node(self, site_name, **kwargs):\n        """"""\n        :param string site_name: the name of the site to be added\n\n        Adds a site to the trace.\n\n        Raises an error when attempting to add a duplicate node\n        instead of silently overwriting.\n        """"""\n        if site_name in self:\n            site = self.nodes[site_name]\n            if site[\'type\'] != kwargs[\'type\']:\n                # Cannot sample or observe after a param statement.\n                raise RuntimeError(""{} is already in the trace as a {}"".format(site_name, site[\'type\']))\n            elif kwargs[\'type\'] != ""param"":\n                # Cannot sample after a previous sample statement.\n                raise RuntimeError(""Multiple {} sites named \'{}\'"".format(kwargs[\'type\'], site_name))\n\n        # XXX should copy in case site gets mutated, or dont bother?\n        self.nodes[site_name] = kwargs\n        self._pred[site_name] = set()\n        self._succ[site_name] = set()\n\n    def add_edge(self, site1, site2):\n        for site in (site1, site2):\n            if site not in self.nodes:\n                self.add_node(site)\n        self._succ[site1].add(site2)\n        self._pred[site2].add(site1)\n\n    def remove_node(self, site_name):\n        self.nodes.pop(site_name)\n        for p in self._pred[site_name]:\n            self._succ[p].remove(site_name)\n        for s in self._succ[site_name]:\n            self._pred[s].remove(site_name)\n        self._pred.pop(site_name)\n        self._succ.pop(site_name)\n\n    def predecessors(self, site_name):\n        return self._pred[site_name]\n\n    def successors(self, site_name):\n        return self._succ[site_name]\n\n    def copy(self):\n        """"""\n        Makes a shallow copy of self with nodes and edges preserved.\n        """"""\n        new_tr = Trace(graph_type=self.graph_type)\n        new_tr.nodes.update(self.nodes)\n        new_tr._succ.update(self._succ)\n        new_tr._pred.update(self._pred)\n        return new_tr\n\n    def _dfs(self, site, visited):\n        if site in visited:\n            return\n        for s in self._succ[site]:\n            for node in self._dfs(s, visited):\n                yield node\n        visited.add(site)\n        yield site\n\n    def topological_sort(self, reverse=False):\n        """"""\n        Return a list of nodes (site names) in topologically sorted order.\n\n        :param bool reverse: Return the list in reverse order.\n        :return: list of topologically sorted nodes (site names).\n        """"""\n        visited = set()\n        top_sorted = []\n        for s in self._succ:\n            for node in self._dfs(s, visited):\n                top_sorted.append(node)\n        return top_sorted if reverse else list(reversed(top_sorted))\n\n    def log_prob_sum(self, site_filter=lambda name, site: True):\n        """"""\n        Compute the site-wise log probabilities of the trace.\n        Each ``log_prob`` has shape equal to the corresponding ``batch_shape``.\n        Each ``log_prob_sum`` is a scalar.\n        The computation of ``log_prob_sum`` is memoized.\n\n        :returns: total log probability.\n        :rtype: torch.Tensor\n        """"""\n        result = 0.0\n        for name, site in self.nodes.items():\n            if site[""type""] == ""sample"" and site_filter(name, site):\n                if ""log_prob_sum"" in site:\n                    log_p = site[""log_prob_sum""]\n                else:\n                    try:\n                        log_p = site[""fn""].log_prob(site[""value""], *site[""args""], **site[""kwargs""])\n                    except ValueError:\n                        _, exc_value, traceback = sys.exc_info()\n                        shapes = self.format_shapes(last_site=site[""name""])\n                        raise ValueError(""Error while computing log_prob_sum at site \'{}\':\\n{}\\n{}\\n""\n                                         .format(name, exc_value, shapes)).with_traceback(traceback)\n                    log_p = scale_and_mask(log_p, site[""scale""], site[""mask""]).sum()\n                    site[""log_prob_sum""] = log_p\n                    if is_validation_enabled():\n                        warn_if_nan(log_p, ""log_prob_sum at site \'{}\'"".format(name))\n                        warn_if_inf(log_p, ""log_prob_sum at site \'{}\'"".format(name), allow_neginf=True)\n                result = result + log_p\n        return result\n\n    def compute_log_prob(self, site_filter=lambda name, site: True):\n        """"""\n        Compute the site-wise log probabilities of the trace.\n        Each ``log_prob`` has shape equal to the corresponding ``batch_shape``.\n        Each ``log_prob_sum`` is a scalar.\n        Both computations are memoized.\n        """"""\n        for name, site in self.nodes.items():\n            if site[""type""] == ""sample"" and site_filter(name, site):\n                if ""log_prob"" not in site:\n                    try:\n                        log_p = site[""fn""].log_prob(site[""value""], *site[""args""], **site[""kwargs""])\n                    except ValueError:\n                        _, exc_value, traceback = sys.exc_info()\n                        shapes = self.format_shapes(last_site=site[""name""])\n                        raise ValueError(""Error while computing log_prob at site \'{}\':\\n{}\\n{}""\n                                         .format(name, exc_value, shapes)).with_traceback(traceback)\n                    site[""unscaled_log_prob""] = log_p\n                    log_p = scale_and_mask(log_p, site[""scale""], site[""mask""])\n                    site[""log_prob""] = log_p\n                    site[""log_prob_sum""] = log_p.sum()\n                    if is_validation_enabled():\n                        warn_if_nan(site[""log_prob_sum""], ""log_prob_sum at site \'{}\'"".format(name))\n                        warn_if_inf(site[""log_prob_sum""], ""log_prob_sum at site \'{}\'"".format(name),\n                                    allow_neginf=True)\n\n    def compute_score_parts(self):\n        """"""\n        Compute the batched local score parts at each site of the trace.\n        Each ``log_prob`` has shape equal to the corresponding ``batch_shape``.\n        Each ``log_prob_sum`` is a scalar.\n        All computations are memoized.\n        """"""\n        for name, site in self.nodes.items():\n            if site[""type""] == ""sample"" and ""score_parts"" not in site:\n                # Note that ScoreParts overloads the multiplication operator\n                # to correctly scale each of its three parts.\n                try:\n                    value = site[""fn""].score_parts(site[""value""], *site[""args""], **site[""kwargs""])\n                except ValueError:\n                    _, exc_value, traceback = sys.exc_info()\n                    shapes = self.format_shapes(last_site=site[""name""])\n                    raise ValueError(""Error while computing score_parts at site \'{}\':\\n{}\\n{}""\n                                     .format(name, exc_value, shapes)).with_traceback(traceback)\n                site[""unscaled_log_prob""] = value.log_prob\n                value = value.scale_and_mask(site[""scale""], site[""mask""])\n                site[""score_parts""] = value\n                site[""log_prob""] = value.log_prob\n                site[""log_prob_sum""] = value.log_prob.sum()\n                if is_validation_enabled():\n                    warn_if_nan(site[""log_prob_sum""], ""log_prob_sum at site \'{}\'"".format(name))\n                    warn_if_inf(site[""log_prob_sum""], ""log_prob_sum at site \'{}\'"".format(name), allow_neginf=True)\n\n    def detach_(self):\n        """"""\n        Detach values (in-place) at each sample site of the trace.\n        """"""\n        for _, site in self.nodes.items():\n            if site[""type""] == ""sample"":\n                site[""value""] = site[""value""].detach()\n\n    @property\n    def observation_nodes(self):\n        """"""\n        :return: a list of names of observe sites\n        """"""\n        return [name for name, node in self.nodes.items()\n                if node[""type""] == ""sample"" and\n                node[""is_observed""]]\n\n    @property\n    def param_nodes(self):\n        """"""\n        :return: a list of names of param sites\n        """"""\n        return [name for name, node in self.nodes.items()\n                if node[""type""] == ""param""]\n\n    @property\n    def stochastic_nodes(self):\n        """"""\n        :return: a list of names of sample sites\n        """"""\n        return [name for name, node in self.nodes.items()\n                if node[""type""] == ""sample"" and\n                not node[""is_observed""]]\n\n    @property\n    def reparameterized_nodes(self):\n        """"""\n        :return: a list of names of sample sites whose stochastic functions\n            are reparameterizable primitive distributions\n        """"""\n        return [name for name, node in self.nodes.items()\n                if node[""type""] == ""sample"" and\n                not node[""is_observed""] and\n                getattr(node[""fn""], ""has_rsample"", False)]\n\n    @property\n    def nonreparam_stochastic_nodes(self):\n        """"""\n        :return: a list of names of sample sites whose stochastic functions\n            are not reparameterizable primitive distributions\n        """"""\n        return list(set(self.stochastic_nodes) - set(self.reparameterized_nodes))\n\n    def iter_stochastic_nodes(self):\n        """"""\n        :return: an iterator over stochastic nodes in the trace.\n        """"""\n        for name, node in self.nodes.items():\n            if node[""type""] == ""sample"" and not node[""is_observed""]:\n                yield name, node\n\n    def symbolize_dims(self, plate_to_symbol=None):\n        """"""\n        Assign unique symbols to all tensor dimensions.\n        """"""\n        plate_to_symbol = {} if plate_to_symbol is None else plate_to_symbol\n        symbol_to_dim = {}\n        for site in self.nodes.values():\n            if site[""type""] != ""sample"":\n                continue\n\n            # allocate even symbols for plate dims\n            dim_to_symbol = {}\n            for frame in site[""cond_indep_stack""]:\n                if frame.vectorized:\n                    if frame.name in plate_to_symbol:\n                        symbol = plate_to_symbol[frame.name]\n                    else:\n                        symbol = opt_einsum.get_symbol(2 * len(plate_to_symbol))\n                        plate_to_symbol[frame.name] = symbol\n                    symbol_to_dim[symbol] = frame.dim\n                    dim_to_symbol[frame.dim] = symbol\n\n            # allocate odd symbols for enum dims\n            for dim, id_ in site[""infer""].get(""_dim_to_id"", {}).items():\n                symbol = opt_einsum.get_symbol(1 + 2 * id_)\n                symbol_to_dim[symbol] = dim\n                dim_to_symbol[dim] = symbol\n            enum_dim = site[""infer""].get(""_enumerate_dim"")\n            if enum_dim is not None:\n                site[""infer""][""_enumerate_symbol""] = dim_to_symbol[enum_dim]\n            site[""infer""][""_dim_to_symbol""] = dim_to_symbol\n\n        self.plate_to_symbol = plate_to_symbol\n        self.symbol_to_dim = symbol_to_dim\n\n    def pack_tensors(self, plate_to_symbol=None):\n        """"""\n        Computes packed representations of tensors in the trace.\n        This should be called after :meth:`compute_log_prob` or :meth:`compute_score_parts`.\n        """"""\n        self.symbolize_dims(plate_to_symbol)\n        for site in self.nodes.values():\n            if site[""type""] != ""sample"":\n                continue\n            dim_to_symbol = site[""infer""][""_dim_to_symbol""]\n            packed = site.setdefault(""packed"", {})\n            try:\n                packed[""mask""] = pack(site[""mask""], dim_to_symbol)\n                if ""score_parts"" in site:\n                    log_prob, score_function, entropy_term = site[""score_parts""]\n                    log_prob = pack(log_prob, dim_to_symbol)\n                    score_function = pack(score_function, dim_to_symbol)\n                    entropy_term = pack(entropy_term, dim_to_symbol)\n                    packed[""score_parts""] = ScoreParts(log_prob, score_function, entropy_term)\n                    packed[""log_prob""] = log_prob\n                    packed[""unscaled_log_prob""] = pack(site[""unscaled_log_prob""], dim_to_symbol)\n                elif ""log_prob"" in site:\n                    packed[""log_prob""] = pack(site[""log_prob""], dim_to_symbol)\n                    packed[""unscaled_log_prob""] = pack(site[""unscaled_log_prob""], dim_to_symbol)\n            except ValueError:\n                _, exc_value, traceback = sys.exc_info()\n                shapes = self.format_shapes(last_site=site[""name""])\n                raise ValueError(""Error while packing tensors at site \'{}\':\\n  {}\\n{}""\n                                 .format(site[""name""], exc_value, shapes)).with_traceback(traceback)\n\n    def format_shapes(self, title=\'Trace Shapes:\', last_site=None):\n        """"""\n        Returns a string showing a table of the shapes of all sites in the\n        trace.\n        """"""\n        if not self.nodes:\n            return title\n        rows = [[title]]\n\n        rows.append([\'Param Sites:\'])\n        for name, site in self.nodes.items():\n            if site[""type""] == ""param"":\n                rows.append([name, None] + [str(size) for size in site[""value""].shape])\n            if name == last_site:\n                break\n\n        rows.append([\'Sample Sites:\'])\n        for name, site in self.nodes.items():\n            if site[""type""] == ""sample"":\n                # param shape\n                batch_shape = getattr(site[""fn""], ""batch_shape"", ())\n                event_shape = getattr(site[""fn""], ""event_shape"", ())\n                rows.append([name + "" dist"", None] + [str(size) for size in batch_shape] +\n                            [""|"", None] + [str(size) for size in event_shape])\n\n                # value shape\n                event_dim = len(event_shape)\n                shape = getattr(site[""value""], ""shape"", ())\n                batch_shape = shape[:len(shape) - event_dim]\n                event_shape = shape[len(shape) - event_dim:]\n                rows.append([""value"", None] + [str(size) for size in batch_shape] +\n                            [""|"", None] + [str(size) for size in event_shape])\n\n                # log_prob shape\n                if ""log_prob"" in site:\n                    batch_shape = getattr(site[""log_prob""], ""shape"", ())\n                    rows.append([""log_prob"", None] + [str(size) for size in batch_shape] + [""|"", None])\n            if name == last_site:\n                break\n\n        return _format_table(rows)\n\n\ndef _format_table(rows):\n    """"""\n    Formats a right justified table using None as column separator.\n    """"""\n    # compute column widths\n    column_widths = [0, 0, 0]\n    for row in rows:\n        widths = [0, 0, 0]\n        j = 0\n        for cell in row:\n            if cell is None:\n                j += 1\n            else:\n                widths[j] += 1\n        for j in range(3):\n            column_widths[j] = max(column_widths[j], widths[j])\n\n    # justify columns\n    for i, row in enumerate(rows):\n        cols = [[], [], []]\n        j = 0\n        for cell in row:\n            if cell is None:\n                j += 1\n            else:\n                cols[j].append(cell)\n        cols = [[""""] * (width - len(col)) + col\n                if direction == \'r\' else\n                col + [""""] * (width - len(col))\n                for width, col, direction in zip(column_widths, cols, \'rrl\')]\n        rows[i] = sum(cols, [])\n\n    # compute cell widths\n    cell_widths = [0] * len(rows[0])\n    for row in rows:\n        for j, cell in enumerate(row):\n            cell_widths[j] = max(cell_widths[j], len(cell))\n\n    # justify cells\n    return ""\\n"".join("" "".join(cell.rjust(width)\n                              for cell, width in zip(row, cell_widths))\n                     for row in rows)\n'"
pyro/poutine/uncondition_messenger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .messenger import Messenger\n\n\nclass UnconditionMessenger(Messenger):\n    """"""\n    Messenger to force the value of observed nodes to be sampled from their\n    distribution, ignoring observations.\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def _pyro_sample(self, msg):\n        """"""\n        :param msg: current message at a trace site.\n\n        Samples value from distribution, irrespective of whether or not the\n        node has an observed value.\n        """"""\n        if msg[""is_observed""]:\n            msg[""is_observed""] = False\n            msg[""infer""][""was_observed""] = True\n            msg[""infer""][""obs""] = msg[""value""]\n            msg[""value""] = None\n            msg[""done""] = False\n        return None\n'"
pyro/poutine/util.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n_VALIDATION_ENABLED = False\n\n\ndef enable_validation(is_validate):\n    global _VALIDATION_ENABLED\n    _VALIDATION_ENABLED = is_validate\n\n\ndef is_validation_enabled():\n    return _VALIDATION_ENABLED\n\n\ndef site_is_subsample(site):\n    """"""\n    Determines whether a trace site originated from a subsample statement inside an `plate`.\n    """"""\n    return site[""type""] == ""sample"" and type(site[""fn""]).__name__ == ""_Subsample""\n\n\ndef prune_subsample_sites(trace):\n    """"""\n    Copies and removes all subsample sites from a trace.\n    """"""\n    trace = trace.copy()\n    for name, site in list(trace.nodes.items()):\n        if site_is_subsample(site):\n            trace.remove_node(name)\n    return trace\n\n\ndef enum_extend(trace, msg, num_samples=None):\n    """"""\n    :param trace: a partial trace\n    :param msg: the message at a Pyro primitive site\n    :param num_samples: maximum number of extended traces to return.\n    :returns: a list of traces, copies of input trace with one extra site\n\n    Utility function to copy and extend a trace with sites based on the input site\n    whose values are enumerated from the support of the input site\'s distribution.\n\n    Used for exact inference and integrating out discrete variables.\n    """"""\n    if num_samples is None:\n        num_samples = -1\n\n    extended_traces = []\n    for i, s in enumerate(msg[""fn""].enumerate_support(*msg[""args""], **msg[""kwargs""])):\n        if i > num_samples and num_samples >= 0:\n            break\n        msg_copy = msg.copy()\n        msg_copy.update(value=s)\n        tr_cp = trace.copy()\n        tr_cp.add_node(msg[""name""], **msg_copy)\n        extended_traces.append(tr_cp)\n    return extended_traces\n\n\ndef mc_extend(trace, msg, num_samples=None):\n    """"""\n    :param trace: a partial trace\n    :param msg: the message at a Pyro primitive site\n    :param num_samples: maximum number of extended traces to return.\n    :returns: a list of traces, copies of input trace with one extra site\n\n    Utility function to copy and extend a trace with sites based on the input site\n    whose values are sampled from the input site\'s function.\n\n    Used for Monte Carlo marginalization of individual sample sites.\n    """"""\n    if num_samples is None:\n        num_samples = 1\n\n    extended_traces = []\n    for i in range(num_samples):\n        msg_copy = msg.copy()\n        msg_copy[""value""] = msg_copy[""fn""](*msg_copy[""args""], **msg_copy[""kwargs""])\n        tr_cp = trace.copy()\n        tr_cp.add_node(msg_copy[""name""], **msg_copy)\n        extended_traces.append(tr_cp)\n    return extended_traces\n\n\ndef discrete_escape(trace, msg):\n    """"""\n    :param trace: a partial trace\n    :param msg: the message at a Pyro primitive site\n    :returns: boolean decision value\n\n    Utility function that checks if a sample site is discrete and not already in a trace.\n\n    Used by EscapeMessenger to decide whether to do a nonlocal exit at a site.\n    Subroutine for integrating out discrete variables for variance reduction.\n    """"""\n    return (msg[""type""] == ""sample"") and \\\n        (not msg[""is_observed""]) and \\\n        (msg[""name""] not in trace) and \\\n        (getattr(msg[""fn""], ""has_enumerate_support"", False))\n\n\ndef all_escape(trace, msg):\n    """"""\n    :param trace: a partial trace\n    :param msg: the message at a Pyro primitive site\n    :returns: boolean decision value\n\n    Utility function that checks if a site is not already in a trace.\n\n    Used by EscapeMessenger to decide whether to do a nonlocal exit at a site.\n    Subroutine for approximately integrating out variables for variance reduction.\n    """"""\n    return (msg[""type""] == ""sample"") and \\\n        (not msg[""is_observed""]) and \\\n        (msg[""name""] not in trace)\n'"
tests/contrib/__init__.py,0,b''
tests/contrib/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/contrib""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""integration_batch_1""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/contrib/test_hessian.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.ops.hessian import hessian\nfrom tests.common import assert_equal\n\n\ndef test_hessian_mvn():\n    tmp = torch.randn(3, 10)\n    cov = torch.matmul(tmp, tmp.t())\n    mvn = dist.MultivariateNormal(cov.new_zeros(3), cov)\n\n    x = torch.randn(3, requires_grad=True)\n    y = mvn.log_prob(x)\n    assert_equal(hessian(y, x), -mvn.precision_matrix)\n\n\ndef test_hessian_multi_variables():\n    x = torch.randn(3, requires_grad=True)\n    z = torch.randn(3, requires_grad=True)\n    y = (x ** 2 * z + z ** 3).sum()\n\n    H = hessian(y, (x, z))\n    Hxx = (2 * z).diag()\n    Hxz = (2 * x).diag()\n    Hzz = (6 * z).diag()\n    target_H = torch.cat([torch.cat([Hxx, Hxz]), torch.cat([Hxz, Hzz])], dim=1)\n    assert_equal(H, target_H)\n'"
tests/contrib/test_minipyro.py,24,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nimport pytest\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.generic import distributions as dist\nfrom pyro.generic import infer, ops, optim, pyro, pyro_backend\nfrom tests.common import assert_close, xfail_param\n\n# This file tests a variety of model,guide pairs with valid and invalid structure.\n# See https://github.com/pyro-ppl/pyro/blob/0.3.1/tests/infer/test_valid_models.py\n\n\ndef assert_ok(model, guide, elbo, *args, **kwargs):\n    """"""\n    Assert that inference works without warnings or errors.\n    """"""\n    pyro.get_param_store().clear()\n    adam = optim.Adam({""lr"": 1e-6})\n    inference = infer.SVI(model, guide, adam, elbo)\n    for i in range(2):\n        inference.step(*args, **kwargs)\n\n\ndef assert_error(model, guide, elbo, match=None):\n    """"""\n    Assert that inference fails with an error.\n    """"""\n    pyro.get_param_store().clear()\n    adam = optim.Adam({""lr"": 1e-6})\n    inference = infer.SVI(model,  guide, adam, elbo)\n    with pytest.raises((NotImplementedError, UserWarning, KeyError, ValueError, RuntimeError),\n                       match=match):\n        inference.step()\n\n\ndef assert_warning(model, guide, elbo):\n    """"""\n    Assert that inference works but with a warning.\n    """"""\n    pyro.get_param_store().clear()\n    adam = optim.Adam({""lr"": 1e-6})\n    inference = infer.SVI(model, guide, adam, elbo)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        inference.step()\n        assert len(w), \'No warnings were raised\'\n        for warning in w:\n            print(warning)\n\n\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro""])\ndef test_generate_data(backend):\n\n    def model(data=None):\n        loc = pyro.param(""loc"", torch.tensor(2.0))\n        scale = pyro.param(""scale"", torch.tensor(1.0))\n        x = pyro.sample(""x"", dist.Normal(loc, scale), obs=data)\n        return x\n\n    with pyro_backend(backend):\n        data = model().data\n        assert data.shape == ()\n\n\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro""])\ndef test_generate_data_plate(backend):\n    num_points = 1000\n\n    def model(data=None):\n        loc = pyro.param(""loc"", torch.tensor(2.0))\n        scale = pyro.param(""scale"", torch.tensor(1.0))\n        with pyro.plate(""data"", 1000, dim=-1):\n            x = pyro.sample(""x"", dist.Normal(loc, scale), obs=data)\n        return x\n\n    with pyro_backend(backend):\n        data = model().data\n        assert data.shape == (num_points,)\n        mean = float(ops.sum(data)) / num_points\n        assert 1.9 <= mean <= 2.1\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro""])\ndef test_nonempty_model_empty_guide_ok(backend, jit):\n\n    def model(data):\n        loc = pyro.param(""loc"", torch.tensor(0.0))\n        pyro.sample(""x"", dist.Normal(loc, 1.), obs=data)\n\n    def guide(data):\n        pass\n\n    data = torch.tensor(2.)\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo, data)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro""])\ndef test_plate_ok(backend, jit):\n    data = torch.randn(10)\n\n    def model():\n        locs = pyro.param(""locs"", torch.tensor([0.2, 0.3, 0.5]))\n        p = torch.tensor([0.2, 0.3, 0.5])\n        with pyro.plate(""plate"", len(data), dim=-1):\n            x = pyro.sample(""x"", dist.Categorical(p))\n            pyro.sample(""obs"", dist.Normal(locs[x], 1.), obs=data)\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor([0.5, 0.3, 0.2]))\n        with pyro.plate(""plate"", len(data), dim=-1):\n            pyro.sample(""x"", dist.Categorical(p))\n\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro""])\ndef test_nested_plate_plate_ok(backend, jit):\n    data = torch.randn(2, 3)\n\n    def model():\n        loc = torch.tensor(3.0)\n        with pyro.plate(""plate_outer"", data.size(-1), dim=-1):\n            x = pyro.sample(""x"", dist.Normal(loc, 1.))\n            with pyro.plate(""plate_inner"", data.size(-2), dim=-2):\n                pyro.sample(""y"", dist.Normal(x, 1.), obs=data)\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        scale = pyro.param(""scale"", torch.tensor(1.))\n        with pyro.plate(""plate_outer"", data.size(-1), dim=-1):\n            pyro.sample(""x"", dist.Normal(loc, scale))\n\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [\n    ""pyro"",\n    xfail_param(""minipyro"", reason=""not implemented""),\n])\ndef test_local_param_ok(backend, jit):\n    data = torch.randn(10)\n\n    def model():\n        locs = pyro.param(""locs"", torch.tensor([-1., 0., 1.]))\n        with pyro.plate(""plate"", len(data), dim=-1):\n            x = pyro.sample(""x"", dist.Categorical(torch.ones(3) / 3))\n            pyro.sample(""obs"", dist.Normal(locs[x], 1.), obs=data)\n\n    def guide():\n        with pyro.plate(""plate"", len(data), dim=-1):\n            p = pyro.param(""p"", torch.ones(len(data), 3) / 3, event_dim=1)\n            pyro.sample(""x"", dist.Categorical(p))\n        return p\n\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo)\n\n        # Check that pyro.param() can be called without init_value.\n        expected = guide()\n        actual = pyro.param(""p"")\n        assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro""])\ndef test_constraints(backend, jit):\n    data = torch.tensor(0.5)\n\n    def model():\n        locs = pyro.param(""locs"", torch.randn(3), constraint=constraints.real)\n        scales = pyro.param(""scales"", ops.exp(torch.randn(3)), constraint=constraints.positive)\n        p = torch.tensor([0.5, 0.3, 0.2])\n        x = pyro.sample(""x"", dist.Categorical(p))\n        pyro.sample(""obs"", dist.Normal(locs[x], scales[x]), obs=data)\n\n    def guide():\n        q = pyro.param(""q"", ops.exp(torch.randn(3)), constraint=constraints.simplex)\n        pyro.sample(""x"", dist.Categorical(q))\n\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo)\n'"
tests/contrib/test_util.py,22,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\nimport pytest\nimport torch\n\nfrom pyro.contrib.util import (\n    get_indices, tensor_to_dict, rmv, rvv, lexpand, rexpand, rdiag, rtril\n)\nfrom tests.common import assert_equal\n\n\ndef test_get_indices_sizes():\n    sizes = OrderedDict([(""a"", 2), (""b"", 2), (""c"", 2)])\n    assert_equal(get_indices([""b""], sizes=sizes), torch.tensor([2, 3]))\n    assert_equal(get_indices([""b"", ""c""], sizes=sizes), torch.tensor([2, 3, 4, 5]))\n    tensors = OrderedDict([(""a"", torch.ones(2)), (""b"", torch.ones(2)), (""c"", torch.ones(2))])\n    assert_equal(get_indices([""b""], tensors=tensors), torch.tensor([2, 3]))\n    assert_equal(get_indices([""b"", ""c""], tensors=tensors), torch.tensor([2, 3, 4, 5]))\n\n\ndef test_tensor_to_dict():\n    sizes = OrderedDict([(""a"", 2), (""b"", 2), (""c"", 2)])\n    vector = torch.tensor([1., 2, 3, 4, 5, 6])\n    assert_equal(tensor_to_dict(sizes, vector), {""a"": torch.tensor([1., 2.]),\n                                                 ""b"": torch.tensor([3., 4.]),\n                                                 ""c"": torch.tensor([5., 6.])})\n    assert_equal(tensor_to_dict(sizes, vector, subset=[""b""]),\n                 {""b"": torch.tensor([3., 4.])})\n\n\n@pytest.mark.parametrize(""A,b"", [\n    (torch.tensor([[1., 2.], [2., -3.]]), torch.tensor([-1., 2.]))\n    ])\ndef test_rmv(A, b):\n    assert_equal(rmv(A, b), A.mv(b), prec=1e-8)\n    batched_A = lexpand(A, 5, 4)\n    batched_b = lexpand(b, 5, 4)\n    expected_Ab = lexpand(A.mv(b), 5, 4)\n    assert_equal(rmv(batched_A, batched_b), expected_Ab, prec=1e-8)\n\n\n@pytest.mark.parametrize(""a,b"", [\n    (torch.tensor([1., 2.]), torch.tensor([-1., 2.]))\n    ])\ndef test_rvv(a, b):\n    assert_equal(rvv(a, b), torch.dot(a, b), prec=1e-8)\n    batched_a = lexpand(a, 5, 4)\n    batched_b = lexpand(b, 5, 4)\n    expected_ab = lexpand(torch.dot(a, b), 5, 4)\n    assert_equal(rvv(batched_a, batched_b), expected_ab, prec=1e-8)\n\n\ndef test_lexpand():\n    A = torch.tensor([[1., 2.], [-2., 0]])\n    assert_equal(lexpand(A), A, prec=1e-8)\n    assert_equal(lexpand(A, 4), A.expand(4, 2, 2), prec=1e-8)\n    assert_equal(lexpand(A, 4, 2), A.expand(4, 2, 2, 2), prec=1e-8)\n\n\ndef test_rexpand():\n    A = torch.tensor([[1., 2.], [-2., 0]])\n    assert_equal(rexpand(A), A, prec=1e-8)\n    assert_equal(rexpand(A, 4), A.unsqueeze(-1).expand(2, 2, 4), prec=1e-8)\n    assert_equal(rexpand(A, 4, 2), A.unsqueeze(-1).unsqueeze(-1).expand(2, 2, 4, 2), prec=1e-8)\n\n\ndef test_rtril():\n    A = torch.tensor([[1., 2.], [-2., 0]])\n    assert_equal(rtril(A), torch.tril(A), prec=1e-8)\n    expanded = lexpand(A, 5, 4)\n    expected = lexpand(torch.tril(A), 5, 4)\n    assert_equal(rtril(expanded), expected, prec=1e-8)\n\n\ndef test_rdiag():\n    v = torch.tensor([1., 2., -1.])\n    assert_equal(rdiag(v), torch.diag(v), prec=1e-8)\n    expanded = lexpand(v, 5, 4)\n    expeceted = lexpand(torch.diag(v), 5, 4)\n    assert_equal(rdiag(expanded), expeceted, prec=1e-8)\n'"
tests/distributions/__init__.py,0,b''
tests/distributions/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport numpy as np\nimport pytest\nimport scipy.stats as sp\n\nimport pyro.distributions as dist\nfrom pyro.distributions.testing.naive_dirichlet import NaiveBeta, NaiveDirichlet\nfrom pyro.distributions.testing.rejection_exponential import RejectionExponential\nfrom pyro.distributions.testing.rejection_gamma import ShapeAugmentedBeta, ShapeAugmentedDirichlet, ShapeAugmentedGamma\nfrom tests.distributions.dist_fixture import Fixture\n\n\nclass FoldedNormal(dist.FoldedDistribution):\n    def __init__(self, loc, scale):\n        super().__init__(dist.Normal(loc, scale))\n\n\ncontinuous_dists = [\n    Fixture(pyro_dist=dist.Uniform,\n            scipy_dist=sp.uniform,\n            examples=[\n                {\'low\': [2.], \'high\': [2.5],\n                 \'test_data\': [2.2]},\n                {\'low\': [2., 4.], \'high\': [3., 5.],\n                 \'test_data\': [[[2.5, 4.5]], [[2.5, 4.5]], [[2.5, 4.5]]]},\n                {\'low\': [[2.], [-3.], [0.]],\n                 \'high\': [[2.5], [0.], [1.]],\n                 \'test_data\': [[2.2], [-2], [0.7]]},\n            ],\n            scipy_arg_fn=lambda low, high: ((), {""loc"": np.array(low),\n                                                 ""scale"": np.array(high) - np.array(low)})),\n    Fixture(pyro_dist=dist.Exponential,\n            scipy_dist=sp.expon,\n            examples=[\n                {\'rate\': [2.4],\n                 \'test_data\': [5.5]},\n                {\'rate\': [2.4, 5.5],\n                 \'test_data\': [[[5.5, 3.2]], [[5.5, 3.2]], [[5.5, 3.2]]]},\n                {\'rate\': [[2.4, 5.5]],\n                 \'test_data\': [[[5.5, 3.2]], [[5.5, 3.2]], [[5.5, 3.2]]]},\n                {\'rate\': [[2.4], [5.5]],\n                 \'test_data\': [[5.5], [3.2]]},\n            ],\n            scipy_arg_fn=lambda rate: ((), {""scale"": 1.0 / np.array(rate)})),\n    Fixture(pyro_dist=RejectionExponential,\n            scipy_dist=sp.expon,\n            examples=[\n                {\'rate\': [2.4], \'factor\': [0.5],\n                 \'test_data\': [5.5]},\n                {\'rate\': [2.4, 5.5], \'factor\': [0.5],\n                 \'test_data\': [[[5.5, 3.2]], [[5.5, 3.2]], [[5.5, 3.2]]]},\n                {\'rate\': [[2.4, 5.5]], \'factor\': [0.5],\n                 \'test_data\': [[[5.5, 3.2]], [[5.5, 3.2]], [[5.5, 3.2]]]},\n                {\'rate\': [[2.4], [5.5]], \'factor\': [0.5],\n                 \'test_data\': [[5.5], [3.2]]},\n            ],\n            scipy_arg_fn=lambda rate, factor: ((), {""scale"": 1.0 / np.array(rate)})),\n    Fixture(pyro_dist=dist.Gamma,\n            scipy_dist=sp.gamma,\n            examples=[\n                {\'concentration\': [2.4], \'rate\': [3.2],\n                 \'test_data\': [5.5]},\n                {\'concentration\': [[2.4, 2.4], [3.2, 3.2]], \'rate\': [[2.4, 2.4], [3.2, 3.2]],\n                 \'test_data\': [[[5.5, 4.4], [5.5, 4.4]]]},\n                {\'concentration\': [[2.4], [2.4]], \'rate\': [[3.2], [3.2]], \'test_data\': [[5.5], [4.4]]}\n            ],\n            scipy_arg_fn=lambda concentration, rate: ((np.array(concentration),),\n                                                      {""scale"": 1.0 / np.array(rate)})),\n    Fixture(pyro_dist=ShapeAugmentedGamma,\n            scipy_dist=sp.gamma,\n            examples=[\n                {\'concentration\': [2.4], \'rate\': [3.2],\n                 \'test_data\': [5.5]},\n                {\'concentration\': [[2.4, 2.4], [3.2, 3.2]], \'rate\': [[2.4, 2.4], [3.2, 3.2]],\n                 \'test_data\': [[[5.5, 4.4], [5.5, 4.4]]]},\n                {\'concentration\': [[2.4], [2.4]], \'rate\': [[3.2], [3.2]], \'test_data\': [[5.5], [4.4]]}\n            ],\n            scipy_arg_fn=lambda concentration, rate: ((np.array(concentration),),\n                                                      {""scale"": 1.0 / np.array(rate)})),\n    Fixture(pyro_dist=dist.Beta,\n            scipy_dist=sp.beta,\n            examples=[\n                {\'concentration1\': [2.4], \'concentration0\': [3.6],\n                 \'test_data\': [0.4]},\n                {\'concentration1\': [[2.4, 2.4], [3.6, 3.6]], \'concentration0\': [[2.5, 2.5], [2.5, 2.5]],\n                 \'test_data\': [[[0.5, 0.4], [0.5, 0.4]]]},\n                {\'concentration1\': [[2.4], [3.7]], \'concentration0\': [[3.6], [2.5]],\n                 \'test_data\': [[0.4], [0.6]]}\n            ],\n            scipy_arg_fn=lambda concentration1, concentration0:\n            ((np.array(concentration1), np.array(concentration0)), {})),\n\n    Fixture(pyro_dist=NaiveBeta,\n            scipy_dist=sp.beta,\n            examples=[\n                {\'concentration1\': [2.4], \'concentration0\': [3.6],\n                 \'test_data\': [0.4]},\n                {\'concentration1\': [[2.4, 2.4], [3.6, 3.6]], \'concentration0\': [[2.5, 2.5], [2.5, 2.5]],\n                 \'test_data\': [[[0.5, 0.4], [0.5, 0.4]]]},\n                {\'concentration1\': [[2.4], [3.7]], \'concentration0\': [[3.6], [2.5]],\n                 \'test_data\': [[0.4], [0.6]]}\n            ],\n            scipy_arg_fn=lambda concentration1, concentration0:\n            ((np.array(concentration1), np.array(concentration0)), {})),\n    Fixture(pyro_dist=ShapeAugmentedBeta,\n            scipy_dist=sp.beta,\n            examples=[\n                {\'concentration1\': [2.4], \'concentration0\': [3.6],\n                 \'test_data\': [0.4]},\n                {\'concentration1\': [[2.4, 2.4], [3.6, 3.6]], \'concentration0\': [[2.5, 2.5], [2.5, 2.5]],\n                 \'test_data\': [[[0.5, 0.4], [0.5, 0.4]]]},\n                {\'concentration1\': [[2.4], [3.7]], \'concentration0\': [[3.6], [2.5]],\n                 \'test_data\': [[0.4], [0.6]]}\n            ],\n            scipy_arg_fn=lambda concentration1, concentration0:\n            ((np.array(concentration1), np.array(concentration0)), {})),\n    Fixture(pyro_dist=dist.LogNormal,\n            scipy_dist=sp.lognorm,\n            examples=[\n                {\'loc\': [1.4], \'scale\': [0.4],\n                 \'test_data\': [5.5]},\n                {\'loc\': [1.4], \'scale\': [0.4],\n                 \'test_data\': [[5.5]]},\n                {\'loc\': [[1.4, 0.4], [1.4, 0.4]], \'scale\': [[2.6, 0.5], [2.6, 0.5]],\n                 \'test_data\': [[5.5, 6.4], [5.5, 6.4]]},\n                {\'loc\': [[1.4], [0.4]], \'scale\': [[2.6], [0.5]],\n                 \'test_data\': [[5.5], [6.4]]}\n            ],\n            scipy_arg_fn=lambda loc, scale: ((np.array(scale),), {""scale"": np.exp(np.array(loc))})),\n    Fixture(pyro_dist=dist.Normal,\n            scipy_dist=sp.norm,\n            examples=[\n                {\'loc\': [2.0], \'scale\': [4.0],\n                 \'test_data\': [2.0]},\n                {\'loc\': [[2.0]], \'scale\': [[4.0]],\n                 \'test_data\': [[2.0]]},\n                {\'loc\': [[[2.0]]], \'scale\': [[[4.0]]],\n                 \'test_data\': [[[2.0]]]},\n                {\'loc\': [2.0, 50.0], \'scale\': [4.0, 100.0],\n                 \'test_data\': [[2.0, 50.0], [2.0, 50.0]]},\n            ],\n            scipy_arg_fn=lambda loc, scale: ((), {""loc"": np.array(loc), ""scale"": np.array(scale)}),\n            prec=0.07,\n            min_samples=50000),\n    Fixture(pyro_dist=dist.MultivariateNormal,\n            scipy_dist=sp.multivariate_normal,\n            examples=[\n                {\'loc\': [2.0, 1.0], \'covariance_matrix\': [[1.0, 0.5], [0.5, 1.0]],\n                 \'test_data\': [[2.0, 1.0], [9.0, 3.4]]},\n            ],\n            # This hack seems to be the best option right now, as \'scale\' is not handled well by get_scipy_batch_logpdf\n            scipy_arg_fn=lambda loc, covariance_matrix=None:\n                ((), {""mean"": np.array(loc), ""cov"": np.array([[1.0, 0.5], [0.5, 1.0]])}),\n            prec=0.01,\n            min_samples=500000),\n    Fixture(pyro_dist=dist.LowRankMultivariateNormal,\n            scipy_dist=sp.multivariate_normal,\n            examples=[\n                {\'loc\': [2.0, 1.0], \'cov_diag\': [0.5, 0.5], \'cov_factor\': [[1.0], [0.5]],\n                 \'test_data\': [[2.0, 1.0], [9.0, 3.4]]},\n            ],\n            scipy_arg_fn=lambda loc, cov_diag=None, cov_factor=None:\n                ((), {""mean"": np.array(loc), ""cov"": np.array([[1.5, 0.5], [0.5, 0.75]])}),\n            prec=0.01,\n            min_samples=500000),\n    Fixture(pyro_dist=FoldedNormal,\n            examples=[\n                {\'loc\': [2.0], \'scale\': [4.0],\n                 \'test_data\': [2.0]},\n                {\'loc\': [[2.0]], \'scale\': [[4.0]],\n                 \'test_data\': [[2.0]]},\n                {\'loc\': [[[2.0]]], \'scale\': [[[4.0]]],\n                 \'test_data\': [[[2.0]]]},\n                {\'loc\': [2.0, 50.0], \'scale\': [4.0, 100.0],\n                 \'test_data\': [[2.0, 50.0], [2.0, 50.0]]},\n            ]),\n    Fixture(pyro_dist=dist.Dirichlet,\n            scipy_dist=sp.dirichlet,\n            examples=[\n                {\'concentration\': [2.4, 3, 6],\n                 \'test_data\': [0.2, 0.45, 0.35]},\n                {\'concentration\': [2.4, 3, 6],\n                 \'test_data\': [[0.2, 0.45, 0.35], [0.2, 0.45, 0.35]]},\n                {\'concentration\': [[2.4, 3, 6], [3.2, 1.2, 0.4]],\n                 \'test_data\': [[0.2, 0.45, 0.35], [0.3, 0.4, 0.3]]}\n            ],\n            scipy_arg_fn=lambda concentration: ((concentration,), {})),\n    Fixture(pyro_dist=NaiveDirichlet,\n            scipy_dist=sp.dirichlet,\n            examples=[\n                {\'concentration\': [2.4, 3, 6],\n                 \'test_data\': [0.2, 0.45, 0.35]},\n                {\'concentration\': [2.4, 3, 6],\n                 \'test_data\': [[0.2, 0.45, 0.35], [0.2, 0.45, 0.35]]},\n                {\'concentration\': [[2.4, 3, 6], [3.2, 1.2, 0.4]],\n                 \'test_data\': [[0.2, 0.45, 0.35], [0.3, 0.4, 0.3]]}\n            ],\n            scipy_arg_fn=lambda concentration: ((concentration,), {})),\n    Fixture(pyro_dist=ShapeAugmentedDirichlet,\n            scipy_dist=sp.dirichlet,\n            examples=[\n                {\'concentration\': [2.4, 3, 6],\n                 \'test_data\': [0.2, 0.45, 0.35]},\n                {\'concentration\': [2.4, 3, 6],\n                 \'test_data\': [[0.2, 0.45, 0.35], [0.2, 0.45, 0.35]]},\n                {\'concentration\': [[2.4, 3, 6], [3.2, 1.2, 0.4]],\n                 \'test_data\': [[0.2, 0.45, 0.35], [0.3, 0.4, 0.3]]}\n            ],\n            scipy_arg_fn=lambda concentration: ((concentration,), {})),\n    Fixture(pyro_dist=dist.Cauchy,\n            scipy_dist=sp.cauchy,\n            examples=[\n                {\'loc\': [0.5], \'scale\': [1.2],\n                 \'test_data\': [1.0]},\n                {\'loc\': [0.5, 0.5], \'scale\': [1.2, 1.2],\n                 \'test_data\': [[1.0, 1.0], [1.0, 1.0]]},\n                {\'loc\': [[0.5], [0.3]], \'scale\': [[1.2], [1.0]],\n                 \'test_data\': [[0.4], [0.35]]}\n            ],\n            scipy_arg_fn=lambda loc, scale: ((), {""loc"": np.array(loc), ""scale"": np.array(scale)})),\n    Fixture(pyro_dist=dist.HalfCauchy,\n            scipy_dist=sp.halfcauchy,\n            examples=[\n                {\'scale\': [1.2],\n                 \'test_data\': [1.0]},\n                {\'scale\': [1.2, 1.2],\n                 \'test_data\': [[1.0, -1.0], [1.0, -1.0]]},\n                {\'scale\': [[1.2], [1.0]],\n                 \'test_data\': [[0.54], [0.35]]}\n            ],\n            scipy_arg_fn=lambda scale: ((), {""scale"": np.array(scale)})),\n    Fixture(pyro_dist=dist.VonMises,\n            scipy_dist=sp.vonmises,\n            examples=[\n                {\'loc\': [0.5], \'concentration\': [1.2],\n                 \'test_data\': [1.0]},\n                {\'loc\': [0.5, 3.0], \'concentration\': [2.0, 0.5],\n                 \'test_data\': [[1.0, 2.0], [1.0, 2.0]]},\n                {\'loc\': [[0.5], [0.3]], \'concentration\': [[2.0], [0.5]],\n                 \'test_data\': [[1.0], [2.0]]}\n            ],\n            scipy_arg_fn=lambda loc, concentration: ((), {""loc"": np.array(loc), ""kappa"": np.array(concentration)})),\n    Fixture(pyro_dist=dist.LKJCorrCholesky,\n            examples=[\n                {\'d\': 3, \'eta\': [1.], \'test_data\':\n                    [[[1.0000,  0.0000,  0.0000], [-0.8221,  0.5693,  0.0000], [0.7655,  0.1756,  0.6190]],\n                     [[1.0000,  0.0000,  0.0000], [-0.5345,  0.8451,  0.0000], [-0.5459, -0.3847,  0.7444]],\n                     [[1.0000,  0.0000,  0.0000], [-0.3758,  0.9267,  0.0000], [-0.2409,  0.4044,  0.8823]],\n                     [[1.0000,  0.0000,  0.0000], [-0.8800,  0.4750,  0.0000], [-0.9493,  0.1546,  0.2737]],\n                     [[1.0000,  0.0000,  0.0000], [0.2284,  0.9736,  0.0000], [-0.1283,  0.0451,  0.9907]]]},\n                ]),\n    Fixture(pyro_dist=dist.Stable,\n            examples=[\n                {\'stability\': [1.5], \'skew\': 0.1, \'test_data\': [-10.]},\n                {\'stability\': [1.5], \'skew\': 0.1, \'scale\': 2.0, \'loc\': -2.0, \'test_data\': [10.]},\n                ]),\n    Fixture(pyro_dist=dist.MultivariateStudentT,\n            examples=[\n                {\'df\': 1.5, \'loc\': [0.2, 0.3], \'scale_tril\': [[0.8, 0.0], [1.3, 0.4]],\n                 \'test_data\': [-3., 2]},\n                ]),\n]\n\ndiscrete_dists = [\n    Fixture(pyro_dist=dist.Multinomial,\n            scipy_dist=sp.multinomial,\n            examples=[\n                {\'probs\': [0.1, 0.6, 0.3],\n                 \'test_data\': [0., 1., 0.]},\n                {\'probs\': [0.1, 0.6, 0.3], \'total_count\': 8,\n                 \'test_data\': [2., 4., 2.]},\n                {\'probs\': [0.1, 0.6, 0.3], \'total_count\': 8,\n                 \'test_data\': [[2., 4., 2.], [2., 4., 2.]]},\n                {\'probs\': [[0.1, 0.6, 0.3], [0.2, 0.4, 0.4]], \'total_count\': 8,\n                 \'test_data\': [[2., 4., 2.], [1., 4., 3.]]}\n            ],\n            scipy_arg_fn=lambda probs, total_count=[1]: ((total_count[0], np.array(probs)), {}),\n            prec=0.05,\n            min_samples=10000,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.Bernoulli,\n            scipy_dist=sp.bernoulli,\n            examples=[\n                {\'probs\': [0.25],\n                 \'test_data\': [1.]},\n                {\'probs\': [0.25, 0.25],\n                 \'test_data\': [[[0., 1.]], [[1., 0.]], [[0., 0.]]]},\n                {\'logits\': [math.log(p / (1 - p)) for p in (0.25, 0.25)],\n                 \'test_data\': [[[0., 1.]], [[1., 0.]], [[0., 0.]]]},\n                # for now, avoid tests on infinite logits\n                # {\'logits\': [-float(\'inf\'), 0],\n                #  \'test_data\': [[0, 1], [0, 1], [0, 1]]},\n                {\'logits\': [[math.log(p / (1 - p)) for p in (0.25, 0.25)],\n                            [math.log(p / (1 - p)) for p in (0.3, 0.3)]],\n                 \'test_data\': [[1., 1.], [0., 0.]]},\n                {\'probs\': [[0.25, 0.25], [0.3, 0.3]],\n                 \'test_data\': [[1., 1.], [0., 0.]]}\n            ],\n            # for now, avoid tests on infinite logits\n            # test_data_indices=[0, 1, 2, 3],\n            batch_data_indices=[-1, -2],\n            scipy_arg_fn=lambda **kwargs: ((), {\'p\': kwargs[\'probs\']}),\n            prec=0.01,\n            min_samples=10000,\n            is_discrete=True,\n            expected_support_non_vec=[[0.], [1.]],\n            expected_support=[[[0., 0.], [0., 0.]], [[1., 1.], [1., 1.]]]),\n    Fixture(pyro_dist=dist.BetaBinomial,\n            examples=[\n                {\'concentration1\': [2.], \'concentration0\': [5.], \'total_count\': 8,\n                 \'test_data\': [4.]},\n                {\'concentration1\': [2.], \'concentration0\': [5.], \'total_count\': 8,\n                 \'test_data\': [[2.], [4.]]},\n                {\'concentration1\': [[2.], [2.]], \'concentration0\': [[5.], [5.]], \'total_count\': 8,\n                 \'test_data\': [[4.], [3.]]},\n                {\'concentration1\': [2., 2.], \'concentration0\': [5., 5.], \'total_count\': [0., 0.],\n                 \'test_data\': [[0., 0.], [0., 0.]]},\n                {\'concentration1\': [2., 2.], \'concentration0\': [5., 5.], \'total_count\': [[8., 7.], [5., 9.]],\n                 \'test_data\': [[6., 3.], [2., 8.]]},\n            ],\n            batch_data_indices=[-1, -2],\n            prec=0.01,\n            min_samples=10000,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.Binomial,\n            scipy_dist=sp.binom,\n            examples=[\n                {\'probs\': [0.6], \'total_count\': 8,\n                 \'test_data\': [4.]},\n                {\'probs\': [0.3], \'total_count\': 8,\n                 \'test_data\': [[2.], [4.]]},\n                {\'probs\': [[0.2], [0.4]], \'total_count\': 8,\n                 \'test_data\': [[4.], [3.]]},\n                {\'probs\': [0.2, 0.4], \'total_count\': [0., 0.],\n                 \'test_data\': [[0., 0.], [0., 0.]]},\n                {\'probs\': [0.2, 0.4], \'total_count\': [[8., 7.], [5., 9.]],\n                 \'test_data\': [[6., 3.], [2., 8.]]},\n            ],\n            scipy_arg_fn=lambda probs, total_count: ((total_count, probs), {}),\n            prec=0.05,\n            min_samples=10000,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.ExtendedBetaBinomial,\n            examples=[\n                {\'concentration1\': [2.], \'concentration0\': [5.], \'total_count\': 8,\n                 \'test_data\': [4.]},\n                {\'concentration1\': [2.], \'concentration0\': [5.], \'total_count\': 8,\n                 \'test_data\': [[2.], [4.]]},\n                {\'concentration1\': [[2.], [2.]], \'concentration0\': [[5.], [5.]], \'total_count\': 8,\n                 \'test_data\': [[4.], [3.]]},\n                {\'concentration1\': [2., 2.], \'concentration0\': [5., 5.], \'total_count\': [0., 0.],\n                 \'test_data\': [[0., 0.], [0., 0.]]},\n                {\'concentration1\': [2., 2.], \'concentration0\': [5., 5.], \'total_count\': [[8., 7.], [5., 9.]],\n                 \'test_data\': [[6., 3.], [2., 8.]]},\n            ],\n            batch_data_indices=[-1, -2],\n            prec=0.01,\n            min_samples=10000,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.ExtendedBinomial,\n            scipy_dist=sp.binom,\n            examples=[\n                {\'probs\': [0.6], \'total_count\': 8,\n                 \'test_data\': [4.]},\n                {\'probs\': [0.3], \'total_count\': 8,\n                 \'test_data\': [[2.], [4.]]},\n                {\'probs\': [[0.2], [0.4]], \'total_count\': 8,\n                 \'test_data\': [[4.], [3.]]},\n                {\'probs\': [0.2, 0.4], \'total_count\': [0., 0.],\n                 \'test_data\': [[0., 0.], [0., 0.]]},\n                {\'probs\': [0.2, 0.4], \'total_count\': [[8., 7.], [5., 9.]],\n                 \'test_data\': [[6., 3.], [2., 8.]]},\n            ],\n            scipy_arg_fn=lambda probs, total_count: ((total_count, probs), {}),\n            prec=0.05,\n            min_samples=10000,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.Categorical,\n            scipy_dist=sp.multinomial,\n            examples=[\n                {\'probs\': [0.1, 0.6, 0.3],\n                 \'test_data\': [2]},\n                {\'logits\': list(map(math.log, [0.1, 0.6, 0.3])),\n                 \'test_data\': [2]},\n                {\'logits\': [list(map(math.log, [0.1, 0.6, 0.3])),\n                            list(map(math.log, [0.2, 0.4, 0.4]))],\n                 \'test_data\': [2, 0]},\n                {\'probs\': [[0.1, 0.6, 0.3],\n                           [0.2, 0.4, 0.4]],\n                 \'test_data\': [2, 0]}\n            ],\n            test_data_indices=[0, 1, 2],\n            batch_data_indices=[-1, -2],\n            scipy_arg_fn=None,\n            prec=0.05,\n            min_samples=10000,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.DirichletMultinomial,\n            examples=[\n                {\'concentration\': [0.1, 0.6, 0.3],\n                 \'test_data\': [0., 1., 0.]},\n                {\'concentration\': [0.5, 1.0, 2.0], \'total_count\': 8,\n                 \'test_data\': [0., 2., 6.]},\n                {\'concentration\': [[0.5, 1.0, 2.0], [3., 3., 0.1]], \'total_count\': 8,\n                 \'test_data\': [[0., 2., 6.], [5., 2., 1.]]},\n            ],\n            prec=0.08,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.GammaPoisson,\n            examples=[\n                {\'concentration\': [1.], \'rate\': [2.],\n                 \'test_data\': [0.]},\n                {\'concentration\': [1.], \'rate\': [2.],\n                 \'test_data\': [1.]},\n                {\'concentration\': [1.], \'rate\': [2.],\n                 \'test_data\': [4.]},\n                {\'concentration\': [1., 1., 1.], \'rate\': [2., 2., 3.],\n                 \'test_data\': [[0., 1., 4.], [0., 1., 4.]]},\n                {\'concentration\': [[1.0], [1.0], [1.0]], \'rate\': [[2.0], [2.0], [3.0]],\n                 \'test_data\': [[0.], [1.], [4.]]}\n            ],\n            prec=0.08,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.OneHotCategorical,\n            scipy_dist=sp.multinomial,\n            examples=[\n                {\'probs\': [0.1, 0.6, 0.3],\n                 \'test_data\': [0., 0., 1.]},\n                {\'logits\': list(map(math.log, [0.1, 0.6, 0.3])),\n                 \'test_data\': [0., 0., 1.]},\n                {\'logits\': [list(map(math.log, [0.1, 0.6, 0.3])),\n                            list(map(math.log, [0.2, 0.4, 0.4]))],\n                 \'test_data\': [[0., 0., 1.], [1., 0., 0.]]},\n                {\'probs\': [[0.1, 0.6, 0.3],\n                           [0.2, 0.4, 0.4]],\n                 \'test_data\': [[0., 0., 1.], [1., 0., 0.]]}\n            ],\n            test_data_indices=[0, 1, 2],\n            batch_data_indices=[-1, -2],\n            scipy_arg_fn=lambda probs: ((1, np.array(probs)), {}),\n            prec=0.05,\n            min_samples=10000,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.Poisson,\n            scipy_dist=sp.poisson,\n            examples=[\n                {\'rate\': [2.0],\n                 \'test_data\': [0.]},\n                {\'rate\': [3.0],\n                 \'test_data\': [1.]},\n                {\'rate\': [6.0],\n                 \'test_data\': [4.]},\n                {\'rate\': [2.0, 3.0, 6.0],\n                 \'test_data\': [[0., 1., 4.], [0., 1., 4.]]},\n                {\'rate\': [[2.0], [3.0], [6.0]],\n                 \'test_data\': [[0.], [1.], [4.]]}\n            ],\n            scipy_arg_fn=lambda rate: ((np.array(rate),), {}),\n            prec=0.08,\n            is_discrete=True),\n    Fixture(pyro_dist=dist.Geometric,\n            scipy_dist=sp.geom,\n            examples=[\n                {\'logits\': [2.0],\n                 \'test_data\': [0.]},\n                {\'logits\': [3.0],\n                 \'test_data\': [1.]},\n                {\'logits\': [-6.0],\n                 \'test_data\': [4.]},\n                {\'logits\': [2.0, 3.0, -6.0],\n                 \'test_data\': [[0., 1., 4.], [0., 1., 4.]]},\n                {\'logits\': [[2.0], [3.0], [-6.0]],\n                 \'test_data\': [[0.], [1.], [4.]]}\n            ],\n            scipy_arg_fn=lambda probs: ((np.array(probs), -1), {}),\n            prec=0.08,\n            is_discrete=True),\n]\n\n\n@pytest.fixture(name=\'dist\',\n                params=continuous_dists + discrete_dists,\n                ids=lambda x: x.get_test_distribution_name())\ndef all_distributions(request):\n    return request.param\n\n\n@pytest.fixture(name=\'discrete_dist\',\n                params=discrete_dists,\n                ids=lambda x: x.get_test_distribution_name())\ndef discrete_distributions(request):\n    return request.param\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/distributions""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""unit""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/distributions/dist_fixture.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport numpy as np\nimport torch\nfrom torch.distributions.utils import logits_to_probs\n\nfrom pyro.distributions.util import broadcast_shape\n\nSINGLE_TEST_DATUM_IDX = [0]\nBATCH_TEST_DATA_IDX = [-1]\n\n\nclass Fixture:\n    def __init__(self,\n                 pyro_dist=None,\n                 scipy_dist=None,\n                 examples=None,\n                 scipy_arg_fn=None,\n                 prec=0.05,\n                 min_samples=None,\n                 is_discrete=False,\n                 expected_support_non_vec=None,\n                 expected_support=None,\n                 test_data_indices=None,\n                 batch_data_indices=None):\n        self.pyro_dist = pyro_dist\n        self.scipy_dist = scipy_dist\n        self.dist_params, self.test_data = self._extract_fixture_data(examples)\n        self.scipy_arg_fn = scipy_arg_fn\n        self.min_samples = min_samples\n        self.prec = prec\n        self.is_discrete = is_discrete\n        self.expected_support_non_vec = expected_support_non_vec\n        self.expected_support = expected_support\n        self.test_data_indices = test_data_indices\n        self.batch_data_indices = batch_data_indices\n\n    def get_batch_data_indices(self):\n        if not self.batch_data_indices:\n            return BATCH_TEST_DATA_IDX\n        return self.batch_data_indices\n\n    def get_test_data_indices(self):\n        if not self.test_data_indices:\n            return SINGLE_TEST_DATUM_IDX\n        return self.test_data_indices\n\n    def _extract_fixture_data(self, examples):\n        dist_params, test_data = [], []\n        for ex in examples:\n            test_data.append(ex.pop(\'test_data\'))\n            dist_params.append(ex)\n        return dist_params, test_data\n\n    def get_num_test_data(self):\n        return len(self.test_data)\n\n    def get_samples(self, num_samples, **dist_params):\n        return self.pyro_dist(**dist_params).sample(sample_shape=torch.Size((num_samples,)))\n\n    def get_test_data(self, idx, wrap_tensor=True):\n        if not wrap_tensor:\n            return self.test_data[idx]\n        return tensor_wrap(self.test_data[idx])[0]\n\n    def get_dist_params(self, idx, wrap_tensor=True):\n        if not wrap_tensor:\n            return self.dist_params[idx]\n        return tensor_wrap(**self.dist_params[idx])\n\n    def _convert_logits_to_ps(self, dist_params):\n        if \'logits\' in dist_params:\n            logits = torch.tensor(dist_params.pop(\'logits\'))\n            is_multidimensional = self.get_test_distribution_name() not in [\'Bernoulli\', \'Geometric\']\n            probs = logits_to_probs(logits, is_binary=not is_multidimensional)\n            dist_params[\'probs\'] = list(probs.detach().cpu().numpy())\n        return dist_params\n\n    def get_scipy_logpdf(self, idx):\n        if not self.scipy_arg_fn:\n            return\n        dist_params = self.get_dist_params(idx, wrap_tensor=False)\n        dist_params = self._convert_logits_to_ps(dist_params)\n        args, kwargs = self.scipy_arg_fn(**dist_params)\n        if self.is_discrete:\n            log_prob = self.scipy_dist.logpmf(self.get_test_data(idx, wrap_tensor=False), *args, **kwargs)\n        else:\n            log_prob = self.scipy_dist.logpdf(self.get_test_data(idx, wrap_tensor=False), *args, **kwargs)\n        return np.sum(log_prob)\n\n    def get_scipy_batch_logpdf(self, idx):\n        if not self.scipy_arg_fn:\n            return\n        dist_params = self.get_dist_params(idx, wrap_tensor=False)\n        dist_params_wrapped = self.get_dist_params(idx)\n        dist_params = self._convert_logits_to_ps(dist_params)\n        test_data = self.get_test_data(idx, wrap_tensor=False)\n        test_data_wrapped = self.get_test_data(idx)\n        shape = broadcast_shape(self.pyro_dist(**dist_params_wrapped).shape(), test_data_wrapped.size())\n        log_prob = []\n        for i in range(len(test_data)):\n            batch_params = {}\n            for k in dist_params:\n                param = np.broadcast_to(dist_params[k], shape)\n                batch_params[k] = param[i]\n            args, kwargs = self.scipy_arg_fn(**batch_params)\n            if self.is_discrete:\n                log_prob.append(self.scipy_dist.logpmf(test_data[i], *args, **kwargs))\n            else:\n                log_prob.append(self.scipy_dist.logpdf(test_data[i], *args, **kwargs))\n        return log_prob\n\n    def get_num_samples(self, idx):\n        r""""""\n        Number of samples needed to estimate the population variance within the tolerance limit\n        Sample variance is normally distributed http://stats.stackexchange.com/a/105338/71884\n        (see warning below).\n        Var(s^2) /approx 1/n * (\\loc_4 - \\scale^4)\n        Adjust n as per the tolerance needed to estimate the sample variance\n        warning: does not work for some distributions like bernoulli - https://stats.stackexchange.com/a/104911\n        use the min_samples for explicitly controlling the number of samples to be drawn\n        """"""\n        if self.min_samples:\n            return self.min_samples\n        min_samples = 1000\n        tol = 10.0\n        required_precision = self.prec / tol\n        if not self.scipy_dist:\n            return min_samples\n        args, kwargs = self.scipy_arg_fn(**self.get_dist_params(idx, wrap_tensor=False))\n        try:\n            fourth_moment = np.max(self.scipy_dist.moment(4, *args, **kwargs))\n            var = np.max(self.scipy_dist.var(*args, **kwargs))\n            min_computed_samples = int(math.ceil((fourth_moment - math.pow(var, 2)) / required_precision))\n        except (AttributeError, ValueError):\n            return min_samples\n        return max(min_samples, min_computed_samples)\n\n    def get_test_distribution_name(self):\n        return self.pyro_dist.__name__\n\n\ndef tensor_wrap(*args, **kwargs):\n    tensor_list, tensor_map = [], {}\n    for arg in args:\n        wrapped_arg = torch.tensor(arg) if isinstance(arg, list) else arg\n        tensor_list.append(wrapped_arg)\n    for k in kwargs:\n        kwarg = kwargs[k]\n        wrapped_kwarg = torch.tensor(kwarg) if isinstance(kwarg, list) else kwarg\n        tensor_map[k] = wrapped_kwarg\n    if args and not kwargs:\n        return tensor_list\n    if kwargs and not args:\n        return tensor_map\n    return tensor_list, tensor_map\n'"
tests/distributions/test_binomial.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.contrib.epidemiology.distributions import set_approx_log_prob_tol, set_approx_sample_thresh\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""total_count"", [10, 100, 1000, 4000])\n@pytest.mark.parametrize(""prob"", [0.01, 0.1, 0.5, 0.9, 0.99])\ndef test_binomial_approx_sample(total_count, prob):\n    sample_shape = (10000,)\n    d = dist.Binomial(total_count, prob)\n    expected = d.sample(sample_shape)\n    with set_approx_sample_thresh(200):\n        actual = d.sample(sample_shape)\n\n    assert_close(expected.mean(), actual.mean(), rtol=0.05)\n    assert_close(expected.std(), actual.std(), rtol=0.05)\n\n\n@pytest.mark.parametrize(""total_count"", [10, 100, 1000, 4000])\n@pytest.mark.parametrize(""concentration1"", [0.1, 1.0, 10.])\n@pytest.mark.parametrize(""concentration0"", [0.1, 1.0, 10.])\ndef test_beta_binomial_approx_sample(concentration1, concentration0, total_count):\n    sample_shape = (10000,)\n    d = dist.BetaBinomial(concentration1, concentration0, total_count)\n    expected = d.sample(sample_shape)\n    with set_approx_sample_thresh(200):\n        actual = d.sample(sample_shape)\n\n    assert_close(expected.mean(), actual.mean(), rtol=0.1)\n    assert_close(expected.std(), actual.std(), rtol=0.1)\n\n\n@pytest.mark.parametrize(""tol"", [\n    1e-8, 1e-6, 1e-4, 1e-2, 0.02, 0.05, 0.1, 0.2, 0.1, 1.,\n])\ndef test_binomial_approx_log_prob(tol):\n    logits = torch.linspace(-10., 10., 100)\n    k = torch.arange(100.).unsqueeze(-1)\n    n_minus_k = torch.arange(100.).unsqueeze(-1).unsqueeze(-1)\n    n = k + n_minus_k\n\n    expected = torch.distributions.Binomial(n, logits=logits).log_prob(k)\n    with set_approx_log_prob_tol(tol):\n        actual = dist.Binomial(n, logits=logits).log_prob(k)\n\n    assert_close(actual, expected, atol=tol)\n'"
tests/distributions/test_categorical.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom unittest import TestCase\n\nimport numpy as np\nimport pytest\nimport scipy.stats as sp\nimport torch\n\nimport pyro.distributions as dist\nfrom tests.common import assert_equal\n\n\nclass TestCategorical(TestCase):\n    """"""\n    Tests methods specific to the Categorical distribution\n    """"""\n\n    def setUp(self):\n        n = 1\n        self.probs = torch.tensor([0.1, 0.6, 0.3])\n        self.batch_ps = torch.tensor([[0.1, 0.6, 0.3], [0.2, 0.4, 0.4]])\n        self.n = torch.tensor([n])\n        self.test_data = torch.tensor([2.0])\n        self.analytic_mean = n * self.probs\n        one = torch.ones(3)\n        self.analytic_var = n * torch.mul(self.probs, one.sub(self.probs))\n\n        # Discrete Distribution\n        self.d_ps = torch.tensor([[0.2, 0.3, 0.5], [0.1, 0.1, 0.8]])\n        self.d_test_data = torch.tensor([[0.0], [5.0]])\n\n        self.n_samples = 50000\n\n        self.support_non_vec = torch.tensor([0.0, 1.0, 2.0])\n        self.support = torch.tensor([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]])\n\n    def test_log_prob_sum(self):\n        log_px_torch = dist.Categorical(self.probs).log_prob(self.test_data).sum().item()\n        log_px_np = float(sp.multinomial.logpmf(np.array([0, 0, 1]), 1, self.probs.detach().cpu().numpy()))\n        assert_equal(log_px_torch, log_px_np, prec=1e-4)\n\n    def test_mean_and_var(self):\n        torch_samples = [dist.Categorical(self.probs).sample().detach().cpu().numpy()\n                         for _ in range(self.n_samples)]\n        _, counts = np.unique(torch_samples, return_counts=True)\n        computed_mean = float(counts[0]) / self.n_samples\n        assert_equal(computed_mean, self.analytic_mean.detach().cpu().numpy()[0], prec=0.05)\n\n    def test_support_non_vectorized(self):\n        s = dist.Categorical(self.d_ps[0].squeeze(0)).enumerate_support()\n        assert_equal(s.data, self.support_non_vec)\n\n    def test_support(self):\n        s = dist.Categorical(self.d_ps).enumerate_support()\n        assert_equal(s.data, self.support)\n\n\ndef wrap_nested(x, dim):\n    if dim == 0:\n        return x\n    return wrap_nested([x], dim-1)\n\n\n@pytest.fixture(params=[1, 2, 3], ids=lambda x: ""dim="" + str(x))\ndef dim(request):\n    return request.param\n\n\n@pytest.fixture(params=[[0.3, 0.5, 0.2]], ids=None)\ndef probs(request):\n    return request.param\n\n\ndef modify_params_using_dims(probs, dim):\n    return torch.tensor(wrap_nested(probs, dim-1))\n\n\ndef test_support_dims(dim, probs):\n    probs = modify_params_using_dims(probs, dim)\n    support = dist.Categorical(probs).enumerate_support()\n    assert_equal(support.size(), torch.Size((probs.size(-1),) + probs.size()[:-1]))\n\n\ndef test_sample_dims(dim, probs):\n    probs = modify_params_using_dims(probs, dim)\n    sample = dist.Categorical(probs).sample()\n    expected_shape = dist.Categorical(probs).shape()\n    assert_equal(sample.size(), expected_shape)\n\n\ndef test_batch_log_dims(dim, probs):\n    probs = modify_params_using_dims(probs, dim)\n    log_prob_shape = torch.Size((3,) + dist.Categorical(probs).batch_shape)\n    support = dist.Categorical(probs).enumerate_support()\n    log_prob = dist.Categorical(probs).log_prob(support)\n    assert_equal(log_prob.size(), log_prob_shape)\n\n\ndef test_view_reshape_bug():\n    batch_shape = (1, 2, 1, 3, 1)\n    sample_shape = (4,)\n    cardinality = 2\n    logits = torch.randn(batch_shape + (cardinality,))\n    dist.Categorical(logits=logits).sample(sample_shape)\n'"
tests/distributions/test_coalescent.py,15,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nfrom pyro.distributions import CoalescentTimes, CoalescentTimesWithRate\nfrom pyro.distributions.coalescent import CoalescentRateLikelihood, CoalescentTimesConstraint, _sample_coalescent_times\nfrom pyro.distributions.util import broadcast_shape\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""num_leaves"", range(2, 30))\ndef test_sample_is_valid(num_leaves):\n    pyro.set_rng_seed(num_leaves)\n\n    # Check with disperse leaves.\n    leaf_times = torch.randn(num_leaves)\n    coal_times = _sample_coalescent_times(leaf_times)\n    assert CoalescentTimesConstraint(leaf_times).check(coal_times)\n    assert len(set(coal_times.tolist())) == len(coal_times)\n\n    # Check with simultaneous leaves.\n    leaf_times = torch.zeros(num_leaves)\n    coal_times = _sample_coalescent_times(leaf_times)\n    assert CoalescentTimesConstraint(leaf_times).check(coal_times)\n    assert len(set(coal_times.tolist())) == len(coal_times)\n\n\n@pytest.mark.parametrize(""num_steps"", [9])\n@pytest.mark.parametrize(""sample_shape"", [(), (7,), (4, 5)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (6,), (2, 3)], ids=str)\n@pytest.mark.parametrize(""num_leaves"", [2, 3, 5, 11])\ndef test_simple_smoke(num_leaves, num_steps, batch_shape, sample_shape):\n    leaf_times = torch.rand(batch_shape + (num_leaves,)).pow(0.5) * num_steps\n    d = CoalescentTimes(leaf_times)\n    coal_times = d.sample(sample_shape)\n    assert coal_times.shape == sample_shape + batch_shape + (num_leaves-1,)\n\n    actual = d.log_prob(coal_times)\n    assert actual.shape == sample_shape + batch_shape\n\n\n@pytest.mark.parametrize(""num_steps"", [9])\n@pytest.mark.parametrize(""sample_shape"", [(), (6,), (4, 5)], ids=str)\n@pytest.mark.parametrize(""rate_grid_shape"", [(), (2,), (3, 1), (3, 2)], ids=str)\n@pytest.mark.parametrize(""leaf_times_shape"", [(), (2,), (3, 1), (3, 2)], ids=str)\n@pytest.mark.parametrize(""num_leaves"", [2, 7, 11])\ndef test_with_rate_smoke(num_leaves, num_steps, leaf_times_shape, rate_grid_shape, sample_shape):\n    batch_shape = broadcast_shape(leaf_times_shape, rate_grid_shape)\n    leaf_times = torch.rand(leaf_times_shape + (num_leaves,)).pow(0.5) * num_steps\n    rate_grid = torch.rand(rate_grid_shape + (num_steps,))\n    d = CoalescentTimesWithRate(leaf_times, rate_grid)\n    coal_times = _sample_coalescent_times(\n        leaf_times.expand(sample_shape + batch_shape + (-1,)))\n    assert coal_times.shape == sample_shape + batch_shape + (num_leaves-1,)\n\n    actual = d.log_prob(coal_times)\n    assert actual.shape == sample_shape + batch_shape\n\n\n@pytest.mark.parametrize(""num_steps"", [9])\n@pytest.mark.parametrize(""sample_shape"", [(), (5,)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""num_leaves"", [2, 7, 11])\ndef test_log_prob_unit_rate(num_leaves, num_steps, batch_shape, sample_shape):\n    leaf_times = torch.rand(batch_shape + (num_leaves,)).pow(0.5) * num_steps\n    d1 = CoalescentTimes(leaf_times)\n\n    rate_grid = torch.ones(batch_shape + (num_steps,))\n    d2 = CoalescentTimesWithRate(leaf_times, rate_grid)\n\n    coal_times = d1.sample(sample_shape)\n    assert_close(d1.log_prob(coal_times), d2.log_prob(coal_times))\n\n\n@pytest.mark.parametrize(""num_steps"", [9])\n@pytest.mark.parametrize(""sample_shape"", [(), (5,)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""num_leaves"", [2, 7, 11])\ndef test_log_prob_constant_rate(num_leaves, num_steps, batch_shape, sample_shape):\n    rate = torch.randn(batch_shape).exp()\n    rate_grid = rate.unsqueeze(-1).expand(batch_shape + (num_steps,))\n    leaf_times_2 = torch.rand(batch_shape + (num_leaves,)).pow(0.5) * num_steps\n    leaf_times_1 = leaf_times_2 * rate.unsqueeze(-1)\n\n    d1 = CoalescentTimes(leaf_times_1)\n    coal_times_1 = d1.sample(sample_shape)\n    log_prob_1 = d1.log_prob(coal_times_1)\n\n    d2 = CoalescentTimesWithRate(leaf_times_2, rate_grid)\n    coal_times_2 = coal_times_1 / rate.unsqueeze(-1)\n    log_prob_2 = d2.log_prob(coal_times_2)\n\n    log_abs_det_jacobian = -coal_times_2.size(-1) * rate.log()\n    assert_close(log_prob_1 - log_abs_det_jacobian, log_prob_2)\n\n\n@pytest.mark.parametrize(""clamped"", [True, False], ids=[""clamped"", ""unclamped""])\n@pytest.mark.parametrize(""num_steps"", [2, 5, 10, 20])\n@pytest.mark.parametrize(""num_leaves"", [2, 5, 10, 20])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\ndef test_likelihood_vectorized(num_leaves, num_steps, batch_shape, clamped):\n    if clamped:\n        leaf_times = torch.rand(batch_shape + (num_leaves,)).pow(0.5) * num_steps\n        coal_times = CoalescentTimes(leaf_times).sample().clamp(min=0)\n    else:\n        leaf_times = torch.randn(batch_shape + (num_leaves,))\n        leaf_times.mul_(0.25).add_(0.75).mul_(num_steps)\n        coal_times = CoalescentTimes(leaf_times).sample()\n\n    rate_grid = torch.rand(batch_shape + (num_steps,)) + 0.5\n\n    d = CoalescentTimesWithRate(leaf_times, rate_grid)\n    expected = d.log_prob(coal_times)\n\n    likelihood = CoalescentRateLikelihood(leaf_times, coal_times, num_steps)\n    actual = likelihood(rate_grid).sum(-1)\n\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(""clamped"", [True, False], ids=[""clamped"", ""unclamped""])\n@pytest.mark.parametrize(""num_steps"", [2, 5, 10, 20])\n@pytest.mark.parametrize(""num_leaves"", [2, 5, 10, 20])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\ndef test_likelihood_sequential(num_leaves, num_steps, batch_shape, clamped):\n    if clamped:\n        leaf_times = torch.rand(batch_shape + (num_leaves,)).pow(0.5) * num_steps\n        coal_times = CoalescentTimes(leaf_times).sample().clamp(min=0)\n    else:\n        leaf_times = torch.randn(batch_shape + (num_leaves,))\n        leaf_times.mul_(0.25).add_(0.75).mul_(num_steps)\n        coal_times = CoalescentTimes(leaf_times).sample()\n\n    rate_grid = torch.rand(batch_shape + (num_steps,)) + 0.5\n\n    d = CoalescentTimesWithRate(leaf_times, rate_grid)\n    expected = d.log_prob(coal_times)\n\n    likelihood = CoalescentRateLikelihood(leaf_times, coal_times, num_steps)\n    actual = sum(likelihood(rate_grid[..., t], t)\n                 for t in range(num_steps))\n\n    assert_close(actual, expected)\n'"
tests/distributions/test_conjugate.py,19,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.distributions import BetaBinomial, DirichletMultinomial, GammaPoisson\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""dist"", [\n    BetaBinomial(2., 5., 10.),\n    BetaBinomial(torch.tensor([2., 4.]), torch.tensor([5., 8.]), torch.tensor([10., 12.])),\n    DirichletMultinomial(torch.tensor([0.5, 1.0, 2.0]), 5),\n    DirichletMultinomial(torch.tensor([[0.5, 1.0, 2.0], [0.2, 0.5, 0.8]]), torch.tensor(10.)),\n    GammaPoisson(2., 2.),\n    GammaPoisson(torch.tensor([6., 2]), torch.tensor([2., 8.])),\n])\ndef test_mean(dist):\n    analytic_mean = dist.mean\n    num_samples = 500000\n    sample_mean = dist.sample((num_samples,)).mean(0)\n    assert_close(sample_mean, analytic_mean, atol=0.01)\n\n\n@pytest.mark.parametrize(""dist"", [\n    BetaBinomial(2., 5., 10.),\n    BetaBinomial(torch.tensor([2., 4.]), torch.tensor([5., 8.]), torch.tensor([10., 12.])),\n    DirichletMultinomial(torch.tensor([0.5, 1.0, 2.0]), 5),\n    DirichletMultinomial(torch.tensor([[0.5, 1.0, 2.0], [0.2, 0.5, 0.8]]), torch.tensor(10.)),\n    GammaPoisson(2., 2.),\n    GammaPoisson(torch.tensor([6., 2]), torch.tensor([2., 8.])),\n])\ndef test_variance(dist):\n    analytic_var = dist.variance\n    num_samples = 500000\n    sample_var = dist.sample((num_samples,)).var(0)\n    assert_close(sample_var, analytic_var, rtol=0.01)\n\n\n@pytest.mark.parametrize(""dist, values"", [\n    (BetaBinomial(2., 5., 10), None),\n    (BetaBinomial(2., 5., 10), None),\n    (GammaPoisson(2., 2.), torch.arange(10.)),\n    (GammaPoisson(6., 2.), torch.arange(20.)),\n])\ndef test_log_prob_support(dist, values):\n    if values is None:\n        values = dist.enumerate_support()\n    log_probs = dist.log_prob(values)\n    assert_close(log_probs.logsumexp(0), torch.tensor(0.), atol=0.01)\n\n\n@pytest.mark.parametrize(""total_count"", [1, 2, 3, 10])\n@pytest.mark.parametrize(""shape"", [(1,), (3, 1), (2, 3, 1)])\ndef test_beta_binomial_log_prob(total_count, shape):\n    concentration0 = torch.randn(shape).exp()\n    concentration1 = torch.randn(shape).exp()\n    value = torch.arange(1. + total_count)\n\n    num_samples = 100000\n    probs = dist.Beta(concentration1, concentration0).sample((num_samples,))\n    log_probs = dist.Binomial(total_count, probs).log_prob(value)\n    expected = log_probs.logsumexp(0) - math.log(num_samples)\n\n    actual = BetaBinomial(concentration1, concentration0, total_count).log_prob(value)\n    assert_close(actual, expected, rtol=0.02)\n\n\n@pytest.mark.parametrize(""total_count"", [1, 2, 3, 10])\n@pytest.mark.parametrize(""batch_shape"", [(1,), (3, 1), (2, 3, 1)])\n@pytest.mark.parametrize(""is_sparse"", [False, True], ids=[""dense"", ""sparse""])\ndef test_dirichlet_multinomial_log_prob(total_count, batch_shape, is_sparse):\n    event_shape = (3,)\n    concentration = torch.rand(batch_shape + event_shape).exp()\n    # test on one-hots\n    value = total_count * torch.eye(3).reshape(event_shape + (1,) * len(batch_shape) + event_shape)\n\n    num_samples = 100000\n    probs = dist.Dirichlet(concentration).sample((num_samples, 1))\n    log_probs = dist.Multinomial(total_count, probs).log_prob(value)\n    assert log_probs.shape == (num_samples,) + event_shape + batch_shape\n    expected = log_probs.logsumexp(0) - math.log(num_samples)\n\n    actual = DirichletMultinomial(concentration, total_count, is_sparse).log_prob(value)\n    assert_close(actual, expected, atol=0.05)\n\n\n@pytest.mark.parametrize(""shape"", [(1,), (3, 1), (2, 3, 1)])\ndef test_gamma_poisson_log_prob(shape):\n    gamma_conc = torch.randn(shape).exp()\n    gamma_rate = torch.randn(shape).exp()\n    value = torch.arange(20.)\n\n    num_samples = 300000\n    poisson_rate = dist.Gamma(gamma_conc, gamma_rate).sample((num_samples,))\n    log_probs = dist.Poisson(poisson_rate).log_prob(value)\n    expected = log_probs.logsumexp(0) - math.log(num_samples)\n    actual = GammaPoisson(gamma_conc, gamma_rate).log_prob(value)\n    assert_close(actual, expected, rtol=0.05)\n'"
tests/distributions/test_conjugate_update.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\ndef test_beta_binomial(sample_shape, batch_shape):\n    concentration1 = torch.randn(batch_shape).exp()\n    concentration0 = torch.randn(batch_shape).exp()\n    total = 10\n    obs = dist.Binomial(total, 0.2).sample(sample_shape + batch_shape)\n\n    f = dist.Beta(concentration1, concentration0)\n    g = dist.Beta(1 + obs, 1 + total - obs)\n    fg, log_normalizer = f.conjugate_update(g)\n\n    x = fg.sample(sample_shape)\n    assert_close(f.log_prob(x) + g.log_prob(x), fg.log_prob(x) + log_normalizer)\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\ndef test_dirichlet_multinomial(sample_shape, batch_shape):\n    concentration = torch.randn(batch_shape + (3,)).exp()\n    total = 10\n    probs = torch.tensor([0.2, 0.3, 0.5])\n    obs = dist.Multinomial(total, probs).sample(sample_shape + batch_shape)\n\n    f = dist.Dirichlet(concentration)\n    g = dist.Dirichlet(1 + obs)\n    fg, log_normalizer = f.conjugate_update(g)\n\n    x = fg.sample(sample_shape)\n    assert_close(f.log_prob(x) + g.log_prob(x), fg.log_prob(x) + log_normalizer)\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\ndef test_gamma_poisson(sample_shape, batch_shape):\n    concentration = torch.randn(batch_shape).exp()\n    rate = torch.randn(batch_shape).exp()\n    nobs = 5\n    obs = dist.Poisson(10.).sample((nobs,) + sample_shape + batch_shape).sum(0)\n\n    f = dist.Gamma(concentration, rate)\n    g = dist.Gamma(1 + obs, nobs)\n    fg, log_normalizer = f.conjugate_update(g)\n\n    x = fg.sample(sample_shape)\n    assert_close(f.log_prob(x) + g.log_prob(x), fg.log_prob(x) + log_normalizer)\n'"
tests/distributions/test_cuda.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nfrom tests.common import assert_equal, requires_cuda, tensors_default_to, xfail_if_not_implemented\n\n\n@requires_cuda\ndef test_sample(dist):\n    for idx in range(len(dist.dist_params)):\n\n        # Compute CPU value.\n        with tensors_default_to(""cpu""):\n            params = dist.get_dist_params(idx)\n        try:\n            with xfail_if_not_implemented():\n                cpu_value = dist.pyro_dist(**params).sample()\n        except ValueError as e:\n            pytest.xfail(\'CPU version fails: {}\'.format(e))\n        assert not cpu_value.is_cuda\n\n        # Compute GPU value.\n        with tensors_default_to(""cuda""):\n            params = dist.get_dist_params(idx)\n        cuda_value = dist.pyro_dist(**params).sample()\n        assert cuda_value.is_cuda\n\n        assert_equal(cpu_value.size(), cuda_value.size())\n\n\n@requires_cuda\ndef test_rsample(dist):\n    if not dist.pyro_dist.has_rsample:\n        return\n    for idx in range(len(dist.dist_params)):\n\n        # Compute CPU value.\n        with tensors_default_to(""cpu""):\n            params = dist.get_dist_params(idx)\n            grad_params = [key for key, val in params.items()\n                           if torch.is_tensor(val) and val.dtype in (torch.float32, torch.float64)]\n            for key in grad_params:\n                val = params[key].clone()\n                val.requires_grad = True\n                params[key] = val\n        try:\n            with xfail_if_not_implemented():\n                cpu_value = dist.pyro_dist(**params).rsample()\n                cpu_grads = grad(cpu_value.sum(), [params[key] for key in grad_params])\n        except ValueError as e:\n            pytest.xfail(\'CPU version fails: {}\'.format(e))\n        assert not cpu_value.is_cuda\n\n        # Compute GPU value.\n        with tensors_default_to(""cuda""):\n            params = dist.get_dist_params(idx)\n            for key in grad_params:\n                val = params[key].clone()\n                val.requires_grad = True\n                params[key] = val\n        cuda_value = dist.pyro_dist(**params).rsample()\n        assert cuda_value.is_cuda\n        assert_equal(cpu_value.size(), cuda_value.size())\n\n        cuda_grads = grad(cuda_value.sum(), [params[key] for key in grad_params])\n        for cpu_grad, cuda_grad in zip(cpu_grads, cuda_grads):\n            assert_equal(cpu_grad.size(), cuda_grad.size())\n\n\n@requires_cuda\ndef test_log_prob(dist):\n    for idx in range(len(dist.dist_params)):\n\n        # Compute CPU value.\n        with tensors_default_to(""cpu""):\n            data = dist.get_test_data(idx)\n            params = dist.get_dist_params(idx)\n        with xfail_if_not_implemented():\n            cpu_value = dist.pyro_dist(**params).log_prob(data)\n        assert not cpu_value.is_cuda\n\n        # Compute GPU value.\n        with tensors_default_to(""cuda""):\n            data = dist.get_test_data(idx)\n            params = dist.get_dist_params(idx)\n        cuda_value = dist.pyro_dist(**params).log_prob(data)\n        assert cuda_value.is_cuda\n\n        assert_equal(cpu_value, cuda_value.cpu())\n'"
tests/distributions/test_delta.py,14,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom unittest import TestCase\n\nimport numpy as np\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom tests.common import assert_equal\n\n\nclass TestDelta(TestCase):\n    def setUp(self):\n        self.v = torch.tensor([3.0])\n        self.vs = torch.tensor([[0.0], [1.0], [2.0], [3.0]])\n        self.vs_expanded = self.vs.expand(4, 3)\n        self.test_data = torch.tensor([[3.0], [3.0], [3.0]])\n        self.batch_test_data_1 = torch.arange(0., 4.).unsqueeze(1).expand(4, 3)\n        self.batch_test_data_2 = torch.arange(4., 8.).unsqueeze(1).expand(4, 3)\n        self.batch_test_data_3 = torch.Tensor([[3.], [3.], [3.], [3.]])\n        self.expected_support = [[[0.], [1.], [2.], [3.]]]\n        self.expected_support_non_vec = [[3.]]\n        self.analytic_mean = 3.\n        self.analytic_var = 0.\n        self.n_samples = 10\n\n    def test_log_prob_sum(self):\n        log_px_torch = dist.Delta(self.v).log_prob(self.test_data).sum()\n        assert_equal(log_px_torch.item(), 0)\n\n    def test_batch_log_prob(self):\n        log_px_torch = dist.Delta(self.vs_expanded).log_prob(self.batch_test_data_1).data\n        assert_equal(log_px_torch.sum().item(), 0)\n        log_px_torch = dist.Delta(self.vs_expanded).log_prob(self.batch_test_data_2).data\n        assert_equal(log_px_torch.sum().item(), float('-inf'))\n\n    def test_batch_log_prob_shape(self):\n        assert dist.Delta(self.vs).log_prob(self.batch_test_data_3).size() == (4, 1)\n        assert dist.Delta(self.v).log_prob(self.batch_test_data_3).size() == (4, 1)\n\n    def test_mean_and_var(self):\n        torch_samples = [dist.Delta(self.v).sample().detach().cpu().numpy()\n                         for _ in range(self.n_samples)]\n        torch_mean = np.mean(torch_samples)\n        torch_var = np.var(torch_samples)\n        assert_equal(torch_mean, self.analytic_mean)\n        assert_equal(torch_var, self.analytic_var)\n\n\n@pytest.mark.parametrize('batch_dim,event_dim',\n                         [(b, e) for b in range(4) for e in range(1+b)])\n@pytest.mark.parametrize('has_log_density', [False, True])\ndef test_shapes(batch_dim, event_dim, has_log_density):\n    shape = tuple(range(2, 2 + batch_dim + event_dim))\n    batch_shape = shape[:batch_dim]\n    v = torch.randn(shape)\n    log_density = torch.randn(batch_shape) if has_log_density else 0\n\n    d = dist.Delta(v, log_density=log_density, event_dim=event_dim)\n    x = d.rsample()\n    assert (x == v).all()\n    assert (d.log_prob(x) == log_density).all()\n\n\n@pytest.mark.parametrize('batch_shape', [(), [], (2,), [2], torch.Size([2]), [2, 3]])\ndef test_expand(batch_shape):\n    d1 = dist.Delta(torch.tensor(1.234))\n    d2 = d1.expand(batch_shape)\n    assert d2.batch_shape == torch.Size(batch_shape)\n"""
tests/distributions/test_distributions.py,18,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as np\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions import TorchDistribution\nfrom pyro.distributions.util import broadcast_shape\nfrom tests.common import assert_equal, xfail_if_not_implemented\n\n\ndef _log_prob_shape(dist, x_size=torch.Size()):\n    event_dims = len(dist.event_shape)\n    expected_shape = broadcast_shape(dist.shape(), x_size, strict=True)\n    if event_dims > 0:\n        expected_shape = expected_shape[:-event_dims]\n    return expected_shape\n\n# Distribution tests - all distributions\n\n\ndef test_batch_log_prob(dist):\n    if dist.scipy_arg_fn is None:\n        pytest.skip(\'{}.log_prob_sum has no scipy equivalent\'.format(dist.pyro_dist.__name__))\n    for idx in dist.get_batch_data_indices():\n        dist_params = dist.get_dist_params(idx)\n        d = dist.pyro_dist(**dist_params)\n        test_data = dist.get_test_data(idx)\n        log_prob_sum_pyro = d.log_prob(test_data).sum().item()\n        log_prob_sum_np = np.sum(dist.get_scipy_batch_logpdf(-1))\n        assert_equal(log_prob_sum_pyro, log_prob_sum_np)\n\n\ndef test_batch_log_prob_shape(dist):\n    for idx in range(dist.get_num_test_data()):\n        dist_params = dist.get_dist_params(idx)\n        d = dist.pyro_dist(**dist_params)\n        x = dist.get_test_data(idx)\n        with xfail_if_not_implemented():\n            # Get log_prob shape after broadcasting.\n            expected_shape = _log_prob_shape(d, x.size())\n            log_p_obj = d.log_prob(x)\n            assert log_p_obj.size() == expected_shape\n\n\ndef test_batch_entropy_shape(dist):\n    for idx in range(dist.get_num_test_data()):\n        dist_params = dist.get_dist_params(idx)\n        d = dist.pyro_dist(**dist_params)\n        with xfail_if_not_implemented():\n            # Get entropy shape after broadcasting.\n            expected_shape = _log_prob_shape(d)\n            entropy_obj = d.entropy()\n            assert entropy_obj.size() == expected_shape\n\n\ndef test_score_errors_event_dim_mismatch(dist):\n    for idx in dist.get_batch_data_indices():\n        dist_params = dist.get_dist_params(idx)\n        d = dist.pyro_dist(**dist_params)\n        test_data_wrong_dims = torch.ones(d.shape() + (1,))\n        if len(d.event_shape) > 0:\n            if dist.get_test_distribution_name() == \'MultivariateNormal\':\n                pytest.skip(\'MultivariateNormal does not do shape validation in log_prob.\')\n            elif dist.get_test_distribution_name() == \'LowRankMultivariateNormal\':\n                pytest.skip(\'LowRankMultivariateNormal does not do shape validation in log_prob.\')\n            with pytest.raises((ValueError, RuntimeError)):\n                d.log_prob(test_data_wrong_dims)\n\n\ndef test_score_errors_non_broadcastable_data_shape(dist):\n    for idx in dist.get_batch_data_indices():\n        dist_params = dist.get_dist_params(idx)\n        d = dist.pyro_dist(**dist_params)\n        shape = d.shape()\n        non_broadcastable_shape = (shape[0] + 1,) + shape[1:]\n        test_data_non_broadcastable = torch.ones(non_broadcastable_shape)\n        with pytest.raises((ValueError, RuntimeError)):\n            d.log_prob(test_data_non_broadcastable)\n\n\n# Distributions tests - discrete distributions\n\ndef test_enumerate_support(discrete_dist):\n    expected_support = discrete_dist.expected_support\n    expected_support_non_vec = discrete_dist.expected_support_non_vec\n    if not expected_support:\n        pytest.skip(""enumerate_support not tested for distribution"")\n    Dist = discrete_dist.pyro_dist\n    actual_support_non_vec = Dist(**discrete_dist.get_dist_params(0)).enumerate_support()\n    actual_support = Dist(**discrete_dist.get_dist_params(-1)).enumerate_support()\n    assert_equal(actual_support.data, torch.tensor(expected_support))\n    assert_equal(actual_support_non_vec.data, torch.tensor(expected_support_non_vec))\n\n\ndef test_enumerate_support_shape(dist):\n    if not dist.pyro_dist.has_enumerate_support:\n        pytest.skip()\n    for idx in range(dist.get_num_test_data()):\n        dist_params = dist.get_dist_params(idx)\n        d = dist.pyro_dist(**dist_params)\n        with xfail_if_not_implemented():\n            support = d.enumerate_support()\n            n = support.shape[0]\n            assert support.shape == (n,) + d.batch_shape + d.event_shape\n\n            support_expanded = d.enumerate_support(expand=True)\n            assert_equal(support, support_expanded)\n\n            support_unexpanded = d.enumerate_support(expand=False)\n            assert support_unexpanded.shape == (n,) + (1,) * len(d.batch_shape) + d.event_shape\n            assert (support_expanded == support_unexpanded).all()\n\n\n@pytest.mark.parametrize(""dist_class, args"", [\n    (dist.Normal, {""loc"": torch.tensor(0.0), ""scale"": torch.tensor(-1.0)}),\n    (dist.Gamma, {""concentration"": -1.0, ""rate"": 1.0}),\n    (dist.Exponential, {""rate"": -2})\n])\n@pytest.mark.parametrize(""validate_args"", [True, False])\ndef test_distribution_validate_args(dist_class, args, validate_args):\n    with pyro.validation_enabled(validate_args):\n        if not validate_args:\n            dist_class(**args)\n        else:\n            with pytest.raises(ValueError):\n                dist_class(**args)\n\n\ndef check_sample_shapes(small, large):\n    dist_instance = small\n    if isinstance(dist_instance, (dist.LogNormal, dist.LowRankMultivariateNormal, dist.VonMises)):\n        # Ignore broadcasting bug in LogNormal:\n        # https://github.com/pytorch/pytorch/pull/7269\n        return\n    x = small.sample()\n    assert_equal(small.log_prob(x).expand(large.batch_shape), large.log_prob(x))\n    x = large.sample()\n    assert_equal(small.log_prob(x), large.log_prob(x))\n\n\n@pytest.mark.parametrize(\'sample_shape\', [(), (2,), (2, 3)])\n@pytest.mark.parametrize(\'shape_type\', [torch.Size, tuple, list])\ndef test_expand_by(dist, sample_shape, shape_type):\n    for idx in range(dist.get_num_test_data()):\n        small = dist.pyro_dist(**dist.get_dist_params(idx))\n        large = small.expand_by(shape_type(sample_shape))\n        assert large.batch_shape == sample_shape + small.batch_shape\n        if dist.get_test_distribution_name() == \'Stable\':\n            pytest.skip(\'Stable does not implement a log_prob method.\')\n        check_sample_shapes(small, large)\n\n\n@pytest.mark.parametrize(\'sample_shape\', [(), (2,), (2, 3)])\n@pytest.mark.parametrize(\'shape_type\', [torch.Size, tuple, list])\n@pytest.mark.parametrize(\'default\', [False, True])\ndef test_expand_new_dim(dist, sample_shape, shape_type, default):\n    for idx in range(dist.get_num_test_data()):\n        small = dist.pyro_dist(**dist.get_dist_params(idx))\n        if default:\n            large = TorchDistribution.expand(small, shape_type(sample_shape + small.batch_shape))\n        else:\n            with xfail_if_not_implemented():\n                large = small.expand(shape_type(sample_shape + small.batch_shape))\n        assert large.batch_shape == sample_shape + small.batch_shape\n        if dist.get_test_distribution_name() == \'Stable\':\n            pytest.skip(\'Stable does not implement a log_prob method.\')\n        check_sample_shapes(small, large)\n\n\n@pytest.mark.parametrize(\'shape_type\', [torch.Size, tuple, list])\n@pytest.mark.parametrize(\'default\', [False, True])\ndef test_expand_existing_dim(dist, shape_type, default):\n    for idx in range(dist.get_num_test_data()):\n        small = dist.pyro_dist(**dist.get_dist_params(idx))\n        for dim, size in enumerate(small.batch_shape):\n            if size != 1:\n                continue\n            batch_shape = list(small.batch_shape)\n            batch_shape[dim] = 5\n            batch_shape = torch.Size(batch_shape)\n            if default:\n                large = TorchDistribution.expand(small, shape_type(batch_shape))\n            else:\n                with xfail_if_not_implemented():\n                    large = small.expand(shape_type(batch_shape))\n            assert large.batch_shape == batch_shape\n            if dist.get_test_distribution_name() == \'Stable\':\n                pytest.skip(\'Stable does not implement a log_prob method.\')\n            check_sample_shapes(small, large)\n\n\n@pytest.mark.parametrize(""sample_shapes"", [\n    [(2, 1), (2, 3)],\n    [(2, 1, 1), (2, 1, 3), (2, 5, 3)],\n])\n@pytest.mark.parametrize(\'default\', [False, True])\ndef test_subsequent_expands_ok(dist, sample_shapes, default):\n    for idx in range(dist.get_num_test_data()):\n        d = dist.pyro_dist(**dist.get_dist_params(idx))\n        original_batch_shape = d.batch_shape\n        for shape in sample_shapes:\n            proposed_batch_shape = torch.Size(shape) + original_batch_shape\n            if default:\n                n = TorchDistribution.expand(d, proposed_batch_shape)\n            else:\n                with xfail_if_not_implemented():\n                    n = d.expand(proposed_batch_shape)\n            assert n.batch_shape == proposed_batch_shape\n            with xfail_if_not_implemented():\n                check_sample_shapes(d, n)\n            d = n\n\n\n@pytest.mark.parametrize(""initial_shape, proposed_shape"", [\n    [(2, 1), (4, 3)],\n    [(2, 4), (2, 2, 1)],\n    [(1, 2, 1), (2, 1)],\n])\n@pytest.mark.parametrize(""default"", [False, True])\ndef test_expand_error(dist, initial_shape, proposed_shape, default):\n    for idx in range(dist.get_num_test_data()):\n        small = dist.pyro_dist(**dist.get_dist_params(idx))\n        if default:\n            large = TorchDistribution.expand(small, initial_shape + small.batch_shape)\n        else:\n            with xfail_if_not_implemented():\n                large = small.expand(torch.Size(initial_shape) + small.batch_shape)\n        proposed_batch_shape = torch.Size(proposed_shape) + small.batch_shape\n        if dist.get_test_distribution_name() == \'LKJCorrCholesky\':\n            pytest.skip(\'LKJCorrCholesky can expand to a shape not\' +\n                        \'broadcastable with its original batch_shape.\')\n        with pytest.raises((RuntimeError, ValueError)):\n            large.expand(proposed_batch_shape)\n\n\n@pytest.mark.parametrize(""extra_event_dims,expand_shape"", [\n    (0, [4, 3, 2, 1]),\n    (0, [4, 3, 2, 2]),\n    (1, [5, 4, 3, 2]),\n    (2, [5, 4, 3]),\n])\n@pytest.mark.parametrize(\'default\', [False, True])\ndef test_expand_reshaped_distribution(extra_event_dims, expand_shape, default):\n    probs = torch.ones(1, 6) / 6\n    d = dist.OneHotCategorical(probs)\n    full_shape = torch.Size([4, 1, 1, 1, 6])\n    if default:\n        reshaped_dist = TorchDistribution.expand(d, [4, 1, 1, 1]).to_event(extra_event_dims)\n    else:\n        reshaped_dist = d.expand_by([4, 1, 1]).to_event(extra_event_dims)\n    cut = 4 - extra_event_dims\n    batch_shape, event_shape = full_shape[:cut], full_shape[cut:]\n    assert reshaped_dist.batch_shape == batch_shape\n    assert reshaped_dist.event_shape == event_shape\n    large = reshaped_dist.expand(expand_shape)\n    assert large.batch_shape == torch.Size(expand_shape)\n    assert large.event_shape == torch.Size(event_shape)\n\n    # Throws error when batch shape cannot be broadcasted\n    with pytest.raises((RuntimeError, ValueError)):\n        reshaped_dist.expand(expand_shape + [3])\n\n    # Throws error when trying to shrink existing batch shape\n    with pytest.raises((RuntimeError, ValueError)):\n        large.expand(expand_shape[1:])\n\n\ndef test_expand_enumerate_support():\n    probs = torch.ones(3, 6) / 6\n    d = dist.Categorical(probs)\n    actual_enum_shape = TorchDistribution.expand(d, (4, 3)).enumerate_support(expand=True).shape\n    assert actual_enum_shape == (6, 4, 3)\n'"
tests/distributions/test_empirical.py,51,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.distributions.empirical import Empirical\nfrom tests.common import assert_equal, assert_close\n\n\n@pytest.mark.parametrize(""size"", [[], [1], [2, 3]])\n@pytest.mark.parametrize(""dtype"", [torch.float32, torch.float64])\ndef test_unweighted_mean_and_var(size, dtype):\n    samples = []\n    for i in range(5):\n        samples.append(torch.ones(size, dtype=dtype) * i)\n    samples = torch.stack(samples)\n    empirical_dist = Empirical(samples, torch.ones(5, dtype=dtype))\n    true_mean = torch.ones(size) * 2\n    true_var = torch.ones(size) * 2\n    assert_equal(empirical_dist.mean, true_mean)\n    assert_equal(empirical_dist.variance, true_var)\n\n\n@pytest.mark.parametrize(""batch_shape, event_shape"", [\n    ([], []),\n    ([2], []),\n    ([2], [5]),\n    ([2], [5, 3]),\n    ([2, 5], [3]),\n])\n@pytest.mark.parametrize(""sample_shape"", [[], [20], [20, 3, 4]])\n@pytest.mark.parametrize(""dtype"", [torch.long, torch.float32, torch.float64])\ndef test_unweighted_samples(batch_shape, event_shape, sample_shape, dtype):\n    agg_dim_size = 5\n    # empirical samples with desired shape\n    dim_ordering = list(range(len(batch_shape + event_shape) + 1))  # +1 for agg dim\n    dim_ordering.insert(len(batch_shape), dim_ordering.pop())\n    emp_samples = torch.arange(agg_dim_size, dtype=dtype)\\\n        .expand(batch_shape + event_shape + [agg_dim_size])\\\n        .permute(dim_ordering)\n    # initial weight assignment\n    weights = torch.ones(batch_shape + [agg_dim_size])\n    empirical_dist = Empirical(emp_samples, weights)\n    samples = empirical_dist.sample(sample_shape=torch.Size(sample_shape))\n    assert_equal(samples.size(), torch.Size(sample_shape + batch_shape + event_shape))\n\n\n@pytest.mark.parametrize(""sample, weights, expected_mean, expected_var"", [(\n        torch.tensor([[0., 0., 0.], [1., 1., 1.]]),\n        torch.ones(2),\n        torch.tensor([0.5, 0.5, 0.5]),\n        torch.tensor([0.25, 0.25, 0.25]),\n     ), (\n        torch.tensor([[0., 0., 0.], [1., 1., 1.]]),\n        torch.ones(2, 3),\n        torch.tensor([0., 1.]),\n        torch.tensor([0., 0.]),\n    ),\n])\ndef test_sample_examples(sample, weights, expected_mean, expected_var):\n    emp_dist = Empirical(sample, weights)\n    num_samples = 10000\n    assert_equal(emp_dist.mean, expected_mean)\n    assert_equal(emp_dist.variance, expected_var)\n    emp_samples = emp_dist.sample((num_samples,))\n    assert_close(emp_samples.mean(0), emp_dist.mean, rtol=1e-2)\n    assert_close(emp_samples.var(0), emp_dist.variance, rtol=1e-2)\n\n\n@pytest.mark.parametrize(""batch_shape, event_shape"", [\n    ([], []),\n    ([1], []),\n    ([10], []),\n    ([10, 8], [3]),\n    ([10, 8], [3, 4]),\n])\n@pytest.mark.parametrize(""dtype"", [torch.long, torch.float32, torch.float64])\ndef test_log_prob(batch_shape, event_shape, dtype):\n    samples = []\n    for i in range(5):\n        samples.append(torch.ones(event_shape, dtype=dtype) * i)\n    samples = torch.stack(samples).expand(batch_shape + [5] + event_shape)\n    weights = torch.tensor(1.).expand(batch_shape + [5])\n    empirical_dist = Empirical(samples, weights)\n    sample_to_score = torch.tensor(1, dtype=dtype).expand(batch_shape + event_shape)\n    log_prob = empirical_dist.log_prob(sample_to_score)\n    assert_equal(log_prob, (weights.new_ones(batch_shape + [1]) * 0.2).sum(-1).log())\n\n    # Value outside support returns -Inf\n    sample_to_score = torch.tensor(1, dtype=dtype).expand(batch_shape + event_shape) * 6\n    log_prob = empirical_dist.log_prob(sample_to_score)\n    assert log_prob.shape == torch.Size(batch_shape)\n    assert torch.isinf(log_prob).all()\n\n    # Vectorized ``log_prob`` raises ValueError\n    with pytest.raises(ValueError):\n        sample_to_score = torch.ones([3] + batch_shape + event_shape, dtype=dtype)\n        empirical_dist.log_prob(sample_to_score)\n\n\n@pytest.mark.parametrize(""event_shape"", [[], [1], [2, 3]])\n@pytest.mark.parametrize(""dtype"", [torch.long, torch.float32, torch.float64])\ndef test_weighted_sample_coherence(event_shape, dtype):\n    data = [(1.0, 0.5), (0.0, 1.5), (1.0, 0.5), (0.0, 1.5)]\n    samples, weights = [], []\n    for sample, weight in data:\n        samples.append(sample * torch.ones(event_shape, dtype=dtype))\n        weights.append(torch.tensor(weight).log())\n    samples, weights = torch.stack(samples), torch.stack(weights)\n    empirical_dist = Empirical(samples, weights)\n    assert_equal(empirical_dist.event_shape, torch.Size(event_shape))\n    assert_equal(empirical_dist.sample_size, 4)\n    sample_to_score = torch.ones(event_shape, dtype=dtype) * 1.0\n    assert_equal(empirical_dist.log_prob(sample_to_score), torch.tensor(0.25).log())\n    samples = empirical_dist.sample(sample_shape=torch.Size((1000,)))\n    zeros = torch.zeros(event_shape, dtype=dtype)\n    ones = torch.ones(event_shape, dtype=dtype)\n    num_zeros = samples.eq(zeros).contiguous().view(1000, -1).min(dim=-1)[0].float().sum()\n    num_ones = samples.eq(ones).contiguous().view(1000, -1).min(dim=-1)[0].float().sum()\n    assert_equal(num_zeros.item() / 1000, 0.75, prec=0.02)\n    assert_equal(num_ones.item() / 1000, 0.25, prec=0.02)\n\n\n@pytest.mark.parametrize(""batch_shape"", [[], [1], [2], [2, 3]])\n@pytest.mark.parametrize(""event_shape"", [[], [1], [2, 3]])\n@pytest.mark.parametrize(""dtype"", [torch.long, torch.float32, torch.float64])\ndef test_weighted_mean_var(event_shape, dtype, batch_shape):\n    data = [(1, 0.5), (0, 1.5), (1, 0.5), (0, 1.5)]\n    samples, weights = [], []\n    for sample, weight in data:\n        samples.append(sample * torch.ones(event_shape, dtype=dtype))\n        weight_dtype = dtype if dtype is not torch.long else None\n        weights.append(torch.tensor(weight, dtype=weight_dtype).log())\n    samples = torch.stack(samples).expand(batch_shape + [4] + event_shape)\n    weights = torch.stack(weights).expand(batch_shape + [4])\n    empirical_dist = Empirical(samples, weights)\n    if dtype in (torch.float32, torch.float64):\n        true_mean = torch.ones(batch_shape + event_shape, dtype=dtype) * 0.25\n        true_var = torch.ones(batch_shape + event_shape, dtype=dtype) * 0.1875\n        assert_equal(empirical_dist.mean, true_mean)\n        assert_equal(empirical_dist.variance, true_var)\n    else:\n        with pytest.raises(ValueError):\n            empirical_dist.mean\n            empirical_dist.variance\n\n\ndef test_mean_var_non_nan():\n    true_mean = torch.randn([1, 2, 3])\n    samples, weights = [], []\n    for i in range(10):\n        samples.append(true_mean)\n        weights.append(torch.tensor(-1000.))\n    samples, weights = torch.stack(samples), torch.stack(weights)\n    empirical_dist = Empirical(samples, weights)\n    assert_equal(empirical_dist.mean, true_mean)\n    assert_equal(empirical_dist.variance, torch.zeros_like(true_mean))\n'"
tests/distributions/test_extended.py,18,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nimport pyro.distributions as dist\nfrom pyro.contrib.epidemiology.distributions import set_approx_log_prob_tol\nfrom tests.common import assert_equal\n\n\ndef check_grad(value, *params):\n    grads = grad(value.sum(), params, create_graph=True)\n    assert all(torch.isfinite(g).all() for g in grads)\n\n\n@pytest.mark.parametrize(""tol"", [0., 0.02, 0.05, 0.1])\ndef test_extended_binomial(tol):\n    with set_approx_log_prob_tol(tol):\n        total_count = torch.tensor([0., 1., 2., 10.])\n        probs = torch.tensor([0.5, 0.5, 0.4, 0.2]).requires_grad_()\n\n        d1 = dist.Binomial(total_count, probs)\n        d2 = dist.ExtendedBinomial(total_count, probs)\n        # Check on good data.\n        data = d1.sample((100,))\n        assert_equal(d1.log_prob(data), d2.log_prob(data))\n\n        # Check on extended data.\n        data = torch.arange(-10., 20.).unsqueeze(-1)\n        with pytest.raises(ValueError):\n            d1.log_prob(data)\n        log_prob = d2.log_prob(data)\n        valid = d1.support.check(data)\n        assert ((log_prob > -math.inf) == valid).all()\n        check_grad(log_prob, probs)\n\n        # Check on shape error.\n        with pytest.raises(ValueError):\n            d2.log_prob(torch.tensor([0., 0.]))\n\n        # Check on value error.\n        with pytest.raises(ValueError):\n            d2.log_prob(torch.tensor(0.5))\n\n        # Check on negative total_count.\n        total_count = torch.arange(-10, 0.)\n        probs = torch.tensor(0.5).requires_grad_()\n        d = dist.ExtendedBinomial(total_count, probs)\n        log_prob = d.log_prob(data)\n        assert (log_prob == -math.inf).all()\n        check_grad(log_prob, probs)\n\n\n@pytest.mark.parametrize(""tol"", [0., 0.02, 0.05, 0.1])\ndef test_extended_beta_binomial(tol):\n    with set_approx_log_prob_tol(tol):\n        concentration1 = torch.tensor([0.2, 1.0, 2.0, 1.0]).requires_grad_()\n        concentration0 = torch.tensor([0.2, 0.5, 1.0, 2.0]).requires_grad_()\n        total_count = torch.tensor([0., 1., 2., 10.])\n\n        d1 = dist.BetaBinomial(concentration1, concentration0, total_count)\n        d2 = dist.ExtendedBetaBinomial(concentration1, concentration0, total_count)\n\n        # Check on good data.\n        data = d1.sample((100,))\n        assert_equal(d1.log_prob(data), d2.log_prob(data))\n\n        # Check on extended data.\n        data = torch.arange(-10., 20.).unsqueeze(-1)\n        with pytest.raises(ValueError):\n            d1.log_prob(data)\n        log_prob = d2.log_prob(data)\n        valid = d1.support.check(data)\n        assert ((log_prob > -math.inf) == valid).all()\n        check_grad(log_prob, concentration1, concentration0)\n\n        # Check on shape error.\n        with pytest.raises(ValueError):\n            d2.log_prob(torch.tensor([0., 0.]))\n\n        # Check on value error.\n        with pytest.raises(ValueError):\n            d2.log_prob(torch.tensor(0.5))\n\n        # Check on negative total_count.\n        concentration1 = torch.tensor(1.5).requires_grad_()\n        concentration0 = torch.tensor(1.5).requires_grad_()\n        total_count = torch.arange(-10, 0.)\n        d = dist.ExtendedBetaBinomial(concentration1, concentration0, total_count)\n        log_prob = d.log_prob(data)\n        assert (log_prob == -math.inf).all()\n        check_grad(log_prob, concentration1, concentration0)\n'"
tests/distributions/test_gaussian_mixtures.py,31,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport math\n\nimport torch\n\nimport pytest\nfrom pyro.distributions import MixtureOfDiagNormalsSharedCovariance, GaussianScaleMixture\nfrom pyro.distributions import MixtureOfDiagNormals\nfrom tests.common import assert_equal\n\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.parametrize(\'mix_dist\', [MixtureOfDiagNormals, MixtureOfDiagNormalsSharedCovariance, GaussianScaleMixture])\n@pytest.mark.parametrize(\'K\', [3])\n@pytest.mark.parametrize(\'D\', [2, 4])\n@pytest.mark.parametrize(\'batch_mode\', [True, False])\n@pytest.mark.parametrize(\'flat_logits\', [True, False])\n@pytest.mark.parametrize(\'cost_function\', [\'quadratic\'])\ndef test_mean_gradient(K, D, flat_logits, cost_function, mix_dist, batch_mode):\n    n_samples = 200000\n    if batch_mode:\n        sample_shape = torch.Size(())\n    else:\n        sample_shape = torch.Size((n_samples,))\n    if mix_dist == GaussianScaleMixture:\n        locs = torch.zeros(K, D, requires_grad=True)\n    else:\n        locs = torch.rand(K, D).requires_grad_(True)\n    if mix_dist == GaussianScaleMixture:\n        component_scale = 1.5 * torch.ones(K) + 0.5 * torch.rand(K)\n        component_scale.requires_grad_(True)\n    else:\n        component_scale = torch.ones(K, requires_grad=True)\n    if mix_dist == MixtureOfDiagNormals:\n        coord_scale = torch.ones(K, D) + 0.5 * torch.rand(K, D)\n        coord_scale.requires_grad_(True)\n    else:\n        coord_scale = torch.ones(D) + 0.5 * torch.rand(D)\n        coord_scale.requires_grad_(True)\n    if not flat_logits:\n        component_logits = (1.5 * torch.rand(K)).requires_grad_(True)\n    else:\n        component_logits = (0.1 * torch.rand(K)).requires_grad_(True)\n    omega = (0.2 * torch.ones(D) + 0.1 * torch.rand(D)).requires_grad_(False)\n\n    _pis = torch.exp(component_logits)\n    pis = _pis / _pis.sum()\n\n    if cost_function == \'cosine\':\n        analytic1 = torch.cos((omega * locs).sum(-1))\n        analytic2 = torch.exp(-0.5 * torch.pow(omega * coord_scale * component_scale.unsqueeze(-1), 2.0).sum(-1))\n        analytic = (pis * analytic1 * analytic2).sum()\n        analytic.backward()\n    elif cost_function == \'quadratic\':\n        analytic = torch.pow(coord_scale * component_scale.unsqueeze(-1), 2.0).sum(-1) + torch.pow(locs, 2.0).sum(-1)\n        analytic = (pis * analytic).sum()\n        analytic.backward()\n\n    analytic_grads = {}\n    analytic_grads[\'locs\'] = locs.grad.clone()\n    analytic_grads[\'coord_scale\'] = coord_scale.grad.clone()\n    analytic_grads[\'component_logits\'] = component_logits.grad.clone()\n    analytic_grads[\'component_scale\'] = component_scale.grad.clone()\n\n    assert locs.grad.shape == locs.shape\n    assert coord_scale.grad.shape == coord_scale.shape\n    assert component_logits.grad.shape == component_logits.shape\n    assert component_scale.grad.shape == component_scale.shape\n\n    coord_scale.grad.zero_()\n    component_logits.grad.zero_()\n    locs.grad.zero_()\n    component_scale.grad.zero_()\n\n    if mix_dist == MixtureOfDiagNormalsSharedCovariance:\n        params = {\'locs\': locs, \'coord_scale\': coord_scale, \'component_logits\': component_logits}\n        if batch_mode:\n            locs = locs.unsqueeze(0).expand(n_samples, K, D)\n            coord_scale = coord_scale.unsqueeze(0).expand(n_samples, D)\n            component_logits = component_logits.unsqueeze(0).expand(n_samples, K)\n            dist_params = {\'locs\': locs, \'coord_scale\': coord_scale, \'component_logits\': component_logits}\n        else:\n            dist_params = params\n    elif mix_dist == MixtureOfDiagNormals:\n        params = {\'locs\': locs, \'coord_scale\': coord_scale, \'component_logits\': component_logits}\n        if batch_mode:\n            locs = locs.unsqueeze(0).expand(n_samples, K, D)\n            coord_scale = coord_scale.unsqueeze(0).expand(n_samples, K, D)\n            component_logits = component_logits.unsqueeze(0).expand(n_samples, K)\n            dist_params = {\'locs\': locs, \'coord_scale\': coord_scale, \'component_logits\': component_logits}\n        else:\n            dist_params = params\n    elif mix_dist == GaussianScaleMixture:\n        params = {\'coord_scale\': coord_scale, \'component_logits\': component_logits, \'component_scale\': component_scale}\n        if batch_mode:\n            return  # distribution does not support batched parameters\n        else:\n            dist_params = params\n\n    dist = mix_dist(**dist_params)\n    z = dist.rsample(sample_shape=sample_shape)\n    assert z.shape == (n_samples, D)\n    if cost_function == \'cosine\':\n        cost = torch.cos((omega * z).sum(-1)).sum() / float(n_samples)\n    elif cost_function == \'quadratic\':\n        cost = torch.pow(z, 2.0).sum() / float(n_samples)\n    cost.backward()\n\n    assert_equal(analytic, cost, prec=0.1,\n                 msg=\'bad cost function evaluation for {} test (expected {}, got {})\'.format(\n                     mix_dist.__name__, analytic.item(), cost.item()))\n    logger.debug(""analytic_grads_logit: {}""\n                 .format(analytic_grads[\'component_logits\'].detach().cpu().numpy()))\n\n    for param_name, param in params.items():\n        assert_equal(param.grad, analytic_grads[param_name], prec=0.1,\n                     msg=\'bad {} grad for {} (expected {}, got {})\'.format(\n                         param_name, mix_dist.__name__, analytic_grads[param_name], param.grad))\n\n\n@pytest.mark.parametrize(\'batch_size\', [1, 3])\ndef test_mix_of_diag_normals_shared_cov_log_prob(batch_size):\n    locs = torch.tensor([[-1.0, -1.0], [1.0, 1.0]])\n    sigmas = torch.tensor([2.0, 2.0])\n    logits = torch.tensor([math.log(0.25), math.log(0.75)])\n    value = torch.tensor([0.5, 0.5])\n    if batch_size > 1:\n        locs = locs.unsqueeze(0).expand(batch_size, 2, 2)\n        sigmas = sigmas.unsqueeze(0).expand(batch_size, 2)\n        logits = logits.unsqueeze(0).expand(batch_size, 2)\n        value = value.unsqueeze(0).expand(batch_size, 2)\n    dist = MixtureOfDiagNormalsSharedCovariance(locs, sigmas, logits)\n    log_prob = dist.log_prob(value)\n    correct_log_prob = 0.25 * math.exp(- 2.25 / 4.0)\n    correct_log_prob += 0.75 * math.exp(- 0.25 / 4.0)\n    correct_log_prob /= 8.0 * math.pi\n    correct_log_prob = math.log(correct_log_prob)\n    if batch_size > 1:\n        correct_log_prob = [correct_log_prob] * batch_size\n    correct_log_prob = torch.tensor(correct_log_prob)\n    assert_equal(log_prob, correct_log_prob, msg=\'bad log prob for MixtureOfDiagNormalsSharedCovariance\')\n\n\ndef test_gsm_log_prob():\n    sigmas = torch.tensor([2.0, 2.0])\n    component_scale = torch.tensor([1.5, 2.5])\n    logits = torch.tensor([math.log(0.25), math.log(0.75)])\n    dist = GaussianScaleMixture(sigmas, logits, component_scale)\n    value = torch.tensor([math.sqrt(0.33), math.sqrt(0.67)])\n    log_prob = dist.log_prob(value).item()\n    correct_log_prob = 0.25 * math.exp(-0.50 / (4.0 * 2.25)) / 2.25\n    correct_log_prob += 0.75 * math.exp(-0.50 / (4.0 * 6.25)) / 6.25\n    correct_log_prob /= (2.0 * math.pi) * 4.0\n    correct_log_prob = math.log(correct_log_prob)\n    assert_equal(log_prob, correct_log_prob, msg=\'bad log prob for GaussianScaleMixture\')\n\n\n@pytest.mark.parametrize(\'batch_size\', [1, 3])\ndef test_mix_of_diag_normals_log_prob(batch_size):\n    sigmas = torch.tensor([[2.0, 1.5], [1.5, 2.0]])\n    locs = torch.tensor([[0.0, 1.0], [-1.0, 0.0]])\n    logits = torch.tensor([math.log(0.25), math.log(0.75)])\n    value = torch.tensor([0.5, 0.25])\n    if batch_size > 1:\n        locs = locs.unsqueeze(0).expand(batch_size, 2, 2)\n        sigmas = sigmas.unsqueeze(0).expand(batch_size, 2, 2)\n        logits = logits.unsqueeze(0).expand(batch_size, 2)\n        value = value.unsqueeze(0).expand(batch_size, 2)\n    dist = MixtureOfDiagNormals(locs, sigmas, logits)\n    log_prob = dist.log_prob(value)\n    correct_log_prob = 0.25 * math.exp(-0.5 * (0.25 / 4.0 + 0.5625 / 2.25)) / 3.0\n    correct_log_prob += 0.75 * math.exp(-0.5 * (2.25 / 2.25 + 0.0625 / 4.0)) / 3.0\n    correct_log_prob /= (2.0 * math.pi)\n    correct_log_prob = math.log(correct_log_prob)\n    if batch_size > 1:\n        correct_log_prob = [correct_log_prob] * batch_size\n    correct_log_prob = torch.tensor(correct_log_prob)\n    assert_equal(log_prob, correct_log_prob, msg=\'bad log prob for MixtureOfDiagNormals\')\n'"
tests/distributions/test_haar.py,1,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pytest\nfrom pyro.distributions.transforms import HaarTransform\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize('size', [1, 3, 4, 7, 8, 9])\ndef test_haar_ortho(size):\n    haar = HaarTransform()\n    eye = torch.eye(size)\n    mat = haar(eye)\n    assert_equal(eye, mat @ mat.t())\n"""
tests/distributions/test_hmm.py,47,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport operator\nfrom functools import reduce\n\nimport opt_einsum\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions.hmm import (_sequential_gamma_gaussian_tensordot, _sequential_gaussian_filter_sample,\n                                    _sequential_gaussian_tensordot, _sequential_logmatmulexp)\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.infer import TraceEnum_ELBO, config_enumerate\nfrom pyro.ops.gamma_gaussian import (gamma_and_mvn_to_gamma_gaussian, gamma_gaussian_tensordot,\n                                     matrix_and_mvn_to_gamma_gaussian)\nfrom pyro.ops.gaussian import gaussian_tensordot, matrix_and_mvn_to_gaussian, mvn_to_gaussian\nfrom pyro.ops.indexing import Vindex\nfrom tests.common import assert_close\nfrom tests.ops.gamma_gaussian import assert_close_gamma_gaussian, random_gamma, random_gamma_gaussian\nfrom tests.ops.gaussian import assert_close_gaussian, random_gaussian, random_mvn\n\nlogger = logging.getLogger(__name__)\n\n\ndef check_expand(old_dist, old_data):\n    new_batch_shape = (2,) + old_dist.batch_shape\n    new_dist = old_dist.expand(new_batch_shape)\n    assert new_dist.batch_shape == new_batch_shape\n\n    old_log_prob = new_dist.log_prob(old_data)\n    assert old_log_prob.shape == new_batch_shape\n\n    new_data = old_data.expand(new_batch_shape + new_dist.event_shape)\n    new_log_prob = new_dist.log_prob(new_data)\n    assert_close(old_log_prob, new_log_prob)\n    assert new_dist.log_prob(new_data).shape == new_batch_shape\n\n\n@pytest.mark.parametrize(\'num_steps\', list(range(1, 20)))\n@pytest.mark.parametrize(\'state_dim\', [2, 3])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 4)], ids=str)\ndef test_sequential_logmatmulexp(batch_shape, state_dim, num_steps):\n    logits = torch.randn(batch_shape + (num_steps, state_dim, state_dim))\n    actual = _sequential_logmatmulexp(logits)\n    assert actual.shape == batch_shape + (state_dim, state_dim)\n\n    # Check against einsum.\n    operands = list(logits.unbind(-3))\n    symbol = (opt_einsum.get_symbol(i) for i in range(1000))\n    batch_symbols = \'\'.join(next(symbol) for _ in batch_shape)\n    state_symbols = [next(symbol) for _ in range(num_steps + 1)]\n    equation = (\',\'.join(batch_symbols + state_symbols[t] + state_symbols[t + 1]\n                         for t in range(num_steps)) +\n                \'->\' + batch_symbols + state_symbols[0] + state_symbols[-1])\n    expected = opt_einsum.contract(equation, *operands, backend=\'pyro.ops.einsum.torch_log\')\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'num_steps\', list(range(1, 20)))\n@pytest.mark.parametrize(\'state_dim\', [1, 2, 3])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 4)], ids=str)\ndef test_sequential_gaussian_tensordot(batch_shape, state_dim, num_steps):\n    g = random_gaussian(batch_shape + (num_steps,), state_dim + state_dim)\n    actual = _sequential_gaussian_tensordot(g)\n    assert actual.dim() == g.dim()\n    assert actual.batch_shape == batch_shape\n\n    # Check against hand computation.\n    expected = g[..., 0]\n    for t in range(1, num_steps):\n        expected = gaussian_tensordot(expected, g[..., t], state_dim)\n    assert_close_gaussian(actual, expected)\n\n\n@pytest.mark.parametrize(\'num_steps\', list(range(1, 20)))\n@pytest.mark.parametrize(\'state_dim\', [1, 2, 3])\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'sample_shape\', [(), (4,), (3, 2)], ids=str)\ndef test_sequential_gaussian_filter_sample(sample_shape, batch_shape, state_dim, num_steps):\n    init = random_gaussian(batch_shape, state_dim)\n    trans = random_gaussian(batch_shape + (num_steps,), state_dim + state_dim)\n    sample = _sequential_gaussian_filter_sample(init, trans, sample_shape)\n    assert sample.shape == sample_shape + batch_shape + (num_steps, state_dim)\n\n\n@pytest.mark.parametrize(\'num_steps\', list(range(1, 20)))\n@pytest.mark.parametrize(\'state_dim\', [1, 2, 3])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 4)], ids=str)\ndef test_sequential_gamma_gaussian_tensordot(batch_shape, state_dim, num_steps):\n    g = random_gamma_gaussian(batch_shape + (num_steps,), state_dim + state_dim)\n    actual = _sequential_gamma_gaussian_tensordot(g)\n    assert actual.dim() == g.dim()\n    assert actual.batch_shape == batch_shape\n\n    # Check against hand computation.\n    expected = g[..., 0]\n    for t in range(1, num_steps):\n        expected = gamma_gaussian_tensordot(expected, g[..., t], state_dim)\n    assert_close_gamma_gaussian(actual, expected)\n\n\n@pytest.mark.parametrize(\'state_dim\', [2, 3])\n@pytest.mark.parametrize(\'event_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'ok,init_shape,trans_shape,obs_shape\', [\n    (True, (), (), (1,)),\n    (True, (), (1,), (1,)),\n    (True, (), (), (7,)),\n    (True, (), (7,), (7,)),\n    (True, (), (1,), (7,)),\n    (True, (), (7,), (11, 7)),\n    (True, (), (11, 7), (7,)),\n    (True, (), (11, 7), (11, 7)),\n    (True, (11,), (7,), (7,)),\n    (True, (11,), (7,), (11, 7)),\n    (True, (11,), (11, 7), (7,)),\n    (True, (11,), (11, 7), (11, 7)),\n    (True, (4, 1, 1), (3, 1, 7), (2, 7)),\n    (False, (), (1,), ()),\n    (False, (), (7,), ()),\n    (False, (), (7,), (1,)),\n    (False, (), (7,), (6,)),\n    (False, (3,), (4, 7), (7,)),\n    (False, (3,), (7,), (4, 7)),\n    (False, (), (3, 7), (4, 7)),\n], ids=str)\ndef test_discrete_hmm_shape(ok, init_shape, trans_shape, obs_shape, event_shape, state_dim):\n    init_logits = torch.randn(init_shape + (state_dim,))\n    trans_logits = torch.randn(trans_shape + (state_dim, state_dim))\n    obs_logits = torch.randn(obs_shape + (state_dim,) + event_shape)\n    obs_dist = dist.Bernoulli(logits=obs_logits).to_event(len(event_shape))\n    data = obs_dist.sample()[(slice(None),) * len(obs_shape) + (0,)]\n\n    if not ok:\n        with pytest.raises(ValueError):\n            d = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n            d.log_prob(data)\n        return\n\n    d = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n\n    actual = d.log_prob(data)\n    expected_shape = broadcast_shape(init_shape, trans_shape[:-1], obs_shape[:-1])\n    assert actual.shape == expected_shape\n    check_expand(d, data)\n\n    final = d.filter(data)\n    assert isinstance(final, dist.Categorical)\n    assert final.batch_shape == d.batch_shape\n    assert final.event_shape == ()\n    assert final.support.upper_bound == state_dim - 1\n\n\n@pytest.mark.parametrize(\'event_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'state_dim\', [2, 3])\n@pytest.mark.parametrize(\'num_steps\', [1, 2, 3])\n@pytest.mark.parametrize(\'init_shape,trans_shape,obs_shape\', [\n    ((), (), ()),\n    ((), (1,), ()),\n    ((), (), (1,)),\n    ((), (1,), (7, 1)),\n    ((), (7, 1), (1,)),\n    ((), (7, 1), (7, 1)),\n    ((7,), (1,), (1,)),\n    ((7,), (1,), (7, 1)),\n    ((7,), (7, 1), (1,)),\n    ((7,), (7, 1), (7, 1)),\n    ((4, 1, 1), (3, 1, 1), (2, 1)),\n], ids=str)\ndef test_discrete_hmm_homogeneous_trick(init_shape, trans_shape, obs_shape, event_shape, state_dim, num_steps):\n    batch_shape = broadcast_shape(init_shape, trans_shape[:-1], obs_shape[:-1])\n    init_logits = torch.randn(init_shape + (state_dim,))\n    trans_logits = torch.randn(trans_shape + (state_dim, state_dim))\n    obs_logits = torch.randn(obs_shape + (state_dim,) + event_shape)\n    obs_dist = dist.Bernoulli(logits=obs_logits).to_event(len(event_shape))\n\n    d = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n    assert d.event_shape == (1,) + event_shape\n\n    data = obs_dist.expand(batch_shape + (num_steps, state_dim)).sample()\n    data = data[(slice(None),) * (len(batch_shape) + 1) + (0,)]\n    assert data.shape == batch_shape + (num_steps,) + event_shape\n    actual = d.log_prob(data)\n    assert actual.shape == batch_shape\n\n\ndef empty_guide(*args, **kwargs):\n    pass\n\n\n@pytest.mark.parametrize(\'num_steps\', list(range(1, 10)))\ndef test_discrete_hmm_categorical(num_steps):\n    state_dim = 3\n    obs_dim = 4\n    init_logits = torch.randn(state_dim)\n    trans_logits = torch.randn(num_steps, state_dim, state_dim)\n    obs_dist = dist.Categorical(logits=torch.randn(num_steps, state_dim, obs_dim))\n    d = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n    data = dist.Categorical(logits=torch.zeros(num_steps, obs_dim)).sample()\n    actual = d.log_prob(data)\n    assert actual.shape == d.batch_shape\n    check_expand(d, data)\n\n    # Check loss against TraceEnum_ELBO.\n    @config_enumerate\n    def model(data):\n        x = pyro.sample(""x_init"", dist.Categorical(logits=init_logits))\n        for t in range(num_steps):\n            x = pyro.sample(""x_{}"".format(t),\n                            dist.Categorical(logits=Vindex(trans_logits)[..., t, x, :]))\n            pyro.sample(""obs_{}"".format(t),\n                        dist.Categorical(logits=Vindex(obs_dist.logits)[..., t, x, :]),\n                        obs=data[..., t])\n\n    expected_loss = TraceEnum_ELBO().loss(model, empty_guide, data)\n    actual_loss = -float(actual.sum())\n    assert_close(actual_loss, expected_loss)\n\n\n@pytest.mark.parametrize(\'num_steps\', list(range(1, 10)))\ndef test_discrete_hmm_diag_normal(num_steps):\n    state_dim = 3\n    event_size = 2\n    init_logits = torch.randn(state_dim)\n    trans_logits = torch.randn(num_steps, state_dim, state_dim)\n    loc = torch.randn(num_steps, state_dim, event_size)\n    scale = torch.randn(num_steps, state_dim, event_size).exp()\n    obs_dist = dist.Normal(loc, scale).to_event(1)\n    d = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n    data = obs_dist.sample()[:, 0]\n    actual = d.log_prob(data)\n    assert actual.shape == d.batch_shape\n    check_expand(d, data)\n\n    # Check loss against TraceEnum_ELBO.\n    @config_enumerate\n    def model(data):\n        x = pyro.sample(""x_init"", dist.Categorical(logits=init_logits))\n        for t in range(num_steps):\n            x = pyro.sample(""x_{}"".format(t),\n                            dist.Categorical(logits=Vindex(trans_logits)[..., t, x, :]))\n            pyro.sample(""obs_{}"".format(t),\n                        dist.Normal(Vindex(loc)[..., t, x, :],\n                                    Vindex(scale)[..., t, x, :]).to_event(1),\n                        obs=data[..., t, :])\n\n    expected_loss = TraceEnum_ELBO().loss(model, empty_guide, data)\n    actual_loss = -float(actual.sum())\n    assert_close(actual_loss, expected_loss)\n\n\n@pytest.mark.parametrize(\'obs_dim\', [1, 2])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 3])\n@pytest.mark.parametrize(\'init_shape,trans_mat_shape,trans_mvn_shape,obs_mat_shape,obs_mvn_shape\', [\n    ((), (), (), (), ()),\n    ((), (6,), (), (), ()),\n    ((), (), (6,), (), ()),\n    ((), (), (), (6,), ()),\n    ((), (), (), (), (6,)),\n    ((), (6,), (6,), (6,), (6,)),\n    ((5,), (6,), (), (), ()),\n    ((), (5, 1), (6,), (), ()),\n    ((), (), (5, 1), (6,), ()),\n    ((), (), (), (5, 1), (6,)),\n    ((), (6,), (5, 1), (), ()),\n    ((), (), (6,), (5, 1), ()),\n    ((), (), (), (6,), (5, 1)),\n    ((5,), (), (), (), (6,)),\n    ((5,), (5, 6), (5, 6), (5, 6), (5, 6)),\n], ids=str)\n@pytest.mark.parametrize(""diag"", [False, True], ids=[""full"", ""diag""])\ndef test_gaussian_hmm_shape(diag, init_shape, trans_mat_shape, trans_mvn_shape,\n                            obs_mat_shape, obs_mvn_shape, hidden_dim, obs_dim):\n    init_dist = random_mvn(init_shape, hidden_dim)\n    trans_mat = torch.randn(trans_mat_shape + (hidden_dim, hidden_dim))\n    trans_dist = random_mvn(trans_mvn_shape, hidden_dim)\n    obs_mat = torch.randn(obs_mat_shape + (hidden_dim, obs_dim))\n    obs_dist = random_mvn(obs_mvn_shape, obs_dim)\n    if diag:\n        scale = obs_dist.scale_tril.diagonal(dim1=-2, dim2=-1)\n        obs_dist = dist.Normal(obs_dist.loc, scale).to_event(1)\n    d = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist,\n                         duration=6)\n\n    shape = broadcast_shape(init_shape + (6,),\n                            trans_mat_shape,\n                            trans_mvn_shape,\n                            obs_mat_shape,\n                            obs_mvn_shape)\n    expected_batch_shape, time_shape = shape[:-1], shape[-1:]\n    expected_event_shape = time_shape + (obs_dim,)\n    assert d.batch_shape == expected_batch_shape\n    assert d.event_shape == expected_event_shape\n\n    data = obs_dist.expand(shape).sample()\n    assert data.shape == d.shape()\n    actual = d.log_prob(data)\n    assert actual.shape == expected_batch_shape\n    check_expand(d, data)\n\n    x = d.rsample()\n    assert x.shape == d.shape()\n    x = d.rsample((6,))\n    assert x.shape == (6,) + d.shape()\n    x = d.expand((6, 5)).rsample()\n    assert x.shape == (6, 5) + d.event_shape\n\n    likelihood = dist.Normal(data, 1).to_event(2)\n    p, log_normalizer = d.conjugate_update(likelihood)\n    assert p.batch_shape == d.batch_shape\n    assert p.event_shape == d.event_shape\n    x = p.rsample()\n    assert x.shape == d.shape()\n    x = p.rsample((6,))\n    assert x.shape == (6,) + d.shape()\n    x = p.expand((6, 5)).rsample()\n    assert x.shape == (6, 5) + d.event_shape\n\n    final = d.filter(data)\n    assert isinstance(final, dist.MultivariateNormal)\n    assert final.batch_shape == d.batch_shape\n    assert final.event_shape == (hidden_dim,)\n\n    z = d.rsample_posterior(data)\n    assert z.shape == expected_batch_shape + time_shape + (hidden_dim,)\n\n    for t in range(1, d.duration - 1):\n        f = d.duration - t\n        d2 = d.prefix_condition(data[..., :t, :])\n        assert d2.batch_shape == d.batch_shape\n        assert d2.event_shape == (f, obs_dim)\n\n\ndef test_gaussian_hmm_high_obs_dim():\n    hidden_dim = 1\n    obs_dim = 1000\n    duration = 10\n    sample_shape = (100,)\n    init_dist = random_mvn((), hidden_dim)\n    trans_mat = torch.randn((duration,) + (hidden_dim, hidden_dim))\n    trans_dist = random_mvn((duration,), hidden_dim)\n    obs_mat = torch.randn((duration,) + (hidden_dim, obs_dim))\n    loc = torch.randn((duration, obs_dim))\n    scale = torch.randn((duration, obs_dim)).exp()\n    obs_dist = dist.Normal(loc, scale).to_event(1)\n    d = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist,\n                         duration=duration)\n    x = d.rsample(sample_shape)\n    assert x.shape == sample_shape + (duration, obs_dim)\n\n\n@pytest.mark.parametrize(\'sample_shape\', [(), (5,)], ids=str)\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'obs_dim\', [1, 2])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 2])\n@pytest.mark.parametrize(\'num_steps\', [1, 2, 3, 4])\n@pytest.mark.parametrize(""diag"", [False, True], ids=[""full"", ""diag""])\ndef test_gaussian_hmm_distribution(diag, sample_shape, batch_shape, num_steps, hidden_dim, obs_dim):\n    init_dist = random_mvn(batch_shape, hidden_dim)\n    trans_mat = torch.randn(batch_shape + (num_steps, hidden_dim, hidden_dim))\n    trans_dist = random_mvn(batch_shape + (num_steps,), hidden_dim)\n    obs_mat = torch.randn(batch_shape + (num_steps, hidden_dim, obs_dim))\n    obs_dist = random_mvn(batch_shape + (num_steps,), obs_dim)\n    if diag:\n        scale = obs_dist.scale_tril.diagonal(dim1=-2, dim2=-1)\n        obs_dist = dist.Normal(obs_dist.loc, scale).to_event(1)\n    d = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist, duration=num_steps)\n    if diag:\n        obs_mvn = dist.MultivariateNormal(obs_dist.base_dist.loc,\n                                          scale_tril=obs_dist.base_dist.scale.diag_embed())\n    else:\n        obs_mvn = obs_dist\n    data = obs_dist.sample(sample_shape)\n    assert data.shape == sample_shape + d.shape()\n    actual_log_prob = d.log_prob(data)\n\n    # Compare against hand-computed density.\n    # We will construct enormous unrolled joint gaussians with shapes:\n    #       t | 0 1 2 3 1 2 3      T = 3 in this example\n    #   ------+-----------------------------------------\n    #    init | H\n    #   trans | H H H H            H = hidden\n    #     obs |   H H H O O O      O = observed\n    #    like |         O O O\n    # and then combine these using gaussian_tensordot().\n    T = num_steps\n    init = mvn_to_gaussian(init_dist)\n    trans = matrix_and_mvn_to_gaussian(trans_mat, trans_dist)\n    obs = matrix_and_mvn_to_gaussian(obs_mat, obs_mvn)\n    like_dist = dist.Normal(torch.randn(data.shape), 1).to_event(2)\n    like = mvn_to_gaussian(like_dist)\n\n    unrolled_trans = reduce(operator.add, [\n        trans[..., t].event_pad(left=t * hidden_dim, right=(T - t - 1) * hidden_dim)\n        for t in range(T)\n    ])\n    unrolled_obs = reduce(operator.add, [\n        obs[..., t].event_pad(left=t * obs.dim(), right=(T - t - 1) * obs.dim())\n        for t in range(T)\n    ])\n    unrolled_like = reduce(operator.add, [\n        like[..., t].event_pad(left=t * obs_dim, right=(T - t - 1) * obs_dim)\n        for t in range(T)\n    ])\n    # Permute obs from HOHOHO to HHHOOO.\n    perm = torch.cat([torch.arange(hidden_dim) + t * obs.dim() for t in range(T)] +\n                     [torch.arange(obs_dim) + hidden_dim + t * obs.dim() for t in range(T)])\n    unrolled_obs = unrolled_obs.event_permute(perm)\n    unrolled_data = data.reshape(data.shape[:-2] + (T * obs_dim,))\n\n    assert init.dim() == hidden_dim\n    assert unrolled_trans.dim() == (1 + T) * hidden_dim\n    assert unrolled_obs.dim() == T * (hidden_dim + obs_dim)\n    logp = gaussian_tensordot(init, unrolled_trans, hidden_dim)\n    logp = gaussian_tensordot(logp, unrolled_obs, T * hidden_dim)\n    expected_log_prob = logp.log_density(unrolled_data)\n    assert_close(actual_log_prob, expected_log_prob)\n\n    d_posterior, log_normalizer = d.conjugate_update(like_dist)\n    assert_close(d.log_prob(data) + like_dist.log_prob(data),\n                 d_posterior.log_prob(data) + log_normalizer)\n\n    if batch_shape or sample_shape:\n        return\n\n    # Test mean and covariance.\n    prior = ""prior"", d, logp\n    posterior = ""posterior"", d_posterior, logp + unrolled_like\n    for name, d, g in [prior, posterior]:\n        logging.info(""testing {} moments"".format(name))\n        with torch.no_grad():\n            num_samples = 100000\n            samples = d.sample([num_samples]).reshape(num_samples, T * obs_dim)\n            actual_mean = samples.mean(0)\n            delta = samples - actual_mean\n            actual_cov = (delta.unsqueeze(-1) * delta.unsqueeze(-2)).mean(0)\n            actual_std = actual_cov.diagonal(dim1=-2, dim2=-1).sqrt()\n            actual_corr = actual_cov / (actual_std.unsqueeze(-1) * actual_std.unsqueeze(-2))\n\n            expected_cov = g.precision.cholesky().cholesky_inverse()\n            expected_mean = expected_cov.matmul(g.info_vec.unsqueeze(-1)).squeeze(-1)\n            expected_std = expected_cov.diagonal(dim1=-2, dim2=-1).sqrt()\n            expected_corr = expected_cov / (expected_std.unsqueeze(-1) * expected_std.unsqueeze(-2))\n\n            assert_close(actual_mean, expected_mean, atol=0.05, rtol=0.02)\n            assert_close(actual_std, expected_std, atol=0.05, rtol=0.02)\n            assert_close(actual_corr, expected_corr, atol=0.02)\n\n\n@pytest.mark.parametrize(\'obs_dim\', [1, 2, 3])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 2, 3])\n@pytest.mark.parametrize(\'init_shape,trans_shape,obs_shape\', [\n    ((), (7,), ()),\n    ((), (), (7,)),\n    ((), (7,), (1,)),\n    ((), (1,), (7,)),\n    ((), (7,), (11, 7)),\n    ((), (11, 7), (7,)),\n    ((), (11, 7), (11, 7)),\n    ((11,), (7,), (7,)),\n    ((11,), (7,), (11, 7)),\n    ((11,), (11, 7), (7,)),\n    ((11,), (11, 7), (11, 7)),\n    ((4, 1, 1), (3, 1, 7), (2, 7)),\n], ids=str)\ndef test_gaussian_mrf_shape(init_shape, trans_shape, obs_shape, hidden_dim, obs_dim):\n    init_dist = random_mvn(init_shape, hidden_dim)\n    trans_dist = random_mvn(trans_shape, hidden_dim + hidden_dim)\n    obs_dist = random_mvn(obs_shape, hidden_dim + obs_dim)\n    d = dist.GaussianMRF(init_dist, trans_dist, obs_dist)\n\n    shape = broadcast_shape(init_shape + (1,), trans_shape, obs_shape)\n    expected_batch_shape, time_shape = shape[:-1], shape[-1:]\n    expected_event_shape = time_shape + (obs_dim,)\n    assert d.batch_shape == expected_batch_shape\n    assert d.event_shape == expected_event_shape\n\n    data = obs_dist.expand(shape).sample()[..., hidden_dim:]\n    assert data.shape == d.shape()\n    actual = d.log_prob(data)\n    assert actual.shape == expected_batch_shape\n    check_expand(d, data)\n\n\n@pytest.mark.parametrize(\'sample_shape\', [(), (5,)], ids=str)\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'obs_dim\', [1, 2])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 2])\n@pytest.mark.parametrize(\'num_steps\', [1, 2, 3, 4])\ndef test_gaussian_mrf_log_prob(sample_shape, batch_shape, num_steps, hidden_dim, obs_dim):\n    init_dist = random_mvn(batch_shape, hidden_dim)\n    trans_dist = random_mvn(batch_shape + (num_steps,), hidden_dim + hidden_dim)\n    obs_dist = random_mvn(batch_shape + (num_steps,), hidden_dim + obs_dim)\n    d = dist.GaussianMRF(init_dist, trans_dist, obs_dist)\n    data = obs_dist.sample(sample_shape)[..., hidden_dim:]\n    assert data.shape == sample_shape + d.shape()\n    actual_log_prob = d.log_prob(data)\n\n    # Compare against hand-computed density.\n    # We will construct enormous unrolled joint gaussians with shapes:\n    #       t | 0 1 2 3 1 2 3      T = 3 in this example\n    #   ------+-----------------------------------------\n    #    init | H\n    #   trans | H H H H            H = hidden\n    #     obs |   H H H O O O      O = observed\n    # and then combine these using gaussian_tensordot().\n    T = num_steps\n    init = mvn_to_gaussian(init_dist)\n    trans = mvn_to_gaussian(trans_dist)\n    obs = mvn_to_gaussian(obs_dist)\n\n    unrolled_trans = reduce(operator.add, [\n        trans[..., t].event_pad(left=t * hidden_dim, right=(T - t - 1) * hidden_dim)\n        for t in range(T)\n    ])\n    unrolled_obs = reduce(operator.add, [\n        obs[..., t].event_pad(left=t * obs.dim(), right=(T - t - 1) * obs.dim())\n        for t in range(T)\n    ])\n    # Permute obs from HOHOHO to HHHOOO.\n    perm = torch.cat([torch.arange(hidden_dim) + t * obs.dim() for t in range(T)] +\n                     [torch.arange(obs_dim) + hidden_dim + t * obs.dim() for t in range(T)])\n    unrolled_obs = unrolled_obs.event_permute(perm)\n    unrolled_data = data.reshape(data.shape[:-2] + (T * obs_dim,))\n\n    assert init.dim() == hidden_dim\n    assert unrolled_trans.dim() == (1 + T) * hidden_dim\n    assert unrolled_obs.dim() == T * (hidden_dim + obs_dim)\n    logp_h = gaussian_tensordot(init, unrolled_trans, hidden_dim)\n    logp_oh = gaussian_tensordot(logp_h, unrolled_obs, T * hidden_dim)\n    logp_h += unrolled_obs.marginalize(right=T * obs_dim)\n    expected_log_prob = logp_oh.log_density(unrolled_data) - logp_h.event_logsumexp()\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(\'sample_shape\', [(), (5,)], ids=str)\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'obs_dim\', [1, 2])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 2])\n@pytest.mark.parametrize(\'num_steps\', [1, 2, 3, 4])\ndef test_gaussian_mrf_log_prob_block_diag(sample_shape, batch_shape, num_steps, hidden_dim, obs_dim):\n    # Construct a block-diagonal obs dist, so observations are independent of hidden state.\n    obs_dist = random_mvn(batch_shape + (num_steps,), hidden_dim + obs_dim)\n    precision = obs_dist.precision_matrix\n    precision[..., :hidden_dim, hidden_dim:] = 0\n    precision[..., hidden_dim:, :hidden_dim] = 0\n    obs_dist = dist.MultivariateNormal(obs_dist.loc, precision_matrix=precision)\n    marginal_obs_dist = dist.MultivariateNormal(\n        obs_dist.loc[..., hidden_dim:],\n        precision_matrix=precision[..., hidden_dim:, hidden_dim:])\n\n    init_dist = random_mvn(batch_shape, hidden_dim)\n    trans_dist = random_mvn(batch_shape + (num_steps,), hidden_dim + hidden_dim)\n    d = dist.GaussianMRF(init_dist, trans_dist, obs_dist)\n    data = obs_dist.sample(sample_shape)[..., hidden_dim:]\n    assert data.shape == sample_shape + d.shape()\n    actual_log_prob = d.log_prob(data)\n    expected_log_prob = marginal_obs_dist.log_prob(data).sum(-1)\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(\'obs_dim\', [1, 2])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 3])\n@pytest.mark.parametrize(\'scale_shape,init_shape,trans_mat_shape,trans_mvn_shape,obs_mat_shape,obs_mvn_shape\', [\n    ((5,), (), (6,), (), (), ()),\n    ((), (), (6,), (), (), ()),\n    ((), (), (), (6,), (), ()),\n    ((), (), (), (), (6,), ()),\n    ((), (), (), (), (), (6,)),\n    ((), (), (6,), (6,), (6,), (6,)),\n    ((), (5,), (6,), (), (), ()),\n    ((), (), (5, 1), (6,), (), ()),\n    ((), (), (), (5, 1), (6,), ()),\n    ((), (), (), (), (5, 1), (6,)),\n    ((), (), (6,), (5, 1), (), ()),\n    ((), (), (), (6,), (5, 1), ()),\n    ((), (), (), (), (6,), (5, 1)),\n    ((), (5,), (), (), (), (6,)),\n    ((5,), (5,), (5, 6), (5, 6), (5, 6), (5, 6)),\n], ids=str)\ndef test_gamma_gaussian_hmm_shape(scale_shape, init_shape, trans_mat_shape, trans_mvn_shape,\n                                  obs_mat_shape, obs_mvn_shape, hidden_dim, obs_dim):\n    init_dist = random_mvn(init_shape, hidden_dim)\n    trans_mat = torch.randn(trans_mat_shape + (hidden_dim, hidden_dim))\n    trans_dist = random_mvn(trans_mvn_shape, hidden_dim)\n    obs_mat = torch.randn(obs_mat_shape + (hidden_dim, obs_dim))\n    obs_dist = random_mvn(obs_mvn_shape, obs_dim)\n    scale_dist = random_gamma(scale_shape)\n    d = dist.GammaGaussianHMM(scale_dist, init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n\n    shape = broadcast_shape(scale_shape + (1,),\n                            init_shape + (1,),\n                            trans_mat_shape,\n                            trans_mvn_shape,\n                            obs_mat_shape,\n                            obs_mvn_shape)\n    expected_batch_shape, time_shape = shape[:-1], shape[-1:]\n    expected_event_shape = time_shape + (obs_dim,)\n    assert d.batch_shape == expected_batch_shape\n    assert d.event_shape == expected_event_shape\n\n    data = obs_dist.expand(shape).sample()\n    assert data.shape == d.shape()\n    actual = d.log_prob(data)\n    assert actual.shape == expected_batch_shape\n    check_expand(d, data)\n\n    mixing, final = d.filter(data)\n    assert isinstance(mixing, dist.Gamma)\n    assert mixing.batch_shape == d.batch_shape\n    assert mixing.event_shape == ()\n    assert isinstance(final, dist.MultivariateNormal)\n    assert final.batch_shape == d.batch_shape\n    assert final.event_shape == (hidden_dim,)\n\n\n@pytest.mark.parametrize(\'sample_shape\', [(), (5,)], ids=str)\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'obs_dim\', [1, 2])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 2])\n@pytest.mark.parametrize(\'num_steps\', [1, 2, 3, 4])\ndef test_gamma_gaussian_hmm_log_prob(sample_shape, batch_shape, num_steps, hidden_dim, obs_dim):\n    init_dist = random_mvn(batch_shape, hidden_dim)\n    trans_mat = torch.randn(batch_shape + (num_steps, hidden_dim, hidden_dim))\n    trans_dist = random_mvn(batch_shape + (num_steps,), hidden_dim)\n    obs_mat = torch.randn(batch_shape + (num_steps, hidden_dim, obs_dim))\n    obs_dist = random_mvn(batch_shape + (num_steps,), obs_dim)\n    scale_dist = random_gamma(batch_shape)\n    d = dist.GammaGaussianHMM(scale_dist, init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n    obs_mvn = obs_dist\n    data = obs_dist.sample(sample_shape)\n    assert data.shape == sample_shape + d.shape()\n    actual_log_prob = d.log_prob(data)\n\n    # Compare against hand-computed density.\n    # We will construct enormous unrolled joint gaussian-gammas with shapes:\n    #       t | 0 1 2 3 1 2 3      T = 3 in this example\n    #   ------+-----------------------------------------\n    #    init | H\n    #   trans | H H H H            H = hidden\n    #     obs |   H H H O O O      O = observed\n    # and then combine these using gamma_gaussian_tensordot().\n    T = num_steps\n    init = gamma_and_mvn_to_gamma_gaussian(scale_dist, init_dist)\n    trans = matrix_and_mvn_to_gamma_gaussian(trans_mat, trans_dist)\n    obs = matrix_and_mvn_to_gamma_gaussian(obs_mat, obs_mvn)\n\n    unrolled_trans = reduce(operator.add, [\n        trans[..., t].event_pad(left=t * hidden_dim, right=(T - t - 1) * hidden_dim)\n        for t in range(T)\n    ])\n    unrolled_obs = reduce(operator.add, [\n        obs[..., t].event_pad(left=t * obs.dim(), right=(T - t - 1) * obs.dim())\n        for t in range(T)\n    ])\n    # Permute obs from HOHOHO to HHHOOO.\n    perm = torch.cat([torch.arange(hidden_dim) + t * obs.dim() for t in range(T)] +\n                     [torch.arange(obs_dim) + hidden_dim + t * obs.dim() for t in range(T)])\n    unrolled_obs = unrolled_obs.event_permute(perm)\n    unrolled_data = data.reshape(data.shape[:-2] + (T * obs_dim,))\n\n    assert init.dim() == hidden_dim\n    assert unrolled_trans.dim() == (1 + T) * hidden_dim\n    assert unrolled_obs.dim() == T * (hidden_dim + obs_dim)\n    logp = gamma_gaussian_tensordot(init, unrolled_trans, hidden_dim)\n    logp = gamma_gaussian_tensordot(logp, unrolled_obs, T * hidden_dim)\n    # compute log_prob of the joint student-t distribution\n    expected_log_prob = logp.compound().log_prob(unrolled_data)\n    assert_close(actual_log_prob, expected_log_prob)\n\n\ndef random_stable(stability, skew_scale_loc_shape):\n    skew = dist.Uniform(-1, 1).sample(skew_scale_loc_shape)\n    scale = torch.rand(skew_scale_loc_shape).exp()\n    loc = torch.randn(skew_scale_loc_shape)\n    return dist.Stable(stability, skew, scale, loc)\n\n\n@pytest.mark.parametrize(\'obs_dim\', [1, 2])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 3])\n@pytest.mark.parametrize(\'init_shape,trans_mat_shape,trans_dist_shape,obs_mat_shape,obs_dist_shape\', [\n    ((), (), (), (), ()),\n    ((), (4,), (), (), ()),\n    ((), (), (4,), (), ()),\n    ((), (), (), (4,), ()),\n    ((), (), (), (), (4,)),\n    ((), (4,), (4,), (4,), (4,)),\n    ((5,), (4,), (), (), ()),\n    ((), (5, 1), (4,), (), ()),\n    ((), (), (5, 1), (4,), ()),\n    ((), (), (), (5, 1), (4,)),\n    ((), (4,), (5, 1), (), ()),\n    ((), (), (4,), (5, 1), ()),\n    ((), (), (), (4,), (5, 1)),\n    ((5,), (), (), (), (4,)),\n    ((5,), (5, 4), (5, 4), (5, 4), (5, 4)),\n], ids=str)\ndef test_stable_hmm_shape(init_shape, trans_mat_shape, trans_dist_shape,\n                          obs_mat_shape, obs_dist_shape, hidden_dim, obs_dim):\n    stability = dist.Uniform(0, 2).sample()\n    init_dist = random_stable(stability, init_shape + (hidden_dim,)).to_event(1)\n    trans_mat = torch.randn(trans_mat_shape + (hidden_dim, hidden_dim))\n    trans_dist = random_stable(stability, trans_dist_shape + (hidden_dim,)).to_event(1)\n    obs_mat = torch.randn(obs_mat_shape + (hidden_dim, obs_dim))\n    obs_dist = random_stable(stability, obs_dist_shape + (obs_dim,)).to_event(1)\n    d = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist,\n                       duration=4)\n\n    shape = broadcast_shape(init_shape + (4,),\n                            trans_mat_shape,\n                            trans_dist_shape,\n                            obs_mat_shape,\n                            obs_dist_shape)\n    expected_batch_shape, time_shape = shape[:-1], shape[-1:]\n    expected_event_shape = time_shape + (obs_dim,)\n    assert d.batch_shape == expected_batch_shape\n    assert d.event_shape == expected_event_shape\n\n    x = d.rsample()\n    assert x.shape == d.shape()\n    x = d.rsample((6,))\n    assert x.shape == (6,) + d.shape()\n    x = d.expand((6, 5)).rsample()\n    assert x.shape == (6, 5) + d.event_shape\n\n\ndef random_studentt(shape):\n    df = torch.rand(shape).exp()\n    loc = torch.randn(shape)\n    scale = torch.rand(shape).exp()\n    return dist.StudentT(df, loc, scale)\n\n\n@pytest.mark.parametrize(\'obs_dim\', [1, 2])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 3])\n@pytest.mark.parametrize(\'init_shape,trans_mat_shape,trans_dist_shape,obs_mat_shape,obs_dist_shape\', [\n    ((), (4,), (), (), ()),\n    ((), (), (4,), (), ()),\n    ((), (), (), (4,), ()),\n    ((), (), (), (), (4,)),\n    ((), (4,), (4,), (4,), (4,)),\n    ((5,), (4,), (), (), ()),\n    ((), (5, 1), (4,), (), ()),\n    ((), (), (5, 1), (4,), ()),\n    ((), (), (), (5, 1), (4,)),\n    ((), (4,), (5, 1), (), ()),\n    ((), (), (4,), (5, 1), ()),\n    ((), (), (), (4,), (5, 1)),\n    ((5,), (), (), (), (4,)),\n    ((5,), (5, 4), (5, 4), (5, 4), (5, 4)),\n], ids=str)\ndef test_studentt_hmm_shape(init_shape, trans_mat_shape, trans_dist_shape,\n                            obs_mat_shape, obs_dist_shape, hidden_dim, obs_dim):\n    init_dist = random_studentt(init_shape + (hidden_dim,)).to_event(1)\n    trans_mat = torch.randn(trans_mat_shape + (hidden_dim, hidden_dim))\n    trans_dist = random_studentt(trans_dist_shape + (hidden_dim,)).to_event(1)\n    obs_mat = torch.randn(obs_mat_shape + (hidden_dim, obs_dim))\n    obs_dist = random_studentt(obs_dist_shape + (obs_dim,)).to_event(1)\n    d = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n\n    shape = broadcast_shape(init_shape + (1,),\n                            trans_mat_shape,\n                            trans_dist_shape,\n                            obs_mat_shape,\n                            obs_dist_shape)\n    expected_batch_shape, time_shape = shape[:-1], shape[-1:]\n    expected_event_shape = time_shape + (obs_dim,)\n    assert d.batch_shape == expected_batch_shape\n    assert d.event_shape == expected_event_shape\n\n    x = d.rsample()\n    assert x.shape == d.shape()\n    x = d.rsample((6,))\n    assert x.shape == (6,) + d.shape()\n    x = d.expand((6, 5)).rsample()\n    assert x.shape == (6, 5) + d.event_shape\n\n\n@pytest.mark.parametrize(\'obs_dim\', [1, 3])\n@pytest.mark.parametrize(\'hidden_dim\', [1, 2])\n@pytest.mark.parametrize(\'init_shape,trans_mat_shape,trans_mvn_shape,obs_mat_shape,obs_mvn_shape\', [\n    ((), (), (), (), ()),\n    ((), (6,), (), (), ()),\n    ((), (), (6,), (), ()),\n    ((), (), (), (6,), ()),\n    ((), (), (), (), (6,)),\n    ((), (6,), (6,), (6,), (6,)),\n    ((5,), (6,), (), (), ()),\n    ((), (5, 1), (6,), (), ()),\n    ((), (), (5, 1), (6,), ()),\n    ((), (), (), (5, 1), (6,)),\n    ((), (6,), (5, 1), (), ()),\n    ((), (), (6,), (5, 1), ()),\n    ((), (), (), (6,), (5, 1)),\n    ((5,), (), (), (), (6,)),\n    ((5,), (5, 6), (5, 6), (5, 6), (5, 6)),\n], ids=str)\ndef test_independent_hmm_shape(init_shape, trans_mat_shape, trans_mvn_shape,\n                               obs_mat_shape, obs_mvn_shape, hidden_dim, obs_dim):\n    base_init_shape = init_shape + (obs_dim,)\n    base_trans_mat_shape = trans_mat_shape[:-1] + (obs_dim, trans_mat_shape[-1] if trans_mat_shape else 6)\n    base_trans_mvn_shape = trans_mvn_shape[:-1] + (obs_dim, trans_mvn_shape[-1] if trans_mvn_shape else 6)\n    base_obs_mat_shape = obs_mat_shape[:-1] + (obs_dim, obs_mat_shape[-1] if obs_mat_shape else 6)\n    base_obs_mvn_shape = obs_mvn_shape[:-1] + (obs_dim, obs_mvn_shape[-1] if obs_mvn_shape else 6)\n\n    init_dist = random_mvn(base_init_shape, hidden_dim)\n    trans_mat = torch.randn(base_trans_mat_shape + (hidden_dim, hidden_dim))\n    trans_dist = random_mvn(base_trans_mvn_shape, hidden_dim)\n    obs_mat = torch.randn(base_obs_mat_shape + (hidden_dim, 1))\n    obs_dist = random_mvn(base_obs_mvn_shape, 1)\n    d = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist, duration=6)\n    d = dist.IndependentHMM(d)\n\n    shape = broadcast_shape(init_shape + (6,),\n                            trans_mat_shape,\n                            trans_mvn_shape,\n                            obs_mat_shape,\n                            obs_mvn_shape)\n    expected_batch_shape, time_shape = shape[:-1], shape[-1:]\n    expected_event_shape = time_shape + (obs_dim,)\n    assert d.batch_shape == expected_batch_shape\n    assert d.event_shape == expected_event_shape\n\n    data = torch.randn(shape + (obs_dim,))\n    assert data.shape == d.shape()\n    actual = d.log_prob(data)\n    assert actual.shape == expected_batch_shape\n    check_expand(d, data)\n\n    x = d.rsample()\n    assert x.shape == d.shape()\n    x = d.rsample((6,))\n    assert x.shape == (6,) + d.shape()\n    x = d.expand((6, 5)).rsample()\n    assert x.shape == (6, 5) + d.event_shape\n'"
tests/distributions/test_ig.py,1,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\n\nimport pytest\nfrom pyro.distributions import Gamma, InverseGamma\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize('concentration', [3.3, 4.0])\n@pytest.mark.parametrize('rate', [2.5, 3.0])\ndef test_sample(concentration, rate, n_samples=int(1e6)):\n    samples = InverseGamma(concentration, rate).sample((n_samples,))\n    mean, std = samples.mean().item(), samples.std().item()\n    expected_mean = rate / (concentration - 1.0)\n    expected_std = rate / ((concentration - 1.0) * math.sqrt(concentration - 2.0))\n    assert_equal(mean, expected_mean, prec=1e-2)\n    assert_equal(std, expected_std, prec=0.03)\n\n\n@pytest.mark.parametrize('concentration', [2.5, 4.0])\n@pytest.mark.parametrize('rate', [2.5, 3.0])\n@pytest.mark.parametrize('value', [0.5, 1.7])\ndef test_log_prob(concentration, rate, value):\n    value = torch.tensor(value)\n    log_prob = InverseGamma(concentration, rate).log_prob(value)\n    expected_log_prob = Gamma(concentration, rate).log_prob(1.0 / value) - 2.0 * value.log()\n    assert_equal(log_prob, expected_log_prob, prec=1e-6)\n"""
tests/distributions/test_improper_uniform.py,3,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.distributions import constraints, transform_to\n\nimport pyro.distributions as dist\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize(""constraint"", [\n    constraints.real,\n    constraints.positive,\n    constraints.unit_interval,\n], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""event_shape"", [(), (4,), (3, 2)], ids=str)\ndef test_improper_uniform(constraint, batch_shape, event_shape):\n    d = dist.ImproperUniform(constraint, batch_shape, event_shape)\n\n    value = transform_to(constraint)(torch.randn(batch_shape + event_shape))\n    assert_equal(d.log_prob(value), torch.zeros(batch_shape))\n\n    with pytest.raises(NotImplementedError):\n        d.sample()\n    with pytest.raises(NotImplementedError):\n        d.sample(sample_shape=(5, 6))\n'"
tests/distributions/test_independent.py,7,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.distributions.utils import _sum_rightmost\n\nimport pyro.distributions as dist\nfrom pyro.util import torch_isnan\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize('sample_shape', [(), (6,), (4, 2)])\n@pytest.mark.parametrize('batch_shape', [(), (7,), (5, 3), (5, 3, 2)])\n@pytest.mark.parametrize('reinterpreted_batch_ndims', [0, 1, 2, 3])\n@pytest.mark.parametrize('base_dist',\n                         [dist.Normal(1., 2.), dist.Exponential(2.),\n                          dist.MultivariateNormal(torch.zeros(2), torch.eye(2))],\n                         ids=['normal', 'exponential', 'mvn'])\ndef test_independent(base_dist, sample_shape, batch_shape, reinterpreted_batch_ndims):\n    if batch_shape:\n        base_dist = base_dist.expand_by(batch_shape)\n    if reinterpreted_batch_ndims > len(base_dist.batch_shape):\n        with pytest.raises(ValueError):\n            d = dist.Independent(base_dist, reinterpreted_batch_ndims)\n    else:\n        d = dist.Independent(base_dist, reinterpreted_batch_ndims)\n        assert d.batch_shape == batch_shape[:len(batch_shape) - reinterpreted_batch_ndims]\n        assert d.event_shape == batch_shape[len(batch_shape) - reinterpreted_batch_ndims:] + base_dist.event_shape\n\n        assert d.sample().shape == batch_shape + base_dist.event_shape\n        assert d.mean.shape == batch_shape + base_dist.event_shape\n        assert d.variance.shape == batch_shape + base_dist.event_shape\n        x = d.sample(sample_shape)\n        assert x.shape == sample_shape + d.batch_shape + d.event_shape\n\n        log_prob = d.log_prob(x)\n        assert log_prob.shape == sample_shape + batch_shape[:len(batch_shape) - reinterpreted_batch_ndims]\n        assert not torch_isnan(log_prob)\n        log_prob_0 = base_dist.log_prob(x)\n        assert_equal(log_prob, _sum_rightmost(log_prob_0, reinterpreted_batch_ndims))\n\n\n@pytest.mark.parametrize('base_dist',\n                         [dist.Normal(1., 2.), dist.Exponential(2.),\n                          dist.MultivariateNormal(torch.zeros(2), torch.eye(2))],\n                         ids=['normal', 'exponential', 'mvn'])\ndef test_to_event(base_dist):\n    base_dist = base_dist.expand([2, 3])\n    d = base_dist\n    expected_event_dim = d.event_dim\n\n    d = d.to_event(0)\n    assert d is base_dist\n\n    d = d.to_event(1)\n    expected_event_dim += 1\n    assert d.event_dim == expected_event_dim\n    assert d.base_dist is base_dist\n\n    d = d.to_event(0)\n    assert d.event_dim == expected_event_dim\n    assert d.base_dist is base_dist\n\n    d = d.to_event(1)\n    expected_event_dim += 1\n    assert d.event_dim == expected_event_dim\n    assert d.base_dist is base_dist\n\n    d = d.to_event(0)\n    assert d.event_dim == expected_event_dim\n    assert d.base_dist is base_dist\n\n    d = d.to_event(-1)\n    expected_event_dim += -1\n    assert d.event_dim == expected_event_dim\n    assert d.base_dist is base_dist\n\n    d = d.to_event(0)\n    assert d.event_dim == expected_event_dim\n    assert d.base_dist is base_dist\n\n    d = d.to_event(-1)\n    expected_event_dim += -1\n    assert d is base_dist\n\n\n@pytest.mark.parametrize('event_shape', [(), (2,), (2, 3)])\n@pytest.mark.parametrize('batch_shape', [(), (3,), (5, 3)])\n@pytest.mark.parametrize('sample_shape', [(), (2,), (4, 2)])\ndef test_expand(sample_shape, batch_shape, event_shape):\n    ones_shape = torch.Size((1,) * len(batch_shape))\n    zero = torch.zeros(ones_shape + event_shape)\n    d0 = dist.Uniform(zero - 2, zero + 1).to_event(len(event_shape))\n\n    assert d0.sample().shape == ones_shape + event_shape\n    assert d0.mean.shape == ones_shape + event_shape\n    assert d0.variance.shape == ones_shape + event_shape\n    assert d0.sample(sample_shape).shape == sample_shape + ones_shape + event_shape\n\n    assert d0.expand(sample_shape + batch_shape).batch_shape == sample_shape + batch_shape\n    assert d0.expand(sample_shape + batch_shape).sample().shape == sample_shape + batch_shape + event_shape\n    assert d0.expand(sample_shape + batch_shape).mean.shape == sample_shape + batch_shape + event_shape\n    assert d0.expand(sample_shape + batch_shape).variance.shape == sample_shape + batch_shape + event_shape\n\n    base_dist = dist.MultivariateNormal(torch.zeros(2).expand(*(event_shape + (2,))),\n                                        torch.eye(2).expand(*(event_shape + (2, 2))))\n    if len(event_shape) > len(base_dist.batch_shape):\n        with pytest.raises(ValueError):\n            base_dist.to_event(len(event_shape)).expand(batch_shape)\n    else:\n        expanded = base_dist.to_event(len(event_shape)).expand(batch_shape)\n        expanded_batch_ndims = getattr(expanded, 'reinterpreted_batch_ndims', 0)\n        assert expanded.batch_shape == batch_shape\n        assert expanded.event_shape == (base_dist.batch_shape[len(base_dist.batch_shape) -\n                                                              expanded_batch_ndims:] +\n                                        base_dist.event_shape)\n"""
tests/distributions/test_kl.py,21,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.distributions import kl_divergence, transforms\n\nimport pyro.distributions as dist\nfrom pyro.distributions.util import sum_rightmost\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize('batch_shape', [(), (4,), (2, 3)], ids=str)\ndef test_kl_delta_normal_shape(batch_shape):\n    v = torch.randn(batch_shape)\n    loc = torch.randn(batch_shape)\n    scale = torch.randn(batch_shape).exp()\n    p = dist.Delta(v)\n    q = dist.Normal(loc, scale)\n    assert kl_divergence(p, q).shape == batch_shape\n\n\n@pytest.mark.parametrize('batch_shape', [(), (4,), (2, 3)], ids=str)\n@pytest.mark.parametrize('size', [1, 2, 3])\ndef test_kl_delta_mvn_shape(batch_shape, size):\n    v = torch.randn(batch_shape + (size,))\n    p = dist.Delta(v, event_dim=1)\n\n    loc = torch.randn(batch_shape + (size,))\n    cov = torch.randn(batch_shape + (size, size))\n    cov = cov @ cov.transpose(-1, -2) + 0.01 * torch.eye(size)\n    q = dist.MultivariateNormal(loc, covariance_matrix=cov)\n    assert kl_divergence(p, q).shape == batch_shape\n\n\n@pytest.mark.parametrize('batch_shape', [(), (4,), (2, 3)], ids=str)\n@pytest.mark.parametrize('event_shape', [(), (4,), (2, 3)], ids=str)\ndef test_kl_independent_normal(batch_shape, event_shape):\n    shape = batch_shape + event_shape\n    p = dist.Normal(torch.randn(shape), torch.randn(shape).exp())\n    q = dist.Normal(torch.randn(shape), torch.randn(shape).exp())\n    actual = kl_divergence(dist.Independent(p, len(event_shape)),\n                           dist.Independent(q, len(event_shape)))\n    expected = sum_rightmost(kl_divergence(p, q), len(event_shape))\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize('batch_shape', [(), (4,), (2, 3)], ids=str)\n@pytest.mark.parametrize('size', [1, 2, 3])\ndef test_kl_independent_delta_mvn_shape(batch_shape, size):\n    v = torch.randn(batch_shape + (size,))\n    p = dist.Independent(dist.Delta(v), 1)\n\n    loc = torch.randn(batch_shape + (size,))\n    cov = torch.randn(batch_shape + (size, size))\n    cov = cov @ cov.transpose(-1, -2) + 0.01 * torch.eye(size)\n    q = dist.MultivariateNormal(loc, covariance_matrix=cov)\n    assert kl_divergence(p, q).shape == batch_shape\n\n\n@pytest.mark.parametrize('batch_shape', [(), (4,), (2, 3)], ids=str)\n@pytest.mark.parametrize('size', [1, 2, 3])\ndef test_kl_independent_normal_mvn(batch_shape, size):\n    loc = torch.randn(batch_shape + (size,))\n    scale = torch.randn(batch_shape + (size,)).exp()\n    p1 = dist.Normal(loc, scale).to_event(1)\n    p2 = dist.MultivariateNormal(loc, scale_tril=scale.diag_embed())\n\n    loc = torch.randn(batch_shape + (size,))\n    cov = torch.randn(batch_shape + (size, size))\n    cov = cov @ cov.transpose(-1, -2) + 0.01 * torch.eye(size)\n    q = dist.MultivariateNormal(loc, covariance_matrix=cov)\n\n    actual = kl_divergence(p1, q)\n    expected = kl_divergence(p2, q)\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize('shape', [(5,), (4, 5), (2, 3, 5)], ids=str)\n@pytest.mark.parametrize('event_dim', [0, 1])\n@pytest.mark.parametrize('transform', [transforms.ExpTransform(), transforms.StickBreakingTransform()])\ndef test_kl_transformed_transformed(shape, event_dim, transform):\n    p_base = dist.Normal(torch.zeros(shape), torch.ones(shape)).to_event(event_dim)\n    q_base = dist.Normal(torch.ones(shape) * 2, torch.ones(shape)).to_event(event_dim)\n    p = dist.TransformedDistribution(p_base, transform)\n    q = dist.TransformedDistribution(q_base, transform)\n    kl = kl_divergence(q, p)\n    expected_shape = shape[:-1] if max(transform.event_dim, event_dim) == 1 else shape\n    assert kl.shape == expected_shape\n"""
tests/distributions/test_lkj.py,11,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\nfrom torch.distributions import AffineTransform, Beta, TransformedDistribution, biject_to, transform_to\n\nfrom pyro.distributions import constraints, transforms\nfrom pyro.distributions.lkj import LKJCorrCholesky\nfrom tests.common import assert_equal, assert_tensors_equal\n\n\n@pytest.mark.parametrize(""value_shape"", [(1, 1), (3, 3), (5, 5)])\ndef test_constraint(value_shape):\n    value = torch.randn(value_shape).clamp(-2, 2).tril()\n    value.diagonal(dim1=-2, dim2=-1).exp_()\n    value = value / value.norm(2, dim=-1, keepdim=True)\n\n    assert (constraints.corr_cholesky_constraint.check(value) == 1).all()\n\n\ndef _autograd_log_det(ys, x):\n    # computes log_abs_det_jacobian of y w.r.t. x\n    return torch.stack([torch.autograd.grad(y, (x,), retain_graph=True)[0]\n                        for y in ys]).det().abs().log()\n\n\n@pytest.mark.parametrize(""y_shape"", [(1,), (3, 1), (6,), (1, 6), (2, 6)])\ndef test_unconstrained_to_corr_cholesky_transform(y_shape):\n    transform = transforms.CorrLCholeskyTransform()\n    y = torch.empty(y_shape).uniform_(-4, 4).requires_grad_()\n    x = transform(y)\n\n    # test codomain\n    assert (transform.codomain.check(x) == 1).all()\n\n    # test inv\n    y_prime = transform.inv(x)\n    assert_tensors_equal(y, y_prime, prec=1e-4)\n\n    # test domain\n    assert (transform.domain.check(y_prime) == 1).all()\n\n    # test log_abs_det_jacobian\n    log_det = transform.log_abs_det_jacobian(y, x)\n    assert log_det.shape == y_shape[:-1]\n    if len(y_shape) == 1:\n        triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.bool)\n        x_tril_vector = x.t()[triu_index]\n        assert_tensors_equal(_autograd_log_det(x_tril_vector, y), log_det, prec=1e-4)\n\n        x_tril_vector = x_tril_vector.detach().requires_grad_()\n        x = x.new_zeros(x.shape)\n        x[triu_index] = x_tril_vector\n        x = x.t()\n        z = transform.inv(x)\n        assert_tensors_equal(_autograd_log_det(z, x_tril_vector), -log_det, prec=1e-4)\n\n\n@pytest.mark.parametrize(""x_shape"", [(1,), (3, 1), (6,), (1, 6), (5, 6)])\n@pytest.mark.parametrize(""mapping"", [biject_to, transform_to])\ndef test_corr_cholesky_transform(x_shape, mapping):\n    transform = mapping(constraints.corr_cholesky_constraint)\n    x = torch.randn(x_shape, requires_grad=True).clamp(-2, 2)\n    y = transform(x)\n\n    # test codomain\n    assert (transform.codomain.check(y) == 1).all()\n\n    # test inv\n    z = transform.inv(y)\n    assert_tensors_equal(x, z, prec=1e-4)\n\n    # test domain\n    assert (transform.domain.check(z) == 1).all()\n\n    # test log_abs_det_jacobian\n    log_det = transform.log_abs_det_jacobian(x, y)\n    assert log_det.shape == x_shape[:-1]\n\n\n@pytest.mark.parametrize(""d"", [2, 3, 4, 10])\ndef test_log_prob_eta1(d):\n    dist = LKJCorrCholesky(d, torch.tensor([1.]))\n\n    a_sample = dist.sample(torch.Size([100]))\n    lp = dist.log_prob(a_sample)\n\n    if d == 2:\n        assert_equal(lp, lp.new_full(lp.size(), -math.log(2)))\n    else:\n        ladj = a_sample.diagonal(dim1=-2, dim2=-1).log().mul(\n            torch.linspace(start=d-1, end=0, steps=d, device=a_sample.device, dtype=a_sample.dtype)\n        ).sum(-1)\n        lps_less_ladj = lp - ladj\n        assert (lps_less_ladj - lps_less_ladj.min()).abs().sum() < 1e-4\n\n\n@pytest.mark.parametrize(""eta"", [.1, .5, 1., 2., 5.])\ndef test_log_prob_d2(eta):\n    dist = LKJCorrCholesky(2, torch.tensor([eta]))\n    test_dist = TransformedDistribution(Beta(eta, eta), AffineTransform(loc=-1., scale=2.0))\n\n    samples = dist.sample(torch.Size([100]))\n    lp = dist.log_prob(samples)\n    x = samples[..., 1, 0]\n    tst = test_dist.log_prob(x)\n\n    assert_tensors_equal(lp, tst, prec=1e-6)\n'"
tests/distributions/test_lowrank_mvn.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.distributions import LowRankMultivariateNormal, MultivariateNormal\nfrom tests.common import assert_equal\n\n\ndef test_scale_tril():\n    loc = torch.tensor([1.0, 2.0, 1.0, 2.0, 0.0])\n    D = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n    W = torch.tensor([[1.0, 2.0], [-1.0, 3.0], [2.0, 1.0], [3.0, 2.0], [4.0, 4.0]])\n    cov = D.diag() + W.matmul(W.t())\n\n    mvn = MultivariateNormal(loc, cov)\n    lowrank_mvn = LowRankMultivariateNormal(loc, W, D)\n\n    assert_equal(mvn.scale_tril, lowrank_mvn.scale_tril)\n\n\ndef test_log_prob():\n    loc = torch.tensor([2.0, 1.0, 1.0, 2.0, 2.0])\n    D = torch.tensor([1.0, 2.0, 3.0, 1.0, 3.0])\n    W = torch.tensor([[1.0, 2.0], [-1.0, 1.0], [2.0, 1.0], [2.0, 2.0], [4.0, 6.0]])\n    x = torch.tensor([2.0, 3.0, 4.0, 1.0, 7.0])\n    cov = D.diag() + W.matmul(W.t())\n\n    mvn = MultivariateNormal(loc, cov)\n    lowrank_mvn = LowRankMultivariateNormal(loc, W, D)\n\n    assert_equal(mvn.log_prob(x), lowrank_mvn.log_prob(x))\n\n\ndef test_variance():\n    loc = torch.tensor([1.0, 1.0, 1.0, 2.0, 0.0])\n    D = torch.tensor([1.0, 2.0, 2.0, 4.0, 5.0])\n    W = torch.tensor([[3.0, 2.0], [-1.0, 3.0], [3.0, 1.0], [3.0, 3.0], [4.0, 4.0]])\n    cov = D.diag() + W.matmul(W.t())\n\n    mvn = MultivariateNormal(loc, cov)\n    lowrank_mvn = LowRankMultivariateNormal(loc, W, D)\n\n    assert_equal(mvn.variance, lowrank_mvn.variance)\n'"
tests/distributions/test_mask.py,20,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch import tensor\nfrom torch.distributions import kl_divergence\n\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.distributions.torch import Bernoulli, Normal\nfrom pyro.distributions.util import scale_and_mask\nfrom tests.common import assert_equal\n\n\ndef checker_mask(shape):\n    mask = tensor(0.)\n    for size in shape:\n        mask = mask.unsqueeze(-1) + torch.arange(float(size))\n    return mask.fmod(2).bool()\n\n\n@pytest.mark.parametrize(\'batch_dim,mask_dim\',\n                         [(b, m) for b in range(3) for m in range(1 + b)])\n@pytest.mark.parametrize(\'event_dim\', [0, 1, 2])\ndef test_mask(batch_dim, event_dim, mask_dim):\n    # Construct base distribution.\n    shape = torch.Size([2, 3, 4, 5, 6][:batch_dim + event_dim])\n    batch_shape = shape[:batch_dim]\n    mask_shape = batch_shape[batch_dim - mask_dim:]\n    base_dist = Bernoulli(0.1).expand_by(shape).to_event(event_dim)\n\n    # Construct masked distribution.\n    mask = checker_mask(mask_shape)\n    dist = base_dist.mask(mask)\n\n    # Check shape.\n    sample = base_dist.sample()\n    assert dist.batch_shape == base_dist.batch_shape\n    assert dist.event_shape == base_dist.event_shape\n    assert sample.shape == sample.shape\n    assert dist.log_prob(sample).shape == base_dist.log_prob(sample).shape\n\n    # Check values.\n    assert_equal(dist.mean, base_dist.mean)\n    assert_equal(dist.variance, base_dist.variance)\n    assert_equal(dist.log_prob(sample),\n                 scale_and_mask(base_dist.log_prob(sample), mask=mask))\n    assert_equal(dist.score_parts(sample),\n                 base_dist.score_parts(sample).scale_and_mask(mask=mask), prec=0)\n    if not dist.event_shape:\n        assert_equal(dist.enumerate_support(), base_dist.enumerate_support())\n        assert_equal(dist.enumerate_support(expand=True), base_dist.enumerate_support(expand=True))\n        assert_equal(dist.enumerate_support(expand=False), base_dist.enumerate_support(expand=False))\n\n\n@pytest.mark.parametrize(""mask"", [False, True, torch.tensor(False), torch.tensor(True)])\ndef test_mask_type(mask):\n    p = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())\n    p_masked = p.mask(mask)\n    if isinstance(mask, bool):\n        mask = torch.tensor(mask)\n\n    x = p.sample()\n    actual = p_masked.log_prob(x)\n    expected = p.log_prob(x) * mask.float()\n    assert_equal(actual, expected)\n\n    actual = p_masked.score_parts(x)\n    expected = p.score_parts(x)\n    for a, e in zip(actual, expected):\n        if isinstance(e, torch.Tensor):\n            e = e * mask.float()\n        assert_equal(a, e)\n\n\n@pytest.mark.parametrize(""event_shape"", [(), (4,)])\n@pytest.mark.parametrize(""dist_shape"", [(), (3,), (2, 1), (2, 3)])\n@pytest.mark.parametrize(""mask_shape"", [(), (3,), (2, 1), (2, 3)])\ndef test_broadcast(event_shape, dist_shape, mask_shape):\n    mask = torch.empty(torch.Size(mask_shape)).bernoulli_(0.5).bool()\n    base_dist = Normal(torch.zeros(dist_shape + event_shape), 1.)\n    base_dist = base_dist.to_event(len(event_shape))\n    assert base_dist.batch_shape == dist_shape\n    assert base_dist.event_shape == event_shape\n\n    d = base_dist.mask(mask)\n    d_shape = broadcast_shape(mask.shape, base_dist.batch_shape)\n    assert d.batch_shape == d_shape\n    assert d.event_shape == event_shape\n\n\ndef test_kl_divergence():\n    mask = torch.tensor([[0, 1], [1, 1]]).bool()\n    p = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())\n    q = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())\n    expected = kl_divergence(p.to_event(2), q.to_event(2))\n    actual = (kl_divergence(p.mask(mask).to_event(2),\n                            q.mask(mask).to_event(2)) +\n              kl_divergence(p.mask(~mask).to_event(2),\n                            q.mask(~mask).to_event(2)))\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(""p_mask"", [False, True, torch.tensor(False), torch.tensor(True)])\n@pytest.mark.parametrize(""q_mask"", [False, True, torch.tensor(False), torch.tensor(True)])\ndef test_kl_divergence_type(p_mask, q_mask):\n    p = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())\n    q = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())\n    mask = ((torch.tensor(p_mask) if isinstance(p_mask, bool) else p_mask) &\n            (torch.tensor(q_mask) if isinstance(q_mask, bool) else q_mask)).expand(2, 2)\n\n    expected = kl_divergence(p, q)\n    expected[~mask] = 0\n\n    actual = kl_divergence(p.mask(p_mask), q.mask(q_mask))\n    if p_mask is False or q_mask is False:\n        assert isinstance(actual, float) and actual == 0.\n    else:\n        assert_equal(actual, expected)\n\n\nclass NormalBomb(Normal):\n    def log_prob(self, value):\n        raise ValueError(""Should not be called"")\n\n    def score_parts(self, value):\n        raise ValueError(""Should not be called"")\n\n\n@pytest.mark.parametrize(""shape"", [None, (), (4,), (3, 2)], ids=str)\ndef test_mask_noop(shape):\n    d = NormalBomb(0, 1).mask(False)\n    if shape is not None:\n        d = d.expand(shape)\n    x = d.sample()\n\n    actual = d.log_prob(x)\n    assert_equal(actual, torch.zeros(shape if shape else ()))\n\n    actual = d.score_parts(x)\n    assert_equal(actual.log_prob, torch.zeros(shape if shape else ()))\n'"
tests/distributions/test_mixture.py,12,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.util import torch_isnan\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize('sample_shape', [(), (6,), (4, 2)])\n@pytest.mark.parametrize('batch_shape', [(), (7,), (5, 3)])\n@pytest.mark.parametrize('component1',\n                         [dist.Normal(1., 2.), dist.Exponential(2.)],\n                         ids=['normal', 'exponential'])\n@pytest.mark.parametrize('component0',\n                         [dist.Normal(1., 2.), dist.Exponential(2.)],\n                         ids=['normal', 'exponential'])\ndef test_masked_mixture_univariate(component0, component1, sample_shape, batch_shape):\n    if batch_shape:\n        component0 = component0.expand_by(batch_shape)\n        component1 = component1.expand_by(batch_shape)\n    mask = torch.empty(batch_shape).bernoulli_(0.5).bool()\n    d = dist.MaskedMixture(mask, component0, component1)\n    assert d.batch_shape == batch_shape\n    assert d.event_shape == ()\n\n    assert d.sample().shape == batch_shape\n    assert d.mean.shape == batch_shape\n    assert d.variance.shape == batch_shape\n    x = d.sample(sample_shape)\n    assert x.shape == sample_shape + batch_shape\n\n    log_prob = d.log_prob(x)\n    assert log_prob.shape == sample_shape + batch_shape\n    assert not torch_isnan(log_prob)\n    log_prob_0 = component0.log_prob(x)\n    log_prob_1 = component1.log_prob(x)\n    mask = mask.expand(sample_shape + batch_shape)\n    assert_equal(log_prob[mask], log_prob_1[mask])\n    assert_equal(log_prob[~mask], log_prob_0[~mask])\n\n\n@pytest.mark.parametrize('sample_shape', [(), (6,), (4, 2)])\n@pytest.mark.parametrize('batch_shape', [(), (7,), (5, 3)])\ndef test_masked_mixture_multivariate(sample_shape, batch_shape):\n    event_shape = torch.Size((8,))\n    component0 = dist.MultivariateNormal(torch.zeros(event_shape), torch.eye(event_shape[0]))\n    component1 = dist.Uniform(torch.zeros(event_shape), torch.ones(event_shape)).to_event(1)\n    if batch_shape:\n        component0 = component0.expand_by(batch_shape)\n        component1 = component1.expand_by(batch_shape)\n    mask = torch.empty(batch_shape).bernoulli_(0.5).bool()\n    d = dist.MaskedMixture(mask, component0, component1)\n    assert d.batch_shape == batch_shape\n    assert d.event_shape == event_shape\n\n    assert d.sample().shape == batch_shape + event_shape\n    assert d.mean.shape == batch_shape + event_shape\n    assert d.variance.shape == batch_shape + event_shape\n    x = d.sample(sample_shape)\n    assert x.shape == sample_shape + batch_shape + event_shape\n\n    log_prob = d.log_prob(x)\n    assert log_prob.shape == sample_shape + batch_shape\n    assert not torch_isnan(log_prob)\n    log_prob_0 = component0.log_prob(x)\n    log_prob_1 = component1.log_prob(x)\n    mask = mask.expand(sample_shape + batch_shape)\n    assert_equal(log_prob[mask], log_prob_1[mask])\n    assert_equal(log_prob[~mask], log_prob_0[~mask])\n\n\n@pytest.mark.parametrize('value_shape', [(), (5, 1, 1, 1), (6, 1, 1, 1, 1)])\n@pytest.mark.parametrize('component1_shape', [(), (4, 1, 1), (6, 1, 1, 1, 1)])\n@pytest.mark.parametrize('component0_shape', [(), (3, 1), (6, 1, 1, 1, 1)])\n@pytest.mark.parametrize('mask_shape', [(), (2,), (6, 1, 1, 1, 1)])\ndef test_broadcast(mask_shape, component0_shape, component1_shape, value_shape):\n    mask = torch.empty(torch.Size(mask_shape)).bernoulli_(0.5).bool()\n    component0 = dist.Normal(torch.zeros(component0_shape), 1.)\n    component1 = dist.Exponential(torch.ones(component1_shape))\n    value = torch.ones(value_shape)\n\n    d = dist.MaskedMixture(mask, component0, component1)\n    d_shape = broadcast_shape(mask_shape, component0_shape, component1_shape)\n    assert d.batch_shape == d_shape\n\n    log_prob_shape = broadcast_shape(d_shape, value_shape)\n    assert d.log_prob(value).shape == log_prob_shape\n\n\n@pytest.mark.parametrize('event_shape', [(), (2,), (2, 3)])\n@pytest.mark.parametrize('batch_shape', [(), (3,), (5, 3)])\n@pytest.mark.parametrize('sample_shape', [(), (2,), (4, 2)])\ndef test_expand(sample_shape, batch_shape, event_shape):\n    ones_shape = torch.Size((1,) * len(batch_shape))\n    mask = torch.empty(ones_shape).bernoulli_(0.5).bool()\n    zero = torch.zeros(ones_shape + event_shape)\n    d0 = dist.Uniform(zero - 2, zero + 1).to_event(len(event_shape))\n    d1 = dist.Uniform(zero - 1, zero + 2).to_event(len(event_shape))\n    d = dist.MaskedMixture(mask, d0, d1)\n\n    assert d.sample().shape == ones_shape + event_shape\n    assert d.mean.shape == ones_shape + event_shape\n    assert d.variance.shape == ones_shape + event_shape\n    assert d.sample(sample_shape).shape == sample_shape + ones_shape + event_shape\n\n    assert d.expand(sample_shape + batch_shape).batch_shape == sample_shape + batch_shape\n    assert d.expand(sample_shape + batch_shape).sample().shape == sample_shape + batch_shape + event_shape\n    assert d.expand(sample_shape + batch_shape).mean.shape == sample_shape + batch_shape + event_shape\n    assert d.expand(sample_shape + batch_shape).variance.shape == sample_shape + batch_shape + event_shape\n"""
tests/distributions/test_mvn.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.distributions import MultivariateNormal\nfrom tests.common import assert_equal\n\n\ndef random_mvn(loc_shape, cov_shape, dim):\n    """"""\n    Generate a random MultivariateNormal distribution for testing.\n    """"""\n    rank = dim + dim\n    loc = torch.randn(loc_shape + (dim,), requires_grad=True)\n    cov = torch.randn(cov_shape + (dim, rank), requires_grad=True)\n    cov = cov.matmul(cov.transpose(-1, -2))\n    return MultivariateNormal(loc, cov)\n\n\n@pytest.mark.parametrize(\'loc_shape\', [\n    (), (2,), (3, 2),\n])\n@pytest.mark.parametrize(\'cov_shape\', [\n    (), (2,), (3, 2),\n])\n@pytest.mark.parametrize(\'dim\', [\n    1, 3, 5,\n])\ndef test_shape(loc_shape, cov_shape, dim):\n    mvn = random_mvn(loc_shape, cov_shape, dim)\n    assert mvn.loc.shape == mvn.batch_shape + mvn.event_shape\n    assert mvn.covariance_matrix.shape == mvn.batch_shape + mvn.event_shape * 2\n    assert mvn.scale_tril.shape == mvn.covariance_matrix.shape\n    assert mvn.precision_matrix.shape == mvn.covariance_matrix.shape\n\n    assert_equal(mvn.precision_matrix, mvn.covariance_matrix.inverse())\n\n    # smoke test for precision/log_prob backward\n    (mvn.precision_matrix.sum() + mvn.log_prob(torch.zeros(dim)).sum()).backward()\n'"
tests/distributions/test_mvt.py,21,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\n\nimport torch\nfrom torch.distributions import Gamma, MultivariateNormal, StudentT\n\nfrom pyro.distributions import MultivariateStudentT\nfrom tests.common import assert_equal\n\n\ndef random_mvt(df_shape, loc_shape, cov_shape, dim):\n    """"""\n    Generate a random MultivariateStudentT distribution for testing.\n    """"""\n    rank = dim + dim\n    df = torch.rand(df_shape, requires_grad=True).exp()\n    loc = torch.randn(loc_shape + (dim,), requires_grad=True)\n    cov = torch.randn(cov_shape + (dim, rank), requires_grad=True)\n    cov = cov.matmul(cov.transpose(-1, -2))\n    scale_tril = cov.cholesky()\n    return MultivariateStudentT(df, loc, scale_tril)\n\n\n@pytest.mark.parametrize(\'df_shape\', [\n    (), (2,), (3, 2),\n])\n@pytest.mark.parametrize(\'loc_shape\', [\n    (), (2,), (3, 2),\n])\n@pytest.mark.parametrize(\'cov_shape\', [\n    (), (2,), (3, 2),\n])\n@pytest.mark.parametrize(\'dim\', [\n    1, 3, 5,\n])\ndef test_shape(df_shape, loc_shape, cov_shape, dim):\n    mvt = random_mvt(df_shape, loc_shape, cov_shape, dim)\n    assert mvt.df.shape == mvt.batch_shape\n    assert mvt.loc.shape == mvt.batch_shape + mvt.event_shape\n    assert mvt.covariance_matrix.shape == mvt.batch_shape + mvt.event_shape * 2\n    assert mvt.scale_tril.shape == mvt.covariance_matrix.shape\n    assert mvt.precision_matrix.shape == mvt.covariance_matrix.shape\n\n    assert_equal(mvt.precision_matrix, mvt.covariance_matrix.inverse())\n\n    # smoke test for precision/log_prob backward\n    (mvt.precision_matrix.sum() + mvt.log_prob(torch.zeros(dim)).sum()).backward()\n\n\n@pytest.mark.parametrize(""batch_shape"", [\n    (),\n    (3, 2),\n    (4,),\n], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2])\ndef test_log_prob(batch_shape, dim):\n    loc = torch.randn(batch_shape + (dim,))\n    A = torch.randn(batch_shape + (dim, dim + dim))\n    scale_tril = A.matmul(A.transpose(-2, -1)).cholesky()\n    x = torch.randn(batch_shape + (dim,))\n    df = torch.randn(batch_shape).exp() + 2\n    actual_log_prob = MultivariateStudentT(df, loc, scale_tril).log_prob(x)\n\n    if dim == 1:\n        expected_log_prob = StudentT(df.unsqueeze(-1), loc, scale_tril[..., 0]).log_prob(x).sum(-1)\n        assert_equal(actual_log_prob, expected_log_prob)\n\n    # test the fact MVT(df, loc, scale)(x) = int MVN(loc, scale / m)(x) Gamma(df/2,df/2)(m) dm\n    num_samples = 100000\n    gamma_samples = Gamma(df / 2, df / 2).sample(sample_shape=(num_samples,))\n    mvn_scale_tril = scale_tril / gamma_samples.sqrt().unsqueeze(-1).unsqueeze(-1)\n    mvn = MultivariateNormal(loc, scale_tril=mvn_scale_tril)\n    expected_log_prob = mvn.log_prob(x).logsumexp(0) - math.log(num_samples)\n    assert_equal(actual_log_prob, expected_log_prob, prec=0.01)\n\n\n@pytest.mark.parametrize(""df"", [3.9, 9.1])\n@pytest.mark.parametrize(""dim"", [1, 2])\ndef test_rsample(dim, df, num_samples=200 * 1000):\n    scale_tril = (0.5 * torch.randn(dim)).exp().diag() + 0.1 * torch.randn(dim, dim)\n    scale_tril = scale_tril.tril(0)\n    scale_tril.requires_grad_(True)\n\n    d = MultivariateStudentT(torch.tensor(df), torch.zeros(dim), scale_tril)\n    z = d.rsample(sample_shape=(num_samples,))\n    loss = z.pow(2.0).sum(-1).mean()\n    loss.backward()\n\n    actual_scale_tril_grad = scale_tril.grad.data.clone()\n    scale_tril.grad.zero_()\n\n    analytic = (df / (df - 2.0)) * torch.mm(scale_tril, scale_tril.t()).diag().sum()\n    analytic.backward()\n    expected_scale_tril_grad = scale_tril.grad.data\n\n    assert_equal(expected_scale_tril_grad, actual_scale_tril_grad, prec=0.1)\n\n\n@pytest.mark.parametrize(""dim"", [1, 2])\ndef test_log_prob_normalization(dim, df=6.1, grid_size=2000, domain_width=5.0):\n    scale_tril = (0.2 * torch.randn(dim) - 1.5).exp().diag() + 0.1 * torch.randn(dim, dim)\n    scale_tril = 0.1 * scale_tril.tril(0)\n\n    volume_factor = domain_width\n    prec = 0.01\n    if dim == 2:\n        volume_factor = volume_factor ** 2\n        prec = 0.05\n\n    sample_shape = (grid_size * grid_size, dim)\n    z = torch.distributions.Uniform(-0.5 * domain_width, 0.5 * domain_width).sample(sample_shape)\n\n    d = MultivariateStudentT(torch.tensor(df), torch.zeros(dim), scale_tril)\n    normalizer = d.log_prob(z).exp().mean().item() * volume_factor\n\n    assert_equal(normalizer, 1.0, prec=prec)\n\n\n@pytest.mark.parametrize(""batch_shape"", [\n    (),\n    (3, 2),\n    (4,),\n], ids=str)\ndef test_mean_var(batch_shape):\n    dim = 2\n    loc = torch.randn(batch_shape + (dim,))\n    A = torch.randn(batch_shape + (dim, dim + dim))\n    scale_tril = A.matmul(A.transpose(-2, -1)).cholesky()\n    df = torch.randn(batch_shape).exp() + 4\n    num_samples = 100000\n    d = MultivariateStudentT(df, loc, scale_tril)\n    samples = d.sample(sample_shape=(num_samples,))\n    expected_mean = samples.mean(0)\n    expected_variance = samples.var(0)\n    assert_equal(d.mean, expected_mean, prec=0.1)\n    assert_equal(d.variance, expected_variance, prec=0.2)\n\n    assert_equal(MultivariateStudentT(0.5, loc, scale_tril).mean,\n                 torch.full(batch_shape + (dim,), float(\'nan\')))\n    assert_equal(MultivariateStudentT(0.5, loc, scale_tril).variance,\n                 torch.full(batch_shape + (dim,), float(\'nan\')))\n    assert_equal(MultivariateStudentT(1.5, loc, scale_tril).variance,\n                 torch.full(batch_shape + (dim,), float(\'inf\')))\n'"
tests/distributions/test_omt_mvn.py,22,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as np\nimport torch\n\nimport pytest\nfrom pyro.distributions import AVFMultivariateNormal, MultivariateNormal, OMTMultivariateNormal\nfrom tests.common import assert_equal\n\n\ndef analytic_grad(L11=1.0, L22=1.0, L21=1.0, omega1=1.0, omega2=1.0):\n    dp = L11 * omega1 + L21 * omega2\n    fact_1 = - omega2 * dp\n    fact_2 = np.exp(- 0.5 * (L22 * omega2) ** 2)\n    fact_3 = np.exp(- 0.5 * dp ** 2)\n    return fact_1 * fact_2 * fact_3\n\n\n@pytest.mark.parametrize(\'L21\', [0.4, 1.1])\n@pytest.mark.parametrize(\'L11\', [0.6])\n@pytest.mark.parametrize(\'omega1\', [0.5])\n@pytest.mark.parametrize(\'sample_shape\', [torch.Size([1000, 2000]), torch.Size([200000])])\n@pytest.mark.parametrize(\'k\', [1])\n@pytest.mark.parametrize(\'mvn_dist\', [\'OMTMultivariateNormal\', \'AVFMultivariateNormal\'])\ndef test_mean_gradient(mvn_dist, k, sample_shape, L21, omega1, L11, L22=0.8, L33=0.9, omega2=0.75):\n    if mvn_dist == \'OMTMultivariateNormal\' and k > 1:\n        return\n\n    omega = torch.tensor([omega1, omega2, 0.0])\n    loc = torch.zeros(3, requires_grad=True)\n    zero_vec = [0.0, 0.0, 0.0]\n    off_diag = torch.tensor([zero_vec, [L21, 0.0, 0.0], zero_vec], requires_grad=True)\n    L = torch.diag(torch.tensor([L11, L22, L33])) + off_diag\n\n    if mvn_dist == \'OMTMultivariateNormal\':\n        dist = OMTMultivariateNormal(loc, L)\n    elif mvn_dist == \'AVFMultivariateNormal\':\n        CV = (1.1 * torch.rand(2, k, 3)).requires_grad_(True)\n        dist = AVFMultivariateNormal(loc, L, CV)\n\n    z = dist.rsample(sample_shape)\n    torch.cos((omega*z).sum(-1)).mean().backward()\n\n    computed_grad = off_diag.grad.cpu().data.numpy()[1, 0]\n    analytic = analytic_grad(L11=L11, L22=L22, L21=L21, omega1=omega1, omega2=omega2)\n    assert(off_diag.grad.size() == off_diag.size())\n    assert(loc.grad.size() == loc.size())\n    assert(torch.triu(off_diag.grad, 1).sum() == 0.0)\n    assert_equal(analytic, computed_grad, prec=0.005,\n                 msg=\'bad cholesky grad for %s (expected %.5f, got %.5f)\' %\n                 (mvn_dist, analytic, computed_grad))\n\n\n@pytest.mark.skip(reason=""Slow; tests to be run when refactoring"")\n@pytest.mark.parametrize(\'L21\', [0.4, 1.1])\n@pytest.mark.parametrize(\'L11\', [0.6, 0.95])\n@pytest.mark.parametrize(\'omega1\', [0.5, 0.9])\n@pytest.mark.parametrize(\'k\', [3])\n@pytest.mark.parametrize(\'mvn_dist\', [\'OMTMultivariateNormal\', \'AVFMultivariateNormal\'])\ndef test_mean_single_gradient(mvn_dist, k, L21, omega1, L11, L22=0.8, L33=0.9, omega2=0.75, n_samples=20000):\n    omega = torch.tensor([omega1, omega2, 0.0])\n    loc = torch.zeros(3, requires_grad=True)\n    zero_vec = [0.0, 0.0, 0.0]\n    off_diag = torch.tensor([zero_vec, [L21, 0.0, 0.0], zero_vec], requires_grad=True)\n    L = torch.diag(torch.tensor([L11, L22, L33])) + off_diag\n\n    if mvn_dist == \'OMTMultivariateNormal\':\n        dist = OMTMultivariateNormal(loc, L)\n    elif mvn_dist == \'AVFMultivariateNormal\':\n        CV = (0.2 * torch.rand(2, k, 3)).requires_grad_(True)\n        dist = AVFMultivariateNormal(loc, L, CV)\n\n    computed_grads = []\n\n    for _ in range(n_samples):\n        z = dist.rsample()\n        torch.cos((omega*z).sum(-1)).mean().backward()\n        assert(off_diag.grad.size() == off_diag.size())\n        assert(loc.grad.size() == loc.size())\n        assert(torch.triu(off_diag.grad, 1).sum() == 0.0)\n\n        computed_grad = off_diag.grad.cpu()[1, 0].item()\n        computed_grads.append(computed_grad)\n        off_diag.grad.zero_()\n        loc.grad.zero_()\n\n    computed_grad = np.mean(computed_grads)\n    analytic = analytic_grad(L11=L11, L22=L22, L21=L21, omega1=omega1, omega2=omega2)\n    assert_equal(analytic, computed_grad, prec=0.01,\n                 msg=\'bad cholesky grad for %s (expected %.5f, got %.5f)\' % (mvn_dist, analytic, computed_grad))\n\n\n@pytest.mark.parametrize(\'mvn_dist\', [OMTMultivariateNormal, AVFMultivariateNormal])\ndef test_log_prob(mvn_dist):\n    loc = torch.tensor([2.0, 1.0, 1.0, 2.0, 2.0])\n    D = torch.tensor([1.0, 2.0, 3.0, 1.0, 3.0])\n    W = torch.tensor([[1.0, -1.0, 2.0, 2.0, 4.0], [2.0, 1.0, 1.0, 2.0, 6.0]])\n    x = torch.tensor([2.0, 3.0, 4.0, 1.0, 7.0])\n    L = D.diag() + torch.tril(W.t().matmul(W))\n    cov = torch.mm(L, L.t())\n\n    mvn = MultivariateNormal(loc, cov)\n    if mvn_dist == OMTMultivariateNormal:\n        mvn_prime = OMTMultivariateNormal(loc, L)\n    elif mvn_dist == AVFMultivariateNormal:\n        CV = 0.2 * torch.rand(2, 2, 5)\n        mvn_prime = AVFMultivariateNormal(loc, L, CV)\n    assert_equal(mvn.log_prob(x), mvn_prime.log_prob(x))\n'"
tests/distributions/test_one_hot_categorical.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom unittest import TestCase\n\nimport numpy as np\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom tests.common import assert_equal\n\n\nclass TestOneHotCategorical(TestCase):\n    """"""\n    Tests methods specific to the OneHotCategorical distribution\n    """"""\n\n    def setUp(self):\n        n = 1\n        self.probs = torch.tensor([0.1, 0.6, 0.3])\n        self.batch_ps = torch.tensor([[0.1, 0.6, 0.3], [0.2, 0.4, 0.4]])\n        self.n = torch.tensor([n])\n        self.test_data = torch.tensor([0.0, 1.0, 0.0])\n        self.test_data_nhot = torch.tensor([2.0])\n        self.analytic_mean = n * self.probs\n        one = torch.ones(3)\n        self.analytic_var = n * torch.mul(self.probs, one.sub(self.probs))\n\n        # Discrete Distribution\n        self.d_ps = torch.tensor([[0.2, 0.3, 0.5], [0.1, 0.1, 0.8]])\n        self.d_test_data = torch.tensor([[0.0], [5.0]])\n        self.d_v_test_data = [[\'a\'], [\'f\']]\n\n        self.n_samples = 50000\n\n        self.support_one_hot_non_vec = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n        self.support_one_hot = torch.tensor([[[1, 0, 0], [1, 0, 0]],\n                                             [[0, 1, 0], [0, 1, 0]],\n                                             [[0, 0, 1], [0, 0, 1]]])\n        self.support_non_vec = torch.LongTensor([[0], [1], [2]])\n        self.support = torch.LongTensor([[[0], [0]], [[1], [1]], [[2], [2]]])\n        self.discrete_support_non_vec = torch.tensor([[0.0], [1.0], [2.0]])\n        self.discrete_support = torch.tensor([[[0.0], [3.0]], [[1.0], [4.0]], [[2.0], [5.0]]])\n        self.discrete_arr_support_non_vec = [[\'a\'], [\'b\'], [\'c\']]\n        self.discrete_arr_support = [[[\'a\'], [\'d\']], [[\'b\'], [\'e\']], [[\'c\'], [\'f\']]]\n\n    def test_support_non_vectorized(self):\n        s = dist.OneHotCategorical(self.d_ps[0].squeeze(0)).enumerate_support()\n        assert_equal(s.data, self.support_one_hot_non_vec)\n\n    def test_support(self):\n        s = dist.OneHotCategorical(self.d_ps).enumerate_support()\n        assert_equal(s.data, self.support_one_hot)\n\n\ndef wrap_nested(x, dim):\n    if dim == 0:\n        return x\n    return wrap_nested([x], dim-1)\n\n\ndef assert_correct_dimensions(sample, probs):\n    ps_shape = list(probs.data.size())\n    sample_shape = list(sample.shape)\n    assert_equal(sample_shape, ps_shape)\n\n\n@pytest.fixture(params=[1, 2, 3], ids=lambda x: ""dim="" + str(x))\ndef dim(request):\n    return request.param\n\n\n@pytest.fixture(params=[[0.3, 0.5, 0.2]], ids=None)\ndef probs(request):\n    return request.param\n\n\ndef modify_params_using_dims(probs, dim):\n    return torch.tensor(wrap_nested(probs, dim-1))\n\n\ndef test_support_dims(dim, probs):\n    probs = modify_params_using_dims(probs, dim)\n    d = dist.OneHotCategorical(probs)\n    support = d.enumerate_support()\n    for s in support:\n        assert_correct_dimensions(s, probs)\n    n = len(support)\n    assert support.shape == (n,) + d.batch_shape + d.event_shape\n    support_expanded = d.enumerate_support(expand=True)\n    assert support_expanded.shape == (n,) + d.batch_shape + d.event_shape\n    support_unexpanded = d.enumerate_support(expand=False)\n    assert support_unexpanded.shape == (n,) + (1,) * len(d.batch_shape) + d.event_shape\n\n\ndef test_sample_dims(dim, probs):\n    probs = modify_params_using_dims(probs, dim)\n    sample = dist.OneHotCategorical(probs).sample()\n    assert_correct_dimensions(sample, probs)\n\n\ndef test_batch_log_dims(dim, probs):\n    batch_pdf_shape = (3,) + (1,) * (dim-1)\n    expected_log_prob_sum = np.array(wrap_nested(list(np.log(probs)), dim-1)).reshape(*batch_pdf_shape)\n    probs = modify_params_using_dims(probs, dim)\n    support = dist.OneHotCategorical(probs).enumerate_support()\n    log_prob = dist.OneHotCategorical(probs).log_prob(support)\n    assert_equal(log_prob.detach().cpu().numpy(), expected_log_prob_sum)\n'"
tests/distributions/test_pickle.py,20,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport inspect\nimport io\n\nimport pytest\nimport pickle\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.distributions.torch_distribution import TorchDistributionMixin\nfrom tests.common import xfail_param\n\n# Collect distributions.\nBLACKLIST = [\n    dist.TorchDistribution,\n    dist.ExponentialFamily,\n    dist.OMTMultivariateNormal,\n]\nXFAIL = {\n    dist.Gumbel: xfail_param(dist.Gumbel, reason='cannot pickle weakref'),\n}\nDISTRIBUTIONS = [d for d in dist.__dict__.values()\n                 if isinstance(d, type)\n                 if issubclass(d, TorchDistributionMixin)\n                 if d not in BLACKLIST]\nDISTRIBUTIONS.sort(key=lambda d: d.__name__)\nDISTRIBUTIONS = [XFAIL.get(d, d) for d in DISTRIBUTIONS]\n\n# Provide default args if Dist(1, 1, ..., 1) is known to fail.\nARGS = {\n    dist.AVFMultivariateNormal: [torch.zeros(3), torch.eye(3), torch.rand(2, 4, 3)],\n    dist.Bernoulli: [0.5],\n    dist.Binomial: [2, 0.5],\n    dist.Categorical: [torch.ones(2)],\n    dist.Delta: [torch.tensor(0.)],\n    dist.Dirichlet: [torch.ones(2)],\n    dist.GaussianScaleMixture: [torch.ones(2), torch.ones(3), torch.ones(3)],\n    dist.Geometric: [0.5],\n    dist.Independent: [dist.Normal(torch.zeros(2), torch.ones(2)), 1],\n    dist.LowRankMultivariateNormal: [torch.zeros(2), torch.ones(2, 2), torch.ones(2)],\n    dist.MaskedMixture: [torch.tensor([1, 0]).bool(), dist.Normal(0, 1), dist.Normal(0, 2)],\n    dist.MixtureOfDiagNormals: [torch.ones(2, 3), torch.ones(2, 3), torch.ones(2)],\n    dist.MixtureOfDiagNormalsSharedCovariance: [torch.ones(2, 3), torch.ones(3), torch.ones(2)],\n    dist.Multinomial: [2, torch.ones(2)],\n    dist.MultivariateNormal: [torch.ones(2), torch.eye(2)],\n    dist.OneHotCategorical: [torch.ones(2)],\n    dist.RelaxedBernoulli: [1.0, 0.5],\n    dist.RelaxedBernoulliStraightThrough: [1.0, 0.5],\n    dist.RelaxedOneHotCategorical: [1., torch.ones(2)],\n    dist.RelaxedOneHotCategoricalStraightThrough: [1., torch.ones(2)],\n    dist.TransformedDistribution: [dist.Normal(0, 1), torch.distributions.ExpTransform()],\n    dist.Uniform: [0, 1],\n    dist.VonMises3D: [torch.tensor([1., 0., 0.])],\n}\n\n\n@pytest.mark.parametrize('Dist', DISTRIBUTIONS)\ndef test_pickle(Dist):\n    if Dist in ARGS:\n        args = ARGS[Dist]\n    else:\n        # Optimistically try to initialize with Dist(1, 1, ..., 1).\n        try:\n            # Python 3.6+\n            spec = list(inspect.signature(Dist.__init__).parameters.values())\n            nargs = sum(1 for p in spec if p.default is p.empty) - 1\n        except AttributeError:\n            # Python 2.6-3.5\n            spec = inspect.getargspec(Dist.__init__)\n            nargs = len(spec.args) - 1 - (len(spec.defaults) if spec.defaults else 0)\n        args = (1,) * nargs\n    try:\n        dist = Dist(*args)\n    except Exception:\n        pytest.skip(msg='cannot construct distribution')\n\n    buffer = io.BytesIO()\n    # Note that pickling torch.Size() requires protocol >= 2\n    torch.save(dist, buffer, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n    buffer.seek(0)\n    deserialized = torch.load(buffer)\n    assert isinstance(deserialized, Dist)\n"""
tests/distributions/test_polya_gamma.py,5,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.distributions import TruncatedPolyaGamma\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (3,), (2, 1)])\ndef test_polya_gamma(batch_shape, num_points=20000):\n    d = TruncatedPolyaGamma(prototype=torch.ones(1)).expand(batch_shape)\n\n    # test density approximately normalized\n    x = torch.linspace(1.0e-6, d.truncation_point, num_points).expand(batch_shape + (num_points,))\n    prob = (d.truncation_point / num_points) * torch.logsumexp(d.log_prob(x), dim=-1).exp()\n    assert_close(prob, torch.tensor(1.0).expand(batch_shape), rtol=1.0e-4)\n\n    # test mean of approximate sampler\n    z = d.sample(sample_shape=(3000,))\n    mean = z.mean(-1)\n    assert_close(mean, torch.tensor(0.25).expand(batch_shape), rtol=0.07)\n'"
tests/distributions/test_rejector.py,21,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nfrom pyro.distributions import Exponential, Gamma\nfrom pyro.distributions.testing.rejection_exponential import RejectionExponential\nfrom pyro.distributions.testing.rejection_gamma import (RejectionGamma, RejectionStandardGamma, ShapeAugmentedBeta,\n                                                        ShapeAugmentedGamma)\nfrom tests.common import assert_equal\n\nSIZES = list(map(torch.Size, [[], [1], [2], [3], [1, 1], [1, 2], [2, 3, 4]]))\n\n\n@pytest.mark.parametrize('sample_shape', SIZES)\n@pytest.mark.parametrize('batch_shape', filter(bool, SIZES))\ndef test_rejection_standard_gamma_sample_shape(sample_shape, batch_shape):\n    alphas = torch.ones(batch_shape)\n    dist = RejectionStandardGamma(alphas)\n    x = dist.rsample(sample_shape)\n    assert x.shape == sample_shape + batch_shape\n\n\n@pytest.mark.parametrize('sample_shape', SIZES)\n@pytest.mark.parametrize('batch_shape', filter(bool, SIZES))\ndef test_rejection_exponential_sample_shape(sample_shape, batch_shape):\n    rates = torch.ones(batch_shape)\n    factors = torch.ones(batch_shape) * 0.5\n    dist = RejectionExponential(rates, factors)\n    x = dist.rsample(sample_shape)\n    assert x.shape == sample_shape + batch_shape\n\n\ndef compute_elbo_grad(model, guide, variables):\n    x = guide.rsample()\n    model_log_prob = model.log_prob(x)\n    guide_log_prob, score_function, entropy_term = guide.score_parts(x)\n    log_r = model_log_prob - guide_log_prob\n    surrogate_elbo = model_log_prob + log_r.detach() * score_function - entropy_term\n    return grad(surrogate_elbo.sum(), variables, create_graph=True)\n\n\n@pytest.mark.parametrize('rate', [0.5, 1.0, 2.0])\n@pytest.mark.parametrize('factor', [0.25, 0.5, 1.0])\ndef test_rejector(rate, factor):\n    num_samples = 100000\n    rates = torch.tensor(rate).expand(num_samples, 1)\n    factors = torch.tensor(factor).expand(num_samples, 1)\n\n    dist1 = Exponential(rates)\n    dist2 = RejectionExponential(rates, factors)  # implemented using Rejector\n    x1 = dist1.rsample()\n    x2 = dist2.rsample()\n    assert_equal(x1.mean(), x2.mean(), prec=0.02, msg='bug in .rsample()')\n    assert_equal(x1.std(), x2.std(), prec=0.02, msg='bug in .rsample()')\n    assert_equal(dist1.log_prob(x1), dist2.log_prob(x1), msg='bug in .log_prob()')\n\n\n@pytest.mark.parametrize('rate', [0.5, 1.0, 2.0])\n@pytest.mark.parametrize('factor', [0.25, 0.5, 1.0])\ndef test_exponential_elbo(rate, factor):\n    num_samples = 100000\n    rates = torch.full((num_samples, 1), rate).requires_grad_()\n    factors = torch.full((num_samples, 1), factor).requires_grad_()\n    model = Exponential(torch.ones(num_samples, 1))\n    guide1 = Exponential(rates)\n    guide2 = RejectionExponential(rates, factors)  # implemented using Rejector\n\n    grads = []\n    for guide in [guide1, guide2]:\n        grads.append(compute_elbo_grad(model, guide, [rates])[0])\n    expected, actual = grads\n    assert_equal(actual.mean(), expected.mean(), prec=0.05, msg='bad grad for rate')\n\n    actual = compute_elbo_grad(model, guide2, [factors])[0]\n    assert_equal(actual.mean().item(), 0.0, prec=0.05, msg='bad grad for factor')\n\n\n@pytest.mark.parametrize('alpha', [1.0, 2.0, 5.0])\ndef test_standard_gamma_elbo(alpha):\n    num_samples = 100000\n    alphas = torch.full((num_samples, 1), alpha).requires_grad_()\n    betas = torch.ones(num_samples, 1)\n\n    model = Gamma(torch.ones(num_samples, 1), betas)\n    guide1 = Gamma(alphas, betas)\n    guide2 = RejectionStandardGamma(alphas)  # implemented using Rejector\n\n    grads = []\n    for guide in [guide1, guide2]:\n        grads.append(compute_elbo_grad(model, guide, [alphas])[0].data)\n    expected, actual = grads\n    assert_equal(actual.mean(), expected.mean(), prec=0.01, msg='bad grad for alpha')\n\n\n@pytest.mark.parametrize('alpha', [1.0, 2.0, 5.0])\n@pytest.mark.parametrize('beta', [0.2, 0.5, 1.0, 2.0, 5.0])\ndef test_gamma_elbo(alpha, beta):\n    num_samples = 100000\n    alphas = torch.full((num_samples, 1), alpha).requires_grad_()\n    betas = torch.full((num_samples, 1), beta).requires_grad_()\n\n    model = Gamma(torch.ones(num_samples, 1), torch.ones(num_samples, 1))\n    guide1 = Gamma(alphas, betas)\n    guide2 = RejectionGamma(alphas, betas)  # implemented using Rejector\n\n    grads = []\n    for guide in [guide1, guide2]:\n        grads.append(compute_elbo_grad(model, guide, [alphas, betas]))\n    expected, actual = grads\n    expected = [g.mean() for g in expected]\n    actual = [g.mean() for g in actual]\n    scale = [(1 + abs(g)) for g in expected]\n    assert_equal(actual[0] / scale[0], expected[0] / scale[0], prec=0.01, msg='bad grad for alpha')\n    assert_equal(actual[1] / scale[1], expected[1] / scale[1], prec=0.01, msg='bad grad for beta')\n\n\n@pytest.mark.parametrize('alpha', [0.2, 0.5, 1.0, 2.0, 5.0])\n@pytest.mark.parametrize('beta', [0.2, 0.5, 1.0, 2.0, 5.0])\ndef test_shape_augmented_gamma_elbo(alpha, beta):\n    num_samples = 100000\n    alphas = torch.full((num_samples, 1), alpha).requires_grad_()\n    betas = torch.full((num_samples, 1), beta).requires_grad_()\n\n    model = Gamma(torch.ones(num_samples, 1), torch.ones(num_samples, 1))\n    guide1 = Gamma(alphas, betas)\n    guide2 = ShapeAugmentedGamma(alphas, betas)  # implemented using Rejector\n\n    grads = []\n    for guide in [guide1, guide2]:\n        grads.append(compute_elbo_grad(model, guide, [alphas, betas]))\n    expected, actual = grads\n    expected = [g.mean() for g in expected]\n    actual = [g.mean() for g in actual]\n    scale = [(1 + abs(g)) for g in expected]\n    assert_equal(actual[0] / scale[0], expected[0] / scale[0], prec=0.05, msg='bad grad for alpha')\n    assert_equal(actual[1] / scale[1], expected[1] / scale[1], prec=0.05, msg='bad grad for beta')\n\n\n@pytest.mark.parametrize('alpha', [0.5, 1.0, 4.0])\n@pytest.mark.parametrize('beta', [0.5, 1.0, 4.0])\ndef test_shape_augmented_beta(alpha, beta):\n    num_samples = 10000\n    alphas = torch.full((num_samples, 1), alpha).requires_grad_()\n    betas = torch.full((num_samples, 1), beta).requires_grad_()\n    dist = ShapeAugmentedBeta(alphas, betas)  # implemented using Rejector\n    z = dist.rsample()\n    cost = z.sum()\n    (cost + cost.detach() * dist.score_parts(z)[1]).backward()\n    mean_alpha_grad = alphas.grad.mean().item()\n    mean_beta_grad = betas.grad.mean().item()\n    expected_alpha_grad = beta / (alpha + beta) ** 2\n    expected_beta_grad = -alpha / (alpha + beta) ** 2\n    assert_equal(mean_alpha_grad, expected_alpha_grad, prec=0.02, msg='bad grad for alpha')\n    assert_equal(mean_beta_grad, expected_beta_grad, prec=0.02, msg='bad grad for beta')\n"""
tests/distributions/test_relaxed_straight_through.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.optim as optim\nfrom pyro.distributions import (OneHotCategorical, RelaxedBernoulli, RelaxedBernoulliStraightThrough,\n                                RelaxedOneHotCategorical, RelaxedOneHotCategoricalStraightThrough)\nfrom pyro.infer import SVI, Trace_ELBO\nfrom tests.common import assert_equal\n\nONEHOT_PROBS = [\n                [0.25, 0.75],\n                [0.25, 0.5, 0.25],\n                [[0.25, 0.75], [0.75, 0.25]],\n                [[[0.25, 0.75]], [[0.75, 0.25]]],\n                [0.1] * 10,\n]\n\nBERN_PROBS = [\n               [0.25, 0.75],\n               [[0.25, 0.75], [0.75, 0.25]]\n]\n\n\n@pytest.mark.parametrize(\'probs\', ONEHOT_PROBS)\ndef test_onehot_shapes(probs):\n    temperature = torch.tensor(0.5)\n    probs = torch.tensor(probs, requires_grad=True)\n    d = RelaxedOneHotCategoricalStraightThrough(temperature, probs=probs)\n    sample = d.rsample()\n    log_prob = d.log_prob(sample)\n    grad_probs = grad(log_prob.sum(), [probs])[0]\n    assert grad_probs.shape == probs.shape\n\n\n@pytest.mark.parametrize(\'temp\', [0.3, 0.5, 1.0])\ndef test_onehot_entropy_grad(temp):\n    num_samples = 2000000\n    q = torch.tensor([0.1, 0.2, 0.3, 0.4], requires_grad=True)\n    temp = torch.tensor(temp)\n\n    dist_q = RelaxedOneHotCategorical(temperature=temp, probs=q)\n    z = dist_q.rsample(sample_shape=(num_samples,))\n    expected = grad(dist_q.log_prob(z).sum(), [q])[0] / num_samples\n\n    dist_q = RelaxedOneHotCategoricalStraightThrough(temperature=temp, probs=q)\n    z = dist_q.rsample(sample_shape=(num_samples,))\n    actual = grad(dist_q.log_prob(z).sum(), [q])[0] / num_samples\n\n    assert_equal(expected, actual, prec=0.08,\n                 msg=\'bad grad for RelaxedOneHotCategoricalStraightThrough (expected {}, got {})\'.\n                 format(expected, actual))\n\n\ndef test_onehot_svi_usage():\n\n    def model():\n        p = torch.tensor([0.25] * 4)\n        pyro.sample(\'z\', OneHotCategorical(probs=p))\n\n    def guide():\n        q = pyro.param(\'q\', torch.tensor([0.1, 0.2, 0.3, 0.4]), constraint=constraints.simplex)\n        temp = torch.tensor(0.10)\n        pyro.sample(\'z\', RelaxedOneHotCategoricalStraightThrough(temperature=temp, probs=q))\n\n    adam = optim.Adam({""lr"": .001, ""betas"": (0.95, 0.999)})\n    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n\n    for k in range(6000):\n        svi.step()\n\n    assert_equal(pyro.param(\'q\'), torch.tensor([0.25] * 4), prec=0.01,\n                 msg=\'test svi usage of RelaxedOneHotCategoricalStraightThrough failed\')\n\n\n@pytest.mark.parametrize(\'probs\', BERN_PROBS)\ndef test_bernoulli_shapes(probs):\n    temperature = torch.tensor(0.5)\n    probs = torch.tensor(probs, requires_grad=True)\n    d = RelaxedBernoulliStraightThrough(temperature, probs=probs)\n    sample = d.rsample()\n    log_prob = d.log_prob(sample)\n    grad_probs = grad(log_prob.sum(), [probs])[0]\n    assert grad_probs.shape == probs.shape\n\n\n@pytest.mark.parametrize(\'temp\', [0.5, 1.0])\ndef test_bernoulli_entropy_grad(temp):\n    num_samples = 1500000\n    q = torch.tensor([0.1, 0.2, 0.3, 0.4], requires_grad=True)\n    temp = torch.tensor(temp)\n\n    dist_q = RelaxedBernoulli(temperature=temp, probs=q)\n    z = dist_q.rsample(sample_shape=(num_samples,))\n    expected = grad(dist_q.log_prob(z).sum(), [q])[0] / num_samples\n\n    dist_q = RelaxedBernoulliStraightThrough(temperature=temp, probs=q)\n    z = dist_q.rsample(sample_shape=(num_samples,))\n    actual = grad(dist_q.log_prob(z).sum(), [q])[0] / num_samples\n\n    assert_equal(expected, actual, prec=0.04,\n                 msg=\'bad grad for RelaxedBernoulliStraightThrough (expected {}, got {})\'.\n                 format(expected, actual))\n'"
tests/distributions/test_reshape.py,11,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.distributions.torch import Bernoulli\nfrom tests.common import assert_equal\n\n\ndef test_sample_shape_order():\n    shape12 = torch.Size((1, 2))\n    shape34 = torch.Size((3, 4))\n    d = Bernoulli(0.5)\n\n    # .expand_by(...) should add dimensions on the left.\n    actual = d.expand_by(shape34).expand_by(shape12)\n    expected = d.expand_by(shape12 + shape34)\n    assert actual.event_shape == expected.event_shape\n    assert actual.batch_shape == expected.batch_shape\n\n\n@pytest.mark.parametrize('batch_dim', [0, 1, 2])\n@pytest.mark.parametrize('event_dim', [0, 1, 2])\ndef test_idempotent(batch_dim, event_dim):\n    shape = torch.Size((1, 2, 3, 4))[:batch_dim + event_dim]\n    batch_shape = shape[:batch_dim]\n    event_shape = shape[batch_dim:]\n\n    # Construct a base dist of desired starting shape.\n    dist0 = Bernoulli(0.5).expand_by(shape).to_event(event_dim)\n    assert dist0.batch_shape == batch_shape\n    assert dist0.event_shape == event_shape\n\n    # Check that an .expand_by() an empty shape is a no-op.\n    dist = dist0.expand_by([])\n    assert dist.batch_shape == dist0.batch_shape\n    assert dist.event_shape == dist0.event_shape\n\n\n@pytest.mark.parametrize('sample_dim,extra_event_dims',\n                         [(s, e) for s in range(4) for e in range(4 + s)])\ndef test_reshape(sample_dim, extra_event_dims):\n    batch_dim = 3\n    batch_shape, event_shape = torch.Size((5, 4, 3)), torch.Size()\n    sample_shape = torch.Size((8, 7, 6))[3 - sample_dim:]\n    shape = sample_shape + batch_shape + event_shape\n\n    # Construct a base dist of desired starting shape.\n    dist0 = Bernoulli(0.5 * torch.ones(batch_shape))\n    assert dist0.event_shape == event_shape\n    assert dist0.batch_shape == batch_shape\n\n    # Check that reshaping has the desired final shape.\n    dist = dist0.expand_by(sample_shape).to_event(extra_event_dims)\n    sample = dist.sample()\n    assert sample.shape == shape\n    assert dist.mean.shape == shape\n    assert dist.variance.shape == shape\n    assert dist.log_prob(sample).shape == shape[:sample_dim + batch_dim - extra_event_dims]\n\n    # Check enumerate support.\n    if dist.event_shape:\n        with pytest.raises(NotImplementedError):\n            dist.enumerate_support()\n        with pytest.raises(NotImplementedError):\n            dist.enumerate_support(expand=True)\n        with pytest.raises(NotImplementedError):\n            dist.enumerate_support(expand=False)\n    else:\n        assert dist.enumerate_support().shape == (2,) + shape\n        assert dist.enumerate_support(expand=True).shape == (2,) + shape\n        assert dist.enumerate_support(expand=False).shape == (2,) + (1,) * len(sample_shape + batch_shape) + event_shape\n\n\n@pytest.mark.parametrize('sample_dim,extra_event_dims',\n                         [(s, e) for s in range(3) for e in range(3 + s)])\ndef test_reshape_reshape(sample_dim, extra_event_dims):\n    batch_dim = 2\n    batch_shape, event_shape = torch.Size((6, 5)), torch.Size((4, 3))\n    sample_shape = torch.Size((8, 7))[2 - sample_dim:]\n    shape = sample_shape + batch_shape + event_shape\n\n    # Construct a base dist of desired starting shape.\n    dist0 = Bernoulli(0.5 * torch.ones(event_shape))\n    dist1 = dist0.expand_by(batch_shape).to_event(2)\n    assert dist1.event_shape == event_shape\n    assert dist1.batch_shape == batch_shape\n\n    # Check that reshaping has the desired final shape.\n    dist = dist1.expand_by(sample_shape).to_event(extra_event_dims)\n    sample = dist.sample()\n    assert sample.shape == shape\n    assert dist.mean.shape == shape\n    assert dist.variance.shape == shape\n    assert dist.log_prob(sample).shape == shape[:sample_dim + batch_dim - extra_event_dims]\n\n    # Check enumerate support.\n    if dist.event_shape:\n        with pytest.raises(NotImplementedError):\n            dist.enumerate_support()\n        with pytest.raises(NotImplementedError):\n            dist.enumerate_support(expand=True)\n        with pytest.raises(NotImplementedError):\n            dist.enumerate_support(expand=False)\n    else:\n        assert dist.enumerate_support().shape == (2,) + shape\n        assert dist.enumerate_support(expand=True).shape == (2,) + shape\n        assert dist.enumerate_support(expand=False).shape == (2,) + (1,) * len(sample_shape + batch_shape) + event_shape\n\n\n@pytest.mark.parametrize('sample_dim', [0, 1, 2])\n@pytest.mark.parametrize('batch_dim', [0, 1, 2])\n@pytest.mark.parametrize('event_dim', [0, 1, 2])\ndef test_extra_event_dim_overflow(sample_dim, batch_dim, event_dim):\n    shape = torch.Size(range(sample_dim + batch_dim + event_dim))\n    sample_shape = shape[:sample_dim]\n    batch_shape = shape[sample_dim:sample_dim+batch_dim]\n    event_shape = shape[sample_dim + batch_dim:]\n\n    # Construct a base dist of desired starting shape.\n    dist0 = Bernoulli(0.5).expand_by(batch_shape + event_shape).to_event(event_dim)\n    assert dist0.batch_shape == batch_shape\n    assert dist0.event_shape == event_shape\n\n    # Check .to_event(...) for valid values.\n    for extra_event_dims in range(1 + sample_dim + batch_dim):\n        dist = dist0.expand_by(sample_shape).to_event(extra_event_dims)\n        assert dist.batch_shape == shape[:sample_dim + batch_dim - extra_event_dims]\n        assert dist.event_shape == shape[sample_dim + batch_dim - extra_event_dims:]\n\n    # Check .to_event(...) for invalid values.\n    for extra_event_dims in range(1 + sample_dim + batch_dim, 20):\n        with pytest.raises(ValueError):\n            dist0.expand_by(sample_shape).to_event(extra_event_dims)\n\n\ndef test_independent_entropy():\n    dist_univ = Bernoulli(0.5)\n    dist_multi = Bernoulli(torch.Tensor([0.5, 0.5])).to_event(1)\n    assert_equal(dist_multi.entropy(), 2*dist_univ.entropy())\n"""
tests/distributions/test_shapes.py,19,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro.distributions as dist\n\n\ndef test_categorical_shape():\n    probs = torch.ones(3, 2) / 2\n    d = dist.Categorical(probs)\n    assert d.batch_shape == (3,)\n    assert d.event_shape == ()\n    assert d.shape() == (3,)\n    assert d.sample().size() == d.shape()\n\n\ndef test_one_hot_categorical_shape():\n    probs = torch.ones(3, 2) / 2\n    d = dist.OneHotCategorical(probs)\n    assert d.batch_shape == (3,)\n    assert d.event_shape == (2,)\n    assert d.shape() == (3, 2)\n    assert d.sample().size() == d.shape()\n\n\ndef test_normal_shape():\n    loc = torch.zeros(3, 2)\n    scale = torch.ones(3, 2)\n    d = dist.Normal(loc, scale)\n    assert d.batch_shape == (3, 2)\n    assert d.event_shape == ()\n    assert d.shape() == (3, 2)\n    assert d.sample().size() == d.shape()\n\n\ndef test_dirichlet_shape():\n    alpha = torch.ones(3, 2) / 2\n    d = dist.Dirichlet(alpha)\n    assert d.batch_shape == (3,)\n    assert d.event_shape == (2,)\n    assert d.shape() == (3, 2)\n    assert d.sample().size() == d.shape()\n\n\ndef test_zip_shape():\n    gate = torch.ones(3, 2) / 2\n    rate = torch.ones(3, 2) / 2\n    d = dist.ZeroInflatedPoisson(gate, rate)\n    assert d.batch_shape == (3, 2)\n    assert d.event_shape == ()\n    assert d.shape() == (3, 2)\n    assert d.sample().size() == d.shape()\n\n\ndef test_bernoulli_log_prob_shape():\n    probs = torch.ones(3, 2)\n    x = torch.ones(3, 2)\n    d = dist.Bernoulli(probs)\n    assert d.log_prob(x).size() == (3, 2)\n\n\ndef test_categorical_log_prob_shape():\n    probs = torch.ones(3, 2, 4) / 4\n    x = torch.zeros(3, 2)\n    d = dist.Categorical(probs)\n    assert d.log_prob(x).size() == (3, 2)\n\n\ndef test_one_hot_categorical_log_prob_shape():\n    probs = torch.ones(3, 2, 4) / 4\n    x = torch.zeros(3, 2, 4)\n    x[:, :, 0] = 1\n    d = dist.OneHotCategorical(probs)\n    assert d.log_prob(x).size() == (3, 2)\n\n\ndef test_normal_log_prob_shape():\n    loc = torch.zeros(3, 2)\n    scale = torch.ones(3, 2)\n    x = torch.zeros(3, 2)\n    d = dist.Normal(loc, scale)\n    assert d.log_prob(x).size() == (3, 2)\n\n\ndef test_diag_normal_log_prob_shape():\n    loc1 = torch.zeros(2, 3)\n    loc2 = torch.zeros(2, 4)\n    scale = torch.ones(2, 1)\n    d1 = dist.Normal(loc1, scale.expand_as(loc1)).to_event(1)\n    d2 = dist.Normal(loc2, scale.expand_as(loc2)).to_event(1)\n    x1 = d1.sample()\n    x2 = d2.sample()\n    assert d1.log_prob(x1).size() == (2,)\n    assert d2.log_prob(x2).size() == (2,)\n'"
tests/distributions/test_spanning_tree.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport os\nfrom collections import Counter\n\nimport pytest\nimport torch\n\nimport pyro\nfrom pyro.distributions.spanning_tree import NUM_SPANNING_TREES, SpanningTree, make_complete_graph, sample_tree\nfrom tests.common import assert_equal, xfail_if_not_implemented\n\npytestmark = pytest.mark.skipif(""CUDA_TEST"" in os.environ, reason=""spanning_tree unsupported on CUDA."")\n\n\n@pytest.mark.filterwarnings(""always"")\n@pytest.mark.parametrize(\'num_vertices,expected_grid\', [\n    (2, [[0], [1]]),\n    (3, [[0, 0, 1], [1, 2, 2]]),\n    (4, [[0, 0, 1, 0, 1, 2], [1, 2, 2, 3, 3, 3]]),\n])\n@pytest.mark.parametrize(\'backend\', [""python"", ""cpp""])\ndef test_make_complete_graph(num_vertices, expected_grid, backend):\n    V = num_vertices\n    K = V * (V - 1) // 2\n    expected_grid = torch.tensor(expected_grid, dtype=torch.long).reshape(2, K)\n\n    grid = make_complete_graph(V, backend=backend)\n    assert_equal(grid, expected_grid)\n\n\n@pytest.mark.filterwarnings(""always"")\n@pytest.mark.parametrize(\'num_edges\', [1, 3, 10, 30, 100])\n@pytest.mark.parametrize(\'backend\', [""python"", ""cpp""])\ndef test_sample_tree_mcmc_smoke(num_edges, backend):\n    pyro.set_rng_seed(num_edges)\n    E = num_edges\n    V = 1 + E\n    K = V * (V - 1) // 2\n    edge_logits = torch.rand(K)\n    edges = torch.tensor([(v, v + 1) for v in range(V - 1)], dtype=torch.long)\n    for _ in range(10 if backend == ""cpp"" or num_edges <= 30 else 1):\n        edges = sample_tree(edge_logits, edges, backend=backend)\n\n\n@pytest.mark.filterwarnings(""always"")\n@pytest.mark.parametrize(\'num_edges\', [1, 3, 10, 30, 100])\n@pytest.mark.parametrize(\'backend\', [""python"", ""cpp""])\ndef test_sample_tree_approx_smoke(num_edges, backend):\n    pyro.set_rng_seed(num_edges)\n    E = num_edges\n    V = 1 + E\n    K = V * (V - 1) // 2\n    edge_logits = torch.rand(K)\n    for _ in range(10 if backend == ""cpp"" or num_edges <= 30 else 1):\n        sample_tree(edge_logits, backend=backend)\n\n\n@pytest.mark.parametrize(\'num_edges\', [1, 2, 3, 4, 5, 6])\ndef test_enumerate_support(num_edges):\n    pyro.set_rng_seed(2 ** 32 - num_edges)\n    E = num_edges\n    V = 1 + E\n    K = V * (V - 1) // 2\n    edge_logits = torch.randn(K)\n    d = SpanningTree(edge_logits)\n    with xfail_if_not_implemented():\n        support = d.enumerate_support()\n    assert support.dim() == 3\n    assert support.shape[1:] == d.event_shape\n    assert support.size(0) == NUM_SPANNING_TREES[V]\n\n\n@pytest.mark.parametrize(\'num_edges\', [1, 2, 3, 4, 5, 6])\ndef test_partition_function(num_edges):\n    pyro.set_rng_seed(2 ** 32 - num_edges)\n    E = num_edges\n    V = 1 + E\n    K = V * (V - 1) // 2\n    edge_logits = torch.randn(K)\n    d = SpanningTree(edge_logits)\n    with xfail_if_not_implemented():\n        support = d.enumerate_support()\n    v1 = support[..., 0]\n    v2 = support[..., 1]\n    k = v1 + v2 * (v2 - 1) // 2\n    expected = edge_logits[k].sum(-1).logsumexp(0)\n    actual = d.log_partition_function\n    assert (actual - expected).abs() < 1e-6, (actual, expected)\n\n\n@pytest.mark.parametrize(\'num_edges\', [1, 2, 3, 4, 5, 6])\ndef test_log_prob(num_edges):\n    pyro.set_rng_seed(2 ** 32 - num_edges)\n    E = num_edges\n    V = 1 + E\n    K = V * (V - 1) // 2\n    edge_logits = torch.randn(K)\n    d = SpanningTree(edge_logits)\n    with xfail_if_not_implemented():\n        support = d.enumerate_support()\n    log_probs = d.log_prob(support)\n    assert log_probs.shape == (len(support),)\n    log_total = log_probs.logsumexp(0).item()\n    assert abs(log_total) < 1e-6, log_total\n\n\n@pytest.mark.filterwarnings(""always"")\n@pytest.mark.parametrize(\'pattern\', [""uniform"", ""random"", ""sparse""])\n@pytest.mark.parametrize(\'num_edges\', [1, 2, 3, 4, 5])\n@pytest.mark.parametrize(\'backend\', [""python"", ""cpp""])\n@pytest.mark.parametrize(\'method\', [""mcmc"", ""approx""])\ndef test_sample_tree_gof(method, backend, num_edges, pattern):\n    goftests = pytest.importorskip(\'goftests\')\n    pyro.set_rng_seed(2 ** 32 - num_edges)\n    E = num_edges\n    V = 1 + E\n    K = V * (V - 1) // 2\n\n    if pattern == ""uniform"":\n        edge_logits = torch.zeros(K)\n        num_samples = 10 * NUM_SPANNING_TREES[V]\n    elif pattern == ""random"":\n        edge_logits = torch.rand(K)\n        num_samples = 30 * NUM_SPANNING_TREES[V]\n    elif pattern == ""sparse"":\n        edge_logits = torch.rand(K)\n        for v2 in range(V):\n            for v1 in range(v2):\n                if v1 + 1 < v2:\n                    edge_logits[v1 + v2 * (v2 - 1) // 2] = -float(\'inf\')\n        num_samples = 10 * NUM_SPANNING_TREES[V]\n\n    # Generate many samples.\n    counts = Counter()\n    tensors = {}\n    # Initialize using approximate sampler, to ensure feasibility.\n    edges = sample_tree(edge_logits, backend=backend)\n    for _ in range(num_samples):\n        if method == ""approx"":\n            # Reset the chain with an approximate sample, then perform 1 step of mcmc.\n            edges = sample_tree(edge_logits, backend=backend)\n        edges = sample_tree(edge_logits, edges, backend=backend)\n        key = tuple((v1.item(), v2.item()) for v1, v2 in edges)\n        counts[key] += 1\n        tensors[key] = edges\n    if pattern != ""sparse"":\n        assert len(counts) == NUM_SPANNING_TREES[V]\n\n    # Check accuracy using a Pearson\'s chi-squared test.\n    keys = [k for k, _ in counts.most_common(100)]\n    truncated = (len(keys) < len(counts))\n    counts = torch.tensor([counts[k] for k in keys])\n    tensors = torch.stack([tensors[k] for k in keys])\n    probs = SpanningTree(edge_logits).log_prob(tensors).exp()\n    gof = goftests.multinomial_goodness_of_fit(\n        probs.numpy(), counts.numpy(), num_samples, plot=True, truncated=truncated)\n    logging.info(\'gof = {}\'.format(gof))\n    if method == ""approx"":\n        assert gof >= 0.0001\n    else:\n        assert gof >= 0.005\n'"
tests/distributions/test_stable.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nimport pytest\nimport torch\nfrom scipy.integrate.quadpack import IntegrationWarning\nfrom scipy.stats import ks_2samp, kstest, levy_stable\n\nimport pyro.distributions as dist\nimport pyro.distributions.stable\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (7,), (6, 5)])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)])\ndef test_shape(sample_shape, batch_shape):\n    stability = torch.empty(batch_shape).uniform_(0, 2).requires_grad_()\n    skew = torch.empty(batch_shape).uniform_(-1, 1).requires_grad_()\n    scale = torch.randn(batch_shape).exp().requires_grad_()\n    loc = torch.randn(batch_shape).requires_grad_()\n\n    d = dist.Stable(stability, skew, scale, loc)\n    assert d.batch_shape == batch_shape\n\n    x = d.rsample(sample_shape)\n    assert x.shape == sample_shape + batch_shape\n\n    x.sum().backward()\n\n\n@pytest.mark.parametrize(""beta"", [-1.0, -0.5, 0.0, 0.5, 1.0])\n@pytest.mark.parametrize(""alpha"", [0.1, 0.4, 0.8, 0.99, 1.0, 1.01, 1.3, 1.7, 2.0])\ndef test_sample(alpha, beta):\n    num_samples = 100\n    d = dist.Stable(alpha, beta, coords=""S"")\n\n    def sampler(size):\n        # Temporarily increase radius to test hole-patching logic.\n        # Scipy doesn\'t handle values of alpha very close to 1.\n        try:\n            old = pyro.distributions.stable.RADIUS\n            pyro.distributions.stable.RADIUS = 0.02\n            return d.sample([size])\n        finally:\n            pyro.distributions.stable.RADIUS = old\n\n    def cdf(x):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(""always"", category=IntegrationWarning)\n            result = levy_stable.cdf(x, alpha, beta)\n        # Scipy has only an experimental .cdf() function for alpha=1, beta!=0.\n        # It sometimes passes and sometimes xfails.\n        if w and alpha == 1 and beta != 0:\n            pytest.xfail(reason=""scipy.stats.levy_stable.cdf is unstable"")\n        return result\n\n    assert kstest(sampler, cdf, N=num_samples).pvalue > 0.1\n\n\n@pytest.mark.parametrize(""beta"", [-1.0, -0.5, 0.0, 0.5, 1.0])\n@pytest.mark.parametrize(""alpha"", [\n    0.1, 0.4, 0.8, 0.99,\n    0.999999, 1.000001,  # scipy sampler is buggy very close to 1\n    1.01, 1.3, 1.7, 2.0,\n])\ndef test_sample_2(alpha, beta):\n    num_samples = 10000\n\n    d = dist.Stable(alpha, beta, coords=""S"")\n    # Temporarily increase radius to test hole-patching logic.\n    # Scipy doesn\'t handle values of alpha very close to 1.\n    try:\n        old = pyro.distributions.stable.RADIUS\n        pyro.distributions.stable.RADIUS = 0.02\n        actual = d.sample([num_samples])\n    finally:\n        pyro.distributions.stable.RADIUS = old\n    actual = d.sample([num_samples])\n\n    expected = levy_stable.rvs(alpha, beta, size=num_samples)\n\n    assert ks_2samp(expected, actual).pvalue > 0.05\n\n\n@pytest.mark.parametrize(""loc"", [0, 1, -1, 2, 2])\n@pytest.mark.parametrize(""scale"", [0.5, 1, 2])\ndef test_normal(loc, scale):\n    num_samples = 100000\n    expected = dist.Normal(loc, scale).sample([num_samples])\n    actual = dist.Stable(2, 0, scale * 0.5 ** 0.5, loc).sample([num_samples])\n    assert_close(actual.mean(), expected.mean(), atol=0.01)\n    assert_close(actual.std(), expected.std(), atol=0.01)\n\n\n@pytest.mark.parametrize(""skew0"", [-0.9, -0.5, 0.0, 0.5, 0.9])\n@pytest.mark.parametrize(""skew1"", [-0.9, -0.5, 0.0, 0.5, 0.9])\n@pytest.mark.parametrize(""scale0,scale1"", [(0.1, 0.9), (0.2, 0.8), (0.4, 0.6), (0.5, 0.5)])\n@pytest.mark.parametrize(""stability"", [0.5, 0.99, 1.01, 1.5, 1.9])\ndef test_additive(stability, skew0, skew1, scale0, scale1):\n    num_samples = 10000\n    d0 = dist.Stable(stability, skew0, scale0, coords=""S"")\n    d1 = dist.Stable(stability, skew1, scale1, coords=""S"")\n    expected = d0.sample([num_samples]) + d1.sample([num_samples])\n\n    scale = (scale0 ** stability + scale1 ** stability) ** (1 / stability)\n    skew = ((skew0 * scale0 ** stability + skew1 * scale1 ** stability) /\n            (scale0 ** stability + scale1 ** stability))\n    d = dist.Stable(stability, skew, scale, coords=""S"")\n    actual = d.sample([num_samples])\n\n    assert ks_2samp(expected, actual).pvalue > 0.05\n\n\n@pytest.mark.parametrize(""scale"", [0.5, 1.5])\n@pytest.mark.parametrize(""skew"", [-0.5, 0.0, 0.5, 0.9])\n@pytest.mark.parametrize(""stability"", [0.5, 1.0, 1.7, 2.0])\n@pytest.mark.parametrize(""coords"", [""S0"", ""S""])\ndef test_mean(stability, skew, scale, coords):\n    loc = torch.randn(10)\n    d = dist.Stable(stability, skew, scale, loc, coords=coords)\n    if stability <= 1:\n        assert torch.isnan(d.mean).all()\n    else:\n        expected = d.sample((100000,)).mean(0)\n        assert_close(d.mean, expected, atol=0.1)\n\n\n@pytest.mark.parametrize(""scale"", [0.5, 1.5])\n@pytest.mark.parametrize(""stability"", [1.7, 2.0])\ndef test_variance(stability, scale):\n    skew = dist.Uniform(-1, 1).sample((10,))\n    loc = torch.randn(10)\n    d = dist.Stable(stability, skew, scale, loc)\n    if stability < 2:\n        assert torch.isinf(d.variance).all()\n    else:\n        expected = d.sample((100000,)).var(0)\n        assert_close(d.variance, expected, rtol=0.02)\n'"
tests/distributions/test_tensor_type.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport scipy.stats as sp\nimport torch\n\nimport pyro.distributions as dist\nfrom tests.common import assert_equal\n\n\n@pytest.fixture()\ndef test_data():\n    return torch.DoubleTensor([0.4])\n\n\n@pytest.fixture()\ndef alpha():\n    """"""\n    alpha parameter for the Beta distribution.\n    """"""\n    return torch.DoubleTensor([2.4])\n\n\n@pytest.fixture()\ndef beta():\n    """"""\n    beta parameter for the Beta distribution.\n    """"""\n    return torch.DoubleTensor([3.7])\n\n\n@pytest.fixture()\ndef float_test_data(test_data):\n    return torch.FloatTensor(test_data.detach().cpu().numpy())\n\n\n@pytest.fixture()\ndef float_alpha(alpha):\n    return torch.FloatTensor(alpha.detach().cpu().numpy())\n\n\n@pytest.fixture()\ndef float_beta(beta):\n    return torch.FloatTensor(beta.detach().cpu().numpy())\n\n\ndef test_double_type(test_data, alpha, beta):\n    log_px_torch = dist.Beta(alpha, beta).log_prob(test_data).data\n    assert isinstance(log_px_torch, torch.DoubleTensor)\n    log_px_val = log_px_torch.numpy()\n    log_px_np = sp.beta.logpdf(\n        test_data.detach().cpu().numpy(),\n        alpha.detach().cpu().numpy(),\n        beta.detach().cpu().numpy())\n    assert_equal(log_px_val, log_px_np, prec=1e-4)\n\n\ndef test_float_type(float_test_data, float_alpha, float_beta, test_data, alpha, beta):\n    log_px_torch = dist.Beta(float_alpha, float_beta).log_prob(float_test_data).data\n    assert isinstance(log_px_torch, torch.FloatTensor)\n    log_px_val = log_px_torch.numpy()\n    log_px_np = sp.beta.logpdf(\n        test_data.detach().cpu().numpy(),\n        alpha.detach().cpu().numpy(),\n        beta.detach().cpu().numpy())\n    assert_equal(log_px_val, log_px_np, prec=1e-4)\n\n\ndef test_conflicting_types(test_data, float_alpha, beta):\n    with pytest.raises((TypeError, RuntimeError)):\n        dist.Beta(float_alpha, beta).log_prob(test_data)\n'"
tests/distributions/test_torch_patch.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom tests.common import assert_close, requires_cuda\n\n\n@requires_cuda\ndef test_dirichlet_grad_cuda():\n    concentration = torch.ones(3, requires_grad=True)\n    dist.Dirichlet(concentration).rsample().sum().backward()\n\n\n@requires_cuda\ndef test_linspace():\n    x = torch.linspace(-1., 1., 100, device=""cuda"")\n    assert x.device.type == ""cuda""\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3, 4])\ndef test_lower_cholesky_transform(batch_shape, dim):\n    t = torch.distributions.transform_to(torch.distributions.constraints.lower_cholesky)\n    x = torch.randn(batch_shape + (dim, dim))\n    y = t(x)\n    assert y.shape == x.shape\n    actual = y.matmul(y.transpose(-1, -2)).cholesky()\n    assert_close(actual, y)\n    x2 = t.inv(y)\n    assert x2.shape == x.shape\n    y2 = t(x2)\n    assert_close(y2, y)\n'"
tests/distributions/test_transforms.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom unittest import TestCase\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nimport pyro.distributions.transforms as T\n\nfrom functools import partial, reduce\nimport operator\n\npytestmark = pytest.mark.init(rng_seed=123)\n\n\nclass Flatten(dist.TransformModule):\n    """"""\n    Used to handle transforms with `event_dim > 1` until we have a Reshape transform in PyTorch\n    """"""\n    event_dim = 1\n\n    def __init__(self, transform, input_shape):\n        super().__init__(cache_size=1)\n        assert(transform.event_dim == len(input_shape))\n\n        self.transform = transform\n        self.input_shape = input_shape\n\n    def _unflatten(self, x):\n        return x.view(x.shape[:-1] + self.input_shape)\n\n    def _call(self, x):\n        return self.transform._call(self._unflatten(x)).view_as(x)\n\n    def _inverse(self, x):\n        return self.transform._inverse(self._unflatten(x)).view_as(x)\n\n    def log_abs_det_jacobian(self, x, y):\n        return self.transform.log_abs_det_jacobian(self._unflatten(x), self._unflatten(y))\n\n\nclass TransformTests(TestCase):\n    def setUp(self):\n        # Epsilon is used to compare numerical gradient to analytical one\n        self.epsilon = 1e-4\n\n        # Delta is tolerance for testing inverse, f(f^{-1}(x)) = x\n        self.delta = 1e-6\n\n    def _test_jacobian(self, input_dim, transform):\n        jacobian = torch.zeros(input_dim, input_dim)\n\n        def nonzero(x):\n            return torch.sign(torch.abs(x))\n\n        x = torch.randn(1, input_dim)\n        y = transform(x)\n        if transform.event_dim == 1:\n            analytic_ldt = transform.log_abs_det_jacobian(x, y).data\n        else:\n            analytic_ldt = transform.log_abs_det_jacobian(x, y).sum(-1).data\n\n        for j in range(input_dim):\n            for k in range(input_dim):\n                epsilon_vector = torch.zeros(1, input_dim)\n                epsilon_vector[0, j] = self.epsilon\n                delta = (transform(x + 0.5 * epsilon_vector) - transform(x - 0.5 * epsilon_vector)) / self.epsilon\n                jacobian[j, k] = float(delta[0, k].data.sum())\n\n        # Apply permutation for autoregressive flows with a network\n        if hasattr(transform, \'arn\') and \'get_permutation\' in dir(transform.arn):\n            permutation = transform.arn.get_permutation()\n            permuted_jacobian = jacobian.clone()\n            for j in range(input_dim):\n                for k in range(input_dim):\n                    permuted_jacobian[j, k] = jacobian[permutation[j], permutation[k]]\n            jacobian = permuted_jacobian\n\n        # For autoregressive flow, Jacobian is sum of diagonal, otherwise need full determinate\n        if hasattr(transform, \'autoregressive\') and transform.autoregressive:\n            numeric_ldt = torch.sum(torch.log(torch.diag(jacobian)))\n        else:\n            numeric_ldt = torch.log(torch.abs(jacobian.det()))\n\n        ldt_discrepancy = (analytic_ldt - numeric_ldt).abs()\n        assert ldt_discrepancy < self.epsilon\n\n        # Test that lower triangular with unit diagonal for autoregressive flows\n        if hasattr(transform, \'autoregressive\'):\n            diag_sum = torch.sum(torch.diag(nonzero(jacobian)))\n            lower_sum = torch.sum(torch.tril(nonzero(jacobian), diagonal=-1))\n            assert diag_sum == float(input_dim)\n            assert lower_sum == float(0.0)\n\n    def _test_inverse(self, shape, transform):\n        # Test g^{-1}(g(x)) = x\n        # NOTE: Calling _call and _inverse directly bypasses caching\n        base_dist = dist.Normal(torch.zeros(shape), torch.ones(shape))\n        x_true = base_dist.sample(torch.Size([10]))\n        y = transform._call(x_true)\n        J_1 = transform.log_abs_det_jacobian(x_true, y)\n        x_calculated = transform._inverse(y)\n        J_2 = transform.log_abs_det_jacobian(x_true, y)\n        assert (x_true - x_calculated).abs().max().item() < self.delta\n\n        # Test that Jacobian after inverse op is same as after forward\n        assert (J_1 - J_2).abs().max().item() < self.delta\n\n    def _test_shape(self, base_shape, transform):\n        base_dist = dist.Normal(torch.zeros(base_shape), torch.ones(base_shape))\n        sample = dist.TransformedDistribution(base_dist, [transform]).sample()\n        assert sample.shape == base_shape\n\n    def _test(self, transform_factory, shape=True, jacobian=True, inverse=True, event_dim=1):\n        for event_shape in [(2,), (5,)]:\n            if event_dim > 1:\n                event_shape = tuple([event_shape[0] + i for i in range(event_dim)])\n            transform = transform_factory(event_shape[0] if len(event_shape) == 1 else event_shape)\n\n            if inverse:\n                self._test_inverse(event_shape, transform)\n            if shape:\n                for shape in [(3,), (3, 4), (3, 4, 5)]:\n                    base_shape = shape + event_shape\n                    self._test_shape(base_shape, transform)\n            if jacobian:\n                if event_dim > 1:\n                    transform = Flatten(transform, event_shape)\n                self._test_jacobian(reduce(operator.mul, event_shape, 1), transform)\n\n    def _test_conditional(self, conditional_transform_factory, context_dim=3, event_dim=1, **kwargs):\n        def transform_factory(input_dim, context_dim=context_dim):\n            z = torch.rand(1, context_dim)\n            return conditional_transform_factory(input_dim, context_dim).condition(z)\n        self._test(transform_factory, event_dim=event_dim, **kwargs)\n\n    def test_affine_autoregressive(self):\n        for stable in [True, False]:\n            self._test(partial(T.affine_autoregressive, stable=stable))\n\n    def test_affine_coupling(self):\n        for dim in [-1, -2]:\n            self._test(partial(T.affine_coupling, dim=dim), event_dim=-dim)\n\n    def test_batchnorm(self):\n        # Need to make moving average statistics non-zeros/ones and set to eval so inverse is valid\n        # (see the docs about the differing behaviour of BatchNorm in train and eval modes)\n        def transform_factory(input_dim):\n            transform = T.batchnorm(input_dim)\n            transform._inverse(torch.normal(torch.arange(0., input_dim), torch.arange(1., 1. + input_dim) / input_dim))\n            transform.eval()\n            return transform\n\n        self._test(transform_factory)\n\n    def test_block_autoregressive_jacobians(self):\n        for activation in [\'ELU\', \'LeakyReLU\', \'sigmoid\', \'tanh\']:\n            self._test(partial(T.block_autoregressive, activation=activation), inverse=False)\n\n        for residual in [None, \'normal\', \'gated\']:\n            self._test(partial(T.block_autoregressive, residual=residual), inverse=False)\n\n    def test_conditional_affine_autoregressive(self):\n        self._test_conditional(T.conditional_affine_autoregressive)\n\n    def test_conditional_affine_coupling(self):\n        for dim in [-1, -2]:\n            self._test_conditional(partial(T.conditional_affine_coupling, dim=dim), event_dim=-dim)\n\n    def test_conditional_generalized_channel_permute(self, context_dim=3):\n        for shape in [(3, 16, 16), (1, 3, 32, 32), (2, 5, 3, 64, 64)]:\n            # NOTE: Without changing the interface to generalized_channel_permute I can\'t reuse general\n            # test for `event_dim > 1` transforms\n            z = torch.rand(context_dim)\n            transform = T.conditional_generalized_channel_permute(context_dim=3, channels=shape[-3]).condition(z)\n            self._test_shape(shape, transform)\n            self._test_inverse(shape, transform)\n\n        for width_dim in [2, 4, 6]:\n            input_dim = (width_dim**2) * 3\n            self._test_jacobian(input_dim, Flatten(transform, (3, width_dim, width_dim)))\n\n    def test_conditional_householder(self):\n        self._test_conditional(T.conditional_householder)\n        self._test_conditional(partial(T.conditional_householder, count_transforms=2))\n\n    def test_conditional_neural_autoregressive(self):\n        self._test_conditional(T.conditional_neural_autoregressive, inverse=False)\n\n    def test_conditional_planar(self):\n        self._test_conditional(T.conditional_planar, inverse=False)\n\n    def test_conditional_radial(self):\n        self._test_conditional(T.conditional_radial, inverse=False)\n\n    def test_conditional_spline(self):\n        self._test_conditional(T.conditional_spline)\n\n    def test_discrete_cosine(self):\n        # NOTE: Need following since helper function unimplemented\n        self._test(lambda input_dim: T.DiscreteCosineTransform())\n        self._test(lambda input_dim: T.DiscreteCosineTransform(smooth=0.5))\n        self._test(lambda input_dim: T.DiscreteCosineTransform(smooth=1.0))\n        self._test(lambda input_dim: T.DiscreteCosineTransform(smooth=2.0))\n\n    def test_haar_transform(self):\n        # NOTE: Need following since helper function unimplemented\n        self._test(lambda input_dim: T.HaarTransform(flip=True))\n        self._test(lambda input_dim: T.HaarTransform(flip=False))\n\n    def test_elu(self):\n        # NOTE: Need following since helper function mistakenly doesn\'t take input dim\n        self._test(lambda input_dim: T.elu())\n\n    def test_generalized_channel_permute(self):\n        for shape in [(3, 16, 16), (1, 3, 32, 32), (2, 5, 3, 64, 64)]:\n            # NOTE: Without changing the interface to generalized_channel_permute I can\'t reuse general\n            # test for `event_dim > 1` transforms\n            transform = T.generalized_channel_permute(channels=shape[-3])\n            self._test_shape(shape, transform)\n            self._test_inverse(shape, transform)\n\n        for width_dim in [2, 4, 6]:\n            input_dim = (width_dim**2) * 3\n            self._test_jacobian(input_dim, Flatten(transform, (3, width_dim, width_dim)))\n\n    def test_householder(self):\n        self._test(partial(T.householder, count_transforms=2))\n\n    def test_leaky_relu(self):\n        # NOTE: Need following since helper function mistakenly doesn\'t take input dim\n        self._test(lambda input_dim: T.leaky_relu())\n\n    def test_lower_cholesky_affine(self):\n        # NOTE: Need following since helper function unimplemented\n        def transform_factory(input_dim):\n            loc = torch.randn(input_dim)\n            scale_tril = torch.randn(input_dim).exp().diag() + 0.03 * torch.randn(input_dim, input_dim)\n            scale_tril = scale_tril.tril(0)\n            return T.LowerCholeskyAffine(loc, scale_tril)\n\n        self._test(transform_factory)\n\n    def test_neural_autoregressive(self):\n        for activation in [\'ELU\', \'LeakyReLU\', \'sigmoid\', \'tanh\']:\n            self._test(partial(T.neural_autoregressive, activation=activation), inverse=False)\n\n    def test_permute(self):\n        for dim in [-1, -2]:\n            self._test(partial(T.permute, dim=dim), event_dim=-dim)\n\n    def test_planar(self):\n        self._test(T.planar, inverse=False)\n\n    def test_polynomial(self):\n        self._test(T.polynomial, inverse=False)\n\n    def test_radial(self):\n        self._test(T.radial, inverse=False)\n\n    def test_spline(self):\n        self._test(T.spline)\n\n    def test_sylvester(self):\n        self._test(T.sylvester, inverse=False)\n'"
tests/distributions/test_unit.py,2,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize('batch_shape', [(), (4,), (3, 2)])\ndef test_shapes(batch_shape):\n    log_factor = torch.randn(batch_shape)\n\n    d = dist.Unit(log_factor=log_factor)\n    x = d.sample()\n    assert x.shape == batch_shape + (0,)\n    assert (d.log_prob(x) == log_factor).all()\n\n\n@pytest.mark.parametrize('sample_shape', [(), (4,), (3, 2)])\n@pytest.mark.parametrize('batch_shape', [(), (7,), (6, 5)])\ndef test_expand(sample_shape, batch_shape):\n    log_factor = torch.randn(batch_shape)\n    d1 = dist.Unit(log_factor)\n    v1 = d1.sample()\n\n    d2 = d1.expand(sample_shape + batch_shape)\n    assert d2.batch_shape == sample_shape + batch_shape\n    v2 = d2.sample()\n    assert v2.shape == sample_shape + batch_shape + (0,)\n    assert_equal(d1.log_prob(v2), d2.log_prob(v1))\n"""
tests/distributions/test_util.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport weakref\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom pyro.distributions.util import broadcast_shape, sum_leftmost, sum_rightmost, weakmethod\n\nINF = float(\'inf\')\n\n\n@pytest.mark.parametrize(\'shapes\', [\n    ([],),\n    ([1],),\n    ([2],),\n    ([], []),\n    ([], [1]),\n    ([], [2]),\n    ([1], []),\n    ([2], []),\n    ([1], [2]),\n    ([2], [1]),\n    ([2], [2]),\n    ([2], [3, 1]),\n    ([2, 1], [3]),\n    ([2, 1], [1, 3]),\n    ([1, 2, 4, 1, 3], [6, 7, 1, 1, 5, 1]),\n    ([], [3, 1], [2], [4, 3, 1], [5, 4, 1, 1]),\n])\ndef test_broadcast_shape(shapes):\n    assert broadcast_shape(*shapes) == np.broadcast(*map(np.empty, shapes)).shape\n\n\n@pytest.mark.parametrize(\'shapes\', [\n    ([3], [4]),\n    ([2, 1], [1, 3, 1]),\n])\ndef test_broadcast_shape_error(shapes):\n    with pytest.raises((ValueError, RuntimeError)):\n        broadcast_shape(*shapes)\n\n\n@pytest.mark.parametrize(\'shapes\', [\n    ([],),\n    ([1],),\n    ([2],),\n    ([], []),\n    ([], [1]),\n    ([], [2]),\n    ([1], []),\n    ([2], []),\n    ([1], [1]),\n    ([2], [2]),\n    ([2], [2]),\n    ([2], [3, 2]),\n    ([2, 3], [3]),\n    ([2, 3], [2, 3]),\n    ([4], [1, 2, 3, 4], [2, 3, 4], [3, 4]),\n])\ndef test_broadcast_shape_strict(shapes):\n    assert broadcast_shape(*shapes, strict=True) == np.broadcast(*map(np.empty, shapes)).shape\n\n\n@pytest.mark.parametrize(\'shapes\', [\n    ([1], [2]),\n    ([2], [1]),\n    ([3], [4]),\n    ([2], [3, 1]),\n    ([2, 1], [3]),\n    ([2, 1], [1, 3]),\n    ([2, 1], [1, 3, 1]),\n    ([1, 2, 4, 1, 3], [6, 7, 1, 1, 5, 1]),\n    ([], [3, 1], [2], [4, 3, 1], [5, 4, 1, 1]),\n])\ndef test_broadcast_shape_strict_error(shapes):\n    with pytest.raises(ValueError):\n        broadcast_shape(*shapes, strict=True)\n\n\ndef test_sum_rightmost():\n    x = torch.ones(2, 3, 4)\n    assert sum_rightmost(x, 0).shape == (2, 3, 4)\n    assert sum_rightmost(x, 1).shape == (2, 3)\n    assert sum_rightmost(x, 2).shape == (2,)\n    assert sum_rightmost(x, -1).shape == (2,)\n    assert sum_rightmost(x, -2).shape == (2, 3)\n    assert sum_rightmost(x, INF).shape == ()\n\n\ndef test_sum_leftmost():\n    x = torch.ones(2, 3, 4)\n    assert sum_leftmost(x, 0).shape == (2, 3, 4)\n    assert sum_leftmost(x, 1).shape == (3, 4)\n    assert sum_leftmost(x, 2).shape == (4,)\n    assert sum_leftmost(x, -1).shape == (4,)\n    assert sum_leftmost(x, -2).shape == (3, 4)\n    assert sum_leftmost(x, INF).shape == ()\n\n\ndef test_weakmethod():\n\n    class Foo:\n        def __init__(self, state):\n            self.state = state\n            self.method = self._method\n\n        @weakmethod\n        def _method(self, *args, **kwargs):\n            return self.state, args, kwargs\n\n    foo = Foo(42)\n    assert foo.method(1, 2, 3, x=0) == (42, (1, 2, 3), {""x"": 0})\n\n    foo_ref = weakref.ref(foo)\n    assert foo_ref() is foo\n    del foo\n    assert foo_ref() is None\n'"
tests/distributions/test_von_mises.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport os\n\nimport pytest\nimport torch\nfrom torch import optim\n\nfrom pyro.distributions import VonMises, VonMises3D\nfrom tests.common import skipif_param\n\n\ndef _eval_poly(y, coef):\n    coef = list(coef)\n    result = coef.pop()\n    while coef:\n        result = coef.pop() + y * result\n    return result\n\n\n_I0_COEF_SMALL = [1.0, 3.5156229, 3.0899424, 1.2067492, 0.2659732, 0.360768e-1, 0.45813e-2]\n_I0_COEF_LARGE = [0.39894228, 0.1328592e-1, 0.225319e-2, -0.157565e-2, 0.916281e-2,\n                  -0.2057706e-1, 0.2635537e-1, -0.1647633e-1,  0.392377e-2]\n_I1_COEF_SMALL = [0.5, 0.87890594, 0.51498869, 0.15084934, 0.2658733e-1, 0.301532e-2, 0.32411e-3]\n_I1_COEF_LARGE = [0.39894228, -0.3988024e-1, -0.362018e-2, 0.163801e-2, -0.1031555e-1,\n                  0.2282967e-1, -0.2895312e-1, 0.1787654e-1, -0.420059e-2]\n\n_COEF_SMALL = [_I0_COEF_SMALL, _I1_COEF_SMALL]\n_COEF_LARGE = [_I0_COEF_LARGE, _I1_COEF_LARGE]\n\n\ndef _log_modified_bessel_fn(x, order=0):\n    """"""\n    Returns ``log(I_order(x))`` for ``x > 0``,\n    where `order` is either 0 or 1.\n    """"""\n    assert order == 0 or order == 1\n\n    # compute small solution\n    y = (x / 3.75).pow(2)\n    small = _eval_poly(y, _COEF_SMALL[order])\n    if order == 1:\n        small = x.abs() * small\n    small = small.log()\n\n    # compute large solution\n    y = 3.75 / x\n    large = x - 0.5 * x.log() + _eval_poly(y, _COEF_LARGE[order]).log()\n\n    mask = (x < 3.75)\n    result = large\n    if mask.any():\n        result[mask] = small[mask]\n    return result\n\n\ndef _fit_params_from_samples(samples, n_iter):\n    assert samples.dim() == 1\n    samples_count = samples.size(0)\n    samples_cs = samples.cos().sum()\n    samples_ss = samples.sin().sum()\n    mu = torch.atan2(samples_ss / samples_count, samples_cs / samples_count)\n    samples_r = (samples_cs ** 2 + samples_ss ** 2).sqrt() / samples_count\n    # From Banerjee, Arindam, et al.\n    # ""Clustering on the unit hypersphere using von Mises-Fisher distributions.""\n    # Journal of Machine Learning Research 6.Sep (2005): 1345-1382.\n    # By mic (https://stats.stackexchange.com/users/67168/mic),\n    # Estimating kappa of von Mises distribution, URL (version: 2015-06-12):\n    # https://stats.stackexchange.com/q/156692\n    kappa = (samples_r * 2 - samples_r ** 3) / (1 - samples_r ** 2)\n    lr = 1e-2\n    kappa.requires_grad = True\n    bfgs = optim.LBFGS([kappa], lr=lr)\n\n    def bfgs_closure():\n        bfgs.zero_grad()\n        obj = (_log_modified_bessel_fn(kappa, order=1)\n               - _log_modified_bessel_fn(kappa, order=0))\n        obj = (obj - samples_r.log()).abs()\n        obj.backward()\n        return obj\n\n    for i in range(n_iter):\n        bfgs.step(bfgs_closure)\n    return mu, kappa.detach()\n\n\n@pytest.mark.parametrize(\'loc\', [-math.pi/2.0, 0.0, math.pi/2.0])\n@pytest.mark.parametrize(\'concentration\', [skipif_param(0.01, condition=\'CUDA_TEST\' in os.environ,\n                                                        reason=\'low precision.\'),\n                                           0.03, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0, 100.0])\ndef test_sample(loc, concentration, n_samples=int(1e6), n_iter=50):\n    prob = VonMises(loc, concentration)\n    samples = prob.sample((n_samples,))\n    mu, kappa = _fit_params_from_samples(samples, n_iter=n_iter)\n    assert abs(loc - mu) < 0.1\n    assert abs(concentration - kappa) < concentration * 0.1\n\n\n@pytest.mark.parametrize(\'concentration\', [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0, 30.0, 100.0])\ndef test_log_prob_normalized(concentration):\n    grid = torch.arange(0., 2 * math.pi, 1e-4)\n    prob = VonMises(0.0, concentration).log_prob(grid).exp()\n    norm = prob.mean().item() * 2 * math.pi\n    assert abs(norm - 1) < 1e-3, norm\n\n\n@pytest.mark.parametrize(\'scale\', [0.1, 0.5, 0.9, 1.0, 1.1, 2.0, 10.0])\ndef test_von_mises_3d(scale):\n    concentration = torch.randn(3)\n    concentration = concentration * (scale / concentration.norm(2))\n\n    num_samples = 100000\n    samples = torch.randn(num_samples, 3)\n    samples = samples / samples.norm(2, dim=-1, keepdim=True)\n\n    d = VonMises3D(concentration, validate_args=True)\n    actual_total = d.log_prob(samples).exp().mean()\n    expected_total = 1 / (4 * math.pi)\n    ratio = actual_total / expected_total\n    assert torch.abs(ratio - 1) < 0.01, ratio\n'"
tests/distributions/test_zero_inflated.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.distributions import (\n    ZeroInflatedPoisson,\n    Poisson,\n    ZeroInflatedNegativeBinomial,\n    NegativeBinomial,\n    Delta,\n)\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""rate"", [0.1, 0.5, 0.9, 1.0, 1.1, 2.0, 10.0])\ndef test_zip_0_gate(rate):\n    # if gate is 0 ZIP is Poisson\n    zip_ = ZeroInflatedPoisson(torch.zeros(1), torch.tensor(rate))\n    pois = Poisson(torch.tensor(rate))\n    s = pois.sample((20,))\n    zip_prob = zip_.log_prob(s)\n    pois_prob = pois.log_prob(s)\n    assert_close(zip_prob, pois_prob)\n\n\n@pytest.mark.parametrize(""rate"", [0.1, 0.5, 0.9, 1.0, 1.1, 2.0, 10.0])\ndef test_zip_1_gate(rate):\n    # if gate is 1 ZIP is Delta(0)\n    zip_ = ZeroInflatedPoisson(torch.ones(1), torch.tensor(rate))\n    delta = Delta(torch.zeros(1))\n    s = torch.tensor([0.0, 1.0])\n    zip_prob = zip_.log_prob(s)\n    delta_prob = delta.log_prob(s)\n    assert_close(zip_prob, delta_prob)\n\n\n@pytest.mark.parametrize(""gate"", [0.0, 0.25, 0.5, 0.75, 1.0])\n@pytest.mark.parametrize(""rate"", [0.1, 0.5, 0.9, 1.0, 1.1, 2.0, 10.0])\ndef test_zip_mean_variance(gate, rate):\n    num_samples = 1000000\n    zip_ = ZeroInflatedPoisson(torch.tensor(gate), torch.tensor(rate))\n    s = zip_.sample((num_samples,))\n    expected_mean = zip_.mean\n    estimated_mean = s.mean()\n    expected_std = zip_.stddev\n    estimated_std = s.std()\n    assert_close(expected_mean, estimated_mean, atol=1e-02)\n    assert_close(expected_std, estimated_std, atol=1e-02)\n\n\n@pytest.mark.parametrize(""total_count"", [0.1, 0.5, 0.9, 1.0, 1.1, 2.0, 10.0])\n@pytest.mark.parametrize(""probs"", [0.1, 0.5, 0.9])\ndef test_zinb_0_gate(total_count, probs):\n    # if gate is 0 ZINB is NegativeBinomial\n    zinb_ = ZeroInflatedNegativeBinomial(\n        torch.zeros(1), total_count=torch.tensor(total_count), probs=torch.tensor(probs)\n    )\n    neg_bin = NegativeBinomial(torch.tensor(total_count), probs=torch.tensor(probs))\n    s = neg_bin.sample((20,))\n    zinb_prob = zinb_.log_prob(s)\n    neg_bin_prob = neg_bin.log_prob(s)\n    assert_close(zinb_prob, neg_bin_prob)\n\n\n@pytest.mark.parametrize(""total_count"", [0.1, 0.5, 0.9, 1.0, 1.1, 2.0, 10.0])\n@pytest.mark.parametrize(""probs"", [0.1, 0.5, 0.9])\ndef test_zinb_1_gate(total_count, probs):\n    # if gate is 1 ZINB is Delta(0)\n    zinb_ = ZeroInflatedNegativeBinomial(\n        torch.ones(1), total_count=torch.tensor(total_count), probs=torch.tensor(probs)\n    )\n    delta = Delta(torch.zeros(1))\n    s = torch.tensor([0.0, 1.0])\n    zinb_prob = zinb_.log_prob(s)\n    delta_prob = delta.log_prob(s)\n    assert_close(zinb_prob, delta_prob)\n\n\n@pytest.mark.parametrize(""gate"", [0.0, 0.25, 0.5, 0.75, 1.0])\n@pytest.mark.parametrize(""total_count"", [0.1, 0.5, 0.9, 1.0, 1.1, 2.0, 10.0])\n@pytest.mark.parametrize(""logits"", [-0.5, 0.5, -0.9, 1.9])\ndef test_zinb_mean_variance(gate, total_count, logits):\n    num_samples = 1000000\n    zinb_ = ZeroInflatedNegativeBinomial(\n        torch.tensor(gate),\n        total_count=torch.tensor(total_count),\n        logits=torch.tensor(logits),\n    )\n    s = zinb_.sample((num_samples,))\n    expected_mean = zinb_.mean\n    estimated_mean = s.mean()\n    expected_std = zinb_.stddev\n    estimated_std = s.std()\n    assert_close(expected_mean, estimated_mean, atol=1e-01)\n    assert_close(expected_std, estimated_std, atol=1e-1)\n'"
tests/infer/__init__.py,0,b''
tests/infer/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/infer""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""unit""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/infer/test_abstract_infer.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim as optim\nimport pyro.poutine as poutine\nfrom pyro.infer.autoguide import AutoLaplaceApproximation\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.infer.mcmc import MCMC, NUTS\nfrom tests.common import assert_equal\n\n\npytestmark = pytest.mark.filterwarnings(""ignore::PendingDeprecationWarning"")\n\n\ndef model(num_trials):\n    with pyro.plate(""data"", num_trials.size(0)):\n        phi_prior = dist.Uniform(num_trials.new_tensor(0.), num_trials.new_tensor(1.))\n        success_prob = pyro.sample(""phi"", phi_prior)\n        return pyro.sample(""obs"", dist.Binomial(num_trials, success_prob))\n\n\ndef test_nesting():\n    def nested():\n        true_probs = torch.ones(5) * 0.7\n        num_trials = torch.ones(5) * 1000\n        num_success = dist.Binomial(num_trials, true_probs).sample()\n        conditioned_model = poutine.condition(model, data={""obs"": num_success})\n        nuts_kernel = NUTS(conditioned_model, adapt_step_size=True)\n        mcmc_run = MCMC(nuts_kernel, num_samples=10, warmup_steps=2).run(num_trials)\n        return mcmc_run\n\n    with poutine.trace() as tp:\n        nested()\n        nested()\n\n    assert len(tp.trace.nodes) == 0\n\n\n# TODO: Make this available directly in `SVI` if needed.\n@pytest.mark.filterwarnings(\'ignore::FutureWarning\')\ndef test_information_criterion():\n    # milk dataset: https://github.com/rmcelreath/rethinking/blob/master/data/milk.csv\n    kcal = torch.tensor([0.49, 0.47, 0.56, 0.89, 0.92, 0.8, 0.46, 0.71, 0.68,\n                         0.97, 0.84, 0.62, 0.54, 0.49, 0.48, 0.55, 0.71])\n    kcal_mean = kcal.mean()\n    kcal_logstd = kcal.std().log()\n\n    def model():\n        mu = pyro.sample(""mu"", dist.Normal(kcal_mean, 1))\n        log_sigma = pyro.sample(""log_sigma"", dist.Normal(kcal_logstd, 1))\n        with pyro.plate(""plate""):\n            pyro.sample(""kcal"", dist.Normal(mu, log_sigma.exp()), obs=kcal)\n\n    delta_guide = AutoLaplaceApproximation(model)\n\n    svi = SVI(model, delta_guide, optim.Adam({""lr"": 0.05}), loss=Trace_ELBO(),\n              num_steps=0, num_samples=3000)\n    for i in range(100):\n        svi.step()\n\n    svi.guide = delta_guide.laplace_approximation()\n    posterior = svi.run()\n\n    ic = posterior.information_criterion()\n    assert_equal(ic[""waic""], torch.tensor(-8.3), prec=0.2)\n    assert_equal(ic[""p_waic""], torch.tensor(1.8), prec=0.2)\n'"
tests/infer/test_autoguide.py,53,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport io\nimport warnings\nfrom operator import attrgetter\n\nimport numpy as np\nimport pytest\nimport torch\nfrom torch import nn\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\n\nfrom pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, TraceGraph_ELBO, Predictive\nfrom pyro.infer.autoguide import (AutoCallable, AutoDelta, AutoDiagonalNormal, AutoDiscreteParallel, AutoGuide,\n                                  AutoGuideList, AutoIAFNormal, AutoLaplaceApproximation, AutoLowRankMultivariateNormal,\n                                  AutoNormal, AutoMultivariateNormal, init_to_feasible, init_to_mean, init_to_median,\n                                  init_to_sample)\nfrom pyro.nn.module import PyroModule, PyroParam, PyroSample\nfrom pyro.optim import Adam\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import check_model_guide_match\nfrom tests.common import assert_close, assert_equal\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoIAFNormal,\n])\ndef test_scores(auto_class):\n    def model():\n        if auto_class is AutoIAFNormal:\n            pyro.sample(""z"", dist.Normal(0.0, 1.0).expand([10]))\n        else:\n            pyro.sample(""z"", dist.Normal(0.0, 1.0))\n\n    guide = auto_class(model)\n    guide_trace = poutine.trace(guide).get_trace()\n    model_trace = poutine.trace(poutine.replay(model, guide_trace)).get_trace()\n\n    guide_trace.compute_log_prob()\n    model_trace.compute_log_prob()\n\n    prefix = auto_class.__name__\n    if prefix != \'AutoNormal\':\n        assert \'_{}_latent\'.format(prefix) not in model_trace.nodes\n        assert guide_trace.nodes[\'_{}_latent\'.format(prefix)][\'log_prob_sum\'].item() != 0.0\n    assert model_trace.nodes[\'z\'][\'log_prob_sum\'].item() != 0.0\n    assert guide_trace.nodes[\'z\'][\'log_prob_sum\'].item() == 0.0\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoIAFNormal,\n    AutoLaplaceApproximation,\n])\ndef test_factor(auto_class, Elbo):\n\n    def model(log_factor):\n        pyro.sample(""z1"", dist.Normal(0.0, 1.0))\n        pyro.factor(""f1"", log_factor)\n        pyro.sample(""z2"", dist.Normal(torch.zeros(2), torch.ones(2)).to_event(1))\n        with pyro.plate(""plate"", 3):\n            pyro.factor(""f2"", log_factor)\n            pyro.sample(""z3"", dist.Normal(torch.zeros(3), torch.ones(3)))\n\n    guide = auto_class(model)\n    elbo = Elbo(strict_enumeration_warning=False)\n    elbo.loss(model, guide, torch.tensor(0.))  # initialize param store\n\n    pyro.set_rng_seed(123)\n    loss_5 = elbo.loss(model, guide, torch.tensor(5.))\n    pyro.set_rng_seed(123)\n    loss_4 = elbo.loss(model, guide, torch.tensor(4.))\n    assert_close(loss_5 - loss_4, -1 - 3)\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\n@pytest.mark.parametrize(""init_loc_fn"", [\n    init_to_feasible,\n    init_to_mean,\n    init_to_median,\n    init_to_sample,\n])\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoIAFNormal,\n    AutoLaplaceApproximation,\n])\n@pytest.mark.filterwarnings(""ignore::FutureWarning"")\ndef test_shapes(auto_class, init_loc_fn, Elbo):\n\n    def model():\n        pyro.sample(""z1"", dist.Normal(0.0, 1.0))\n        pyro.sample(""z2"", dist.Normal(torch.zeros(2), torch.ones(2)).to_event(1))\n        with pyro.plate(""plate"", 3):\n            pyro.sample(""z3"", dist.Normal(torch.zeros(3), torch.ones(3)))\n        pyro.sample(""z4"", dist.MultivariateNormal(torch.zeros(2), torch.eye(2)))\n        pyro.sample(""z5"", dist.Dirichlet(torch.ones(3)))\n        pyro.sample(""z6"", dist.Normal(0, 1).expand((2,)).mask(torch.arange(2) > 0).to_event(1))\n\n    guide = auto_class(model, init_loc_fn=init_loc_fn)\n    elbo = Elbo(strict_enumeration_warning=False)\n    loss = elbo.loss(model, guide)\n    assert np.isfinite(loss), loss\n\n\n@pytest.mark.xfail(reason=""sequential plate is not yet supported"")\n@pytest.mark.parametrize(\'auto_class\', [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoIAFNormal,\n    AutoLaplaceApproximation,\n])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO])\ndef test_iplate_smoke(auto_class, Elbo):\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0, 1))\n        assert x.shape == ()\n\n        for i in pyro.plate(""plate"", 3):\n            y = pyro.sample(""y_{}"".format(i), dist.Normal(0, 1).expand_by([2, 1 + i, 2]).to_event(3))\n            assert y.shape == (2, 1 + i, 2)\n\n        z = pyro.sample(""z"", dist.Normal(0, 1).expand_by([2]).to_event(1))\n        assert z.shape == (2,)\n\n        pyro.sample(""obs"", dist.Bernoulli(0.1), obs=torch.tensor(0))\n\n    guide = auto_class(model)\n    infer = SVI(model, guide, Adam({""lr"": 1e-6}), Elbo(strict_enumeration_warning=False))\n    infer.step()\n\n\ndef auto_guide_list_x(model):\n    guide = AutoGuideList(model)\n    guide.append(AutoDelta(poutine.block(model, expose=[""x""])))\n    guide.append(AutoDiagonalNormal(poutine.block(model, hide=[""x""])))\n    return guide\n\n\ndef auto_guide_callable(model):\n    def guide_x():\n        x_loc = pyro.param(""x_loc"", torch.tensor(1.))\n        x_scale = pyro.param(""x_scale"", torch.tensor(.1), constraint=constraints.positive)\n        pyro.sample(""x"", dist.Normal(x_loc, x_scale))\n\n    def median_x():\n        return {""x"": pyro.param(""x_loc"", torch.tensor(1.))}\n\n    guide = AutoGuideList(model)\n    guide.append(AutoCallable(model, guide_x, median_x))\n    guide.append(AutoDiagonalNormal(poutine.block(model, hide=[""x""])))\n    return guide\n\n\ndef auto_guide_module_callable(model):\n    class GuideX(AutoGuide):\n        def __init__(self, model):\n            super().__init__(model)\n            self.x_loc = nn.Parameter(torch.tensor(1.))\n            self.x_scale = PyroParam(torch.tensor(.1), constraint=constraints.positive)\n\n        def forward(self, *args, **kwargs):\n            return {""x"": pyro.sample(""x"", dist.Normal(self.x_loc, self.x_scale))}\n\n        def median(self, *args, **kwargs):\n            return {""x"": self.x_loc.detach()}\n\n    guide = AutoGuideList(model)\n    guide.custom = GuideX(model)\n    guide.diagnorm = AutoDiagonalNormal(poutine.block(model, hide=[""x""]))\n    return guide\n\n\ndef nested_auto_guide_callable(model):\n    guide = AutoGuideList(model)\n    guide.append(AutoDelta(poutine.block(model, expose=[\'x\'])))\n    guide_y = AutoGuideList(poutine.block(model, expose=[\'y\']))\n    guide_y.z = AutoIAFNormal(poutine.block(model, expose=[\'y\']))\n    guide.append(guide_y)\n    return guide\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n    auto_guide_list_x,\n    auto_guide_callable,\n    auto_guide_module_callable,\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_feasible),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_mean),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_median),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_sample),\n])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_median(auto_class, Elbo):\n\n    def model():\n        pyro.sample(""x"", dist.Normal(0.0, 1.0))\n        pyro.sample(""y"", dist.LogNormal(0.0, 1.0))\n        pyro.sample(""z"", dist.Beta(2.0, 2.0))\n\n    guide = auto_class(model)\n    optim = Adam({\'lr\': 0.05, \'betas\': (0.8, 0.99)})\n    elbo = Elbo(strict_enumeration_warning=False,\n                num_particles=100, vectorize_particles=True)\n    infer = SVI(model, guide, optim, elbo)\n    for _ in range(100):\n        infer.step()\n\n    if auto_class is AutoLaplaceApproximation:\n        guide = guide.laplace_approximation()\n\n    median = guide.median()\n    assert_equal(median[""x""], torch.tensor(0.0), prec=0.1)\n    if auto_class is AutoDelta:\n        assert_equal(median[""y""], torch.tensor(-1.0).exp(), prec=0.1)\n    else:\n        assert_equal(median[""y""], torch.tensor(1.0), prec=0.1)\n    assert_equal(median[""z""], torch.tensor(0.5), prec=0.1)\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n    auto_guide_list_x,\n    auto_guide_module_callable,\n    nested_auto_guide_callable,\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_feasible),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_mean),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_median),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_sample),\n])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_autoguide_serialization(auto_class, Elbo):\n    def model():\n        pyro.sample(""x"", dist.Normal(0.0, 1.0))\n        with pyro.plate(""plate"", 2):\n            pyro.sample(""y"", dist.LogNormal(0.0, 1.0))\n            pyro.sample(""z"", dist.Beta(2.0, 2.0))\n    guide = auto_class(model)\n    guide()\n    if auto_class is AutoLaplaceApproximation:\n        guide = guide.laplace_approximation()\n    pyro.set_rng_seed(0)\n    expected = guide.call()\n    names = sorted(guide())\n\n    # Ignore tracer warnings\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n        # XXX: check_trace=True fails for AutoLaplaceApproximation\n        traced_guide = torch.jit.trace_module(guide, {""call"": ()}, check_trace=False)\n    f = io.BytesIO()\n    torch.jit.save(traced_guide, f)\n    f.seek(0)\n    guide_deser = torch.jit.load(f)\n\n    # Check .call() result.\n    pyro.set_rng_seed(0)\n    actual = guide_deser.call()\n    assert len(actual) == len(expected)\n    for name, a, e in zip(names, actual, expected):\n        assert_equal(a, e, msg=""{}: {} vs {}"".format(name, a, e))\n\n    # Check named_parameters.\n    expected_names = {name for name, _ in guide.named_parameters()}\n    actual_names = {name for name, _ in guide_deser.named_parameters()}\n    assert actual_names == expected_names\n    for name in actual_names:\n        # Get nested attributes.\n        attr_get = attrgetter(name)\n        assert_equal(attr_get(guide_deser), attr_get(guide).data)\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_quantiles(auto_class, Elbo):\n\n    def model():\n        pyro.sample(""x"", dist.Normal(0.0, 1.0))\n        pyro.sample(""y"", dist.LogNormal(0.0, 1.0))\n        pyro.sample(""z"", dist.Beta(2.0, 2.0))\n\n    guide = auto_class(model)\n    optim = Adam({\'lr\': 0.05, \'betas\': (0.8, 0.99)})\n    elbo = Elbo(strict_enumeration_warning=False,\n                num_particles=100, vectorize_particles=True)\n    infer = SVI(model, guide, optim, elbo)\n    for _ in range(100):\n        infer.step()\n\n    if auto_class is AutoLaplaceApproximation:\n        guide = guide.laplace_approximation()\n\n    quantiles = guide.quantiles([0.1, 0.5, 0.9])\n    median = guide.median()\n    for name in [""x"", ""y"", ""z""]:\n        assert_equal(median[name], quantiles[name][1])\n    quantiles = {name: [v.item() for v in value] for name, value in quantiles.items()}\n\n    assert -3.0 < quantiles[""x""][0]\n    assert quantiles[""x""][0] + 1.0 < quantiles[""x""][1]\n    assert quantiles[""x""][1] + 1.0 < quantiles[""x""][2]\n    assert quantiles[""x""][2] < 3.0\n\n    assert 0.01 < quantiles[""y""][0]\n    assert quantiles[""y""][0] * 2.0 < quantiles[""y""][1]\n    assert quantiles[""y""][1] * 2.0 < quantiles[""y""][2]\n    assert quantiles[""y""][2] < 100.0\n\n    assert 0.01 < quantiles[""z""][0]\n    assert quantiles[""z""][0] + 0.1 < quantiles[""z""][1]\n    assert quantiles[""z""][1] + 0.1 < quantiles[""z""][2]\n    assert quantiles[""z""][2] < 0.99\n\n\n@pytest.mark.parametrize(""continuous_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoIAFNormal,\n    AutoLaplaceApproximation,\n])\ndef test_discrete_parallel(continuous_class):\n    K = 2\n    data = torch.tensor([0., 1., 10., 11., 12.])\n\n    def model(data):\n        weights = pyro.sample(\'weights\', dist.Dirichlet(0.5 * torch.ones(K)))\n        locs = pyro.sample(\'locs\', dist.Normal(0, 10).expand_by([K]).to_event(1))\n        scale = pyro.sample(\'scale\', dist.LogNormal(0, 1))\n\n        with pyro.plate(\'data\', len(data)):\n            weights = weights.expand(torch.Size((len(data),)) + weights.shape)\n            assignment = pyro.sample(\'assignment\', dist.Categorical(weights))\n            pyro.sample(\'obs\', dist.Normal(locs[assignment], scale), obs=data)\n\n    guide = AutoGuideList(model)\n    guide.append(continuous_class(poutine.block(model, hide=[""assignment""])))\n    guide.append(AutoDiscreteParallel(poutine.block(model, expose=[""assignment""])))\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    loss = elbo.loss_and_grads(model, guide, data)\n    assert np.isfinite(loss), loss\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoIAFNormal,\n    AutoLaplaceApproximation,\n])\ndef test_guide_list(auto_class):\n\n    def model():\n        pyro.sample(""x"", dist.Normal(0., 1.).expand([2]))\n        pyro.sample(""y"", dist.MultivariateNormal(torch.zeros(5), torch.eye(5, 5)))\n\n    guide = AutoGuideList(model)\n    guide.append(auto_class(poutine.block(model, expose=[""x""])))\n    guide.append(auto_class(poutine.block(model, expose=[""y""])))\n    guide()\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoNormal,\n    AutoMultivariateNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n])\ndef test_callable(auto_class):\n\n    def model():\n        pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.MultivariateNormal(torch.zeros(5), torch.eye(5, 5)))\n\n    def guide_x():\n        x_loc = pyro.param(""x_loc"", torch.tensor(0.))\n        pyro.sample(""x"", dist.Delta(x_loc))\n\n    guide = AutoGuideList(model)\n    guide.append(guide_x)\n    guide.append(auto_class(poutine.block(model, expose=[""y""])))\n    values = guide()\n    assert set(values) == set([""y""])\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n])\ndef test_callable_return_dict(auto_class):\n\n    def model():\n        pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.MultivariateNormal(torch.zeros(5), torch.eye(5, 5)))\n\n    def guide_x():\n        x_loc = pyro.param(""x_loc"", torch.tensor(0.))\n        x = pyro.sample(""x"", dist.Delta(x_loc))\n        return {""x"": x}\n\n    guide = AutoGuideList(model)\n    guide.append(guide_x)\n    guide.append(auto_class(poutine.block(model, expose=[""y""])))\n    values = guide()\n    assert set(values) == set([""x"", ""y""])\n\n\ndef test_empty_model_error():\n    def model():\n        pass\n    guide = AutoDiagonalNormal(model)\n    with pytest.raises(RuntimeError):\n        guide()\n\n\ndef test_unpack_latent():\n    def model():\n        return pyro.sample(\'x\', dist.LKJCorrCholesky(2, torch.tensor(1.)))\n\n    guide = AutoDiagonalNormal(model)\n    assert guide()[\'x\'].shape == model().shape\n    latent = guide.sample_latent()\n    assert list(guide._unpack_latent(latent))[0][1].shape == (1,)\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoLowRankMultivariateNormal,\n])\ndef test_init_loc_fn(auto_class):\n\n    def model():\n        pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.MultivariateNormal(torch.zeros(5), torch.eye(5, 5)))\n\n    inits = {""x"": torch.randn(()), ""y"": torch.randn(5)}\n\n    def init_loc_fn(site):\n        return inits[site[""name""]]\n\n    guide = auto_class(model, init_loc_fn=init_loc_fn)\n    guide()\n    median = guide.median()\n    assert_equal(median[""x""], inits[""x""])\n    assert_equal(median[""y""], inits[""y""])\n\n\n# testing helper\nclass AutoLowRankMultivariateNormal_100(AutoLowRankMultivariateNormal):\n    def __init__(self, *args, **kwargs):\n        return super().__init__(*args, **kwargs, rank=100)\n\n\n@pytest.mark.parametrize(""init_scale"", [1e-1, 1e-4, 1e-8])\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLowRankMultivariateNormal_100,\n])\ndef test_init_scale(auto_class, init_scale):\n\n    def model():\n        pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.MultivariateNormal(torch.zeros(5), torch.eye(5, 5)))\n        with pyro.plate(""plate"", 100):\n            pyro.sample(""z"", dist.Normal(0., 1.))\n\n    guide = auto_class(model, init_scale=init_scale)\n    guide()\n    loc, scale = guide._loc_scale()\n    scale_rms = scale.pow(2).mean().sqrt().item()\n    assert init_scale * 0.5 < scale_rms < 2.0 * init_scale\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n    auto_guide_list_x,\n    auto_guide_callable,\n    auto_guide_module_callable,\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_mean),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_median),\n])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_median_module(auto_class, Elbo):\n\n    class Model(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.x_loc = nn.Parameter(torch.tensor(1.))\n            self.x_scale = PyroParam(torch.tensor(0.1), constraints.positive)\n\n        def forward(self):\n            pyro.sample(""x"", dist.Normal(self.x_loc, self.x_scale))\n            pyro.sample(""y"", dist.Normal(2., 0.1))\n\n    model = Model()\n    guide = auto_class(model)\n    infer = SVI(model, guide, Adam({\'lr\': 0.005}), Elbo(strict_enumeration_warning=False))\n    for _ in range(20):\n        infer.step()\n\n    if auto_class is AutoLaplaceApproximation:\n        guide = guide.laplace_approximation()\n\n    median = guide.median()\n    assert_equal(median[""x""].detach(), torch.tensor(1.0), prec=0.1)\n    assert_equal(median[""y""].detach(), torch.tensor(2.0), prec=0.1)\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_nested_autoguide(Elbo):\n\n    class Model(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.x_loc = nn.Parameter(torch.tensor(1.))\n            self.x_scale = PyroParam(torch.tensor(0.1), constraints.positive)\n\n        def forward(self):\n            pyro.sample(""x"", dist.Normal(self.x_loc, self.x_scale))\n            with pyro.plate(""plate"", 2):\n                pyro.sample(""y"", dist.Normal(2., 0.1))\n\n    model = Model()\n    guide = nested_auto_guide_callable(model)\n\n    # Check master ref for all nested components.\n    for _, m in guide.named_modules():\n        if m is guide:\n            continue\n        assert m.master is not None and m.master() is guide, ""master ref wrong for {}"".format(m._pyro_name)\n\n    infer = SVI(model, guide, Adam({\'lr\': 0.005}), Elbo(strict_enumeration_warning=False))\n    for _ in range(20):\n        infer.step()\n\n    guide_trace = poutine.trace(guide).get_trace()\n    model_trace = poutine.trace(model).get_trace()\n    check_model_guide_match(model_trace, guide_trace)\n    assert all(p.startswith(""AutoGuideList.0"") or p.startswith(""AutoGuideList.1.z"")\n               for p in guide_trace.param_nodes)\n    stochastic_nodes = set(guide_trace.stochastic_nodes)\n    assert ""x"" in stochastic_nodes\n    assert ""y"" in stochastic_nodes\n    # Only latent sampled is for the IAF.\n    assert ""_AutoGuideList.1.z_latent"" in stochastic_nodes\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_mean),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_median),\n])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_linear_regression_smoke(auto_class, Elbo):\n    N, D = 10, 3\n\n    class RandomLinear(nn.Linear, PyroModule):\n        def __init__(self, in_features, out_features):\n            super().__init__(in_features, out_features)\n            self.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n            self.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1))\n\n    class LinearRegression(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.linear = RandomLinear(D, 1)\n\n        def forward(self, x, y=None):\n            mean = self.linear(x).squeeze(-1)\n            sigma = pyro.sample(""sigma"", dist.LogNormal(0., 1.))\n            with pyro.plate(\'plate\', N):\n                return pyro.sample(\'obs\', dist.Normal(mean, sigma), obs=y)\n\n    x, y = torch.randn(N, D), torch.randn(N)\n    model = LinearRegression()\n    guide = auto_class(model)\n    infer = SVI(model, guide, Adam({\'lr\': 0.005}), Elbo(strict_enumeration_warning=False))\n    infer.step(x, y)\n\n\n@pytest.mark.parametrize(""auto_class"", [\n    AutoDelta,\n    AutoDiagonalNormal,\n    AutoMultivariateNormal,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_mean),\n    functools.partial(AutoDiagonalNormal, init_loc_fn=init_to_median),\n])\ndef test_predictive(auto_class):\n    N, D = 3, 2\n\n    class RandomLinear(nn.Linear, PyroModule):\n        def __init__(self, in_features, out_features):\n            super().__init__(in_features, out_features)\n            self.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n            self.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1))\n\n    class LinearRegression(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.linear = RandomLinear(D, 1)\n\n        def forward(self, x, y=None):\n            mean = self.linear(x).squeeze(-1)\n            sigma = pyro.sample(""sigma"", dist.LogNormal(0., 1.))\n            with pyro.plate(\'plate\', N):\n                return pyro.sample(\'obs\', dist.Normal(mean, sigma), obs=y)\n\n    x, y = torch.randn(N, D), torch.randn(N)\n    model = LinearRegression()\n    guide = auto_class(model)\n    # XXX: Record `y` as observed in the prototype trace\n    # Is there a better pattern to follow?\n    guide(x, y=y)\n    # Test predictive module\n    model_trace = poutine.trace(model).get_trace(x, y=None)\n    predictive = Predictive(model, guide=guide, num_samples=10)\n    pyro.set_rng_seed(0)\n    samples = predictive(x)\n    for site in prune_subsample_sites(model_trace).stochastic_nodes:\n        assert site in samples\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n        traced_predictive = torch.jit.trace_module(predictive, {""call"": (x,)})\n    f = io.BytesIO()\n    torch.jit.save(traced_predictive, f)\n    f.seek(0)\n    predictive_deser = torch.jit.load(f)\n    pyro.set_rng_seed(0)\n    samples_deser = predictive_deser.call(x)\n    # Note that the site values are different in the serialized guide\n    assert len(samples) == len(samples_deser)\n\n\n@pytest.mark.parametrize(""init_fn"", [None, init_to_mean, init_to_median])\n@pytest.mark.parametrize(""auto_class"", [AutoDelta, AutoNormal, AutoGuideList])\ndef test_subsample_guide(auto_class, init_fn):\n\n    # The model from tutorial/source/easyguide.ipynb\n    def model(batch, subsample, full_size):\n        num_time_steps = len(batch)\n        result = [None] * num_time_steps\n        drift = pyro.sample(""drift"", dist.LogNormal(-1, 0.5))\n        plate = pyro.plate(""data"", full_size, subsample=subsample)\n        assert plate.size == 50\n        with plate:\n            z = 0.\n            for t in range(num_time_steps):\n                z = pyro.sample(""state_{}"".format(t), dist.Normal(z, drift))\n                result[t] = pyro.sample(""obs_{}"".format(t), dist.Bernoulli(logits=z),\n                                        obs=batch[t])\n\n        return torch.stack(result)\n\n    def create_plates(batch, subsample, full_size):\n        return pyro.plate(""data"", full_size, subsample=subsample)\n\n    if auto_class == AutoGuideList:\n        guide = AutoGuideList(model, create_plates=create_plates)\n        guide.add(AutoDelta(poutine.block(model, expose=[""drift""])))\n        guide.add(AutoNormal(poutine.block(model, hide=[""drift""])))\n    else:\n        guide = auto_class(model, create_plates=create_plates)\n\n    full_size = 50\n    batch_size = 20\n    num_time_steps = 8\n    pyro.set_rng_seed(123456789)\n    data = model([None] * num_time_steps, torch.arange(full_size), full_size)\n    assert data.shape == (num_time_steps, full_size)\n\n    pyro.get_param_store().clear()\n    pyro.set_rng_seed(123456789)\n    svi = SVI(model, guide, Adam({""lr"": 0.02}), Trace_ELBO())\n    for epoch in range(2):\n        beg = 0\n        while beg < full_size:\n            end = min(full_size, beg + batch_size)\n            subsample = torch.arange(beg, end)\n            batch = data[:, beg:end]\n            beg = end\n            svi.step(batch, subsample, full_size=full_size)\n\n\n@pytest.mark.parametrize(""independent"", [True, False], ids=[""independent"", ""dependent""])\n@pytest.mark.parametrize(""auto_class"", [AutoDelta, AutoNormal])\ndef test_subsample_guide_2(auto_class, independent):\n\n    # Simplified from Model2 in tutorial/source/forecasting_iii.ipynb\n    def model(data):\n        size, size = data.shape\n        origin_plate = pyro.plate(""origin"", size, dim=-2)\n        destin_plate = pyro.plate(""destin"", size, dim=-1)\n        with origin_plate, destin_plate:\n            batch = pyro.subsample(data, event_dim=0)\n            assert batch.size(0) == batch.size(1), batch.shape\n            pyro.sample(""obs"", dist.Normal(0, 1), obs=batch)\n\n    def create_plates(data):\n        size, size = data.shape\n        origin_plate = pyro.plate(""origin"", size, subsample_size=5, dim=-2)\n        if independent:\n            destin_plate = pyro.plate(""destin"", size, subsample_size=5, dim=-1)\n        else:\n            with origin_plate as subsample:\n                pass\n            destin_plate = pyro.plate(""destin"", size, subsample=subsample, dim=-1)\n        return origin_plate, destin_plate\n\n    guide = auto_class(model, create_plates=create_plates)\n    svi = SVI(model, guide, Adam({""lr"": 0.01}), Trace_ELBO())\n\n    data = torch.randn(10, 10)\n    for step in range(2):\n        svi.step(data)\n'"
tests/infer/test_compute_downstream_costs.py,18,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer.tracegraph_elbo import _compute_downstream_costs\nfrom pyro.infer.util import MultiFrameTensor, get_plate_stacks\nfrom pyro.poutine.util import prune_subsample_sites\nfrom tests.common import assert_equal\n\n\ndef _brute_force_compute_downstream_costs(model_trace, guide_trace,  #\n                                          non_reparam_nodes):\n\n    guide_nodes = [x for x in guide_trace.nodes if guide_trace.nodes[x][""type""] == ""sample""]\n    downstream_costs, downstream_guide_cost_nodes = {}, {}\n    stacks = get_plate_stacks(model_trace)\n\n    for node in guide_nodes:\n        downstream_costs[node] = MultiFrameTensor((stacks[node],\n                                                   model_trace.nodes[node][\'log_prob\'] -\n                                                   guide_trace.nodes[node][\'log_prob\']))\n        downstream_guide_cost_nodes[node] = set([node])\n\n        descendants = guide_trace.successors(node)\n\n        for desc in descendants:\n            desc_mft = MultiFrameTensor((stacks[desc],\n                                         model_trace.nodes[desc][\'log_prob\'] -\n                                         guide_trace.nodes[desc][\'log_prob\']))\n            downstream_costs[node].add(*desc_mft.items())\n            downstream_guide_cost_nodes[node].update([desc])\n\n    for site in non_reparam_nodes:\n        children_in_model = set()\n        for node in downstream_guide_cost_nodes[site]:\n            children_in_model.update(model_trace.successors(node))\n        children_in_model.difference_update(downstream_guide_cost_nodes[site])\n        for child in children_in_model:\n            assert (model_trace.nodes[child][""type""] == ""sample"")\n            child_mft = MultiFrameTensor((stacks[child],\n                                          model_trace.nodes[child][\'log_prob\']))\n            downstream_costs[site].add(*child_mft.items())\n            downstream_guide_cost_nodes[site].update([child])\n\n    for k in non_reparam_nodes:\n        downstream_costs[k] = downstream_costs[k].sum_to(guide_trace.nodes[k][""cond_indep_stack""])\n\n    return downstream_costs, downstream_guide_cost_nodes\n\n\ndef big_model_guide(include_obs=True, include_single=False, include_inner_1=False, flip_c23=False,\n                    include_triple=False, include_z1=False):\n    p0 = torch.tensor(math.exp(-0.20), requires_grad=True)\n    p1 = torch.tensor(math.exp(-0.33), requires_grad=True)\n    p2 = torch.tensor(math.exp(-0.70), requires_grad=True)\n    if include_triple:\n        with pyro.plate(""plate_triple1"", 6) as ind_triple1:\n            with pyro.plate(""plate_triple2"", 7) as ind_triple2:\n                if include_z1:\n                    pyro.sample(""z1"", dist.Bernoulli(p2).expand_by([len(ind_triple2), len(ind_triple1)]))\n                with pyro.plate(""plate_triple3"", 9) as ind_triple3:\n                    pyro.sample(""z0"",\n                                dist.Bernoulli(p2).expand_by(\n                                    [len(ind_triple3), len(ind_triple2), len(ind_triple1)]))\n    pyro.sample(""a1"", dist.Bernoulli(p0))\n    if include_single:\n        with pyro.plate(""plate_single"", 5) as ind_single:\n            b0 = pyro.sample(""b0"", dist.Bernoulli(p0).expand_by([len(ind_single)]))\n            assert b0.shape == (5,)\n    with pyro.plate(""plate_outer"", 2) as ind_outer:\n        pyro.sample(""b1"", dist.Bernoulli(p0).expand_by([len(ind_outer)]))\n        if include_inner_1:\n            with pyro.plate(""plate_inner_1"", 3) as ind_inner:\n                pyro.sample(""c1"", dist.Bernoulli(p1).expand_by([len(ind_inner), len(ind_outer)]))\n                if flip_c23 and not include_obs:\n                    pyro.sample(""c3"", dist.Bernoulli(p0).expand_by([len(ind_inner), len(ind_outer)]))\n                    pyro.sample(""c2"", dist.Bernoulli(p1).expand_by([len(ind_inner), len(ind_outer)]))\n                else:\n                    pyro.sample(""c2"", dist.Bernoulli(p0).expand_by([len(ind_inner), len(ind_outer)]))\n                    pyro.sample(""c3"", dist.Bernoulli(p2).expand_by([len(ind_inner), len(ind_outer)]))\n        with pyro.plate(""plate_inner_2"", 4) as ind_inner:\n            pyro.sample(""d1"", dist.Bernoulli(p0).expand_by([len(ind_inner), len(ind_outer)]))\n            d2 = pyro.sample(""d2"", dist.Bernoulli(p2).expand_by([len(ind_inner), len(ind_outer)]))\n            assert d2.shape == (4, 2)\n            if include_obs:\n                pyro.sample(""obs"", dist.Bernoulli(p0).expand_by([len(ind_inner), len(ind_outer)]),\n                            obs=torch.ones(d2.size()))\n\n\n@pytest.mark.parametrize(""include_inner_1"", [True, False])\n@pytest.mark.parametrize(""include_single"", [True, False])\n@pytest.mark.parametrize(""flip_c23"", [True, False])\n@pytest.mark.parametrize(""include_triple"", [True, False])\n@pytest.mark.parametrize(""include_z1"", [True, False])\ndef test_compute_downstream_costs_big_model_guide_pair(include_inner_1, include_single, flip_c23,\n                                                       include_triple, include_z1):\n    guide_trace = poutine.trace(big_model_guide,\n                                graph_type=""dense"").get_trace(include_obs=False, include_inner_1=include_inner_1,\n                                                              include_single=include_single, flip_c23=flip_c23,\n                                                              include_triple=include_triple, include_z1=include_z1)\n    model_trace = poutine.trace(poutine.replay(big_model_guide, trace=guide_trace),\n                                graph_type=""dense"").get_trace(include_obs=True, include_inner_1=include_inner_1,\n                                                              include_single=include_single, flip_c23=flip_c23,\n                                                              include_triple=include_triple, include_z1=include_z1)\n\n    guide_trace = prune_subsample_sites(guide_trace)\n    model_trace = prune_subsample_sites(model_trace)\n    model_trace.compute_log_prob()\n    guide_trace.compute_log_prob()\n    non_reparam_nodes = set(guide_trace.nonreparam_stochastic_nodes)\n\n    dc, dc_nodes = _compute_downstream_costs(model_trace, guide_trace,\n                                             non_reparam_nodes)\n\n    dc_brute, dc_nodes_brute = _brute_force_compute_downstream_costs(model_trace, guide_trace,\n                                                                     non_reparam_nodes)\n\n    assert dc_nodes == dc_nodes_brute\n\n    expected_nodes_full_model = {\'a1\': {\'c2\', \'a1\', \'d1\', \'c1\', \'obs\', \'b1\', \'d2\', \'c3\', \'b0\'}, \'d2\': {\'obs\', \'d2\'},\n                                 \'d1\': {\'obs\', \'d1\', \'d2\'}, \'c3\': {\'d2\', \'obs\', \'d1\', \'c3\'},\n                                 \'b0\': {\'b0\', \'d1\', \'c1\', \'obs\', \'b1\', \'d2\', \'c3\', \'c2\'},\n                                 \'b1\': {\'obs\', \'b1\', \'d1\', \'d2\', \'c3\', \'c1\', \'c2\'},\n                                 \'c1\': {\'d1\', \'c1\', \'obs\', \'d2\', \'c3\', \'c2\'},\n                                 \'c2\': {\'obs\', \'d1\', \'c3\', \'d2\', \'c2\'}}\n    if not include_triple and include_inner_1 and include_single and not flip_c23:\n        assert(dc_nodes == expected_nodes_full_model)\n\n    expected_b1 = (model_trace.nodes[\'b1\'][\'log_prob\'] - guide_trace.nodes[\'b1\'][\'log_prob\'])\n    expected_b1 += (model_trace.nodes[\'d2\'][\'log_prob\'] - guide_trace.nodes[\'d2\'][\'log_prob\']).sum(0)\n    expected_b1 += (model_trace.nodes[\'d1\'][\'log_prob\'] - guide_trace.nodes[\'d1\'][\'log_prob\']).sum(0)\n    expected_b1 += model_trace.nodes[\'obs\'][\'log_prob\'].sum(0, keepdim=False)\n    if include_inner_1:\n        expected_b1 += (model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\']).sum(0)\n        expected_b1 += (model_trace.nodes[\'c2\'][\'log_prob\'] - guide_trace.nodes[\'c2\'][\'log_prob\']).sum(0)\n        expected_b1 += (model_trace.nodes[\'c3\'][\'log_prob\'] - guide_trace.nodes[\'c3\'][\'log_prob\']).sum(0)\n    assert_equal(expected_b1, dc[\'b1\'], prec=1.0e-6)\n\n    if include_single:\n        expected_b0 = (model_trace.nodes[\'b0\'][\'log_prob\'] - guide_trace.nodes[\'b0\'][\'log_prob\'])\n        expected_b0 += (model_trace.nodes[\'b1\'][\'log_prob\'] - guide_trace.nodes[\'b1\'][\'log_prob\']).sum()\n        expected_b0 += (model_trace.nodes[\'d2\'][\'log_prob\'] - guide_trace.nodes[\'d2\'][\'log_prob\']).sum()\n        expected_b0 += (model_trace.nodes[\'d1\'][\'log_prob\'] - guide_trace.nodes[\'d1\'][\'log_prob\']).sum()\n        expected_b0 += model_trace.nodes[\'obs\'][\'log_prob\'].sum()\n        if include_inner_1:\n            expected_b0 += (model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\']).sum()\n            expected_b0 += (model_trace.nodes[\'c2\'][\'log_prob\'] - guide_trace.nodes[\'c2\'][\'log_prob\']).sum()\n            expected_b0 += (model_trace.nodes[\'c3\'][\'log_prob\'] - guide_trace.nodes[\'c3\'][\'log_prob\']).sum()\n        assert_equal(expected_b0, dc[\'b0\'], prec=1.0e-6)\n        assert dc[\'b0\'].size() == (5,)\n\n    if include_inner_1:\n        expected_c3 = (model_trace.nodes[\'c3\'][\'log_prob\'] - guide_trace.nodes[\'c3\'][\'log_prob\'])\n        expected_c3 += (model_trace.nodes[\'d1\'][\'log_prob\'] - guide_trace.nodes[\'d1\'][\'log_prob\']).sum(0)\n        expected_c3 += (model_trace.nodes[\'d2\'][\'log_prob\'] - guide_trace.nodes[\'d2\'][\'log_prob\']).sum(0)\n        expected_c3 += model_trace.nodes[\'obs\'][\'log_prob\'].sum(0)\n\n        expected_c2 = (model_trace.nodes[\'c2\'][\'log_prob\'] - guide_trace.nodes[\'c2\'][\'log_prob\'])\n        expected_c2 += (model_trace.nodes[\'d1\'][\'log_prob\'] - guide_trace.nodes[\'d1\'][\'log_prob\']).sum(0)\n        expected_c2 += (model_trace.nodes[\'d2\'][\'log_prob\'] - guide_trace.nodes[\'d2\'][\'log_prob\']).sum(0)\n        expected_c2 += model_trace.nodes[\'obs\'][\'log_prob\'].sum(0)\n\n        expected_c1 = (model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\'])\n\n        if flip_c23:\n            expected_c3 += model_trace.nodes[\'c2\'][\'log_prob\'] - guide_trace.nodes[\'c2\'][\'log_prob\']\n            expected_c2 += model_trace.nodes[\'c3\'][\'log_prob\']\n        else:\n            expected_c2 += model_trace.nodes[\'c3\'][\'log_prob\'] - guide_trace.nodes[\'c3\'][\'log_prob\']\n            expected_c2 += model_trace.nodes[\'c2\'][\'log_prob\'] - guide_trace.nodes[\'c2\'][\'log_prob\']\n        expected_c1 += expected_c3\n\n        assert_equal(expected_c1, dc[\'c1\'], prec=1.0e-6)\n        assert_equal(expected_c2, dc[\'c2\'], prec=1.0e-6)\n        assert_equal(expected_c3, dc[\'c3\'], prec=1.0e-6)\n\n    expected_d1 = model_trace.nodes[\'d1\'][\'log_prob\'] - guide_trace.nodes[\'d1\'][\'log_prob\']\n    expected_d1 += model_trace.nodes[\'d2\'][\'log_prob\'] - guide_trace.nodes[\'d2\'][\'log_prob\']\n    expected_d1 += model_trace.nodes[\'obs\'][\'log_prob\']\n\n    expected_d2 = (model_trace.nodes[\'d2\'][\'log_prob\'] - guide_trace.nodes[\'d2\'][\'log_prob\'])\n    expected_d2 += model_trace.nodes[\'obs\'][\'log_prob\']\n\n    if include_triple:\n        expected_z0 = dc[\'a1\'] + model_trace.nodes[\'z0\'][\'log_prob\'] - guide_trace.nodes[\'z0\'][\'log_prob\']\n        assert_equal(expected_z0, dc[\'z0\'], prec=1.0e-6)\n    assert_equal(expected_d2, dc[\'d2\'], prec=1.0e-6)\n    assert_equal(expected_d1, dc[\'d1\'], prec=1.0e-6)\n\n    assert dc[\'b1\'].size() == (2,)\n    assert dc[\'d2\'].size() == (4, 2)\n\n    for k in dc:\n        assert(guide_trace.nodes[k][\'log_prob\'].size() == dc[k].size())\n        assert_equal(dc[k], dc_brute[k])\n\n\ndef diamond_model(dim):\n    p0 = torch.tensor(math.exp(-0.20), requires_grad=True)\n    p1 = torch.tensor(math.exp(-0.33), requires_grad=True)\n    pyro.sample(""a1"", dist.Bernoulli(p0))\n    pyro.sample(""c1"", dist.Bernoulli(p1))\n    for i in pyro.plate(""plate"", 2):\n        b_i = pyro.sample(""b{}"".format(i), dist.Bernoulli(p0 * p1))\n        assert b_i.shape == ()\n    pyro.sample(""obs"", dist.Bernoulli(p0), obs=torch.tensor(1.0))\n\n\ndef diamond_guide(dim):\n    p0 = torch.tensor(math.exp(-0.70), requires_grad=True)\n    p1 = torch.tensor(math.exp(-0.43), requires_grad=True)\n    pyro.sample(""a1"", dist.Bernoulli(p0))\n    for i in pyro.plate(""plate"", dim):\n        pyro.sample(""b{}"".format(i), dist.Bernoulli(p1))\n    pyro.sample(""c1"", dist.Bernoulli(p0))\n\n\n@pytest.mark.parametrize(""dim"", [2, 3, 7, 11])\ndef test_compute_downstream_costs_duplicates(dim):\n    guide_trace = poutine.trace(diamond_guide,\n                                graph_type=""dense"").get_trace(dim=dim)\n    model_trace = poutine.trace(poutine.replay(diamond_model, trace=guide_trace),\n                                graph_type=""dense"").get_trace(dim=dim)\n\n    guide_trace = prune_subsample_sites(guide_trace)\n    model_trace = prune_subsample_sites(model_trace)\n    model_trace.compute_log_prob()\n    guide_trace.compute_log_prob()\n\n    non_reparam_nodes = set(guide_trace.nonreparam_stochastic_nodes)\n\n    dc, dc_nodes = _compute_downstream_costs(model_trace, guide_trace,\n                                             non_reparam_nodes)\n    dc_brute, dc_nodes_brute = _brute_force_compute_downstream_costs(model_trace, guide_trace,\n                                                                     non_reparam_nodes)\n\n    assert dc_nodes == dc_nodes_brute\n\n    expected_a1 = (model_trace.nodes[\'a1\'][\'log_prob\'] - guide_trace.nodes[\'a1\'][\'log_prob\'])\n    for d in range(dim):\n        expected_a1 += model_trace.nodes[\'b{}\'.format(d)][\'log_prob\']\n        expected_a1 -= guide_trace.nodes[\'b{}\'.format(d)][\'log_prob\']\n    expected_a1 += (model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\'])\n    expected_a1 += model_trace.nodes[\'obs\'][\'log_prob\']\n\n    expected_b1 = - guide_trace.nodes[\'b1\'][\'log_prob\']\n    for d in range(dim):\n        expected_b1 += model_trace.nodes[\'b{}\'.format(d)][\'log_prob\']\n    expected_b1 += (model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\'])\n    expected_b1 += model_trace.nodes[\'obs\'][\'log_prob\']\n\n    expected_c1 = (model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\'])\n    for d in range(dim):\n        expected_c1 += model_trace.nodes[\'b{}\'.format(d)][\'log_prob\']\n    expected_c1 += model_trace.nodes[\'obs\'][\'log_prob\']\n\n    assert_equal(expected_a1, dc[\'a1\'], prec=1.0e-6)\n    assert_equal(expected_b1, dc[\'b1\'], prec=1.0e-6)\n    assert_equal(expected_c1, dc[\'c1\'], prec=1.0e-6)\n\n    for k in dc:\n        assert(guide_trace.nodes[k][\'log_prob\'].size() == dc[k].size())\n        assert_equal(dc[k], dc_brute[k])\n\n\ndef nested_model_guide(include_obs=True, dim1=11, dim2=7):\n    p0 = torch.tensor(math.exp(-0.40 - include_obs * 0.2), requires_grad=True)\n    p1 = torch.tensor(math.exp(-0.33 - include_obs * 0.1), requires_grad=True)\n    pyro.sample(""a1"", dist.Bernoulli(p0 * p1))\n    for i in pyro.plate(""plate"", dim1):\n        pyro.sample(""b{}"".format(i), dist.Bernoulli(p0))\n        with pyro.plate(""plate_{}"".format(i), dim2 + i) as ind:\n            c_i = pyro.sample(""c{}"".format(i), dist.Bernoulli(p1).expand_by([len(ind)]))\n            assert c_i.shape == (dim2 + i,)\n            if include_obs:\n                obs_i = pyro.sample(""obs{}"".format(i), dist.Bernoulli(c_i), obs=torch.ones(c_i.size()))\n                assert obs_i.shape == (dim2 + i,)\n\n\n@pytest.mark.parametrize(""dim1"", [2, 5, 9])\ndef test_compute_downstream_costs_plate_in_iplate(dim1):\n    guide_trace = poutine.trace(nested_model_guide,\n                                graph_type=""dense"").get_trace(include_obs=False, dim1=dim1)\n    model_trace = poutine.trace(poutine.replay(nested_model_guide, trace=guide_trace),\n                                graph_type=""dense"").get_trace(include_obs=True, dim1=dim1)\n\n    guide_trace = prune_subsample_sites(guide_trace)\n    model_trace = prune_subsample_sites(model_trace)\n    model_trace.compute_log_prob()\n    guide_trace.compute_log_prob()\n\n    non_reparam_nodes = set(guide_trace.nonreparam_stochastic_nodes)\n\n    dc, dc_nodes = _compute_downstream_costs(model_trace, guide_trace,\n                                             non_reparam_nodes)\n    dc_brute, dc_nodes_brute = _brute_force_compute_downstream_costs(model_trace, guide_trace,\n                                                                     non_reparam_nodes)\n\n    assert dc_nodes == dc_nodes_brute\n\n    expected_c1 = (model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\'])\n    expected_c1 += model_trace.nodes[\'obs1\'][\'log_prob\']\n\n    expected_b1 = (model_trace.nodes[\'b1\'][\'log_prob\'] - guide_trace.nodes[\'b1\'][\'log_prob\'])\n    expected_b1 += (model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\']).sum()\n    expected_b1 += model_trace.nodes[\'obs1\'][\'log_prob\'].sum()\n\n    expected_c0 = (model_trace.nodes[\'c0\'][\'log_prob\'] - guide_trace.nodes[\'c0\'][\'log_prob\'])\n    expected_c0 += model_trace.nodes[\'obs0\'][\'log_prob\']\n\n    expected_b0 = (model_trace.nodes[\'b0\'][\'log_prob\'] - guide_trace.nodes[\'b0\'][\'log_prob\'])\n    expected_b0 += (model_trace.nodes[\'c0\'][\'log_prob\'] - guide_trace.nodes[\'c0\'][\'log_prob\']).sum()\n    expected_b0 += model_trace.nodes[\'obs0\'][\'log_prob\'].sum()\n\n    assert_equal(expected_c1, dc[\'c1\'], prec=1.0e-6)\n    assert_equal(expected_b1, dc[\'b1\'], prec=1.0e-6)\n    assert_equal(expected_c0, dc[\'c0\'], prec=1.0e-6)\n    assert_equal(expected_b0, dc[\'b0\'], prec=1.0e-6)\n\n    for k in dc:\n        assert(guide_trace.nodes[k][\'log_prob\'].size() == dc[k].size())\n        assert_equal(dc[k], dc_brute[k])\n\n\ndef nested_model_guide2(include_obs=True, dim1=3, dim2=2):\n    p0 = torch.tensor(math.exp(-0.40 - include_obs * 0.2), requires_grad=True)\n    p1 = torch.tensor(math.exp(-0.33 - include_obs * 0.1), requires_grad=True)\n    pyro.sample(""a1"", dist.Bernoulli(p0 * p1))\n    with pyro.plate(""plate1"", dim1) as ind:\n        c = pyro.sample(""c"", dist.Bernoulli(p1).expand_by([len(ind)]))\n        assert c.shape == (dim1,)\n        for i in pyro.plate(""plate2"", dim2):\n            b_i = pyro.sample(""b{}"".format(i), dist.Bernoulli(p0).expand_by([len(ind)]))\n            assert b_i.shape == (dim1,)\n            if include_obs:\n                obs_i = pyro.sample(""obs{}"".format(i), dist.Bernoulli(b_i), obs=torch.ones(b_i.size()))\n                assert obs_i.shape == (dim1,)\n\n\n@pytest.mark.parametrize(""dim1"", [2, 5])\n@pytest.mark.parametrize(""dim2"", [3, 4])\ndef test_compute_downstream_costs_iplate_in_plate(dim1, dim2):\n    guide_trace = poutine.trace(nested_model_guide2,\n                                graph_type=""dense"").get_trace(include_obs=False, dim1=dim1, dim2=dim2)\n    model_trace = poutine.trace(poutine.replay(nested_model_guide2, trace=guide_trace),\n                                graph_type=""dense"").get_trace(include_obs=True, dim1=dim1, dim2=dim2)\n\n    guide_trace = prune_subsample_sites(guide_trace)\n    model_trace = prune_subsample_sites(model_trace)\n    model_trace.compute_log_prob()\n    guide_trace.compute_log_prob()\n\n    non_reparam_nodes = set(guide_trace.nonreparam_stochastic_nodes)\n    dc, dc_nodes = _compute_downstream_costs(model_trace, guide_trace, non_reparam_nodes)\n    dc_brute, dc_nodes_brute = _brute_force_compute_downstream_costs(model_trace, guide_trace, non_reparam_nodes)\n\n    assert dc_nodes == dc_nodes_brute\n\n    for k in dc:\n        assert(guide_trace.nodes[k][\'log_prob\'].size() == dc[k].size())\n        assert_equal(dc[k], dc_brute[k])\n\n    expected_b1 = model_trace.nodes[\'b1\'][\'log_prob\'] - guide_trace.nodes[\'b1\'][\'log_prob\']\n    expected_b1 += model_trace.nodes[\'obs1\'][\'log_prob\']\n    assert_equal(expected_b1, dc[\'b1\'])\n\n    expected_c = model_trace.nodes[\'c\'][\'log_prob\'] - guide_trace.nodes[\'c\'][\'log_prob\']\n    for i in range(dim2):\n        expected_c += model_trace.nodes[\'b{}\'.format(i)][\'log_prob\'] - \\\n            guide_trace.nodes[\'b{}\'.format(i)][\'log_prob\']\n        expected_c += model_trace.nodes[\'obs{}\'.format(i)][\'log_prob\']\n    assert_equal(expected_c, dc[\'c\'])\n\n    expected_a1 = model_trace.nodes[\'a1\'][\'log_prob\'] - guide_trace.nodes[\'a1\'][\'log_prob\']\n    expected_a1 += expected_c.sum()\n    assert_equal(expected_a1, dc[\'a1\'])\n\n\ndef plate_reuse_model_guide(include_obs=True, dim1=3, dim2=2):\n    p0 = torch.tensor(math.exp(-0.40 - include_obs * 0.2), requires_grad=True)\n    p1 = torch.tensor(math.exp(-0.33 - include_obs * 0.1), requires_grad=True)\n    pyro.sample(""a1"", dist.Bernoulli(p0 * p1))\n    my_plate1 = pyro.plate(""plate1"", dim1)\n    my_plate2 = pyro.plate(""plate2"", dim2)\n    with my_plate1 as ind1:\n        with my_plate2 as ind2:\n            pyro.sample(""c1"", dist.Bernoulli(p1).expand_by([len(ind2), len(ind1)]))\n    pyro.sample(""b1"", dist.Bernoulli(p0 * p1))\n    with my_plate2 as ind2:\n        with my_plate1 as ind1:\n            c2 = pyro.sample(""c2"", dist.Bernoulli(p1).expand_by([len(ind2), len(ind1)]))\n            if include_obs:\n                pyro.sample(""obs"", dist.Bernoulli(c2), obs=torch.ones(c2.size()))\n\n\n@pytest.mark.parametrize(""dim1"", [2, 5])\n@pytest.mark.parametrize(""dim2"", [3, 4])\ndef test_compute_downstream_costs_plate_reuse(dim1, dim2):\n    guide_trace = poutine.trace(plate_reuse_model_guide,\n                                graph_type=""dense"").get_trace(include_obs=False, dim1=dim1, dim2=dim2)\n    model_trace = poutine.trace(poutine.replay(plate_reuse_model_guide, trace=guide_trace),\n                                graph_type=""dense"").get_trace(include_obs=True, dim1=dim1, dim2=dim2)\n\n    guide_trace = prune_subsample_sites(guide_trace)\n    model_trace = prune_subsample_sites(model_trace)\n    model_trace.compute_log_prob()\n    guide_trace.compute_log_prob()\n\n    non_reparam_nodes = set(guide_trace.nonreparam_stochastic_nodes)\n    dc, dc_nodes = _compute_downstream_costs(model_trace, guide_trace,\n                                             non_reparam_nodes)\n    dc_brute, dc_nodes_brute = _brute_force_compute_downstream_costs(model_trace, guide_trace, non_reparam_nodes)\n    assert dc_nodes == dc_nodes_brute\n\n    for k in dc:\n        assert(guide_trace.nodes[k][\'log_prob\'].size() == dc[k].size())\n        assert_equal(dc[k], dc_brute[k])\n\n    expected_c1 = model_trace.nodes[\'c1\'][\'log_prob\'] - guide_trace.nodes[\'c1\'][\'log_prob\']\n    expected_c1 += (model_trace.nodes[\'b1\'][\'log_prob\'] - guide_trace.nodes[\'b1\'][\'log_prob\']).sum()\n    expected_c1 += model_trace.nodes[\'c2\'][\'log_prob\'] - guide_trace.nodes[\'c2\'][\'log_prob\']\n    expected_c1 += model_trace.nodes[\'obs\'][\'log_prob\']\n    assert_equal(expected_c1, dc[\'c1\'])\n'"
tests/infer/test_conjugate_gradients.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro\nfrom pyro.infer.tracegraph_elbo import TraceGraph_ELBO\nfrom tests.common import assert_equal\nfrom tests.integration_tests.test_conjugate_gaussian_models import GaussianChain\n\n\nclass ConjugateChainGradientTests(GaussianChain):\n\n    def test_gradients(self):\n        for N in [3, 5]:\n            for reparameterized in [True, False]:\n                self.do_test_gradients(N, reparameterized)\n\n    def do_test_gradients(self, N, reparameterized):\n        pyro.clear_param_store()\n        self.setup_chain(N)\n\n        elbo = TraceGraph_ELBO(num_particles=100000, vectorize_particles=True, max_plate_nesting=1)\n        elbo.loss_and_grads(self.model, self.guide, reparameterized=reparameterized)\n\n        for i in range(1, N + 1):\n            for param_prefix in [""loc_q_%d"", ""log_sig_q_%d"", ""kappa_q_%d""]:\n                if i == N and param_prefix == \'kappa_q_%d\':\n                    continue\n                actual_grad = pyro.param(param_prefix % i).grad\n                assert_equal(actual_grad, 0.0 * actual_grad, prec=0.10, msg="""".join([\n                             ""parameter %s%d"" % (param_prefix[:-2], i),\n                             ""\\nexpected = zero vector"",\n                             ""\\n  actual = {}"".format(actual_grad.detach().cpu().numpy())]))\n'"
tests/infer/test_csis.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nimport torch.nn as nn\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.infer\nimport pyro.optim\nfrom tests.common import assert_equal, assert_not_equal\n\n\ndef model(observations={""y1"": 0, ""y2"": 0}):\n    x = pyro.sample(""x"", dist.Normal(torch.tensor(0.), torch.tensor(5**0.5)))\n    pyro.sample(""y1"", dist.Normal(x, torch.tensor(2**0.5)), obs=observations[""y1""])\n    pyro.sample(""y2"", dist.Normal(x, torch.tensor(2**0.5)), obs=observations[""y2""])\n    return x\n\n\nclass Guide(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(1, 1, bias=False)\n        self.std = torch.nn.Parameter(torch.tensor(1.))\n\n    def forward(self, observations={""y1"": 0, ""y2"": 0}):\n        pyro.module(""guide"", self)\n        summed_obs = observations[""y1""] + observations[""y2""]\n        mean = self.linear(summed_obs.view(1, 1))[0, 0]\n        pyro.sample(""x"", dist.Normal(mean, self.std))\n\n\n@pytest.mark.init(rng_seed=7)\ndef test_csis_sampling():\n    pyro.clear_param_store()\n    guide = Guide()\n    csis = pyro.infer.CSIS(model,\n                           guide,\n                           pyro.optim.Adam({}),\n                           num_inference_samples=500)\n    # observations chosen so that proposal distribution and true posterior will both have zero mean\n    posterior = csis.run({""y1"": torch.tensor(-1.0),\n                          ""y2"": torch.tensor(1.0)})\n    assert_equal(len(posterior.exec_traces), 500)\n    marginal = pyro.infer.EmpiricalMarginal(posterior, ""x"")\n    assert_equal(marginal.mean, torch.tensor(0.0), prec=0.1)\n\n\n@pytest.mark.init(rng_seed=7)\ndef test_csis_parameter_update():\n    pyro.clear_param_store()\n    guide = Guide()\n    initial_parameters = {k: v.item() for k, v in guide.named_parameters()}\n    csis = pyro.infer.CSIS(model,\n                           guide,\n                           pyro.optim.Adam({\'lr\': 1e-2}))\n    csis.step()\n    updated_parameters = {k: v.item() for k, v in guide.named_parameters()}\n    for k, init_v in initial_parameters.items():\n        assert_not_equal(init_v, updated_parameters[k])\n\n\n@pytest.mark.init(rng_seed=7)\ndef test_csis_validation_batch():\n    pyro.clear_param_store()\n    guide = Guide()\n    csis = pyro.infer.CSIS(model,\n                           guide,\n                           pyro.optim.Adam({}),\n                           validation_batch_size=5)\n    init_loss_1 = csis.validation_loss()\n    init_loss_2 = csis.validation_loss()\n    csis.step()\n    next_loss = csis.validation_loss()\n    assert_equal(init_loss_1, init_loss_2)\n    assert_not_equal(init_loss_1, next_loss)\n'"
tests/infer/test_discrete.py,30,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport math\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer import TraceEnum_ELBO\nfrom pyro.infer.discrete import TraceEnumSample_ELBO, infer_discrete\nfrom pyro.infer.enum import config_enumerate\nfrom tests.common import assert_equal\n\nlogger = logging.getLogger(__name__)\n\n\ndef elbo_infer_discrete(model, first_available_dim, temperature):\n    """"""\n    Wrapper around ``TraceEnumSample_ELBO`` to test agreement with\n    ``TraceEnum_ELBO`` and then return ``.sample_saved()``.\n    """"""\n    assert temperature == 1\n    max_plate_nesting = -first_available_dim - 1\n    expected_elbo = TraceEnum_ELBO(max_plate_nesting=max_plate_nesting)\n    actual_elbo = TraceEnumSample_ELBO(max_plate_nesting=max_plate_nesting)\n\n    def empty_guide(*args, **kwargs):\n        pass\n\n    def inferred_model(*args, **kwargs):\n        with poutine.block():\n            expected_loss = expected_elbo.loss(model, empty_guide, *args, **kwargs)\n            actual_loss = actual_elbo.loss(model, empty_guide, *args, **kwargs)\n        assert_equal(actual_loss, expected_loss)\n        return actual_elbo.sample_saved()\n\n    return inferred_model\n\n\ndef log_mean_prob(trace, particle_dim):\n    """"""\n    Marginalizes out particle_dim from a trace.\n    """"""\n    assert particle_dim < 0\n    trace.compute_log_prob()\n    total = 0.\n    for node in trace.nodes.values():\n        if node[""type""] == ""sample"" and type(node[""fn""]).__name__ != ""_Subsample"":\n            log_prob = node[""log_prob""]\n            assert log_prob.dim() == -particle_dim\n            num_particles = log_prob.size(0)\n            total = total + log_prob.reshape(num_particles, -1).sum(-1)\n    return total.logsumexp(0) - math.log(num_particles)\n\n\n@pytest.mark.parametrize(\'infer,temperature\', [\n    (infer_discrete, 0),\n    (infer_discrete, 1),\n    (elbo_infer_discrete, 1),\n], ids=[\'map\', \'sample\', \'sample-elbo\'])\n@pytest.mark.parametrize(\'plate_size\', [2])\ndef test_plate_smoke(infer, temperature, plate_size):\n    #       +-----------------+\n    #  z1 --|--> z2 ---> x2   |\n    #       |               N | for N in {1,2}\n    #       +-----------------+\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p"", torch.tensor([0.25, 0.75]))\n        q = pyro.param(""q"", torch.tensor([[0.25, 0.75], [0.75, 0.25]]))\n        loc = pyro.param(""loc"", torch.tensor([-1., 1.]))\n        z1 = pyro.sample(""z1"", dist.Categorical(p))\n        with pyro.plate(""plate"", plate_size):\n            z2 = pyro.sample(""z2"", dist.Categorical(q[z1]))\n            pyro.sample(""x2"", dist.Normal(loc[z2], 1.), obs=torch.ones(plate_size))\n\n    first_available_dim = -2\n    infer(model, first_available_dim, temperature)()\n\n\n@pytest.mark.parametrize(\'infer,temperature\', [\n    (infer_discrete, 0),\n    (infer_discrete, 1),\n    (elbo_infer_discrete, 1),\n], ids=[\'map\', \'sample\', \'sample-elbo\'])\ndef test_distribution_1(infer, temperature):\n    #      +-------+\n    #  z --|--> x  |\n    #      +-------+\n    num_particles = 10000\n    data = torch.tensor([1., 2., 3.])\n\n    @config_enumerate\n    def model(num_particles=1, z=None):\n        p = pyro.param(""p"", torch.tensor(0.25))\n        with pyro.plate(""num_particles"", num_particles, dim=-2):\n            z = pyro.sample(""z"", dist.Bernoulli(p), obs=z)\n            logger.info(""z.shape = {}"".format(z.shape))\n            with pyro.plate(""data"", 3):\n                pyro.sample(""x"", dist.Normal(z, 1.), obs=data)\n\n    first_available_dim = -3\n    sampled_model = infer(model, first_available_dim, temperature)\n    sampled_trace = poutine.trace(sampled_model).get_trace(num_particles)\n    conditioned_traces = {z: poutine.trace(model).get_trace(z=torch.tensor(z)) for z in [0., 1.]}\n\n    # Check  posterior over z.\n    actual_z_mean = sampled_trace.nodes[""z""][""value""].mean()\n    if temperature:\n        expected_z_mean = 1 / (1 + (conditioned_traces[0].log_prob_sum() -\n                                    conditioned_traces[1].log_prob_sum()).exp())\n    else:\n        expected_z_mean = (conditioned_traces[1].log_prob_sum() >\n                           conditioned_traces[0].log_prob_sum()).float()\n    assert_equal(actual_z_mean, expected_z_mean, prec=1e-2)\n\n\n@pytest.mark.parametrize(\'infer,temperature\', [\n    (infer_discrete, 0),\n    (infer_discrete, 1),\n    (elbo_infer_discrete, 1),\n], ids=[\'map\', \'sample\', \'sample-elbo\'])\ndef test_distribution_2(infer, temperature):\n    #       +--------+\n    #  z1 --|--> x1  |\n    #   |   |        |\n    #   V   |        |\n    #  z2 --|--> x2  |\n    #       +--------+\n    num_particles = 10000\n    data = torch.tensor([[-1., -1., 0.], [-1., 1., 1.]])\n\n    @config_enumerate\n    def model(num_particles=1, z1=None, z2=None):\n        p = pyro.param(""p"", torch.tensor([[0.25, 0.75], [0.1, 0.9]]))\n        loc = pyro.param(""loc"", torch.tensor([-1., 1.]))\n        with pyro.plate(""num_particles"", num_particles, dim=-2):\n            z1 = pyro.sample(""z1"", dist.Categorical(p[0]), obs=z1)\n            z2 = pyro.sample(""z2"", dist.Categorical(p[z1]), obs=z2)\n            logger.info(""z1.shape = {}"".format(z1.shape))\n            logger.info(""z2.shape = {}"".format(z2.shape))\n            with pyro.plate(""data"", 3):\n                pyro.sample(""x1"", dist.Normal(loc[z1], 1.), obs=data[0])\n                pyro.sample(""x2"", dist.Normal(loc[z2], 1.), obs=data[1])\n\n    first_available_dim = -3\n    sampled_model = infer(model, first_available_dim, temperature)\n    sampled_trace = poutine.trace(\n        sampled_model).get_trace(num_particles)\n    conditioned_traces = {(z1, z2): poutine.trace(model).get_trace(z1=torch.tensor(z1),\n                                                                   z2=torch.tensor(z2))\n                          for z1 in [0, 1] for z2 in [0, 1]}\n\n    # Check joint posterior over (z1, z2).\n    actual_probs = torch.empty(2, 2)\n    expected_probs = torch.empty(2, 2)\n    for (z1, z2), tr in conditioned_traces.items():\n        expected_probs[z1, z2] = tr.log_prob_sum().exp()\n        actual_probs[z1, z2] = ((sampled_trace.nodes[""z1""][""value""] == z1) &\n                                (sampled_trace.nodes[""z2""][""value""] == z2)).float().mean()\n    if temperature:\n        expected_probs = expected_probs / expected_probs.sum()\n    else:\n        argmax = expected_probs.reshape(-1).max(0)[1]\n        expected_probs[:] = 0\n        expected_probs.reshape(-1)[argmax] = 1\n    assert_equal(expected_probs, actual_probs, prec=1e-2)\n\n\n@pytest.mark.parametrize(\'infer,temperature\', [\n    (infer_discrete, 0),\n    (infer_discrete, 1),\n    (elbo_infer_discrete, 1),\n], ids=[\'map\', \'sample\', \'sample-elbo\'])\ndef test_distribution_3(infer, temperature):\n    #       +---------+  +---------------+\n    #  z1 --|--> x1   |  |  z2 ---> x2   |\n    #       |       3 |  |             2 |\n    #       +---------+  +---------------+\n    num_particles = 10000\n    data = [torch.tensor([-1., -1., 0.]), torch.tensor([-1., 1.])]\n\n    @config_enumerate\n    def model(num_particles=1, z1=None, z2=None):\n        p = pyro.param(""p"", torch.tensor([0.25, 0.75]))\n        loc = pyro.param(""loc"", torch.tensor([-1., 1.]))\n        with pyro.plate(""num_particles"", num_particles, dim=-2):\n            z1 = pyro.sample(""z1"", dist.Categorical(p), obs=z1)\n            with pyro.plate(""data[0]"", 3):\n                pyro.sample(""x1"", dist.Normal(loc[z1], 1.), obs=data[0])\n            with pyro.plate(""data[1]"", 2):\n                z2 = pyro.sample(""z2"", dist.Categorical(p), obs=z2)\n                pyro.sample(""x2"", dist.Normal(loc[z2], 1.), obs=data[1])\n\n    first_available_dim = -3\n    sampled_model = infer(model, first_available_dim, temperature)\n    sampled_trace = poutine.trace(\n        sampled_model).get_trace(num_particles)\n    conditioned_traces = {(z1, z20, z21): poutine.trace(model).get_trace(z1=torch.tensor(z1),\n                                                                         z2=torch.tensor([z20, z21]))\n                          for z1 in [0, 1] for z20 in [0, 1] for z21 in [0, 1]}\n\n    # Check joint posterior over (z1, z2[0], z2[1]).\n    actual_probs = torch.empty(2, 2, 2)\n    expected_probs = torch.empty(2, 2, 2)\n    for (z1, z20, z21), tr in conditioned_traces.items():\n        expected_probs[z1, z20, z21] = tr.log_prob_sum().exp()\n        actual_probs[z1, z20, z21] = ((sampled_trace.nodes[""z1""][""value""] == z1) &\n                                      (sampled_trace.nodes[""z2""][""value""][..., :1] == z20) &\n                                      (sampled_trace.nodes[""z2""][""value""][..., 1:] == z21)).float().mean()\n    if temperature:\n        expected_probs = expected_probs / expected_probs.sum()\n    else:\n        argmax = expected_probs.reshape(-1).max(0)[1]\n        expected_probs[:] = 0\n        expected_probs.reshape(-1)[argmax] = 1\n    assert_equal(expected_probs.reshape(-1), actual_probs.reshape(-1), prec=1e-2)\n\n\n@pytest.mark.parametrize(\'infer,temperature\', [\n    (infer_discrete, 0),\n    (infer_discrete, 1),\n    (elbo_infer_discrete, 1),\n], ids=[\'map\', \'sample\', \'sample-elbo\'])\ndef test_distribution_masked(infer, temperature):\n    #      +-------+\n    #  z --|--> x  |\n    #      +-------+\n    num_particles = 10000\n    data = torch.tensor([1., 2., 3.])\n    mask = torch.tensor([True, False, False])\n\n    @config_enumerate\n    def model(num_particles=1, z=None):\n        p = pyro.param(""p"", torch.tensor(0.25))\n        with pyro.plate(""num_particles"", num_particles, dim=-2):\n            z = pyro.sample(""z"", dist.Bernoulli(p), obs=z)\n            logger.info(""z.shape = {}"".format(z.shape))\n            with pyro.plate(""data"", 3), poutine.mask(mask=mask):\n                pyro.sample(""x"", dist.Normal(z, 1.), obs=data)\n\n    first_available_dim = -3\n    sampled_model = infer(model, first_available_dim, temperature)\n    sampled_trace = poutine.trace(sampled_model).get_trace(num_particles)\n    conditioned_traces = {z: poutine.trace(model).get_trace(z=torch.tensor(z)) for z in [0., 1.]}\n\n    # Check  posterior over z.\n    actual_z_mean = sampled_trace.nodes[""z""][""value""].mean()\n    if temperature:\n        expected_z_mean = 1 / (1 + (conditioned_traces[0].log_prob_sum() -\n                                    conditioned_traces[1].log_prob_sum()).exp())\n    else:\n        expected_z_mean = (conditioned_traces[1].log_prob_sum() >\n                           conditioned_traces[0].log_prob_sum()).float()\n    assert_equal(actual_z_mean, expected_z_mean, prec=1e-2)\n\n\n@pytest.mark.parametrize(\'length\', [1, 2, 10, 100])\n@pytest.mark.parametrize(\'infer,temperature\', [\n    (infer_discrete, 0),\n    (infer_discrete, 1),\n    (elbo_infer_discrete, 1),\n], ids=[\'map\', \'sample\', \'sample-elbo\'])\ndef test_hmm_smoke(infer, temperature, length):\n\n    # This should match the example in the infer_discrete docstring.\n    def hmm(data, hidden_dim=10):\n        transition = 0.3 / hidden_dim + 0.7 * torch.eye(hidden_dim)\n        means = torch.arange(float(hidden_dim))\n        states = [0]\n        for t in pyro.markov(range(len(data))):\n            states.append(pyro.sample(""states_{}"".format(t),\n                                      dist.Categorical(transition[states[-1]])))\n            data[t] = pyro.sample(""obs_{}"".format(t),\n                                  dist.Normal(means[states[-1]], 1.),\n                                  obs=data[t])\n        return states, data\n\n    true_states, data = hmm([None] * length)\n    assert len(data) == length\n    assert len(true_states) == 1 + len(data)\n\n    decoder = infer(config_enumerate(hmm),\n                    first_available_dim=-1, temperature=temperature)\n    inferred_states, _ = decoder(data)\n    assert len(inferred_states) == len(true_states)\n\n    logger.info(""true states: {}"".format(list(map(int, true_states))))\n    logger.info(""inferred states: {}"".format(list(map(int, inferred_states))))\n\n\n@pytest.mark.xfail(reason=\'infer_discrete log_prob is incorrect\')\n@pytest.mark.parametrize(\'nderivs\', [0, 1], ids=[\'value\', \'grad\'])\ndef test_prob(nderivs):\n    #      +-------+\n    #  z --|--> x  |\n    #      +-------+\n    num_particles = 10000\n    data = torch.tensor([0.5, 1., 1.5])\n    p = pyro.param(""p"", torch.tensor(0.25))\n\n    @config_enumerate\n    def model(num_particles):\n        p = pyro.param(""p"")\n        with pyro.plate(""num_particles"", num_particles, dim=-2):\n            z = pyro.sample(""z"", dist.Bernoulli(p))\n            with pyro.plate(""data"", 3):\n                pyro.sample(""x"", dist.Normal(z, 1.), obs=data)\n\n    def guide(num_particles):\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    expected_logprob = -elbo.differentiable_loss(model, guide, num_particles=1)\n\n    posterior_model = infer_discrete(config_enumerate(model, ""parallel""),\n                                     first_available_dim=-3)\n    posterior_trace = poutine.trace(posterior_model).get_trace(num_particles=num_particles)\n    actual_logprob = log_mean_prob(posterior_trace, particle_dim=-2)\n\n    if nderivs == 0:\n        assert_equal(expected_logprob, actual_logprob, prec=1e-3)\n    elif nderivs == 1:\n        expected_grad = grad(expected_logprob, [p])[0]\n        actual_grad = grad(actual_logprob, [p])[0]\n        assert_equal(expected_grad, actual_grad, prec=1e-3)\n'"
tests/infer/test_elbo_mapdata.py,19,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim as optim\nfrom pyro.infer import SVI, TraceGraph_ELBO\nfrom tests.common import assert_equal\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_1"")\n@pytest.mark.init(rng_seed=161)\n@pytest.mark.parametrize(""map_type,batch_size,n_steps,lr"",  [(""iplate"", 3, 7000, 0.0008), (""iplate"", 8, 100, 0.018),\n                                                             (""iplate"", None, 100, 0.013), (""range"", 3, 100, 0.018),\n                                                             (""range"", 8, 100, 0.01), (""range"", None, 100, 0.011),\n                                                             (""plate"", 3, 7000, 0.0008), (""plate"", 8, 7000, 0.0008),\n                                                             (""plate"", None, 7000, 0.0008)])\ndef test_elbo_mapdata(map_type, batch_size, n_steps, lr):\n    # normal-normal: known covariance\n    lam0 = torch.tensor([0.1, 0.1])   # precision of prior\n    loc0 = torch.tensor([0.0, 0.5])   # prior mean\n    # known precision of observation noise\n    lam = torch.tensor([6.0, 4.0])\n    data = []\n    sum_data = torch.zeros(2)\n\n    def add_data_point(x, y):\n        data.append(torch.tensor([x, y]))\n        sum_data.data.add_(data[-1].data)\n\n    add_data_point(0.1, 0.21)\n    add_data_point(0.16, 0.11)\n    add_data_point(0.06, 0.31)\n    add_data_point(-0.01, 0.07)\n    add_data_point(0.23, 0.25)\n    add_data_point(0.19, 0.18)\n    add_data_point(0.09, 0.41)\n    add_data_point(-0.04, 0.17)\n\n    data = torch.stack(data)\n    n_data = torch.tensor([float(len(data))])\n    analytic_lam_n = lam0 + n_data.expand_as(lam) * lam\n    analytic_log_sig_n = -0.5 * torch.log(analytic_lam_n)\n    analytic_loc_n = sum_data * (lam / analytic_lam_n) +\\\n        loc0 * (lam0 / analytic_lam_n)\n\n    logger.debug(""DOING ELBO TEST [bs = {}, map_type = {}]"".format(batch_size, map_type))\n    pyro.clear_param_store()\n\n    def model():\n        loc_latent = pyro.sample(""loc_latent"",\n                                 dist.Normal(loc0, torch.pow(lam0, -0.5)).to_event(1))\n        if map_type == ""iplate"":\n            for i in pyro.plate(""aaa"", len(data), batch_size):\n                pyro.sample(""obs_%d"" % i, dist.Normal(loc_latent, torch.pow(lam, -0.5)) .to_event(1),\n                            obs=data[i]),\n        elif map_type == ""plate"":\n            with pyro.plate(""aaa"", len(data), batch_size) as ind:\n                pyro.sample(""obs"", dist.Normal(loc_latent, torch.pow(lam, -0.5)) .to_event(1),\n                            obs=data[ind]),\n        else:\n            for i, x in enumerate(data):\n                pyro.sample(\'obs_%d\' % i,\n                            dist.Normal(loc_latent, torch.pow(lam, -0.5))\n                            .to_event(1),\n                            obs=x)\n        return loc_latent\n\n    def guide():\n        loc_q = pyro.param(""loc_q"", analytic_loc_n.detach().clone() + torch.tensor([-0.18, 0.23]))\n        log_sig_q = pyro.param(""log_sig_q"", analytic_log_sig_n.detach().clone() - torch.tensor([-0.18, 0.23]))\n        sig_q = torch.exp(log_sig_q)\n        pyro.sample(""loc_latent"", dist.Normal(loc_q, sig_q).to_event(1))\n        if map_type == ""iplate"" or map_type is None:\n            for i in pyro.plate(""aaa"", len(data), batch_size):\n                pass\n        elif map_type == ""plate"":\n            # dummy plate to do subsampling for observe\n            with pyro.plate(""aaa"", len(data), batch_size):\n                pass\n        else:\n            pass\n\n    adam = optim.Adam({""lr"": lr})\n    svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())\n\n    for k in range(n_steps):\n        svi.step()\n\n        loc_error = torch.sum(\n            torch.pow(\n                analytic_loc_n -\n                pyro.param(""loc_q""),\n                2.0))\n        log_sig_error = torch.sum(\n            torch.pow(\n                analytic_log_sig_n -\n                pyro.param(""log_sig_q""),\n                2.0))\n\n        if k % 500 == 0:\n            logger.debug(""errors - {}, {}"".format(loc_error, log_sig_error))\n\n    assert_equal(loc_error.item(), 0, prec=0.05)\n    assert_equal(log_sig_error.item(), 0, prec=0.06)\n'"
tests/infer/test_enum.py,255,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport math\nimport os\nimport timeit\nfrom collections import defaultdict\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\nfrom torch.distributions import constraints, kl_divergence\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim\nimport pyro.poutine as poutine\nfrom pyro import infer\nfrom pyro.distributions.testing.rejection_gamma import ShapeAugmentedGamma\nfrom pyro.infer import SVI, config_enumerate\nfrom pyro.infer.enum import iter_discrete_traces\nfrom pyro.infer.importance import vectorized_importance_weights\nfrom pyro.infer.trace_elbo import Trace_ELBO\nfrom pyro.infer.traceenum_elbo import TraceEnum_ELBO\nfrom pyro.infer.util import LAST_CACHE_SIZE\nfrom pyro.ops.indexing import Vindex\nfrom pyro.util import torch_isnan\nfrom tests.common import assert_equal, skipif_param\n\ntry:\n    from contextlib import ExitStack  # python 3\nexcept ImportError:\n    from contextlib2 import ExitStack  # python 2\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _skip_cuda(*args):\n    return skipif_param(*args,\n                        condition=""CUDA_TEST"" in os.environ,\n                        reason=""https://github.com/pyro-ppl/pyro/issues/1380"")\n\n\n@pytest.mark.parametrize(""depth"", [1, 2, 3, 4, 5])\n@pytest.mark.parametrize(""graph_type"", [""flat"", ""dense""])\ndef test_iter_discrete_traces_order(depth, graph_type):\n\n    @config_enumerate(default=""sequential"")\n    def model(depth):\n        for i in range(depth):\n            pyro.sample(""x{}"".format(i), dist.Bernoulli(0.5))\n\n    traces = list(iter_discrete_traces(graph_type, model, depth))\n\n    assert len(traces) == 2 ** depth\n    for trace in traces:\n        sites = [name for name, site in trace.nodes.items() if site[""type""] == ""sample""]\n        assert sites == [""x{}"".format(i) for i in range(depth)]\n\n\n@pytest.mark.parametrize(""graph_type"", [""flat"", ""dense""])\ndef test_iter_discrete_traces_scalar(graph_type):\n    pyro.clear_param_store()\n\n    @config_enumerate(default=""sequential"")\n    def model():\n        p = pyro.param(""p"", torch.tensor(0.05))\n        probs = pyro.param(""probs"", torch.tensor([0.1, 0.2, 0.3, 0.4]))\n        x = pyro.sample(""x"", dist.Bernoulli(p))\n        y = pyro.sample(""y"", dist.Categorical(probs))\n        return dict(x=x, y=y)\n\n    traces = list(iter_discrete_traces(graph_type, model))\n\n    probs = pyro.param(""probs"")\n    assert len(traces) == 2 * len(probs)\n\n\n@pytest.mark.parametrize(""graph_type"", [""flat"", ""dense""])\n@pytest.mark.parametrize(""expand"", [False, True])\ndef test_iter_discrete_traces_vector(expand, graph_type):\n    pyro.clear_param_store()\n\n    @config_enumerate(default=""sequential"", expand=expand)\n    def model():\n        p = pyro.param(""p"", torch.tensor([0.05, 0.15]))\n        probs = pyro.param(""probs"", torch.tensor([[0.1, 0.2, 0.3, 0.4],\n                                                  [0.4, 0.3, 0.2, 0.1]]))\n        with pyro.plate(""plate"", 2):\n            x = pyro.sample(""x"", dist.Bernoulli(p))\n            y = pyro.sample(""y"", dist.Categorical(probs))\n            if expand:\n                assert x.size() == (2,)\n                assert y.size() == (2,)\n            else:\n                assert x.shape == (1,)\n                assert y.shape == (1,)\n        return dict(x=x, y=y)\n\n    traces = list(iter_discrete_traces(graph_type, model))\n\n    probs = pyro.param(""probs"")\n    assert len(traces) == 2 * probs.size(-1)\n\n\n# The usual dist.Bernoulli avoids NANs by clamping log prob. This unsafe version\n# allows us to test additional NAN avoidance in _compute_dice_elbo().\nclass UnsafeBernoulli(dist.Bernoulli):\n    def log_prob(self, value):\n        i = value.long()\n        j = torch.arange(len(self.probs), dtype=torch.long)\n        return torch.stack([(-self.probs).log1p(), self.probs.log()])[i, j]\n\n\n@pytest.mark.parametrize(\'sample_shape\', [(), (2,), (3, 4)])\ndef test_unsafe_bernoulli(sample_shape):\n    logits = torch.randn(10)\n    p = dist.Bernoulli(logits=logits)\n    q = UnsafeBernoulli(logits=logits)\n    x = p.sample(sample_shape)\n    assert_equal(p.log_prob(x), q.log_prob(x))\n\n\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\ndef test_avoid_nan(enumerate1):\n    pyro.clear_param_store()\n\n    def model():\n        p = torch.tensor([0.0, 0.5, 1.0])\n        with pyro.plate(""batch"", 3):\n            pyro.sample(""z"", UnsafeBernoulli(p))\n\n    @config_enumerate(default=enumerate1)\n    def guide():\n        p = pyro.param(""p"", torch.tensor([0.0, 0.5, 1.0], requires_grad=True))\n        with pyro.plate(""batch"", 3):\n            pyro.sample(""z"", UnsafeBernoulli(p))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1]))\n    loss = elbo.loss(model, guide)\n    assert not math.isnan(loss), loss\n    loss = elbo.differentiable_loss(model, guide)\n    assert not torch_isnan(loss), loss\n    loss = elbo.loss_and_grads(model, guide)\n    assert not math.isnan(loss), loss\n\n\n# A simple Gaussian mixture model, with no vectorization.\ndef gmm_model(data, verbose=False):\n    p = pyro.param(""p"", torch.tensor(0.3, requires_grad=True))\n    scale = pyro.param(""scale"", torch.tensor(1.0, requires_grad=True))\n    mus = torch.tensor([-1.0, 1.0])\n    for i in pyro.plate(""data"", len(data)):\n        z = pyro.sample(""z_{}"".format(i), dist.Bernoulli(p))\n        z = z.long()\n        if verbose:\n            logger.debug(""M{} z_{} = {}"".format(""  "" * int(i), int(i), z.cpu().numpy()))\n        pyro.sample(""x_{}"".format(i), dist.Normal(mus[z], scale), obs=data[i])\n\n\ndef gmm_guide(data, verbose=False):\n    for i in pyro.plate(""data"", len(data)):\n        p = pyro.param(""p_{}"".format(i), torch.tensor(0.6, requires_grad=True))\n        z = pyro.sample(""z_{}"".format(i), dist.Bernoulli(p))\n        z = z.long()\n        if verbose:\n            logger.debug(""G{} z_{} = {}"".format(""  "" * int(i), int(i), z.cpu().numpy()))\n\n\n@pytest.mark.parametrize(""data_size"", [1, 2, 3])\n@pytest.mark.parametrize(""graph_type"", [""flat"", ""dense""])\n@pytest.mark.parametrize(""model"", [gmm_model, gmm_guide])\ndef test_gmm_iter_discrete_traces(data_size, graph_type, model):\n    pyro.clear_param_store()\n    data = torch.arange(0., float(data_size))\n    model = config_enumerate(model, ""sequential"")\n    traces = list(iter_discrete_traces(graph_type, model, data=data, verbose=True))\n    # This non-vectorized version is exponential in data_size:\n    assert len(traces) == 2**data_size\n\n\n# A Gaussian mixture model, with vectorized batching.\ndef gmm_batch_model(data):\n    p = pyro.param(""p"", torch.tensor([0.3], requires_grad=True))\n    p = torch.cat([p, 1 - p])\n    scale = pyro.param(""scale"", torch.tensor([1.0], requires_grad=True))\n    mus = torch.tensor([-1.0, 1.0])\n    with pyro.plate(""data"", len(data)) as batch:\n        n = len(batch)\n        z = pyro.sample(""z"", dist.OneHotCategorical(p).expand_by([n]))\n        assert z.shape[-1] == 2\n        loc = (z * mus).sum(-1)\n        pyro.sample(""x"", dist.Normal(loc, scale.expand(n)), obs=data[batch])\n\n\ndef gmm_batch_guide(data):\n    with pyro.plate(""data"", len(data)) as batch:\n        n = len(batch)\n        probs = pyro.param(""probs"", torch.ones(n, 1) * 0.6)\n        probs = torch.cat([probs, 1 - probs], dim=1)\n        z = pyro.sample(""z"", dist.OneHotCategorical(probs))\n        assert z.shape[-1] == 2\n\n\n@pytest.mark.parametrize(""data_size"", [1, 2, 3])\n@pytest.mark.parametrize(""graph_type"", [""flat"", ""dense""])\n@pytest.mark.parametrize(""model"", [gmm_batch_model, gmm_batch_guide])\ndef test_gmm_batch_iter_discrete_traces(model, data_size, graph_type):\n    pyro.clear_param_store()\n    data = torch.arange(0., float(data_size))\n    model = config_enumerate(model, ""sequential"")\n    traces = list(iter_discrete_traces(graph_type, model, data=data))\n    # This vectorized version is independent of data_size:\n    assert len(traces) == 2\n\n\n@pytest.mark.parametrize(""model,guide"", [\n    (gmm_model, gmm_guide),\n    (gmm_batch_model, gmm_batch_guide),\n], ids=[""single"", ""batch""])\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\ndef test_svi_step_smoke(model, guide, enumerate1):\n    pyro.clear_param_store()\n    data = torch.tensor([0.0, 1.0, 9.0])\n\n    guide = config_enumerate(guide, default=enumerate1)\n    optimizer = pyro.optim.Adam({""lr"": .001})\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1]))\n    inference = SVI(model, guide, optimizer, loss=elbo)\n    inference.step(data)\n\n\n@pytest.mark.parametrize(""model,guide"", [\n    (gmm_model, gmm_guide),\n    (gmm_batch_model, gmm_batch_guide),\n], ids=[""single"", ""batch""])\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\ndef test_differentiable_loss(model, guide, enumerate1):\n    pyro.clear_param_store()\n    data = torch.tensor([0.0, 1.0, 9.0])\n\n    guide = config_enumerate(guide, default=enumerate1)\n    elbo = TraceEnum_ELBO(max_plate_nesting=1,\n                          strict_enumeration_warning=any([enumerate1]))\n\n    pyro.set_rng_seed(0)\n    loss = elbo.differentiable_loss(model, guide, data)\n    param_names = sorted(pyro.get_param_store())\n    actual_loss = loss.item()\n    actual_grads = grad(loss, [pyro.param(name).unconstrained() for name in param_names])\n\n    pyro.set_rng_seed(0)\n    expected_loss = elbo.loss_and_grads(model, guide, data)\n    expected_grads = [pyro.param(name).unconstrained().grad for name in param_names]\n\n    assert_equal(actual_loss, expected_loss)\n    for name, actual_grad, expected_grad in zip(param_names, actual_grads, expected_grads):\n        assert_equal(actual_grad, expected_grad, msg=\'bad {} gradient. Expected:\\n{}\\nActual:\\n{}\'.format(\n            name, expected_grad, actual_grad))\n\n\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\ndef test_svi_step_guide_uses_grad(enumerate1):\n    data = torch.tensor([0., 1., 3.])\n\n    def model():\n        scale = pyro.param(""scale"")\n        loc = pyro.sample(""loc"", dist.Normal(0., 10.))\n        pyro.sample(""b"", dist.Bernoulli(0.5))\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", dist.Normal(loc, scale), obs=data)\n\n    @config_enumerate(default=enumerate1)\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5), constraint=constraints.unit_interval)\n        scale = pyro.param(""scale"", torch.tensor(1.0), constraint=constraints.positive)\n        var = pyro.param(""var"", torch.tensor(1.0), constraint=constraints.positive)\n\n        x = torch.tensor(0., requires_grad=True)\n        prior = dist.Normal(0., 10.).log_prob(x)\n        likelihood = dist.Normal(x, scale).log_prob(data).sum()\n        loss = -(prior + likelihood)\n        g = grad(loss, [x], create_graph=True)[0]\n        H = grad(g, [x], create_graph=True)[0]\n        loc = x.detach() - g / H  # newton step\n        pyro.sample(""loc"", dist.Normal(loc, var))\n        pyro.sample(""b"", dist.Bernoulli(p))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1]))\n    inference = SVI(model, guide, pyro.optim.Adam({}), elbo)\n    inference.step()\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\n@pytest.mark.parametrize(""method"", [""loss"", ""differentiable_loss"", ""loss_and_grads""])\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\ndef test_elbo_bern(method, enumerate1, scale):\n    pyro.clear_param_store()\n    num_particles = 1 if enumerate1 else 10000\n    prec = 0.001 if enumerate1 else 0.22\n    q = pyro.param(""q"", torch.tensor(0.5, requires_grad=True))\n    kl = kl_divergence(dist.Bernoulli(q), dist.Bernoulli(0.25))\n\n    @poutine.scale(scale=scale)\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""z"", dist.Bernoulli(0.25).expand_by([num_particles]))\n\n    @config_enumerate(default=enumerate1)\n    @poutine.scale(scale=scale)\n    def guide():\n        q = pyro.param(""q"")\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""z"", dist.Bernoulli(q).expand_by([num_particles]))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1]))\n\n    if method == ""loss"":\n        actual = elbo.loss(model, guide) / num_particles\n        expected = kl.item() * scale\n        assert_equal(actual, expected, prec=prec, msg="""".join([\n            ""\\nexpected = {}"".format(expected),\n            ""\\n  actual = {}"".format(actual),\n        ]))\n    else:\n        if method == ""differentiable_loss"":\n            loss = elbo.differentiable_loss(model, guide)\n            actual = grad(loss, [q])[0] / num_particles\n        elif method == ""loss_and_grads"":\n            elbo.loss_and_grads(model, guide)\n            actual = q.grad / num_particles\n        expected = grad(kl, [q])[0] * scale\n        assert_equal(actual, expected, prec=prec, msg="""".join([\n            ""\\nexpected = {}"".format(expected.detach().cpu().numpy()),\n            ""\\n  actual = {}"".format(actual.detach().cpu().numpy()),\n        ]))\n\n\n@pytest.mark.parametrize(""method"", [""loss"", ""differentiable_loss"", ""loss_and_grads""])\n@pytest.mark.parametrize(""enumerate1"", [None, ""parallel""])\ndef test_elbo_normal(method, enumerate1):\n    pyro.clear_param_store()\n    num_particles = 1 if enumerate1 else 10000\n    prec = 0.01\n    q = pyro.param(""q"", torch.tensor(1., requires_grad=True))\n    kl = kl_divergence(dist.Normal(q, 1.), dist.Normal(0., 1.))\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""z"", dist.Normal(0., 1.).expand_by([num_particles]))\n\n    @config_enumerate(default=enumerate1, num_samples=20000)\n    def guide():\n        q = pyro.param(""q"")\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""z"", dist.Normal(q, 1.).expand_by([num_particles]))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1]))\n\n    if method == ""loss"":\n        actual = elbo.loss(model, guide) / num_particles\n        expected = kl.item()\n        assert_equal(actual, expected, prec=prec, msg="""".join([\n            ""\\nexpected = {}"".format(expected),\n            ""\\n  actual = {}"".format(actual),\n        ]))\n    else:\n        if method == ""differentiable_loss"":\n            loss = elbo.differentiable_loss(model, guide)\n            actual = grad(loss, [q])[0] / num_particles\n        elif method == ""loss_and_grads"":\n            elbo.loss_and_grads(model, guide)\n            actual = q.grad / num_particles\n        expected = grad(kl, [q])[0]\n        assert_equal(actual, expected, prec=prec, msg="""".join([\n            ""\\nexpected = {}"".format(expected.detach().cpu().numpy()),\n            ""\\n  actual = {}"".format(actual.detach().cpu().numpy()),\n        ]))\n\n\n@pytest.mark.parametrize(""enumerate1,num_samples1"", [\n    (None, None),\n    (""sequential"", None),\n    (""parallel"", None),\n    (""parallel"", 300),\n])\n@pytest.mark.parametrize(""enumerate2,num_samples2"", [\n    (None, None),\n    (""sequential"", None),\n    (""parallel"", None),\n    (""parallel"", 300),\n])\n@pytest.mark.parametrize(""method"", [""differentiable_loss"", ""loss_and_grads""])\ndef test_elbo_bern_bern(method, enumerate1, enumerate2, num_samples1, num_samples2):\n    pyro.clear_param_store()\n    if enumerate1 and enumerate2 and num_samples1 is None and num_samples2 is None:\n        num_particles = 1\n        prec = 0.001\n    else:\n        num_particles = 2 * 300 * 300\n        for n in [num_samples1, num_samples2]:\n            if n is not None:\n                num_particles = num_particles // n\n        prec = 0.2\n\n    q = pyro.param(""q"", torch.tensor(0.75, requires_grad=True))\n\n    def model():\n        pyro.sample(""x1"", dist.Bernoulli(0.2))\n        pyro.sample(""x2"", dist.Bernoulli(0.4))\n\n    def guide():\n        q = pyro.param(""q"")\n        pyro.sample(""x1"", dist.Bernoulli(q), infer={""enumerate"": enumerate1, ""num_samples"": num_samples1})\n        pyro.sample(""x2"", dist.Bernoulli(q), infer={""enumerate"": enumerate2, ""num_samples"": num_samples2})\n\n    kl = sum(kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p)) for p in [0.2, 0.4])\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(num_particles=num_particles,\n                          vectorize_particles=True,\n                          strict_enumeration_warning=any([enumerate1, enumerate2]))\n    if method == ""differentiable_loss"":\n        loss = elbo.differentiable_loss(model, guide)\n        actual_loss = loss.item()\n        actual_grad = grad(loss, [q])[0]\n    else:\n        actual_loss = elbo.loss_and_grads(model, guide)\n        actual_grad = q.grad\n\n    assert_equal(actual_loss, expected_loss, prec=prec, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=prec, msg="""".join([\n        ""\\nexpected grads = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grads = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""enumerate1,enumerate2,enumerate3,num_samples"", [\n    (e1, e2, e3, num_samples)\n    for e1 in [None, ""sequential"", ""parallel""]\n    for e2 in [None, ""sequential"", ""parallel""]\n    for e3 in [None, ""sequential"", ""parallel""]\n    for num_samples in [None, 10000]\n    if num_samples is None or (e1, e2, e3) == (""parallel"", ""parallel"", ""parallel"")\n])\n@pytest.mark.parametrize(""method"", [""differentiable_loss"", ""loss_and_grads""])\ndef test_elbo_berns(method, enumerate1, enumerate2, enumerate3, num_samples):\n    pyro.clear_param_store()\n    num_particles = 1 if all([enumerate1, enumerate2, enumerate3]) else 10000\n    prec = 0.001 if all([enumerate1, enumerate2, enumerate3]) and not num_samples else 0.1\n    q = pyro.param(""q"", torch.tensor(0.75, requires_grad=True))\n\n    def model():\n        pyro.sample(""x1"", dist.Bernoulli(0.1))\n        pyro.sample(""x2"", dist.Bernoulli(0.2))\n        pyro.sample(""x3"", dist.Bernoulli(0.3))\n\n    def guide():\n        q = pyro.param(""q"")\n        pyro.sample(""x1"", dist.Bernoulli(q), infer={""enumerate"": enumerate1, ""num_samples"": num_samples})\n        pyro.sample(""x2"", dist.Bernoulli(q), infer={""enumerate"": enumerate2, ""num_samples"": num_samples})\n        pyro.sample(""x3"", dist.Bernoulli(q), infer={""enumerate"": enumerate3, ""num_samples"": num_samples})\n\n    kl = sum(kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p)) for p in [0.1, 0.2, 0.3])\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(num_particles=num_particles,\n                          vectorize_particles=True,\n                          strict_enumeration_warning=any([enumerate1, enumerate2, enumerate3]))\n    if method == ""differentiable_loss"":\n        loss = elbo.differentiable_loss(model, guide)\n        actual_loss = loss.item()\n        actual_grad = grad(loss, [q])[0]\n    else:\n        actual_loss = elbo.loss_and_grads(model, guide)\n        actual_grad = q.grad\n\n    assert_equal(actual_loss, expected_loss, prec=prec, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=prec, msg="""".join([\n        ""\\nexpected grads = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grads = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""num_samples"", [None, 2000])\n@pytest.mark.parametrize(""max_plate_nesting"", [0, 1])\n@pytest.mark.parametrize(""enumerate1"", [""sequential"", ""parallel""])\n@pytest.mark.parametrize(""enumerate2"", [""sequential"", ""parallel""])\n@pytest.mark.parametrize(""enumerate3"", [""sequential"", ""parallel""])\ndef test_elbo_categoricals(enumerate1, enumerate2, enumerate3, max_plate_nesting, num_samples):\n    pyro.clear_param_store()\n    p1 = torch.tensor([0.6, 0.4])\n    p2 = torch.tensor([0.3, 0.3, 0.4])\n    p3 = torch.tensor([0.1, 0.2, 0.3, 0.4])\n    q1 = pyro.param(""q1"", torch.tensor([0.4, 0.6], requires_grad=True))\n    q2 = pyro.param(""q2"", torch.tensor([0.4, 0.3, 0.3], requires_grad=True))\n    q3 = pyro.param(""q3"", torch.tensor([0.4, 0.3, 0.2, 0.1], requires_grad=True))\n\n    def model():\n        pyro.sample(""x1"", dist.Categorical(p1))\n        pyro.sample(""x2"", dist.Categorical(p2))\n        pyro.sample(""x3"", dist.Categorical(p3))\n\n    def guide():\n        pyro.sample(""x1"", dist.Categorical(pyro.param(""q1"")),\n                    infer={""enumerate"": enumerate1,\n                           ""num_samples"": num_samples if enumerate1 == ""parallel"" else None})\n        pyro.sample(""x2"", dist.Categorical(pyro.param(""q2"")),\n                    infer={""enumerate"": enumerate2,\n                           ""num_samples"": num_samples if enumerate2 == ""parallel"" else None})\n        pyro.sample(""x3"", dist.Categorical(pyro.param(""q3"")),\n                    infer={""enumerate"": enumerate3,\n                           ""num_samples"": num_samples if enumerate3 == ""parallel"" else None})\n\n    kl = (kl_divergence(dist.Categorical(q1), dist.Categorical(p1)) +\n          kl_divergence(dist.Categorical(q2), dist.Categorical(p2)) +\n          kl_divergence(dist.Categorical(q3), dist.Categorical(p3)))\n    expected_loss = kl.item()\n    expected_grads = grad(kl, [q1, q2, q3])\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=max_plate_nesting,\n                          strict_enumeration_warning=any([enumerate1, enumerate2, enumerate3]))\n    actual_loss = elbo.loss_and_grads(model, guide)\n    actual_grads = [q1.grad, q2.grad, q3.grad]\n\n    assert_equal(actual_loss, expected_loss, prec=0.001 if not num_samples else 0.1, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    for actual_grad, expected_grad in zip(actual_grads, expected_grads):\n        assert_equal(actual_grad, expected_grad, prec=0.001 if not num_samples else 0.1, msg="""".join([\n            ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n            ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n        ]))\n\n\n@pytest.mark.parametrize(""enumerate1"", [None, ""parallel""])\n@pytest.mark.parametrize(""enumerate2"", [None, ""parallel""])\n@pytest.mark.parametrize(""enumerate3"", [None, ""parallel""])\n@pytest.mark.parametrize(""method"", [""differentiable_loss"", ""loss_and_grads""])\ndef test_elbo_normals(method, enumerate1, enumerate2, enumerate3):\n    pyro.clear_param_store()\n    num_particles = 100 * 10 ** sum(1 for e in [enumerate1, enumerate2, enumerate3] if not e)\n    prec = 0.1\n    q = pyro.param(""q"", torch.tensor(0.0, requires_grad=True))\n\n    def model():\n        pyro.sample(""x1"", dist.Normal(0.25, 1.))\n        pyro.sample(""x2"", dist.Normal(0.5, 1.))\n        pyro.sample(""x3"", dist.Normal(1., 1.))\n\n    def guide():\n        q = pyro.param(""q"")\n        pyro.sample(""x1"", dist.Normal(q, 1.), infer={""enumerate"": enumerate1, ""num_samples"": 10})\n        pyro.sample(""x2"", dist.Normal(q, 1.), infer={""enumerate"": enumerate2, ""num_samples"": 10})\n        pyro.sample(""x3"", dist.Normal(q, 1.), infer={""enumerate"": enumerate3, ""num_samples"": 10})\n\n    kl = sum(kl_divergence(dist.Normal(q, 1.), dist.Normal(p, 1.)) for p in [0.25, 0.5, 1.])\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(num_particles=num_particles,\n                          vectorize_particles=True,\n                          strict_enumeration_warning=any([enumerate1, enumerate2, enumerate3]))\n    if method == ""differentiable_loss"":\n        loss = elbo.differentiable_loss(model, guide)\n        actual_loss = loss.item()\n        actual_grad = grad(loss, [q])[0]\n    else:\n        actual_loss = elbo.loss_and_grads(model, guide)\n        actual_grad = q.grad\n\n    assert_equal(actual_loss, expected_loss, prec=prec, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=prec, msg="""".join([\n        ""\\nexpected grads = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grads = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""enumerate1,enumerate2,num_samples"", [\n    (e1, e2, num_samples)\n    for e1 in [None, ""sequential"", ""parallel""]\n    for e2 in [None, ""sequential"", ""parallel""]\n    for num_samples in [None, 10000]\n    if num_samples is None or (e1, e2) == (""parallel"", ""parallel"")\n])\n@pytest.mark.parametrize(""plate_dim"", [1, 2])\ndef test_elbo_plate(plate_dim, enumerate1, enumerate2, num_samples):\n    pyro.clear_param_store()\n    num_particles = 1 if all([enumerate1, enumerate2]) else 10000\n    q = pyro.param(""q"", torch.tensor(0.75, requires_grad=True))\n    p = 0.2693204236205713  # for which kl(Bernoulli(q), Bernoulli(p)) = 0.5\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""y"", dist.Bernoulli(p).expand_by([num_particles]))\n            with pyro.plate(""plate"", plate_dim):\n                pyro.sample(""z"", dist.Bernoulli(p).expand_by([plate_dim, num_particles]))\n\n    def guide():\n        q = pyro.param(""q"")\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""y"", dist.Bernoulli(q).expand_by([num_particles]),\n                        infer={""enumerate"": enumerate1, ""num_samples"": num_samples})\n            with pyro.plate(""plate"", plate_dim):\n                pyro.sample(""z"", dist.Bernoulli(q).expand_by([plate_dim, num_particles]),\n                            infer={""enumerate"": enumerate2, ""num_samples"": num_samples})\n\n    kl = (1 + plate_dim) * kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p))\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1, enumerate2]))\n    actual_loss = elbo.loss_and_grads(model, guide) / num_particles\n    actual_grad = pyro.param(\'q\').grad / num_particles\n\n    assert_equal(actual_loss, expected_loss, prec=0.1, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=0.1, msg="""".join([\n        ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""enumerate2"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""plate_dim"", [1, 2])\ndef test_elbo_iplate(plate_dim, enumerate1, enumerate2):\n    pyro.clear_param_store()\n    num_particles = 1 if all([enumerate1, enumerate2]) else 20000\n    q = pyro.param(""q"", torch.tensor(0.75, requires_grad=True))\n    p = 0.2693204236205713  # for which kl(Bernoulli(q), Bernoulli(p)) = 0.5\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([num_particles]))\n            for i in pyro.plate(""plate"", plate_dim):\n                pyro.sample(""y_{}"".format(i), dist.Bernoulli(p).expand_by([num_particles]))\n\n    def guide():\n        q = pyro.param(""q"")\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""x"", dist.Bernoulli(q).expand_by([num_particles]),\n                        infer={""enumerate"": enumerate1})\n            for i in pyro.plate(""plate"", plate_dim):\n                pyro.sample(""y_{}"".format(i), dist.Bernoulli(q).expand_by([num_particles]),\n                            infer={""enumerate"": enumerate2})\n\n    kl = (1 + plate_dim) * kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p))\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1, enumerate2]))\n    actual_loss = elbo.loss_and_grads(model, guide) / num_particles\n    actual_grad = pyro.param(\'q\').grad / num_particles\n\n    assert_equal(actual_loss, expected_loss, prec=0.1, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=0.1, msg="""".join([\n        ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""enumerate1,enumerate2,enumerate3,enumerate4,num_samples"", [\n    (e1, e2, e3, e4, num_samples)\n    for e1 in [None, ""sequential"", ""parallel""]\n    for e2 in [None, ""sequential"", ""parallel""]\n    for e3 in [None, ""sequential"", ""parallel""]\n    for e4 in [None, ""sequential"", ""parallel""]\n    for num_samples in [None, 10000]\n    if num_samples is None or (e1, e2, e3, e4) == (""parallel"",) * 4\n])\n@pytest.mark.parametrize(""inner_dim"", [2])\n@pytest.mark.parametrize(""outer_dim"", [2])\ndef test_elbo_plate_plate(outer_dim, inner_dim, enumerate1, enumerate2, enumerate3, enumerate4, num_samples):\n    pyro.clear_param_store()\n    num_particles = 1 if all([enumerate1, enumerate2, enumerate3, enumerate4]) else 100000\n    q = pyro.param(""q"", torch.tensor(0.75, requires_grad=True))\n    p = 0.2693204236205713  # for which kl(Bernoulli(q), Bernoulli(p)) = 0.5\n\n    def model():\n        d = dist.Bernoulli(p)\n        context1 = pyro.plate(""outer"", outer_dim, dim=-1)\n        context2 = pyro.plate(""inner"", inner_dim, dim=-2)\n        pyro.sample(""w"", d)\n        with context1:\n            pyro.sample(""x"", d)\n        with context2:\n            pyro.sample(""y"", d)\n        with context1, context2:\n            pyro.sample(""z"", d)\n\n    def guide():\n        d = dist.Bernoulli(pyro.param(""q""))\n        context1 = pyro.plate(""outer"", outer_dim, dim=-1)\n        context2 = pyro.plate(""inner"", inner_dim, dim=-2)\n        pyro.sample(""w"", d, infer={""enumerate"": enumerate1, ""num_samples"": num_samples})\n        with context1:\n            pyro.sample(""x"", d, infer={""enumerate"": enumerate2, ""num_samples"": num_samples})\n        with context2:\n            pyro.sample(""y"", d, infer={""enumerate"": enumerate3, ""num_samples"": num_samples})\n        with context1, context2:\n            pyro.sample(""z"", d, infer={""enumerate"": enumerate4, ""num_samples"": num_samples})\n\n    kl_node = kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p))\n    kl = (1 + outer_dim + inner_dim + outer_dim * inner_dim) * kl_node\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(num_particles=num_particles,\n                          vectorize_particles=True,\n                          strict_enumeration_warning=any([enumerate1, enumerate2, enumerate3]))\n    actual_loss = elbo.loss_and_grads(model, guide)\n    actual_grad = pyro.param(\'q\').grad\n\n    assert_equal(actual_loss, expected_loss, prec=0.1, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=0.1, msg="""".join([\n        ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""enumerate1,enumerate2,enumerate3,num_samples"", [\n    (e1, e2, e3, num_samples)\n    for e1 in [None, ""sequential"", ""parallel""]\n    for e2 in [None, ""sequential"", ""parallel""]\n    for e3 in [None, ""sequential"", ""parallel""]\n    for num_samples in [None, 2000]\n    if num_samples is None or (e1, e2, e3) == (""parallel"",) * 3\n])\n@pytest.mark.parametrize(""inner_dim"", [2])\n@pytest.mark.parametrize(""outer_dim"", [3])\ndef test_elbo_plate_iplate(outer_dim, inner_dim, enumerate1, enumerate2, enumerate3, num_samples):\n    pyro.clear_param_store()\n    num_particles = 1 if all([enumerate1, enumerate2, enumerate3]) else 100000\n    q = pyro.param(""q"", torch.tensor(0.75, requires_grad=True))\n    p = 0.2693204236205713  # for which kl(Bernoulli(q), Bernoulli(p)) = 0.5\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([num_particles]))\n            with pyro.plate(""outer"", outer_dim):\n                pyro.sample(""y"", dist.Bernoulli(p).expand_by([outer_dim, num_particles]))\n                for i in pyro.plate(""inner"", inner_dim):\n                    pyro.sample(""z_{}"".format(i), dist.Bernoulli(p).expand_by([outer_dim, num_particles]))\n\n    def guide():\n        q = pyro.param(""q"")\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""x"", dist.Bernoulli(q).expand_by([num_particles]),\n                        infer={""enumerate"": enumerate1, ""num_samples"": num_samples})\n            with pyro.plate(""outer"", outer_dim):\n                pyro.sample(""y"", dist.Bernoulli(q).expand_by([outer_dim, num_particles]),\n                            infer={""enumerate"": enumerate2, ""num_samples"": num_samples})\n                for i in pyro.plate(""inner"", inner_dim):\n                    pyro.sample(""z_{}"".format(i), dist.Bernoulli(q).expand_by([outer_dim, num_particles]),\n                                infer={""enumerate"": enumerate3, ""num_samples"": num_samples})\n\n    kl = (1 + outer_dim * (1 + inner_dim)) * kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p))\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1, enumerate2, enumerate3]))\n    actual_loss = elbo.loss_and_grads(model, guide) / num_particles\n    actual_grad = pyro.param(\'q\').grad / num_particles\n\n    assert_equal(actual_loss, expected_loss, prec=0.1, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=0.1, msg="""".join([\n        ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""enumerate3"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""enumerate2"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""inner_dim"", [2])\n@pytest.mark.parametrize(""outer_dim"", [2])\ndef test_elbo_iplate_plate(outer_dim, inner_dim, enumerate1, enumerate2, enumerate3):\n    pyro.clear_param_store()\n    num_particles = 1 if all([enumerate1, enumerate2, enumerate3]) else 50000\n    q = pyro.param(""q"", torch.tensor(0.75, requires_grad=True))\n    p = 0.2693204236205713  # for which kl(Bernoulli(q), Bernoulli(p)) = 0.5\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([num_particles]))\n            inner_plate = pyro.plate(""inner"", inner_dim)\n            for i in pyro.plate(""outer"", outer_dim):\n                pyro.sample(""y_{}"".format(i), dist.Bernoulli(p).expand_by([num_particles]))\n                with inner_plate:\n                    pyro.sample(""z_{}"".format(i), dist.Bernoulli(p).expand_by([inner_dim, num_particles]))\n\n    def guide():\n        q = pyro.param(""q"")\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""x"", dist.Bernoulli(q).expand_by([num_particles]),\n                        infer={""enumerate"": enumerate1})\n            inner_plate = pyro.plate(""inner"", inner_dim)\n            for i in pyro.plate(""outer"", outer_dim):\n                pyro.sample(""y_{}"".format(i), dist.Bernoulli(q).expand_by([num_particles]),\n                            infer={""enumerate"": enumerate2})\n                with inner_plate:\n                    pyro.sample(""z_{}"".format(i), dist.Bernoulli(q).expand_by([inner_dim, num_particles]),\n                                infer={""enumerate"": enumerate3})\n\n    kl = (1 + outer_dim * (1 + inner_dim)) * kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p))\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1, enumerate2, enumerate3]))\n    actual_loss = elbo.loss_and_grads(model, guide) / num_particles\n    actual_grad = pyro.param(\'q\').grad / num_particles\n\n    assert_equal(actual_loss, expected_loss, prec=0.1, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=0.1, msg="""".join([\n        ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""enumerate3"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""enumerate2"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""inner_dim"", [2])\n@pytest.mark.parametrize(""outer_dim"", [2])\ndef test_elbo_iplate_iplate(outer_dim, inner_dim, enumerate1, enumerate2, enumerate3):\n    pyro.clear_param_store()\n    num_particles = 1 if all([enumerate1, enumerate2, enumerate3]) else 150000\n    q = pyro.param(""q"", torch.tensor(0.75, requires_grad=True))\n    p = 0.2693204236205713  # for which kl(Bernoulli(q), Bernoulli(p)) = 0.5\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([num_particles]))\n            inner_iplate = pyro.plate(""inner"", outer_dim)\n            for i in pyro.plate(""outer"", inner_dim):\n                pyro.sample(""y_{}"".format(i), dist.Bernoulli(p).expand_by([num_particles]))\n                for j in inner_iplate:\n                    pyro.sample(""z_{}_{}"".format(i, j), dist.Bernoulli(p).expand_by([num_particles]))\n\n    def guide():\n        q = pyro.param(""q"")\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""x"", dist.Bernoulli(q).expand_by([num_particles]),\n                        infer={""enumerate"": enumerate1})\n            inner_iplate = pyro.plate(""inner"", inner_dim)\n            for i in pyro.plate(""outer"", outer_dim):\n                pyro.sample(""y_{}"".format(i), dist.Bernoulli(q).expand_by([num_particles]),\n                            infer={""enumerate"": enumerate2})\n                for j in inner_iplate:\n                    pyro.sample(""z_{}_{}"".format(i, j), dist.Bernoulli(q).expand_by([num_particles]),\n                                infer={""enumerate"": enumerate3})\n\n    kl = (1 + outer_dim * (1 + inner_dim)) * kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p))\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q])[0]\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1, enumerate2, enumerate3]))\n    actual_loss = elbo.loss_and_grads(model, guide) / num_particles\n    actual_grad = pyro.param(\'q\').grad / num_particles\n\n    assert_equal(actual_loss, expected_loss, prec=0.1, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=0.2, msg="""".join([\n        ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""pi1"", [0.33, 0.43])\n@pytest.mark.parametrize(""pi2"", [0.55, 0.27])\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\ndef test_non_mean_field_bern_bern_elbo_gradient(enumerate1, pi1, pi2):\n    pyro.clear_param_store()\n    num_particles = 1 if enumerate1 else 20000\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            y = pyro.sample(""y"", dist.Bernoulli(0.33).expand_by([num_particles]))\n            pyro.sample(""z"", dist.Bernoulli(0.55 * y + 0.10))\n\n    def guide():\n        q1 = pyro.param(""q1"", torch.tensor(pi1, requires_grad=True))\n        q2 = pyro.param(""q2"", torch.tensor(pi2, requires_grad=True))\n        with pyro.plate(""particles"", num_particles):\n            y = pyro.sample(""y"", dist.Bernoulli(q1).expand_by([num_particles]))\n            pyro.sample(""z"", dist.Bernoulli(q2 * y + 0.10))\n\n    logger.info(""Computing gradients using surrogate loss"")\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1]))\n    elbo.loss_and_grads(model, config_enumerate(guide, default=enumerate1))\n    actual_grad_q1 = pyro.param(\'q1\').grad / num_particles\n    actual_grad_q2 = pyro.param(\'q2\').grad / num_particles\n\n    logger.info(""Computing analytic gradients"")\n    q1 = torch.tensor(pi1, requires_grad=True)\n    q2 = torch.tensor(pi2, requires_grad=True)\n    elbo = kl_divergence(dist.Bernoulli(q1), dist.Bernoulli(0.33))\n    elbo = elbo + q1 * kl_divergence(dist.Bernoulli(q2 + 0.10), dist.Bernoulli(0.65))\n    elbo = elbo + (1.0 - q1) * kl_divergence(dist.Bernoulli(0.10), dist.Bernoulli(0.10))\n    expected_grad_q1, expected_grad_q2 = grad(elbo, [q1, q2])\n\n    prec = 0.03 if enumerate1 is None else 0.001\n\n    assert_equal(actual_grad_q1, expected_grad_q1, prec=prec, msg="""".join([\n        ""\\nq1 expected = {}"".format(expected_grad_q1.data.cpu().numpy()),\n        ""\\nq1  actual = {}"".format(actual_grad_q1.data.cpu().numpy()),\n    ]))\n    assert_equal(actual_grad_q2, expected_grad_q2, prec=prec, msg="""".join([\n        ""\\nq2 expected = {}"".format(expected_grad_q2.data.cpu().numpy()),\n        ""\\nq2   actual = {}"".format(actual_grad_q2.data.cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""pi1"", [0.33, 0.44])\n@pytest.mark.parametrize(""pi2"", [0.55, 0.39])\n@pytest.mark.parametrize(""pi3"", [0.22, 0.29])\n@pytest.mark.parametrize(""enumerate1,num_samples"", [\n    (None, None),\n    (""sequential"", None),\n    (""parallel"", None),\n    (""parallel"", 2),\n])\ndef test_non_mean_field_bern_normal_elbo_gradient(enumerate1, pi1, pi2, pi3, num_samples):\n    pyro.clear_param_store()\n    include_z = True\n    num_particles = 10000\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            q3 = pyro.param(""q3"", torch.tensor(pi3, requires_grad=True))\n            y = pyro.sample(""y"", dist.Bernoulli(q3).expand_by([num_particles]))\n            if include_z:\n                pyro.sample(""z"", dist.Normal(0.55 * y + q3, 1.0))\n\n    def guide():\n        q1 = pyro.param(""q1"", torch.tensor(pi1, requires_grad=True))\n        q2 = pyro.param(""q2"", torch.tensor(pi2, requires_grad=True))\n        with pyro.plate(""particles"", num_particles):\n            y = pyro.sample(""y"", dist.Bernoulli(q1).expand_by([num_particles]), infer={""enumerate"": enumerate1})\n            if include_z:\n                pyro.sample(""z"", dist.Normal(q2 * y + 0.10, 1.0))\n\n    logger.info(""Computing gradients using surrogate loss"")\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1]))\n    elbo.loss_and_grads(model, guide)\n    actual_grad_q1 = pyro.param(\'q1\').grad / num_particles\n    if include_z:\n        actual_grad_q2 = pyro.param(\'q2\').grad / num_particles\n    actual_grad_q3 = pyro.param(\'q3\').grad / num_particles\n\n    logger.info(""Computing analytic gradients"")\n    q1 = torch.tensor(pi1, requires_grad=True)\n    q2 = torch.tensor(pi2, requires_grad=True)\n    q3 = torch.tensor(pi3, requires_grad=True)\n    elbo = kl_divergence(dist.Bernoulli(q1), dist.Bernoulli(q3))\n    if include_z:\n        elbo = elbo + q1 * kl_divergence(dist.Normal(q2 + 0.10, 1.0), dist.Normal(q3 + 0.55, 1.0))\n        elbo = elbo + (1.0 - q1) * kl_divergence(dist.Normal(0.10, 1.0), dist.Normal(q3, 1.0))\n        expected_grad_q1, expected_grad_q2, expected_grad_q3 = grad(elbo, [q1, q2, q3])\n    else:\n        expected_grad_q1, expected_grad_q3 = grad(elbo, [q1, q3])\n\n    prec = 0.04 if enumerate1 is None else 0.02\n\n    assert_equal(actual_grad_q1, expected_grad_q1, prec=prec, msg="""".join([\n        ""\\nq1 expected = {}"".format(expected_grad_q1.data.cpu().numpy()),\n        ""\\nq1   actual = {}"".format(actual_grad_q1.data.cpu().numpy()),\n    ]))\n    if include_z:\n        assert_equal(actual_grad_q2, expected_grad_q2, prec=prec, msg="""".join([\n            ""\\nq2 expected = {}"".format(expected_grad_q2.data.cpu().numpy()),\n            ""\\nq2   actual = {}"".format(actual_grad_q2.data.cpu().numpy()),\n        ]))\n    assert_equal(actual_grad_q3, expected_grad_q3, prec=prec, msg="""".join([\n        ""\\nq3 expected = {}"".format(expected_grad_q3.data.cpu().numpy()),\n        ""\\nq3   actual = {}"".format(actual_grad_q3.data.cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""pi1"", [0.33, 0.41])\n@pytest.mark.parametrize(""pi2"", [0.44, 0.17])\n@pytest.mark.parametrize(""pi3"", [0.22, 0.29])\ndef test_non_mean_field_normal_bern_elbo_gradient(pi1, pi2, pi3):\n\n    def model(num_particles):\n        with pyro.plate(""particles"", num_particles):\n            q3 = pyro.param(""q3"", torch.tensor(pi3, requires_grad=True))\n            q4 = pyro.param(""q4"", torch.tensor(0.5 * (pi1 + pi2), requires_grad=True))\n            z = pyro.sample(""z"", dist.Normal(q3, 1.0).expand_by([num_particles]))\n            zz = torch.exp(z) / (1.0 + torch.exp(z))\n            pyro.sample(""y"", dist.Bernoulli(q4 * zz))\n\n    def guide(num_particles):\n        q1 = pyro.param(""q1"", torch.tensor(pi1, requires_grad=True))\n        q2 = pyro.param(""q2"", torch.tensor(pi2, requires_grad=True))\n        with pyro.plate(""particles"", num_particles):\n            z = pyro.sample(""z"", dist.Normal(q2, 1.0).expand_by([num_particles]))\n            zz = torch.exp(z) / (1.0 + torch.exp(z))\n            pyro.sample(""y"", dist.Bernoulli(q1 * zz))\n\n    qs = [\'q1\', \'q2\', \'q3\', \'q4\']\n    results = {}\n\n    for ed, num_particles in zip([None, \'parallel\', \'sequential\'], [30000, 20000, 20000]):\n        pyro.clear_param_store()\n        elbo = TraceEnum_ELBO(strict_enumeration_warning=any([ed]))\n        elbo.loss_and_grads(model, config_enumerate(guide, default=ed), num_particles)\n        results[str(ed)] = {}\n        for q in qs:\n            results[str(ed)][\'actual_grad_%s\' % q] = pyro.param(q).grad.detach().cpu().numpy() / num_particles\n\n    prec = 0.03\n    for ed in [\'parallel\', \'sequential\']:\n        logger.info(\'\\n*** {} ***\'.format(ed))\n        for q in qs:\n            logger.info(""[{}] actual: {}"".format(q, results[ed][\'actual_grad_%s\' % q]))\n            assert_equal(results[ed][\'actual_grad_%s\' % q], results[\'None\'][\'actual_grad_%s\' % q], prec=prec,\n                         msg="""".join([\n                             ""\\nexpected (MC estimate) = {}"".format(results[\'None\'][\'actual_grad_%s\' % q]),\n                             ""\\n  actual ({} estimate) = {}"".format(ed, results[ed][\'actual_grad_%s\' % q]),\n                         ]))\n\n\n@pytest.mark.parametrize(""enumerate1"", [None, ""sequential"", ""parallel""])\ndef test_elbo_rsvi(enumerate1):\n    pyro.clear_param_store()\n    num_particles = 40000\n    prec = 0.01 if enumerate1 else 0.022\n    q = pyro.param(""q"", torch.tensor(0.5, requires_grad=True))\n    a = pyro.param(""a"", torch.tensor(1.5, requires_grad=True))\n    kl1 = kl_divergence(dist.Bernoulli(q), dist.Bernoulli(0.25))\n    kl2 = kl_divergence(dist.Gamma(a, 1.0), dist.Gamma(0.5, 1.0))\n\n    def model():\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""z"", dist.Bernoulli(0.25).expand_by([num_particles]))\n            pyro.sample(""y"", dist.Gamma(0.50, 1.0).expand_by([num_particles]))\n\n    @config_enumerate(default=enumerate1)\n    def guide():\n        q = pyro.param(""q"")\n        a = pyro.param(""a"")\n        with pyro.plate(""particles"", num_particles):\n            pyro.sample(""z"", dist.Bernoulli(q).expand_by([num_particles]))\n            pyro.sample(""y"", ShapeAugmentedGamma(a, torch.tensor(1.0)).expand_by([num_particles]))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=any([enumerate1]))\n    elbo.loss_and_grads(model, guide)\n\n    actual_q = q.grad / num_particles\n    expected_q = grad(kl1, [q])[0]\n    assert_equal(actual_q, expected_q, prec=prec, msg="""".join([\n        ""\\nexpected q.grad = {}"".format(expected_q.detach().cpu().numpy()),\n        ""\\n  actual q.grad = {}"".format(actual_q.detach().cpu().numpy()),\n    ]))\n    actual_a = a.grad / num_particles\n    expected_a = grad(kl2, [a])[0]\n    assert_equal(actual_a, expected_a, prec=prec, msg="""".join([\n        ""\\nexpected a.grad= {}"".format(expected_a.detach().cpu().numpy()),\n        ""\\n  actual a.grad = {}"".format(actual_a.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""enumerate1,num_steps,expand"", [\n    (""sequential"", 2, True),\n    (""sequential"", 2, False),\n    (""sequential"", 3, True),\n    (""sequential"", 3, False),\n    (""parallel"", 2, True),\n    (""parallel"", 2, False),\n    (""parallel"", 3, True),\n    (""parallel"", 3, False),\n    (""parallel"", 10, False),\n    (""parallel"", 20, False),\n    _skip_cuda(""parallel"", 30, False),\n])\ndef test_elbo_hmm_in_model(enumerate1, num_steps, expand):\n    pyro.clear_param_store()\n    data = torch.ones(num_steps)\n    init_probs = torch.tensor([0.5, 0.5])\n\n    def model(data):\n        transition_probs = pyro.param(""transition_probs"",\n                                      torch.tensor([[0.9, 0.1], [0.1, 0.9]]),\n                                      constraint=constraints.simplex)\n        locs = pyro.param(""obs_locs"", torch.tensor([-1.0, 1.0]))\n        scale = pyro.param(""obs_scale"", torch.tensor(1.0),\n                           constraint=constraints.positive)\n\n        x = None\n        for i, y in pyro.markov(enumerate(data)):\n            probs = init_probs if x is None else transition_probs[x]\n            x = pyro.sample(""x_{}"".format(i), dist.Categorical(probs))\n            pyro.sample(""y_{}"".format(i), dist.Normal(locs[x], scale), obs=y)\n\n    @config_enumerate(default=enumerate1, expand=expand)\n    def guide(data):\n        mean_field_probs = pyro.param(""mean_field_probs"", torch.ones(num_steps, 2) / 2,\n                                      constraint=constraints.simplex)\n        for i in pyro.markov(range(num_steps)):\n            pyro.sample(""x_{}"".format(i), dist.Categorical(mean_field_probs[i]))\n\n    elbo = TraceEnum_ELBO()\n    elbo.loss_and_grads(model, guide, data)\n\n    expected_unconstrained_grads = {\n        ""transition_probs"": torch.tensor([[0.2, -0.2], [-0.2, 0.2]]) * (num_steps - 1),\n        ""obs_locs"": torch.tensor([-num_steps, 0]),\n        ""obs_scale"": torch.tensor(-num_steps),\n        ""mean_field_probs"": torch.tensor([[0.5, -0.5]] * num_steps),\n    }\n\n    for name, value in pyro.get_param_store().named_parameters():\n        actual = value.grad\n        expected = expected_unconstrained_grads[name]\n        assert_equal(actual, expected, msg=\'\'.join([\n            \'\\nexpected {}.grad = {}\'.format(name, expected.cpu().numpy()),\n            \'\\n  actual {}.grad = {}\'.format(name, actual.detach().cpu().numpy()),\n        ]))\n\n\n@pytest.mark.parametrize(""enumerate1,num_steps,expand"", [\n    (""sequential"", 2, True),\n    (""sequential"", 2, False),\n    (""sequential"", 3, True),\n    (""sequential"", 3, False),\n    (""parallel"", 2, True),\n    (""parallel"", 2, False),\n    (""parallel"", 3, True),\n    (""parallel"", 3, False),\n    (""parallel"", 10, False),\n    (""parallel"", 20, False),\n    _skip_cuda(""parallel"", 30, False),\n    _skip_cuda(""parallel"", 40, False),\n    _skip_cuda(""parallel"", 50, False),\n])\ndef test_elbo_hmm_in_guide(enumerate1, num_steps, expand):\n    pyro.clear_param_store()\n    data = torch.ones(num_steps)\n    init_probs = torch.tensor([0.5, 0.5])\n\n    def model(data):\n        transition_probs = pyro.param(""transition_probs"",\n                                      torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                      constraint=constraints.simplex)\n        emission_probs = pyro.param(""emission_probs"",\n                                    torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                    constraint=constraints.simplex)\n        x = None\n        for i, y in pyro.markov(enumerate(data)):\n            probs = init_probs if x is None else transition_probs[x]\n            x = pyro.sample(""x_{}"".format(i), dist.Categorical(probs))\n            pyro.sample(""y_{}"".format(i), dist.Categorical(emission_probs[x]), obs=y)\n\n    @config_enumerate(default=enumerate1, expand=expand)\n    def guide(data):\n        transition_probs = pyro.param(""transition_probs"",\n                                      torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                      constraint=constraints.simplex)\n        x = None\n        for i, y in pyro.markov(enumerate(data)):\n            probs = init_probs if x is None else transition_probs[x]\n            x = pyro.sample(""x_{}"".format(i), dist.Categorical(probs))\n\n    elbo = TraceEnum_ELBO()\n    elbo.loss_and_grads(model, guide, data)\n\n    # These golden values simply test agreement between parallel and sequential.\n    expected_grads = {\n        2: {\n            ""transition_probs"": [[0.1029949, -0.1029949], [0.1029949, -0.1029949]],\n            ""emission_probs"": [[0.75, -0.75], [0.25, -0.25]],\n        },\n        3: {\n            ""transition_probs"": [[0.25748726, -0.25748726], [0.25748726, -0.25748726]],\n            ""emission_probs"": [[1.125, -1.125], [0.375, -0.375]],\n        },\n        10: {\n            ""transition_probs"": [[1.64832076, -1.64832076], [1.64832076, -1.64832076]],\n            ""emission_probs"": [[3.75, -3.75], [1.25, -1.25]],\n        },\n        20: {\n            ""transition_probs"": [[3.70781687, -3.70781687], [3.70781687, -3.70781687]],\n            ""emission_probs"": [[7.5, -7.5], [2.5, -2.5]],\n        },\n        22: {\n            ""transition_probs"": [[4.11979618, -4.11979618], [4.11979618, -4.11979618]],\n            ""emission_probs"": [[8.25, -8.25], [2.75, -2.75]],\n        },\n        30: {\n            ""transition_probs"": [[5.76771452, -5.76771452], [5.76771452, -5.76771452]],\n            ""emission_probs"": [[11.25, -11.25], [3.75, -3.75]],\n        },\n    }\n\n    if num_steps not in expected_grads:\n        return\n    for name, value in pyro.get_param_store().named_parameters():\n        actual = value.grad\n        expected = torch.tensor(expected_grads[num_steps][name])\n        assert_equal(actual, expected, msg=\'\'.join([\n            \'\\nexpected {}.grad = {}\'.format(name, expected.cpu().numpy()),\n            \'\\n  actual {}.grad = {}\'.format(name, actual.detach().cpu().numpy()),\n        ]))\n\n\n@pytest.mark.parametrize(\'num_steps\', [2, 3, 4, 5, 10, 20, _skip_cuda(30)])\ndef test_hmm_enumerate_model(num_steps):\n    data = dist.Categorical(torch.tensor([0.5, 0.5])).sample((num_steps,))\n\n    @config_enumerate\n    def model(data):\n        transition_probs = pyro.param(""transition_probs"",\n                                      torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                      constraint=constraints.simplex)\n        emission_probs = pyro.param(""emission_probs"",\n                                    torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                    constraint=constraints.simplex)\n        x = 0\n        for t, y in pyro.markov(enumerate(data)):\n            x = pyro.sample(""x_{}"".format(t), dist.Categorical(transition_probs[x]))\n            pyro.sample(""y_{}"".format(t), dist.Categorical(emission_probs[x]), obs=y)\n            logger.debug(\'{}\\t{}\'.format(t, tuple(x.shape)))\n\n    def guide(data):\n        pass\n\n    elbo = TraceEnum_ELBO()\n    elbo.differentiable_loss(model, guide, data)\n\n\n@pytest.mark.parametrize(\'num_steps\', [2, 3, 4, 5, 10, 20, _skip_cuda(30)])\ndef test_hmm_enumerate_model_and_guide(num_steps):\n    data = dist.Categorical(torch.tensor([0.5, 0.5])).sample((num_steps,))\n\n    def model(data):\n        transition_probs = pyro.param(""transition_probs"",\n                                      torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                      constraint=constraints.simplex)\n        emission_probs = pyro.param(""emission_probs"",\n                                    torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                    constraint=constraints.simplex)\n        x = pyro.sample(""x"", dist.Categorical(torch.tensor([0.5, 0.5])))\n        logger.debug(\'-1\\t{}\'.format(tuple(x.shape)))\n        for t, y in pyro.markov(enumerate(data)):\n            x = pyro.sample(""x_{}"".format(t), dist.Categorical(transition_probs[x]),\n                            infer={""enumerate"": ""parallel""})\n            pyro.sample(""y_{}"".format(t), dist.Categorical(emission_probs[x]), obs=y)\n            logger.debug(\'{}\\t{}\'.format(t, tuple(x.shape)))\n\n    def guide(data):\n        init_probs = pyro.param(""init_probs"",\n                                torch.tensor([0.75, 0.25]),\n                                constraint=constraints.simplex)\n        pyro.sample(""x"", dist.Categorical(init_probs),\n                    infer={""enumerate"": ""parallel""})\n\n    elbo = TraceEnum_ELBO()\n    elbo.differentiable_loss(model, guide, data)\n\n\ndef _check_loss_and_grads(expected_loss, actual_loss):\n    assert_equal(actual_loss, expected_loss,\n                 msg=\'Expected:\\n{}\\nActual:\\n{}\'.format(expected_loss.detach().cpu().numpy(),\n                                                         actual_loss.detach().cpu().numpy()))\n\n    names = pyro.get_param_store().keys()\n    params = [pyro.param(name).unconstrained() for name in names]\n    actual_grads = grad(actual_loss, params, allow_unused=True, retain_graph=True)\n    expected_grads = grad(expected_loss, params, allow_unused=True, retain_graph=True)\n    for name, actual_grad, expected_grad in zip(names, actual_grads, expected_grads):\n        if actual_grad is None or expected_grad is None:\n            continue\n        assert_equal(actual_grad, expected_grad,\n                     msg=\'{}\\nExpected:\\n{}\\nActual:\\n{}\'.format(name,\n                                                                 expected_grad.detach().cpu().numpy(),\n                                                                 actual_grad.detach().cpu().numpy()))\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_1(scale):\n    pyro.param(""guide_probs_x"",\n               torch.tensor([0.1, 0.9]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_x"",\n               torch.tensor([0.4, 0.6]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_y"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_z"",\n               torch.tensor([0.3, 0.7]),\n               constraint=constraints.simplex)\n\n    @poutine.scale(scale=scale)\n    def auto_model():\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                    infer={""enumerate"": ""parallel""})\n        pyro.sample(""z"", dist.Categorical(probs_z), obs=torch.tensor(0))\n\n    @poutine.scale(scale=scale)\n    def hand_model():\n        probs_x = pyro.param(""model_probs_x"")\n        probs_z = pyro.param(""model_probs_z"")\n        pyro.sample(""x"", dist.Categorical(probs_x))\n        pyro.sample(""z"", dist.Categorical(probs_z), obs=torch.tensor(0))\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def guide():\n        probs_x = pyro.param(""guide_probs_x"")\n        pyro.sample(""x"", dist.Categorical(probs_x))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=False)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_2(scale):\n    pyro.param(""guide_probs_x"",\n               torch.tensor([0.1, 0.9]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_x"",\n               torch.tensor([0.4, 0.6]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_y"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_z"",\n               torch.tensor([[0.3, 0.7], [0.2, 0.8]]),\n               constraint=constraints.simplex)\n\n    @poutine.scale(scale=scale)\n    def auto_model():\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        y = pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                        infer={""enumerate"": ""parallel""})\n        pyro.sample(""z"", dist.Categorical(probs_z[y]), obs=torch.tensor(0))\n\n    @poutine.scale(scale=scale)\n    def hand_model():\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        probs_yz = probs_y.mm(probs_z)\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        pyro.sample(""z"", dist.Categorical(probs_yz[x]), obs=torch.tensor(0))\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def guide():\n        probs_x = pyro.param(""guide_probs_x"")\n        pyro.sample(""x"", dist.Categorical(probs_x))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=False)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_3(scale):\n    pyro.param(""guide_probs_x"",\n               torch.tensor([0.1, 0.9]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_x"",\n               torch.tensor([0.4, 0.6]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_y"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_z"",\n               torch.tensor([[0.3, 0.7], [0.2, 0.8]]),\n               constraint=constraints.simplex)\n\n    def auto_model():\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        with poutine.scale(scale=scale):\n            y = pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                            infer={""enumerate"": ""parallel""})\n            pyro.sample(""z"", dist.Categorical(probs_z[y]), obs=torch.tensor(0))\n\n    def hand_model():\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        probs_yz = probs_y.mm(probs_z)\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        with poutine.scale(scale=scale):\n            pyro.sample(""z"", dist.Categorical(probs_yz[x]), obs=torch.tensor(0))\n\n    @config_enumerate\n    def guide():\n        probs_x = pyro.param(""guide_probs_x"")\n        pyro.sample(""x"", dist.Categorical(probs_x))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=False)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\n@pytest.mark.parametrize(\'num_samples,num_masked\',\n                         [(1, 1), (2, 2), (3, 2)],\n                         ids=[""single"", ""batch"", ""masked""])\ndef test_elbo_enumerate_plate_1(num_samples, num_masked, scale):\n    #              +---------+\n    #  x ----> y ----> z     |\n    #              |       N |\n    #              +---------+\n    pyro.param(""guide_probs_x"",\n               torch.tensor([0.1, 0.9]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_x"",\n               torch.tensor([0.4, 0.6]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_y"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_z"",\n               torch.tensor([[0.3, 0.7], [0.2, 0.8]]),\n               constraint=constraints.simplex)\n\n    def auto_model(data):\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        with poutine.scale(scale=scale):\n            y = pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                            infer={""enumerate"": ""parallel""})\n            if num_masked == num_samples:\n                with pyro.plate(""data"", len(data)):\n                    pyro.sample(""z"", dist.Categorical(probs_z[y]), obs=data)\n            else:\n                with pyro.plate(""data"", len(data)):\n                    with poutine.mask(mask=torch.arange(num_samples) < num_masked):\n                        pyro.sample(""z"", dist.Categorical(probs_z[y]), obs=data)\n\n    def hand_model(data):\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        with poutine.scale(scale=scale):\n            y = pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                            infer={""enumerate"": ""parallel""})\n            for i in pyro.plate(""data"", num_masked):\n                pyro.sample(""z_{}"".format(i), dist.Categorical(probs_z[y]), obs=data[i])\n\n    @config_enumerate\n    def guide(data):\n        probs_x = pyro.param(""guide_probs_x"")\n        pyro.sample(""x"", dist.Categorical(probs_x))\n\n    data = dist.Categorical(torch.tensor([0.3, 0.7])).sample((num_samples,))\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    auto_loss = elbo.differentiable_loss(auto_model, guide, data)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    hand_loss = elbo.differentiable_loss(hand_model, guide, data)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\n@pytest.mark.parametrize(\'num_samples,num_masked\',\n                         [(1, 1), (2, 2), (3, 2)],\n                         ids=[""single"", ""batch"", ""masked""])\ndef test_elbo_enumerate_plate_2(num_samples, num_masked, scale):\n    #      +-----------------+\n    #  x ----> y ----> z     |\n    #      |               N |\n    #      +-----------------+\n    pyro.param(""guide_probs_x"",\n               torch.tensor([0.1, 0.9]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_x"",\n               torch.tensor([0.4, 0.6]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_y"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_z"",\n               torch.tensor([[0.3, 0.7], [0.2, 0.8]]),\n               constraint=constraints.simplex)\n\n    def auto_model(data):\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        with poutine.scale(scale=scale):\n            with pyro.plate(""data"", len(data)):\n                if num_masked == num_samples:\n                    y = pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                                    infer={""enumerate"": ""parallel""})\n                    pyro.sample(""z"", dist.Categorical(probs_z[y]), obs=data)\n                else:\n                    with poutine.mask(mask=torch.arange(num_samples) < num_masked):\n                        y = pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                                        infer={""enumerate"": ""parallel""})\n                        pyro.sample(""z"", dist.Categorical(probs_z[y]), obs=data)\n\n    def hand_model(data):\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        x = pyro.sample(""x"", dist.Categorical(probs_x))\n        with poutine.scale(scale=scale):\n            for i in pyro.plate(""data"", num_masked):\n                y = pyro.sample(""y_{}"".format(i), dist.Categorical(probs_y[x]),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""z_{}"".format(i), dist.Categorical(probs_z[y]), obs=data[i])\n\n    @config_enumerate\n    def guide(data):\n        probs_x = pyro.param(""guide_probs_x"")\n        pyro.sample(""x"", dist.Categorical(probs_x))\n\n    data = dist.Categorical(torch.tensor([0.3, 0.7])).sample((num_samples,))\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    auto_loss = elbo.differentiable_loss(auto_model, guide, data)\n    hand_loss = elbo.differentiable_loss(hand_model, guide, data)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\n@pytest.mark.parametrize(\'num_samples,num_masked\',\n                         [(1, 1), (2, 2), (3, 2)],\n                         ids=[""single"", ""batch"", ""masked""])\ndef test_elbo_enumerate_plate_3(num_samples, num_masked, scale):\n    #  +-----------------------+\n    #  | x ----> y ----> z     |\n    #  |                     N |\n    #  +-----------------------+\n    # This plate should remain unreduced since all enumeration is in a single plate.\n    pyro.param(""guide_probs_x"",\n               torch.tensor([0.1, 0.9]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_x"",\n               torch.tensor([0.4, 0.6]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_y"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_z"",\n               torch.tensor([[0.3, 0.7], [0.2, 0.8]]),\n               constraint=constraints.simplex)\n\n    @poutine.scale(scale=scale)\n    def auto_model(data):\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        with pyro.plate(""data"", len(data)):\n            if num_masked == num_samples:\n                x = pyro.sample(""x"", dist.Categorical(probs_x))\n                y = pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""z"", dist.Categorical(probs_z[y]), obs=data)\n            else:\n                with poutine.mask(mask=torch.arange(num_samples) < num_masked):\n                    x = pyro.sample(""x"", dist.Categorical(probs_x))\n                    y = pyro.sample(""y"", dist.Categorical(probs_y[x]),\n                                    infer={""enumerate"": ""parallel""})\n                    pyro.sample(""z"", dist.Categorical(probs_z[y]), obs=data)\n\n    @poutine.scale(scale=scale)\n    @config_enumerate\n    def auto_guide(data):\n        probs_x = pyro.param(""guide_probs_x"")\n        with pyro.plate(""data"", len(data)):\n            if num_masked == num_samples:\n                pyro.sample(""x"", dist.Categorical(probs_x))\n            else:\n                with poutine.mask(mask=torch.arange(num_samples) < num_masked):\n                    pyro.sample(""x"", dist.Categorical(probs_x))\n\n    @poutine.scale(scale=scale)\n    def hand_model(data):\n        probs_x = pyro.param(""model_probs_x"")\n        probs_y = pyro.param(""model_probs_y"")\n        probs_z = pyro.param(""model_probs_z"")\n        for i in pyro.plate(""data"", num_masked):\n            x = pyro.sample(""x_{}"".format(i), dist.Categorical(probs_x))\n            y = pyro.sample(""y_{}"".format(i), dist.Categorical(probs_y[x]),\n                            infer={""enumerate"": ""parallel""})\n            pyro.sample(""z_{}"".format(i), dist.Categorical(probs_z[y]), obs=data[i])\n\n    @poutine.scale(scale=scale)\n    @config_enumerate\n    def hand_guide(data):\n        probs_x = pyro.param(""guide_probs_x"")\n        for i in pyro.plate(""data"", num_masked):\n            pyro.sample(""x_{}"".format(i), dist.Categorical(probs_x))\n\n    data = dist.Categorical(torch.tensor([0.3, 0.7])).sample((num_samples,))\n    elbo = TraceEnum_ELBO(max_plate_nesting=1, strict_enumeration_warning=False)\n    auto_loss = elbo.differentiable_loss(auto_model, auto_guide, data)\n    hand_loss = elbo.differentiable_loss(hand_model, hand_guide, data)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\n@pytest.mark.parametrize(\'outer_obs,inner_obs\',\n                         [(False, True), (True, False), (True, True)])\ndef test_elbo_enumerate_plate_4(outer_obs, inner_obs, scale):\n    #    a ---> outer_obs\n    #      \\\n    #  +-----\\------------------+\n    #  |       \\                |\n    #  | b ---> inner_obs   N=2 |\n    #  +------------------------+\n    # This tests two different observations, one outside and one inside an plate.\n    pyro.param(""probs_a"", torch.tensor([0.4, 0.6]), constraint=constraints.simplex)\n    pyro.param(""probs_b"", torch.tensor([0.6, 0.4]), constraint=constraints.simplex)\n    pyro.param(""locs"", torch.tensor([-1., 1.]))\n    pyro.param(""scales"", torch.tensor([1., 2.]), constraint=constraints.positive)\n    outer_data = torch.tensor(2.0)\n    inner_data = torch.tensor([0.5, 1.5])\n\n    @poutine.scale(scale=scale)\n    def auto_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        locs = pyro.param(""locs"")\n        scales = pyro.param(""scales"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a),\n                        infer={""enumerate"": ""parallel""})\n        if outer_obs:\n            pyro.sample(""outer_obs"", dist.Normal(0., scales[a]),\n                        obs=outer_data)\n        with pyro.plate(""inner"", 2):\n            b = pyro.sample(""b"", dist.Categorical(probs_b),\n                            infer={""enumerate"": ""parallel""})\n            if inner_obs:\n                pyro.sample(""inner_obs"", dist.Normal(locs[b], scales[a]),\n                            obs=inner_data)\n\n    @poutine.scale(scale=scale)\n    def hand_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        locs = pyro.param(""locs"")\n        scales = pyro.param(""scales"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a),\n                        infer={""enumerate"": ""parallel""})\n        if outer_obs:\n            pyro.sample(""outer_obs"", dist.Normal(0., scales[a]),\n                        obs=outer_data)\n        for i in pyro.plate(""inner"", 2):\n            b = pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b),\n                            infer={""enumerate"": ""parallel""})\n            if inner_obs:\n                pyro.sample(""inner_obs_{}"".format(i), dist.Normal(locs[b], scales[a]),\n                            obs=inner_data[i])\n\n    def guide():\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\ndef test_elbo_enumerate_plate_5():\n    #        Guide   Model\n    #                  a\n    #  +---------------|--+\n    #  | M=2           V  |\n    #  |       b ----> c  |\n    #  +------------------+\n    pyro.param(""model_probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_b"",\n               torch.tensor([0.6, 0.4]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_c"",\n               torch.tensor([[[0.4, 0.5, 0.1], [0.3, 0.5, 0.2]],\n                             [[0.3, 0.4, 0.3], [0.4, 0.4, 0.2]]]),\n               constraint=constraints.simplex)\n    pyro.param(""guide_probs_b"",\n               torch.tensor([0.8, 0.2]),\n               constraint=constraints.simplex)\n    data = torch.tensor([1, 2])\n\n    @config_enumerate\n    def model_plate():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with pyro.plate(""b_axis"", 2):\n            b = pyro.sample(""b"", dist.Categorical(probs_b))\n            pyro.sample(""c"", dist.Categorical(Vindex(probs_c)[a, b]),\n                        obs=data)\n\n    @config_enumerate\n    def guide_plate():\n        probs_b = pyro.param(""guide_probs_b"")\n        with pyro.plate(""b_axis"", 2):\n            pyro.sample(""b"", dist.Categorical(probs_b))\n\n    @config_enumerate\n    def model_iplate():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        for i in pyro.plate(""b_axis"", 2):\n            b = pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b))\n            pyro.sample(""c_{}"".format(i),\n                        dist.Categorical(Vindex(probs_c)[a, b]),\n                        obs=data[i])\n\n    @config_enumerate\n    def guide_iplate():\n        probs_b = pyro.param(""guide_probs_b"")\n        for i in pyro.plate(""b_axis"", 2):\n            pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b))\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    expected_loss = elbo.differentiable_loss(model_iplate, guide_iplate)\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    with pytest.raises(ValueError, match=""Expected model enumeration to be no more global than guide""):\n        actual_loss = elbo.differentiable_loss(model_plate, guide_plate)\n        # This never gets run because we don\'t support this yet.\n        _check_loss_and_grads(expected_loss, actual_loss)\n\n\n@pytest.mark.parametrize(\'enumerate1\', [\'parallel\', \'sequential\'])\ndef test_elbo_enumerate_plate_6(enumerate1):\n    #     Guide           Model\n    #           +-------+\n    #       b ----> c <---- a\n    #           |  M=2  |\n    #           +-------+\n    # This tests that sequential enumeration over b works, even though\n    # model-side enumeration moves c into b\'s plate via contraction.\n    pyro.param(""model_probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_b"",\n               torch.tensor([0.6, 0.4]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_c"",\n               torch.tensor([[[0.4, 0.5, 0.1], [0.3, 0.5, 0.2]],\n                             [[0.3, 0.4, 0.3], [0.4, 0.4, 0.2]]]),\n               constraint=constraints.simplex)\n    pyro.param(""guide_probs_b"",\n               torch.tensor([0.8, 0.2]),\n               constraint=constraints.simplex)\n    data = torch.tensor([1, 2])\n\n    @config_enumerate\n    def model_plate():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        b = pyro.sample(""b"", dist.Categorical(probs_b))\n        with pyro.plate(""b_axis"", 2):\n            pyro.sample(""c"", dist.Categorical(Vindex(probs_c)[a, b]),\n                        obs=data)\n\n    @config_enumerate\n    def model_iplate():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        b = pyro.sample(""b"", dist.Categorical(probs_b))\n        for i in pyro.plate(""b_axis"", 2):\n            pyro.sample(""c_{}"".format(i),\n                        dist.Categorical(Vindex(probs_c)[a, b]),\n                        obs=data[i])\n\n    @config_enumerate(default=enumerate1)\n    def guide():\n        probs_b = pyro.param(""guide_probs_b"")\n        pyro.sample(""b"", dist.Categorical(probs_b))\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    expected_loss = elbo.differentiable_loss(model_iplate, guide)\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    actual_loss = elbo.differentiable_loss(model_plate, guide)\n    _check_loss_and_grads(expected_loss, actual_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_plate_7(scale):\n    #  Guide    Model\n    #    a -----> b\n    #    |        |\n    #  +-|--------|----------------+\n    #  | V        V                |\n    #  | c -----> d -----> e   N=2 |\n    #  +---------------------------+\n    # This tests a mixture of model and guide enumeration.\n    pyro.param(""model_probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_b"",\n               torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_c"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_d"",\n               torch.tensor([[[0.4, 0.6], [0.3, 0.7]], [[0.3, 0.7], [0.2, 0.8]]]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_e"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""guide_probs_a"",\n               torch.tensor([0.35, 0.64]),\n               constraint=constraints.simplex)\n    pyro.param(""guide_probs_c"",\n               torch.tensor([[0., 1.], [1., 0.]]),  # deterministic\n               constraint=constraints.simplex)\n\n    @poutine.scale(scale=scale)\n    def auto_model(data):\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        probs_d = pyro.param(""model_probs_d"")\n        probs_e = pyro.param(""model_probs_e"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        b = pyro.sample(""b"", dist.Categorical(probs_b[a]),\n                        infer={""enumerate"": ""parallel""})\n        with pyro.plate(""data"", 2):\n            c = pyro.sample(""c"", dist.Categorical(probs_c[a]))\n            d = pyro.sample(""d"", dist.Categorical(Vindex(probs_d)[b, c]),\n                            infer={""enumerate"": ""parallel""})\n            pyro.sample(""obs"", dist.Categorical(probs_e[d]), obs=data)\n\n    @poutine.scale(scale=scale)\n    def auto_guide(data):\n        probs_a = pyro.param(""guide_probs_a"")\n        probs_c = pyro.param(""guide_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a),\n                        infer={""enumerate"": ""parallel""})\n        with pyro.plate(""data"", 2):\n            pyro.sample(""c"", dist.Categorical(probs_c[a]))\n\n    @poutine.scale(scale=scale)\n    def hand_model(data):\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        probs_d = pyro.param(""model_probs_d"")\n        probs_e = pyro.param(""model_probs_e"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        b = pyro.sample(""b"", dist.Categorical(probs_b[a]),\n                        infer={""enumerate"": ""parallel""})\n        for i in pyro.plate(""data"", 2):\n            c = pyro.sample(""c_{}"".format(i), dist.Categorical(probs_c[a]))\n            d = pyro.sample(""d_{}"".format(i),\n                            dist.Categorical(Vindex(probs_d)[b, c]),\n                            infer={""enumerate"": ""parallel""})\n            pyro.sample(""obs_{}"".format(i), dist.Categorical(probs_e[d]), obs=data[i])\n\n    @poutine.scale(scale=scale)\n    def hand_guide(data):\n        probs_a = pyro.param(""guide_probs_a"")\n        probs_c = pyro.param(""guide_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a),\n                        infer={""enumerate"": ""parallel""})\n        for i in pyro.plate(""data"", 2):\n            pyro.sample(""c_{}"".format(i), dist.Categorical(probs_c[a]))\n\n    data = torch.tensor([0, 0])\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    auto_loss = elbo.differentiable_loss(auto_model, auto_guide, data)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    hand_loss = elbo.differentiable_loss(hand_model, hand_guide, data)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_plates_1(scale):\n    #  +-----------------+\n    #  | a ----> b   M=2 |\n    #  +-----------------+\n    #  +-----------------+\n    #  | c ----> d   N=3 |\n    #  +-----------------+\n    # This tests two unrelated plates.\n    # Each should remain uncontracted.\n    pyro.param(""probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_b"",\n               torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_c"",\n               torch.tensor([0.75, 0.25]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_d"",\n               torch.tensor([[0.4, 0.6], [0.3, 0.7]]),\n               constraint=constraints.simplex)\n    b_data = torch.tensor([0, 1])\n    d_data = torch.tensor([0, 0, 1])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def auto_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        with pyro.plate(""a_axis"", 2):\n            a = pyro.sample(""a"", dist.Categorical(probs_a))\n            pyro.sample(""b"", dist.Categorical(probs_b[a]), obs=b_data)\n        with pyro.plate(""c_axis"", 3):\n            c = pyro.sample(""c"", dist.Categorical(probs_c))\n            pyro.sample(""d"", dist.Categorical(probs_d[c]), obs=d_data)\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def hand_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        for i in pyro.plate(""a_axis"", 2):\n            a = pyro.sample(""a_{}"".format(i), dist.Categorical(probs_a))\n            pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a]), obs=b_data[i])\n        for j in pyro.plate(""c_axis"", 3):\n            c = pyro.sample(""c_{}"".format(j), dist.Categorical(probs_c))\n            pyro.sample(""d_{}"".format(j), dist.Categorical(probs_d[c]), obs=d_data[j])\n\n    def guide():\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_plates_2(scale):\n    #  +---------+       +---------+\n    #  |     b <---- a ----> c     |\n    #  | M=2     |       |     N=3 |\n    #  +---------+       +---------+\n    # This tests two different plates with recycled dimension.\n    pyro.param(""probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_b"",\n               torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_c"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    b_data = torch.tensor([0, 1])\n    c_data = torch.tensor([0, 0, 1])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def auto_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with pyro.plate(""b_axis"", 2):\n            pyro.sample(""b"", dist.Categorical(probs_b[a]),\n                        obs=b_data)\n        with pyro.plate(""c_axis"", 3):\n            pyro.sample(""c"", dist.Categorical(probs_c[a]),\n                        obs=c_data)\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def hand_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        for i in pyro.plate(""b_axis"", 2):\n            pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a]),\n                        obs=b_data[i])\n        for j in pyro.plate(""c_axis"", 3):\n            pyro.sample(""c_{}"".format(j), dist.Categorical(probs_c[a]),\n                        obs=c_data[j])\n\n    def guide():\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_plates_3(scale):\n    #      +--------------------+\n    #      |  +----------+      |\n    #  a -------> b      |      |\n    #      |  |      N=2 |      |\n    #      |  +----------+  M=2 |\n    #      +--------------------+\n    # This is tests the case of multiple plate contractions in\n    # a single step.\n    pyro.param(""probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_b"",\n               torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n               constraint=constraints.simplex)\n    data = torch.tensor([[0, 1], [0, 0]])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def auto_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with pyro.plate(""outer"", 2):\n            with pyro.plate(""inner"", 2):\n                pyro.sample(""b"", dist.Categorical(probs_b[a]),\n                            obs=data)\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def hand_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        inner = pyro.plate(""inner"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        for i in pyro.plate(""outer"", 2):\n            for j in inner:\n                pyro.sample(""b_{}_{}"".format(i, j), dist.Categorical(probs_b[a]),\n                            obs=data[i, j])\n\n    def guide():\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_plates_4(scale):\n    #      +--------------------+\n    #      |       +----------+ |\n    #  a ----> b ----> c      | |\n    #      |       |      N=2 | |\n    #      | M=2   +----------+ |\n    #      +--------------------+\n    pyro.param(""probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_b"",\n               torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_c"",\n               torch.tensor([[0.4, 0.6], [0.3, 0.7]]),\n               constraint=constraints.simplex)\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def auto_model(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with pyro.plate(""outer"", 2):\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]))\n            with pyro.plate(""inner"", 2):\n                pyro.sample(""c"", dist.Categorical(probs_c[b]), obs=data)\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def hand_model(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        inner = pyro.plate(""inner"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        for i in pyro.plate(""outer"", 2):\n            b = pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a]))\n            for j in inner:\n                pyro.sample(""c_{}_{}"".format(i, j), dist.Categorical(probs_c[b]),\n                            obs=data[i, j])\n\n    def guide(data):\n        pass\n\n    data = torch.tensor([[0, 1], [0, 0]])\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    auto_loss = elbo.differentiable_loss(auto_model, guide, data)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    hand_loss = elbo.differentiable_loss(hand_model, guide, data)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_plates_5(scale):\n    #     a\n    #     | \\\n    #  +--|---\\------------+\n    #  |  V   +-\\--------+ |\n    #  |  b ----> c      | |\n    #  |      |      N=2 | |\n    #  | M=2  +----------+ |\n    #  +-------------------+\n    pyro.param(""probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_b"",\n               torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_c"",\n               torch.tensor([[[0.4, 0.6], [0.3, 0.7]],\n                             [[0.2, 0.8], [0.1, 0.9]]]),\n               constraint=constraints.simplex)\n    data = torch.tensor([[0, 1], [0, 0]])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def auto_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with pyro.plate(""outer"", 2):\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]))\n            with pyro.plate(""inner"", 2):\n                pyro.sample(""c"", dist.Categorical(Vindex(probs_c)[a, b]),\n                            obs=data)\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def hand_model():\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        inner = pyro.plate(""inner"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        for i in pyro.plate(""outer"", 2):\n            b = pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a]))\n            for j in inner:\n                pyro.sample(""c_{}_{}"".format(i, j),\n                            dist.Categorical(Vindex(probs_c)[a, b]),\n                            obs=data[i, j])\n\n    def guide():\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_plates_6(scale):\n    #         +----------+\n    #         |      M=2 |\n    #     a ----> b      |\n    #     |   |   |      |\n    #  +--|-------|--+   |\n    #  |  V   |   V  |   |\n    #  |  c ----> d  |   |\n    #  |      |      |   |\n    #  | N=2  +------|---+\n    #  +-------------+\n    # This tests different ways of mixing two independence contexts,\n    # where each can be either sequential or vectorized plate.\n    pyro.param(""probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_b"",\n               torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_c"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_d"",\n               torch.tensor([[[0.4, 0.6], [0.3, 0.7]], [[0.3, 0.7], [0.2, 0.8]]]),\n               constraint=constraints.simplex)\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def model_iplate_iplate(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        b_axis = pyro.plate(""b_axis"", 2)\n        c_axis = pyro.plate(""c_axis"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        b = [pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a])) for i in b_axis]\n        c = [pyro.sample(""c_{}"".format(j), dist.Categorical(probs_c[a])) for j in c_axis]\n        for i in b_axis:\n            for j in c_axis:\n                pyro.sample(""d_{}_{}"".format(i, j),\n                            dist.Categorical(Vindex(probs_d)[b[i], c[j]]),\n                            obs=data[i, j])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def model_iplate_plate(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        b_axis = pyro.plate(""b_axis"", 2)\n        c_axis = pyro.plate(""c_axis"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        b = [pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a])) for i in b_axis]\n        with c_axis:\n            c = pyro.sample(""c"", dist.Categorical(probs_c[a]))\n        for i in b_axis:\n            with c_axis:\n                pyro.sample(""d_{}"".format(i),\n                            dist.Categorical(Vindex(probs_d)[b[i], c]),\n                            obs=data[i])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def model_plate_iplate(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        b_axis = pyro.plate(""b_axis"", 2)\n        c_axis = pyro.plate(""c_axis"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with b_axis:\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]))\n        c = [pyro.sample(""c_{}"".format(j), dist.Categorical(probs_c[a])) for j in c_axis]\n        with b_axis:\n            for j in c_axis:\n                pyro.sample(""d_{}"".format(j),\n                            dist.Categorical(Vindex(probs_d)[b, c[j]]),\n                            obs=data[:, j])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def model_plate_plate(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        b_axis = pyro.plate(""b_axis"", 2, dim=-1)\n        c_axis = pyro.plate(""c_axis"", 2, dim=-2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with b_axis:\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]))\n        with c_axis:\n            c = pyro.sample(""c"", dist.Categorical(probs_c[a]))\n        with b_axis, c_axis:\n            pyro.sample(""d"",\n                        dist.Categorical(Vindex(probs_d)[b, c]),\n                        obs=data)\n\n    def guide(data):\n        pass\n\n    # Check that either one of the sequential plates can be promoted to be vectorized.\n    data = torch.tensor([[0, 1], [0, 0]])\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    loss_iplate_iplate = elbo.differentiable_loss(model_iplate_iplate, guide, data)\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    loss_plate_iplate = elbo.differentiable_loss(model_plate_iplate, guide, data)\n    loss_iplate_plate = elbo.differentiable_loss(model_iplate_plate, guide, data)\n    _check_loss_and_grads(loss_iplate_iplate, loss_plate_iplate)\n    _check_loss_and_grads(loss_iplate_iplate, loss_iplate_plate)\n\n    # But promoting both to plates should result in an error.\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    with pytest.raises(NotImplementedError, match=""Expected tree-structured plate nesting.*""):\n        elbo.differentiable_loss(model_plate_plate, guide, data)\n\n\n@pytest.mark.parametrize(\'scale\', [1, 10])\ndef test_elbo_enumerate_plates_7(scale):\n    #         +-------------+\n    #         |         N=2 |\n    #     a -------> c      |\n    #     |   |      |      |\n    #  +--|----------|--+   |\n    #  |  |   |      V  |   |\n    #  |  V   |      e  |   |\n    #  |  b ----> d     |   |\n    #  |      |         |   |\n    #  | M=2  +---------|---+\n    #  +----------------+\n    # This tests tree-structured dependencies among variables but\n    # non-tree dependencies among plate nestings.\n    pyro.param(""probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_b"",\n               torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_c"",\n               torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_d"",\n               torch.tensor([[0.3, 0.7], [0.2, 0.8]]),\n               constraint=constraints.simplex)\n    pyro.param(""probs_e"",\n               torch.tensor([[0.4, 0.6], [0.3, 0.7]]),\n               constraint=constraints.simplex)\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def model_iplate_iplate(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        probs_e = pyro.param(""probs_e"")\n        b_axis = pyro.plate(""b_axis"", 2)\n        c_axis = pyro.plate(""c_axis"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        b = [pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a])) for i in b_axis]\n        c = [pyro.sample(""c_{}"".format(j), dist.Categorical(probs_c[a])) for j in c_axis]\n        for i in b_axis:\n            for j in c_axis:\n                pyro.sample(""d_{}_{}"".format(i, j), dist.Categorical(probs_d[b[i]]),\n                            obs=data[i, j])\n                pyro.sample(""e_{}_{}"".format(i, j), dist.Categorical(probs_e[c[j]]),\n                            obs=data[i, j])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def model_iplate_plate(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        probs_e = pyro.param(""probs_e"")\n        b_axis = pyro.plate(""b_axis"", 2)\n        c_axis = pyro.plate(""c_axis"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        b = [pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a])) for i in b_axis]\n        with c_axis:\n            c = pyro.sample(""c"", dist.Categorical(probs_c[a]))\n        for i in b_axis:\n            with c_axis:\n                pyro.sample(""d_{}"".format(i), dist.Categorical(probs_d[b[i]]),\n                            obs=data[i])\n                pyro.sample(""e_{}"".format(i), dist.Categorical(probs_e[c]),\n                            obs=data[i])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def model_plate_iplate(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        probs_e = pyro.param(""probs_e"")\n        b_axis = pyro.plate(""b_axis"", 2)\n        c_axis = pyro.plate(""c_axis"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with b_axis:\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]))\n        c = [pyro.sample(""c_{}"".format(j), dist.Categorical(probs_c[a])) for j in c_axis]\n        with b_axis:\n            for j in c_axis:\n                pyro.sample(""d_{}"".format(j), dist.Categorical(probs_d[b]),\n                            obs=data[:, j])\n                pyro.sample(""e_{}"".format(j), dist.Categorical(probs_e[c[j]]),\n                            obs=data[:, j])\n\n    @config_enumerate\n    @poutine.scale(scale=scale)\n    def model_plate_plate(data):\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        probs_e = pyro.param(""probs_e"")\n        b_axis = pyro.plate(""b_axis"", 2, dim=-1)\n        c_axis = pyro.plate(""c_axis"", 2, dim=-2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with b_axis:\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]))\n        with c_axis:\n            c = pyro.sample(""c"", dist.Categorical(probs_c[a]))\n        with b_axis, c_axis:\n            pyro.sample(""d"", dist.Categorical(probs_d[b]), obs=data)\n            pyro.sample(""e"", dist.Categorical(probs_e[c]), obs=data)\n\n    def guide(data):\n        pass\n\n    # Check that any combination of sequential plates can be promoted to be vectorized.\n    data = torch.tensor([[0, 1], [0, 0]])\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    loss_iplate_iplate = elbo.differentiable_loss(model_iplate_iplate, guide, data)\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    loss_plate_iplate = elbo.differentiable_loss(model_plate_iplate, guide, data)\n    loss_iplate_plate = elbo.differentiable_loss(model_iplate_plate, guide, data)\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    loss_plate_plate = elbo.differentiable_loss(model_plate_plate, guide, data)\n    _check_loss_and_grads(loss_iplate_iplate, loss_plate_iplate)\n    _check_loss_and_grads(loss_iplate_iplate, loss_iplate_plate)\n    _check_loss_and_grads(loss_iplate_iplate, loss_plate_plate)\n\n\n@pytest.mark.parametrize(\'guide_scale\', [1])\n@pytest.mark.parametrize(\'model_scale\', [1])\n@pytest.mark.parametrize(\'outer_vectorized,inner_vectorized,xfail\',\n                         [(False, True, False), (True, False, True), (True, True, True)],\n                         ids=[\'iplate-plate\', \'plate-iplate\', \'plate-plate\'])\ndef test_elbo_enumerate_plates_8(model_scale, guide_scale, inner_vectorized, outer_vectorized, xfail):\n    #        Guide   Model\n    #                  a\n    #      +-----------|--------+\n    #      | M=2   +---|------+ |\n    #      |       |   V  N=2 | |\n    #      |   b ----> c      | |\n    #      |       +----------+ |\n    #      +--------------------+\n    pyro.param(""model_probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_b"",\n               torch.tensor([0.6, 0.4]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_c"",\n               torch.tensor([[[0.4, 0.5, 0.1], [0.3, 0.5, 0.2]],\n                             [[0.3, 0.4, 0.3], [0.4, 0.4, 0.2]]]),\n               constraint=constraints.simplex)\n    pyro.param(""guide_probs_b"",\n               torch.tensor([0.8, 0.2]),\n               constraint=constraints.simplex)\n    data = torch.tensor([[0, 1], [0, 2]])\n\n    @config_enumerate\n    @poutine.scale(scale=model_scale)\n    def model_plate_plate():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with pyro.plate(""outer"", 2):\n            b = pyro.sample(""b"", dist.Categorical(probs_b))\n            with pyro.plate(""inner"", 2):\n                pyro.sample(""c"",\n                            dist.Categorical(Vindex(probs_c)[a, b]),\n                            obs=data)\n\n    @config_enumerate\n    @poutine.scale(scale=model_scale)\n    def model_iplate_plate():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        inner = pyro.plate(""inner"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        for i in pyro.plate(""outer"", 2):\n            b = pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b))\n            with inner:\n                pyro.sample(""c_{}"".format(i),\n                            dist.Categorical(Vindex(probs_c)[a, b]),\n                            obs=data[:, i])\n\n    @config_enumerate\n    @poutine.scale(scale=model_scale)\n    def model_plate_iplate():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with pyro.plate(""outer"", 2):\n            b = pyro.sample(""b"", dist.Categorical(probs_b))\n            for j in pyro.plate(""inner"", 2):\n                pyro.sample(""c_{}"".format(j),\n                            dist.Categorical(Vindex(probs_c)[a, b]),\n                            obs=data[j])\n\n    @config_enumerate\n    @poutine.scale(scale=model_scale)\n    def model_iplate_iplate():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        inner = pyro.plate(""inner"", 2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        for i in pyro.plate(""outer"", 2):\n            b = pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b))\n            for j in inner:\n                pyro.sample(""c_{}_{}"".format(i, j),\n                            dist.Categorical(Vindex(probs_c)[a, b]),\n                            obs=data[j, i])\n\n    @config_enumerate\n    @poutine.scale(scale=guide_scale)\n    def guide_plate():\n        probs_b = pyro.param(""guide_probs_b"")\n        with pyro.plate(""outer"", 2):\n            pyro.sample(""b"", dist.Categorical(probs_b))\n\n    @config_enumerate\n    @poutine.scale(scale=guide_scale)\n    def guide_iplate():\n        probs_b = pyro.param(""guide_probs_b"")\n        for i in pyro.plate(""outer"", 2):\n            pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b))\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    expected_loss = elbo.differentiable_loss(model_iplate_iplate, guide_iplate)\n    with ExitStack() as stack:\n        if xfail:\n            stack.enter_context(pytest.raises(\n                ValueError,\n                match=""Expected model enumeration to be no more global than guide""))\n        if inner_vectorized:\n            if outer_vectorized:\n                elbo = TraceEnum_ELBO(max_plate_nesting=2)\n                actual_loss = elbo.differentiable_loss(model_plate_plate, guide_plate)\n            else:\n                elbo = TraceEnum_ELBO(max_plate_nesting=1)\n                actual_loss = elbo.differentiable_loss(model_iplate_plate, guide_iplate)\n        else:\n            elbo = TraceEnum_ELBO(max_plate_nesting=1)\n            actual_loss = elbo.differentiable_loss(model_plate_iplate, guide_plate)\n        _check_loss_and_grads(expected_loss, actual_loss)\n\n\ndef test_elbo_scale():\n    # Consider a mixture model with two components, toggled by `which`.\n    def component_model(data, which, suffix=""""):\n        loc = pyro.param(""locs"", torch.tensor([-1., 1.]))[which]\n        with pyro.plate(""data"" + suffix, len(data)):\n            pyro.sample(""obs"" + suffix, dist.Normal(loc, 1.), obs=data)\n\n    pyro.param(""mixture_probs"", torch.tensor([0.25, 0.75]), constraint=constraints.simplex)\n\n    # We can implement this in two ways.\n    # First consider automatic enumeration in the guide.\n    def auto_model(data):\n        mixture_probs = pyro.param(""mixture_probs"")\n        which = pyro.sample(""which"", dist.Categorical(mixture_probs))\n        component_model(data, which)\n\n    def auto_guide(data):\n        mixture_probs = pyro.param(""mixture_probs"")\n        pyro.sample(""which"", dist.Categorical(mixture_probs),\n                    infer={""enumerate"": ""parallel""})\n\n    # Second consider explicit enumeration in the model, where we\n    # marginalize out the `which` variable by hand.\n    def hand_model(data):\n        mixture_probs = pyro.param(""mixture_probs"")\n        for which in pyro.plate(""which"", len(mixture_probs)):\n            with pyro.poutine.scale(scale=mixture_probs[which]):\n                component_model(data, which, suffix=""_{}"".format(which))\n\n    def hand_guide(data):\n        pass\n\n    data = dist.Normal(0., 2.).sample((3,))\n    elbo = TraceEnum_ELBO(max_plate_nesting=1, strict_enumeration_warning=False)\n    auto_loss = elbo.differentiable_loss(auto_model, auto_guide, data)\n    hand_loss = elbo.differentiable_loss(hand_model, hand_guide, data)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\ndef test_elbo_hmm_growth():\n    pyro.clear_param_store()\n    init_probs = torch.tensor([0.5, 0.5])\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n\n    def model(data):\n        transition_probs = pyro.param(""transition_probs"",\n                                      torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                      constraint=constraints.simplex)\n        emission_probs = pyro.param(""emission_probs"",\n                                    torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                    constraint=constraints.simplex)\n        x = None\n        for i, y in pyro.markov(enumerate(data)):\n            probs = init_probs if x is None else transition_probs[x]\n            x = pyro.sample(""x_{}"".format(i), dist.Categorical(probs))\n            pyro.sample(""y_{}"".format(i), dist.Categorical(emission_probs[x]), obs=y)\n\n    @config_enumerate\n    def guide(data):\n        transition_probs = pyro.param(""transition_probs"",\n                                      torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                                      constraint=constraints.simplex)\n        x = None\n        for i, y in pyro.markov(enumerate(data)):\n            probs = init_probs if x is None else transition_probs[x]\n            x = pyro.sample(""x_{}"".format(i), dist.Categorical(probs))\n\n    sizes = range(3, 1 + int(os.environ.get(\'GROWTH_SIZE\', 15)))\n    costs = []\n    times1 = []\n    times2 = []\n    for size in sizes:\n        data = torch.ones(size)\n\n        time0 = timeit.default_timer()\n        elbo.loss_and_grads(model, guide, data)  # compiles paths\n        time1 = timeit.default_timer()\n        elbo.loss_and_grads(model, guide, data)  # reuses compiled path\n        time2 = timeit.default_timer()\n\n        times1.append(time1 - time0)\n        times2.append(time2 - time1)\n        costs.append(LAST_CACHE_SIZE[0])\n\n    collated_costs = defaultdict(list)\n    for counts in costs:\n        for key, cost in counts.items():\n            collated_costs[key].append(cost)\n    logger.debug(\'\\n\'.join([\n        \'HMM Growth:\',\n        \'sizes = {}\'.format(repr(sizes)),\n        \'costs = {}\'.format(repr(dict(collated_costs))),\n        \'times1 = {}\'.format(repr(times1)),\n        \'times2 = {}\'.format(repr(times2)),\n    ]))\n\n\n@pytest.mark.skipif(""CUDA_TEST"" in os.environ, reason=""https://github.com/pyro-ppl/pyro/issues/1380"")\ndef test_elbo_dbn_growth():\n    pyro.clear_param_store()\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n\n    def model(data):\n        uniform = torch.tensor([0.5, 0.5])\n        probs_z = pyro.param(""probs_z"",\n                             torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                             constraint=constraints.simplex)\n        for i, z in pyro.markov(enumerate(data)):\n            pyro.sample(""x_{}"".format(i), dist.Categorical(uniform))\n            y = pyro.sample(""y_{}"".format(i), dist.Categorical(uniform))\n            pyro.sample(""z_{}"".format(i), dist.Categorical(probs_z[y]), obs=z)\n\n    @config_enumerate\n    def guide(data):\n        probs_x = pyro.param(""probs_x"",\n                             torch.tensor([[0.75, 0.25], [0.25, 0.75]]),\n                             constraint=constraints.simplex)\n        probs_y = pyro.param(""probs_y"",\n                             torch.tensor([[[0.75, 0.25], [0.45, 0.55]],\n                                           [[0.55, 0.45], [0.25, 0.75]]]),\n                             constraint=constraints.simplex)\n        x = 0\n        y = 0\n        for i in pyro.markov(range(len(data))):\n            x = pyro.sample(""x_{}"".format(i), dist.Categorical(probs_x[x]))\n            y = pyro.sample(""y_{}"".format(i), dist.Categorical(probs_y[x, y]))\n\n    sizes = range(3, 1 + int(os.environ.get(\'GROWTH_SIZE\', 15)))\n    costs = []\n    times1 = []\n    times2 = []\n    for size in sizes:\n        data = torch.ones(size)\n\n        time0 = timeit.default_timer()\n        elbo.loss_and_grads(model, guide, data)  # compiles paths\n        time1 = timeit.default_timer()\n        elbo.loss_and_grads(model, guide, data)  # reuses compiled path\n        time2 = timeit.default_timer()\n\n        times1.append(time1 - time0)\n        times2.append(time2 - time1)\n        costs.append(LAST_CACHE_SIZE[0])\n\n    collated_costs = defaultdict(list)\n    for counts in costs:\n        for key, cost in counts.items():\n            collated_costs[key].append(cost)\n    logger.debug(\'\\n\'.join([\n        \'DBN Growth:\',\n        \'sizes = {}\'.format(repr(sizes)),\n        \'costs = {}\'.format(repr(dict(collated_costs))),\n        \'times1 = {}\'.format(repr(times1)),\n        \'times2 = {}\'.format(repr(times2)),\n    ]))\n\n\n@pytest.mark.parametrize(""pi_a"", [0.33])\n@pytest.mark.parametrize(""pi_b"", [0.51, 0.77])\n@pytest.mark.parametrize(""pi_c"", [0.37])\n@pytest.mark.parametrize(""N_b"", [3, 4])\n@pytest.mark.parametrize(""N_c"", [5, 6])\n@pytest.mark.parametrize(""enumerate1"", [""sequential"", ""parallel""])\n@pytest.mark.parametrize(""expand"", [True, False])\ndef test_bernoulli_pyramid_elbo_gradient(enumerate1, N_b, N_c, pi_a, pi_b, pi_c, expand):\n    pyro.clear_param_store()\n\n    def model():\n        a = pyro.sample(""a"", dist.Bernoulli(0.33))\n        with pyro.plate(""b_plate"", N_b):\n            b = pyro.sample(""b"", dist.Bernoulli(0.25 * a + 0.50))\n            with pyro.plate(""c_plate"", N_c):\n                pyro.sample(""c"", dist.Bernoulli(0.15 * a + 0.20 * b + 0.32))\n\n    def guide():\n        qa = pyro.param(""qa"", torch.tensor(pi_a, requires_grad=True))\n        qb = pyro.param(""qb"", torch.tensor(pi_b, requires_grad=True))\n        qc = pyro.param(""qc"", torch.tensor(pi_c, requires_grad=True))\n        pyro.sample(""a"", dist.Bernoulli(qa))\n        with pyro.plate(""b_plate"", N_b):\n            pyro.sample(""b"", dist.Bernoulli(qb).expand_by([N_b]))\n            with pyro.plate(""c_plate"", N_c):\n                pyro.sample(""c"", dist.Bernoulli(qc).expand_by([N_c, N_b]))\n\n    logger.info(""Computing gradients using surrogate loss"")\n    elbo = TraceEnum_ELBO(max_plate_nesting=2,\n                          strict_enumeration_warning=True)\n    elbo.loss_and_grads(model, config_enumerate(guide, default=enumerate1, expand=expand))\n    actual_grad_qa = pyro.param(\'qa\').grad\n    actual_grad_qb = pyro.param(\'qb\').grad\n    actual_grad_qc = pyro.param(\'qc\').grad\n\n    logger.info(""Computing analytic gradients"")\n    qa = torch.tensor(pi_a, requires_grad=True)\n    qb = torch.tensor(pi_b, requires_grad=True)\n    qc = torch.tensor(pi_c, requires_grad=True)\n    elbo = kl_divergence(dist.Bernoulli(qa), dist.Bernoulli(0.33))\n    elbo = elbo + N_b * qa * kl_divergence(dist.Bernoulli(qb), dist.Bernoulli(0.75))\n    elbo = elbo + N_b * (1.0 - qa) * kl_divergence(dist.Bernoulli(qb), dist.Bernoulli(0.50))\n    elbo = elbo + N_c * N_b * qa * qb * kl_divergence(dist.Bernoulli(qc), dist.Bernoulli(0.67))\n    elbo = elbo + N_c * N_b * (1.0 - qa) * qb * kl_divergence(dist.Bernoulli(qc), dist.Bernoulli(0.52))\n    elbo = elbo + N_c * N_b * qa * (1.0 - qb) * kl_divergence(dist.Bernoulli(qc), dist.Bernoulli(0.47))\n    elbo = elbo + N_c * N_b * (1.0 - qa) * (1.0 - qb) * kl_divergence(dist.Bernoulli(qc), dist.Bernoulli(0.32))\n    expected_grad_qa, expected_grad_qb, expected_grad_qc = grad(elbo, [qa, qb, qc])\n\n    prec = 0.001\n\n    assert_equal(actual_grad_qa, expected_grad_qa, prec=prec, msg="""".join([\n        ""\\nqa expected = {}"".format(expected_grad_qa.data.cpu().numpy()),\n        ""\\nqa  actual = {}"".format(actual_grad_qa.data.cpu().numpy()),\n    ]))\n    assert_equal(actual_grad_qb, expected_grad_qb, prec=prec, msg="""".join([\n        ""\\nqb expected = {}"".format(expected_grad_qb.data.cpu().numpy()),\n        ""\\nqb   actual = {}"".format(actual_grad_qb.data.cpu().numpy()),\n    ]))\n    assert_equal(actual_grad_qc, expected_grad_qc, prec=prec, msg="""".join([\n        ""\\nqc expected = {}"".format(expected_grad_qc.data.cpu().numpy()),\n        ""\\nqc   actual = {}"".format(actual_grad_qc.data.cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""pi_a"", [0.33])\n@pytest.mark.parametrize(""pi_b"", [0.51])\n@pytest.mark.parametrize(""pi_c"", [0.37])\n@pytest.mark.parametrize(""pi_d"", [0.29])\n@pytest.mark.parametrize(""b_factor"", [0.03, 0.04])\n@pytest.mark.parametrize(""c_factor"", [0.04, 0.06])\n@pytest.mark.parametrize(""d_offset"", [0.32])\n@pytest.mark.parametrize(""enumerate1"", [""sequential"", ""parallel""])\n@pytest.mark.parametrize(""expand"", [True, False])\ndef test_bernoulli_non_tree_elbo_gradient(enumerate1, b_factor, c_factor, pi_a, pi_b, pi_c, pi_d,\n                                          expand, d_offset, N_b=2, N_c=2):\n    pyro.clear_param_store()\n\n    def model():\n        a = pyro.sample(""a"", dist.Bernoulli(0.33))\n        b = pyro.sample(""b"", dist.Bernoulli(0.25 * a + 0.50))\n        c = pyro.sample(""c"", dist.Bernoulli(0.25 * a + 0.10 * b + 0.50))\n        pyro.sample(""d"", dist.Bernoulli(b_factor * b + c_factor * c + d_offset))\n\n    def guide():\n        qa = pyro.param(""qa"", torch.tensor(pi_a, requires_grad=True))\n        qb = pyro.param(""qb"", torch.tensor(pi_b, requires_grad=True))\n        qc = pyro.param(""qc"", torch.tensor(pi_c, requires_grad=True))\n        qd = pyro.param(""qd"", torch.tensor(pi_d, requires_grad=True))\n        pyro.sample(""a"", dist.Bernoulli(qa))\n        pyro.sample(""b"", dist.Bernoulli(qb))\n        pyro.sample(""c"", dist.Bernoulli(qc))\n        pyro.sample(""d"", dist.Bernoulli(qd))\n\n    logger.info(""Computing gradients using surrogate loss"")\n    elbo = TraceEnum_ELBO(max_plate_nesting=2,\n                          strict_enumeration_warning=True)\n    elbo.loss_and_grads(model, config_enumerate(guide, default=enumerate1, expand=expand))\n    actual_grad_qa = pyro.param(\'qa\').grad\n    actual_grad_qb = pyro.param(\'qb\').grad\n    actual_grad_qc = pyro.param(\'qc\').grad\n    actual_grad_qd = pyro.param(\'qd\').grad\n\n    logger.info(""Computing analytic gradients"")\n    qa = torch.tensor(pi_a, requires_grad=True)\n    qb = torch.tensor(pi_b, requires_grad=True)\n    qc = torch.tensor(pi_c, requires_grad=True)\n    qd = torch.tensor(pi_d, requires_grad=True)\n\n    elbo = kl_divergence(dist.Bernoulli(qa), dist.Bernoulli(0.33))\n    elbo = elbo + qa * kl_divergence(dist.Bernoulli(qb), dist.Bernoulli(0.75))\n    elbo = elbo + (1.0 - qa) * kl_divergence(dist.Bernoulli(qb), dist.Bernoulli(0.50))\n\n    elbo = elbo + qa * qb * kl_divergence(dist.Bernoulli(qc), dist.Bernoulli(0.85))\n    elbo = elbo + (1.0 - qa) * qb * kl_divergence(dist.Bernoulli(qc), dist.Bernoulli(0.60))\n    elbo = elbo + qa * (1.0 - qb) * kl_divergence(dist.Bernoulli(qc), dist.Bernoulli(0.75))\n    elbo = elbo + (1.0 - qa) * (1.0 - qb) * kl_divergence(dist.Bernoulli(qc), dist.Bernoulli(0.50))\n\n    elbo = elbo + qb * qc * kl_divergence(dist.Bernoulli(qd), dist.Bernoulli(b_factor + c_factor + d_offset))\n    elbo = elbo + (1.0 - qb) * qc * kl_divergence(dist.Bernoulli(qd), dist.Bernoulli(c_factor + d_offset))\n    elbo = elbo + qb * (1.0 - qc) * kl_divergence(dist.Bernoulli(qd), dist.Bernoulli(b_factor + d_offset))\n    elbo = elbo + (1.0 - qb) * (1.0 - qc) * kl_divergence(dist.Bernoulli(qd), dist.Bernoulli(d_offset))\n\n    expected_grad_qa, expected_grad_qb, expected_grad_qc, expected_grad_qd = grad(elbo, [qa, qb, qc, qd])\n\n    prec = 0.0001\n\n    assert_equal(actual_grad_qa, expected_grad_qa, prec=prec, msg="""".join([\n        ""\\nqa expected = {}"".format(expected_grad_qa.data.cpu().numpy()),\n        ""\\nqa  actual = {}"".format(actual_grad_qa.data.cpu().numpy()),\n    ]))\n    assert_equal(actual_grad_qb, expected_grad_qb, prec=prec, msg="""".join([\n        ""\\nqb expected = {}"".format(expected_grad_qb.data.cpu().numpy()),\n        ""\\nqb   actual = {}"".format(actual_grad_qb.data.cpu().numpy()),\n    ]))\n    assert_equal(actual_grad_qc, expected_grad_qc, prec=prec, msg="""".join([\n        ""\\nqc expected = {}"".format(expected_grad_qc.data.cpu().numpy()),\n        ""\\nqc   actual = {}"".format(actual_grad_qc.data.cpu().numpy()),\n    ]))\n    assert_equal(actual_grad_qd, expected_grad_qd, prec=prec, msg="""".join([\n        ""\\nqd expected = {}"".format(expected_grad_qd.data.cpu().numpy()),\n        ""\\nqd   actual = {}"".format(actual_grad_qd.data.cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(""gate"", [0.1, 0.25, 0.5, 0.75, 0.9])\n@pytest.mark.parametrize(""rate"", [0.1, 1., 3.])\ndef test_elbo_zip(gate, rate):\n    # test for ZIP distribution\n    def zip_model(data):\n        gate = pyro.param(""gate"")\n        rate = pyro.param(""rate"")\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", dist.ZeroInflatedPoisson(gate, rate), obs=data)\n\n    def composite_model(data):\n        gate = pyro.param(""gate"")\n        rate = pyro.param(""rate"")\n        dist1 = dist.Delta(torch.tensor(0.))\n        dist0 = dist.Poisson(rate)\n        with pyro.plate(""data"", len(data)):\n            mask = pyro.sample(""mask"", dist.Bernoulli(gate), infer={""enumerate"": ""parallel""}).bool()\n            pyro.sample(""obs"", dist.MaskedMixture(mask, dist0, dist1), obs=data)\n\n    def guide(data):\n        pass\n\n    pyro.param(""gate"", torch.tensor(gate), constraint=constraints.unit_interval)\n    pyro.param(""rate"", torch.tensor(rate), constraint=constraints.positive)\n\n    data = torch.tensor([0., 1., 2.])\n    elbo = TraceEnum_ELBO(max_plate_nesting=1, strict_enumeration_warning=False)\n    zip_loss = elbo.differentiable_loss(zip_model, guide, data)\n    composite_loss = elbo.differentiable_loss(composite_model, guide, data)\n    _check_loss_and_grads(zip_loss, composite_loss)\n\n\n@pytest.mark.parametrize(""mixture,scale"", [\n    (dist.MixtureOfDiagNormals, [[2., 1.], [1., 2], [4., 4.]]),\n    (dist.MixtureOfDiagNormalsSharedCovariance, [2., 1.]),\n])\ndef test_mixture_of_diag_normals(mixture, scale):\n    # K = 3, D = 2\n    pyro.param(""locs"", torch.tensor([[0., 0.], [0., 1.], [0., 10.]]))\n    pyro.param(""coord_scale"", torch.tensor(scale), constraint=constraints.positive)\n    pyro.param(""component_logits"", torch.tensor([0., -1., 2.]))\n    data = torch.tensor([[0., 0.], [1., 1.], [2., 3.], [1., 11.]])\n\n    def auto_model():\n        locs = pyro.param(""locs"")\n        coord_scale = pyro.param(""coord_scale"")\n        component_logits = pyro.param(""component_logits"")\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", mixture(locs, coord_scale, component_logits), obs=data)\n\n    def hand_model():\n        locs = pyro.param(""locs"")\n        coord_scale = pyro.param(""coord_scale"")\n        component_logits = pyro.param(""component_logits"")\n        with pyro.plate(""data"", len(data), dim=-2):\n            which = pyro.sample(""mask"", dist.Categorical(logits=component_logits),\n                                infer={""enumerate"": ""parallel""})\n            with pyro.plate(""components"", len(component_logits), dim=-1) as component_ind:\n                with poutine.mask(mask=(which == component_ind)):\n                    pyro.sample(""obs"", dist.Normal(locs, coord_scale).to_event(1),\n                                obs=data.unsqueeze(-2))\n\n    def guide():\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=2, strict_enumeration_warning=False)\n    auto_loss = elbo.differentiable_loss(auto_model, guide)\n    hand_loss = elbo.differentiable_loss(hand_model, guide)\n    _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(""Dist, prior"", [\n    (dist.Bernoulli, 0.2),\n    (dist.Categorical, [0.2, 0.8]),\n    (dist.Categorical, [0.2, 0.3, 0.5]),\n    (dist.Categorical, [0.2, 0.3, 0.3, 0.2]),\n    (dist.OneHotCategorical, [0.2, 0.8]),\n    (dist.OneHotCategorical, [0.2, 0.3, 0.5]),\n    (dist.OneHotCategorical, [0.2, 0.3, 0.3, 0.2]),\n])\ndef test_compute_marginals_single(Dist, prior):\n    prior = torch.tensor(prior)\n    data = torch.tensor([0., 0.1, 0.2, 0.9, 1.0, 1.1])\n\n    @config_enumerate\n    def model():\n        locs = torch.tensor([-1., 0., 1., 2.])\n        x = pyro.sample(""x"", Dist(prior))\n        if Dist is dist.Bernoulli:\n            x = x.long()\n        elif Dist is dist.OneHotCategorical:\n            x = x.max(-1)[1]\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", dist.Normal(locs[x], 1.), obs=data)\n\n    # First compute marginals using an empty guide.\n    def empty_guide():\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    marginals = elbo.compute_marginals(model, empty_guide)\n    assert len(marginals) == 1\n    assert type(marginals[""x""]) is Dist\n    probs = marginals[""x""].probs\n    assert probs.shape == prior.shape\n\n    # Next insert the computed marginals in an enumerating guide\n    # and ensure that they are exact, or at least locally optimal.\n    pyro.param(""probs"", probs)\n\n    @config_enumerate\n    def exact_guide():\n        probs = pyro.param(""probs"")\n        pyro.sample(""x"", Dist(probs))\n\n    loss = elbo.differentiable_loss(model, exact_guide)\n    assert_equal(grad(loss, [pyro.param(""probs"")])[0], torch.zeros_like(probs))\n\n\n@pytest.mark.parametrize(\'ok,enumerate_guide,num_particles,vectorize_particles\', [\n    (True, None, 1, False),\n    (False, ""sequential"", 1, False),\n    (False, ""parallel"", 1, False),\n    (False, None, 2, False),\n    (False, None, 2, True),\n])\ndef test_compute_marginals_restrictions(ok, enumerate_guide, num_particles, vectorize_particles):\n\n    @config_enumerate\n    def model():\n        w = pyro.sample(""w"", dist.Bernoulli(0.1))\n        x = pyro.sample(""x"", dist.Bernoulli(0.2))\n        y = pyro.sample(""y"", dist.Bernoulli(0.3))\n        z = pyro.sample(""z"", dist.Bernoulli(0.4))\n        pyro.sample(""obs"", dist.Normal(0., 1.), obs=w + x + y + z)\n\n    @config_enumerate(default=enumerate_guide)\n    def guide():\n        pyro.sample(""w"", dist.Bernoulli(0.4))\n        pyro.sample(""y"", dist.Bernoulli(0.7))\n\n    # Check that the ELBO works fine.\n    elbo = TraceEnum_ELBO(max_plate_nesting=0,\n                          num_particles=num_particles,\n                          vectorize_particles=vectorize_particles)\n    loss = elbo.loss(model, guide)\n    assert not torch_isnan(loss)\n\n    if ok:\n        marginals = elbo.compute_marginals(model, guide)\n        assert set(marginals.keys()) == {""x"", ""z""}\n    else:\n        with pytest.raises(NotImplementedError, match=""compute_marginals""):\n            elbo.compute_marginals(model, guide)\n\n\n@pytest.mark.parametrize(\'size\', [1, 2, 3, 4, 10, 20, _skip_cuda(30)])\ndef test_compute_marginals_hmm(size):\n\n    @config_enumerate\n    def model(data):\n        transition_probs = torch.tensor([[0.75, 0.25], [0.25, 0.75]])\n        emission_probs = torch.tensor([[0.75, 0.25], [0.25, 0.75]])\n        x = torch.tensor(0)\n        for i in pyro.markov(range(len(data) + 1)):\n            if i < len(data):\n                x = pyro.sample(""x_{}"".format(i), dist.Categorical(transition_probs[x]))\n                pyro.sample(""y_{}"".format(i), dist.Categorical(emission_probs[x]), obs=data[i])\n            else:\n                pyro.sample(""x_{}"".format(i), dist.Categorical(transition_probs[x]),\n                            obs=torch.tensor(1))\n\n    def guide(data):\n        pass\n\n    data = torch.zeros(size, dtype=torch.long)\n    elbo = TraceEnum_ELBO(max_plate_nesting=0)\n    marginals = elbo.compute_marginals(model, guide, data)\n    assert set(marginals.keys()) == {""x_{}"".format(i) for i in range(size)}\n    for i in range(size):\n        d = marginals[""x_{}"".format(i)]\n        assert d.batch_shape == ()\n\n    # The x\'s should be monotonically increasing, since we\'ve observed x[-1]==0\n    # and x[size]==1, and since the y\'s are constant.\n    for i in range(size - 1):\n        d1 = marginals[""x_{}"".format(i)]\n        d2 = marginals[""x_{}"".format(i + 1)]\n        assert d1.probs[0] > d2.probs[0]\n        assert d1.probs[1] < d2.probs[1]\n\n\n@pytest.mark.parametrize(""data"", [\n    [None, None],\n    [torch.tensor(0.), None],\n    [None, torch.tensor(0.)],\n    [torch.tensor(0.), torch.tensor(0)],\n])\ndef test_backwardsample_posterior_smoke(data):\n\n    @config_enumerate\n    def model(data):\n        xs = list(data)\n        zs = []\n        for i in range(2):\n            K = i + 2  # number of mixture components\n            zs.append(pyro.sample(""z_{}"".format(i),\n                                  dist.Categorical(torch.ones(K))))\n            if i == 0:\n                loc = pyro.param(""loc"", torch.randn(K))[zs[i]]\n                xs[i] = pyro.sample(""x_{}"".format(i),\n                                    dist.Normal(loc, 1.), obs=data[i])\n            elif i == 1:\n                logits = pyro.param(""logits"", torch.randn(K, 2))[zs[i]]\n                xs[i] = pyro.sample(""x_{}"".format(i),\n                                    dist.Categorical(logits=logits),\n                                    obs=data[i])\n\n        z12 = zs[0] + 2 * zs[1]\n        pyro.sample(""z_12"", dist.Categorical(torch.arange(6.)), obs=z12)\n        return xs, zs\n\n    def guide(data):\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    xs, zs = elbo.sample_posterior(model, guide, data)\n    for x, datum in zip(xs, data):\n        assert datum is None or datum is x\n    for z in zs:\n        assert z.shape == ()\n\n\ndef test_backwardsample_posterior_2():\n    num_particles = 10000\n\n    @config_enumerate\n    def model(data):\n        with pyro.plate(""particles"", num_particles):\n            p_z = torch.tensor([0.1, 0.9])\n            x = pyro.sample(""x"", dist.Categorical(torch.tensor([0.5, 0.5])))\n            z = pyro.sample(""z"", dist.Bernoulli(p_z[x]), obs=data)\n        return x, z\n\n    def guide(data):\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n    x, z = elbo.sample_posterior(model, guide, data=torch.zeros(num_particles))\n    expected = 0.9\n    actual = (x.type_as(z) == z).float().mean().item()\n    assert abs(expected - actual) < 0.05\n\n\ndef test_backwardsample_posterior_3():\n    num_particles = 10000\n\n    @config_enumerate\n    def model(data):\n        with pyro.plate(""particles"", num_particles):\n            p_z = torch.tensor([[0.9, 0.1], [0.1, 0.9]])\n            x = pyro.sample(""x"", dist.Categorical(torch.tensor([0.5, 0.5])))\n            y = pyro.sample(""y"", dist.Categorical(torch.tensor([0.5, 0.5])))\n            z = pyro.sample(""z"", dist.Bernoulli(p_z[x, y]), obs=data)\n        return x, y, z\n\n    def guide(data):\n        pass\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=1)\n\n    x, y, z = elbo.sample_posterior(model, guide, data=torch.ones(num_particles))\n    expected = 0.9\n    actual = (x == y).float().mean().item()\n    assert abs(expected - actual) < 0.05\n\n    x, y, z = elbo.sample_posterior(model, guide, data=torch.zeros(num_particles))\n    expected = 0.1\n    actual = (x == y).float().mean().item()\n    assert abs(expected - actual) < 0.05\n\n\n@pytest.mark.parametrize(\'ok,enumerate_guide,num_particles,vectorize_particles\', [\n    (True, None, 1, False),\n    (False, ""sequential"", 1, False),\n    (False, ""parallel"", 1, False),\n    (False, None, 2, False),\n    (False, None, 2, True),\n])\ndef test_backwardsample_posterior_restrictions(ok, enumerate_guide, num_particles, vectorize_particles):\n\n    @config_enumerate\n    def model():\n        w = pyro.sample(""w"", dist.Bernoulli(0.1))\n        x = pyro.sample(""x"", dist.Bernoulli(0.2))\n        y = pyro.sample(""y"", dist.Bernoulli(0.3))\n        z = pyro.sample(""z"", dist.Bernoulli(0.4))\n        pyro.sample(""obs"", dist.Normal(0., 1.), obs=w + x + y + z)\n        return w, x, y, z\n\n    @config_enumerate(default=enumerate_guide)\n    def guide():\n        pyro.sample(""w"", dist.Bernoulli(0.4))\n        pyro.sample(""y"", dist.Bernoulli(0.7))\n\n    # Check that the ELBO works fine.\n    elbo = TraceEnum_ELBO(max_plate_nesting=0,\n                          num_particles=num_particles,\n                          vectorize_particles=vectorize_particles)\n    loss = elbo.loss(model, guide)\n    assert not torch_isnan(loss)\n\n    if ok:\n        w, x, y, z = elbo.sample_posterior(model, guide)\n        assert w.shape == ()\n        assert x.shape == ()\n        assert y.shape == ()\n        assert z.shape == ()\n    else:\n        with pytest.raises(NotImplementedError, match=""sample_posterior""):\n            elbo.sample_posterior(model, guide)\n\n\n@pytest.mark.parametrize(""num_samples"", [10000, 100000])\ndef test_vectorized_importance(num_samples):\n\n    pyro.param(""model_probs_a"",\n               torch.tensor([0.45, 0.55]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_b"",\n               torch.tensor([0.6, 0.4]),\n               constraint=constraints.simplex)\n    pyro.param(""model_probs_c"",\n               torch.tensor([[[0.4, 0.5, 0.1], [0.3, 0.5, 0.2]],\n                             [[0.3, 0.4, 0.3], [0.4, 0.4, 0.2]]]),\n               constraint=constraints.simplex)\n\n    pyro.param(""guide_probs_a"",\n               torch.tensor([0.33, 0.67]),\n               constraint=constraints.simplex)\n\n    pyro.param(""guide_probs_b"",\n               torch.tensor([0.8, 0.2]),\n               constraint=constraints.simplex)\n\n    data = torch.tensor([[0, 1], [0, 2]])\n\n    def model():\n        probs_a = pyro.param(""model_probs_a"")\n        probs_b = pyro.param(""model_probs_b"")\n        probs_c = pyro.param(""model_probs_c"")\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with pyro.plate(""outer"", 2):\n            b = pyro.sample(""b"", dist.Categorical(probs_b))\n            with pyro.plate(""inner"", 2):\n                pyro.sample(""c"", dist.Categorical(Vindex(probs_c)[a, b]),\n                            obs=data)\n\n    def guide():\n        probs_a = pyro.param(""guide_probs_a"")\n        pyro.sample(""a"", dist.Categorical(probs_a))\n        probs_b = pyro.param(""guide_probs_b"")\n        with pyro.plate(""outer"", 2):\n            pyro.sample(""b"", dist.Categorical(probs_b))\n\n    vectorized_weights, _, _ = vectorized_importance_weights(model, guide, max_plate_nesting=4, num_samples=num_samples)\n\n    elbo = Trace_ELBO(vectorize_particles=True, num_particles=num_samples).loss(model, guide)\n\n    assert_equal(vectorized_weights.sum().item() / num_samples, -elbo, prec=0.02)\n\n\ndef test_multi_dependence_enumeration():\n    """"""\n    This test checks whether enumeration works correctly in the case where multiple downstream\n    variables are coupled to the same random discrete variable.\n    This is based on [issue 2223](https://github.com/pyro-ppl/pyro/issues/2223), and should\n    pass when it has been resolved\n    """"""\n    K = 5\n    d = 2\n    N_obs = 3\n\n    @config_enumerate\n    def model(N=1):\n        with pyro.plate(\'data_plate\', N, dim=-2):\n            mixing_weights = pyro.param(\'pi\', torch.ones(K) / K, constraint=constraints.simplex)\n            means = pyro.sample(\'mu\', dist.Normal(torch.zeros(K, d), torch.ones(K, d)).to_event(2))\n\n            with pyro.plate(\'observations\', N_obs, dim=-1):\n                s = pyro.sample(\'s\', dist.Categorical(mixing_weights))\n\n                pyro.sample(\'x\', dist.Normal(Vindex(means)[..., s, :], 0.1).to_event(1))\n                pyro.sample(\'y\', dist.Normal(Vindex(means)[..., s, :], 0.1).to_event(1))\n\n    x = poutine.trace(model).get_trace(N=2).nodes[\'x\'][\'value\']\n\n    pyro.clear_param_store()\n    conditioned_model = pyro.condition(model, data={\'x\': x})\n    guide = infer.autoguide.AutoDelta(poutine.block(conditioned_model, hide=[\'s\']))\n\n    elbo = infer.TraceEnum_ELBO(max_plate_nesting=2)\n\n    elbo.loss_and_grads(conditioned_model, guide, x.size(0))\n    assert pyro.get_param_store()._params[\'pi\'].grad is not None\n'"
tests/infer/test_gradient.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\nimport numpy as np\nimport pytest\nimport torch\nimport torch.optim\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.distributions.testing import fakes\nfrom pyro.infer import (SVI, JitTrace_ELBO, JitTraceEnum_ELBO, JitTraceGraph_ELBO, JitTraceMeanField_ELBO, Trace_ELBO,\n                        TraceEnum_ELBO, TraceGraph_ELBO, TraceMeanField_ELBO, config_enumerate)\nfrom pyro.optim import Adam\nfrom tests.common import assert_equal, xfail_if_not_implemented, xfail_param\n\nlogger = logging.getLogger(__name__)\n\n\ndef DiffTrace_ELBO(*args, **kwargs):\n    return Trace_ELBO(*args, **kwargs).differentiable_loss\n\n\n@pytest.mark.parametrize(""scale"", [1., 2.], ids=[""unscaled"", ""scaled""])\n@pytest.mark.parametrize(""reparameterized,has_rsample"",\n                         [(True, None), (True, False), (True, True), (False, None)],\n                         ids=[""reparam"", ""reparam-False"", ""reparam-True"", ""nonreparam""])\n@pytest.mark.parametrize(""subsample"", [False, True], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo,local_samples"", [\n    (Trace_ELBO, False),\n    (DiffTrace_ELBO, False),\n    (TraceGraph_ELBO, False),\n    (TraceMeanField_ELBO, False),\n    (TraceEnum_ELBO, False),\n    (TraceEnum_ELBO, True),\n])\ndef test_subsample_gradient(Elbo, reparameterized, has_rsample, subsample, local_samples, scale):\n    pyro.clear_param_store()\n    data = torch.tensor([-0.5, 2.0])\n    subsample_size = 1 if subsample else len(data)\n    precision = 0.06 * scale\n    Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n\n    def model(subsample):\n        with pyro.plate(""data"", len(data), subsample_size, subsample) as ind:\n            x = data[ind]\n            z = pyro.sample(""z"", Normal(0, 1))\n            pyro.sample(""x"", Normal(z, 1), obs=x)\n\n    def guide(subsample):\n        scale = pyro.param(""scale"", lambda: torch.tensor([1.0]))\n        with pyro.plate(""data"", len(data), subsample_size, subsample):\n            loc = pyro.param(""loc"", lambda: torch.zeros(len(data)), event_dim=0)\n            z_dist = Normal(loc, scale)\n            if has_rsample is not None:\n                z_dist.has_rsample_(has_rsample)\n            pyro.sample(""z"", z_dist)\n\n    if scale != 1.0:\n        model = poutine.scale(model, scale=scale)\n        guide = poutine.scale(guide, scale=scale)\n\n    num_particles = 50000\n    if local_samples:\n        guide = config_enumerate(guide, num_samples=num_particles)\n        num_particles = 1\n\n    optim = Adam({""lr"": 0.1})\n    elbo = Elbo(max_plate_nesting=1,  # set this to ensure rng agrees across runs\n                num_particles=num_particles,\n                vectorize_particles=True,\n                strict_enumeration_warning=False)\n    inference = SVI(model, guide, optim, loss=elbo)\n    with xfail_if_not_implemented():\n        if subsample_size == 1:\n            inference.loss_and_grads(model, guide, subsample=torch.tensor([0], dtype=torch.long))\n            inference.loss_and_grads(model, guide, subsample=torch.tensor([1], dtype=torch.long))\n        else:\n            inference.loss_and_grads(model, guide, subsample=torch.tensor([0, 1], dtype=torch.long))\n    params = dict(pyro.get_param_store().named_parameters())\n    normalizer = 2 if subsample else 1\n    actual_grads = {name: param.grad.detach().cpu().numpy() / normalizer for name, param in params.items()}\n\n    expected_grads = {\'loc\': scale * np.array([0.5, -2.0]), \'scale\': scale * np.array([2.0])}\n    for name in sorted(params):\n        logger.info(\'expected {} = {}\'.format(name, expected_grads[name]))\n        logger.info(\'actual   {} = {}\'.format(name, actual_grads[name]))\n    assert_equal(actual_grads, expected_grads, prec=precision)\n\n\n@pytest.mark.parametrize(""reparameterized"", [True, False], ids=[""reparam"", ""nonreparam""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, DiffTrace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_plate(Elbo, reparameterized):\n    pyro.clear_param_store()\n    data = torch.tensor([-0.5, 2.0])\n    num_particles = 200000\n    precision = 0.06\n    Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n\n    def model():\n        particles_plate = pyro.plate(""particles"", num_particles, dim=-2)\n        data_plate = pyro.plate(""data"", len(data), dim=-1)\n\n        pyro.sample(""nuisance_a"", Normal(0, 1))\n        with particles_plate, data_plate:\n            z = pyro.sample(""z"", Normal(0, 1))\n        pyro.sample(""nuisance_b"", Normal(2, 3))\n        with data_plate, particles_plate:\n            pyro.sample(""x"", Normal(z, 1), obs=data)\n        pyro.sample(""nuisance_c"", Normal(4, 5))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.zeros(len(data)))\n        scale = pyro.param(""scale"", torch.tensor([1.]))\n\n        pyro.sample(""nuisance_c"", Normal(4, 5))\n        with pyro.plate(""particles"", num_particles, dim=-2):\n            with pyro.plate(""data"", len(data), dim=-1):\n                pyro.sample(""z"", Normal(loc, scale))\n        pyro.sample(""nuisance_b"", Normal(2, 3))\n        pyro.sample(""nuisance_a"", Normal(0, 1))\n\n    optim = Adam({""lr"": 0.1})\n    elbo = Elbo(strict_enumeration_warning=False)\n    inference = SVI(model, guide, optim, loss=elbo)\n    inference.loss_and_grads(model, guide)\n    params = dict(pyro.get_param_store().named_parameters())\n    actual_grads = {name: param.grad.detach().cpu().numpy() / num_particles\n                    for name, param in params.items()}\n\n    expected_grads = {\'loc\': np.array([0.5, -2.0]), \'scale\': np.array([2.0])}\n    for name in sorted(params):\n        logger.info(\'expected {} = {}\'.format(name, expected_grads[name]))\n        logger.info(\'actual   {} = {}\'.format(name, actual_grads[name]))\n    assert_equal(actual_grads, expected_grads, prec=precision)\n\n\n@pytest.mark.parametrize(""reparameterized"", [True, False], ids=[""reparam"", ""nonreparam""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, DiffTrace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_plate_elbo_vectorized_particles(Elbo, reparameterized):\n    pyro.clear_param_store()\n    data = torch.tensor([-0.5, 2.0])\n    num_particles = 200000\n    precision = 0.06\n    Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n\n    def model():\n        data_plate = pyro.plate(""data"", len(data))\n\n        pyro.sample(""nuisance_a"", Normal(0, 1))\n        with data_plate:\n            z = pyro.sample(""z"", Normal(0, 1))\n        pyro.sample(""nuisance_b"", Normal(2, 3))\n        with data_plate:\n            pyro.sample(""x"", Normal(z, 1), obs=data)\n        pyro.sample(""nuisance_c"", Normal(4, 5))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.zeros(len(data)))\n        scale = pyro.param(""scale"", torch.tensor([1.]))\n\n        pyro.sample(""nuisance_c"", Normal(4, 5))\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""z"", Normal(loc, scale))\n        pyro.sample(""nuisance_b"", Normal(2, 3))\n        pyro.sample(""nuisance_a"", Normal(0, 1))\n\n    optim = Adam({""lr"": 0.1})\n    loss = Elbo(num_particles=num_particles,\n                vectorize_particles=True,\n                strict_enumeration_warning=False)\n    inference = SVI(model, guide, optim, loss=loss)\n    inference.loss_and_grads(model, guide)\n    params = dict(pyro.get_param_store().named_parameters())\n    actual_grads = {name: param.grad.detach().cpu().numpy()\n                    for name, param in params.items()}\n\n    expected_grads = {\'loc\': np.array([0.5, -2.0]), \'scale\': np.array([2.0])}\n    for name in sorted(params):\n        logger.info(\'expected {} = {}\'.format(name, expected_grads[name]))\n        logger.info(\'actual   {} = {}\'.format(name, actual_grads[name]))\n    assert_equal(actual_grads, expected_grads, prec=precision)\n\n\n@pytest.mark.parametrize(""reparameterized"", [True, False], ids=[""reparam"", ""nonreparam""])\n@pytest.mark.parametrize(""subsample"", [False, True], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [\n    Trace_ELBO,\n    TraceGraph_ELBO,\n    TraceEnum_ELBO,\n    TraceMeanField_ELBO,\n    xfail_param(JitTrace_ELBO,\n                reason=""in broadcast_all: RuntimeError: expected int at position 0, but got: Tensor""),\n    xfail_param(JitTraceGraph_ELBO,\n                reason=""in broadcast_all: RuntimeError: expected int at position 0, but got: Tensor""),\n    xfail_param(JitTraceEnum_ELBO,\n                reason=""in broadcast_all: RuntimeError: expected int at position 0, but got: Tensor""),\n    xfail_param(JitTraceMeanField_ELBO,\n                reason=""in broadcast_all: RuntimeError: expected int at position 0, but got: Tensor""),\n])\ndef test_subsample_gradient_sequential(Elbo, reparameterized, subsample):\n    pyro.clear_param_store()\n    data = torch.tensor([-0.5, 2.0])\n    subsample_size = 1 if subsample else len(data)\n    num_particles = 5000\n    precision = 0.333\n    Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n\n    def model():\n        with pyro.plate(""data"", len(data), subsample_size) as ind:\n            x = data[ind]\n            z = pyro.sample(""z"", Normal(0, 1).expand_by(x.shape))\n            pyro.sample(""x"", Normal(z, 1), obs=x)\n\n    def guide():\n        loc = pyro.param(""loc"", lambda: torch.zeros(len(data), requires_grad=True))\n        scale = pyro.param(""scale"", lambda: torch.tensor([1.0], requires_grad=True))\n        with pyro.plate(""data"", len(data), subsample_size) as ind:\n            pyro.sample(""z"", Normal(loc[ind], scale))\n\n    optim = Adam({""lr"": 0.1})\n    elbo = Elbo(num_particles=10, strict_enumeration_warning=False)\n    inference = SVI(model, guide, optim, elbo)\n    iters = num_particles // 10\n    with xfail_if_not_implemented():\n        for _ in range(iters):\n            inference.loss_and_grads(model, guide)\n\n    params = dict(pyro.get_param_store().named_parameters())\n    actual_grads = {name: param.grad.detach().cpu().numpy() / iters\n                    for name, param in params.items()}\n\n    expected_grads = {\'loc\': np.array([0.5, -2.0]), \'scale\': np.array([2.0])}\n    for name in sorted(params):\n        logger.info(\'expected {} = {}\'.format(name, expected_grads[name]))\n        logger.info(\'actual   {} = {}\'.format(name, actual_grads[name]))\n    assert_equal(actual_grads, expected_grads, prec=precision)\n'"
tests/infer/test_inference.py,87,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport math\nfrom unittest import TestCase\n\nimport pytest\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.contrib.gp.kernels as kernels\nimport pyro.distributions as dist\nimport pyro.optim as optim\nfrom pyro import poutine\nfrom pyro.distributions.testing import fakes\nfrom pyro.distributions.testing.rejection_gamma import ShapeAugmentedGamma\nfrom pyro.infer import (SVI, EnergyDistance, JitTrace_ELBO, JitTraceEnum_ELBO, JitTraceGraph_ELBO, RenyiELBO,\n                        ReweightedWakeSleep, Trace_ELBO, Trace_MMD, TraceEnum_ELBO, TraceGraph_ELBO,\n                        TraceMeanField_ELBO, TraceTailAdaptive_ELBO)\nfrom pyro.infer.autoguide import AutoDelta\nfrom pyro.infer.reparam import LatentStableReparam\nfrom pyro.infer.util import torch_item\nfrom tests.common import assert_close, assert_equal, xfail_if_not_implemented, xfail_param\n\nlogger = logging.getLogger(__name__)\n\n\ndef param_mse(name, target):\n    return torch.sum(torch.pow(target - pyro.param(name), 2.0)).item()\n\n\ndef param_abs_error(name, target):\n    return torch.sum(torch.abs(target - pyro.param(name))).item()\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_1"")\nclass NormalNormalTests(TestCase):\n\n    def setUp(self):\n        # normal-normal; known covariance\n        self.lam0 = torch.tensor([0.1, 0.1])   # precision of prior\n        self.loc0 = torch.tensor([0.0, 0.5])   # prior mean\n        # known precision of observation noise\n        self.lam = torch.tensor([6.0, 4.0])\n        self.data = torch.tensor([[-0.1, 0.3],\n                                  [0.00, 0.4],\n                                  [0.20, 0.5],\n                                  [0.10, 0.7]])\n        self.n_data = torch.tensor([float(len(self.data))])\n        self.data_sum = self.data.sum(0)\n        self.analytic_lam_n = self.lam0 + self.n_data.expand_as(self.lam) * self.lam\n        self.analytic_log_sig_n = -0.5 * torch.log(self.analytic_lam_n)\n        self.analytic_loc_n = self.data_sum * (self.lam / self.analytic_lam_n) +\\\n            self.loc0 * (self.lam0 / self.analytic_lam_n)\n        self.batch_size = 4\n        self.sample_batch_size = 2\n\n    def test_elbo_reparameterized(self):\n        self.do_elbo_test(True, 5000, Trace_ELBO())\n\n    def test_elbo_analytic_kl(self):\n        self.do_elbo_test(True, 3000, TraceMeanField_ELBO())\n\n    def test_elbo_tail_adaptive(self):\n        self.do_elbo_test(True, 3000, TraceTailAdaptive_ELBO(num_particles=10, vectorize_particles=True))\n\n    def test_elbo_nonreparameterized(self):\n        self.do_elbo_test(False, 15000, Trace_ELBO())\n\n    def test_renyi_reparameterized(self):\n        self.do_elbo_test(True, 2500, RenyiELBO(num_particles=3, vectorize_particles=False))\n\n    def test_renyi_nonreparameterized(self):\n        self.do_elbo_test(False, 7500, RenyiELBO(num_particles=3, vectorize_particles=True))\n\n    def test_rws_reparameterized(self):\n        self.do_elbo_test(True, 2500, ReweightedWakeSleep(num_particles=3))\n\n    def test_rws_nonreparameterized(self):\n        self.do_elbo_test(False, 7500, ReweightedWakeSleep(num_particles=3))\n\n    def test_mmd_vectorized(self):\n        z_size = self.loc0.shape[0]\n        self.do_fit_prior_test(\n            True, 1000, Trace_MMD(\n                kernel=kernels.RBF(\n                    z_size,\n                    lengthscale=torch.sqrt(torch.tensor(z_size, dtype=torch.float))\n                ), vectorize_particles=True, num_particles=100\n            )\n        )\n\n    def test_mmd_nonvectorized(self):\n        z_size = self.loc0.shape[0]\n        self.do_fit_prior_test(\n            True, 100, Trace_MMD(\n                kernel=kernels.RBF(\n                    z_size,\n                    lengthscale=torch.sqrt(torch.tensor(z_size, dtype=torch.float))\n                ), vectorize_particles=False, num_particles=100\n            ), lr=0.0146\n        )\n\n    def do_elbo_test(self, reparameterized, n_steps, loss):\n        pyro.clear_param_store()\n\n        def model():\n            loc_latent = pyro.sample(""loc_latent"",\n                                     dist.Normal(self.loc0, torch.pow(self.lam0, -0.5))\n                                     .to_event(1))\n            with pyro.plate(\'data\', self.batch_size):\n                pyro.sample(""obs"",\n                            dist.Normal(loc_latent, torch.pow(self.lam, -0.5)).to_event(1),\n                            obs=self.data)\n            return loc_latent\n\n        def guide():\n            loc_q = pyro.param(""loc_q"", self.analytic_loc_n.detach() + 0.134)\n            log_sig_q = pyro.param(""log_sig_q"", self.analytic_log_sig_n.data.detach() - 0.14)\n            sig_q = torch.exp(log_sig_q)\n            Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n            pyro.sample(""loc_latent"", Normal(loc_q, sig_q).to_event(1))\n\n        adam = optim.Adam({""lr"": .001})\n        svi = SVI(model, guide, adam, loss=loss)\n\n        for k in range(n_steps):\n            svi.step()\n\n            loc_error = param_mse(""loc_q"", self.analytic_loc_n)\n            log_sig_error = param_mse(""log_sig_q"", self.analytic_log_sig_n)\n\n        assert_equal(0.0, loc_error, prec=0.05)\n        assert_equal(0.0, log_sig_error, prec=0.05)\n\n    def do_fit_prior_test(self, reparameterized, n_steps, loss, debug=False, lr=0.001):\n        pyro.clear_param_store()\n\n        def model():\n            with pyro.plate(\'samples\', self.sample_batch_size):\n                pyro.sample(\n                    ""loc_latent"", dist.Normal(\n                        torch.stack([self.loc0]*self.sample_batch_size, dim=0),\n                        torch.stack([torch.pow(self.lam0, -0.5)]*self.sample_batch_size, dim=0)\n                    ).to_event(1)\n                )\n\n        def guide():\n            loc_q = pyro.param(""loc_q"", self.loc0.detach() + 0.134)\n            log_sig_q = pyro.param(""log_sig_q"", -0.5*torch.log(self.lam0).data.detach() - 0.14)\n            sig_q = torch.exp(log_sig_q)\n            Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n            with pyro.plate(\'samples\', self.sample_batch_size):\n                pyro.sample(\n                    ""loc_latent"", Normal(\n                        torch.stack([loc_q]*self.sample_batch_size, dim=0),\n                        torch.stack([sig_q]*self.sample_batch_size, dim=0)\n                    ).to_event(1)\n                )\n\n        adam = optim.Adam({""lr"": lr})\n        svi = SVI(model, guide, adam, loss=loss)\n\n        alpha = 0.99\n        for k in range(n_steps):\n            svi.step()\n            if debug:\n                loc_error = param_mse(""loc_q"", self.loc0)\n                log_sig_error = param_mse(""log_sig_q"", -0.5*torch.log(self.lam0))\n                with torch.no_grad():\n                    if k == 0:\n                        avg_loglikelihood, avg_penalty = loss._differentiable_loss_parts(model, guide)\n                        avg_loglikelihood = torch_item(avg_loglikelihood)\n                        avg_penalty = torch_item(avg_penalty)\n                    loglikelihood, penalty = loss._differentiable_loss_parts(model, guide)\n                    avg_loglikelihood = alpha * avg_loglikelihood + (1-alpha) * torch_item(loglikelihood)\n                    avg_penalty = alpha * avg_penalty + (1-alpha) * torch_item(penalty)\n                if k % 100 == 0:\n                    print(loc_error, log_sig_error)\n                    print(avg_loglikelihood, avg_penalty)\n                    print()\n\n        loc_error = param_mse(""loc_q"", self.loc0)\n        log_sig_error = param_mse(""log_sig_q"", -0.5 * torch.log(self.lam0))\n        assert_equal(0.0, loc_error, prec=0.05)\n        assert_equal(0.0, log_sig_error, prec=0.05)\n\n\nclass TestFixedModelGuide(TestCase):\n    def setUp(self):\n        self.data = torch.tensor([2.0])\n        self.alpha_q_log_0 = 0.17 * torch.ones(1)\n        self.beta_q_log_0 = 0.19 * torch.ones(1)\n        self.alpha_p_log_0 = 0.11 * torch.ones(1)\n        self.beta_p_log_0 = 0.13 * torch.ones(1)\n\n    def do_test_fixedness(self, fixed_parts):\n        pyro.clear_param_store()\n\n        def model():\n            alpha_p_log = pyro.param(\n                ""alpha_p_log"", self.alpha_p_log_0.clone())\n            beta_p_log = pyro.param(\n                ""beta_p_log"", self.beta_p_log_0.clone())\n            alpha_p, beta_p = torch.exp(alpha_p_log), torch.exp(beta_p_log)\n            lambda_latent = pyro.sample(""lambda_latent"", dist.Gamma(alpha_p, beta_p))\n            pyro.sample(""obs"", dist.Poisson(lambda_latent), obs=self.data)\n            return lambda_latent\n\n        def guide():\n            alpha_q_log = pyro.param(\n                ""alpha_q_log"", self.alpha_q_log_0.clone())\n            beta_q_log = pyro.param(\n                ""beta_q_log"", self.beta_q_log_0.clone())\n            alpha_q, beta_q = torch.exp(alpha_q_log), torch.exp(beta_q_log)\n            pyro.sample(""lambda_latent"", dist.Gamma(alpha_q, beta_q))\n\n        def per_param_args(module_name, param_name):\n            if \'model\' in fixed_parts and \'p_\' in param_name:\n                return {\'lr\': 0.0}\n            if \'guide\' in fixed_parts and \'q_\' in param_name:\n                return {\'lr\': 0.0}\n            return {\'lr\': 0.01}\n\n        adam = optim.Adam(per_param_args)\n        svi = SVI(model, guide, adam, loss=Trace_ELBO())\n\n        for _ in range(3):\n            svi.step()\n\n        model_unchanged = (torch.equal(pyro.param(""alpha_p_log"").data, self.alpha_p_log_0)) and\\\n                          (torch.equal(pyro.param(""beta_p_log"").data, self.beta_p_log_0))\n        guide_unchanged = (torch.equal(pyro.param(""alpha_q_log"").data, self.alpha_q_log_0)) and\\\n                          (torch.equal(pyro.param(""beta_q_log"").data, self.beta_q_log_0))\n        model_changed = not model_unchanged\n        guide_changed = not guide_unchanged\n        error = (\'model\' in fixed_parts and model_changed) or (\'guide\' in fixed_parts and guide_changed)\n        return (not error)\n\n    def test_model_fixed(self):\n        assert self.do_test_fixedness(fixed_parts=[""model""])\n\n    def test_guide_fixed(self):\n        assert self.do_test_fixedness(fixed_parts=[""guide""])\n\n    def test_guide_and_model_both_fixed(self):\n        assert self.do_test_fixedness(fixed_parts=[""model"", ""guide""])\n\n    def test_guide_and_model_free(self):\n        assert self.do_test_fixedness(fixed_parts=[""bogus_tag""])\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_2"")\nclass PoissonGammaTests(TestCase):\n    def setUp(self):\n        # poisson-gamma model\n        # gamma prior hyperparameter\n        self.alpha0 = torch.tensor(1.0)\n        # gamma prior hyperparameter\n        self.beta0 = torch.tensor(1.0)\n        self.data = torch.tensor([1.0, 2.0, 3.0])\n        self.n_data = len(self.data)\n        data_sum = self.data.sum(0)\n        self.alpha_n = self.alpha0 + data_sum  # posterior alpha\n        self.beta_n = self.beta0 + torch.tensor(float(self.n_data))  # posterior beta\n        self.sample_batch_size = 2\n\n    def test_elbo_reparameterized(self):\n        self.do_elbo_test(True, 10000, Trace_ELBO())\n\n    def test_elbo_nonreparameterized(self):\n        self.do_elbo_test(False, 25000, Trace_ELBO())\n\n    def test_renyi_reparameterized(self):\n        self.do_elbo_test(True, 5000, RenyiELBO(num_particles=2))\n\n    def test_renyi_nonreparameterized(self):\n        self.do_elbo_test(False, 12500, RenyiELBO(alpha=0.2, num_particles=2))\n\n    def test_rws_reparameterized(self):\n        self.do_elbo_test(True, 5000, ReweightedWakeSleep(num_particles=2))\n\n    def test_rws_nonreparameterized(self):\n        self.do_elbo_test(False, 12500, ReweightedWakeSleep(num_particles=2))\n\n    def test_mmd_vectorized(self):\n        z_size = 1\n        self.do_fit_prior_test(\n            True, 500, Trace_MMD(\n                kernel=kernels.RBF(\n                    z_size,\n                    lengthscale=torch.sqrt(torch.tensor(z_size, dtype=torch.float))\n                ), vectorize_particles=True, num_particles=100\n            ), debug=True, lr=0.09\n        )\n\n    def do_elbo_test(self, reparameterized, n_steps, loss):\n        pyro.clear_param_store()\n        Gamma = dist.Gamma if reparameterized else fakes.NonreparameterizedGamma\n\n        def model():\n            lambda_latent = pyro.sample(""lambda_latent"", Gamma(self.alpha0, self.beta0))\n            with pyro.plate(""data"", self.n_data):\n                pyro.sample(""obs"", dist.Poisson(lambda_latent), obs=self.data)\n            return lambda_latent\n\n        def guide():\n            alpha_q = pyro.param(""alpha_q"", self.alpha_n.detach() + math.exp(0.17),\n                                 constraint=constraints.positive)\n            beta_q = pyro.param(""beta_q"", self.beta_n.detach() / math.exp(0.143),\n                                constraint=constraints.positive)\n            pyro.sample(""lambda_latent"", Gamma(alpha_q, beta_q))\n\n        adam = optim.Adam({""lr"": .0002, ""betas"": (0.97, 0.999)})\n        svi = SVI(model, guide, adam, loss)\n\n        for k in range(n_steps):\n            svi.step()\n\n        assert_equal(pyro.param(""alpha_q""), self.alpha_n, prec=0.2, msg=\'{} vs {}\'.format(\n            pyro.param(""alpha_q"").detach().cpu().numpy(), self.alpha_n.detach().cpu().numpy()))\n        assert_equal(pyro.param(""beta_q""), self.beta_n, prec=0.15, msg=\'{} vs {}\'.format(\n            pyro.param(""beta_q"").detach().cpu().numpy(), self.beta_n.detach().cpu().numpy()))\n\n    def do_fit_prior_test(self, reparameterized, n_steps, loss, debug=False, lr=0.0002):\n        pyro.clear_param_store()\n        Gamma = dist.Gamma if reparameterized else fakes.NonreparameterizedGamma\n\n        def model():\n            with pyro.plate(\'samples\', self.sample_batch_size):\n                pyro.sample(\n                    ""lambda_latent"", Gamma(\n                        torch.stack([torch.stack([self.alpha0])]*self.sample_batch_size),\n                        torch.stack([torch.stack([self.beta0])]*self.sample_batch_size)\n                    ).to_event(1)\n                )\n\n        def guide():\n            alpha_q = pyro.param(""alpha_q"", self.alpha0.detach() + math.exp(0.17),\n                                 constraint=constraints.positive)\n            beta_q = pyro.param(""beta_q"", self.beta0.detach() / math.exp(0.143),\n                                constraint=constraints.positive)\n            with pyro.plate(\'samples\', self.sample_batch_size):\n                pyro.sample(\n                    ""lambda_latent"", Gamma(\n                        torch.stack([torch.stack([alpha_q])]*self.sample_batch_size),\n                        torch.stack([torch.stack([beta_q])]*self.sample_batch_size)\n                    ).to_event(1)\n                )\n\n        adam = optim.Adam({""lr"": lr, ""betas"": (0.97, 0.999)})\n        svi = SVI(model, guide, adam, loss)\n\n        alpha = 0.99\n        for k in range(n_steps):\n            svi.step()\n            if debug:\n                alpha_error = param_mse(""alpha_q"", self.alpha0)\n                beta_error = param_mse(""beta_q"", self.beta0)\n                with torch.no_grad():\n                    if k == 0:\n                        avg_loglikelihood, avg_penalty = loss._differentiable_loss_parts(model, guide, (), {})\n                        avg_loglikelihood = torch_item(avg_loglikelihood)\n                        avg_penalty = torch_item(avg_penalty)\n                    loglikelihood, penalty = loss._differentiable_loss_parts(model, guide, (), {})\n                    avg_loglikelihood = alpha * avg_loglikelihood + (1-alpha) * torch_item(loglikelihood)\n                    avg_penalty = alpha * avg_penalty + (1-alpha) * torch_item(penalty)\n                if k % 100 == 0:\n                    print(alpha_error, beta_error)\n                    print(avg_loglikelihood, avg_penalty)\n                    print()\n\n        assert_equal(pyro.param(""alpha_q""), self.alpha0, prec=0.2, msg=\'{} vs {}\'.format(\n            pyro.param(""alpha_q"").detach().cpu().numpy(), self.alpha0.detach().cpu().numpy()))\n        assert_equal(pyro.param(""beta_q""), self.beta0, prec=0.15, msg=\'{} vs {}\'.format(\n            pyro.param(""beta_q"").detach().cpu().numpy(), self.beta0.detach().cpu().numpy()))\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_1"")\n@pytest.mark.parametrize(\'elbo_impl\', [\n    xfail_param(JitTrace_ELBO, reason=""incorrect gradients"", run=False),\n    xfail_param(JitTraceGraph_ELBO, reason=""incorrect gradients"", run=False),\n    xfail_param(JitTraceEnum_ELBO, reason=""incorrect gradients"", run=False),\n    Trace_ELBO,\n    TraceGraph_ELBO,\n    TraceEnum_ELBO,\n    RenyiELBO,\n    ReweightedWakeSleep\n])\n@pytest.mark.parametrize(\'gamma_dist,n_steps\', [\n    (dist.Gamma, 5000),\n    (fakes.NonreparameterizedGamma, 10000),\n    (ShapeAugmentedGamma, 5000),\n], ids=[\'reparam\', \'nonreparam\', \'rsvi\'])\ndef test_exponential_gamma(gamma_dist, n_steps, elbo_impl):\n    pyro.clear_param_store()\n\n    # gamma prior hyperparameter\n    alpha0 = torch.tensor(1.0)\n    # gamma prior hyperparameter\n    beta0 = torch.tensor(1.0)\n    n_data = 2\n    data = torch.tensor([3.0, 2.0])  # two observations\n    alpha_n = alpha0 + torch.tensor(float(n_data))  # posterior alpha\n    beta_n = beta0 + torch.sum(data)  # posterior beta\n    prec = 0.2 if gamma_dist.has_rsample else 0.25\n\n    def model(alpha0, beta0, alpha_n, beta_n):\n        lambda_latent = pyro.sample(""lambda_latent"", gamma_dist(alpha0, beta0))\n        with pyro.plate(""data"", n_data):\n            pyro.sample(""obs"", dist.Exponential(lambda_latent), obs=data)\n        return lambda_latent\n\n    def guide(alpha0, beta0, alpha_n, beta_n):\n        alpha_q = pyro.param(""alpha_q"", alpha_n * math.exp(0.17), constraint=constraints.positive)\n        beta_q = pyro.param(""beta_q"", beta_n / math.exp(0.143), constraint=constraints.positive)\n        pyro.sample(""lambda_latent"", gamma_dist(alpha_q, beta_q))\n\n    adam = optim.Adam({""lr"": .0003, ""betas"": (0.97, 0.999)})\n    if elbo_impl is RenyiELBO:\n        elbo = elbo_impl(alpha=0.2, num_particles=3, max_plate_nesting=1, strict_enumeration_warning=False)\n    elif elbo_impl is ReweightedWakeSleep:\n        if gamma_dist is ShapeAugmentedGamma:\n            pytest.xfail(reason=""ShapeAugmentedGamma not suported for ReweightedWakeSleep"")\n        else:\n            elbo = elbo_impl(num_particles=3, max_plate_nesting=1, strict_enumeration_warning=False)\n    else:\n        elbo = elbo_impl(max_plate_nesting=1, strict_enumeration_warning=False)\n    svi = SVI(model, guide, adam, loss=elbo)\n\n    with xfail_if_not_implemented():\n        for k in range(n_steps):\n            svi.step(alpha0, beta0, alpha_n, beta_n)\n\n    assert_equal(pyro.param(""alpha_q""), alpha_n, prec=prec, msg=\'{} vs {}\'.format(\n        pyro.param(""alpha_q"").detach().cpu().numpy(), alpha_n.detach().cpu().numpy()))\n    assert_equal(pyro.param(""beta_q""), beta_n, prec=prec, msg=\'{} vs {}\'.format(\n        pyro.param(""beta_q"").detach().cpu().numpy(), beta_n.detach().cpu().numpy()))\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_2"")\nclass BernoulliBetaTests(TestCase):\n    def setUp(self):\n        # bernoulli-beta model\n        # beta prior hyperparameter\n        self.alpha0 = torch.tensor(1.0)\n        self.beta0 = torch.tensor(1.0)  # beta prior hyperparameter\n        self.data = torch.tensor([0.0, 1.0, 1.0, 1.0])\n        self.n_data = len(self.data)\n        self.batch_size = 4\n        data_sum = self.data.sum()\n        self.alpha_n = self.alpha0 + data_sum  # posterior alpha\n        self.beta_n = self.beta0 - data_sum + torch.tensor(float(self.n_data))\n        # posterior beta\n        self.log_alpha_n = torch.log(self.alpha_n)\n        self.log_beta_n = torch.log(self.beta_n)\n        self.sample_batch_size = 2\n\n    def test_elbo_reparameterized(self):\n        self.do_elbo_test(True, 10000, Trace_ELBO())\n\n    def test_elbo_nonreparameterized(self):\n        self.do_elbo_test(False, 10000, Trace_ELBO())\n\n    # this is used to detect bugs related to https://github.com/pytorch/pytorch/issues/9521\n    def test_elbo_reparameterized_vectorized(self):\n        self.do_elbo_test(True, 5000, Trace_ELBO(num_particles=2, vectorize_particles=True,\n                                                 max_plate_nesting=1))\n\n    # this is used to detect bugs related to https://github.com/pytorch/pytorch/issues/9521\n    def test_elbo_nonreparameterized_vectorized(self):\n        self.do_elbo_test(False, 5000, Trace_ELBO(num_particles=2, vectorize_particles=True,\n                                                  max_plate_nesting=1))\n\n    def test_renyi_reparameterized(self):\n        self.do_elbo_test(True, 5000, RenyiELBO(num_particles=2))\n\n    def test_renyi_nonreparameterized(self):\n        self.do_elbo_test(False, 5000, RenyiELBO(alpha=0.2, num_particles=2))\n\n    def test_renyi_reparameterized_vectorized(self):\n        self.do_elbo_test(True, 5000, RenyiELBO(num_particles=2, vectorize_particles=True,\n                                                max_plate_nesting=1))\n\n    def test_renyi_nonreparameterized_vectorized(self):\n        self.do_elbo_test(False, 5000, RenyiELBO(alpha=0.2, num_particles=2, vectorize_particles=True,\n                                                 max_plate_nesting=1))\n\n    def test_rws_reparameterized(self):\n        self.do_elbo_test(True, 5000, ReweightedWakeSleep(num_particles=2))\n\n    def test_rws_nonreparameterized(self):\n        self.do_elbo_test(False, 5000, ReweightedWakeSleep(num_particles=2))\n\n    def test_rws_reparameterized_vectorized(self):\n        self.do_elbo_test(True, 5000, ReweightedWakeSleep(num_particles=2, vectorize_particles=True,\n                                                          max_plate_nesting=1))\n\n    def test_rws_nonreparameterized_vectorized(self):\n        self.do_elbo_test(False, 5000, ReweightedWakeSleep(num_particles=2, vectorize_particles=True,\n                                                           max_plate_nesting=1))\n\n    def test_mmd_vectorized(self):\n        z_size = 1\n        self.do_fit_prior_test(\n            True, 2500, Trace_MMD(\n                kernel=kernels.RBF(\n                    z_size,\n                    lengthscale=torch.sqrt(torch.tensor(z_size, dtype=torch.float))\n                ), vectorize_particles=True, num_particles=100\n            )\n        )\n\n    def do_elbo_test(self, reparameterized, n_steps, loss):\n        pyro.clear_param_store()\n        Beta = dist.Beta if reparameterized else fakes.NonreparameterizedBeta\n\n        def model():\n            p_latent = pyro.sample(""p_latent"", Beta(self.alpha0, self.beta0))\n            with pyro.plate(""data"", self.batch_size):\n                pyro.sample(""obs"", dist.Bernoulli(p_latent), obs=self.data)\n            return p_latent\n\n        def guide():\n            alpha_q_log = pyro.param(""alpha_q_log"",\n                                     self.log_alpha_n + 0.17)\n            beta_q_log = pyro.param(""beta_q_log"",\n                                    self.log_beta_n - 0.143)\n            alpha_q, beta_q = torch.exp(alpha_q_log), torch.exp(beta_q_log)\n            pyro.sample(""p_latent"", Beta(alpha_q, beta_q))\n\n        adam = optim.Adam({""lr"": .001, ""betas"": (0.97, 0.999)})\n        svi = SVI(model, guide, adam, loss=loss)\n\n        for k in range(n_steps):\n            svi.step()\n\n        alpha_error = param_abs_error(""alpha_q_log"", self.log_alpha_n)\n        beta_error = param_abs_error(""beta_q_log"", self.log_beta_n)\n        assert_equal(0.0, alpha_error, prec=0.08)\n        assert_equal(0.0, beta_error, prec=0.08)\n\n    def do_fit_prior_test(self, reparameterized, n_steps, loss, debug=False):\n        pyro.clear_param_store()\n        Beta = dist.Beta if reparameterized else fakes.NonreparameterizedBeta\n\n        def model():\n            with pyro.plate(\'samples\', self.sample_batch_size):\n                pyro.sample(\n                    ""p_latent"", Beta(\n                        torch.stack([torch.stack([self.alpha0])]*self.sample_batch_size),\n                        torch.stack([torch.stack([self.beta0])]*self.sample_batch_size)\n                    ).to_event(1)\n                )\n\n        def guide():\n            alpha_q_log = pyro.param(""alpha_q_log"",\n                                     torch.log(self.alpha0) + 0.17)\n            beta_q_log = pyro.param(""beta_q_log"",\n                                    torch.log(self.beta0) - 0.143)\n            alpha_q, beta_q = torch.exp(alpha_q_log), torch.exp(beta_q_log)\n            with pyro.plate(\'samples\', self.sample_batch_size):\n                pyro.sample(\n                    ""p_latent"", Beta(\n                        torch.stack([torch.stack([alpha_q])]*self.sample_batch_size),\n                        torch.stack([torch.stack([beta_q])]*self.sample_batch_size)\n                    ).to_event(1)\n                )\n\n        adam = optim.Adam({""lr"": .001, ""betas"": (0.97, 0.999)})\n        svi = SVI(model, guide, adam, loss=loss)\n\n        alpha = 0.99\n        for k in range(n_steps):\n            svi.step()\n            if debug:\n                alpha_error = param_abs_error(""alpha_q_log"", torch.log(self.alpha0))\n                beta_error = param_abs_error(""beta_q_log"", torch.log(self.beta0))\n                with torch.no_grad():\n                    if k == 0:\n                        avg_loglikelihood, avg_penalty = loss._differentiable_loss_parts(model, guide)\n                        avg_loglikelihood = torch_item(avg_loglikelihood)\n                        avg_penalty = torch_item(avg_penalty)\n                    loglikelihood, penalty = loss._differentiable_loss_parts(model, guide)\n                    avg_loglikelihood = alpha * avg_loglikelihood + (1-alpha) * torch_item(loglikelihood)\n                    avg_penalty = alpha * avg_penalty + (1-alpha) * torch_item(penalty)\n                if k % 100 == 0:\n                    print(alpha_error, beta_error)\n                    print(avg_loglikelihood, avg_penalty)\n                    print()\n\n        alpha_error = param_abs_error(""alpha_q_log"", torch.log(self.alpha0))\n        beta_error = param_abs_error(""beta_q_log"", torch.log(self.beta0))\n        assert_equal(0.0, alpha_error, prec=0.08)\n        assert_equal(0.0, beta_error, prec=0.08)\n\n\nclass SafetyTests(TestCase):\n\n    def setUp(self):\n        # normal-normal; known covariance\n        def model_dup():\n            pyro.param(""loc_q"", torch.ones(1, requires_grad=True))\n            pyro.sample(""loc_q"", dist.Normal(torch.zeros(1), torch.ones(1)))\n\n        def model_obs_dup():\n            pyro.sample(""loc_q"", dist.Normal(torch.zeros(1), torch.ones(1)))\n            pyro.sample(""loc_q"", dist.Normal(torch.zeros(1), torch.ones(1)), obs=torch.zeros(1))\n\n        def model():\n            pyro.sample(""loc_q"", dist.Normal(torch.zeros(1), torch.ones(1)))\n\n        def guide():\n            p = pyro.param(""p"", torch.ones(1, requires_grad=True))\n            pyro.sample(""loc_q"", dist.Normal(torch.zeros(1), p))\n            pyro.sample(""loc_q_2"", dist.Normal(torch.zeros(1), p))\n\n        self.duplicate_model = model_dup\n        self.duplicate_obs = model_obs_dup\n        self.model = model\n        self.guide = guide\n\n    def test_duplicate_names(self):\n        pyro.clear_param_store()\n\n        adam = optim.Adam({""lr"": .001})\n        svi = SVI(self.duplicate_model, self.guide, adam, loss=Trace_ELBO())\n\n        with pytest.raises(RuntimeError):\n            svi.step()\n\n    def test_extra_samples(self):\n        pyro.clear_param_store()\n\n        adam = optim.Adam({""lr"": .001})\n        svi = SVI(self.model, self.guide, adam, loss=Trace_ELBO())\n\n        with pytest.warns(Warning):\n            svi.step()\n\n    def test_duplicate_obs_name(self):\n        pyro.clear_param_store()\n\n        adam = optim.Adam({""lr"": .001})\n        svi = SVI(self.duplicate_obs, self.guide, adam, loss=Trace_ELBO())\n\n        with pytest.raises(RuntimeError):\n            svi.step()\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_1"")\n@pytest.mark.parametrize(""prior_scale"", [0, 1e-4])\ndef test_energy_distance_univariate(prior_scale):\n\n    def model(data):\n        loc = pyro.sample(""loc"", dist.Normal(0, 100))\n        scale = pyro.sample(""scale"", dist.LogNormal(0, 1))\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", dist.Normal(loc, scale), obs=data)\n\n    def guide(data):\n        loc_loc = pyro.param(""loc_loc"", torch.tensor(0.))\n        loc_scale = pyro.param(""loc_scale"", torch.tensor(1.),\n                               constraint=constraints.positive)\n        log_scale_loc = pyro.param(""log_scale_loc"", torch.tensor(0.))\n        log_scale_scale = pyro.param(""log_scale_scale"", torch.tensor(1.),\n                                     constraint=constraints.positive)\n        pyro.sample(""loc"", dist.Normal(loc_loc, loc_scale))\n        pyro.sample(""scale"", dist.LogNormal(log_scale_loc, log_scale_scale))\n\n    data = 10.0 + torch.randn(8)\n    adam = optim.Adam({""lr"": 0.1})\n    loss_fn = EnergyDistance(num_particles=32, prior_scale=prior_scale)\n    svi = SVI(model, guide, adam, loss_fn)\n    for step in range(2001):\n        loss = svi.step(data)\n        if step % 20 == 0:\n            logger.info(""step {} loss = {:0.4g}, loc = {:0.4g}, scale = {:0.4g}""\n                        .format(step, loss, pyro.param(""loc_loc"").item(),\n                                pyro.param(""log_scale_loc"").exp().item()))\n\n    expected_loc = data.mean()\n    expected_scale = data.std()\n    actual_loc = pyro.param(""loc_loc"").detach()\n    actual_scale = pyro.param(""log_scale_loc"").exp().detach()\n    assert_close(actual_loc, expected_loc, atol=0.05)\n    assert_close(actual_scale, expected_scale, rtol=0.1 if prior_scale else 0.05)\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_1"")\n@pytest.mark.parametrize(""prior_scale"", [0, 1])\ndef test_energy_distance_multivariate(prior_scale):\n\n    def model(data):\n        loc = torch.zeros(2)\n        cov = pyro.sample(""cov"", dist.Normal(0, 100).expand([2, 2]).to_event(2))\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", dist.MultivariateNormal(loc, cov), obs=data)\n\n    def guide(data):\n        scale_tril = pyro.param(""scale_tril"", torch.eye(2),\n                                constraint=constraints.lower_cholesky)\n        pyro.sample(""cov"", dist.Delta(scale_tril @ scale_tril.t(), event_dim=2))\n\n    cov = torch.tensor([[1, 0.8], [0.8, 1]])\n    data = dist.MultivariateNormal(torch.zeros(2), cov).sample([10])\n    loss_fn = EnergyDistance(num_particles=32, prior_scale=prior_scale)\n    svi = SVI(model, guide, optim.Adam({""lr"": 0.1}), loss_fn)\n    for step in range(2001):\n        loss = svi.step(data)\n        if step % 20 == 0:\n            logger.info(""step {} loss = {:0.4g}"".format(step, loss))\n\n    delta = data - data.mean(0)\n    expected_cov = (delta.t() @ delta) / len(data)\n    scale_tril = pyro.param(""scale_tril"").detach()\n    actual_cov = scale_tril @ scale_tril.t()\n    assert_close(actual_cov, expected_cov, atol=0.2)\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_1"")\ndef test_reparam_stable():\n    data = dist.Poisson(torch.randn(8).exp()).sample()\n\n    @poutine.reparam(config={""dz"": LatentStableReparam(), ""y"": LatentStableReparam()})\n    def model():\n        stability = pyro.sample(""stability"", dist.Uniform(1., 2.))\n        trans_skew = pyro.sample(""trans_skew"", dist.Uniform(-1., 1.))\n        obs_skew = pyro.sample(""obs_skew"", dist.Uniform(-1., 1.))\n        scale = pyro.sample(""scale"", dist.Gamma(3, 1))\n\n        # We use separate plates because the .cumsum() op breaks independence.\n        with pyro.plate(""time1"", len(data)):\n            dz = pyro.sample(""dz"", dist.Stable(stability, trans_skew))\n        z = dz.cumsum(-1)\n        with pyro.plate(""time2"", len(data)):\n            y = pyro.sample(""y"", dist.Stable(stability, obs_skew, scale, z))\n            pyro.sample(""x"", dist.Poisson(y.abs()), obs=data)\n\n    guide = AutoDelta(model)\n    svi = SVI(model, guide, optim.Adam({""lr"": 0.01}), Trace_ELBO())\n    for step in range(100):\n        loss = svi.step()\n        if step % 20 == 0:\n            logger.info(""step {} loss = {:0.4g}"".format(step, loss))\n'"
tests/infer/test_initialization.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.autoguide.initialization import InitMessenger, init_to_generated, init_to_value\n\n\ndef test_init_to_generated():\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0, 1))\n        y = pyro.sample(""y"", dist.Normal(0, 1))\n        z = pyro.sample(""z"", dist.Normal(0, 1))\n        return x, y, z\n\n    class MockGenerate:\n        def __init__(self):\n            self.counter = 0\n\n        def __call__(self):\n            values = {""x"": torch.tensor(self.counter + 0.0),\n                      ""y"": torch.tensor(self.counter + 0.5)}\n            self.counter += 1\n            return init_to_value(values=values)\n\n    mock_generate = MockGenerate()\n    with InitMessenger(init_to_generated(generate=mock_generate)):\n        for i in range(5):\n            x, y, z = model()\n            assert x == i\n            assert y == i + 0.5\n    assert mock_generate.counter == 5\n'"
tests/infer/test_jit.py,81,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport warnings\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\nfrom torch.distributions import constraints, kl_divergence\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.ops.jit\nimport pyro.poutine as poutine\nfrom pyro.distributions.util import scale_and_mask\nfrom pyro.infer import (SVI, JitTrace_ELBO, JitTraceEnum_ELBO, JitTraceGraph_ELBO, JitTraceMeanField_ELBO, Trace_ELBO,\n                        TraceEnum_ELBO, TraceGraph_ELBO, TraceMeanField_ELBO, infer_discrete)\nfrom pyro.optim import Adam\nfrom pyro.poutine.indep_messenger import CondIndepStackFrame\nfrom pyro.util import ignore_jit_warnings\nfrom tests.common import assert_equal\n\n\ndef constant(*args, **kwargs):\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n        return torch.tensor(*args, **kwargs)\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_simple():\n    y = torch.ones(2)\n\n    def f(x):\n        logger.debug(\'Inside f\')\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n            assert x is y\n        return y + 1.0\n\n    logger.debug(\'Compiling f\')\n    f = torch.jit.trace(f, (y,), check_trace=False)\n    logger.debug(\'Calling f(y)\')\n    assert_equal(f(y), torch.tensor([2., 2.]))\n    logger.debug(\'Calling f(y)\')\n    assert_equal(f(y), torch.tensor([2., 2.]))\n    logger.debug(\'Calling f(torch.zeros(2))\')\n    assert_equal(f(torch.zeros(2)), torch.tensor([1., 1.]))\n    logger.debug(\'Calling f(torch.zeros(5))\')\n    assert_equal(f(torch.ones(5)), torch.tensor([2., 2., 2., 2., 2.]))\n\n\ndef test_multi_output():\n    y = torch.ones(2)\n\n    def f(x):\n        logger.debug(\'Inside f\')\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n            assert x is y\n        return y - 1.0, y + 1.0\n\n    logger.debug(\'Compiling f\')\n    f = torch.jit.trace(f, (y,), check_trace=False)\n    logger.debug(\'Calling f(y)\')\n    assert_equal(f(y)[1], torch.tensor([2., 2.]))\n    logger.debug(\'Calling f(y)\')\n    assert_equal(f(y)[1], torch.tensor([2., 2.]))\n    logger.debug(\'Calling f(torch.zeros(2))\')\n    assert_equal(f(torch.zeros(2))[1], torch.tensor([1., 1.]))\n    logger.debug(\'Calling f(torch.zeros(5))\')\n    assert_equal(f(torch.ones(5))[1], torch.tensor([2., 2., 2., 2., 2.]))\n\n\ndef test_backward():\n    y = torch.ones(2, requires_grad=True)\n\n    def f(x):\n        logger.debug(\'Inside f\')\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n            assert x is y\n        return (y + 1.0).sum()\n\n    logger.debug(\'Compiling f\')\n    f = torch.jit.trace(f, (y,), check_trace=False)\n    logger.debug(\'Calling f(y)\')\n    f(y).backward()\n    logger.debug(\'Calling f(y)\')\n    f(y)\n    logger.debug(\'Calling f(torch.zeros(2))\')\n    f(torch.zeros(2, requires_grad=True))\n    logger.debug(\'Calling f(torch.zeros(5))\')\n    f(torch.ones(5, requires_grad=True))\n\n\n@pytest.mark.xfail(reason=""grad cannot appear in jitted code"")\ndef test_grad():\n\n    def f(x, y):\n        logger.debug(\'Inside f\')\n        loss = (x - y).pow(2).sum()\n        return torch.autograd.grad(loss, [x, y], allow_unused=True)\n\n    logger.debug(\'Compiling f\')\n    f = torch.jit.trace(f, (torch.zeros(2, requires_grad=True), torch.ones(2, requires_grad=True)))\n    logger.debug(\'Invoking f\')\n    f(torch.zeros(2, requires_grad=True), torch.ones(2, requires_grad=True))\n    logger.debug(\'Invoking f\')\n    f(torch.zeros(2, requires_grad=True), torch.zeros(2, requires_grad=True))\n\n\n@pytest.mark.xfail(reason=""grad cannot appear in jitted code"")\ndef test_grad_expand():\n\n    def f(x, y):\n        logger.debug(\'Inside f\')\n        loss = (x - y).pow(2).sum()\n        return torch.autograd.grad(loss, [x, y], allow_unused=True)\n\n    logger.debug(\'Compiling f\')\n    f = torch.jit.trace(f, (torch.zeros(2, requires_grad=True), torch.ones(1, requires_grad=True)))\n    logger.debug(\'Invoking f\')\n    f(torch.zeros(2, requires_grad=True), torch.ones(1, requires_grad=True))\n    logger.debug(\'Invoking f\')\n    f(torch.zeros(2, requires_grad=True), torch.zeros(1, requires_grad=True))\n\n\ndef test_scale_and_mask():\n    def f(tensor, scale, mask): return scale_and_mask(tensor, scale=scale, mask=mask)\n\n    x = torch.tensor([-float(\'inf\'), -1., 0., 1., float(\'inf\')])\n    y = x / x.unsqueeze(-1)\n    mask = y == y\n    scale = torch.ones(y.shape)\n    jit_f = torch.jit.trace(f, (y, scale, mask))\n    assert_equal(jit_f(y, scale, mask), f(y, scale, mask))\n\n    mask = torch.tensor([True])\n    y = torch.tensor([1.5, 2.5, 3.5, 4.5, 5.5, 6.5])\n    scale = torch.ones(y.shape)\n    assert_equal(jit_f(y, scale, mask), f(y, scale, mask))\n\n\ndef test_masked_fill():\n\n    def f(y, mask):\n        return y.clone().masked_fill_(mask, 0.)\n\n    x = torch.tensor([-float(\'inf\'), -1., 0., 1., float(\'inf\')])\n    y = x / x.unsqueeze(-1)\n    mask = ~(y == y)\n    jit_f = torch.jit.trace(f, (y, mask))\n    assert_equal(jit_f(y, mask), f(y, mask))\n\n    mask = torch.tensor([True, False, False, True, False, False])\n    y = torch.tensor([1.5, 2.5, 3.5, 4.5, 5.5, 6.5])\n    assert_equal(jit_f(y, mask), f(y, mask))\n\n\n@pytest.mark.xfail(reason=""https://github.com/pytorch/pytorch/issues/11614"")\ndef test_scatter():\n\n    def make_one_hot(x, i):\n        return torch.zeros_like(x).scatter(-1, i.unsqueeze(-1), 1.0)\n\n    x = torch.randn(5, 4, 3)\n    i = torch.randint(0, 3, torch.Size((5, 4)))\n    torch.jit.trace(make_one_hot, (x, i))\n\n\n@pytest.mark.filterwarnings(\'ignore:Converting a tensor to a Python integer\')\ndef test_scatter_workaround():\n\n    def make_one_hot_expected(x, i):\n        return torch.zeros_like(x).scatter(-1, i.unsqueeze(-1), 1.0)\n\n    def make_one_hot_actual(x, i):\n        eye = torch.eye(x.shape[-1], dtype=x.dtype, device=x.device)\n        return eye[i].clone()\n\n    x = torch.randn(5, 4, 3)\n    i = torch.randint(0, 3, torch.Size((5, 4)))\n    torch.jit.trace(make_one_hot_actual, (x, i))\n    expected = make_one_hot_expected(x, i)\n    actual = make_one_hot_actual(x, i)\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(\'expand\', [False, True])\n@pytest.mark.parametrize(\'shape\', [(), (4,), (5, 4)])\n@pytest.mark.filterwarnings(\'ignore:Converting a tensor to a Python boolean\')\ndef test_bernoulli_enumerate(shape, expand):\n    shape = torch.Size(shape)\n    probs = torch.full(shape, 0.25)\n\n    @pyro.ops.jit.trace\n    def f(probs):\n        d = dist.Bernoulli(probs)\n        support = d.enumerate_support(expand=expand)\n        return d.log_prob(support)\n\n    log_prob = f(probs)\n    assert log_prob.shape == (2,) + shape\n\n\n@pytest.mark.parametrize(\'expand\', [False, True])\n@pytest.mark.parametrize(\'shape\', [(3,), (4, 3), (5, 4, 3)])\ndef test_categorical_enumerate(shape, expand):\n    shape = torch.Size(shape)\n    probs = torch.ones(shape)\n\n    @pyro.ops.jit.trace\n    def f(probs):\n        d = dist.Categorical(probs)\n        support = d.enumerate_support(expand=expand)\n        return d.log_prob(support)\n\n    log_prob = f(probs)\n    batch_shape = shape[:-1]\n    assert log_prob.shape == shape[-1:] + batch_shape\n\n\n@pytest.mark.parametrize(\'expand\', [False, True])\n@pytest.mark.parametrize(\'shape\', [(3,), (4, 3), (5, 4, 3)])\n@pytest.mark.filterwarnings(\'ignore:Converting a tensor to a Python integer\')\ndef test_one_hot_categorical_enumerate(shape, expand):\n    shape = torch.Size(shape)\n    probs = torch.ones(shape)\n\n    @pyro.ops.jit.trace\n    def f(probs):\n        d = dist.OneHotCategorical(probs)\n        support = d.enumerate_support(expand=expand)\n        return d.log_prob(support)\n\n    log_prob = f(probs)\n    batch_shape = shape[:-1]\n    assert log_prob.shape == shape[-1:] + batch_shape\n\n\n@pytest.mark.parametrize(\'num_particles\', [1, 10])\n@pytest.mark.parametrize(\'Elbo\', [\n    Trace_ELBO,\n    JitTrace_ELBO,\n    TraceGraph_ELBO,\n    JitTraceGraph_ELBO,\n    TraceEnum_ELBO,\n    JitTraceEnum_ELBO,\n    TraceMeanField_ELBO,\n    JitTraceMeanField_ELBO,\n])\ndef test_svi(Elbo, num_particles):\n    pyro.clear_param_store()\n    data = torch.arange(10.)\n\n    def model(data):\n        loc = pyro.param(""loc"", constant(0.0))\n        scale = pyro.param(""scale"", constant(1.0), constraint=constraints.positive)\n        pyro.sample(""x"", dist.Normal(loc, scale).expand_by(data.shape).to_event(1), obs=data)\n\n    def guide(data):\n        pass\n\n    elbo = Elbo(num_particles=num_particles, strict_enumeration_warning=False)\n    inference = SVI(model, guide, Adam({""lr"": 1e-6}), elbo)\n    for i in range(100):\n        inference.step(data)\n\n\n@pytest.mark.parametrize(""enumerate2"", [""sequential"", ""parallel""])\n@pytest.mark.parametrize(""enumerate1"", [""sequential"", ""parallel""])\n@pytest.mark.parametrize(""plate_dim"", [1, 2])\ndef test_svi_enum(plate_dim, enumerate1, enumerate2):\n    pyro.clear_param_store()\n    num_particles = 10\n    q = pyro.param(""q"", constant(0.75), constraint=constraints.unit_interval)\n    p = 0.2693204236205713  # for which kl(Bernoulli(q), Bernoulli(p)) = 0.5\n\n    def model():\n        pyro.sample(""x"", dist.Bernoulli(p))\n        for i in pyro.plate(""plate"", plate_dim):\n            pyro.sample(""y_{}"".format(i), dist.Bernoulli(p))\n\n    def guide():\n        q = pyro.param(""q"")\n        pyro.sample(""x"", dist.Bernoulli(q), infer={""enumerate"": enumerate1})\n        for i in pyro.plate(""plate"", plate_dim):\n            pyro.sample(""y_{}"".format(i), dist.Bernoulli(q), infer={""enumerate"": enumerate2})\n\n    kl = (1 + plate_dim) * kl_divergence(dist.Bernoulli(q), dist.Bernoulli(p))\n    expected_loss = kl.item()\n    expected_grad = grad(kl, [q.unconstrained()])[0]\n\n    inner_particles = 2\n    outer_particles = num_particles // inner_particles\n    elbo = TraceEnum_ELBO(max_plate_nesting=0,\n                          strict_enumeration_warning=any([enumerate1, enumerate2]),\n                          num_particles=inner_particles,\n                          ignore_jit_warnings=True)\n    actual_loss = sum(elbo.loss_and_grads(model, guide)\n                      for i in range(outer_particles)) / outer_particles\n    actual_grad = q.unconstrained().grad / outer_particles\n\n    assert_equal(actual_loss, expected_loss, prec=0.3, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n    assert_equal(actual_grad, expected_grad, prec=0.5, msg="""".join([\n        ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n        ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n    ]))\n\n\n@pytest.mark.parametrize(\'vectorized\', [False, True])\n@pytest.mark.parametrize(\'Elbo\', [TraceEnum_ELBO, JitTraceEnum_ELBO])\ndef test_beta_bernoulli(Elbo, vectorized):\n    pyro.clear_param_store()\n    data = torch.tensor([1.0] * 6 + [0.0] * 4)\n\n    def model1(data):\n        alpha0 = constant(10.0)\n        beta0 = constant(10.0)\n        f = pyro.sample(""latent_fairness"", dist.Beta(alpha0, beta0))\n        for i in pyro.plate(""plate"", len(data)):\n            pyro.sample(""obs_{}"".format(i), dist.Bernoulli(f), obs=data[i])\n\n    def model2(data):\n        alpha0 = constant(10.0)\n        beta0 = constant(10.0)\n        f = pyro.sample(""latent_fairness"", dist.Beta(alpha0, beta0))\n        pyro.sample(""obs"", dist.Bernoulli(f).expand_by(data.shape).to_event(1),\n                    obs=data)\n\n    model = model2 if vectorized else model1\n\n    def guide(data):\n        alpha_q = pyro.param(""alpha_q"", constant(15.0),\n                             constraint=constraints.positive)\n        beta_q = pyro.param(""beta_q"", constant(15.0),\n                            constraint=constraints.positive)\n        pyro.sample(""latent_fairness"", dist.Beta(alpha_q, beta_q))\n\n    elbo = Elbo(num_particles=7, strict_enumeration_warning=False, ignore_jit_warnings=True)\n    optim = Adam({""lr"": 0.0005, ""betas"": (0.90, 0.999)})\n    svi = SVI(model, guide, optim, elbo)\n    for step in range(40):\n        svi.step(data)\n\n\n@pytest.mark.parametrize(\'Elbo\', [\n    Trace_ELBO,\n    JitTrace_ELBO,\n    TraceGraph_ELBO,\n    JitTraceGraph_ELBO,\n    TraceEnum_ELBO,\n    JitTraceEnum_ELBO,\n    TraceMeanField_ELBO,\n    JitTraceMeanField_ELBO,\n])\ndef test_svi_irregular_batch_size(Elbo):\n    pyro.clear_param_store()\n\n    @poutine.broadcast\n    def model(data):\n        loc = pyro.param(""loc"", constant(0.0))\n        scale = pyro.param(""scale"", constant(1.0), constraint=constraints.positive)\n        with pyro.plate(""data"", data.shape[0]):\n            pyro.sample(""x"",\n                        dist.Normal(loc, scale).expand([data.shape[0]]),\n                        obs=data)\n\n    def guide(data):\n        pass\n\n    pyro.clear_param_store()\n    elbo = Elbo(strict_enumeration_warning=False, max_plate_nesting=1)\n    inference = SVI(model, guide, Adam({""lr"": 1e-6}), elbo)\n    inference.step(torch.ones(10))\n    inference.step(torch.ones(3))\n\n\n@pytest.mark.parametrize(\'vectorized\', [False, True])\n@pytest.mark.parametrize(\'Elbo\', [TraceEnum_ELBO, JitTraceEnum_ELBO])\ndef test_dirichlet_bernoulli(Elbo, vectorized):\n    pyro.clear_param_store()\n    data = torch.tensor([1.0] * 6 + [0.0] * 4)\n\n    def model1(data):\n        concentration0 = constant([10.0, 10.0])\n        f = pyro.sample(""latent_fairness"", dist.Dirichlet(concentration0))[1]\n        for i in pyro.plate(""plate"", len(data)):\n            pyro.sample(""obs_{}"".format(i), dist.Bernoulli(f), obs=data[i])\n\n    def model2(data):\n        concentration0 = constant([10.0, 10.0])\n        f = pyro.sample(""latent_fairness"", dist.Dirichlet(concentration0))[1]\n        pyro.sample(""obs"", dist.Bernoulli(f).expand_by(data.shape).to_event(1),\n                    obs=data)\n\n    model = model2 if vectorized else model1\n\n    def guide(data):\n        concentration_q = pyro.param(""concentration_q"", constant([15.0, 15.0]),\n                                     constraint=constraints.positive)\n        pyro.sample(""latent_fairness"", dist.Dirichlet(concentration_q))\n\n    elbo = Elbo(num_particles=7, strict_enumeration_warning=False, ignore_jit_warnings=True)\n    optim = Adam({""lr"": 0.0005, ""betas"": (0.90, 0.999)})\n    svi = SVI(model, guide, optim, elbo)\n    for step in range(40):\n        svi.step(data)\n\n\n@pytest.mark.parametrize(\'length\', [1, 2, 10])\ndef test_traceenum_elbo(length):\n    hidden_dim = 10\n    transition = pyro.param(""transition"",\n                            0.3 / hidden_dim + 0.7 * torch.eye(hidden_dim),\n                            constraint=constraints.positive)\n    means = pyro.param(""means"", torch.arange(float(hidden_dim)))\n    data = 1 + 2 * torch.randn(length)\n\n    @ignore_jit_warnings()\n    def model(data):\n        transition = pyro.param(""transition"")\n        means = pyro.param(""means"")\n        states = [torch.tensor(0)]\n        for t in pyro.markov(range(len(data))):\n            states.append(pyro.sample(""states_{}"".format(t),\n                                      dist.Categorical(transition[states[-1]]),\n                                      infer={""enumerate"": ""parallel""}))\n            pyro.sample(""obs_{}"".format(t),\n                        dist.Normal(means[states[-1]], 1.),\n                        obs=data[t])\n        return tuple(states)\n\n    def guide(data):\n        pass\n\n    expected_loss = TraceEnum_ELBO(max_plate_nesting=0).differentiable_loss(model, guide, data)\n    actual_loss = JitTraceEnum_ELBO(max_plate_nesting=0).differentiable_loss(model, guide, data)\n    assert_equal(expected_loss, actual_loss)\n\n    expected_grads = grad(expected_loss, [transition, means], allow_unused=True)\n    actual_grads = grad(actual_loss, [transition, means], allow_unused=True)\n    for e, a, name in zip(expected_grads, actual_grads, [""transition"", ""means""]):\n        assert_equal(e, a, msg=""bad gradient for {}"".format(name))\n\n\n@pytest.mark.parametrize(\'length\', [1, 2, 10])\n@pytest.mark.parametrize(\'temperature\', [0, 1], ids=[\'map\', \'sample\'])\ndef test_infer_discrete(temperature, length):\n\n    @ignore_jit_warnings()\n    def hmm(transition, means, data):\n        states = [torch.tensor(0)]\n        for t in pyro.markov(range(len(data))):\n            states.append(pyro.sample(""states_{}"".format(t),\n                                      dist.Categorical(transition[states[-1]]),\n                                      infer={""enumerate"": ""parallel""}))\n            pyro.sample(""obs_{}"".format(t),\n                        dist.Normal(means[states[-1]], 1.),\n                        obs=data[t])\n        return tuple(states)\n\n    hidden_dim = 10\n    transition = 0.3 / hidden_dim + 0.7 * torch.eye(hidden_dim)\n    means = torch.arange(float(hidden_dim))\n    data = 1 + 2 * torch.randn(length)\n\n    decoder = infer_discrete(hmm, first_available_dim=-1, temperature=temperature)\n    jit_decoder = pyro.ops.jit.trace(decoder)\n\n    states = decoder(transition, means, data)\n    jit_states = jit_decoder(transition, means, data)\n    assert len(states) == len(jit_states)\n    for state, jit_state in zip(states, jit_states):\n        assert state.shape == jit_state.shape\n        if temperature == 0:\n            assert_equal(state, jit_state)\n\n\n@pytest.mark.parametrize(""x,y"", [\n    (CondIndepStackFrame(""a"", -1, torch.tensor(2000), 2), CondIndepStackFrame(""a"", -1, 2000, 2)),\n    (CondIndepStackFrame(""a"", -1, 1, 2), CondIndepStackFrame(""a"", -1, torch.tensor(1), 2)),\n])\ndef test_cond_indep_equality(x, y):\n    assert x == y\n    assert not x != y\n    assert hash(x) == hash(y)\n\n\ndef test_jit_arange_workaround():\n    def fn(x):\n        y = torch.ones(x.shape[0], dtype=torch.long, device=x.device)\n        return torch.cumsum(y, 0) - 1\n\n    compiled = torch.jit.trace(fn, torch.ones(3))\n    assert_equal(compiled(torch.ones(10)), torch.arange(10))\n'"
tests/infer/test_multi_sample_elbos.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import RenyiELBO\nfrom tests.common import assert_close\n\n\ndef check_elbo(model, guide, Elbo):\n    elbo = Elbo(num_particles=2, vectorize_particles=False)\n    pyro.set_rng_seed(123)\n    loss1 = elbo.loss(model, guide)\n    pyro.set_rng_seed(123)\n    loss2 = elbo.loss_and_grads(model, guide)\n    assert_close(loss1, loss2)\n\n    elbo = Elbo(num_particles=10000, vectorize_particles=True)\n    loss1 = elbo.loss(model, guide)\n    loss2 = elbo.loss_and_grads(model, guide)\n    assert_close(loss1, loss2, atol=0.1)\n\n\n@pytest.mark.parametrize(""Elbo"", [RenyiELBO])\ndef test_inner_outer(Elbo):\n    data = torch.randn(2, 3)\n\n    def model():\n        with pyro.plate(""outer"", 3, dim=-1):\n            x = pyro.sample(""x"", dist.Normal(0, 1))\n            with pyro.plate(""inner"", 2, dim=-2):\n                pyro.sample(""y"", dist.Normal(x, 1),\n                            obs=data)\n\n    def guide():\n        with pyro.plate(""outer"", 3, dim=-1):\n            pyro.sample(""x"", dist.Normal(1, 1))\n\n    check_elbo(model, guide, Elbo)\n\n\n@pytest.mark.parametrize(""Elbo"", [RenyiELBO])\ndef test_outer_inner(Elbo):\n    data = torch.randn(2, 3)\n\n    def model():\n        with pyro.plate(""outer"", 2, dim=-2):\n            x = pyro.sample(""x"", dist.Normal(0, 1))\n            with pyro.plate(""inner"", 3, dim=-1):\n                pyro.sample(""y"", dist.Normal(x, 1),\n                            obs=data)\n\n    def guide():\n        with pyro.plate(""outer"", 2, dim=-2):\n            pyro.sample(""x"", dist.Normal(1, 1))\n\n    check_elbo(model, guide, Elbo)\n'"
tests/infer/test_predictive.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim as optim\nimport pyro.poutine as poutine\nfrom pyro.infer.autoguide import AutoDelta, AutoDiagonalNormal\nfrom pyro.infer import Predictive, SVI, Trace_ELBO\nfrom tests.common import assert_close\n\n\ndef model(num_trials):\n    with pyro.plate(""data"", num_trials.size(0)):\n        phi_prior = dist.Uniform(num_trials.new_tensor(0.), num_trials.new_tensor(1.))\n        success_prob = pyro.sample(""phi"", phi_prior)\n        return pyro.sample(""obs"", dist.Binomial(num_trials, success_prob))\n\n\ndef one_hot_model(pseudocounts, classes=None):\n    probs_prior = dist.Dirichlet(pseudocounts)\n    probs = pyro.sample(""probs"", probs_prior)\n    with pyro.plate(""classes"", classes.size(0) if classes is not None else 1, dim=-1):\n        return pyro.sample(""obs"", dist.OneHotCategorical(probs), obs=classes)\n\n\ndef beta_guide(num_trials):\n    phi_c0 = pyro.param(""phi_c0"", num_trials.new_tensor(5.0).expand([num_trials.size(0)]))\n    phi_c1 = pyro.param(""phi_c1"", num_trials.new_tensor(5.0).expand([num_trials.size(0)]))\n    with pyro.plate(""data"", num_trials.size(0)):\n        phi_posterior = dist.Beta(concentration0=phi_c0, concentration1=phi_c1)\n        pyro.sample(""phi"", phi_posterior)\n\n\n@pytest.mark.parametrize(""parallel"", [False, True])\ndef test_posterior_predictive_svi_manual_guide(parallel):\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={""obs"": num_success})\n    svi = SVI(conditioned_model, beta_guide, optim.Adam(dict(lr=1.0)), Trace_ELBO())\n    for i in range(1000):\n        svi.step(num_trials)\n    posterior_predictive = Predictive(model, guide=beta_guide, num_samples=10000,\n                                      parallel=parallel, return_sites=[""_RETURN""])\n    marginal_return_vals = posterior_predictive.get_samples(num_trials)[""_RETURN""]\n    assert_close(marginal_return_vals.mean(dim=0), torch.ones(5) * 700, rtol=0.05)\n\n\n@pytest.mark.parametrize(""parallel"", [False, True])\ndef test_posterior_predictive_svi_auto_delta_guide(parallel):\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={""obs"": num_success})\n    guide = AutoDelta(conditioned_model)\n    svi = SVI(conditioned_model, guide, optim.Adam(dict(lr=1.0)), Trace_ELBO())\n    for i in range(1000):\n        svi.step(num_trials)\n    posterior_predictive = Predictive(model, guide=guide, num_samples=10000, parallel=parallel)\n    marginal_return_vals = posterior_predictive.get_samples(num_trials)[""obs""]\n    assert_close(marginal_return_vals.mean(dim=0), torch.ones(5) * 700, rtol=0.05)\n\n\n@pytest.mark.parametrize(""return_trace"", [False, True])\ndef test_posterior_predictive_svi_auto_diag_normal_guide(return_trace):\n    true_probs = torch.ones(5) * 0.7\n    num_trials = torch.ones(5) * 1000\n    num_success = dist.Binomial(num_trials, true_probs).sample()\n    conditioned_model = poutine.condition(model, data={""obs"": num_success})\n    guide = AutoDiagonalNormal(conditioned_model)\n    svi = SVI(conditioned_model, guide, optim.Adam(dict(lr=0.1)), Trace_ELBO())\n    for i in range(1000):\n        svi.step(num_trials)\n    posterior_predictive = Predictive(model, guide=guide, num_samples=10000, parallel=True)\n    if return_trace:\n        marginal_return_vals = posterior_predictive.get_vectorized_trace(num_trials).nodes[""obs""][""value""]\n    else:\n        marginal_return_vals = posterior_predictive.get_samples(num_trials)[""obs""]\n    assert_close(marginal_return_vals.mean(dim=0), torch.ones(5) * 700, rtol=0.05)\n\n\ndef test_posterior_predictive_svi_one_hot():\n    pseudocounts = torch.ones(3) * 0.1\n    true_probs = torch.tensor([0.15, 0.6, 0.25])\n    classes = dist.OneHotCategorical(true_probs).sample((10000,))\n    guide = AutoDelta(one_hot_model)\n    svi = SVI(one_hot_model, guide, optim.Adam(dict(lr=0.1)), Trace_ELBO())\n    for i in range(1000):\n        svi.step(pseudocounts, classes=classes)\n    posterior_samples = Predictive(guide, num_samples=10000).get_samples(pseudocounts)\n    posterior_predictive = Predictive(one_hot_model, posterior_samples)\n    marginal_return_vals = posterior_predictive.get_samples(pseudocounts)[""obs""]\n    assert_close(marginal_return_vals.mean(dim=0), true_probs.unsqueeze(0), rtol=0.1)\n\n\n@pytest.mark.parametrize(""parallel"", [False, True])\ndef test_shapes(parallel):\n    num_samples = 10\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0, 1).expand([2]).to_event(1))\n        with pyro.plate(""plate"", 5):\n            loc, log_scale = x.unbind(-1)\n            y = pyro.sample(""y"", dist.Normal(loc, log_scale.exp()))\n        return dict(x=x, y=y)\n\n    guide = AutoDiagonalNormal(model)\n\n    # Compute by hand.\n    vectorize = pyro.plate(""_vectorize"", num_samples, dim=-2)\n    trace = poutine.trace(vectorize(guide)).get_trace()\n    expected = poutine.replay(vectorize(model), trace)()\n\n    # Use Predictive.\n    predictive = Predictive(model, guide=guide, return_sites=[""x"", ""y""],\n                            num_samples=num_samples, parallel=parallel)\n    actual = predictive.get_samples()\n    assert set(actual) == set(expected)\n    assert actual[""x""].shape == expected[""x""].shape\n    assert actual[""y""].shape == expected[""y""].shape\n\n\n@pytest.mark.parametrize(""with_plate"", [True, False])\n@pytest.mark.parametrize(""event_shape"", [(), (2,)])\ndef test_deterministic(with_plate, event_shape):\n    def model(y=None):\n        with pyro.util.optional(pyro.plate(""plate"", 3), with_plate):\n            x = pyro.sample(""x"", dist.Normal(0, 1).expand(event_shape).to_event())\n            x2 = pyro.deterministic(""x2"", x ** 2, event_dim=len(event_shape))\n\n        pyro.deterministic(""x3"", x2)\n        return pyro.sample(""obs"", dist.Normal(x2, 0.1).to_event(), obs=y)\n\n    y = torch.tensor(4.)\n    guide = AutoDiagonalNormal(model)\n    svi = SVI(model, guide, optim.Adam(dict(lr=0.1)), Trace_ELBO())\n    for i in range(100):\n        svi.step(y)\n\n    actual = Predictive(model, guide=guide, return_sites=[""x2"", ""x3""], num_samples=1000)()\n    x2_batch_shape = (3,) if with_plate else ()\n    assert actual[""x2""].shape == (1000,) + x2_batch_shape + event_shape\n    # x3 shape is prepended 1 to match Pyro shape semantics\n    x3_batch_shape = (1, 3) if with_plate else ()\n    assert actual[""x3""].shape == (1000,) + x3_batch_shape + event_shape\n    assert_close(actual[""x2""].mean(), y, rtol=0.1)\n    assert_close(actual[""x3""].mean(), y, rtol=0.1)\n'"
tests/infer/test_sampling.py,19,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom unittest import TestCase\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.infer\nfrom pyro.distributions import Bernoulli, Normal\nfrom pyro.infer import EmpiricalMarginal\nfrom tests.common import assert_equal\n\n\nclass HMMSamplingTestCase(TestCase):\n\n    def setUp(self):\n\n        # simple Gaussian-emission HMM\n        def model():\n            p_latent = pyro.param(""p1"", torch.tensor([[0.7], [0.3]]))\n            p_obs = pyro.param(""p2"", torch.tensor([[0.9], [0.1]]))\n\n            latents = [torch.ones(1, 1)]\n            observes = []\n            for t in range(self.model_steps):\n\n                latents.append(\n                    pyro.sample(""latent_{}"".format(str(t)),\n                                Bernoulli(torch.index_select(p_latent, 0, latents[-1].view(-1).long()))))\n\n                observes.append(\n                    pyro.sample(""observe_{}"".format(str(t)),\n                                Bernoulli(torch.index_select(p_obs, 0, latents[-1].view(-1).long())),\n                                obs=self.data[t]))\n            return torch.sum(torch.cat(latents))\n\n        self.model_steps = 3\n        self.data = [torch.ones(1, 1) for _ in range(self.model_steps)]\n        self.model = model\n\n\nclass NormalNormalSamplingTestCase(TestCase):\n\n    def setUp(self):\n\n        pyro.clear_param_store()\n\n        def model():\n            loc = pyro.sample(""loc"", Normal(torch.zeros(1),\n                                            torch.ones(1)))\n            xd = Normal(loc, torch.ones(1))\n            pyro.sample(""xs"", xd, obs=self.data)\n            return loc\n\n        def guide():\n            return pyro.sample(""loc"", Normal(torch.zeros(1),\n                                             torch.ones(1)))\n\n        # data\n        self.data = torch.zeros(50, 1)\n        self.loc_mean = torch.zeros(1)\n        self.loc_stddev = torch.sqrt(torch.ones(1) / 51.0)\n\n        # model and guide\n        self.model = model\n        self.guide = guide\n\n\nclass ImportanceTest(NormalNormalSamplingTestCase):\n\n    @pytest.mark.init(rng_seed=0)\n    def test_importance_guide(self):\n        posterior = pyro.infer.Importance(self.model, guide=self.guide, num_samples=5000).run()\n        marginal = EmpiricalMarginal(posterior)\n        assert_equal(0, torch.norm(marginal.mean - self.loc_mean).item(), prec=0.01)\n        assert_equal(0, torch.norm(marginal.variance.sqrt() - self.loc_stddev).item(), prec=0.1)\n\n    @pytest.mark.init(rng_seed=0)\n    def test_importance_prior(self):\n        posterior = pyro.infer.Importance(self.model, guide=None, num_samples=10000).run()\n        marginal = EmpiricalMarginal(posterior)\n        assert_equal(0, torch.norm(marginal.mean - self.loc_mean).item(), prec=0.01)\n        assert_equal(0, torch.norm(marginal.variance.sqrt() - self.loc_stddev).item(), prec=0.1)\n'"
tests/infer/test_smcfilter.py,22,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer import SMCFilter\nfrom pyro.infer.smcfilter import _systematic_sample\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""size"", range(1, 32))\ndef test_systematic_sample(size):\n    pyro.set_rng_seed(size)\n    probs = torch.randn(size).exp()\n    probs /= probs.sum()\n\n    num_samples = 20000\n    index = _systematic_sample(probs.expand(num_samples, size))\n    histogram = torch.zeros_like(probs)\n    histogram.scatter_add_(-1, index.reshape(-1),\n                           probs.new_ones(1).expand(num_samples * size))\n\n    expected = probs * size\n    actual = histogram / num_samples\n    assert_close(actual, expected, atol=0.01)\n\n\nclass SmokeModel:\n\n    def __init__(self, state_size, plate_size):\n        self.state_size = state_size\n        self.plate_size = plate_size\n\n    def init(self, state):\n        self.t = 0\n        state[""x_mean""] = pyro.sample(""x_mean"", dist.Normal(0., 1.))\n        state[""y_mean""] = pyro.sample(""y_mean"",\n                                      dist.MultivariateNormal(torch.zeros(self.state_size),\n                                                              torch.eye(self.state_size)))\n\n    def step(self, state, x=None, y=None):\n        v = pyro.sample(""v_{}"".format(self.t), dist.Normal(0., 1.))\n        with pyro.plate(""plate"", self.plate_size):\n            w = pyro.sample(""w_{}"".format(self.t), dist.Normal(v, 1.))\n            x = pyro.sample(""x_{}"".format(self.t),\n                            dist.Normal(state[""x_mean""] + w, 1), obs=x)\n            y = pyro.sample(""y_{}"".format(self.t),\n                            dist.MultivariateNormal(state[""y_mean""] + w.unsqueeze(-1), torch.eye(self.state_size)),\n                            obs=y)\n        self.t += 1\n        return x, y\n\n\nclass SmokeGuide:\n\n    def __init__(self, state_size, plate_size):\n        self.state_size = state_size\n        self.plate_size = plate_size\n\n    def init(self, state):\n        self.t = 0\n        pyro.sample(""x_mean"", dist.Normal(0., 2.))\n        pyro.sample(""y_mean"",\n                    dist.MultivariateNormal(torch.zeros(self.state_size),\n                                            2.*torch.eye(self.state_size)))\n\n    def step(self, state, x=None, y=None):\n        v = pyro.sample(""v_{}"".format(self.t), dist.Normal(0., 2.))\n        with pyro.plate(""plate"", self.plate_size):\n            pyro.sample(""w_{}"".format(self.t), dist.Normal(v, 2.))\n        self.t += 1\n\n\n@pytest.mark.parametrize(""max_plate_nesting"", [1, 2])\n@pytest.mark.parametrize(""state_size"", [2, 5, 1])\n@pytest.mark.parametrize(""plate_size"", [3, 7, 1])\n@pytest.mark.parametrize(""num_steps"", [1, 2, 10])\ndef test_smoke(max_plate_nesting, state_size, plate_size, num_steps):\n    model = SmokeModel(state_size, plate_size)\n    guide = SmokeGuide(state_size, plate_size)\n\n    smc = SMCFilter(model, guide, num_particles=100, max_plate_nesting=max_plate_nesting)\n\n    true_model = SmokeModel(state_size, plate_size)\n\n    state = {}\n    true_model.init(state)\n    truth = [true_model.step(state) for t in range(num_steps)]\n\n    smc.init()\n    assert set(smc.state) == {""x_mean"", ""y_mean""}\n    for x, y in truth:\n        smc.step(x, y)\n    assert set(smc.state) == {""x_mean"", ""y_mean""}\n    smc.get_empirical()\n\n\nclass HarmonicModel:\n\n    def __init__(self):\n        self.A = torch.tensor([[0., 1.],\n                               [-1., 0.]])\n        self.B = torch.tensor([3., 3.])\n        self.sigma_z = torch.tensor(1.)\n        self.sigma_y = torch.tensor(1.)\n\n    def init(self, state):\n        self.t = 0\n        state[""z""] = pyro.sample(""z_init"",\n                                 dist.Delta(torch.tensor([1., 0.]), event_dim=1))\n\n    def step(self, state, y=None):\n        self.t += 1\n        state[""z""] = pyro.sample(""z_{}"".format(self.t),\n                                 dist.Normal(state[""z""].matmul(self.A),\n                                             self.B*self.sigma_z).to_event(1))\n        y = pyro.sample(""y_{}"".format(self.t),\n                        dist.Normal(state[""z""][..., 0], self.sigma_y),\n                        obs=y)\n\n        state[""z_{}"".format(self.t)] = state[""z""]  # saved for testing\n\n        return state[""z""], y\n\n\nclass HarmonicGuide:\n\n    def __init__(self):\n        self.model = HarmonicModel()\n\n    def init(self, state):\n        self.t = 0\n        pyro.sample(""z_init"", dist.Delta(torch.tensor([1., 0.]), event_dim=1))\n\n    def step(self, state, y=None):\n        self.t += 1\n\n        # Proposal distribution\n        pyro.sample(""z_{}"".format(self.t),\n                    dist.Normal(state[""z""].matmul(self.model.A),\n                                torch.tensor([2., 2.])).to_event(1))\n\n\ndef generate_data():\n    model = HarmonicModel()\n\n    state = {}\n    model.init(state)\n    zs = [torch.tensor([1., 0.])]\n    ys = [None]\n    for t in range(50):\n        z, y = model.step(state)\n        zs.append(z)\n        ys.append(y)\n\n    return zs, ys\n\n\ndef score_latent(zs, ys):\n    model = HarmonicModel()\n    with poutine.trace() as trace:\n        with poutine.condition(data={""z_{}"".format(t): z for t, z in enumerate(zs)}):\n            state = {}\n            model.init(state)\n            for y in ys[1:]:\n                model.step(state, y)\n\n    return trace.trace.log_prob_sum()\n\n\ndef test_likelihood_ratio():\n\n    model = HarmonicModel()\n    guide = HarmonicGuide()\n\n    smc = SMCFilter(model, guide, num_particles=100, max_plate_nesting=0)\n\n    zs, ys = generate_data()\n    zs_true, ys_true = generate_data()\n    smc.init()\n    for y in ys_true[1:]:\n        smc.step(y)\n    i = smc.state._log_weights.max(0)[1]\n    values = {k: v[i] for k, v in smc.state.items()}\n\n    zs_pred = [torch.tensor([1., 0.])]\n    zs_pred += [values[""z_{}"".format(t)] for t in range(1, 51)]\n\n    assert(score_latent(zs_true, ys_true) > score_latent(zs, ys_true))\n    assert(score_latent(zs_pred, ys_true) > score_latent(zs_pred, ys))\n    assert(score_latent(zs_pred, ys_true) > score_latent(zs, ys_true))\n\n\ndef test_gaussian_filter():\n    dim = 4\n    init_dist = dist.MultivariateNormal(torch.zeros(dim), scale_tril=torch.eye(dim) * 10)\n    trans_mat = torch.eye(dim)\n    trans_dist = dist.MultivariateNormal(torch.zeros(dim), scale_tril=torch.eye(dim))\n    obs_mat = torch.eye(dim)\n    obs_dist = dist.MultivariateNormal(torch.zeros(dim), scale_tril=torch.eye(dim) * 2)\n    hmm = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n\n    class Model:\n        def init(self, state):\n            state[""z""] = pyro.sample(""z_init"", init_dist)\n            self.t = 0\n\n        def step(self, state, datum=None):\n            state[""z""] = pyro.sample(""z_{}"".format(self.t),\n                                     dist.MultivariateNormal(state[""z""], scale_tril=trans_dist.scale_tril))\n            datum = pyro.sample(""obs_{}"".format(self.t),\n                                dist.MultivariateNormal(state[""z""], scale_tril=obs_dist.scale_tril),\n                                obs=datum)\n            self.t += 1\n            return datum\n\n    class Guide:\n        def init(self, state):\n            pyro.sample(""z_init"", init_dist)\n            self.t = 0\n\n        def step(self, state, datum):\n            pyro.sample(""z_{}"".format(self.t),\n                        dist.MultivariateNormal(state[""z""], scale_tril=trans_dist.scale_tril * 2))\n            self.t += 1\n\n    # Generate data.\n    num_steps = 20\n    model = Model()\n    state = {}\n    model.init(state)\n    data = torch.stack([model.step(state) for _ in range(num_steps)])\n\n    # Perform inference.\n    model = Model()\n    guide = Guide()\n    smc = SMCFilter(model, guide, num_particles=1000, max_plate_nesting=0)\n    smc.init()\n    for t, datum in enumerate(data):\n        smc.step(datum)\n        expected = hmm.filter(data[:1+t])\n        actual = smc.get_empirical()[""z""]\n        assert_close(actual.variance ** 0.5, expected.variance ** 0.5, atol=0.1, rtol=0.5)\n        sigma = actual.variance.max().item() ** 0.5\n        assert_close(actual.mean, expected.mean, atol=3 * sigma)\n'"
tests/infer/test_svgd.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom pyro.infer import SVGD, RBFSteinKernel, IMQSteinKernel\nfrom pyro.optim import Adam\nfrom pyro.infer.autoguide.utils import _product\n\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize(""latent_dist"", [dist.Normal(torch.zeros(2), torch.ones(2)).to_event(1),\n                                         dist.LogNormal(torch.tensor([-1.0]), torch.tensor([0.7])).to_event(1),\n                                         dist.LogNormal(torch.tensor(-1.0), torch.tensor(0.7)),\n                                         dist.Beta(torch.tensor([0.3]), torch.tensor([0.7])).to_event(1)])\n@pytest.mark.parametrize(""mode"", [""univariate"", ""multivariate""])\n@pytest.mark.parametrize(""stein_kernel"", [RBFSteinKernel, IMQSteinKernel])\ndef test_mean_variance(latent_dist, mode, stein_kernel, verbose=True):\n    pyro.clear_param_store()\n\n    def model():\n        pyro.sample(""z"", latent_dist)\n\n    kernel = stein_kernel()\n    adam = Adam({""lr"": 0.05})\n    svgd = SVGD(model, kernel, adam, 200, 0, mode=mode)\n\n    bandwidth_start = 1.0\n    bandwidth_end = 5.0\n    n_steps = 301\n\n    # scramble initial particles\n    svgd.step()\n    pyro.param(\'svgd_particles\').unconstrained().data *= 1.3\n    pyro.param(\'svgd_particles\').unconstrained().data += 0.7\n\n    for step in range(n_steps):\n        kernel.bandwidth_factor = bandwidth_start + (step / n_steps) * (bandwidth_end - bandwidth_start)\n        squared_gradients = svgd.step()\n        if step % 125 == 0:\n            print(""[step %03d] "" % step, squared_gradients)\n\n    final_particles = svgd.get_named_particles()[\'z\']\n\n    if verbose:\n        print(""[mean]: actual, expected = "", final_particles.mean(0).data.numpy(),\n              latent_dist.mean.data.numpy())\n        print(""[var]: actual, expected = "", final_particles.var(0).data.numpy(),\n              latent_dist.variance.data.numpy())\n\n    assert_equal(final_particles.mean(0), latent_dist.mean, prec=0.01)\n    prec = 0.05 if mode == \'multivariate\' else 0.02\n    assert_equal(final_particles.var(0), latent_dist.variance, prec=prec)\n\n\n@pytest.mark.parametrize(""shape"", [(1, 1), (2, 1, 3), (4, 2), (1, 2, 1, 3)])\n@pytest.mark.parametrize(""stein_kernel"", [RBFSteinKernel, IMQSteinKernel])\ndef test_shapes(shape, stein_kernel):\n    pyro.clear_param_store()\n    shape1, shape2 = (5,) + shape, shape + (6,)\n\n    mean_init1 = torch.arange(_product(shape1)).double().reshape(shape1) / 100.0\n    mean_init2 = torch.arange(_product(shape2)).double().reshape(shape2)\n\n    def model():\n        pyro.sample(""z1"", dist.LogNormal(mean_init1, 1.0e-8).to_event(len(shape1)))\n        pyro.sample(""scalar"", dist.Normal(0.0, 1.0))\n        pyro.sample(""z2"", dist.Normal(mean_init2, 1.0e-8).to_event(len(shape2)))\n\n    num_particles = 7\n    svgd = SVGD(model, stein_kernel(), Adam({""lr"": 0.0}), num_particles, 0)\n\n    for step in range(2):\n        svgd.step()\n\n    particles = svgd.get_named_particles()\n    assert particles[\'z1\'].shape == (num_particles,) + shape1\n    assert particles[\'z2\'].shape == (num_particles,) + shape2\n\n    for particle in range(num_particles):\n        assert_equal(particles[\'z1\'][particle, ...], mean_init1.exp(), prec=1.0e-6)\n        assert_equal(particles[\'z2\'][particle, ...], mean_init2, prec=1.0e-6)\n\n\n@pytest.mark.parametrize(""mode"", [""univariate"", ""multivariate""])\n@pytest.mark.parametrize(""stein_kernel"", [RBFSteinKernel, IMQSteinKernel])\ndef test_conjugate(mode, stein_kernel, verbose=False):\n    data = torch.tensor([1.0, 2.0, 3.0, 3.0, 5.0]).unsqueeze(-1).expand(5, 3)\n    alpha0 = torch.tensor([1.0, 1.8, 2.3])\n    beta0 = torch.tensor([2.3, 1.5, 1.2])\n    alpha_n = alpha0 + data.sum(0)  # posterior alpha\n    beta_n = beta0 + data.size(0)   # posterior beta\n\n    def model():\n        with pyro.plate(""rates"", alpha0.size(0)):\n            latent = pyro.sample(""latent"",\n                                 dist.Gamma(alpha0, beta0))\n            with pyro.plate(""data"", data.size(0)):\n                pyro.sample(""obs"", dist.Poisson(latent), obs=data)\n\n    kernel = stein_kernel()\n    adam = Adam({""lr"": 0.05})\n    svgd = SVGD(model, kernel, adam, 200, 2, mode=mode)\n\n    bandwidth_start = 1.0\n    bandwidth_end = 5.0\n    n_steps = 451\n\n    for step in range(n_steps):\n        kernel.bandwidth_factor = bandwidth_start + (step / n_steps) * (bandwidth_end - bandwidth_start)\n        squared_gradients = svgd.step()\n        if step % 150 == 0:\n            print(""[step %03d] "" % step, squared_gradients)\n\n    final_particles = svgd.get_named_particles()[\'latent\']\n    posterior_dist = dist.Gamma(alpha_n, beta_n)\n\n    if verbose:\n        print(""[mean]: actual, expected = "", final_particles.mean(0).data.numpy(),\n              posterior_dist.mean.data.numpy())\n        print(""[var]: actual, expected = "", final_particles.var(0).data.numpy(),\n              posterior_dist.variance.data.numpy())\n\n    assert_equal(final_particles.mean(0)[0], posterior_dist.mean, prec=0.02)\n    prec = 0.05 if mode == \'multivariate\' else 0.02\n    assert_equal(final_particles.var(0)[0], posterior_dist.variance, prec=prec)\n'"
tests/infer/test_tmc.py,11,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport math\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.distributions.testing import fakes\nfrom pyro.infer import config_enumerate\nfrom pyro.infer.importance import vectorized_importance_weights\nfrom pyro.infer.tracetmc_elbo import TraceTMC_ELBO\nfrom pyro.infer.traceenum_elbo import TraceEnum_ELBO\nfrom tests.common import assert_equal\n\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.parametrize(""depth"", [1, 2, 3, 4, 5])\n@pytest.mark.parametrize(""num_samples"", [None, 200])\n@pytest.mark.parametrize(""max_plate_nesting"", [2, 3])\n@pytest.mark.parametrize(""tmc_strategy"", [""diagonal"", ""mixture""])\ndef test_tmc_categoricals(depth, max_plate_nesting, num_samples, tmc_strategy):\n    qs = [pyro.param(""q0"", torch.tensor([0.4, 0.6], requires_grad=True))]\n    for i in range(1, depth):\n        qs.append(pyro.param(\n            ""q{}"".format(i),\n            torch.randn(2, 2).abs().detach().requires_grad_(),\n            constraint=constraints.simplex\n        ))\n    qs.append(pyro.param(""qy"", torch.tensor([0.75, 0.25], requires_grad=True)))\n\n    qs = [q.unconstrained() for q in qs]\n\n    data = (torch.rand(4, 3) > 0.5).to(dtype=qs[-1].dtype, device=qs[-1].device)\n\n    def model():\n        x = pyro.sample(""x0"", dist.Categorical(pyro.param(""q0"")))\n        with pyro.plate(""local"", 3):\n            for i in range(1, depth):\n                x = pyro.sample(""x{}"".format(i),\n                                dist.Categorical(pyro.param(""q{}"".format(i))[..., x, :]))\n            with pyro.plate(""data"", 4):\n                pyro.sample(""y"", dist.Bernoulli(pyro.param(""qy"")[..., x]),\n                            obs=data)\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=max_plate_nesting)\n    enum_model = config_enumerate(model, default=""parallel"", expand=False, num_samples=None, tmc=tmc_strategy)\n    expected_loss = (-elbo.differentiable_loss(enum_model, lambda: None)).exp()\n    expected_grads = grad(expected_loss, qs)\n\n    tmc = TraceTMC_ELBO(max_plate_nesting=max_plate_nesting)\n    tmc_model = config_enumerate(model, default=""parallel"", expand=False, num_samples=num_samples, tmc=tmc_strategy)\n    actual_loss = (-tmc.differentiable_loss(tmc_model, lambda: None)).exp()\n    actual_grads = grad(actual_loss, qs)\n\n    prec = 0.05\n    assert_equal(actual_loss, expected_loss, prec=prec, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n\n    for actual_grad, expected_grad in zip(actual_grads, expected_grads):\n        assert_equal(actual_grad, expected_grad, prec=prec, msg="""".join([\n            ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n            ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n        ]))\n\n\n@pytest.mark.parametrize(""depth"", [1, 2])\n@pytest.mark.parametrize(""num_samples,expand"", [(1000, False)])\n@pytest.mark.parametrize(""max_plate_nesting"", [0])\n@pytest.mark.parametrize(""reparameterized"", [True, False])\n@pytest.mark.parametrize(""guide_type"", [""prior"", ""factorized"", ""nonfactorized""])\n@pytest.mark.parametrize(""tmc_strategy"", [""diagonal"", ""mixture""])\ndef test_tmc_normals_chain_iwae(depth, num_samples, max_plate_nesting,\n                                reparameterized, guide_type, expand, tmc_strategy):\n    # compare iwae and tmc\n    q2 = pyro.param(""q2"", torch.tensor(0.5, requires_grad=True))\n    qs = (q2.unconstrained(),)\n\n    def model(reparameterized):\n        Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n        x = pyro.sample(""x0"", Normal(pyro.param(""q2""), math.sqrt(1. / depth)))\n        for i in range(1, depth):\n            x = pyro.sample(""x{}"".format(i), Normal(x, math.sqrt(1. / depth)))\n        pyro.sample(""y"", Normal(x, 1.), obs=torch.tensor(float(1)))\n\n    def factorized_guide(reparameterized):\n        Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n        pyro.sample(""x0"", Normal(pyro.param(""q2""), math.sqrt(1. / depth)))\n        for i in range(1, depth):\n            pyro.sample(""x{}"".format(i), Normal(0., math.sqrt(float(i+1) / depth)))\n\n    def nonfactorized_guide(reparameterized):\n        Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n        x = pyro.sample(""x0"", Normal(pyro.param(""q2""), math.sqrt(1. / depth)))\n        for i in range(1, depth):\n            x = pyro.sample(""x{}"".format(i), Normal(x, math.sqrt(1. / depth)))\n\n    guide = factorized_guide if guide_type == ""factorized"" else \\\n        nonfactorized_guide if guide_type == ""nonfactorized"" else \\\n        poutine.block(model, hide_fn=lambda msg: msg[""type""] == ""sample"" and msg[""is_observed""])\n    flat_num_samples = num_samples ** min(depth, 2)  # don\'t use too many, expensive\n    vectorized_log_weights, _, _ = vectorized_importance_weights(\n        model, guide, True,\n        max_plate_nesting=max_plate_nesting,\n        num_samples=flat_num_samples)\n    assert vectorized_log_weights.shape == (flat_num_samples,)\n    expected_loss = (vectorized_log_weights.logsumexp(dim=-1) - math.log(float(flat_num_samples))).exp()\n    expected_grads = grad(expected_loss, qs)\n\n    tmc = TraceTMC_ELBO(max_plate_nesting=max_plate_nesting)\n    tmc_model = config_enumerate(\n        model, default=""parallel"", expand=expand, num_samples=num_samples, tmc=tmc_strategy)\n    tmc_guide = config_enumerate(\n        guide, default=""parallel"", expand=expand, num_samples=num_samples, tmc=tmc_strategy)\n    actual_loss = (-tmc.differentiable_loss(tmc_model, tmc_guide, reparameterized)).exp()\n    actual_grads = grad(actual_loss, qs)\n\n    assert_equal(actual_loss, expected_loss, prec=0.05, msg="""".join([\n        ""\\nexpected loss = {}"".format(expected_loss),\n        ""\\n  actual loss = {}"".format(actual_loss),\n    ]))\n\n    grad_prec = 0.05 if reparameterized else 0.1\n    for actual_grad, expected_grad in zip(actual_grads, expected_grads):\n        assert_equal(actual_grad, expected_grad, prec=grad_prec, msg="""".join([\n            ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n            ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n        ]))\n\n\n@pytest.mark.parametrize(""depth"", [1, 2, 3, 4])\n@pytest.mark.parametrize(""num_samples,expand"", [(200, False)])\n@pytest.mark.parametrize(""max_plate_nesting"", [0])\n@pytest.mark.parametrize(""guide_type"", [""prior"", ""factorized"", ""nonfactorized""])\n@pytest.mark.parametrize(""reparameterized"", [False, True])\n@pytest.mark.parametrize(""tmc_strategy"", [""diagonal"", ""mixture""])\ndef test_tmc_normals_chain_gradient(depth, num_samples, max_plate_nesting, expand,\n                                    guide_type, reparameterized, tmc_strategy):\n    # compare reparameterized and nonreparameterized gradient estimates\n    q2 = pyro.param(""q2"", torch.tensor(0.5, requires_grad=True))\n    qs = (q2.unconstrained(),)\n\n    def model(reparameterized):\n        Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n        x = pyro.sample(""x0"", Normal(pyro.param(""q2""), math.sqrt(1. / depth)))\n        for i in range(1, depth):\n            x = pyro.sample(""x{}"".format(i), Normal(x, math.sqrt(1. / depth)))\n        pyro.sample(""y"", Normal(x, 1.), obs=torch.tensor(float(1)))\n\n    def factorized_guide(reparameterized):\n        Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n        pyro.sample(""x0"", Normal(pyro.param(""q2""), math.sqrt(1. / depth)))\n        for i in range(1, depth):\n            pyro.sample(""x{}"".format(i), Normal(0., math.sqrt(float(i+1) / depth)))\n\n    def nonfactorized_guide(reparameterized):\n        Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n        x = pyro.sample(""x0"", Normal(pyro.param(""q2""), math.sqrt(1. / depth)))\n        for i in range(1, depth):\n            x = pyro.sample(""x{}"".format(i), Normal(x, math.sqrt(1. / depth)))\n\n    tmc = TraceTMC_ELBO(max_plate_nesting=max_plate_nesting)\n    tmc_model = config_enumerate(\n        model, default=""parallel"", expand=expand, num_samples=num_samples, tmc=tmc_strategy)\n    guide = factorized_guide if guide_type == ""factorized"" else \\\n        nonfactorized_guide if guide_type == ""nonfactorized"" else \\\n        lambda *args: None\n    tmc_guide = config_enumerate(\n        guide, default=""parallel"", expand=expand, num_samples=num_samples, tmc=tmc_strategy)\n\n    # gold values from Funsor\n    expected_grads = (torch.tensor(\n        {1: 0.0999, 2: 0.0860, 3: 0.0802, 4: 0.0771}[depth]\n    ),)\n\n    # convert to linear space for unbiasedness\n    actual_loss = (-tmc.differentiable_loss(tmc_model, tmc_guide, reparameterized)).exp()\n    actual_grads = grad(actual_loss, qs)\n\n    grad_prec = 0.05 if reparameterized else 0.1\n\n    for actual_grad, expected_grad in zip(actual_grads, expected_grads):\n        print(actual_loss)\n        assert_equal(actual_grad, expected_grad, prec=grad_prec, msg="""".join([\n            ""\\nexpected grad = {}"".format(expected_grad.detach().cpu().numpy()),\n            ""\\n  actual grad = {}"".format(actual_grad.detach().cpu().numpy()),\n        ]))\n'"
tests/infer/test_util.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nimport pytest\nfrom pyro.infer.importance import psis_diagnostic\nfrom pyro.infer.util import MultiFrameTensor\nfrom tests.common import assert_equal\n\n\ndef xy_model():\n    d = dist.Bernoulli(0.5)\n    x_axis = pyro.plate(\'x_axis\', 2, dim=-1)\n    y_axis = pyro.plate(\'y_axis\', 3, dim=-2)\n    pyro.sample(\'b\', d)\n    with x_axis:\n        pyro.sample(\'bx\', d.expand_by([2]))\n    with y_axis:\n        pyro.sample(\'by\', d.expand_by([3, 1]))\n    with x_axis, y_axis:\n        pyro.sample(\'bxy\', d.expand_by([3, 2]))\n\n\ndef test_multi_frame_tensor():\n    stacks = {}\n    actual = MultiFrameTensor()\n    tr = poutine.trace(xy_model).get_trace()\n    for name, site in tr.nodes.items():\n        if site[""type""] == ""sample"":\n            log_prob = site[""fn""].log_prob(site[""value""])\n            stacks[name] = site[""cond_indep_stack""]\n            actual.add((site[""cond_indep_stack""], log_prob))\n\n    assert len(actual) == 4\n\n    logp = math.log(0.5)\n    expected = {\n        \'b\': torch.ones(torch.Size()) * logp * (1 + 2 + 3 + 6),\n        \'bx\': torch.ones(torch.Size((2,))) * logp * (1 + 1 + 3 + 3),\n        \'by\': torch.ones(torch.Size((3, 1))) * logp * (1 + 2 + 1 + 2),\n        \'bxy\': torch.ones(torch.Size((3, 2))) * logp * (1 + 1 + 1 + 1),\n    }\n    for name, expected_sum in expected.items():\n        actual_sum = actual.sum_to(stacks[name])\n        assert_equal(actual_sum, expected_sum, msg=name)\n\n\n@pytest.mark.parametrize(\'max_particles\', [250 * 1000, 500 * 1000])\n@pytest.mark.parametrize(\'scale,krange\', [(0.5, (0.7, 0.9)),\n                                          (0.95, (0.05, 0.2))])\n@pytest.mark.parametrize(\'zdim\', [1, 5])\ndef test_psis_diagnostic(scale, krange, zdim, max_particles, num_particles=500 * 1000):\n\n    def model(zdim=1, scale=1.0):\n        with pyro.plate(""x_axis"", zdim, dim=-1):\n            pyro.sample(""z"", dist.Normal(0.0, 1.0).expand([zdim]))\n\n    def guide(zdim=1, scale=1.0):\n        with pyro.plate(""x_axis"", zdim, dim=-1):\n            pyro.sample(""z"", dist.Normal(0.0, scale).expand([zdim]))\n\n    k = psis_diagnostic(model, guide, num_particles=num_particles, max_simultaneous_particles=max_particles,\n                        zdim=zdim, scale=scale)\n    assert k > krange[0] and k < krange[1]\n'"
tests/infer/test_valid_models.py,207,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport warnings\nfrom collections import defaultdict\n\nimport pytest\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.distributions.testing import fakes\nfrom pyro.infer import (SVI, EnergyDistance, Trace_ELBO, TraceEnum_ELBO, TraceGraph_ELBO, TraceMeanField_ELBO,\n                        TraceTailAdaptive_ELBO, config_enumerate)\nfrom pyro.infer.reparam import LatentStableReparam\nfrom pyro.infer.tracetmc_elbo import TraceTMC_ELBO\nfrom pyro.infer.util import torch_item\nfrom pyro.ops.indexing import Vindex\nfrom pyro.optim import Adam\nfrom tests.common import assert_close\n\nlogger = logging.getLogger(__name__)\n\n# This file tests a variety of model,guide pairs with valid and invalid structure.\n\n\ndef EnergyDistance_prior(**kwargs):\n    kwargs[""prior_scale""] = 0.0\n    kwargs.pop(""strict_enumeration_warning"", None)\n    return EnergyDistance(**kwargs)\n\n\ndef EnergyDistance_noprior(**kwargs):\n    kwargs[""prior_scale""] = 1.0\n    kwargs.pop(""strict_enumeration_warning"", None)\n    return EnergyDistance(**kwargs)\n\n\ndef assert_ok(model, guide, elbo, **kwargs):\n    """"""\n    Assert that inference works without warnings or errors.\n    """"""\n    pyro.clear_param_store()\n    inference = SVI(model, guide, Adam({""lr"": 1e-6}), elbo)\n    inference.step(**kwargs)\n    try:\n        pyro.set_rng_seed(0)\n        loss = elbo.loss(model, guide, **kwargs)\n        if hasattr(elbo, ""differentiable_loss""):\n            try:\n                pyro.set_rng_seed(0)\n                differentiable_loss = torch_item(elbo.differentiable_loss(model, guide, **kwargs))\n            except ValueError:\n                pass  # Ignore cases where elbo cannot be differentiated\n            else:\n                assert_close(differentiable_loss, loss, atol=0.01)\n        if hasattr(elbo, ""loss_and_grads""):\n            pyro.set_rng_seed(0)\n            loss_and_grads = elbo.loss_and_grads(model, guide, **kwargs)\n            assert_close(loss_and_grads, loss, atol=0.01)\n    except NotImplementedError:\n        pass  # Ignore cases where loss isn\'t implemented, eg. TraceTailAdaptive_ELBO\n\n\ndef assert_error(model, guide, elbo, match=None):\n    """"""\n    Assert that inference fails with an error.\n    """"""\n    pyro.clear_param_store()\n    inference = SVI(model,  guide, Adam({""lr"": 1e-6}), elbo)\n    with pytest.raises((NotImplementedError, UserWarning, KeyError, ValueError, RuntimeError),\n                       match=match):\n        inference.step()\n\n\ndef assert_warning(model, guide, elbo):\n    """"""\n    Assert that inference works but with a warning.\n    """"""\n    pyro.clear_param_store()\n    inference = SVI(model,  guide, Adam({""lr"": 1e-6}), elbo)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        inference.step()\n        assert len(w), \'No warnings were raised\'\n        for warning in w:\n            logger.info(warning)\n\n\n@pytest.mark.parametrize(""Elbo"", [\n    Trace_ELBO,\n    TraceGraph_ELBO,\n    TraceEnum_ELBO,\n    TraceTMC_ELBO,\n    EnergyDistance_prior,\n    EnergyDistance_noprior,\n])\n@pytest.mark.parametrize(""strict_enumeration_warning"", [True, False])\ndef test_nonempty_model_empty_guide_ok(Elbo, strict_enumeration_warning):\n\n    def model():\n        loc = torch.tensor([0.0, 0.0])\n        scale = torch.tensor([1.0, 1.0])\n        pyro.sample(""x"", dist.Normal(loc, scale).to_event(1), obs=loc)\n\n    def guide():\n        pass\n\n    elbo = Elbo(strict_enumeration_warning=strict_enumeration_warning)\n    if strict_enumeration_warning and Elbo in (TraceEnum_ELBO, TraceTMC_ELBO):\n        assert_warning(model, guide, elbo)\n    else:\n        assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""Elbo"", [\n    Trace_ELBO,\n    TraceGraph_ELBO,\n    TraceEnum_ELBO,\n    TraceTMC_ELBO,\n    EnergyDistance_prior,\n    EnergyDistance_noprior,\n])\n@pytest.mark.parametrize(""strict_enumeration_warning"", [True, False])\ndef test_nonempty_model_empty_guide_error(Elbo, strict_enumeration_warning):\n\n    def model():\n        pyro.sample(""x"", dist.Normal(0, 1))\n\n    def guide():\n        pass\n\n    elbo = Elbo(strict_enumeration_warning=strict_enumeration_warning)\n    assert_error(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\n@pytest.mark.parametrize(""strict_enumeration_warning"", [True, False])\ndef test_empty_model_empty_guide_ok(Elbo, strict_enumeration_warning):\n\n    def model():\n        pass\n\n    def guide():\n        pass\n\n    elbo = Elbo(strict_enumeration_warning=strict_enumeration_warning)\n    if strict_enumeration_warning and Elbo in (TraceEnum_ELBO, TraceTMC_ELBO):\n        assert_warning(model, guide, elbo)\n    else:\n        assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_variable_clash_in_model_error(Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        pyro.sample(""x"", dist.Bernoulli(p))\n        pyro.sample(""x"", dist.Bernoulli(p))  # Should error here.\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    assert_error(model, guide, Elbo(), match=\'Multiple sample sites named\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_model_guide_dim_mismatch_error(Elbo):\n\n    def model():\n        loc = torch.zeros(2)\n        scale = torch.ones(2)\n        pyro.sample(""x"", dist.Normal(loc, scale).to_event(1))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.zeros(2, 1, requires_grad=True))\n        scale = pyro.param(""scale"", torch.ones(2, 1, requires_grad=True))\n        pyro.sample(""x"", dist.Normal(loc, scale).to_event(2))\n\n    assert_error(model, guide, Elbo(strict_enumeration_warning=False),\n                 match=\'invalid log_prob shape|Model and guide event_dims disagree\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_model_guide_shape_mismatch_error(Elbo):\n\n    def model():\n        loc = torch.zeros(1, 2)\n        scale = torch.ones(1, 2)\n        pyro.sample(""x"", dist.Normal(loc, scale).to_event(2))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.zeros(2, 1, requires_grad=True))\n        scale = pyro.param(""scale"", torch.ones(2, 1, requires_grad=True))\n        pyro.sample(""x"", dist.Normal(loc, scale).to_event(2))\n\n    assert_error(model, guide, Elbo(strict_enumeration_warning=False),\n                 match=\'Model and guide shapes disagree\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_variable_clash_in_guide_error(Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        pyro.sample(""x"", dist.Bernoulli(p))\n        pyro.sample(""x"", dist.Bernoulli(p))  # Should error here.\n\n    assert_error(model, guide, Elbo(), match=\'Multiple sample sites named\')\n\n\n@pytest.mark.parametrize(""has_rsample"", [False, True])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_set_has_rsample_ok(has_rsample, Elbo):\n\n    # This model has sparse gradients, so users may want to disable\n    # reparametrized sampling to reduce variance of gradient estimates.\n    # However both versions should be correct, i.e. with or without has_rsample.\n    def model():\n        z = pyro.sample(""z"", dist.Normal(0, 1))\n        loc = (z * 100).clamp(min=0, max=1)  # sparse gradients\n        pyro.sample(""x"", dist.Normal(loc, 1), obs=torch.tensor(0.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        pyro.sample(""z"", dist.Normal(loc, 1).has_rsample_(has_rsample))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo(strict_enumeration_warning=False))\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_not_has_rsample_ok(Elbo):\n\n    def model():\n        z = pyro.sample(""z"", dist.Normal(0, 1))\n        p = z.round().clamp(min=0.2, max=0.8)  # discontinuous\n        pyro.sample(""x"", dist.Bernoulli(p), obs=torch.tensor(0.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        pyro.sample(""z"", dist.Normal(loc, 1).has_rsample_(False))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo(strict_enumeration_warning=False))\n\n\n@pytest.mark.parametrize(""subsample_size"", [None, 2], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_iplate_ok(subsample_size, Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        for i in pyro.plate(""plate"", 4, subsample_size):\n            pyro.sample(""x_{}"".format(i), dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        for i in pyro.plate(""plate"", 4, subsample_size):\n            pyro.sample(""x_{}"".format(i), dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_iplate_variable_clash_error(Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        for i in pyro.plate(""plate"", 2):\n            # Each loop iteration should give the sample site a different name.\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        for i in pyro.plate(""plate"", 2):\n            # Each loop iteration should give the sample site a different name.\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_error(model, guide, Elbo(), match=\'Multiple sample sites named\')\n\n\n@pytest.mark.parametrize(""subsample_size"", [None, 5], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_plate_ok(subsample_size, Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate(""plate"", 10, subsample_size) as ind:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind)]))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        with pyro.plate(""plate"", 10, subsample_size) as ind:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind)]))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""subsample_size"", [None, 5], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_plate_subsample_param_ok(subsample_size, Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate(""plate"", 10, subsample_size):\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        with pyro.plate(""plate"", 10, subsample_size) as ind:\n            p0 = pyro.param(""p0"", torch.tensor(0.), event_dim=0)\n            assert p0.shape == ()\n            p = pyro.param(""p"", 0.5 * torch.ones(10), event_dim=0)\n            assert len(p) == len(ind)\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""subsample_size"", [None, 5], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_plate_subsample_primitive_ok(subsample_size, Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate(""plate"", 10, subsample_size):\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        with pyro.plate(""plate"", 10, subsample_size) as ind:\n            p0 = torch.tensor(0.)\n            p0 = pyro.subsample(p0, event_dim=0)\n            assert p0.shape == ()\n            p = 0.5 * torch.ones(10)\n            p = pyro.subsample(p, event_dim=0)\n            assert len(p) == len(ind)\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""subsample_size"", [None, 5], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\n@pytest.mark.parametrize(""shape,ok"", [\n    ((), True),\n    ((1,), True),\n    ((10,), True),\n    ((3, 1), True),\n    ((3, 10), True),\n    ((5), False),\n    ((3, 5), False),\n])\ndef test_plate_param_size_mismatch_error(subsample_size, Elbo, shape, ok):\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate(""plate"", 10, subsample_size):\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        with pyro.plate(""plate"", 10, subsample_size):\n            pyro.param(""p0"", torch.ones(shape), event_dim=0)\n            p = pyro.param(""p"", torch.ones(10), event_dim=0)\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    if ok:\n        assert_ok(model, guide, Elbo())\n    else:\n        assert_error(model, guide, Elbo(), match=""invalid shape of pyro.param"")\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_plate_no_size_ok(Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate(""plate""):\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([10]))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        with pyro.plate(""plate""):\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([10]))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, default=""parallel"", num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""max_plate_nesting"", [0, float(\'inf\')])\n@pytest.mark.parametrize(""subsample_size"", [None, 2], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_iplate_iplate_ok(subsample_size, Elbo, max_plate_nesting):\n\n    def model():\n        p = torch.tensor(0.5)\n        outer_iplate = pyro.plate(""plate_0"", 3, subsample_size)\n        inner_iplate = pyro.plate(""plate_1"", 3, subsample_size)\n        for i in outer_iplate:\n            for j in inner_iplate:\n                pyro.sample(""x_{}_{}"".format(i, j), dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        outer_iplate = pyro.plate(""plate_0"", 3, subsample_size)\n        inner_iplate = pyro.plate(""plate_1"", 3, subsample_size)\n        for i in outer_iplate:\n            for j in inner_iplate:\n                pyro.sample(""x_{}_{}"".format(i, j), dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide, ""parallel"")\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo(max_plate_nesting=max_plate_nesting))\n\n\n@pytest.mark.parametrize(""max_plate_nesting"", [0, float(\'inf\')])\n@pytest.mark.parametrize(""subsample_size"", [None, 2], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_iplate_iplate_swap_ok(subsample_size, Elbo, max_plate_nesting):\n\n    def model():\n        p = torch.tensor(0.5)\n        outer_iplate = pyro.plate(""plate_0"", 3, subsample_size)\n        inner_iplate = pyro.plate(""plate_1"", 3, subsample_size)\n        for i in outer_iplate:\n            for j in inner_iplate:\n                pyro.sample(""x_{}_{}"".format(i, j), dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        outer_iplate = pyro.plate(""plate_0"", 3, subsample_size)\n        inner_iplate = pyro.plate(""plate_1"", 3, subsample_size)\n        for j in inner_iplate:\n            for i in outer_iplate:\n                pyro.sample(""x_{}_{}"".format(i, j), dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide, ""parallel"")\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, default=""parallel"", num_samples=2)\n\n    assert_ok(model, guide, Elbo(max_plate_nesting=max_plate_nesting))\n\n\n@pytest.mark.parametrize(""subsample_size"", [None, 5], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_iplate_in_model_not_guide_ok(subsample_size, Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        for i in pyro.plate(""plate"", 10, subsample_size):\n            pass\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""subsample_size"", [None, 5], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\n@pytest.mark.parametrize(""is_validate"", [True, False])\ndef test_iplate_in_guide_not_model_error(subsample_size, Elbo, is_validate):\n\n    def model():\n        p = torch.tensor(0.5)\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        for i in pyro.plate(""plate"", 10, subsample_size):\n            pass\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    with pyro.validation_enabled(is_validate):\n        if is_validate:\n            assert_error(model, guide, Elbo(),\n                         match=\'Found plate statements in guide but not model\')\n        else:\n            assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_plate_broadcast_error(Elbo):\n\n    def model():\n        p = torch.tensor(0.5, requires_grad=True)\n        with pyro.plate(""plate"", 10, 5):\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([2]))\n\n    assert_error(model, model, Elbo(), match=\'Shape mismatch inside plate\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_plate_iplate_ok(Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate(""plate"", 3, 2) as ind:\n            for i in pyro.plate(""iplate"", 3, 2):\n                pyro.sample(""x_{}"".format(i), dist.Bernoulli(p).expand_by([len(ind)]))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        with pyro.plate(""plate"", 3, 2) as ind:\n            for i in pyro.plate(""iplate"", 3, 2):\n                pyro.sample(""x_{}"".format(i), dist.Bernoulli(p).expand_by([len(ind)]))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_iplate_plate_ok(Elbo):\n\n    def model():\n        p = torch.tensor(0.5)\n        inner_plate = pyro.plate(""plate"", 3, 2)\n        for i in pyro.plate(""iplate"", 3, 2):\n            with inner_plate as ind:\n                pyro.sample(""x_{}"".format(i), dist.Bernoulli(p).expand_by([len(ind)]))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        inner_plate = pyro.plate(""plate"", 3, 2)\n        for i in pyro.plate(""iplate"", 3, 2):\n            with inner_plate as ind:\n                pyro.sample(""x_{}"".format(i), dist.Bernoulli(p).expand_by([len(ind)]))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\n@pytest.mark.parametrize(""sizes"", [(3,), (3, 4), (3, 4, 5)])\ndef test_plate_stack_ok(Elbo, sizes):\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate_stack(""plate_stack"", sizes):\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        with pyro.plate_stack(""plate_stack"", sizes):\n            pyro.sample(""x"", dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\n@pytest.mark.parametrize(""sizes"", [(3,), (3, 4), (3, 4, 5)])\ndef test_plate_stack_and_plate_ok(Elbo, sizes):\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate_stack(""plate_stack"", sizes):\n            with pyro.plate(""plate"", 7):\n                pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        with pyro.plate_stack(""plate_stack"", sizes):\n            with pyro.plate(""plate"", 7):\n                pyro.sample(""x"", dist.Bernoulli(p))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(guide, num_samples=2)\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""sizes"", [(3,), (3, 4), (3, 4, 5)])\ndef test_plate_stack_sizes(sizes):\n\n    def model():\n        p = 0.5 * torch.ones(3)\n        with pyro.plate_stack(""plate_stack"", sizes):\n            x = pyro.sample(""x"", dist.Bernoulli(p).to_event(1))\n            assert x.shape == sizes + (3,)\n\n    model()\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_nested_plate_plate_ok(Elbo):\n\n    def model():\n        p = torch.tensor(0.5, requires_grad=True)\n        with pyro.plate(""plate_outer"", 10, 5) as ind_outer:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind_outer)]))\n            with pyro.plate(""plate_inner"", 11, 6) as ind_inner:\n                pyro.sample(""y"", dist.Bernoulli(p).expand_by([len(ind_inner), len(ind_outer)]))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(model)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(model, num_samples=2)\n    else:\n        guide = model\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_plate_reuse_ok(Elbo):\n\n    def model():\n        p = torch.tensor(0.5, requires_grad=True)\n        plate_outer = pyro.plate(""plate_outer"", 10, 5, dim=-1)\n        plate_inner = pyro.plate(""plate_inner"", 11, 6, dim=-2)\n        with plate_outer as ind_outer:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind_outer)]))\n        with plate_inner as ind_inner:\n            pyro.sample(""y"", dist.Bernoulli(p).expand_by([len(ind_inner), 1]))\n        with plate_outer as ind_outer, plate_inner as ind_inner:\n            pyro.sample(""z"", dist.Bernoulli(p).expand_by([len(ind_inner), len(ind_outer)]))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(model)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(model, num_samples=2)\n    else:\n        guide = model\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO, TraceTMC_ELBO])\ndef test_nested_plate_plate_dim_error_1(Elbo):\n\n    def model():\n        p = torch.tensor([0.5], requires_grad=True)\n        with pyro.plate(""plate_outer"", 10, 5) as ind_outer:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind_outer)]))  # error here\n            with pyro.plate(""plate_inner"", 11, 6) as ind_inner:\n                pyro.sample(""y"", dist.Bernoulli(p).expand_by([len(ind_inner)]))\n                pyro.sample(""z"", dist.Bernoulli(p).expand_by([len(ind_outer), len(ind_inner)]))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(model)\n    elif Elbo is TraceTMC_ELBO:\n        guide = config_enumerate(model, num_samples=2)\n    else:\n        guide = model\n\n    assert_error(model, guide, Elbo(), match=\'invalid log_prob shape\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_nested_plate_plate_dim_error_2(Elbo):\n\n    def model():\n        p = torch.tensor([0.5], requires_grad=True)\n        with pyro.plate(""plate_outer"", 10, 5) as ind_outer:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind_outer), 1]))\n            with pyro.plate(""plate_inner"", 11, 6) as ind_inner:\n                pyro.sample(""y"", dist.Bernoulli(p).expand_by([len(ind_outer)]))  # error here\n                pyro.sample(""z"", dist.Bernoulli(p).expand_by([len(ind_outer), len(ind_inner)]))\n\n    guide = config_enumerate(model) if Elbo is TraceEnum_ELBO else model\n    assert_error(model, guide, Elbo(), match=\'Shape mismatch inside plate\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_nested_plate_plate_dim_error_3(Elbo):\n\n    def model():\n        p = torch.tensor([0.5], requires_grad=True)\n        with pyro.plate(""plate_outer"", 10, 5) as ind_outer:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind_outer), 1]))\n            with pyro.plate(""plate_inner"", 11, 6) as ind_inner:\n                pyro.sample(""y"", dist.Bernoulli(p).expand_by([len(ind_inner)]))\n                pyro.sample(""z"", dist.Bernoulli(p).expand_by([len(ind_inner), 1]))  # error here\n\n    guide = config_enumerate(model) if Elbo is TraceEnum_ELBO else model\n    assert_error(model, guide, Elbo(), match=\'invalid log_prob shape|shape mismatch\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_nested_plate_plate_dim_error_4(Elbo):\n\n    def model():\n        p = torch.tensor([0.5], requires_grad=True)\n        with pyro.plate(""plate_outer"", 10, 5) as ind_outer:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind_outer), 1]))\n            with pyro.plate(""plate_inner"", 11, 6) as ind_inner:\n                pyro.sample(""y"", dist.Bernoulli(p).expand_by([len(ind_inner)]))\n                pyro.sample(""z"", dist.Bernoulli(p).expand_by([len(ind_outer), len(ind_outer)]))  # error here\n\n    guide = config_enumerate(model) if Elbo is TraceEnum_ELBO else model\n    assert_error(model, guide, Elbo(), match=\'hape mismatch inside plate\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_nested_plate_plate_subsample_param_ok(Elbo):\n\n    def model():\n        with pyro.plate(""plate_outer"", 10, 5):\n            pyro.sample(""x"", dist.Bernoulli(0.2))\n            with pyro.plate(""plate_inner"", 11, 6):\n                pyro.sample(""y"", dist.Bernoulli(0.2))\n\n    def guide():\n        p0 = pyro.param(""p0"", 0.5 * torch.ones(4, 5), event_dim=2)\n        assert p0.shape == (4, 5)\n        with pyro.plate(""plate_outer"", 10, 5):\n            p1 = pyro.param(""p1"", 0.5 * torch.ones(10, 3), event_dim=1)\n            assert p1.shape == (5, 3)\n            px = pyro.param(""px"", 0.5 * torch.ones(10), event_dim=0)\n            assert px.shape == (5,)\n            pyro.sample(""x"", dist.Bernoulli(px))\n            with pyro.plate(""plate_inner"", 11, 6):\n                py = pyro.param(""py"", 0.5 * torch.ones(11, 10), event_dim=0)\n                assert py.shape == (6, 5)\n                pyro.sample(""y"", dist.Bernoulli(py))\n\n    if Elbo is TraceEnum_ELBO:\n        guide = config_enumerate(guide)\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_nonnested_plate_plate_ok(Elbo):\n\n    def model():\n        p = torch.tensor(0.5, requires_grad=True)\n        with pyro.plate(""plate_0"", 10, 5) as ind1:\n            pyro.sample(""x0"", dist.Bernoulli(p).expand_by([len(ind1)]))\n        with pyro.plate(""plate_1"", 11, 6) as ind2:\n            pyro.sample(""x1"", dist.Bernoulli(p).expand_by([len(ind2)]))\n\n    guide = config_enumerate(model) if Elbo is TraceEnum_ELBO else model\n    assert_ok(model, guide, Elbo())\n\n\ndef test_three_indep_plate_at_different_depths_ok():\n    r""""""\n      /\\\n     /\\ ia\n    ia ia\n    """"""\n    def model():\n        p = torch.tensor(0.5)\n        inner_plate = pyro.plate(""plate2"", 10, 5)\n        for i in pyro.plate(""plate0"", 2):\n            pyro.sample(""x_%d"" % i, dist.Bernoulli(p))\n            if i == 0:\n                for j in pyro.plate(""plate1"", 2):\n                    with inner_plate as ind:\n                        pyro.sample(""y_%d"" % j, dist.Bernoulli(p).expand_by([len(ind)]))\n            elif i == 1:\n                with inner_plate as ind:\n                    pyro.sample(""z"", dist.Bernoulli(p).expand_by([len(ind)]))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        inner_plate = pyro.plate(""plate2"", 10, 5)\n        for i in pyro.plate(""plate0"", 2):\n            pyro.sample(""x_%d"" % i, dist.Bernoulli(p))\n            if i == 0:\n                for j in pyro.plate(""plate1"", 2):\n                    with inner_plate as ind:\n                        pyro.sample(""y_%d"" % j, dist.Bernoulli(p).expand_by([len(ind)]))\n            elif i == 1:\n                with inner_plate as ind:\n                    pyro.sample(""z"", dist.Bernoulli(p).expand_by([len(ind)]))\n\n    assert_ok(model, guide, TraceGraph_ELBO())\n\n\ndef test_plate_wrong_size_error():\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate(""plate"", 10, 5) as ind:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([1 + len(ind)]))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        with pyro.plate(""plate"", 10, 5) as ind:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([1 + len(ind)]))\n\n    assert_error(model, guide, TraceGraph_ELBO(), match=\'Shape mismatch inside plate\')\n\n\n@pytest.mark.parametrize(""enumerate_"", [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_enum_discrete_misuse_warning(Elbo, enumerate_):\n\n    def model():\n        p = torch.tensor(0.5)\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        pyro.sample(""x"", dist.Bernoulli(p), infer={""enumerate"": enumerate_})\n\n    if (enumerate_ is None) == (Elbo is TraceEnum_ELBO):\n        assert_warning(model, guide, Elbo(max_plate_nesting=0))\n    else:\n        assert_ok(model, guide, Elbo(max_plate_nesting=0))\n\n\ndef test_enum_discrete_single_ok():\n\n    def model():\n        p = torch.tensor(0.5)\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    assert_ok(model, config_enumerate(guide), TraceEnum_ELBO())\n\n\n@pytest.mark.parametrize(""strict_enumeration_warning"", [False, True])\ndef test_enum_discrete_missing_config_warning(strict_enumeration_warning):\n\n    def model():\n        p = torch.tensor(0.5)\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        pyro.sample(""x"", dist.Bernoulli(p))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=strict_enumeration_warning)\n    if strict_enumeration_warning:\n        assert_warning(model, guide, elbo)\n    else:\n        assert_ok(model, guide, elbo)\n\n\ndef test_enum_discrete_single_single_ok():\n\n    def model():\n        p = torch.tensor(0.5)\n        pyro.sample(""x"", dist.Bernoulli(p))\n        pyro.sample(""y"", dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        pyro.sample(""x"", dist.Bernoulli(p))\n        pyro.sample(""y"", dist.Bernoulli(p))\n\n    assert_ok(model, config_enumerate(guide), TraceEnum_ELBO())\n\n\ndef test_enum_discrete_iplate_single_ok():\n\n    def model():\n        p = torch.tensor(0.5)\n        for i in pyro.plate(""plate"", 10, 5):\n            pyro.sample(""x_{}"".format(i), dist.Bernoulli(p))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        for i in pyro.plate(""plate"", 10, 5):\n            pyro.sample(""x_{}"".format(i), dist.Bernoulli(p))\n\n    assert_ok(model, config_enumerate(guide), TraceEnum_ELBO())\n\n\ndef test_plate_enum_discrete_batch_ok():\n\n    def model():\n        p = torch.tensor(0.5)\n        with pyro.plate(""plate"", 10, 5) as ind:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind)]))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        with pyro.plate(""plate"", 10, 5) as ind:\n            pyro.sample(""x"", dist.Bernoulli(p).expand_by([len(ind)]))\n\n    assert_ok(model, config_enumerate(guide), TraceEnum_ELBO())\n\n\n@pytest.mark.parametrize(""strict_enumeration_warning"", [False, True])\ndef test_plate_enum_discrete_no_discrete_vars_warning(strict_enumeration_warning):\n\n    def model():\n        loc = torch.tensor(0.0)\n        scale = torch.tensor(1.0)\n        with pyro.plate(""plate"", 10, 5) as ind:\n            pyro.sample(""x"", dist.Normal(loc, scale).expand_by([len(ind)]))\n\n    @config_enumerate(default=""sequential"")\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(1.0, requires_grad=True))\n        scale = pyro.param(""scale"", torch.tensor(2.0, requires_grad=True))\n        with pyro.plate(""plate"", 10, 5) as ind:\n            pyro.sample(""x"", dist.Normal(loc, scale).expand_by([len(ind)]))\n\n    elbo = TraceEnum_ELBO(strict_enumeration_warning=strict_enumeration_warning)\n    if strict_enumeration_warning:\n        assert_warning(model, guide, elbo)\n    else:\n        assert_ok(model, guide, elbo)\n\n\ndef test_no_plate_enum_discrete_batch_error():\n\n    def model():\n        p = torch.tensor(0.5)\n        pyro.sample(""x"", dist.Bernoulli(p).expand_by([5]))\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        pyro.sample(""x"", dist.Bernoulli(p).expand_by([5]))\n\n    assert_error(model, config_enumerate(guide), TraceEnum_ELBO(),\n                 match=\'invalid log_prob shape\')\n\n\n@pytest.mark.parametrize(\'max_plate_nesting\', [0, 1, 2, float(\'inf\')])\ndef test_enum_discrete_parallel_ok(max_plate_nesting):\n    guessed_nesting = 0 if max_plate_nesting == float(\'inf\') else max_plate_nesting\n    plate_shape = torch.Size([1] * guessed_nesting)\n\n    def model():\n        p = torch.tensor(0.5)\n        x = pyro.sample(""x"", dist.Bernoulli(p))\n        if max_plate_nesting != float(\'inf\'):\n            assert x.shape == torch.Size([2]) + plate_shape\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor(0.5, requires_grad=True))\n        x = pyro.sample(""x"", dist.Bernoulli(p))\n        if max_plate_nesting != float(\'inf\'):\n            assert x.shape == torch.Size([2]) + plate_shape\n\n    assert_ok(model, config_enumerate(guide, ""parallel""),\n              TraceEnum_ELBO(max_plate_nesting=max_plate_nesting))\n\n\n@pytest.mark.parametrize(\'max_plate_nesting\', [0, 1, 2, float(\'inf\')])\ndef test_enum_discrete_parallel_nested_ok(max_plate_nesting):\n    guessed_nesting = 0 if max_plate_nesting == float(\'inf\') else max_plate_nesting\n    plate_shape = torch.Size([1] * guessed_nesting)\n\n    def model():\n        p2 = torch.ones(2) / 2\n        p3 = torch.ones(3) / 3\n        x2 = pyro.sample(""x2"", dist.OneHotCategorical(p2))\n        x3 = pyro.sample(""x3"", dist.OneHotCategorical(p3))\n        if max_plate_nesting != float(\'inf\'):\n            assert x2.shape == torch.Size([2]) + plate_shape + p2.shape\n            assert x3.shape == torch.Size([3, 1]) + plate_shape + p3.shape\n\n    assert_ok(model, config_enumerate(model, ""parallel""),\n              TraceEnum_ELBO(max_plate_nesting=max_plate_nesting))\n\n\n@pytest.mark.parametrize(\'enumerate_,expand,num_samples\', [\n    (None, False, None),\n    (""sequential"", False, None),\n    (""sequential"", True, None),\n    (""parallel"", False, None),\n    (""parallel"", True, None),\n    (""parallel"", True, 3),\n])\ndef test_enumerate_parallel_plate_ok(enumerate_, expand, num_samples):\n\n    def model():\n        p2 = torch.ones(2) / 2\n        p34 = torch.ones(3, 4) / 4\n        p536 = torch.ones(5, 3, 6) / 6\n\n        x2 = pyro.sample(""x2"", dist.Categorical(p2))\n        with pyro.plate(""outer"", 3):\n            x34 = pyro.sample(""x34"", dist.Categorical(p34))\n            with pyro.plate(""inner"", 5):\n                x536 = pyro.sample(""x536"", dist.Categorical(p536))\n\n        # check shapes\n        if enumerate_ == ""parallel"":\n            if num_samples:\n                n = num_samples\n                # Meaning of dimensions:    [ enum dims | plate dims ]\n                assert x2.shape == torch.Size([        n, 1, 1])  # noqa: E201\n                assert x34.shape == torch.Size([    n, 1, 1, 3])  # noqa: E201\n                assert x536.shape == torch.Size([n, 1, 1, 5, 3])  # noqa: E201\n            elif expand:\n                # Meaning of dimensions:    [ enum dims | plate dims ]\n                assert x2.shape == torch.Size([        2, 1, 1])  # noqa: E201\n                assert x34.shape == torch.Size([    4, 1, 1, 3])  # noqa: E201\n                assert x536.shape == torch.Size([6, 1, 1, 5, 3])  # noqa: E201\n            else:\n                # Meaning of dimensions:    [ enum dims | plate placeholders ]\n                assert x2.shape == torch.Size([        2, 1, 1])  # noqa: E201\n                assert x34.shape == torch.Size([    4, 1, 1, 1])  # noqa: E201\n                assert x536.shape == torch.Size([6, 1, 1, 1, 1])  # noqa: E201\n        elif enumerate_ == ""sequential"":\n            if expand:\n                # All dimensions are plate dimensions.\n                assert x2.shape == torch.Size([])\n                assert x34.shape == torch.Size([3])\n                assert x536.shape == torch.Size([5, 3])\n            else:\n                # All dimensions are plate placeholders.\n                assert x2.shape == torch.Size([])\n                assert x34.shape == torch.Size([1])\n                assert x536.shape == torch.Size([1, 1])\n        else:\n            # All dimensions are plate dimensions.\n            assert x2.shape == torch.Size([])\n            assert x34.shape == torch.Size([3])\n            assert x536.shape == torch.Size([5, 3])\n\n    elbo = TraceEnum_ELBO(max_plate_nesting=2, strict_enumeration_warning=enumerate_)\n    guide = config_enumerate(model, enumerate_, expand, num_samples)\n    assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(\'max_plate_nesting\', [1, float(\'inf\')])\n@pytest.mark.parametrize(\'enumerate_\', [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(\'is_validate\', [True, False])\ndef test_enum_discrete_plate_dependency_warning(enumerate_, is_validate, max_plate_nesting):\n\n    def model():\n        pyro.sample(""w"", dist.Bernoulli(0.5), infer={\'enumerate\': \'parallel\'})\n        with pyro.plate(""plate"", 10, 5):\n            x = pyro.sample(""x"", dist.Bernoulli(0.5).expand_by([5]),\n                            infer={\'enumerate\': enumerate_})\n        pyro.sample(""y"", dist.Bernoulli(x.mean()))  # user should move this line up\n\n    with pyro.validation_enabled(is_validate):\n        elbo = TraceEnum_ELBO(max_plate_nesting=max_plate_nesting)\n        if enumerate_ and is_validate:\n            assert_warning(model, model, elbo)\n        else:\n            assert_ok(model, model, elbo)\n\n\n@pytest.mark.parametrize(\'max_plate_nesting\', [1, float(\'inf\')])\n@pytest.mark.parametrize(\'enumerate_\', [None, ""sequential"", ""parallel""])\ndef test_enum_discrete_iplate_plate_dependency_ok(enumerate_, max_plate_nesting):\n\n    def model():\n        pyro.sample(""w"", dist.Bernoulli(0.5), infer={\'enumerate\': \'parallel\'})\n        inner_plate = pyro.plate(""plate"", 10, 5)\n        for i in pyro.plate(""iplate"", 3):\n            pyro.sample(""y_{}"".format(i), dist.Bernoulli(0.5))\n            with inner_plate:\n                pyro.sample(""x_{}"".format(i), dist.Bernoulli(0.5).expand_by([5]),\n                            infer={\'enumerate\': enumerate_})\n\n    assert_ok(model, model, TraceEnum_ELBO(max_plate_nesting=max_plate_nesting))\n\n\n@pytest.mark.parametrize(\'max_plate_nesting\', [1, float(\'inf\')])\n@pytest.mark.parametrize(\'enumerate_\', [None, ""sequential"", ""parallel""])\n@pytest.mark.parametrize(\'is_validate\', [True, False])\ndef test_enum_discrete_iplates_plate_dependency_warning(enumerate_, is_validate, max_plate_nesting):\n\n    def model():\n        pyro.sample(""w"", dist.Bernoulli(0.5), infer={\'enumerate\': \'parallel\'})\n        inner_plate = pyro.plate(""plate"", 10, 5)\n\n        for i in pyro.plate(""iplate1"", 2):\n            with inner_plate:\n                pyro.sample(""x_{}"".format(i), dist.Bernoulli(0.5).expand_by([5]),\n                            infer={\'enumerate\': enumerate_})\n\n        for i in pyro.plate(""iplate2"", 2):\n            pyro.sample(""y_{}"".format(i), dist.Bernoulli(0.5))\n\n    with pyro.validation_enabled(is_validate):\n        elbo = TraceEnum_ELBO(max_plate_nesting=max_plate_nesting)\n        if enumerate_ and is_validate:\n            assert_warning(model, model, elbo)\n        else:\n            assert_ok(model, model, elbo)\n\n\n@pytest.mark.parametrize(\'enumerate_\', [None, ""sequential"", ""parallel""])\ndef test_enum_discrete_plates_dependency_ok(enumerate_):\n\n    def model():\n        pyro.sample(""w"", dist.Bernoulli(0.5), infer={\'enumerate\': \'parallel\'})\n        x_plate = pyro.plate(""x_plate"", 10, 5, dim=-1)\n        y_plate = pyro.plate(""y_plate"", 11, 6, dim=-2)\n        pyro.sample(""a"", dist.Bernoulli(0.5))\n        with x_plate:\n            pyro.sample(""b"", dist.Bernoulli(0.5).expand_by([5]))\n        with y_plate:\n            # Note that it is difficult to check that c does not depend on b.\n            pyro.sample(""c"", dist.Bernoulli(0.5).expand_by([6, 1]))\n        with x_plate, y_plate:\n            pyro.sample(""d"", dist.Bernoulli(0.5).expand_by([6, 5]))\n\n    assert_ok(model, model, TraceEnum_ELBO(max_plate_nesting=2))\n\n\n@pytest.mark.parametrize(\'enumerate_\', [None, ""sequential"", ""parallel""])\ndef test_enum_discrete_non_enumerated_plate_ok(enumerate_):\n\n    def model():\n        pyro.sample(""w"", dist.Bernoulli(0.5), infer={\'enumerate\': \'parallel\'})\n\n        with pyro.plate(""non_enum"", 2):\n            a = pyro.sample(""a"", dist.Bernoulli(0.5).expand_by([2]),\n                            infer={\'enumerate\': None})\n\n        p = (1.0 + a.sum(-1)) / (2.0 + a.size(0))  # introduce dependency of b on a\n\n        with pyro.plate(""enum_1"", 3):\n            pyro.sample(""b"", dist.Bernoulli(p).expand_by([3]),\n                        infer={\'enumerate\': enumerate_})\n\n    with pyro.validation_enabled():\n        assert_ok(model, model, TraceEnum_ELBO(max_plate_nesting=1))\n\n\ndef test_plate_shape_broadcasting():\n    data = torch.ones(1000, 2)\n\n    def model():\n        with pyro.plate(""num_particles"", 10, dim=-3):\n            with pyro.plate(""components"", 2, dim=-1):\n                p = pyro.sample(""p"", dist.Beta(torch.tensor(1.1), torch.tensor(1.1)))\n                assert p.shape == torch.Size((10, 1, 2))\n            with pyro.plate(""data"", data.shape[0], dim=-2):\n                pyro.sample(""obs"", dist.Bernoulli(p), obs=data)\n\n    def guide():\n        with pyro.plate(""num_particles"", 10, dim=-3):\n            with pyro.plate(""components"", 2, dim=-1):\n                pyro.sample(""p"", dist.Beta(torch.tensor(1.1), torch.tensor(1.1)))\n\n    assert_ok(model, guide, Trace_ELBO())\n\n\n@pytest.mark.parametrize(\'enumerate_,expand,num_samples\', [\n    (None, True, None),\n    (""sequential"", True, None),\n    (""sequential"", False, None),\n    (""parallel"", True, None),\n    (""parallel"", False, None),\n    (""parallel"", True, 3),\n])\ndef test_enum_discrete_plate_shape_broadcasting_ok(enumerate_, expand, num_samples):\n\n    def model():\n        x_plate = pyro.plate(""x_plate"", 10, 5, dim=-1)\n        y_plate = pyro.plate(""y_plate"", 11, 6, dim=-2)\n        with pyro.plate(""num_particles"", 50, dim=-3):\n            with x_plate:\n                b = pyro.sample(""b"", dist.Beta(torch.tensor(1.1), torch.tensor(1.1)))\n            with y_plate:\n                c = pyro.sample(""c"", dist.Bernoulli(0.5))\n            with x_plate, y_plate:\n                d = pyro.sample(""d"", dist.Bernoulli(b))\n\n        # check shapes\n        if enumerate_ == ""parallel"":\n            if num_samples and expand:\n                assert b.shape == (num_samples, 50, 1, 5)\n                assert c.shape == (num_samples, 1, 50, 6, 1)\n                assert d.shape == (num_samples, 1, num_samples, 50, 6, 5)\n            elif num_samples and not expand:\n                assert b.shape == (num_samples, 50, 1, 5)\n                assert c.shape == (num_samples, 1, 50, 6, 1)\n                assert d.shape == (num_samples, 1, 1, 50, 6, 5)\n            elif expand:\n                assert b.shape == (50, 1, 5)\n                assert c.shape == (2, 50, 6, 1)\n                assert d.shape == (2, 1, 50, 6, 5)\n            else:\n                assert b.shape == (50, 1, 5)\n                assert c.shape == (2, 1, 1, 1)\n                assert d.shape == (2, 1, 1, 1, 1)\n        elif enumerate_ == ""sequential"":\n            if expand:\n                assert b.shape == (50, 1, 5)\n                assert c.shape == (50, 6, 1)\n                assert d.shape == (50, 6, 5)\n            else:\n                assert b.shape == (50, 1, 5)\n                assert c.shape == (1, 1, 1)\n                assert d.shape == (1, 1, 1)\n        else:\n            assert b.shape == (50, 1, 5)\n            assert c.shape == (50, 6, 1)\n            assert d.shape == (50, 6, 5)\n\n    guide = config_enumerate(model, default=enumerate_, expand=expand, num_samples=num_samples)\n    elbo = TraceEnum_ELBO(max_plate_nesting=3,\n                          strict_enumeration_warning=(enumerate_ == ""parallel""))\n    assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""Elbo,expand"", [\n    (Trace_ELBO, False),\n    (TraceGraph_ELBO, False),\n    (TraceEnum_ELBO, False),\n    (TraceEnum_ELBO, True),\n])\ndef test_dim_allocation_ok(Elbo, expand):\n    enumerate_ = (Elbo is TraceEnum_ELBO)\n\n    def model():\n        p = torch.tensor(0.5, requires_grad=True)\n        with pyro.plate(""plate_outer"", 10, 5, dim=-3):\n            x = pyro.sample(""x"", dist.Bernoulli(p))\n            with pyro.plate(""plate_inner_1"", 11, 6):\n                y = pyro.sample(""y"", dist.Bernoulli(p))\n                # allocated dim is rightmost available, i.e. -1\n                with pyro.plate(""plate_inner_2"", 12, 7):\n                    z = pyro.sample(""z"", dist.Bernoulli(p))\n                    # allocated dim is next rightmost available, i.e. -2\n                    # since dim -3 is already allocated, use dim=-4\n                    with pyro.plate(""plate_inner_3"", 13, 8):\n                        q = pyro.sample(""q"", dist.Bernoulli(p))\n\n        # check shapes\n        if enumerate_ and not expand:\n            assert x.shape == (1, 1, 1)\n            assert y.shape == (1, 1, 1)\n            assert z.shape == (1, 1, 1)\n            assert q.shape == (1, 1, 1, 1)\n        else:\n            assert x.shape == (5, 1, 1)\n            assert y.shape == (5, 1, 6)\n            assert z.shape == (5, 7, 6)\n            assert q.shape == (8, 5, 7, 6)\n\n    guide = config_enumerate(model, ""sequential"", expand=expand) if enumerate_ else model\n    assert_ok(model, guide, Elbo(max_plate_nesting=4))\n\n\n@pytest.mark.parametrize(""Elbo,expand"", [\n    (Trace_ELBO, False),\n    (TraceGraph_ELBO, False),\n    (TraceEnum_ELBO, False),\n    (TraceEnum_ELBO, True),\n])\ndef test_dim_allocation_error(Elbo, expand):\n    enumerate_ = (Elbo is TraceEnum_ELBO)\n\n    def model():\n        p = torch.tensor(0.5, requires_grad=True)\n        with pyro.plate(""plate_outer"", 10, 5, dim=-2):\n            x = pyro.sample(""x"", dist.Bernoulli(p))\n            # allocated dim is rightmost available, i.e. -1\n            with pyro.plate(""plate_inner_1"", 11, 6):\n                y = pyro.sample(""y"", dist.Bernoulli(p))\n                # throws an error as dim=-1 is already occupied\n                with pyro.plate(""plate_inner_2"", 12, 7, dim=-1):\n                    pyro.sample(""z"", dist.Bernoulli(p))\n\n        # check shapes\n        if enumerate_ and not expand:\n            assert x.shape == (1, 1)\n            assert y.shape == (1, 1)\n        else:\n            assert x.shape == (5, 1)\n            assert y.shape == (5, 6)\n\n    guide = config_enumerate(model, expand=expand) if Elbo is TraceEnum_ELBO else model\n    assert_error(model, guide, Elbo(), match=\'collide at dim=\')\n\n\ndef test_enum_in_model_ok():\n    infer = {\'enumerate\': \'parallel\'}\n\n    def model():\n        p = pyro.param(\'p\', torch.tensor(0.25))\n        a = pyro.sample(\'a\', dist.Bernoulli(p))\n        b = pyro.sample(\'b\', dist.Bernoulli(p + a / 2))\n        c = pyro.sample(\'c\', dist.Bernoulli(p + b / 2), infer=infer)\n        d = pyro.sample(\'d\', dist.Bernoulli(p + c / 2))\n        e = pyro.sample(\'e\', dist.Bernoulli(p + d / 2))\n        f = pyro.sample(\'f\', dist.Bernoulli(p + e / 2), infer=infer)\n        g = pyro.sample(\'g\', dist.Bernoulli(p + f / 2), obs=torch.tensor(0.))\n\n        # check shapes\n        assert a.shape == ()\n        assert b.shape == (2,)\n        assert c.shape == (2, 1, 1)\n        assert d.shape == (2,)\n        assert e.shape == (2, 1)\n        assert f.shape == (2, 1, 1, 1)\n        assert g.shape == ()\n\n    def guide():\n        p = pyro.param(\'p\', torch.tensor(0.25))\n        a = pyro.sample(\'a\', dist.Bernoulli(p))\n        b = pyro.sample(\'b\', dist.Bernoulli(p + a / 2), infer=infer)\n        d = pyro.sample(\'d\', dist.Bernoulli(p + b / 2))\n        e = pyro.sample(\'e\', dist.Bernoulli(p + d / 2), infer=infer)\n\n        # check shapes\n        assert a.shape == ()\n        assert b.shape == (2,)\n        assert d.shape == (2,)\n        assert e.shape == (2, 1)\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0))\n\n\ndef test_enum_in_model_plate_ok():\n    infer = {\'enumerate\': \'parallel\'}\n\n    def model():\n        p = pyro.param(\'p\', torch.tensor(0.25))\n        a = pyro.sample(\'a\', dist.Bernoulli(p))\n        b = pyro.sample(\'b\', dist.Bernoulli(p + a / 2))\n        with pyro.plate(\'data\', 3):\n            c = pyro.sample(\'c\', dist.Bernoulli(p + b / 2), infer=infer)\n            d = pyro.sample(\'d\', dist.Bernoulli(p + c / 2))\n            e = pyro.sample(\'e\', dist.Bernoulli(p + d / 2))\n            f = pyro.sample(\'f\', dist.Bernoulli(p + e / 2), infer=infer)\n            g = pyro.sample(\'g\', dist.Bernoulli(p + f / 2), obs=torch.zeros(3))\n\n        # check shapes\n        assert a.shape == ()\n        assert b.shape == (2, 1)\n        assert c.shape == (2, 1, 1, 1)\n        assert d.shape == (2, 3)\n        assert e.shape == (2, 1, 1)\n        assert f.shape == (2, 1, 1, 1, 1)\n        assert g.shape == (3,)\n\n    def guide():\n        p = pyro.param(\'p\', torch.tensor(0.25))\n        a = pyro.sample(\'a\', dist.Bernoulli(p))\n        b = pyro.sample(\'b\', dist.Bernoulli(p + a / 2), infer=infer)\n        with pyro.plate(\'data\', 3):\n            d = pyro.sample(\'d\', dist.Bernoulli(p + b / 2))\n            e = pyro.sample(\'e\', dist.Bernoulli(p + d / 2), infer=infer)\n\n        # check shapes\n        assert a.shape == ()\n        assert b.shape == (2, 1)\n        assert d.shape == (2, 3)\n        assert e.shape == (2, 1, 1)\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=1))\n\n\ndef test_enum_sequential_in_model_error():\n\n    def model():\n        p = pyro.param(\'p\', torch.tensor(0.25))\n        pyro.sample(\'a\', dist.Bernoulli(p), infer={\'enumerate\': \'sequential\'})\n\n    def guide():\n        pass\n\n    assert_error(model, guide, TraceEnum_ELBO(max_plate_nesting=0),\n                 match=\'Found vars in model but not guide\')\n\n\ndef test_enum_in_model_plate_reuse_ok():\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p"", torch.tensor([0.2, 0.8]))\n        a = pyro.sample(""a"", dist.Bernoulli(0.3)).long()\n        with pyro.plate(""b_axis"", 2):\n            pyro.sample(""b"", dist.Bernoulli(p[a]), obs=torch.tensor([0., 1.]))\n        c = pyro.sample(""c"", dist.Bernoulli(0.3)).long()\n        with pyro.plate(""c_axis"", 2):\n            pyro.sample(""d"", dist.Bernoulli(p[c]), obs=torch.tensor([0., 0.]))\n\n    def guide():\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=1))\n\n\ndef test_enum_in_model_multi_scale_error():\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p"", torch.tensor([0.2, 0.8]))\n        x = pyro.sample(""x"", dist.Bernoulli(0.3)).long()\n        with poutine.scale(scale=2.):\n            pyro.sample(""y"", dist.Bernoulli(p[x]), obs=torch.tensor(0.))\n\n    def guide():\n        pass\n\n    assert_error(model, guide, TraceEnum_ELBO(max_plate_nesting=0),\n                 match=\'Expected all enumerated sample sites to share a common poutine.scale\')\n\n\n@pytest.mark.parametrize(\'use_vindex\', [False, True])\ndef test_enum_in_model_diamond_error(use_vindex):\n    data = torch.tensor([[0, 1], [0, 0]])\n\n    @config_enumerate\n    def model():\n        pyro.param(""probs_a"", torch.tensor([0.45, 0.55]))\n        pyro.param(""probs_b"", torch.tensor([[0.6, 0.4], [0.4, 0.6]]))\n        pyro.param(""probs_c"", torch.tensor([[0.75, 0.25], [0.55, 0.45]]))\n        pyro.param(""probs_d"", torch.tensor([[[0.4, 0.6], [0.3, 0.7]],\n                                            [[0.3, 0.7], [0.2, 0.8]]]))\n        probs_a = pyro.param(""probs_a"")\n        probs_b = pyro.param(""probs_b"")\n        probs_c = pyro.param(""probs_c"")\n        probs_d = pyro.param(""probs_d"")\n        b_axis = pyro.plate(""b_axis"", 2, dim=-1)\n        c_axis = pyro.plate(""c_axis"", 2, dim=-2)\n        a = pyro.sample(""a"", dist.Categorical(probs_a))\n        with b_axis:\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]))\n        with c_axis:\n            c = pyro.sample(""c"", dist.Categorical(probs_c[a]))\n        with b_axis, c_axis:\n            if use_vindex:\n                probs = Vindex(probs_d)[b, c]\n            else:\n                d_ind = torch.arange(2, dtype=torch.long)\n                probs = probs_d[b.unsqueeze(-1), c.unsqueeze(-1), d_ind]\n            pyro.sample(""d"", dist.Categorical(probs), obs=data)\n\n    def guide():\n        pass\n\n    assert_error(model, guide, TraceEnum_ELBO(max_plate_nesting=2),\n                 match=\'Expected tree-structured plate nesting\')\n\n\n@pytest.mark.parametrize(""Elbo"", [Trace_ELBO, TraceGraph_ELBO, TraceEnum_ELBO])\ndef test_vectorized_num_particles(Elbo):\n    data = torch.ones(1000, 2)\n\n    def model():\n        with pyro.plate(""components"", 2):\n            p = pyro.sample(""p"", dist.Beta(torch.tensor(1.1), torch.tensor(1.1)))\n            assert p.shape == torch.Size((10, 1, 2))\n            with pyro.plate(""data"", data.shape[0]):\n                pyro.sample(""obs"", dist.Bernoulli(p), obs=data)\n\n    def guide():\n        with pyro.plate(""components"", 2):\n            pyro.sample(""p"", dist.Beta(torch.tensor(1.1), torch.tensor(1.1)))\n\n    pyro.clear_param_store()\n    guide = config_enumerate(guide) if Elbo is TraceEnum_ELBO else guide\n    assert_ok(model, guide, Elbo(num_particles=10,\n                                 vectorize_particles=True,\n                                 max_plate_nesting=2,\n                                 strict_enumeration_warning=False))\n\n\n@pytest.mark.parametrize(\'enumerate_,expand,num_samples\', [\n    (None, False, None),\n    (""sequential"", False, None),\n    (""sequential"", True, None),\n    (""parallel"", False, None),\n    (""parallel"", True, None),\n    (""parallel"", True, 3),\n])\n@pytest.mark.parametrize(\'num_particles\', [1, 50])\ndef test_enum_discrete_vectorized_num_particles(enumerate_, expand, num_samples, num_particles):\n\n    @config_enumerate(default=enumerate_, expand=expand, num_samples=num_samples)\n    def model():\n        x_plate = pyro.plate(""x_plate"", 10, 5, dim=-1)\n        y_plate = pyro.plate(""y_plate"", 11, 6, dim=-2)\n        with x_plate:\n            b = pyro.sample(""b"", dist.Beta(torch.tensor(1.1), torch.tensor(1.1)))\n        with y_plate:\n            c = pyro.sample(""c"", dist.Bernoulli(0.5))\n        with x_plate, y_plate:\n            d = pyro.sample(""d"", dist.Bernoulli(b))\n\n        # check shapes\n        if num_particles > 1:\n            if enumerate_ == ""parallel"":\n                if num_samples and expand:\n                    assert b.shape == (num_samples, num_particles, 1, 5)\n                    assert c.shape == (num_samples, 1, num_particles, 6, 1)\n                    assert d.shape == (num_samples, 1, num_samples, num_particles, 6, 5)\n                elif num_samples and not expand:\n                    assert b.shape == (num_samples, num_particles, 1, 5)\n                    assert c.shape == (num_samples, 1, num_particles, 6, 1)\n                    assert d.shape == (num_samples, 1, 1, num_particles, 6, 5)\n                elif expand:\n                    assert b.shape == (num_particles, 1, 5)\n                    assert c.shape == (2, num_particles, 6, 1)\n                    assert d.shape == (2, 1, num_particles, 6, 5)\n                else:\n                    assert b.shape == (num_particles, 1, 5)\n                    assert c.shape == (2, 1, 1, 1)\n                    assert d.shape == (2, 1, 1, 1, 1)\n            elif enumerate_ == ""sequential"":\n                if expand:\n                    assert b.shape == (num_particles, 1, 5)\n                    assert c.shape == (num_particles, 6, 1)\n                    assert d.shape == (num_particles, 6, 5)\n                else:\n                    assert b.shape == (num_particles, 1, 5)\n                    assert c.shape == (1, 1, 1)\n                    assert d.shape == (1, 1, 1)\n            else:\n                assert b.shape == (num_particles, 1, 5)\n                assert c.shape == (num_particles, 6, 1)\n                assert d.shape == (num_particles, 6, 5)\n        else:\n            if enumerate_ == ""parallel"":\n                if num_samples and expand:\n                    assert b.shape == (num_samples, 1, 5,)\n                    assert c.shape == (num_samples, 1, 6, 1)\n                    assert d.shape == (num_samples, 1, num_samples, 6, 5)\n                elif num_samples and not expand:\n                    assert b.shape == (num_samples, 1, 5,)\n                    assert c.shape == (num_samples, 1, 6, 1)\n                    assert d.shape == (num_samples, 1, 1, 6, 5)\n                elif expand:\n                    assert b.shape == (5,)\n                    assert c.shape == (2, 6, 1)\n                    assert d.shape == (2, 1, 6, 5)\n                else:\n                    assert b.shape == (5,)\n                    assert c.shape == (2, 1, 1)\n                    assert d.shape == (2, 1, 1, 1)\n            elif enumerate_ == ""sequential"":\n                if expand:\n                    assert b.shape == (5,)\n                    assert c.shape == (6, 1)\n                    assert d.shape == (6, 5)\n                else:\n                    assert b.shape == (5,)\n                    assert c.shape == (1, 1)\n                    assert d.shape == (1, 1)\n            else:\n                assert b.shape == (5,)\n                assert c.shape == (6, 1)\n                assert d.shape == (6, 5)\n\n    assert_ok(model, model, TraceEnum_ELBO(max_plate_nesting=2,\n                                           num_particles=num_particles,\n                                           vectorize_particles=True,\n                                           strict_enumeration_warning=(enumerate_ == ""parallel"")))\n\n\ndef test_enum_recycling_chain():\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p"", torch.tensor([[0.2, 0.8], [0.1, 0.9]]))\n\n        x = 0\n        for t in pyro.markov(range(100)):\n            x = pyro.sample(""x_{}"".format(t), dist.Categorical(p[x]))\n            assert x.dim() <= 2\n\n    def guide():\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0))\n\n\n@pytest.mark.parametrize(\'use_vindex\', [False, True])\n@pytest.mark.parametrize(\'markov\', [False, True])\ndef test_enum_recycling_dbn(markov, use_vindex):\n    #    x --> x --> x  enum ""state""\n    # y  |  y  |  y  |  enum ""occlusion""\n    #  \\ |   \\ |   \\ |\n    #    z     z     z  obs\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p"", torch.ones(3, 3))\n        q = pyro.param(""q"", torch.ones(2))\n        r = pyro.param(""r"", torch.ones(3, 2, 4))\n\n        x = 0\n        times = pyro.markov(range(100)) if markov else range(11)\n        for t in times:\n            x = pyro.sample(""x_{}"".format(t), dist.Categorical(p[x]))\n            y = pyro.sample(""y_{}"".format(t), dist.Categorical(q))\n            if use_vindex:\n                probs = Vindex(r)[x, y]\n            else:\n                z_ind = torch.arange(4, dtype=torch.long)\n                probs = r[x.unsqueeze(-1), y.unsqueeze(-1), z_ind]\n            pyro.sample(""z_{}"".format(t), dist.Categorical(probs),\n                        obs=torch.tensor(0.))\n\n    def guide():\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0))\n\n\ndef test_enum_recycling_nested():\n    # (x)\n    #   \\\n    #    y0---(y1)--(y2)\n    #    |     |     |\n    #   z00   z10   z20\n    #    |     |     |\n    #   z01   z11  (z21)\n    #    |     |     |\n    #   z02   z12   z22 <-- what can this depend on?\n    #\n    # markov dependencies\n    # -------------------\n    #   x:\n    #  y0: x\n    # z00: x y0\n    # z01: x y0 z00\n    # z02: x y0 z01\n    #  y1: x y0\n    # z10: x y0 y1\n    # z11: x y0 y1 z10\n    # z12: x y0 y1 z11\n    #  y2: x y1\n    # z20: x y1 y2\n    # z21: x y1 y2 z20\n    # z22: x y1 y2 z21\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p"", torch.ones(3, 3))\n        x = pyro.sample(""x"", dist.Categorical(p[0]))\n        y = x\n        for i in pyro.markov(range(10)):\n            y = pyro.sample(""y_{}"".format(i), dist.Categorical(p[y]))\n            z = y\n            for j in pyro.markov(range(10)):\n                z = pyro.sample(""z_{}_{}"".format(i, j), dist.Categorical(p[z]))\n\n    def guide():\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0))\n\n\n@pytest.mark.parametrize(\'use_vindex\', [False, True])\ndef test_enum_recycling_grid(use_vindex):\n    #  x---x---x---x    -----> i\n    #  |   |   |   |   |\n    #  x---x---x---x   |\n    #  |   |   |   |   V\n    #  x---x---x--(x)  j\n    #  |   |   |   |\n    #  x---x--(x)--x <-- what can this depend on?\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p_leaf"", torch.ones(2, 2, 2))\n        x = defaultdict(lambda: torch.tensor(0))\n        y_axis = pyro.markov(range(4), keep=True)\n        for i in pyro.markov(range(4)):\n            for j in y_axis:\n                if use_vindex:\n                    probs = Vindex(p)[x[i - 1, j], x[i, j - 1]]\n                else:\n                    ind = torch.arange(2, dtype=torch.long)\n                    probs = p[x[i - 1, j].unsqueeze(-1),\n                              x[i, j - 1].unsqueeze(-1), ind]\n                x[i, j] = pyro.sample(""x_{}_{}"".format(i, j),\n                                      dist.Categorical(probs))\n\n    def guide():\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0))\n\n\ndef test_enum_recycling_reentrant():\n    data = (True, False)\n    for i in range(5):\n        data = (data, data, False)\n\n    @pyro.markov\n    def model(data, state=0, address=""""):\n        if isinstance(data, bool):\n            p = pyro.param(""p_leaf"", torch.ones(10))\n            pyro.sample(""leaf_{}"".format(address),\n                        dist.Bernoulli(p[state]),\n                        obs=torch.tensor(1. if data else 0.))\n        else:\n            p = pyro.param(""p_branch"", torch.ones(10, 10))\n            for branch, letter in zip(data, ""abcdefg""):\n                next_state = pyro.sample(""branch_{}"".format(address + letter),\n                                         dist.Categorical(p[state]),\n                                         infer={""enumerate"": ""parallel""})\n                model(branch, next_state, address + letter)\n\n    def guide(data):\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0), data=data)\n\n\n@pytest.mark.parametrize(\'history\', [1, 2])\ndef test_enum_recycling_reentrant_history(history):\n    data = (True, False)\n    for i in range(5):\n        data = (data, data, False)\n\n    @pyro.markov(history=history)\n    def model(data, state=0, address=""""):\n        if isinstance(data, bool):\n            p = pyro.param(""p_leaf"", torch.ones(10))\n            pyro.sample(""leaf_{}"".format(address),\n                        dist.Bernoulli(p[state]),\n                        obs=torch.tensor(1. if data else 0.))\n        else:\n            assert isinstance(data, tuple)\n            p = pyro.param(""p_branch"", torch.ones(10, 10))\n            for branch, letter in zip(data, ""abcdefg""):\n                next_state = pyro.sample(""branch_{}"".format(address + letter),\n                                         dist.Categorical(p[state]),\n                                         infer={""enumerate"": ""parallel""})\n                model(branch, next_state, address + letter)\n\n    def guide(data):\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0), data=data)\n\n\ndef test_enum_recycling_mutual_recursion():\n    data = (True, False)\n    for i in range(5):\n        data = (data, data, False)\n\n    def model_leaf(data, state=0, address=""""):\n        p = pyro.param(""p_leaf"", torch.ones(10))\n        pyro.sample(""leaf_{}"".format(address),\n                    dist.Bernoulli(p[state]),\n                    obs=torch.tensor(1. if data else 0.))\n\n    @pyro.markov\n    def model1(data, state=0, address=""""):\n        if isinstance(data, bool):\n            model_leaf(data, state, address)\n        else:\n            p = pyro.param(""p_branch"", torch.ones(10, 10))\n            for branch, letter in zip(data, ""abcdefg""):\n                next_state = pyro.sample(""branch_{}"".format(address + letter),\n                                         dist.Categorical(p[state]),\n                                         infer={""enumerate"": ""parallel""})\n                model2(branch, next_state, address + letter)\n\n    @pyro.markov\n    def model2(data, state=0, address=""""):\n        if isinstance(data, bool):\n            model_leaf(data, state, address)\n        else:\n            p = pyro.param(""p_branch"", torch.ones(10, 10))\n            for branch, letter in zip(data, ""abcdefg""):\n                next_state = pyro.sample(""branch_{}"".format(address + letter),\n                                         dist.Categorical(p[state]),\n                                         infer={""enumerate"": ""parallel""})\n                model1(branch, next_state, address + letter)\n\n    def guide(data):\n        pass\n\n    assert_ok(model1, guide, TraceEnum_ELBO(max_plate_nesting=0), data=data)\n\n\ndef test_enum_recycling_interleave():\n\n    def model():\n        with pyro.markov() as m:\n            with pyro.markov():\n                with m:  # error here\n                    pyro.sample(""x"", dist.Categorical(torch.ones(4)),\n                                infer={""enumerate"": ""parallel""})\n\n    def guide():\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0, strict_enumeration_warning=False))\n\n\ndef test_enum_recycling_plate():\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p"", torch.ones(3, 3))\n        q = pyro.param(""q"", torch.tensor([0.5, 0.5]))\n        plate_x = pyro.plate(""plate_x"", 2, dim=-1)\n        plate_y = pyro.plate(""plate_y"", 3, dim=-1)\n        plate_z = pyro.plate(""plate_z"", 4, dim=-2)\n\n        a = pyro.sample(""a"", dist.Bernoulli(q[0])).long()\n        w = 0\n        for i in pyro.markov(range(5)):\n            w = pyro.sample(""w_{}"".format(i), dist.Categorical(p[w]))\n\n        with plate_x:\n            b = pyro.sample(""b"", dist.Bernoulli(q[a])).long()\n            x = 0\n            for i in pyro.markov(range(6)):\n                x = pyro.sample(""x_{}"".format(i), dist.Categorical(p[x]))\n\n        with plate_y:\n            c = pyro.sample(""c"", dist.Bernoulli(q[a])).long()\n            y = 0\n            for i in pyro.markov(range(7)):\n                y = pyro.sample(""y_{}"".format(i), dist.Categorical(p[y]))\n\n        with plate_z:\n            d = pyro.sample(""d"", dist.Bernoulli(q[a])).long()\n            z = 0\n            for i in pyro.markov(range(8)):\n                z = pyro.sample(""z_{}"".format(i), dist.Categorical(p[z]))\n\n        with plate_x, plate_z:\n            e = pyro.sample(""e"", dist.Bernoulli(q[b])).long()\n            xz = 0\n            for i in pyro.markov(range(9)):\n                xz = pyro.sample(""xz_{}"".format(i), dist.Categorical(p[xz]))\n\n        return a, b, c, d, e\n\n    def guide():\n        pass\n\n    assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=2))\n\n\n@pytest.mark.parametrize(\'history\', [0, 1, 2, 3])\ndef test_markov_history(history):\n\n    @config_enumerate\n    def model():\n        p = pyro.param(""p"", 0.25 * torch.ones(2, 2))\n        q = pyro.param(""q"", 0.25 * torch.ones(2))\n        x_prev = torch.tensor(0)\n        x_curr = torch.tensor(0)\n        for t in pyro.markov(range(10), history=history):\n            probs = p[x_prev, x_curr]\n            x_prev, x_curr = x_curr, pyro.sample(""x_{}"".format(t), dist.Bernoulli(probs)).long()\n            pyro.sample(""y_{}"".format(t), dist.Bernoulli(q[x_curr]),\n                        obs=torch.tensor(0.))\n\n    def guide():\n        pass\n\n    if history < 2:\n        assert_error(model, guide, TraceEnum_ELBO(max_plate_nesting=0),\n                     match=""Enumeration dim conflict"")\n    else:\n        assert_ok(model, guide, TraceEnum_ELBO(max_plate_nesting=0))\n\n\ndef test_mean_field_ok():\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.Normal(x, 1.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        x = pyro.sample(""x"", dist.Normal(loc, 1.))\n        pyro.sample(""y"", dist.Normal(x, 1.))\n\n    assert_ok(model, guide, TraceMeanField_ELBO())\n\n\n@pytest.mark.parametrize(\'mask\', [True, False])\ndef test_mean_field_mask_ok(mask):\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.).mask(mask))\n        pyro.sample(""y"", dist.Normal(x, 1.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        x = pyro.sample(""x"", dist.Normal(loc, 1.).mask(mask))\n        pyro.sample(""y"", dist.Normal(x, 1.))\n\n    assert_ok(model, guide, TraceMeanField_ELBO())\n\n\ndef test_mean_field_warn():\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.Normal(x, 1.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        y = pyro.sample(""y"", dist.Normal(loc, 1.))\n        pyro.sample(""x"", dist.Normal(y, 1.))\n\n    assert_warning(model, guide, TraceMeanField_ELBO())\n\n\ndef test_tail_adaptive_ok():\n\n    def plateless_model():\n        pyro.sample(""x"", dist.Normal(0., 1.))\n\n    def plate_model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.))\n        with pyro.plate(\'observe_data\'):\n            pyro.sample(\'obs\', dist.Normal(x, 1.0), obs=torch.arange(5).type_as(x))\n\n    def rep_guide():\n        pyro.sample(""x"", dist.Normal(0., 2.))\n\n    assert_ok(plateless_model, rep_guide, TraceTailAdaptive_ELBO(vectorize_particles=True, num_particles=2))\n    assert_ok(plate_model, rep_guide, TraceTailAdaptive_ELBO(vectorize_particles=True, num_particles=2))\n\n\ndef test_tail_adaptive_error():\n\n    def plateless_model():\n        pyro.sample(""x"", dist.Normal(0., 1.))\n\n    def rep_guide():\n        pyro.sample(""x"", dist.Normal(0., 2.))\n\n    def nonrep_guide():\n        pyro.sample(""x"", fakes.NonreparameterizedNormal(0., 2.))\n\n    assert_error(plateless_model, rep_guide, TraceTailAdaptive_ELBO(vectorize_particles=False, num_particles=2))\n    assert_error(plateless_model, nonrep_guide, TraceTailAdaptive_ELBO(vectorize_particles=True, num_particles=2))\n\n\ndef test_tail_adaptive_warning():\n\n    def plateless_model():\n        pyro.sample(""x"", dist.Normal(0., 1.))\n\n    def rep_guide():\n        pyro.sample(""x"", dist.Normal(0., 2.))\n\n    assert_warning(plateless_model, rep_guide, TraceTailAdaptive_ELBO(vectorize_particles=True, num_particles=1))\n\n\n@pytest.mark.parametrize(""Elbo"", [\n    Trace_ELBO,\n    TraceMeanField_ELBO,\n    EnergyDistance_prior,\n    EnergyDistance_noprior,\n])\ndef test_reparam_ok(Elbo):\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.Normal(x, 1.), obs=torch.tensor(0.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        pyro.sample(""x"", dist.Normal(loc, 1.))\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""mask"", [True, False, torch.tensor(True), torch.tensor(False)])\n@pytest.mark.parametrize(""Elbo"", [\n    Trace_ELBO,\n    TraceMeanField_ELBO,\n    EnergyDistance_prior,\n    EnergyDistance_noprior,\n])\ndef test_reparam_mask_ok(Elbo, mask):\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.))\n        with poutine.mask(mask=mask):\n            pyro.sample(""y"", dist.Normal(x, 1.), obs=torch.tensor(0.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        pyro.sample(""x"", dist.Normal(loc, 1.))\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""mask"", [\n    True,\n    False,\n    torch.tensor(True),\n    torch.tensor(False),\n    torch.tensor([False, True]),\n])\n@pytest.mark.parametrize(""Elbo"", [\n    Trace_ELBO,\n    TraceMeanField_ELBO,\n    EnergyDistance_prior,\n    EnergyDistance_noprior,\n])\ndef test_reparam_mask_plate_ok(Elbo, mask):\n    data = torch.randn(2, 3).exp()\n    data /= data.sum(-1, keepdim=True)\n\n    def model():\n        c = pyro.sample(""c"", dist.LogNormal(0., 1.).expand([3]).to_event(1))\n        with pyro.plate(""data"", len(data)), poutine.mask(mask=mask):\n            pyro.sample(""obs"", dist.Dirichlet(c), obs=data)\n\n    def guide():\n        loc = pyro.param(""loc"", torch.zeros(3))\n        scale = pyro.param(""scale"", torch.ones(3),\n                           constraint=constraints.positive)\n        pyro.sample(""c"", dist.LogNormal(loc, scale).to_event(1))\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""scale"", [1, 0.1, torch.tensor(0.5)])\n@pytest.mark.parametrize(""Elbo"", [\n    Trace_ELBO,\n    TraceMeanField_ELBO,\n    EnergyDistance_prior,\n    EnergyDistance_noprior,\n])\ndef test_reparam_scale_ok(Elbo, scale):\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.))\n        with poutine.scale(scale=scale):\n            pyro.sample(""y"", dist.Normal(x, 1.), obs=torch.tensor(0.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        pyro.sample(""x"", dist.Normal(loc, 1.))\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""scale"", [\n    1,\n    0.1,\n    torch.tensor(0.5),\n    torch.tensor([0.1, 0.9]),\n])\n@pytest.mark.parametrize(""Elbo"", [\n    Trace_ELBO,\n    TraceMeanField_ELBO,\n    EnergyDistance_prior,\n    EnergyDistance_noprior,\n])\ndef test_reparam_scale_plate_ok(Elbo, scale):\n    data = torch.randn(2, 3).exp()\n    data /= data.sum(-1, keepdim=True)\n\n    def model():\n        c = pyro.sample(""c"", dist.LogNormal(0., 1.).expand([3]).to_event(1))\n        with pyro.plate(""data"", len(data)), poutine.scale(scale=scale):\n            pyro.sample(""obs"", dist.Dirichlet(c), obs=data)\n\n    def guide():\n        loc = pyro.param(""loc"", torch.zeros(3))\n        scale = pyro.param(""scale"", torch.ones(3),\n                           constraint=constraints.positive)\n        pyro.sample(""c"", dist.LogNormal(loc, scale).to_event(1))\n\n    assert_ok(model, guide, Elbo())\n\n\n@pytest.mark.parametrize(""Elbo"", [\n    EnergyDistance_prior,\n    EnergyDistance_noprior,\n])\ndef test_no_log_prob_ok(Elbo):\n\n    def model(data):\n        loc = pyro.sample(""loc"", dist.Normal(0, 1))\n        scale = pyro.sample(""scale"", dist.LogNormal(0, 1))\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", dist.Stable(1.5, 0.5, scale, loc),\n                        obs=data)\n\n    def guide(data):\n        map_loc = pyro.param(""map_loc"", torch.tensor(0.))\n        map_scale = pyro.param(""map_scale"", torch.tensor(1.),\n                               constraint=constraints.positive)\n        pyro.sample(""loc"", dist.Delta(map_loc))\n        pyro.sample(""scale"", dist.Delta(map_scale))\n\n    data = torch.randn(10)\n    assert_ok(model, guide, Elbo(), data=data)\n\n\ndef test_reparam_stable():\n\n    @poutine.reparam(config={""z"": LatentStableReparam()})\n    def model():\n        stability = pyro.sample(""stability"", dist.Uniform(0., 2.))\n        skew = pyro.sample(""skew"", dist.Uniform(-1., 1.))\n        y = pyro.sample(""z"", dist.Stable(stability, skew))\n        pyro.sample(""x"", dist.Poisson(y.abs()), obs=torch.tensor(1.))\n\n    def guide():\n        pyro.sample(""stability"", dist.Delta(torch.tensor(1.5)))\n        pyro.sample(""skew"", dist.Delta(torch.tensor(0.)))\n        pyro.sample(""z_uniform"", dist.Delta(torch.tensor(0.1)))\n        pyro.sample(""z_exponential"", dist.Delta(torch.tensor(1.)))\n\n    assert_ok(model, guide, Trace_ELBO())\n'"
tests/integration_tests/__init__.py,0,b''
tests/integration_tests/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/integration_tests""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""integration""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/integration_tests/test_conjugate_gaussian_models.py,34,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport os\nimport time\nfrom unittest import TestCase\n\nimport numpy as np\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim as optim\nfrom pyro.distributions.testing import fakes\nfrom pyro.infer import SVI, TraceGraph_ELBO\nfrom pyro.poutine import Trace\nfrom tests.common import assert_equal\n\nlogger = logging.getLogger(__name__)\n\n\ndef param_mse(name, target):\n    return torch.sum(torch.pow(target - pyro.param(name), 2.0)).detach().cpu().item()\n\n\nclass GaussianChain(TestCase):\n    # chain of normals with known covariances and latent means\n\n    def setUp(self):\n        self.loc0 = torch.tensor([0.2])\n        self.data = torch.tensor([-0.1, 0.03, 0.20, 0.10])\n        self.n_data = self.data.size(0)\n        self.sum_data = self.data.sum()\n\n    def setup_chain(self, N):\n        self.N = N  # number of latent variables in the chain\n        lambdas = [1.5 * (k + 1) / N for k in range(N + 1)]\n        self.lambdas = list(map(lambda x: torch.tensor([x]), lambdas))\n        self.lambda_tilde_posts = [self.lambdas[0]]\n        for k in range(1, self.N):\n            lambda_tilde_k = (self.lambdas[k] * self.lambda_tilde_posts[k - 1]) /\\\n                (self.lambdas[k] + self.lambda_tilde_posts[k - 1])\n            self.lambda_tilde_posts.append(lambda_tilde_k)\n        self.lambda_posts = [None]  # this is never used (just a way of shifting the indexing by 1)\n        for k in range(1, self.N):\n            lambda_k = self.lambdas[k] + self.lambda_tilde_posts[k - 1]\n            self.lambda_posts.append(lambda_k)\n        lambda_N_post = (self.n_data * torch.tensor(1.0).expand_as(self.lambdas[N]) * self.lambdas[N]) +\\\n            self.lambda_tilde_posts[N - 1]\n        self.lambda_posts.append(lambda_N_post)\n        self.target_kappas = [None]\n        self.target_kappas.extend([self.lambdas[k] / self.lambda_posts[k] for k in range(1, self.N)])\n        self.target_mus = [None]\n        self.target_mus.extend([self.loc0 * self.lambda_tilde_posts[k - 1] / self.lambda_posts[k]\n                                for k in range(1, self.N)])\n        target_loc_N = self.sum_data * self.lambdas[N] / lambda_N_post +\\\n            self.loc0 * self.lambda_tilde_posts[N - 1] / lambda_N_post\n        self.target_mus.append(target_loc_N)\n        self.which_nodes_reparam = self.setup_reparam_mask(N)\n\n    # controls which nodes are reparameterized\n    def setup_reparam_mask(self, N):\n        while True:\n            mask = torch.bernoulli(0.30 * torch.ones(N))\n            if torch.sum(mask) < 0.40 * N and torch.sum(mask) > 0.5:\n                return mask\n\n    def model(self, reparameterized, difficulty=0.0):\n        next_mean = self.loc0\n        for k in range(1, self.N + 1):\n            latent_dist = dist.Normal(next_mean, torch.pow(self.lambdas[k - 1], -0.5))\n            loc_latent = pyro.sample(""loc_latent_%d"" % k, latent_dist)\n            next_mean = loc_latent\n\n        loc_N = next_mean\n        with pyro.plate(""data"", self.data.size(0)):\n            pyro.sample(""obs"", dist.Normal(loc_N,\n                                           torch.pow(self.lambdas[self.N], -0.5)), obs=self.data)\n        return loc_N\n\n    def guide(self, reparameterized, difficulty=0.0):\n        previous_sample = None\n        for k in reversed(range(1, self.N + 1)):\n            loc_q = pyro.param(""loc_q_%d"" % k, self.target_mus[k].detach() + difficulty * (0.1 * torch.randn(1) - 0.53))\n            log_sig_q = pyro.param(""log_sig_q_%d"" % k, -0.5 * torch.log(self.lambda_posts[k]).data +\n                                   difficulty * (0.1 * torch.randn(1) - 0.53))\n            sig_q = torch.exp(log_sig_q)\n            kappa_q = None\n            if k != self.N:\n                kappa_q = pyro.param(""kappa_q_%d"" % k, self.target_kappas[k].data +\n                                     difficulty * (0.1 * torch.randn(1) - 0.53))\n            mean_function = loc_q if k == self.N else kappa_q * previous_sample + loc_q\n            node_flagged = True if self.which_nodes_reparam[k - 1] == 1.0 else False\n            Normal = dist.Normal if reparameterized or node_flagged else fakes.NonreparameterizedNormal\n            loc_latent = pyro.sample(""loc_latent_%d"" % k, Normal(mean_function, sig_q),\n                                     infer=dict(baseline=dict(use_decaying_avg_baseline=True)))\n            previous_sample = loc_latent\n        return previous_sample\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_1"")\n@pytest.mark.init(rng_seed=0)\nclass GaussianChainTests(GaussianChain):\n\n    def test_elbo_reparameterized_N_is_3(self):\n        self.setup_chain(3)\n        self.do_elbo_test(True, 1100, 0.0058, 0.03, difficulty=1.0)\n\n    def test_elbo_reparameterized_N_is_8(self):\n        self.setup_chain(8)\n        self.do_elbo_test(True, 1100, 0.0059, 0.03, difficulty=1.0)\n\n    @pytest.mark.skip(""CI"" in os.environ and os.environ[""CI""] == ""true"",\n                      ""Skip slow test in travis."")\n    def test_elbo_reparameterized_N_is_17(self):\n        self.setup_chain(17)\n        self.do_elbo_test(True, 2700, 0.0044, 0.03, difficulty=1.0)\n\n    def test_elbo_nonreparameterized_N_is_3(self):\n        self.setup_chain(3)\n        self.do_elbo_test(False, 1700, 0.0049, 0.04, difficulty=0.6)\n\n    def test_elbo_nonreparameterized_N_is_5(self):\n        self.setup_chain(5)\n        self.do_elbo_test(False, 1000, 0.0061, 0.06, difficulty=0.6)\n\n    @pytest.mark.skip(""CI"" in os.environ and os.environ[""CI""] == ""true"",\n                      ""Skip slow test in travis."")\n    def test_elbo_nonreparameterized_N_is_7(self):\n        self.setup_chain(7)\n        self.do_elbo_test(False, 1800, 0.0035, 0.05, difficulty=0.6)\n\n    def do_elbo_test(self, reparameterized, n_steps, lr, prec, difficulty=1.0):\n        n_repa_nodes = torch.sum(self.which_nodes_reparam) if not reparameterized else self.N\n        logger.info("" - - - - - DO GAUSSIAN %d-CHAIN ELBO TEST  [reparameterized = %s; %d/%d] - - - - - "" %\n                    (self.N, reparameterized, n_repa_nodes, self.N))\n        if self.N < 0:\n            def array_to_string(y):\n                return str(map(lambda x: ""%.3f"" % x.detach().cpu().numpy()[0], y))\n\n            logger.debug(""lambdas: "" + array_to_string(self.lambdas))\n            logger.debug(""target_mus: "" + array_to_string(self.target_mus[1:]))\n            logger.debug(""target_kappas: "" + array_to_string(self.target_kappas[1:]))\n            logger.debug(""lambda_posts: "" + array_to_string(self.lambda_posts[1:]))\n            logger.debug(""lambda_tilde_posts: "" + array_to_string(self.lambda_tilde_posts))\n            pyro.clear_param_store()\n\n        adam = optim.Adam({""lr"": lr, ""betas"": (0.95, 0.999)})\n        elbo = TraceGraph_ELBO()\n        loss_and_grads = elbo.loss_and_grads\n        # loss_and_grads = elbo.jit_loss_and_grads  # This fails.\n        svi = SVI(self.model, self.guide, adam, loss=elbo.loss, loss_and_grads=loss_and_grads)\n\n        for step in range(n_steps):\n            t0 = time.time()\n            svi.step(reparameterized=reparameterized, difficulty=difficulty)\n\n            if step % 5000 == 0 or step == n_steps - 1:\n                kappa_errors, log_sig_errors, loc_errors = [], [], []\n                for k in range(1, self.N + 1):\n                    if k != self.N:\n                        kappa_error = param_mse(""kappa_q_%d"" % k, self.target_kappas[k])\n                        kappa_errors.append(kappa_error)\n\n                    loc_errors.append(param_mse(""loc_q_%d"" % k, self.target_mus[k]))\n                    log_sig_error = param_mse(""log_sig_q_%d"" % k, -0.5 * torch.log(self.lambda_posts[k]))\n                    log_sig_errors.append(log_sig_error)\n\n                max_errors = (np.max(loc_errors), np.max(log_sig_errors), np.max(kappa_errors))\n                min_errors = (np.min(loc_errors), np.min(log_sig_errors), np.min(kappa_errors))\n                mean_errors = (np.mean(loc_errors), np.mean(log_sig_errors), np.mean(kappa_errors))\n                logger.debug(""[max errors]   (loc, log_scale, kappa) = (%.4f, %.4f, %.4f)"" % max_errors)\n                logger.debug(""[min errors]   (loc, log_scale, kappa) = (%.4f, %.4f, %.4f)"" % min_errors)\n                logger.debug(""[mean errors]  (loc, log_scale, kappa) = (%.4f, %.4f, %.4f)"" % mean_errors)\n                logger.debug(""[step time = %.3f;  N = %d;  step = %d]\\n"" % (time.time() - t0, self.N, step))\n\n        assert_equal(0.0, max_errors[0], prec=prec)\n        assert_equal(0.0, max_errors[1], prec=prec)\n        assert_equal(0.0, max_errors[2], prec=prec)\n\n\n@pytest.mark.stage(""integration"", ""integration_batch_2"")\n@pytest.mark.init(rng_seed=0)\nclass GaussianPyramidTests(TestCase):\n\n    def setUp(self):\n        self.loc0 = torch.tensor([0.52])\n\n    def setup_pyramid(self, N):\n        # pyramid of normals with known covariances and latent means\n        assert(N > 1)\n        self.N = N  # number of layers in the pyramid\n        lambdas = [1.1 * (k + 1) / N for k in range(N + 2)]\n        self.lambdas = list(map(lambda x: torch.tensor([x]), lambdas))\n        # generate data\n        self.data = []\n        self.N_data = 3\n        bottom_layer_size = 2 ** (N - 1)\n        for i in range(bottom_layer_size):\n            data_i = []\n            for k in range(self.N_data):\n                data_i.append(torch.tensor([0.25]) +\n                              (0.1 + 0.4 * (i + 1) / bottom_layer_size) * torch.randn(1))\n            self.data.append(data_i)\n        self.data_sums = [sum(self.data[i]) for i in range(bottom_layer_size)]\n        self.N_data = torch.tensor([float(self.N_data)])\n        self.q_dag = self.construct_q_dag()\n        # compute the order in which guide samples are generated\n        self.q_topo_sort = self.q_dag.topological_sort()\n        self.which_nodes_reparam = self.setup_reparam_mask(len(self.q_topo_sort))\n        self.calculate_variational_targets()\n        self.set_model_permutations()\n\n    # for choosing which latents should be reparameterized\n    def setup_reparam_mask(self, n):\n        while True:\n            mask = torch.bernoulli(0.30 * torch.ones(n))\n            if torch.sum(mask) < 0.40 * n and torch.sum(mask) > 0.5:\n                return mask\n\n    # for doing model sampling in different sequential orders\n    def set_model_permutations(self):\n        self.model_permutations = []\n        self.model_unpermutations = []\n        for n in range(1, self.N):\n            permutation = list(range(2 ** (n - 1)))\n            if n > 1:\n                while permutation == list(range(2 ** (n - 1))):\n                    permutation = torch.randperm(2 ** (n - 1)).numpy().tolist()\n            self.model_permutations.append(permutation)\n\n            unpermutation = list(range(len(permutation)))\n            for i in range(len(permutation)):\n                unpermutation[permutation[i]] = i\n            self.model_unpermutations.append(unpermutation)\n\n    def test_elbo_reparameterized_three_layers(self):\n        self.setup_pyramid(3)\n        self.do_elbo_test(True, 1700, 0.01, 0.04, 0.92,\n                          difficulty=0.8, model_permutation=False)\n\n    @pytest.mark.skipif(""CI"" in os.environ, reason=""slow test"")\n    def test_elbo_reparameterized_four_layers(self):\n        self.setup_pyramid(4)\n        self.do_elbo_test(True, 20000, 0.0015, 0.04, 0.92,\n                          difficulty=0.8, model_permutation=False)\n\n    @pytest.mark.stage(""integration"", ""integration_batch_1"")\n    def test_elbo_nonreparameterized_two_layers(self):\n        self.setup_pyramid(2)\n        self.do_elbo_test(False, 500, 0.012, 0.04, 0.95, difficulty=0.5, model_permutation=False)\n\n    def test_elbo_nonreparameterized_three_layers(self):\n        self.setup_pyramid(3)\n        self.do_elbo_test(False, 9100, 0.00506, 0.04, 0.95, difficulty=0.5, model_permutation=False)\n\n    def test_elbo_nonreparameterized_two_layers_model_permuted(self):\n        self.setup_pyramid(2)\n        self.do_elbo_test(False, 700, 0.018, 0.05, 0.96, difficulty=0.5, model_permutation=True)\n\n    @pytest.mark.skip(""CI"" in os.environ and os.environ[""CI""] == ""true"",\n                      ""Skip slow test in travis."")\n    def test_elbo_nonreparameterized_three_layers_model_permuted(self):\n        self.setup_pyramid(3)\n        self.do_elbo_test(False, 6500, 0.0071, 0.05, 0.96, difficulty=0.4, model_permutation=True)\n\n    def calculate_variational_targets(self):\n        # calculate (some of the) variational parameters corresponding to exact posterior\n\n        def calc_lambda_A(lA, lB, lC):\n            return lA + lB + lC\n\n        def calc_lambda_B(lA, lB):\n            return (lA * lB) / (lA + lB)\n\n        def calc_lambda_C(lA, lB, lC):\n            return ((lA + lB) * lC) / (lA + lB + lC)\n\n        self.target_lambdas = {""1"": self.lambdas[0]}\n        previous_names = [""1""]\n        for n in range(2, self.N + 1):\n            new_names = []\n            for prev_name in previous_names:\n                for LR in [\'L\', \'R\']:\n                    new_names.append(prev_name + LR)\n                    self.target_lambdas[new_names[-1]] = self.lambdas[n - 1]\n            previous_names = new_names\n\n        # recursion to compute the target precisions\n        previous_names = [""1""]\n        old_left_pivot_lambda = None\n        for n in range(2, self.N + 1):\n            new_names = []\n            for prev_name in previous_names:\n                BC_names = []\n                for LR in [\'L\', \'R\']:\n                    new_names.append(prev_name + LR)\n                    BC_names.append(new_names[-1])\n                lambda_A0 = self.target_lambdas[prev_name]\n                if n == self.N:\n                    old_left_pivot_lambda = lambda_A0\n                lambda_B0 = self.target_lambdas[BC_names[0]]\n                lambda_C0 = self.target_lambdas[BC_names[1]]\n                lambda_A = calc_lambda_A(lambda_A0, lambda_B0, lambda_C0)\n                lambda_B = calc_lambda_B(lambda_A0, lambda_B0)\n                lambda_C = calc_lambda_C(lambda_A0, lambda_B0, lambda_C0)\n                self.target_lambdas[prev_name] = lambda_A\n                self.target_lambdas[BC_names[0]] = lambda_B\n                self.target_lambdas[BC_names[1]] = lambda_C\n            previous_names = new_names\n\n        for prev_name in previous_names:\n            new_lambda = self.N_data * self.lambdas[-1] + self.target_lambdas[prev_name]\n            self.target_lambdas[prev_name] = new_lambda\n\n        leftmost_node_suffix = self.q_topo_sort[0][11:]\n        leftmost_lambda = self.target_lambdas[leftmost_node_suffix]\n        self.target_leftmost_constant = self.data_sums[0] * self.lambdas[-1] / leftmost_lambda\n        self.target_leftmost_constant += self.loc0 * (leftmost_lambda - self.N_data * self.lambdas[-1]) /\\\n            leftmost_lambda\n\n        almost_leftmost_node_suffix = leftmost_node_suffix[:-1] + \'R\'\n        almost_leftmost_lambda = self.target_lambdas[almost_leftmost_node_suffix]\n        result = self.lambdas[-1] * self.data_sums[1]\n        result += (almost_leftmost_lambda - self.N_data * self.lambdas[-1]) \\\n            * self.loc0 * old_left_pivot_lambda / (old_left_pivot_lambda + self.lambdas[-2])\n        self.target_almost_leftmost_constant = result / almost_leftmost_lambda\n\n    # construct dependency structure for the guide\n    def construct_q_dag(self):\n        g = Trace()\n\n        def add_edge(s):\n            deps = []\n            if s == ""1"":\n                deps.extend([""1L"", ""1R""])\n            else:\n                if s[-1] == \'R\':\n                    deps.append(s[0:-1] + \'L\')\n                if len(s) < self.N:\n                    deps.extend([s + \'L\', s + \'R\'])\n                for k in range(len(s) - 2):\n                    base = s[1:-1 - k]\n                    if base[-1] == \'R\':\n                        deps.append(\'1\' + base[:-1] + \'L\')\n            for dep in deps:\n                g.add_edge(""loc_latent_"" + dep, ""loc_latent_"" + s)\n\n        previous_names = [""1""]\n        add_edge(""1"")\n        for n in range(2, self.N + 1):\n            new_names = []\n            for prev_name in previous_names:\n                for LR in [\'L\', \'R\']:\n                    new_name = prev_name + LR\n                    new_names.append(new_name)\n                    add_edge(new_name)\n            previous_names = new_names\n\n        return g\n\n    def model(self, reparameterized, model_permutation, difficulty=0.0):\n        top_latent_dist = dist.Normal(self.loc0, torch.pow(self.lambdas[0], -0.5))\n        previous_names = [""loc_latent_1""]\n        top_latent = pyro.sample(previous_names[0], top_latent_dist)\n        previous_latents_and_names = list(zip([top_latent], previous_names))\n\n        # for sampling model variables in different sequential orders\n        def permute(x, n):\n            if model_permutation:\n                return [x[self.model_permutations[n - 1][i]] for i in range(len(x))]\n            return x\n\n        def unpermute(x, n):\n            if model_permutation:\n                return [x[self.model_unpermutations[n - 1][i]] for i in range(len(x))]\n            return x\n\n        for n in range(2, self.N + 1):\n            new_latents_and_names = []\n            for prev_latent, prev_name in permute(previous_latents_and_names, n - 1):\n                latent_dist = dist.Normal(prev_latent, torch.pow(self.lambdas[n - 1], -0.5))\n                couple = []\n                for LR in [\'L\', \'R\']:\n                    new_name = prev_name + LR\n                    loc_latent_LR = pyro.sample(new_name, latent_dist)\n                    couple.append([loc_latent_LR, new_name])\n                new_latents_and_names.append(couple)\n            _previous_latents_and_names = unpermute(new_latents_and_names, n - 1)\n            previous_latents_and_names = []\n            for x in _previous_latents_and_names:\n                previous_latents_and_names.append(x[0])\n                previous_latents_and_names.append(x[1])\n\n        for i, data_i in enumerate(self.data):\n            for k, x in enumerate(data_i):\n                pyro.sample(""obs_%s_%d"" % (previous_latents_and_names[i][1], k),\n                            dist.Normal(previous_latents_and_names[i][0], torch.pow(self.lambdas[-1], -0.5)),\n                            obs=x)\n        return top_latent\n\n    def guide(self, reparameterized, model_permutation, difficulty=0.0):\n        latents_dict = {}\n\n        n_nodes = len(self.q_topo_sort)\n        for i, node in enumerate(self.q_topo_sort):\n            deps = self.q_dag.predecessors(node)\n            node_suffix = node[11:]\n            log_sig_node = pyro.param(""log_sig_"" + node_suffix,\n                                      -0.5 * torch.log(self.target_lambdas[node_suffix]) +\n                                      difficulty * (torch.Tensor([-0.3]) - 0.3 * (torch.randn(1) ** 2)))\n            mean_function_node = pyro.param(""constant_term_"" + node,\n                                            self.loc0 + torch.Tensor([difficulty * i / n_nodes]))\n            for dep in deps:\n                kappa_dep = pyro.param(""kappa_"" + node_suffix + \'_\' + dep[11:],\n                                       torch.tensor([0.5 + difficulty * i / n_nodes]))\n                mean_function_node = mean_function_node + kappa_dep * latents_dict[dep]\n            node_flagged = True if self.which_nodes_reparam[i] == 1.0 else False\n            Normal = dist.Normal if reparameterized or node_flagged else fakes.NonreparameterizedNormal\n            latent_node = pyro.sample(node, Normal(mean_function_node, torch.exp(log_sig_node)),\n                                      infer=dict(baseline=dict(use_decaying_avg_baseline=True,\n                                                               baseline_beta=0.96)))\n            latents_dict[node] = latent_node\n\n        return latents_dict[\'loc_latent_1\']\n\n    def do_elbo_test(self, reparameterized, n_steps, lr, prec, beta1,\n                     difficulty=1.0, model_permutation=False):\n        n_repa_nodes = torch.sum(self.which_nodes_reparam) if not reparameterized \\\n            else len(self.q_topo_sort)\n        logger.info(("" - - - DO GAUSSIAN %d-LAYERED PYRAMID ELBO TEST "" +\n                     ""(with a total of %d RVs) [reparameterized=%s; %d/%d; perm=%s] - - -"") %\n                    (self.N, (2 ** self.N) - 1, reparameterized, n_repa_nodes,\n                     len(self.q_topo_sort), model_permutation))\n        pyro.clear_param_store()\n\n        # check graph structure is as expected but only for N=2\n        if self.N == 2:\n            guide_trace = pyro.poutine.trace(self.guide,\n                                             graph_type=""dense"").get_trace(reparameterized=reparameterized,\n                                                                           model_permutation=model_permutation,\n                                                                           difficulty=difficulty)\n            expected_nodes = set([\'log_sig_1R\', \'kappa_1_1L\', \'_INPUT\', \'constant_term_loc_latent_1R\', \'_RETURN\',\n                                  \'loc_latent_1R\', \'loc_latent_1\', \'constant_term_loc_latent_1\', \'loc_latent_1L\',\n                                  \'constant_term_loc_latent_1L\', \'log_sig_1L\', \'kappa_1_1R\', \'kappa_1R_1L\',\n                                  \'log_sig_1\'])\n            expected_edges = set([(\'loc_latent_1R\', \'loc_latent_1\'), (\'loc_latent_1L\', \'loc_latent_1R\'),\n                                  (\'loc_latent_1L\', \'loc_latent_1\')])\n            assert expected_nodes == set(guide_trace.nodes)\n            assert expected_edges == set(guide_trace.edges)\n\n        adam = optim.Adam({""lr"": lr, ""betas"": (beta1, 0.999)})\n        svi = SVI(self.model, self.guide, adam, loss=TraceGraph_ELBO())\n\n        for step in range(n_steps):\n            t0 = time.time()\n            svi.step(reparameterized=reparameterized, model_permutation=model_permutation, difficulty=difficulty)\n\n            if step % 5000 == 0 or step == n_steps - 1:\n                log_sig_errors = []\n                for node in self.target_lambdas:\n                    target_log_sig = -0.5 * torch.log(self.target_lambdas[node])\n                    log_sig_error = param_mse(\'log_sig_\' + node, target_log_sig)\n                    log_sig_errors.append(log_sig_error)\n                max_log_sig_error = np.max(log_sig_errors)\n                min_log_sig_error = np.min(log_sig_errors)\n                mean_log_sig_error = np.mean(log_sig_errors)\n                leftmost_node = self.q_topo_sort[0]\n                leftmost_constant_error = param_mse(\'constant_term_\' + leftmost_node,\n                                                    self.target_leftmost_constant)\n                almost_leftmost_constant_error = param_mse(\'constant_term_\' + leftmost_node[:-1] + \'R\',\n                                                           self.target_almost_leftmost_constant)\n\n                logger.debug(""[mean function constant errors (partial)]   %.4f  %.4f"" %\n                             (leftmost_constant_error, almost_leftmost_constant_error))\n                logger.debug(""[min/mean/max log(scale) errors]   %.4f  %.4f   %.4f"" %\n                             (min_log_sig_error, mean_log_sig_error, max_log_sig_error))\n                logger.debug(""[step time = %.3f;  N = %d;  step = %d]\\n"" % (time.time() - t0, self.N, step))\n\n        assert_equal(0.0, max_log_sig_error, prec=prec)\n        assert_equal(0.0, leftmost_constant_error, prec=prec)\n        assert_equal(0.0, almost_leftmost_constant_error, prec=prec)\n'"
tests/integration_tests/test_tracegraph_elbo.py,65,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nfrom unittest import TestCase\n\nimport numpy as np\nimport pytest\nimport torch\nfrom torch import nn as nn\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim as optim\nfrom pyro.distributions.testing import fakes\nfrom pyro.infer import SVI, TraceGraph_ELBO\nfrom tests.common import assert_equal\n\npytestmark = pytest.mark.stage(""integration"", ""integration_batch_2"")\nlogger = logging.getLogger(__name__)\n\n\ndef param_mse(name, target):\n    return torch.sum(torch.pow(target - pyro.param(name), 2.0)).item()\n\n\ndef param_abs_error(name, target):\n    return torch.sum(torch.abs(target - pyro.param(name))).item()\n\n\nclass NormalNormalTests(TestCase):\n\n    def setUp(self):\n        # normal-normal; known covariance\n        self.lam0 = torch.tensor([0.1, 0.1])   # precision of prior\n        self.loc0 = torch.tensor([0.0, 0.5])   # prior mean\n        # known precision of observation noise\n        self.lam = torch.tensor([6.0, 4.0])\n        self.data = []\n        self.data.append(torch.tensor([-0.1, 0.3]))\n        self.data.append(torch.tensor([0.00, 0.4]))\n        self.data.append(torch.tensor([0.20, 0.5]))\n        self.data.append(torch.tensor([0.10, 0.7]))\n        self.n_data = torch.tensor(float(len(self.data)))\n        self.sum_data = self.data[0] + \\\n            self.data[1] + self.data[2] + self.data[3]\n        self.analytic_lam_n = self.lam0 + \\\n            self.n_data.expand_as(self.lam) * self.lam\n        self.analytic_log_sig_n = -0.5 * torch.log(self.analytic_lam_n)\n        self.analytic_loc_n = self.sum_data * (self.lam / self.analytic_lam_n) +\\\n            self.loc0 * (self.lam0 / self.analytic_lam_n)\n\n    def test_elbo_reparameterized(self):\n        self.do_elbo_test(True, 1500, 0.02)\n\n    @pytest.mark.init(rng_seed=0)\n    def test_elbo_nonreparameterized(self):\n        self.do_elbo_test(False, 5000, 0.05)\n\n    def do_elbo_test(self, reparameterized, n_steps, prec):\n        logger.info("" - - - - - DO NORMALNORMAL ELBO TEST  [reparameterized = %s] - - - - - "" % reparameterized)\n        pyro.clear_param_store()\n        Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal\n\n        def model():\n            with pyro.plate(""plate"", 2):\n                loc_latent = pyro.sample(""loc_latent"", Normal(self.loc0, torch.pow(self.lam0, -0.5)))\n                for i, x in enumerate(self.data):\n                    pyro.sample(""obs_%d"" % i,\n                                dist.Normal(loc_latent, torch.pow(self.lam, -0.5)),\n                                obs=x)\n            return loc_latent\n\n        def guide():\n            loc_q = pyro.param(""loc_q"", self.analytic_loc_n.expand(2) + 0.334)\n            log_sig_q = pyro.param(""log_sig_q"",\n                                   self.analytic_log_sig_n.expand(2) - 0.29)\n            sig_q = torch.exp(log_sig_q)\n            with pyro.plate(""plate"", 2):\n                loc_latent = pyro.sample(""loc_latent"", Normal(loc_q, sig_q))\n            return loc_latent\n\n        adam = optim.Adam({""lr"": .0015, ""betas"": (0.97, 0.999)})\n        svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())\n\n        for k in range(n_steps):\n            svi.step()\n\n            loc_error = param_mse(""loc_q"", self.analytic_loc_n)\n            log_sig_error = param_mse(""log_sig_q"", self.analytic_log_sig_n)\n            if k % 250 == 0:\n                logger.debug(""loc error, log(scale) error:  %.4f, %.4f"" % (loc_error, log_sig_error))\n\n        assert_equal(0.0, loc_error, prec=prec)\n        assert_equal(0.0, log_sig_error, prec=prec)\n\n\nclass NormalNormalNormalTests(TestCase):\n\n    def setUp(self):\n        # normal-normal-normal; known covariance\n        self.lam0 = torch.tensor([0.1, 0.1])  # precision of prior\n        self.loc0 = torch.tensor([0.0, 0.5])   # prior mean\n        # known precision of observation noise\n        self.lam = torch.tensor([6.0, 4.0])\n        self.data = torch.tensor([[-0.1, 0.3],\n                                 [0.00, 0.4],\n                                 [0.20, 0.5],\n                                 [0.10, 0.7]])\n        self.analytic_lam_n = self.lam0 + float(len(self.data)) * self.lam\n        self.analytic_log_sig_n = -0.5 * torch.log(self.analytic_lam_n)\n        self.analytic_loc_n = self.data.sum(0) * (self.lam / self.analytic_lam_n) +\\\n            self.loc0 * (self.lam0 / self.analytic_lam_n)\n\n    def test_elbo_reparameterized(self):\n        self.do_elbo_test(True, True, 3000, 0.02, 0.002, False, False)\n\n    def test_elbo_nonreparameterized_both_baselines(self):\n        self.do_elbo_test(False, False, 3000, 0.04, 0.001, use_nn_baseline=True,\n                          use_decaying_avg_baseline=True)\n\n    def test_elbo_nonreparameterized_decaying_baseline(self):\n        self.do_elbo_test(True, False, 4000, 0.04, 0.0015, use_nn_baseline=False,\n                          use_decaying_avg_baseline=True)\n\n    def test_elbo_nonreparameterized_nn_baseline(self):\n        self.do_elbo_test(False, True, 4000, 0.04, 0.0015, use_nn_baseline=True,\n                          use_decaying_avg_baseline=False)\n\n    def do_elbo_test(self, repa1, repa2, n_steps, prec, lr, use_nn_baseline, use_decaying_avg_baseline):\n        logger.info("" - - - - - DO NORMALNORMALNORMAL ELBO TEST - - - - - -"")\n        logger.info(""[reparameterized = %s, %s; nn_baseline = %s, decaying_baseline = %s]"" %\n                    (repa1, repa2, use_nn_baseline, use_decaying_avg_baseline))\n        pyro.clear_param_store()\n        Normal1 = dist.Normal if repa1 else fakes.NonreparameterizedNormal\n        Normal2 = dist.Normal if repa2 else fakes.NonreparameterizedNormal\n\n        if use_nn_baseline:\n\n            class VanillaBaselineNN(nn.Module):\n                def __init__(self, dim_input, dim_h):\n                    super().__init__()\n                    self.lin1 = nn.Linear(dim_input, dim_h)\n                    self.lin2 = nn.Linear(dim_h, 2)\n                    self.sigmoid = nn.Sigmoid()\n\n                def forward(self, x):\n                    h = self.sigmoid(self.lin1(x))\n                    return self.lin2(h)\n\n            loc_prime_baseline = pyro.module(""loc_prime_baseline"", VanillaBaselineNN(2, 5))\n        else:\n            loc_prime_baseline = None\n\n        def model():\n            with pyro.plate(""plate"", 2):\n                loc_latent_prime = pyro.sample(""loc_latent_prime"", Normal1(self.loc0, torch.pow(self.lam0, -0.5)))\n                loc_latent = pyro.sample(""loc_latent"", Normal2(loc_latent_prime, torch.pow(self.lam0, -0.5)))\n                with pyro.plate(""data"", len(self.data)):\n                    pyro.sample(""obs"",\n                                dist.Normal(loc_latent, torch.pow(self.lam, -0.5))\n                                    .expand_by(self.data.shape[:1]),\n                                obs=self.data)\n            return loc_latent\n\n        # note that the exact posterior is not mean field!\n        def guide():\n            loc_q = pyro.param(""loc_q"", self.analytic_loc_n.expand(2) + 0.334)\n            log_sig_q = pyro.param(""log_sig_q"",\n                                   self.analytic_log_sig_n.expand(2) - 0.29)\n            loc_q_prime = pyro.param(""loc_q_prime"",\n                                     torch.tensor([-0.34, 0.52]))\n            kappa_q = pyro.param(""kappa_q"", torch.tensor([0.74]))\n            log_sig_q_prime = pyro.param(""log_sig_q_prime"",\n                                         -0.5 * torch.log(1.2 * self.lam0))\n            sig_q, sig_q_prime = torch.exp(log_sig_q), torch.exp(log_sig_q_prime)\n            with pyro.plate(""plate"", 2):\n                loc_latent = pyro.sample(""loc_latent"", Normal2(loc_q, sig_q),\n                                         infer=dict(baseline=dict(use_decaying_avg_baseline=use_decaying_avg_baseline)))\n                pyro.sample(""loc_latent_prime"",\n                            Normal1(kappa_q.expand_as(loc_latent) * loc_latent + loc_q_prime, sig_q_prime),\n                            infer=dict(baseline=dict(nn_baseline=loc_prime_baseline,\n                                                     nn_baseline_input=loc_latent,\n                                                     use_decaying_avg_baseline=use_decaying_avg_baseline)))\n                with pyro.plate(""data"", len(self.data)):\n                    pass\n\n            return loc_latent\n\n        adam = optim.Adam({""lr"": .0015, ""betas"": (0.97, 0.999)})\n        svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())\n\n        for k in range(n_steps):\n            svi.step()\n\n            loc_error = param_mse(""loc_q"", self.analytic_loc_n)\n            log_sig_error = param_mse(""log_sig_q"", self.analytic_log_sig_n)\n            loc_prime_error = param_mse(""loc_q_prime"", 0.5 * self.loc0)\n            kappa_error = param_mse(""kappa_q"", 0.5 * torch.ones(1))\n            log_sig_prime_error = param_mse(""log_sig_q_prime"", -0.5 * torch.log(2.0 * self.lam0))\n\n            if k % 500 == 0:\n                logger.debug(""errors:  %.4f, %.4f"" % (loc_error, log_sig_error))\n                logger.debug("", %.4f, %.4f"" % (loc_prime_error, log_sig_prime_error))\n                logger.debug("", %.4f"" % kappa_error)\n\n        assert_equal(0.0, loc_error, prec=prec)\n        assert_equal(0.0, log_sig_error, prec=prec)\n        assert_equal(0.0, loc_prime_error, prec=prec)\n        assert_equal(0.0, log_sig_prime_error, prec=prec)\n        assert_equal(0.0, kappa_error, prec=prec)\n\n\nclass BernoulliBetaTests(TestCase):\n    def setUp(self):\n        # bernoulli-beta model\n        # beta prior hyperparameter\n        self.alpha0 = torch.tensor(1.0)\n        self.beta0 = torch.tensor(1.0)  # beta prior hyperparameter\n        self.data = torch.tensor([0.0, 1.0, 1.0, 1.0])\n        self.n_data = float(len(self.data))\n        data_sum = self.data.sum()\n        self.alpha_n = self.alpha0 + data_sum  # posterior alpha\n        self.beta_n = self.beta0 - data_sum + torch.tensor(self.n_data)  # posterior beta\n        self.log_alpha_n = torch.log(self.alpha_n)\n        self.log_beta_n = torch.log(self.beta_n)\n\n    def test_elbo_reparameterized(self):\n        self.do_elbo_test(True, 3000, 0.92, 0.0007)\n\n    def test_elbo_nonreparameterized(self):\n        self.do_elbo_test(False, 3000, 0.95, 0.0007)\n\n    def do_elbo_test(self, reparameterized, n_steps, beta1, lr):\n        logger.info("" - - - - - DO BETA-BERNOULLI ELBO TEST [repa = %s] - - - - - "" % reparameterized)\n        pyro.clear_param_store()\n        Beta = dist.Beta if reparameterized else fakes.NonreparameterizedBeta\n\n        def model():\n            p_latent = pyro.sample(""p_latent"", Beta(self.alpha0, self.beta0))\n            with pyro.plate(""data"", len(self.data)):\n                pyro.sample(""obs"", dist.Bernoulli(p_latent), obs=self.data)\n            return p_latent\n\n        def guide():\n            alpha_q_log = pyro.param(""alpha_q_log"",\n                                     self.log_alpha_n + 0.17)\n            beta_q_log = pyro.param(""beta_q_log"",\n                                    self.log_beta_n - 0.143)\n            alpha_q, beta_q = torch.exp(alpha_q_log), torch.exp(beta_q_log)\n            p_latent = pyro.sample(""p_latent"", Beta(alpha_q, beta_q),\n                                   infer=dict(baseline=dict(use_decaying_avg_baseline=True)))\n            with pyro.plate(""data"", len(self.data)):\n                pass\n            return p_latent\n\n        adam = optim.Adam({""lr"": lr, ""betas"": (beta1, 0.999)})\n        svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())\n\n        for k in range(n_steps):\n            svi.step()\n            alpha_error = param_abs_error(""alpha_q_log"", self.log_alpha_n)\n            beta_error = param_abs_error(""beta_q_log"", self.log_beta_n)\n            if k % 500 == 0:\n                logger.debug(""alpha_error, beta_error: %.4f, %.4f"" % (alpha_error, beta_error))\n\n        assert_equal(0.0, alpha_error, prec=0.03)\n        assert_equal(0.0, beta_error, prec=0.04)\n\n\nclass ExponentialGammaTests(TestCase):\n    def setUp(self):\n        # exponential-gamma model\n        # gamma prior hyperparameter\n        self.alpha0 = torch.tensor(1.0)\n        # gamma prior hyperparameter\n        self.beta0 = torch.tensor(1.0)\n        self.n_data = 2\n        self.data = torch.tensor([3.0, 2.0])  # two observations\n        self.alpha_n = self.alpha0 + self.n_data  # posterior alpha\n        self.beta_n = self.beta0 + self.data.sum()  # posterior beta\n        self.log_alpha_n = torch.log(self.alpha_n)\n        self.log_beta_n = torch.log(self.beta_n)\n\n    def test_elbo_reparameterized(self):\n        self.do_elbo_test(True, 8000, 0.90, 0.0007)\n\n    def test_elbo_nonreparameterized(self):\n        self.do_elbo_test(False, 8000, 0.95, 0.0007)\n\n    def do_elbo_test(self, reparameterized, n_steps, beta1, lr):\n        logger.info("" - - - - - DO EXPONENTIAL-GAMMA ELBO TEST [repa = %s] - - - - - "" % reparameterized)\n        pyro.clear_param_store()\n        Gamma = dist.Gamma if reparameterized else fakes.NonreparameterizedGamma\n\n        def model():\n            lambda_latent = pyro.sample(""lambda_latent"", Gamma(self.alpha0, self.beta0))\n            with pyro.plate(""data"", len(self.data)):\n                pyro.sample(""obs"", dist.Exponential(lambda_latent), obs=self.data)\n            return lambda_latent\n\n        def guide():\n            alpha_q_log = pyro.param(\n                ""alpha_q_log"",\n                self.log_alpha_n + 0.17)\n            beta_q_log = pyro.param(\n                ""beta_q_log"",\n                self.log_beta_n - 0.143)\n            alpha_q, beta_q = torch.exp(alpha_q_log), torch.exp(beta_q_log)\n            pyro.sample(""lambda_latent"", Gamma(alpha_q, beta_q),\n                        infer=dict(baseline=dict(use_decaying_avg_baseline=True)))\n            with pyro.plate(""data"", len(self.data)):\n                pass\n\n        adam = optim.Adam({""lr"": lr, ""betas"": (beta1, 0.999)})\n        svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())\n\n        for k in range(n_steps):\n            svi.step()\n            alpha_error = param_abs_error(""alpha_q_log"", self.log_alpha_n)\n            beta_error = param_abs_error(""beta_q_log"", self.log_beta_n)\n            if k % 500 == 0:\n                logger.debug(""alpha_error, beta_error: %.4f, %.4f"" % (alpha_error, beta_error))\n\n        assert_equal(0.0, alpha_error, prec=0.04)\n        assert_equal(0.0, beta_error, prec=0.04)\n\n\n@pytest.mark.init(rng_seed=0)\n@pytest.mark.stage(""integration"", ""integration_batch_1"")\nclass RaoBlackwellizationTests(TestCase):\n    def setUp(self):\n        # normal-normal; known covariance\n        self.lam0 = torch.tensor([0.1, 0.1])   # precision of prior\n        self.loc0 = torch.tensor([0.0, 0.5])   # prior mean\n        # known precision of observation noise\n        self.lam = torch.tensor([6.0, 4.0])\n        self.n_outer = 3\n        self.n_inner = 3\n        self.n_data = torch.tensor(float(self.n_outer * self.n_inner))\n        self.data = []\n        self.sum_data = torch.zeros(2)\n        for _out in range(self.n_outer):\n            data_in = []\n            for _in in range(self.n_inner):\n                data_in.append(torch.tensor([-0.1, 0.3]) + torch.empty(torch.Size((2,))).normal_() / self.lam.sqrt())\n                self.sum_data += data_in[-1]\n            self.data.append(data_in)\n        self.analytic_lam_n = self.lam0 + self.n_data.expand_as(self.lam) * self.lam\n        self.analytic_log_sig_n = -0.5 * torch.log(self.analytic_lam_n)\n        self.analytic_loc_n = self.sum_data * (self.lam / self.analytic_lam_n) +\\\n            self.loc0 * (self.lam0 / self.analytic_lam_n)\n\n    # this tests rao-blackwellization in elbo for nested sequential plates\n    def test_nested_iplate_in_elbo(self, n_steps=4000):\n        pyro.clear_param_store()\n\n        def model():\n            loc_latent = pyro.sample(""loc_latent"",\n                                     fakes.NonreparameterizedNormal(self.loc0, torch.pow(self.lam0, -0.5))\n                                          .to_event(1))\n            for i in pyro.plate(""outer"", self.n_outer):\n                for j in pyro.plate(""inner_%d"" % i, self.n_inner):\n                    pyro.sample(""obs_%d_%d"" % (i, j),\n                                dist.Normal(loc_latent, torch.pow(self.lam, -0.5)).to_event(1),\n                                obs=self.data[i][j])\n\n        def guide():\n            loc_q = pyro.param(""loc_q"", self.analytic_loc_n.expand(2) + 0.234)\n            log_sig_q = pyro.param(""log_sig_q"",\n                                   self.analytic_log_sig_n.expand(2) - 0.27)\n            sig_q = torch.exp(log_sig_q)\n            pyro.sample(""loc_latent"", fakes.NonreparameterizedNormal(loc_q, sig_q).to_event(1),\n                        infer=dict(baseline=dict(use_decaying_avg_baseline=True)))\n\n            for i in pyro.plate(""outer"", self.n_outer):\n                for j in pyro.plate(""inner_%d"" % i, self.n_inner):\n                    pass\n\n        guide_trace = pyro.poutine.trace(guide, graph_type=""dense"").get_trace()\n        model_trace = pyro.poutine.trace(pyro.poutine.replay(model, trace=guide_trace),\n                                         graph_type=""dense"").get_trace()\n        assert len(list(model_trace.edges)) == 27\n        assert len(model_trace.nodes) == 16\n        assert len(list(guide_trace.edges)) == 0\n        assert len(guide_trace.nodes) == 9\n\n        adam = optim.Adam({""lr"": 0.0008, ""betas"": (0.96, 0.999)})\n        svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())\n\n        for k in range(n_steps):\n            svi.step()\n            loc_error = param_mse(""loc_q"", self.analytic_loc_n)\n            log_sig_error = param_mse(""log_sig_q"", self.analytic_log_sig_n)\n            if k % 500 == 0:\n                logger.debug(""loc error, log(scale) error:  %.4f, %.4f"" % (loc_error, log_sig_error))\n\n        assert_equal(0.0, loc_error, prec=0.04)\n        assert_equal(0.0, log_sig_error, prec=0.04)\n\n    # this tests rao-blackwellization and baselines for plate\n    # inside of a sequential plate with superfluous random torch.tensors to complexify the\n    # graph structure and introduce additional baselines\n    def test_plate_in_elbo_with_superfluous_rvs(self):\n        self._test_plate_in_elbo(n_superfluous_top=1, n_superfluous_bottom=1, n_steps=2000, lr=0.0113)\n\n    def _test_plate_in_elbo(self, n_superfluous_top, n_superfluous_bottom, n_steps, lr=0.0012):\n        pyro.clear_param_store()\n        self.data_tensor = torch.zeros(9, 2)\n        for _out in range(self.n_outer):\n            for _in in range(self.n_inner):\n                self.data_tensor[3 * _out + _in, :] = self.data[_out][_in]\n        self.data_as_list = [self.data_tensor[0:4, :], self.data_tensor[4:7, :],\n                             self.data_tensor[7:9, :]]\n\n        def model():\n            loc_latent = pyro.sample(""loc_latent"",\n                                     fakes.NonreparameterizedNormal(self.loc0, torch.pow(self.lam0, -0.5))\n                                     .to_event(1))\n\n            for i in pyro.plate(""outer"", 3):\n                x_i = self.data_as_list[i]\n                with pyro.plate(""inner_%d"" % i, x_i.size(0)):\n                    for k in range(n_superfluous_top):\n                        z_i_k = pyro.sample(""z_%d_%d"" % (i, k),\n                                            fakes.NonreparameterizedNormal(0, 1).expand_by([4 - i]))\n                        assert z_i_k.shape == (4 - i,)\n                    obs_i = pyro.sample(""obs_%d"" % i, dist.Normal(loc_latent, torch.pow(self.lam, -0.5))\n                                                          .to_event(1), obs=x_i)\n                    assert obs_i.shape == (4 - i, 2)\n                    for k in range(n_superfluous_top, n_superfluous_top + n_superfluous_bottom):\n                        z_i_k = pyro.sample(""z_%d_%d"" % (i, k),\n                                            fakes.NonreparameterizedNormal(0, 1).expand_by([4 - i]))\n                        assert z_i_k.shape == (4 - i,)\n\n        pt_loc_baseline = torch.nn.Linear(1, 1)\n        pt_superfluous_baselines = []\n        for k in range(n_superfluous_top + n_superfluous_bottom):\n            pt_superfluous_baselines.extend([torch.nn.Linear(2, 4), torch.nn.Linear(2, 3),\n                                             torch.nn.Linear(2, 2)])\n\n        def guide():\n            loc_q = pyro.param(""loc_q"", self.analytic_loc_n.expand(2) + 0.094)\n            log_sig_q = pyro.param(""log_sig_q"",\n                                   self.analytic_log_sig_n.expand(2) - 0.07)\n            sig_q = torch.exp(log_sig_q)\n            trivial_baseline = pyro.module(""loc_baseline"", pt_loc_baseline)\n            baseline_value = trivial_baseline(torch.ones(1)).squeeze()\n            loc_latent = pyro.sample(""loc_latent"",\n                                     fakes.NonreparameterizedNormal(loc_q, sig_q).to_event(1),\n                                     infer=dict(baseline=dict(baseline_value=baseline_value)))\n\n            for i in pyro.plate(""outer"", 3):\n                with pyro.plate(""inner_%d"" % i, 4 - i):\n                    for k in range(n_superfluous_top + n_superfluous_bottom):\n                        z_baseline = pyro.module(""z_baseline_%d_%d"" % (i, k),\n                                                 pt_superfluous_baselines[3 * k + i])\n                        baseline_value = z_baseline(loc_latent.detach())\n                        mean_i = pyro.param(""mean_%d_%d"" % (i, k),\n                                            0.5 * torch.ones(4 - i))\n                        z_i_k = pyro.sample(""z_%d_%d"" % (i, k),\n                                            fakes.NonreparameterizedNormal(mean_i, 1),\n                                            infer=dict(baseline=dict(baseline_value=baseline_value)))\n                        assert z_i_k.shape == (4 - i,)\n\n        def per_param_callable(module_name, param_name):\n            if \'baseline\' in param_name or \'baseline\' in module_name:\n                return {""lr"": 0.010, ""betas"": (0.95, 0.999)}\n            else:\n                return {""lr"": lr, ""betas"": (0.95, 0.999)}\n\n        adam = optim.Adam(per_param_callable)\n        svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())\n\n        for step in range(n_steps):\n            svi.step()\n\n            loc_error = param_abs_error(""loc_q"", self.analytic_loc_n)\n            log_sig_error = param_abs_error(""log_sig_q"", self.analytic_log_sig_n)\n\n            if n_superfluous_top > 0 or n_superfluous_bottom > 0:\n                superfluous_errors = []\n                for k in range(n_superfluous_top + n_superfluous_bottom):\n                    mean_0_error = torch.sum(torch.pow(pyro.param(""mean_0_%d"" % k), 2.0))\n                    mean_1_error = torch.sum(torch.pow(pyro.param(""mean_1_%d"" % k), 2.0))\n                    mean_2_error = torch.sum(torch.pow(pyro.param(""mean_2_%d"" % k), 2.0))\n                    superfluous_error = torch.max(torch.max(mean_0_error, mean_1_error), mean_2_error)\n                    superfluous_errors.append(superfluous_error.detach().cpu().numpy())\n\n            if step % 500 == 0:\n                logger.debug(""loc error, log(scale) error:  %.4f, %.4f"" % (loc_error, log_sig_error))\n                if n_superfluous_top > 0 or n_superfluous_bottom > 0:\n                    logger.debug(""superfluous error: %.4f"" % np.max(superfluous_errors))\n\n        assert_equal(0.0, loc_error, prec=0.04)\n        assert_equal(0.0, log_sig_error, prec=0.05)\n        if n_superfluous_top > 0 or n_superfluous_bottom > 0:\n            assert_equal(0.0, np.max(superfluous_errors), prec=0.04)\n'"
tests/nn/__init__.py,0,b''
tests/nn/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/nn""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""unit""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/nn/test_autoregressive.py,11,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom unittest import TestCase\n\nimport pytest\nimport torch\n\nfrom pyro.nn import AutoRegressiveNN, ConditionalAutoRegressiveNN\nfrom pyro.nn.auto_reg_nn import create_mask\n\npytestmark = pytest.mark.init(rng_seed=123)\n\n\nclass AutoRegressiveNNTests(TestCase):\n    def setUp(self):\n        self.epsilon = 1.0e-3\n\n    def _test_jacobian(self, input_dim, observed_dim, hidden_dim, param_dim):\n        jacobian = torch.zeros(input_dim, input_dim)\n        if observed_dim > 0:\n            arn = ConditionalAutoRegressiveNN(input_dim, observed_dim, [hidden_dim], param_dims=[param_dim])\n        else:\n            arn = AutoRegressiveNN(input_dim, [hidden_dim], param_dims=[param_dim])\n\n        def nonzero(x):\n            return torch.sign(torch.abs(x))\n\n        x = torch.randn(1, input_dim)\n        y = torch.randn(1, observed_dim)\n\n        for output_index in range(param_dim):\n            for j in range(input_dim):\n                for k in range(input_dim):\n                    epsilon_vector = torch.zeros(1, input_dim)\n                    epsilon_vector[0, j] = self.epsilon\n                    if observed_dim > 0:\n                        delta = (arn(x + 0.5 * epsilon_vector, y) - arn(x - 0.5 * epsilon_vector, y)) / self.epsilon\n                    else:\n                        delta = (arn(x + 0.5 * epsilon_vector) - arn(x - 0.5 * epsilon_vector)) / self.epsilon\n                    jacobian[j, k] = float(delta[0, output_index, k])\n\n            permutation = arn.get_permutation()\n            permuted_jacobian = jacobian.clone()\n            for j in range(input_dim):\n                for k in range(input_dim):\n                    permuted_jacobian[j, k] = jacobian[permutation[j], permutation[k]]\n\n            lower_sum = torch.sum(torch.tril(nonzero(permuted_jacobian), diagonal=0))\n\n            assert lower_sum == float(0.0)\n\n    def _test_masks(self, input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier):\n        masks, mask_skip = create_mask(input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier)\n\n        # First test that hidden layer masks are adequately connected\n        # Tracing backwards, works out what inputs each output is connected to\n        # It's a dictionary of sets indexed by a tuple (input_dim, param_dim)\n        permutation = list(permutation.numpy())\n\n        # Loop over variables\n        for idx in range(input_dim):\n            # Calculate correct answer\n            correct = torch.cat((torch.arange(observed_dim, dtype=torch.long), torch.tensor(\n                sorted(permutation[0:permutation.index(idx)]), dtype=torch.long) + observed_dim))\n\n            # Loop over parameters for each variable\n            for jdx in range(output_dim_multiplier):\n                prev_connections = set()\n                # Do output-to-penultimate hidden layer mask\n                for kdx in range(masks[-1].size(1)):\n                    if masks[-1][idx + jdx * input_dim, kdx]:\n                        prev_connections.add(kdx)\n\n                # Do hidden-to-hidden, and hidden-to-input layer masks\n                for m in reversed(masks[:-1]):\n                    this_connections = set()\n                    for kdx in prev_connections:\n                        for ldx in range(m.size(1)):\n                            if m[kdx, ldx]:\n                                this_connections.add(ldx)\n                    prev_connections = this_connections\n\n                assert (torch.tensor(list(sorted(prev_connections)), dtype=torch.long) == correct).all()\n\n                # Test the skip-connections mask\n                skip_connections = set()\n                for kdx in range(mask_skip.size(1)):\n                    if mask_skip[idx + jdx * input_dim, kdx]:\n                        skip_connections.add(kdx)\n                assert (torch.tensor(list(sorted(skip_connections)), dtype=torch.long) == correct).all()\n\n    def test_jacobians(self):\n        for observed_dim in [0, 5]:\n            for input_dim in [2, 3, 5, 7, 9, 11]:\n                self._test_jacobian(input_dim, observed_dim, 3 * input_dim + 1, 2)\n\n    def test_masks(self):\n        for input_dim in [1, 3, 5]:\n            for observed_dim in [0, 3]:\n                for num_layers in [1, 3]:\n                    for output_dim_multiplier in [1, 2, 3]:\n                        # NOTE: the hidden dimension must be greater than the input_dim for the\n                        # masks to be well-defined!\n                        hidden_dim = input_dim * 5\n                        permutation = torch.randperm(input_dim, device='cpu')\n                        self._test_masks(\n                            input_dim,\n                            observed_dim,\n                            [hidden_dim]*num_layers,\n                            permutation,\n                            output_dim_multiplier)\n"""
tests/nn/test_module.py,75,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport io\nimport warnings\n\nimport pytest\nimport torch\nfrom torch import nn\nfrom torch.distributions import constraints, transform_to\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.nn.module import PyroModule, PyroParam, PyroSample, clear, to_pyro_module_\nfrom pyro.optim import Adam\nfrom tests.common import assert_equal\n\n\ndef test_svi_smoke():\n\n    class Model(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.loc = nn.Parameter(torch.zeros(2))\n            self.scale = PyroParam(torch.ones(2), constraint=constraints.positive)\n            self.z = PyroSample(lambda self: dist.Normal(self.loc, self.scale).to_event(1))\n\n        def forward(self, data):\n            loc, log_scale = self.z.unbind(-1)\n            with pyro.plate(""data""):\n                pyro.sample(""obs"", dist.Cauchy(loc, log_scale.exp()),\n                            obs=data)\n\n    class Guide(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.loc = nn.Parameter(torch.zeros(2))\n            self.scale = PyroParam(torch.ones(2), constraint=constraints.positive)\n            self.z = PyroSample(lambda self: dist.Normal(self.loc, self.scale).to_event(1))\n\n        def forward(self, *args, **kwargs):\n            return self.z\n\n    data = torch.randn(5)\n    model = Model()\n    trace = poutine.trace(model).get_trace(data)\n    assert ""loc"" in trace.nodes.keys()\n    assert trace.nodes[""loc""][""type""] == ""param""\n    assert ""scale"" in trace.nodes\n    assert trace.nodes[""scale""][""type""] == ""param""\n\n    guide = Guide()\n    trace = poutine.trace(guide).get_trace(data)\n    assert ""loc"" in trace.nodes.keys()\n    assert trace.nodes[""loc""][""type""] == ""param""\n    assert ""scale"" in trace.nodes\n    assert trace.nodes[""scale""][""type""] == ""param""\n\n    optim = Adam({""lr"": 0.01})\n    svi = SVI(model, guide, optim, Trace_ELBO())\n    for step in range(3):\n        svi.step(data)\n\n\ndef test_names():\n\n    class Model(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.x = nn.Parameter(torch.tensor(0.))\n            self.y = PyroParam(torch.tensor(1.), constraint=constraints.positive)\n            self.m = nn.Module()\n            self.m.u = nn.Parameter(torch.tensor(2.0))\n            self.p = PyroModule()\n            self.p.v = nn.Parameter(torch.tensor(3.))\n            self.p.w = PyroParam(torch.tensor(4.), constraint=constraints.positive)\n\n        def forward(self):\n            # trigger .__getattr__()\n            self.x\n            self.y\n            self.m\n            self.p.v\n            self.p.w\n\n    model = Model()\n\n    # Check named_parameters.\n    expected = {\n        ""x"",\n        ""y_unconstrained"",\n        ""m.u"",\n        ""p.v"",\n        ""p.w_unconstrained"",\n    }\n    actual = set(name for name, _ in model.named_parameters())\n    assert actual == expected\n\n    # Check pyro.param names.\n    expected = {""x"", ""y"", ""m$$$u"", ""p.v"", ""p.w""}\n    with poutine.trace(param_only=True) as param_capture:\n        model()\n    actual = {name for name, site in param_capture.trace.nodes.items()\n              if site[""type""] == ""param""}\n    assert actual == expected\n\n    # Check pyro_parameters method\n    expected = {""x"", ""y"", ""m.u"", ""p.v"", ""p.w""}\n    actual = set(k for k, v in model.named_pyro_params())\n    assert actual == expected\n\n\ndef test_delete():\n    m = PyroModule()\n    m.a = PyroParam(torch.tensor(1.))\n    del m.a\n    m.a = PyroParam(torch.tensor(0.1))\n    assert_equal(m.a.detach(), torch.tensor(0.1))\n\n\ndef test_nested():\n    class Child(PyroModule):\n        def __init__(self, a):\n            super().__init__()\n            self.a = PyroParam(a, constraints.positive)\n\n    class Family(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.child1 = Child(torch.tensor(1.))\n            self.child2 = Child(torch.tensor(2.))\n\n    f = Family()\n    assert_equal(f.child1.a.detach(), torch.tensor(1.))\n    assert_equal(f.child2.a.detach(), torch.tensor(2.))\n\n\ndef test_module_cache():\n    class Child(PyroModule):\n        def __init__(self, x):\n            super().__init__()\n            self.a = PyroParam(torch.tensor(x))\n\n        def forward(self):\n            return self.a\n\n    class Family(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.c = Child(1.0)\n\n        def forward(self):\n            return self.c.a\n\n    f = Family()\n    assert_equal(f().detach(), torch.tensor(1.))\n    f.c = Child(3.)\n    assert_equal(f().detach(), torch.tensor(3.))\n    assert_equal(f.c().detach(), torch.tensor(3.))\n\n\ndef test_submodule_contains_torch_module():\n    submodule = PyroModule()\n    submodule.linear = nn.Linear(1, 1)\n    module = PyroModule()\n    module.child = submodule\n\n\ndef test_hierarchy_prior_cached():\n    def hierarchy_prior(module):\n        latent = pyro.sample(""a"", dist.Normal(0, 1))\n        return dist.Normal(latent, 1)\n\n    class Model(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.b = PyroSample(hierarchy_prior)\n\n        def forward(self):\n            return self.b + self.b\n\n    model = Model()\n    trace = poutine.trace(model).get_trace()\n    assert ""a"" in trace.nodes\n\n\nSHAPE_CONSTRAINT = [\n    ((), constraints.real),\n    ((4,), constraints.real),\n    ((3, 2), constraints.real),\n    ((), constraints.positive),\n    ((4,), constraints.positive),\n    ((3, 2), constraints.positive),\n    ((5,), constraints.simplex),\n    ((2, 5,), constraints.simplex),\n    ((5, 5), constraints.lower_cholesky),\n    ((2, 5, 5), constraints.lower_cholesky),\n    ((10, ), constraints.greater_than(-torch.randn(10).exp())),\n    ((4, 10), constraints.greater_than(-torch.randn(10).exp())),\n    ((4, 10), constraints.greater_than(-torch.randn(4, 10).exp())),\n    ((3, 2, 10), constraints.greater_than(-torch.randn(10).exp())),\n    ((3, 2, 10), constraints.greater_than(-torch.randn(2, 10).exp())),\n    ((3, 2, 10), constraints.greater_than(-torch.randn(3, 1, 10).exp())),\n    ((3, 2, 10), constraints.greater_than(-torch.randn(3, 2, 10).exp())),\n    ((5,), constraints.real_vector),\n    ((2, 5,), constraints.real_vector),\n    ((), constraints.unit_interval),\n    ((4, ), constraints.unit_interval),\n    ((3, 2), constraints.unit_interval),\n    ((10,), constraints.interval(-torch.randn(10).exp(),\n                                 torch.randn(10).exp())),\n    ((4, 10), constraints.interval(-torch.randn(10).exp(),\n                                   torch.randn(10).exp())),\n    ((3, 2, 10), constraints.interval(-torch.randn(10).exp(),\n                                      torch.randn(10).exp())),\n]\n\n\n@pytest.mark.parametrize(\'shape,constraint_\', SHAPE_CONSTRAINT)\ndef test_constraints(shape, constraint_):\n    module = PyroModule()\n    module.x = PyroParam(torch.full(shape, 1e-4), constraint_)\n\n    assert isinstance(module.x, torch.Tensor)\n    assert isinstance(module.x_unconstrained, nn.Parameter)\n    assert module.x.shape == shape\n    assert constraint_.check(module.x).all()\n\n    module.x = torch.randn(shape).exp() * 1e-6\n    assert isinstance(module.x_unconstrained, nn.Parameter)\n    assert isinstance(module.x, torch.Tensor)\n    assert module.x.shape == shape\n    assert constraint_.check(module.x).all()\n\n    assert isinstance(module.x_unconstrained, torch.Tensor)\n    y = module.x_unconstrained.data.normal_()\n    assert_equal(module.x.data, transform_to(constraint_)(y))\n    assert constraint_.check(module.x).all()\n\n    del module.x\n    assert \'x\' not in module._pyro_params\n    assert not hasattr(module, \'x\')\n    assert not hasattr(module, \'x_unconstrained\')\n\n\ndef test_clear():\n\n    class Model(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.x = nn.Parameter(torch.tensor(0.))\n            self.m = torch.nn.Linear(2, 3)\n            self.m.weight.data.fill_(1.)\n            self.m.bias.data.fill_(2.)\n            self.p = PyroModule()\n            self.p.x = nn.Parameter(torch.tensor(3.))\n\n        def forward(self):\n            return [x.clone() for x in [self.x, self.m.weight, self.m.bias, self.p.x]]\n\n    assert set(pyro.get_param_store().keys()) == set()\n    m = Model()\n    state0 = m()\n    assert set(pyro.get_param_store().keys()) == {""x"", ""m$$$weight"", ""m$$$bias"", ""p.x""}\n\n    # mutate\n    for x in pyro.get_param_store().values():\n        x.unconstrained().data += torch.randn(())\n    state1 = m()\n    for x, y in zip(state0, state1):\n        assert not (x == y).all()\n    assert set(pyro.get_param_store().keys()) == {""x"", ""m$$$weight"", ""m$$$bias"", ""p.x""}\n\n    clear(m)\n    del m\n    assert set(pyro.get_param_store().keys()) == set()\n\n    m = Model()\n    state2 = m()\n    assert set(pyro.get_param_store().keys()) == {""x"", ""m$$$weight"", ""m$$$bias"", ""p.x""}\n    for actual, expected in zip(state2, state0):\n        assert_equal(actual, expected)\n\n\ndef test_sample():\n\n    class Model(nn.Linear, PyroModule):\n        def __init__(self, in_features, out_features):\n            super().__init__(in_features, out_features)\n            self.weight = PyroSample(\n                lambda self: dist.Normal(0, 1)\n                                 .expand([self.out_features,\n                                          self.in_features])\n                                 .to_event(2))\n\n    class Guide(nn.Linear, PyroModule):\n        def __init__(self, in_features, out_features):\n            super().__init__(in_features, out_features)\n            self.loc = PyroParam(torch.zeros_like(self.weight))\n            self.scale = PyroParam(torch.ones_like(self.weight),\n                                   constraint=constraints.positive)\n            self.weight = PyroSample(\n                lambda self: dist.Normal(self.loc, self.scale)\n                                 .to_event(2))\n\n    data = torch.randn(8)\n    model = Model(8, 2)\n    guide = Guide(8, 2)\n\n    optim = Adam({""lr"": 0.01})\n    svi = SVI(model, guide, optim, Trace_ELBO())\n    for step in range(3):\n        svi.step(data)\n\n\ndef test_cache():\n    class MyModule(PyroModule):\n        def forward(self):\n            return [self.gather(), self.gather()]\n\n        def gather(self):\n            return {\n                ""a"": self.a,\n                ""b"": self.b,\n                ""c"": self.c,\n                ""p.d"": self.p.d,\n                ""p.e"": self.p.e,\n                ""p.f"": self.p.f,\n            }\n\n    module = MyModule()\n    module.a = nn.Parameter(torch.tensor(0.))\n    module.b = PyroParam(torch.tensor(1.), constraint=constraints.positive)\n    module.c = PyroSample(dist.Normal(0, 1))\n    module.p = PyroModule()\n    module.p.d = nn.Parameter(torch.tensor(3.))\n    module.p.e = PyroParam(torch.tensor(4.), constraint=constraints.positive)\n    module.p.f = PyroSample(dist.Normal(0, 1))\n\n    assert module._pyro_context is module.p._pyro_context\n\n    # Check that results are cached with an invocation of .__call__().\n    result1 = module()\n    actual, expected = result1\n    for key in [""a"", ""c"", ""p.d"", ""p.f""]:\n        assert actual[key] is expected[key], key\n\n    # Check that results are not cached across invocations of .__call__().\n    result2 = module()\n    for key in [""b"", ""c"", ""p.e"", ""p.f""]:\n        assert result1[0] is not result2[0], key\n\n\nclass AttributeModel(PyroModule):\n    def __init__(self, size):\n        super().__init__()\n        self.x = PyroParam(torch.zeros(size))\n        self.y = PyroParam(lambda: torch.randn(size))\n        self.z = PyroParam(torch.ones(size),\n                           constraint=constraints.positive,\n                           event_dim=1)\n        self.s = PyroSample(dist.Normal(0, 1))\n        self.t = PyroSample(lambda self: dist.Normal(self.s, self.z))\n\n    def forward(self):\n        return self.x + self.y + self.t\n\n\nclass DecoratorModel(PyroModule):\n    def __init__(self, size):\n        super().__init__()\n        self.size = size\n\n    @PyroParam\n    def x(self):\n        return torch.zeros(self.size)\n\n    @PyroParam\n    def y(self):\n        return torch.randn(self.size)\n\n    @PyroParam(constraint=constraints.positive, event_dim=1)\n    def z(self):\n        return torch.ones(self.size)\n\n    @PyroSample\n    def s(self):\n        return dist.Normal(0, 1)\n\n    @PyroSample\n    def t(self):\n        return dist.Normal(self.s, self.z).to_event(1)\n\n    def forward(self):\n        return self.x + self.y + self.t\n\n\n@pytest.mark.parametrize(""Model"", [AttributeModel, DecoratorModel])\n@pytest.mark.parametrize(""size"", [1, 2])\ndef test_decorator(Model, size):\n    model = Model(size)\n    for i in range(2):\n        trace = poutine.trace(model).get_trace()\n        assert set(trace.nodes.keys()) == {""_INPUT"", ""x"", ""y"", ""z"", ""s"", ""t"", ""_RETURN""}\n\n        assert trace.nodes[""x""][""type""] == ""param""\n        assert trace.nodes[""y""][""type""] == ""param""\n        assert trace.nodes[""z""][""type""] == ""param""\n        assert trace.nodes[""s""][""type""] == ""sample""\n        assert trace.nodes[""t""][""type""] == ""sample""\n\n        assert trace.nodes[""x""][""value""].shape == (size,)\n        assert trace.nodes[""y""][""value""].shape == (size,)\n        assert trace.nodes[""z""][""value""].shape == (size,)\n        assert trace.nodes[""s""][""value""].shape == ()\n        assert trace.nodes[""t""][""value""].shape == (size,)\n\n\ndef test_mixin_factory():\n    assert PyroModule[nn.Module] is PyroModule\n    assert PyroModule[PyroModule] is PyroModule\n\n    module = PyroModule[nn.Sequential](\n        PyroModule[nn.Linear](28 * 28, 200),\n        PyroModule[nn.Sigmoid](),\n        PyroModule[nn.Linear](200, 200),\n        PyroModule[nn.Sigmoid](),\n        PyroModule[nn.Linear](200, 10),\n    )\n\n    assert isinstance(module, nn.Sequential)\n    assert isinstance(module, PyroModule)\n    assert type(module).__name__ == ""PyroSequential""\n    assert PyroModule[type(module)] is type(module)\n\n    assert isinstance(module[0], nn.Linear)\n    assert isinstance(module[0], PyroModule)\n    assert type(module[0]).__name__ == ""PyroLinear""\n    assert type(module[2]) is type(module[0])  # noqa: E721\n    assert module[0]._pyro_name == ""0""\n    assert module[1]._pyro_name == ""1""\n\n    # Ensure new types are serializable.\n    data = torch.randn(28 * 28)\n    expected = module(data)\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=UserWarning)\n        f = io.BytesIO()\n        torch.save(module, f)\n        del module\n        pyro.clear_param_store()\n        f.seek(0)\n        module = torch.load(f)\n    assert type(module).__name__ == ""PyroSequential""\n    actual = module(data)\n    assert_equal(actual, expected)\n\n\ndef test_to_pyro_module_():\n\n    pyro.set_rng_seed(123)\n    actual = nn.Sequential(\n        nn.Linear(28 * 28, 200),\n        nn.Sigmoid(),\n        nn.Linear(200, 200),\n        nn.Sigmoid(),\n        nn.Linear(200, 10),\n    )\n    to_pyro_module_(actual)\n    pyro.clear_param_store()\n\n    pyro.set_rng_seed(123)\n    expected = PyroModule[nn.Sequential](\n        PyroModule[nn.Linear](28 * 28, 200),\n        PyroModule[nn.Sigmoid](),\n        PyroModule[nn.Linear](200, 200),\n        PyroModule[nn.Sigmoid](),\n        PyroModule[nn.Linear](200, 10),\n    )\n    pyro.clear_param_store()\n\n    def assert_identical(a, e):\n        assert type(a) is type(e)\n        if isinstance(a, dict):\n            assert set(a) == set(e)\n            for key in a:\n                assert_identical(a[key], e[key])\n        elif isinstance(a, nn.Module):\n            assert_identical(a.__dict__, e.__dict__)\n        elif isinstance(a, (str, int, float, torch.Tensor)):\n            assert_equal(a, e)\n\n    assert_identical(actual, expected)\n\n    # check output\n    data = torch.randn(28 * 28)\n    actual_out = actual(data)\n    pyro.clear_param_store()\n    expected_out = expected(data)\n    assert_equal(actual_out, expected_out)\n\n    # check randomization\n    def randomize(model):\n        for m in model.modules():\n            for name, value in list(m.named_parameters(recurse=False)):\n                setattr(m, name, PyroSample(prior=dist.Normal(0, 1)\n                                                      .expand(value.shape)\n                                                      .to_event(value.dim())))\n    randomize(actual)\n    randomize(expected)\n    assert_identical(actual, expected)\n\n\ndef test_torch_serialize_attributes():\n    module = PyroModule()\n    module.x = PyroParam(torch.tensor(1.234), constraints.positive)\n    module.y = nn.Parameter(torch.randn(3))\n    assert isinstance(module.x, torch.Tensor)\n\n    # Work around https://github.com/pytorch/pytorch/issues/27972\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=UserWarning)\n        f = io.BytesIO()\n        torch.save(module, f)\n        pyro.clear_param_store()\n        f.seek(0)\n        actual = torch.load(f)\n\n    assert_equal(actual.x, module.x)\n    actual_names = {name for name, _ in actual.named_parameters()}\n    expected_names = {name for name, _ in module.named_parameters()}\n    assert actual_names == expected_names\n\n\ndef test_torch_serialize_decorators():\n    module = DecoratorModel(3)\n    module()  # initialize\n\n    # Work around https://github.com/pytorch/pytorch/issues/27972\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=UserWarning)\n        f = io.BytesIO()\n        torch.save(module, f)\n        pyro.clear_param_store()\n        f.seek(0)\n        actual = torch.load(f)\n\n    assert_equal(actual.x, module.x)\n    assert_equal(actual.y, module.y)\n    assert_equal(actual.z, module.z)\n    assert actual.s.shape == module.s.shape\n    assert actual.t.shape == module.t.shape\n    actual_names = {name for name, _ in actual.named_parameters()}\n    expected_names = {name for name, _ in module.named_parameters()}\n    assert actual_names == expected_names\n\n\ndef test_pyro_serialize():\n    class MyModule(PyroModule):\n        def __init__(self):\n            super().__init__()\n            self.x = PyroParam(torch.tensor(1.234), constraints.positive)\n            self.y = nn.Parameter(torch.randn(3))\n\n        def forward(self):\n            return self.x, self.y\n\n    module = MyModule()\n    assert len(pyro.get_param_store()) == 0\n\n    assert isinstance(module.x, torch.Tensor)\n    assert len(pyro.get_param_store()) == 0\n\n    actual = module()  # triggers saving in param store\n    assert_equal(actual[0], module.x)\n    assert_equal(actual[1], module.y)\n    assert set(pyro.get_param_store().keys()) == {""x"", ""y""}\n    assert_equal(pyro.param(""x"").detach(), module.x.detach())\n    assert_equal(pyro.param(""y"").detach(), module.y.detach())\n\n    pyro.get_param_store().save(""/tmp/pyro_module.pt"")\n    pyro.clear_param_store()\n    assert len(pyro.get_param_store()) == 0\n    pyro.get_param_store().load(""/tmp/pyro_module.pt"")\n    assert set(pyro.get_param_store().keys()) == {""x"", ""y""}\n    actual = MyModule()\n    actual()\n\n    assert_equal(actual.x, module.x)\n    actual_names = {name for name, _ in actual.named_parameters()}\n    expected_names = {name for name, _ in module.named_parameters()}\n    assert actual_names == expected_names\n\n\ndef test_bayesian_gru():\n    input_size = 2\n    hidden_size = 3\n    batch_size = 4\n    seq_len = 5\n\n    # Construct a simple GRU.\n    gru = nn.GRU(input_size, hidden_size)\n    input_ = torch.randn(seq_len, batch_size, input_size)\n    output, _ = gru(input_)\n    assert output.shape == (seq_len, batch_size, hidden_size)\n    output2, _ = gru(input_)\n    assert torch.allclose(output2, output)\n\n    # Make it Bayesian.\n    to_pyro_module_(gru)\n    for name, value in list(gru.named_parameters(recurse=False)):\n        prior = dist.Normal(0, 1).expand(value.shape).to_event(value.dim())\n        setattr(gru, name, PyroSample(prior=prior))\n    output, _ = gru(input_)\n    assert output.shape == (seq_len, batch_size, hidden_size)\n    output2, _ = gru(input_)\n    assert not torch.allclose(output2, output)\n'"
tests/ops/__init__.py,0,b''
tests/ops/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/ops""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""unit""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/ops/gamma_gaussian.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.ops.gamma_gaussian import GammaGaussian\nfrom tests.common import assert_close\n\n\ndef random_gamma_gaussian(batch_shape, dim, rank=None):\n    """"""\n    Generate a random Gaussian for testing.\n    """"""\n    if rank is None:\n        rank = dim + dim\n    log_normalizer = torch.randn(batch_shape)\n    loc = torch.randn(batch_shape + (dim,))\n    samples = torch.randn(batch_shape + (dim, rank))\n    precision = torch.matmul(samples, samples.transpose(-2, -1))\n    if dim > 0:\n        info_vec = precision.matmul(loc.unsqueeze(-1)).squeeze(-1)\n    else:\n        info_vec = loc\n    alpha = torch.randn(batch_shape).exp() + 0.5 * dim - 1\n    beta = torch.randn(batch_shape).exp() + 0.5 * (info_vec * loc).sum(-1)\n    result = GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n    assert result.dim() == dim\n    assert result.batch_shape == batch_shape\n    return result\n\n\ndef random_gamma(batch_shape):\n    """"""\n    Generate a random Gamma distribution for testing.\n    """"""\n    concentration = torch.randn(batch_shape).exp()\n    rate = torch.randn(batch_shape).exp()\n    return dist.Gamma(concentration, rate)\n\n\ndef assert_close_gamma_gaussian(actual, expected):\n    assert isinstance(actual, GammaGaussian)\n    assert isinstance(expected, GammaGaussian)\n    assert actual.dim() == expected.dim()\n    assert actual.batch_shape == expected.batch_shape\n    assert_close(actual.log_normalizer, expected.log_normalizer)\n    assert_close(actual.info_vec, expected.info_vec)\n    assert_close(actual.precision, expected.precision)\n    assert_close(actual.alpha, expected.alpha)\n    assert_close(actual.beta, expected.beta)\n'"
tests/ops/gaussian.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.ops.gaussian import Gaussian\nfrom tests.common import assert_close\n\n\ndef random_gaussian(batch_shape, dim, rank=None):\n    """"""\n    Generate a random Gaussian for testing.\n    """"""\n    if rank is None:\n        rank = dim + dim\n    log_normalizer = torch.randn(batch_shape)\n    info_vec = torch.randn(batch_shape + (dim,))\n    samples = torch.randn(batch_shape + (dim, rank))\n    precision = torch.matmul(samples, samples.transpose(-2, -1))\n    result = Gaussian(log_normalizer, info_vec, precision)\n    assert result.dim() == dim\n    assert result.batch_shape == batch_shape\n    return result\n\n\ndef random_mvn(batch_shape, dim):\n    """"""\n    Generate a random MultivariateNormal distribution for testing.\n    """"""\n    rank = dim + dim\n    loc = torch.randn(batch_shape + (dim,))\n    cov = torch.randn(batch_shape + (dim, rank))\n    cov = cov.matmul(cov.transpose(-1, -2))\n    return dist.MultivariateNormal(loc, cov)\n\n\ndef assert_close_gaussian(actual, expected):\n    assert isinstance(actual, Gaussian)\n    assert isinstance(expected, Gaussian)\n    assert actual.dim() == expected.dim()\n    assert actual.batch_shape == expected.batch_shape\n    assert_close(actual.log_normalizer, expected.log_normalizer)\n    assert_close(actual.info_vec, expected.info_vec)\n    assert_close(actual.precision, expected.precision)\n'"
tests/ops/test_arrowhead.py,5,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.ops.arrowhead import SymmArrowhead, sqrt, triu_gram, triu_inverse, triu_matvecmul\n\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize('head_size', [0, 2, 5])\ndef test_utilities(head_size):\n    size = 5\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n\n    mask = torch.ones(size, size)\n    mask[head_size:, head_size:] = 0.\n    mask.view(-1)[::size + 1][head_size:] = 1.\n    arrowhead_full = mask * cov\n    expected = torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1))\n    # test if those flip ops give expected upper triangular values\n    assert_close(expected.triu(), expected)\n    assert_close(expected.matmul(expected.t()), arrowhead_full)\n\n    # test sqrt\n    arrowhead = SymmArrowhead(cov[:head_size], cov.diag()[head_size:])\n    actual = sqrt(arrowhead)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n\n    # test triu_inverse\n    expected = expected.inverse()\n    actual = triu_inverse(actual)\n    assert_close(actual.top, expected[:head_size])\n    assert_close(actual.bottom_diag, expected.diag()[head_size:])\n\n    # test triu_matvecmul\n    v = torch.randn(size)\n    assert_close(triu_matvecmul(actual, v), expected.matmul(v))\n    assert_close(triu_matvecmul(actual, v, transpose=True),\n                 expected.t().matmul(v))\n\n    # test triu_gram\n    actual = triu_gram(actual)\n    expected = arrowhead_full.inverse() if head_size > 0 else arrowhead_full.diag().reciprocal()\n    assert_close(actual, expected)\n"""
tests/ops/test_contract.py,44,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport numbers\nfrom collections import OrderedDict\n\nimport opt_einsum\nimport pytest\nimport torch\n\nimport pyro.ops.jit\nfrom pyro.distributions.util import logsumexp\nfrom pyro.ops.contract import _partition_terms, contract_tensor_tree, contract_to_tensor, einsum, naive_ubersum, ubersum\nfrom pyro.ops.einsum.adjoint import require_backward\nfrom pyro.ops.rings import LogRing\nfrom pyro.poutine.indep_messenger import CondIndepStackFrame\nfrom pyro.util import optional\nfrom tests.common import assert_equal\n\n\ndef deep_copy(x):\n    """"""\n    Deep copy to detect mutation, assuming tensors will not be mutated.\n    """"""\n    if isinstance(x, (tuple, frozenset, numbers.Number, torch.Tensor)):\n        return x  # assume x is immutable\n    if isinstance(x, (list, set)):\n        return type(x)(deep_copy(value) for value in x)\n    if isinstance(x, (dict, OrderedDict)):\n        return type(x)((deep_copy(key), deep_copy(value)) for key, value in x.items())\n    if isinstance(x, str):\n        return x\n    raise TypeError(type(x))\n\n\ndef deep_equal(x, y):\n    """"""\n    Deep comparison, assuming tensors will not be mutated.\n    """"""\n    if type(x) != type(y):\n        return False\n    if isinstance(x, (tuple, frozenset, set, numbers.Number)):\n        return x == y\n    if isinstance(x, torch.Tensor):\n        return x is y\n    if isinstance(x, list):\n        if len(x) != len(y):\n            return False\n        return all((deep_equal(xi, yi) for xi, yi in zip(x, y)))\n    if isinstance(x, (dict, OrderedDict)):\n        if len(x) != len(y):\n            return False\n        if any(key not in y for key in x):\n            return False\n        return all(deep_equal(x[key], y[key]) for key in x)\n    raise TypeError(type(x))\n\n\ndef assert_immutable(fn):\n    """"""\n    Decorator to check that function args are not mutated.\n    """"""\n\n    def checked_fn(*args):\n        copies = tuple(deep_copy(arg) for arg in args)\n        result = fn(*args)\n        for pos, (arg, copy) in enumerate(zip(args, copies)):\n            if not deep_equal(arg, copy):\n                raise AssertionError(\'{} mutated arg {} of type {}.\\nOld:\\n{}\\nNew:\\n{}\'\n                                     .format(fn.__name__, pos, type(arg).__name__, copy, arg))\n        return result\n\n    return checked_fn\n\n\ndef _normalize(tensor, dims, plates):\n    total = tensor\n    for i, dim in enumerate(dims):\n        if dim not in plates:\n            total = logsumexp(total, i, keepdim=True)\n    return tensor - total\n\n\n@pytest.mark.parametrize(\'inputs,dims,expected_num_components\', [\n    ([\'\'], set(), 1),\n    ([\'a\'], set(), 1),\n    ([\'a\'], set(\'a\'), 1),\n    ([\'a\', \'a\'], set(), 2),\n    ([\'a\', \'a\'], set(\'a\'), 1),\n    ([\'a\', \'a\', \'b\', \'b\'], set(), 4),\n    ([\'a\', \'a\', \'b\', \'b\'], set(\'a\'), 3),\n    ([\'a\', \'a\', \'b\', \'b\'], set(\'b\'), 3),\n    ([\'a\', \'a\', \'b\', \'b\'], set(\'ab\'), 2),\n    ([\'a\', \'ab\', \'b\'], set(), 3),\n    ([\'a\', \'ab\', \'b\'], set(\'a\'), 2),\n    ([\'a\', \'ab\', \'b\'], set(\'b\'), 2),\n    ([\'a\', \'ab\', \'b\'], set(\'ab\'), 1),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(), 4),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'c\'), 3),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'b\'), 3),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'a\'), 3),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'ac\'), 2),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'abc\'), 1),\n])\ndef test_partition_terms(inputs, dims, expected_num_components):\n    ring = LogRing()\n    symbol_to_size = dict(zip(\'abc\', [2, 3, 4]))\n    shapes = [tuple(symbol_to_size[s] for s in input_) for input_ in inputs]\n    tensors = [torch.randn(shape) for shape in shapes]\n    for input_, tensor in zip(inputs, tensors):\n        tensor._pyro_dims = input_\n    components = list(_partition_terms(ring, tensors, dims))\n\n    # Check that result is a partition.\n    expected_terms = sorted(tensors, key=id)\n    actual_terms = sorted((x for c in components for x in c[0]), key=id)\n    assert actual_terms == expected_terms\n    assert dims == set.union(set(), *(c[1] for c in components))\n\n    # Check that the partition is not too coarse.\n    assert len(components) == expected_num_components\n\n    # Check that partition is not too fine.\n    component_dict = {x: i for i, (terms, _) in enumerate(components) for x in terms}\n    for x in tensors:\n        for y in tensors:\n            if x is not y:\n                if dims.intersection(x._pyro_dims, y._pyro_dims):\n                    assert component_dict[x] == component_dict[y]\n\n\ndef frame(dim, size):\n    return CondIndepStackFrame(name=""plate_{}"".format(size), dim=dim, size=size, counter=0)\n\n\nEXAMPLES = [\n    # ------------------------------------------------------\n    #  y      max_plate_nesting=1\n    #  | 4    x, y are enumerated in dims:\n    #  x      a, b\n    {\n        \'shape_tree\': {\n            frozenset(): [\'a\'],\n            frozenset(\'i\'): [\'abi\'],\n        },\n        \'sum_dims\': set(\'ab\'),\n        \'target_dims\': set(),\n        \'target_ordinal\': frozenset(),\n        \'expected_dims\': (),\n    },\n    {\n        \'shape_tree\': {\n            frozenset(): [\'a\'],\n            frozenset(\'i\'): [\'abi\'],\n        },\n        \'sum_dims\': set(\'ab\'),\n        \'target_dims\': set(\'a\'),\n        \'target_ordinal\': frozenset(),\n        \'expected_dims\': \'a\',\n    },\n    {\n        \'shape_tree\': {\n            frozenset(): [\'a\'],\n            frozenset(\'i\'): [\'abi\'],\n        },\n        \'sum_dims\': set(\'ab\'),\n        \'target_dims\': set(\'b\'),\n        \'target_ordinal\': frozenset(\'i\'),\n        \'expected_dims\': \'bi\',\n    },\n    {\n        \'shape_tree\': {\n            frozenset(): [\'a\'],\n            frozenset(\'i\'): [\'abi\'],\n        },\n        \'sum_dims\': set(\'ab\'),\n        \'target_dims\': set(\'ab\'),\n        \'target_ordinal\': frozenset(\'i\'),\n        \'expected_dims\': \'abi\',\n    },\n    # ------------------------------------------------------\n    #          z\n    #          | 4    max_plate_nesting=2\n    #    x     y      w, x, y, z are all enumerated in dims:\n    #   2 \\   / 3     a, b, c, d\n    #       w\n    {\n        \'shape_tree\': {\n            frozenset(): [\'a\'],  # w\n            frozenset(\'i\'): [\'abi\'],  # x\n            frozenset(\'j\'): [\'acj\'],  # y\n            frozenset(\'ij\'): [\'cdij\'],  # z\n        },\n        # query for w\n        \'sum_dims\': set(\'abcd\'),\n        \'target_dims\': set(\'a\'),\n        \'target_ordinal\': frozenset(),\n        \'expected_dims\': \'a\',\n    },\n    {\n        \'shape_tree\': {\n            frozenset(): [\'a\'],  # w\n            frozenset(\'i\'): [\'abi\'],  # x\n            frozenset(\'j\'): [\'acj\'],  # y\n            frozenset(\'ij\'): [\'cdij\'],  # z\n        },\n        # query for x\n        \'sum_dims\': set(\'abcd\'),\n        \'target_dims\': set(\'b\'),\n        \'target_ordinal\': frozenset(\'i\'),\n        \'expected_dims\': \'bi\',\n    },\n    {\n        \'shape_tree\': {\n            frozenset(): [\'a\'],  # w\n            frozenset(\'i\'): [\'abi\'],  # x\n            frozenset(\'j\'): [\'acj\'],  # y\n            frozenset(\'ij\'): [\'cdij\'],  # z\n        },\n        # query for y\n        \'sum_dims\': set(\'abcd\'),\n        \'target_dims\': set(\'c\'),\n        \'target_ordinal\': frozenset(\'j\'),\n        \'expected_dims\': \'cj\',\n    },\n    {\n        \'shape_tree\': {\n            frozenset(): [\'a\'],  # w\n            frozenset(\'i\'): [\'abi\'],  # x\n            frozenset(\'j\'): [\'acj\'],  # y\n            frozenset(\'ij\'): [\'cdij\'],  # z\n        },\n        # query for z\n        \'sum_dims\': set(\'abcd\'),\n        \'target_dims\': set(\'d\'),\n        \'target_ordinal\': frozenset(\'ij\'),\n        \'expected_dims\': \'dij\',\n    },\n]\n\n\n@pytest.mark.parametrize(\'example\', EXAMPLES)\ndef test_contract_to_tensor(example):\n    symbol_to_size = dict(zip(\'abcdij\', [4, 5, 6, 7, 2, 3]))\n    tensor_tree = OrderedDict()\n    for t, shapes in example[\'shape_tree\'].items():\n        for dims in shapes:\n            tensor = torch.randn(tuple(symbol_to_size[s] for s in dims))\n            tensor._pyro_dims = dims\n            tensor_tree.setdefault(t, []).append(tensor)\n    sum_dims = example[\'sum_dims\']\n    target_dims = example[\'target_dims\']\n    target_ordinal = example[\'target_ordinal\']\n    expected_dims = example[\'expected_dims\']\n\n    actual = assert_immutable(contract_to_tensor)(tensor_tree, sum_dims, target_ordinal, target_dims)\n    assert set(actual._pyro_dims) == set(expected_dims)\n\n\n@pytest.mark.parametrize(\'example\', EXAMPLES)\ndef test_contract_tensor_tree(example):\n    symbol_to_size = dict(zip(\'abcdij\', [4, 5, 6, 7, 2, 3]))\n    tensor_tree = OrderedDict()\n    for t, shapes in example[\'shape_tree\'].items():\n        for dims in shapes:\n            tensor = torch.randn(tuple(symbol_to_size[s] for s in dims))\n            tensor._pyro_dims = dims\n            tensor_tree.setdefault(t, []).append(tensor)\n    sum_dims = example[\'sum_dims\']\n\n    tensor_tree = assert_immutable(contract_tensor_tree)(tensor_tree, sum_dims)\n    assert tensor_tree\n    for ordinal, terms in tensor_tree.items():\n        for term in terms:\n            for frame in ordinal:\n                assert term.shape[frame.dim] == frame.size\n\n\n# Let abcde be enum dims and ijk be plates.\nUBERSUM_EXAMPLES = [\n    (\'->\', \'\'),\n    (\'a->,a\', \'\'),\n    (\'ab->,a,b,ab,ba\', \'\'),\n    (\'ab,bc->,a,b,c,ab,bc,ac,abc\', \'\'),\n    (\'ab,bc,cd->,a,b,c,d,ab,ac,ad,bc,bd,cd,abc,acd,bcd,abcd\', \'\'),\n    (\'i->,i\', \'i\'),\n    (\',i->,i\', \'i\'),\n    (\',i,i->,i\', \'i\'),\n    (\',i,ia->,i,ia\', \'i\'),\n    (\',i,i,ia,ia->,i,ia\', \'i\'),\n    (\'bi,ia->,i,ia,ib,iab\', \'i\'),\n    (\'abi,b->,b,ai,abi\', \'i\'),\n    (\'ia,ja,ija->,a,i,ia,j,ja,ija\', \'ij\'),\n    (\'i,jb,ijab->,i,j,jb,ij,ija,ijb,ijab\', \'ij\'),\n    (\'ia,jb,ijab->,i,ia,j,jb,ij,ija,ijb,ijab\', \'ij\'),\n    (\',i,j,a,ij,ia,ja,ija->,a,i,j,ia,ja,ij,ija\', \'ij\'),\n    (\'a,b,c,di,ei,fj->,a,b,c,di,ei,fj\', \'ij\'),\n    # {ij}   {ik}\n    #   a\\   /a\n    #     {i}\n    (\'ija,ika->,i,j,k,ij,ik,ijk,ia,ija,ika,ijka\', \'ijk\'),\n    # {ij}   {ik}\n    #   a\\   /a\n    #     {i}      {}\n    (\',ia,ija,ika->,i,j,k,ij,ik,ijk,ia,ija,ika,ijka\', \'ijk\'),\n    #  {i} c\n    #   |b\n    #  {} a\n    (\'ab,bci->,a,b,ab,i,ai,bi,ci,abi,bci,abci\', \'i\'),\n    #  {i} cd\n    #   |b\n    #  {} a\n    (\'ab,bci,bdi->,a,b,ab,i,ai,bi,ci,abi,bci,bdi,cdi,abci,abdi,abcdi\', \'i\'),\n    #  {ij} c\n    #   |b\n    #  {} a\n    (\'ab,bcij->,a,b,ab,i,j,ij,ai,aj,aij,bi,bj,aij,bij,cij,abij,acij,bcij,abcij\', \'ij\'),\n    #  {ij} c\n    #   |b\n    #  {i} a\n    (\'abi,bcij->,i,ai,bi,abi,j,ij,aij,bij,cij,abij,bcij,abcij\', \'ij\'),\n    # {ij} e\n    #   |d\n    #  {i} c\n    #   |b\n    #  {} a\n    (\'ab,bcdi,deij->,a,b,ci,di,eij\', \'ij\'),\n    # {ijk} g\n    #   |f\n    # {ij} e\n    #   |d\n    #  {i} c\n    #   |b\n    #  {} a\n    (\'ab,bcdi,defij,fgijk->,a,b,ci,di,eij,fij,gijk\', \'ijk\'),\n    # {ik}  {ij}   {ij}\n    #   a\\   /b    /e\n    #     {i}    {j}\n    #       c\\  /d\n    #         {}\n    (\'aik,bij,abci,cd,dej,eij->,ai,bi,ej,aik,bij,eij\', \'ijk\'),\n    # {ij}    {ij}\n    #  a|      |d\n    #  {i}    {j}\n    #    b\\  /c\n    #      {}\n    (\'aij,abi,bc,cdj,dij->,bi,cj,aij,dij,adij\', \'ij\'),\n]\n\n\ndef make_example(equation, fill=None, sizes=(2, 3)):\n    symbols = sorted(set(equation) - set(\',->\'))\n    sizes = {dim: size for dim, size in zip(symbols, itertools.cycle(sizes))}\n    inputs, outputs = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n    outputs = outputs.split(\',\')\n    operands = []\n    for dims in inputs:\n        shape = tuple(sizes[dim] for dim in dims)\n        operands.append(torch.randn(shape) if fill is None else torch.full(shape, fill))\n    return inputs, outputs, operands, sizes\n\n\n@pytest.mark.parametrize(\'equation,plates\', UBERSUM_EXAMPLES)\ndef test_naive_ubersum(equation, plates):\n    inputs, outputs, operands, sizes = make_example(equation)\n\n    actual = naive_ubersum(equation, *operands, plates=plates)\n\n    assert isinstance(actual, tuple)\n    assert len(actual) == len(outputs)\n    for output, actual_part in zip(outputs, actual):\n        expected_shape = tuple(sizes[dim] for dim in output)\n        assert actual_part.shape == expected_shape\n        if not plates:\n            equation_part = \',\'.join(inputs) + \'->\' + output\n            expected_part = opt_einsum.contract(equation_part, *operands,\n                                                backend=\'pyro.ops.einsum.torch_log\')\n            assert_equal(expected_part, actual_part,\n                         msg=u""For output \'{}\':\\nExpected:\\n{}\\nActual:\\n{}"".format(\n                             output, expected_part.detach().cpu(), actual_part.detach().cpu()))\n\n\n@pytest.mark.parametrize(\'equation,plates\', UBERSUM_EXAMPLES)\ndef test_ubersum(equation, plates):\n    inputs, outputs, operands, sizes = make_example(equation)\n\n    try:\n        actual = ubersum(equation, *operands, plates=plates, modulo_total=True)\n    except NotImplementedError:\n        pytest.skip()\n\n    assert isinstance(actual, tuple)\n    assert len(actual) == len(outputs)\n    expected = naive_ubersum(equation, *operands, plates=plates)\n    for output, expected_part, actual_part in zip(outputs, expected, actual):\n        actual_part = _normalize(actual_part, output, plates)\n        expected_part = _normalize(expected_part, output, plates)\n        assert_equal(expected_part, actual_part,\n                     msg=u""For output \'{}\':\\nExpected:\\n{}\\nActual:\\n{}"".format(\n                         output, expected_part.detach().cpu(), actual_part.detach().cpu()))\n\n\n@pytest.mark.parametrize(\'equation,plates\', UBERSUM_EXAMPLES)\ndef test_einsum_linear(equation, plates):\n    inputs, outputs, log_operands, sizes = make_example(equation)\n    operands = [x.exp() for x in log_operands]\n\n    try:\n        log_expected = ubersum(equation, *log_operands, plates=plates, modulo_total=True)\n        expected = [x.exp() for x in log_expected]\n    except NotImplementedError:\n        pytest.skip()\n\n    # einsum() is in linear space whereas ubersum() is in log space.\n    actual = einsum(equation, *operands, plates=plates, modulo_total=True)\n    assert isinstance(actual, tuple)\n    assert len(actual) == len(outputs)\n    for output, expected_part, actual_part in zip(outputs, expected, actual):\n        assert_equal(expected_part.log(), actual_part.log(),\n                     msg=u""For output \'{}\':\\nExpected:\\n{}\\nActual:\\n{}"".format(\n                         output, expected_part.detach().cpu(), actual_part.detach().cpu()))\n\n\n@pytest.mark.parametrize(\'equation,plates\', UBERSUM_EXAMPLES)\ndef test_ubersum_jit(equation, plates):\n    inputs, outputs, operands, sizes = make_example(equation)\n\n    try:\n        expected = ubersum(equation, *operands, plates=plates, modulo_total=True)\n    except NotImplementedError:\n        pytest.skip()\n\n    @pyro.ops.jit.trace\n    def jit_ubersum(*operands):\n        return ubersum(equation, *operands, plates=plates, modulo_total=True)\n\n    actual = jit_ubersum(*operands)\n\n    if not isinstance(actual, tuple):\n        pytest.xfail(reason=""https://github.com/pytorch/pytorch/issues/14875"")\n    assert len(expected) == len(actual)\n    for e, a in zip(expected, actual):\n        assert_equal(e, a)\n\n\n@pytest.mark.parametrize(\'equation,plates\', [\n    (\'i->\', \'i\'),\n    (\'i->i\', \'i\'),\n    (\',i->\', \'i\'),\n    (\',i->i\', \'i\'),\n    (\'ai->\', \'i\'),\n    (\'ai->i\', \'i\'),\n    (\'ai->ai\', \'i\'),\n    (\',ai,abij->aij\', \'ij\'),\n    (\'a,ai,bij->bij\', \'ij\'),\n    (\'a,ai,abij->bij\', \'ij\'),\n    (\'a,abi,bcij->a\', \'ij\'),\n    (\'a,abi,bcij->bi\', \'ij\'),\n    (\'a,abi,bcij->bij\', \'ij\'),\n    (\'a,abi,bcij->cij\', \'ij\'),\n    (\'ab,bcdi,deij->eij\', \'ij\'),\n])\ndef test_ubersum_total(equation, plates):\n    inputs, outputs, operands, sizes = make_example(equation, fill=1., sizes=(2,))\n    output = outputs[0]\n\n    expected = naive_ubersum(equation, *operands, plates=plates)[0]\n    actual = ubersum(equation, *operands, plates=plates, modulo_total=True)[0]\n    expected = _normalize(expected, output, plates)\n    actual = _normalize(actual, output, plates)\n    assert_equal(expected, actual,\n                 msg=u""Expected:\\n{}\\nActual:\\n{}"".format(\n                     expected.detach().cpu(), actual.detach().cpu()))\n\n\n@pytest.mark.parametrize(\'a\', [2, 1])\n@pytest.mark.parametrize(\'b\', [3, 1])\n@pytest.mark.parametrize(\'c\', [3, 1])\n@pytest.mark.parametrize(\'d\', [4, 1])\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_sizes(impl, a, b, c, d):\n    X = torch.randn(a, b)\n    Y = torch.randn(b, c)\n    Z = torch.randn(c, d)\n    actual = impl(\'ab,bc,cd->a,b,c,d\', X, Y, Z, plates=\'ad\', modulo_total=True)\n    actual_a, actual_b, actual_c, actual_d = actual\n    assert actual_a.shape == (a,)\n    assert actual_b.shape == (b,)\n    assert actual_c.shape == (c,)\n    assert actual_d.shape == (d,)\n\n\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_1(impl):\n    # y {a}   z {b}\n    #      \\  /\n    #     x {}  <--- target\n    a, b, c, d, e = 2, 3, 4, 5, 6\n    x = torch.randn(c)\n    y = torch.randn(c, d, a)\n    z = torch.randn(e, c, b)\n    actual, = impl(\'c,cda,ecb->\', x, y, z, plates=\'ab\', modulo_total=True)\n    expected = logsumexp(x + logsumexp(y, -2).sum(-1) + logsumexp(z, -3).sum(-1), -1)\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_2(impl):\n    # y {a}   z {b}  <--- target\n    #      \\  /\n    #     x {}\n    a, b, c, d, e = 2, 3, 4, 5, 6\n    x = torch.randn(c)\n    y = torch.randn(c, d, a)\n    z = torch.randn(e, c, b)\n    actual, = impl(\'c,cda,ecb->b\', x, y, z, plates=\'ab\', modulo_total=True)\n    xyz = logsumexp(x + logsumexp(y, -2).sum(-1) + logsumexp(z, -3).sum(-1), -1)\n    expected = xyz.expand(b)\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_3(impl):\n    #       z {b,c}\n    #           |\n    # w {a}  y {b}  <--- target\n    #      \\  /\n    #     x {}\n    a, b, c, d, e = 2, 3, 4, 5, 6\n    w = torch.randn(a, e)\n    x = torch.randn(d)\n    y = torch.randn(b, d)\n    z = torch.randn(b, c, d, e)\n    actual, = impl(\'ae,d,bd,bcde->be\', w, x, y, z, plates=\'abc\', modulo_total=True)\n    yz = y.reshape(b, d, 1) + z.sum(-3)  # eliminate c\n    assert yz.shape == (b, d, e)\n    yz = yz.sum(0)  # eliminate b\n    assert yz.shape == (d, e)\n    wxyz = w.sum(0) + x.reshape(d, 1) + yz  # eliminate a\n    assert wxyz.shape == (d, e)\n    wxyz = logsumexp(wxyz, 0)  # eliminate d\n    assert wxyz.shape == (e,)\n    expected = wxyz.expand(b, e)  # broadcast to b\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_4(impl):\n    # x,y {b}  <--- target\n    #      |\n    #     {}\n    a, b, c, d = 2, 3, 4, 5\n    x = torch.randn(a, b)\n    y = torch.randn(d, b, c)\n    actual, = impl(\'ab,dbc->dc\', x, y, plates=\'d\', modulo_total=True)\n    x_b1 = logsumexp(x, 0).unsqueeze(-1)\n    assert x_b1.shape == (b, 1)\n    y_db1 = logsumexp(y, 2, keepdim=True)\n    assert y_db1.shape == (d, b, 1)\n    y_dbc = y_db1.sum(0) - y_db1 + y  # inclusion-exclusion\n    assert y_dbc.shape == (d, b, c)\n    xy_dc = logsumexp(x_b1 + y_dbc, 1)\n    assert xy_dc.shape == (d, c)\n    expected = xy_dc\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_5(impl):\n    # z {ij}  <--- target\n    #     |\n    #  y {i}\n    #     |\n    #  x {}\n    i, j, a, b, c = 2, 3, 6, 5, 4\n    x = torch.randn(a)\n    y = torch.randn(a, b, i)\n    z = torch.randn(b, c, i, j)\n    actual, = impl(\'a,abi,bcij->cij\', x, y, z, plates=\'ij\', modulo_total=True)\n\n    # contract plate j\n    s1 = logsumexp(z, 1)\n    assert s1.shape == (b, i, j)\n    p1 = s1.sum(2)\n    assert p1.shape == (b, i)\n    q1 = z - s1.unsqueeze(-3)\n    assert q1.shape == (b, c, i, j)\n\n    # contract plate i\n    x2 = y + p1\n    assert x2.shape == (a, b, i)\n    s2 = logsumexp(x2, 1)\n    assert s2.shape == (a, i)\n    p2 = s2.sum(1)\n    assert p2.shape == (a,)\n    q2 = x2 - s2.unsqueeze(-2)\n    assert q2.shape == (a, b, i)\n\n    expected = opt_einsum.contract(\'a,a,abi,bcij->cij\', x, p2, q2, q1,\n                                   backend=\'pyro.ops.einsum.torch_log\')\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(\'impl,implemented\', [(naive_ubersum, True), (ubersum, False)])\ndef test_ubersum_collide_implemented(impl, implemented):\n    # Non-tree plates cause exponential blowup,\n    # so ubersum() refuses to evaluate them.\n    #\n    #   z {a,b}\n    #     /   \\\n    # x {a}  y {b}\n    #      \\  /\n    #       {}  <--- target\n    a, b, c, d = 2, 3, 4, 5\n    x = torch.randn(a, c)\n    y = torch.randn(b, d)\n    z = torch.randn(a, b, c, d)\n    raises = pytest.raises(NotImplementedError, match=\'Expected tree-structured plate nesting\')\n    with optional(raises, not implemented):\n        impl(\'ac,bd,abcd->\', x, y, z, plates=\'ab\', modulo_total=True)\n\n\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_collide_ok_1(impl):\n    # The following is ok because it splits into connected components\n    # {x,z1} and {y,z2}, thereby avoiding exponential blowup.\n    #\n    # z1,z2 {a,b}\n    #       /   \\\n    #   x {a}  y {b}\n    #        \\  /\n    #         {}  <--- target\n    a, b, c, d = 2, 3, 4, 5\n    x = torch.randn(a, c)\n    y = torch.randn(b, d)\n    z1 = torch.randn(a, b, c)\n    z2 = torch.randn(a, b, d)\n    impl(\'ac,bd,abc,abd->\', x, y, z1, z2, plates=\'ab\', modulo_total=True)\n\n\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_collide_ok_2(impl):\n    # The following is ok because z1 can be contracted to x and\n    # z2 can be contracted to y.\n    #\n    # z1,z2 {a,b}\n    #       /   \\\n    #   x {a}  y {b}\n    #        \\  /\n    #       w {}  <--- target\n    a, b, c, d = 2, 3, 4, 5\n    w = torch.randn(c, d)\n    x = torch.randn(a, c)\n    y = torch.randn(b, d)\n    z1 = torch.randn(a, b, c)\n    z2 = torch.randn(a, b, d)\n    impl(\'cd,ac,bd,abc,abd->\', w, x, y, z1, z2, plates=\'ab\', modulo_total=True)\n\n\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_collide_ok_3(impl):\n    # The following is ok because x, y, and z can be independently contracted to w.\n    #\n    #      z {a,b}\n    # x {a}   |   y {b}\n    #      \\  |  /\n    #       \\ | /\n    #       w {}  <--- target\n    a, b, c = 2, 3, 4\n    w = torch.randn(c)\n    x = torch.randn(a, c)\n    y = torch.randn(b, c)\n    z = torch.randn(a, b, c)\n    impl(\'c,ac,bc,abc->\', w, x, y, z, plates=\'ab\', modulo_total=True)\n\n\nUBERSUM_SHAPE_ERRORS = [\n    (\'ab,bc->\', [(2, 3), (4, 5)], \'\'),\n    (\'ab,bc->\', [(2, 3), (4, 5)], \'b\'),\n]\n\n\n@pytest.mark.parametrize(\'equation,shapes,plates\', UBERSUM_SHAPE_ERRORS)\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_size_error(impl, equation, shapes, plates):\n    operands = [torch.randn(shape) for shape in shapes]\n    with pytest.raises(ValueError, match=\'Dimension size mismatch|Size of label\'):\n        impl(equation, *operands, plates=plates, modulo_total=True)\n\n\nUBERSUM_BATCH_ERRORS = [\n    (\'ai->a\', \'i\'),\n    (\',ai->a\', \'i\'),\n    (\'bi,abi->b\', \'i\'),\n    (\',bi,abi->b\', \'i\'),\n    (\'aij->ai\', \'ij\'),\n    (\'aij->aj\', \'ij\'),\n]\n\n\n@pytest.mark.parametrize(\'equation,plates\', UBERSUM_BATCH_ERRORS)\n@pytest.mark.parametrize(\'impl\', [naive_ubersum, ubersum])\ndef test_ubersum_plate_error(impl, equation, plates):\n    inputs, outputs = equation.split(\'->\')\n    operands = [torch.randn(torch.Size((2,) * len(input_)))\n                for input_ in inputs.split(\',\')]\n    with pytest.raises(ValueError, match=\'It is nonsensical to preserve a plated dim\'):\n        impl(equation, *operands, plates=plates, modulo_total=True)\n\n\nADJOINT_EXAMPLES = [\n    (\'a->\', \'\'),\n    (\'a,a->\', \'\'),\n    (\'ab,bc->\', \'\'),\n    (\'a,abi->\', \'i\'),\n    (\'a,abi,bcij->\', \'ij\'),\n    (\'a,abi,bcij,bdik->\', \'ijk\'),\n    (\'ai,ai->i\', \'i\'),\n    (\'ai,abij->i\', \'ij\'),\n    (\'ai,abij,acik->i\', \'ijk\'),\n]\n\n\n@pytest.mark.parametrize(\'equation,plates\', ADJOINT_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', [\'map\', \'sample\', \'marginal\'])\ndef test_adjoint_shape(backend, equation, plates):\n    backend = \'pyro.ops.einsum.torch_{}\'.format(backend)\n    inputs, output = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n    operands = [torch.randn(torch.Size((2,) * len(input_)))\n                for input_ in inputs]\n    for input_, x in zip(inputs, operands):\n        x._pyro_dims = input_\n\n    # run forward-backward algorithm\n    for x in operands:\n        require_backward(x)\n    result, = ubersum(equation, *operands, plates=plates,\n                      modulo_total=True, backend=backend)\n    result._pyro_backward()\n\n    for input_, x in zip(inputs, operands):\n        backward_result = x._pyro_backward_result\n        contract_dims = set(input_) - set(output) - set(plates)\n        if contract_dims:\n            assert backward_result is not None\n        else:\n            assert backward_result is None\n\n\n@pytest.mark.parametrize(\'equation,plates\', ADJOINT_EXAMPLES)\ndef test_adjoint_marginal(equation, plates):\n    inputs, output = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n    operands = [torch.randn(torch.Size((2,) * len(input_)))\n                for input_ in inputs]\n    for input_, x in zip(inputs, operands):\n        x._pyro_dims = input_\n\n    # check forward pass\n    for x in operands:\n        require_backward(x)\n    actual, = ubersum(equation, *operands, plates=plates, modulo_total=True,\n                      backend=\'pyro.ops.einsum.torch_marginal\')\n    expected, = ubersum(equation, *operands, plates=plates, modulo_total=True,\n                        backend=\'pyro.ops.einsum.torch_log\')\n    assert_equal(expected, actual)\n\n    # check backward pass\n    actual._pyro_backward()\n    for input_, operand in zip(inputs, operands):\n        marginal_equation = \',\'.join(inputs) + \'->\' + input_\n        expected, = ubersum(marginal_equation, *operands, plates=plates, modulo_total=True,\n                            backend=\'pyro.ops.einsum.torch_log\')\n        actual = operand._pyro_backward_result\n        assert_equal(expected, actual)\n'"
tests/ops/test_gamma_gaussian.py,29,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\nfrom torch.nn.functional import pad\n\nimport pyro.distributions as dist\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.ops.gamma_gaussian import (\n    GammaGaussian,\n    gamma_gaussian_tensordot,\n    matrix_and_mvn_to_gamma_gaussian,\n    gamma_and_mvn_to_gamma_gaussian,\n)\nfrom tests.common import assert_close\nfrom tests.ops.gamma_gaussian import assert_close_gamma_gaussian, random_gamma, random_gamma_gaussian\nfrom tests.ops.gaussian import random_mvn\n\n\n@pytest.mark.parametrize(""extra_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""log_normalizer_shape,info_vec_shape,precision_shape,alpha_shape,beta_shape"", [\n    ((), (), (), (), ()),\n    ((5,), (), (), (), ()),\n    ((), (5,), (), (), ()),\n    ((), (), (5,), (), ()),\n    ((), (), (), (5,), ()),\n    ((), (), (), (), (5,)),\n    ((3, 1, 1), (1, 4, 1), (1, 1, 5), (3, 4, 1), (1, 4, 5)),\n], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_expand(extra_shape, log_normalizer_shape, info_vec_shape, precision_shape, alpha_shape, beta_shape, dim):\n    rank = dim + dim\n    log_normalizer = torch.randn(log_normalizer_shape)\n    info_vec = torch.randn(info_vec_shape + (dim,))\n    precision = torch.randn(precision_shape + (dim, rank))\n    precision = precision.matmul(precision.transpose(-1, -2))\n    alpha = torch.randn(alpha_shape).exp()\n    beta = torch.randn(beta_shape).exp()\n    gamma_gaussian = GammaGaussian(log_normalizer, info_vec, precision, alpha, beta)\n\n    expected_shape = extra_shape + broadcast_shape(\n        log_normalizer_shape, info_vec_shape, precision_shape, alpha_shape, beta_shape)\n    actual = gamma_gaussian.expand(expected_shape)\n    assert actual.batch_shape == expected_shape\n\n\n@pytest.mark.parametrize(""old_shape,new_shape"", [\n    ((6,), (3, 2)),\n    ((5, 6), (5, 3, 2)),\n], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_reshape(old_shape, new_shape, dim):\n    gamma_gaussian = random_gamma_gaussian(old_shape, dim)\n\n    # reshape to new\n    new = gamma_gaussian.reshape(new_shape)\n    assert new.batch_shape == new_shape\n\n    # reshape back to old\n    g = new.reshape(old_shape)\n    assert_close_gamma_gaussian(g, gamma_gaussian)\n\n\n@pytest.mark.parametrize(""shape,cat_dim,split"", [\n    ((4, 7, 6), -1, (2, 1, 3)),\n    ((4, 7, 6), -2, (1, 1, 2, 3)),\n    ((4, 7, 6), 1, (1, 1, 2, 3)),\n], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_cat(shape, cat_dim, split, dim):\n    assert sum(split) == shape[cat_dim]\n    gamma_gaussian = random_gamma_gaussian(shape, dim)\n    parts = []\n    end = 0\n    for size in split:\n        beg, end = end, end + size\n        if cat_dim == -1:\n            part = gamma_gaussian[..., beg: end]\n        elif cat_dim == -2:\n            part = gamma_gaussian[..., beg: end, :]\n        elif cat_dim == 1:\n            part = gamma_gaussian[:, beg: end]\n        else:\n            raise ValueError\n        parts.append(part)\n\n    actual = GammaGaussian.cat(parts, cat_dim)\n    assert_close_gamma_gaussian(actual, gamma_gaussian)\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\n@pytest.mark.parametrize(""left"", [0, 1, 2])\n@pytest.mark.parametrize(""right"", [0, 1, 2])\ndef test_pad(shape, left, right, dim):\n    expected = random_gamma_gaussian(shape, dim)\n    padded = expected.event_pad(left=left, right=right)\n    assert padded.batch_shape == expected.batch_shape\n    assert padded.dim() == left + expected.dim() + right\n    mid = slice(left, padded.dim() - right)\n    assert_close(padded.info_vec[..., mid], expected.info_vec)\n    assert_close(padded.precision[..., mid, mid], expected.precision)\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_add(shape, dim):\n    x = random_gamma_gaussian(shape, dim)\n    y = random_gamma_gaussian(shape, dim)\n    value = torch.randn(dim)\n    s = torch.randn(()).exp()\n    assert_close((x + y).log_density(value, s), x.log_density(value, s) + y.log_density(value, s))\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""left"", [1, 2, 3])\n@pytest.mark.parametrize(""right"", [1, 2, 3])\ndef test_marginalize_shape(batch_shape, left, right):\n    dim = left + right\n    g = random_gamma_gaussian(batch_shape, dim)\n    assert g.marginalize(left=left).dim() == right\n    assert g.marginalize(right=right).dim() == left\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""left"", [1, 2, 3])\n@pytest.mark.parametrize(""right"", [1, 2, 3])\ndef test_marginalize(batch_shape, left, right):\n    dim = left + right\n    g = random_gamma_gaussian(batch_shape, dim)\n    s = torch.randn(batch_shape).exp()\n    assert_close(g.marginalize(left=left).event_logsumexp().log_density(s),\n                 g.event_logsumexp().log_density(s))\n    assert_close(g.marginalize(right=right).event_logsumexp().log_density(s),\n                 g.event_logsumexp().log_density(s))\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""left"", [1, 2, 3])\n@pytest.mark.parametrize(""right"", [1, 2, 3])\ndef test_marginalize_condition(sample_shape, batch_shape, left, right):\n    dim = left + right\n    g = random_gamma_gaussian(batch_shape, dim)\n    x = torch.randn(sample_shape + (1,) * len(batch_shape) + (right,))\n    s = torch.randn(batch_shape).exp()\n    assert_close(g.marginalize(left=left).log_density(x, s),\n                 g.condition(x).event_logsumexp().log_density(s))\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""left"", [1, 2, 3])\n@pytest.mark.parametrize(""right"", [1, 2, 3])\ndef test_condition(sample_shape, batch_shape, left, right):\n    dim = left + right\n    g = random_gamma_gaussian(batch_shape, dim)\n    g.precision += torch.eye(dim) * 0.1\n    value = torch.randn(sample_shape + (1,) * len(batch_shape) + (dim,))\n    left_value, right_value = value[..., :left], value[..., left:]\n\n    conditioned = g.condition(right_value)\n    assert conditioned.batch_shape == sample_shape + g.batch_shape\n    assert conditioned.dim() == left\n\n    s = torch.randn(batch_shape).exp()\n    actual = conditioned.log_density(left_value, s)\n    expected = g.log_density(value, s)\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_logsumexp(batch_shape, dim):\n    g = random_gamma_gaussian(batch_shape, dim)\n    g.info_vec *= 0.1  # approximately centered\n    g.precision += torch.eye(dim) * 0.1\n    s = torch.randn(batch_shape).exp() + 0.2\n\n    num_samples = 200000\n    scale = 10\n    samples = torch.rand((num_samples,) + (1,) * len(batch_shape) + (dim,)) * scale - scale / 2\n    expected = g.log_density(samples, s).logsumexp(0) + math.log(scale ** dim / num_samples)\n    actual = g.event_logsumexp().log_density(s)\n    assert_close(actual, expected, atol=0.05, rtol=0.05)\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (7,), (6, 5)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_gamma_and_mvn_to_gamma_gaussian(sample_shape, batch_shape, dim):\n    gamma = random_gamma(batch_shape)\n    mvn = random_mvn(batch_shape, dim)\n    g = gamma_and_mvn_to_gamma_gaussian(gamma, mvn)\n    value = mvn.sample(sample_shape)\n    s = gamma.sample(sample_shape)\n    actual_log_prob = g.log_density(value, s)\n\n    s_log_prob = gamma.log_prob(s)\n    scaled_prec = mvn.precision_matrix * s.unsqueeze(-1).unsqueeze(-1)\n    mvn_log_prob = dist.MultivariateNormal(mvn.loc, precision_matrix=scaled_prec).log_prob(value)\n    expected_log_prob = s_log_prob + mvn_log_prob\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (7,), (6, 5)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""x_dim"", [1, 2, 3])\n@pytest.mark.parametrize(""y_dim"", [1, 2, 3])\ndef test_matrix_and_mvn_to_gamma_gaussian(sample_shape, batch_shape, x_dim, y_dim):\n    matrix = torch.randn(batch_shape + (x_dim, y_dim))\n    y_mvn = random_mvn(batch_shape, y_dim)\n    g = matrix_and_mvn_to_gamma_gaussian(matrix, y_mvn)\n    xy = torch.randn(sample_shape + batch_shape + (x_dim + y_dim,))\n    s = torch.rand(sample_shape + batch_shape)\n    actual_log_prob = g.log_density(xy, s)\n\n    x, y = xy[..., :x_dim], xy[..., x_dim:]\n    y_pred = x.unsqueeze(-2).matmul(matrix).squeeze(-2)\n    loc = y_pred + y_mvn.loc\n    scaled_prec = y_mvn.precision_matrix * s.unsqueeze(-1).unsqueeze(-1)\n    expected_log_prob = dist.MultivariateNormal(loc, precision_matrix=scaled_prec).log_prob(y)\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(""x_batch_shape,y_batch_shape"", [\n    ((), ()),\n    ((3,), ()),\n    ((), (3,)),\n    ((2, 1), (3,)),\n    ((2, 3), (2, 3,)),\n], ids=str)\n@pytest.mark.parametrize(""x_dim,y_dim,dot_dims"", [\n    (0, 0, 0),\n    (0, 2, 0),\n    (1, 0, 0),\n    (2, 1, 0),\n    (3, 3, 3),\n    (3, 2, 1),\n    (3, 2, 2),\n    (5, 4, 2),\n], ids=str)\n@pytest.mark.parametrize(""x_rank,y_rank"", [\n    (1, 1), (4, 1), (1, 4), (4, 4)\n], ids=str)\ndef test_gamma_gaussian_tensordot(dot_dims,\n                                  x_batch_shape, x_dim, x_rank,\n                                  y_batch_shape, y_dim, y_rank):\n    x_rank = min(x_rank, x_dim)\n    y_rank = min(y_rank, y_dim)\n    x = random_gamma_gaussian(x_batch_shape, x_dim, x_rank)\n    y = random_gamma_gaussian(y_batch_shape, y_dim, y_rank)\n    na = x_dim - dot_dims\n    nb = dot_dims\n    nc = y_dim - dot_dims\n    try:\n        torch.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])\n    except RuntimeError:\n        pytest.skip(""Cannot marginalize the common variables of two Gaussians."")\n\n    z = gamma_gaussian_tensordot(x, y, dot_dims)\n    assert z.dim() == x_dim + y_dim - 2 * dot_dims\n\n    # We make these precision matrices positive definite to test the math\n    x.precision = x.precision + 3 * torch.eye(x.dim())\n    y.precision = y.precision + 3 * torch.eye(y.dim())\n    z = gamma_gaussian_tensordot(x, y, dot_dims)\n    # compare against broadcasting, adding, and marginalizing\n    precision = pad(x.precision, (0, nc, 0, nc)) + pad(y.precision, (na, 0, na, 0))\n    info_vec = pad(x.info_vec, (0, nc)) + pad(y.info_vec, (na, 0))\n    covariance = torch.inverse(precision)\n    loc = covariance.matmul(info_vec.unsqueeze(-1)).squeeze(-1) if info_vec.size(-1) > 0 else info_vec\n    z_covariance = torch.inverse(z.precision)\n    z_loc = z_covariance.matmul(z.info_vec.view(z.info_vec.shape + (int(z.dim() > 0),))).sum(-1)\n    assert_close(loc[..., :na], z_loc[..., :na])\n    assert_close(loc[..., x_dim:], z_loc[..., na:])\n    assert_close(covariance[..., :na, :na], z_covariance[..., :na, :na])\n    assert_close(covariance[..., :na, x_dim:], z_covariance[..., :na, na:])\n    assert_close(covariance[..., x_dim:, :na], z_covariance[..., na:, :na])\n    assert_close(covariance[..., x_dim:, x_dim:], z_covariance[..., na:, na:])\n\n    s = torch.randn(z.batch_shape).exp()\n    # Assume a = c = 0, integrate out b\n    num_samples = 200000\n    scale = 10\n    # generate samples in [-10, 10]\n    value_b = torch.rand((num_samples,) + z.batch_shape + (nb,)) * scale - scale / 2\n    value_x = pad(value_b, (na, 0))\n    value_y = pad(value_b, (0, nc))\n    expect = torch.logsumexp(x.log_density(value_x, s) + y.log_density(value_y, s), dim=0)\n    expect += math.log(scale ** nb / num_samples)\n    actual = z.log_density(torch.zeros(z.batch_shape + (z.dim(),)), s)\n    assert_close(actual.clamp(max=10.), expect.clamp(max=10.), atol=0.1, rtol=0.1)\n'"
tests/ops/test_gaussian.py,26,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\nfrom torch.nn.functional import pad\n\nimport pyro.distributions as dist\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.ops.gaussian import AffineNormal, Gaussian, gaussian_tensordot, matrix_and_mvn_to_gaussian, mvn_to_gaussian\nfrom tests.common import assert_close\nfrom tests.ops.gaussian import assert_close_gaussian, random_gaussian, random_mvn\n\n\n@pytest.mark.parametrize(""extra_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""log_normalizer_shape,info_vec_shape,precision_shape"", [\n    ((), (), ()),\n    ((5,), (), ()),\n    ((), (5,), ()),\n    ((), (), (5,)),\n    ((3, 1, 1), (1, 4, 1), (1, 1, 5)),\n], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_expand(extra_shape, log_normalizer_shape, info_vec_shape, precision_shape, dim):\n    rank = dim + dim\n    log_normalizer = torch.randn(log_normalizer_shape)\n    info_vec = torch.randn(info_vec_shape + (dim,))\n    precision = torch.randn(precision_shape + (dim, rank))\n    precision = precision.matmul(precision.transpose(-1, -2))\n    gaussian = Gaussian(log_normalizer, info_vec, precision)\n\n    expected_shape = extra_shape + broadcast_shape(\n        log_normalizer_shape, info_vec_shape, precision_shape)\n    actual = gaussian.expand(expected_shape)\n    assert actual.batch_shape == expected_shape\n\n\n@pytest.mark.parametrize(""old_shape,new_shape"", [\n    ((6,), (3, 2)),\n    ((5, 6), (5, 3, 2)),\n], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_reshape(old_shape, new_shape, dim):\n    gaussian = random_gaussian(old_shape, dim)\n\n    # reshape to new\n    new = gaussian.reshape(new_shape)\n    assert new.batch_shape == new_shape\n\n    # reshape back to old\n    g = new.reshape(old_shape)\n    assert_close_gaussian(g, gaussian)\n\n\n@pytest.mark.parametrize(""shape,cat_dim,split"", [\n    ((4, 7, 6), -1, (2, 1, 3)),\n    ((4, 7, 6), -2, (1, 1, 2, 3)),\n    ((4, 7, 6), 1, (1, 1, 2, 3)),\n], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_cat(shape, cat_dim, split, dim):\n    assert sum(split) == shape[cat_dim]\n    gaussian = random_gaussian(shape, dim)\n    parts = []\n    end = 0\n    for size in split:\n        beg, end = end, end + size\n        if cat_dim == -1:\n            part = gaussian[..., beg: end]\n        elif cat_dim == -2:\n            part = gaussian[..., beg: end, :]\n        elif cat_dim == 1:\n            part = gaussian[:, beg: end]\n        else:\n            raise ValueError\n        parts.append(part)\n\n    actual = Gaussian.cat(parts, cat_dim)\n    assert_close_gaussian(actual, gaussian)\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\n@pytest.mark.parametrize(""left"", [0, 1, 2])\n@pytest.mark.parametrize(""right"", [0, 1, 2])\ndef test_pad(shape, left, right, dim):\n    expected = random_gaussian(shape, dim)\n    padded = expected.event_pad(left=left, right=right)\n    assert padded.batch_shape == expected.batch_shape\n    assert padded.dim() == left + expected.dim() + right\n    mid = slice(left, padded.dim() - right)\n    assert_close(padded.info_vec[..., mid], expected.info_vec)\n    assert_close(padded.precision[..., mid, mid], expected.precision)\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_add(shape, dim):\n    x = random_gaussian(shape, dim)\n    y = random_gaussian(shape, dim)\n    value = torch.randn(dim)\n    assert_close((x + y).log_density(value), x.log_density(value) + y.log_density(value))\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_rsample_shape(sample_shape, batch_shape, dim):\n    mvn = random_mvn(batch_shape, dim)\n    g = mvn_to_gaussian(mvn)\n    expected = mvn.rsample(sample_shape)\n    actual = g.rsample(sample_shape)\n    assert actual.dtype == expected.dtype\n    assert actual.shape == expected.shape\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_rsample_distribution(batch_shape, dim):\n    num_samples = 20000\n    mvn = random_mvn(batch_shape, dim)\n    g = mvn_to_gaussian(mvn)\n    expected = mvn.rsample((num_samples,))\n    actual = g.rsample((num_samples,))\n\n    def get_moments(x):\n        mean = x.mean(0)\n        x = x - mean\n        cov = (x.unsqueeze(-1) * x.unsqueeze(-2)).mean(0)\n        std = cov.diagonal(dim1=-1, dim2=-2).sqrt()\n        corr = cov / (std.unsqueeze(-1) * std.unsqueeze(-2))\n        return mean, std, corr\n\n    expected_mean, expected_std, expected_corr = get_moments(expected)\n    actual_mean, actual_std, actual_corr = get_moments(actual)\n    assert_close(actual_mean, expected_mean, atol=0.1, rtol=0.02)\n    assert_close(actual_std, expected_std, atol=0.1, rtol=0.02)\n    assert_close(actual_corr, expected_corr, atol=0.05)\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""left"", [1, 2, 3])\n@pytest.mark.parametrize(""right"", [1, 2, 3])\ndef test_marginalize_shape(batch_shape, left, right):\n    dim = left + right\n    g = random_gaussian(batch_shape, dim)\n    assert g.marginalize(left=left).dim() == right\n    assert g.marginalize(right=right).dim() == left\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""left"", [1, 2, 3])\n@pytest.mark.parametrize(""right"", [1, 2, 3])\ndef test_marginalize(batch_shape, left, right):\n    dim = left + right\n    g = random_gaussian(batch_shape, dim)\n    assert_close(g.marginalize(left=left).event_logsumexp(),\n                 g.event_logsumexp())\n    assert_close(g.marginalize(right=right).event_logsumexp(),\n                 g.event_logsumexp())\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""left"", [1, 2, 3])\n@pytest.mark.parametrize(""right"", [1, 2, 3])\ndef test_marginalize_condition(sample_shape, batch_shape, left, right):\n    dim = left + right\n    g = random_gaussian(batch_shape, dim)\n    x = torch.randn(sample_shape + (1,) * len(batch_shape) + (right,))\n    assert_close(g.marginalize(left=left).log_density(x),\n                 g.condition(x).event_logsumexp())\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""left"", [1, 2, 3])\n@pytest.mark.parametrize(""right"", [1, 2, 3])\ndef test_condition(sample_shape, batch_shape, left, right):\n    dim = left + right\n    gaussian = random_gaussian(batch_shape, dim)\n    gaussian.precision += torch.eye(dim) * 0.1\n    value = torch.randn(sample_shape + (1,) * len(batch_shape) + (dim,))\n    left_value, right_value = value[..., :left], value[..., left:]\n\n    conditioned = gaussian.condition(right_value)\n    assert conditioned.batch_shape == sample_shape + gaussian.batch_shape\n    assert conditioned.dim() == left\n\n    actual = conditioned.log_density(left_value)\n    expected = gaussian.log_density(value)\n    assert_close(actual, expected)\n\n    # test left_condition\n    permute_conditioned = gaussian.left_condition(left_value)\n    assert permute_conditioned.batch_shape == sample_shape + gaussian.batch_shape\n    assert permute_conditioned.dim() == right\n\n    permute_actual = permute_conditioned.log_density(right_value)\n    assert_close(permute_actual, expected)\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_logsumexp(batch_shape, dim):\n    gaussian = random_gaussian(batch_shape, dim)\n    gaussian.info_vec *= 0.1  # approximately centered\n    gaussian.precision += torch.eye(dim) * 0.1\n\n    num_samples = 200000\n    scale = 10\n    samples = torch.rand((num_samples,) + (1,) * len(batch_shape) + (dim,)) * scale - scale / 2\n    expected = gaussian.log_density(samples).logsumexp(0) + math.log(scale ** dim / num_samples)\n    actual = gaussian.event_logsumexp()\n    assert_close(actual, expected, atol=0.05, rtol=0.05)\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""x_dim"", [1, 2, 3])\n@pytest.mark.parametrize(""y_dim"", [1, 2, 3])\ndef test_affine_normal(batch_shape, x_dim, y_dim):\n    matrix = torch.randn(batch_shape + (x_dim, y_dim))\n    loc = torch.randn(batch_shape + (y_dim,))\n    scale = torch.randn(batch_shape + (y_dim,)).exp()\n    y = torch.randn(batch_shape + (y_dim,))\n\n    normal = dist.Normal(loc, scale).to_event(1)\n    actual = matrix_and_mvn_to_gaussian(matrix, normal)\n    assert isinstance(actual, AffineNormal)\n    actual_like = actual.condition(y)\n    assert isinstance(actual_like, Gaussian)\n\n    mvn = dist.MultivariateNormal(loc, scale_tril=scale.diag_embed())\n    expected = matrix_and_mvn_to_gaussian(matrix, mvn)\n    assert isinstance(expected, Gaussian)\n    expected_like = expected.condition(y)\n    assert isinstance(expected_like, Gaussian)\n\n    assert_close(actual_like.log_normalizer, expected_like.log_normalizer)\n    assert_close(actual_like.info_vec, expected_like.info_vec)\n    assert_close(actual_like.precision, expected_like.precision)\n\n    x = torch.randn(batch_shape + (x_dim,))\n    permute_actual = actual.left_condition(x)\n    assert isinstance(permute_actual, AffineNormal)\n    permute_actual = permute_actual.to_gaussian()\n\n    permute_expected = expected.left_condition(y)\n    assert isinstance(permute_expected, Gaussian)\n\n    assert_close(permute_actual.log_normalizer, permute_actual.log_normalizer)\n    assert_close(permute_actual.info_vec, permute_actual.info_vec)\n    assert_close(permute_actual.precision, permute_actual.precision)\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (7,), (6, 5)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_mvn_to_gaussian(sample_shape, batch_shape, dim):\n    mvn = random_mvn(batch_shape, dim)\n    gaussian = mvn_to_gaussian(mvn)\n    value = mvn.sample(sample_shape)\n    actual_log_prob = gaussian.log_density(value)\n    expected_log_prob = mvn.log_prob(value)\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (7,), (6, 5)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""x_dim"", [1, 2, 3])\n@pytest.mark.parametrize(""y_dim"", [1, 2, 3])\ndef test_matrix_and_mvn_to_gaussian(sample_shape, batch_shape, x_dim, y_dim):\n    matrix = torch.randn(batch_shape + (x_dim, y_dim))\n    y_mvn = random_mvn(batch_shape, y_dim)\n    xy_mvn = random_mvn(batch_shape, x_dim + y_dim)\n    gaussian = matrix_and_mvn_to_gaussian(matrix, y_mvn) + mvn_to_gaussian(xy_mvn)\n    xy = torch.randn(sample_shape + (1,) * len(batch_shape) + (x_dim + y_dim,))\n    x, y = xy[..., :x_dim], xy[..., x_dim:]\n    y_pred = x.unsqueeze(-2).matmul(matrix).squeeze(-2)\n    actual_log_prob = gaussian.log_density(xy)\n    expected_log_prob = xy_mvn.log_prob(xy) + y_mvn.log_prob(y - y_pred)\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(""sample_shape"", [(), (7,), (6, 5)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""x_dim"", [1, 2, 3])\n@pytest.mark.parametrize(""y_dim"", [1, 2, 3])\ndef test_matrix_and_mvn_to_gaussian_2(sample_shape, batch_shape, x_dim, y_dim):\n    matrix = torch.randn(batch_shape + (x_dim, y_dim))\n    y_mvn = random_mvn(batch_shape, y_dim)\n    x_mvn = random_mvn(batch_shape, x_dim)\n    Mx_cov = matrix.transpose(-2, -1).matmul(x_mvn.covariance_matrix).matmul(matrix)\n    Mx_loc = matrix.transpose(-2, -1).matmul(x_mvn.loc.unsqueeze(-1)).squeeze(-1)\n    mvn = dist.MultivariateNormal(Mx_loc + y_mvn.loc, Mx_cov + y_mvn.covariance_matrix)\n    expected = mvn_to_gaussian(mvn)\n\n    actual = gaussian_tensordot(mvn_to_gaussian(x_mvn),\n                                matrix_and_mvn_to_gaussian(matrix, y_mvn), dims=x_dim)\n    assert_close_gaussian(expected, actual)\n\n\n@pytest.mark.parametrize(""x_batch_shape,y_batch_shape"", [\n    ((), ()),\n    ((3,), ()),\n    ((), (3,)),\n    ((2, 1), (3,)),\n    ((2, 3), (2, 3,)),\n], ids=str)\n@pytest.mark.parametrize(""x_dim,y_dim,dot_dims"", [\n    (0, 0, 0),\n    (0, 2, 0),\n    (1, 0, 0),\n    (2, 1, 0),\n    (3, 3, 3),\n    (3, 2, 1),\n    (3, 2, 2),\n    (5, 4, 2),\n], ids=str)\n@pytest.mark.parametrize(""x_rank,y_rank"", [\n    (1, 1), (4, 1), (1, 4), (4, 4)\n], ids=str)\ndef test_gaussian_tensordot(dot_dims,\n                            x_batch_shape, x_dim, x_rank,\n                            y_batch_shape, y_dim, y_rank):\n    x_rank = min(x_rank, x_dim)\n    y_rank = min(y_rank, y_dim)\n    x = random_gaussian(x_batch_shape, x_dim, x_rank)\n    y = random_gaussian(y_batch_shape, y_dim, y_rank)\n    na = x_dim - dot_dims\n    nb = dot_dims\n    nc = y_dim - dot_dims\n    try:\n        torch.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])\n    except RuntimeError:\n        pytest.skip(""Cannot marginalize the common variables of two Gaussians."")\n\n    z = gaussian_tensordot(x, y, dot_dims)\n    assert z.dim() == x_dim + y_dim - 2 * dot_dims\n\n    # We make these precision matrices positive definite to test the math\n    x.precision = x.precision + 1e-1 * torch.eye(x.dim())\n    y.precision = y.precision + 1e-1 * torch.eye(y.dim())\n    z = gaussian_tensordot(x, y, dot_dims)\n    # compare against broadcasting, adding, and marginalizing\n    precision = pad(x.precision, (0, nc, 0, nc)) + pad(y.precision, (na, 0, na, 0))\n    info_vec = pad(x.info_vec, (0, nc)) + pad(y.info_vec, (na, 0))\n    covariance = torch.inverse(precision)\n    loc = covariance.matmul(info_vec.unsqueeze(-1)).squeeze(-1) if info_vec.size(-1) > 0 else info_vec\n    z_covariance = torch.inverse(z.precision)\n    z_loc = z_covariance.matmul(z.info_vec.view(z.info_vec.shape + (int(z.dim() > 0),))).sum(-1)\n    assert_close(loc[..., :na], z_loc[..., :na])\n    assert_close(loc[..., x_dim:], z_loc[..., na:])\n    assert_close(covariance[..., :na, :na], z_covariance[..., :na, :na])\n    assert_close(covariance[..., :na, x_dim:], z_covariance[..., :na, na:])\n    assert_close(covariance[..., x_dim:, :na], z_covariance[..., na:, :na])\n    assert_close(covariance[..., x_dim:, x_dim:], z_covariance[..., na:, na:])\n\n    # Assume a = c = 0, integrate out b\n    # FIXME: this might be not a stable way to compute integral\n    num_samples = 200000\n    scale = 20\n    # generate samples in [-10, 10]\n    value_b = torch.rand((num_samples,) + z.batch_shape + (nb,)) * scale - scale / 2\n    value_x = pad(value_b, (na, 0))\n    value_y = pad(value_b, (0, nc))\n    expect = torch.logsumexp(x.log_density(value_x) + y.log_density(value_y), dim=0)\n    expect += math.log(scale ** nb / num_samples)\n    actual = z.log_density(torch.zeros(z.batch_shape + (z.dim(),)))\n    # TODO(fehiepsi): find some condition to make this test stable, so we can compare large value\n    # log densities.\n    assert_close(actual.clamp(max=10.), expect.clamp(max=10.), atol=0.1, rtol=0.1)\n'"
tests/ops/test_indexing.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.ops.indexing import Index, Vindex\nfrom tests.common import assert_equal\n\n\nclass TensorMock:\n    def __getitem__(self, args):\n        return args\n\n\ntensor_mock = TensorMock()\n\n\ndef z(*args):\n    return torch.zeros(*args, dtype=torch.long)\n\n\nSHAPE_EXAMPLES = [\n    (\'Vindex(z(()))[...]\', ()),\n    (\'Vindex(z(2))[...]\', (2,)),\n    (\'Vindex(z(2))[...,0]\', ()),\n    (\'Vindex(z(2))[...,:]\', (2,)),\n    (\'Vindex(z(2))[...,z(3)]\', (3,)),\n    (\'Vindex(z(2))[0]\', ()),\n    (\'Vindex(z(2))[:]\', (2,)),\n    (\'Vindex(z(2))[z(3)]\', (3,)),\n    (\'Vindex(z(2,3))[...]\', (2, 3)),\n    (\'Vindex(z(2,3))[...,0]\', (2,)),\n    (\'Vindex(z(2,3))[...,:]\', (2, 3)),\n    (\'Vindex(z(2,3))[...,z(2)]\', (2,)),\n    (\'Vindex(z(2,3))[...,z(4,1)]\', (4, 2)),\n    (\'Vindex(z(2,3))[...,0,0]\', ()),\n    (\'Vindex(z(2,3))[...,0,:]\', (3,)),\n    (\'Vindex(z(2,3))[...,0,z(4)]\', (4,)),\n    (\'Vindex(z(2,3))[...,:,0]\', (2,)),\n    (\'Vindex(z(2,3))[...,:,:]\', (2, 3)),\n    (\'Vindex(z(2,3))[...,:,z(4)]\', (4, 2)),\n    (\'Vindex(z(2,3))[...,z(4),0]\', (4,)),\n    (\'Vindex(z(2,3))[...,z(4),:]\', (4, 3)),\n    (\'Vindex(z(2,3))[...,z(4),z(4)]\', (4,)),\n    (\'Vindex(z(2,3))[...,z(5,1),z(4)]\', (5, 4)),\n    (\'Vindex(z(2,3))[...,z(4),z(5,1)]\', (5, 4)),\n    (\'Vindex(z(2,3))[0,0]\', ()),\n    (\'Vindex(z(2,3))[0,:]\', (3,)),\n    (\'Vindex(z(2,3))[0,z(4)]\', (4,)),\n    (\'Vindex(z(2,3))[:,0]\', (2,)),\n    (\'Vindex(z(2,3))[:,:]\', (2, 3)),\n    (\'Vindex(z(2,3))[:,z(4)]\', (4, 2)),\n    (\'Vindex(z(2,3))[z(4),0]\', (4,)),\n    (\'Vindex(z(2,3))[z(4),:]\', (4, 3)),\n    (\'Vindex(z(2,3))[z(4)]\', (4, 3)),\n    (\'Vindex(z(2,3))[z(4),z(4)]\', (4,)),\n    (\'Vindex(z(2,3))[z(5,1),z(4)]\', (5, 4)),\n    (\'Vindex(z(2,3))[z(4),z(5,1)]\', (5, 4)),\n    (\'Vindex(z(2,3,4))[...]\', (2, 3, 4)),\n    (\'Vindex(z(2,3,4))[...,z(3)]\', (2, 3)),\n    (\'Vindex(z(2,3,4))[...,z(2,1)]\', (2, 3)),\n    (\'Vindex(z(2,3,4))[...,z(2,3)]\', (2, 3)),\n    (\'Vindex(z(2,3,4))[...,z(5,1,1)]\', (5, 2, 3)),\n    (\'Vindex(z(2,3,4))[...,z(2),0]\', (2,)),\n    (\'Vindex(z(2,3,4))[...,z(5,1),0]\', (5, 2)),\n    (\'Vindex(z(2,3,4))[...,z(2),:]\', (2, 4)),\n    (\'Vindex(z(2,3,4))[...,z(5,1),:]\', (5, 2, 4)),\n    (\'Vindex(z(2,3,4))[...,z(5),0,0]\', (5,)),\n    (\'Vindex(z(2,3,4))[...,z(5),0,:]\', (5, 4)),\n    (\'Vindex(z(2,3,4))[...,z(5),:,0]\', (5, 3)),\n    (\'Vindex(z(2,3,4))[...,z(5),:,:]\', (5, 3, 4)),\n    (\'Vindex(z(2,3,4))[0,0,z(5)]\', (5,)),\n    (\'Vindex(z(2,3,4))[0,:,z(5)]\', (5, 3)),\n    (\'Vindex(z(2,3,4))[0,z(5),0]\', (5,)),\n    (\'Vindex(z(2,3,4))[0,z(5),:]\', (5, 4)),\n    (\'Vindex(z(2,3,4))[0,z(5),z(5)]\', (5,)),\n    (\'Vindex(z(2,3,4))[0,z(5,1),z(6)]\', (5, 6)),\n    (\'Vindex(z(2,3,4))[0,z(6),z(5,1)]\', (5, 6)),\n    (\'Vindex(z(2,3,4))[:,0,z(5)]\', (5, 2)),\n    (\'Vindex(z(2,3,4))[:,:,z(5)]\', (5, 2, 3)),\n    (\'Vindex(z(2,3,4))[:,z(5),0]\', (5, 2)),\n    (\'Vindex(z(2,3,4))[:,z(5),:]\', (5, 2, 4)),\n    (\'Vindex(z(2,3,4))[:,z(5),z(5)]\', (5, 2)),\n    (\'Vindex(z(2,3,4))[:,z(5,1),z(6)]\', (5, 6, 2)),\n    (\'Vindex(z(2,3,4))[:,z(6),z(5,1)]\', (5, 6, 2)),\n    (\'Vindex(z(2,3,4))[z(5),0,0]\', (5,)),\n    (\'Vindex(z(2,3,4))[z(5),0,:]\', (5, 4)),\n    (\'Vindex(z(2,3,4))[z(5),:,0]\', (5, 3)),\n    (\'Vindex(z(2,3,4))[z(5),:,:]\', (5, 3, 4)),\n    (\'Vindex(z(2,3,4))[z(5),0,z(5)]\', (5,)),\n    (\'Vindex(z(2,3,4))[z(5,1),0,z(6)]\', (5, 6)),\n    (\'Vindex(z(2,3,4))[z(6),0,z(5,1)]\', (5, 6)),\n    (\'Vindex(z(2,3,4))[z(5),:,z(5)]\', (5, 3)),\n    (\'Vindex(z(2,3,4))[z(5,1),:,z(6)]\', (5, 6, 3)),\n    (\'Vindex(z(2,3,4))[z(6),:,z(5,1)]\', (5, 6, 3)),\n]\n\n\n@pytest.mark.parametrize(\'expression,expected_shape\', SHAPE_EXAMPLES, ids=str)\ndef test_shape(expression, expected_shape):\n    result = eval(expression)\n    assert result.shape == expected_shape\n\n\n@pytest.mark.parametrize(\'event_shape\', [(), (7,)], ids=str)\n@pytest.mark.parametrize(\'j_shape\', [(), (2,), (3, 1), (4, 1, 1), (4, 3, 2)], ids=str)\n@pytest.mark.parametrize(\'i_shape\', [(), (2,), (3, 1), (4, 1, 1), (4, 3, 2)], ids=str)\n@pytest.mark.parametrize(\'x_shape\', [(), (2,), (3, 1), (4, 1, 1), (4, 3, 2)], ids=str)\ndef test_value(x_shape, i_shape, j_shape, event_shape):\n    x = torch.rand(x_shape + (5, 6) + event_shape)\n    i = dist.Categorical(torch.ones(5)).sample(i_shape)\n    j = dist.Categorical(torch.ones(6)).sample(j_shape)\n    if event_shape:\n        actual = Vindex(x)[..., i, j, :]\n    else:\n        actual = Vindex(x)[..., i, j]\n\n    shape = broadcast_shape(x_shape, i_shape, j_shape)\n    x = x.expand(shape + (5, 6) + event_shape)\n    i = i.expand(shape)\n    j = j.expand(shape)\n    expected = x.new_empty(shape + event_shape)\n    for ind in (itertools.product(*map(range, shape)) if shape else [()]):\n        expected[ind] = x[ind + (i[ind].item(), j[ind].item())]\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(\'prev_enum_dim,curr_enum_dim\', [(-3, -4), (-4, -5), (-5, -3)])\ndef test_hmm_example(prev_enum_dim, curr_enum_dim):\n    hidden_dim = 8\n    probs_x = torch.rand(hidden_dim, hidden_dim, hidden_dim)\n    x_prev = torch.arange(hidden_dim).reshape((-1,) + (1,) * (-1 - prev_enum_dim))\n    x_curr = torch.arange(hidden_dim).reshape((-1,) + (1,) * (-1 - curr_enum_dim))\n\n    expected = probs_x[x_prev.unsqueeze(-1), x_curr.unsqueeze(-1), torch.arange(hidden_dim)]\n    actual = Vindex(probs_x)[x_prev, x_curr, :]\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(""args,expected"", [\n    (0, 0),\n    (1, 1),\n    (None, None),\n    (slice(1, 2, 3), slice(1, 2, 3)),\n    (Ellipsis, Ellipsis),\n    ((0, 1, None, slice(1, 2, 3), Ellipsis), (0, 1, None, slice(1, 2, 3), Ellipsis)),\n    (((0, 1), (None, slice(1, 2, 3)), Ellipsis), (0, 1, None, slice(1, 2, 3), Ellipsis)),\n    ((Ellipsis, None), (Ellipsis, None)),\n    ((Ellipsis, (Ellipsis, None)), (Ellipsis, None)),\n    ((Ellipsis, (Ellipsis, None, None)), (Ellipsis, None, None)),\n])\ndef test_index(args, expected):\n    assert Index(tensor_mock)[args] == expected\n'"
tests/ops/test_integrator.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nfrom collections import namedtuple\n\nimport pytest\nimport torch\n\nfrom pyro.ops.integrator import velocity_verlet\nfrom tests.common import assert_equal\n\nlogger = logging.getLogger(__name__)\n\n\nTEST_EXAMPLES = []\nEXAMPLE_IDS = []\n\nModelArgs = namedtuple(\'model_args\', [\'step_size\', \'num_steps\', \'q_i\', \'p_i\', \'q_f\', \'p_f\', \'prec\'])\nExample = namedtuple(\'test_case\', [\'model\', \'args\'])\n\n\ndef register_model(init_args):\n    """"""\n    Register the model along with each of the model arguments\n    as test examples.\n    """"""\n    def register_fn(model):\n        for args in init_args:\n            test_example = Example(model, args)\n            TEST_EXAMPLES.append(test_example)\n            EXAMPLE_IDS.append(model.__name__)\n    return register_fn\n\n\n@register_model([\n    ModelArgs(\n        step_size=0.01,\n        num_steps=100,\n        q_i={\'x\': torch.tensor([0.0])},\n        p_i={\'x\': torch.tensor([1.0])},\n        q_f={\'x\': torch.sin(torch.tensor([1.0]))},\n        p_f={\'x\': torch.cos(torch.tensor([1.0]))},\n        prec=1e-4\n    )\n])\nclass HarmonicOscillator:\n    @staticmethod\n    def kinetic_grad(p):\n        return p\n\n    @staticmethod\n    def energy(q, p):\n        return 0.5 * p[\'x\'] ** 2 + 0.5 * q[\'x\'] ** 2\n\n    @staticmethod\n    def potential_fn(q):\n        return 0.5 * q[\'x\'] ** 2\n\n\n@register_model([\n    ModelArgs(\n        step_size=0.01,\n        num_steps=628,\n        q_i={\'x\': torch.tensor([1.0]), \'y\': torch.tensor([0.0])},\n        p_i={\'x\': torch.tensor([0.0]), \'y\': torch.tensor([1.0])},\n        q_f={\'x\': torch.tensor([1.0]), \'y\': torch.tensor([0.0])},\n        p_f={\'x\': torch.tensor([0.0]), \'y\': torch.tensor([1.0])},\n        prec=5.0e-3\n    )\n])\nclass CircularPlanetaryMotion:\n    @staticmethod\n    def kinetic_grad(p):\n        return p\n\n    @staticmethod\n    def energy(q, p):\n        return 0.5 * p[\'x\'] ** 2 + 0.5 * p[\'y\'] ** 2 - \\\n               1.0 / torch.pow(q[\'x\'] ** 2 + q[\'y\'] ** 2, 0.5)\n\n    @staticmethod\n    def potential_fn(q):\n        return - 1.0 / torch.pow(q[\'x\'] ** 2 + q[\'y\'] ** 2, 0.5)\n\n\n@register_model([\n    ModelArgs(\n        step_size=0.1,\n        num_steps=1810,\n        q_i={\'x\': torch.tensor([0.02])},\n        p_i={\'x\': torch.tensor([0.0])},\n        q_f={\'x\': torch.tensor([-0.02])},\n        p_f={\'x\': torch.tensor([0.0])},\n        prec=1.0e-4\n    )\n])\nclass QuarticOscillator:\n    @staticmethod\n    def kinetic_grad(p):\n        return p\n\n    @staticmethod\n    def energy(q, p):\n        return 0.5 * p[\'x\'] ** 2 + 0.25 * torch.pow(q[\'x\'], 4.0)\n\n    @staticmethod\n    def potential_fn(q):\n        return 0.25 * torch.pow(q[\'x\'], 4.0)\n\n\n@pytest.mark.parametrize(\'example\', TEST_EXAMPLES, ids=EXAMPLE_IDS)\ndef test_trajectory(example):\n    model, args = example\n    q_f, p_f, _, _ = velocity_verlet(args.q_i,\n                                     args.p_i,\n                                     model.potential_fn,\n                                     model.kinetic_grad,\n                                     args.step_size,\n                                     args.num_steps)\n    logger.info(""initial q: {}"".format(args.q_i))\n    logger.info(""final q: {}"".format(q_f))\n    assert_equal(q_f, args.q_f, args.prec)\n    assert_equal(p_f, args.p_f, args.prec)\n\n\n@pytest.mark.parametrize(\'example\', TEST_EXAMPLES, ids=EXAMPLE_IDS)\ndef test_energy_conservation(example):\n    model, args = example\n    q_f, p_f, _, _ = velocity_verlet(args.q_i,\n                                     args.p_i,\n                                     model.potential_fn,\n                                     model.kinetic_grad,\n                                     args.step_size,\n                                     args.num_steps)\n    energy_initial = model.energy(args.q_i, args.p_i)\n    energy_final = model.energy(q_f, p_f)\n    logger.info(""initial energy: {}"".format(energy_initial.item()))\n    logger.info(""final energy: {}"".format(energy_final.item()))\n    assert_equal(energy_final, energy_initial)\n\n\n@pytest.mark.parametrize(\'example\', TEST_EXAMPLES, ids=EXAMPLE_IDS)\ndef test_time_reversibility(example):\n    model, args = example\n    q_forward, p_forward, _, _ = velocity_verlet(args.q_i,\n                                                 args.p_i,\n                                                 model.potential_fn,\n                                                 model.kinetic_grad,\n                                                 args.step_size,\n                                                 args.num_steps)\n    p_reverse = {key: -val for key, val in p_forward.items()}\n    q_f, p_f, _, _ = velocity_verlet(q_forward,\n                                     p_reverse,\n                                     model.potential_fn,\n                                     model.kinetic_grad,\n                                     args.step_size,\n                                     args.num_steps)\n    assert_equal(q_f, args.q_i, 1e-5)\n'"
tests/ops/test_jit.py,5,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro.ops.jit\nfrom tests.common import assert_equal\n\n\ndef test_varying_len_args():\n\n    def fn(*args):\n        return sum(args)\n\n    jit_fn = pyro.ops.jit.trace(fn)\n    examples = [\n        [torch.tensor(1.)],\n        [torch.tensor(2.), torch.tensor(3.)],\n        [torch.tensor(4.), torch.tensor(5.), torch.tensor(6.)],\n    ]\n    for args in examples:\n        assert_equal(jit_fn(*args), fn(*args))\n\n\ndef test_varying_kwargs():\n\n    def fn(x, scale=1.):\n        return x * scale\n\n    jit_fn = pyro.ops.jit.trace(fn)\n    x = torch.tensor(1.)\n    for scale in [-1., 0., 1., 10.]:\n        assert_equal(jit_fn(x, scale=scale), fn(x, scale=scale))\n\n\ndef test_varying_unhashable_kwargs():\n\n    def fn(x, config={}):\n        return x * config.get(scale, 1.)\n\n    jit_fn = pyro.ops.jit.trace(fn)\n    x = torch.tensor(1.)\n    for scale in [-1., 0., 1., 10.]:\n        config = {'scale': scale}\n        assert_equal(jit_fn(x, config=config), fn(x, config=config))\n"""
tests/ops/test_linalg.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.ops.linalg import rinverse\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize(""A"", [\n    torch.tensor([[17.]]),\n    torch.tensor([[1., 2.], [2., -3.]]),\n    torch.tensor([[1., 2, 0], [2, -2, 4], [0, 4, 5]]),\n    torch.tensor([[1., 2, 0, 7], [2, -2, 4, -1], [0, 4, 5, 8], [7, -1, 8, 1]]),\n    torch.tensor([[1., 2, 0, 7, 0], [2, -2, 4, -1, 2], [0, 4, 5, 8, -4], [7, -1, 8, 1, -3], [0, 2, -4, -3, -1]]),\n    torch.eye(40)\n    ])\n@pytest.mark.parametrize(""use_sym"", [True, False])\ndef test_sym_rinverse(A, use_sym):\n    d = A.shape[-1]\n    assert_equal(rinverse(A, sym=use_sym), torch.inverse(A), prec=1e-8)\n    assert_equal(torch.mm(A, rinverse(A, sym=use_sym)), torch.eye(d), prec=1e-8)\n    batched_A = A.unsqueeze(0).unsqueeze(0).expand(5, 4, d, d)\n    expected_A = torch.inverse(A).unsqueeze(0).unsqueeze(0).expand(5, 4, d, d)\n    assert_equal(rinverse(batched_A, sym=use_sym), expected_A, prec=1e-8)\n'"
tests/ops/test_newton.py,13,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport logging\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nfrom pyro.ops.newton import newton_step\nfrom tests.common import assert_equal\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef random_inside_unit_circle(shape, requires_grad=False):\n    x = torch.randn(shape)\n    x = x / (1 + x.pow(2).sum(-1, True))\n    assert (x.pow(2).sum(-1) < 1).all()\n    x = x.detach()\n    if requires_grad:\n        x.requires_grad = requires_grad\n    return x\n\n\n@pytest.mark.parametrize('batch_shape', [(), (1,), (2,), (10,), (3, 2), (2, 3)])\n@pytest.mark.parametrize('trust_radius', [None, 2.0, 100.0])\n@pytest.mark.parametrize('dims', [1, 2, 3])\ndef test_newton_step(batch_shape, trust_radius, dims):\n    batch_shape = torch.Size(batch_shape)\n    mode = 0.5 * random_inside_unit_circle(batch_shape + (dims,), requires_grad=True)\n    x = 0.5 * random_inside_unit_circle(batch_shape + (dims,), requires_grad=True)\n    if trust_radius is not None:\n        assert trust_radius >= 2, '(x, mode) may be farther apart than trust_radius'\n\n    # create a quadratic loss function\n    flat_x = x.reshape(-1, dims)\n    flat_mode = mode.reshape(-1, dims)\n    noise = torch.randn(flat_x.shape[0], dims, 1)\n    flat_hessian = noise.matmul(noise.transpose(-1, -2)) + torch.eye(dims)\n    hessian = flat_hessian.reshape(batch_shape + (dims, dims))\n    diff = (flat_x - flat_mode).unsqueeze(-2)\n    loss = 0.5 * diff.bmm(flat_hessian).bmm(diff.transpose(-1, -2)).sum()\n\n    # run method under test\n    x_updated, cov = newton_step(loss, x, trust_radius=trust_radius)\n\n    # check shapes\n    assert x_updated.shape == x.shape\n    assert cov.shape == hessian.shape\n\n    # check values\n    assert_equal(x_updated, mode, prec=1e-6,\n                 msg='{} vs {}'.format(x_updated, mode))\n    flat_cov = cov.reshape(flat_hessian.shape)\n    assert_equal(flat_cov, flat_cov.transpose(-1, -2),\n                 msg='covariance is not symmetric: {}'.format(flat_cov))\n    actual_eye = torch.bmm(flat_cov, flat_hessian)\n    expected_eye = torch.eye(dims).expand(actual_eye.shape)\n    assert_equal(actual_eye, expected_eye, prec=1e-4,\n                 msg='bad covariance {}'.format(actual_eye))\n\n    # check gradients\n    for i in itertools.product(*map(range, mode.shape)):\n        expected_grad = torch.zeros(mode.shape)\n        expected_grad[i] = 1\n        actual_grad = grad(x_updated[i], [mode], create_graph=True)[0]\n        assert_equal(actual_grad, expected_grad, prec=1e-5, msg='\\n'.join([\n            'bad gradient at index {}'.format(i),\n            'expected {}'.format(expected_grad),\n            'actual   {}'.format(actual_grad),\n        ]))\n\n\n@pytest.mark.parametrize('trust_radius', [None, 0.1, 1.0, 10.0])\n@pytest.mark.parametrize('dims', [1, 2, 3])\ndef test_newton_step_trust(trust_radius, dims):\n    batch_size = 100\n    batch_shape = torch.Size((batch_size,))\n    mode = random_inside_unit_circle(batch_shape + (dims,), requires_grad=True) + 1\n    x = random_inside_unit_circle(batch_shape + (dims,), requires_grad=True) - 1\n\n    # create a quadratic loss function\n    noise = torch.randn(batch_size, dims, dims)\n    hessian = noise + noise.transpose(-1, -2)\n    diff = (x - mode).unsqueeze(-2)\n    loss = 0.5 * diff.bmm(hessian).bmm(diff.transpose(-1, -2)).sum()\n\n    # run method under test\n    x_updated, cov = newton_step(loss, x, trust_radius=trust_radius)\n\n    # check shapes\n    assert x_updated.shape == x.shape\n    assert cov.shape == hessian.shape\n\n    # check values\n    if trust_radius is None:\n        assert ((x - x_updated).pow(2).sum(-1) > 1.0).any(), 'test is too weak'\n    else:\n        assert ((x - x_updated).pow(2).sum(-1) <= 1e-8 + trust_radius**2).all(), 'trust region violated'\n\n\n@pytest.mark.parametrize('trust_radius', [None, 0.1, 1.0, 10.0])\n@pytest.mark.parametrize('dims', [1, 2, 3])\ndef test_newton_step_converges(trust_radius, dims):\n    batch_size = 100\n    batch_shape = torch.Size((batch_size,))\n    mode = random_inside_unit_circle(batch_shape + (dims,), requires_grad=True) - 1\n    x = random_inside_unit_circle(batch_shape + (dims,), requires_grad=True) + 1\n\n    # create a quadratic loss function\n    noise = torch.randn(batch_size, dims, 1)\n    hessian = noise.matmul(noise.transpose(-1, -2)) + 0.01 * torch.eye(dims)\n\n    def loss_fn(x):\n        diff = (x - mode).unsqueeze(-2)\n        return 0.5 * diff.bmm(hessian).bmm(diff.transpose(-1, -2)).sum()\n\n    # check convergence\n    for i in range(100):\n        x = x.detach()\n        x.requires_grad = True\n        loss = loss_fn(x)\n        x, cov = newton_step(loss, x, trust_radius=trust_radius)\n        if ((x - mode).pow(2).sum(-1) < 1e-4).all():\n            logger.debug('Newton iteration converged after {} steps'.format(2 + i))\n            return\n    pytest.fail('Newton iteration did not converge')\n"""
tests/ops/test_packed.py,3,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport random\n\nimport pytest\nimport torch\nfrom torch.distributions.utils import broadcast_all\n\nfrom pyro.ops import packed\nfrom tests.common import assert_equal\n\nEXAMPLE_DIMS = [\n    ''.join(dims)\n    for num_dims in range(5)\n    for dims in itertools.permutations('abcd'[:num_dims])\n]\n\n\n@pytest.mark.parametrize('dims', EXAMPLE_DIMS)\ndef test_unpack_pack(dims):\n    dim_to_symbol = {}\n    symbol_to_dim = {}\n    for symbol, dim in zip('abcd', range(-1, -5, -1)):\n        dim_to_symbol[dim] = symbol\n        symbol_to_dim[symbol] = dim\n    shape = tuple(range(2, 2 + len(dims)))\n    x = torch.randn(shape)\n\n    pack_x = packed.pack(x, dim_to_symbol)\n    unpack_pack_x = packed.unpack(pack_x, symbol_to_dim)\n    assert_equal(unpack_pack_x, x)\n\n    sort_dims = ''.join(sorted(dims))\n    if sort_dims != pack_x._pyro_dims:\n        sort_pack_x = pack_x.permute(*(pack_x._pyro_dims.index(d) for d in sort_dims))\n        sort_pack_x._pyro_dims = sort_dims\n        unpack_sort_pack_x = packed.unpack(sort_pack_x, symbol_to_dim)\n        assert_equal(unpack_sort_pack_x, x)\n\n\nEXAMPLE_SHAPES = [\n    [],\n    [()],\n    [(), ()],\n    [(2,), (3, 1)],\n    [(2,), (3, 1), (3, 2)],\n]\n\n\ndef make_inputs(shapes, num_numbers=0):\n    inputs = [torch.randn(shape) for shape in shapes]\n    num_symbols = max(map(len, shapes)) if shapes else 0\n    for _ in range(num_numbers):\n        inputs.append(random.random())\n    dim_to_symbol = {}\n    symbol_to_dim = {}\n    for dim, symbol in zip(range(-num_symbols, 0), 'abcdefghijklmnopqrstuvwxyz'):\n        dim_to_symbol[dim] = symbol\n        symbol_to_dim[symbol] = dim\n    return inputs, dim_to_symbol, symbol_to_dim\n\n\n@pytest.mark.parametrize('shapes', EXAMPLE_SHAPES)\ndef test_broadcast_all(shapes):\n    inputs, dim_to_symbol, symbol_to_dim = make_inputs(shapes)\n    packed_inputs = [packed.pack(x, dim_to_symbol) for x in inputs]\n    packed_outputs = packed.broadcast_all(*packed_inputs)\n    actual = tuple(packed.unpack(x, symbol_to_dim) for x in packed_outputs)\n    expected = broadcast_all(*inputs) if inputs else []\n    assert len(actual) == len(expected)\n    for a, e in zip(actual, expected):\n        assert_equal(a, e)\n"""
tests/ops/test_special.py,7,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nfrom pyro.ops.special import log_beta, log_binomial, safe_log\nfrom tests.common import assert_equal\n\n\ndef test_safe_log():\n    # Test values.\n    x = torch.randn(1000).exp().requires_grad_()\n    expected = x.log()\n    actual = safe_log(x)\n    assert_equal(actual, expected)\n    assert_equal(grad(actual.sum(), [x])[0], grad(expected.sum(), [x])[0])\n\n    # Test gradients.\n    x = torch.tensor(0., requires_grad=True)\n    assert not torch.isfinite(grad(x.log(), [x])[0])\n    assert torch.isfinite(grad(safe_log(x), [x])[0])\n\n\n@pytest.mark.parametrize(""tol"", [\n    1e-8, 1e-6, 1e-4, 1e-2, 0.02, 0.05, 0.1, 0.2, 0.1, 1.,\n])\ndef test_log_beta_stirling(tol):\n    x = torch.logspace(-5, 5, 200)\n    y = x.unsqueeze(-1)\n\n    expected = log_beta(x, y)\n    actual = log_beta(x, y, tol=tol)\n\n    assert (actual <= expected).all()\n    assert (expected < actual + tol).all()\n\n\n@pytest.mark.parametrize(""tol"", [\n    1e-8, 1e-6, 1e-4, 1e-2, 0.02, 0.05, 0.1, 0.2, 0.1, 1.,\n])\ndef test_log_binomial_stirling(tol):\n    k = torch.arange(200.)\n    n_minus_k = k.unsqueeze(-1)\n    n = k + n_minus_k\n\n    # Test binomial coefficient choose(n, k).\n    expected = (n + 1).lgamma() - (k + 1).lgamma() - (n_minus_k + 1).lgamma()\n    actual = log_binomial(n, k, tol=tol)\n\n    assert (actual - expected).abs().max() < tol\n'"
tests/ops/test_ssm_gp.py,5,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.ops.ssm_gp import MaternKernel\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize('num_gps', [1, 2, 3])\n@pytest.mark.parametrize('nu', [0.5, 1.5, 2.5])\ndef test_matern_kernel(num_gps, nu):\n    mk = MaternKernel(nu=nu, num_gps=num_gps, length_scale_init=0.1 + torch.rand(num_gps))\n\n    dt = torch.rand(1).item()\n    forward = mk.transition_matrix(dt)\n    backward = mk.transition_matrix(-dt)\n    forward_backward = torch.matmul(forward, backward)\n\n    # going forward dt in time and then backward dt in time should bring us back to the identity\n    eye = torch.eye(mk.state_dim).unsqueeze(0).expand(num_gps, mk.state_dim, mk.state_dim)\n    assert_equal(forward_backward, eye)\n\n    # let's just check that these are PSD\n    mk.stationary_covariance().cholesky()\n    mk.process_covariance(forward).cholesky()\n\n    # evolving forward infinitesimally should yield the identity\n    nudge = mk.transition_matrix(torch.tensor([1.0e-9]))\n    assert_equal(nudge, eye)\n"""
tests/ops/test_stats.py,51,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nimport pytest\nimport torch\n\nfrom pyro.ops.stats import (_cummin, autocorrelation, autocovariance, crps_empirical, effective_sample_size,\n                            fit_generalized_pareto, gelman_rubin, hpdi, pi, quantile, resample, split_gelman_rubin,\n                            waic)\nfrom tests.common import assert_close, assert_equal, xfail_if_not_implemented\n\n\n@pytest.mark.parametrize(\'replacement\', [True, False])\ndef test_resample(replacement):\n    x = torch.empty(10000, 2)\n    x[:, 0].normal_(3, 4)\n    x[:, 1].normal_(5, 6)\n\n    num_samples = 5000\n    y = resample(x, num_samples=num_samples, replacement=replacement)\n    z = resample(x.t(), num_samples=num_samples, dim=1, replacement=replacement)\n    if not replacement:\n        assert_equal(torch.unique(y.reshape(-1)).numel(), y.numel())\n        assert_equal(torch.unique(z.reshape(-1)).numel(), z.numel())\n    assert_equal(y.shape, torch.Size([num_samples, 2]))\n    assert_equal(z.shape, torch.Size([2, num_samples]))\n    assert_equal(y.mean(dim=0), torch.tensor([3., 5.]), prec=0.2)\n    assert_equal(z.mean(dim=1), torch.tensor([3., 5.]), prec=0.2)\n    assert_equal(y.std(dim=0), torch.tensor([4., 6.]), prec=0.2)\n    assert_equal(z.std(dim=1), torch.tensor([4., 6.]), prec=0.2)\n\n\n@pytest.mark.init(rng_seed=3)\ndef test_quantile():\n    x = torch.tensor([0., 1., 2.])\n    y = torch.rand(2000)\n    z = torch.randn(2000)\n\n    assert_equal(quantile(x, probs=[0., 0.4, 0.5, 1.]), torch.tensor([0., 0.8, 1., 2.]))\n    assert_equal(quantile(y, probs=0.2), torch.tensor(0.2), prec=0.02)\n    assert_equal(quantile(z, probs=0.8413), torch.tensor(1.), prec=0.02)\n\n\ndef test_pi():\n    x = torch.randn(1000).exp()\n    assert_equal(pi(x, prob=0.8), quantile(x, probs=[0.1, 0.9]))\n\n\n@pytest.mark.init(rng_seed=3)\ndef test_hpdi():\n    x = torch.randn(20000)\n    assert_equal(hpdi(x, prob=0.8), pi(x, prob=0.8), prec=0.01)\n\n    x = torch.empty(20000).exponential_(1)\n    assert_equal(hpdi(x, prob=0.2), torch.tensor([0., 0.22]), prec=0.01)\n\n\ndef _quantile(x, dim=0):\n    return quantile(x, probs=[0.1, 0.6], dim=dim)\n\n\ndef _pi(x, dim=0):\n    return pi(x, prob=0.8, dim=dim)\n\n\ndef _hpdi(x, dim=0):\n    return hpdi(x, prob=0.8, dim=dim)\n\n\n@pytest.mark.parametrize(\'statistics\', [_quantile, _pi, _hpdi])\n@pytest.mark.parametrize(\'sample_shape\', [(), (3,), (2, 3)])\ndef test_statistics_A_ok_with_sample_shape(statistics, sample_shape):\n    xs = torch.rand((10,) + torch.Size(sample_shape))\n    y = statistics(xs)\n\n    # test correct shape\n    assert_equal(y.shape, torch.Size([2]) + xs.shape[1:])\n\n    # test correct batch calculation\n    batch_statistics = []\n    for x in xs.reshape(10, -1).split(1, dim=1):\n        batch_statistics.append(statistics(x))\n    assert_equal(torch.cat(batch_statistics, dim=1).reshape(y.shape), y)\n\n    # test dim=-1\n    a = xs.transpose(0, -1)\n    assert_equal(statistics(a, dim=-1), y.transpose(0, -1))\n\n\ndef test_autocorrelation():\n    x = torch.arange(10.)\n    with xfail_if_not_implemented():\n        actual = autocorrelation(x)\n    assert_equal(actual,\n                 torch.tensor([1, 0.78, 0.52, 0.21, -0.13,\n                               -0.52, -0.94, -1.4, -1.91, -2.45]), prec=0.01)\n\n\ndef test_autocovariance():\n    x = torch.arange(10.)\n    with xfail_if_not_implemented():\n        actual = autocovariance(x)\n    assert_equal(actual,\n                 torch.tensor([8.25, 6.42, 4.25, 1.75, -1.08,\n                               -4.25, -7.75, -11.58, -15.75, -20.25]), prec=0.01)\n\n\ndef test_cummin():\n    x = torch.rand(10)\n    y = torch.empty(x.shape)\n    y[0] = x[0]\n    for i in range(1, x.size(0)):\n        y[i] = min(x[i], y[i-1])\n\n    assert_equal(_cummin(x), y)\n\n\n@pytest.mark.parametrize(\'statistics\', [autocorrelation, autocovariance, _cummin])\n@pytest.mark.parametrize(\'sample_shape\', [(), (3,), (2, 3)])\ndef test_statistics_B_ok_with_sample_shape(statistics, sample_shape):\n    xs = torch.rand((10,) + torch.Size(sample_shape))\n    with xfail_if_not_implemented():\n        y = statistics(xs)\n\n    # test correct shape\n    assert_equal(y.shape, xs.shape)\n\n    # test correct batch calculation\n    batch_statistics = []\n    for x in xs.reshape(10, -1).split(1, dim=1):\n        batch_statistics.append(statistics(x))\n    assert_equal(torch.cat(batch_statistics, dim=1).reshape(xs.shape), y)\n\n    # test dim=-1\n    if statistics is not _cummin:\n        a = xs.transpose(0, -1)\n        assert_equal(statistics(a, dim=-1), y.transpose(0, -1))\n\n\ndef test_gelman_rubin():\n    # only need to test precision for small data\n    x = torch.empty(2, 10)\n    x[0, :] = torch.arange(10.)\n    x[1, :] = torch.arange(10.) + 1\n\n    r_hat = gelman_rubin(x)\n    assert_equal(r_hat.item(), 0.98, prec=0.01)\n\n\ndef test_split_gelman_rubin_agree_with_gelman_rubin():\n    x = torch.rand(2, 10)\n    r_hat1 = gelman_rubin(x.reshape(2, 2, 5).reshape(4, 5))\n    r_hat2 = split_gelman_rubin(x)\n    assert_equal(r_hat1, r_hat2)\n\n\ndef test_effective_sample_size():\n    x = torch.arange(1000.).reshape(100, 10)\n\n    with xfail_if_not_implemented():\n        # test against arviz\n        assert_equal(effective_sample_size(x).item(), 52.64, prec=0.01)\n\n\n@pytest.mark.parametrize(\'diagnostics\', [gelman_rubin, split_gelman_rubin, effective_sample_size])\n@pytest.mark.parametrize(\'sample_shape\', [(), (3,), (2, 3)])\ndef test_diagnostics_ok_with_sample_shape(diagnostics, sample_shape):\n    sample_shape = torch.Size(sample_shape)\n    xs = torch.rand((4, 100) + sample_shape)\n\n    with xfail_if_not_implemented():\n        y = diagnostics(xs)\n\n        # test correct shape\n        assert_equal(y.shape, sample_shape)\n\n        # test correct batch calculation\n        batch_diagnostics = []\n        for x in xs.reshape(4, 100, -1).split(1, dim=2):\n            batch_diagnostics.append(diagnostics(x))\n        assert_equal(torch.cat(batch_diagnostics, dim=0).reshape(sample_shape), y)\n\n        # test chain_dim, sample_dim at different positions\n        a = xs.transpose(0, 1)\n        b = xs.unsqueeze(-1).transpose(0, -1).squeeze(0)\n        c = xs.unsqueeze(-1).transpose(1, -1).squeeze(1)\n        assert_equal(diagnostics(a, chain_dim=1, sample_dim=0), y)\n        assert_equal(diagnostics(b, chain_dim=-1, sample_dim=0), y)\n        assert_equal(diagnostics(c, sample_dim=-1), y)\n\n\ndef test_waic():\n    x = - torch.arange(1., 101).log().reshape(25, 4)\n    w_pw, p_pw = waic(x, pointwise=True)\n    w, p = waic(x)\n    w1, p1 = waic(x.t(), dim=1)\n\n    # test against loo package: http://mc-stan.org/loo/reference/waic.html\n    assert_equal(w_pw, torch.tensor([7.49, 7.75, 7.86, 7.92]), prec=0.01)\n    assert_equal(p_pw, torch.tensor([1.14, 0.91, 0.79, 0.70]), prec=0.01)\n\n    assert_equal(w, w_pw.sum())\n    assert_equal(p, p_pw.sum())\n\n    assert_equal(w, w1)\n    assert_equal(p, p1)\n\n\ndef test_weighted_waic():\n    a = 1 + torch.rand(10)\n    b = 1 + torch.rand(10)\n    c = 1 + torch.rand(10)\n    expanded_x = torch.stack([a, b, c, a, b, a, c, a, c]).log()\n    x = torch.stack([a, b, c]).log()\n    log_weights = torch.tensor([4., 2, 3]).log()\n    # assume weights are unnormalized\n    log_weights = log_weights - torch.randn(1)\n\n    w1, p1 = waic(x, log_weights)\n    w2, p2 = waic(expanded_x)\n\n    # test lpd\n    lpd1 = -0.5 * w1 + p1\n    lpd2 = -0.5 * w2 + p2\n    assert_equal(lpd1, lpd2)\n\n    # test p_waic (also test for weighted_variance)\n    unbiased_p1 = p1 * 2 / 3\n    unbiased_p2 = p2 * 8 / 9\n    assert_equal(unbiased_p1, unbiased_p2)\n\n    # test correctness for dim=-1\n    w3, p3 = waic(x.t(), log_weights, dim=-1)\n    assert_equal(w1, w3)\n    assert_equal(p1, p3)\n\n\n@pytest.mark.parametrize(\'k\', [0.2, 0.5])\n@pytest.mark.parametrize(\'sigma\', [0.8, 1.3])\ndef test_fit_generalized_pareto(k, sigma, n_samples=5000):\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=RuntimeWarning)\n        from scipy.stats import genpareto\n\n    X = genpareto.rvs(c=k, scale=sigma, size=n_samples)\n    fit_k, fit_sigma = fit_generalized_pareto(torch.tensor(X))\n    assert_equal(k, fit_k, prec=0.02)\n    assert_equal(sigma, fit_sigma, prec=0.02)\n\n\n@pytest.mark.parametrize(\'event_shape\', [(), (4,), (3, 2)])\n@pytest.mark.parametrize(\'num_samples\', [1, 2, 3, 4, 10])\ndef test_crps_empirical(num_samples, event_shape):\n    truth = torch.randn(event_shape)\n    pred = truth + 0.1 * torch.randn((num_samples,) + event_shape)\n\n    actual = crps_empirical(pred, truth)\n    assert actual.shape == truth.shape\n\n    expected = ((pred - truth).abs().mean(0)\n                - 0.5 * (pred - pred.unsqueeze(1)).abs().mean([0, 1]))\n    assert_close(actual, expected)\n'"
tests/ops/test_tensor_utils.py,25,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport numpy as np\nimport pytest\nimport scipy.fftpack as fftpack\nimport torch\n\nimport pyro\nfrom pyro.ops.tensor_utils import (block_diag_embed, block_diagonal, convolve, dct, idct, next_fast_len,\n                                   periodic_cumsum, periodic_features, periodic_repeat, precision_to_scale_tril,\n                                   repeated_matmul)\nfrom tests.common import assert_close, assert_equal\n\npytestmark = pytest.mark.stage(\'unit\')\n\n\n@pytest.mark.parametrize(\'batch_size\', [1, 2, 3])\n@pytest.mark.parametrize(\'block_size\', [torch.Size([2, 2]), torch.Size([3, 1]), torch.Size([4, 2])])\ndef test_block_diag_embed(batch_size, block_size):\n    m = torch.randn(block_size).unsqueeze(0).expand((batch_size,) + block_size)\n    b = block_diag_embed(m)\n\n    assert b.shape == (batch_size * block_size[0], batch_size * block_size[1])\n\n    assert_equal(b.sum(), m.sum())\n\n    for k in range(batch_size):\n        bottom, top = k * block_size[0], (k + 1) * block_size[0]\n        left, right = k * block_size[1], (k + 1) * block_size[1]\n        assert_equal(b[bottom:top, left:right], m[k])\n\n\n@pytest.mark.parametrize(\'batch_shape\', [torch.Size([]), torch.Size([7])])\n@pytest.mark.parametrize(\'mat_size,block_size\', [(torch.Size([2, 2]), 2), (torch.Size([3, 1]), 1),\n                                                 (torch.Size([6, 3]), 3)])\ndef test_block_diag(batch_shape, mat_size, block_size):\n    mat = torch.randn(batch_shape + (block_size,) + mat_size)\n    mat_embed = block_diag_embed(mat)\n    mat_embed_diag = block_diagonal(mat_embed, block_size)\n    assert_equal(mat_embed_diag, mat)\n\n\n@pytest.mark.parametrize(""size"", [5, 6, 7, 8])\n@pytest.mark.parametrize(""period"", [2, 3, 4])\n@pytest.mark.parametrize(""left_shape"", [(), (6,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""right_shape"", [(), (7,), (5, 4)], ids=str)\ndef test_periodic_repeat(period, size, left_shape, right_shape):\n    dim = -1 - len(right_shape)\n    tensor = torch.randn(left_shape + (period,) + right_shape)\n    actual = periodic_repeat(tensor, size, dim)\n    assert actual.shape == left_shape + (size,) + right_shape\n    dots = (slice(None),) * len(left_shape)\n    for t in range(size):\n        assert_equal(actual[dots + (t,)], tensor[dots + (t % period,)])\n\n\n@pytest.mark.parametrize(""duration"", range(3, 100))\ndef test_periodic_features(duration):\n    pyro.set_rng_seed(duration)\n    max_period = torch.distributions.Uniform(2, duration).sample().item()\n    for max_period in [max_period, duration]:\n        min_period = torch.distributions.Uniform(2, max_period).sample().item()\n        for min_period in [min_period, 2]:\n            actual = periodic_features(duration, max_period, min_period)\n            assert actual.shape == (duration, 2 * math.ceil(max_period / min_period) - 2)\n            assert (-1 <= actual).all()\n            assert (actual <= 1).all()\n\n\n@pytest.mark.parametrize(""size"", [5, 6, 7, 8])\n@pytest.mark.parametrize(""period"", [2, 3, 4])\n@pytest.mark.parametrize(""left_shape"", [(), (6,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""right_shape"", [(), (7,), (5, 4)], ids=str)\ndef test_periodic_cumsum(period, size, left_shape, right_shape):\n    dim = -1 - len(right_shape)\n    tensor = torch.randn(left_shape + (size,) + right_shape)\n    actual = periodic_cumsum(tensor, period, dim)\n    assert actual.shape == tensor.shape\n    dots = (slice(None),) * len(left_shape)\n    for t in range(period):\n        assert_equal(actual[dots + (t,)], tensor[dots + (t,)])\n    for t in range(period, size):\n        assert_close(actual[dots + (t,)], tensor[dots + (t,)] + actual[dots + (t - period,)])\n\n\n@pytest.mark.parametrize(\'m\', [2, 3, 4, 5, 6, 10])\n@pytest.mark.parametrize(\'n\', [2, 3, 4, 5, 6, 10])\n@pytest.mark.parametrize(\'mode\', [\'full\', \'valid\', \'same\'])\ndef test_convolve_shape(m, n, mode):\n    signal = torch.randn(m)\n    kernel = torch.randn(n)\n    actual = convolve(signal, kernel, mode)\n    expected = np.convolve(signal, kernel, mode=mode)\n    assert actual.shape == expected.shape\n\n\n@pytest.mark.parametrize(\'m\', [2, 3, 4, 5, 6, 10])\n@pytest.mark.parametrize(\'n\', [2, 3, 4, 5, 6, 10])\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'mode\', [\'full\', \'valid\', \'same\'])\ndef test_convolve(batch_shape, m, n, mode):\n    signal = torch.randn(*batch_shape, m)\n    kernel = torch.randn(*batch_shape, n)\n    actual = convolve(signal, kernel, mode)\n    expected = torch.stack([\n        torch.tensor(np.convolve(s, k, mode=mode))\n        for s, k in zip(signal.reshape(-1, m), kernel.reshape(-1, n))\n    ]).reshape(*batch_shape, -1)\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'size\', [torch.Size([2, 2]), torch.Size([4, 3, 3]), torch.Size([4, 1, 2, 2])])\n@pytest.mark.parametrize(\'n\', [1, 2, 3, 7, 8])\ndef test_repeated_matmul(size, n):\n    M = torch.randn(size)\n    result = repeated_matmul(M, n)\n    assert result.shape == ((n,) + size)\n\n    serial_result = M\n    for i in range(n):\n        assert_equal(result[i, ...], serial_result)\n        serial_result = torch.matmul(serial_result, M)\n\n\n@pytest.mark.parametrize(\'shape\', [(3, 4), (5,), (2, 1, 6)])\ndef test_dct(shape):\n    x = torch.randn(shape)\n    actual = dct(x)\n    expected = torch.from_numpy(fftpack.dct(x.numpy(), norm=\'ortho\'))\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'shape\', [(3, 4), (5,), (2, 1, 6)])\ndef test_idct(shape):\n    x = torch.randn(shape)\n    actual = idct(x)\n    expected = torch.from_numpy(fftpack.idct(x.numpy(), norm=\'ortho\'))\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(""dim"", [-4, -3, -2, -1, 0, 1, 2, 3])\n@pytest.mark.parametrize(""fn"", [dct, idct])\ndef test_dct_dim(fn, dim):\n    x = torch.randn(4, 5, 6, 7)\n    actual = fn(x, dim=dim)\n    if dim == -1 or dim == 3:\n        expected = fn(x)\n    else:\n        expected = fn(x.transpose(-1, dim)).transpose(-1, dim)\n    assert_close(actual, expected)\n\n\ndef test_next_fast_len():\n    for size in range(1, 1000):\n        assert next_fast_len(size) == fftpack.next_fast_len(size)\n\n\n@pytest.mark.parametrize(\'batch_shape,event_shape\', [\n    ((), (5,)),\n    ((3,), (4,)),\n])\ndef test_precision_to_scale_tril(batch_shape, event_shape):\n    x = torch.randn(batch_shape + event_shape + event_shape)\n    precision = x.matmul(x.transpose(-2, -1))\n    actual = precision_to_scale_tril(precision)\n    expected = precision.inverse().cholesky()\n    assert_close(actual, expected)\n'"
tests/ops/test_welford.py,19,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom pyro.ops.welford import WelfordArrowheadCovariance, WelfordCovariance\nfrom pyro.util import optional\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize('n_samples,dim_size', [(1000, 1),\n                                                (1000, 7),\n                                                (1, 1)])\n@pytest.mark.init(rng_seed=7)\ndef test_welford_diagonal(n_samples, dim_size):\n    w = WelfordCovariance(diagonal=True)\n    loc = torch.zeros(dim_size)\n    cov_diagonal = torch.rand(dim_size)\n    cov = torch.diag(cov_diagonal)\n    dist = torch.distributions.MultivariateNormal(loc=loc, covariance_matrix=cov)\n    samples = []\n    for _ in range(n_samples):\n        sample = dist.sample()\n        samples.append(sample)\n        w.update(sample)\n\n    sample_variance = torch.stack(samples).var(dim=0, unbiased=True)\n    with optional(pytest.raises(RuntimeError), n_samples == 1):\n        estimates = w.get_covariance(regularize=False)\n        assert_equal(estimates, sample_variance)\n\n\n@pytest.mark.parametrize('n_samples,dim_size', [(1000, 1),\n                                                (1000, 7),\n                                                (1, 1)])\n@pytest.mark.init(rng_seed=7)\ndef test_welford_dense(n_samples, dim_size):\n    w = WelfordCovariance(diagonal=False)\n    loc = torch.zeros(dim_size)\n    cov = torch.randn(dim_size, dim_size)\n    cov = torch.mm(cov, cov.t())\n    dist = torch.distributions.MultivariateNormal(loc=loc, covariance_matrix=cov)\n    samples = dist.sample(torch.Size([n_samples]))\n    for sample in samples:\n        w.update(sample)\n\n    with optional(pytest.raises(RuntimeError), n_samples == 1):\n        estimates = w.get_covariance(regularize=False).cpu().numpy()\n        sample_cov = np.cov(samples.cpu().numpy(), bias=False, rowvar=False)\n        assert_equal(estimates, sample_cov)\n\n\n@pytest.mark.parametrize('n_samples,dim_size,head_size', [\n    (1000, 5, 0),\n    (1000, 5, 1),\n    (1000, 5, 4),\n    (1000, 5, 5)\n])\n@pytest.mark.parametrize('regularize', [True, False])\ndef test_welford_arrowhead(n_samples, dim_size, head_size, regularize):\n    adapt_scheme = WelfordArrowheadCovariance(head_size=head_size)\n    loc = torch.zeros(dim_size)\n    cov = torch.randn(dim_size, dim_size)\n    cov = torch.mm(cov, cov.t())\n    dist = torch.distributions.MultivariateNormal(loc=loc, covariance_matrix=cov)\n    samples = dist.sample(sample_shape=torch.Size([n_samples]))\n\n    for sample in samples:\n        adapt_scheme.update(sample)\n    top, bottom_diag = adapt_scheme.get_covariance(regularize=regularize)\n    actual = torch.cat([top, torch.cat([top[:, head_size:].t(), bottom_diag.diag()], -1)])\n\n    mask = torch.ones(dim_size, dim_size)\n    mask[head_size:, head_size:] = 0.\n    mask.view(-1)[::dim_size + 1][head_size:] = 1.\n    expected = np.cov(samples.cpu().numpy(), bias=False, rowvar=False)\n    expected = torch.from_numpy(expected).type_as(mask)\n    if regularize:\n        expected = (expected * n_samples + 1e-3 * torch.eye(dim_size) * 5) / (n_samples + 5)\n    expected = expected * mask\n    assert_equal(actual, expected)\n"""
tests/optim/__init__.py,0,b''
tests/optim/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/optim""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""unit""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/optim/test_multi.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim\nimport pyro.poutine as poutine\nfrom pyro.optim.multi import MixedMultiOptimizer, Newton, PyroMultiOptimizer, TorchMultiOptimizer\nfrom tests.common import assert_equal\n\nFACTORIES = [\n    lambda: PyroMultiOptimizer(pyro.optim.Adam({\'lr\': 0.05})),\n    lambda: TorchMultiOptimizer(torch.optim.Adam, {\'lr\': 0.05}),\n    lambda: Newton(trust_radii={\'z\': 0.2}),\n    lambda: MixedMultiOptimizer([([\'y\'], PyroMultiOptimizer(pyro.optim.Adam({\'lr\': 0.05}))),\n                                 ([\'x\', \'z\'], Newton())]),\n    lambda: MixedMultiOptimizer([([\'y\'], pyro.optim.Adam({\'lr\': 0.05})),\n                                 ([\'x\', \'z\'], Newton())]),\n]\n\n\n@pytest.mark.parametrize(\'factory\', FACTORIES)\ndef test_optimizers(factory):\n    optim = factory()\n\n    def model(loc, cov):\n        x = pyro.param(""x"", torch.randn(2))\n        y = pyro.param(""y"", torch.randn(3, 2))\n        z = pyro.param(""z"", torch.randn(4, 2).abs(), constraint=constraints.greater_than(-1))\n        pyro.sample(""obs_x"", dist.MultivariateNormal(loc, cov), obs=x)\n        with pyro.plate(""y_plate"", 3):\n            pyro.sample(""obs_y"", dist.MultivariateNormal(loc, cov), obs=y)\n        with pyro.plate(""z_plate"", 4):\n            pyro.sample(""obs_z"", dist.MultivariateNormal(loc, cov), obs=z)\n\n    loc = torch.tensor([-0.5, 0.5])\n    cov = torch.tensor([[1.0, 0.09], [0.09, 0.1]])\n    for step in range(200):\n        tr = poutine.trace(model).get_trace(loc, cov)\n        loss = -tr.log_prob_sum()\n        params = {name: site[\'value\'].unconstrained()\n                  for name, site in tr.nodes.items()\n                  if site[\'type\'] == \'param\'}\n        optim.step(loss, params)\n\n    for name in [""x"", ""y"", ""z""]:\n        actual = pyro.param(name)\n        expected = loc.expand(actual.shape)\n        assert_equal(actual, expected, prec=1e-2,\n                     msg=\'{} in correct: {} vs {}\'.format(name, actual, expected))\n\n\ndef test_multi_optimizer_disjoint_ok():\n    parts = [([\'w\', \'x\'], pyro.optim.Adam({\'lr\': 0.1})),\n             ([\'y\', \'z\'], pyro.optim.Adam({\'lr\': 0.01}))]\n    MixedMultiOptimizer(parts)\n\n\ndef test_multi_optimizer_overlap_error():\n    parts = [([\'x\', \'y\'], pyro.optim.Adam({\'lr\': 0.1})),\n             ([\'y\', \'z\'], pyro.optim.Adam({\'lr\': 0.01}))]\n    with pytest.raises(ValueError):\n        MixedMultiOptimizer(parts)\n'"
tests/optim/test_optim.py,35,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom unittest import TestCase\n\nimport pytest\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.optim as optim\nfrom pyro.distributions import Normal, Uniform\nfrom pyro.infer import SVI, TraceGraph_ELBO\nfrom tests.common import assert_equal\n\n\nclass OptimTests(TestCase):\n\n    def setUp(self):\n        # normal-normal; known covariance\n        self.lam0 = torch.tensor([0.1])  # precision of prior\n        self.loc0 = torch.tensor([0.5])  # prior mean\n        # known precision of observation noise\n        self.lam = torch.tensor([6.0])\n        self.data = torch.tensor([1.0])  # a single observation\n\n    def test_per_param_optim(self):\n        self.do_test_per_param_optim(""loc_q"", ""log_sig_q"")\n        self.do_test_per_param_optim(""log_sig_q"", ""loc_q"")\n\n    # make sure lr=0 gets propagated correctly to parameters of our choice\n    def do_test_per_param_optim(self, fixed_param, free_param):\n        pyro.clear_param_store()\n\n        def model():\n            prior_dist = Normal(self.loc0, torch.pow(self.lam0, -0.5))\n            loc_latent = pyro.sample(""loc_latent"", prior_dist)\n            x_dist = Normal(loc_latent, torch.pow(self.lam, -0.5))\n            pyro.sample(""obs"", x_dist, obs=self.data)\n            return loc_latent\n\n        def guide():\n            loc_q = pyro.param(\n                ""loc_q"",\n                torch.zeros(1, requires_grad=True))\n            log_sig_q = pyro.param(\n                ""log_sig_q"",\n                torch.zeros(1, requires_grad=True))\n            sig_q = torch.exp(log_sig_q)\n            pyro.sample(""loc_latent"", Normal(loc_q, sig_q))\n\n        def optim_params(module_name, param_name):\n            if param_name == fixed_param:\n                return {\'lr\': 0.00}\n            elif param_name == free_param:\n                return {\'lr\': 0.01}\n\n        adam = optim.Adam(optim_params)\n        adam2 = optim.Adam(optim_params)\n        svi = SVI(model, guide, adam, loss=TraceGraph_ELBO())\n        svi2 = SVI(model, guide, adam2, loss=TraceGraph_ELBO())\n\n        svi.step()\n        adam_initial_step_count = list(adam.get_state()[\'loc_q\'][\'state\'].items())[0][1][\'step\']\n        adam.save(\'adam.unittest.save\')\n        svi.step()\n        adam_final_step_count = list(adam.get_state()[\'loc_q\'][\'state\'].items())[0][1][\'step\']\n        adam2.load(\'adam.unittest.save\')\n        svi2.step()\n        adam2_step_count_after_load_and_step = list(adam2.get_state()[\'loc_q\'][\'state\'].items())[0][1][\'step\']\n\n        assert adam_initial_step_count == 1\n        assert adam_final_step_count == 2\n        assert adam2_step_count_after_load_and_step == 2\n\n        free_param_unchanged = torch.equal(pyro.param(free_param).data, torch.zeros(1))\n        fixed_param_unchanged = torch.equal(pyro.param(fixed_param).data, torch.zeros(1))\n        assert fixed_param_unchanged and not free_param_unchanged\n\n\n@pytest.mark.parametrize(\'scheduler\', [optim.LambdaLR({\'optimizer\': torch.optim.SGD, \'optim_args\': {\'lr\': 0.01},\n                                                       \'lr_lambda\': lambda epoch: 2. ** epoch}),\n                                       optim.StepLR({\'optimizer\': torch.optim.SGD, \'optim_args\': {\'lr\': 0.01},\n                                                     \'gamma\': 2, \'step_size\': 1}),\n                                       optim.ExponentialLR({\'optimizer\': torch.optim.SGD, \'optim_args\': {\'lr\': 0.01},\n                                                            \'gamma\': 2}),\n                                       optim.ReduceLROnPlateau({\'optimizer\': torch.optim.SGD, \'optim_args\': {\'lr\': 1.0},\n                                                                \'factor\': 0.1, \'patience\': 1})])\ndef test_dynamic_lr(scheduler):\n    pyro.clear_param_store()\n\n    def model():\n        sample = pyro.sample(\'latent\', Normal(torch.tensor(0.), torch.tensor(0.3)))\n        return pyro.sample(\'obs\', Normal(sample, torch.tensor(0.2)), obs=torch.tensor(0.1))\n\n    def guide():\n        loc = pyro.param(\'loc\', torch.tensor(0.))\n        scale = pyro.param(\'scale\', torch.tensor(0.5), constraint=constraints.positive)\n        pyro.sample(\'latent\', Normal(loc, scale))\n\n    svi = SVI(model, guide, scheduler, loss=TraceGraph_ELBO())\n    for epoch in range(4):\n        svi.step()\n        svi.step()\n        loc = pyro.param(\'loc\').unconstrained()\n        opt_loc = scheduler.optim_objs[loc].optimizer\n        opt_scale = scheduler.optim_objs[loc].optimizer\n        if issubclass(scheduler.pt_scheduler_constructor, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(1.)\n            if epoch == 2:\n                assert opt_loc.state_dict()[\'param_groups\'][0][\'lr\'] == 0.1\n                assert opt_scale.state_dict()[\'param_groups\'][0][\'lr\'] == 0.1\n            if epoch == 4:\n                assert opt_loc.state_dict()[\'param_groups\'][0][\'lr\'] == 0.01\n                assert opt_scale.state_dict()[\'param_groups\'][0][\'lr\'] == 0.01\n            continue\n        assert opt_loc.state_dict()[\'param_groups\'][0][\'initial_lr\'] == 0.01\n        assert opt_scale.state_dict()[\'param_groups\'][0][\'initial_lr\'] == 0.01\n        if epoch == 0:\n            scheduler.step()\n            assert opt_loc.state_dict()[\'param_groups\'][0][\'lr\'] == 0.02\n            assert opt_scale.state_dict()[\'param_groups\'][0][\'lr\'] == 0.02\n            assert abs(pyro.param(\'loc\').item()) > 1e-5\n            assert abs(pyro.param(\'scale\').item() - 0.5) > 1e-5\n        if epoch == 2:\n            scheduler.step()\n            assert opt_loc.state_dict()[\'param_groups\'][0][\'lr\'] == 0.04\n            assert opt_scale.state_dict()[\'param_groups\'][0][\'lr\'] == 0.04\n\n\n@pytest.mark.parametrize(\'factory\', [optim.Adam, optim.ClippedAdam, optim.DCTAdam, optim.RMSprop, optim.SGD])\ndef test_autowrap(factory):\n    instance = factory({})\n    assert instance.pt_optim_constructor.__name__ == factory.__name__\n\n\n@pytest.mark.parametrize(\'pyro_optim\', [optim.Adam, optim.SGD])\n@pytest.mark.parametrize(\'clip\', [\'clip_norm\', \'clip_value\'])\n@pytest.mark.parametrize(\'value\', [1., 3., 5.])\ndef test_clip_norm(pyro_optim, clip, value):\n    x1 = torch.tensor(0., requires_grad=True)\n    x2 = torch.tensor(0., requires_grad=True)\n    opt_c = pyro_optim({""lr"": 1.}, {clip: value})\n    opt = pyro_optim({""lr"": 1.})\n    for step in range(3):\n        x1.backward(Uniform(value, value + 3.).sample())\n        x2.backward(torch.tensor(value))\n        opt_c([x1])\n        opt([x2])\n        assert_equal(x1.grad, torch.tensor(value))\n        assert_equal(x2.grad, torch.tensor(value))\n        assert_equal(x1, x2)\n        opt_c.optim_objs[x1].zero_grad()\n        opt.optim_objs[x2].zero_grad()\n\n\n@pytest.mark.parametrize(\'clip_norm\', [1., 3., 5.])\ndef test_clippedadam_clip(clip_norm):\n    x1 = torch.tensor(0., requires_grad=True)\n    x2 = torch.tensor(0., requires_grad=True)\n    opt_ca = optim.clipped_adam.ClippedAdam(params=[x1], lr=1., lrd=1., clip_norm=clip_norm)\n    opt_a = torch.optim.Adam(params=[x2], lr=1.)\n    for step in range(3):\n        opt_ca.zero_grad()\n        opt_a.zero_grad()\n        x1.backward(Uniform(clip_norm, clip_norm + 3.).sample())\n        x2.backward(torch.tensor(clip_norm))\n        opt_ca.step()\n        opt_a.step()\n        assert_equal(x1, x2)\n\n\n@pytest.mark.parametrize(\'clip_norm\', [1., 3., 5.])\ndef test_clippedadam_pass(clip_norm):\n    x1 = torch.tensor(0., requires_grad=True)\n    x2 = torch.tensor(0., requires_grad=True)\n    opt_ca = optim.clipped_adam.ClippedAdam(params=[x1], lr=1., lrd=1., clip_norm=clip_norm)\n    opt_a = torch.optim.Adam(params=[x2], lr=1.)\n    for step in range(3):\n        g = Uniform(-clip_norm, clip_norm).sample()\n        opt_ca.zero_grad()\n        opt_a.zero_grad()\n        x1.backward(g)\n        x2.backward(g)\n        opt_ca.step()\n        opt_a.step()\n        assert_equal(x1, x2)\n\n\n@pytest.mark.parametrize(\'lrd\', [1., 3., 5.])\ndef test_clippedadam_lrd(lrd):\n    x1 = torch.tensor(0., requires_grad=True)\n    orig_lr = 1.0\n    opt_ca = optim.clipped_adam.ClippedAdam(params=[x1], lr=orig_lr, lrd=lrd)\n    for step in range(3):\n        g = Uniform(-5., 5.).sample()\n        x1.backward(g)\n        opt_ca.step()\n        assert opt_ca.param_groups[0][\'lr\'] == orig_lr * lrd**(step + 1)\n\n\ndef test_dctadam_param_subsample():\n    outer_size = 7\n    middle_size = 2\n    inner_size = 11\n    outer_subsize = 3\n    inner_subsize = 4\n    event_size = 5\n\n    def model():\n        with pyro.plate(""outer"", outer_size, subsample_size=outer_subsize, dim=-3):\n            with pyro.plate(""inner"", inner_size, subsample_size=inner_subsize, dim=-1):\n                pyro.param(""loc"",\n                           torch.randn(outer_size, middle_size, inner_size, event_size),\n                           event_dim=1)\n\n    optimizer = optim.DCTAdam({""lr"": 1., ""subsample_aware"": True})\n    model()\n    param = pyro.param(""loc"").unconstrained()\n    param.sum().backward()\n    pre_optimized_value = param.detach().clone()\n    optimizer({param})\n    expected_num_changes = outer_subsize * middle_size * inner_subsize * event_size\n    actual_num_changes = ((param - pre_optimized_value) != 0).sum().item()\n    assert actual_num_changes == expected_num_changes\n\n    for i in range(1000):\n        pyro.infer.util.zero_grads({param})\n        model()  # generate new subsample indices\n        param.pow(2).sum().backward()\n        optimizer({param})\n\n    assert_equal(param, param.new_zeros(param.shape), prec=1e-2)\n'"
tests/params/__init__.py,0,b''
tests/params/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/params""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""unit""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/params/test_module.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim\n\n\nclass outest(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.l0 = outer()\n        self.l1 = nn.Linear(2, 2)\n        self.l2 = inner()\n\n    def forward(self, s):\n        pass\n\n\nclass outer(torch.nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.l0 = inner()\n        self.l1 = nn.Linear(2, 2)\n\n    def forward(self, s):\n        pass\n\n\nclass inner(torch.nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.l0 = nn.Linear(2, 2)\n        self.l1 = nn.ReLU()\n\n    def forward(self, s):\n        pass\n\n\nsequential = nn.Sequential(\n          nn.Conv2d(1, 20, 5),\n          nn.ReLU(),\n          nn.Conv2d(20, 64, 5)\n          )\n\n\n@pytest.mark.parametrize(""nn_module"", [outest, outer])\ndef test_module_nn(nn_module):\n    pyro.clear_param_store()\n    nn_module = nn_module()\n    assert pyro.get_param_store()._params == {}\n    pyro.module(""module"", nn_module)\n    for name in pyro.get_param_store():\n        assert pyro.params.user_param_name(name) in nn_module.state_dict().keys()\n\n\n@pytest.mark.parametrize(""nn_module"", [outest, outer])\ndef test_param_no_grad(nn_module):\n    class net(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.x = Parameter(torch.zeros(1))\n            self.y = Parameter(torch.zeros(1), requires_grad=False)\n\n        def forward(self, s):\n            pass\n\n    with pytest.warns(UserWarning):\n        pyro.module(\'net\', net())\n    assert \'net$$$x\' in pyro.get_param_store().keys()\n    assert \'net$$$y\' not in pyro.get_param_store().keys()\n\n\n@pytest.mark.parametrize(""nn_module"", [sequential])\ndef test_module_sequential(nn_module):\n    pyro.clear_param_store()\n    assert pyro.get_param_store()._params == {}\n    pyro.module(""module"", nn_module)\n    for name in pyro.get_param_store():\n        assert pyro.params.user_param_name(name) in nn_module.state_dict().keys()\n\n\n@pytest.mark.parametrize(""nn_module"", [outest, outer])\n@pytest.mark.filterwarnings(""ignore::FutureWarning"")\ndef test_random_module(nn_module):\n    pyro.clear_param_store()\n    nn_module = nn_module()\n    p = torch.ones(2, 2)\n    prior = dist.Bernoulli(p)\n    lifted_mod = pyro.random_module(""module"", nn_module, prior)\n    nn_module = lifted_mod()\n    for name, parameter in nn_module.named_parameters():\n        assert torch.equal(torch.ones(2, 2), parameter.data)\n'"
tests/params/test_param.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom copy import copy\nfrom unittest import TestCase\n\nimport numpy as np\nimport torch\nimport torch.optim\nfrom torch import nn as nn\nfrom torch.distributions import constraints\n\nimport pyro\nfrom tests.common import assert_equal\n\n\nclass ParamStoreDictTests(TestCase):\n\n    def setUp(self):\n        pyro.clear_param_store()\n        self.linear_module = nn.Linear(3, 2)\n        self.linear_module2 = nn.Linear(3, 2)\n        self.linear_module3 = nn.Linear(3, 2)\n\n    def test_save_and_load(self):\n        lin = pyro.module(""mymodule"", self.linear_module)\n        pyro.module(""mymodule2"", self.linear_module2)\n        x = torch.randn(1, 3)\n        myparam = pyro.param(""myparam"", 1.234 * torch.ones(1))\n\n        cost = torch.sum(torch.pow(lin(x), 2.0)) * torch.pow(myparam, 4.0)\n        cost.backward()\n        params = list(self.linear_module.parameters()) + [myparam]\n        optim = torch.optim.Adam(params, lr=.01)\n        myparam_copy_stale = copy(pyro.param(""myparam"").detach().cpu().numpy())\n\n        optim.step()\n\n        myparam_copy = copy(pyro.param(""myparam"").detach().cpu().numpy())\n        param_store_params = copy(pyro.get_param_store()._params)\n        param_store_param_to_name = copy(pyro.get_param_store()._param_to_name)\n        assert len(list(param_store_params.keys())) == 5\n        assert len(list(param_store_param_to_name.values())) == 5\n\n        pyro.get_param_store().save(\'paramstore.unittest.out\')\n        pyro.clear_param_store()\n        assert len(list(pyro.get_param_store()._params)) == 0\n        assert len(list(pyro.get_param_store()._param_to_name)) == 0\n        pyro.get_param_store().load(\'paramstore.unittest.out\')\n\n        def modules_are_equal():\n            weights_equal = np.sum(np.fabs(self.linear_module3.weight.detach().cpu().numpy() -\n                                   self.linear_module.weight.detach().cpu().numpy())) == 0.0\n            bias_equal = np.sum(np.fabs(self.linear_module3.bias.detach().cpu().numpy() -\n                                self.linear_module.bias.detach().cpu().numpy())) == 0.0\n            return (weights_equal and bias_equal)\n\n        assert not modules_are_equal()\n        pyro.module(""mymodule"", self.linear_module3, update_module_params=False)\n        assert id(self.linear_module3.weight) != id(pyro.param(\'mymodule$$$weight\'))\n        assert not modules_are_equal()\n        pyro.module(""mymodule"", self.linear_module3, update_module_params=True)\n        assert id(self.linear_module3.weight) == id(pyro.param(\'mymodule$$$weight\'))\n        assert modules_are_equal()\n\n        myparam = pyro.param(""myparam"")\n        store = pyro.get_param_store()\n        assert myparam_copy_stale != myparam.detach().cpu().numpy()\n        assert myparam_copy == myparam.detach().cpu().numpy()\n        assert sorted(param_store_params.keys()) == sorted(store._params.keys())\n        assert sorted(param_store_param_to_name.values()) == sorted(store._param_to_name.values())\n        assert sorted(store._params.keys()) == sorted(store._param_to_name.values())\n\n\ndef test_dict_interface():\n    param_store = pyro.get_param_store()\n\n    # start empty\n    param_store.clear()\n    assert not param_store\n    assert len(param_store) == 0\n    assert \'x\' not in param_store\n    assert \'y\' not in param_store\n    assert list(param_store.items()) == []\n    assert list(param_store.keys()) == []\n    assert list(param_store.values()) == []\n\n    # add x\n    param_store[\'x\'] = torch.zeros(1, 2, 3)\n    assert param_store\n    assert len(param_store) == 1\n    assert \'x\' in param_store\n    assert \'y\' not in param_store\n    assert list(param_store.keys()) == [\'x\']\n    assert [key for key, value in param_store.items()] == [\'x\']\n    assert len(list(param_store.values())) == 1\n    assert param_store[\'x\'].shape == (1, 2, 3)\n    assert_equal(param_store.setdefault(\'x\', torch.ones(1, 2, 3)), torch.zeros(1, 2, 3))\n    assert param_store[\'x\'].unconstrained() is param_store[\'x\']\n\n    # add y\n    param_store.setdefault(\'y\', torch.ones(4, 5), constraint=constraints.positive)\n    assert param_store\n    assert len(param_store) == 2\n    assert \'x\' in param_store\n    assert \'y\' in param_store\n    assert sorted(param_store.keys()) == [\'x\', \'y\']\n    assert sorted(key for key, value in param_store.items()) == [\'x\', \'y\']\n    assert len(list(param_store.values())) == 2\n    assert param_store[\'x\'].shape == (1, 2, 3)\n    assert param_store[\'y\'].shape == (4, 5)\n    assert_equal(param_store.setdefault(\'y\', torch.zeros(4, 5)), torch.ones(4, 5))\n    assert_equal(param_store[\'y\'].unconstrained(), torch.zeros(4, 5))\n\n    # remove x\n    del param_store[\'x\']\n    assert param_store\n    assert len(param_store) == 1\n    assert \'x\' not in param_store\n    assert \'y\' in param_store\n    assert list(param_store.keys()) == [\'y\']\n    assert list(key for key, value in param_store.items()) == [\'y\']\n    assert len(list(param_store.values())) == 1\n    assert param_store[\'y\'].shape == (4, 5)\n    assert_equal(param_store.setdefault(\'y\', torch.zeros(4, 5)), torch.ones(4, 5))\n    assert_equal(param_store[\'y\'].unconstrained(), torch.zeros(4, 5))\n\n    # remove y\n    del param_store[\'y\']\n    assert not param_store\n    assert len(param_store) == 0\n    assert \'x\' not in param_store\n    assert \'y\' not in param_store\n    assert list(param_store.keys()) == []\n    assert list(key for key, value in param_store.items()) == []\n    assert len(list(param_store.values())) == 0\n'"
tests/perf/__init__.py,0,b''
tests/perf/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/perf""):\n            if ""perf"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""perf""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/perf/test_benchmark.py,17,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport cProfile\nimport os\nimport re\nfrom collections import namedtuple\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.contrib.gp as gp\nimport pyro.distributions as dist\nimport pyro.optim as optim\nfrom pyro.distributions.testing import fakes\nfrom pyro.infer import SVI, Trace_ELBO, TraceGraph_ELBO\nfrom pyro.infer.mcmc.hmc import HMC\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.infer.mcmc.nuts import NUTS\n\nModel = namedtuple(\'TestModel\', [\'model\', \'model_args\', \'model_id\'])\n\n\nTEST_MODELS = []\nMODEL_IDS = []\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))\nPROF_DIR = os.path.join(ROOT_DIR, "".benchmarks"")\nif not os.path.exists(PROF_DIR):\n    os.makedirs(PROF_DIR)\n\n\ndef register_model(**model_kwargs):\n    def register_fn(model):\n        model_id = model_kwargs.pop(""id"")\n        test_model = Model(model, model_kwargs, model_id)\n        TEST_MODELS.append(test_model)\n        MODEL_IDS.append(model_id)\n        return model\n    return register_fn\n\n\n@register_model(reparameterized=True, Elbo=TraceGraph_ELBO, id=\'PoissonGamma::reparam=True_TraceGraph\')\n@register_model(reparameterized=True, Elbo=Trace_ELBO, id=\'PoissonGamma::reparam=True_Trace\')\n@register_model(reparameterized=False, Elbo=TraceGraph_ELBO, id=\'PoissonGamma::reparam=False_TraceGraph\')\n@register_model(reparameterized=False, Elbo=Trace_ELBO, id=\'PoissonGamma::reparam=False_Trace\')\ndef poisson_gamma_model(reparameterized, Elbo):\n    pyro.set_rng_seed(0)\n    alpha0 = torch.tensor(1.0)\n    beta0 = torch.tensor(1.0)\n    data = torch.tensor([1.0, 2.0, 3.0])\n    n_data = len(data)\n    data_sum = data.sum(0)\n    alpha_n = alpha0 + data_sum  # posterior alpha\n    beta_n = beta0 + torch.tensor(float(n_data))  # posterior beta\n    log_alpha_n = torch.log(alpha_n)\n    log_beta_n = torch.log(beta_n)\n\n    pyro.clear_param_store()\n    Gamma = dist.Gamma if reparameterized else fakes.NonreparameterizedGamma\n\n    def model():\n        lambda_latent = pyro.sample(""lambda_latent"", Gamma(alpha0, beta0))\n        with pyro.plate(""data"", n_data):\n            pyro.sample(""obs"", dist.Poisson(lambda_latent), obs=data)\n        return lambda_latent\n\n    def guide():\n        alpha_q_log = pyro.param(""alpha_q_log"", log_alpha_n + 0.17)\n        beta_q_log = pyro.param(""beta_q_log"", log_beta_n - 0.143)\n        alpha_q, beta_q = torch.exp(alpha_q_log), torch.exp(beta_q_log)\n        pyro.sample(""lambda_latent"", Gamma(alpha_q, beta_q))\n\n    adam = optim.Adam({""lr"": .0002, ""betas"": (0.97, 0.999)})\n    svi = SVI(model, guide, adam, loss=Elbo())\n    for k in range(3000):\n        svi.step()\n\n\n@register_model(kernel=NUTS, step_size=0.02, num_samples=300, id=\'BernoulliBeta::NUTS\')\n@register_model(kernel=HMC, step_size=0.02, num_steps=3, num_samples=1000, id=\'BernoulliBeta::HMC\')\ndef bernoulli_beta_hmc(**kwargs):\n    def model(data):\n        alpha = pyro.param(\'alpha\', torch.tensor([1.1, 1.1]))\n        beta = pyro.param(\'beta\', torch.tensor([1.1, 1.1]))\n        p_latent = pyro.sample(""p_latent"", dist.Beta(alpha, beta))\n        pyro.sample(""obs"", dist.Bernoulli(p_latent), obs=data)\n        return p_latent\n\n    pyro.set_rng_seed(0)\n    true_probs = torch.tensor([0.9, 0.1])\n    data = dist.Bernoulli(true_probs).sample(sample_shape=(torch.Size((1000,))))\n    kernel = kwargs.pop(\'kernel\')\n    num_samples = kwargs.pop(\'num_samples\')\n    mcmc_kernel = kernel(model, **kwargs)\n    mcmc = MCMC(mcmc_kernel, num_samples=num_samples, warmup_steps=100)\n    mcmc.run(data)\n    return mcmc.get_samples()[\'p_latent\']\n\n\n@register_model(num_steps=2000, whiten=False, id=\'VSGP::MultiClass_whiten=False\')\n@register_model(num_steps=2000, whiten=True, id=\'VSGP::MultiClass_whiten=True\')\ndef vsgp_multiclass(num_steps, whiten):\n    # adapted from http://gpflow.readthedocs.io/en/latest/notebooks/multiclass.html\n    pyro.set_rng_seed(0)\n    X = torch.rand(100, 1)\n    K = (-0.5 * (X - X.t()).pow(2) / 0.01).exp() + torch.eye(100) * 1e-6\n    f = K.cholesky().matmul(torch.randn(100, 3))\n    y = f.argmax(dim=-1)\n\n    kernel = gp.kernels.Sum(gp.kernels.Matern32(1),\n                            gp.kernels.WhiteNoise(1, variance=torch.tensor(0.01)))\n    likelihood = gp.likelihoods.MultiClass(num_classes=3)\n    Xu = X[::5].clone()\n\n    gpmodule = gp.models.VariationalSparseGP(X, y, kernel, Xu, likelihood,\n                                             latent_shape=torch.Size([3]),\n                                             whiten=whiten)\n\n    gpmodule.Xu.requires_grad_(False)\n    gpmodule.kernel.kern1.variance_unconstrained.requires_grad_(False)\n\n    optimizer = torch.optim.Adam(gpmodule.parameters(), lr=0.0001)\n    gp.util.train(gpmodule, optimizer, num_steps=num_steps)\n\n\n@pytest.mark.parametrize(\'model, model_args, id\', TEST_MODELS, ids=MODEL_IDS)\n@pytest.mark.benchmark(\n    min_rounds=5,\n    disable_gc=True,\n)\n@pytest.mark.disable_validation()\ndef test_benchmark(benchmark, model, model_args, id):\n    print(""Running - {}"".format(id))\n    benchmark(model, **model_args)\n\n\ndef profile_fn(test_model):\n    def wrapped():\n        test_model.model(**test_model.model_args)\n    return wrapped\n\n\nif __name__ == ""__main__"":\n    """"""\n    This script is invoked to run cProfile on one of the models specified above.\n    """"""\n    parser = argparse.ArgumentParser(description=""Profiling different Pyro models."")\n    parser.add_argument(""-m"", ""--models"", nargs=""*"",\n                        help=""model name to match against model id, partial match (e.g. *NAME*) is acceptable."")\n    parser.add_argument(""-b"", ""--suffix"", default=""current_branch"",\n                        help=""suffix to append to the cprofile output dump."")\n    parser.add_argument(""-d"", ""--benchmark_dir"", default=PROF_DIR,\n                        help=""directory to save profiling benchmarks."")\n    args = parser.parse_args()\n    search_regexp = [re.compile("".*"" + m + "".*"") for m in args.models]\n    profile_ids = []\n    for r in search_regexp:\n        profile_ids.append(filter(r.match, MODEL_IDS))\n    profile_ids = set().union(*profile_ids)\n    to_profile = [m for m in TEST_MODELS if m.model_id in profile_ids]\n    # run cProfile for all models if not specified\n    if not args.models:\n        to_profile = TEST_MODELS\n    for test_model in to_profile:\n        print(""Running model - {}"".format(test_model.model_id))\n        pr = cProfile.Profile()\n        fn = profile_fn(test_model)\n        pr.runctx(""fn()"", globals(), locals())\n        profile_file = os.path.join(args.benchmark_dir, test_model.model_id + ""#"" + args.suffix + "".prof"")\n        pr.dump_stats(profile_file)\n        print(""Results in - {}"".format(profile_file))\n'"
tests/poutine/__init__.py,0,b''
tests/poutine/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/poutine""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""unit""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/poutine/test_counterfactual.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom tests.common import assert_equal, assert_not_equal\n\n\ndef _item(x):\n    if isinstance(x, torch.Tensor):\n        x = x.item()\n    return x\n\n\n@pytest.mark.parametrize(\'intervene,observe,flip\', [\n    (True, False, False),\n    (False, True, False),\n    (True, True, False),\n    (True, True, True),\n])\ndef test_counterfactual_query(intervene, observe, flip):\n    # x -> y -> z -> w\n\n    sites = [""x"", ""y"", ""z"", ""w""]\n    observations = {""x"": 1., ""y"": None, ""z"": 1., ""w"": 1.}\n    interventions = {""x"": None, ""y"": 0., ""z"": 2., ""w"": 1.}\n\n    def model():\n        x = _item(pyro.sample(""x"", dist.Normal(0, 1)))\n        y = _item(pyro.sample(""y"", dist.Normal(x, 1)))\n        z = _item(pyro.sample(""z"", dist.Normal(y, 1)))\n        w = _item(pyro.sample(""w"", dist.Normal(z, 1)))\n        return dict(x=x, y=y, z=z, w=w)\n\n    if not flip:\n        if intervene:\n            model = poutine.do(model, data=interventions)\n        if observe:\n            model = poutine.condition(model, data=observations)\n    elif flip and intervene and observe:\n        model = poutine.do(\n            poutine.condition(model, data=observations),\n            data=interventions)\n\n    tr = poutine.trace(model).get_trace()\n    actual_values = tr.nodes[""_RETURN""][""value""]\n    for name in sites:\n        # case 1: purely observational query like poutine.condition\n        if not intervene and observe:\n            if observations[name] is not None:\n                assert tr.nodes[name][\'is_observed\']\n                assert_equal(observations[name], actual_values[name])\n                assert_equal(observations[name], tr.nodes[name][\'value\'])\n            if interventions[name] != observations[name]:\n                assert_not_equal(interventions[name], actual_values[name])\n        # case 2: purely interventional query like old poutine.do\n        elif intervene and not observe:\n            assert not tr.nodes[name][\'is_observed\']\n            if interventions[name] is not None:\n                assert_equal(interventions[name], actual_values[name])\n            assert_not_equal(observations[name], tr.nodes[name][\'value\'])\n            assert_not_equal(interventions[name], tr.nodes[name][\'value\'])\n        # case 3: counterfactual query mixing intervention and observation\n        elif intervene and observe:\n            if observations[name] is not None:\n                assert tr.nodes[name][\'is_observed\']\n                assert_equal(observations[name], tr.nodes[name][\'value\'])\n            if interventions[name] is not None:\n                assert_equal(interventions[name], actual_values[name])\n            if interventions[name] != observations[name]:\n                assert_not_equal(interventions[name], tr.nodes[name][\'value\'])\n'"
tests/poutine/test_mapdata.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom tests.common import requires_cuda\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_nested_iplate():\n    means = [torch.randn(2) for i in range(8)]\n    mean_batch_size = 2\n    stds = [torch.abs(torch.randn(2)) for i in range(6)]\n    std_batch_size = 3\n\n    def model(means, stds):\n        a_plate = pyro.plate(""a"", len(means), mean_batch_size)\n        b_plate = pyro.plate(""b"", len(stds), std_batch_size)\n        return [[pyro.sample(""x_{}{}"".format(i, j), dist.Normal(means[i], stds[j]))\n                 for j in b_plate] for i in a_plate]\n\n    xs = model(means, stds)\n    assert len(xs) == mean_batch_size\n    assert len(xs[0]) == std_batch_size\n\n    tr = poutine.trace(model).get_trace(means, stds)\n    for name in tr.nodes.keys():\n        if tr.nodes[name][""type""] == ""sample"" and name.startswith(""x_""):\n            assert tr.nodes[name][""scale""] == 4.0 * 2.0\n\n\ndef plate_model(subsample_size):\n    loc = torch.zeros(20)\n    scale = torch.ones(20)\n    with pyro.plate(\'plate\', 20, subsample_size) as batch:\n        pyro.sample(""x"", dist.Normal(loc[batch], scale[batch]))\n        result = list(batch.data)\n    return result\n\n\ndef iplate_model(subsample_size):\n    loc = torch.zeros(20)\n    scale = torch.ones(20)\n    result = []\n    for i in pyro.plate(\'plate\', 20, subsample_size):\n        pyro.sample(""x_{}"".format(i), dist.Normal(loc[i], scale[i]))\n        result.append(i)\n    return result\n\n\ndef nested_iplate_model(subsample_size):\n    loc = torch.zeros(20)\n    scale = torch.ones(20)\n    result = []\n    inner_iplate = pyro.plate(""inner"", 20, 5)\n    for i in pyro.plate(""outer"", 20, subsample_size):\n        result.append([])\n        for j in inner_iplate:\n            pyro.sample(""x_{}_{}"".format(i, j), dist.Normal(loc[i] + loc[j], scale[i] + scale[j]))\n            result[-1].append(j)\n    return result\n\n\n@pytest.mark.parametrize(\'subsample_size\', [5, 20])\n@pytest.mark.parametrize(\'model\', [plate_model, iplate_model, nested_iplate_model],\n                         ids=[\'plate\', \'iplate\', \'nested_iplate\'])\ndef test_cond_indep_stack(model, subsample_size):\n    tr = poutine.trace(model).get_trace(subsample_size)\n    for name, node in tr.nodes.items():\n        if name.startswith(""x""):\n            assert node[""cond_indep_stack""], ""missing cond_indep_stack at node {}"".format(name)\n\n\n@pytest.mark.parametrize(\'subsample_size\', [5, 20])\n@pytest.mark.parametrize(\'model\', [plate_model, iplate_model, nested_iplate_model],\n                         ids=[\'plate\', \'iplate\', \'nested_iplate\'])\ndef test_replay(model, subsample_size):\n    pyro.set_rng_seed(0)\n\n    traced_model = poutine.trace(model)\n    original = traced_model(subsample_size)\n\n    replayed = poutine.replay(model, trace=traced_model.trace)(subsample_size)\n    assert replayed == original\n\n    if subsample_size < 20:\n        different = traced_model(subsample_size)\n        assert different != original\n\n\ndef plate_custom_model(subsample):\n    with pyro.plate(\'plate\', 20, subsample=subsample) as batch:\n        result = batch\n    return result\n\n\ndef iplate_custom_model(subsample):\n    result = []\n    for i in pyro.plate(\'plate\', 20, subsample=subsample):\n        result.append(i)\n    return result\n\n\n@pytest.mark.parametrize(\'model\', [plate_custom_model, iplate_custom_model],\n                         ids=[\'plate\', \'iplate\'])\ndef test_custom_subsample(model):\n    pyro.set_rng_seed(0)\n\n    subsample = [1, 3, 5, 7]\n    assert model(subsample) == subsample\n    assert poutine.trace(model)(subsample) == subsample\n\n\ndef plate_cuda_model(subsample_size):\n    loc = torch.zeros(20).cuda()\n    scale = torch.ones(20).cuda()\n    with pyro.plate(""data"", 20, subsample_size, device=loc.device) as batch:\n        pyro.sample(""x"", dist.Normal(loc[batch], scale[batch]))\n\n\ndef iplate_cuda_model(subsample_size):\n    loc = torch.zeros(20).cuda()\n    scale = torch.ones(20).cuda()\n    for i in pyro.plate(""data"", 20, subsample_size, device=loc.device):\n        pyro.sample(""x_{}"".format(i), dist.Normal(loc[i], scale[i]))\n\n\n@requires_cuda\n@pytest.mark.parametrize(\'subsample_size\', [5, 20])\n@pytest.mark.parametrize(\'model\', [plate_cuda_model, iplate_cuda_model], ids=[""plate"", ""iplate""])\ndef test_cuda(model, subsample_size):\n    tr = poutine.trace(model).get_trace(subsample_size)\n    assert tr.log_prob_sum().is_cuda\n\n\n@pytest.mark.parametrize(\'model\', [plate_model, iplate_model], ids=[\'plate\', \'iplate\'])\n@pytest.mark.parametrize(""behavior,model_size,guide_size"", [\n    (""error"", 20, 5),\n    (""error"", 5, 20),\n    (""error"", 5, None),\n    (""ok"", 20, 20),\n    (""ok"", 20, None),\n    (""ok"", 5, 5),\n    (""ok"", None, 20),\n    (""ok"", None, 5),\n    (""ok"", None, None),\n])\ndef test_model_guide_mismatch(behavior, model_size, guide_size, model):\n    model = poutine.trace(model)\n    expected_ind = model(guide_size)\n    if behavior == ""ok"":\n        actual_ind = poutine.replay(model, trace=model.trace)(model_size)\n        assert actual_ind == expected_ind\n    else:\n        with pytest.raises(ValueError):\n            poutine.replay(model, trace=model.trace)(model_size)\n'"
tests/poutine/test_nesting.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\nimport pyro\nimport pyro.poutine as poutine\nimport pyro.distributions as dist\nimport pyro.poutine.runtime\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_nested_reset():\n\n    def nested_model():\n        pyro.sample(""internal0"", dist.Bernoulli(0.5))\n        with poutine.escape(escape_fn=lambda msg: msg[""name""] == ""internal2""):\n            pyro.sample(""internal1"", dist.Bernoulli(0.5))\n            pyro.sample(""internal2"", dist.Bernoulli(0.5))\n            pyro.sample(""internal3"", dist.Bernoulli(0.5))\n\n    with poutine.trace() as t2:\n        with poutine.block(hide=[""internal2""]):\n            with poutine.trace() as t1:\n                try:\n                    nested_model()\n                except poutine.NonlocalExit as site_container:\n                    site_container.reset_stack()\n                    logger.debug(pyro.poutine.runtime._PYRO_STACK)\n                    assert ""internal1"" not in t1.trace\n                    assert ""internal1"" in t2.trace\n'"
tests/poutine/test_poutines.py,74,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport io\nimport logging\nimport pickle\nimport warnings\nfrom unittest import TestCase\n\nimport pytest\nimport torch\nimport torch.nn as nn\nfrom queue import Queue\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.distributions import Bernoulli, Categorical, Normal\nfrom pyro.poutine.runtime import _DIM_ALLOCATOR, NonlocalExit\nfrom pyro.poutine.util import all_escape, discrete_escape\nfrom tests.common import assert_equal, assert_not_equal, assert_close\n\nlogger = logging.getLogger(__name__)\n\n\ndef eq(x, y, prec=1e-10):\n    return (torch.norm(x - y).item() < prec)\n\n\n# XXX name is a bit silly\nclass NormalNormalNormalHandlerTestCase(TestCase):\n\n    def setUp(self):\n        pyro.clear_param_store()\n\n        def model():\n            latent1 = pyro.sample(""latent1"",\n                                  Normal(torch.zeros(2),\n                                         torch.ones(2)))\n            latent2 = pyro.sample(""latent2"",\n                                  Normal(latent1,\n                                         5 * torch.ones(2)))\n            x_dist = Normal(latent2, torch.ones(2))\n            pyro.sample(""obs"", x_dist, obs=torch.ones(2))\n            return latent1\n\n        def guide():\n            loc1 = pyro.param(""loc1"", torch.randn(2, requires_grad=True))\n            scale1 = pyro.param(""scale1"", torch.ones(2, requires_grad=True))\n            pyro.sample(""latent1"", Normal(loc1, scale1))\n\n            loc2 = pyro.param(""loc2"", torch.randn(2, requires_grad=True))\n            scale2 = pyro.param(""scale2"", torch.ones(2, requires_grad=True))\n            latent2 = pyro.sample(""latent2"", Normal(loc2, scale2))\n            return latent2\n\n        self.model = model\n        self.guide = guide\n\n        self.model_sites = [""latent1"", ""latent2"",\n                            ""obs"",\n                            ""_INPUT"", ""_RETURN""]\n\n        self.guide_sites = [""latent1"", ""latent2"",\n                            ""loc1"", ""scale1"",\n                            ""loc2"", ""scale2"",\n                            ""_INPUT"", ""_RETURN""]\n\n        self.full_sample_sites = {""latent1"": ""latent1"", ""latent2"": ""latent2""}\n        self.partial_sample_sites = {""latent1"": ""latent1""}\n\n\nclass TraceHandlerTests(NormalNormalNormalHandlerTestCase):\n\n    def test_trace_full(self):\n        guide_trace = poutine.trace(self.guide).get_trace()\n        model_trace = poutine.trace(self.model).get_trace()\n        for name in model_trace.nodes.keys():\n            assert name in self.model_sites\n\n        for name in guide_trace.nodes.keys():\n            assert name in self.guide_sites\n            assert guide_trace.nodes[name][""type""] in \\\n                (""args"", ""return"", ""sample"", ""param"")\n            if guide_trace.nodes[name][""type""] == ""sample"":\n                assert not guide_trace.nodes[name][""is_observed""]\n\n    def test_trace_return(self):\n        model_trace = poutine.trace(self.model).get_trace()\n        assert_equal(model_trace.nodes[""latent1""][""value""],\n                     model_trace.nodes[""_RETURN""][""value""])\n\n    def test_trace_param_only(self):\n        model_trace = poutine.trace(self.model, param_only=True).get_trace()\n        assert all(site[""type""] == ""param"" for site in model_trace.nodes.values())\n\n\nclass ReplayHandlerTests(NormalNormalNormalHandlerTestCase):\n\n    def test_replay_full(self):\n        guide_trace = poutine.trace(self.guide).get_trace()\n        model_trace = poutine.trace(poutine.replay(self.model, trace=guide_trace)).get_trace()\n        for name in self.full_sample_sites.keys():\n            assert_equal(model_trace.nodes[name][""value""],\n                         guide_trace.nodes[name][""value""])\n\n    def test_replay_full_repeat(self):\n        model_trace = poutine.trace(self.model).get_trace()\n        ftr = poutine.trace(poutine.replay(self.model, trace=model_trace))\n        tr11 = ftr.get_trace()\n        tr12 = ftr.get_trace()\n        tr2 = poutine.trace(poutine.replay(self.model, trace=model_trace)).get_trace()\n        for name in self.full_sample_sites.keys():\n            assert_equal(tr11.nodes[name][""value""], tr12.nodes[name][""value""])\n            assert_equal(tr11.nodes[name][""value""], tr2.nodes[name][""value""])\n            assert_equal(model_trace.nodes[name][""value""], tr11.nodes[name][""value""])\n            assert_equal(model_trace.nodes[name][""value""], tr2.nodes[name][""value""])\n\n\nclass BlockHandlerTests(NormalNormalNormalHandlerTestCase):\n\n    def test_block_hide_fn(self):\n        model_trace = poutine.trace(\n            poutine.block(self.model,\n                          hide_fn=lambda msg: ""latent"" in msg[""name""],\n                          expose=[""latent1""])\n        ).get_trace()\n        assert ""latent1"" not in model_trace\n        assert ""latent2"" not in model_trace\n        assert ""obs"" in model_trace\n\n    def test_block_expose_fn(self):\n        model_trace = poutine.trace(\n            poutine.block(self.model,\n                          expose_fn=lambda msg: ""latent"" in msg[""name""],\n                          hide=[""latent1""])\n        ).get_trace()\n        assert ""latent1"" in model_trace\n        assert ""latent2"" in model_trace\n        assert ""obs"" not in model_trace\n\n    def test_block_full(self):\n        model_trace = poutine.trace(poutine.block(self.model)).get_trace()\n        guide_trace = poutine.trace(poutine.block(self.guide)).get_trace()\n        for name in model_trace.nodes.keys():\n            assert model_trace.nodes[name][""type""] in (""args"", ""return"")\n        for name in guide_trace.nodes.keys():\n            assert guide_trace.nodes[name][""type""] in (""args"", ""return"")\n\n    def test_block_full_hide(self):\n        model_trace = poutine.trace(poutine.block(self.model,\n                                                  hide=self.model_sites)).get_trace()\n        guide_trace = poutine.trace(poutine.block(self.guide,\n                                                  hide=self.guide_sites)).get_trace()\n        for name in model_trace.nodes.keys():\n            assert model_trace.nodes[name][""type""] in (""args"", ""return"")\n        for name in guide_trace.nodes.keys():\n            assert guide_trace.nodes[name][""type""] in (""args"", ""return"")\n\n    def test_block_full_expose(self):\n        model_trace = poutine.trace(poutine.block(self.model,\n                                                  expose=self.model_sites)).get_trace()\n        guide_trace = poutine.trace(poutine.block(self.guide,\n                                                  expose=self.guide_sites)).get_trace()\n        for name in self.model_sites:\n            assert name in model_trace\n        for name in self.guide_sites:\n            assert name in guide_trace\n\n    def test_block_full_hide_expose(self):\n        try:\n            poutine.block(self.model,\n                          hide=self.partial_sample_sites.keys(),\n                          expose=self.partial_sample_sites.keys())()\n            assert False\n        except AssertionError:\n            assert True\n\n    def test_block_partial_hide(self):\n        model_trace = poutine.trace(\n            poutine.block(self.model, hide=self.partial_sample_sites.keys())).get_trace()\n        guide_trace = poutine.trace(\n            poutine.block(self.guide, hide=self.partial_sample_sites.keys())).get_trace()\n        for name in self.full_sample_sites.keys():\n            if name in self.partial_sample_sites:\n                assert name not in model_trace\n                assert name not in guide_trace\n            else:\n                assert name in model_trace\n                assert name in guide_trace\n\n    def test_block_partial_expose(self):\n        model_trace = poutine.trace(\n            poutine.block(self.model, expose=self.partial_sample_sites.keys())).get_trace()\n        guide_trace = poutine.trace(\n            poutine.block(self.guide, expose=self.partial_sample_sites.keys())).get_trace()\n        for name in self.full_sample_sites.keys():\n            if name in self.partial_sample_sites:\n                assert name in model_trace\n                assert name in guide_trace\n            else:\n                assert name not in model_trace\n                assert name not in guide_trace\n\n    def test_block_tutorial_case(self):\n        model_trace = poutine.trace(self.model).get_trace()\n        guide_trace = poutine.trace(\n            poutine.block(self.guide, hide_types=[""observe""])).get_trace()\n\n        assert ""latent1"" in model_trace\n        assert ""latent1"" in guide_trace\n        assert ""obs"" in model_trace\n        assert ""obs"" not in guide_trace\n\n\nclass QueueHandlerDiscreteTest(TestCase):\n\n    def setUp(self):\n\n        # simple Gaussian-mixture HMM\n        def model():\n            probs = pyro.param(""probs"", torch.tensor([[0.8], [0.3]]))\n            loc = pyro.param(""loc"", torch.tensor([[-0.1], [0.9]]))\n            scale = torch.ones(1, 1)\n\n            latents = [torch.ones(1)]\n            observes = []\n            for t in range(3):\n\n                latents.append(\n                    pyro.sample(""latent_{}"".format(str(t)),\n                                Bernoulli(probs[latents[-1][0].long().data])))\n\n                observes.append(\n                    pyro.sample(""observe_{}"".format(str(t)),\n                                Normal(loc[latents[-1][0].long().data], scale),\n                                obs=torch.ones(1)))\n            return latents\n\n        self.sites = [""observe_{}"".format(str(t)) for t in range(3)] + \\\n                     [""latent_{}"".format(str(t)) for t in range(3)] + \\\n                     [""_INPUT"", ""_RETURN""]\n        self.model = model\n        self.queue = Queue()\n        self.queue.put(poutine.Trace())\n\n    def test_queue_single(self):\n        f = poutine.trace(poutine.queue(self.model, queue=self.queue))\n        tr = f.get_trace()\n        for name in self.sites:\n            assert name in tr\n\n    def test_queue_enumerate(self):\n        f = poutine.trace(poutine.queue(self.model, queue=self.queue))\n        trs = []\n        while not self.queue.empty():\n            trs.append(f.get_trace())\n        assert len(trs) == 2 ** 3\n\n        true_latents = set()\n        for i1 in range(2):\n            for i2 in range(2):\n                for i3 in range(2):\n                    true_latents.add((i1, i2, i3))\n\n        tr_latents = []\n        for tr in trs:\n            tr_latents.append(tuple([int(tr.nodes[name][""value""].view(-1).item()) for name in tr\n                                     if tr.nodes[name][""type""] == ""sample"" and\n                                     not tr.nodes[name][""is_observed""]]))\n\n        assert true_latents == set(tr_latents)\n\n    def test_queue_max_tries(self):\n        f = poutine.queue(self.model, queue=self.queue, max_tries=3)\n        with pytest.raises(ValueError):\n            f()\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(2, 1)\n\n    def forward(self, x):\n        return self.fc(x)\n\n\nclass LiftHandlerTests(TestCase):\n\n    def setUp(self):\n        pyro.clear_param_store()\n\n        def loc1_prior(tensor, *args, **kwargs):\n            flat_tensor = tensor.view(-1)\n            m = torch.zeros(flat_tensor.size(0))\n            s = torch.ones(flat_tensor.size(0))\n            return Normal(m, s).sample().view(tensor.size())\n\n        def scale1_prior(tensor, *args, **kwargs):\n            flat_tensor = tensor.view(-1)\n            m = torch.zeros(flat_tensor.size(0))\n            s = torch.ones(flat_tensor.size(0))\n            return Normal(m, s).sample().view(tensor.size()).exp()\n\n        def loc2_prior(tensor, *args, **kwargs):\n            flat_tensor = tensor.view(-1)\n            m = torch.zeros(flat_tensor.size(0))\n            return Bernoulli(m).sample().view(tensor.size())\n\n        def scale2_prior(tensor, *args, **kwargs):\n            return scale1_prior(tensor)\n\n        def bias_prior(tensor, *args, **kwargs):\n            return loc2_prior(tensor)\n\n        def weight_prior(tensor, *args, **kwargs):\n            return scale1_prior(tensor)\n\n        def stoch_fn(tensor, *args, **kwargs):\n            loc = torch.zeros(tensor.size())\n            scale = torch.ones(tensor.size())\n            return pyro.sample(""sample"", Normal(loc, scale))\n\n        def guide():\n            loc1 = pyro.param(""loc1"", torch.randn(2, requires_grad=True))\n            scale1 = pyro.param(""scale1"", torch.ones(2, requires_grad=True))\n            pyro.sample(""latent1"", Normal(loc1, scale1))\n\n            loc2 = pyro.param(""loc2"", torch.randn(2, requires_grad=True))\n            scale2 = pyro.param(""scale2"", torch.ones(2, requires_grad=True))\n            latent2 = pyro.sample(""latent2"", Normal(loc2, scale2))\n            return latent2\n\n        def dup_param_guide():\n            a = pyro.param(""loc"")\n            b = pyro.param(""loc"")\n            assert a == b\n\n        self.model = Model()\n        self.guide = guide\n        self.dup_param_guide = dup_param_guide\n        self.prior = scale1_prior\n        self.prior_dict = {""loc1"": loc1_prior, ""scale1"": scale1_prior, ""loc2"": loc2_prior, ""scale2"": scale2_prior}\n        self.partial_dict = {""loc1"": loc1_prior, ""scale1"": scale1_prior}\n        self.nn_prior = {""fc.bias"": bias_prior, ""fc.weight"": weight_prior}\n        self.fn = stoch_fn\n        self.data = torch.randn(2, 2)\n\n    def test_splice(self):\n        tr = poutine.trace(self.guide).get_trace()\n        lifted_tr = poutine.trace(poutine.lift(self.guide, prior=self.prior)).get_trace()\n        for name in tr.nodes.keys():\n            if name in (\'loc1\', \'loc2\', \'scale1\', \'scale2\'):\n                assert name not in lifted_tr\n            else:\n                assert name in lifted_tr\n\n    def test_memoize(self):\n        poutine.trace(poutine.lift(self.dup_param_guide, prior=dist.Normal(0, 1)))()\n\n    def test_prior_dict(self):\n        tr = poutine.trace(self.guide).get_trace()\n        lifted_tr = poutine.trace(poutine.lift(self.guide, prior=self.prior_dict)).get_trace()\n        for name in tr.nodes.keys():\n            assert name in lifted_tr\n            if name in {\'scale1\', \'loc1\', \'scale2\', \'loc2\'}:\n                assert name + ""_prior"" == lifted_tr.nodes[name][\'fn\'].__name__\n            if tr.nodes[name][""type""] == ""param"":\n                assert lifted_tr.nodes[name][""type""] == ""sample""\n                assert not lifted_tr.nodes[name][""is_observed""]\n\n    def test_unlifted_param(self):\n        tr = poutine.trace(self.guide).get_trace()\n        lifted_tr = poutine.trace(poutine.lift(self.guide, prior=self.partial_dict)).get_trace()\n        for name in tr.nodes.keys():\n            assert name in lifted_tr\n            if name in (\'scale1\', \'loc1\'):\n                assert name + ""_prior"" == lifted_tr.nodes[name][\'fn\'].__name__\n                assert lifted_tr.nodes[name][""type""] == ""sample""\n                assert not lifted_tr.nodes[name][""is_observed""]\n            if name in (\'scale2\', \'loc2\'):\n                assert lifted_tr.nodes[name][""type""] == ""param""\n\n    @pytest.mark.filterwarnings(\'ignore::FutureWarning\')\n    def test_random_module(self):\n        pyro.clear_param_store()\n        with pyro.validation_enabled():\n            lifted_tr = poutine.trace(pyro.random_module(""name"", self.model, prior=self.prior)).get_trace()\n        for name in lifted_tr.nodes.keys():\n            if lifted_tr.nodes[name][""type""] == ""param"":\n                assert lifted_tr.nodes[name][""type""] == ""sample""\n                assert not lifted_tr.nodes[name][""is_observed""]\n\n    @pytest.mark.filterwarnings(\'ignore::FutureWarning\')\n    def test_random_module_warn(self):\n        pyro.clear_param_store()\n        bad_prior = {\'foo\': None}\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(""always"")\n            with pyro.validation_enabled():\n                poutine.trace(pyro.random_module(""name"", self.model, prior=bad_prior)).get_trace()\n            assert len(w), \'No warnings were raised\'\n            for warning in w:\n                logger.info(warning)\n\n    @pytest.mark.filterwarnings(\'ignore::FutureWarning\')\n    def test_random_module_prior_dict(self):\n        pyro.clear_param_store()\n        lifted_nn = pyro.random_module(""name"", self.model, prior=self.nn_prior)\n        lifted_tr = poutine.trace(lifted_nn).get_trace()\n        for key_name in lifted_tr.nodes.keys():\n            name = pyro.params.user_param_name(key_name)\n            if name in {\'fc.weight\', \'fc.prior\'}:\n                dist_name = name[3:]\n                assert dist_name + ""_prior"" == lifted_tr.nodes[key_name][\'fn\'].__name__\n                assert lifted_tr.nodes[key_name][""type""] == ""sample""\n                assert not lifted_tr.nodes[key_name][""is_observed""]\n\n\nclass QueueHandlerMixedTest(TestCase):\n\n    def setUp(self):\n\n        # Simple model with 1 continuous + 1 discrete + 1 continuous variable.\n        def model():\n            p = torch.tensor([0.5])\n            loc = torch.zeros(1)\n            scale = torch.ones(1)\n\n            x = pyro.sample(""x"", Normal(loc, scale))  # Before the discrete variable.\n            y = pyro.sample(""y"", Bernoulli(p))\n            z = pyro.sample(""z"", Normal(loc, scale))  # After the discrete variable.\n            return dict(x=x, y=y, z=z)\n\n        self.sites = [""x"", ""y"", ""z"", ""_INPUT"", ""_RETURN""]\n        self.model = model\n        self.queue = Queue()\n        self.queue.put(poutine.Trace())\n\n    def test_queue_single(self):\n        f = poutine.trace(poutine.queue(self.model, queue=self.queue))\n        tr = f.get_trace()\n        for name in self.sites:\n            assert name in tr\n\n    def test_queue_enumerate(self):\n        f = poutine.trace(poutine.queue(self.model, queue=self.queue))\n        trs = []\n        while not self.queue.empty():\n            trs.append(f.get_trace())\n        assert len(trs) == 2\n\n        values = [\n            {name: tr.nodes[name][\'value\'].view(-1).item() for name in tr.nodes.keys()\n             if tr.nodes[name][\'type\'] == \'sample\'}\n            for tr in trs\n        ]\n\n        expected_ys = set([0, 1])\n        actual_ys = set([value[""y""] for value in values])\n        assert actual_ys == expected_ys\n\n        # Check that x was sampled the same on all each paths.\n        assert values[0][""x""] == values[1][""x""]\n\n        # Check that y was sampled differently on each path.\n        assert values[0][""z""] != values[1][""z""]  # Almost surely true.\n\n\nclass IndirectLambdaHandlerTests(TestCase):\n\n    def setUp(self):\n\n        def model(batch_size_outer=2, batch_size_inner=2):\n            data = [[torch.ones(1)] * 2] * 2\n            loc_latent = pyro.sample(""loc_latent"", dist.Normal(torch.zeros(1), torch.ones(1)))\n            for i in pyro.plate(""plate_outer"", 2, batch_size_outer):\n                for j in pyro.plate(""plate_inner_%d"" % i, 2, batch_size_inner):\n                    pyro.sample(""z_%d_%d"" % (i, j), dist.Normal(loc_latent + data[i][j], torch.ones(1)))\n\n        self.model = model\n        self.expected_nodes = set([""z_0_0"", ""z_0_1"", ""z_1_0"", ""z_1_1"", ""loc_latent"",\n                                   ""_INPUT"", ""_RETURN""])\n        self.expected_edges = set([\n            (""loc_latent"", ""z_0_0""), (""loc_latent"", ""z_0_1""),\n            (""loc_latent"", ""z_1_0""), (""loc_latent"", ""z_1_1""),\n        ])\n\n    def test_graph_structure(self):\n        tracegraph = poutine.trace(self.model, graph_type=""dense"").get_trace()\n        # Ignore structure on plate_* nodes.\n        actual_nodes = set(n for n in tracegraph.nodes if not n.startswith(""plate_""))\n        actual_edges = set((n1, n2) for n1, n2 in tracegraph.edges\n                           if not n1.startswith(""plate_"") if not n2.startswith(""plate_""))\n        assert actual_nodes == self.expected_nodes\n        assert actual_edges == self.expected_edges\n\n    def test_scale_factors(self):\n        def _test_scale_factor(batch_size_outer, batch_size_inner, expected):\n            trace = poutine.trace(self.model, graph_type=""dense"").get_trace(batch_size_outer=batch_size_outer,\n                                                                            batch_size_inner=batch_size_inner)\n            scale_factors = []\n            for node in [\'z_0_0\', \'z_0_1\', \'z_1_0\', \'z_1_1\']:\n                if node in trace:\n                    scale_factors.append(trace.nodes[node][\'scale\'])\n            assert scale_factors == expected\n\n        _test_scale_factor(1, 1, [4.0])\n        _test_scale_factor(2, 2, [1.0] * 4)\n        _test_scale_factor(1, 2, [2.0] * 2)\n        _test_scale_factor(2, 1, [2.0] * 2)\n\n\nclass ConditionHandlerTests(NormalNormalNormalHandlerTestCase):\n\n    def test_condition(self):\n        data = {""latent2"": torch.randn(2)}\n        tr2 = poutine.trace(poutine.condition(self.model, data=data)).get_trace()\n        assert ""latent2"" in tr2\n        assert tr2.nodes[""latent2""][""type""] == ""sample"" and \\\n            tr2.nodes[""latent2""][""is_observed""]\n        assert tr2.nodes[""latent2""][""value""] is data[""latent2""]\n\n    def test_trace_data(self):\n        tr1 = poutine.trace(\n            poutine.block(self.model, expose_types=[""sample""])).get_trace()\n        tr2 = poutine.trace(\n            poutine.condition(self.model, data=tr1)).get_trace()\n        assert tr2.nodes[""latent2""][""type""] == ""sample"" and \\\n            tr2.nodes[""latent2""][""is_observed""]\n        assert tr2.nodes[""latent2""][""value""] is tr1.nodes[""latent2""][""value""]\n\n    def test_stack_overwrite_behavior(self):\n        data1 = {""latent2"": torch.randn(2)}\n        data2 = {""latent2"": torch.randn(2)}\n        with poutine.trace() as tr:\n            cm = poutine.condition(poutine.condition(self.model, data=data1),\n                                   data=data2)\n            cm()\n        assert tr.trace.nodes[\'latent2\'][\'value\'] is data2[\'latent2\']\n\n    def test_stack_success(self):\n        data1 = {""latent1"": torch.randn(2)}\n        data2 = {""latent2"": torch.randn(2)}\n        tr = poutine.trace(\n            poutine.condition(poutine.condition(self.model, data=data1),\n                              data=data2)).get_trace()\n        assert tr.nodes[""latent1""][""type""] == ""sample"" and \\\n            tr.nodes[""latent1""][""is_observed""]\n        assert tr.nodes[""latent1""][""value""] is data1[""latent1""]\n        assert tr.nodes[""latent2""][""type""] == ""sample"" and \\\n            tr.nodes[""latent2""][""is_observed""]\n        assert tr.nodes[""latent2""][""value""] is data2[""latent2""]\n\n\nclass UnconditionHandlerTests(NormalNormalNormalHandlerTestCase):\n\n    def test_uncondition(self):\n        unconditioned_model = poutine.uncondition(self.model)\n        unconditioned_trace = poutine.trace(unconditioned_model).get_trace()\n        conditioned_trace = poutine.trace(self.model).get_trace()\n        assert_equal(conditioned_trace.nodes[""obs""][""value""], torch.ones(2))\n        assert_not_equal(unconditioned_trace.nodes[""obs""][""value""], torch.ones(2))\n\n    def test_undo_uncondition(self):\n        unconditioned_model = poutine.uncondition(self.model)\n        reconditioned_model = pyro.condition(unconditioned_model, {""obs"": torch.ones(2)})\n        reconditioned_trace = poutine.trace(reconditioned_model).get_trace()\n        assert_equal(reconditioned_trace.nodes[""obs""][""value""], torch.ones(2))\n\n\nclass EscapeHandlerTests(TestCase):\n\n    def setUp(self):\n\n        # Simple model with 1 continuous + 1 discrete + 1 continuous variable.\n        def model():\n            p = torch.tensor([0.5])\n            loc = torch.zeros(1)\n            scale = torch.ones(1)\n\n            x = pyro.sample(""x"", Normal(loc, scale))  # Before the discrete variable.\n            y = pyro.sample(""y"", Bernoulli(p))\n            z = pyro.sample(""z"", Normal(loc, scale))  # After the discrete variable.\n            return dict(x=x, y=y, z=z)\n\n        self.sites = [""x"", ""y"", ""z"", ""_INPUT"", ""_RETURN""]\n        self.model = model\n\n    def test_discrete_escape(self):\n        try:\n            poutine.escape(self.model,\n                           escape_fn=functools.partial(discrete_escape,\n                                                       poutine.Trace()))()\n            assert False\n        except NonlocalExit as e:\n            assert e.site[""name""] == ""y""\n\n    def test_all_escape(self):\n        try:\n            poutine.escape(self.model,\n                           escape_fn=functools.partial(all_escape,\n                                                       poutine.Trace()))()\n            assert False\n        except NonlocalExit as e:\n            assert e.site[""name""] == ""x""\n\n    def test_trace_compose(self):\n        tm = poutine.trace(self.model)\n        try:\n            poutine.escape(tm,\n                           escape_fn=functools.partial(all_escape,\n                                                       poutine.Trace()))()\n            assert False\n        except NonlocalExit:\n            assert ""x"" in tm.trace\n            try:\n                tem = poutine.trace(\n                    poutine.escape(self.model,\n                                   escape_fn=functools.partial(all_escape,\n                                                               poutine.Trace())))\n                tem()\n                assert False\n            except NonlocalExit:\n                assert ""x"" not in tem.trace\n\n\nclass InferConfigHandlerTests(TestCase):\n    def setUp(self):\n        def model():\n            pyro.param(""p"", torch.zeros(1, requires_grad=True))\n            pyro.sample(""a"", Bernoulli(torch.tensor([0.5])),\n                        infer={""enumerate"": ""parallel""})\n            pyro.sample(""b"", Bernoulli(torch.tensor([0.5])))\n\n        self.model = model\n\n        def config_fn(site):\n            if site[""type""] == ""sample"":\n                return {""blah"": True}\n            else:\n                return {}\n\n        self.config_fn = config_fn\n\n    def test_infer_config_sample(self):\n        cfg_model = poutine.infer_config(self.model, config_fn=self.config_fn)\n\n        tr = poutine.trace(cfg_model).get_trace()\n\n        assert tr.nodes[""a""][""infer""] == {""enumerate"": ""parallel"", ""blah"": True}\n        assert tr.nodes[""b""][""infer""] == {""blah"": True}\n        assert tr.nodes[""p""][""infer""] == {}\n\n\n@pytest.mark.parametrize(\'first_available_dim\', [-1, -2, -3])\n@pytest.mark.parametrize(\'depth\', [0, 1, 2])\ndef test_enumerate_poutine(depth, first_available_dim):\n    num_particles = 2\n\n    def model():\n        pyro.sample(""x"", Bernoulli(0.5))\n        for i in range(depth):\n            pyro.sample(""a_{}"".format(i), Bernoulli(0.5), infer={""enumerate"": ""parallel""})\n\n    model = poutine.enum(model, first_available_dim=first_available_dim)\n    model = poutine.trace(model)\n\n    for i in range(num_particles):\n        tr = model.get_trace()\n        tr.compute_log_prob()\n        log_prob = sum(site[""log_prob""] for name, site in tr.iter_stochastic_nodes())\n        actual_shape = log_prob.shape\n        expected_shape = (2,) * depth\n        if depth:\n            expected_shape = expected_shape + (1,) * (-1 - first_available_dim)\n        assert actual_shape == expected_shape, \'error on iteration {}\'.format(i)\n\n\n@pytest.mark.parametrize(\'first_available_dim\', [-1, -2, -3])\n@pytest.mark.parametrize(\'depth\', [0, 1, 2])\ndef test_replay_enumerate_poutine(depth, first_available_dim):\n    num_particles = 2\n    y_dist = Categorical(torch.tensor([0.5, 0.25, 0.25]))\n\n    def guide():\n        pyro.sample(""y"", y_dist, infer={""enumerate"": ""parallel""})\n\n    guide = poutine.enum(guide, first_available_dim=first_available_dim - depth)\n    guide = poutine.trace(guide)\n    guide_trace = guide.get_trace()\n\n    def model():\n        pyro.sample(""x"", Bernoulli(0.5))\n        for i in range(depth):\n            pyro.sample(""a_{}"".format(i), Bernoulli(0.5), infer={""enumerate"": ""parallel""})\n        pyro.sample(""y"", y_dist, infer={""enumerate"": ""parallel""})\n        for i in range(depth):\n            pyro.sample(""b_{}"".format(i), Bernoulli(0.5), infer={""enumerate"": ""parallel""})\n\n    model = poutine.enum(model, first_available_dim=first_available_dim)\n    model = poutine.replay(model, trace=guide_trace)\n    model = poutine.trace(model)\n\n    for i in range(num_particles):\n        tr = model.get_trace()\n        assert tr.nodes[""y""][""value""] is guide_trace.nodes[""y""][""value""]\n        tr.compute_log_prob()\n        log_prob = sum(site[""log_prob""] for name, site in tr.iter_stochastic_nodes())\n        actual_shape = log_prob.shape\n        expected_shape = (2,) * depth + (3,) + (2,) * depth + (1,) * (-1 - first_available_dim)\n        assert actual_shape == expected_shape, \'error on iteration {}\'.format(i)\n\n\n@pytest.mark.parametrize(""has_rsample"", [False, True])\n@pytest.mark.parametrize(""depth"", [0, 1, 2])\ndef test_plate_preserves_has_rsample(has_rsample, depth):\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        with pyro.plate_stack(""plates"", (2,) * depth):\n            return pyro.sample(""x"", dist.Normal(loc, 1).has_rsample_(has_rsample))\n\n    x = guide()\n    assert x.dim() == depth\n    assert x.requires_grad == has_rsample\n\n\ndef test_plate_error_on_enter():\n    def model():\n        with pyro.plate(\'foo\', 0):\n            pass\n\n    assert len(_DIM_ALLOCATOR._stack) == 0\n    with pytest.raises(ZeroDivisionError):\n        poutine.trace(model)()\n    assert len(_DIM_ALLOCATOR._stack) == 0, \'stack was not cleaned on error\'\n\n\ndef test_decorator_interface_primitives():\n\n    @poutine.trace\n    def model():\n        pyro.param(""p"", torch.zeros(1, requires_grad=True))\n        pyro.sample(""a"", Bernoulli(torch.tensor([0.5])),\n                    infer={""enumerate"": ""parallel""})\n        pyro.sample(""b"", Bernoulli(torch.tensor([0.5])))\n\n    tr = model.get_trace()\n    assert isinstance(tr, poutine.Trace)\n    assert tr.graph_type == ""flat""\n\n    @poutine.trace(graph_type=""dense"")\n    def model():\n        pyro.param(""p"", torch.zeros(1, requires_grad=True))\n        pyro.sample(""a"", Bernoulli(torch.tensor([0.5])),\n                    infer={""enumerate"": ""parallel""})\n        pyro.sample(""b"", Bernoulli(torch.tensor([0.5])))\n\n    tr = model.get_trace()\n    assert isinstance(tr, poutine.Trace)\n    assert tr.graph_type == ""dense""\n\n    tr2 = poutine.trace(poutine.replay(model, trace=tr)).get_trace()\n\n    assert_equal(tr2.nodes[""a""][""value""], tr.nodes[""a""][""value""])\n\n\ndef test_decorator_interface_queue():\n\n    sites = [""x"", ""y"", ""z"", ""_INPUT"", ""_RETURN""]\n    queue = Queue()\n    queue.put(poutine.Trace())\n\n    @poutine.queue(queue=queue)\n    def model():\n        p = torch.tensor([0.5])\n        loc = torch.zeros(1)\n        scale = torch.ones(1)\n\n        x = pyro.sample(""x"", Normal(loc, scale))\n        y = pyro.sample(""y"", Bernoulli(p))\n        z = pyro.sample(""z"", Normal(loc, scale))\n        return dict(x=x, y=y, z=z)\n\n    tr = poutine.trace(model).get_trace()\n    for name in sites:\n        assert name in tr\n\n\ndef test_method_decorator_interface_condition():\n\n    class cls_model:\n\n        @poutine.condition(data={""b"": torch.tensor(1.)})\n        def model(self, p):\n            self._model(p)\n\n        def _model(self, p):\n            pyro.sample(""a"", Bernoulli(p))\n            pyro.sample(""b"", Bernoulli(torch.tensor([0.5])))\n\n    tr = poutine.trace(cls_model().model).get_trace(0.5)\n    assert isinstance(tr, poutine.Trace)\n    assert tr.graph_type == ""flat""\n    assert tr.nodes[""b""][""is_observed""] and tr.nodes[""b""][""value""].item() == 1.\n\n\ndef test_trace_log_prob_err_msg():\n    def model(v):\n        pyro.sample(""test_site"", dist.Beta(1., 1.), obs=v)\n\n    tr = poutine.trace(model).get_trace(torch.tensor(2.))\n    exp_msg = r""Error while computing log_prob at site \'test_site\':\\s*"" \\\n              r""The value argument must be within the support""\n    with pytest.raises(ValueError, match=exp_msg):\n        tr.compute_log_prob()\n\n\ndef test_trace_log_prob_sum_err_msg():\n    def model(v):\n        pyro.sample(""test_site"", dist.Beta(1., 1.), obs=v)\n\n    tr = poutine.trace(model).get_trace(torch.tensor(2.))\n    exp_msg = r""Error while computing log_prob_sum at site \'test_site\':\\s*"" \\\n              r""The value argument must be within the support""\n    with pytest.raises(ValueError, match=exp_msg):\n        tr.log_prob_sum()\n\n\ndef test_trace_score_parts_err_msg():\n    def guide(v):\n        pyro.sample(""test_site"", dist.Beta(1., 1.), obs=v)\n\n    tr = poutine.trace(guide).get_trace(torch.tensor(2.))\n    exp_msg = r""Error while computing score_parts at site \'test_site\':\\s*"" \\\n              r""The value argument must be within the support""\n    with pytest.raises(ValueError, match=exp_msg):\n        tr.compute_score_parts()\n\n\ndef _model(a=torch.tensor(1.), b=torch.tensor(1.)):\n    latent = pyro.sample(""latent"", dist.Beta(a, b))\n    return pyro.sample(""test_site"", dist.Bernoulli(latent), obs=torch.tensor(1))\n\n\n@pytest.mark.parametrize(\'wrapper\', [\n    lambda fn: poutine.block(fn),\n    lambda fn: poutine.condition(fn, {\'latent\': 0.9}),\n    lambda fn: poutine.enum(fn, -1),\n    lambda fn: poutine.replay(fn, poutine.trace(fn).get_trace()),\n])\ndef test_pickling(wrapper):\n    wrapped = wrapper(_model)\n    buffer = io.BytesIO()\n    # default protocol cannot serialize torch.Size objects (see https://github.com/pytorch/pytorch/issues/20823)\n    torch.save(wrapped, buffer, pickle_protocol=pickle.HIGHEST_PROTOCOL)\n    buffer.seek(0)\n    deserialized = torch.load(buffer)\n    obs = torch.tensor(0.5)\n    pyro.set_rng_seed(0)\n    actual_trace = poutine.trace(deserialized).get_trace(obs)\n    pyro.set_rng_seed(0)\n    expected_trace = poutine.trace(wrapped).get_trace(obs)\n    assert tuple(actual_trace) == tuple(expected_trace.nodes)\n    assert_close([actual_trace.nodes[site][\'value\'] for site in actual_trace.stochastic_nodes],\n                 [expected_trace.nodes[site][\'value\'] for site in expected_trace.stochastic_nodes])\n\n\ndef test_arg_kwarg_error():\n\n    def model():\n        pyro.param(""p"", torch.zeros(1, requires_grad=True))\n        pyro.sample(""a"", Bernoulli(torch.tensor([0.5])),\n                    infer={""enumerate"": ""parallel""})\n        pyro.sample(""b"", Bernoulli(torch.tensor([0.5])))\n\n    with pytest.raises(ValueError, match=""not callable""):\n        with poutine.mask(False):\n            model()\n\n    with poutine.mask(mask=False):\n        model()\n'"
tests/poutine/test_properties.py,17,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.util import set_rng_seed\nfrom tests.common import assert_equal\n\nEXAMPLE_MODELS = []\nEXAMPLE_MODEL_IDS = []\n\n\nclass ExampleModel:\n    def __init__(self, fn, poutine_kwargs):\n        self.fn = fn\n        self.poutine_kwargs = poutine_kwargs\n\n    def __call__(self, *args, **kwargs):\n        return self.fn(*args, **kwargs)\n\n    def bind_poutine(self, poutine_name):\n        """"""\n        Bind model-specific kwargs to the poutine.\n        """"""\n        p = getattr(poutine, poutine_name)\n        kwargs = self.poutine_kwargs.get(poutine_name, {})\n        return lambda fn: p(fn, **kwargs)\n\n\ndef register_model(**poutine_kwargs):\n    """"""\n    Decorator to register a model as an example model for testing.\n    """"""\n\n    def register_fn(fn):\n        model = ExampleModel(fn, poutine_kwargs)\n        EXAMPLE_MODELS.append(model)\n        EXAMPLE_MODEL_IDS.append(model.fn.__name__)\n        return model\n\n    return register_fn\n\n\n@register_model(replay={\'trace\': poutine.Trace()},\n                block={},\n                condition={\'data\': {}},\n                do={\'data\': {}})\ndef trivial_model():\n    return []\n\n\ntr_normal = poutine.Trace()\ntr_normal.add_node(""normal_0"", type=""sample"", is_observed=False, value=torch.zeros(1), infer={})\n\n\n@register_model(replay={\'trace\': tr_normal},\n                block={\'hide\': [\'normal_0\']},\n                condition={\'data\': {\'normal_0\': torch.zeros(1)}},\n                do={\'data\': {\'normal_0\': torch.zeros(1)}})\ndef normal_model():\n    normal_0 = pyro.sample(\'normal_0\', dist.Normal(torch.zeros(1), torch.ones(1)))\n    return [normal_0]\n\n\ntr_normal_normal = poutine.Trace()\ntr_normal_normal.add_node(""normal_0"", type=""sample"", is_observed=False, value=torch.zeros(1), infer={})\n\n\n@register_model(replay={\'trace\': tr_normal_normal},\n                block={\'hide\': [\'normal_0\']},\n                condition={\'data\': {\'normal_0\': torch.zeros(1)}},\n                do={\'data\': {\'normal_0\': torch.zeros(1)}})\ndef normal_normal_model():\n    normal_0 = pyro.sample(\'normal_0\', dist.Normal(torch.zeros(1), torch.ones(1)))\n    normal_1 = torch.ones(1)\n    pyro.sample(\'normal_1\', dist.Normal(normal_0, torch.ones(1)),\n                obs=normal_1)\n    return [normal_0, normal_1]\n\n\ntr_bernoulli_normal = poutine.Trace()\ntr_bernoulli_normal.add_node(""bern_0"", type=""sample"", is_observed=False, value=torch.ones(1), infer={})\n\n\n@register_model(replay={\'trace\': tr_bernoulli_normal},\n                block={\'hide\': [\'bern_0\']},\n                condition={\'data\': {\'bern_0\': torch.ones(1)}},\n                do={\'data\': {\'bern_0\': torch.ones(1)}})\ndef bernoulli_normal_model():\n    bern_0 = pyro.sample(\'bern_0\', dist.Bernoulli(torch.zeros(1) * 1e-2))\n    loc = torch.ones(1) if bern_0.item() else -torch.ones(1)\n    normal_0 = torch.ones(1)\n    pyro.sample(\'normal_0\', dist.Normal(loc, torch.ones(1) * 1e-2),\n                obs=normal_0)\n    return [bern_0, normal_0]\n\n\ndef get_trace(fn, *args, **kwargs):\n    set_rng_seed(123)\n    return poutine.trace(fn).get_trace(*args, **kwargs)\n\n\n@pytest.mark.parametrize(\'model\', EXAMPLE_MODELS, ids=EXAMPLE_MODEL_IDS)\n@pytest.mark.parametrize(\'poutine_name\', [\n    \'block\',\n    \'replay\',\n    \'trace\',\n])\ndef test_idempotent(poutine_name, model):\n    p = model.bind_poutine(poutine_name)\n    expected_trace = get_trace(p(model))\n    actual_trace = get_trace(p(p(model)))\n    assert_equal(actual_trace, expected_trace, prec=0)\n\n\n@pytest.mark.parametrize(\'model\', EXAMPLE_MODELS, ids=EXAMPLE_MODEL_IDS)\n@pytest.mark.parametrize(\'p1_name,p2_name\', [\n    (\'trace\', \'condition\'),\n    (\'trace\', \'do\'),\n    (\'trace\', \'replay\'),\n])\ndef test_commutes(p1_name, p2_name, model):\n    p1 = model.bind_poutine(p1_name)\n    p2 = model.bind_poutine(p2_name)\n    expected_trace = get_trace(p1(p2(model)))\n    actual_trace = get_trace(p2(p1(model)))\n    assert_equal(actual_trace, expected_trace, prec=0)\n'"
tests/poutine/test_trace_struct.py,0,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\n\nimport pytest\n\nfrom pyro.poutine import Trace\nfrom tests.common import assert_equal\n\n\nEDGE_SETS = [\n    #   1\n    #  / \\\n    # 2   3\n    #    / \\\n    #   4   5\n    #  / \\\n    # 6   7\n    [(1, 2), (1, 3), (3, 4), (3, 5), (4, 6), (4, 7)],\n    #   1\n    #  /|\\\n    # 2 3 4\n    #   |\n    #   5\n    #   |\n    #   6\n    #   |\n    #   7\n    [(1, 2), (3, 5), (1, 4), (1, 3), (5, 6), (6, 7)],\n]\n\n\n@pytest.mark.parametrize('edges', [\n    perm for edges in EDGE_SETS\n    for perm in itertools.permutations(edges)\n])\ndef test_topological_sort(edges):\n    tr = Trace()\n    for n1, n2 in edges:\n        tr.add_edge(n1, n2)\n    top_sort = tr.topological_sort()\n\n    # check all nodes are accounted for exactly once\n    expected_nodes = set().union(*edges)\n    assert len(top_sort) == len(expected_nodes)\n    assert set(top_sort) == expected_nodes\n\n    # check no edge ordering is violated\n    ranks = {n: rank for rank, n in enumerate(top_sort)}\n    for n1, n2 in edges:\n        assert ranks[n1] < ranks[n2]\n\n\n@pytest.mark.parametrize('edges', [\n    perm for edges in EDGE_SETS\n    for perm in itertools.permutations(edges)\n])\ndef test_connectivity_on_removal(edges):\n    # check that when nodes are removed in reverse topological order\n    # connectivity of the DAG is maintained, i.e. remaining nodes\n    # are reachable from the root.\n    root = 1\n    tr = Trace()\n    for e1, e2 in edges:\n        tr.add_edge(e1, e2)\n    top_sort = tr.topological_sort()\n    while top_sort:\n        num_nodes = len([n for n in tr._dfs(root, set())])\n        num_expected = len(top_sort)\n        assert_equal(num_nodes, num_expected)\n        tr.remove_node(top_sort.pop())\n"""
tests/pyroapi/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_runtest_call(item):\n    try:\n        item.runtest()\n    except NotImplementedError as e:\n        pytest.xfail(str(e))\n'"
tests/pyroapi/test_pyroapi.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\nfrom pyroapi import pyro_backend\nfrom pyroapi.tests import *  # noqa F401\n\npytestmark = pytest.mark.stage(\'unit\')\n\n\n@pytest.fixture(params=[""pyro"", ""minipyro""])\ndef backend(request):\n    with pyro_backend(request.param):\n        yield\n'"
tutorial/source/cleannb.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport io  # for py2/py3 compatible\n\nimport nbformat\n\n\ndef cleannb(nbfile):\n    with io.open(nbfile, ""r"", encoding=""utf8"") as f:\n        nb = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n\n    nb[""metadata""][""kernelspec""][""display_name""] = ""Python 3""\n    nb[""metadata""][""kernelspec""][""name""] = ""python3""\n    nb[""metadata""][""language_info""][""codemirror_mode""][""version""] = 3\n    nb[""metadata""][""language_info""][""pygments_lexer""] = ""ipython3""\n    nb[""metadata""][""language_info""][""version""] = ""3.6.10""\n\n    with io.open(nbfile, ""w"", encoding=""utf8"") as f:\n        nbformat.write(nb, f)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Clean kernelspec metadata of a notebook"")\n    parser.add_argument(""nbfiles"", nargs=""*"", help=""Files to clean kernelspec metadata"")\n    args = parser.parse_args()\n\n    for nbfile in args.nbfiles:\n        cleannb(nbfile)\n'"
tutorial/source/conf.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport sphinx_rtd_theme\nfrom pyro import __version__\n\n# -*- coding: utf-8 -*-\n#\n# Pyro Tutorials documentation build configuration file, created by\n# sphinx-quickstart on Tue Oct 31 11:33:17 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.intersphinx\',\n              \'sphinx.ext.todo\',\n              \'sphinx.ext.mathjax\',\n              \'sphinx.ext.githubpages\',\n              \'nbsphinx\',\n              \'sphinx.ext.autodoc\'\n              ]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = [\'.rst\', \'.ipynb\']\n\n# do not execute cells\nnbsphinx_execute = \'never\'\n\n# allow errors because not all tutorials build\nnbsphinx_allow_errors = True\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Pyro Tutorials\'\ncopyright = u\'2017-2018, Uber Technologies, Inc\'\nauthor = u\'Uber AI Labs\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n\nversion = __version__\n\n# release version\nrelease = version  # eg pyro 0.1.2\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'.ipynb_checkpoints\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# extend timeout\nnbsphinx_timeout = 120\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n# logo\nhtml_logo = \'_static/img/pyro_logo_wide.png\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'logo_only\': True\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\nhtml_style = \'css/pyro.css\'\n\n# html_favicon = \'../img/favicon/favicon.ico\'\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'PyroTutorialsdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'PyroTutorials.tex\', u\'Pyro Examples and Tutorials\',\n     u\'Uber AI Labs\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'pyrotutorials\', u\'Pyro Examples and Tutorials\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'PyroTutorials\', u\'Pyro Examples and Tutorials\',\n     author, \'PyroTutorials\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
tutorial/source/search_inference.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nInference algorithms and utilities used in the RSA example models.\n\nAdapted from: http://dippl.org/chapters/03-enumeration.html\n""""""\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\n\nfrom pyro.distributions.util import logsumexp\nfrom pyro.infer.abstract_infer import TracePosterior\nfrom pyro.poutine.runtime import NonlocalExit\n\nimport queue\nimport collections\nimport functools\n\n\ndef memoize(fn=None, **kwargs):\n    if fn is None:\n        return lambda _fn: memoize(_fn, **kwargs)\n    return functools.lru_cache(**kwargs)(fn)\n\n\ndef factor(name, value):\n    """"""\n    Like factor in webPPL, adds a scalar weight to the log-probability of the trace\n    """"""\n    value = value if torch.is_tensor(value) else torch.tensor(value)\n    d = dist.Bernoulli(logits=value)\n    pyro.sample(name, d, obs=torch.ones(value.size()))\n\n\nclass HashingMarginal(dist.Distribution):\n    """"""\n    :param trace_dist: a TracePosterior instance representing a Monte Carlo posterior\n\n    Marginal histogram distribution.\n    Turns a TracePosterior object into a Distribution\n    over the return values of the TracePosterior\'s model.\n    """"""\n    def __init__(self, trace_dist, sites=None):\n        assert isinstance(trace_dist, TracePosterior), \\\n            ""trace_dist must be trace posterior distribution object""\n\n        if sites is None:\n            sites = ""_RETURN""\n\n        assert isinstance(sites, (str, list)), \\\n            ""sites must be either \'_RETURN\' or list""\n\n        self.sites = sites\n        super().__init__()\n        self.trace_dist = trace_dist\n\n    has_enumerate_support = True\n\n    @memoize(maxsize=10)\n    def _dist_and_values(self):\n        # XXX currently this whole object is very inefficient\n        values_map, logits = collections.OrderedDict(), collections.OrderedDict()\n        for tr, logit in zip(self.trace_dist.exec_traces,\n                             self.trace_dist.log_weights):\n            if isinstance(self.sites, str):\n                value = tr.nodes[self.sites][""value""]\n            else:\n                value = {site: tr.nodes[site][""value""] for site in self.sites}\n            if not torch.is_tensor(logit):\n                logit = torch.tensor(logit)\n\n            if torch.is_tensor(value):\n                value_hash = hash(value.cpu().contiguous().numpy().tobytes())\n            elif isinstance(value, dict):\n                value_hash = hash(self._dict_to_tuple(value))\n            else:\n                value_hash = hash(value)\n            if value_hash in logits:\n                # Value has already been seen.\n                logits[value_hash] = logsumexp(torch.stack([logits[value_hash], logit]), dim=-1)\n            else:\n                logits[value_hash] = logit\n                values_map[value_hash] = value\n\n        logits = torch.stack(list(logits.values())).contiguous().view(-1)\n        logits = logits - logsumexp(logits, dim=-1)\n        d = dist.Categorical(logits=logits)\n        return d, values_map\n\n    def sample(self):\n        d, values_map = self._dist_and_values()\n        ix = d.sample()\n        return list(values_map.values())[ix]\n\n    def log_prob(self, val):\n        d, values_map = self._dist_and_values()\n        if torch.is_tensor(val):\n            value_hash = hash(val.cpu().contiguous().numpy().tobytes())\n        elif isinstance(val, dict):\n            value_hash = hash(self._dict_to_tuple(val))\n        else:\n            value_hash = hash(val)\n        return d.log_prob(torch.tensor([list(values_map.keys()).index(value_hash)]))\n\n    def enumerate_support(self):\n        d, values_map = self._dist_and_values()\n        return list(values_map.values())[:]\n\n    def _dict_to_tuple(self, d):\n        """"""\n        Recursively converts a dictionary to a list of key-value tuples\n        Only intended for use as a helper function inside HashingMarginal!!\n        May break when keys cant be sorted, but that is not an expected use-case\n        """"""\n        if isinstance(d, dict):\n            return tuple([(k, self._dict_to_tuple(d[k])) for k in sorted(d.keys())])\n        else:\n            return d\n\n    def _weighted_mean(self, value, dim=0):\n        weights = self._log_weights.reshape([-1] + (value.dim() - 1) * [1])\n        max_weight = weights.max(dim=dim)[0]\n        relative_probs = (weights - max_weight).exp()\n        return (value * relative_probs).sum(dim=dim) / relative_probs.sum(dim=dim)\n\n    @property\n    def mean(self):\n        samples = torch.stack(list(self._dist_and_values()[1].values()))\n        return self._weighted_mean(samples)\n\n    @property\n    def variance(self):\n        samples = torch.stack(list(self._dist_and_values()[1].values()))\n        deviation_squared = torch.pow(samples - self.mean, 2)\n        return self._weighted_mean(deviation_squared)\n\n\n########################\n# Exact Search inference\n########################\n\nclass Search(TracePosterior):\n    """"""\n    Exact inference by enumerating over all possible executions\n    """"""\n    def __init__(self, model, max_tries=int(1e6), **kwargs):\n        self.model = model\n        self.max_tries = max_tries\n        super().__init__(**kwargs)\n\n    def _traces(self, *args, **kwargs):\n        q = queue.Queue()\n        q.put(poutine.Trace())\n        p = poutine.trace(\n            poutine.queue(self.model, queue=q, max_tries=self.max_tries))\n        while not q.empty():\n            tr = p.get_trace(*args, **kwargs)\n            yield tr, tr.log_prob_sum()\n\n\n###############################################\n# Best-first Search Inference\n###############################################\n\n\ndef pqueue(fn, queue):\n\n    def sample_escape(tr, site):\n        return (site[""name""] not in tr) and \\\n            (site[""type""] == ""sample"") and \\\n            (not site[""is_observed""])\n\n    def _fn(*args, **kwargs):\n\n        for i in range(int(1e6)):\n            assert not queue.empty(), \\\n                ""trying to get() from an empty queue will deadlock""\n\n            priority, next_trace = queue.get()\n            try:\n                ftr = poutine.trace(poutine.escape(poutine.replay(fn, next_trace),\n                                                   functools.partial(sample_escape,\n                                                                     next_trace)))\n                return ftr(*args, **kwargs)\n            except NonlocalExit as site_container:\n                site_container.reset_stack()\n                for tr in poutine.util.enum_extend(ftr.trace.copy(),\n                                                   site_container.site):\n                    # add a little bit of noise to the priority to break ties...\n                    queue.put((tr.log_prob_sum().item() - torch.rand(1).item() * 1e-2, tr))\n\n        raise ValueError(""max tries ({}) exceeded"".format(str(1e6)))\n\n    return _fn\n\n\nclass BestFirstSearch(TracePosterior):\n    """"""\n    Inference by enumerating executions ordered by their probabilities.\n    Exact (and results equivalent to Search) if all executions are enumerated.\n    """"""\n    def __init__(self, model, num_samples=None, **kwargs):\n        if num_samples is None:\n            num_samples = 100\n        self.num_samples = num_samples\n        self.model = model\n        super().__init__(**kwargs)\n\n    def _traces(self, *args, **kwargs):\n        q = queue.PriorityQueue()\n        # add a little bit of noise to the priority to break ties...\n        q.put((torch.zeros(1).item() - torch.rand(1).item() * 1e-2, poutine.Trace()))\n        q_fn = pqueue(self.model, queue=q)\n        for i in range(self.num_samples):\n            if q.empty():\n                # num_samples was too large!\n                break\n            tr = poutine.trace(q_fn).get_trace(*args, **kwargs)  # XXX should block\n            yield tr, tr.log_prob_sum()\n'"
examples/contrib/autoname/mixture.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.autoname import named\nfrom pyro.infer import SVI, JitTrace_ELBO, Trace_ELBO\nfrom pyro.optim import Adam\n\n# This is a simple gaussian mixture model.\n#\n# The example demonstrates how to pass named.Objects() from a global model to\n# a local model implemented as a helper function.\n\n\ndef model(data, k):\n    latent = named.Object(""latent"")\n\n    # Create parameters for a Gaussian mixture model.\n    latent.probs.param_(torch.ones(k) / k, constraint=constraints.simplex)\n    latent.locs.param_(torch.zeros(k))\n    latent.scales.param_(torch.ones(k), constraint=constraints.positive)\n\n    # Observe all the data. We pass a local latent in to the local_model.\n    latent.local = named.List()\n    for x in data:\n        local_model(latent.local.add(), latent.probs, latent.locs, latent.scales, obs=x)\n\n\ndef local_model(latent, ps, locs, scales, obs=None):\n    i = latent.id.sample_(dist.Categorical(ps))\n    return latent.x.sample_(dist.Normal(locs[i], scales[i]), obs=obs)\n\n\ndef guide(data, k):\n    latent = named.Object(""latent"")\n    latent.local = named.List()\n    for x in data:\n        # We pass a local latent in to the local_guide.\n        local_guide(latent.local.add(), k)\n\n\ndef local_guide(latent, k):\n    # The local guide simply guesses category assignments.\n    latent.probs.param_(torch.ones(k) / k, constraint=constraints.positive)\n    latent.id.sample_(dist.Categorical(latent.probs))\n\n\ndef main(args):\n    pyro.set_rng_seed(0)\n    pyro.enable_validation(__debug__)\n\n    optim = Adam({""lr"": 0.1})\n    elbo = JitTrace_ELBO() if args.jit else Trace_ELBO()\n    inference = SVI(model, guide, optim, loss=elbo)\n    data = torch.tensor([0.0, 1.0, 2.0, 20.0, 30.0, 40.0])\n    k = 2\n\n    print(\'Step\\tLoss\')\n    loss = 0.0\n    for step in range(args.num_epochs):\n        if step and step % 10 == 0:\n            print(\'{}\\t{:0.5g}\'.format(step, loss))\n            loss = 0.0\n        loss += inference.step(data, k=k)\n\n    print(\'Parameters:\')\n    for name, value in sorted(pyro.get_param_store().items()):\n        print(\'{} = {}\'.format(name, value.detach().cpu().numpy()))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-epochs\', default=200, type=int)\n    parser.add_argument(\'--jit\', action=\'store_true\')\n    args = parser.parse_args()\n    main(args)\n'"
examples/contrib/autoname/scoping_mixture.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.optim\nimport pyro.distributions as dist\n\nfrom pyro.infer import SVI, config_enumerate, TraceEnum_ELBO\n\nfrom pyro.contrib.autoname import scope\n\n\ndef model(K, data):\n    # Global parameters.\n    weights = pyro.param(\'weights\', torch.ones(K) / K, constraint=constraints.simplex)\n    locs = pyro.param(\'locs\', 10 * torch.randn(K))\n    scale = pyro.param(\'scale\', torch.tensor(0.5), constraint=constraints.positive)\n\n    with pyro.plate(\'data\'):\n        return local_model(weights, locs, scale, data)\n\n\n@scope(prefix=""local"")\ndef local_model(weights, locs, scale, data):\n    assignment = pyro.sample(\'assignment\',\n                             dist.Categorical(weights).expand_by([len(data)]))\n    return pyro.sample(\'obs\', dist.Normal(locs[assignment], scale), obs=data)\n\n\ndef guide(K, data):\n    assignment_probs = pyro.param(\'assignment_probs\', torch.ones(len(data), K) / K,\n                                  constraint=constraints.unit_interval)\n    with pyro.plate(\'data\'):\n        return local_guide(assignment_probs)\n\n\n@scope(prefix=""local"")\ndef local_guide(probs):\n    return pyro.sample(\'assignment\', dist.Categorical(probs))\n\n\ndef main(args):\n    pyro.set_rng_seed(0)\n    pyro.clear_param_store()\n    K = 2\n\n    data = torch.tensor([0.0, 1.0, 2.0, 20.0, 30.0, 40.0])\n    optim = pyro.optim.Adam({\'lr\': 0.1})\n    inference = SVI(model, config_enumerate(guide), optim,\n                    loss=TraceEnum_ELBO(max_plate_nesting=1))\n\n    print(\'Step\\tLoss\')\n    loss = 0.0\n    for step in range(args.num_epochs):\n        if step and step % 10 == 0:\n            print(\'{}\\t{:0.5g}\'.format(step, loss))\n            loss = 0.0\n        loss += inference.step(K, data)\n\n    print(\'Parameters:\')\n    for name, value in sorted(pyro.get_param_store().items()):\n        print(\'{} = {}\'.format(name, value.detach().cpu().numpy()))\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-epochs\', default=200, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/contrib/autoname/tree_data.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.autoname import named\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\n\n# This is a linear mixed-effects model over arbitrary json-like data.\n# Data can be a number, a list of data, or a dict with data values.\n#\n# The goal is to learn a mean field approximation to the posterior\n# values z, parameterized by parameters post_loc and post_scale.\n#\n# Notice that the named.Objects allow for modularity that fits well\n# with the recursive model and guide functions.\n\n\ndef model(data):\n    latent = named.Object(""latent"")\n    latent.z.sample_(dist.Normal(0.0, 1.0))\n    model_recurse(data, latent)\n\n\ndef model_recurse(data, latent):\n    if torch.is_tensor(data):\n        latent.x.sample_(dist.Normal(latent.z, 1.0), obs=data)\n    elif isinstance(data, list):\n        latent.prior_scale.param_(torch.tensor(1.0), constraint=constraints.positive)\n        latent.list = named.List()\n        for data_i in data:\n            latent_i = latent.list.add()\n            latent_i.z.sample_(dist.Normal(latent.z, latent.prior_scale))\n            model_recurse(data_i, latent_i)\n    elif isinstance(data, dict):\n        latent.prior_scale.param_(torch.tensor(1.0), constraint=constraints.positive)\n        latent.dict = named.Dict()\n        for key, value in data.items():\n            latent.dict[key].z.sample_(dist.Normal(latent.z, latent.prior_scale))\n            model_recurse(value, latent.dict[key])\n    else:\n        raise TypeError(""Unsupported type {}"".format(type(data)))\n\n\ndef guide(data):\n    guide_recurse(data, named.Object(""latent""))\n\n\ndef guide_recurse(data, latent):\n    latent.post_loc.param_(torch.tensor(0.0))\n    latent.post_scale.param_(torch.tensor(1.0), constraint=constraints.positive)\n    latent.z.sample_(dist.Normal(latent.post_loc, latent.post_scale))\n    if torch.is_tensor(data):\n        pass\n    elif isinstance(data, list):\n        latent.list = named.List()\n        for datum in data:\n            guide_recurse(datum, latent.list.add())\n    elif isinstance(data, dict):\n        latent.dict = named.Dict()\n        for key, value in data.items():\n            guide_recurse(value, latent.dict[key])\n    else:\n        raise TypeError(""Unsupported type {}"".format(type(data)))\n\n\ndef main(args):\n    pyro.set_rng_seed(0)\n    pyro.enable_validation(__debug__)\n\n    optim = Adam({""lr"": 0.1})\n    inference = SVI(model, guide, optim, loss=Trace_ELBO())\n\n    # Data is an arbitrary json-like structure with tensors at leaves.\n    one = torch.tensor(1.0)\n    data = {\n        ""foo"": one,\n        ""bar"": [0 * one, 1 * one, 2 * one],\n        ""baz"": {\n            ""noun"": {\n                ""concrete"": 4 * one,\n                ""abstract"": 6 * one,\n            },\n            ""verb"": 2 * one,\n        },\n    }\n\n    print(\'Step\\tLoss\')\n    loss = 0.0\n    for step in range(args.num_epochs):\n        loss += inference.step(data)\n        if step and step % 10 == 0:\n            print(\'{}\\t{:0.5g}\'.format(step, loss))\n            loss = 0.0\n\n    print(\'Parameters:\')\n    for name, value in sorted(pyro.get_param_store().items()):\n        print(\'{} = {}\'.format(name, value.detach().cpu().numpy()))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-epochs\', default=100, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/contrib/cevae/synthetic.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis example demonstrates how to use the Causal Effect Variational Autoencoder\n[1] implemented in pyro.contrib.cevae.CEVAE, documented at\nhttp://docs.pyro.ai/en/latest/contrib.cevae.html\n\n**References**\n\n[1] C. Louizos, U. Shalit, J. Mooij, D. Sontag, R. Zemel, M. Welling (2017).\n    Causal Effect Inference with Deep Latent-Variable Models.\n    http://papers.nips.cc/paper/7223-causal-effect-inference-with-deep-latent-variable-models.pdf\n    https://github.com/AMLab-Amsterdam/CEVAE\n""""""\nimport argparse\nimport logging\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.cevae import CEVAE\n\nlogging.getLogger(""pyro"").setLevel(logging.DEBUG)\nlogging.getLogger(""pyro"").handlers[0].setLevel(logging.DEBUG)\n\n\ndef generate_data(args):\n    """"""\n    This implements the generative process of [1], but using larger feature and\n    latent spaces ([1] assumes ``feature_dim=1`` and ``latent_dim=5``).\n    """"""\n    z = dist.Bernoulli(0.5).sample([args.num_data])\n    x = dist.Normal(z, 5 * z + 3 * (1 - z)).sample([args.feature_dim]).t()\n    t = dist.Bernoulli(0.75 * z + 0.25 * (1 - z)).sample()\n    y = dist.Bernoulli(logits=3 * (z + 2 * (2 * t - 2))).sample()\n\n    # Compute true ite for evaluation (via Monte Carlo approximation).\n    t0_t1 = torch.tensor([[0.], [1.]])\n    y_t0, y_t1 = dist.Bernoulli(logits=3 * (z + 2 * (2 * t0_t1 - 2))).mean\n    true_ite = y_t1 - y_t0\n    return x, t, y, true_ite\n\n\ndef main(args):\n    pyro.enable_validation(__debug__)\n    if args.cuda:\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n    # Generate synthetic data.\n    pyro.set_rng_seed(args.seed)\n    x_train, t_train, y_train, _ = generate_data(args)\n\n    # Train.\n    pyro.set_rng_seed(args.seed)\n    pyro.clear_param_store()\n    cevae = CEVAE(feature_dim=args.feature_dim,\n                  latent_dim=args.latent_dim,\n                  hidden_dim=args.hidden_dim,\n                  num_layers=args.num_layers,\n                  num_samples=10)\n    cevae.fit(x_train, t_train, y_train,\n              num_epochs=args.num_epochs,\n              batch_size=args.batch_size,\n              learning_rate=args.learning_rate,\n              learning_rate_decay=args.learning_rate_decay,\n              weight_decay=args.weight_decay)\n\n    # Evaluate.\n    x_test, t_test, y_test, true_ite = generate_data(args)\n    true_ate = true_ite.mean()\n    print(""true ATE = {:0.3g}"".format(true_ate.item()))\n    naive_ate = y_test[t_test == 1].mean() - y_test[t_test == 0].mean()\n    print(""naive ATE = {:0.3g}"".format(naive_ate))\n    if args.jit:\n        cevae = cevae.to_script_module()\n    est_ite = cevae.ite(x_test)\n    est_ate = est_ite.mean()\n    print(""estimated ATE = {:0.3g}"".format(est_ate.item()))\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""Causal Effect Variational Autoencoder"")\n    parser.add_argument(""--num-data"", default=1000, type=int)\n    parser.add_argument(""--feature-dim"", default=5, type=int)\n    parser.add_argument(""--latent-dim"", default=20, type=int)\n    parser.add_argument(""--hidden-dim"", default=200, type=int)\n    parser.add_argument(""--num-layers"", default=3, type=int)\n    parser.add_argument(""-n"", ""--num-epochs"", default=50, type=int)\n    parser.add_argument(""-b"", ""--batch-size"", default=100, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=1e-3, type=float)\n    parser.add_argument(""-lrd"", ""--learning-rate-decay"", default=0.1, type=float)\n    parser.add_argument(""--weight-decay"", default=1e-4, type=float)\n    parser.add_argument(""--seed"", default=1234567890, type=int)\n    parser.add_argument(""--jit"", action=""store_true"")\n    parser.add_argument(""--cuda"", action=""store_true"")\n    args = parser.parse_args()\n    main(args)\n'"
examples/contrib/epidemiology/regional.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport logging\n\nimport torch\n\nimport pyro\nfrom pyro.contrib.epidemiology import RegionalSIRModel\n\nlogging.basicConfig(format=\'%(message)s\', level=logging.INFO)\n\n\ndef Model(args, data):\n    assert 0 <= args.coupling <= 1, args.coupling\n    population = torch.full((args.num_regions,), float(args.population))\n    coupling = torch.eye(args.num_regions).clamp(min=args.coupling)\n    return RegionalSIRModel(population, coupling, args.recovery_time, data)\n\n\ndef generate_data(args):\n    extended_data = [None] * (args.duration + args.forecast)\n    model = Model(args, extended_data)\n    logging.info(""Simulating from a {}"".format(type(model).__name__))\n    for attempt in range(100):\n        samples = model.generate({""R0"": args.basic_reproduction_number,\n                                  ""rho_c1"": 10 * args.response_rate,\n                                  ""rho_c0"": 10 * (1 - args.response_rate)})\n        obs = samples[""obs""][:args.duration]\n        S2I = samples[""S2I""]\n\n        obs_sum = int(obs.sum())\n        S2I_sum = int(S2I[:args.duration].sum())\n        if obs_sum >= args.min_observations:\n            logging.info(""Observed {:d}/{:d} infections:\\n{}"".format(\n                obs_sum, S2I_sum, "" "".join(str(int(x)) for x in obs[:, 0])))\n            return {""S2I"": S2I, ""obs"": obs}\n\n    raise ValueError(""Failed to generate {} observations. Try increasing ""\n                     ""--population or decreasing --min-observations""\n                     .format(args.min_observations))\n\n\ndef infer(args, model):\n    energies = []\n\n    def hook_fn(kernel, *unused):\n        e = float(kernel._potential_energy_last)\n        energies.append(e)\n        if args.verbose:\n            logging.info(""potential = {:0.6g}"".format(e))\n\n    mcmc = model.fit(heuristic_num_particles=args.num_particles,\n                     heuristic_ess_threshold=args.ess_threshold,\n                     warmup_steps=args.warmup_steps,\n                     num_samples=args.num_samples,\n                     max_tree_depth=args.max_tree_depth,\n                     num_quant_bins=args.num_bins,\n                     haar=args.haar,\n                     haar_full_mass=args.haar_full_mass,\n                     hook_fn=hook_fn)\n\n    mcmc.summary()\n    if args.plot:\n        import matplotlib.pyplot as plt\n        plt.figure(figsize=(6, 3))\n        plt.plot(energies)\n        plt.xlabel(""MCMC step"")\n        plt.ylabel(""potential energy"")\n        plt.title(""MCMC energy trace"")\n        plt.tight_layout()\n\n    return model.samples\n\n\ndef predict(args, model, truth):\n    samples = model.predict(forecast=args.forecast)\n    S2I = samples[""S2I""]\n    median = S2I.median(dim=0).values\n    lines = [""Median prediction of new infections (starting on day 0):""]\n    for r in range(args.num_regions):\n        lines.append(""Region {}: {}"".format(r, "" "".join(map(str, map(int, median[:, r])))))\n    logging.info(""\\n"".join(lines))\n\n    # Optionally plot the latent and forecasted series of new infections.\n    if args.plot:\n        import matplotlib.pyplot as plt\n        fig, axes = plt.subplots(args.num_regions, sharex=True,\n                                 figsize=(6, 1 + args.num_regions))\n        time = torch.arange(args.duration + args.forecast)\n        p05 = S2I.kthvalue(int(round(0.5 + 0.05 * args.num_samples)), dim=0).values\n        p95 = S2I.kthvalue(int(round(0.5 + 0.95 * args.num_samples)), dim=0).values\n        for r, ax in enumerate(axes):\n            ax.fill_between(time, p05[:, r], p95[:, r], color=""red"", alpha=0.3, label=""90% CI"")\n            ax.plot(time, median[:, r], ""r-"", label=""median"")\n            ax.plot(time[:args.duration], model.data[:, r], ""k."", label=""observed"")\n            ax.plot(time, truth[:, r], ""k--"", label=""truth"")\n            ax.axvline(args.duration - 0.5, color=""gray"", lw=1)\n            ax.set_xlim(0, len(time) - 1)\n            ax.set_ylim(0, None)\n        axes[0].set_title(""New infections among {} regions each of size {}""\n                          .format(args.num_regions, args.population))\n        axes[args.num_regions // 2].set_ylabel(""inf./day"")\n        axes[-1].set_xlabel(""day after first infection"")\n        axes[-1].legend(loc=""upper left"")\n        plt.tight_layout()\n        plt.subplots_adjust(hspace=0)\n\n\ndef main(args):\n    pyro.enable_validation(__debug__)\n    pyro.set_rng_seed(args.rng_seed)\n\n    # Generate data.\n    dataset = generate_data(args)\n    obs = dataset[""obs""]\n\n    # Run inference.\n    model = Model(args, obs)\n    infer(args, model)\n\n    # Predict latent time series.\n    predict(args, model, truth=dataset[""S2I""])\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(\n        description=""Regional compartmental epidemiology modeling using HMC"")\n    parser.add_argument(""-p"", ""--population"", default=1000, type=int)\n    parser.add_argument(""-r"", ""--num-regions"", default=2, type=int)\n    parser.add_argument(""-c"", ""--coupling"", default=0.1, type=float)\n    parser.add_argument(""-m"", ""--min-observations"", default=3, type=int)\n    parser.add_argument(""-d"", ""--duration"", default=20, type=int)\n    parser.add_argument(""-f"", ""--forecast"", default=10, type=int)\n    parser.add_argument(""-R0"", ""--basic-reproduction-number"", default=1.5, type=float)\n    parser.add_argument(""-tau"", ""--recovery-time"", default=7.0, type=float)\n    parser.add_argument(""-rho"", ""--response-rate"", default=0.5, type=float)\n    parser.add_argument(""--haar"", action=""store_true"")\n    parser.add_argument(""-hfm"", ""--haar-full-mass"", default=0, type=int)\n    parser.add_argument(""-n"", ""--num-samples"", default=200, type=int)\n    parser.add_argument(""-np"", ""--num-particles"", default=1024, type=int)\n    parser.add_argument(""-ess"", ""--ess-threshold"", default=0.5, type=float)\n    parser.add_argument(""-w"", ""--warmup-steps"", default=100, type=int)\n    parser.add_argument(""-t"", ""--max-tree-depth"", default=5, type=int)\n    parser.add_argument(""-nb"", ""--num-bins"", default=4, type=int)\n    parser.add_argument(""--double"", action=""store_true"", default=True)\n    parser.add_argument(""--single"", action=""store_false"", dest=""double"")\n    parser.add_argument(""--rng-seed"", default=0, type=int)\n    parser.add_argument(""--cuda"", action=""store_true"")\n    parser.add_argument(""--verbose"", action=""store_true"")\n    parser.add_argument(""--plot"", action=""store_true"")\n    args = parser.parse_args()\n\n    if args.double:\n        if args.cuda:\n            torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n        else:\n            torch.set_default_dtype(torch.float64)\n    elif args.cuda:\n        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n    main(args)\n\n    if args.plot:\n        import matplotlib.pyplot as plt\n        plt.show()\n'"
examples/contrib/epidemiology/sir.py,7,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# This script aims to replicate the behavior of examples/sir_hmc.py but using\n# the high-level components of pyro.contrib.epidemiology. Command line\n# arguments and results should be similar.\n\nimport argparse\nimport logging\nimport math\n\nimport torch\nfrom torch.distributions import biject_to, constraints\n\nimport pyro\nfrom pyro.contrib.epidemiology import (HeterogeneousSIRModel, OverdispersedSEIRModel, OverdispersedSIRModel,\n                                       SimpleSEIRModel, SimpleSIRModel, SuperspreadingSEIRModel, SuperspreadingSIRModel)\n\nlogging.basicConfig(format=\'%(message)s\', level=logging.INFO)\n\n\ndef Model(args, data):\n    """"""Dispatch between different model classes.""""""\n    if args.heterogeneous:\n        assert args.incubation_time == 0\n        assert args.overdispersion == 0\n        return HeterogeneousSIRModel(args.population, args.recovery_time, data)\n    elif args.incubation_time > 0:\n        assert args.incubation_time > 1\n        if args.concentration < math.inf:\n            return SuperspreadingSEIRModel(args.population, args.incubation_time,\n                                           args.recovery_time, data)\n        elif args.overdispersion > 0:\n            return OverdispersedSEIRModel(args.population, args.incubation_time,\n                                          args.recovery_time, data)\n        else:\n            return SimpleSEIRModel(args.population, args.incubation_time,\n                                   args.recovery_time, data)\n    else:\n        if args.concentration < math.inf:\n            return SuperspreadingSIRModel(args.population, args.recovery_time, data)\n        elif args.overdispersion > 0:\n            return OverdispersedSIRModel(args.population, args.recovery_time, data)\n        else:\n            return SimpleSIRModel(args.population, args.recovery_time, data)\n\n\ndef generate_data(args):\n    extended_data = [None] * (args.duration + args.forecast)\n    model = Model(args, extended_data)\n    logging.info(""Simulating from a {}"".format(type(model).__name__))\n    for attempt in range(100):\n        samples = model.generate({""R0"": args.basic_reproduction_number,\n                                  ""rho"": args.response_rate,\n                                  ""k"": args.concentration,\n                                  ""od"": args.overdispersion})\n        obs = samples[""obs""][:args.duration]\n        new_I = samples.get(""S2I"", samples.get(""E2I""))\n\n        obs_sum = int(obs.sum())\n        new_I_sum = int(new_I[:args.duration].sum())\n        assert 0 <= args.min_obs_portion < args.max_obs_portion <= 1\n        min_obs = int(math.ceil(args.min_obs_portion * args.population))\n        max_obs = int(math.floor(args.max_obs_portion * args.population))\n        if min_obs <= obs_sum <= max_obs:\n            logging.info(""Observed {:d}/{:d} infections:\\n{}"".format(\n                obs_sum, new_I_sum, "" "".join(str(int(x)) for x in obs)))\n            return {""new_I"": new_I, ""obs"": obs}\n\n    if obs_sum < min_obs:\n        raise ValueError(""Failed to generate >={} observations. ""\n                         ""Try decreasing --min-obs-portion (currently {}).""\n                         .format(min_obs, args.min_obs_portion))\n    else:\n        raise ValueError(""Failed to generate <={} observations. ""\n                         ""Try increasing --max-obs-portion (currently {}).""\n                         .format(max_obs, args.max_obs_portion))\n\n\ndef infer(args, model):\n    energies = []\n\n    def hook_fn(kernel, *unused):\n        e = float(kernel._potential_energy_last)\n        energies.append(e)\n        if args.verbose:\n            logging.info(""potential = {:0.6g}"".format(e))\n\n    mcmc = model.fit(heuristic_num_particles=args.num_particles,\n                     heuristic_ess_threshold=args.ess_threshold,\n                     warmup_steps=args.warmup_steps,\n                     num_samples=args.num_samples,\n                     max_tree_depth=args.max_tree_depth,\n                     arrowhead_mass=args.arrowhead_mass,\n                     num_quant_bins=args.num_bins,\n                     haar=args.haar,\n                     haar_full_mass=args.haar_full_mass,\n                     hook_fn=hook_fn)\n\n    mcmc.summary()\n    if args.plot:\n        import matplotlib.pyplot as plt\n        plt.figure(figsize=(6, 3))\n        plt.plot(energies)\n        plt.xlabel(""MCMC step"")\n        plt.ylabel(""potential energy"")\n        plt.title(""MCMC energy trace"")\n        plt.tight_layout()\n\n    return model.samples\n\n\ndef evaluate(args, model, samples):\n    # Print estimated values.\n    names = {""basic_reproduction_number"": ""R0""}\n    if not args.heterogeneous:\n        names[""response_rate""] = ""rho""\n    if args.concentration < math.inf:\n        names[""concentration""] = ""k""\n    if ""od"" in samples:\n        names[""overdispersion""] = ""od""\n    for name, key in names.items():\n        mean = samples[key].mean().item()\n        std = samples[key].std().item()\n        logging.info(""{}: truth = {:0.3g}, estimate = {:0.3g} \\u00B1 {:0.3g}""\n                     .format(key, getattr(args, name), mean, std))\n\n    # Optionally plot histograms and pairwise correlations.\n    if args.plot:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        # Plot individual histograms.\n        fig, axes = plt.subplots(len(names), 1, figsize=(5, 2.5 * len(names)))\n        if len(names) == 1:\n            axes = [axes]\n        axes[0].set_title(""Posterior parameter estimates"")\n        for ax, (name, key) in zip(axes, names.items()):\n            truth = getattr(args, name)\n            sns.distplot(samples[key], ax=ax, label=""posterior"")\n            ax.axvline(truth, color=""k"", label=""truth"")\n            ax.set_xlabel(key + "" = "" + name.replace(""_"", "" ""))\n            ax.set_yticks(())\n            ax.legend(loc=""best"")\n        plt.tight_layout()\n\n        # Plot pairwise joint distributions for selected variables.\n        covariates = [(name, samples[name]) for name in names.values()]\n        for i, aux in enumerate(samples[""auxiliary""].squeeze(1).unbind(-2)):\n            covariates.append((""aux[{},0]"".format(i), aux[:, 0]))\n            covariates.append((""aux[{},-1]"".format(i), aux[:, -1]))\n        N = len(covariates)\n        fig, axes = plt.subplots(N, N, figsize=(8, 8), sharex=""col"", sharey=""row"")\n        for i in range(N):\n            axes[i][0].set_ylabel(covariates[i][0])\n            axes[0][i].set_xlabel(covariates[i][0])\n            axes[0][i].xaxis.set_label_position(""top"")\n            for j in range(N):\n                ax = axes[i][j]\n                ax.set_xticks(())\n                ax.set_yticks(())\n                ax.scatter(covariates[j][1], -covariates[i][1],\n                           lw=0, color=""darkblue"", alpha=0.3)\n        plt.tight_layout()\n        plt.subplots_adjust(wspace=0, hspace=0)\n\n        # Plot Pearson correlation for every pair of unconstrained variables.\n        def unconstrain(constraint, value):\n            value = biject_to(constraint).inv(value)\n            return value.reshape(args.num_samples, -1)\n\n        covariates = [(""R1"", unconstrain(constraints.positive, samples[""R0""]))]\n        if not args.heterogeneous:\n            covariates.append(\n                (""rho"", unconstrain(constraints.unit_interval, samples[""rho""])))\n        if ""k"" in samples:\n            covariates.append(\n                (""k"", unconstrain(constraints.positive, samples[""k""])))\n        constraint = constraints.interval(-0.5, model.population + 0.5)\n        for name, aux in zip(model.compartments, samples[""auxiliary""].unbind(-2)):\n            covariates.append((name, unconstrain(constraint, aux)))\n        x = torch.cat([v for _, v in covariates], dim=-1)\n        x -= x.mean(0)\n        x /= x.std(0)\n        x = x.t().matmul(x)\n        x /= args.num_samples\n        x.clamp_(min=-1, max=1)\n        plt.figure(figsize=(8, 8))\n        plt.imshow(x, cmap=""bwr"")\n        ticks = torch.tensor([0] + [v.size(-1) for _, v in covariates]).cumsum(0)\n        ticks = (ticks[1:] + ticks[:-1]) / 2\n        plt.yticks(ticks, [name for name, _ in covariates])\n        plt.xticks(())\n        plt.tick_params(length=0)\n        plt.title(""Pearson correlation (unconstrained coordinates)"")\n        plt.tight_layout()\n\n\ndef predict(args, model, truth):\n    samples = model.predict(forecast=args.forecast)\n\n    obs = model.data\n\n    new_I = samples.get(""S2I"", samples.get(""E2I""))\n    median = new_I.median(dim=0).values\n    logging.info(""Median prediction of new infections (starting on day 0):\\n{}""\n                 .format("" "".join(map(str, map(int, median)))))\n\n    # Optionally plot the latent and forecasted series of new infections.\n    if args.plot:\n        import matplotlib.pyplot as plt\n        plt.figure()\n        time = torch.arange(args.duration + args.forecast)\n        p05 = new_I.kthvalue(int(round(0.5 + 0.05 * args.num_samples)), dim=0).values\n        p95 = new_I.kthvalue(int(round(0.5 + 0.95 * args.num_samples)), dim=0).values\n        plt.fill_between(time, p05, p95, color=""red"", alpha=0.3, label=""90% CI"")\n        plt.plot(time, median, ""r-"", label=""median"")\n        plt.plot(time[:args.duration], obs, ""k."", label=""observed"")\n        if truth is not None:\n            plt.plot(time, truth, ""k--"", label=""truth"")\n        plt.axvline(args.duration - 0.5, color=""gray"", lw=1)\n        plt.xlim(0, len(time) - 1)\n        plt.ylim(0, None)\n        plt.xlabel(""day after first infection"")\n        plt.ylabel(""new infections per day"")\n        plt.title(""New infections in population of {}"".format(args.population))\n        plt.legend(loc=""upper left"")\n        plt.tight_layout()\n\n        # Plot Re time series.\n        if args.heterogeneous:\n            plt.figure()\n            Re = samples[""Re""]\n            median = Re.median(dim=0).values\n            p05 = Re.kthvalue(int(round(0.5 + 0.05 * args.num_samples)), dim=0).values\n            p95 = Re.kthvalue(int(round(0.5 + 0.95 * args.num_samples)), dim=0).values\n            plt.fill_between(time, p05, p95, color=""red"", alpha=0.3, label=""90% CI"")\n            plt.plot(time, median, ""r-"", label=""median"")\n            plt.plot(time[:args.duration], obs, ""k."", label=""observed"")\n            plt.axvline(args.duration - 0.5, color=""gray"", lw=1)\n            plt.xlim(0, len(time) - 1)\n            plt.ylim(0, None)\n            plt.xlabel(""day after first infection"")\n            plt.ylabel(""Re"")\n            plt.title(""Effective reproductive number over time"")\n            plt.legend(loc=""upper left"")\n            plt.tight_layout()\n\n\ndef main(args):\n    pyro.enable_validation(__debug__)\n    pyro.set_rng_seed(args.rng_seed)\n\n    # Generate data.\n    dataset = generate_data(args)\n    obs = dataset[""obs""]\n\n    # Run inference.\n    model = Model(args, obs)\n    samples = infer(args, model)\n\n    # Evaluate fit.\n    evaluate(args, model, samples)\n\n    # Predict latent time series.\n    if args.forecast:\n        predict(args, model, truth=dataset[""new_I""])\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(\n        description=""Compartmental epidemiology modeling using HMC"")\n    parser.add_argument(""-p"", ""--population"", default=1000, type=float)\n    parser.add_argument(""-m"", ""--min-obs-portion"", default=0.01, type=float)\n    parser.add_argument(""-M"", ""--max-obs-portion"", default=0.99, type=float)\n    parser.add_argument(""-d"", ""--duration"", default=20, type=int)\n    parser.add_argument(""-f"", ""--forecast"", default=10, type=int)\n    parser.add_argument(""-R0"", ""--basic-reproduction-number"", default=1.5, type=float)\n    parser.add_argument(""-tau"", ""--recovery-time"", default=7.0, type=float)\n    parser.add_argument(""-e"", ""--incubation-time"", default=0.0, type=float,\n                        help=""If zero, use SIR model; if > 1 use SEIR model."")\n    parser.add_argument(""-k"", ""--concentration"", default=math.inf, type=float,\n                        help=""If finite, use a superspreader model."")\n    parser.add_argument(""-rho"", ""--response-rate"", default=0.5, type=float)\n    parser.add_argument(""-o"", ""--overdispersion"", default=0., type=float)\n    parser.add_argument(""-hg"", ""--heterogeneous"", action=""store_true"")\n    parser.add_argument(""--haar"", action=""store_true"")\n    parser.add_argument(""-hfm"", ""--haar-full-mass"", default=0, type=int)\n    parser.add_argument(""-n"", ""--num-samples"", default=200, type=int)\n    parser.add_argument(""-np"", ""--num-particles"", default=1024, type=int)\n    parser.add_argument(""-ess"", ""--ess-threshold"", default=0.5, type=float)\n    parser.add_argument(""-w"", ""--warmup-steps"", default=100, type=int)\n    parser.add_argument(""-t"", ""--max-tree-depth"", default=5, type=int)\n    parser.add_argument(""-a"", ""--arrowhead-mass"", action=""store_true"")\n    parser.add_argument(""-r"", ""--rng-seed"", default=0, type=int)\n    parser.add_argument(""-nb"", ""--num-bins"", default=4, type=int)\n    parser.add_argument(""--double"", action=""store_true"", default=True)\n    parser.add_argument(""--single"", action=""store_false"", dest=""double"")\n    parser.add_argument(""--cuda"", action=""store_true"")\n    parser.add_argument(""--verbose"", action=""store_true"")\n    parser.add_argument(""--plot"", action=""store_true"")\n    args = parser.parse_args()\n    args.population = int(args.population)  # to allow e.g. --population=1e6\n\n    if args.double:\n        if args.cuda:\n            torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n        else:\n            torch.set_default_dtype(torch.float64)\n    elif args.cuda:\n        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n    main(args)\n\n    if args.plot:\n        import matplotlib.pyplot as plt\n        plt.show()\n'"
examples/contrib/forecast/bart.py,13,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport logging\n\nimport numpy as np\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.examples.bart import load_bart_od\nfrom pyro.contrib.forecast import ForecastingModel, backtest\nfrom pyro.ops.tensor_utils import periodic_cumsum, periodic_repeat\n\nlogging.getLogger(""pyro"").setLevel(logging.DEBUG)\nlogging.getLogger(""pyro"").handlers[0].setLevel(logging.DEBUG)\n\n\ndef preprocess(args):\n    """"""\n    Extract a tensor of (arrivals,departures) to Embarcadero station.\n    """"""\n    print(""Loading data"")\n    dataset = load_bart_od()\n\n    # The full dataset has all station->station ridership counts for all of 50\n    # train stations. In this simple example we will model only the aggretate\n    # counts to and from a single station, Embarcadero.\n    i = dataset[""stations""].index(""EMBR"")\n    arrivals = dataset[""counts""][:, :, i].sum(-1)\n    departures = dataset[""counts""][:, i, :].sum(-1)\n    data = torch.stack([arrivals, departures], dim=-1)\n\n    # This simple example uses no covariates, so we will construct a\n    # zero-element tensor of the correct length as empty covariates.\n    covariates = torch.zeros(len(data), 0)\n\n    return data, covariates\n\n\n# We define a model by subclassing the ForecastingModel class and implementing\n# a single .model() method.\nclass Model(ForecastingModel):\n    # The .model() method inputs two tensors: a fake tensor zero_data that is\n    # the same size and dtype as the real data (but of course the generative\n    # model shouldn\'t depend on the value of the data it generates!), and a\n    # tensor of covariates. Our simple model depends on no covariates, so we\n    # simply pass in an empty tensor (see  the preprocess() function above).\n    def model(self, zero_data, covariates):\n        period = 24 * 7\n        duration, dim = zero_data.shape[-2:]\n        assert dim == 2  # Data is bivariate: (arrivals, departures).\n\n        # Sample global parameters.\n        noise_scale = pyro.sample(""noise_scale"",\n                                  dist.LogNormal(torch.full((dim,), -3), 1).to_event(1))\n        assert noise_scale.shape[-1:] == (dim,)\n        trans_timescale = pyro.sample(""trans_timescale"",\n                                      dist.LogNormal(torch.zeros(dim), 1).to_event(1))\n        assert trans_timescale.shape[-1:] == (dim,)\n\n        trans_loc = pyro.sample(""trans_loc"", dist.Cauchy(0, 1 / period))\n        trans_loc = trans_loc.unsqueeze(-1).expand(trans_loc.shape + (dim,))\n        assert trans_loc.shape[-1:] == (dim,)\n        trans_scale = pyro.sample(""trans_scale"",\n                                  dist.LogNormal(torch.zeros(dim), 0.1).to_event(1))\n        trans_corr = pyro.sample(""trans_corr"",\n                                 dist.LKJCorrCholesky(dim, torch.ones(())))\n        trans_scale_tril = trans_scale.unsqueeze(-1) * trans_corr\n        assert trans_scale_tril.shape[-2:] == (dim, dim)\n\n        obs_scale = pyro.sample(""obs_scale"",\n                                dist.LogNormal(torch.zeros(dim), 0.1).to_event(1))\n        obs_corr = pyro.sample(""obs_corr"",\n                               dist.LKJCorrCholesky(dim, torch.ones(())))\n        obs_scale_tril = obs_scale.unsqueeze(-1) * obs_corr\n        assert obs_scale_tril.shape[-2:] == (dim, dim)\n\n        # Note the initial seasonality should be sampled in a plate with the\n        # same dim as the time_plate, dim=-1. That way we can repeat the dim\n        # below using periodic_repeat().\n        with pyro.plate(""season_plate"", period,  dim=-1):\n            season_init = pyro.sample(""season_init"",\n                                      dist.Normal(torch.zeros(dim), 1).to_event(1))\n            assert season_init.shape[-2:] == (period, dim)\n\n        # Sample independent noise at each time step.\n        with self.time_plate:\n            season_noise = pyro.sample(""season_noise"",\n                                       dist.Normal(0, noise_scale).to_event(1))\n            assert season_noise.shape[-2:] == (duration, dim)\n\n        # Construct a prediction. This prediction has an exactly repeated\n        # seasonal part plus slow seasonal drift. We use two deterministic,\n        # linear functions to transform our diagonal Normal noise to nontrivial\n        # samples from a Gaussian process.\n        prediction = (periodic_repeat(season_init, duration, dim=-2) +\n                      periodic_cumsum(season_noise, period, dim=-2))\n        assert prediction.shape[-2:] == (duration, dim)\n\n        # Construct a joint noise model. This model is a GaussianHMM, whose\n        # .rsample() and .log_prob() methods are parallelized over time; this\n        # this entire model is parallelized over time.\n        init_dist = dist.Normal(torch.zeros(dim), 100).to_event(1)\n        trans_mat = trans_timescale.neg().exp().diag_embed()\n        trans_dist = dist.MultivariateNormal(trans_loc, scale_tril=trans_scale_tril)\n        obs_mat = torch.eye(dim)\n        obs_dist = dist.MultivariateNormal(torch.zeros(dim), scale_tril=obs_scale_tril)\n        noise_model = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist,\n                                       duration=duration)\n        assert noise_model.event_shape == (duration, dim)\n\n        # The final statement registers our noise model and prediction.\n        self.predict(noise_model, prediction)\n\n\ndef main(args):\n    pyro.enable_validation(__debug__)\n    data, covariates = preprocess(args)\n\n    # We will model positive count data by log1p-transforming it into real\n    # valued data.  But since we want to evaluate back in the count domain, we\n    # will also define a transform to apply during evaluation, transforming\n    # from real back to count-valued data. Truth is mapped by the log1p()\n    # inverse expm1(), but the prediction will be sampled from a Poisson\n    # distribution.\n    data = data.log1p()\n\n    def transform(pred, truth):\n        pred = torch.poisson(pred.clamp(min=1e-4).expm1())\n        truth = truth.expm1()\n        return pred, truth\n\n    # The backtest() function automatically trains and evaluates our model on\n    # different windows of data.\n    forecaster_options = {\n        ""num_steps"": args.num_steps,\n        ""learning_rate"": args.learning_rate,\n        ""log_every"": args.log_every,\n        ""dct_gradients"": args.dct,\n    }\n    metrics = backtest(data, covariates, Model,\n                       train_window=args.train_window,\n                       test_window=args.test_window,\n                       stride=args.stride,\n                       num_samples=args.num_samples,\n                       forecaster_options=forecaster_options)\n\n    for name in [""mae"", ""rmse"", ""crps""]:\n        values = [m[name] for m in metrics]\n        mean = np.mean(values)\n        std = np.std(values)\n        print(""{} = {:0.3g} +- {:0.3g}"".format(name, mean, std))\n    return metrics\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""Bart Ridership Forecasting Example"")\n    parser.add_argument(""--train-window"", default=2160, type=int)\n    parser.add_argument(""--test-window"", default=336, type=int)\n    parser.add_argument(""--stride"", default=168, type=int)\n    parser.add_argument(""-n"", ""--num-steps"", default=501, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.05, type=float)\n    parser.add_argument(""--dct"", action=""store_true"")\n    parser.add_argument(""--num-samples"", default=100, type=int)\n    parser.add_argument(""--log-every"", default=50, type=int)\n    parser.add_argument(""--seed"", default=1234567890, type=int)\n    args = parser.parse_args()\n    main(args)\n'"
examples/contrib/gp/sv-dkl.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nAn example to use Pyro Gaussian Process module to classify MNIST and binary MNIST.\n\nFollow the idea from reference [1], we will combine a convolutional neural network\n(CNN) with a RBF kernel to create a ""deep"" kernel:\n\n    >>> deep_kernel = gp.kernels.Warping(rbf, iwarping_fn=cnn)\n\nSparseVariationalGP model allows us train the data in mini-batch (time complexity\nscales linearly to the number of data points).\n\nNote that the implementation here is different from [1]. There the authors\nuse CNN as a feature extraction layer, then add a Gaussian Process layer on the\ntop of CNN. Hence, their inducing points lie in the space of extracted features.\nHere we join CNN module and RBF kernel together to make it a deep kernel.\nHence, our inducing points lie in the space of original images.\n\nAfter 16 epochs with default hyperparameters, the accuaracy of 10-class MNIST\nis 98.45% and the accuaracy of binary MNIST is 99.41%.\n\nReference:\n\n[1] Stochastic Variational Deep Kernel Learning\n    Andrew G. Wilson, Zhiting Hu, Ruslan R. Salakhutdinov, Eric P. Xing\n""""""\n\n# Code adapted from https://github.com/pytorch/examples/tree/master/mnist\nimport argparse\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\n\nimport pyro\nimport pyro.contrib.gp as gp\nimport pyro.infer as infer\nfrom pyro.contrib.examples.util import get_data_loader, get_data_directory\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\ndef train(args, train_loader, gpmodule, optimizer, loss_fn, epoch):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        if args.binary:\n            target = (target % 2).float()  # convert numbers 0->9 to 0 or 1\n\n        gpmodule.set_data(data, target)\n        optimizer.zero_grad()\n        loss = loss_fn(gpmodule.model, gpmodule.guide)\n        loss.backward()\n        optimizer.step()\n        batch_idx = batch_idx + 1\n        if batch_idx % args.log_interval == 0:\n            print(""Train Epoch: {:2d} [{:5d}/{} ({:2.0f}%)]\\tLoss: {:.6f}""\n                  .format(epoch, batch_idx * len(data), len(train_loader.dataset),\n                          100. * batch_idx / len(train_loader), loss))\n\n\ndef test(args, test_loader, gpmodule):\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        if args.binary:\n            target = (target % 2).float()  # convert numbers 0->9 to 0 or 1\n\n        # get prediction of GP model on new data\n        f_loc, f_var = gpmodule(data)\n        # use its likelihood to give prediction class\n        pred = gpmodule.likelihood(f_loc, f_var)\n        # compare prediction and target to count accuracy\n        correct += pred.eq(target).long().cpu().sum().item()\n\n    print(""\\nTest set: Accuracy: {}/{} ({:.2f}%)\\n""\n          .format(correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n\n\ndef main(args):\n    data_dir = args.data_dir if args.data_dir is not None else get_data_directory(__file__)\n    train_loader = get_data_loader(dataset_name=\'MNIST\',\n                                   data_dir=data_dir,\n                                   batch_size=args.batch_size,\n                                   dataset_transforms=[transforms.Normalize((0.1307,), (0.3081,))],\n                                   is_training_set=True,\n                                   shuffle=True)\n    test_loader = get_data_loader(dataset_name=\'MNIST\',\n                                  data_dir=data_dir,\n                                  batch_size=args.test_batch_size,\n                                  dataset_transforms=[transforms.Normalize((0.1307,), (0.3081,))],\n                                  is_training_set=False,\n                                  shuffle=False)\n    if args.cuda:\n        train_loader.num_workers = 1\n        test_loader.num_workers = 1\n\n    cnn = CNN()\n\n    # Create deep kernel by warping RBF with CNN.\n    # CNN will transform a high dimension image into a low dimension 2D tensors for RBF kernel.\n    # This kernel accepts inputs are inputs of CNN and gives outputs are covariance matrix of RBF\n    # on outputs of CNN.\n    rbf = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10))\n    deep_kernel = gp.kernels.Warping(rbf, iwarping_fn=cnn)\n\n    # init inducing points (taken randomly from dataset)\n    batches = []\n    for i, (data, _) in enumerate(train_loader):\n        batches.append(data)\n        if i >= ((args.num_inducing - 1) // args.batch_size):\n            break\n    Xu = torch.cat(batches)[:args.num_inducing].clone()\n\n    if args.binary:\n        likelihood = gp.likelihoods.Binary()\n        latent_shape = torch.Size([])\n    else:\n        # use MultiClass likelihood for 10-class classification problem\n        likelihood = gp.likelihoods.MultiClass(num_classes=10)\n        # Because we use Categorical distribution in MultiClass likelihood, we need GP model\n        # returns a list of probabilities of each class. Hence it is required to use\n        # latent_shape = 10.\n        latent_shape = torch.Size([10])\n\n    # Turns on ""whiten"" flag will help optimization for variational models.\n    gpmodule = gp.models.VariationalSparseGP(X=Xu, y=None, kernel=deep_kernel, Xu=Xu,\n                                             likelihood=likelihood, latent_shape=latent_shape,\n                                             num_data=60000, whiten=True, jitter=2e-6)\n    if args.cuda:\n        gpmodule.cuda()\n\n    optimizer = torch.optim.Adam(gpmodule.parameters(), lr=args.lr)\n\n    elbo = infer.JitTraceMeanField_ELBO() if args.jit else infer.TraceMeanField_ELBO()\n    loss_fn = elbo.differentiable_loss\n\n    for epoch in range(1, args.epochs + 1):\n        start_time = time.time()\n        train(args, train_loader, gpmodule, optimizer, loss_fn, epoch)\n        with torch.no_grad():\n            test(args, test_loader, gpmodule)\n        print(""Amount of time spent for epoch {}: {}s\\n""\n              .format(epoch, int(time.time() - start_time)))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=\'Pyro GP MNIST Example\')\n    parser.add_argument(\'--data-dir\', type=str, default=None, metavar=\'PATH\',\n                        help=\'default directory to cache MNIST data\')\n    parser.add_argument(\'--num-inducing\', type=int, default=70, metavar=\'N\',\n                        help=\'number of inducing input (default: 70)\')\n    parser.add_argument(\'--binary\', action=\'store_true\', default=False,\n                        help=\'do binary classification\')\n    parser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                        help=\'input batch size for training (default: 64)\')\n    parser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                        help=\'input batch size for testing (default: 1000)\')\n    parser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                        help=\'number of epochs to train (default: 10)\')\n    parser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                        help=\'learning rate (default: 0.01)\')\n    parser.add_argument(\'--cuda\', action=\'store_true\', default=False,\n                        help=\'enables CUDA training\')\n    parser.add_argument(\'--jit\', action=\'store_true\', default=False,\n                        help=\'enables PyTorch jit\')\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    parser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                        help=\'how many batches to wait before logging training status\')\n    args = parser.parse_args()\n\n    pyro.set_rng_seed(args.seed)\n    if args.cuda:\n        torch.backends.cudnn.deterministic = True\n\n    main(args)\n'"
examples/contrib/oed/ab_test.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nfrom functools import partial\nimport torch\nfrom torch.distributions import constraints\nimport numpy as np\n\nimport pyro\nfrom pyro import optim\nfrom pyro.infer import TraceEnum_ELBO\nfrom pyro.contrib.oed.eig import vi_eig\nimport pyro.contrib.gp as gp\nfrom pyro.contrib.oed.glmm import (\n    zero_mean_unit_obs_sd_lm, group_assignment_matrix, analytic_posterior_cov\n)\n\nfrom gp_bayes_opt import GPBayesOptimizer\n\n""""""\nExample builds on the Bayesian regression tutorial [1]. It demonstrates how\nto estimate the average posterior entropy (APE) under a model and use it to\nmake an optimal decision about experiment design.\n\nThe context is a Gaussian linear model in which the design matrix `X` is a\none-hot-encoded matrix with 2 columns. This corresponds to the simplest form\nof an A/B test. Assume no data has yet be collected. The aim is to find the optimal\nallocation of participants to the two groups to maximise the expected gain in\ninformation from actually performing the experiment.\n\nFor details of the implementation of average posterior entropy estimation, see\nthe docs for :func:`pyro.contrib.oed.eig.vi_eig`.\n\nWe recommend the technical report from Long Ouyang et al [2] as an introduction\nto optimal experiment design within probabilistic programs.\n\nTo optimize the APE (which is required to be minimized) we used Gaussian Process\nbased Bayesian Optimization. See the BO tutorial [3] for details of optimizing noisy\nand expensive-to-compute functions in pyro.\n\n[1] [""Bayesian Regression""](http://pyro.ai/examples/bayesian_regression.html)\n[2] Long Ouyang, Michael Henry Tessler, Daniel Ly, Noah Goodman (2016),\n    ""Practical optimal experiment design with probabilistic programs"",\n    (https://arxiv.org/abs/1608.05046)\n[3] [""Bayesian Optimization""](http://pyro.ai/examples/bo.html)\n""""""\n\n# Set up regression model dimensions\nN = 100  # number of participants\np = 2    # number of features\nprior_sds = torch.tensor([10., 2.5])\n\n# Model and guide using known obs_sd\nmodel, guide = zero_mean_unit_obs_sd_lm(prior_sds)\n\n\ndef estimated_ape(ns, num_vi_steps):\n    designs = [group_assignment_matrix(torch.tensor([n1, N-n1])) for n1 in ns]\n    X = torch.stack(designs)\n    est_ape = vi_eig(\n        model,\n        X,\n        observation_labels=""y"",\n        target_labels=""w"",\n        vi_parameters={\n            ""guide"": guide,\n            ""optim"": optim.Adam({""lr"": 0.05}),\n            ""loss"": TraceEnum_ELBO(strict_enumeration_warning=False).differentiable_loss,\n            ""num_steps"": num_vi_steps},\n        is_parameters={""num_samples"": 1},\n        eig=False\n    )\n    return est_ape\n\n\ndef true_ape(ns):\n    """"""Analytic APE""""""\n    true_ape = []\n    prior_cov = torch.diag(prior_sds**2)\n    designs = [group_assignment_matrix(torch.tensor([n1, N-n1])) for n1 in ns]\n    for i in range(len(ns)):\n        x = designs[i]\n        posterior_cov = analytic_posterior_cov(prior_cov, x, torch.tensor(1.))\n        true_ape.append(0.5*torch.logdet(2*np.pi*np.e*posterior_cov))\n    return torch.tensor(true_ape)\n\n\ndef main(num_vi_steps, num_bo_steps, seed):\n\n    pyro.set_rng_seed(seed)\n    pyro.clear_param_store()\n\n    est_ape = partial(estimated_ape, num_vi_steps=num_vi_steps)\n    est_ape.__doc__ = ""Estimated APE by VI""\n\n    estimators = [true_ape, est_ape]\n    noises = [0.0001, 0.25]\n    num_acqs = [2, 10]\n\n    for f, noise, num_acquisitions in zip(estimators, noises, num_acqs):\n        X = torch.tensor([25., 75.])\n        y = f(X)\n        gpmodel = gp.models.GPRegression(\n            X, y, gp.kernels.Matern52(input_dim=1, lengthscale=torch.tensor(10.)),\n            noise=torch.tensor(noise), jitter=1e-6)\n        gpbo = GPBayesOptimizer(constraints.interval(0, 100), gpmodel,\n                                num_acquisitions=num_acquisitions)\n        pyro.clear_param_store()\n        for i in range(num_bo_steps):\n            result = gpbo.get_step(f, None, verbose=True)\n\n        print(f.__doc__)\n        print(result)\n\n\nif __name__ == ""__main__"":\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""A/B test experiment design using VI"")\n    parser.add_argument(""-n"", ""--num-vi-steps"", nargs=""?"", default=5000, type=int)\n    parser.add_argument(\'--num-bo-steps\', nargs=""?"", default=5, type=int)\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    args = parser.parse_args()\n    main(args.num_vi_steps, args.num_bo_steps, args.seed)\n'"
examples/contrib/oed/gp_bayes_opt.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torch.autograd as autograd\nimport torch.optim as optim\nfrom torch.distributions import transform_to\n\nimport pyro.contrib.gp as gp\nfrom pyro.infer import TraceEnum_ELBO\nimport pyro.optim\n\n\nclass GPBayesOptimizer(pyro.optim.multi.MultiOptimizer):\n    """"""Performs Bayesian Optimization using a Gaussian Process as an\n    emulator for the unknown function.\n    """"""\n\n    def __init__(self, constraints, gpmodel, num_acquisitions, acquisition_func=None):\n        """"""\n        :param torch.constraint constraints: constraints defining the domain of `f`\n        :param gp.models.GPRegression gpmodel: a (possibly initialized) GP\n            regression model. The kernel, etc is specified via `gpmodel`.\n        :param int num_acquisitions: number of points to acquire at each step\n        :param function acquisition_func: a function to generate acquisitions.\n            It should return a torch.Tensor of new points to query.\n        """"""\n        if acquisition_func is None:\n            acquisition_func = self.acquire_thompson\n\n        self.constraints = constraints\n        self.gpmodel = gpmodel\n        self.num_acquisitions = num_acquisitions\n        self.acquisition_func = acquisition_func\n\n    def update_posterior(self, X, y):\n        X = torch.cat([self.gpmodel.X, X])\n        y = torch.cat([self.gpmodel.y, y])\n        self.gpmodel.set_data(X, y)\n        optimizer = torch.optim.Adam(self.gpmodel.parameters(), lr=0.001)\n        gp.util.train(self.gpmodel, optimizer,\n                      loss_fn=TraceEnum_ELBO(strict_enumeration_warning=False).differentiable_loss,\n                      retain_graph=True)\n\n    def find_a_candidate(self, differentiable, x_init):\n        """"""Given a starting point, `x_init`, takes one LBFGS step\n        to optimize the differentiable function.\n\n        :param function differentiable: a function amenable to torch\n            autograd\n        :param torch.Tensor x_init: the initial point\n\n        """"""\n        # transform x to an unconstrained domain\n        unconstrained_x_init = transform_to(self.constraints).inv(x_init)\n        unconstrained_x = unconstrained_x_init.detach().clone().requires_grad_(True)\n        # TODO: Use LBFGS with line search by pytorch #8824 merged\n        minimizer = optim.LBFGS([unconstrained_x], max_eval=20)\n\n        def closure():\n            minimizer.zero_grad()\n            if (torch.log(torch.abs(unconstrained_x)) > 25.).any():\n                return torch.tensor(float(\'inf\'))\n            x = transform_to(self.constraints)(unconstrained_x)\n            y = differentiable(x)\n            autograd.backward(unconstrained_x,\n                              autograd.grad(y, unconstrained_x, retain_graph=True))\n            return y\n\n        minimizer.step(closure)\n        # after finding a candidate in the unconstrained domain,\n        # convert it back to original domain.\n        x = transform_to(self.constraints)(unconstrained_x)\n        opt_y = differentiable(x)\n        return x.detach(), opt_y.detach()\n\n    def opt_differentiable(self, differentiable, num_candidates=5):\n        """"""Optimizes a differentiable function by choosing `num_candidates`\n        initial points at random and calling :func:`find_a_candidate` on\n        each. The best candidate is returned with its function value.\n\n        :param function differentiable: a function amenable to torch autograd\n        :param int num_candidates: the number of random starting points to\n            use\n        :return: the minimiser and its function value\n        :rtype: tuple\n        """"""\n\n        candidates = []\n        values = []\n        for j in range(num_candidates):\n            x_init = (torch.empty(1, dtype=self.gpmodel.X.dtype, device=self.gpmodel.X.device)\n                      .uniform_(self.constraints.lower_bound, self.constraints.upper_bound))\n            x, y = self.find_a_candidate(differentiable, x_init)\n            if torch.isnan(y):\n                continue\n            candidates.append(x)\n            values.append(y)\n\n        mvalue, argmin = torch.min(torch.cat(values), dim=0)\n        return candidates[argmin.item()], mvalue\n\n    def acquire_thompson(self, num_acquisitions=1, **opt_params):\n        """"""Selects `num_acquisitions` query points at which to query the\n        original function by Thompson sampling.\n\n        :param int num_acquisitions: the number of points to generate\n        :param dict opt_params: additional parameters for optimization\n            routines\n        :return: a tensor of points to evaluate `loss` at\n        :rtype: torch.Tensor\n        """"""\n\n        # Initialize the return tensor\n        X = self.gpmodel.X\n        X = torch.empty(num_acquisitions, *X.shape[1:], dtype=X.dtype, device=X.device)\n\n        for i in range(num_acquisitions):\n            sampler = self.gpmodel.iter_sample(noiseless=False)\n            x, _ = self.opt_differentiable(sampler, **opt_params)\n            X[i, ...] = x\n\n        return X\n\n    def get_step(self, loss, params, verbose=False):\n        X = self.acquisition_func(num_acquisitions=self.num_acquisitions)\n        y = loss(X)\n        if verbose:\n            print(""Acquire at: X"")\n            print(X)\n            print(""y"")\n            print(y)\n        self.update_posterior(X, y)\n        return self.opt_differentiable(lambda x: self.gpmodel(x)[0])\n'"
examples/contrib/timeseries/gp_models.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as np\nimport torch\n\nimport pyro\nfrom pyro.contrib.timeseries import IndependentMaternGP, LinearlyCoupledMaternGP\n\nimport argparse\nfrom os.path import exists\nfrom urllib.request import urlopen\n\n\npyro.enable_validation(__debug__)\n\n\n# download dataset from UCI archive\ndef download_data():\n    if not exists(""eeg.dat""):\n        url = ""http://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff""\n        with open(""eeg.dat"", ""wb"") as f:\n            f.write(urlopen(url).read())\n\n\ndef main(args):\n    # download and pre-process EEG data if not in test mode\n    if not args.test:\n        download_data()\n        T_forecast = 349\n        data = np.loadtxt(\'eeg.dat\', delimiter=\',\', skiprows=19)\n        print(""[raw data shape] {}"".format(data.shape))\n        data = torch.tensor(data[::20, :-1]).double()\n        print(""[data shape after thinning] {}"".format(data.shape))\n    # in test mode (for continuous integration on github) so create fake data\n    else:\n        data = torch.randn(20, 3).double()\n        T_forecast = 10\n\n    T, obs_dim = data.shape\n    T_train = T - T_forecast\n\n    # standardize data\n    data_mean = data[0:T_train, :].mean(0)\n    data -= data_mean\n    data_std = data[0:T_train, :].std(0)\n    data /= data_std\n\n    torch.manual_seed(args.seed)\n\n    # set up model\n    if args.model == ""imgp"":\n        gp = IndependentMaternGP(nu=1.5, obs_dim=obs_dim,\n                                 length_scale_init=1.5 * torch.ones(obs_dim)).double()\n    elif args.model == ""lcmgp"":\n        num_gps = 9\n        gp = LinearlyCoupledMaternGP(nu=1.5, obs_dim=obs_dim, num_gps=num_gps,\n                                     length_scale_init=1.5 * torch.ones(num_gps)).double()\n\n    # set up optimizer\n    adam = torch.optim.Adam(gp.parameters(), lr=args.init_learning_rate,\n                            betas=(args.beta1, 0.999), amsgrad=True)\n    # we decay the learning rate over the course of training\n    gamma = (args.final_learning_rate / args.init_learning_rate) ** (1.0 / args.num_steps)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(adam, gamma=gamma)\n\n    report_frequency = 10\n\n    # training loop\n    for step in range(args.num_steps):\n        loss = -gp.log_prob(data[0:T_train, :]).sum() / T_train\n        loss.backward()\n        adam.step()\n        scheduler.step()\n\n        if step % report_frequency == 0 or step == args.num_steps - 1:\n            print(""[step %03d]  loss: %.3f"" % (step, loss.item()))\n\n    # plot predictions for three output dimensions\n    if args.plot:\n        assert not args.test\n\n        T_multistep = 49\n        T_onestep = T_forecast - T_multistep\n\n        # do rolling prediction\n        print(""doing one-step-ahead forecasting..."")\n        onestep_means, onestep_stds = np.zeros((T_onestep, obs_dim)), np.zeros((T_onestep, obs_dim))\n        for t in range(T_onestep):\n            # predict one step into the future, conditioning on all previous data.\n            # note that each call to forecast() conditions on more data than the previous call\n            dts = torch.tensor([1.0]).double()\n            pred_dist = gp.forecast(data[0:T_train + t, :], dts)\n            onestep_means[t, :] = pred_dist.loc.data.numpy()\n            if args.model == ""imgp"":\n                onestep_stds[t, :] = pred_dist.scale.data.numpy()\n            elif args.model == ""lcmgp"":\n                onestep_stds[t, :] = pred_dist.covariance_matrix.diagonal(dim1=-1, dim2=-2).data.numpy()\n\n        # do (non-rolling) multi-step forecasting\n        print(""doing multi-step forecasting..."")\n        dts = (1 + torch.arange(T_multistep)).double()\n        pred_dist = gp.forecast(data[0:T_train + T_onestep, :], dts)\n        multistep_means = pred_dist.loc.data.numpy()\n        if args.model == ""imgp"":\n            multistep_stds = pred_dist.scale.data.numpy()\n        elif args.model == ""lcmgp"":\n            multistep_stds = pred_dist.covariance_matrix.diagonal(dim1=-1, dim2=-2).data.numpy()\n\n        import matplotlib\n        matplotlib.use(\'Agg\')  # noqa: E402\n        import matplotlib.pyplot as plt\n\n        f, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n        T = data.size(0)\n        to_seconds = 117.0 / T\n\n        for k, ax in enumerate(axes):\n            which = [0, 4, 10][k]\n\n            # plot raw data\n            ax.plot(to_seconds * np.arange(T), data[:, which], \'ko\', markersize=2, label=\'Data\')\n\n            # plot mean predictions for one-step-ahead forecasts\n            ax.plot(to_seconds * (T_train + np.arange(T_onestep)),\n                    onestep_means[:, which], ls=\'solid\', color=\'b\', label=\'One-step\')\n            # plot 90% confidence intervals for one-step-ahead forecasts\n            ax.fill_between(to_seconds * (T_train + np.arange(T_onestep)),\n                            onestep_means[:, which] - 1.645 * onestep_stds[:, which],\n                            onestep_means[:, which] + 1.645 * onestep_stds[:, which],\n                            color=\'b\', alpha=0.20)\n\n            # plot mean predictions for multi-step-ahead forecasts\n            ax.plot(to_seconds * (T_train + T_onestep + np.arange(T_multistep)),\n                    multistep_means[:, which], ls=\'solid\', color=\'r\', label=\'Multi-step\')\n            # plot 90% confidence intervals for multi-step-ahead forecasts\n            ax.fill_between(to_seconds * (T_train + T_onestep + np.arange(T_multistep)),\n                            multistep_means[:, which] - 1.645 * multistep_stds[:, which],\n                            multistep_means[:, which] + 1.645 * multistep_stds[:, which],\n                            color=\'r\', alpha=0.20)\n\n            ax.set_ylabel(""$y_{%d}$"" % (which + 1), fontsize=20)\n            ax.tick_params(axis=\'both\', which=\'major\', labelsize=14)\n            if k == 1:\n                ax.legend(loc=\'upper left\', fontsize=16)\n\n        plt.tight_layout(pad=0.7)\n        plt.savefig(\'eeg.{}.pdf\'.format(args.model))\n\n\nif __name__ == \'__main__\':\n    assert pyro.__version__.startswith(\'1.3.1\')\n    parser = argparse.ArgumentParser(description=""contrib.timeseries example usage"")\n    parser.add_argument(""-n"", ""--num-steps"", default=300, type=int)\n    parser.add_argument(""-s"", ""--seed"", default=0, type=int)\n    parser.add_argument(""-m"", ""--model"", default=""imgp"", type=str, choices=[""imgp"", ""lcmgp""])\n    parser.add_argument(""-ilr"", ""--init-learning-rate"", default=0.01, type=float)\n    parser.add_argument(""-flr"", ""--final-learning-rate"", default=0.0003, type=float)\n    parser.add_argument(""-b1"", ""--beta1"", default=0.50, type=float)\n    parser.add_argument(""--test"", action=\'store_true\')\n    parser.add_argument(""--plot"", action=\'store_true\')\n    args = parser.parse_args()\n\n    main(args)\n'"
examples/vae/utils/__init__.py,0,b''
examples/vae/utils/custom_mlp.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom inspect import isclass\n\nimport torch\nimport torch.nn as nn\n\nfrom pyro.distributions.util import broadcast_shape\n\n\nclass Exp(nn.Module):\n    """"""\n    a custom module for exponentiation of tensors\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, val):\n        return torch.exp(val)\n\n\nclass ConcatModule(nn.Module):\n    """"""\n    a custom module for concatenation of tensors\n    """"""\n    def __init__(self, allow_broadcast=False):\n        self.allow_broadcast = allow_broadcast\n        super().__init__()\n\n    def forward(self, *input_args):\n        # we have a single object\n        if len(input_args) == 1:\n            # regardless of type,\n            # we don\'t care about single objects\n            # we just index into the object\n            input_args = input_args[0]\n\n        # don\'t concat things that are just single objects\n        if torch.is_tensor(input_args):\n            return input_args\n        else:\n            if self.allow_broadcast:\n                shape = broadcast_shape(*[s.shape[:-1] for s in input_args]) + (-1,)\n                input_args = [s.expand(shape) for s in input_args]\n            return torch.cat(input_args, dim=-1)\n\n\nclass ListOutModule(nn.ModuleList):\n    """"""\n    a custom module for outputting a list of tensors from a list of nn modules\n    """"""\n    def __init__(self, modules):\n        super().__init__(modules)\n\n    def forward(self, *args, **kwargs):\n        # loop over modules in self, apply same args\n        return [mm.forward(*args, **kwargs) for mm in self]\n\n\ndef call_nn_op(op):\n    """"""\n    a helper function that adds appropriate parameters when calling\n    an nn module representing an operation like Softmax\n\n    :param op: the nn.Module operation to instantiate\n    :return: instantiation of the op module with appropriate parameters\n    """"""\n    if op in [nn.Softmax, nn.LogSoftmax]:\n        return op(dim=1)\n    else:\n        return op()\n\n\nclass MLP(nn.Module):\n\n    def __init__(self, mlp_sizes, activation=nn.ReLU, output_activation=None,\n                 post_layer_fct=lambda layer_ix, total_layers, layer: None,\n                 post_act_fct=lambda layer_ix, total_layers, layer: None,\n                 allow_broadcast=False, use_cuda=False):\n        # init the module object\n        super().__init__()\n\n        assert len(mlp_sizes) >= 2, ""Must have input and output layer sizes defined""\n\n        # get our inputs, outputs, and hidden\n        input_size, hidden_sizes, output_size = mlp_sizes[0], mlp_sizes[1:-1], mlp_sizes[-1]\n\n        # assume int or list\n        assert isinstance(input_size, (int, list, tuple)), ""input_size must be int, list, tuple""\n\n        # everything in MLP will be concatted if it\'s multiple arguments\n        last_layer_size = input_size if type(input_size) == int else sum(input_size)\n\n        # everything sent in will be concatted together by default\n        all_modules = [ConcatModule(allow_broadcast)]\n\n        # loop over l\n        for layer_ix, layer_size in enumerate(hidden_sizes):\n            assert type(layer_size) == int, ""Hidden layer sizes must be ints""\n\n            # get our nn layer module (in this case nn.Linear by default)\n            cur_linear_layer = nn.Linear(last_layer_size, layer_size)\n\n            # for numerical stability -- initialize the layer properly\n            cur_linear_layer.weight.data.normal_(0, 0.001)\n            cur_linear_layer.bias.data.normal_(0, 0.001)\n\n            # use GPUs to share data during training (if available)\n            if use_cuda:\n                cur_linear_layer = nn.DataParallel(cur_linear_layer)\n\n            # add our linear layer\n            all_modules.append(cur_linear_layer)\n\n            # handle post_linear\n            post_linear = post_layer_fct(layer_ix + 1, len(hidden_sizes), all_modules[-1])\n\n            # if we send something back, add it to sequential\n            # here we could return a batch norm for example\n            if post_linear is not None:\n                all_modules.append(post_linear)\n\n            # handle activation (assumed no params -- deal with that later)\n            all_modules.append(activation())\n\n            # now handle after activation\n            post_activation = post_act_fct(layer_ix + 1, len(hidden_sizes), all_modules[-1])\n\n            # handle post_activation if not null\n            # could add batch norm for example\n            if post_activation is not None:\n                all_modules.append(post_activation)\n\n            # save the layer size we just created\n            last_layer_size = layer_size\n\n        # now we have all of our hidden layers\n        # we handle outputs\n        assert isinstance(output_size, (int, list, tuple)), ""output_size must be int, list, tuple""\n\n        if type(output_size) == int:\n            all_modules.append(nn.Linear(last_layer_size, output_size))\n            if output_activation is not None:\n                all_modules.append(call_nn_op(output_activation)\n                                   if isclass(output_activation) else output_activation)\n        else:\n\n            # we\'re going to have a bunch of separate layers we can spit out (a tuple of outputs)\n            out_layers = []\n\n            # multiple outputs? handle separately\n            for out_ix, out_size in enumerate(output_size):\n\n                # for a single output object, we create a linear layer and some weights\n                split_layer = []\n\n                # we have an activation function\n                split_layer.append(nn.Linear(last_layer_size, out_size))\n\n                # then we get our output activation (either we repeat all or we index into a same sized array)\n                act_out_fct = output_activation if not isinstance(output_activation, (list, tuple)) \\\n                    else output_activation[out_ix]\n\n                if(act_out_fct):\n                    # we check if it\'s a class. if so, instantiate the object\n                    # otherwise, use the object directly (e.g. pre-instaniated)\n                    split_layer.append(call_nn_op(act_out_fct)\n                                       if isclass(act_out_fct) else act_out_fct)\n\n                # our outputs is just a sequential of the two\n                out_layers.append(nn.Sequential(*split_layer))\n\n            all_modules.append(ListOutModule(out_layers))\n\n        # now we have all of our modules, we\'re ready to build our sequential!\n        # process mlps in order, pretty standard here\n        self.sequential_mlp = nn.Sequential(*all_modules)\n\n    # pass through our sequential for the output!\n    def forward(self, *args, **kwargs):\n        return self.sequential_mlp.forward(*args, **kwargs)\n'"
examples/vae/utils/mnist_cached.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport errno\nimport os\nfrom functools import reduce\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pyro.contrib.examples.util import get_data_directory\n\n# This file contains utilities for caching, transforming and splitting MNIST data\n# efficiently. By default, a PyTorch DataLoader will apply the transform every epoch\n# we avoid this by caching the data early on in MNISTCached class\n\n\n# transformations for MNIST data\ndef fn_x_mnist(x, use_cuda):\n    # normalize pixel values of the image to be in [0,1] instead of [0,255]\n    xp = x * (1. / 255)\n\n    # transform x to a linear tensor from bx * a1 * a2 * ... --> bs * A\n    xp_1d_size = reduce(lambda a, b: a * b, xp.size()[1:])\n    xp = xp.view(-1, xp_1d_size)\n\n    # send the data to GPU(s)\n    if use_cuda:\n        xp = xp.cuda()\n\n    return xp\n\n\ndef fn_y_mnist(y, use_cuda):\n    yp = torch.zeros(y.size(0), 10)\n\n    # send the data to GPU(s)\n    if use_cuda:\n        yp = yp.cuda()\n        y = y.cuda()\n\n    # transform the label y (integer between 0 and 9) to a one-hot\n    yp = yp.scatter_(1, y.view(-1, 1), 1.0)\n    return yp\n\n\ndef get_ss_indices_per_class(y, sup_per_class):\n    # number of indices to consider\n    n_idxs = y.size()[0]\n\n    # calculate the indices per class\n    idxs_per_class = {j: [] for j in range(10)}\n\n    # for each index identify the class and add the index to the right class\n    for i in range(n_idxs):\n        curr_y = y[i]\n        for j in range(10):\n            if curr_y[j] == 1:\n                idxs_per_class[j].append(i)\n                break\n\n    idxs_sup = []\n    idxs_unsup = []\n    for j in range(10):\n        np.random.shuffle(idxs_per_class[j])\n        idxs_sup.extend(idxs_per_class[j][:sup_per_class])\n        idxs_unsup.extend(idxs_per_class[j][sup_per_class:len(idxs_per_class[j])])\n\n    return idxs_sup, idxs_unsup\n\n\ndef split_sup_unsup_valid(X, y, sup_num, validation_num=10000):\n    """"""\n    helper function for splitting the data into supervised, un-supervised and validation parts\n    :param X: images\n    :param y: labels (digits)\n    :param sup_num: what number of examples is supervised\n    :param validation_num: what number of last examples to use for validation\n    :return: splits of data by sup_num number of supervised examples\n    """"""\n\n    # validation set is the last 10,000 examples\n    X_valid = X[-validation_num:]\n    y_valid = y[-validation_num:]\n\n    X = X[0:-validation_num]\n    y = y[0:-validation_num]\n\n    assert sup_num % 10 == 0, ""unable to have equal number of images per class""\n\n    # number of supervised examples per class\n    sup_per_class = int(sup_num / 10)\n\n    idxs_sup, idxs_unsup = get_ss_indices_per_class(y, sup_per_class)\n    X_sup = X[idxs_sup]\n    y_sup = y[idxs_sup]\n    X_unsup = X[idxs_unsup]\n    y_unsup = y[idxs_unsup]\n\n    return X_sup, y_sup, X_unsup, y_unsup, X_valid, y_valid\n\n\ndef print_distribution_labels(y):\n    """"""\n    helper function for printing the distribution of class labels in a dataset\n    :param y: tensor of class labels given as one-hots\n    :return: a dictionary of counts for each label from y\n    """"""\n    counts = {j: 0 for j in range(10)}\n    for i in range(y.size()[0]):\n        for j in range(10):\n            if y[i][j] == 1:\n                counts[j] += 1\n                break\n    print(counts)\n\n\nclass MNISTCached(MNIST):\n    """"""\n    a wrapper around MNIST to load and cache the transformed data\n    once at the beginning of the inference\n    """"""\n\n    # static class variables for caching training data\n    train_data_size = 50000\n    train_data_sup, train_labels_sup = None, None\n    train_data_unsup, train_labels_unsup = None, None\n    validation_size = 10000\n    data_valid, labels_valid = None, None\n    test_size = 10000\n\n    def __init__(self, mode, sup_num, use_cuda=True, *args, **kwargs):\n        super().__init__(train=mode in [""sup"", ""unsup"", ""valid""], *args, **kwargs)\n\n        # transformations on MNIST data (normalization and one-hot conversion for labels)\n        def transform(x):\n            return fn_x_mnist(x, use_cuda)\n\n        def target_transform(y):\n            return fn_y_mnist(y, use_cuda)\n\n        self.mode = mode\n\n        assert mode in [""sup"", ""unsup"", ""test"", ""valid""], ""invalid train/test option values""\n\n        if mode in [""sup"", ""unsup"", ""valid""]:\n\n            # transform the training data if transformations are provided\n            if transform is not None:\n                self.data = (transform(self.data.float()))\n            if target_transform is not None:\n                self.targets = (target_transform(self.targets))\n\n            if MNISTCached.train_data_sup is None:\n                if sup_num is None:\n                    assert mode == ""unsup""\n                    MNISTCached.train_data_unsup, MNISTCached.train_labels_unsup = \\\n                        self.data, self.targets\n                else:\n                    MNISTCached.train_data_sup, MNISTCached.train_labels_sup, \\\n                        MNISTCached.train_data_unsup, MNISTCached.train_labels_unsup, \\\n                        MNISTCached.data_valid, MNISTCached.labels_valid = \\\n                        split_sup_unsup_valid(self.data, self.targets, sup_num)\n\n            if mode == ""sup"":\n                self.data, self.targets = MNISTCached.train_data_sup, MNISTCached.train_labels_sup\n            elif mode == ""unsup"":\n                self.data = MNISTCached.train_data_unsup\n\n                # making sure that the unsupervised labels are not available to inference\n                self.targets = (torch.Tensor(\n                    MNISTCached.train_labels_unsup.shape[0]).view(-1, 1)) * np.nan\n            else:\n                self.data, self.targets = MNISTCached.data_valid, MNISTCached.labels_valid\n\n        else:\n            # transform the testing data if transformations are provided\n            if transform is not None:\n                self.data = (transform(self.data.float()))\n            if target_transform is not None:\n                self.targets = (target_transform(self.targets))\n\n    def __getitem__(self, index):\n        """"""\n        :param index: Index or slice object\n        :returns tuple: (image, target) where target is index of the target class.\n        """"""\n        if self.mode in [""sup"", ""unsup"", ""valid""]:\n            img, target = self.data[index], self.targets[index]\n        elif self.mode == ""test"":\n            img, target = self.data[index], self.targets[index]\n        else:\n            assert False, ""invalid mode: {}"".format(self.mode)\n        return img, target\n\n\ndef setup_data_loaders(dataset, use_cuda, batch_size, sup_num=None, root=None, download=True, **kwargs):\n    """"""\n        helper function for setting up pytorch data loaders for a semi-supervised dataset\n    :param dataset: the data to use\n    :param use_cuda: use GPU(s) for training\n    :param batch_size: size of a batch of data to output when iterating over the data loaders\n    :param sup_num: number of supervised data examples\n    :param download: download the dataset (if it doesn\'t exist already)\n    :param kwargs: other params for the pytorch data loader\n    :return: three data loaders: (supervised data for training, un-supervised data for training,\n                                  supervised data for testing)\n    """"""\n    # instantiate the dataset as training/testing sets\n    if root is None:\n        root = get_data_directory(__file__)\n    if \'num_workers\' not in kwargs:\n        kwargs = {\'num_workers\': 0, \'pin_memory\': False}\n\n    cached_data = {}\n    loaders = {}\n    for mode in [""unsup"", ""test"", ""sup"", ""valid""]:\n        if sup_num is None and mode == ""sup"":\n            # in this special case, we do not want ""sup"" and ""valid"" data loaders\n            return loaders[""unsup""], loaders[""test""]\n        cached_data[mode] = dataset(root=root, mode=mode, download=download,\n                                    sup_num=sup_num, use_cuda=use_cuda)\n        loaders[mode] = DataLoader(cached_data[mode], batch_size=batch_size, shuffle=True, **kwargs)\n\n    return loaders\n\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n\nEXAMPLE_DIR = os.path.dirname(os.path.abspath(os.path.join(__file__, os.pardir)))\nDATA_DIR = os.path.join(EXAMPLE_DIR, \'data\')\nRESULTS_DIR = os.path.join(EXAMPLE_DIR, \'results\')\n'"
examples/vae/utils/vae_plots.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\n\ndef plot_conditional_samples_ssvae(ssvae, visdom_session):\n    """"""\n    This is a method to do conditional sampling in visdom\n    """"""\n    vis = visdom_session\n    ys = {}\n    for i in range(10):\n        ys[i] = torch.zeros(1, 10)\n        ys[i][0, i] = 1\n    xs = torch.zeros(1, 784)\n\n    for i in range(10):\n        images = []\n        for rr in range(100):\n            # get the loc from the model\n            sample_loc_i = ssvae.model(xs, ys[i])\n            img = sample_loc_i[0].view(1, 28, 28).cpu().data.numpy()\n            images.append(img)\n        vis.images(images, 10, 2)\n\n\ndef plot_llk(train_elbo, test_elbo):\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    import scipy as sp\n    import seaborn as sns\n    plt.figure(figsize=(30, 10))\n    sns.set_style(""whitegrid"")\n    data = np.concatenate([np.arange(len(test_elbo))[:, sp.newaxis], -test_elbo[:, sp.newaxis]], axis=1)\n    df = pd.DataFrame(data=data, columns=[\'Training Epoch\', \'Test ELBO\'])\n    g = sns.FacetGrid(df, size=10, aspect=1.5)\n    g.map(plt.scatter, ""Training Epoch"", ""Test ELBO"")\n    g.map(plt.plot, ""Training Epoch"", ""Test ELBO"")\n    plt.savefig(\'./vae_results/test_elbo_vae.png\')\n    plt.close(\'all\')\n\n\ndef plot_vae_samples(vae, visdom_session):\n    vis = visdom_session\n    x = torch.zeros([1, 784])\n    for i in range(10):\n        images = []\n        for rr in range(100):\n            # get loc from the model\n            sample_loc_i = vae.model(x)\n            img = sample_loc_i[0].view(1, 28, 28).cpu().data.numpy()\n            images.append(img)\n        vis.images(images, 10, 2)\n\n\ndef mnist_test_tsne(vae=None, test_loader=None):\n    """"""\n    This is used to generate a t-sne embedding of the vae\n    """"""\n    name = \'VAE\'\n    data = test_loader.dataset.test_data.float()\n    mnist_labels = test_loader.dataset.test_labels\n    z_loc, z_scale = vae.encoder(data)\n    plot_tsne(z_loc, mnist_labels, name)\n\n\ndef mnist_test_tsne_ssvae(name=None, ssvae=None, test_loader=None):\n    """"""\n    This is used to generate a t-sne embedding of the ss-vae\n    """"""\n    if name is None:\n        name = \'SS-VAE\'\n    data = test_loader.dataset.test_data.float()\n    mnist_labels = test_loader.dataset.test_labels\n    z_loc, z_scale = ssvae.encoder_z([data, mnist_labels])\n    plot_tsne(z_loc, mnist_labels, name)\n\n\ndef plot_tsne(z_loc, classes, name):\n    import matplotlib\n    matplotlib.use(\'Agg\')\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn.manifold import TSNE\n    model_tsne = TSNE(n_components=2, random_state=0)\n    z_states = z_loc.detach().cpu().numpy()\n    z_embed = model_tsne.fit_transform(z_states)\n    classes = classes.detach().cpu().numpy()\n    fig = plt.figure()\n    for ic in range(10):\n        ind_vec = np.zeros_like(classes)\n        ind_vec[:, ic] = 1\n        ind_class = classes[:, ic] == 1\n        color = plt.cm.Set1(ic)\n        plt.scatter(z_embed[ind_class, 0], z_embed[ind_class, 1], s=10, color=color)\n        plt.title(""Latent Variable T-SNE per Class"")\n        fig.savefig(\'./vae_results/\'+str(name)+\'_embedding_\'+str(ic)+\'.png\')\n    fig.savefig(\'./vae_results/\'+str(name)+\'_embedding.png\')\n'"
pyro/contrib/autoname/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThe :mod:`pyro.contrib.autoname` module provides tools for automatically\ngenerating unique, semantically meaningful names for sample sites.\n""""""\nfrom pyro.contrib.autoname import named\nfrom pyro.contrib.autoname.scoping import scope, name_count\n\n\n__all__ = [\n    ""named"",\n    ""scope"",\n    ""name_count"",\n]\n'"
pyro/contrib/autoname/named.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThe ``pyro.contrib.named`` module is a thin syntactic layer on top of Pyro.  It\nallows Pyro models to be written to look like programs with operating on Python\ndata structures like ``latent.x.sample_(...)``, rather than programs with\nstring-labeled statements like ``x = pyro.sample(""x"", ...)``.\n\nThis module provides three container data structures ``named.Object``,\n``named.List``, and ``named.Dict``. These data structures are intended to be\nnested in each other. Together they track the address of each piece of data\nin each data structure, so that this address can be used as a Pyro site. For\nexample::\n\n    >>> state = named.Object(""state"")\n    >>> print(str(state))\n    state\n\n    >>> z = state.x.y.z  # z is just a placeholder.\n    >>> print(str(z))\n    state.x.y.z\n\n    >>> state.xs = named.List()  # Create a contained list.\n    >>> x0 = state.xs.add()\n    >>> print(str(x0))\n    state.xs[0]\n\n    >>> state.ys = named.Dict()\n    >>> foo = state.ys[\'foo\']\n    >>> print(str(foo))\n    state.ys[\'foo\']\n\nThese addresses can now be used inside ``sample``, ``observe`` and ``param``\nstatements. These named data structures even provide in-place methods that\nalias Pyro statements. For example::\n\n    >>> state = named.Object(""state"")\n    >>> loc = state.loc.param_(torch.zeros(1, requires_grad=True))\n    >>> scale = state.scale.param_(torch.ones(1, requires_grad=True))\n    >>> z = state.z.sample_(dist.Normal(loc, scale))\n    >>> obs = state.x.sample_(dist.Normal(loc, scale), obs=z)\n\nFor deeper examples of how these can be used in model code, see the\n`Tree Data <https://github.com/pyro-ppl/pyro/blob/dev/examples/contrib/named/tree_data.py>`_\nand\n`Mixture <https://github.com/pyro-ppl/pyro/blob/dev/examples/contrib/named/mixture.py>`_\nexamples.\n\nAuthors: Fritz Obermeyer, Alexander Rush\n""""""\nimport functools\n\nimport pyro\n\n\nclass Object:\n    """"""\n    Object to hold immutable latent state.\n\n    This object can serve either as a container for nested latent state\n    or as a placeholder to be replaced by a tensor via a named.sample,\n    named.observe, or named.param statement. When used as a placeholder,\n    Object objects take the place of strings in normal pyro.sample statements.\n\n    :param str name: The name of the object.\n\n    Example::\n\n        state = named.Object(""state"")\n        state.x = 0\n        state.ys = named.List()\n        state.zs = named.Dict()\n        state.a.b.c.d.e.f.g = 0  # Creates a chain of named.Objects.\n\n    .. warning:: This data structure is write-once: data may be added but may\n        not be mutated or removed. Trying to mutate this data structure may\n        result in silent errors.\n    """"""\n    def __init__(self, name):\n        super().__setattr__(""_name"", name)\n        super().__setattr__(""_is_placeholder"", True)\n\n    def __str__(self):\n        return super().__getattribute__(""_name"")\n\n    def __getattribute__(self, key):\n        try:\n            return super().__getattribute__(key)\n        except AttributeError:\n            name = ""{}.{}"".format(self, key)\n            value = Object(name)\n            super(Object, value).__setattr__(\n                ""_set_value"", lambda value: super(Object, self).__setattr__(key, value))\n            super().__setattr__(key, value)\n            super().__setattr__(""_is_placeholder"", False)\n            return value\n\n    def __setattr__(self, key, value):\n        if isinstance(value, (List, Dict)):\n            value._set_name(""{}.{}"".format(self, key))\n        if hasattr(self, key):\n            old = super().__getattribute__(key)\n            if not isinstance(old, Object) or not old._is_placeholder:\n                raise RuntimeError(""Cannot overwrite {}.{}"".format(self, key))\n        super().__setattr__(key, value)\n\n    @functools.wraps(pyro.sample)\n    def sample_(self, fn, *args, **kwargs):\n        if not self._is_placeholder:\n            raise RuntimeError(""Cannot .sample_ an initialized named.Object {}"".format(self))\n        value = pyro.sample(str(self), fn, *args, **kwargs)\n        self._set_value(value)\n        return value\n\n    @functools.wraps(pyro.param)\n    def param_(self, *args, **kwargs):\n        if not self._is_placeholder:\n            raise RuntimeError(""Cannot .param_ an initialized named.Object"")\n        value = pyro.param(str(self), *args, **kwargs)\n        self._set_value(value)\n        return value\n\n\nclass List(list):\n    """"""\n    List-like object to hold immutable latent state.\n\n    This must either be given a name when constructed::\n\n        latent = named.List(""root"")\n\n    or must be immediately stored in a ``named.Object``::\n\n        latent = named.Object(""root"")\n        latent.xs = named.List()  # Must be bound to a Object before use.\n\n    .. warning:: This data structure is write-once: data may be added but may\n        not be mutated or removed. Trying to mutate this data structure may\n        result in silent errors.\n    """"""\n    def __init__(self, name=None):\n        self._name = name\n\n    def __str__(self):\n        return self._name\n\n    def _set_name(self, name):\n        if self:\n            raise RuntimeError(""Cannot name a named.List after data has been added"")\n        if self._name is not None:\n            raise RuntimeError(""Cannot rename named.List: {}"".format(self._name))\n        self._name = name\n\n    def add(self):\n        """"""\n        Append one new named.Object.\n\n        :returns: a new latent object at the end\n        :rtype: named.Object\n        """"""\n        if self._name is None:\n            raise RuntimeError(""Cannot .add() to a named.List before storing it in a named.Object"")\n        i = len(self)\n        value = Object(""{}[{}]"".format(self._name, i))\n        super(Object, value).__setattr__(\n            ""_set_value"", lambda value, i=i: self.__setitem__(i, value))\n        self.append(value)\n        return value\n\n    def __setitem__(self, pos, value):\n        name = ""{}[{}]"".format(self._name, pos)\n        if isinstance(value, Object):\n            raise RuntimeError(""Cannot store named.Object {} in named.Dict {}"".format(value, self._name))\n        elif isinstance(value, (List, Dict)):\n            value._set_name(name)\n        old = self[pos]\n        if not isinstance(old, Object) or not old._is_placeholder:\n            raise RuntimeError(""Cannot overwrite {}"".format(name))\n        super().__setitem__(pos, value)\n\n\nclass Dict(dict):\n    """"""\n    Dict-like object to hold immutable latent state.\n\n    This must either be given a name when constructed::\n\n        latent = named.Dict(""root"")\n\n    or must be immediately stored in a ``named.Object``::\n\n        latent = named.Object(""root"")\n        latent.xs = named.Dict()  # Must be bound to a Object before use.\n\n    .. warning:: This data structure is write-once: data may be added but may\n        not be mutated or removed. Trying to mutate this data structure may\n        result in silent errors.\n    """"""\n    def __init__(self, name=None):\n        self._name = name\n\n    def __str__(self):\n        return self._name\n\n    def _set_name(self, name):\n        if self:\n            raise RuntimeError(""Cannot name a named.Dict after data has been added"")\n        if self._name is not None:\n            raise RuntimeError(""Cannot rename named.Dict: {}"".format(self._name))\n        self._name = name\n\n    def __getitem__(self, key):\n        try:\n            return super().__getitem__(key)\n        except KeyError:\n            if self._name is None:\n                raise RuntimeError(""Cannot access an unnamed named.Dict"")\n            value = Object(""{}[{!r}]"".format(self._name, key))\n            super(Object, value).__setattr__(\n                ""_set_value"", lambda value: self.__setitem__(key, value))\n            super().__setitem__(key, value)\n            return value\n\n    def __setitem__(self, key, value):\n        name = ""{}[{!r}]"".format(self._name, key)\n        if key in self:\n            old = super().__getitem__(key)\n            if not isinstance(old, Object) or not old._is_placeholder:\n                raise RuntimeError(""Cannot overwrite {}"".format(name))\n        if isinstance(value, Object):\n            raise RuntimeError(""Cannot store named.Object {} in named.Dict {}"".format(value, self._name))\n        elif isinstance(value, (List, Dict)):\n            value._set_name(name)\n        super().__setitem__(key, value)\n'"
pyro/contrib/autoname/scoping.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\n``pyro.contrib.autoname.scoping`` contains the implementation of\n:func:`pyro.contrib.autoname.scope`, a tool for automatically appending\na semantically meaningful prefix to names of sample sites.\n""""""\nimport functools\n\nfrom pyro.poutine.messenger import Messenger\nfrom pyro.poutine.runtime import effectful\n\n\nclass NameCountMessenger(Messenger):\n    """"""\n    ``NameCountMessenger`` is the implementation of :func:`pyro.contrib.autoname.name_count`\n    """"""\n    def __enter__(self):\n        self._names = set()\n        return super().__enter__()\n\n    def _increment_name(self, name, label):\n        while (name, label) in self._names:\n            split_name = name.split(""__"")\n            if ""__"" in name and split_name[-1].isdigit():\n                counter = int(split_name[-1]) + 1\n                name = ""__"".join(split_name[:-1] + [str(counter)])\n            else:\n                name = name + ""__1""\n        return name\n\n    def _pyro_sample(self, msg):\n        msg[""name""] = self._increment_name(msg[""name""], ""sample"")\n\n    def _pyro_post_sample(self, msg):\n        self._names.add((msg[""name""], ""sample""))\n\n    def _pyro_post_scope(self, msg):\n        self._names.add((msg[""args""][0], ""scope""))\n\n    def _pyro_scope(self, msg):\n        msg[""args""] = (self._increment_name(msg[""args""][0], ""scope""),)\n\n\nclass ScopeMessenger(Messenger):\n    """"""\n    ``ScopeMessenger`` is the implementation of :func:`pyro.contrib.autoname.scope`\n    """"""\n    def __init__(self, prefix=None, inner=None):\n        super().__init__()\n        self.prefix = prefix\n        self.inner = inner\n\n    @staticmethod\n    @effectful(type=""scope"")\n    def _collect_scope(prefixed_scope):\n        return prefixed_scope.split(""/"")[-1]\n\n    def __enter__(self):\n        if self.prefix is None:\n            raise ValueError(""no prefix was provided"")\n        if not self.inner:\n            # to accomplish adding a counter to duplicate scopes,\n            # we make ScopeMessenger.__enter__ effectful\n            # so that the same mechanism that adds counters to sample names\n            # can be used to add a counter to a scope name\n            self.prefix = self._collect_scope(self.prefix)\n        return super().__enter__()\n\n    def __call__(self, fn):\n        if self.prefix is None:\n            self.prefix = fn.__code__.co_name  # fn.__name__\n\n        @functools.wraps(fn)\n        def _fn(*args, **kwargs):\n            with type(self)(prefix=self.prefix, inner=self.inner):\n                return fn(*args, **kwargs)\n        return _fn\n\n    def _pyro_scope(self, msg):\n        msg[""args""] = (""{}/{}"".format(self.prefix, msg[""args""][0]),)\n\n    def _pyro_sample(self, msg):\n        msg[""name""] = ""{}/{}"".format(self.prefix, msg[""name""])\n\n\ndef scope(fn=None, prefix=None, inner=None):\n    """"""\n    :param fn: a stochastic function (callable containing Pyro primitive calls)\n    :param prefix: a string to prepend to sample names (optional if ``fn`` is provided)\n    :param inner: switch to determine where duplicate name counters appear\n    :returns: ``fn`` decorated with a :class:`~pyro.contrib.autoname.scoping.ScopeMessenger`\n\n    ``scope`` prepends a prefix followed by a ``/`` to the name at a Pyro sample site.\n    It works much like TensorFlow\'s ``name_scope`` and ``variable_scope``,\n    and can be used as a context manager, a decorator, or a higher-order function.\n\n    ``scope`` is very useful for aligning compositional models with guides or data.\n\n    Example::\n\n        >>> @scope(prefix=""a"")\n        ... def model():\n        ...     return pyro.sample(""x"", dist.Bernoulli(0.5))\n        ...\n        >>> assert ""a/x"" in poutine.trace(model).get_trace()\n\n\n    Example::\n\n        >>> def model():\n        ...     with scope(prefix=""a""):\n        ...         return pyro.sample(""x"", dist.Bernoulli(0.5))\n        ...\n        >>> assert ""a/x"" in poutine.trace(model).get_trace()\n\n    Scopes compose as expected, with outer scopes appearing before inner scopes in names::\n\n        >>> @scope(prefix=""b"")\n        ... def model():\n        ...     with scope(prefix=""a""):\n        ...         return pyro.sample(""x"", dist.Bernoulli(0.5))\n        ...\n        >>> assert ""b/a/x"" in poutine.trace(model).get_trace()\n\n    When used as a decorator or higher-order function,\n    ``scope`` will use the name of the input function as the prefix\n    if no user-specified prefix is provided.\n\n    Example::\n\n        >>> @scope\n        ... def model():\n        ...     return pyro.sample(""x"", dist.Bernoulli(0.5))\n        ...\n        >>> assert ""model/x"" in poutine.trace(model).get_trace()\n    """"""\n    msngr = ScopeMessenger(prefix=prefix, inner=inner)\n    return msngr(fn) if fn is not None else msngr\n\n\ndef name_count(fn=None):\n    """"""\n    ``name_count`` is a very simple autonaming scheme that simply appends a suffix `""__""`\n    plus a counter to any name that appears multiple tims in an execution.\n    Only duplicate instances of a name get a suffix; the first instance is not modified.\n\n    Example::\n\n        >>> @name_count\n        ... def model():\n        ...     for i in range(3):\n        ...         pyro.sample(""x"", dist.Bernoulli(0.5))\n        ...\n        >>> assert ""x"" in poutine.trace(model).get_trace()\n        >>> assert ""x__1"" in poutine.trace(model).get_trace()\n        >>> assert ""x__2"" in poutine.trace(model).get_trace()\n\n    ``name_count`` also composes with :func:`~pyro.contrib.autoname.scope`\n    by adding a suffix to duplicate scope entrances:\n\n    Example::\n\n        >>> @name_count\n        ... def model():\n        ...     for i in range(3):\n        ...         with pyro.contrib.autoname.scope(prefix=""a""):\n        ...             pyro.sample(""x"", dist.Bernoulli(0.5))\n        ...\n        >>> assert ""a/x"" in poutine.trace(model).get_trace()\n        >>> assert ""a__1/x"" in poutine.trace(model).get_trace()\n        >>> assert ""a__2/x"" in poutine.trace(model).get_trace()\n\n    Example::\n\n        >>> @name_count\n        ... def model():\n        ...     with pyro.contrib.autoname.scope(prefix=""a""):\n        ...         for i in range(3):\n        ...             pyro.sample(""x"", dist.Bernoulli(0.5))\n        ...\n        >>> assert ""a/x"" in poutine.trace(model).get_trace()\n        >>> assert ""a/x__1"" in poutine.trace(model).get_trace()\n        >>> assert ""a/x__2"" in poutine.trace(model).get_trace()\n    """"""\n    msngr = NameCountMessenger()\n    return msngr(fn) if fn is not None else msngr\n'"
pyro/contrib/bnn/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.bnn.hidden_layer import HiddenLayer\n\n__all__ = [\n    ""HiddenLayer"",\n]\n'"
pyro/contrib/bnn/hidden_layer.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions.utils import lazy_property\nimport torch.nn.functional as F\n\nfrom pyro.contrib.bnn.utils import adjoin_ones_vector\nfrom pyro.distributions.torch_distribution import TorchDistribution\n\n\nclass HiddenLayer(TorchDistribution):\n    r""""""\n    This distribution is a basic building block in a Bayesian neural network.\n    It represents a single hidden layer, i.e. an affine transformation applied\n    to a set of inputs `X` followed by a non-linearity. The uncertainty in the\n    weights is encoded in a Normal variational distribution specified by the\n    parameters `A_scale` and `A_mean`. The so-called \'local reparameterization\n    trick\' is used to reduce variance (see reference below). In effect, this\n    means the weights are never sampled directly; instead one samples in\n    pre-activation space (i.e. before the non-linearity is applied). Since the\n    weights are never directly sampled, when this distribution is used within\n    the context of variational inference, care must be taken to correctly scale\n    the KL divergence term that corresponds to the weight matrix. This term is\n    folded into the `log_prob` method of this distributions.\n\n    In effect, this distribution encodes the following generative process:\n\n    A ~ Normal(A_mean, A_scale)\n    output ~ non_linearity(AX)\n\n    :param torch.Tensor X: B x D dimensional mini-batch of inputs\n    :param torch.Tensor A_mean:  D x H dimensional specifiying weight mean\n    :param torch.Tensor A_scale: D x H dimensional (diagonal covariance matrix)\n                                 specifying weight uncertainty\n    :param callable non_linearity: a callable that specifies the\n                                   non-linearity used. defaults to ReLU.\n    :param float KL_factor: scaling factor for the KL divergence. prototypically\n                            this is equal to the size of the mini-batch divided\n                            by the size of the whole dataset. defaults to `1.0`.\n    :param A_prior: the prior over the weights is assumed to be normal with\n                    mean zero and scale factor `A_prior`. default value is 1.0.\n    :type A_prior: float or torch.Tensor\n    :param bool include_hidden_bias: controls whether the activations should be\n                                     augmented with a 1, which can be used to\n                                     incorporate bias terms. defaults to `True`.\n    :param bool weight_space_sampling: controls whether the local reparameterization\n                                       trick is used. this is only intended to be\n                                       used for internal testing.\n                                       defaults to `False`.\n\n    Reference:\n\n    Kingma, Diederik P., Tim Salimans, and Max Welling.\n    ""Variational dropout and the local reparameterization trick.""\n    Advances in Neural Information Processing Systems. 2015.\n    """"""\n    has_rsample = True\n\n    def __init__(self, X=None, A_mean=None, A_scale=None, non_linearity=F.relu,\n                 KL_factor=1.0, A_prior_scale=1.0, include_hidden_bias=True,\n                 weight_space_sampling=False):\n        self.X = X\n        self.dim_X = X.size(-1)\n        self.dim_H = A_mean.size(-1)\n        assert A_mean.size(0) == self.dim_X, \\\n            ""The dimensions of X and A_mean and A_scale must match accordingly; see documentation""\n        self.A_mean = A_mean\n        self.A_scale = A_scale\n        self.non_linearity = non_linearity\n        assert callable(non_linearity), ""non_linearity must be callable""\n        if A_scale.dim() != 2:\n            raise NotImplementedError(""A_scale must be 2-dimensional"")\n\n        self.KL_factor = KL_factor\n        self.A_prior_scale = A_prior_scale\n        self.weight_space_sampling = weight_space_sampling\n        self.include_hidden_bias = include_hidden_bias\n\n    def log_prob(self, value):\n        return -self.KL_factor * self.KL\n\n    @lazy_property\n    def KL(self):\n        KL_A = torch.pow(self.A_mean / self.A_prior_scale, 2.0).sum()\n        KL_A -= self.dim_X * self.dim_H\n        KL_A += torch.pow(self.A_scale / self.A_prior_scale, 2.0).sum()\n        KL_A -= 2.0 * torch.log(self.A_scale / self.A_prior_scale).sum()\n        return 0.5 * KL_A\n\n    def rsample(self, sample_shape=torch.Size()):\n        # note: weight space sampling is only meant for testing\n        if self.weight_space_sampling:\n            A = self.A_mean + torch.randn(sample_shape + self.A_scale.shape).type_as(self.A_mean) * self.A_scale\n            activation = torch.matmul(self.X, A)\n        else:\n            _mean = torch.matmul(self.X, self.A_mean)\n            X_sqr = torch.pow(self.X, 2.0).unsqueeze(-1)\n            A_scale_sqr = torch.pow(self.A_scale, 2.0)\n            _std = (X_sqr * A_scale_sqr).sum(-2).sqrt()\n            activation = _mean + torch.randn(sample_shape + _std.shape).type_as(_std) * _std\n\n        # apply non-linearity\n        activation = self.non_linearity(activation)\n\n        # add 1 element to activations\n        if self.include_hidden_bias:\n            activation = adjoin_ones_vector(activation)\n\n        return activation\n'"
pyro/contrib/bnn/utils.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport math\n\n\ndef xavier_uniform(D_in, D_out):\n    scale = math.sqrt(6.0 / float(D_in + D_out))\n    noise = torch.rand(D_in, D_out)\n    return 2.0 * scale * noise - scale\n\n\ndef adjoin_ones_vector(x):\n    return torch.cat([x, torch.ones(x.shape[:-1] + (1,)).type_as(x)], dim=-1)\n\n\ndef adjoin_zeros_vector(x):\n    return torch.cat([x, torch.zeros(x.shape[:-1] + (1,)).type_as(x)], dim=-1)\n'"
pyro/contrib/cevae/__init__.py,30,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis module implements the Causal Effect Variational Autoencoder [1], which\ndemonstrates a number of innovations including:\n\n- a generative model for causal effect inference with hidden confounders;\n- a model and guide with twin neural nets to allow imbalanced treatment; and\n- a custom training loss that includes both ELBO terms and extra terms needed\n  to train the guide to be able to answer counterfactual queries.\n\nThe main interface is the :class:`CEVAE` class, but users may customize by\nusing components :class:`Model`, :class:`Guide`,\n:class:`TraceCausalEffect_ELBO` and utilities.\n\n**References**\n\n[1] C. Louizos, U. Shalit, J. Mooij, D. Sontag, R. Zemel, M. Welling (2017).\n    | Causal Effect Inference with Deep Latent-Variable Models.\n    | http://papers.nips.cc/paper/7223-causal-effect-inference-with-deep-latent-variable-models.pdf\n    | https://github.com/AMLab-Amsterdam/CEVAE\n""""""\nimport logging\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.infer.util import torch_item\nfrom pyro.nn import PyroModule\nfrom pyro.optim import ClippedAdam\nfrom pyro.util import torch_isnan\n\nlogger = logging.getLogger(__name__)\n\n\nclass FullyConnected(nn.Sequential):\n    """"""\n    Fully connected multi-layer network with ELU activations.\n    """"""\n    def __init__(self, sizes, final_activation=None):\n        layers = []\n        for in_size, out_size in zip(sizes, sizes[1:]):\n            layers.append(nn.Linear(in_size, out_size))\n            layers.append(nn.ELU())\n        layers.pop(-1)\n        if final_activation is not None:\n            layers.append(final_activation)\n        super().__init__(*layers)\n\n    def append(self, layer):\n        assert isinstance(layer, nn.Module)\n        self.add_module(str(len(self)), layer)\n\n\nclass DistributionNet(nn.Module):\n    """"""\n    Base class for distribution nets.\n    """"""\n    @staticmethod\n    def get_class(dtype):\n        """"""\n        Get a subclass by a prefix of its name, e.g.::\n\n            assert DistributionNet.get_class(""bernoulli"") is BernoulliNet\n        """"""\n        for cls in DistributionNet.__subclasses__():\n            if cls.__name__.lower() == dtype + ""net"":\n                return cls\n        raise ValueError(""dtype not supported: {}"".format(dtype))\n\n\nclass BernoulliNet(DistributionNet):\n    """"""\n    :class:`FullyConnected` network outputting a single ``logits`` value.\n\n    This is used to represent a conditional probability distribution of a\n    single Bernoulli random variable conditioned on a ``sizes[0]``-sized real\n    value, for example::\n\n        net = BernoulliNet([3, 4])\n        z = torch.randn(3)\n        logits, = net(z)\n        t = net.make_dist(logits).sample()\n    """"""\n    def __init__(self, sizes):\n        assert len(sizes) >= 1\n        super().__init__()\n        self.fc = FullyConnected(sizes + [1])\n\n    def forward(self, x):\n        logits = self.fc(x).squeeze(-1).clamp(min=-10, max=10)\n        return logits,\n\n    @staticmethod\n    def make_dist(logits):\n        return dist.Bernoulli(logits=logits)\n\n\nclass ExponentialNet(DistributionNet):\n    """"""\n    :class:`FullyConnected` network outputting a constrained ``rate``.\n\n    This is used to represent a conditional probability distribution of a\n    single Normal random variable conditioned on a ``sizes[0]``-size real\n    value, for example::\n\n        net = ExponentialNet([3, 4])\n        x = torch.randn(3)\n        rate, = net(x)\n        y = net.make_dist(rate).sample()\n    """"""\n    def __init__(self, sizes):\n        assert len(sizes) >= 1\n        super().__init__()\n        self.fc = FullyConnected(sizes + [1])\n\n    def forward(self, x):\n        scale = nn.functional.softplus(self.fc(x).squeeze(-1)).clamp(min=1e-3, max=1e6)\n        rate = scale.reciprocal()\n        return rate,\n\n    @staticmethod\n    def make_dist(rate):\n        return dist.Exponential(rate)\n\n\nclass LaplaceNet(DistributionNet):\n    """"""\n    :class:`FullyConnected` network outputting a constrained ``loc,scale``\n    pair.\n\n    This is used to represent a conditional probability distribution of a\n    single Laplace random variable conditioned on a ``sizes[0]``-size real\n    value, for example::\n\n        net = LaplaceNet([3, 4])\n        x = torch.randn(3)\n        loc, scale = net(x)\n        y = net.make_dist(loc, scale).sample()\n    """"""\n    def __init__(self, sizes):\n        assert len(sizes) >= 1\n        super().__init__()\n        self.fc = FullyConnected(sizes + [2])\n\n    def forward(self, x):\n        loc_scale = self.fc(x)\n        loc = loc_scale[..., 0].clamp(min=-1e6, max=1e6)\n        scale = nn.functional.softplus(loc_scale[..., 1]).clamp(min=1e-3, max=1e6)\n        return loc, scale\n\n    @staticmethod\n    def make_dist(loc, scale):\n        return dist.Laplace(loc, scale)\n\n\nclass NormalNet(DistributionNet):\n    """"""\n    :class:`FullyConnected` network outputting a constrained ``loc,scale``\n    pair.\n\n    This is used to represent a conditional probability distribution of a\n    single Normal random variable conditioned on a ``sizes[0]``-size real\n    value, for example::\n\n        net = NormalNet([3, 4])\n        x = torch.randn(3)\n        loc, scale = net(x)\n        y = net.make_dist(loc, scale).sample()\n    """"""\n    def __init__(self, sizes):\n        assert len(sizes) >= 1\n        super().__init__()\n        self.fc = FullyConnected(sizes + [2])\n\n    def forward(self, x):\n        loc_scale = self.fc(x)\n        loc = loc_scale[..., 0].clamp(min=-1e6, max=1e6)\n        scale = nn.functional.softplus(loc_scale[..., 1]).clamp(min=1e-3, max=1e6)\n        return loc, scale\n\n    @staticmethod\n    def make_dist(loc, scale):\n        return dist.Normal(loc, scale)\n\n\nclass StudentTNet(DistributionNet):\n    """"""\n    :class:`FullyConnected` network outputting a constrained ``df,loc,scale``\n    triple, with shared ``df > 1``.\n\n    This is used to represent a conditional probability distribution of a\n    single Student\'s t random variable conditioned on a ``sizes[0]``-size real\n    value, for example::\n\n        net = StudentTNet([3, 4])\n        x = torch.randn(3)\n        df, loc, scale = net(x)\n        y = net.make_dist(df, loc, scale).sample()\n    """"""\n    def __init__(self, sizes):\n        assert len(sizes) >= 1\n        super().__init__()\n        self.fc = FullyConnected(sizes + [2])\n        self.df_unconstrained = nn.Parameter(torch.tensor(0.0))\n\n    def forward(self, x):\n        loc_scale = self.fc(x)\n        loc = loc_scale[..., 0].clamp(min=-1e6, max=1e6)\n        scale = nn.functional.softplus(loc_scale[..., 1]).clamp(min=1e-3, max=1e6)\n        df = nn.functional.softplus(self.df_unconstrained).add(1).expand_as(loc)\n        return df, loc, scale\n\n    @staticmethod\n    def make_dist(df, loc, scale):\n        return dist.StudentT(df, loc, scale)\n\n\nclass DiagNormalNet(nn.Module):\n    """"""\n    :class:`FullyConnected` network outputting a constrained ``loc,scale``\n    pair.\n\n    This is used to represent a conditional probability distribution of a\n    ``sizes[-1]``-sized diagonal Normal random variable conditioned on a\n    ``sizes[0]``-size real value, for example::\n\n        net = DiagNormalNet([3, 4, 5])\n        z = torch.randn(3)\n        loc, scale = net(z)\n        x = dist.Normal(loc, scale).sample()\n\n    This is intended for the latent ``z`` distribution and the prewhitened\n    ``x`` features, and conservatively clips ``loc`` and ``scale`` values.\n    """"""\n    def __init__(self, sizes):\n        assert len(sizes) >= 2\n        self.dim = sizes[-1]\n        super().__init__()\n        self.fc = FullyConnected(sizes[:-1] + [self.dim * 2])\n\n    def forward(self, x):\n        loc_scale = self.fc(x)\n        loc = loc_scale[..., :self.dim].clamp(min=-1e2, max=1e2)\n        scale = nn.functional.softplus(loc_scale[..., self.dim:]).add(1e-3).clamp(max=1e2)\n        return loc, scale\n\n\nclass PreWhitener(nn.Module):\n    """"""\n    Data pre-whitener.\n    """"""\n    def __init__(self, data):\n        super().__init__()\n        with torch.no_grad():\n            loc = data.mean(0)\n            scale = data.std(0)\n            scale[~(scale > 0)] = 1.\n            self.register_buffer(""loc"", loc)\n            self.register_buffer(""inv_scale"", scale.reciprocal())\n\n    def forward(self, data):\n        return (data - self.loc) * self.inv_scale\n\n\nclass Model(PyroModule):\n    """"""\n    Generative model for a causal model with latent confounder ``z`` and binary\n    treatment ``t``::\n\n        z ~ p(z)      # latent confounder\n        x ~ p(x|z)    # partial noisy observation of z\n        t ~ p(t|z)    # treatment, whose application is biased by z\n        y ~ p(y|t,z)  # outcome\n\n    Each of these distributions is defined by a neural network.  The ``y``\n    distribution is defined by a disjoint pair of neural networks defining\n    ``p(y|t=0,z)`` and ``p(y|t=1,z)``; this allows highly imbalanced treatment.\n\n    :param dict config: A dict specifying ``feature_dim``, ``latent_dim``,\n        ``hidden_dim``, ``num_layers``, and ``outcome_dist``.\n    """"""\n    def __init__(self, config):\n        self.latent_dim = config[""latent_dim""]\n        super().__init__()\n        self.x_nn = DiagNormalNet([config[""latent_dim""]] +\n                                  [config[""hidden_dim""]] * config[""num_layers""] +\n                                  [config[""feature_dim""]])\n        OutcomeNet = DistributionNet.get_class(config[""outcome_dist""])\n        # The y network is split between the two t values.\n        self.y0_nn = OutcomeNet([config[""latent_dim""]] +\n                                [config[""hidden_dim""]] * config[""num_layers""])\n        self.y1_nn = OutcomeNet([config[""latent_dim""]] +\n                                [config[""hidden_dim""]] * config[""num_layers""])\n        self.t_nn = BernoulliNet([config[""latent_dim""]])\n\n    def forward(self, x, t=None, y=None, size=None):\n        if size is None:\n            size = x.size(0)\n        with pyro.plate(""data"", size, subsample=x):\n            z = pyro.sample(""z"", self.z_dist())\n            x = pyro.sample(""x"", self.x_dist(z), obs=x)\n            t = pyro.sample(""t"", self.t_dist(z), obs=t)\n            y = pyro.sample(""y"", self.y_dist(t, z), obs=y)\n        return y\n\n    def y_mean(self, x, t=None):\n        with pyro.plate(""data"", x.size(0)):\n            z = pyro.sample(""z"", self.z_dist())\n            x = pyro.sample(""x"", self.x_dist(z), obs=x)\n            t = pyro.sample(""t"", self.t_dist(z), obs=t)\n        return self.y_dist(t, z).mean\n\n    def z_dist(self):\n        return dist.Normal(0, 1).expand([self.latent_dim]).to_event(1)\n\n    def x_dist(self, z):\n        loc, scale = self.x_nn(z)\n        return dist.Normal(loc, scale).to_event(1)\n\n    def y_dist(self, t, z):\n        # Parameters are not shared among t values.\n        params0 = self.y0_nn(z)\n        params1 = self.y1_nn(z)\n        t = t.bool()\n        params = [torch.where(t, p1, p0) for p0, p1 in zip(params0, params1)]\n        return self.y0_nn.make_dist(*params)\n\n    def t_dist(self, z):\n        logits, = self.t_nn(z)\n        return dist.Bernoulli(logits=logits)\n\n\nclass Guide(PyroModule):\n    """"""\n    Inference model for causal effect estimation with latent confounder ``z``\n    and binary treatment ``t``::\n\n        t ~ p(t|x)      # treatment\n        y ~ p(y|t,x)    # outcome\n        z ~ p(t|y,t,x)  # latent confounder, an embedding\n\n    Each of these distributions is defined by a neural network.  The ``y`` and\n    ``z`` distributions are defined by disjoint pairs of neural networks\n    defining ``p(-|t=0,...)`` and ``p(-|t=1,...)``; this allows highly\n    imbalanced treatment.\n\n    :param dict config: A dict specifying ``feature_dim``, ``latent_dim``,\n        ``hidden_dim``, ``num_layers``, and ``outcome_dist``.\n    """"""\n    def __init__(self, config):\n        self.latent_dim = config[""latent_dim""]\n        OutcomeNet = DistributionNet.get_class(config[""outcome_dist""])\n        super().__init__()\n        self.t_nn = BernoulliNet([config[""feature_dim""]])\n        # The y and z networks both follow an architecture where the first few\n        # layers are shared for t in {0,1}, but the final layer is split\n        # between the two t values.\n        self.y_nn = FullyConnected([config[""feature_dim""]] +\n                                   [config[""hidden_dim""]] * (config[""num_layers""] - 1),\n                                   final_activation=nn.ELU())\n        self.y0_nn = OutcomeNet([config[""hidden_dim""]])\n        self.y1_nn = OutcomeNet([config[""hidden_dim""]])\n        self.z_nn = FullyConnected([1 + config[""feature_dim""]] +\n                                   [config[""hidden_dim""]] * (config[""num_layers""] - 1),\n                                   final_activation=nn.ELU())\n        self.z0_nn = DiagNormalNet([config[""hidden_dim""], config[""latent_dim""]])\n        self.z1_nn = DiagNormalNet([config[""hidden_dim""], config[""latent_dim""]])\n\n    def forward(self, x, t=None, y=None, size=None):\n        if size is None:\n            size = x.size(0)\n        with pyro.plate(""data"", size, subsample=x):\n            # The t and y sites are needed for prediction, and participate in\n            # the auxiliary CEVAE loss. We mark them auxiliary to indicate they\n            # do not correspond to latent variables during training.\n            t = pyro.sample(""t"", self.t_dist(x), obs=t, infer={""is_auxiliary"": True})\n            y = pyro.sample(""y"", self.y_dist(t, x), obs=y, infer={""is_auxiliary"": True})\n            # The z site participates only in the usual ELBO loss.\n            pyro.sample(""z"", self.z_dist(y, t, x))\n\n    def t_dist(self, x):\n        logits, = self.t_nn(x)\n        return dist.Bernoulli(logits=logits)\n\n    def y_dist(self, t, x):\n        # The first n-1 layers are identical for all t values.\n        hidden = self.y_nn(x)\n        # In the final layer params are not shared among t values.\n        params0 = self.y0_nn(hidden)\n        params1 = self.y1_nn(hidden)\n        t = t.bool()\n        params = [torch.where(t, p1, p0) for p0, p1 in zip(params0, params1)]\n        return self.y0_nn.make_dist(*params)\n\n    def z_dist(self, y, t, x):\n        # The first n-1 layers are identical for all t values.\n        y_x = torch.cat([y.unsqueeze(-1), x], dim=-1)\n        hidden = self.z_nn(y_x)\n        # In the final layer params are not shared among t values.\n        params0 = self.z0_nn(hidden)\n        params1 = self.z1_nn(hidden)\n        t = t.bool().unsqueeze(-1)\n        params = [torch.where(t, p1, p0) for p0, p1 in zip(params0, params1)]\n        return dist.Normal(*params).to_event(1)\n\n\nclass TraceCausalEffect_ELBO(Trace_ELBO):\n    """"""\n    Loss function for training a :class:`CEVAE`.\n    From [1], the CEVAE objective (to maximize) is::\n\n        -loss = ELBO + log q(t|x) + log q(y|t,x)\n    """"""\n    def _differentiable_loss_particle(self, model_trace, guide_trace):\n        # Construct -ELBO part.\n        blocked_names = [name for name, site in guide_trace.nodes.items()\n                         if site[""type""] == ""sample"" and site[""is_observed""]]\n        blocked_guide_trace = guide_trace.copy()\n        for name in blocked_names:\n            del blocked_guide_trace.nodes[name]\n        loss, surrogate_loss = super()._differentiable_loss_particle(\n            model_trace, blocked_guide_trace)\n\n        # Add log q terms.\n        for name in blocked_names:\n            log_q = guide_trace.nodes[name][""log_prob_sum""]\n            loss = loss - torch_item(log_q)\n            surrogate_loss = surrogate_loss - log_q\n\n        return loss, surrogate_loss\n\n    @torch.no_grad()\n    def loss(self, model, guide, *args, **kwargs):\n        return torch_item(self.differentiable_loss(model, guide, *args, **kwargs))\n\n\nclass CEVAE(nn.Module):\n    """"""\n    Main class implementing a Causal Effect VAE [1]. This assumes a graphical model\n\n    .. graphviz:: :graphviz_dot: neato\n\n        digraph {\n            Z [pos=""1,2!"",style=filled];\n            X [pos=""2,1!""];\n            y [pos=""1,0!""];\n            t [pos=""0,1!""];\n            Z -> X;\n            Z -> t;\n            Z -> y;\n            t -> y;\n        }\n\n    where `t` is a binary treatment variable, `y` is an outcome, `Z` is\n    an unobserved confounder, and `X` is a noisy function of the hidden\n    confounder `Z`.\n\n    Example::\n\n        cevae = CEVAE(feature_dim=5)\n        cevae.fit(x_train, t_train, y_train)\n        ite = cevae.ite(x_test)  # individual treatment effect\n        ate = ite.mean()         # average treatment effect\n\n    :ivar Model ~CEVAE.model: Generative model.\n    :ivar Guide ~CEVAE.guide: Inference model.\n    :param int feature_dim: Dimension of the feature space `x`.\n    :param str outcome_dist: One of: ""bernoulli"" (default), ""exponential"", ""laplace"",\n        ""normal"", ""studentt"".\n    :param int latent_dim: Dimension of the latent variable `z`.\n        Defaults to 20.\n    :param int hidden_dim: Dimension of hidden layers of fully connected\n        networks. Defaults to 200.\n    :param int num_layers: Number of hidden layers in fully connected networks.\n    :param int num_samples: Default number of samples for the :meth:`ite`\n        method. Defaults to 100.\n    """"""\n    def __init__(self, feature_dim, outcome_dist=""bernoulli"",\n                 latent_dim=20, hidden_dim=200, num_layers=3, num_samples=100):\n        config = dict(feature_dim=feature_dim, latent_dim=latent_dim,\n                      hidden_dim=hidden_dim, num_layers=num_layers,\n                      num_samples=num_samples)\n        for name, size in config.items():\n            if not (isinstance(size, int) and size > 0):\n                raise ValueError(""Expected {} > 0 but got {}"".format(name, size))\n        config[""outcome_dist""] = outcome_dist\n        self.feature_dim = feature_dim\n        self.num_samples = num_samples\n\n        super().__init__()\n        self.model = Model(config)\n        self.guide = Guide(config)\n\n    def fit(self, x, t, y,\n            num_epochs=100,\n            batch_size=100,\n            learning_rate=1e-3,\n            learning_rate_decay=0.1,\n            weight_decay=1e-4):\n        """"""\n        Train using :class:`~pyro.infer.svi.SVI` with the\n        :class:`TraceCausalEffect_ELBO` loss.\n\n        :param ~torch.Tensor x:\n        :param ~torch.Tensor t:\n        :param ~torch.Tensor y:\n        :param int num_epochs: Number of training epochs. Defaults to 100.\n        :param int batch_size: Batch size. Defaults to 100.\n        :param float learning_rate: Learning rate. Defaults to 1e-3.\n        :param float learning_rate_decay: Learning rate decay over all epochs;\n            the per-step decay rate will depend on batch size and number of epochs\n            such that the initial learning rate will be ``learning_rate`` and the final\n            learning rate will be ``learning_rate * learning_rate_decay``.\n            Defaults to 0.1.\n        :param float weight_decay: Weight decay. Defaults to 1e-4.\n        :return: list of epoch losses\n        """"""\n        assert x.dim() == 2 and x.size(-1) == self.feature_dim\n        assert t.shape == x.shape[:1]\n        assert y.shape == y.shape[:1]\n        self.whiten = PreWhitener(x)\n\n        dataset = TensorDataset(x, t, y)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        logger.info(""Training with {} minibatches per epoch"".format(len(dataloader)))\n        num_steps = num_epochs * len(dataloader)\n        optim = ClippedAdam({""lr"": learning_rate,\n                             ""weight_decay"": weight_decay,\n                             ""lrd"": learning_rate_decay ** (1 / num_steps)})\n        svi = SVI(self.model, self.guide, optim, TraceCausalEffect_ELBO())\n        losses = []\n        for epoch in range(num_epochs):\n            for x, t, y in dataloader:\n                x = self.whiten(x)\n                loss = svi.step(x, t, y, size=len(dataset)) / len(dataset)\n                logger.debug(""step {: >5d} loss = {:0.6g}"".format(len(losses), loss))\n                assert not torch_isnan(loss)\n                losses.append(loss)\n        return losses\n\n    @torch.no_grad()\n    def ite(self, x, num_samples=None, batch_size=None):\n        r""""""\n        Computes Individual Treatment Effect for a batch of data ``x``.\n\n        .. math::\n\n            ITE(x) = \\mathbb E\\bigl[ \\mathbf y \\mid \\mathbf X=x, do(\\mathbf t=1) \\bigr]\n                   - \\mathbb E\\bigl[ \\mathbf y \\mid \\mathbf X=x, do(\\mathbf t=0) \\bigr]\n\n        This has complexity ``O(len(x) * num_samples ** 2)``.\n\n        :param ~torch.Tensor x: A batch of data.\n        :param int num_samples: The number of monte carlo samples.\n            Defaults to ``self.num_samples`` which defaults to ``100``.\n        :param int batch_size: Batch size. Defaults to ``len(x)``.\n        :return: A ``len(x)``-sized tensor of estimated effects.\n        :rtype: ~torch.Tensor\n        """"""\n        if num_samples is None:\n            num_samples = self.num_samples\n        if not torch._C._get_tracing_state():\n            assert x.dim() == 2 and x.size(-1) == self.feature_dim\n\n        dataloader = [x] if batch_size is None else DataLoader(x, batch_size=batch_size)\n        logger.info(""Evaluating {} minibatches"".format(len(dataloader)))\n        result = []\n        for x in dataloader:\n            x = self.whiten(x)\n            with pyro.plate(""num_particles"", num_samples, dim=-2):\n                with poutine.trace() as tr, poutine.block(hide=[""y"", ""t""]):\n                    self.guide(x)\n                with poutine.do(data=dict(t=torch.zeros(()))):\n                    y0 = poutine.replay(self.model.y_mean, tr.trace)(x)\n                with poutine.do(data=dict(t=torch.ones(()))):\n                    y1 = poutine.replay(self.model.y_mean, tr.trace)(x)\n            ite = (y1 - y0).mean(0)\n            if not torch._C._get_tracing_state():\n                logger.debug(""batch ate = {:0.6g}"".format(ite.mean()))\n            result.append(ite)\n        return torch.cat(result)\n\n    def to_script_module(self):\n        """"""\n        Compile this module using :func:`torch.jit.trace_module` ,\n        assuming self has already been fit to data.\n\n        :return: A traced version of self with an :meth:`ite` method.\n        :rtype: torch.jit.ScriptModule\n        """"""\n        self.train(False)\n        fake_x = torch.randn(2, self.feature_dim)\n        with pyro.validation_enabled(False):\n            # Disable check_trace due to nondeterministic nodes.\n            result = torch.jit.trace_module(self, {""ite"": (fake_x,)}, check_trace=False)\n        return result\n'"
pyro/contrib/conjugate/__init__.py,0,b''
pyro/contrib/conjugate/infer.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import defaultdict\n\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.distributions.util import sum_leftmost\nfrom pyro import poutine\nfrom pyro.poutine.messenger import Messenger\nfrom pyro.poutine.util import site_is_subsample\n\n\ndef _make_cls(base, static_attrs, instance_attrs, parent_linkage=None):\n    r""""""\n    Dynamically create classes named `_ + base.__name__`, which extend the\n    base class with other optional instance and class attributes, and have\n    a custom `.expand` method to propagate these attributes on expanded\n    instances.\n\n    :param cls base: Base class.\n    :param dict static_attrs: static attributes to add to class.\n    :param dict instance_attrs: instance attributes for initialization.\n    :param str parent_linkage: attribute in the parent class that holds\n        a reference to the distribution class.\n    :return cls: dynamically generated class.\n    """"""\n    def _expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(cls, _instance)\n        for attr in instance_attrs:\n            setattr(new, attr, getattr(self, attr))\n        if parent_linkage:\n            setattr(new.parent, parent_linkage, new)\n        return base.expand(self, batch_shape, _instance=new)\n\n    name = ""_"" + base.__name__\n    cls = type(name, (base,), instance_attrs)\n    for k, v in static_attrs.items():\n        setattr(cls, k, v)\n    cls.expand = _expand\n    return cls\n\n\ndef _latent(base, parent):\n    return _make_cls(base, {""collapsible"": True}, {""site_name"": None, ""parent"": parent}, ""_latent"")\n\n\ndef _conditional(base, parent):\n    return _make_cls(base, {""marginalize_latent"": True}, {""parent"": parent}, ""_conditional"")\n\n\ndef _compound(base, parent):\n    return _make_cls(base, {}, {""parent"": parent})\n\n\nclass BetaBinomialPair:\n    def __init__(self):\n        self._latent = None\n        self._conditional = None\n\n    def latent(self, *args, **kwargs):\n        self._latent = _latent(dist.Beta, parent=self)(*args, **kwargs)\n        return self._latent\n\n    def conditional(self, *args, **kwargs):\n        self._conditional = _conditional(dist.Binomial, parent=self)(*args, **kwargs)\n        return self._conditional\n\n    def posterior(self, obs):\n        concentration1 = self._latent.concentration1\n        concentration0 = self._latent.concentration0\n        total_count = self._conditional.total_count\n        reduce_dims = len(obs.size()) - len(concentration1.size())\n        # Unexpand total_count to have the same shape as concentration0.\n        # Raise exception if this isn\'t possible.\n        total_count = sum_leftmost(total_count, reduce_dims)\n        summed_obs = sum_leftmost(obs, reduce_dims)\n        return dist.Beta(concentration1 + summed_obs,\n                         total_count + concentration0 - summed_obs,\n                         validate_args=self._latent._validate_args)\n\n    def compound(self):\n        return _compound(dist.BetaBinomial, parent=self)(concentration1=self._latent.concentration1,\n                                                         concentration0=self._latent.concentration0,\n                                                         total_count=self._conditional.total_count)\n\n\nclass GammaPoissonPair:\n    def __init__(self):\n        self._latent = None\n        self._conditional = None\n\n    def latent(self, *args, **kwargs):\n        self._latent = _latent(dist.Gamma, parent=self)(*args, **kwargs)\n        return self._latent\n\n    def conditional(self, *args, **kwargs):\n        self._conditional = _conditional(dist.Poisson, parent=self)(*args, **kwargs)\n        return self._conditional\n\n    def posterior(self, obs):\n        concentration = self._latent.concentration\n        rate = self._latent.rate\n        reduce_dims = len(obs.size()) - len(rate.size())\n        num_obs = obs.shape[:reduce_dims].numel()\n        summed_obs = sum_leftmost(obs, reduce_dims)\n        return dist.Gamma(concentration + summed_obs, rate + num_obs)\n\n    def compound(self):\n        return _compound(dist.GammaPoisson, parent=self)(concentration=self._latent.concentration,\n                                                         rate=self._latent.rate)\n\n\nclass UncollapseConjugateMessenger(Messenger):\n    r""""""\n    Replay regular sample sites in addition to uncollapsing any collapsed\n    conjugate sites.\n    """"""\n    def __init__(self, trace):\n        """"""\n        :param trace: a trace whose values should be reused\n\n        Constructor.\n        Stores trace in an attribute.\n        """"""\n        self.trace = trace\n        super().__init__()\n\n    def _pyro_sample(self, msg):\n        is_collapsible = getattr(msg[""fn""], ""collapsible"", False)\n        # uncollapse conjugate sites.\n        if is_collapsible:\n            conj_node, parent = None, None\n            for site_name in self.trace.observation_nodes + self.trace.stochastic_nodes:\n                parent = getattr(self.trace.nodes[site_name][""fn""], ""parent"", None)\n                if parent is not None and parent._latent.site_name == msg[""name""]:\n                    conj_node = self.trace.nodes[site_name]\n                    break\n            assert conj_node is not None, ""Collapsible latent site `{}` with no corresponding conjugate site.""\\\n                .format(msg[""name""])\n            msg[""fn""] = parent.posterior(conj_node[""value""])\n            msg[""value""] = msg[""fn""].sample()\n        # regular replay behavior.\n        else:\n            name = msg[""name""]\n            if name in self.trace:\n                guide_msg = self.trace.nodes[name]\n                if msg[""is_observed""]:\n                    return None\n                if guide_msg[""type""] != ""sample"":\n                    raise RuntimeError(""site {} must be sample in trace"".format(name))\n                msg[""done""] = True\n                msg[""value""] = guide_msg[""value""]\n                msg[""infer""] = guide_msg[""infer""]\n\n\ndef uncollapse_conjugate(fn=None, trace=None):\n    r""""""\n    This is similar to :function:`~pyro.poutine.replay` poutine, but in addition to\n    replaying the values at sample sites from the ``trace`` in the original callable\n    ``fn`` when the same sites are sampled, this also ""uncollapses"" any observed\n    compound distributions (defined in :module:`pyro.distributions.conjugate`)\n    by sampling the originally collapsed parameter values from its posterior distribution\n    followed by observing the data with the sampled parameter values.\n    """"""\n    msngr = UncollapseConjugateMessenger(trace)\n    return msngr(fn) if fn is not None else msngr\n\n\nclass CollapseConjugateMessenger(Messenger):\n    def _pyro_sample(self, msg):\n        is_collapsible = getattr(msg[""fn""], ""collapsible"", False)\n        marginalize_latent = getattr(msg[""fn""], ""marginalize_latent"", False)\n        if is_collapsible:\n            msg[""fn""].site_name = msg[""name""]\n            msg[""stop""] = True\n        elif marginalize_latent:\n            msg[""fn""] = msg[""fn""].parent.compound()\n        else:\n            return\n\n\ndef collapse_conjugate(fn=None):\n    r""""""\n    This replaces a latent-observed pair by collapsing the latent site\n    (whose distribution has attribute `collapsible=True`), and replacing the\n    observed site (whose distribution has attribute `marginalize_latent=True`)\n    with a compound probability distribution that marginalizes out the latent\n    site.\n    """"""\n    msngr = CollapseConjugateMessenger()\n    return msngr(fn) if fn is not None else msngr\n\n\ndef posterior_replay(model, posterior_samples, *args, **kwargs):\n    r""""""\n    Given a model and samples from the posterior (potentially with conjugate sites\n    collapsed), return a `dict` of samples from the posterior with conjugate sites\n    uncollapsed. Note that this can also be used to generate samples from the\n    posterior predictive distribution.\n\n    :param model: Python callable.\n    :param dict posterior_samples: posterior samples keyed by site name.\n    :param args: arguments to `model`.\n    :param kwargs: keyword arguments to `model`.\n    :return: `dict` of samples from the posterior.\n    """"""\n    posterior_samples = posterior_samples.copy()\n    num_samples = kwargs.pop(""num_samples"", None)\n    assert posterior_samples or num_samples, ""`num_samples` must be provided if `posterior_samples` is empty.""\n    if num_samples is None:\n        num_samples = list(posterior_samples.values())[0].shape[0]\n\n    return_samples = defaultdict(list)\n    for i in range(num_samples):\n        conditioned_nodes = {k: v[i] for k, v in posterior_samples.items()}\n        collapsed_trace = poutine.trace(poutine.condition(collapse_conjugate(model), conditioned_nodes))\\\n            .get_trace(*args, **kwargs)\n        trace = poutine.trace(uncollapse_conjugate(model, collapsed_trace)).get_trace(*args, **kwargs)\n        for name, site in trace.iter_stochastic_nodes():\n            if not site_is_subsample(site):\n                return_samples[name].append(site[""value""])\n\n    return {k: torch.stack(v) for k, v in return_samples.items()}\n'"
pyro/contrib/easyguide/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.easyguide.easyguide import EasyGuide, easy_guide\n\n\n__all__ = [\n    ""EasyGuide"",\n    ""easy_guide"",\n]\n'"
pyro/contrib/easyguide/easyguide.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport re\nimport weakref\nfrom abc import ABCMeta, abstractmethod\nfrom contextlib import ExitStack\n\nimport torch\nfrom torch.distributions import biject_to\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nimport pyro.poutine.runtime as runtime\nfrom pyro.distributions.util import broadcast_shape, sum_rightmost\nfrom pyro.infer.autoguide.initialization import InitMessenger\nfrom pyro.infer.autoguide.guides import prototype_hide_fn\nfrom pyro.nn.module import PyroModule, PyroParam\n\n\nclass _EasyGuideMeta(type(PyroModule), ABCMeta):\n    pass\n\n\nclass EasyGuide(PyroModule, metaclass=_EasyGuideMeta):\n    """"""\n    Base class for ""easy guides"", which are more flexible than\n    :class:`~pyro.infer.AutoGuide` s, but are easier to write than raw Pyro guides.\n\n    Derived classes should define a :meth:`guide` method. This :meth:`guide`\n    method can combine ordinary guide statements (e.g. ``pyro.sample`` and\n    ``pyro.param``) with the following special statements:\n\n    - ``group = self.group(...)`` selects multiple ``pyro.sample`` sites in the\n      model. See :class:`Group` for subsequent methods.\n    - ``with self.plate(...): ...`` should be used instead of ``pyro.plate``.\n    - ``self.map_estimate(...)`` uses a ``Delta`` guide for a single site.\n\n    Derived classes may also override the :meth:`init` method to provide custom\n    initialization for models sites.\n\n    :param callable model: A Pyro model.\n    """"""\n    def __init__(self, model):\n        super().__init__()\n        self._pyro_name = type(self).__name__\n        self._model = (model,)\n        self.prototype_trace = None\n        self.frames = {}\n        self.groups = {}\n        self.plates = {}\n\n    @property\n    def model(self):\n        return self._model[0]\n\n    def _setup_prototype(self, *args, **kwargs):\n        # run the model so we can inspect its structure\n        model = poutine.block(InitMessenger(self.init)(self.model), prototype_hide_fn)\n        self.prototype_trace = poutine.block(poutine.trace(model).get_trace)(*args, **kwargs)\n\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            for frame in site[""cond_indep_stack""]:\n                if not frame.vectorized:\n                    raise NotImplementedError(""EasyGuide does not support sequential pyro.plate"")\n                self.frames[frame.name] = frame\n\n    @abstractmethod\n    def guide(self, *args, **kargs):\n        """"""\n        Guide implementation, to be overridden by user.\n        """"""\n        raise NotImplementedError\n\n    def init(self, site):\n        """"""\n        Model initialization method, may be overridden by user.\n\n        This should input a site and output a valid sample from that site.\n        The default behavior is to draw a random sample::\n\n            return site[""fn""]()\n\n        For other possible initialization functions see\n        http://docs.pyro.ai/en/stable/infer.autoguide.html#module-pyro.infer.autoguide.initialization\n        """"""\n        return site[""fn""]()\n\n    def forward(self, *args, **kwargs):\n        """"""\n        Runs the guide. This is typically used by inference algorithms.\n        """"""\n        if self.prototype_trace is None:\n            self._setup_prototype(*args, **kwargs)\n        result = self.guide(*args, **kwargs)\n        self.plates.clear()\n        return result\n\n    def plate(self, name, size=None, subsample_size=None, subsample=None, *args, **kwargs):\n        """"""\n        A wrapper around :class:`pyro.plate` to allow `EasyGuide` to\n        automatically construct plates. You should use this rather than\n        :class:`pyro.plate` inside your :meth:`guide` implementation.\n        """"""\n        if name not in self.plates:\n            self.plates[name] = pyro.plate(name, size, subsample_size, subsample, *args, **kwargs)\n        return self.plates[name]\n\n    def group(self, match="".*""):\n        """"""\n        Select a :class:`Group` of model sites for joint guidance.\n\n        :param str match: A regex string matching names of model sample sites.\n        :return: A group of model sites.\n        :rtype: Group\n        """"""\n        if match not in self.groups:\n            sites = [site\n                     for name, site in self.prototype_trace.iter_stochastic_nodes()\n                     if re.match(match, name)]\n            if not sites:\n                raise ValueError(""EasyGuide.group() pattern {} matched no model sites""\n                                 .format(repr(match)))\n            self.groups[match] = Group(self, sites)\n        return self.groups[match]\n\n    def map_estimate(self, name):\n        """"""\n        Construct a maximum a posteriori (MAP) guide using Delta distributions.\n\n        :param str name: The name of a model sample site.\n        :return: A sampled value.\n        :rtype: torch.Tensor\n        """"""\n        site = self.prototype_trace.nodes[name]\n        fn = site[""fn""]\n        event_dim = fn.event_dim\n        init_needed = not hasattr(self, name)\n        if init_needed:\n            init_value = site[""value""].detach()\n        with ExitStack() as stack:\n            for frame in site[""cond_indep_stack""]:\n                plate = self.plate(frame.name)\n                if plate not in runtime._PYRO_STACK:\n                    stack.enter_context(plate)\n                elif init_needed and plate.subsample_size < plate.size:\n                    # Repeat the init_value to full size.\n                    dim = plate.dim - event_dim\n                    assert init_value.size(dim) == plate.subsample_size\n                    ind = torch.arange(plate.size, device=init_value.device)\n                    ind = ind % plate.subsample_size\n                    init_value = init_value.index_select(dim, ind)\n            if init_needed:\n                setattr(self, name, PyroParam(init_value, fn.support, event_dim))\n            value = getattr(self, name)\n            return pyro.sample(name, dist.Delta(value, event_dim=event_dim))\n\n\nclass Group:\n    """"""\n    An autoguide helper to match a group of model sites.\n\n    :ivar torch.Size event_shape: The total flattened concatenated shape of all\n        matching sample sites in the model.\n    :ivar list prototype_sites: A list of all matching sample sites in a\n        prototype trace of the model.\n    :param EasyGuide guide: An easyguide instance.\n    :param list sites: A list of model sites.\n    """"""\n    def __init__(self, guide, sites):\n        assert isinstance(sites, list)\n        assert sites\n        self._guide = weakref.ref(guide)\n        self.prototype_sites = sites\n        self._site_sizes = {}\n        self._site_batch_shapes = {}\n\n        # A group is in a frame only if all its sample sites are in that frame.\n        # Thus a group can be subsampled only if all its sites can be subsampled.\n        self.common_frames = frozenset.intersection(*(\n            frozenset(f for f in site[""cond_indep_stack""] if f.vectorized)\n            for site in sites))\n        rightmost_common_dim = -float(\'inf\')\n        if self.common_frames:\n            rightmost_common_dim = max(f.dim for f in self.common_frames)\n\n        # Compute flattened concatenated event_shape and split batch_shape into\n        # a common batch_shape (which can change each SVI step due to\n        # subsampling) and site batch_shapes (which must remain constant size).\n        for site in sites:\n            site_event_numel = torch.Size(site[""fn""].event_shape).numel()\n            site_batch_shape = list(site[""fn""].batch_shape)\n            for f in self.common_frames:\n                # Consider this dim part of the common_batch_shape.\n                site_batch_shape[f.dim] = 1\n            while site_batch_shape and site_batch_shape[0] == 1:\n                site_batch_shape = site_batch_shape[1:]\n            if len(site_batch_shape) > -rightmost_common_dim:\n                raise ValueError(\n                    ""Group expects all per-site plates to be right of all common plates, ""\n                    ""but found a per-site plate {} on left at site {}""\n                    .format(-len(site_batch_shape), repr(site[""name""])))\n            site_batch_shape = torch.Size(site_batch_shape)\n            self._site_batch_shapes[site[""name""]] = site_batch_shape\n            self._site_sizes[site[""name""]] = site_batch_shape.numel() * site_event_numel\n        self.event_shape = torch.Size([sum(self._site_sizes.values())])\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[\'_guide\'] = state[\'_guide\']()  # weakref -> ref\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        self._guide = weakref.ref(self._guide)  # ref -> weakref\n\n    @property\n    def guide(self):\n        return self._guide()\n\n    def sample(self, guide_name, fn, infer=None):\n        """"""\n        Wrapper around ``pyro.sample()`` to create a single auxiliary sample\n        site and then unpack to multiple sample sites for model replay.\n\n        :param str guide_name: The name of the auxiliary guide site.\n        :param callable fn: A distribution with shape ``self.event_shape``.\n        :param dict infer: Optional inference configuration dict.\n        :returns: A pair ``(guide_z, model_zs)`` where ``guide_z`` is the\n            single concatenated blob and ``model_zs`` is a dict mapping\n            site name to constrained model sample.\n        :rtype: tuple\n        """"""\n        # Sample a packed tensor.\n        if fn.event_shape != self.event_shape:\n            raise ValueError(""Invalid fn.event_shape for group: expected {}, actual {}""\n                             .format(tuple(self.event_shape), tuple(fn.event_shape)))\n        if infer is None:\n            infer = {}\n        infer[""is_auxiliary""] = True\n        guide_z = pyro.sample(guide_name, fn, infer=infer)\n        common_batch_shape = guide_z.shape[:-1]\n\n        model_zs = {}\n        pos = 0\n        for site in self.prototype_sites:\n            name = site[""name""]\n            fn = site[""fn""]\n\n            # Extract slice from packed sample.\n            size = self._site_sizes[name]\n            batch_shape = broadcast_shape(common_batch_shape, self._site_batch_shapes[name])\n            unconstrained_z = guide_z[..., pos: pos + size]\n            unconstrained_z = unconstrained_z.reshape(batch_shape + fn.event_shape)\n            pos += size\n\n            # Transform to constrained space.\n            transform = biject_to(fn.support)\n            z = transform(unconstrained_z)\n            log_density = transform.inv.log_abs_det_jacobian(z, unconstrained_z)\n            log_density = sum_rightmost(log_density, log_density.dim() - z.dim() + fn.event_dim)\n            delta_dist = dist.Delta(z, log_density=log_density, event_dim=fn.event_dim)\n\n            # Replay model sample statement.\n            with ExitStack() as stack:\n                for frame in site[""cond_indep_stack""]:\n                    plate = self.guide.plate(frame.name)\n                    if plate not in runtime._PYRO_STACK:\n                        stack.enter_context(plate)\n                model_zs[name] = pyro.sample(name, delta_dist)\n\n        return guide_z, model_zs\n\n    def map_estimate(self):\n        """"""\n        Construct a maximum a posteriori (MAP) guide using Delta distributions.\n\n        :return: A dict mapping model site name to sampled value.\n        :rtype: dict\n        """"""\n        return {site[""name""]: self.guide.map_estimate(site[""name""])\n                for site in self.prototype_sites}\n\n\ndef easy_guide(model):\n    """"""\n    Convenience decorator to create an :class:`EasyGuide` .\n    The following are equivalent::\n\n        # Version 1. Decorate a function.\n        @easy_guide(model)\n        def guide(self, foo, bar):\n            return my_guide(foo, bar)\n\n        # Version 2. Create and instantiate a subclass of EasyGuide.\n        class Guide(EasyGuide):\n            def guide(self, foo, bar):\n                return my_guide(foo, bar)\n        guide = Guide(model)\n\n    Note ``@easy_guide`` wrappers cannot be pickled; to build a guide that can\n    be pickled, instead subclass from :class:`EasyGuide`.\n\n    :param callable model: a Pyro model.\n    """"""\n\n    def decorator(fn):\n        Guide = type(fn.__name__, (EasyGuide,), {""guide"": fn})\n        return Guide(model)\n\n    return decorator\n'"
pyro/contrib/epidemiology/__init__.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .compartmental import CompartmentalModel\nfrom .distributions import beta_binomial_dist, binomial_dist, infection_dist\nfrom .models import (HeterogeneousSIRModel, OverdispersedSEIRModel, OverdispersedSIRModel, RegionalSIRModel,\n                     SimpleSEIRModel, SimpleSIRModel, SparseSIRModel, SuperspreadingSEIRModel, SuperspreadingSIRModel,\n                     UnknownStartSIRModel)\n\n__all__ = [\n    ""CompartmentalModel"",\n    ""HeterogeneousSIRModel"",\n    ""OverdispersedSEIRModel"",\n    ""OverdispersedSIRModel"",\n    ""RegionalSIRModel"",\n    ""SimpleSEIRModel"",\n    ""SimpleSIRModel"",\n    ""SparseSIRModel"",\n    ""SuperspreadingSEIRModel"",\n    ""SuperspreadingSIRModel"",\n    ""UnknownStartSIRModel"",\n    ""beta_binomial_dist"",\n    ""binomial_dist"",\n    ""infection_dist"",\n]\n'"
pyro/contrib/epidemiology/compartmental.py,22,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport operator\nimport re\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom contextlib import ExitStack\nfrom functools import reduce\n\nimport torch\nfrom torch.distributions import biject_to, constraints\nfrom torch.distributions.utils import lazy_property\n\nimport pyro.distributions as dist\nimport pyro.distributions.hmm\nimport pyro.poutine as poutine\nfrom pyro.distributions.transforms import HaarTransform\nfrom pyro.infer import MCMC, NUTS, SMCFilter, infer_discrete\nfrom pyro.infer.autoguide import init_to_generated, init_to_value\nfrom pyro.infer.mcmc import ArrowheadMassMatrix\nfrom pyro.infer.reparam import HaarReparam, SplitReparam\nfrom pyro.infer.smcfilter import SMCFailed\nfrom pyro.infer.util import is_validation_enabled, site_is_subsample\nfrom pyro.util import optional, warn_if_nan\n\nfrom .distributions import set_approx_log_prob_tol, set_approx_sample_thresh\nfrom .util import align_samples, cat2, clamp, quantize, quantize_enumerate\n\nlogger = logging.getLogger(__name__)\n\n\ndef _require_double_precision():\n    if torch.get_default_dtype() != torch.float64:\n        warnings.warn(""CompartmentalModel is unstable for dtypes less than torch.float64; ""\n                      ""try torch.set_default_dtype(torch.float64)"",\n                      RuntimeWarning)\n\n\nclass CompartmentalModel(ABC):\n    """"""\n    Abstract base class for discrete-time discrete-value stochastic\n    compartmental models.\n\n    Derived classes must implement methods :meth:`initialize` and\n    :meth:`transition`. Derived classes may optionally implement\n    :meth:`global_model`, :meth:`compute_flows`, and :meth:`heuristic`.\n\n    Example usage::\n\n        # First implement a concrete derived class.\n        class MyModel(CompartmentalModel):\n            def __init__(self, ...): ...\n            def global_model(self): ...\n            def initialize(self, params): ...\n            def transition(self, params, state, t): ...\n\n        # Run inference to fit the model to data.\n        model = MyModel(...)\n        model.fit(num_samples=100)\n        R0 = model.samples[""R0""]  # An example parameter.\n        print(""R0 = {:0.3g} \\u00B1 {:0.3g}"".format(R0.mean(), R0.std()))\n\n        # Predict latent variables.\n        samples = model.predict()\n\n        # Forecast forward.\n        samples = model.predict(forecast=30)\n\n        # You can assess future interventions (applied after ``duration``) by\n        # storing them as attributes that are read by your derived methods.\n        model.my_intervention = False\n        samples1 = model.predict(forecast=30)\n        model.my_intervention = True\n        samples2 = model.predict(forecast=30)\n        effect = samples2[""my_result""].mean() - samples1[""my_result""].mean()\n        print(""average effect = {:0.3g}"".format(effect))\n\n    :ivar dict samples: Dictionary of posterior samples.\n    :param list compartments: A list of strings of compartment names.\n    :param int duration: The number of discrete time steps in this model.\n    :param population: Either the total population of a single-region model or\n        a tensor of each region\'s population in a regional model.\n    :type population: int or torch.Tensor\n    :param tuple approximate: Names of compartments for which pointwise\n        approximations should be provided in :meth:`transition`, e.g. if you\n        specify ``approximate=(""I"")`` then the ``state[""I_approx""]`` will be a\n        continuous-valued non-enumerated point estimate of ``state[""I""]``.\n        Approximations are useful to reduce computational cost. Approximations\n        are continuous-valued with support ``(-0.5, population + 0.5)``.\n    :param int num_quant_bins: Number of quantization bins in the auxiliary\n        variable spline. Defaults to 4.\n    """"""\n\n    def __init__(self, compartments, duration, population, *,\n                 num_quant_bins=4, approximate=()):\n        super().__init__()\n\n        assert isinstance(duration, int)\n        assert duration >= 1\n        self.duration = duration\n\n        if isinstance(population, torch.Tensor):\n            assert population.dim() == 1\n            assert (population >= 1).all()\n            self.is_regional = True\n            self.max_plate_nesting = 2  # [time, region]\n        else:\n            assert isinstance(population, int)\n            assert population >= 2\n            self.is_regional = False\n            self.max_plate_nesting = 1  # [time]\n        self.population = population\n\n        compartments = tuple(compartments)\n        assert all(isinstance(name, str) for name in compartments)\n        assert len(compartments) == len(set(compartments))\n        self.compartments = compartments\n\n        assert isinstance(approximate, tuple)\n        assert all(name in compartments for name in approximate)\n        self.approximate = approximate\n\n        # Inference state.\n        self.samples = {}\n        self._clear_plates()\n\n    @property\n    def time_plate(self):\n        """"""\n        A ``pyro.plate`` for the time dimension.\n        """"""\n        if self._time_plate is None:\n            self._time_plate = pyro.plate(""time"", self.duration,\n                                          dim=-2 if self.is_regional else -1)\n        return self._time_plate\n\n    @property\n    def region_plate(self):\n        """"""\n        Either a ``pyro.plate`` or a trivial ``ExitStack`` depending on whether\n        this model ``.is_regional``.\n        """"""\n        if self._region_plate is None:\n            if self.is_regional:\n                self._region_plate = pyro.plate(""region"", len(self.population), dim=-1)\n            else:\n                self._region_plate = ExitStack()  # Trivial context manager.\n        return self._region_plate\n\n    def _clear_plates(self):\n        self._time_plate = None\n        self._region_plate = None\n\n    @lazy_property\n    def full_mass(self):\n        """"""\n        A list of a single tuple of the names of global random variables.\n        """"""\n        with torch.no_grad(), poutine.block(), poutine.trace() as tr:\n            self.global_model()\n        return [tuple(name for name, site in tr.trace.iter_stochastic_nodes()\n                      if not site_is_subsample(site))]\n\n    @lazy_property\n    def series(self):\n        """"""\n        A frozenset of names of sample sites that are sampled each time step.\n        """"""\n        # Trace a simple invocation of .transition().\n        with torch.no_grad(), poutine.block():\n            params = self.global_model()\n            prev = self.initialize(params)\n            for name in self.approximate:\n                prev[name + ""_approx""] = prev[name]\n            curr = prev.copy()\n            with poutine.trace() as tr:\n                self.transition(params, curr, 0)\n        return frozenset(re.match(""(.*)_0"", name).group(1)\n                         for name, site in tr.trace.nodes.items()\n                         if site[""type""] == ""sample""\n                         if not site_is_subsample(site))\n\n    # Overridable attributes and methods ########################################\n\n    def global_model(self):\n        """"""\n        Samples and returns any global parameters.\n\n        :returns: An arbitrary object of parameters (e.g. ``None`` or a tuple).\n        """"""\n        return None\n\n    # TODO Allow stochastic initialization.\n    @abstractmethod\n    def initialize(self, params):\n        """"""\n        Returns initial counts in each compartment.\n\n        :param params: The global params returned by :meth:`global_model`.\n        :returns: A dict mapping compartment name to initial value.\n        :rtype: dict\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def transition(self, params, state, t):\n        """"""\n        Forward generative process for dynamics.\n\n        This inputs a current ``state`` and stochastically updates that\n        state in-place.\n\n        Note that this method is called under multiple different\n        interpretations, including batched and vectorized interpretations.\n        During :meth:`generate` this is called to generate a single sample.\n        During :meth:`heuristic` this is called to generate a batch of samples\n        for SMC.  During :meth:`fit` this is called both in vectorized form\n        (vectorizing over time) and in sequential form (for a single time\n        step); both forms enumerate over discrete latent variables.  During\n        :meth:`predict` this is called to forecast a batch of samples,\n        conditioned on posterior samples for the time interval\n        ``[0:duration]``.\n\n        :param params: The global params returned by :meth:`global_model`.\n        :param dict state: A dictionary mapping compartment name to current\n            tensor value. This should be updated in-place.\n        :param t: A time-like index. During inference ``t`` may be either a\n            slice (for vectorized inference) or an integer time index. During\n            prediction ``t`` will be integer time index.\n        :type t: int or slice\n        """"""\n        raise NotImplementedError\n\n    def compute_flows(self, prev, curr, t):\n        """"""\n        Computes flows between compartments, given compartment populations\n        before and after time step t.\n\n        The default implementation assumes sequential flows terminating in an\n        implicit compartment named ""R"". For example if::\n\n            compartment_names = (""S"", ""E"", ""I"")\n\n        the default implementation computes at time step ``t = 9``::\n\n            flows[""S2E_9""] = prev[""S""] - curr[""S""]\n            flows[""E2I_9""] = prev[""E""] - curr[""E""] + flows[""S2E_9""]\n            flows[""I2R_9""] = prev[""I""] - curr[""I""] + flows[""E2I_9""]\n\n        For more complex flows (non-sequential, branching, looping,\n        duplicating, etc.), users may override this method.\n\n        :param dict state: A dictionary mapping compartment name to current\n            tensor value. This should be updated in-place.\n        :param t: A time-like index. During inference ``t`` may be either a\n            slice (for vectorized inference) or an integer time index. During\n            prediction ``t`` will be integer time index.\n        :type t: int or slice\n        :returns: A dict mapping flow name to tensor value.\n        :rtype: dict\n        """"""\n        flows = {}\n        flow = 0\n        for source, destin in zip(self.compartments, self.compartments[1:] + (""R"",)):\n            flow = prev[source] - curr[source] + flow\n            flows[""{}2{}_{}"".format(source, destin, t)] = flow\n        return flows\n\n    # Inference interface ########################################\n\n    @torch.no_grad()\n    @set_approx_sample_thresh(1000)\n    def generate(self, fixed={}):\n        """"""\n        Generate data from the prior.\n\n        :pram dict fixed: A dictionary of parameters on which to condition.\n            These must be top-level parentless nodes, i.e. have no\n            upstream stochastic dependencies.\n        :returns: A dictionary mapping sample site name to sampled value.\n        :rtype: dict\n        """"""\n        fixed = {k: torch.as_tensor(v) for k, v in fixed.items()}\n        model = self._generative_model\n        model = poutine.condition(model, fixed)\n        trace = poutine.trace(model).get_trace()\n        samples = OrderedDict((name, site[""value""])\n                              for name, site in trace.nodes.items()\n                              if site[""type""] == ""sample"")\n\n        self._concat_series(samples)\n        return samples\n\n    @set_approx_log_prob_tol(0.1)\n    def fit(self, **options):\n        r""""""\n        Runs inference to generate posterior samples.\n\n        This uses the :class:`~pyro.infer.mcmc.nuts.NUTS` kernel to run\n        :class:`~pyro.infer.mcmc.api.MCMC`, setting the ``.samples``\n        attribute on completion.\n\n        :param \\*\\*options: Options passed to\n            :class:`~pyro.infer.mcmc.api.MCMC`. The remaining options are\n            pulled out and have special meaning.\n        :param int max_tree_depth: (Default 5). Max tree depth of the\n            :class:`~pyro.infer.mcmc.nuts.NUTS` kernel.\n        :param full_mass: Specification of mass matrix of the\n            :class:`~pyro.infer.mcmc.nuts.NUTS` kernel. Defaults to full mass\n            over global random variables.\n        :param bool arrowhead_mass: Whether to treat ``full_mass`` as the head\n            of an arrowhead matrix versus simply as a block. Defaults to False.\n        :param int num_quant_bins: The number of quantization bins to use. Note\n            that computational cost is exponential in `num_quant_bins`.\n            Defaults to 4.\n        :param bool haar: Whether to use a Haar wavelet reparameterizer.\n        :param int haar_full_mass: Number of low frequency Haar components to\n            include in the full mass matrix. If nonzero this implies\n            ``haar=True``.\n        :param int heuristic_num_particles: Passed to :meth:`heuristic` as\n            ``num_particles``. Defaults to 1024.\n        :returns: An MCMC object for diagnostics, e.g. ``MCMC.summary()``.\n        :rtype: ~pyro.infer.mcmc.api.MCMC\n        """"""\n        _require_double_precision()\n\n        # Parse options, saving some for use in .predict().\n        self.num_quant_bins = options.pop(""num_quant_bins"", 4)\n        haar = options.pop(""haar"", False)\n        assert isinstance(haar, bool)\n        haar_full_mass = options.pop(""haar_full_mass"", 0)\n        assert isinstance(haar_full_mass, int)\n        assert haar_full_mass >= 0\n        haar_full_mass = min(haar_full_mass, self.duration)\n        haar = haar or (haar_full_mass > 0)\n\n        # Heuristically initialize to feasible latents.\n        heuristic_options = {k.replace(""heuristic_"", """"): options.pop(k)\n                             for k in list(options)\n                             if k.startswith(""heuristic_"")}\n\n        def heuristic():\n            with poutine.block():\n                init_values = self.heuristic(**heuristic_options)\n            assert isinstance(init_values, dict)\n            assert ""auxiliary"" in init_values, \\\n                "".heuristic() did not define auxiliary value""\n            if haar:\n                # Also initialize Haar transformed coordinates.\n                x = init_values[""auxiliary""]\n                x = biject_to(constraints.interval(-0.5, self.population + 0.5)).inv(x)\n                x = HaarTransform(dim=-2 if self.is_regional else -1, flip=True)(x)\n                init_values[""auxiliary_haar""] = x\n            if haar_full_mass:\n                # Also split into low- and high-frequency parts.\n                x0, x1 = init_values[""auxiliary_haar""].split(\n                    [haar_full_mass, self.duration - haar_full_mass],\n                    dim=-2 if self.is_regional else -1)\n                init_values[""auxiliary_haar_split_0""] = x0\n                init_values[""auxiliary_haar_split_1""] = x1\n            logger.info(""Heuristic init: {}"".format("", "".join(\n                ""{}={:0.3g}"".format(k, v.item())\n                for k, v in init_values.items()\n                if v.numel() == 1)))\n            return init_to_value(values=init_values)\n\n        # Configure a kernel.\n        logger.info(""Running inference..."")\n        max_tree_depth = options.pop(""max_tree_depth"", 5)\n        full_mass = options.pop(""full_mass"", self.full_mass)\n        model = self._vectorized_model\n        if haar:\n            rep = HaarReparam(dim=-2 if self.is_regional else -1, flip=True)\n            model = poutine.reparam(model, {""auxiliary"": rep})\n        if haar_full_mass:\n            assert full_mass and isinstance(full_mass, list)\n            full_mass = full_mass[:]\n            full_mass[0] = full_mass[0] + (""auxiliary_haar_split_0"",)\n            rep = SplitReparam([haar_full_mass, self.duration - haar_full_mass],\n                               dim=-2 if self.is_regional else -1)\n            model = poutine.reparam(model, {""auxiliary_haar"": rep})\n        kernel = NUTS(model,\n                      full_mass=full_mass,\n                      init_strategy=init_to_generated(generate=heuristic),\n                      max_plate_nesting=self.max_plate_nesting,\n                      max_tree_depth=max_tree_depth)\n        if options.pop(""arrowhead_mass"", False):\n            kernel.mass_matrix_adapter = ArrowheadMassMatrix()\n\n        # Run mcmc.\n        options.setdefault(""disable_validation"", None)\n        mcmc = MCMC(kernel, **options)\n        mcmc.run()\n        self.samples = mcmc.get_samples()\n        if haar_full_mass:\n            # Transform back from SplitReparam coordinates.\n            self.samples[""auxiliary_haar""] = torch.cat([\n                self.samples.pop(""auxiliary_haar_split_0""),\n                self.samples.pop(""auxiliary_haar_split_1""),\n            ], dim=-2 if self.is_regional else -1)\n        if haar:\n            # Transform back from Haar coordinates.\n            x = self.samples.pop(""auxiliary_haar"")\n            x = HaarTransform(dim=-2 if self.is_regional else -1, flip=True).inv(x)\n            x = biject_to(constraints.interval(-0.5, self.population + 0.5))(x)\n            self.samples[""auxiliary""] = x\n\n        # Unsqueeze samples to align particle dim for use in poutine.condition.\n        # TODO refactor to an align_samples or particle_dim kwarg to MCMC.get_samples().\n        self.samples = align_samples(self.samples, self._vectorized_model,\n                                     particle_dim=-1 - self.max_plate_nesting)\n        return mcmc  # E.g. so user can run mcmc.summary().\n\n    @torch.no_grad()\n    @set_approx_log_prob_tol(0.1)\n    @set_approx_sample_thresh(10000)\n    def predict(self, forecast=0):\n        """"""\n        Predict latent variables and optionally forecast forward.\n\n        This may be run only after :meth:`fit` and draws the same\n        ``num_samples`` as passed to :meth:`fit`.\n\n        :param int forecast: The number of time steps to forecast forward.\n        :returns: A dictionary mapping sample site name (or compartment name)\n            to a tensor whose first dimension corresponds to sample batching.\n        :rtype: dict\n        """"""\n        _require_double_precision()\n        if not self.samples:\n            raise RuntimeError(""Missing samples, try running .fit() first"")\n\n        samples = self.samples\n        num_samples = len(next(iter(samples.values())))\n        particle_plate = pyro.plate(""particles"", num_samples,\n                                    dim=-1 - self.max_plate_nesting)\n\n        # Sample discrete auxiliary variables conditioned on the continuous\n        # variables sampled by _vectorized_model. This samples only time steps\n        # [0:duration]. Here infer_discrete runs a forward-filter\n        # backward-sample algorithm.\n        logger.info(""Predicting latent variables for {} time steps...""\n                    .format(self.duration))\n        model = self._sequential_model\n        model = poutine.condition(model, samples)\n        model = particle_plate(model)\n        model = infer_discrete(model, first_available_dim=-2 - self.max_plate_nesting)\n        trace = poutine.trace(model).get_trace()\n        samples = OrderedDict((name, site[""value""])\n                              for name, site in trace.nodes.items()\n                              if site[""type""] == ""sample"")\n\n        # Optionally forecast with the forward _generative_model. This samples\n        # time steps [duration:duration+forecast].\n        if forecast:\n            logger.info(""Forecasting {} steps ahead..."".format(forecast))\n            model = self._generative_model\n            model = poutine.condition(model, samples)\n            model = particle_plate(model)\n            trace = poutine.trace(model).get_trace(forecast)\n            samples = OrderedDict((name, site[""value""])\n                                  for name, site in trace.nodes.items()\n                                  if site[""type""] == ""sample"")\n\n        self._concat_series(samples, forecast, vectorized=True)\n        return samples\n\n    @torch.no_grad()\n    @set_approx_log_prob_tol(0.1)\n    @set_approx_sample_thresh(100)  # This is robust to gross approximation.\n    def heuristic(self, num_particles=1024, ess_threshold=0.5, retries=10):\n        """"""\n        Finds an initial feasible guess of all latent variables, consistent\n        with observed data. This is needed because not all hypotheses are\n        feasible and HMC needs to start at a feasible solution to progress.\n\n        The default implementation attempts to find a feasible state using\n        :class:`~pyro.infer.smcfilter.SMCFilter` with proprosals from the\n        prior.  However this method may be overridden in cases where SMC\n        performs poorly e.g. in high-dimensional models.\n\n        :param int num_particles: Number of particles used for SMC.\n        :param float ess_threshold: Effective sample size threshold for SMC.\n        :returns: A dictionary mapping sample site name to tensor value.\n        :rtype: dict\n        """"""\n        # Run SMC.\n        model = _SMCModel(self)\n        guide = _SMCGuide(self)\n        for attempt in range(1, 1 + retries):\n            smc = SMCFilter(model, guide, num_particles=num_particles,\n                            ess_threshold=ess_threshold,\n                            max_plate_nesting=self.max_plate_nesting)\n            try:\n                smc.init()\n                for t in range(1, self.duration):\n                    smc.step()\n                break\n            except SMCFailed as e:\n                if attempt == retries:\n                    raise\n                logger.info(""{}. Retrying..."".format(e))\n                continue\n\n        # Select the most probable hypothesis.\n        i = int(smc.state._log_weights.max(0).indices)\n        init = {key: value[i, 0] for key, value in smc.state.items()}\n\n        # Fill in sample site values.\n        init = self.generate(init)\n        aux = torch.stack([init[name] for name in self.compartments], dim=0)\n        init[""auxiliary""] = clamp(aux, min=0.5, max=self.population - 0.5)\n        return init\n\n    # Internal helpers ########################################\n\n    def _concat_series(self, samples, forecast=0, vectorized=False):\n        """"""\n        Concatenate sequential time series into tensors, in-place.\n\n        :param dict samples: A dictionary of samples.\n        """"""\n        for name in set(self.compartments).union(self.series):\n            pattern = name + ""_[0-9]+""\n            series = []\n            for key in list(samples):\n                if re.match(pattern, key):\n                    series.append(samples.pop(key))\n            if not series:\n                continue\n            assert len(series) == self.duration + forecast\n            series = torch.broadcast_tensors(*map(torch.as_tensor, series))\n            if vectorized and name != ""obs"":  # TODO Generalize.\n                samples[name] = torch.cat(series, dim=1)\n            else:\n                samples[name] = torch.stack(series)\n\n    @lazy_property\n    @torch.no_grad()\n    def _non_compartmental(self):\n        """"""\n        A dict mapping name -> (distribution, is_regional) for all\n        non-compartmental sites in :meth:`transition`. For simple models this\n        is often empty; for time-heterogeneous models this may contain\n        time-local latent variables.\n        """"""\n        # Trace a simple invocation of .transition().\n        with torch.no_grad(), poutine.block():\n            params = self.global_model()\n            prev = self.initialize(params)\n            for name in self.approximate:\n                prev[name + ""_approx""] = prev[name]\n            curr = prev.copy()\n            with poutine.trace() as tr:\n                self.transition(params, curr, 0)\n            flows = self.compute_flows(prev, curr, 0)\n\n        # Extract latent variables that are not compartmental flows.\n        result = OrderedDict()\n        for name, site in tr.trace.iter_stochastic_nodes():\n            if name in flows:\n                continue\n            assert name.endswith(""_0""), name\n            name = name[:-2]\n            assert name in self.series, name\n            # TODO This supports only the region_plate. For full plate support,\n            # this could be replaced by a self.plate() method as in EasyGuide.\n            is_regional = any(f.name == ""region"" for f in site[""cond_indep_stack""])\n            result[name] = site[""fn""], is_regional\n        return result\n\n    def _transition_bwd(self, params, prev, curr, t):\n        """"""\n        Helper to collect probabilty factors from .transition() conditioned on\n        previous and current enumerated states.\n        """"""\n        # Run .transition() conditioned on computed flows.\n        cond_data = {""{}_{}"".format(k, t): v for k, v in curr.items()}\n        cond_data.update(self.compute_flows(prev, curr, t))\n        with poutine.condition(data=cond_data):\n            state = prev.copy()\n            self.transition(params, state, t)  # Mutates state.\n\n        # Validate that .transition() matches .compute_flows().\n        if is_validation_enabled():\n            for key in self.compartments:\n                if not (state[key] - curr[key]).eq(0).all():\n                    raise ValueError(""Incorrect state[\'{}\'] update in .transition(), ""\n                                     ""check that .transition() matches .compute_flows().""\n                                     .format(key))\n\n    def _generative_model(self, forecast=0):\n        """"""\n        Forward generative model used for simulation and forecasting.\n        """"""\n        # Sample global parameters.\n        params = self.global_model()\n\n        # Sample initial values.\n        state = self.initialize(params)\n        state = {k: v if isinstance(v, torch.Tensor) else torch.tensor(float(v))\n                 for k, v in state.items()}\n\n        # Sequentially transition.\n        for t in range(self.duration + forecast):\n            for name in self.approximate:\n                state[name + ""_approx""] = state[name]\n            self.transition(params, state, t)\n            with self.region_plate:\n                for name in self.compartments:\n                    pyro.deterministic(""{}_{}"".format(name, t), state[name], event_dim=0)\n\n        self._clear_plates()\n\n    def _sequential_model(self):\n        """"""\n        Sequential model used to sample latents in the interval [0:duration].\n        """"""\n        C = len(self.compartments)\n        T = self.duration\n        R_shape = getattr(self.population, ""shape"", ())  # Region shape.\n\n        # Sample global parameters.\n        params = self.global_model()\n\n        # Sample the compartmental continuous reparameterizing variable.\n        shape = (C, T) + R_shape\n        auxiliary = pyro.sample(""auxiliary"",\n                                dist.Uniform(-0.5, self.population + 0.5)\n                                    .mask(False).expand(shape).to_event())\n        num_samples = auxiliary.size(0)\n        if self.is_regional:\n            auxiliary = auxiliary.squeeze(1)\n        assert auxiliary.shape == (num_samples, 1, C, T) + R_shape\n        aux = [aux.unbind(2) for aux in auxiliary.unbind(2)]\n\n        # Sample any non-compartmental time series in batch.\n        # TODO Consider using pyro.contrib.forecast.util.reshape_batch to\n        # support DiscreteCosineReparam and HaarReparam along the time dim.\n        non_compartmental = OrderedDict()\n        for name, (fn, is_regional) in self._non_compartmental.items():\n            fn = dist.ImproperUniform(fn.support, fn.batch_shape, fn.event_shape)\n            with self.time_plate, optional(self.region_plate, is_regional):\n                non_compartmental[name] = pyro.sample(name, fn)\n\n        # Sequentially transition.\n        curr = self.initialize(params)\n        for t in poutine.markov(range(T)):\n            with self.region_plate:\n                prev, curr = curr, {}\n\n                # Extract any non-compartmental variables.\n                for name, value in non_compartmental.items():\n                    curr[name] = value[:, t:t+1]\n\n                # Extract and enumerate all compartmental variables.\n                for c, name in enumerate(self.compartments):\n                    curr[name] = quantize(""{}_{}"".format(name, t), aux[c][t],\n                                          min=0, max=self.population,\n                                          num_quant_bins=self.num_quant_bins)\n                    # Enable approximate inference by using aux as a\n                    # non-enumerated proxy for enumerated compartment values.\n                    if name in self.approximate:\n                        curr[name + ""_approx""] = aux[c][t]\n                        prev.setdefault(name + ""_approx"", prev[name])\n\n            self._transition_bwd(params, prev, curr, t)\n\n        self._clear_plates()\n\n    def _vectorized_model(self):\n        """"""\n        Vectorized model used for inference.\n        """"""\n        C = len(self.compartments)\n        T = self.duration\n        Q = self.num_quant_bins\n        R_shape = getattr(self.population, ""shape"", ())  # Region shape.\n\n        # Sample global parameters.\n        params = self.global_model()\n\n        # Sample the compartmental continuous reparameterizing variable.\n        shape = (C, T) + R_shape\n        auxiliary = pyro.sample(""auxiliary"",\n                                dist.Uniform(-0.5, self.population + 0.5)\n                                    .mask(False).expand(shape).to_event())\n        assert auxiliary.shape == shape, ""particle plates are not supported""\n\n        # Manually enumerate.\n        curr, logp = quantize_enumerate(auxiliary, min=0, max=self.population,\n                                        num_quant_bins=self.num_quant_bins)\n        curr = OrderedDict(zip(self.compartments, curr))\n        logp = OrderedDict(zip(self.compartments, logp))\n\n        # Sample any non-compartmental time series in batch.\n        # TODO Consider using pyro.contrib.forecast.util.reshape_batch to\n        # support DiscreteCosineReparam and HaarReparam along the time dim.\n        for name, (fn, is_regional) in self._non_compartmental.items():\n            fn = dist.ImproperUniform(fn.support, fn.batch_shape, fn.event_shape)\n            with self.time_plate, optional(self.region_plate, is_regional):\n                curr[name] = pyro.sample(name, fn)\n\n        # Truncate final value from the right then pad initial value onto the left.\n        init = self.initialize(params)\n        prev = {}\n        for name, value in init.items():\n            if name in self.compartments:\n                if isinstance(value, torch.Tensor):\n                    value = value[..., None]  # Because curr is enumerated on the right.\n                prev[name] = cat2(value, curr[name][:-1],\n                                  dim=-3 if self.is_regional else -2)\n            else:  # non-compartmental\n                prev[name] = cat2(init[name], curr[name][:-1], dim=-curr[name].dim())\n\n        # Reshape to support broadcasting, similar to EnumMessenger.\n        def enum_reshape(tensor, position):\n            assert tensor.size(-1) == Q\n            assert tensor.dim() <= self.max_plate_nesting + 2\n            tensor = tensor.permute(tensor.dim() - 1, *range(tensor.dim() - 1))\n            shape = [Q] + [1] * (position + self.max_plate_nesting - (tensor.dim() - 2))\n            shape.extend(tensor.shape[1:])\n            return tensor.reshape(shape)\n\n        for e, name in enumerate(self.compartments):\n            curr[name] = enum_reshape(curr[name], e)\n            logp[name] = enum_reshape(logp[name], e)\n            prev[name] = enum_reshape(prev[name], e + C)\n\n        # Enable approximate inference by using aux as a non-enumerated proxy\n        # for enumerated compartment values.\n        for name in self.approximate:\n            aux = auxiliary[self.compartments.index(name)]\n            curr[name + ""_approx""] = aux\n            prev[name + ""_approx""] = cat2(init[name], aux[:-1],\n                                          dim=-2 if self.is_regional else -1)\n\n        # Record transition factors.\n        with poutine.block(), poutine.trace() as tr:\n            with self.time_plate:\n                t = slice(0, T, 1)  # Used to slice data tensors.\n                self._transition_bwd(params, prev, curr, t)\n        tr.trace.compute_log_prob()\n        for name, site in tr.trace.nodes.items():\n            if site[""type""] == ""sample"":\n                log_prob = site[""log_prob""]\n                if log_prob.dim() <= self.max_plate_nesting:  # Not enumerated.\n                    pyro.factor(""transition_"" + name, site[""log_prob_sum""])\n                    continue\n                if self.is_regional and log_prob.shape[-1:] != R_shape:\n                    # Poor man\'s tensor variable elimination.\n                    log_prob = log_prob.expand(log_prob.shape[:-1] + R_shape) / R_shape[0]\n                logp[name] = site[""log_prob""]\n\n        # Manually perform variable elimination.\n        logp = reduce(operator.add, logp.values())\n        logp = logp.reshape(Q ** C, Q ** C, T, -1)  # prev, curr, T, batch\n        logp = logp.permute(3, 2, 0, 1).squeeze(0)  # batch, T, prev, curr\n        logp = pyro.distributions.hmm._sequential_logmatmulexp(logp)  # batch, prev, curr\n        logp = logp.reshape(-1, Q ** C * Q ** C).logsumexp(-1).sum()\n        warn_if_nan(logp)\n        pyro.factor(""transition"", logp)\n\n        self._clear_plates()\n\n\nclass _SMCModel:\n    """"""\n    Helper to initialize a CompartmentalModel to a feasible initial state.\n    """"""\n    def __init__(self, model):\n        assert isinstance(model, CompartmentalModel)\n        self.model = model\n\n    def init(self, state):\n        with poutine.trace() as tr:\n            params = self.model.global_model()\n        for name, site in tr.trace.nodes.items():\n            if site[""type""] == ""sample"":\n                state[name] = site[""value""]\n\n        self.t = 0\n        state.update(self.model.initialize(params))\n        self.step(state)  # Take one step since model.initialize is deterministic.\n\n    def step(self, state):\n        with poutine.block(), poutine.condition(data=state):\n            params = self.model.global_model()\n\n        with poutine.trace() as tr:\n            # Temporarily extend state with approximations.\n            extended_state = dict(state)\n            for name in self.model.approximate:\n                extended_state[name + ""_approx""] = state[name]\n\n            self.model.transition(params, extended_state, self.t)\n\n            for name in self.model.approximate:\n                del extended_state[name + ""_approx""]\n            state.update(extended_state)\n\n        for name, site in tr.trace.nodes.items():\n            if site[""type""] == ""sample"" and not site[""is_observed""]:\n                state[name] = site[""value""]\n        self.t += 1\n\n\nclass _SMCGuide(_SMCModel):\n    """"""\n    Like _SMCModel but does not update state and does not observe.\n    """"""\n    def init(self, state):\n        super().init(state.copy())\n\n    def step(self, state):\n        with poutine.block(hide_types=[""observe""]):\n            super().step(state.copy())\n'"
pyro/contrib/epidemiology/distributions.py,11,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nfrom contextlib import contextmanager\n\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.distributions.util import is_validation_enabled\n\n\ndef _all(x):\n    return x.all() if isinstance(x, torch.Tensor) else x\n\n\ndef _is_zero(x):\n    return _all(x == 0)\n\n\n@contextmanager\ndef set_approx_sample_thresh(thresh):\n    """"""\n    EXPERIMENTAL Context manager / decorator to temporarily set the global\n    default value of ``Binomial.approx_sample_thresh``, thereby decreasing the\n    computational complexity of sampling from\n    :class:`~pyro.distributions.Binomial`,\n    :class:`~pyro.distributions.BetaBinomial`,\n    :class:`~pyro.distributions.ExtendedBinomial`,\n    :class:`~pyro.distributions.ExtendedBetaBinomial`, and distributions\n    returned by :func:`infection_dist`.\n\n    This is useful for sampling from very large ``total_count``.\n\n    This is used internally by\n    :class:`~pyro.contrib.epidemiology.compartmental.CompartmentalModel`.\n\n    :param thresh: New temporary threshold.\n    :type thresh: int or float.\n    """"""\n    assert isinstance(thresh, (float, int))\n    assert thresh > 0\n    old = dist.Binomial.approx_sample_thresh\n    try:\n        dist.Binomial.approx_sample_thresh = thresh\n        yield\n    finally:\n        dist.Binomial.approx_sample_thresh = old\n\n\n@contextmanager\ndef set_approx_log_prob_tol(tol):\n    """"""\n    EXPERIMENTAL Context manager / decorator to temporarily set the global\n    default value of ``Binomial.approx_log_prob_tol`` and\n    ``BetaBinomial.approx_log_prob_tol``, thereby decreasing the computational\n    complexity of scoring :class:`~pyro.distributions.Binomial` and\n    :class:`~pyro.distributions.BetaBinomial` distributions.\n\n    This is used internally by\n    :class:`~pyro.contrib.epidemiology.compartmental.CompartmentalModel`.\n\n    :param tol: New temporary tolold.\n    :type tol: int or float.\n    """"""\n    assert isinstance(tol, (float, int))\n    assert tol >= 0\n    old1 = dist.Binomial.approx_log_prob_tol\n    old2 = dist.BetaBinomial.approx_log_prob_tol\n    try:\n        dist.Binomial.approx_log_prob_tol = tol\n        dist.BetaBinomial.approx_log_prob_tol = tol\n        yield\n    finally:\n        dist.Binomial.approx_log_prob_tol = old1\n        dist.BetaBinomial.approx_log_prob_tol = old2\n\n\ndef _validate_overdispersion(overdispersion):\n    if is_validation_enabled():\n        if not _all(0 <= overdispersion):\n            raise ValueError(""Expected overdispersion >= 0"")\n        if not _all(overdispersion < 2):\n            raise ValueError(""Expected overdispersion < 2"")\n\n\ndef binomial_dist(total_count, probs, *,\n                  overdispersion=0.):\n    """"""\n    Returns a Beta-Binomial distribution that is an overdispersed version of a\n    Binomial distribution, according to a parameter ``overdispersion``,\n    typically set in the range 0.1 to 0.5.\n\n    This is useful for (1) fitting real data that is overdispersed relative to\n    a Binomial distribution, and (2) relaxing models of large populations to\n    improve inference. In particular the ``overdispersion`` parameter lower\n    bounds the relative uncertainty in stochastic models such that increasing\n    population leads to a limiting scale-free dynamical system with bounded\n    stochasticity, in contrast to Binomial-based SDEs that converge to\n    deterministic ODEs in the large population limit.\n\n    This parameterization satisfies the following properties:\n\n    1.  Variance increases monotonically in ``overdispersion``.\n    2.  ``overdispersion = 0`` results in a Binomial distribution.\n    3.  ``overdispersion`` lower bounds the relative uncertainty ``std_dev /\n        (total_count * p * q)``, where ``probs = p = 1 - q``, and serves as an\n        asymptote for relative uncertainty as ``total_count \xe2\x86\x92 \xe2\x88\x9e``. This\n        contrasts the Binomial whose relative uncertainty tends to zero.\n    4.  If ``X ~ binomial_dist(n, p, overdispersion=\xcf\x83)`` then in the large\n        population limit ``n \xe2\x86\x92 \xe2\x88\x9e``, the scaled random variable ``X / n``\n        converges in distribution to ``LogitNormal(log(p/(1-p)), \xcf\x83)``.\n\n    To achieve these properties we set ``p = probs``, ``q = 1 - p``, and::\n\n        concentration = 1 / (p * q * overdispersion**2) - 1\n\n    :param total_count: Number of Bernoulli trials.\n    :type total_count: int or torch.Tensor\n    :param probs: Event probabilities.\n    :type probs: float or torch.Tensor\n    :param overdispersion: Amount of overdispersion, in the half open interval\n        [0,2). Defaults to zero.\n    :type overdispersion: float or torch.tensor\n    """"""\n    _validate_overdispersion(overdispersion)\n    if _is_zero(overdispersion):\n        return dist.ExtendedBinomial(total_count, probs)\n\n    p = probs\n    q = 1 - p\n    od2 = (overdispersion + 1e-8) ** 2\n    concentration1 = 1 / (q * od2 + 1e-8) - p\n    concentration0 = 1 / (p * od2 + 1e-8) - q\n    # At this point we have\n    #   concentration1 + concentration0 == 1 / (p + q + od2 + 1e-8) - 1\n\n    return dist.ExtendedBetaBinomial(concentration1, concentration0, total_count)\n\n\ndef beta_binomial_dist(concentration1, concentration0, total_count, *,\n                       overdispersion=0.):\n    """"""\n    Returns a Beta-Binomial distribution that is an overdispersed version of a\n    the usual Beta-Binomial distribution, according to an extra parameter\n    ``overdispersion``, typically set in the range 0.1 to 0.5.\n\n    :param concentration1: 1st concentration parameter (alpha) for the\n        Beta distribution.\n    :type concentration1: float or torch.Tensor\n    :param concentration0: 2nd concentration parameter (beta) for the\n        Beta distribution.\n    :type concentration0: float or torch.Tensor\n    :param total_count: Number of Bernoulli trials.\n    :type total_count: float or torch.Tensor\n    :param overdispersion: Amount of overdispersion, in the half open interval\n        [0,2). Defaults to zero.\n    :type overdispersion: float or torch.tensor\n    """"""\n    _validate_overdispersion(overdispersion)\n    if not _is_zero(overdispersion):\n        # Compute harmonic sum of two sources of concentration resulting in\n        # final concentration c = 1 / (1 / c_1 + 1 / c_2)\n        od2 = (overdispersion + 1e-8) ** 2\n        c_1 = concentration1 + concentration0\n        c_2 = c_1 ** 2 / (concentration1 * concentration0 * od2 + 1e-8) - 1\n        factor = 1 + c_1 / c_2\n        concentration1 = concentration1 / factor\n        concentration0 = concentration0 / factor\n\n    return dist.ExtendedBetaBinomial(concentration1, concentration0, total_count)\n\n\ndef poisson_dist(rate, *, overdispersion=0.):\n    _validate_overdispersion(overdispersion)\n    if _is_zero(overdispersion):\n        return dist.Poisson(rate)\n    raise NotImplementedError(""TODO return a NegativeBinomial or GammaPoisson"")\n\n\ndef negative_binomial_dist(concentration, probs=None, *,\n                           logits=None, overdispersion=0.):\n    _validate_overdispersion(overdispersion)\n    if _is_zero(overdispersion):\n        return dist.NegativeBinomial(concentration, probs=probs, logits=logits)\n    raise NotImplementedError(""TODO return a NegativeBinomial or GammaPoisson"")\n\n\ndef infection_dist(*,\n                   individual_rate,\n                   num_infectious,\n                   num_susceptible=math.inf,\n                   population=math.inf,\n                   concentration=math.inf,\n                   overdispersion=0.):\n    """"""\n    Create a :class:`~pyro.distributions.Distribution` over the number of new\n    infections at a discrete time step.\n\n    This returns a Poisson, Negative-Binomial, Binomial, or Beta-Binomial\n    distribution depending on whether ``population`` and ``concentration`` are\n    finite. In Pyro models, the population is usually finite. In the limit\n    ``population \xe2\x86\x92 \xe2\x88\x9e`` and ``num_susceptible/population \xe2\x86\x92 1``, the Binomial\n    converges to Poisson and the Beta-Binomial converges to Negative-Binomial.\n    In the limit ``concentration \xe2\x86\x92 \xe2\x88\x9e``, the Negative-Binomial converges to\n    Poisson and the Beta-Binomial converges to Binomial.\n\n    The overdispersed distributions (Negative-Binomial and Beta-Binomial\n    returned when ``concentration < \xe2\x88\x9e``) are useful for modeling superspreader\n    individuals [1,2]. The finitely supported distributions Binomial and\n    Negative-Binomial are useful in small populations and in probabilistic\n    programming systems where truncation or censoring are expensive [3].\n\n    **References**\n\n    [1] J. O. Lloyd-Smith, S. J. Schreiber, P. E. Kopp, W. M. Getz (2005)\n        ""Superspreading and the effect of individual variation on disease\n        emergence""\n        https://www.nature.com/articles/nature04153.pdf\n    [2] Lucy M. Li, Nicholas C. Grassly, Christophe Fraser (2017)\n        ""Quantifying Transmission Heterogeneity Using Both Pathogen Phylogenies\n        and Incidence Time Series""\n        https://academic.oup.com/mbe/article/34/11/2982/3952784\n    [3] Lawrence Murray et al. (2018)\n        ""Delayed Sampling and Automatic Rao-Blackwellization of Probabilistic\n        Programs""\n        https://arxiv.org/pdf/1708.07787.pdf\n\n    :param individual_rate: The mean number of infections per infectious\n        individual per time step in the limit of large population, equal to\n        ``R0 / tau`` where ``R0`` is the basic reproductive number and ``tau``\n        is the mean duration of infectiousness.\n    :param num_infectious: The number of infectious individuals at this\n        time step, sometimes ``I``, sometimes ``E+I``.\n    :param num_susceptible: The number ``S`` of susceptible individuals at this\n        time step. This defaults to an infinite population.\n    :param population: The total number of individuals in a population.\n        This defaults to an infinite population.\n    :param concentration: The concentration or dispersion parameter ``k`` in\n        overdispersed models of superspreaders [1,2]. This defaults to minimum\n        variance ``concentration = \xe2\x88\x9e``.\n    :param overdispersion: Amount of overdispersion, in the half open interval\n        [0,2). Defaults to zero.\n    :type overdispersion: float or torch.tensor\n    """"""\n    # Convert to colloquial variable names.\n    R = individual_rate\n    I = num_infectious\n    S = num_susceptible\n    N = population\n    k = concentration\n\n    if isinstance(N, float) and N == math.inf:\n        if isinstance(k, float) and k == math.inf:\n            # Return a Poisson distribution.\n            return poisson_dist(R * I, overdispersion=overdispersion)\n        else:\n            # Return an overdispersed Negative-Binomial distribution.\n            combined_k = k * I\n            logits = torch.as_tensor(R / k).log()\n            return negative_binomial_dist(combined_k, logits=logits,\n                                          overdispersion=overdispersion)\n    else:\n        # Compute the probability that any given (susceptible, infectious)\n        # pair of individuals results in an infection at this time step.\n        p = torch.as_tensor(R / N).clamp(max=1 - 1e-6)\n        # Combine infections from all individuals.\n        combined_p = p.neg().log1p().mul(I).expm1().neg()  # = 1 - (1 - p)**I\n        combined_p = combined_p.clamp(min=1e-6)\n\n        if isinstance(k, float) and k == math.inf:\n            # Return a pure Binomial model, combining the independent Binomial\n            # models of each infectious individual.\n            return binomial_dist(S, combined_p, overdispersion=overdispersion)\n        else:\n            # Return an overdispersed Beta-Binomial model, combining\n            # independent BetaBinomial(c1,c0,S) models for each infectious\n            # individual.\n            c1 = (k * I).clamp(min=1e-6)\n            c0 = c1 * (combined_p.reciprocal() - 1).clamp(min=1e-6)\n            return beta_binomial_dist(c1, c0, S, overdispersion=overdispersion)\n'"
pyro/contrib/epidemiology/models.py,9,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport re\n\nimport torch\nfrom torch.nn.functional import pad\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom .compartmental import CompartmentalModel\nfrom .distributions import binomial_dist, infection_dist\n\n\nclass SimpleSIRModel(CompartmentalModel):\n    """"""\n    Susceptible-Infected-Recovered model.\n\n    To customize this model we recommend forking and editing this class.\n\n    This is a stochastic discrete-time discrete-state model with three\n    compartments: ""S"" for susceptible, ""I"" for infected, and ""R"" for\n    recovered individuals (the recovered individuals are implicit: ``R =\n    population - S - I``) with transitions ``S -> I -> R``.\n\n    :param int population: Total ``population = S + I + R``.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param iterable data: Time series of new observed infections. Each time\n        step is Binomial distributed between 0 and the number of ``S -> I``\n        transitions. This allows false negative but no false positives.\n    """"""\n\n    def __init__(self, population, recovery_time, data):\n        compartments = (""S"", ""I"")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        self.data = data\n\n    def global_model(self):\n        tau = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n        rho = pyro.sample(""rho"", dist.Beta(10, 10))\n        return R0, tau, rho\n\n    def initialize(self, params):\n        # Start with a single infection.\n        return {""S"": self.population - 1, ""I"": 1}\n\n    def transition(self, params, state, t):\n        R0, tau, rho = params\n\n        # Sample flows between compartments.\n        S2I = pyro.sample(""S2I_{}"".format(t),\n                          infection_dist(individual_rate=R0 / tau,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""],\n                                         population=self.population))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          dist.ExtendedBinomial(state[""I""], 1 / tau))\n\n        # Update compartments with flows.\n        state[""S""] = state[""S""] - S2I\n        state[""I""] = state[""I""] + S2I - I2R\n\n        # Condition on observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2I, rho),\n                    obs=self.data[t] if t_is_observed else None)\n\n\nclass SimpleSEIRModel(CompartmentalModel):\n    """"""\n    Susceptible-Exposed-Infected-Recovered model.\n\n    To customize this model we recommend forking and editing this class.\n\n    This is a stochastic discrete-time discrete-state model with four\n    compartments: ""S"" for susceptible, ""E"" for exposed, ""I"" for infected,\n    and ""R"" for recovered individuals (the recovered individuals are\n    implicit: ``R = population - S - E - I``) with transitions\n    ``S -> E -> I -> R``.\n\n    :param int population: Total ``population = S + E + I + R``.\n    :param float incubation_time: Mean incubation time (duration in state\n        ``E``). Must be greater than 1.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param iterable data: Time series of new observed infections. Each time\n        step is Binomial distributed between 0 and the number of ``S -> E``\n        transitions. This allows false negative but no false positives.\n    """"""\n\n    def __init__(self, population, incubation_time, recovery_time, data):\n        compartments = (""S"", ""E"", ""I"")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(incubation_time, float)\n        assert incubation_time > 1\n        self.incubation_time = incubation_time\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        self.data = data\n\n    def global_model(self):\n        tau_e = self.incubation_time\n        tau_i = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n        rho = pyro.sample(""rho"", dist.Beta(10, 10))\n        return R0, tau_e, tau_i, rho\n\n    def initialize(self, params):\n        # Start with a single infection.\n        return {""S"": self.population - 1, ""E"": 0, ""I"": 1}\n\n    def transition(self, params, state, t):\n        R0, tau_e, tau_i, rho = params\n\n        # Sample flows between compartments.\n        S2E = pyro.sample(""S2E_{}"".format(t),\n                          infection_dist(individual_rate=R0 / tau_i,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""],\n                                         population=self.population))\n        E2I = pyro.sample(""E2I_{}"".format(t),\n                          dist.ExtendedBinomial(state[""E""], 1 / tau_e))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          dist.ExtendedBinomial(state[""I""], 1 / tau_i))\n\n        # Update compartments with flows.\n        state[""S""] = state[""S""] - S2E\n        state[""E""] = state[""E""] + S2E - E2I\n        state[""I""] = state[""I""] + E2I - I2R\n\n        # Condition on observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2E, rho),\n                    obs=self.data[t] if t_is_observed else None)\n\n\nclass OverdispersedSIRModel(CompartmentalModel):\n    """"""\n    Generalizes :class:`SimpleSIRModel` with overdispersed distributions.\n\n    To customize this model we recommend forking and editing this class.\n\n    This adds a single global overdispersion parameter controlling\n    overdispersion of the transition and observation distributions. See\n    :func:`~pyro.contrib.epidemiology.distributions.binomial_dist` and\n    :func:`~pyro.contrib.epidemiology.distributions.beta_binomial_dist` for\n    distributional details. For prior work incorporating overdispersed\n    distributions see [1,2,3,4].\n\n    **References:**\n\n    [1] D. Champredon, M. Li, B. Bolker. J. Dushoff (2018)\n        ""Two approaches to forecast Ebola synthetic epidemics""\n        https://www.sciencedirect.com/science/article/pii/S1755436517300233\n    [2] Carrie Reed et al. (2015)\n        ""Estimating Influenza Disease Burden from Population-Based Surveillance\n        Data in the United States""\n        https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349859/\n    [3] A. Leonard, D. Weissman, B. Greenbaum, E. Ghedin, K. Koelle (2017)\n        ""Transmission Bottleneck Size Estimation from Pathogen Deep-Sequencing\n        Data, with an Application to Human Influenza A Virus""\n        https://jvi.asm.org/content/jvi/91/14/e00171-17.full.pdf\n    [4] A. Miller, N. Foti, J. Lewnard, N. Jewell, C. Guestrin, E. Fox (2020)\n        ""Mobility trends provide a leading indicator of changes in\n        SARS-CoV-2 transmission""\n        https://www.medrxiv.org/content/medrxiv/early/2020/05/11/2020.05.07.20094441.full.pdf\n\n    :param int population: Total ``population = S + I + R``.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param iterable data: Time series of new observed infections. Each time\n        step is Binomial distributed between 0 and the number of ``S -> I``\n        transitions. This allows false negative but no false positives.\n    """"""\n\n    def __init__(self, population, recovery_time, data):\n        compartments = (""S"", ""I"")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        self.data = data\n\n    def global_model(self):\n        tau = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n        rho = pyro.sample(""rho"", dist.Beta(10, 10))\n        od = pyro.sample(""od"", dist.Beta(2, 6))\n        return R0, tau, rho, od\n\n    def initialize(self, params):\n        # Start with a single infection.\n        return {""S"": self.population - 1, ""I"": 1}\n\n    def transition(self, params, state, t):\n        R0, tau, rho, od = params\n\n        # Sample flows between compartments.\n        S2I = pyro.sample(""S2I_{}"".format(t),\n                          infection_dist(individual_rate=R0 / tau,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""],\n                                         population=self.population,\n                                         overdispersion=od))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          binomial_dist(state[""I""], 1 / tau,\n                                        overdispersion=od))\n\n        # Update compartments with flows.\n        state[""S""] = state[""S""] - S2I\n        state[""I""] = state[""I""] + S2I - I2R\n\n        # Condition on observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        pyro.sample(""obs_{}"".format(t),\n                    binomial_dist(S2I, rho, overdispersion=od),\n                    obs=self.data[t] if t_is_observed else None)\n\n\nclass OverdispersedSEIRModel(CompartmentalModel):\n    """"""\n    Generalizes :class:`SimpleSEIRModel` with overdispersed distributions.\n\n    To customize this model we recommend forking and editing this class.\n\n    This adds a single global overdispersion parameter controlling\n    overdispersion of the transition and observation distributions. See\n    :func:`~pyro.contrib.epidemiology.distributions.binomial_dist` and\n    :func:`~pyro.contrib.epidemiology.distributions.beta_binomial_dist` for\n    distributional details. For prior work incorporating overdispersed\n    distributions see [1,2,3,4].\n\n    **References:**\n\n    [1] D. Champredon, M. Li, B. Bolker. J. Dushoff (2018)\n        ""Two approaches to forecast Ebola synthetic epidemics""\n        https://www.sciencedirect.com/science/article/pii/S1755436517300233\n    [2] Carrie Reed et al. (2015)\n        ""Estimating Influenza Disease Burden from Population-Based Surveillance\n        Data in the United States""\n        https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349859/\n    [3] A. Leonard, D. Weissman, B. Greenbaum, E. Ghedin, K. Koelle (2017)\n        ""Transmission Bottleneck Size Estimation from Pathogen Deep-Sequencing\n        Data, with an Application to Human Influenza A Virus""\n        https://jvi.asm.org/content/jvi/91/14/e00171-17.full.pdf\n    [4] A. Miller, N. Foti, J. Lewnard, N. Jewell, C. Guestrin, E. Fox (2020)\n        ""Mobility trends provide a leading indicator of changes in\n        SARS-CoV-2 transmission""\n        https://www.medrxiv.org/content/medrxiv/early/2020/05/11/2020.05.07.20094441.full.pdf\n\n    :param int population: Total ``population = S + E + I + R``.\n    :param float incubation_time: Mean incubation time (duration in state\n        ``E``). Must be greater than 1.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param iterable data: Time series of new observed infections. Each time\n        step is Binomial distributed between 0 and the number of ``S -> E``\n        transitions. This allows false negative but no false positives.\n    """"""\n\n    def __init__(self, population, incubation_time, recovery_time, data):\n        compartments = (""S"", ""E"", ""I"")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(incubation_time, float)\n        assert incubation_time > 1\n        self.incubation_time = incubation_time\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        self.data = data\n\n    def global_model(self):\n        tau_e = self.incubation_time\n        tau_i = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n        rho = pyro.sample(""rho"", dist.Beta(10, 10))\n        od = pyro.sample(""od"", dist.Beta(2, 6))\n        return R0, tau_e, tau_i, rho, od\n\n    def initialize(self, params):\n        # Start with a single infection.\n        return {""S"": self.population - 1, ""E"": 0, ""I"": 1}\n\n    def transition(self, params, state, t):\n        R0, tau_e, tau_i, rho, od = params\n\n        # Sample flows between compartments.\n        S2E = pyro.sample(""S2E_{}"".format(t),\n                          infection_dist(individual_rate=R0 / tau_i,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""],\n                                         population=self.population,\n                                         overdispersion=od))\n        E2I = pyro.sample(""E2I_{}"".format(t),\n                          binomial_dist(state[""E""], 1 / tau_e, overdispersion=od))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          binomial_dist(state[""I""], 1 / tau_i, overdispersion=od))\n\n        # Update compartments with flows.\n        state[""S""] = state[""S""] - S2E\n        state[""E""] = state[""E""] + S2E - E2I\n        state[""I""] = state[""I""] + E2I - I2R\n\n        # Condition on observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        pyro.sample(""obs_{}"".format(t),\n                    binomial_dist(S2E, rho, overdispersion=od),\n                    obs=self.data[t] if t_is_observed else None)\n\n\nclass SuperspreadingSIRModel(CompartmentalModel):\n    """"""\n    Generalizes :class:`SimpleSIRModel` by adding superspreading effects.\n\n    To customize this model we recommend forking and editing this class.\n\n    This model accounts for superspreading (overdispersed individual\n    reproductive number) by assuming each infected individual infects\n    BetaBinomial-many susceptible individuals, where the BetaBinomial\n    distribution acts as an overdispersed Binomial distribution, adapting the\n    more standard NegativeBinomial distribution that acts as an overdispersed\n    Poisson distribution [1,2] to the setting of finite populations. To\n    preserve Markov structure, we follow [2] and assume all infections by a\n    single individual occur on the single time step where that individual makes\n    an ``I -> R`` transition. That is, whereas the :class:`SimpleSIRModel`\n    assumes infected individuals infect `Binomial(S,R/tau)`-many susceptible\n    individuals during each infected time step (over `tau`-many steps on\n    average), this model assumes they infect `BetaBinomial(k,...,S)`-many\n    susceptible individuals but only on the final time step before recovering.\n\n    **References**\n\n    [1] J. O. Lloyd-Smith, S. J. Schreiber, P. E. Kopp, W. M. Getz (2005)\n        ""Superspreading and the effect of individual variation on disease\n        emergence""\n        https://www.nature.com/articles/nature04153.pdf\n    [2] Lucy M. Li, Nicholas C. Grassly, Christophe Fraser (2017)\n        ""Quantifying Transmission Heterogeneity Using Both Pathogen Phylogenies\n        and Incidence Time Series""\n        https://academic.oup.com/mbe/article/34/11/2982/3952784\n\n    :param int population: Total ``population = S + I + R``.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param iterable data: Time series of new observed infections. Each time\n        step is Binomial distributed between 0 and the number of ``S -> I``\n        transitions. This allows false negative but no false positives.\n    """"""\n\n    def __init__(self, population, recovery_time, data):\n        compartments = (""S"", ""I"")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        self.data = data\n\n    def global_model(self):\n        tau = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n        k = pyro.sample(""k"", dist.Exponential(1.))\n        rho = pyro.sample(""rho"", dist.Beta(10, 10))\n        return R0, k, tau, rho\n\n    def initialize(self, params):\n        # Start with a single infection.\n        return {""S"": self.population - 1, ""I"": 1}\n\n    def transition(self, params, state, t):\n        R0, k, tau, rho = params\n\n        # Sample flows between compartments.\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          dist.ExtendedBinomial(state[""I""], 1 / tau))\n        S2I = pyro.sample(""S2I_{}"".format(t),\n                          infection_dist(individual_rate=R0,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""],\n                                         population=self.population,\n                                         concentration=k))\n\n        # Update compartments with flows.\n        state[""S""] = state[""S""] - S2I\n        state[""I""] = state[""I""] + S2I - I2R\n\n        # Condition on observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2I, rho),\n                    obs=self.data[t] if t_is_observed else None)\n\n\nclass SuperspreadingSEIRModel(CompartmentalModel):\n    r""""""\n    Generalizes :class:`SimpleSEIRModel` by adding superspreading effects.\n\n    To customize this model we recommend forking and editing this class.\n\n    This model accounts for superspreading (overdispersed individual\n    reproductive number) by assuming each infected individual infects\n    BetaBinomial-many susceptible individuals, where the BetaBinomial\n    distribution acts as an overdispersed Binomial distribution, adapting the\n    more standard NegativeBinomial distribution that acts as an overdispersed\n    Poisson distribution [1,2] to the setting of finite populations. To\n    preserve Markov structure, we follow [2] and assume all infections by a\n    single individual occur on the single time step where that individual makes\n    an ``I -> R`` transition. That is, whereas the :class:`SimpleSEIRModel`\n    assumes infected individuals infect `Binomial(S,R/tau)`-many susceptible\n    individuals during each infected time step (over `tau`-many steps on\n    average), this model assumes they infect `BetaBinomial(k,...,S)`-many\n    susceptible individuals but only on the final time step before recovering.\n\n    This model also adds an optional likelihood for observed phylogenetic data\n    in the form of coalescent times. These are provided as a pair\n    ``(leaf_times, coal_times)`` of times at which genomes are sequenced and\n    lineages coalesce, respectively. We incorporate this data using the\n    :class:`~pyro.distributions.CoalescentRateLikelihood` with base coalescence\n    rate computed from the ``S`` and ``I`` populations. This likelihood is\n    independent across time and preserves the Markov propert needed for\n    inference.\n\n    **References**\n\n    [1] J. O. Lloyd-Smith, S. J. Schreiber, P. E. Kopp, W. M. Getz (2005)\n        ""Superspreading and the effect of individual variation on disease\n        emergence""\n        https://www.nature.com/articles/nature04153.pdf\n    [2] Lucy M. Li, Nicholas C. Grassly, Christophe Fraser (2017)\n        ""Quantifying Transmission Heterogeneity Using Both Pathogen Phylogenies\n        and Incidence Time Series""\n        https://academic.oup.com/mbe/article/34/11/2982/3952784\n\n    :param int population: Total ``population = S + E + I + R``.\n    :param float incubation_time: Mean incubation time (duration in state\n        ``E``). Must be greater than 1.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param iterable data: Time series of new observed infections. Each time\n        step is Binomial distributed between 0 and the number of ``S -> E``\n        transitions. This allows false negative but no false positives.\n    """"""\n\n    def __init__(self, population, incubation_time, recovery_time, data, *,\n                 leaf_times=None, coal_times=None):\n        compartments = (""S"", ""E"", ""I"")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(incubation_time, float)\n        assert incubation_time > 1\n        self.incubation_time = incubation_time\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        self.data = data\n\n        assert (leaf_times is None) == (coal_times is None)\n        if leaf_times is None:\n            self.coal_likelihood = None\n        else:\n            self.coal_likelihood = dist.CoalescentRateLikelihood(\n                leaf_times, coal_times, duration)\n\n    def global_model(self):\n        tau_e = self.incubation_time\n        tau_i = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n        k = pyro.sample(""k"", dist.Exponential(1.))\n        rho = pyro.sample(""rho"", dist.Beta(10, 10))\n        return R0, k, tau_e, tau_i, rho\n\n    def initialize(self, params):\n        # Start with a single exposure.\n        return {""S"": self.population - 1, ""E"": 0, ""I"": 1}\n\n    def transition(self, params, state, t):\n        R0, k, tau_e, tau_i, rho = params\n\n        # Sample flows between compartments.\n        E2I = pyro.sample(""E2I_{}"".format(t),\n                          dist.ExtendedBinomial(state[""E""], 1 / tau_e))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          dist.ExtendedBinomial(state[""I""], 1 / tau_i))\n        S2E = pyro.sample(""S2E_{}"".format(t),\n                          infection_dist(individual_rate=R0,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""],\n                                         population=self.population,\n                                         concentration=k))\n\n        # Condition on observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2E, rho),\n                    obs=self.data[t] if t_is_observed else None)\n        if self.coal_likelihood is not None:\n            R = R0 * state[""S""] / self.population\n            coal_rate = R * (1. + 1. / k) / (tau_i * state[""I""] + 1e-8)\n            pyro.factor(""coalescent_{}"".format(t),\n                        self.coal_likelihood(coal_rate, t)\n                        if t_is_observed else torch.tensor(0.))\n\n        # Update compartements with flows.\n        state[""S""] = state[""S""] - S2E\n        state[""E""] = state[""E""] + S2E - E2I\n        state[""I""] = state[""I""] + E2I - I2R\n\n\nclass HeterogeneousSIRModel(CompartmentalModel):\n    """"""\n    Generalizes :class:`SimpleSIRModel` by allowing ``Re`` and ``rho`` to vary\n    in time.\n\n    To customize this model we recommend forking and editing this class.\n\n    In this model, the response rate ``rho`` is piecewise constant with unknown\n    value over three pieces. The reproductive number ``Re`` is a product of a\n    constant ``R0`` with a factor ``beta`` that drifts via Brownian motion in\n    log space. Both ``rho`` and ``Re`` are available as time series.\n\n    :param int population: Total ``population = S + I + R``.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param iterable data: Time series of new observed infections. Each time\n        step is Binomial distributed between 0 and the number of ``S -> I``\n        transitions. This allows false negative but no false positives.\n    """"""\n\n    def __init__(self, population, recovery_time, data):\n        compartments = (""S"", ""I"")  # R is implicit.\n        duration = len(data)\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        self.data = data\n\n    def global_model(self):\n        tau = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n\n        # Let\'s consider a piecewise constant response rate, say low rate for\n        # two weeks, then intermediate rate as testing capacity increases, and\n        # finally high rate for a few months (as far into the future as we\'d\n        # like to forecast). We don\'t know exactly what the rates are, but we\n        # can specify increasingly informative priors.\n        rho0 = pyro.sample(""rho0"", dist.Beta(2, 4))\n        rho1 = pyro.sample(""rho1"", dist.Beta(4, 4))\n        rho2 = pyro.sample(""rho2"", dist.Beta(8, 4))\n        # Later .transition() will index into this time series as rho[..., t].\n        rho = torch.cat([rho0.unsqueeze(-1).expand(rho0.shape + (14,)),\n                         rho1.unsqueeze(-1).expand(rho1.shape + (7,)),\n                         rho2.unsqueeze(-1).expand(rho2.shape + (60,))], dim=-1)\n        # We can also save the time series for output in self.samples.\n        pyro.deterministic(""rho"", rho, event_dim=1)\n\n        return R0, tau, rho\n\n    def initialize(self, params):\n        R0, tau, rho = params\n        # Start with a single infection.\n        # We also store the initial beta value in the state dict.\n        return {""S"": self.population - 1, ""I"": 1, ""beta"": torch.tensor(1.)}\n\n    def transition(self, params, state, t):\n        R0, tau, rho = params\n\n        # Sample heterogeneous variables.\n        # This assumes beta slowly drifts via Brownian motion in log space.\n        beta = pyro.sample(""beta_{}"".format(t),\n                           dist.LogNormal(state[""beta""].log(), 0.1))\n        Re = pyro.deterministic(""Re_{}"".format(t), R0 * beta)\n\n        # Sample flows between compartments.\n        S2I = pyro.sample(""S2I_{}"".format(t),\n                          infection_dist(individual_rate=Re / tau,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""],\n                                         population=self.population))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          dist.ExtendedBinomial(state[""I""], 1 / tau))\n\n        # Update compartments and heterogeneous variables.\n        state[""S""] = state[""S""] - S2I\n        state[""I""] = state[""I""] + S2I - I2R\n        state[""beta""] = beta  # We store the latest beta value in the state dict.\n\n        # Condition on observations.\n        # Note that, since rho may be batched over particles or samples, we\n        # need to index it via rho[..., t] rather than a simple rho[t].\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2I, rho[..., t]),\n                    obs=self.data[t] if t_is_observed else None)\n\n\nclass SparseSIRModel(CompartmentalModel):\n    """"""\n    Generalizes :class:`SimpleSIRModel` to allow sparsely observed infections.\n\n    To customize this model we recommend forking and editing this class.\n\n    This model allows observations of **cumulative** infections at uneven time\n    intervals. To preserve Markov structure (and hence tractable inference)\n    this model adds an auxiliary compartment ``O`` denoting the fully-observed\n    cumulative number of observations at each time point. At observed times\n    (when ``mask[t] == True``) ``O`` must exactly match the provided data;\n    between observed times ``O`` stochastically imputes the provided data.\n\n    This model demonstrates how to implement a custom :meth:`compute_flows`\n    method. A custom method is needed in this model because inhabitants of the\n    ``S`` compartment can transition to both the ``I`` and ``O`` compartments,\n    allowing duplication.\n\n    :param int population: Total ``population = S + I + R``.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param iterable data: Time series of **cumulative** observed infections.\n        Whenever ``mask[t] == True``, ``data[t]`` corresponds to an\n        observation; otherwise ``data[t]`` can be arbitrary, e.g. NAN.\n    :param iterable mask: Boolean time series denoting whether an observation\n        is made at each time step. Should satisfy ``len(mask) == len(data)``.\n    """"""\n\n    def __init__(self, population, recovery_time, data, mask):\n        assert len(data) == len(mask)\n        duration = len(data)\n        compartments = (""S"", ""I"", ""O"")  # O is auxiliary, R is implicit.\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        self.data = data\n        self.mask = mask\n\n    def global_model(self):\n        tau = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n        rho = pyro.sample(""rho"", dist.Beta(10, 10))\n        return R0, tau, rho\n\n    def initialize(self, params):\n        # Start with a single infection.\n        return {""S"": self.population - 1, ""I"": 1, ""O"": 0}\n\n    def transition(self, params, state, t):\n        R0, tau, rho = params\n\n        # Sample flows between compartments.\n        S2I = pyro.sample(""S2I_{}"".format(t),\n                          infection_dist(individual_rate=R0 / tau,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""],\n                                         population=self.population))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          dist.ExtendedBinomial(state[""I""], 1 / tau))\n        S2O = pyro.sample(""S2O_{}"".format(t),\n                          dist.ExtendedBinomial(S2I, rho))\n\n        # Update compartments with flows.\n        state[""S""] = state[""S""] - S2I\n        state[""I""] = state[""I""] + S2I - I2R\n        state[""O""] = state[""O""] + S2O\n\n        # Condition on cumulative observations.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        mask_t = self.mask[t] if t_is_observed else False\n        data_t = self.data[t] if t_is_observed else None\n        pyro.sample(""obs_{}"".format(t),\n                    dist.Delta(state[""O""]).mask(mask_t),\n                    obs=data_t)\n\n    def compute_flows(self, prev, curr, t):\n        # Reverse the flow computation.\n        S2I = prev[""S""] - curr[""S""]\n        I2R = prev[""I""] - curr[""I""] + S2I\n        S2O = curr[""O""] - prev[""O""]\n        return {\n            ""S2I_{}"".format(t): S2I,\n            ""I2R_{}"".format(t): I2R,\n            ""S2O_{}"".format(t): S2O,\n        }\n\n\nclass UnknownStartSIRModel(CompartmentalModel):\n    """"""\n    Generalizes :class:`SimpleSIRModel` by allowing unknown date of first\n    infection.\n\n    To customize this model we recommend forking and editing this class.\n\n    This model demonstrates:\n\n    1.  How to incorporate spontaneous infections from external sources;\n    2.  How to incorporate time-varying piecewise ``rho`` by supporting\n        forecasting in :meth:`transition`.\n    3.  How to override the :meth:`predict` method to compute extra\n        statistics.\n\n    :param int population: Total ``population = S + I + R``.\n    :param float recovery_time: Mean recovery time (duration in state\n        ``I``). Must be greater than 1.\n    :param int pre_obs_window: Number of time steps before beginning ``data``\n        where the initial infection may have occurred. Must be positive.\n    :param iterable data: Time series of new observed infections. Each time\n        step is Binomial distributed between 0 and the number of ``S -> I``\n        transitions. This allows false negative but no false positives.\n    """"""\n\n    def __init__(self, population, recovery_time, pre_obs_window, data):\n        compartments = (""S"", ""I"")  # R is implicit.\n        duration = pre_obs_window + len(data)\n        super().__init__(compartments, duration, population)\n\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        self.recovery_time = recovery_time\n\n        assert isinstance(pre_obs_window, int) and pre_obs_window > 0\n        self.pre_obs_window = pre_obs_window\n        self.post_obs_window = len(data)\n\n        # We set a small time-constant external infecton rate such that on\n        # average there is a single external infection during the\n        # pre_obs_window. This allows unknown time of initial infection\n        # without introducing long-range coupling across time.\n        self.external_rate = 1 / pre_obs_window\n\n        # Prepend data with zeros.\n        if isinstance(data, list):\n            data = [0.] * self.pre_obs_window + data\n        else:\n            data = pad(data, (self.pre_obs_window, 0), value=0.)\n        self.data = data\n\n    def global_model(self):\n        tau = self.recovery_time\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n\n        # Assume two different response rates: rho0 before any observations\n        # were made (in pre_obs_window), followed by a higher response rate rho1\n        # after observations were made (in post_obs_window).\n        rho0 = pyro.sample(""rho0"", dist.Beta(10, 10))\n        rho1 = pyro.sample(""rho1"", dist.Beta(10, 10))\n        # Whereas each of rho0,rho1 are scalars (possibly batched over samples),\n        # we construct a time series rho with an extra time dim on the right.\n        rho = torch.cat([\n            rho0.unsqueeze(-1).expand(rho0.shape + (self.pre_obs_window,)),\n            rho1.unsqueeze(-1).expand(rho1.shape + (self.post_obs_window,)),\n        ], dim=-1)\n\n        # Model external infections as an infectious pseudo-individual added\n        # to num_infectious when sampling S2I below.\n        X = self.external_rate * tau / R0\n\n        return R0, X, tau, rho\n\n    def initialize(self, params):\n        # Start with no internal infections.\n        return {""S"": self.population, ""I"": 0}\n\n    def transition(self, params, state, t):\n        R0, X, tau, rho = params\n\n        # Sample flows between compartments.\n        S2I = pyro.sample(""S2I_{}"".format(t),\n                          infection_dist(individual_rate=R0 / tau,\n                                         num_susceptible=state[""S""],\n                                         num_infectious=state[""I""] + X,\n                                         population=self.population))\n        I2R = pyro.sample(""I2R_{}"".format(t),\n                          dist.ExtendedBinomial(state[""I""], 1 / tau))\n\n        # Update compartments with flows.\n        state[""S""] = state[""S""] - S2I\n        state[""I""] = state[""I""] + S2I - I2R\n\n        # In .transition() t will always be an integer but may lie outside\n        # of [0,self.duration) when forecasting.\n        t_is_observed = isinstance(t, slice) or t < self.duration\n        rho_t = rho[..., t] if t_is_observed else rho[..., -1]\n        data_t = self.data[t] if t_is_observed else None\n\n        # Condition on observations.\n        pyro.sample(""obs_{}"".format(t),\n                    dist.ExtendedBinomial(S2I, rho_t),\n                    obs=data_t)\n\n    def predict(self, forecast=0):\n        """"""\n        Augments\n        :meth:`~pyro.contrib.epidemiology.compartmental.Compartmental.predict`\n        with samples of ``first_infection`` i.e. the first time index at which\n        the infection ``I`` becomes nonzero. Note this is measured from the\n        beginning of ``pre_obs_window``, not the beginning of data.\n\n        :param int forecast: The number of time steps to forecast forward.\n        :returns: A dictionary mapping sample site name (or compartment name)\n            to a tensor whose first dimension corresponds to sample batching.\n        :rtype: dict\n        """"""\n        samples = super().predict(forecast)\n\n        # Extract the time index of the first infection (samples[""I""] > 0)\n        # for each sample trajectory in the samples[""I""] tensor.\n        samples[""first_infection""] = samples[""I""].cumsum(-1).eq(0).sum(-1)\n\n        return samples\n\n\nclass RegionalSIRModel(CompartmentalModel):\n    r""""""\n    Generalizes :class:`SimpleSIRModel` to simultaneously model multiple\n    regions with weak coupling across regions.\n\n    To customize this model we recommend forking and editing this class.\n\n    Regions are coupled by a ``coupling`` matrix with entries in ``[0,1]``.\n    The all ones matrix is equivalent to a single region. The identity matrix\n    is equivalent to a set of independent regions. This need not be symmetric,\n    but symmetric matrices are probably more physically plausible. The expected\n    number of new infections each time step ``S2I`` is Binomial distributed\n    with mean::\n\n        E[S2I] = S (1 - (1 - R0 / (population @ coupling)) ** (I @ coupling))\n               \xe2\x89\x88 R0 S (I @ coupling) / (population @ coupling)  # for small I\n\n    Thus in a nearly entirely susceptible population, a single infected\n    individual infects approximately ``R0`` new individuals on average,\n    independent of ``coupling``.\n\n    This model demonstrates:\n\n    1.  How to create a regional model with a ``population`` vector.\n    2.  How to model both homogeneous parameters (here ``R0``) and\n        heterogeneous parameters with hierarchical structure (here ``rho``)\n        using ``self.region_plate``.\n    3.  How to approximately couple regions in :meth:`transition` using\n        ``state[""I_approx""]``.\n\n    :param torch.Tensor population: Tensor of per-region populations, defining\n        ``population = S + I + R``.\n    :param torch.Tensor coupling: Pairwise coupling matrix. Entries should be\n        in ``[0,1]``.\n    :param float recovery_time: Mean recovery time (duration in state ``I``).\n        Must be greater than 1.\n    :param iterable data: Time x Region sized tensor of new observed\n        infections. Each time step is vector of Binomials distributed between\n        0 and the number of ``S -> I`` transitions. This allows false negative\n        but no false positives.\n    """"""\n\n    def __init__(self, population, coupling, recovery_time, data):\n        duration = len(data)\n        num_regions, = population.shape\n        assert coupling.shape == (num_regions, num_regions)\n        assert (0 <= coupling).all()\n        assert (coupling <= 1).all()\n        assert isinstance(recovery_time, float)\n        assert recovery_time > 1\n        if isinstance(data, torch.Tensor):\n            # Data tensors should be oriented as (time, region).\n            assert data.shape == (duration, num_regions)\n        compartments = (""S"", ""I"")  # R is implicit.\n\n        # We create a regional model by passing a vector of populations.\n        super().__init__(compartments, duration, population, approximate=(""I"",))\n\n        self.coupling = coupling\n        self.recovery_time = recovery_time\n        self.data = data\n\n    def global_model(self):\n        # Assume recovery time is a known constant.\n        tau = self.recovery_time\n\n        # Assume reproductive number is unknown but homogeneous.\n        R0 = pyro.sample(""R0"", dist.LogNormal(0., 1.))\n\n        # Assume response rate is heterogeneous and model it with a\n        # hierarchical Gamma-Beta prior.\n        rho_c1 = pyro.sample(""rho_c1"", dist.Gamma(10, 1))\n        rho_c0 = pyro.sample(""rho_c0"", dist.Gamma(10, 1))\n        with self.region_plate:\n            rho = pyro.sample(""rho"", dist.Beta(rho_c1, rho_c0))\n\n        return R0, tau, rho\n\n    def initialize(self, params):\n        # Start with a single infection in region 0.\n        I = torch.zeros_like(self.population)\n        I[0] += 1\n        S = self.population - I\n        return {""S"": S, ""I"": I}\n\n    def transition(self, params, state, t):\n        R0, tau, rho = params\n\n        # Account for infections from all regions. This uses approximate (point\n        # estimate) counts I_approx for infection from other regions, but uses\n        # the exact (enumerated) count I for infections from one\'s own region.\n        I_coupled = state[""I_approx""] @ self.coupling\n        I_coupled = I_coupled + (state[""I""] - state[""I_approx""]) * self.coupling.diag()\n        I_coupled = I_coupled.clamp(min=0)  # In case I_approx is negative.\n        pop_coupled = self.population @ self.coupling\n\n        with self.region_plate:\n            # Sample flows between compartments.\n            S2I = pyro.sample(""S2I_{}"".format(t),\n                              infection_dist(individual_rate=R0 / tau,\n                                             num_susceptible=state[""S""],\n                                             num_infectious=I_coupled,\n                                             population=pop_coupled))\n            I2R = pyro.sample(""I2R_{}"".format(t),\n                              dist.ExtendedBinomial(state[""I""], 1 / tau))\n\n            # Update compartments with flows.\n            state[""S""] = state[""S""] - S2I\n            state[""I""] = state[""I""] + S2I - I2R\n\n            # Condition on observations.\n            t_is_observed = isinstance(t, slice) or t < self.duration\n            pyro.sample(""obs_{}"".format(t),\n                        dist.ExtendedBinomial(S2I, rho),\n                        obs=self.data[t] if t_is_observed else None)\n\n\n# Create sphinx documentation.\n__all__ = []\nfor _name, _Model in list(locals().items()):\n    if isinstance(_Model, type) and issubclass(_Model, CompartmentalModel):\n        if _Model is not CompartmentalModel:\n            __all__.append(_name)\n__all__.sort(key=lambda name, vals=locals(): vals[name].__init__.__code__.co_firstlineno)\n__doc__ = ""\\n\\n"".join([\n    """"""\n    {}\n    ----------------------------------------------------------------\n    .. autoclass:: pyro.contrib.epidemiology.models.{}\n    """""".format(re.sub(""([A-Z][a-z]+)"", r""\\1 "", _name[:-5]), _name)\n    for _name in __all__\n])\n'"
pyro/contrib/epidemiology/util.py,30,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.distributions.util import broadcast_shape\nfrom pyro.ops.special import safe_log\n\n\ndef clamp(tensor, *, min=None, max=None):\n    """"""\n    Like :func:`torch.clamp` but dispatches to :func:`torch.min` and/or\n    :func:`torch.max` if ``min`` and/or ``max`` is a :class:`~torch.Tensor`.\n    """"""\n    if isinstance(min, torch.Tensor):\n        tensor = torch.max(tensor, min)\n        min = None\n    if isinstance(max, torch.Tensor):\n        tensor = torch.min(tensor, max)\n        max = None\n    if min is None and max is None:\n        return tensor\n    return tensor.clamp(min=min, max=max)\n\n\ndef cat2(lhs, rhs, *, dim=-1):\n    """"""\n    Like ``torch.cat([lhs, rhs], dim=dim)`` but dispatches to\n    :func:`torch.nn.functional.pad` in case one of ``lhs`` or ``rhs`` is a\n    scalar.\n    """"""\n    assert dim < 0\n    if not isinstance(lhs, torch.Tensor):\n        pad = (0, 0) * (-1 - dim) + (1, 0)\n        return torch.nn.functional.pad(rhs, pad, value=lhs)\n    if not isinstance(rhs, torch.Tensor):\n        pad = (0, 0) * (-1 - dim) + (0, 1)\n        return torch.nn.functional.pad(lhs, pad, value=rhs)\n\n    diff = lhs.dim() - rhs.dim()\n    if diff > 0:\n        rhs = rhs.expand((1,) * diff + rhs.shape)\n    elif diff < 0:\n        diff = -diff\n        lhs = lhs.expand((1,) * diff + lhs.shape)\n    shape = list(broadcast_shape(lhs.shape, rhs.shape))\n    shape[dim] = -1\n    return torch.cat([lhs.expand(shape), rhs.expand(shape)], dim=dim)\n\n\n@torch.no_grad()\ndef align_samples(samples, model, particle_dim):\n    """"""\n    Unsqueeze stacked samples such that their particle dim all aligns.\n    This traces ``model`` to determine the ``event_dim`` of each site.\n    """"""\n    assert particle_dim < 0\n\n    sample = {name: value[0] for name, value in samples.items()}\n    with poutine.block(), poutine.trace() as tr, poutine.condition(data=sample):\n        model()\n\n    samples = samples.copy()\n    for name, value in samples.items():\n        event_dim = tr.trace.nodes[name][""fn""].event_dim\n        pad = event_dim - particle_dim - value.dim()\n        if pad < 0:\n            raise ValueError(""Cannot align samples, try moving particle_dim left"")\n        if pad > 0:\n            shape = value.shape[:1] + (1,) * pad + value.shape[1:]\n            samples[name] = value.reshape(shape)\n\n    return samples\n\n\n# this 8 x 10 tensor encodes the coefficients of 8 10-dimensional polynomials\n# that are used to construct the num_quant_bins=16 quantization strategy\n\nW16 = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1562511562511555e-07],\n       [1.1562511562511557e-07, 1.04062604062604e-06, 4.16250416250416e-06,\n        9.712509712509707e-06, 1.456876456876456e-05, 1.4568764568764562e-05,\n        9.712509712509707e-06, 4.16250416250416e-06, 1.04062604062604e-06, -6.937506937506934e-07],\n       [5.839068339068337e-05, 0.0002591158841158841, 0.0005036630036630038,\n        0.0005536130536130536, 0.00036421911421911425, 0.00013111888111888106,\n        9.712509712509736e-06, -1.2487512487512482e-05, -5.2031302031302014e-06, 1.6187516187516182e-06],\n       [0.0018637612387612374, 0.004983558108558107, 0.005457042957042955,\n        0.0029234654234654212, 0.000568181818181818, -0.0001602564102564102,\n        -8.741258741258739e-05, 4.162504162504162e-06, 9.365634365634364e-06, -1.7536475869809201e-06],\n       [0.015560115039281694, 0.025703289765789755, 0.015009296259296255,\n        0.0023682336182336166, -0.000963966588966589, -0.00029380341880341857,\n        5.6656306656306665e-05, 1.5956265956265953e-05, -6.417193917193917e-06, 7.515632515632516e-07],\n       [0.057450111616778265, 0.05790875790875791, 0.014424464424464418,\n        -0.0030303030303030303, -0.0013791763791763793, 0.00011655011655011669,\n        5.180005180005181e-05, -8.325008325008328e-06, 3.4687534687534703e-07, 0.0],\n       [0.12553422657589322, 0.072988122988123, -0.0011641136641136712,\n        -0.006617456617456618, -0.00028651903651903725, 0.00027195027195027195,\n        3.2375032375032334e-06, -5.550005550005552e-06, 3.4687534687534703e-07, 0.0],\n       [0.21761806865973532, 1.7482707128494565e-17, -0.028320290820290833,\n        0.0, 0.0014617327117327117, 0.0,\n        -3.561253561253564e-05, 0.0, 3.4687534687534714e-07, 0.0]]\n\nW16 = numpy.array(W16)\n\n\ndef compute_bin_probs(s, num_quant_bins):\n    """"""\n    Compute categorical probabilities for a quantization scheme with num_quant_bins many\n    bins. `s` is a real-valued tensor with values in [0, 1]. Returns probabilities\n    of shape `s.shape` + `(num_quant_bins,)`\n    """"""\n\n    t = 1 - s\n\n    if num_quant_bins == 2:\n        probs = torch.stack([t, s], dim=-1)\n        return probs\n\n    ss = s * s\n    tt = t * t\n\n    if num_quant_bins == 4:\n        # This cubic spline interpolates over the nearest four integers, ensuring\n        # piecewise quadratic gradients.\n        probs = torch.stack([\n            t * tt,\n            4 + ss * (3 * s - 6),\n            4 + tt * (3 * t - 6),\n            s * ss,\n        ], dim=-1) * (1/6)\n        return probs\n\n    if num_quant_bins == 8:\n        # This quintic spline interpolates over the nearest eight integers, ensuring\n        # piecewise quartic gradients.\n        s3 = ss * s\n        s4 = ss * ss\n        s5 = s3 * ss\n\n        t3 = tt * t\n        t4 = tt * tt\n        t5 = t3 * tt\n\n        probs = torch.stack([\n            2 * t5,\n            2 + 10 * t + 20 * tt + 20 * t3 + 10 * t4 - 7 * t5,\n            55 + 115 * t + 70 * tt - 9 * t3 - 25 * t4 + 7 * t5,\n            302 - 100 * ss + 10 * s4,\n            302 - 100 * tt + 10 * t4,\n            55 + 115 * s + 70 * ss - 9 * s3 - 25 * s4 + 7 * s5,\n            2 + 10 * s + 20 * ss + 20 * s3 + 10 * s4 - 7 * s5,\n            2 * s5\n        ], dim=-1) * (1/840)\n        return probs\n\n    if num_quant_bins == 12:\n        # This septic spline interpolates over the nearest 12 integers\n        s3 = ss * s\n        s4 = ss * ss\n        s5 = s3 * ss\n        s6 = s3 * s3\n        s7 = s4 * s3\n\n        t3 = tt * t\n        t4 = tt * tt\n        t5 = t3 * tt\n        t6 = t3 * t3\n        t7 = t4 * t3\n\n        probs = torch.stack([\n            693 * t7,\n            693 + 4851 * t + 14553 * tt + 24255 * t3 + 24255 * t4 + 14553 * t5 + 4851 * t6 - 3267 * t7,\n            84744 + 282744 * t + 382536 * tt + 249480 * t3 + 55440 * t4 - 24948 * t5 - 18018 * t6 + 5445 * t7,\n            1017423 + 1823283 * t + 1058211 * tt + 51975 * t3 - 148995 * t4 - 18711 * t5 + 20097 * t6 - 3267 * t7,\n            3800016 + 3503808 * t + 365904 * tt - 443520 * t3 - 55440 * t4 + 33264 * t5 - 2772 * t6,\n            8723088 - 1629936 * ss + 110880.0 * s4 - 2772 * s6,\n            8723088 - 1629936 * tt + 110880.0 * t4 - 2772 * t6,\n            3800016 + 3503808 * s + 365904 * ss - 443520 * s3 - 55440 * s4 + 33264 * s5 - 2772 * s6,\n            1017423 + 1823283 * s + 1058211 * ss + 51975 * s3 - 148995 * s4 - 18711 * s5 + 20097 * s6 - 3267 * s7,\n            84744 + 282744 * s + 382536 * ss + 249480 * s3 + 55440 * s4 - 24948 * s5 - 18018 * s6 + 5445 * s7,\n            693 + 4851 * s + 14553 * ss + 24255 * s3 + 24255 * s4 + 14553 * s5 + 4851 * s6 - 3267 * s7,\n            693 * s7,\n        ], dim=-1) * (1/32931360)\n        return probs\n\n    if num_quant_bins == 16:\n        # This nonic spline interpolates over the nearest 16 integers\n        w16 = torch.from_numpy(W16).to(s.device).type_as(s)\n        s_powers = s.unsqueeze(-1).unsqueeze(-1).pow(torch.arange(10.))\n        t_powers = t.unsqueeze(-1).unsqueeze(-1).pow(torch.arange(10.))\n        splines_t = (w16 * t_powers).sum(-1)\n        splines_s = (w16 * s_powers).sum(-1)\n        index = [0, 1, 2, 3, 4, 5, 6, 15, 7, 14, 13, 12, 11, 10, 9, 8]\n        probs = torch.cat([splines_t, splines_s], dim=-1)\n        probs = probs.index_select(-1, torch.tensor(index))\n        return probs\n\n    raise ValueError(""Unsupported num_quant_bins: {}"".format(num_quant_bins))\n\n\ndef _all(x):\n    return x.all() if isinstance(x, torch.Tensor) else x\n\n\ndef _unsqueeze(x):\n    return x.unsqueeze(-1) if isinstance(x, torch.Tensor) else x\n\n\ndef quantize(name, x_real, min, max, num_quant_bins=4):\n    """"""Randomly quantize in a way that preserves probability mass.""""""\n    assert _all(min < max)\n    lb = x_real.detach().floor()\n\n    probs = compute_bin_probs(x_real - lb, num_quant_bins=num_quant_bins)\n\n    q = pyro.sample(""Q_"" + name, dist.Categorical(probs),\n                    infer={""enumerate"": ""parallel""})\n    q = q.type_as(x_real) - (num_quant_bins // 2 - 1)\n\n    x = lb + q\n    x = torch.max(x, 2 * min - 1 - x)\n    x = torch.min(x, 2 * max + 1 - x)\n\n    return pyro.deterministic(name, x, event_dim=0)\n\n\ndef quantize_enumerate(x_real, min, max, num_quant_bins=4):\n    """"""Quantize, then manually enumerate.""""""\n    assert _all(min < max)\n    lb = x_real.detach().floor()\n\n    probs = compute_bin_probs(x_real - lb, num_quant_bins=num_quant_bins)\n    logits = safe_log(probs)\n\n    arange_min = 1 - num_quant_bins // 2\n    arange_max = 1 + num_quant_bins // 2\n    q = torch.arange(arange_min, arange_max)\n\n    x = lb.unsqueeze(-1) + q\n    x = torch.max(x, 2 * _unsqueeze(min) - 1 - x)\n    x = torch.min(x, 2 * _unsqueeze(max) + 1 - x)\n\n    return x, logits\n'"
pyro/contrib/examples/__init__.py,0,b''
pyro/contrib/examples/bart.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport bz2\nimport csv\nimport datetime\nimport logging\nimport multiprocessing\nimport os\nimport subprocess\nimport sys\nimport urllib\n\nimport torch\n\nfrom pyro.contrib.examples.util import get_data_directory\n\nDATA = get_data_directory(__file__)\n\n# https://www.bart.gov/about/reports/ridership\nSOURCE_DIR = ""http://64.111.127.166/origin-destination/""\nSOURCE_FILES = [\n    ""date-hour-soo-dest-2011.csv.gz"",\n    ""date-hour-soo-dest-2012.csv.gz"",\n    ""date-hour-soo-dest-2013.csv.gz"",\n    ""date-hour-soo-dest-2014.csv.gz"",\n    ""date-hour-soo-dest-2015.csv.gz"",\n    ""date-hour-soo-dest-2016.csv.gz"",\n    ""date-hour-soo-dest-2017.csv.gz"",\n    ""date-hour-soo-dest-2018.csv.gz"",\n    ""date-hour-soo-dest-2019.csv.gz"",\n]\nCACHE_URL = ""https://d2hg8soec8ck9v.cloudfront.net/datasets/bart_full.pkl.bz2""\n\n\ndef _mkdir_p(dirname):\n    if not os.path.exists(dirname):\n        try:\n            os.makedirs(dirname)\n        except FileExistsError:\n            pass\n\n\ndef _load_hourly_od(basename):\n    filename = os.path.join(DATA, basename.replace("".csv.gz"", "".pkl""))\n    if os.path.exists(filename):\n        return filename\n\n    # Download source files.\n    gz_filename = os.path.join(DATA, basename)\n    if not os.path.exists(gz_filename):\n        url = SOURCE_DIR + basename\n        logging.debug(""downloading {}"".format(url))\n        urllib.request.urlretrieve(url, gz_filename)\n    csv_filename = gz_filename[:-3]\n    assert csv_filename.endswith("".csv"")\n    if not os.path.exists(csv_filename):\n        logging.debug(""unzipping {}"".format(gz_filename))\n        subprocess.check_call([""gunzip"", ""-k"", gz_filename])\n    assert os.path.exists(csv_filename)\n\n    # Convert to PyTorch.\n    logging.debug(""converting {}"".format(csv_filename))\n    start_date = datetime.datetime.strptime(""2000-01-01"", ""%Y-%m-%d"")\n    stations = {}\n    num_rows = sum(1 for _ in open(csv_filename))\n    logging.info(""Formatting {} rows"".format(num_rows))\n    rows = torch.empty((num_rows, 4), dtype=torch.long)\n    with open(csv_filename) as f:\n        for i, (date, hour, origin, destin, trip_count) in enumerate(csv.reader(f)):\n            date = datetime.datetime.strptime(date, ""%Y-%m-%d"")\n            date += datetime.timedelta(hours=int(hour))\n            rows[i, 0] = int((date - start_date).total_seconds() / 3600)\n            rows[i, 1] = stations.setdefault(origin, len(stations))\n            rows[i, 2] = stations.setdefault(destin, len(stations))\n            rows[i, 3] = int(trip_count)\n            if i % 10000 == 0:\n                sys.stderr.write(""."")\n                sys.stderr.flush()\n\n    # Save data with metadata.\n    dataset = {\n        ""basename"": basename,\n        ""start_date"": start_date,\n        ""stations"": stations,\n        ""rows"": rows,\n        ""schema"": [""time_hours"", ""origin"", ""destin"", ""trip_count""],\n    }\n    dataset[""rows""]\n    logging.debug(""saving {}"".format(filename))\n    torch.save(dataset, filename)\n    return filename\n\n\ndef load_bart_od():\n    """"""\n    Load a dataset of hourly origin-destination ridership counts for every pair\n    of BART stations during the years 2011-2019.\n\n    **Source** https://www.bart.gov/about/reports/ridership\n\n    This downloads the dataset the first time it is called. On subsequent calls\n    this reads from a local cached file ``.pkl.bz2``. This attempts to\n    download a preprocessed compressed cached file maintained by the Pyro team.\n    On cache hit this should be very fast. On cache miss this falls back to\n    downloading the original data source and preprocessing the dataset,\n    requiring about 350MB of file transfer, storing a few GB of temp files, and\n    taking upwards of 30 minutes.\n\n    :returns: a dataset is a dictionary with fields:\n\n        -   ""stations"": a list of strings of station names\n        -   ""start_date"": a :py:class:`datetime.datetime` for the first observaion\n        -   ""counts"": a ``torch.FloatTensor`` of ridership counts, with shape\n            ``(num_hours, len(stations), len(stations))``.\n    """"""\n    _mkdir_p(DATA)\n    filename = os.path.join(DATA, ""bart_full.pkl.bz2"")\n    # Work around apparent bug in torch.load(),torch.save().\n    pkl_file = filename.rsplit(""."", 1)[0]\n    if not os.path.exists(pkl_file):\n        try:\n            urllib.request.urlretrieve(CACHE_URL, filename)\n            logging.debug(""cache hit, uncompressing"")\n            with bz2.BZ2File(filename) as src, open(filename[:-4], ""wb"") as dst:\n                dst.write(src.read())\n        except urllib.error.HTTPError:\n            logging.debug(""cache miss, preprocessing from scratch"")\n    if os.path.exists(pkl_file):\n        return torch.load(pkl_file)\n\n    filenames = multiprocessing.Pool(len(SOURCE_FILES)).map(_load_hourly_od, SOURCE_FILES)\n    datasets = list(map(torch.load, filenames))\n\n    stations = sorted(set().union(*(d[""stations""].keys() for d in datasets)))\n    min_time = min(int(d[""rows""][:, 0].min()) for d in datasets)\n    max_time = max(int(d[""rows""][:, 0].max()) for d in datasets)\n    num_rows = max_time - min_time + 1\n    start_date = datasets[0][""start_date""] + datetime.timedelta(hours=min_time),\n    logging.info(""Loaded data from {} stations, {} hours""\n                 .format(len(stations), num_rows))\n\n    result = torch.zeros(num_rows, len(stations), len(stations))\n    for dataset in datasets:\n        part_stations = sorted(dataset[""stations""], key=dataset[""stations""].__getitem__)\n        part_to_whole = torch.tensor(list(map(stations.index, part_stations)))\n        time = dataset[""rows""][:, 0] - min_time\n        origin = part_to_whole[dataset[""rows""][:, 1]]\n        destin = part_to_whole[dataset[""rows""][:, 2]]\n        count = dataset[""rows""][:, 3].float()\n        result[time, origin, destin] = count\n        dataset.clear()\n    logging.info(""Loaded {} shaped data of mean {:0.3g}""\n                 .format(result.shape, result.mean()))\n\n    dataset = {\n        ""stations"": stations,\n        ""start_date"": start_date,\n        ""counts"": result,\n    }\n    torch.save(dataset, pkl_file)\n    subprocess.check_call([""bzip2"", ""-k"", pkl_file])\n    assert os.path.exists(filename)\n    return dataset\n\n\ndef load_fake_od():\n    """"""\n    Create a tiny synthetic dataset for smoke testing.\n    """"""\n    dataset = {\n        ""stations"": [""12TH"", ""EMBR"", ""SFIA""],\n        ""start_date"": datetime.datetime.strptime(""2000-01-01"", ""%Y-%m-%d""),\n        ""counts"": torch.distributions.Poisson(100).sample([24 * 7 * 8, 3, 3]),\n    }\n    return dataset\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""BART data preprocessor"")\n    parser.add_argument(""-v"", ""--verbose"", action=""store_true"")\n    args = parser.parse_args()\n\n    logging.basicConfig(format=\'%(relativeCreated) 9d %(message)s\',\n                        level=logging.DEBUG if args.verbose else logging.INFO)\n    load_bart_od()\n'"
pyro/contrib/examples/finance.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport urllib\n\nimport pandas as pd\n\nfrom pyro.contrib.examples.util import get_data_directory\n\nDATA = get_data_directory(__file__)\n\n# https://finance.yahoo.com/quote/%5EGSPC/history/\nCACHE_URL = ""https://d2hg8soec8ck9v.cloudfront.net/datasets/snp500.csv.bz2""\n\n\ndef load_snp500():\n    """"""\n    Loads pandas dataframe of S&P 500 daily values from 1927-12-30 thru 2020-01-10.\n    """"""\n    filename = os.path.join(DATA, ""snp500.csv.bz2"")\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(CACHE_URL, filename)\n    df = pd.read_csv(filename)\n    return df\n'"
pyro/contrib/examples/multi_mnist.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis script generates a dataset similar to the Multi-MNIST dataset\ndescribed in [1].\n\n[1] Eslami, SM Ali, et al. ""Attend, infer, repeat: Fast scene\nunderstanding with generative models."" Advances in Neural Information\nProcessing Systems. 2016.\n""""""\n\nimport os\n\nimport numpy as np\nfrom PIL import Image\n\nfrom pyro.contrib.examples.util import get_data_loader\n\n\ndef imresize(arr, size):\n    return np.array(Image.fromarray(arr).resize(size))\n\n\ndef sample_one(canvas_size, mnist):\n    i = np.random.randint(mnist[\'digits\'].shape[0])\n    digit = mnist[\'digits\'][i]\n    label = mnist[\'labels\'][i].item()\n    scale = 0.1 * np.random.randn() + 1.3\n    new_size = tuple(int(s / scale) for s in digit.shape)\n    resized = imresize(digit, new_size)\n    w = resized.shape[0]\n    assert w == resized.shape[1]\n    padding = canvas_size - w\n    pad_l = np.random.randint(0, padding)\n    pad_r = np.random.randint(0, padding)\n    pad_width = ((pad_l, padding - pad_l), (pad_r, padding - pad_r))\n    positioned = np.pad(resized, pad_width, \'constant\', constant_values=0)\n    return positioned, label\n\n\ndef sample_multi(num_digits, canvas_size, mnist):\n    canvas = np.zeros((canvas_size, canvas_size))\n    labels = []\n    for _ in range(num_digits):\n        positioned_digit, label = sample_one(canvas_size, mnist)\n        canvas += positioned_digit\n        labels.append(label)\n    # Crude check for overlapping digits.\n    if np.max(canvas) > 255:\n        return sample_multi(num_digits, canvas_size, mnist)\n    else:\n        return canvas, labels\n\n\ndef mk_dataset(n, mnist, max_digits, canvas_size):\n    x = []\n    y = []\n    for _ in range(n):\n        num_digits = np.random.randint(max_digits + 1)\n        canvas, labels = sample_multi(num_digits, canvas_size, mnist)\n        x.append(canvas)\n        y.append(labels)\n    return np.array(x, dtype=np.uint8), y\n\n\ndef load_mnist(root_path):\n    loader = get_data_loader(\'MNIST\', root_path)\n    return {\n        \'digits\': loader.dataset.data.cpu().numpy(),\n        \'labels\': loader.dataset.targets\n    }\n\n\ndef load(root_path):\n    file_path = os.path.join(root_path, \'multi_mnist_uint8.npz\')\n    if os.path.exists(file_path):\n        data = np.load(file_path, allow_pickle=True)\n        return data[\'x\'], data[\'y\']\n    else:\n        # Set RNG to known state.\n        rng_state = np.random.get_state()\n        np.random.seed(681307)\n        mnist = load_mnist(root_path)\n        print(\'Generating multi-MNIST dataset...\')\n        x, y = mk_dataset(60000, mnist, 2, 50)\n        # Revert RNG state.\n        np.random.set_state(rng_state)\n        # Crude checksum.\n        # assert x.sum() == 883114919, \'Did not generate the expected data.\'\n        with open(file_path, \'wb\') as f:\n            np.savez_compressed(f, x=x, y=y)\n        print(\'Done!\')\n        return x, y\n'"
pyro/contrib/examples/util.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport sys\n\nimport torchvision\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nfrom pyro.distributions.torch_patch import patch_dependency\n\n\n@patch_dependency(\'torchvision.datasets.MNIST\', torchvision)\nclass _MNIST(getattr(MNIST, \'_pyro_unpatched\', MNIST)):\n    # For older torchvision.\n    urls = [\n        ""https://d2hg8soec8ck9v.cloudfront.net/datasets/mnist/train-images-idx3-ubyte.gz"",\n        ""https://d2hg8soec8ck9v.cloudfront.net/datasets/mnist/train-labels-idx1-ubyte.gz"",\n        ""https://d2hg8soec8ck9v.cloudfront.net/datasets/mnist/t10k-images-idx3-ubyte.gz"",\n        ""https://d2hg8soec8ck9v.cloudfront.net/datasets/mnist/t10k-labels-idx1-ubyte.gz"",\n    ]\n    # For newer torchvision.\n    resources = list(zip(urls, [\n        ""f68b3c2dcbeaaa9fbdd348bbdeb94873"",\n        ""d53e105ee54ea40749a09fcbcd1e9432"",\n        ""9fb629c4189551a2d022fa330f9573f3"",\n        ""ec29112dd5afa0611ce80d1b7f02629c""\n    ]))\n\n\ndef get_data_loader(dataset_name,\n                    data_dir,\n                    batch_size=1,\n                    dataset_transforms=None,\n                    is_training_set=True,\n                    shuffle=True):\n    if not dataset_transforms:\n        dataset_transforms = []\n    trans = transforms.Compose([transforms.ToTensor()] + dataset_transforms)\n    dataset = getattr(datasets, dataset_name)\n    print(""downloading data"")\n    dset = dataset(root=data_dir,\n                   train=is_training_set,\n                   transform=trans,\n                   download=True)\n    print(""download complete."")\n    return DataLoader(\n        dset,\n        batch_size=batch_size,\n        shuffle=shuffle\n    )\n\n\ndef print_and_log(logger, msg):\n    # print and log a message (if a logger is present)\n    print(msg)\n    sys.stdout.flush()\n    if logger is not None:\n        logger.write(""{}\\n"".format(msg))\n        logger.flush()\n\n\ndef get_data_directory(filepath=None):\n    if \'CI\' in os.environ:\n        return os.path.expanduser(\'~/.data\')\n    return os.path.abspath(os.path.join(os.path.dirname(filepath),\n                                        \'.data\'))\n'"
pyro/contrib/forecast/__init__.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .evaluate import backtest, eval_crps, eval_mae, eval_rmse\nfrom .forecaster import Forecaster, ForecastingModel, HMCForecaster\n\n__all__ = [\n    ""Forecaster"",\n    ""ForecastingModel"",\n    ""HMCForecaster"",\n    ""backtest"",\n    ""eval_crps"",\n    ""eval_mae"",\n    ""eval_rmse"",\n]\n'"
pyro/contrib/forecast/evaluate.py,12,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport warnings\nfrom timeit import default_timer\n\nimport torch\n\nimport pyro\nfrom pyro.ops.stats import crps_empirical\n\nfrom .forecaster import Forecaster\n\nlogger = logging.getLogger(__name__)\n\n\n@torch.no_grad()\ndef eval_mae(pred, truth):\n    """"""\n    Evaluate mean absolute error, using sample median as point estimate.\n\n    :param torch.Tensor pred: Forecasted samples.\n    :param torch.Tensor truth: Ground truth.\n    :rtype: float\n    """"""\n    pred = pred.median(0).values\n    return (pred - truth).abs().mean().cpu().item()\n\n\n@torch.no_grad()\ndef eval_rmse(pred, truth):\n    """"""\n    Evaluate root mean squared error, using sample mean as point estimate.\n\n    :param torch.Tensor pred: Forecasted samples.\n    :param torch.Tensor truth: Ground truth.\n    :rtype: float\n    """"""\n    pred = pred.mean(0)\n    error = pred - truth\n    return (error * error).mean().cpu().item() ** 0.5\n\n\n@torch.no_grad()\ndef eval_crps(pred, truth):\n    """"""\n    Evaluate continuous ranked probability score, averaged over all data\n    elements.\n\n    **References**\n\n    [1] Tilmann Gneiting, Adrian E. Raftery (2007)\n        `Strictly Proper Scoring Rules, Prediction, and Estimation`\n        https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf\n\n    :param torch.Tensor pred: Forecasted samples.\n    :param torch.Tensor truth: Ground truth.\n    :rtype: float\n    """"""\n    return crps_empirical(pred, truth).mean().cpu().item()\n\n\nDEFAULT_METRICS = {\n    ""mae"": eval_mae,\n    ""rmse"": eval_rmse,\n    ""crps"": eval_crps,\n}\n\n\ndef backtest(data, covariates, model_fn, *,\n             forecaster_fn=Forecaster,\n             metrics=None,\n             transform=None,\n             train_window=None,\n             min_train_window=1,\n             test_window=None,\n             min_test_window=1,\n             stride=1,\n             seed=1234567890,\n             num_samples=100,\n             batch_size=None,\n             forecaster_options={}):\n    """"""\n    Backtest a forecasting model on a moving window of (train,test) data.\n\n    :param data: A tensor dataset with time dimension -2.\n    :type data: ~torch.Tensor\n    :param covariates: A tensor of covariates with time dimension -2.\n        For models not using covariates, pass a shaped empty tensor\n        ``torch.empty(duration, 0)``.\n    :type covariates: ~torch.Tensor\n    :param callable model_fn: Function that returns an\n        :class:`~pyro.contrib.forecast.forecaster.ForecastingModel` object.\n    :param callable forecaster_fn: Function that returns a forecaster object\n        (for example, :class:`~pyro.contrib.forecast.forecaster.Forecaster`\n        or :class:`~pyro.contrib.forecast.forecaster.HMCForecaster`)\n        given arguments model, training data, training covariates and\n        keyword arguments defined in `forecaster_options`.\n    :param dict metrics: A dictionary mapping metric name to metric function.\n        The metric function should input a forecast ``pred`` and ground\n        ``truth`` and can output anything, often a number. Example metrics\n        include: :func:`eval_mae`, :func:`eval_rmse`, and :func:`eval_crps`.\n    :param callable transform: An optional transform to apply before computing\n        metrics. If provided this will be applied as\n        ``pred, truth = transform(pred, truth)``.\n    :param int train_window: Size of the training window. Be default trains\n        from beginning of data. This must be None if forecaster is\n        :class:`~pyro.contrib.forecast.forecaster.Forecaster` and\n        ``forecaster_options[""warm_start""]`` is true.\n    :param int min_train_window: If ``train_window`` is None, this specifies\n        the min training window size. Defaults to 1.\n    :param int test_window: Size of the test window. By default forecasts to\n        end of data.\n    :param int min_test_window: If ``test_window`` is None, this specifies\n        the min test window size. Defaults to 1.\n    :param int stride: Optional stride for test/train split. Defaults to 1.\n    :param int seed: Random number seed.\n    :param int num_samples: Number of samples for forecast. Defaults to 100.\n    :param int batch_size: Batch size for forecast sampling. Defaults to\n        ``num_samples``.\n    :param forecaster_options: Options dict to pass to forecaster, or callable\n        inputting time window ``t0,t1,t2`` and returning such a dict. See\n        :class:`~pyro.contrib.forecaster.Forecaster` for details.\n    :type forecaster_options: dict or callable\n\n    :returns: A list of dictionaries of evaluation data. Caller is responsible\n        for aggregating the per-window metrics. Dictionary keys include: train\n        begin time ""t0"", train/test split time ""t1"", test end  time ""t2"",\n        ""seed"", ""num_samples"", ""train_walltime"", ""test_walltime"", and one key\n        for each metric.\n    :rtype: list\n    """"""\n    assert data.size(-2) == covariates.size(-2)\n    assert isinstance(min_train_window, int) and min_train_window >= 1\n    assert isinstance(min_test_window, int) and min_test_window >= 1\n    if metrics is None:\n        metrics = DEFAULT_METRICS\n    assert metrics, ""no metrics specified""\n\n    if callable(forecaster_options):\n        forecaster_options_fn = forecaster_options\n    else:\n        def forecaster_options_fn(*args, **kwargs):\n            return forecaster_options\n    if train_window is not None and forecaster_options_fn().get(""warm_start""):\n        raise ValueError(""Cannot warm start with moving training window; ""\n                         ""either set warm_start=False or train_window=None"")\n\n    duration = data.size(-2)\n    if test_window is None:\n        stop = duration - min_test_window + 1\n    else:\n        stop = duration - test_window + 1\n    if train_window is None:\n        start = min_train_window\n    else:\n        start = train_window\n\n    pyro.clear_param_store()\n    results = []\n    for t1 in range(start, stop, stride):\n        t0 = 0 if train_window is None else t1 - train_window\n        t2 = duration if test_window is None else t1 + test_window\n        assert 0 <= t0 < t1 < t2 <= duration\n        logger.info(""Training on window [{t0}:{t1}], testing on window [{t1}:{t2}]""\n                    .format(t0=t0, t1=t1, t2=t2))\n\n        # Train a forecaster on the training window.\n        pyro.set_rng_seed(seed)\n        forecaster_options = forecaster_options_fn(t0=t0, t1=t1, t2=t2)\n        if not forecaster_options.get(""warm_start""):\n            pyro.clear_param_store()\n        train_data = data[..., t0:t1, :]\n        train_covariates = covariates[..., t0:t1, :]\n        start_time = default_timer()\n        model = model_fn()\n        forecaster = forecaster_fn(model, train_data, train_covariates,\n                                   **forecaster_options)\n        train_walltime = default_timer() - start_time\n\n        # Forecast forward to testing window.\n        test_covariates = covariates[..., t0:t2, :]\n        start_time = default_timer()\n        # Gradually reduce batch_size to avoid OOM errors.\n        while True:\n            try:\n                pred = forecaster(train_data, test_covariates, num_samples=num_samples,\n                                  batch_size=batch_size)\n                break\n            except RuntimeError as e:\n                if ""out of memory"" in str(e) and batch_size > 1:\n                    batch_size = (batch_size + 1) // 2\n                    warnings.warn(""out of memory, decreasing batch_size to {}""\n                                  .format(batch_size), RuntimeWarning)\n                else:\n                    raise\n        test_walltime = default_timer() - start_time\n        truth = data[..., t1:t2, :]\n\n        # We aggressively garbage collect because Monte Carlo forecast are memory intensive.\n        del forecaster\n\n        # Evaluate the forecasts.\n        if transform is not None:\n            pred, truth = transform(pred, truth)\n        result = {\n            ""t0"": t0,\n            ""t1"": t1,\n            ""t2"": t2,\n            ""seed"": seed,\n            ""num_samples"": num_samples,\n            ""train_walltime"": train_walltime,\n            ""test_walltime"": test_walltime,\n            ""params"": {},\n        }\n        results.append(result)\n        for name, fn in metrics.items():\n            result[name] = fn(pred, truth)\n        for name, value in pyro.get_param_store().items():\n            if value.numel() == 1:\n                value = value.cpu().item()\n                result[""params""][name] = value\n        for dct in (result, result[""params""]):\n            for key, value in sorted(dct.items()):\n                if isinstance(value, (int, float)):\n                    logger.debug(""{} = {:0.6g}"".format(key, value))\n\n        del pred\n\n    return results\n'"
pyro/contrib/forecast/forecaster.py,25,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nfrom abc import ABCMeta, abstractmethod\n\nimport torch\nimport torch.nn as nn\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer import MCMC, NUTS, SVI, Trace_ELBO\nfrom pyro.infer.autoguide import AutoNormal, init_to_sample\nfrom pyro.infer.predictive import _guess_max_plate_nesting\nfrom pyro.nn.module import PyroModule\nfrom pyro.optim import DCTAdam\n\nfrom .util import (MarkDCTParamMessenger, PrefixConditionMessenger, PrefixReplayMessenger, PrefixWarmStartMessenger,\n                   reshape_batch)\n\nlogger = logging.getLogger(__name__)\n\n\nclass _ForecastingModelMeta(type(PyroModule), ABCMeta):\n    pass\n\n\nclass ForecastingModel(PyroModule, metaclass=_ForecastingModelMeta):\n    """"""\n    Abstract base class for forecasting models.\n\n    Derived classes must implement the :meth:`model` method.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self._prefix_condition_data = {}\n\n    @abstractmethod\n    def model(self, zero_data, covariates):\n        """"""\n        Generative model definition.\n\n        Implementations must call the :meth:`predict` method exactly once.\n\n        Implementations must draw all time-dependent noise inside the\n        :meth:`time_plate`. The prediction passed to :meth:`predict` must be a\n        deterministic function of noise tensors that are independent over time.\n        This requirement is slightly more general than state space models.\n\n        :param zero_data: A zero tensor like the input data, but extended to\n            the duration of the :meth:`time_plate`. This allows models to\n            depend on the shape and device of data but not its value.\n        :type zero_data: ~torch.Tensor\n        :param covariates: A tensor of covariates with time dimension -2.\n        :type covariates: ~torch.Tensor\n        :returns: Return value is ignored.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def time_plate(self):\n        """"""\n        :returns: A plate named ""time"" with size ``covariates.size(-2)`` and\n            ``dim=-1``. This is available only during model execution.\n        :rtype: :class:`~pyro.plate`\n        """"""\n        assert self._time_plate is not None, "".time_plate accessed outside of .model()""\n        return self._time_plate\n\n    def predict(self, noise_dist, prediction):\n        """"""\n        Prediction function, to be called by :meth:`model` implementations.\n\n        This should be called outside of the  :meth:`time_plate`.\n\n        This is similar to an observe statement in Pyro::\n\n            pyro.sample(""residual"", noise_dist,\n                        obs=(data - prediction))\n\n        but with (1) additional reshaping logic to allow time-dependent\n        ``noise_dist`` (most often a :class:`~pyro.distributions.GaussianHMM`\n        or variant); and (2) additional logic to allow only a partial\n        observation and forecast the remaining data.\n\n        :param noise_dist: A noise distribution with ``.event_dim in {0,1,2}``.\n            ``noise_dist`` is typically zero-mean or zero-median or zero-mode\n            or somehow centered.\n        :type noise_dist: ~pyro.distributions.Distribution\n        :param prediction: A prediction for the data. This should have the same\n            shape as ``data``, but broadcastable to full duration of the\n            ``covariates``.\n        :type prediction: ~torch.Tensor\n        """"""\n        assert self._data is not None, "".predict() called outside .model()""\n        assert self._forecast is None, "".predict() called twice""\n        assert isinstance(noise_dist, dist.Distribution)\n        assert isinstance(prediction, torch.Tensor)\n        if noise_dist.event_dim == 0:\n            if noise_dist.batch_shape[-2:] != prediction.shape[-2:]:\n                noise_dist = noise_dist.expand(\n                    noise_dist.batch_shape[:-2] + prediction.shape[-2:])\n            noise_dist = noise_dist.to_event(2)\n        elif noise_dist.event_dim == 1:\n            if noise_dist.batch_shape[-1:] != prediction.shape[-2:-1]:\n                noise_dist = noise_dist.expand(\n                    noise_dist.batch_shape[:-1] + prediction.shape[-2:-1])\n            noise_dist = noise_dist.to_event(1)\n        assert noise_dist.event_dim == 2\n        assert noise_dist.event_shape == prediction.shape[-2:]\n\n        # The following reshaping logic is required to reconcile batch and\n        # event shapes. This would be unnecessary if Pyro used name dimensions\n        # internally, e.g. using Funsor.\n        #\n        #     batch_shape                    | event_shape\n        #     -------------------------------+----------------\n        #  1. sample_shape + shape + (time,) | (obs_dim,)\n        #  2.           sample_shape + shape | (time, obs_dim)\n        #  3.    sample_shape + shape + (1,) | (time, obs_dim)\n        #\n        # Parameters like noise_dist.loc typically have shape as in 1.  However\n        # calling .to_event(1) will shift the shapes resulting in 2., where\n        # sample_shape+shape will be misaligned with other batch shapes in the\n        # trace. To fix this the following logic ""unsqueezes"" the distribution,\n        # resulting in correctly aligned shapes 3. Note the ""time"" dimension is\n        # effectively moved from a batch dimension to an event dimension.\n        noise_dist = reshape_batch(noise_dist, noise_dist.batch_shape + (1,))\n        data = pyro.subsample(self._data.unsqueeze(-3), event_dim=2)\n        prediction = prediction.unsqueeze(-3)\n\n        # Create a sample site.\n        t_obs = data.size(-2)\n        t_cov = prediction.size(-2)\n        if t_obs == t_cov:  # training\n            pyro.sample(""residual"", noise_dist, obs=data - prediction)\n            self._forecast = data.new_zeros(data.shape[:-2] + (0,) + data.shape[-1:])\n        else:  # forecasting\n            left_pred = prediction[..., :t_obs, :]\n            right_pred = prediction[..., t_obs:, :]\n\n            # This prefix_condition indirection is needed to ensure that\n            # PrefixConditionMessenger is handled outside of the .model() call.\n            self._prefix_condition_data[""residual""] = data - left_pred\n            noise = pyro.sample(""residual"", noise_dist)\n            del self._prefix_condition_data[""residual""]\n\n            assert noise.shape[-data.dim():] == right_pred.shape[-data.dim():]\n            self._forecast = right_pred + noise\n\n        # Move the ""time"" batch dim back to its original place.\n        assert self._forecast.size(-3) == 1\n        self._forecast = self._forecast.squeeze(-3)\n\n    def forward(self, data, covariates):\n        assert data.dim() >= 2\n        assert covariates.dim() >= 2\n        t_obs = data.size(-2)\n        t_cov = covariates.size(-2)\n        assert t_obs <= t_cov\n\n        try:\n            self._data = data\n            self._time_plate = pyro.plate(""time"", t_cov, dim=-1)\n            if t_obs == t_cov:  # training\n                zero_data = data.new_zeros(()).expand(data.shape)\n            else:  # forecasting\n                zero_data = data.new_zeros(()).expand(\n                    data.shape[:-2] + covariates.shape[-2:-1] + data.shape[-1:])\n            self._forecast = None\n\n            self.model(zero_data, covariates)\n\n            assert self._forecast is not None, "".predict() was not called by .model()""\n            return self._forecast\n        finally:\n            self._data = None\n            self._time_plate = None\n            self._forecast = None\n\n\nclass Forecaster(nn.Module):\n    """"""\n    Forecaster for a :class:`ForecastingModel` using variational inference.\n\n    On initialization, this fits a distribution using variational inference\n    over latent variables and exact inference over the noise distribution,\n    typically a :class:`~pyro.distributions.GaussianHMM` or variant.\n\n    After construction this can be called to generate sample forecasts.\n\n    :ivar list losses: A list of losses recorded during training, typically\n        used to debug convergence. Defined by ``loss = -elbo / data.numel()``.\n\n    :param ForecastingModel model: A forecasting model subclass instance.\n    :param data: A tensor dataset with time dimension -2.\n    :type data: ~torch.Tensor\n    :param covariates: A tensor of covariates with time dimension -2.\n        For models not using covariates, pass a shaped empty tensor\n        ``torch.empty(duration, 0)``.\n    :type covariates: ~torch.Tensor\n\n    :param guide: Optional guide instance. Defaults to a\n        :class:`~pyro.infer.autoguide.AutoNormal`.\n    :type guide: ~pyro.nn.module.PyroModule\n    :param callable init_loc_fn: A per-site initialization function for the\n        :class:`~pyro.infer.autoguide.AutoNormal` guide. Defaults to\n        :func:`~pyro.infer.autoguide.initialization.init_to_sample`. See\n        :ref:`autoguide-initialization` section for available functions.\n    :param float init_scale: Initial uncertainty scale of the\n        :class:`~pyro.infer.autoguide.AutoNormal` guide.\n    :param callable create_plates: An optional function to create plates for\n        subsampling with the :class:`~pyro.infer.autoguide.AutoNormal` guide.\n    :param optim: An optional Pyro optimizer. Defaults to a freshly constructed\n        :class:`~pyro.optim.optim.DCTAdam`.\n    :type optim: ~pyro.optim.optim.PyroOptim\n    :param float learning_rate: Learning rate used by\n        :class:`~pyro.optim.optim.DCTAdam`.\n    :param tuple betas: Coefficients for running averages used by\n        :class:`~pyro.optim.optim.DCTAdam`.\n    :param float learning_rate_decay: Learning rate decay used by\n        :class:`~pyro.optim.optim.DCTAdam`. Note this is the total decay\n        over all ``num_steps``, not the per-step decay factor.\n    :param float clip_norm: Norm used for gradient clipping during\n        optimization. Defaults to 10.0.\n    :param bool dct_gradients: Whether to discrete cosine transform gradients\n        in :class:`~pyro.optim.optim.DCTAdam`. Defaults to False.\n    :param bool subsample_aware: whether to update gradient statistics only\n        for those elements that appear in a subsample. This is used\n        by :class:`~pyro.optim.optim.DCTAdam`.\n    :param int num_steps: Number of :class:`~pyro.infer.svi.SVI` steps.\n    :param int num_particles: Number of particles used to compute the\n        :class:`~pyro.infer.elbo.ELBO`.\n    :param bool vectorize_particles: If ``num_particles > 1``, determines\n        whether to vectorize computation of the :class:`~pyro.infer.elbo.ELBO`.\n        Defaults to True. Set to False for models with dynamic control flow.\n    :param bool warm_start: Whether to warm start parameters from a smaller\n        time window. Note this may introduce statistical leakage; usage is\n        recommended for model exploration purposes only and should be disabled\n        when publishing metrics.\n    :param int log_every: Number of training steps between logging messages.\n    """"""\n    def __init__(self, model, data, covariates, *,\n                 guide=None,\n                 init_loc_fn=init_to_sample,\n                 init_scale=0.1,\n                 create_plates=None,\n                 optim=None,\n                 learning_rate=0.01,\n                 betas=(0.9, 0.99),\n                 learning_rate_decay=0.1,\n                 clip_norm=10.0,\n                 dct_gradients=False,\n                 subsample_aware=False,\n                 num_steps=1001,\n                 num_particles=1,\n                 vectorize_particles=True,\n                 warm_start=False,\n                 log_every=100):\n        assert data.size(-2) == covariates.size(-2)\n        super().__init__()\n        self.model = model\n        if guide is None:\n            guide = AutoNormal(self.model, init_loc_fn=init_loc_fn, init_scale=init_scale,\n                               create_plates=create_plates)\n        self.guide = guide\n\n        # Initialize.\n        if warm_start:\n            model = PrefixWarmStartMessenger()(model)\n            guide = PrefixWarmStartMessenger()(guide)\n        if dct_gradients:\n            model = MarkDCTParamMessenger(""time"")(model)\n            guide = MarkDCTParamMessenger(""time"")(guide)\n        elbo = Trace_ELBO(num_particles=num_particles,\n                          vectorize_particles=vectorize_particles)\n        elbo._guess_max_plate_nesting(model, guide, (data, covariates), {})\n        elbo.max_plate_nesting = max(elbo.max_plate_nesting, 1)  # force a time plate\n\n        losses = []\n        if num_steps:\n            if optim is None:\n                optim = DCTAdam({""lr"": learning_rate, ""betas"": betas,\n                                 ""lrd"": learning_rate_decay ** (1 / num_steps),\n                                 ""clip_norm"": clip_norm,\n                                 ""subsample_aware"": subsample_aware})\n            svi = SVI(self.model, self.guide, optim, elbo)\n            for step in range(num_steps):\n                loss = svi.step(data, covariates) / data.numel()\n                if log_every and step % log_every == 0:\n                    logger.info(""step {: >4d} loss = {:0.6g}"".format(step, loss))\n                losses.append(loss)\n\n        self.guide.create_plates = None  # Disable subsampling after training.\n        self.max_plate_nesting = elbo.max_plate_nesting\n        self.losses = losses\n\n    def __call__(self, data, covariates, num_samples, batch_size=None):\n        """"""\n        Samples forecasted values of data for time steps in ``[t1,t2)``, where\n        ``t1 = data.size(-2)`` is the duration of observed data and ``t2 =\n        covariates.size(-2)`` is the extended duration of covariates. For\n        example to forecast 7 days forward conditioned on 30 days of\n        observations, set ``t1=30`` and ``t2=37``.\n\n        :param data: A tensor dataset with time dimension -2.\n        :type data: ~torch.Tensor\n        :param covariates: A tensor of covariates with time dimension -2.\n            For models not using covariates, pass a shaped empty tensor\n            ``torch.empty(duration, 0)``.\n        :type covariates: ~torch.Tensor\n        :param int num_samples: The number of samples to generate.\n        :param int batch_size: Optional batch size for sampling. This is useful\n            for generating many samples from models with large memory\n            footprint. Defaults to ``num_samples``.\n        :returns: A batch of joint posterior samples of shape\n            ``(num_samples,1,...,1) + data.shape[:-2] + (t2-t1,data.size(-1))``,\n            where the ``1``\'s are inserted to avoid conflict with model plates.\n        :rtype: ~torch.Tensor\n        """"""\n        return super().__call__(data, covariates, num_samples, batch_size)\n\n    def forward(self, data, covariates, num_samples, batch_size=None):\n        assert data.size(-2) < covariates.size(-2)\n        assert isinstance(num_samples, int) and num_samples > 0\n        if batch_size is not None:\n            batches = []\n            while num_samples > 0:\n                batch = self.forward(data, covariates, min(num_samples, batch_size))\n                batches.append(batch)\n                num_samples -= batch_size\n            return torch.cat(batches)\n\n        assert self.max_plate_nesting >= 1\n        dim = -1 - self.max_plate_nesting\n\n        with torch.no_grad():\n            with poutine.trace() as tr:\n                with pyro.plate(""particles"", num_samples, dim=dim):\n                    self.guide(data, covariates)\n            with PrefixReplayMessenger(tr.trace):\n                with PrefixConditionMessenger(self.model._prefix_condition_data):\n                    with pyro.plate(""particles"", num_samples, dim=dim):\n                        return self.model(data, covariates)\n\n\nclass HMCForecaster(nn.Module):\n    """"""\n    Forecaster for a :class:`ForecastingModel` using Hamiltonian Monte Carlo.\n\n    On initialization, this will run :class:`~pyro.infer.mcmc.nuts.NUTS`\n    sampler to get posterior samples of the model.\n\n    After construction, this can be called to generate sample forecasts.\n\n    :param ForecastingModel model: A forecasting model subclass instance.\n    :param data: A tensor dataset with time dimension -2.\n    :type data: ~torch.Tensor\n    :param covariates: A tensor of covariates with time dimension -2.\n        For models not using covariates, pass a shaped empty tensor\n        ``torch.empty(duration, 0)``.\n    :type covariates: ~torch.Tensor\n\n    :param int num_warmup: number of MCMC warmup steps.\n    :param int num_samples: number of MCMC samples.\n    :param int num_chains: number of parallel MCMC chains.\n    :param bool dense_mass: a flag to control whether the mass matrix is dense\n        or diagonal. Defaults to False.\n    :param bool jit_compile: whether to use the PyTorch JIT to trace the log\n        density computation, and use this optimized executable trace in the\n        integrator. Defaults to False.\n    :param int max_tree_depth: Max depth of the binary tree created during the\n        doubling scheme of the :class:`~pyro.infer.mcmc.nuts.NUTS` sampler.\n        Defaults to 10.\n    """"""\n    def __init__(self, model, data, covariates=None, *,\n                 num_warmup=1000, num_samples=1000, num_chains=1,\n                 dense_mass=False, jit_compile=False, max_tree_depth=10):\n        assert data.size(-2) == covariates.size(-2)\n        super().__init__()\n        self.model = model\n        max_plate_nesting = _guess_max_plate_nesting(model, (data, covariates), {})\n        self.max_plate_nesting = max(max_plate_nesting, 1)  # force a time plate\n\n        kernel = NUTS(model, full_mass=dense_mass, jit_compile=jit_compile, ignore_jit_warnings=True,\n                      max_tree_depth=max_tree_depth, max_plate_nesting=max_plate_nesting)\n        mcmc = MCMC(kernel, warmup_steps=num_warmup, num_samples=num_samples, num_chains=num_chains)\n        mcmc.run(data, covariates)\n        # conditions to compute rhat\n        if (num_chains == 1 and num_samples >= 4) or (num_chains > 1 and num_samples >= 2):\n            mcmc.summary()\n\n        # inspect the model with particles plate = 1, so that we can reshape samples to\n        # add any missing plate dim in front.\n        with poutine.trace() as tr:\n            with pyro.plate(""particles"", 1, dim=-self.max_plate_nesting - 1):\n                model(data, covariates)\n\n        self._trace = tr.trace\n        self._samples = mcmc.get_samples()\n        self._num_samples = num_samples * num_chains\n        for name, node in list(self._trace.nodes.items()):\n            if name not in self._samples:\n                del self._trace.nodes[name]\n\n    def __call__(self, data, covariates, num_samples, batch_size=None):\n        """"""\n        Samples forecasted values of data for time steps in ``[t1,t2)``, where\n        ``t1 = data.size(-2)`` is the duration of observed data and ``t2 =\n        covariates.size(-2)`` is the extended duration of covariates. For\n        example to forecast 7 days forward conditioned on 30 days of\n        observations, set ``t1=30`` and ``t2=37``.\n\n        :param data: A tensor dataset with time dimension -2.\n        :type data: ~torch.Tensor\n        :param covariates: A tensor of covariates with time dimension -2.\n            For models not using covariates, pass a shaped empty tensor\n            ``torch.empty(duration, 0)``.\n        :type covariates: ~torch.Tensor\n        :param int num_samples: The number of samples to generate.\n        :param int batch_size: Optional batch size for sampling. This is useful\n            for generating many samples from models with large memory\n            footprint. Defaults to ``num_samples``.\n        :returns: A batch of joint posterior samples of shape\n            ``(num_samples,1,...,1) + data.shape[:-2] + (t2-t1,data.size(-1))``,\n            where the ``1``\'s are inserted to avoid conflict with model plates.\n        :rtype: ~torch.Tensor\n        """"""\n        return super().__call__(data, covariates, num_samples, batch_size)\n\n    def forward(self, data, covariates, num_samples, batch_size=None):\n        assert data.size(-2) < covariates.size(-2)\n        assert isinstance(num_samples, int) and num_samples > 0\n        if batch_size is not None:\n            batches = []\n            while num_samples > 0:\n                batch = self.forward(data, covariates, min(num_samples, batch_size))\n                batches.append(batch)\n                num_samples -= batch_size\n            return torch.cat(batches)\n\n        assert self.max_plate_nesting >= 1\n        dim = -1 - self.max_plate_nesting\n\n        with torch.no_grad():\n            weights = torch.ones(self._num_samples, device=data.device)\n            indices = torch.multinomial(weights, num_samples, replacement=num_samples > self._num_samples)\n            for name, node in list(self._trace.nodes.items()):\n                sample = self._samples[name].index_select(0, indices)\n                node[\'value\'] = sample.reshape(\n                    (num_samples,) + (1,) * (node[\'value\'].dim() - sample.dim()) + sample.shape[1:])\n\n            with PrefixReplayMessenger(self._trace):\n                with PrefixConditionMessenger(self.model._prefix_condition_data):\n                    with pyro.plate(""particles"", num_samples, dim=dim):\n                        return self.model(data, covariates)\n'"
pyro/contrib/forecast/util.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import singledispatch\n\nimport torch\nfrom torch.distributions import transform_to, transforms\n\nimport pyro.distributions as dist\nfrom pyro.poutine.messenger import Messenger\nfrom pyro.poutine.util import site_is_subsample\nfrom pyro.primitives import get_param_store\n\n\nclass MarkDCTParamMessenger(Messenger):\n    """"""\n    EXPERIMENTAL Messenger to mark DCT dimension of parameter, for use with\n    :class:`pyro.optim.optim.DCTAdam`.\n\n    :param str name: The name of the plate along which to apply discrete cosine\n        transforms on gradients.\n    """"""\n    def __init__(self, name):\n        super().__init__()\n        self.name = name\n\n    def _postprocess_message(self, msg):\n        if msg[""type""] != ""param"":\n            return\n        event_dim = msg[""kwargs""].get(""event_dim"")\n        if event_dim is None:\n            return\n        for frame in msg[""cond_indep_stack""]:\n            if frame.name == self.name:\n                value = msg[""value""]\n                event_dim += value.unconstrained().dim() - value.dim()\n                value.unconstrained()._pyro_dct_dim = frame.dim - event_dim\n                return\n\n\nclass PrefixWarmStartMessenger(Messenger):\n    """"""\n    EXPERIMENTAL Assuming the global param store has been populated with params\n    defined on a short time window, re-initialize by splicing old params with\n    new initial params defined on a longer time window.\n    """"""\n    def _pyro_param(self, msg):\n        store = get_param_store()\n        name = msg[""name""]\n        if name not in store:\n            return\n\n        if len(msg[""args""]) >= 2:\n            new = msg[""args""][1]\n        elif ""init_tensor"" in msg[""kwargs""]:\n            new = msg[""kwargs""][""init_tensor""]\n        else:\n            return  # no init tensor specified\n\n        if callable(new):\n            new = new()\n        old = store[name]\n        assert new.dim() == old.dim()\n        if new.shape == old.shape:\n            return\n\n        # Splice old (warm start) and new (init) tensors.\n        # This only works for time-homogeneous constraints.\n        t = transform_to(store._constraints[name])\n        new = t.inv(new)\n        old = t.inv(old)\n        for dim in range(new.dim()):\n            if new.size(dim) != old.size(dim):\n                break\n        assert new.size(dim) > old.size(dim)\n        assert new.shape[dim + 1:] == old.shape[dim + 1:]\n        split = old.size(dim)\n        index = (slice(None),) * dim + (slice(split, None),)\n        new = torch.cat([old, new[index]], dim=dim)\n        store[name] = t(new)\n\n\nclass PrefixReplayMessenger(Messenger):\n    """"""\n    EXPERIMENTAL Given a trace of training data, replay a model with batched\n    sites extended to include both training and forecast time, using the guide\n    trace for the training prefix and samples from the prior on the forecast\n    postfix.\n\n    :param trace: a guide trace.\n    :type trace: ~pyro.poutine.trace_struct.Trace\n    """"""\n    def __init__(self, trace):\n        super().__init__()\n        self.trace = trace\n\n    def _pyro_post_sample(self, msg):\n        if site_is_subsample(msg):\n            return\n\n        name = msg[""name""]\n        if name not in self.trace:\n            return\n\n        model_value = msg[""value""]\n        guide_value = self.trace.nodes[name][""value""]\n        if model_value.shape == guide_value.shape:\n            msg[""value""] = guide_value\n            return\n\n        # Search for a single dim with mismatched size.\n        assert model_value.dim() == guide_value.dim()\n        for dim in range(model_value.dim()):\n            if model_value.size(dim) != guide_value.size(dim):\n                break\n        assert model_value.size(dim) > guide_value.size(dim)\n        assert model_value.shape[dim + 1:] == guide_value.shape[dim + 1:]\n        split = guide_value.size(dim)\n        index = (slice(None),) * dim + (slice(split, None),)\n        msg[""value""] = torch.cat([guide_value, model_value[index]], dim=dim)\n\n\nclass PrefixConditionMessenger(Messenger):\n    """"""\n    EXPERIMENTAL Given a prefix of t-many observations, condition a (t+f)-long\n    distribution on the observations, converting it to an f-long distribution.\n\n    :param dict data: A dict mapping site name to tensors of observations.\n    """"""\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    def _pyro_sample(self, msg):\n        if msg[""name""] not in self.data:\n            return\n\n        assert msg[""value""] is None\n        data = self.data[msg[""name""]]\n        msg[""fn""] = prefix_condition(msg[""fn""], data)\n\n\n# ----------------------------------------------------------------------------\n# The pattern-matching code in the remainder of this file could be eventually\n# replace by much simpler Funsor logic.\n\nUNIVARIATE_DISTS = {\n    dist.Bernoulli: (""logits"",),\n    dist.Beta: (""concentration1"", ""concentration0""),\n    dist.BetaBinomial: (""concentration1"", ""concentration0""),\n    dist.Cauchy: (""loc"", ""scale""),\n    dist.Dirichlet: (""concentration"",),\n    dist.DirichletMultinomial: (""concentration"",),\n    dist.Exponential: (""rate"",),\n    dist.Gamma: (""concentration"", ""rate""),\n    dist.GammaPoisson: (""concentration"", ""rate""),\n    dist.Geometric: (""logits"",),\n    dist.InverseGamma: (""concentration"", ""rate""),\n    dist.Laplace: (""loc"", ""scale""),\n    dist.LogNormal: (""loc"", ""scale""),\n    dist.NegativeBinomial: (""total_count"", ""logits""),\n    dist.Normal: (""loc"", ""scale""),\n    dist.Poisson: (""rate"",),\n    dist.Stable: (""stability"", ""skew"", ""scale"", ""loc""),\n    dist.StudentT: (""df"", ""loc"", ""scale""),\n    dist.ZeroInflatedPoisson: (""gate"", ""rate""),\n    dist.ZeroInflatedNegativeBinomial: (""gate"", ""total_count"", ""logits""),\n}\n\n\n@singledispatch\ndef prefix_condition(d, data):\n    """"""\n    EXPERIMENTAL Given a distribution ``d`` of shape ``batch_shape + (t+f, d)``\n    and data ``x`` of shape ``batch_shape + (t, d)``, compute a conditional\n    distribution of shape ``batch_shape + (f, d)``. Typically ``t`` is the\n    number of training time steps, ``f`` is the number of forecast time steps,\n    and ``d`` is the data dimension.\n\n    :param d: a distribution with ``len(d.shape()) >= 2``\n    :type d: ~pyro.distributions.Distribution\n    :param data: data of dimension at least 2.\n    :type data: ~torch.Tensor\n    """"""\n    try:\n        return d.prefix_condition(data)\n    except AttributeError:\n        raise NotImplementedError(""prefix_condition() does not suport {}"".format(type(d)))\n\n\n@prefix_condition.register(dist.Independent)\ndef _(d, data):\n    base_dist = prefix_condition(d.base_dist, data)\n    return base_dist.to_event(d.reinterpreted_batch_ndims)\n\n\n@prefix_condition.register(dist.IndependentHMM)\ndef _(d, data):\n    base_data = data.transpose(-1, -2).unsqueeze(-1)\n    base_dist = prefix_condition(d.base_dist, base_data)\n    return dist.IndependentHMM(base_dist)\n\n\n@prefix_condition.register(dist.FoldedDistribution)\ndef _(d, data):\n    base_dist = prefix_condition(d.base_dist, data)\n    return dist.FoldedDistribution(base_dist)\n\n\ndef _prefix_condition_univariate(d, data):\n    t = data.size(-2)\n    params = {name: getattr(d, name)[..., t:, :]\n              for name in UNIVARIATE_DISTS[type(d)]}\n    return type(d)(**params)\n\n\nfor _type in UNIVARIATE_DISTS:\n    prefix_condition.register(_type)(_prefix_condition_univariate)\n\n\n@prefix_condition.register(dist.MultivariateNormal)\ndef _(d, data):\n    t = data.size(-2)\n    loc = d.loc[..., t:, :]\n    scale_tril = d.scale_tril[..., t:, :, :]\n    return dist.MultivariateNormal(loc, scale_tril=scale_tril)\n\n\n@singledispatch\ndef reshape_batch(d, batch_shape):\n    """"""\n    EXPERIMENTAL Given a distribution ``d``, reshape to different batch shape\n    of same number of elements.\n\n    This is typically used to move the the rightmost batch dimension ""time"" to\n    an event dimension, while preserving the positions of other batch\n    dimensions.\n\n    :param d: A distribution.\n    :type d: ~pyro.distributions.Distribution\n    :param tuple batch_shape: A new batch shape.\n    :returns: A distribution with the same type but given batch shape.\n    :rtype: ~pyro.distributions.Distribution\n    """"""\n    raise NotImplementedError(""reshape_batch() does not suport {}"".format(type(d)))\n\n\n@reshape_batch.register(dist.Independent)\ndef _(d, batch_shape):\n    base_shape = batch_shape + d.event_shape[:d.reinterpreted_batch_ndims]\n    base_dist = reshape_batch(d.base_dist, base_shape)\n    return base_dist.to_event(d.reinterpreted_batch_ndims)\n\n\n@reshape_batch.register(dist.IndependentHMM)\ndef _(d, batch_shape):\n    base_shape = batch_shape + d.event_shape[-1:]\n    base_dist = reshape_batch(d.base_dist, base_shape)\n    return dist.IndependentHMM(base_dist)\n\n\n@reshape_batch.register(dist.FoldedDistribution)\ndef _(d, batch_shape):\n    base_dist = reshape_batch(d.base_dist, batch_shape)\n    return dist.FoldedDistribution(base_dist)\n\n\ndef _reshape_batch_univariate(d, batch_shape):\n    batch_shape = batch_shape + (-1,) * d.event_dim\n    params = {name: getattr(d, name).reshape(batch_shape)\n              for name in UNIVARIATE_DISTS[type(d)]}\n    return type(d)(**params)\n\n\nfor _type in UNIVARIATE_DISTS:\n    reshape_batch.register(_type)(_reshape_batch_univariate)\n\n\n@reshape_batch.register(dist.MultivariateNormal)\ndef _(d, batch_shape):\n    dim = d.event_shape[0]\n    loc = d.loc.reshape(batch_shape + (dim,))\n    scale_tril = d.scale_tril.reshape(batch_shape + (dim, dim))\n    return dist.MultivariateNormal(loc, scale_tril=scale_tril)\n\n\n@reshape_batch.register(dist.GaussianHMM)\ndef _(d, batch_shape):\n    init = d._init\n    if init.batch_shape:\n        init = init.expand(d.batch_shape)\n        init = init.reshape(batch_shape)\n\n    trans = d._trans\n    if len(trans.batch_shape) > 1:\n        trans = trans.expand(d.batch_shape + (-1,))\n        trans = trans.reshape(batch_shape + (-1,))\n\n    obs = d._obs\n    if len(obs.batch_shape) > 1:\n        obs = obs.expand(d.batch_shape + (-1,))\n        obs = obs.reshape(batch_shape + (-1,))\n\n    new = d._get_checked_instance(dist.GaussianHMM)\n    new.hidden_dim = d.hidden_dim\n    new.obs_dim = d.obs_dim\n    new._init = init\n    new._trans = trans\n    new._obs = obs\n    super(dist.GaussianHMM, new).__init__(d.duration, batch_shape, d.event_shape,\n                                          validate_args=d._validate_args)\n    return new\n\n\n@reshape_batch.register(dist.LinearHMM)\ndef _(d, batch_shape):\n    init_dist = reshape_batch(d.initial_dist, batch_shape)\n    trans_mat = d.transition_matrix.reshape(batch_shape + (-1, d.hidden_dim, d.hidden_dim))\n    trans_dist = reshape_batch(d.transition_dist, batch_shape + (-1,))\n    obs_mat = d.observation_matrix.reshape(batch_shape + (-1, d.hidden_dim, d.obs_dim))\n    obs_dist = reshape_batch(d.observation_dist, batch_shape + (-1,))\n\n    new = d._get_checked_instance(dist.LinearHMM)\n    new.hidden_dim = d.hidden_dim\n    new.obs_dim = d.obs_dim\n    new.initial_dist = init_dist\n    new.transition_matrix = trans_mat\n    new.transition_dist = trans_dist\n    new.observation_matrix = obs_mat\n    new.observation_dist = obs_dist\n    transforms = []\n    for transform in d.transforms:\n        assert type(transform) in UNIVARIATE_TRANSFORMS, \\\n            ""Currently, reshape_batch only supports AbsTransform, "" + \\\n            ""ExpTransform, SigmoidTransform transform""\n        current_shape = d.observation_dist.shape()\n        new_shape = obs_dist.shape()\n        transforms.append(reshape_transform_batch(transform, current_shape, new_shape))\n    new.transforms = transforms\n    super(dist.LinearHMM, new).__init__(d.duration, batch_shape, d.event_shape,\n                                        validate_args=d._validate_args)\n    return new\n\n\nUNIVARIATE_TRANSFORMS = {\n    transforms.AbsTransform: (),\n    transforms.AffineTransform: (""loc"", ""scale""),\n    transforms.ExpTransform: (),\n    transforms.PowerTransform: (""exponent"",),\n    transforms.SigmoidTransform: (),\n}\n\n\n@singledispatch\ndef reshape_transform_batch(t, current_batch_shape, new_batch_shape):\n    """"""\n    EXPERIMENTAL Given a transform ``t``, reshape to different batch shape\n    of same number of elements.\n\n    This is typically used to correct the transform parameters\' shapes after\n    reshaping the base distribution of a transformed distribution.\n\n    :param t: A transform.\n    :type t: ~torch.distributions.transforms.Transform\n    :param tuple current_batch_shape: The current batch shape.\n    :param tuple new_batch_shape: A new batch shape.\n    :returns: A transform with the same type but given new batch shape.\n    :rtype: ~torch.distributions.transforms.Transform\n    """"""\n    raise NotImplementedError(""reshape_transform_batch() does not suport {}"".format(type(t)))\n\n\ndef _reshape_batch_univariate_transform(t, current_batch_shape, new_batch_shape):\n    params = {name: getattr(t, name).expand(current_batch_shape).reshape(new_batch_shape)\n              for name in UNIVARIATE_TRANSFORMS[type(t)]}\n    params[""cache_size""] = t._cache_size\n    return type(t)(**params)\n\n\nfor _type in UNIVARIATE_TRANSFORMS:\n    reshape_transform_batch.register(_type)(_reshape_batch_univariate_transform)\n'"
pyro/contrib/gp/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.gp import kernels, likelihoods, models, parameterized, util\nfrom pyro.contrib.gp.parameterized import Parameterized\n\n__all__ = [\n    ""Parameterized"",\n    ""kernels"",\n    ""likelihoods"",\n    ""models"",\n    ""parameterized"",\n    ""util"",\n]\n'"
pyro/contrib/gp/parameterized.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\nfrom collections import OrderedDict\nfrom functools import partial\n\nfrom torch.distributions import biject_to, constraints\nfrom torch.nn import Parameter\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions.util import eye_like\nfrom pyro.nn.module import PyroModule, PyroParam, PyroSample, pyro_method\n\n\ndef _is_real_support(support):\n    if isinstance(support, pyro.distributions.constraints.IndependentConstraint):\n        return _is_real_support(support.base_constraint)\n    else:\n        return support in [constraints.real, constraints.real_vector]\n\n\ndef _get_sample_fn(module, name):\n    if module.mode == ""model"":\n        return module._priors[name]\n\n    dist_constructor, dist_args = module._guides[name]\n\n    if dist_constructor is dist.Delta:\n        p_map = getattr(module, ""{}_map"".format(name))\n        return dist.Delta(p_map, event_dim=p_map.dim())\n\n    # create guide\n    dist_args = {arg: getattr(module, ""{}_{}"".format(name, arg)) for arg in dist_args}\n    guide = dist_constructor(**dist_args)\n\n    # no need to do transforms when support is real (for mean field ELBO)\n    support = module._priors[name].support\n    if _is_real_support(support):\n        return guide.to_event()\n\n    # otherwise, we do inference in unconstrained space and transform the value\n    # back to original space\n    # TODO: move this logic to infer.autoguide or somewhere else\n    unconstrained_value = pyro.sample(module._pyro_get_fullname(""{}_latent"".format(name)),\n                                      guide.to_event(),\n                                      infer={""is_auxiliary"": True})\n    transform = biject_to(support)\n    value = transform(unconstrained_value)\n    log_density = transform.inv.log_abs_det_jacobian(value, unconstrained_value)\n    return dist.Delta(value, log_density.sum(), event_dim=value.dim())\n\n\nclass Parameterized(PyroModule):\n    """"""\n    A wrapper of :class:`~pyro.nn.module.PyroModule` whose parameters can be set\n    constraints, set priors.\n\n    By default, when we set a prior to a parameter, an auto Delta guide will be\n    created. We can use the method :meth:`autoguide` to setup other auto guides.\n\n    Example::\n\n        >>> class Linear(Parameterized):\n        ...     def __init__(self, a, b):\n        ...         super().__init__()\n        ...         self.a = Parameter(a)\n        ...         self.b = Parameter(b)\n        ...\n        ...     def forward(self, x):\n        ...         return self.a * x + self.b\n        ...\n        >>> linear = Linear(torch.tensor(1.), torch.tensor(0.))\n        >>> linear.a = PyroParam(torch.tensor(1.), constraints.positive)\n        >>> linear.b = PyroSample(dist.Normal(0, 1))\n        >>> linear.autoguide(""b"", dist.Normal)\n        >>> assert ""a_unconstrained"" in dict(linear.named_parameters())\n        >>> assert ""b_loc"" in dict(linear.named_parameters())\n        >>> assert ""b_scale_unconstrained"" in dict(linear.named_parameters())\n\n    Note that by default, data of a parameter is a float :class:`torch.Tensor`\n    (unless we use :func:`torch.set_default_tensor_type` to change default\n    tensor type). To cast these parameters to a correct data type or GPU device,\n    we can call methods such as :meth:`~torch.nn.Module.double` or\n    :meth:`~torch.nn.Module.cuda`. See :class:`torch.nn.Module` for more\n    information.\n    """"""\n    def __init__(self):\n        super().__init__()\n        self._priors = OrderedDict()\n        self._guides = OrderedDict()\n        self._mode = ""model""\n\n    def set_prior(self, name, prior):\n        """"""\n        Sets prior for a parameter.\n\n        :param str name: Name of the parameter.\n        :param ~pyro.distributions.distribution.Distribution prior: A Pyro prior\n            distribution.\n        """"""\n        warnings.warn(""The method `self.set_prior({}, prior)` has been deprecated""\n                      "" in favor of `self.{} = PyroSample(prior)`."".format(name, name), UserWarning)\n        setattr(self, name, PyroSample(prior))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, PyroSample):\n            prior = value.prior\n            if hasattr(prior, ""sample""):\n                self._priors[name] = prior\n                self.autoguide(name, dist.Delta)\n                value = PyroSample(partial(_get_sample_fn, name=name))\n        super().__setattr__(name, value)\n\n    def autoguide(self, name, dist_constructor):\n        """"""\n        Sets an autoguide for an existing parameter with name ``name`` (mimic\n        the behavior of module :mod:`pyro.infer.autoguide`).\n\n        .. note:: `dist_constructor` should be one of\n            :class:`~pyro.distributions.Delta`,\n            :class:`~pyro.distributions.Normal`, and\n            :class:`~pyro.distributions.MultivariateNormal`. More distribution\n            constructor will be supported in the future if needed.\n\n        :param str name: Name of the parameter.\n        :param dist_constructor: A\n            :class:`~pyro.distributions.distribution.Distribution` constructor.\n        """"""\n        if name not in self._priors:\n            raise ValueError(""There is no prior for parameter: {}"".format(name))\n\n        if dist_constructor not in [dist.Delta, dist.Normal, dist.MultivariateNormal]:\n            raise NotImplementedError(""Unsupported distribution type: {}""\n                                      .format(dist_constructor))\n\n        # delete old guide\n        if name in self._guides:\n            dist_args = self._guides[name][1]\n            for arg in dist_args:\n                delattr(self, ""{}_{}"".format(name, arg))\n\n        p = self._priors[name]()  # init_to_sample strategy\n        if dist_constructor is dist.Delta:\n            support = self._priors[name].support\n            if _is_real_support(support):\n                p_map = Parameter(p.detach())\n            else:\n                p_map = PyroParam(p.detach(), support)\n            setattr(self, ""{}_map"".format(name), p_map)\n            dist_args = (""map"",)\n        elif dist_constructor is dist.Normal:\n            loc = Parameter(biject_to(self._priors[name].support).inv(p).detach())\n            scale = PyroParam(loc.new_ones(loc.shape), constraints.positive)\n            setattr(self, ""{}_loc"".format(name), loc)\n            setattr(self, ""{}_scale"".format(name), scale)\n            dist_args = (""loc"", ""scale"")\n        elif dist_constructor is dist.MultivariateNormal:\n            loc = Parameter(biject_to(self._priors[name].support).inv(p).detach())\n            identity = eye_like(loc, loc.size(-1))\n            scale_tril = PyroParam(identity.repeat(loc.shape[:-1] + (1, 1)),\n                                   constraints.lower_cholesky)\n            setattr(self, ""{}_loc"".format(name), loc)\n            setattr(self, ""{}_scale_tril"".format(name), scale_tril)\n            dist_args = (""loc"", ""scale_tril"")\n        else:\n            raise NotImplementedError\n\n        self._guides[name] = (dist_constructor, dist_args)\n\n    @pyro_method\n    def _load_pyro_samples(self):\n        """"""\n        Runs `pyro.sample` primitives for all `PyroSample` attributes.\n        """"""\n        for module in self.modules():\n            if ""_pyro_samples"" in module.__dict__:\n                for name in module._pyro_samples:\n                    getattr(module, name)\n\n    def set_mode(self, mode):\n        """"""\n        Sets ``mode`` of this object to be able to use its parameters in\n        stochastic functions. If ``mode=""model""``, a parameter will get its\n        value from its prior. If ``mode=""guide""``, the value will be drawn from\n        its guide.\n\n        .. note:: This method automatically sets ``mode`` for submodules which\n            belong to :class:`Parameterized` class.\n\n        :param str mode: Either ""model"" or ""guide"".\n        """"""\n        for module in self.modules():\n            if isinstance(module, Parameterized):\n                module.mode = mode\n\n    @property\n    def mode(self):\n        return self._mode\n\n    @mode.setter\n    def mode(self, mode):\n        self._mode = mode\n'"
pyro/contrib/gp/util.py,11,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.infer import TraceMeanField_ELBO\nfrom pyro.infer.util import torch_backward, torch_item\n\n\ndef conditional(Xnew, X, kernel, f_loc, f_scale_tril=None, Lff=None, full_cov=False,\n                whiten=False, jitter=1e-6):\n    r""""""\n    Given :math:`X_{new}`, predicts loc and covariance matrix of the conditional\n    multivariate normal distribution\n\n    .. math:: p(f^*(X_{new}) \\mid X, k, f_{loc}, f_{scale\\_tril}).\n\n    Here ``f_loc`` and ``f_scale_tril`` are variation parameters of the variational\n    distribution\n\n    .. math:: q(f \\mid f_{loc}, f_{scale\\_tril}) \\sim p(f | X, y),\n\n    where :math:`f` is the function value of the Gaussian Process given input :math:`X`\n\n    .. math:: p(f(X)) \\sim \\mathcal{N}(0, k(X, X))\n\n    and :math:`y` is computed from :math:`f` by some likelihood function\n    :math:`p(y|f)`.\n\n    In case ``f_scale_tril=None``, we consider :math:`f = f_{loc}` and computes\n\n    .. math:: p(f^*(X_{new}) \\mid X, k, f).\n\n    In case ``f_scale_tril`` is not ``None``, we follow the derivation from reference\n    [1]. For the case ``f_scale_tril=None``, we follow the popular reference [2].\n\n    References:\n\n    [1] `Sparse GPs: approximate the posterior, not the model\n    <https://www.prowler.io/sparse-gps-approximate-the-posterior-not-the-model/>`_\n\n    [2] `Gaussian Processes for Machine Learning`,\n    Carl E. Rasmussen, Christopher K. I. Williams\n\n    :param torch.Tensor Xnew: A new input data.\n    :param torch.Tensor X: An input data to be conditioned on.\n    :param ~pyro.contrib.gp.kernels.kernel.Kernel kernel: A Pyro kernel object.\n    :param torch.Tensor f_loc: Mean of :math:`q(f)`. In case ``f_scale_tril=None``,\n        :math:`f_{loc} = f`.\n    :param torch.Tensor f_scale_tril: Lower triangular decomposition of covariance\n        matrix of :math:`q(f)`\'s .\n    :param torch.Tensor Lff: Lower triangular decomposition of :math:`kernel(X, X)`\n        (optional).\n    :param bool full_cov: A flag to decide if we want to return full covariance\n        matrix or just variance.\n    :param bool whiten: A flag to tell if ``f_loc`` and ``f_scale_tril`` are\n        already transformed by the inverse of ``Lff``.\n    :param float jitter: A small positive term which is added into the diagonal part of\n        a covariance matrix to help stablize its Cholesky decomposition.\n    :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))`\n    :rtype: tuple(torch.Tensor, torch.Tensor)\n    """"""\n    # p(f* | Xnew, X, kernel, f_loc, f_scale_tril) ~ N(f* | loc, cov)\n    # Kff = Lff @ Lff.T\n    # v = inv(Lff) @ f_loc  <- whitened f_loc\n    # S = inv(Lff) @ f_scale_tril  <- whitened f_scale_tril\n    # Denote:\n    #     W = (inv(Lff) @ Kf*).T\n    #     K = W @ S @ S.T @ W.T\n    #     Q** = K*f @ inv(Kff) @ Kf* = W @ W.T\n    # loc = K*f @ inv(Kff) @ f_loc = W @ v\n    # Case 1: f_scale_tril = None\n    #     cov = K** - K*f @ inv(Kff) @ Kf* = K** - Q**\n    # Case 2: f_scale_tril != None\n    #     cov = K** - Q** + K*f @ inv(Kff) @ f_cov @ inv(Kff) @ Kf*\n    #         = K** - Q** + W @ S @ S.T @ W.T\n    #         = K** - Q** + K\n\n    N = X.size(0)\n    M = Xnew.size(0)\n    latent_shape = f_loc.shape[:-1]\n\n    if Lff is None:\n        Kff = kernel(X).contiguous()\n        Kff.view(-1)[::N + 1] += jitter  # add jitter to diagonal\n        Lff = Kff.cholesky()\n    Kfs = kernel(X, Xnew)\n\n    # convert f_loc_shape from latent_shape x N to N x latent_shape\n    f_loc = f_loc.permute(-1, *range(len(latent_shape)))\n    # convert f_loc to 2D tensor for packing\n    f_loc_2D = f_loc.reshape(N, -1)\n    if f_scale_tril is not None:\n        # convert f_scale_tril_shape from latent_shape x N x N to N x N x latent_shape\n        f_scale_tril = f_scale_tril.permute(-2, -1, *range(len(latent_shape)))\n        # convert f_scale_tril to 2D tensor for packing\n        f_scale_tril_2D = f_scale_tril.reshape(N, -1)\n\n    if whiten:\n        v_2D = f_loc_2D\n        W = Kfs.triangular_solve(Lff, upper=False)[0].t()\n        if f_scale_tril is not None:\n            S_2D = f_scale_tril_2D\n    else:\n        pack = torch.cat((f_loc_2D, Kfs), dim=1)\n        if f_scale_tril is not None:\n            pack = torch.cat((pack, f_scale_tril_2D), dim=1)\n\n        Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0]\n        # unpack\n        v_2D = Lffinv_pack[:, :f_loc_2D.size(1)]\n        W = Lffinv_pack[:, f_loc_2D.size(1):f_loc_2D.size(1) + M].t()\n        if f_scale_tril is not None:\n            S_2D = Lffinv_pack[:, -f_scale_tril_2D.size(1):]\n\n    loc_shape = latent_shape + (M,)\n    loc = W.matmul(v_2D).t().reshape(loc_shape)\n\n    if full_cov:\n        Kss = kernel(Xnew)\n        Qss = W.matmul(W.t())\n        cov = Kss - Qss\n    else:\n        Kssdiag = kernel(Xnew, diag=True)\n        Qssdiag = W.pow(2).sum(dim=-1)\n        # Theoretically, Kss - Qss is non-negative; but due to numerical\n        # computation, that might not be the case in practice.\n        var = (Kssdiag - Qssdiag).clamp(min=0)\n\n    if f_scale_tril is not None:\n        W_S_shape = (Xnew.size(0),) + f_scale_tril.shape[1:]\n        W_S = W.matmul(S_2D).reshape(W_S_shape)\n        # convert W_S_shape from M x N x latent_shape to latent_shape x M x N\n        W_S = W_S.permute(list(range(2, W_S.dim())) + [0, 1])\n\n        if full_cov:\n            St_Wt = W_S.transpose(-2, -1)\n            K = W_S.matmul(St_Wt)\n            cov = cov + K\n        else:\n            Kdiag = W_S.pow(2).sum(dim=-1)\n            var = var + Kdiag\n    else:\n        if full_cov:\n            cov = cov.expand(latent_shape + (M, M))\n        else:\n            var = var.expand(latent_shape + (M,))\n\n    return (loc, cov) if full_cov else (loc, var)\n\n\ndef train(gpmodule, optimizer=None, loss_fn=None, retain_graph=None, num_steps=1000):\n    """"""\n    A helper to optimize parameters for a GP module.\n\n    :param ~pyro.contrib.gp.models.GPModel gpmodule: A GP module.\n    :param ~torch.optim.Optimizer optimizer: A PyTorch optimizer instance.\n        By default, we use Adam with ``lr=0.01``.\n    :param callable loss_fn: A loss function which takes inputs are\n        ``gpmodule.model``, ``gpmodule.guide``, and returns ELBO loss.\n        By default, ``loss_fn=TraceMeanField_ELBO().differentiable_loss``.\n    :param bool retain_graph: An optional flag of ``torch.autograd.backward``.\n    :param int num_steps: Number of steps to run SVI.\n    :returns: a list of losses during the training procedure\n    :rtype: list\n    """"""\n    optimizer = (torch.optim.Adam(gpmodule.parameters(), lr=0.01)\n                 if optimizer is None else optimizer)\n    # TODO: add support for JIT loss\n    loss_fn = TraceMeanField_ELBO().differentiable_loss if loss_fn is None else loss_fn\n\n    def closure():\n        optimizer.zero_grad()\n        loss = loss_fn(gpmodule.model, gpmodule.guide)\n        torch_backward(loss, retain_graph)\n        return loss\n\n    losses = []\n    for i in range(num_steps):\n        loss = optimizer.step(closure)\n        losses.append(torch_item(loss))\n    return losses\n'"
pyro/contrib/oed/__init__.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""Tasks such as choosing the next question to ask in a psychology study, designing an election polling\nstrategy, and deciding which compounds to synthesize and test in biological sciences are all fundamentally asking the\nsame question: how do we design an experiment to maximize the information gathered?  Pyro is designed to support\nautomated optimal experiment design: specifying a model and guide is enough to obtain optimal designs for many different\nkinds of experiment scenarios. Check out our experimental design tutorials that use Pyro to\n`design an adaptive psychology study <https://pyro.ai/examples/working_memory.html>`_ that uses past data to\nselect the next question, and `design an election polling strategy <https://pyro.ai/examples/elections.html>`_ that\naims to give the strongest prediction about the eventual winner of the election.\n\nBayesian optimal experimental design (BOED) is a powerful methodology for tackling experimental design problems and\nis the framework adopted by Pyro.\nIn the BOED framework, we begin with a Bayesian model with a likelihood :math:`p(y|\\\\theta,d)` and a prior\n:math:`p(\\\\theta)` on the target latent variables. In Pyro, any fully Bayesian model can be used in the BOED framework.\nThe sample sites corresponding to experimental outcomes are the *observation* sites, those corresponding to\nlatent variables of interest are the *target* sites. The design :math:`d` is the argument to the model, and is not\na random variable.\n\nIn the BOED framework, we choose the design that optimizes the expected information gain (EIG) on the targets\n:math:`\\\\theta` from running the experiment\n\n    :math:`\\\\text{EIG}(d) = \\\\mathbf{E}_{p(y|d)} [H[p(\\\\theta)] \xe2\x88\x92 H[p(\\\\theta|y, d)]]` ,\n\nwhere :math:`H[\xc2\xb7]` represents the entropy and :math:`p(\\\\theta|y, d) \\\\propto p(\\\\theta)p(y|\\\\theta, d)` is the\nposterior we get from\nrunning the experiment with design :math:`d` and observing :math:`y`. In other words, the optimal design is the one\nthat, in expectation over possible future observations, most reduces posterior entropy\nover the target latent variables. If the predictive model is correct, this forms a design strategy that is\n(one-step) optimal from an information-theoretic viewpoint. For further details, see [1, 2].\n\nThe :mod:`pyro.contrib.oed` module provides tools to create optimal experimental\ndesigns for Pyro models. In particular, it provides estimators for the\nexpected information gain (EIG).\n\nTo estimate the EIG for a particular design, we first set up our Pyro model. For example::\n\n    def model(design):\n\n        # This line allows batching of designs, treating all batch dimensions as independent\n        with pyro.plate_stack(""plate_stack"", design.shape):\n\n            # We use a Normal prior for theta\n            theta = pyro.sample(""theta"", dist.Normal(torch.tensor(0.0), torch.tensor(1.0)))\n\n            # We use a simple logistic regression model for the likelihood\n            logit_p = theta - design\n            y = pyro.sample(""y"", dist.Bernoulli(logits=logit_p))\n\n            return y\n\nWe then select an appropriate EIG estimator, such as::\n\n    eig = nmc_eig(model, design, observation_labels=[""y""], target_labels=[""theta""], N=2500, M=50)\n\nIt is possible to estimate the EIG across a grid of designs::\n\n    designs = torch.stack([design1, design2], dim=0)\n\nto find the best design from a number of options.\n\n[1] Chaloner, Kathryn, and Isabella Verdinelli. ""Bayesian experimental design: A review.""\nStatistical Science (1995): 273-304.\n\n[2] Foster, Adam, et al. ""Variational Bayesian Optimal Experimental Design."" arXiv preprint arXiv:1903.05480 (2019).\n\n""""""\n\nfrom pyro.contrib.oed import search, eig\n\n__all__ = [\n    ""search"",\n    ""eig""\n]\n'"
pyro/contrib/oed/eig.py,39,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport math\nimport warnings\n\nimport pyro\nfrom pyro import poutine\nfrom pyro.infer.autoguide.utils import mean_field_entropy\nfrom pyro.contrib.oed.search import Search\nfrom pyro.infer import EmpiricalMarginal, Importance, SVI\nfrom pyro.util import torch_isnan, torch_isinf\nfrom pyro.contrib.util import lexpand\n\n__all__ = [\n    ""laplace_eig"",\n    ""vi_eig"",\n    ""nmc_eig"",\n    ""donsker_varadhan_eig"",\n    ""posterior_eig"",\n    ""marginal_eig"",\n    ""lfire_eig"",\n    ""vnmc_eig""\n]\n\n\ndef laplace_eig(model, design, observation_labels, target_labels, guide, loss, optim, num_steps,\n                final_num_samples, y_dist=None, eig=True, **prior_entropy_kwargs):\n    """"""\n    Estimates the expected information gain (EIG) by making repeated Laplace approximations to the posterior.\n\n    :param function model: Pyro stochastic function taking `design` as only argument.\n    :param torch.Tensor design: Tensor of possible designs.\n    :param list observation_labels: labels of sample sites to be regarded as observables.\n    :param list target_labels: labels of sample sites to be regarded as latent variables of interest, i.e. the sites\n        that we wish to gain information about.\n    :param function guide: Pyro stochastic function corresponding to `model`.\n    :param loss: a Pyro loss such as `pyro.infer.Trace_ELBO().differentiable_loss`.\n    :param optim: optimizer for the loss\n    :param int num_steps: Number of gradient steps to take per sampled pseudo-observation.\n    :param int final_num_samples: Number of `y` samples (pseudo-observations) to take.\n    :param y_dist: Distribution to sample `y` from- if `None` we use the Bayesian marginal distribution.\n    :param bool eig: Whether to compute the EIG or the average posterior entropy (APE). The EIG is given by\n        `EIG = prior entropy - APE`. If `True`, the prior entropy will be estimated analytically,\n        or by Monte Carlo as appropriate for the `model`. If `False` the APE is returned.\n    :param dict prior_entropy_kwargs: parameters for estimating the prior entropy: `num_prior_samples` indicating the\n        number of samples for a MC estimate of prior entropy, and `mean_field` indicating if an analytic form for\n        a mean-field prior should be tried.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor\n    """"""\n\n    if isinstance(observation_labels, str):\n        observation_labels = [observation_labels]\n    if target_labels is not None and isinstance(target_labels, str):\n        target_labels = [target_labels]\n\n    ape = _laplace_vi_ape(model, design, observation_labels, target_labels, guide, loss, optim, num_steps,\n                          final_num_samples, y_dist=y_dist)\n    return _eig_from_ape(model, design, target_labels, ape, eig, prior_entropy_kwargs)\n\n\ndef _eig_from_ape(model, design, target_labels, ape, eig, prior_entropy_kwargs):\n    mean_field = prior_entropy_kwargs.get(""mean_field"", True)\n    if eig:\n        if mean_field:\n            try:\n                prior_entropy = mean_field_entropy(model, [design], whitelist=target_labels)\n            except NotImplemented:\n                prior_entropy = monte_carlo_entropy(model, design, target_labels, **prior_entropy_kwargs)\n        else:\n            prior_entropy = monte_carlo_entropy(model, design, target_labels, **prior_entropy_kwargs)\n        return prior_entropy - ape\n    else:\n        return ape\n\n\ndef _laplace_vi_ape(model, design, observation_labels, target_labels, guide, loss, optim, num_steps,\n                    final_num_samples, y_dist=None):\n\n    def posterior_entropy(y_dist, design):\n        # Important that y_dist is sampled *within* the function\n        y = pyro.sample(""conditioning_y"", y_dist)\n        y_dict = {label: y[i, ...] for i, label in enumerate(observation_labels)}\n        conditioned_model = pyro.condition(model, data=y_dict)\n        # Here just using SVI to run the MAP optimization\n        guide.train()\n        svi = SVI(conditioned_model, guide=guide, loss=loss, optim=optim)\n        with poutine.block():\n            for _ in range(num_steps):\n                svi.step(design)\n        # Recover the entropy\n        with poutine.block():\n            final_loss = loss(conditioned_model, guide, design)\n            guide.finalize(final_loss, target_labels)\n            entropy = mean_field_entropy(guide, [design], whitelist=target_labels)\n        return entropy\n\n    if y_dist is None:\n        y_dist = EmpiricalMarginal(Importance(model, num_samples=final_num_samples).run(design),\n                                   sites=observation_labels)\n\n    # Calculate the expected posterior entropy under this distn of y\n    loss_dist = EmpiricalMarginal(Search(posterior_entropy).run(y_dist, design))\n    ape = loss_dist.mean\n\n    return ape\n\n\n# Deprecated\ndef vi_eig(model, design, observation_labels, target_labels, vi_parameters, is_parameters, y_dist=None,\n           eig=True, **prior_entropy_kwargs):\n    """""".. deprecated:: 0.4.1\n        Use `posterior_eig` instead.\n\n    Estimates the expected information gain (EIG) using variational inference (VI).\n\n    The APE is defined as\n\n        :math:`APE(d)=E_{Y\\\\sim p(y|\\\\theta, d)}[H(p(\\\\theta|Y, d))]`\n\n    where :math:`H[p(x)]` is the `differential entropy\n    <https://en.wikipedia.org/wiki/Differential_entropy>`_.\n    The APE is related to expected information gain (EIG) by the equation\n\n        :math:`EIG(d)=H[p(\\\\theta)]-APE(d)`\n\n    in particular, minimising the APE is equivalent to maximising EIG.\n\n    :param function model: A pyro model accepting `design` as only argument.\n    :param torch.Tensor design: Tensor representation of design\n    :param list observation_labels: A subset of the sample sites\n        present in `model`. These sites are regarded as future observations\n        and other sites are regarded as latent variables over which a\n        posterior is to be inferred.\n    :param list target_labels: A subset of the sample sites over which the posterior\n        entropy is to be measured.\n    :param dict vi_parameters: Variational inference parameters which should include:\n        `optim`: an instance of :class:`pyro.Optim`, `guide`: a guide function\n        compatible with `model`, `num_steps`: the number of VI steps to make,\n        and `loss`: the loss function to use for VI\n    :param dict is_parameters: Importance sampling parameters for the\n        marginal distribution of :math:`Y`. May include `num_samples`: the number\n        of samples to draw from the marginal.\n    :param pyro.distributions.Distribution y_dist: (optional) the distribution\n        assumed for the response variable :math:`Y`\n    :param bool eig: Whether to compute the EIG or the average posterior entropy (APE). The EIG is given by\n        `EIG = prior entropy - APE`. If `True`, the prior entropy will be estimated analytically,\n        or by Monte Carlo as appropriate for the `model`. If `False` the APE is returned.\n    :param dict prior_entropy_kwargs: parameters for estimating the prior entropy: `num_prior_samples` indicating the\n        number of samples for a MC estimate of prior entropy, and `mean_field` indicating if an analytic form for\n        a mean-field prior should be tried.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor\n\n    """"""\n\n    warnings.warn(""`vi_eig` is deprecated in favour of the amortized version: `posterior_eig`."", DeprecationWarning)\n\n    if isinstance(observation_labels, str):\n        observation_labels = [observation_labels]\n    if target_labels is not None and isinstance(target_labels, str):\n        target_labels = [target_labels]\n\n    ape = _vi_ape(model, design, observation_labels, target_labels, vi_parameters, is_parameters, y_dist=y_dist)\n    return _eig_from_ape(model, design, target_labels, ape, eig, prior_entropy_kwargs)\n\n\ndef _vi_ape(model, design, observation_labels, target_labels, vi_parameters, is_parameters, y_dist=None):\n    svi_num_steps = vi_parameters.pop(\'num_steps\')\n\n    def posterior_entropy(y_dist, design):\n        # Important that y_dist is sampled *within* the function\n        y = pyro.sample(""conditioning_y"", y_dist)\n        y_dict = {label: y[i, ...] for i, label in enumerate(observation_labels)}\n        conditioned_model = pyro.condition(model, data=y_dict)\n        svi = SVI(conditioned_model, **vi_parameters)\n        with poutine.block():\n            for _ in range(svi_num_steps):\n                svi.step(design)\n        # Recover the entropy\n        with poutine.block():\n            guide = vi_parameters[""guide""]\n            entropy = mean_field_entropy(guide, [design], whitelist=target_labels)\n        return entropy\n\n    if y_dist is None:\n        y_dist = EmpiricalMarginal(Importance(model, **is_parameters).run(design),\n                                   sites=observation_labels)\n\n    # Calculate the expected posterior entropy under this distn of y\n    loss_dist = EmpiricalMarginal(Search(posterior_entropy).run(y_dist, design))\n    loss = loss_dist.mean\n\n    return loss\n\n\ndef nmc_eig(model, design, observation_labels, target_labels=None,\n            N=100, M=10, M_prime=None, independent_priors=False):\n    """"""Nested Monte Carlo estimate of the expected information\n    gain (EIG). The estimate is, when there are not any random effects,\n\n    .. math::\n\n        \\\\frac{1}{N}\\\\sum_{n=1}^N \\\\log p(y_n | \\\\theta_n, d) -\n        \\\\frac{1}{N}\\\\sum_{n=1}^N \\\\log \\\\left(\\\\frac{1}{M}\\\\sum_{m=1}^M p(y_n | \\\\theta_m, d)\\\\right)\n\n    where :math:`\\\\theta_n, y_n \\\\sim p(\\\\theta, y | d)` and :math:`\\\\theta_m \\\\sim p(\\\\theta)`.\n    The estimate in the presence of random effects is\n\n    .. math::\n\n        \\\\frac{1}{N}\\\\sum_{n=1}^N  \\\\log \\\\left(\\\\frac{1}{M\'}\\\\sum_{m=1}^{M\'}\n        p(y_n | \\\\theta_n, \\\\widetilde{\\\\theta}_{nm}, d)\\\\right)-\n        \\\\frac{1}{N}\\\\sum_{n=1}^N \\\\log \\\\left(\\\\frac{1}{M}\\\\sum_{m=1}^{M}\n        p(y_n | \\\\theta_m, \\\\widetilde{\\\\theta}_{m}, d)\\\\right)\n\n    where :math:`\\\\widetilde{\\\\theta}` are the random effects with\n    :math:`\\\\widetilde{\\\\theta}_{nm} \\\\sim p(\\\\widetilde{\\\\theta}|\\\\theta=\\\\theta_n)` and\n    :math:`\\\\theta_m,\\\\widetilde{\\\\theta}_m \\\\sim p(\\\\theta,\\\\widetilde{\\\\theta})`.\n    The latter form is used when `M_prime != None`.\n\n    :param function model: A pyro model accepting `design` as only argument.\n    :param torch.Tensor design: Tensor representation of design\n    :param list observation_labels: A subset of the sample sites\n        present in `model`. These sites are regarded as future observations\n        and other sites are regarded as latent variables over which a\n        posterior is to be inferred.\n    :param list target_labels: A subset of the sample sites over which the posterior\n        entropy is to be measured.\n    :param int N: Number of outer expectation samples.\n    :param int M: Number of inner expectation samples for `p(y|d)`.\n    :param int M_prime: Number of samples for `p(y | theta, d)` if required.\n    :param bool independent_priors: Only used when `M_prime` is not `None`. Indicates whether the prior distributions\n        for the target variables and the nuisance variables are independent. In this case, it is not necessary to\n        sample the targets conditional on the nuisance variables.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor\n    """"""\n\n    if isinstance(observation_labels, str):  # list of strings instead of strings\n        observation_labels = [observation_labels]\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n\n    # Take N samples of the model\n    expanded_design = lexpand(design, N)  # N copies of the model\n    trace = poutine.trace(model).get_trace(expanded_design)\n    trace.compute_log_prob()\n\n    if M_prime is not None:\n        y_dict = {l: lexpand(trace.nodes[l][""value""], M_prime) for l in observation_labels}\n        theta_dict = {l: lexpand(trace.nodes[l][""value""], M_prime) for l in target_labels}\n        theta_dict.update(y_dict)\n        # Resample M values of u and compute conditional probabilities\n        # WARNING: currently the use of condition does not actually sample\n        # the conditional distribution!\n        # We need to use some importance weighting\n        conditional_model = pyro.condition(model, data=theta_dict)\n        if independent_priors:\n            reexpanded_design = lexpand(design, M_prime, 1)\n        else:\n            # Not acceptable to use (M_prime, 1) here - other variables may occur after\n            # theta, so need to be sampled conditional upon it\n            reexpanded_design = lexpand(design, M_prime, N)\n        retrace = poutine.trace(conditional_model).get_trace(reexpanded_design)\n        retrace.compute_log_prob()\n        conditional_lp = sum(retrace.nodes[l][""log_prob""] for l in observation_labels).logsumexp(0) \\\n            - math.log(M_prime)\n    else:\n        # This assumes that y are independent conditional on theta\n        # Furthermore assume that there are no other variables besides theta\n        conditional_lp = sum(trace.nodes[l][""log_prob""] for l in observation_labels)\n\n    y_dict = {l: lexpand(trace.nodes[l][""value""], M) for l in observation_labels}\n    # Resample M values of theta and compute conditional probabilities\n    conditional_model = pyro.condition(model, data=y_dict)\n    # Using (M, 1) instead of (M, N) - acceptable to re-use thetas between ys because\n    # theta comes before y in graphical model\n    reexpanded_design = lexpand(design, M, 1)  # sample M theta\n    retrace = poutine.trace(conditional_model).get_trace(reexpanded_design)\n    retrace.compute_log_prob()\n    marginal_lp = sum(retrace.nodes[l][""log_prob""] for l in observation_labels).logsumexp(0) \\\n        - math.log(M)\n\n    terms = conditional_lp - marginal_lp\n    nonnan = (~torch.isnan(terms)).sum(0).type_as(terms)\n    terms[torch.isnan(terms)] = 0.\n    return terms.sum(0)/nonnan\n\n\ndef donsker_varadhan_eig(model, design, observation_labels, target_labels,\n                         num_samples, num_steps, T, optim, return_history=False,\n                         final_design=None, final_num_samples=None):\n    """"""\n    Donsker-Varadhan estimate of the expected information gain (EIG).\n\n    The Donsker-Varadhan representation of EIG is\n\n    .. math::\n\n        \\\\sup_T E_{p(y, \\\\theta | d)}[T(y, \\\\theta)] - \\\\log E_{p(y|d)p(\\\\theta)}[\\\\exp(T(\\\\bar{y}, \\\\bar{\\\\theta}))]\n\n    where :math:`T` is any (measurable) function.\n\n    This methods optimises the loss function over a pre-specified class of\n    functions `T`.\n\n    :param function model: A pyro model accepting `design` as only argument.\n    :param torch.Tensor design: Tensor representation of design\n    :param list observation_labels: A subset of the sample sites\n        present in `model`. These sites are regarded as future observations\n        and other sites are regarded as latent variables over which a\n        posterior is to be inferred.\n    :param list target_labels: A subset of the sample sites over which the posterior\n        entropy is to be measured.\n    :param int num_samples: Number of samples per iteration.\n    :param int num_steps: Number of optimization steps.\n    :param function or torch.nn.Module T: optimisable function `T` for use in the\n        Donsker-Varadhan loss function.\n    :param pyro.optim.Optim optim: Optimiser to use.\n    :param bool return_history: If `True`, also returns a tensor giving the loss function\n        at each step of the optimization.\n    :param torch.Tensor final_design: The final design tensor to evaluate at. If `None`, uses\n        `design`.\n    :param int final_num_samples: The number of samples to use at the final evaluation, If `None,\n        uses `num_samples`.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor or tuple\n    """"""\n    if isinstance(observation_labels, str):\n        observation_labels = [observation_labels]\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n    loss = _donsker_varadhan_loss(model, T, observation_labels, target_labels)\n    return opt_eig_ape_loss(design, loss, num_samples, num_steps, optim, return_history,\n                            final_design, final_num_samples)\n\n\ndef posterior_eig(model, design, observation_labels, target_labels, num_samples, num_steps, guide, optim,\n                  return_history=False, final_design=None, final_num_samples=None, eig=True, prior_entropy_kwargs={},\n                  *args, **kwargs):\n    """"""\n    Posterior estimate of expected information gain (EIG) computed from the average posterior entropy (APE)\n    using :math:`EIG(d) = H[p(\\\\theta)] - APE(d)`. See [1] for full details.\n\n    The posterior representation of APE is\n\n        :math:`\\\\sup_{q}\\\\ E_{p(y, \\\\theta | d)}[\\\\log q(\\\\theta | y, d)]`\n\n    where :math:`q` is any distribution on :math:`\\\\theta`.\n\n    This method optimises the loss over a given `guide` family representing :math:`q`.\n\n    [1] Foster, Adam, et al. ""Variational Bayesian Optimal Experimental Design."" arXiv preprint arXiv:1903.05480 (2019).\n\n    :param function model: A pyro model accepting `design` as only argument.\n    :param torch.Tensor design: Tensor representation of design\n    :param list observation_labels: A subset of the sample sites\n        present in `model`. These sites are regarded as future observations\n        and other sites are regarded as latent variables over which a\n        posterior is to be inferred.\n    :param list target_labels: A subset of the sample sites over which the posterior\n        entropy is to be measured.\n    :param int num_samples: Number of samples per iteration.\n    :param int num_steps: Number of optimization steps.\n    :param function guide: guide family for use in the (implicit) posterior estimation.\n        The parameters of `guide` are optimised to maximise the posterior\n        objective.\n    :param pyro.optim.Optim optim: Optimiser to use.\n    :param bool return_history: If `True`, also returns a tensor giving the loss function\n        at each step of the optimization.\n    :param torch.Tensor final_design: The final design tensor to evaluate at. If `None`, uses\n        `design`.\n    :param int final_num_samples: The number of samples to use at the final evaluation, If `None,\n        uses `num_samples`.\n    :param bool eig: Whether to compute the EIG or the average posterior entropy (APE). The EIG is given by\n        `EIG = prior entropy - APE`. If `True`, the prior entropy will be estimated analytically,\n        or by Monte Carlo as appropriate for the `model`. If `False` the APE is returned.\n    :param dict prior_entropy_kwargs: parameters for estimating the prior entropy: `num_prior_samples` indicating the\n        number of samples for a MC estimate of prior entropy, and `mean_field` indicating if an analytic form for\n        a mean-field prior should be tried.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor or tuple\n    """"""\n    if isinstance(observation_labels, str):\n        observation_labels = [observation_labels]\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n\n    ape = _posterior_ape(model, design, observation_labels, target_labels, num_samples, num_steps, guide, optim,\n                         return_history=return_history, final_design=final_design, final_num_samples=final_num_samples,\n                         *args, **kwargs)\n    return _eig_from_ape(model, design, target_labels, ape, eig, prior_entropy_kwargs)\n\n\ndef _posterior_ape(model, design, observation_labels, target_labels,\n                   num_samples, num_steps, guide, optim, return_history=False,\n                   final_design=None, final_num_samples=None, *args, **kwargs):\n\n    loss = _posterior_loss(model, guide, observation_labels, target_labels, *args, **kwargs)\n    return opt_eig_ape_loss(design, loss, num_samples, num_steps, optim, return_history,\n                            final_design, final_num_samples)\n\n\ndef marginal_eig(model, design, observation_labels, target_labels,\n                 num_samples, num_steps, guide, optim, return_history=False,\n                 final_design=None, final_num_samples=None):\n    """"""Estimate EIG by estimating the marginal entropy :math:`p(y|d)`. See [1] for full details.\n\n    The marginal representation of EIG is\n\n        :math:`\\\\inf_{q}\\\\ E_{p(y, \\\\theta | d)}\\\\left[\\\\log \\\\frac{p(y | \\\\theta, d)}{q(y | d)} \\\\right]`\n\n    where :math:`q` is any distribution on :math:`y`. A variational family for :math:`q` is specified in the `guide`.\n\n    .. warning :: This method does **not** estimate the correct quantity in the presence of random effects.\n\n    [1] Foster, Adam, et al. ""Variational Bayesian Optimal Experimental Design."" arXiv preprint arXiv:1903.05480 (2019).\n\n    :param function model: A pyro model accepting `design` as only argument.\n    :param torch.Tensor design: Tensor representation of design\n    :param list observation_labels: A subset of the sample sites\n        present in `model`. These sites are regarded as future observations\n        and other sites are regarded as latent variables over which a\n        posterior is to be inferred.\n    :param list target_labels: A subset of the sample sites over which the posterior\n        entropy is to be measured.\n    :param int num_samples: Number of samples per iteration.\n    :param int num_steps: Number of optimization steps.\n    :param function guide: guide family for use in the marginal estimation.\n        The parameters of `guide` are optimised to maximise the log-likelihood objective.\n    :param pyro.optim.Optim optim: Optimiser to use.\n    :param bool return_history: If `True`, also returns a tensor giving the loss function\n        at each step of the optimization.\n    :param torch.Tensor final_design: The final design tensor to evaluate at. If `None`, uses\n        `design`.\n    :param int final_num_samples: The number of samples to use at the final evaluation, If `None,\n        uses `num_samples`.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor or tuple\n    """"""\n\n    if isinstance(observation_labels, str):\n        observation_labels = [observation_labels]\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n    loss = _marginal_loss(model, guide, observation_labels, target_labels)\n    return opt_eig_ape_loss(design, loss, num_samples, num_steps, optim, return_history,\n                            final_design, final_num_samples)\n\n\ndef marginal_likelihood_eig(model, design, observation_labels, target_labels,\n                            num_samples, num_steps, marginal_guide, cond_guide, optim,\n                            return_history=False, final_design=None, final_num_samples=None):\n    """"""Estimates EIG by estimating the marginal entropy, that of :math:`p(y|d)`,\n    *and* the conditional entropy, of :math:`p(y|\\\\theta, d)`, both via Gibbs\' Inequality. See [1] for full details.\n\n    [1] Foster, Adam, et al. ""Variational Bayesian Optimal Experimental Design."" arXiv preprint arXiv:1903.05480 (2019).\n\n    :param function model: A pyro model accepting `design` as only argument.\n    :param torch.Tensor design: Tensor representation of design\n    :param list observation_labels: A subset of the sample sites\n        present in `model`. These sites are regarded as future observations\n        and other sites are regarded as latent variables over which a\n        posterior is to be inferred.\n    :param list target_labels: A subset of the sample sites over which the posterior\n        entropy is to be measured.\n    :param int num_samples: Number of samples per iteration.\n    :param int num_steps: Number of optimization steps.\n    :param function marginal_guide: guide family for use in the marginal estimation.\n        The parameters of `guide` are optimised to maximise the log-likelihood objective.\n    :param function cond_guide: guide family for use in the likelihood (conditional) estimation.\n        The parameters of `guide` are optimised to maximise the log-likelihood objective.\n    :param pyro.optim.Optim optim: Optimiser to use.\n    :param bool return_history: If `True`, also returns a tensor giving the loss function\n        at each step of the optimization.\n    :param torch.Tensor final_design: The final design tensor to evaluate at. If `None`, uses\n        `design`.\n    :param int final_num_samples: The number of samples to use at the final evaluation, If `None,\n        uses `num_samples`.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor or tuple\n    """"""\n\n    if isinstance(observation_labels, str):\n        observation_labels = [observation_labels]\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n    loss = _marginal_likelihood_loss(model, marginal_guide, cond_guide, observation_labels, target_labels)\n    return opt_eig_ape_loss(design, loss, num_samples, num_steps, optim, return_history,\n                            final_design, final_num_samples)\n\n\ndef lfire_eig(model, design, observation_labels, target_labels,\n              num_y_samples, num_theta_samples, num_steps, classifier, optim, return_history=False,\n              final_design=None, final_num_samples=None):\n    """"""Estimates the EIG using the method of Likelihood-Free Inference by Ratio Estimation (LFIRE) as in [1].\n    LFIRE is run separately for several samples of :math:`\\\\theta`.\n\n    [1] Kleinegesse, Steven, and Michael Gutmann. ""Efficient Bayesian Experimental Design for Implicit Models.""\n    arXiv preprint arXiv:1810.09912 (2018).\n\n    :param function model: A pyro model accepting `design` as only argument.\n    :param torch.Tensor design: Tensor representation of design\n    :param list observation_labels: A subset of the sample sites\n        present in `model`. These sites are regarded as future observations\n        and other sites are regarded as latent variables over which a\n        posterior is to be inferred.\n    :param list target_labels: A subset of the sample sites over which the posterior\n        entropy is to be measured.\n    :param int num_y_samples: Number of samples to take in :math:`y` for each :math:`\\\\theta`.\n    :param: int num_theta_samples: Number of initial samples in :math:`\\\\theta` to take. The likelihood ratio\n                                   is estimated by LFIRE for each sample.\n    :param int num_steps: Number of optimization steps.\n    :param function classifier: a Pytorch or Pyro classifier used to distinguish between samples of :math:`y` under\n                                :math:`p(y|d)` and samples under :math:`p(y|\\\\theta,d)` for some :math:`\\\\theta`.\n    :param pyro.optim.Optim optim: Optimiser to use.\n    :param bool return_history: If `True`, also returns a tensor giving the loss function\n        at each step of the optimization.\n    :param torch.Tensor final_design: The final design tensor to evaluate at. If `None`, uses\n        `design`.\n    :param int final_num_samples: The number of samples to use at the final evaluation, If `None,\n        uses `num_samples`.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor or tuple\n    """"""\n    if isinstance(observation_labels, str):\n        observation_labels = [observation_labels]\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n\n    # Take N samples of the model\n    expanded_design = lexpand(design, num_theta_samples)\n    trace = poutine.trace(model).get_trace(expanded_design)\n\n    theta_dict = {l: trace.nodes[l][""value""] for l in target_labels}\n    cond_model = pyro.condition(model, data=theta_dict)\n\n    loss = _lfire_loss(model, cond_model, classifier, observation_labels, target_labels)\n    out = opt_eig_ape_loss(expanded_design, loss, num_y_samples, num_steps, optim, return_history,\n                           final_design, final_num_samples)\n    if return_history:\n        return out[0], out[1].sum(0) / num_theta_samples\n    else:\n        return out.sum(0) / num_theta_samples\n\n\ndef vnmc_eig(model, design, observation_labels, target_labels,\n             num_samples, num_steps, guide, optim, return_history=False,\n             final_design=None, final_num_samples=None):\n    """"""Estimates the EIG using Variational Nested Monte Carlo (VNMC). The VNMC estimate [1] is\n\n    .. math::\n\n        \\\\frac{1}{N}\\\\sum_{n=1}^N \\\\left[ \\\\log p(y_n | \\\\theta_n, d) -\n         \\\\log \\\\left(\\\\frac{1}{M}\\\\sum_{m=1}^M \\\\frac{p(\\\\theta_{mn})p(y_n | \\\\theta_{mn}, d)}\n         {q(\\\\theta_{mn} | y_n)} \\\\right) \\\\right]\n\n    where :math:`q(\\\\theta | y)` is the learned variational posterior approximation and\n    :math:`\\\\theta_n, y_n \\\\sim p(\\\\theta, y | d)`, :math:`\\\\theta_{mn} \\\\sim q(\\\\theta|y=y_n)`.\n\n    As :math:`N \\\\to \\\\infty` this is an upper bound on EIG. We minimise this upper bound by stochastic gradient\n    descent.\n\n    .. warning :: This method cannot be used in the presence of random effects.\n\n    [1] Foster, Adam, et al. ""Variational Bayesian Optimal Experimental Design."" arXiv preprint arXiv:1903.05480 (2019).\n\n    :param function model: A pyro model accepting `design` as only argument.\n    :param torch.Tensor design: Tensor representation of design\n    :param list observation_labels: A subset of the sample sites\n        present in `model`. These sites are regarded as future observations\n        and other sites are regarded as latent variables over which a\n        posterior is to be inferred.\n    :param list target_labels: A subset of the sample sites over which the posterior\n        entropy is to be measured.\n    :param tuple num_samples: Number of (:math:`N, M`) samples per iteration.\n    :param int num_steps: Number of optimization steps.\n    :param function guide: guide family for use in the posterior estimation.\n        The parameters of `guide` are optimised to minimise the VNMC upper bound.\n    :param pyro.optim.Optim optim: Optimiser to use.\n    :param bool return_history: If `True`, also returns a tensor giving the loss function\n        at each step of the optimization.\n    :param torch.Tensor final_design: The final design tensor to evaluate at. If `None`, uses\n        `design`.\n    :param tuple final_num_samples: The number of (:math:`N, M`) samples to use at the final evaluation, If `None,\n        uses `num_samples`.\n    :return: EIG estimate, optionally includes full optimization history\n    :rtype: torch.Tensor or tuple\n    """"""\n    if isinstance(observation_labels, str):\n        observation_labels = [observation_labels]\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n    loss = _vnmc_eig_loss(model, guide, observation_labels, target_labels)\n    return opt_eig_ape_loss(design, loss, num_samples, num_steps, optim, return_history,\n                            final_design, final_num_samples)\n\n\ndef opt_eig_ape_loss(design, loss_fn, num_samples, num_steps, optim, return_history=False,\n                     final_design=None, final_num_samples=None):\n\n    if final_design is None:\n        final_design = design\n    if final_num_samples is None:\n        final_num_samples = num_samples\n\n    params = None\n    history = []\n    for step in range(num_steps):\n        if params is not None:\n            pyro.infer.util.zero_grads(params)\n        with poutine.trace(param_only=True) as param_capture:\n            agg_loss, loss = loss_fn(design, num_samples, evaluation=return_history)\n        params = set(site[""value""].unconstrained()\n                     for site in param_capture.trace.nodes.values())\n        if torch.isnan(agg_loss):\n            raise ArithmeticError(""Encountered NaN loss in opt_eig_ape_loss"")\n        agg_loss.backward(retain_graph=True)\n        if return_history:\n            history.append(loss)\n        optim(params)\n        try:\n            optim.step()\n        except AttributeError:\n            pass\n\n    _, loss = loss_fn(final_design, final_num_samples, evaluation=True)\n    if return_history:\n        return torch.stack(history), loss\n    else:\n        return loss\n\n\ndef monte_carlo_entropy(model, design, target_labels, num_prior_samples=1000):\n    """"""Computes a Monte Carlo estimate of the entropy of `model` assuming that each of sites in `target_labels` is\n    independent and the entropy is to be computed for that subset of sites only.\n    """"""\n\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n\n    expanded_design = lexpand(design, num_prior_samples)\n    trace = pyro.poutine.trace(model).get_trace(expanded_design)\n    trace.compute_log_prob()\n    lp = sum(trace.nodes[l][""log_prob""] for l in target_labels)\n    return -lp.sum(0) / num_prior_samples\n\n\ndef _donsker_varadhan_loss(model, T, observation_labels, target_labels):\n    """"""DV loss: to evaluate directly use `donsker_varadhan_eig` setting `num_steps=0`.""""""\n\n    ewma_log = EwmaLog(alpha=0.90)\n\n    def loss_fn(design, num_particles, **kwargs):\n\n        try:\n            pyro.module(""T"", T)\n        except AssertionError:\n            pass\n\n        expanded_design = lexpand(design, num_particles)\n\n        # Unshuffled data\n        unshuffled_trace = poutine.trace(model).get_trace(expanded_design)\n        y_dict = {l: unshuffled_trace.nodes[l][""value""] for l in observation_labels}\n\n        # Shuffled data\n        # Not actually shuffling, resimulate for safety\n        conditional_model = pyro.condition(model, data=y_dict)\n        shuffled_trace = poutine.trace(conditional_model).get_trace(expanded_design)\n\n        T_joint = T(expanded_design, unshuffled_trace, observation_labels, target_labels)\n        T_independent = T(expanded_design, shuffled_trace, observation_labels, target_labels)\n\n        joint_expectation = T_joint.sum(0)/num_particles\n\n        A = T_independent - math.log(num_particles)\n        s, _ = torch.max(A, dim=0)\n        independent_expectation = s + ewma_log((A - s).exp().sum(dim=0), s)\n\n        loss = joint_expectation - independent_expectation\n        # Switch sign, sum over batch dimensions for scalar loss\n        agg_loss = -loss.sum()\n        return agg_loss, loss\n\n    return loss_fn\n\n\ndef _posterior_loss(model, guide, observation_labels, target_labels, analytic_entropy=False):\n    """"""Posterior loss: to evaluate directly use `posterior_eig` setting `num_steps=0`, `eig=False`.""""""\n\n    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n\n        expanded_design = lexpand(design, num_particles)\n\n        # Sample from p(y, theta | d)\n        trace = poutine.trace(model).get_trace(expanded_design)\n        y_dict = {l: trace.nodes[l][""value""] for l in observation_labels}\n        theta_dict = {l: trace.nodes[l][""value""] for l in target_labels}\n\n        # Run through q(theta | y, d)\n        conditional_guide = pyro.condition(guide, data=theta_dict)\n        cond_trace = poutine.trace(conditional_guide).get_trace(\n            y_dict, expanded_design, observation_labels, target_labels)\n        cond_trace.compute_log_prob()\n        if evaluation and analytic_entropy:\n            loss = mean_field_entropy(\n                guide, [y_dict, expanded_design, observation_labels, target_labels],\n                whitelist=target_labels).sum(0) / num_particles\n            agg_loss = loss.sum()\n        else:\n            terms = -sum(cond_trace.nodes[l][""log_prob""] for l in target_labels)\n            agg_loss, loss = _safe_mean_terms(terms)\n\n        return agg_loss, loss\n\n    return loss_fn\n\n\ndef _marginal_loss(model, guide, observation_labels, target_labels):\n    """"""Marginal loss: to evaluate directly use `marginal_eig` setting `num_steps=0`.""""""\n\n    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n\n        expanded_design = lexpand(design, num_particles)\n\n        # Sample from p(y | d)\n        trace = poutine.trace(model).get_trace(expanded_design)\n        y_dict = {l: trace.nodes[l][""value""] for l in observation_labels}\n\n        # Run through q(y | d)\n        conditional_guide = pyro.condition(guide, data=y_dict)\n        cond_trace = poutine.trace(conditional_guide).get_trace(\n             expanded_design, observation_labels, target_labels)\n        cond_trace.compute_log_prob()\n\n        terms = -sum(cond_trace.nodes[l][""log_prob""] for l in observation_labels)\n\n        # At eval time, add p(y | theta, d) terms\n        if evaluation:\n            trace.compute_log_prob()\n            terms += sum(trace.nodes[l][""log_prob""] for l in observation_labels)\n\n        return _safe_mean_terms(terms)\n\n    return loss_fn\n\n\ndef _marginal_likelihood_loss(model, marginal_guide, likelihood_guide, observation_labels, target_labels):\n    """"""Marginal_likelihood loss: to evaluate directly use `marginal_likelihood_eig` setting `num_steps=0`.""""""\n\n    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n\n        expanded_design = lexpand(design, num_particles)\n\n        # Sample from p(y | d)\n        trace = poutine.trace(model).get_trace(expanded_design)\n        y_dict = {l: trace.nodes[l][""value""] for l in observation_labels}\n        theta_dict = {l: trace.nodes[l][""value""] for l in target_labels}\n\n        # Run through q(y | d)\n        qyd = pyro.condition(marginal_guide, data=y_dict)\n        marginal_trace = poutine.trace(qyd).get_trace(\n             expanded_design, observation_labels, target_labels)\n        marginal_trace.compute_log_prob()\n\n        # Run through q(y | theta, d)\n        qythetad = pyro.condition(likelihood_guide, data=y_dict)\n        cond_trace = poutine.trace(qythetad).get_trace(\n                theta_dict, expanded_design, observation_labels, target_labels)\n        cond_trace.compute_log_prob()\n        terms = -sum(marginal_trace.nodes[l][""log_prob""] for l in observation_labels)\n\n        # At evaluation time, use the right estimator, q(y | theta, d) - q(y | d)\n        # At training time, use -q(y | theta, d) - q(y | d) so gradients go the same way\n        if evaluation:\n            terms += sum(cond_trace.nodes[l][""log_prob""] for l in observation_labels)\n        else:\n            terms -= sum(cond_trace.nodes[l][""log_prob""] for l in observation_labels)\n\n        return _safe_mean_terms(terms)\n\n    return loss_fn\n\n\ndef _lfire_loss(model_marginal, model_conditional, h, observation_labels, target_labels):\n    """"""LFIRE loss: to evaluate directly use `lfire_eig` setting `num_steps=0`.""""""\n\n    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n\n        try:\n            pyro.module(""h"", h)\n        except AssertionError:\n            pass\n\n        expanded_design = lexpand(design, num_particles)\n        model_conditional_trace = poutine.trace(model_conditional).get_trace(expanded_design)\n\n        if not evaluation:\n            model_marginal_trace = poutine.trace(model_marginal).get_trace(expanded_design)\n\n            h_joint = h(expanded_design, model_conditional_trace, observation_labels, target_labels)\n            h_independent = h(expanded_design, model_marginal_trace, observation_labels, target_labels)\n\n            terms = torch.nn.functional.softplus(-h_joint) + torch.nn.functional.softplus(h_independent)\n            return _safe_mean_terms(terms)\n\n        else:\n            h_joint = h(expanded_design, model_conditional_trace, observation_labels, target_labels)\n            return _safe_mean_terms(h_joint)\n\n    return loss_fn\n\n\ndef _vnmc_eig_loss(model, guide, observation_labels, target_labels):\n    """"""VNMC loss: to evaluate directly use `vnmc_eig` setting `num_steps=0`.""""""\n\n    def loss_fn(design, num_particles, evaluation=False, **kwargs):\n        N, M = num_particles\n        expanded_design = lexpand(design, N)\n\n        # Sample from p(y, theta | d)\n        trace = poutine.trace(model).get_trace(expanded_design)\n        y_dict = {l: lexpand(trace.nodes[l][""value""], M) for l in observation_labels}\n\n        # Sample M times from q(theta | y, d) for each y\n        reexpanded_design = lexpand(expanded_design, M)\n        conditional_guide = pyro.condition(guide, data=y_dict)\n        guide_trace = poutine.trace(conditional_guide).get_trace(\n            y_dict, reexpanded_design, observation_labels, target_labels)\n        theta_y_dict = {l: guide_trace.nodes[l][""value""] for l in target_labels}\n        theta_y_dict.update(y_dict)\n        guide_trace.compute_log_prob()\n\n        # Re-run that through the model to compute the joint\n        modelp = pyro.condition(model, data=theta_y_dict)\n        model_trace = poutine.trace(modelp).get_trace(reexpanded_design)\n        model_trace.compute_log_prob()\n\n        terms = -sum(guide_trace.nodes[l][""log_prob""] for l in target_labels)\n        terms += sum(model_trace.nodes[l][""log_prob""] for l in target_labels)\n        terms += sum(model_trace.nodes[l][""log_prob""] for l in observation_labels)\n        terms = -terms.logsumexp(0) + math.log(M)\n\n        # At eval time, add p(y | theta, d) terms\n        if evaluation:\n            trace.compute_log_prob()\n            terms += sum(trace.nodes[l][""log_prob""] for l in observation_labels)\n\n        return _safe_mean_terms(terms)\n\n    return loss_fn\n\n\ndef _safe_mean_terms(terms):\n    mask = torch.isnan(terms) | (terms == float(\'-inf\')) | (terms == float(\'inf\'))\n    if terms.dtype is torch.float32:\n        nonnan = (~mask).sum(0).float()\n    elif terms.dtype is torch.float64:\n        nonnan = (~mask).sum(0).double()\n    terms[mask] = 0.\n    loss = terms.sum(0) / nonnan\n    agg_loss = loss.sum()\n    return agg_loss, loss\n\n\ndef xexpx(a):\n    """"""Computes `a*exp(a)`.\n\n    This function makes the outputs more stable when the inputs of this function converge to :math:`-\\\\infty`.\n\n    :param torch.Tensor a:\n    :return: Equivalent of `a*torch.exp(a)`.\n    """"""\n    mask = (a == float(\'-inf\'))\n    y = a*torch.exp(a)\n    y[mask] = 0.\n    return y\n\n\nclass _EwmaLogFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, ewma):\n        ctx.save_for_backward(ewma)\n        return input.log()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ewma, = ctx.saved_tensors\n        return grad_output / ewma, None\n\n\n_ewma_log_fn = _EwmaLogFn.apply\n\n\nclass EwmaLog:\n    """"""Logarithm function with exponentially weighted moving average\n    for gradients.\n\n    For input `inputs` this function return :code:`inputs.log()`. However, it\n    computes the gradient as\n\n        :math:`\\\\frac{\\\\sum_{t=0}^{T-1} \\\\alpha^t}{\\\\sum_{t=0}^{T-1} \\\\alpha^t x_{T-t}}`\n\n    where :math:`x_t` are historical input values passed to this function,\n    :math:`x_T` being the most recently seen value.\n\n    This gradient may help with numerical stability when the sequence of\n    inputs to the function form a convergent sequence.\n    """"""\n\n    def __init__(self, alpha):\n        self.alpha = alpha\n        self.ewma = 0.\n        self.n = 0\n        self.s = 0.\n\n    def __call__(self, inputs, s, dim=0, keepdim=False):\n        """"""Updates the moving average, and returns :code:`inputs.log()`.\n        """"""\n        self.n += 1\n        if torch_isnan(self.ewma) or torch_isinf(self.ewma):\n            ewma = inputs\n        else:\n            ewma = inputs * (1. - self.alpha) / (1 - self.alpha**self.n) \\\n                    + torch.exp(self.s - s) * self.ewma \\\n                    * (self.alpha - self.alpha**self.n) / (1 - self.alpha**self.n)\n        self.ewma = ewma.detach()\n        self.s = s.detach()\n        return _ewma_log_fn(inputs, ewma)\n'"
pyro/contrib/oed/search.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport queue\nfrom pyro.infer.abstract_infer import TracePosterior\nimport pyro.poutine as poutine\n\n###################################\n# Search borrowed from RSA example\n###################################\n\n\nclass Search(TracePosterior):\n    """"""\n    Exact inference by enumerating over all possible executions\n    """"""\n    def __init__(self, model, max_tries=int(1e6), **kwargs):\n        self.model = model\n        self.max_tries = max_tries\n        super().__init__(**kwargs)\n\n    def _traces(self, *args, **kwargs):\n        q = queue.Queue()\n        q.put(poutine.Trace())\n        p = poutine.trace(\n            poutine.queue(self.model, queue=q, max_tries=self.max_tries))\n        while not q.empty():\n            tr = p.get_trace(*args, **kwargs)\n            yield tr, tr.log_prob_sum()\n'"
pyro/contrib/oed/util.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport torch\n\nfrom pyro.contrib.util import get_indices\nfrom pyro.contrib.oed.glmm import analytic_posterior_cov\nfrom pyro.infer.autoguide.utils import mean_field_entropy\n\n\ndef linear_model_ground_truth(model, design, observation_labels, target_labels, eig=True):\n    if isinstance(target_labels, str):\n        target_labels = [target_labels]\n\n    w_sd = torch.cat(list(model.w_sds.values()), dim=-1)\n    prior_cov = torch.diag(w_sd**2)\n    design_shape = design.shape\n    posterior_covs = [analytic_posterior_cov(prior_cov, x, model.obs_sd) for x in\n                      torch.unbind(design.reshape(-1, design_shape[-2], design_shape[-1]))]\n    target_indices = get_indices(target_labels, tensors=model.w_sds)\n    target_posterior_covs = [S[target_indices, :][:, target_indices] for S in posterior_covs]\n    output = torch.tensor([0.5 * torch.logdet(2 * math.pi * math.e * C)\n                           for C in target_posterior_covs])\n    if eig:\n        prior_entropy = mean_field_entropy(model, [design], whitelist=target_labels)\n        output = prior_entropy - output\n\n    return output.reshape(design.shape[:-2])\n'"
pyro/contrib/randomvariable/__init__.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.randomvariable.random_variable import RandomVariable\n\n__all__ = [\n    ""RandomVariable"",\n]\n'"
pyro/contrib/randomvariable/random_variable.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom typing import Union\n\nfrom torch import Tensor\nfrom pyro.distributions import TransformedDistribution\nfrom pyro.distributions.transforms import (\n    Transform,\n    AffineTransform,\n    AbsTransform,\n    PowerTransform,\n    ExpTransform,\n    TanhTransform,\n    SoftmaxTransform,\n    SigmoidTransform\n)\n\n\nclass RVMagicOps:\n    """"""Mixin class for overloading __magic__ operations on random variables.\n    """"""\n\n    def __add__(self, x: Union[float, Tensor]):\n        return RandomVariable(TransformedDistribution(self.distribution, AffineTransform(x, 1)))\n\n    def __radd__(self, x: Union[float, Tensor]):\n        return RandomVariable(TransformedDistribution(self.distribution, AffineTransform(x, 1)))\n\n    def __sub__(self, x: Union[float, Tensor]):\n        return RandomVariable(TransformedDistribution(self.distribution, AffineTransform(-x, 1)))\n\n    def __rsub__(self, x: Union[float, Tensor]):\n        return RandomVariable(TransformedDistribution(self.distribution, AffineTransform(x, -1)))\n\n    def __mul__(self, x: Union[float, Tensor]):\n        return RandomVariable(TransformedDistribution(self.distribution, AffineTransform(0, x)))\n\n    def __rmul__(self, x: Union[float, Tensor]):\n        return RandomVariable(TransformedDistribution(self.distribution, AffineTransform(0, x)))\n\n    def __truediv__(self, x: Union[float, Tensor]):\n        return RandomVariable(TransformedDistribution(self.distribution, AffineTransform(0, 1/x)))\n\n    def __neg__(self):\n        return RandomVariable(TransformedDistribution(self.distribution, AffineTransform(0, -1)))\n\n    def __abs__(self):\n        return RandomVariable(TransformedDistribution(self.distribution, AbsTransform()))\n\n    def __pow__(self, x):\n        return RandomVariable(TransformedDistribution(self.distribution, PowerTransform(x)))\n\n\nclass RVChainOps:\n    """"""Mixin class for performing common unary/binary operations on/between\n    random variables/constant tensors using method chaining syntax.\n    """"""\n\n    def add(self, x):\n        return self + x\n\n    def sub(self, x):\n        return self - x\n\n    def mul(self, x):\n        return self * x\n\n    def div(self, x):\n        return self / x\n\n    def abs(self):\n        return abs(self)\n\n    def pow(self, x):\n        return self ** x\n\n    def neg(self):\n        return -self\n\n    def exp(self):\n        return self.transform(ExpTransform())\n\n    def log(self):\n        return self.transform(ExpTransform().inv)\n\n    def sigmoid(self):\n        return self.transform(SigmoidTransform())\n\n    def tanh(self):\n        return self.transform(TanhTransform())\n\n    def softmax(self):\n        return self.transform(SoftmaxTransform())\n\n\nclass RandomVariable(RVMagicOps, RVChainOps):\n    """"""EXPERIMENTAL random variable container class around a distribution\n\n    Representation of a distribution interpreted as a random variable. Rather\n    than directly manipulating a probability density by applying pointwise\n    transformations to it, this allows for simple arithmetic transformations of\n    the random variable the distribution represents. For more flexibility,\n    consider using the `transform` method. Note that if you perform a\n    non-invertible transform (like `abs(X)` or `X**2`), certain things might\n    not work properly.\n\n    Can switch between `RandomVariable` and `Distribution` objects with the\n    convenient `Distribution.rv` and `RandomVariable.dist` properties.\n\n    Supports either chaining operations or arithmetic operator overloading.\n\n    Example usage::\n\n        # This should be equivalent to an Exponential distribution.\n        RandomVariable(Uniform(0, 1)).log().neg().dist\n\n        # These two distributions Y1, Y2 should be the same\n        X = Uniform(0, 1).rv\n        Y1 = X.mul(4).pow(0.5).sub(1).abs().neg().dist\n        Y2 = (-abs((4*X)**(0.5) - 1)).dist\n    """"""\n\n    def __init__(self, distribution):\n        """"""Wraps a distribution as a RandomVariable\n\n        :param distribution: The `Distribution` object to wrap\n        :type distribution: ~pyro.distributions.distribution.Distribution\n        """"""\n        self.distribution = distribution\n\n    def transform(self, t: Transform):\n        """"""Performs a transformation on the distribution underlying the RV.\n\n        :param t: The transformation (or sequence of transformations) to be\n            applied to the distribution. There are many examples to be found in\n            `torch.distributions.transforms` and `pyro.distributions.transforms`,\n            or you can subclass directly from `Transform`.\n        :type t: ~pyro.distributions.transforms.Transform\n\n        :return: The transformed `RandomVariable`\n        :rtype: ~pyro.contrib.randomvariable.random_variable.RandomVariable\n        """"""\n        dist = TransformedDistribution(self.distribution, t)\n        return RandomVariable(dist)\n\n    @property\n    def dist(self):\n        """"""Convenience property for exposing the distribution underlying the\n        random variable.\n\n        :return: The `Distribution` object underlying the random variable\n        :rtype: ~pyro.distributions.distribution.Distribution\n        """"""\n        return self.distribution\n'"
pyro/contrib/timeseries/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThe :mod:`pyro.contrib.timeseries` module provides a collection of Bayesian time series\nmodels useful for forecasting applications.\n""""""\nfrom pyro.contrib.timeseries.base import TimeSeriesModel\nfrom pyro.contrib.timeseries.gp import IndependentMaternGP, LinearlyCoupledMaternGP, DependentMaternGP\nfrom pyro.contrib.timeseries.lgssm import GenericLGSSM\nfrom pyro.contrib.timeseries.lgssmgp import GenericLGSSMWithGPNoiseModel\n\n__all__ = [\n    ""DependentMaternGP"",\n    ""GenericLGSSM"",\n    ""GenericLGSSMWithGPNoiseModel"",\n    ""IndependentMaternGP"",\n    ""LinearlyCoupledMaternGP"",\n    ""TimeSeriesModel"",\n]\n'"
pyro/contrib/timeseries/base.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.nn import PyroModule, pyro_method\n\n\nclass TimeSeriesModel(PyroModule):\n    """"""\n    Base class for univariate and multivariate time series models.\n    """"""\n    @pyro_method\n    def log_prob(self, targets):\n        """"""\n        Log probability function.\n\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued ``targets`` at each time step\n        :returns torch.Tensor: A 0-dimensional log probability for the case of properly\n            multivariate time series models in which the output dimensions are correlated;\n            otherwise returns a 1-dimensional tensor of log probabilities for batched\n            univariate time series models.\n        """"""\n        raise NotImplementedError\n\n    @pyro_method\n    def forecast(self, targets, dts):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued targets at each time step. These\n            represent the training data that are conditioned on for the purpose of making\n            forecasts.\n        :param torch.Tensor dts: A 1-dimensional tensor of times to forecast into the future,\n            with zero corresponding to the time of the final target ``targets[-1]``.\n        :returns torch.distributions.Distribution: Returns a predictive distribution with batch shape ``(S,)`` and\n            event shape ``(obs_dim,)``, where ``S`` is the size of ``dts``. That is, the resulting\n            predictive distributions do not encode correlations between distinct times in ``dts``.\n        """"""\n        raise NotImplementedError\n\n    def get_dist(self):\n        """"""\n        Get a :class:`~pyro.distributions.Distribution` object corresponding to\n        this time series model.  Often this is a\n        :class:`~pyro.distributions.GaussianHMM`.\n        """"""\n        raise NotImplementedError\n'"
pyro/contrib/timeseries/gp.py,59,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import MultivariateNormal, constraints\n\nimport pyro.distributions as dist\nfrom pyro.contrib.timeseries.base import TimeSeriesModel\nfrom pyro.nn import PyroParam, pyro_method\nfrom pyro.ops.ssm_gp import MaternKernel\nfrom pyro.ops.tensor_utils import block_diag_embed\n\n\nclass IndependentMaternGP(TimeSeriesModel):\n    """"""\n    A time series model in which each output dimension is modeled independently\n    with a univariate Gaussian Process with a Matern kernel. The targets are assumed\n    to be evenly spaced in time. Training and inference are logarithmic in the length\n    of the time series T.\n\n    :param float nu: The order of the Matern kernel; one of 0.5, 1.5 or 2.5.\n    :param float dt: The time spacing between neighboring observations of the time series.\n    :param int obs_dim: The dimension of the targets at each time step.\n    :param torch.Tensor length_scale_init: optional initial values for the kernel length scale\n        given as a ``obs_dim``-dimensional tensor\n    :param torch.Tensor kernel_scale_init: optional initial values for the kernel scale\n        given as a ``obs_dim``-dimensional tensor\n    :param torch.Tensor obs_noise_scale_init: optional initial values for the observation noise scale\n        given as a ``obs_dim``-dimensional tensor\n    """"""\n    def __init__(self, nu=1.5, dt=1.0, obs_dim=1,\n                 length_scale_init=None, kernel_scale_init=None,\n                 obs_noise_scale_init=None):\n        self.nu = nu\n        self.dt = dt\n        self.obs_dim = obs_dim\n\n        if obs_noise_scale_init is None:\n            obs_noise_scale_init = 0.2 * torch.ones(obs_dim)\n        assert obs_noise_scale_init.shape == (obs_dim,)\n\n        super().__init__()\n\n        self.kernel = MaternKernel(nu=nu, num_gps=obs_dim,\n                                   length_scale_init=length_scale_init,\n                                   kernel_scale_init=kernel_scale_init)\n\n        self.obs_noise_scale = PyroParam(obs_noise_scale_init,\n                                         constraint=constraints.positive)\n\n        obs_matrix = [1.0] + [0.0] * (self.kernel.state_dim - 1)\n        self.register_buffer(""obs_matrix"", torch.tensor(obs_matrix).unsqueeze(-1))\n\n    def _get_init_dist(self):\n        return torch.distributions.MultivariateNormal(self.obs_matrix.new_zeros(self.obs_dim, self.kernel.state_dim),\n                                                      self.kernel.stationary_covariance().squeeze(-3))\n\n    def _get_obs_dist(self):\n        return dist.Normal(self.obs_matrix.new_zeros(self.obs_dim, 1, 1),\n                           self.obs_noise_scale.unsqueeze(-1).unsqueeze(-1)).to_event(1)\n\n    def get_dist(self, duration=None):\n        """"""\n        Get the :class:`~pyro.distributions.GaussianHMM` distribution that corresponds\n        to ``obs_dim``-many independent Matern GPs.\n\n        :param int duration: Optional size of the time axis ``event_shape[0]``.\n            This is required when sampling from homogeneous HMMs whose parameters\n            are not expanded along the time axis.\n        """"""\n        trans_matrix, process_covar = self.kernel.transition_matrix_and_covariance(dt=self.dt)\n        trans_dist = MultivariateNormal(self.obs_matrix.new_zeros(self.obs_dim, 1, self.kernel.state_dim),\n                                        process_covar.unsqueeze(-3))\n        trans_matrix = trans_matrix.unsqueeze(-3)\n        return dist.GaussianHMM(self._get_init_dist(), trans_matrix, trans_dist,\n                                self.obs_matrix, self._get_obs_dist(), duration=duration)\n\n    @pyro_method\n    def log_prob(self, targets):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued ``targets`` at each time step\n        :returns torch.Tensor: A 1-dimensional tensor of log probabilities of shape ``(obs_dim,)``\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().log_prob(targets.t().unsqueeze(-1))\n\n    @torch.no_grad()\n    def _filter(self, targets):\n        """"""\n        Return the filtering state for the associated state space model.\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().filter(targets.t().unsqueeze(-1))\n\n    @torch.no_grad()\n    def _forecast(self, dts, filtering_state, include_observation_noise=True):\n        """"""\n        Internal helper for forecasting.\n        """"""\n        assert dts.dim() == 1\n        dts = dts.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        trans_matrix, process_covar = self.kernel.transition_matrix_and_covariance(dt=dts)\n        trans_matrix = trans_matrix[..., 0:1]\n        predicted_mean = torch.matmul(filtering_state.loc.unsqueeze(-2), trans_matrix).squeeze(-2)[..., 0]\n        predicted_function_covar = torch.matmul(trans_matrix.transpose(-1, -2), torch.matmul(\n                                                filtering_state.covariance_matrix, trans_matrix))[..., 0, 0] + \\\n            process_covar[..., 0, 0]\n\n        if include_observation_noise:\n            predicted_function_covar = predicted_function_covar + self.obs_noise_scale.pow(2.0)\n        return predicted_mean, predicted_function_covar\n\n    @pyro_method\n    def forecast(self, targets, dts):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued targets at each time step. These\n            represent the training data that are conditioned on for the purpose of making\n            forecasts.\n        :param torch.Tensor dts: A 1-dimensional tensor of times to forecast into the future,\n            with zero corresponding to the time of the final target ``targets[-1]``.\n        :returns torch.distributions.Normal: Returns a predictive Normal distribution with batch shape ``(S,)`` and\n            event shape ``(obs_dim,)``, where ``S`` is the size of ``dts``.\n        """"""\n        filtering_state = self._filter(targets)\n        predicted_mean, predicted_covar = self._forecast(dts, filtering_state)\n        return torch.distributions.Normal(predicted_mean, predicted_covar.sqrt())\n\n\nclass LinearlyCoupledMaternGP(TimeSeriesModel):\n    """"""\n    A time series model in which each output dimension is modeled as a linear combination\n    of shared univariate Gaussian Processes with Matern kernels.\n\n    In more detail, the generative process is:\n\n        :math:`y_i(t) = \\\\sum_j A_{ij} f_j(t) + \\\\epsilon_i(t)`\n\n    The targets :math:`y_i` are assumed to be evenly spaced in time. Training and inference\n    are logarithmic in the length of the time series T.\n\n    :param float nu: The order of the Matern kernel; one of 0.5, 1.5 or 2.5.\n    :param float dt: The time spacing between neighboring observations of the time series.\n    :param int obs_dim: The dimension of the targets at each time step.\n    :param int num_gps: The number of independent GPs that are mixed to model the time series.\n        Typical values might be :math:`\\\\N_{\\\\rm gp} \\\\in [\\\\D_{\\\\rm obs} / 2, \\\\D_{\\\\rm obs}]`\n    :param torch.Tensor length_scale_init: optional initial values for the kernel length scale\n        given as a ``num_gps``-dimensional tensor\n    :param torch.Tensor kernel_scale_init: optional initial values for the kernel scale\n        given as a ``num_gps``-dimensional tensor\n    :param torch.Tensor obs_noise_scale_init: optional initial values for the observation noise scale\n        given as a ``obs_dim``-dimensional tensor\n    """"""\n    def __init__(self, nu=1.5, dt=1.0, obs_dim=2, num_gps=1,\n                 length_scale_init=None, kernel_scale_init=None,\n                 obs_noise_scale_init=None):\n        self.nu = nu\n        self.dt = dt\n        assert obs_dim > 1, ""If obs_dim==1 you should use IndependentMaternGP""\n        self.obs_dim = obs_dim\n        self.num_gps = num_gps\n\n        if obs_noise_scale_init is None:\n            obs_noise_scale_init = 0.2 * torch.ones(obs_dim)\n        assert obs_noise_scale_init.shape == (obs_dim,)\n\n        self.dt = dt\n        self.obs_dim = obs_dim\n        self.num_gps = num_gps\n\n        super().__init__()\n\n        self.kernel = MaternKernel(nu=nu, num_gps=num_gps,\n                                   length_scale_init=length_scale_init,\n                                   kernel_scale_init=kernel_scale_init)\n        self.full_state_dim = num_gps * self.kernel.state_dim\n\n        self.obs_noise_scale = PyroParam(obs_noise_scale_init,\n                                         constraint=constraints.positive)\n        self.A = nn.Parameter(0.3 * torch.randn(self.num_gps, self.obs_dim))\n\n    def _get_obs_matrix(self):\n        # (num_gps, obs_dim) => (state_dim * num_gps, obs_dim)\n        return self.A.repeat_interleave(self.kernel.state_dim, dim=0) * \\\n            self.A.new_tensor([1.0] + [0.0] * (self.kernel.state_dim - 1)).repeat(self.num_gps).unsqueeze(-1)\n\n    def _stationary_covariance(self):\n        return block_diag_embed(self.kernel.stationary_covariance())\n\n    def _get_init_dist(self):\n        loc = self.A.new_zeros(self.full_state_dim)\n        return MultivariateNormal(loc, self._stationary_covariance())\n\n    def _get_obs_dist(self):\n        loc = self.A.new_zeros(self.obs_dim)\n        return dist.Normal(loc, self.obs_noise_scale).to_event(1)\n\n    def get_dist(self, duration=None):\n        """"""\n        Get the :class:`~pyro.distributions.GaussianHMM` distribution that corresponds\n        to a :class:`LinearlyCoupledMaternGP`.\n\n        :param int duration: Optional size of the time axis ``event_shape[0]``.\n            This is required when sampling from homogeneous HMMs whose parameters\n            are not expanded along the time axis.\n        """"""\n        trans_matrix, process_covar = self.kernel.transition_matrix_and_covariance(dt=self.dt)\n        trans_matrix = block_diag_embed(trans_matrix)\n        process_covar = block_diag_embed(process_covar)\n        loc = self.A.new_zeros(self.full_state_dim)\n        trans_dist = MultivariateNormal(loc, process_covar)\n        return dist.GaussianHMM(self._get_init_dist(), trans_matrix, trans_dist,\n                                self._get_obs_matrix(), self._get_obs_dist(), duration=duration)\n\n    @pyro_method\n    def log_prob(self, targets):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued ``targets`` at each time step\n        :returns torch.Tensor: a (scalar) log probability\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().log_prob(targets)\n\n    @torch.no_grad()\n    def _filter(self, targets):\n        """"""\n        Return the filtering state for the associated state space model.\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().filter(targets)\n\n    @torch.no_grad()\n    def _forecast(self, dts, filtering_state, include_observation_noise=True, full_covar=True):\n        """"""\n        Internal helper for forecasting.\n        """"""\n        assert dts.dim() == 1\n        dts = dts.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        trans_mat, process_covar = self.kernel.transition_matrix_and_covariance(dt=dts)\n        trans_mat = block_diag_embed(trans_mat)  # S x full_state_dim x full_state_dim\n        process_covar = block_diag_embed(process_covar)  # S x full_state_dim x full_state_dim\n        obs_matrix = self._get_obs_matrix()  # full_state_dim x obs_dim\n        trans_obs = torch.matmul(trans_mat, obs_matrix)  # S x full_state_dim x obs_dim\n        predicted_mean = torch.matmul(filtering_state.loc.unsqueeze(-2), trans_obs).squeeze(-2)\n        predicted_function_covar = torch.matmul(trans_obs.transpose(-1, -2),\n                                                torch.matmul(filtering_state.covariance_matrix,\n                                                trans_obs))\n        predicted_function_covar = predicted_function_covar + \\\n            torch.matmul(obs_matrix.transpose(-1, -2), torch.matmul(process_covar, obs_matrix))\n\n        if include_observation_noise:\n            obs_noise = self.obs_noise_scale.pow(2.0).diag_embed()\n            predicted_function_covar = predicted_function_covar + obs_noise\n        if not full_covar:\n            predicted_function_covar = predicted_function_covar.diagonal(dim1=-1, dim2=-2)\n\n        return predicted_mean, predicted_function_covar\n\n    @pyro_method\n    def forecast(self, targets, dts):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued targets at each time step. These\n            represent the training data that are conditioned on for the purpose of making\n            forecasts.\n        :param torch.Tensor dts: A 1-dimensional tensor of times to forecast into the future,\n            with zero corresponding to the time of the final target ``targets[-1]``.\n        :returns torch.distributions.MultivariateNormal: Returns a predictive MultivariateNormal\n            distribution with batch shape ``(S,)`` and event shape ``(obs_dim,)``,\n            where ``S`` is the size of ``dts``.\n        """"""\n        filtering_state = self._filter(targets)\n        predicted_mean, predicted_covar = self._forecast(dts, filtering_state)\n        return MultivariateNormal(predicted_mean, predicted_covar)\n\n\nclass DependentMaternGP(TimeSeriesModel):\n    """"""\n    A time series model in which each output dimension is modeled as a univariate Gaussian Process\n    with a Matern kernel. The different output dimensions become correlated because the Gaussian\n    Processes are driven by a correlated Wiener process; see reference [1] for details.\n    If, in addition, `linearly_coupled` is True, additional correlation is achieved through\n    linear mixing as in :class:`LinearlyCoupledMaternGP`. The targets are assumed to be evenly\n    spaced in time. Training and inference are logarithmic in the length of the time series T.\n\n    :param float nu: The order of the Matern kernel; must be 1.5.\n    :param float dt: The time spacing between neighboring observations of the time series.\n    :param int obs_dim: The dimension of the targets at each time step.\n    :param bool linearly_coupled: Whether to linearly mix the various gaussian processes in the likelihood.\n        Defaults to False.\n    :param torch.Tensor length_scale_init: optional initial values for the kernel length scale\n        given as a ``obs_dim``-dimensional tensor\n    :param torch.Tensor obs_noise_scale_init: optional initial values for the observation noise scale\n        given as a ``obs_dim``-dimensional tensor\n\n    References\n    [1] ""Dependent Matern Processes for Multivariate Time Series,"" Alexander Vandenberg-Rodes, Babak Shahbaba.\n    """"""\n    def __init__(self, nu=1.5, dt=1.0, obs_dim=1, linearly_coupled=False,\n                 length_scale_init=None, obs_noise_scale_init=None):\n\n        if nu != 1.5:\n            raise NotImplementedError(""The only supported value of nu is 1.5"")\n\n        self.dt = dt\n        self.obs_dim = obs_dim\n\n        if obs_noise_scale_init is None:\n            obs_noise_scale_init = 0.2 * torch.ones(obs_dim)\n        assert obs_noise_scale_init.shape == (obs_dim,)\n\n        super().__init__()\n\n        self.kernel = MaternKernel(nu=nu, num_gps=obs_dim,\n                                   length_scale_init=length_scale_init)\n        self.full_state_dim = self.kernel.state_dim * obs_dim\n\n        # we demote self.kernel.kernel_scale from being a nn.Parameter\n        # since the relevant scales are now encoded in the wiener noise matrix\n        del self.kernel.kernel_scale\n        self.kernel.register_buffer(""kernel_scale"", torch.ones(obs_dim))\n\n        self.obs_noise_scale = PyroParam(obs_noise_scale_init,\n                                         constraint=constraints.positive)\n        self.wiener_noise_tril = PyroParam(torch.eye(obs_dim) +\n                                           0.03 * torch.randn(obs_dim, obs_dim).tril(-1),\n                                           constraint=constraints.lower_cholesky)\n\n        if linearly_coupled:\n            self.obs_matrix = nn.Parameter(0.3 * torch.randn(self.obs_dim, self.obs_dim))\n        else:\n            obs_matrix = torch.zeros(self.full_state_dim, obs_dim)\n            for i in range(obs_dim):\n                obs_matrix[self.kernel.state_dim * i, i] = 1.0\n            self.register_buffer(""obs_matrix"", obs_matrix)\n\n    def _get_obs_matrix(self):\n        if self.obs_matrix.size(0) == self.obs_dim:\n            # (num_gps, obs_dim) => (state_dim * num_gps, obs_dim)\n            selector = [1.0] + [0.0] * (self.kernel.state_dim - 1)\n            return self.obs_matrix.repeat_interleave(self.kernel.state_dim, dim=0) * \\\n                self.obs_matrix.new_tensor(selector).repeat(self.obs_dim).unsqueeze(-1)\n        else:\n            return self.obs_matrix\n\n    def _get_init_dist(self, stationary_covariance):\n        return torch.distributions.MultivariateNormal(self.obs_matrix.new_zeros(self.full_state_dim),\n                                                      stationary_covariance)\n\n    def _get_obs_dist(self):\n        return dist.Normal(self.obs_matrix.new_zeros(self.obs_dim),\n                           self.obs_noise_scale).to_event(1)\n\n    def _get_wiener_cov(self):\n        chol = self.wiener_noise_tril\n        wiener_cov = torch.mm(chol, chol.t()).reshape(self.obs_dim, 1, self.obs_dim, 1)\n        wiener_cov = wiener_cov * wiener_cov.new_ones(self.kernel.state_dim, 1, self.kernel.state_dim)\n        return wiener_cov.reshape(self.full_state_dim, self.full_state_dim)\n\n    def _stationary_covariance(self):\n        rho_j = math.sqrt(3.0) / self.kernel.length_scale.unsqueeze(-1).unsqueeze(-1)\n        rho_i = rho_j.unsqueeze(-1)\n        block = 2.0 * self.kernel.mask00 + \\\n            (rho_i - rho_j) * (self.kernel.mask01 - self.kernel.mask10) + \\\n            (2.0 * rho_i * rho_j) * self.kernel.mask11\n        block = block / (rho_i + rho_j).pow(3.0)\n        block = block.transpose(-2, -3).reshape(self.full_state_dim, self.full_state_dim)\n        return self._get_wiener_cov() * block\n\n    def _get_trans_dist(self, trans_matrix, stationary_covariance):\n        covar = stationary_covariance - torch.matmul(trans_matrix.transpose(-1, -2),\n                                                     torch.matmul(stationary_covariance, trans_matrix))\n        return MultivariateNormal(covar.new_zeros(self.full_state_dim), covar)\n\n    def _trans_matrix_distribution_stat_covar(self, dts):\n        stationary_covariance = self._stationary_covariance()\n        trans_matrix = self.kernel.transition_matrix(dt=dts)\n        trans_matrix = block_diag_embed(trans_matrix)\n        trans_dist = self._get_trans_dist(trans_matrix, stationary_covariance)\n        return trans_matrix, trans_dist, stationary_covariance\n\n    def get_dist(self, duration=None):\n        """"""\n        Get the :class:`~pyro.distributions.GaussianHMM` distribution that corresponds to a :class:`DependentMaternGP`\n\n        :param int duration: Optional size of the time axis ``event_shape[0]``.\n            This is required when sampling from homogeneous HMMs whose parameters\n            are not expanded along the time axis.\n        """"""\n        trans_matrix, trans_dist, stat_covar = self._trans_matrix_distribution_stat_covar(self.dt)\n        return dist.GaussianHMM(self._get_init_dist(stat_covar), trans_matrix,\n                                trans_dist, self._get_obs_matrix(), self._get_obs_dist(), duration=duration)\n\n    @pyro_method\n    def log_prob(self, targets):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued ``targets`` at each time step\n        :returns torch.Tensor: A (scalar) log probability\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().log_prob(targets)\n\n    @torch.no_grad()\n    def _filter(self, targets):\n        """"""\n        Return the filtering state for the associated state space model.\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().filter(targets)\n\n    @torch.no_grad()\n    def _forecast(self, dts, filtering_state, include_observation_noise=True):\n        """"""\n        Internal helper for forecasting.\n        """"""\n        assert dts.dim() == 1\n        dts = dts.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n        trans_matrix, trans_dist, _ = self._trans_matrix_distribution_stat_covar(dts)\n        obs_matrix = self._get_obs_matrix()\n        trans_obs = torch.matmul(trans_matrix, obs_matrix)\n\n        predicted_mean = torch.matmul(filtering_state.loc.unsqueeze(-2), trans_obs).squeeze(-2)\n        predicted_function_covar = torch.matmul(trans_obs.transpose(-1, -2),\n                                                torch.matmul(filtering_state.covariance_matrix, trans_obs)) + \\\n            torch.matmul(obs_matrix.t(), torch.matmul(trans_dist.covariance_matrix, obs_matrix))\n\n        if include_observation_noise:\n            predicted_function_covar = predicted_function_covar + self.obs_noise_scale.pow(2.0)\n\n        return predicted_mean, predicted_function_covar\n\n    @pyro_method\n    def forecast(self, targets, dts):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued targets at each time step. These\n            represent the training data that are conditioned on for the purpose of making\n            forecasts.\n        :param torch.Tensor dts: A 1-dimensional tensor of times to forecast into the future,\n            with zero corresponding to the time of the final target ``targets[-1]``.\n        :returns torch.distributions.MultivariateNormal: Returns a predictive MultivariateNormal\n            distribution with batch shape ``(S,)`` and event shape ``(obs_dim,)``, where ``S`` is the size of ``dts``.\n        """"""\n        filtering_state = self._filter(targets)\n        predicted_mean, predicted_covar = self._forecast(dts, filtering_state)\n        return MultivariateNormal(predicted_mean, predicted_covar)\n'"
pyro/contrib/timeseries/lgssm.py,24,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import MultivariateNormal, constraints\n\nimport pyro.distributions as dist\nfrom pyro.contrib.timeseries.base import TimeSeriesModel\nfrom pyro.nn import PyroParam, pyro_method\nfrom pyro.ops.tensor_utils import repeated_matmul\n\n\nclass GenericLGSSM(TimeSeriesModel):\n    """"""\n    A generic Linear Gaussian State Space Model parameterized with arbitrary time invariant\n    transition and observation dynamics. The targets are (implicitly) assumed to be evenly\n    spaced in time. Training and inference are logarithmic in the length of the time series T.\n\n    :param int obs_dim: The dimension of the targets at each time step.\n    :param int state_dim: The dimension of latent state at each time step.\n    :param bool learnable_observation_loc: whether the mean of the observation model should be learned or not;\n        defaults to False.\n    """"""\n    def __init__(self, obs_dim=1, state_dim=2, obs_noise_scale_init=None,\n                 learnable_observation_loc=False):\n        self.obs_dim = obs_dim\n        self.state_dim = state_dim\n\n        if obs_noise_scale_init is None:\n            obs_noise_scale_init = 0.2 * torch.ones(obs_dim)\n        assert obs_noise_scale_init.shape == (obs_dim,)\n\n        super().__init__()\n\n        self.obs_noise_scale = PyroParam(obs_noise_scale_init,\n                                         constraint=constraints.positive)\n        self.trans_noise_scale_sq = PyroParam(torch.ones(state_dim),\n                                              constraint=constraints.positive)\n        self.trans_matrix = nn.Parameter(torch.eye(state_dim) + 0.03 * torch.randn(state_dim, state_dim))\n        self.obs_matrix = nn.Parameter(0.3 * torch.randn(state_dim, obs_dim))\n        self.init_noise_scale_sq = PyroParam(torch.ones(state_dim),\n                                             constraint=constraints.positive)\n\n        if learnable_observation_loc:\n            self.obs_loc = nn.Parameter(torch.zeros(obs_dim))\n        else:\n            self.register_buffer(\'obs_loc\', torch.zeros(obs_dim))\n\n    def _get_init_dist(self):\n        loc = self.obs_matrix.new_zeros(self.state_dim)\n        return MultivariateNormal(loc, self.init_noise_scale_sq.diag_embed())\n\n    def _get_obs_dist(self):\n        return dist.Normal(self.obs_loc, self.obs_noise_scale).to_event(1)\n\n    def _get_trans_dist(self):\n        loc = self.obs_matrix.new_zeros(self.state_dim)\n        return MultivariateNormal(loc, self.trans_noise_scale_sq.diag_embed())\n\n    def get_dist(self, duration=None):\n        """"""\n        Get the :class:`~pyro.distributions.GaussianHMM` distribution that corresponds to :class:`GenericLGSSM`.\n\n        :param int duration: Optional size of the time axis ``event_shape[0]``.\n            This is required when sampling from homogeneous HMMs whose parameters\n            are not expanded along the time axis.\n        """"""\n        return dist.GaussianHMM(self._get_init_dist(), self.trans_matrix, self._get_trans_dist(),\n                                self.obs_matrix, self._get_obs_dist(), duration=duration)\n\n    @pyro_method\n    def log_prob(self, targets):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued ``targets`` at each time step\n        :returns torch.Tensor: A (scalar) log probability.\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().log_prob(targets)\n\n    @torch.no_grad()\n    def _filter(self, targets):\n        """"""\n        Return the filtering state for the associated state space model.\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().filter(targets)\n\n    @torch.no_grad()\n    def _forecast(self, N_timesteps, filtering_state, include_observation_noise=True):\n        """"""\n        Internal helper for forecasting.\n        """"""\n        N_trans_matrix = repeated_matmul(self.trans_matrix, N_timesteps)\n        N_trans_obs = torch.matmul(N_trans_matrix, self.obs_matrix)\n        predicted_mean = torch.matmul(filtering_state.loc, N_trans_obs)\n\n        # first compute the contribution from filtering_state.covariance_matrix\n        predicted_covar1 = torch.matmul(N_trans_obs.transpose(-1, -2),\n                                        torch.matmul(filtering_state.covariance_matrix,\n                                        N_trans_obs))  # N O O\n\n        # next compute the contribution from process noise that is injected at each timestep.\n        # (we need to do a cumulative sum to integrate across time)\n        process_covar = self._get_trans_dist().covariance_matrix\n        N_trans_obs_shift = torch.cat([self.obs_matrix.unsqueeze(0), N_trans_obs[:-1]])\n        predicted_covar2 = torch.matmul(N_trans_obs_shift.transpose(-1, -2),\n                                        torch.matmul(process_covar, N_trans_obs_shift))  # N O O\n\n        predicted_covar = predicted_covar1 + torch.cumsum(predicted_covar2, dim=0)\n\n        if include_observation_noise:\n            predicted_covar = predicted_covar + self.obs_noise_scale.pow(2.0).diag_embed()\n\n        return predicted_mean, predicted_covar\n\n    @pyro_method\n    def forecast(self, targets, N_timesteps):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued targets at each time step. These\n            represent the training data that are conditioned on for the purpose of making\n            forecasts.\n        :param int N_timesteps: The number of timesteps to forecast into the future from\n            the final target ``targets[-1]``.\n        :returns torch.distributions.MultivariateNormal: Returns a predictive MultivariateNormal distribution\n            with batch shape ``(N_timesteps,)`` and event shape ``(obs_dim,)``\n        """"""\n        filtering_state = self._filter(targets)\n        predicted_mean, predicted_covar = self._forecast(N_timesteps, filtering_state)\n        return torch.distributions.MultivariateNormal(predicted_mean, predicted_covar)\n'"
pyro/contrib/timeseries/lgssmgp.py,33,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import MultivariateNormal, constraints\n\nimport pyro.distributions as dist\nfrom pyro.contrib.timeseries.base import TimeSeriesModel\nfrom pyro.nn import PyroParam, pyro_method\nfrom pyro.ops.ssm_gp import MaternKernel\nfrom pyro.ops.tensor_utils import block_diag_embed, repeated_matmul\n\n\nclass GenericLGSSMWithGPNoiseModel(TimeSeriesModel):\n    """"""\n    A generic Linear Gaussian State Space Model parameterized with arbitrary time invariant\n    transition and observation dynamics together with separate Gaussian Process noise models\n    for each output dimension. In more detail, the generative process is:\n\n        :math:`y_i(t) = \\\\sum_j A_{ij} z_j(t) + f_i(t) + \\\\epsilon_i(t)`\n\n    where the latent variables :math:`{\\\\bf z}(t)` follow generic time invariant Linear Gaussian dynamics\n    and the :math:`f_i(t)` are Gaussian Processes with Matern kernels.\n\n    The targets are (implicitly) assumed to be evenly spaced in time. In particular a timestep of\n    :math:`dt=1.0` for the continuous-time GP dynamics corresponds to a single discrete step of\n    the :math:`{\\\\bf z}`-space dynamics. Training and inference are logarithmic in the length of\n    the time series T.\n\n    :param int obs_dim: The dimension of the targets at each time step.\n    :param int state_dim: The dimension of the :math:`{\\\\bf z}` latent state at each time step.\n    :param float nu: The order of the Matern kernel; one of 0.5, 1.5 or 2.5.\n    :param torch.Tensor length_scale_init: optional initial values for the kernel length scale\n        given as a ``obs_dim``-dimensional tensor\n    :param torch.Tensor kernel_scale_init: optional initial values for the kernel scale\n        given as a ``obs_dim``-dimensional tensor\n    :param torch.Tensor obs_noise_scale_init: optional initial values for the observation noise scale\n        given as a ``obs_dim``-dimensional tensor\n    :param bool learnable_observation_loc: whether the mean of the observation model should be learned or not;\n            defaults to False.\n    """"""\n    def __init__(self, obs_dim=1, state_dim=2, nu=1.5, obs_noise_scale_init=None,\n                 length_scale_init=None, kernel_scale_init=None,\n                 learnable_observation_loc=False):\n        self.obs_dim = obs_dim\n        self.state_dim = state_dim\n        self.nu = nu\n\n        if obs_noise_scale_init is None:\n            obs_noise_scale_init = 0.2 * torch.ones(obs_dim)\n        assert obs_noise_scale_init.shape == (obs_dim,)\n\n        super().__init__()\n\n        self.kernel = MaternKernel(nu=nu, num_gps=obs_dim,\n                                   length_scale_init=length_scale_init,\n                                   kernel_scale_init=kernel_scale_init)\n        self.dt = 1.0\n        self.full_state_dim = self.kernel.state_dim * obs_dim + state_dim\n        self.full_gp_state_dim = self.kernel.state_dim * obs_dim\n\n        self.obs_noise_scale = PyroParam(obs_noise_scale_init,\n                                         constraint=constraints.positive)\n        self.trans_noise_scale_sq = PyroParam(torch.ones(state_dim),\n                                              constraint=constraints.positive)\n        self.z_trans_matrix = nn.Parameter(torch.eye(state_dim) + 0.03 * torch.randn(state_dim, state_dim))\n        self.z_obs_matrix = nn.Parameter(0.3 * torch.randn(state_dim, obs_dim))\n        self.init_noise_scale_sq = PyroParam(torch.ones(state_dim),\n                                             constraint=constraints.positive)\n\n        gp_obs_matrix = torch.zeros(self.kernel.state_dim * obs_dim, obs_dim)\n        for i in range(obs_dim):\n            gp_obs_matrix[self.kernel.state_dim * i, i] = 1.0\n        self.register_buffer(""gp_obs_matrix"", gp_obs_matrix)\n\n        self.obs_selector = torch.tensor([self.kernel.state_dim * d for d in range(obs_dim)], dtype=torch.long)\n\n        if learnable_observation_loc:\n            self.obs_loc = nn.Parameter(torch.zeros(obs_dim))\n        else:\n            self.register_buffer(\'obs_loc\', torch.zeros(obs_dim))\n\n    def _get_obs_matrix(self):\n        # (obs_dim + state_dim, obs_dim) => (gp_state_dim * obs_dim + state_dim, obs_dim)\n        return torch.cat([self.gp_obs_matrix, self.z_obs_matrix], dim=0)\n\n    def _get_init_dist(self):\n        loc = self.z_trans_matrix.new_zeros(self.full_state_dim)\n        covar = self.z_trans_matrix.new_zeros(self.full_state_dim, self.full_state_dim)\n        covar[:self.full_gp_state_dim, :self.full_gp_state_dim] = block_diag_embed(self.kernel.stationary_covariance())\n        covar[self.full_gp_state_dim:, self.full_gp_state_dim:] = self.init_noise_scale_sq.diag_embed()\n        return MultivariateNormal(loc, covar)\n\n    def _get_obs_dist(self):\n        return dist.Normal(self.obs_loc, self.obs_noise_scale).to_event(1)\n\n    def get_dist(self, duration=None):\n        """"""\n        Get the :class:`~pyro.distributions.GaussianHMM` distribution that corresponds\n        to :class:`GenericLGSSMWithGPNoiseModel`.\n\n        :param int duration: Optional size of the time axis ``event_shape[0]``.\n            This is required when sampling from homogeneous HMMs whose parameters\n            are not expanded along the time axis.\n        """"""\n        gp_trans_matrix, gp_process_covar = self.kernel.transition_matrix_and_covariance(dt=self.dt)\n\n        trans_covar = self.z_trans_matrix.new_zeros(self.full_state_dim, self.full_state_dim)\n        trans_covar[:self.full_gp_state_dim, :self.full_gp_state_dim] = block_diag_embed(gp_process_covar)\n        trans_covar[self.full_gp_state_dim:, self.full_gp_state_dim:] = self.trans_noise_scale_sq.diag_embed()\n        trans_dist = MultivariateNormal(trans_covar.new_zeros(self.full_state_dim), trans_covar)\n\n        full_trans_mat = trans_covar.new_zeros(self.full_state_dim, self.full_state_dim)\n        full_trans_mat[:self.full_gp_state_dim, :self.full_gp_state_dim] = block_diag_embed(gp_trans_matrix)\n        full_trans_mat[self.full_gp_state_dim:, self.full_gp_state_dim:] = self.z_trans_matrix\n\n        return dist.GaussianHMM(self._get_init_dist(), full_trans_mat, trans_dist,\n                                self._get_obs_matrix(), self._get_obs_dist(), duration=duration)\n\n    @pyro_method\n    def log_prob(self, targets):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued ``targets`` at each time step\n        :returns torch.Tensor: A (scalar) log probability.\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().log_prob(targets)\n\n    @torch.no_grad()\n    def _filter(self, targets):\n        """"""\n        Return the filtering state for the associated state space model.\n        """"""\n        assert targets.dim() == 2 and targets.size(-1) == self.obs_dim\n        return self.get_dist().filter(targets)\n\n    @torch.no_grad()\n    def _forecast(self, N_timesteps, filtering_state, include_observation_noise=True):\n        """"""\n        Internal helper for forecasting.\n        """"""\n        dts = torch.arange(N_timesteps, dtype=self.z_trans_matrix.dtype, device=self.z_trans_matrix.device) + 1.0\n        dts = dts.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n\n        gp_trans_matrix, gp_process_covar = self.kernel.transition_matrix_and_covariance(dt=dts)\n        gp_trans_matrix = block_diag_embed(gp_trans_matrix)\n        gp_process_covar = block_diag_embed(gp_process_covar[..., 0:1, 0:1])\n\n        N_trans_matrix = repeated_matmul(self.z_trans_matrix, N_timesteps)\n        N_trans_obs = torch.matmul(N_trans_matrix, self.z_obs_matrix)\n\n        # z-state contribution + gp contribution\n        predicted_mean1 = torch.matmul(filtering_state.loc[-self.state_dim:].unsqueeze(-2), N_trans_obs).squeeze(-2)\n        predicted_mean2 = torch.matmul(filtering_state.loc[:self.full_gp_state_dim].unsqueeze(-2),\n                                       gp_trans_matrix[..., self.obs_selector]).squeeze(-2)\n        predicted_mean = predicted_mean1 + predicted_mean2\n\n        # first compute the contributions from filtering_state.covariance_matrix: z-space and gp\n        fs_cov = filtering_state.covariance_matrix\n        predicted_covar1z = torch.matmul(N_trans_obs.transpose(-1, -2),\n                                         torch.matmul(fs_cov[self.full_gp_state_dim:, self.full_gp_state_dim:],\n                                         N_trans_obs))  # N O O\n        gp_trans = gp_trans_matrix[..., self.obs_selector]\n        predicted_covar1gp = torch.matmul(gp_trans.transpose(-1, -2),\n                                          torch.matmul(fs_cov[:self.full_gp_state_dim:, :self.full_gp_state_dim],\n                                          gp_trans))\n\n        # next compute the contribution from process noise that is injected at each timestep.\n        # (we need to do a cumulative sum to integrate across time for the z-state contribution)\n        z_process_covar = self.trans_noise_scale_sq.diag_embed()\n        N_trans_obs_shift = torch.cat([self.z_obs_matrix.unsqueeze(0), N_trans_obs[0:-1]])\n        predicted_covar2z = torch.matmul(N_trans_obs_shift.transpose(-1, -2),\n                                         torch.matmul(z_process_covar, N_trans_obs_shift))  # N O O\n\n        predicted_covar = predicted_covar1z + predicted_covar1gp + gp_process_covar + \\\n            torch.cumsum(predicted_covar2z, dim=0)\n\n        if include_observation_noise:\n            predicted_covar = predicted_covar + self.obs_noise_scale.pow(2.0).diag_embed()\n\n        return predicted_mean, predicted_covar\n\n    @pyro_method\n    def forecast(self, targets, N_timesteps):\n        """"""\n        :param torch.Tensor targets: A 2-dimensional tensor of real-valued targets\n            of shape ``(T, obs_dim)``, where ``T`` is the length of the time series and ``obs_dim``\n            is the dimension of the real-valued targets at each time step. These\n            represent the training data that are conditioned on for the purpose of making\n            forecasts.\n        :param int N_timesteps: The number of timesteps to forecast into the future from\n            the final target ``targets[-1]``.\n        :returns torch.distributions.MultivariateNormal: Returns a predictive MultivariateNormal distribution\n            with batch shape ``(N_timesteps,)`` and event shape ``(obs_dim,)``\n        """"""\n        filtering_state = self._filter(targets)\n        predicted_mean, predicted_covar = self._forecast(N_timesteps, filtering_state)\n        return MultivariateNormal(predicted_mean, predicted_covar)\n'"
pyro/contrib/tracking/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.tracking import assignment\n\n__all__ = [\n    ""assignment"",\n]\n'"
pyro/contrib/tracking/assignment.py,25,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport math\nimport numbers\n\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.util import warn_if_nan\n\n\ndef _product(factors):\n    result = 1.\n    for factor in factors:\n        result = result * factor\n    return result\n\n\ndef _exp(value):\n    if isinstance(value, numbers.Number):\n        return math.exp(value)\n    return value.exp()\n\n\nclass MarginalAssignment:\n    """"""\n    Computes marginal data associations between objects and detections.\n\n    This assumes that each detection corresponds to zero or one object,\n    and each object corresponds to zero or more detections. Specifically\n    this does not assume detections have been partitioned into frames of\n    mutual exclusion as is common in 2-D assignment problems.\n\n    :param torch.Tensor exists_logits: a tensor of shape ``[num_objects]``\n        representing per-object factors for existence of each potential object.\n    :param torch.Tensor assign_logits: a tensor of shape\n        ``[num_detections, num_objects]`` representing per-edge factors of\n        assignment probability, where each edge denotes that a given detection\n        associates with a single object.\n    :param int bp_iters: optional number of belief propagation iterations. If\n        unspecified or ``None`` an expensive exact algorithm will be used.\n\n    :ivar int num_detections: the number of detections\n    :ivar int num_objects: the number of (potentially existing) objects\n    :ivar pyro.distributions.Bernoulli exists_dist: a mean field posterior\n        distribution over object existence.\n    :ivar pyro.distributions.Categorical assign_dist: a mean field posterior\n        distribution over the object (or None) to which each detection\n        associates.  This has ``.event_shape == (num_objects + 1,)`` where the\n        final element denotes spurious detection, and\n        ``.batch_shape == (num_frames, num_detections)``.\n    """"""\n    def __init__(self, exists_logits, assign_logits, bp_iters=None):\n        assert exists_logits.dim() == 1, exists_logits.shape\n        assert assign_logits.dim() == 2, assign_logits.shape\n        assert assign_logits.shape[-1] == exists_logits.shape[-1]\n        self.num_detections, self.num_objects = assign_logits.shape\n\n        # Clamp to avoid NANs.\n        exists_logits = exists_logits.clamp(min=-40, max=40)\n        assign_logits = assign_logits.clamp(min=-40, max=40)\n\n        # This does all the work.\n        if bp_iters is None:\n            exists, assign = compute_marginals(exists_logits, assign_logits)\n        else:\n            exists, assign = compute_marginals_bp(exists_logits, assign_logits, bp_iters)\n\n        # Wrap the results in Distribution objects.\n        # This adds a final logit=0 element denoting spurious detection.\n        padded_assign = torch.nn.functional.pad(assign, (0, 1), ""constant"", 0.0)\n        self.assign_dist = dist.Categorical(logits=padded_assign)\n        self.exists_dist = dist.Bernoulli(logits=exists)\n\n\nclass MarginalAssignmentSparse:\n    """"""\n    A cheap sparse version of :class:`MarginalAssignment`.\n\n    :param int num_detections: the number of detections\n    :param int num_objects: the number of (potentially existing) objects\n    :param torch.LongTensor edges: a ``[2, num_edges]``-shaped tensor of\n        (detection, object) index pairs specifying feasible associations.\n    :param torch.Tensor exists_logits: a tensor of shape ``[num_objects]``\n        representing per-object factors for existence of each potential object.\n    :param torch.Tensor assign_logits: a tensor of shape ``[num_edges]``\n        representing per-edge factors of assignment probability, where each\n        edge denotes that a given detection associates with a single object.\n    :param int bp_iters: optional number of belief propagation iterations. If\n        unspecified or ``None`` an expensive exact algorithm will be used.\n\n    :ivar int num_detections: the number of detections\n    :ivar int num_objects: the number of (potentially existing) objects\n    :ivar pyro.distributions.Bernoulli exists_dist: a mean field posterior\n        distribution over object existence.\n    :ivar pyro.distributions.Categorical assign_dist: a mean field posterior\n        distribution over the object (or None) to which each detection\n        associates.  This has ``.event_shape == (num_objects + 1,)`` where the\n        final element denotes spurious detection, and\n        ``.batch_shape == (num_frames, num_detections)``.\n    """"""\n    def __init__(self, num_objects, num_detections, edges, exists_logits, assign_logits, bp_iters):\n        assert edges.dim() == 2, edges.shape\n        assert edges.shape[0] == 2, edges.shape\n        assert exists_logits.shape == (num_objects,), exists_logits.shape\n        assert assign_logits.shape == edges.shape[1:], assign_logits.shape\n        self.num_objects = num_objects\n        self.num_detections = num_detections\n        self.edges = edges\n\n        # Clamp to avoid NANs.\n        exists_logits = exists_logits.clamp(min=-40, max=40)\n        assign_logits = assign_logits.clamp(min=-40, max=40)\n\n        # This does all the work.\n        exists, assign = compute_marginals_sparse_bp(\n            num_objects, num_detections, edges, exists_logits, assign_logits, bp_iters)\n\n        # Wrap the results in Distribution objects.\n        # This adds a final logit=0 element denoting spurious detection.\n        padded_assign = torch.full((num_detections, num_objects + 1), -float(\'inf\'),\n                                   dtype=assign.dtype, device=assign.device)\n        padded_assign[:, -1] = 0\n        padded_assign[edges[0], edges[1]] = assign\n        self.assign_dist = dist.Categorical(logits=padded_assign)\n        self.exists_dist = dist.Bernoulli(logits=exists)\n\n\nclass MarginalAssignmentPersistent:\n    """"""\n    This computes marginal distributions of a multi-frame multi-object\n    data association problem with an unknown number of persistent objects.\n\n    The inputs are factors in a factor graph (existence probabilites for each\n    potential object and assignment probabilities for each object-detection\n    pair), and the outputs are marginal distributions of posterior existence\n    probability of each potential object and posterior assignment probabilites\n    of each object-detection pair.\n\n    This assumes a shared (maximum) number of detections per frame; to handle\n    variable number of detections, simply set corresponding elements of\n    ``assign_logits`` to ``-float(\'inf\')``.\n\n    :param torch.Tensor exists_logits: a tensor of shape ``[num_objects]``\n        representing per-object factors for existence of each potential object.\n    :param torch.Tensor assign_logits: a tensor of shape\n        ``[num_frames, num_detections, num_objects]`` representing per-edge\n        factors of assignment probability, where each edge denotes that at a\n        given time frame a given detection associates with a single object.\n    :param int bp_iters: optional number of belief propagation iterations. If\n        unspecified or ``None`` an expensive exact algorithm will be used.\n    :param float bp_momentum: optional momentum to use for belief propagation.\n        Should be in the interval ``[0,1)``.\n\n    :ivar int num_frames: the number of time frames\n    :ivar int num_detections: the (maximum) number of detections per frame\n    :ivar int num_objects: the number of (potentially existing) objects\n    :ivar pyro.distributions.Bernoulli exists_dist: a mean field posterior\n        distribution over object existence.\n    :ivar pyro.distributions.Categorical assign_dist: a mean field posterior\n        distribution over the object (or None) to which each detection\n        associates.  This has ``.event_shape == (num_objects + 1,)`` where the\n        final element denotes spurious detection, and\n        ``.batch_shape == (num_frames, num_detections)``.\n    """"""\n    def __init__(self, exists_logits, assign_logits, bp_iters=None, bp_momentum=0.5):\n        assert exists_logits.dim() == 1, exists_logits.shape\n        assert assign_logits.dim() == 3, assign_logits.shape\n        assert assign_logits.shape[-1] == exists_logits.shape[-1]\n        self.num_frames, self.num_detections, self.num_objects = assign_logits.shape\n\n        # Clamp to avoid NANs.\n        exists_logits = exists_logits.clamp(min=-40, max=40)\n        assign_logits = assign_logits.clamp(min=-40, max=40)\n\n        # This does all the work.\n        if bp_iters is None:\n            exists, assign = compute_marginals_persistent(exists_logits, assign_logits)\n        else:\n            exists, assign = compute_marginals_persistent_bp(\n                exists_logits, assign_logits, bp_iters, bp_momentum)\n\n        # Wrap the results in Distribution objects.\n        # This adds a final logit=0 element denoting spurious detection.\n        padded_assign = torch.nn.functional.pad(assign, (0, 1), ""constant"", 0.0)\n        self.assign_dist = dist.Categorical(logits=padded_assign)\n        self.exists_dist = dist.Bernoulli(logits=exists)\n        assert self.assign_dist.batch_shape == (self.num_frames, self.num_detections)\n        assert self.exists_dist.batch_shape == (self.num_objects,)\n\n\ndef compute_marginals(exists_logits, assign_logits):\n    """"""\n    This implements exact inference of pairwise marginals via\n    enumeration. This is very expensive and is only useful for testing.\n\n    See :class:`MarginalAssignment` for args and problem description.\n    """"""\n    num_detections, num_objects = assign_logits.shape\n    assert exists_logits.shape == (num_objects,)\n    dtype = exists_logits.dtype\n    device = exists_logits.device\n\n    exists_probs = torch.zeros(2, num_objects, dtype=dtype, device=device)  # [not exist, exist]\n    assign_probs = torch.zeros(num_detections, num_objects + 1, dtype=dtype, device=device)\n    for assign in itertools.product(range(num_objects + 1), repeat=num_detections):\n        assign_part = sum(assign_logits[j, i] for j, i in enumerate(assign) if i < num_objects)\n        for exists in itertools.product(*[[1] if i in assign else [0, 1] for i in range(num_objects)]):\n            exists_part = sum(exists_logits[i] for i, e in enumerate(exists) if e)\n            prob = _exp(exists_part + assign_part)\n            for i, e in enumerate(exists):\n                exists_probs[e, i] += prob\n            for j, i in enumerate(assign):\n                assign_probs[j, i] += prob\n\n    # Convert from probs to logits.\n    exists = exists_probs.log()\n    assign = assign_probs.log()\n    exists = exists[1] - exists[0]\n    assign = assign[:, :-1] - assign[:, -1:]\n    warn_if_nan(exists, \'exists\')\n    warn_if_nan(assign, \'assign\')\n    return exists, assign\n\n\ndef compute_marginals_bp(exists_logits, assign_logits, bp_iters):\n    """"""\n    This implements approximate inference of pairwise marginals via\n    loopy belief propagation, adapting the approach of [1].\n\n    See :class:`MarginalAssignment` for args and problem description.\n\n    [1] Jason L. Williams, Roslyn A. Lau (2014)\n        Approximate evaluation of marginal association probabilities with\n        belief propagation\n        https://arxiv.org/abs/1209.6299\n    """"""\n    message_e_to_a = torch.zeros_like(assign_logits)\n    message_a_to_e = torch.zeros_like(assign_logits)\n    for i in range(bp_iters):\n        message_e_to_a = -(message_a_to_e - message_a_to_e.sum(0, True) - exists_logits).exp().log1p()\n        joint = (assign_logits + message_e_to_a).exp()\n        message_a_to_e = (assign_logits - torch.log1p(joint.sum(1, True) - joint)).exp().log1p()\n        warn_if_nan(message_e_to_a, \'message_e_to_a iter {}\'.format(i))\n        warn_if_nan(message_a_to_e, \'message_a_to_e iter {}\'.format(i))\n\n    # Convert from probs to logits.\n    exists = exists_logits + message_a_to_e.sum(0)\n    assign = assign_logits + message_e_to_a\n    warn_if_nan(exists, \'exists\')\n    warn_if_nan(assign, \'assign\')\n    return exists, assign\n\n\ndef compute_marginals_sparse_bp(num_objects, num_detections, edges,\n                                exists_logits, assign_logits, bp_iters):\n    """"""\n    This implements approximate inference of pairwise marginals via\n    loopy belief propagation, adapting the approach of [1].\n\n    See :class:`MarginalAssignmentSparse` for args and problem description.\n\n    [1] Jason L. Williams, Roslyn A. Lau (2014)\n        Approximate evaluation of marginal association probabilities with\n        belief propagation\n        https://arxiv.org/abs/1209.6299\n    """"""\n    exists_factor = exists_logits[edges[1]]\n\n    def sparse_sum(x, dim, keepdim=False):\n        assert dim in (0, 1)\n        x = (torch.zeros([num_objects, num_detections][dim], dtype=x.dtype, device=x.device)\n                  .scatter_add_(0, edges[1 - dim], x))\n        if keepdim:\n            x = x[edges[1 - dim]]\n        return x\n\n    message_e_to_a = torch.zeros_like(assign_logits)\n    message_a_to_e = torch.zeros_like(assign_logits)\n    for i in range(bp_iters):\n        message_e_to_a = -(message_a_to_e - sparse_sum(message_a_to_e, 0, True) - exists_factor).exp().log1p()\n        joint = (assign_logits + message_e_to_a).exp()\n        message_a_to_e = (assign_logits - torch.log1p(sparse_sum(joint, 1, True) - joint)).exp().log1p()\n        warn_if_nan(message_e_to_a, \'message_e_to_a iter {}\'.format(i))\n        warn_if_nan(message_a_to_e, \'message_a_to_e iter {}\'.format(i))\n\n    # Convert from probs to logits.\n    exists = exists_logits + sparse_sum(message_a_to_e, 0)\n    assign = assign_logits + message_e_to_a\n    warn_if_nan(exists, \'exists\')\n    warn_if_nan(assign, \'assign\')\n    return exists, assign\n\n\ndef compute_marginals_persistent(exists_logits, assign_logits):\n    """"""\n    This implements exact inference of pairwise marginals via\n    enumeration. This is very expensive and is only useful for testing.\n\n    See :class:`MarginalAssignmentPersistent` for args and problem description.\n    """"""\n    num_frames, num_detections, num_objects = assign_logits.shape\n    assert exists_logits.shape == (num_objects,)\n    dtype = exists_logits.dtype\n    device = exists_logits.device\n\n    total = 0\n    exists_probs = torch.zeros(num_objects, dtype=dtype, device=device)\n    assign_probs = torch.zeros(num_frames, num_detections, num_objects, dtype=dtype, device=device)\n    for exists in itertools.product([0, 1], repeat=num_objects):\n        exists = [i for i, e in enumerate(exists) if e]\n        exists_part = _exp(sum(exists_logits[i] for i in exists))\n\n        # The remaining variables are conditionally independent conditioned on exists.\n        assign_parts = []\n        assign_sums = []\n        for t in range(num_frames):\n            assign_map = {}\n            for n in range(1 + min(len(exists), num_detections)):\n                for objects in itertools.combinations(exists, n):\n                    for detections in itertools.permutations(range(num_detections), n):\n                        assign = tuple(zip(objects, detections))\n                        assign_map[assign] = _exp(sum(assign_logits[t, j, i] for i, j in assign))\n            assign_parts.append(assign_map)\n            assign_sums.append(sum(assign_map.values()))\n\n        prob = exists_part * _product(assign_sums)\n        total += prob\n        for i in exists:\n            exists_probs[i] += prob\n        for t in range(num_frames):\n            other_part = exists_part * _product(assign_sums[:t] + assign_sums[t + 1:])\n            for assign, assign_part in assign_parts[t].items():\n                prob = other_part * assign_part\n                for i, j in assign:\n                    assign_probs[t, j, i] += prob\n\n    # Convert from probs to logits.\n    exists = exists_probs.log() - (total - exists_probs).log()\n    assign = assign_probs.log() - (total - assign_probs.sum(-1, True)).log()\n    warn_if_nan(exists, \'exists\')\n    warn_if_nan(assign, \'assign\')\n    return exists, assign\n\n\ndef compute_marginals_persistent_bp(exists_logits, assign_logits, bp_iters, bp_momentum=0.5):\n    """"""\n    This implements approximate inference of pairwise marginals via\n    loopy belief propagation, adapting the approach of [1], [2].\n\n    See :class:`MarginalAssignmentPersistent` for args and problem description.\n\n    [1] Jason L. Williams, Roslyn A. Lau (2014)\n        Approximate evaluation of marginal association probabilities with\n        belief propagation\n        https://arxiv.org/abs/1209.6299\n    [2] Ryan Turner, Steven Bottone, Bhargav Avasarala (2014)\n        A Complete Variational Tracker\n        https://papers.nips.cc/paper/5572-a-complete-variational-tracker.pdf\n    """"""\n    # This implements forward-backward message passing among three sets of variables:\n    #\n    #   a[t,j] ~ Categorical(num_objects + 1), detection -> object assignment\n    #   b[t,i] ~ Categorical(num_detections + 1), object -> detection assignment\n    #     e[i] ~ Bernonulli, whether each object exists\n    #\n    # Only assign = a and exists = e are returned.\n    assert 0 <= bp_momentum < 1, bp_momentum\n    old, new = bp_momentum, 1 - bp_momentum\n    num_frames, num_detections, num_objects = assign_logits.shape\n    dtype = assign_logits.dtype\n    device = assign_logits.device\n    message_b_to_a = torch.zeros(num_frames, num_detections, num_objects, dtype=dtype, device=device)\n    message_a_to_b = torch.zeros(num_frames, num_detections, num_objects, dtype=dtype, device=device)\n    message_b_to_e = torch.zeros(num_frames, num_objects, dtype=dtype, device=device)\n    message_e_to_b = torch.zeros(num_frames, num_objects, dtype=dtype, device=device)\n\n    for i in range(bp_iters):\n        odds_a = (assign_logits + message_b_to_a).exp()\n        message_a_to_b = (old * message_a_to_b +\n                          new * (assign_logits - (odds_a.sum(2, True) - odds_a).log1p()))\n        message_b_to_e = (old * message_b_to_e +\n                          new * message_a_to_b.exp().sum(1).log1p())\n        message_e_to_b = (old * message_e_to_b +\n                          new * (exists_logits + message_b_to_e.sum(0) - message_b_to_e))\n        odds_b = message_a_to_b.exp()\n        message_b_to_a = (old * message_b_to_a -\n                          new * ((-message_e_to_b).exp().unsqueeze(1) + (1 + odds_b.sum(1, True) - odds_b)).log())\n\n        warn_if_nan(message_a_to_b, \'message_a_to_b iter {}\'.format(i))\n        warn_if_nan(message_b_to_e, \'message_b_to_e iter {}\'.format(i))\n        warn_if_nan(message_e_to_b, \'message_e_to_b iter {}\'.format(i))\n        warn_if_nan(message_b_to_a, \'message_b_to_a iter {}\'.format(i))\n\n    # Convert from probs to logits.\n    exists = exists_logits + message_b_to_e.sum(0)\n    assign = assign_logits + message_b_to_a\n    warn_if_nan(exists, \'exists\')\n    warn_if_nan(assign, \'assign\')\n    return exists, assign\n'"
pyro/contrib/tracking/distributions.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro.distributions as dist\nfrom pyro.distributions.torch_distribution import TorchDistribution\nfrom pyro.contrib.tracking.extended_kalman_filter import EKFState\nfrom pyro.contrib.tracking.measurements import PositionMeasurement\n\n\nclass EKFDistribution(TorchDistribution):\n    r""""""\n    Distribution over EKF states.  See :class:`~pyro.contrib.tracking.extended_kalman_filter.EKFState`.\n    Currently only supports `log_prob`.\n\n    :param x0: PV tensor (mean)\n    :type x0: torch.Tensor\n    :param P0: covariance\n    :type P0: torch.Tensor\n    :param dynamic_model: :class:`~pyro.contrib.tracking.dynamic_models.DynamicModel` object\n    :param measurement_cov: measurement covariance\n    :type measurement_cov: torch.Tensor\n    :param time_steps: number time step\n    :type time_steps: int\n    :param dt: time step\n    :type dt: torch.Tensor\n    """"""\n    arg_constraints = {\'measurement_cov\': constraints.positive_definite,\n                       \'P0\': constraints.positive_definite,\n                       \'x0\': constraints.real_vector}\n    has_rsample = True\n\n    def __init__(self, x0, P0, dynamic_model, measurement_cov, time_steps=1, dt=1., validate_args=None):\n        self.x0 = x0\n        self.P0 = P0\n        self.dynamic_model = dynamic_model\n        self.measurement_cov = measurement_cov\n        self.dt = dt\n        assert not x0.shape[-1] % 2, \'position and velocity vectors must be the same dimension\'\n        batch_shape = x0.shape[:-1]\n        event_shape = (time_steps, x0.shape[-1] // 2)\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    def rsample(self, sample_shape=torch.Size()):\n        raise NotImplementedError(\'TODO: implement forward filter backward sample\')\n\n    def filter_states(self, value):\n        """"""\n        Returns the ekf states given measurements\n\n        :param value: measurement means of shape `(time_steps, event_shape)`\n        :type value: torch.Tensor\n        """"""\n        states = []\n        state = EKFState(self.dynamic_model, self.x0, self.P0, time=0.)\n        assert value.shape[-1] == self.event_shape[-1]\n        for i, measurement_mean in enumerate(value):\n            if i:\n                state = state.predict(self.dt)\n            measurement = PositionMeasurement(measurement_mean, self.measurement_cov,\n                                              time=state.time)\n            state, (dz, S) = state.update(measurement)\n            states.append(state)\n        return states\n\n    def log_prob(self, value):\n        """"""\n        Returns the joint log probability of the innovations of a tensor of measurements\n\n        :param value: measurement means of shape `(time_steps, event_shape)`\n        :type value: torch.Tensor\n        """"""\n        state = EKFState(self.dynamic_model, self.x0, self.P0, time=0.)\n        result = 0.\n        assert value.shape == self.event_shape\n        zero = torch.zeros(self.event_shape[-1], dtype=value.dtype, device=value.device)\n        for i, measurement_mean in enumerate(value):\n            if i:\n                state = state.predict(self.dt)\n            measurement = PositionMeasurement(measurement_mean, self.measurement_cov,\n                                              time=state.time)\n            state, (dz, S) = state.update(measurement)\n            result = result + dist.MultivariateNormal(dz, S).log_prob(zero)\n        return result\n'"
pyro/contrib/tracking/dynamic_models.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABCMeta, abstractmethod\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Parameter\nimport pyro.distributions as dist\nfrom pyro.distributions.util import eye_like\n\n\nclass DynamicModel(nn.Module, metaclass=ABCMeta):\n    \'\'\'\n    Dynamic model interface.\n\n    :param dimension: native state dimension.\n    :param dimension_pv: PV state dimension.\n    :param num_process_noise_parameters: process noise parameter space dimension.\n          This for UKF applications. Can be left as ``None`` for EKF and most\n          other filters.\n    \'\'\'\n    def __init__(self, dimension, dimension_pv, num_process_noise_parameters=None):\n        self._dimension = dimension\n        self._dimension_pv = dimension_pv\n        self._num_process_noise_parameters = num_process_noise_parameters\n        super().__init__()\n\n    @property\n    def dimension(self):\n        \'\'\'\n        Native state dimension access.\n        \'\'\'\n        return self._dimension\n\n    @property\n    def dimension_pv(self):\n        \'\'\'\n        PV state dimension access.\n        \'\'\'\n        return self._dimension_pv\n\n    @property\n    def num_process_noise_parameters(self):\n        \'\'\'\n        Process noise parameters space dimension access.\n        \'\'\'\n        return self._num_process_noise_parameters\n\n    @abstractmethod\n    def forward(self, x, dt, do_normalization=True):\n        \'\'\'\n        Integrate native state ``x`` over time interval ``dt``.\n\n        :param x: current native state. If the DynamicModel is non-differentiable,\n              be sure to handle the case of ``x`` being augmented with process\n              noise parameters.\n        :param dt: time interval to integrate over.\n        :param do_normalization: whether to perform normalization on output, e.g.,\n              mod\'ing angles into an interval.\n        :return: Native state x integrated dt into the future.\n        \'\'\'\n        raise NotImplementedError\n\n    def geodesic_difference(self, x1, x0):\n        \'\'\'\n        Compute and return the geodesic difference between 2 native states.\n        This is a generalization of the Euclidean operation ``x1 - x0``.\n\n        :param x1: native state.\n        :param x0: native state.\n        :return: Geodesic difference between native states ``x1`` and ``x2``.\n        \'\'\'\n        return x1 - x0  # Default to Euclidean behavior.\n\n    @abstractmethod\n    def mean2pv(self, x):\n        \'\'\'\n        Compute and return PV state from native state. Useful for combining\n        state estimates of different types in IMM (Interacting Multiple Model)\n        filtering.\n\n        :param x: native state estimate mean.\n        :return: PV state estimate mean.\n        \'\'\'\n        raise NotImplementedError\n\n    @abstractmethod\n    def cov2pv(self, P):\n        \'\'\'\n        Compute and return PV covariance from native covariance. Useful for\n        combining state estimates of different types in IMM (Interacting\n        Multiple Model) filtering.\n\n        :param P: native state estimate covariance.\n        :return: PV state estimate covariance.\n        \'\'\'\n        raise NotImplementedError\n\n    @abstractmethod\n    def process_noise_cov(self, dt=0.):\n        \'\'\'\n        Compute and return process noise covariance (Q).\n\n        :param dt: time interval to integrate over.\n        :return: Read-only covariance (Q). For a DifferentiableDynamicModel, this is\n            the covariance of the native state ``x`` resulting from stochastic\n            integration (for use with EKF). Otherwise, it is the covariance\n            directly of the process noise parameters (for use with UKF).\n        \'\'\'\n        raise NotImplementedError\n\n    def process_noise_dist(self, dt=0.):\n        \'\'\'\n        Return a distribution object of state displacement from the process noise\n        distribution over a time interval.\n\n        :param dt: time interval that process noise accumulates over.\n        :return: :class:`~pyro.distributions.torch.MultivariateNormal`.\n        \'\'\'\n        Q = self.process_noise_cov(dt)\n        return dist.MultivariateNormal(torch.zeros(Q.shape[-1], dtype=Q.dtype, device=Q.device), Q)\n\n\nclass DifferentiableDynamicModel(DynamicModel):\n    \'\'\'\n    DynamicModel for which state transition Jacobians can be efficiently\n    calculated, usu. analytically or by automatic differentiation.\n    \'\'\'\n    @abstractmethod\n    def jacobian(self, dt):\n        \'\'\'\n        Compute and return native state transition Jacobian (F) over time\n        interval ``dt``.\n\n        :param  dt: time interval to integrate over.\n        :return: Read-only Jacobian (F) of integration map (f).\n        \'\'\'\n        raise NotImplementedError\n\n\nclass Ncp(DifferentiableDynamicModel):\n    \'\'\'\n    NCP (Nearly-Constant Position) dynamic model. May be subclassed, e.g., with\n    CWNV (Continuous White Noise Velocity) or DWNV (Discrete White Noise\n    Velocity).\n\n    :param dimension: native state dimension.\n    :param sv2: variance of velocity. Usually chosen so that the standard\n          deviation is roughly half of the max velocity one would ever expect\n          to observe.\n    \'\'\'\n    def __init__(self, dimension, sv2):\n        dimension_pv = 2 * dimension\n        super().__init__(dimension, dimension_pv, num_process_noise_parameters=1)\n        if not isinstance(sv2, torch.Tensor):\n            sv2 = torch.tensor(sv2)\n        self.sv2 = Parameter(sv2)\n        self._F_cache = eye_like(sv2, dimension)  # State transition matrix cache\n        self._Q_cache = {}  # Process noise cov cache\n\n    def forward(self, x, dt, do_normalization=True):\n        \'\'\'\n        Integrate native state ``x`` over time interval ``dt``.\n\n        :param x: current native state. If the DynamicModel is non-differentiable,\n              be sure to handle the case of ``x`` being augmented with process\n              noise parameters.\n        :param dt: time interval to integrate over.\n            do_normalization: whether to perform normalization on output, e.g.,\n            mod\'ing angles into an interval. Has no effect for this subclass.\n        :return: Native state x integrated dt into the future.\n        \'\'\'\n        return x\n\n    def mean2pv(self, x):\n        \'\'\'\n        Compute and return PV state from native state. Useful for combining\n        state estimates of different types in IMM (Interacting Multiple Model)\n        filtering.\n\n        :param x: native state estimate mean.\n        :return: PV state estimate mean.\n        \'\'\'\n        with torch.no_grad():\n            x_pv = torch.zeros(2 * self._dimension, dtype=x.dtype, device=x.device)\n            x_pv[:self._dimension] = x\n        return x_pv\n\n    def cov2pv(self, P):\n        \'\'\'\n        Compute and return PV covariance from native covariance. Useful for\n        combining state estimates of different types in IMM (Interacting\n        Multiple Model) filtering.\n\n        :param P: native state estimate covariance.\n        :return: PV state estimate covariance.\n        \'\'\'\n        d = 2*self._dimension\n        with torch.no_grad():\n            P_pv = torch.zeros(d, d, dtype=P.dtype, device=P.device)\n            P_pv[:self._dimension, :self._dimension] = P\n        return P_pv\n\n    def jacobian(self, dt):\n        \'\'\'\n        Compute and return cached native state transition Jacobian (F) over\n        time interval ``dt``.\n\n        :param dt: time interval to integrate over.\n        :return: Read-only Jacobian (F) of integration map (f).\n        \'\'\'\n        return self._F_cache\n\n    @abstractmethod\n    def process_noise_cov(self, dt=0.):\n        \'\'\'\n        Compute and return cached process noise covariance (Q).\n\n        :param dt: time interval to integrate over.\n        :return: Read-only covariance (Q) of the native state ``x`` resulting from\n            stochastic integration (for use with EKF).\n        \'\'\'\n        raise NotImplementedError\n\n\nclass Ncv(DifferentiableDynamicModel):\n    \'\'\'\n    NCV (Nearly-Constant Velocity) dynamic model. May be subclassed, e.g., with\n    CWNA (Continuous White Noise Acceleration) or DWNA (Discrete White Noise\n    Acceleration).\n\n    :param dimension: native state dimension.\n    :param sa2: variance of acceleration. Usually chosen so that the standard\n          deviation is roughly half of the max acceleration one would ever\n          expect to observe.\n    \'\'\'\n    def __init__(self, dimension, sa2):\n        dimension_pv = dimension\n        super().__init__(dimension, dimension_pv, num_process_noise_parameters=1)\n        if not isinstance(sa2, torch.Tensor):\n            sa2 = torch.tensor(sa2)\n        self.sa2 = Parameter(sa2)\n        self._F_cache = {}  # State transition matrix cache\n        self._Q_cache = {}  # Process noise cov cache\n\n    def forward(self, x, dt, do_normalization=True):\n        \'\'\'\n        Integrate native state ``x`` over time interval ``dt``.\n\n        :param x: current native state. If the DynamicModel is non-differentiable,\n              be sure to handle the case of ``x`` being augmented with process\n              noise parameters.\n        :param dt: time interval to integrate over.\n        :param do_normalization: whether to perform normalization on output, e.g.,\n              mod\'ing angles into an interval. Has no effect for this subclass.\n\n        :return: Native state x integrated dt into the future.\n        \'\'\'\n        F = self.jacobian(dt)\n        return F.mm(x.unsqueeze(1)).squeeze(1)\n\n    def mean2pv(self, x):\n        \'\'\'\n        Compute and return PV state from native state. Useful for combining\n        state estimates of different types in IMM (Interacting Multiple Model)\n        filtering.\n\n        :param x: native state estimate mean.\n        :return: PV state estimate mean.\n        \'\'\'\n        return x\n\n    def cov2pv(self, P):\n        \'\'\'\n        Compute and return PV covariance from native covariance. Useful for\n        combining state estimates of different types in IMM (Interacting\n        Multiple Model) filtering.\n\n        :param P: native state estimate covariance.\n        :return: PV state estimate covariance.\n        \'\'\'\n        return P\n\n    def jacobian(self, dt):\n        \'\'\'\n        Compute and return cached native state transition Jacobian (F) over\n        time interval ``dt``.\n\n        :param dt: time interval to integrate over.\n        :return: Read-only Jacobian (F) of integration map (f).\n        \'\'\'\n        if dt not in self._F_cache:\n            d = self._dimension\n            with torch.no_grad():\n                F = eye_like(self.sa2, d)\n                F[:d//2, d//2:] = dt * eye_like(self.sa2, d//2)\n            self._F_cache[dt] = F\n\n        return self._F_cache[dt]\n\n    @abstractmethod\n    def process_noise_cov(self, dt=0.):\n        \'\'\'\n        Compute and return cached process noise covariance (Q).\n\n        :param dt: time interval to integrate over.\n        :return: Read-only covariance (Q) of the native state ``x`` resulting from\n            stochastic integration (for use with EKF).\n        \'\'\'\n        raise NotImplementedError\n\n\nclass NcpContinuous(Ncp):\n    \'\'\'\n    NCP (Nearly-Constant Position) dynamic model with CWNV (Continuous White\n    Noise Velocity).\n\n    References:\n        ""Estimation with Applications to Tracking and Navigation"" by Y. Bar-\n        Shalom et al, 2001, p.269.\n\n    :param dimension: native state dimension.\n    :param sv2: variance of velocity. Usually chosen so that the standard\n          deviation is roughly half of the max velocity one would ever expect\n          to observe.\n    \'\'\'\n    def process_noise_cov(self, dt=0.):\n        \'\'\'\n        Compute and return cached process noise covariance (Q).\n\n        :param dt: time interval to integrate over.\n        :return: Read-only covariance (Q) of the native state ``x`` resulting from\n            stochastic integration (for use with EKF).\n        \'\'\'\n        if dt not in self._Q_cache:\n            # q: continuous-time process noise intensity with units\n            #   length^2/time (m^2/s). Choose ``q`` so that changes in position,\n            #   over a sampling period ``dt``, are roughly ``sqrt(q*dt)``.\n            q = self.sv2 * dt\n            Q = q * dt * eye_like(self.sv2, self._dimension)\n            self._Q_cache[dt] = Q\n\n        return self._Q_cache[dt]\n\n\nclass NcvContinuous(Ncv):\n    \'\'\'\n    NCV (Nearly-Constant Velocity) dynamic model with CWNA (Continuous White\n    Noise Acceleration).\n\n    References:\n        ""Estimation with Applications to Tracking and Navigation"" by Y. Bar-\n        Shalom et al, 2001, p.269.\n\n    :param dimension: native state dimension.\n    :param sa2: variance of acceleration. Usually chosen so that the standard\n          deviation is roughly half of the max acceleration one would ever\n          expect to observe.\n    \'\'\'\n    def process_noise_cov(self, dt=0.):\n        \'\'\'\n        Compute and return cached process noise covariance (Q).\n\n        :param dt: time interval to integrate over.\n\n        :return: Read-only covariance (Q) of the native state ``x`` resulting from\n            stochastic integration (for use with EKF).\n        \'\'\'\n        if dt not in self._Q_cache:\n\n            with torch.no_grad():\n                d = self._dimension\n                dt2 = dt * dt\n                dt3 = dt2 * dt\n                Q = torch.zeros(d, d, dtype=self.sa2.dtype, device=self.sa2.device)\n                eye = eye_like(self.sa2, d//2)\n                Q[:d//2, :d//2] = dt3 * eye / 3.0\n                Q[:d//2, d//2:] = dt2 * eye / 2.0\n                Q[d//2:, :d//2] = dt2 * eye / 2.0\n                Q[d//2:, d//2:] = dt * eye\n            # sa2 * dt is an intensity factor that changes in velocity\n            # over a sampling period ``dt``, ideally should be ~``sqrt(q*dt)``.\n            Q = Q * (self.sa2 * dt)\n            self._Q_cache[dt] = Q\n\n        return self._Q_cache[dt]\n\n\nclass NcpDiscrete(Ncp):\n    \'\'\'\n    NCP (Nearly-Constant Position) dynamic model with DWNV (Discrete White\n    Noise Velocity).\n\n    :param dimension: native state dimension.\n    :param sv2: variance of velocity. Usually chosen so that the standard\n          deviation is roughly half of the max velocity one would ever expect\n          to observe.\n\n    References:\n        ""Estimation with Applications to Tracking and Navigation"" by Y. Bar-\n        Shalom et al, 2001, p.273.\n    \'\'\'\n    def process_noise_cov(self, dt=0.):\n        \'\'\'\n        Compute and return cached process noise covariance (Q).\n\n        :param dt: time interval to integrate over.\n        :return: Read-only covariance (Q) of the native state `x` resulting from\n            stochastic integration (for use with EKF).\n        \'\'\'\n        if dt not in self._Q_cache:\n            Q = self.sv2 * dt * dt * eye_like(self.sv2, self._dimension)\n            self._Q_cache[dt] = Q\n\n        return self._Q_cache[dt]\n\n\nclass NcvDiscrete(Ncv):\n    \'\'\'\n    NCV (Nearly-Constant Velocity) dynamic model with DWNA (Discrete White\n    Noise Acceleration).\n\n    :param dimension: native state dimension.\n    :param sa2: variance of acceleration. Usually chosen so that the standard\n          deviation is roughly half of the max acceleration one would ever\n          expect to observe.\n\n    References:\n        ""Estimation with Applications to Tracking and Navigation"" by Y. Bar-\n        Shalom et al, 2001, p.273.\n    \'\'\'\n    def process_noise_cov(self, dt=0.):\n        \'\'\'\n        Compute and return cached process noise covariance (Q).\n\n        :param dt: time interval to integrate over.\n        :return: Read-only covariance (Q) of the native state `x` resulting from\n            stochastic integration (for use with EKF). (Note that this Q, modulo\n            numerical error, has rank `dimension/2`. So, it is only positive\n            semi-definite.)\n        \'\'\'\n        if dt not in self._Q_cache:\n            with torch.no_grad():\n                d = self._dimension\n                dt2 = dt*dt\n                dt3 = dt2*dt\n                dt4 = dt2*dt2\n                Q = torch.zeros(d, d, dtype=self.sa2.dtype, device=self.sa2.device)\n                Q[:d//2, :d//2] = 0.25 * dt4 * eye_like(self.sa2, d//2)\n                Q[:d//2, d//2:] = 0.5 * dt3 * eye_like(self.sa2, d//2)\n                Q[d//2:, :d//2] = 0.5 * dt3 * eye_like(self.sa2, d//2)\n                Q[d//2:, d//2:] = dt2 * eye_like(self.sa2, d//2)\n            Q = Q * self.sa2\n            self._Q_cache[dt] = Q\n\n        return self._Q_cache[dt]\n'"
pyro/contrib/tracking/extended_kalman_filter.py,6,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions.utils import lazy_property\n\nimport pyro.distributions as dist\nfrom pyro.distributions.util import eye_like\n\n\nclass EKFState:\n    '''\n    State-Centric EKF (Extended Kalman Filter) for use with either an NCP\n    (Nearly-Constant Position) or NCV (Nearly-Constant Velocity) target dynamic\n    model. Stores a target dynamic model, state estimate, and state time.\n    Incoming ``Measurement`` provide sensor information for updates.\n\n    .. warning:: For efficiency, the dynamic model is only shallow-copied. Make\n        a deep copy outside as necessary to protect against unexpected changes.\n\n    :param dynamic_model: target dynamic model.\n    :param mean: mean of target state estimate.\n    :param cov: covariance of target state estimate.\n    :param time: time of state estimate.\n    '''\n    def __init__(self, dynamic_model, mean, cov, time=None, frame_num=None):\n        self._dynamic_model = dynamic_model\n        self._mean = mean\n        self._cov = cov\n        if time is None and frame_num is None:\n            raise ValueError('Must provide time or frame_num!')\n        self._time = time\n        self._frame_num = frame_num\n\n    @property\n    def dynamic_model(self):\n        '''\n        Dynamic model access.\n        '''\n        return self._dynamic_model\n\n    @property\n    def dimension(self):\n        '''\n        Native state dimension access.\n        '''\n        return self._dynamic_model.dimension\n\n    @property\n    def mean(self):\n        '''\n        Native state estimate mean access.\n        '''\n        return self._mean\n\n    @property\n    def cov(self):\n        '''\n        Native state estimate covariance access.\n        '''\n        return self._cov\n\n    @property\n    def dimension_pv(self):\n        '''\n        PV state dimension access.\n        '''\n        return self._dynamic_model.dimension_pv\n\n    @lazy_property\n    def mean_pv(self):\n        '''\n        Compute and return cached PV state estimate mean.\n        '''\n        return self._dynamic_model.mean2pv(self._mean)\n\n    @lazy_property\n    def cov_pv(self):\n        '''\n        Compute and return cached PV state estimate covariance.\n        '''\n        return self._dynamic_model.cov2pv(self._cov)\n\n    @property\n    def time(self):\n        '''\n        Continuous State time access.\n        '''\n        return self._time\n\n    @property\n    def frame_num(self):\n        '''\n        Discrete State time access.\n        '''\n        return self._frame_num\n\n    def predict(self, dt=None, destination_time=None, destination_frame_num=None):\n        '''\n        Use dynamic model to predict (aka propagate aka integrate) state\n        estimate in-place.\n\n        :param dt: time to integrate over. The state time will be automatically\n                   incremented this amount unless you provide ``destination_time``.\n                   Using ``destination_time`` may be preferable for prevention of\n                   roundoff error accumulation.\n        :param destination_time: optional value to set continuous state time to\n            after integration. If this is not provided, then\n            `destination_frame_num` must be.\n        :param destination_frame_num: optional value to set discrete state time to\n            after integration. If this is not provided, then\n            `destination_frame_num` must be.\n        '''\n        assert (dt is None) ^ (destination_time is None)\n        if dt is None:\n            dt = destination_time - self._time\n        elif destination_time is None:\n            destination_time = self._time + dt\n        pred_mean = self._dynamic_model(self._mean, dt)\n\n        F = self._dynamic_model.jacobian(dt)\n        Q = self._dynamic_model.process_noise_cov(dt)\n        pred_cov = F.mm(self._cov).mm(F.transpose(-1, -2)) + Q\n\n        if destination_time is None and destination_frame_num is None:\n            raise ValueError('destination_time or destination_frame_num must be specified!')\n\n        return EKFState(self._dynamic_model, pred_mean, pred_cov,\n                        destination_time, destination_frame_num)\n\n    def innovation(self, measurement):\n        '''\n        Compute and return the innovation that a measurement would induce if\n        it were used for an update, but don't actually perform the update.\n        Assumes state and measurement are time-aligned. Useful for computing\n        Chi^2 stats and likelihoods.\n\n        :param measurement: measurement\n        :return: Innovation mean and covariance of hypothetical update.\n        :rtype: tuple(``torch.Tensor``, ``torch.Tensor``)\n        '''\n        assert self._time == measurement.time, \\\n            'State time and measurement time must be aligned!'\n\n        # Compute innovation.\n        x_pv = self._dynamic_model.mean2pv(self._mean)\n        H = measurement.jacobian(x_pv)[:, :self.dimension]\n        R = measurement.cov\n        z = measurement.mean\n        z_predicted = measurement(x_pv)\n        dz = measurement.geodesic_difference(z, z_predicted)\n        S = H.mm(self._cov).mm(H.transpose(-1, -2)) + R  # innovation cov\n\n        return dz, S\n\n    def log_likelihood_of_update(self, measurement):\n        '''\n        Compute and return the likelihood of a potential update, but don't\n        actually perform the update. Assumes state and measurement are time-\n        aligned. Useful for gating and calculating costs in assignment problems\n        for data association.\n\n        :param: measurement.\n        :return: Likelihood of hypothetical update.\n        '''\n        dz, S = self.innovation(measurement)\n        return dist.MultivariateNormal(torch.zeros(S.size(-1), dtype=S.dtype, device=S.device),\n                                       S).log_prob(dz)\n\n    def update(self, measurement):\n        '''\n        Use measurement to update state estimate in-place and return\n        innovation. The innovation is useful, e.g., for evaluating filter\n        consistency or updating model likelihoods when the ``EKFState`` is part\n        of an ``IMMFState``.\n\n        :param: measurement.\n        :returns: EKF State, Innovation mean and covariance.\n        '''\n        if self._time is not None:\n            assert self._time == measurement.time, \\\n                'State time and measurement time must be aligned!'\n        if self._frame_num is not None:\n            assert self._frame_num == measurement.frame_num, \\\n                'State time and measurement time must be aligned!'\n\n        x = self._mean\n        x_pv = self._dynamic_model.mean2pv(x)\n        P = self.cov\n        H = measurement.jacobian(x_pv)[:, :self.dimension]\n        R = measurement.cov\n        z = measurement.mean\n        z_predicted = measurement(x_pv)\n        dz = measurement.geodesic_difference(z, z_predicted)\n        S = H.mm(P).mm(H.transpose(-1, -2)) + R  # innovation cov\n\n        K_prefix = self._cov.mm(H.transpose(-1, -2))\n        dx = K_prefix.mm(torch.solve(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz\n        x = self._dynamic_model.geodesic_difference(x, -dx)\n\n        I = eye_like(x, self._dynamic_model.dimension)  # noqa: E741\n        ImKH = I - K_prefix.mm(torch.solve(H, S)[0])\n        # *Joseph form* of covariance update for numerical stability.\n        P = ImKH.mm(self.cov).mm(ImKH.transpose(-1, -2)) \\\n            + K_prefix.mm(torch.solve((K_prefix.mm(torch.solve(R, S)[0])).transpose(-1, -2),\n                          S)[0])\n\n        pred_mean = x\n        pred_cov = P\n        state = EKFState(self._dynamic_model, pred_mean, pred_cov, self._time, self._frame_num)\n\n        return state, (dz, S)\n"""
pyro/contrib/tracking/hashing.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport heapq\nimport itertools\nfrom collections import defaultdict\nfrom numbers import Number\n\nimport torch\n\n\nclass LSH:\n    """"""\n    Implements locality-sensitive hashing for low-dimensional euclidean space.\n\n\n    Allows to efficiently find neighbours of a point. Provides 2 guarantees:\n\n    -   Difference between coordinates of points not returned by :meth:`nearby`\n        and input point is larger than ``radius``.\n    -   Difference between coordinates of points returned by :meth:`nearby` and\n        input point is smaller than 2 ``radius``.\n\n    Example:\n\n        >>> radius = 1\n        >>> lsh = LSH(radius)\n        >>> a = torch.tensor([-0.51, -0.51]) # hash(a)=(-1,-1)\n        >>> b = torch.tensor([-0.49, -0.49]) # hash(a)=(0,0)\n        >>> c = torch.tensor([1.0, 1.0]) # hash(b)=(1,1)\n        >>> lsh.add(\'a\', a)\n        >>> lsh.add(\'b\', b)\n        >>> lsh.add(\'c\', c)\n        >>> # even though c is within 2radius of a\n        >>> lsh.nearby(\'a\') # doctest: +SKIP\n        {\'b\'}\n        >>> lsh.nearby(\'b\') # doctest: +SKIP\n        {\'a\', \'c\'}\n        >>> lsh.remove(\'b\')\n        >>> lsh.nearby(\'a\') # doctest: +SKIP\n        set()\n\n\n    :param float radius: Scaling parameter used in hash function. Determines the size of the neighbourhood.\n\n    """"""\n    def __init__(self, radius):\n        if not (isinstance(radius, Number) and radius > 0):\n            raise ValueError(""radius must be float greater than 0, given: {}"".format(radius))\n        self._radius = radius\n        self._hash_to_key = defaultdict(set)\n        self._key_to_hash = {}\n\n    def _hash(self, point):\n        coords = (point / self._radius).round()\n        return tuple(map(int, coords))\n\n    def add(self, key, point):\n        """"""\n        Adds (``key``, ``point``) pair to the hash.\n\n\n        :param key: Key used identify ``point``.\n        :param torch.Tensor point: data, should be detached and on cpu.\n        """"""\n        _hash = self._hash(point)\n        if key in self._key_to_hash:\n            self.remove(key)\n        self._key_to_hash[key] = _hash\n        self._hash_to_key[_hash].add(key)\n\n    def remove(self, key):\n        """"""\n        Removes ``key`` and corresponding point from the hash.\n\n\n        Raises :exc:`KeyError` if key is not in hash.\n\n\n        :param key: key used to identify point.\n        """"""\n        _hash = self._key_to_hash.pop(key)\n        self._hash_to_key[_hash].remove(key)\n\n    def nearby(self, key):\n        r""""""\n        Returns a set of keys which are neighbours of the point identified by ``key``.\n\n\n        Two points are nearby if difference of each element of their hashes is smaller than 2. In euclidean space, this\n        corresponds to all points :math:`\\mathbf{p}` where :math:`|\\mathbf{p}_k-(\\mathbf{p_{key}})_k|<r`,\n        and some points (all points not guaranteed) where :math:`|\\mathbf{p}_k-(\\mathbf{p_{key}})_k|<2r`.\n\n\n        :param key: key used to identify input point.\n        :return: a set of keys identifying neighbours of the input point.\n        :rtype: set\n        """"""\n        _hash = self._key_to_hash[key]\n        result = set()\n        for nearby_hash in itertools.product(*[[i - 1, i, i + 1] for i in _hash]):\n            result |= self._hash_to_key[nearby_hash]\n        result.remove(key)\n        return result\n\n\nclass ApproxSet:\n    """"""\n    Queries low-dimensional euclidean space for approximate occupancy.\n\n\n    :param float radius: scaling parameter used in hash function. Determines the size of the bin.\n                         See :class:`LSH` for details.\n    """"""\n    def __init__(self, radius):\n        if not (isinstance(radius, Number) and radius > 0):\n            raise ValueError(""radius must be float greater than 0, given: {}"".format(radius))\n        self._radius = radius\n        self._bins = set()\n\n    def _hash(self, point):\n        coords = (point / self._radius).round()\n        return tuple(map(int, coords))\n\n    def try_add(self, point):\n        """"""\n        Attempts to add ``point`` to set. Only adds there are no points in the ``point``\'s bin.\n\n\n        :param torch.Tensor point: Point to be queried, should be detached and on cpu.\n        :return: ``True`` if point is successfully added, ``False`` if there is already a point in ``point``\'s bin.\n        :rtype: bool\n        """"""\n        _hash = self._hash(point)\n        if _hash in self._bins:\n            return False\n        self._bins.add(_hash)\n        return True\n\n\ndef merge_points(points, radius):\n    """"""\n    Greedily merge points that are closer than given radius.\n\n    This uses :class:`LSH` to achieve complexity that is linear in the number\n    of merged clusters and quadratic in the size of the largest merged cluster.\n\n    :param torch.Tensor points: A tensor of shape ``(K,D)`` where ``K`` is\n        the number of points and ``D`` is the number of dimensions.\n    :param float radius: The minimum distance nearer than which\n        points will be merged.\n    :return: A tuple ``(merged_points, groups)`` where ``merged_points`` is a\n        tensor of shape ``(J,D)`` where ``J <= K``, and ``groups`` is a list of\n        tuples of indices mapping merged points to original points. Note that\n        ``len(groups) == J`` and ``sum(len(group) for group in groups) == K``.\n    :rtype: tuple\n    """"""\n    if points.dim() != 2:\n        raise ValueError(\'Expected points.shape == (K,D), but got {}\'.format(points.shape))\n    if not (isinstance(radius, Number) and radius > 0):\n        raise ValueError(\'Expected radius to be a positive number, but got {}\'.format(radius))\n    radius = 0.99 * radius  # avoid merging points exactly radius apart, e.g. grid points\n    threshold = radius ** 2\n\n    # setup data structures to cheaply search for nearest pairs\n    lsh = LSH(radius)\n    priority_queue = []\n    groups = [(i,) for i in range(len(points))]\n    for i, point in enumerate(points):\n        lsh.add(i, point)\n        for j in lsh.nearby(i):\n            d2 = (point - points[j]).pow(2).sum().item()\n            if d2 < threshold:\n                heapq.heappush(priority_queue, (d2, j, i))\n    if not priority_queue:\n        return points, groups\n\n    # convert from dense to sparse representation\n    next_id = len(points)\n    points = dict(enumerate(points))\n    groups = dict(enumerate(groups))\n\n    # greedily merge\n    while priority_queue:\n        d1, i, j = heapq.heappop(priority_queue)\n        if i not in points or j not in points:\n            continue\n        k = next_id\n        next_id += 1\n        points[k] = (points.pop(i) + points.pop(j)) / 2\n        groups[k] = groups.pop(i) + groups.pop(j)\n        lsh.remove(i)\n        lsh.remove(j)\n        lsh.add(k, points[k])\n        for i in lsh.nearby(k):\n            if i == k:\n                continue\n            d2 = (points[i] - points[k]).pow(2).sum().item()\n            if d2 < threshold:\n                heapq.heappush(priority_queue, (d2, i, k))\n\n    # convert from sparse to dense representation\n    ids = sorted(points.keys())\n    points = torch.stack([points[i] for i in ids])\n    groups = [groups[i] for i in ids]\n\n    return points, groups\n'"
pyro/contrib/tracking/measurements.py,2,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABCMeta, abstractmethod\n\nimport torch\nfrom pyro.distributions.util import eye_like\n\n\nclass Measurement(object, metaclass=ABCMeta):\n    '''\n    Gaussian measurement interface.\n\n    :param mean: mean of measurement distribution.\n    :param cov: covariance of measurement distribution.\n    :param time: continuous time of measurement. If this is not\n          provided, `frame_num` must be.\n    :param frame_num: discrete time of measurement. If this is not\n          provided, `time` must be.\n    '''\n    def __init__(self, mean, cov, time=None, frame_num=None):\n        self._dimension = len(mean)\n        self._mean = mean\n        self._cov = cov\n        if time is None and frame_num is None:\n            raise ValueError('Must provide time or frame_num!')\n        self._time = time\n        self._frame_num = frame_num\n\n    @property\n    def dimension(self):\n        '''\n        Measurement space dimension access.\n        '''\n        return self._dimension\n\n    @property\n    def mean(self):\n        '''\n        Measurement mean (``z`` in most Kalman Filtering literature).\n        '''\n        return self._mean\n\n    @property\n    def cov(self):\n        '''\n        Noise covariance (``R`` in most Kalman Filtering literature).\n        '''\n        return self._cov\n\n    @property\n    def time(self):\n        '''\n        Continuous time of measurement.\n        '''\n        return self._time\n\n    @property\n    def frame_num(self):\n        '''\n        Discrete time of measurement.\n        '''\n        return self._frame_num\n\n    @abstractmethod\n    def __call__(self, x, do_normalization=True):\n        '''\n        Measurement map (h) for predicting a measurement ``z`` from target\n        state ``x``.\n\n        :param x: PV state.\n        :param do_normalization: whether to normalize output, e.g., mod'ing angles\n              into an interval.\n        :return Measurement predicted from state ``x``.\n        '''\n        raise NotImplementedError\n\n    def geodesic_difference(self, z1, z0):\n        '''\n        Compute and return the geodesic difference between 2 measurements.\n        This is a generalization of the Euclidean operation ``z1 - z0``.\n\n        :param z1: measurement.\n        :param z0: measurement.\n        :return: Geodesic difference between ``z1`` and ``z2``.\n        '''\n        return z1 - z0  # Default to Euclidean behavior.\n\n\nclass DifferentiableMeasurement(Measurement):\n    '''\n    Interface for Gaussian measurement for which Jacobians can be efficiently\n    calculated, usu. analytically or by automatic differentiation.\n    '''\n    @abstractmethod\n    def jacobian(self, x=None):\n        '''\n        Compute and return Jacobian (H) of measurement map (h) at target PV\n        state ``x`` .\n\n        :param x: PV state. Use default argument ``None`` when the Jacobian is not\n              state-dependent.\n        :return: Read-only Jacobian (H) of measurement map (h).\n        '''\n        raise NotImplementedError\n\n\nclass PositionMeasurement(DifferentiableMeasurement):\n    '''\n    Full-rank Gaussian position measurement in Euclidean space.\n\n    :param mean: mean of measurement distribution.\n    :param cov: covariance of measurement distribution.\n    :param time: time of measurement.\n    '''\n    def __init__(self, mean, cov, time=None, frame_num=None):\n        super().__init__(mean, cov, time=time, frame_num=frame_num)\n        self._jacobian = torch.cat([\n            eye_like(mean, self.dimension),\n            torch.zeros(self.dimension, self.dimension, dtype=mean.dtype, device=mean.device)], dim=1)\n\n    def __call__(self, x, do_normalization=True):\n        '''\n        Measurement map (h) for predicting a measurement ``z`` from target\n        state ``x``.\n\n        :param x: PV state.\n        :param do_normalization: whether to normalize output. Has no effect for\n              this subclass.\n        :return: Measurement predicted from state ``x``.\n        '''\n        return x[:self._dimension]\n\n    def jacobian(self, x=None):\n        '''\n        Compute and return Jacobian (H) of measurement map (h) at target PV\n        state ``x`` .\n\n        :param x: PV state. The default argument ``None`` may be used in this\n              subclass since the Jacobian is not state-dependent.\n        :return: Read-only Jacobian (H) of measurement map (h).\n        '''\n        return self._jacobian\n"""
pyro/distributions/testing/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis module contains distributions that are useful for testing new Pyro\ninference algorithms.\n""""""\n'"
pyro/distributions/testing/fakes.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.distributions.torch import Beta, Dirichlet, Gamma, Normal\n\n\nclass NonreparameterizedBeta(Beta):\n    has_rsample = False\n\n\nclass NonreparameterizedDirichlet(Dirichlet):\n    has_rsample = False\n\n\nclass NonreparameterizedGamma(Gamma):\n    has_rsample = False\n\n\nclass NonreparameterizedNormal(Normal):\n    has_rsample = False\n'"
pyro/distributions/testing/naive_dirichlet.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.distributions.torch import Beta, Dirichlet, Gamma\nfrom pyro.distributions.util import copy_docs_from\n\n\n@copy_docs_from(Dirichlet)\nclass NaiveDirichlet(Dirichlet):\n    """"""\n    Implementation of ``Dirichlet`` via ``Gamma``.\n\n    This naive implementation has stochastic reparameterized gradients, which\n    have higher variance than PyTorch\'s ``Dirichlet`` implementation.\n    """"""\n    def __init__(self, concentration, validate_args=None):\n        super().__init__(concentration)\n        self._gamma = Gamma(concentration, torch.ones_like(concentration), validate_args=validate_args)\n\n    def rsample(self, sample_shape=torch.Size()):\n        gammas = self._gamma.rsample(sample_shape)\n        return gammas / gammas.sum(-1, True)\n\n\n@copy_docs_from(Beta)\nclass NaiveBeta(Beta):\n    """"""\n    Implementation of ``Beta`` via ``Gamma``.\n\n    This naive implementation has stochastic reparameterized gradients, which\n    have higher variance than PyTorch\'s ``Beta`` implementation.\n    """"""\n    def __init__(self, concentration1, concentration0, validate_args=None):\n        super().__init__(concentration1, concentration0, validate_args=validate_args)\n        alpha_beta = torch.stack([concentration1, concentration0], -1)\n        self._gamma = Gamma(alpha_beta, torch.ones_like(alpha_beta))\n\n    def rsample(self, sample_shape=torch.Size()):\n        gammas = self._gamma.rsample(sample_shape)\n        probs = gammas / gammas.sum(-1, True)\n        return probs[..., 0]\n'"
pyro/distributions/testing/rejection_exponential.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions.utils import broadcast_all\n\nfrom pyro.distributions.rejector import Rejector\nfrom pyro.distributions.torch import Exponential\nfrom pyro.distributions.util import copy_docs_from, weakmethod\n\n\n@copy_docs_from(Exponential)\nclass RejectionExponential(Rejector):\n    def __init__(self, rate, factor):\n        assert (factor <= 1).all()\n        self.rate, self.factor = broadcast_all(rate, factor)\n        propose = Exponential(self.factor * self.rate)\n        log_scale = self.factor.log()\n        super().__init__(propose, self.log_prob_accept, log_scale)\n\n    @weakmethod\n    def log_prob_accept(self, x):\n        result = (self.factor - 1) * self.rate * x\n        assert result.max() <= 0, result.max()\n        return result\n\n    @property\n    def batch_shape(self):\n        return self.rate.shape\n\n    @property\n    def event_shape(self):\n        return torch.Size()\n'"
pyro/distributions/testing/rejection_gamma.py,21,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.distributions.rejector import Rejector\nfrom pyro.distributions.score_parts import ScoreParts\nfrom pyro.distributions.torch import Beta, Dirichlet, Gamma, Normal\nfrom pyro.distributions.util import copy_docs_from, weakmethod\n\n\n@copy_docs_from(Gamma)\nclass RejectionStandardGamma(Rejector):\n    """"""\n    Naive Marsaglia & Tsang rejection sampler for standard Gamma distibution.\n    This assumes `concentration >= 1` and does not boost `concentration` or augment shape.\n    """"""\n    def __init__(self, concentration):\n        if concentration.data.min() < 1:\n            raise NotImplementedError(\'concentration < 1 is not supported\')\n        self.concentration = concentration\n        self._standard_gamma = Gamma(concentration, concentration.new([1.]).squeeze().expand_as(concentration))\n        # The following are Marsaglia & Tsang\'s variable names.\n        self._d = self.concentration - 1.0 / 3.0\n        self._c = 1.0 / torch.sqrt(9.0 * self._d)\n        # Compute log scale using Gamma.log_prob().\n        x = self._d.detach()  # just an arbitrary x.\n        log_scale = self.propose_log_prob(x) + self.log_prob_accept(x) - self.log_prob(x)\n        super().__init__(self.propose, self.log_prob_accept, log_scale,\n                         batch_shape=concentration.shape, event_shape=())\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(RejectionStandardGamma, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new.concentration = self.concentration.expand(batch_shape)\n        new._standard_gamma = self._standard_gamma.expand(batch_shape)\n        new._d = self._d.expand(batch_shape)\n        new._c = self._c.expand(batch_shape)\n        # Compute log scale using Gamma.log_prob().\n        x = new._d.detach()  # just an arbitrary x.\n        log_scale = new.propose_log_prob(x) + new.log_prob_accept(x) - new.log_prob(x)\n        super(RejectionStandardGamma, new).__init__(new.propose, new.log_prob_accept, log_scale,\n                                                    batch_shape=batch_shape, event_shape=())\n        new._validate_args = self._validate_args\n        return new\n\n    @weakmethod\n    def propose(self, sample_shape=torch.Size()):\n        # Marsaglia & Tsang\'s x == Naesseth\'s epsilon`\n        x = torch.randn(sample_shape + self.concentration.shape,\n                        dtype=self.concentration.dtype,\n                        device=self.concentration.device)\n        y = 1.0 + self._c * x\n        v = y * y * y\n        return (self._d * v).clamp_(1e-30, 1e30)\n\n    def propose_log_prob(self, value):\n        v = value / self._d\n        result = -self._d.log()\n        y = v.pow(1 / 3)\n        result -= torch.log(3 * y ** 2)\n        x = (y - 1) / self._c\n        result -= self._c.log()\n        result += Normal(torch.zeros_like(self.concentration), torch.ones_like(self.concentration)).log_prob(x)\n        return result\n\n    @weakmethod\n    def log_prob_accept(self, value):\n        v = value / self._d\n        y = torch.pow(v, 1.0 / 3.0)\n        x = (y - 1.0) / self._c\n        log_prob_accept = 0.5 * x * x + self._d * (1.0 - v + torch.log(v))\n        log_prob_accept[y <= 0] = -float(\'inf\')\n        return log_prob_accept\n\n    def log_prob(self, x):\n        return self._standard_gamma.log_prob(x)\n\n\n@copy_docs_from(Gamma)\nclass RejectionGamma(Gamma):\n    has_rsample = True\n\n    def __init__(self, concentration, rate, validate_args=None):\n        super().__init__(concentration, rate, validate_args=validate_args)\n        self._standard_gamma = RejectionStandardGamma(concentration)\n        self.rate = rate\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(RejectionGamma, _instance)\n        new = super().expand(batch_shape, new)\n        new._standard_gamma = self._standard_gamma.expand(batch_shape)\n        new._validate_args = self._validate_args\n        return new\n\n    def rsample(self, sample_shape=torch.Size()):\n        return self._standard_gamma.rsample(sample_shape) / self.rate\n\n    def log_prob(self, x):\n        return self._standard_gamma.log_prob(x * self.rate) + torch.log(self.rate)\n\n    def score_parts(self, x):\n        log_prob, score_function, _ = self._standard_gamma.score_parts(x * self.rate)\n        log_prob = log_prob + torch.log(self.rate)\n        return ScoreParts(log_prob, score_function, log_prob)\n\n\n@copy_docs_from(Gamma)\nclass ShapeAugmentedGamma(Gamma):\n    """"""\n    This implements the shape augmentation trick of\n    Naesseth, Ruiz, Linderman, Blei (2017) https://arxiv.org/abs/1610.05683\n    """"""\n    has_rsample = True\n\n    def __init__(self, concentration, rate, boost=1, validate_args=None):\n        if concentration.min() + boost < 1:\n            raise ValueError(\'Need to boost at least once for concentration < 1\')\n        super().__init__(concentration, rate, validate_args=validate_args)\n        self.concentration = concentration\n        self._boost = boost\n        self._rejection_gamma = RejectionGamma(concentration + boost, rate)\n        self._unboost_x_cache = None, None\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(ShapeAugmentedGamma, _instance)\n        new = super().expand(batch_shape, new)\n        batch_shape = torch.Size(batch_shape)\n        new.concentration = self.concentration.expand(batch_shape)\n        new._boost = self._boost\n        new._rejection_gamma = self._rejection_gamma.expand(batch_shape)\n        new._validate_args = self._validate_args\n        return new\n\n    def rsample(self, sample_shape=torch.Size()):\n        x = self._rejection_gamma.rsample(sample_shape)\n        boosted_x = x.clone()\n        for i in range(self._boost):\n            u = torch.rand(x.shape, dtype=x.dtype, device=x.device)\n            boosted_x *= (1 - u) ** (1 / (i + self.concentration))\n        self._unboost_x_cache = boosted_x, x\n        return boosted_x\n\n    def score_parts(self, boosted_x=None):\n        if boosted_x is None:\n            boosted_x = self._unboost_x_cache[0]\n        assert boosted_x is self._unboost_x_cache[0]\n        x = self._unboost_x_cache[1]\n        _, score_function, _ = self._rejection_gamma.score_parts(x)\n        log_prob = self.log_prob(boosted_x)\n        return ScoreParts(log_prob, score_function, log_prob)\n\n\n@copy_docs_from(Dirichlet)\nclass ShapeAugmentedDirichlet(Dirichlet):\n    """"""\n    Implementation of ``Dirichlet`` via ``ShapeAugmentedGamma``.\n\n    This naive implementation has stochastic reparameterized gradients, which\n    have higher variance than PyTorch\'s ``Dirichlet`` implementation.\n    """"""\n    def __init__(self, concentration, boost=1, validate_args=None):\n        super().__init__(concentration, validate_args=validate_args)\n        self._gamma = ShapeAugmentedGamma(concentration, torch.ones_like(concentration), boost)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(ShapeAugmentedDirichlet, _instance)\n        new = super().expand(batch_shape, new)\n        batch_shape = torch.Size(batch_shape)\n        new._gamma = self._gamma.expand(batch_shape + self._gamma.concentration.shape[-1:])\n        new._validate_args = self._validate_args\n        return new\n\n    def rsample(self, sample_shape=torch.Size()):\n        gammas = self._gamma.rsample(sample_shape)\n        return gammas / gammas.sum(-1, True)\n\n\n@copy_docs_from(Beta)\nclass ShapeAugmentedBeta(Beta):\n    """"""\n    Implementation of ``rate`` via ``ShapeAugmentedGamma``.\n\n    This naive implementation has stochastic reparameterized gradients, which\n    have higher variance than PyTorch\'s ``rate`` implementation.\n    """"""\n    def __init__(self, concentration1, concentration0, boost=1, validate_args=None):\n        super().__init__(concentration1, concentration0, validate_args=validate_args)\n        alpha_beta = torch.stack([concentration1, concentration0], -1)\n        self._gamma = ShapeAugmentedGamma(alpha_beta, torch.ones_like(alpha_beta), boost)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(ShapeAugmentedBeta, _instance)\n        new = super().expand(batch_shape, new)\n        batch_shape = torch.Size(batch_shape)\n        new._gamma = self._gamma.expand(batch_shape + self._gamma.concentration.shape[-1:])\n        new._validate_args = self._validate_args\n        return new\n\n    def rsample(self, sample_shape=torch.Size()):\n        gammas = self._gamma.rsample(sample_shape)\n        probs = gammas / gammas.sum(-1, True)\n        return probs[..., 0]\n'"
pyro/distributions/transforms/__init__.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch.distributions import biject_to, transform_to\nfrom torch.distributions.transforms import *  # noqa F403\nfrom torch.distributions.transforms import __all__ as torch_transforms\n\nfrom pyro.distributions.constraints import IndependentConstraint, corr_cholesky_constraint\nfrom pyro.distributions.torch_transform import ComposeTransformModule\nfrom pyro.distributions.transforms.affine_autoregressive import (AffineAutoregressive, ConditionalAffineAutoregressive,\n                                                                 affine_autoregressive,\n                                                                 conditional_affine_autoregressive)\nfrom pyro.distributions.transforms.affine_coupling import (AffineCoupling, ConditionalAffineCoupling, affine_coupling,\n                                                           conditional_affine_coupling)\nfrom pyro.distributions.transforms.basic import ELUTransform, LeakyReLUTransform, elu, leaky_relu\nfrom pyro.distributions.transforms.batchnorm import BatchNorm, batchnorm\nfrom pyro.distributions.transforms.block_autoregressive import BlockAutoregressive, block_autoregressive\nfrom pyro.distributions.transforms.cholesky import CorrLCholeskyTransform\nfrom pyro.distributions.transforms.discrete_cosine import DiscreteCosineTransform\nfrom pyro.distributions.transforms.generalized_channel_permute import (ConditionalGeneralizedChannelPermute,\n                                                                       GeneralizedChannelPermute,\n                                                                       conditional_generalized_channel_permute,\n                                                                       generalized_channel_permute)\nfrom pyro.distributions.transforms.haar import HaarTransform\nfrom pyro.distributions.transforms.householder import (ConditionalHouseholder, Householder, conditional_householder,\n                                                       householder)\nfrom pyro.distributions.transforms.lower_cholesky_affine import LowerCholeskyAffine\nfrom pyro.distributions.transforms.neural_autoregressive import (ConditionalNeuralAutoregressive, NeuralAutoregressive,\n                                                                 conditional_neural_autoregressive,\n                                                                 neural_autoregressive)\nfrom pyro.distributions.transforms.permute import Permute, permute\nfrom pyro.distributions.transforms.planar import ConditionalPlanar, Planar, conditional_planar, planar\nfrom pyro.distributions.transforms.polynomial import Polynomial, polynomial\nfrom pyro.distributions.transforms.radial import ConditionalRadial, Radial, conditional_radial, radial\nfrom pyro.distributions.transforms.spline import ConditionalSpline, Spline, conditional_spline, spline\nfrom pyro.distributions.transforms.sylvester import Sylvester, sylvester\n\n########################################\n# register transforms\n\nbiject_to.register(IndependentConstraint, lambda c: biject_to(c.base_constraint))\ntransform_to.register(IndependentConstraint, lambda c: transform_to(c.base_constraint))\n\n\n@biject_to.register(corr_cholesky_constraint)\n@transform_to.register(corr_cholesky_constraint)\ndef _transform_to_corr_cholesky(constraint):\n    return CorrLCholeskyTransform()\n\n\ndef iterated(repeats, base_fn, *args, **kwargs):\n    """"""\n    Helper function to compose a sequence of bijective transforms with potentially\n    learnable parameters using :class:`~pyro.distributions.ComposeTransformModule`.\n\n    :param repeats: number of repeated transforms.\n    :param base_fn: function to construct the bijective transform.\n    :param args: arguments taken by `base_fn`.\n    :param kwargs: keyword arguments taken by `base_fn`.\n    :return: instance of :class:`~pyro.distributions.TransformModule`.\n    """"""\n    assert isinstance(repeats, int) and repeats >= 1\n    return ComposeTransformModule([base_fn(*args, **kwargs) for _ in range(repeats)])\n\n\n__all__ = [\n    \'iterated\',\n    \'AffineAutoregressive\',\n    \'AffineCoupling\',\n    \'BatchNorm\',\n    \'BlockAutoregressive\',\n    \'ComposeTransformModule\',\n    \'ConditionalAffineAutoregressive\',\n    \'ConditionalAffineCoupling\',\n    \'ConditionalGeneralizedChannelPermute\',\n    \'ConditionalHouseholder\',\n    \'ConditionalNeuralAutoregressive\',\n    \'ConditionalPlanar\',\n    \'ConditionalRadial\',\n    \'ConditionalSpline\',\n    \'CorrLCholeskyTransform\',\n    \'DiscreteCosineTransform\',\n    \'ELUTransform\',\n    \'GeneralizedChannelPermute\',\n    \'HaarTransform\',\n    \'Householder\',\n    \'LeakyReLUTransform\',\n    \'LowerCholeskyAffine\',\n    \'NeuralAutoregressive\',\n    \'Permute\',\n    \'Planar\',\n    \'Polynomial\',\n    \'Radial\',\n    \'Spline\',\n    \'Sylvester\',\n    \'affine_autoregressive\',\n    \'affine_coupling\',\n    \'batchnorm\',\n    \'block_autoregressive\',\n    \'conditional_affine_autoregressive\',\n    \'conditional_affine_coupling\',\n    \'conditional_generalized_channel_permute\',\n    \'conditional_householder\',\n    \'conditional_neural_autoregressive\',\n    \'conditional_planar\',\n    \'conditional_radial\',\n    \'conditional_spline\',\n    \'elu\',\n    \'generalized_channel_permute\',\n    \'householder\',\n    \'leaky_relu\',\n    \'neural_autoregressive\',\n    \'permute\',\n    \'planar\',\n    \'polynomial\',\n    \'radial\',\n    \'spline\',\n    \'sylvester\'\n]\n\n__all__.extend(torch_transforms)\ndel torch_transforms\n'"
pyro/distributions/transforms/affine_autoregressive.py,19,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.conditional import ConditionalTransformModule\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.transforms.utils import clamp_preserve_gradients\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import AutoRegressiveNN, ConditionalAutoRegressiveNN\n\n\n@copy_docs_from(TransformModule)\nclass AffineAutoregressive(TransformModule):\n    r""""""\n    An implementation of the bijective transform of Inverse Autoregressive Flow\n    (IAF), using by default Eq (10) from Kingma Et Al., 2016,\n\n        :math:`\\mathbf{y} = \\mu_t + \\sigma_t\\odot\\mathbf{x}`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    :math:`\\mu_t,\\sigma_t` are calculated from an autoregressive network on\n    :math:`\\mathbf{x}`, and :math:`\\sigma_t>0`.\n\n    If the stable keyword argument is set to True then the transformation used is,\n\n        :math:`\\mathbf{y} = \\sigma_t\\odot\\mathbf{x} + (1-\\sigma_t)\\odot\\mu_t`\n\n    where :math:`\\sigma_t` is restricted to :math:`(0,1)`. This variant of IAF is\n    claimed by the authors to be more numerically stable than one using Eq (10),\n    although in practice it leads to a restriction on the distributions that can be\n    represented, presumably since the input is restricted to rescaling by a number\n    on :math:`(0,1)`.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` this provides\n    a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> from pyro.nn import AutoRegressiveNN\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> transform = AffineAutoregressive(AutoRegressiveNN(10, [40]))\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    The inverse of the Bijector is required when, e.g., scoring the log density of a\n    sample with :class:`~pyro.distributions.TransformedDistribution`. This\n    implementation caches the inverse of the Bijector when its forward operation is\n    called, e.g., when sampling from\n    :class:`~pyro.distributions.TransformedDistribution`. However, if the cached\n    value isn\'t available, either because it was overwritten during sampling a new\n    value or an arbitrary value is being scored, it will calculate it manually. Note\n    that this is an operation that scales as O(D) where D is the input dimension,\n    and so should be avoided for large dimensional uses. So in general, it is cheap\n    to sample from IAF and score a value that was sampled by IAF, but expensive to\n    score an arbitrary value.\n\n    :param autoregressive_nn: an autoregressive neural network whose forward call\n        returns a real-valued mean and logit-scale as a tuple\n    :type autoregressive_nn: nn.Module\n    :param log_scale_min_clip: The minimum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_min_clip: float\n    :param log_scale_max_clip: The maximum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_max_clip: float\n    :param sigmoid_bias: A term to add the logit of the input when using the stable\n        tranform.\n    :type sigmoid_bias: float\n    :param stable: When true, uses the alternative ""stable"" version of the transform\n        (see above).\n    :type stable: bool\n\n    References:\n\n    [1] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever,\n    Max Welling. Improving Variational Inference with Inverse Autoregressive Flow.\n    [arXiv:1606.04934]\n\n    [2] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with\n    Normalizing Flows. [arXiv:1505.05770]\n\n    [3] Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle. MADE: Masked\n    Autoencoder for Distribution Estimation. [arXiv:1502.03509]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n    sign = +1\n    autoregressive = True\n\n    def __init__(\n            self,\n            autoregressive_nn,\n            log_scale_min_clip=-5.,\n            log_scale_max_clip=3.,\n            sigmoid_bias=2.0,\n            stable=False\n    ):\n        super().__init__(cache_size=1)\n        self.arn = autoregressive_nn\n        self._cached_log_scale = None\n        self.log_scale_min_clip = log_scale_min_clip\n        self.log_scale_max_clip = log_scale_max_clip\n        self.sigmoid = nn.Sigmoid()\n        self.logsigmoid = nn.LogSigmoid()\n        self.sigmoid_bias = sigmoid_bias\n        self.stable = stable\n\n        if stable:\n            self._call = self._call_stable\n            self._inverse = self._inverse_stable\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        mean, log_scale = self.arn(x)\n        log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)\n        self._cached_log_scale = log_scale\n        scale = torch.exp(log_scale)\n\n        y = scale * x + mean\n        return y\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x. Uses a previously cached inverse if available, otherwise\n        performs the inversion afresh.\n        """"""\n        x_size = y.size()[:-1]\n        perm = self.arn.permutation\n        input_dim = y.size(-1)\n        x = [torch.zeros(x_size, device=y.device)] * input_dim\n\n        # NOTE: Inversion is an expensive operation that scales in the dimension of the input\n        for idx in perm:\n            mean, log_scale = self.arn(torch.stack(x, dim=-1))\n            inverse_scale = torch.exp(-clamp_preserve_gradients(\n                log_scale[..., idx], min=self.log_scale_min_clip, max=self.log_scale_max_clip))\n            mean = mean[..., idx]\n            x[idx] = (y[..., idx] - mean) * inverse_scale\n\n        x = torch.stack(x, dim=-1)\n        log_scale = clamp_preserve_gradients(log_scale, min=self.log_scale_min_clip, max=self.log_scale_max_clip)\n        self._cached_log_scale = log_scale\n        return x\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian\n        """"""\n        x_old, y_old = self._cached_x_y\n        if x is not x_old or y is not y_old:\n            # This call to the parent class Transform will update the cache\n            # as well as calling self._call and recalculating y and log_detJ\n            self(x)\n\n        if self._cached_log_scale is not None:\n            log_scale = self._cached_log_scale\n        elif not self.stable:\n            _, log_scale = self.arn(x)\n            log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)\n        else:\n            _, logit_scale = self.arn(x)\n            log_scale = self.logsigmoid(logit_scale + self.sigmoid_bias)\n        return log_scale.sum(-1)\n\n    def _call_stable(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        mean, logit_scale = self.arn(x)\n        logit_scale = logit_scale + self.sigmoid_bias\n        scale = self.sigmoid(logit_scale)\n        log_scale = self.logsigmoid(logit_scale)\n        self._cached_log_scale = log_scale\n\n        y = scale * x + (1 - scale) * mean\n        return y\n\n    def _inverse_stable(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x.\n        """"""\n        x_size = y.size()[:-1]\n        perm = self.arn.permutation\n        input_dim = y.size(-1)\n        x = [torch.zeros(x_size, device=y.device)] * input_dim\n\n        # NOTE: Inversion is an expensive operation that scales in the dimension of the input\n        for idx in perm:\n            mean, logit_scale = self.arn(torch.stack(x, dim=-1))\n            inverse_scale = 1 + torch.exp(-logit_scale[..., idx] - self.sigmoid_bias)\n            x[idx] = inverse_scale * y[..., idx] + (1 - inverse_scale) * mean[..., idx]\n\n        self._cached_log_scale = self.logsigmoid(logit_scale + self.sigmoid_bias)\n        x = torch.stack(x, dim=-1)\n        return x\n\n\n@copy_docs_from(ConditionalTransformModule)\nclass ConditionalAffineAutoregressive(ConditionalTransformModule):\n    r""""""\n    An implementation of the bijective transform of Inverse Autoregressive Flow\n    (IAF) that conditions on an additional context variable and uses, by default,\n    Eq (10) from Kingma Et Al., 2016,\n\n        :math:`\\mathbf{y} = \\mu_t + \\sigma_t\\odot\\mathbf{x}`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    :math:`\\mu_t,\\sigma_t` are calculated from an autoregressive network on\n    :math:`\\mathbf{x}` and context :math:`\\mathbf{z}\\in\\mathbb{R}^M`, and\n    :math:`\\sigma_t>0`.\n\n    If the stable keyword argument is set to True then the transformation used is,\n\n        :math:`\\mathbf{y} = \\sigma_t\\odot\\mathbf{x} + (1-\\sigma_t)\\odot\\mu_t`\n\n    where :math:`\\sigma_t` is restricted to :math:`(0,1)`. This variant of IAF is\n    claimed by the authors to be more numerically stable than one using Eq (10),\n    although in practice it leads to a restriction on the distributions that can be\n    represented, presumably since the input is restricted to rescaling by a number\n    on :math:`(0,1)`.\n\n    Together with :class:`~pyro.distributions.ConditionalTransformedDistribution`\n    this provides a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> from pyro.nn import ConditionalAutoRegressiveNN\n    >>> input_dim = 10\n    >>> context_dim = 4\n    >>> batch_size = 3\n    >>> hidden_dims = [10*input_dim, 10*input_dim]\n    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n    >>> hypernet = ConditionalAutoRegressiveNN(input_dim, context_dim, hidden_dims)\n    >>> transform = ConditionalAffineAutoregressive(hypernet)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> z = torch.rand(batch_size, context_dim)\n    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,\n    ... [transform]).condition(z)\n    >>> flow_dist.sample(sample_shape=torch.Size([batch_size]))  # doctest: +SKIP\n\n    The inverse of the Bijector is required when, e.g., scoring the log density of a\n    sample with :class:`~pyro.distributions.TransformedDistribution`. This\n    implementation caches the inverse of the Bijector when its forward operation is\n    called, e.g., when sampling from\n    :class:`~pyro.distributions.TransformedDistribution`. However, if the cached\n    value isn\'t available, either because it was overwritten during sampling a new\n    value or an arbitrary value is being scored, it will calculate it manually. Note\n    that this is an operation that scales as O(D) where D is the input dimension,\n    and so should be avoided for large dimensional uses. So in general, it is cheap\n    to sample from IAF and score a value that was sampled by IAF, but expensive to\n    score an arbitrary value.\n\n    :param autoregressive_nn: an autoregressive neural network whose forward call\n        returns a real-valued mean and logit-scale as a tuple\n    :type autoregressive_nn: nn.Module\n    :param log_scale_min_clip: The minimum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_min_clip: float\n    :param log_scale_max_clip: The maximum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_max_clip: float\n    :param sigmoid_bias: A term to add the logit of the input when using the stable\n        tranform.\n    :type sigmoid_bias: float\n    :param stable: When true, uses the alternative ""stable"" version of the transform\n        (see above).\n    :type stable: bool\n\n    References:\n\n    [1] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever,\n    Max Welling. Improving Variational Inference with Inverse Autoregressive Flow.\n    [arXiv:1606.04934]\n\n    [2] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with\n    Normalizing Flows. [arXiv:1505.05770]\n\n    [3] Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle. MADE: Masked\n    Autoencoder for Distribution Estimation. [arXiv:1502.03509]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, autoregressive_nn, **kwargs):\n        super().__init__()\n        self.nn = autoregressive_nn\n        self.kwargs = kwargs\n\n    def condition(self, context):\n        """"""\n        Conditions on a context variable, returning a non-conditional transform of\n        of type :class:`~pyro.distributions.transforms.AffineAutoregressive`.\n        """"""\n\n        cond_nn = partial(self.nn, context=context)\n        cond_nn.permutation = cond_nn.func.permutation\n        cond_nn.get_permutation = cond_nn.func.get_permutation\n        return AffineAutoregressive(cond_nn, **self.kwargs)\n\n\ndef affine_autoregressive(input_dim, hidden_dims=None, **kwargs):\n    """"""\n    A helper function to create an\n    :class:`~pyro.distributions.transforms.AffineAutoregressive` object that takes\n    care of constructing an autoregressive network with the correct input/output\n    dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param hidden_dims: The desired hidden dimensions of the autoregressive network.\n        Defaults to using [3*input_dim + 1]\n    :type hidden_dims: list[int]\n    :param log_scale_min_clip: The minimum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_min_clip: float\n    :param log_scale_max_clip: The maximum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_max_clip: float\n    :param sigmoid_bias: A term to add the logit of the input when using the stable\n        tranform.\n    :type sigmoid_bias: float\n    :param stable: When true, uses the alternative ""stable"" version of the transform\n        (see above).\n    :type stable: bool\n\n    """"""\n\n    if hidden_dims is None:\n        hidden_dims = [3 * input_dim + 1]\n    arn = AutoRegressiveNN(input_dim, hidden_dims)\n    return AffineAutoregressive(arn, **kwargs)\n\n\ndef conditional_affine_autoregressive(input_dim, context_dim, hidden_dims=None, **kwargs):\n    """"""\n    A helper function to create an\n    :class:`~pyro.distributions.transforms.ConditionalAffineAutoregressive` object\n    that takes care of constructing a dense network with the correct input/output\n    dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param context_dim: Dimension of context variable\n    :type context_dim: int\n    :param hidden_dims: The desired hidden dimensions of the dense network. Defaults\n        to using [10*input_dim]\n    :type hidden_dims: list[int]\n    :param log_scale_min_clip: The minimum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_min_clip: float\n    :param log_scale_max_clip: The maximum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_max_clip: float\n    :param sigmoid_bias: A term to add the logit of the input when using the stable\n        tranform.\n    :type sigmoid_bias: float\n    :param stable: When true, uses the alternative ""stable"" version of the transform\n        (see above).\n    :type stable: bool\n\n    """"""\n    if hidden_dims is None:\n        hidden_dims = [10 * input_dim]\n    nn = ConditionalAutoRegressiveNN(input_dim, context_dim, hidden_dims)\n    return ConditionalAffineAutoregressive(nn, **kwargs)\n'"
pyro/distributions/transforms/affine_coupling.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport operator\nfrom functools import partial, reduce\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.utils import _sum_rightmost\n\nfrom pyro.distributions.conditional import ConditionalTransformModule\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.transforms.utils import clamp_preserve_gradients\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import ConditionalDenseNN, DenseNN\n\n\n@copy_docs_from(TransformModule)\nclass AffineCoupling(TransformModule):\n    r""""""\n    An implementation of the affine coupling layer of RealNVP (Dinh et al., 2017)\n    that uses the bijective transform,\n\n        :math:`\\mathbf{y}_{1:d} = \\mathbf{x}_{1:d}`\n        :math:`\\mathbf{y}_{(d+1):D} = \\mu + \\sigma\\odot\\mathbf{x}_{(d+1):D}`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    e.g. :math:`\\mathbf{x}_{1:d}` represents the first :math:`d` elements of the\n    inputs, and :math:`\\mu,\\sigma` are shift and translation parameters calculated\n    as the output of a function inputting only :math:`\\mathbf{x}_{1:d}`.\n\n    That is, the first :math:`d` components remain unchanged, and the subsequent\n    :math:`D-d` are shifted and translated by a function of the previous components.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` this provides\n    a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> from pyro.nn import DenseNN\n    >>> input_dim = 10\n    >>> split_dim = 6\n    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n    >>> param_dims = [input_dim-split_dim, input_dim-split_dim]\n    >>> hypernet = DenseNN(split_dim, [10*input_dim], param_dims)\n    >>> transform = AffineCoupling(split_dim, hypernet)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    The inverse of the Bijector is required when, e.g., scoring the log density of a\n    sample with :class:`~pyro.distributions.TransformedDistribution`. This\n    implementation caches the inverse of the Bijector when its forward operation is\n    called, e.g., when sampling from\n    :class:`~pyro.distributions.TransformedDistribution`. However, if the cached\n    value isn\'t available, either because it was overwritten during sampling a new\n    value or an arbitary value is being scored, it will calculate it manually.\n\n    This is an operation that scales as O(1), i.e. constant in the input dimension.\n    So in general, it is cheap to sample *and* score (an arbitrary value) from\n    :class:`~pyro.distributions.transforms.AffineCoupling`.\n\n    :param split_dim: Zero-indexed dimension :math:`d` upon which to perform input/\n        output split for transformation.\n    :type split_dim: int\n    :param hypernet: an autoregressive neural network whose forward call returns a\n        real-valued mean and logit-scale as a tuple. The input should have final\n        dimension split_dim and the output final dimension input_dim-split_dim for\n        each member of the tuple.\n    :type hypernet: callable\n    :param dim: the tensor dimension on which to split. This value must be negative\n        and defines the event dim as `abs(dim)`.\n    :type dim: int\n    :param log_scale_min_clip: The minimum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_min_clip: float\n    :param log_scale_max_clip: The maximum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_max_clip: float\n\n    References:\n\n    [1] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation\n    using Real NVP. ICLR 2017.\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n\n    def __init__(self, split_dim, hypernet, *, dim=-1, log_scale_min_clip=-5., log_scale_max_clip=3.):\n        super().__init__(cache_size=1)\n        if dim >= 0:\n            raise ValueError(""\'dim\' keyword argument must be negative"")\n\n        self.split_dim = split_dim\n        self.nn = hypernet\n        self.dim = dim\n        self.event_dim = -dim\n        self._cached_log_scale = None\n        self.log_scale_min_clip = log_scale_min_clip\n        self.log_scale_max_clip = log_scale_max_clip\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        x1, x2 = x.split([self.split_dim, x.size(self.dim) - self.split_dim], dim=self.dim)\n\n        # Now that we can split on an arbitrary dimension, we have do a bit of reshaping...\n        mean, log_scale = self.nn(x1.reshape(x1.shape[:-self.event_dim] + (-1,)))\n        mean = mean.reshape(mean.shape[:-1] + x2.shape[-self.event_dim:])\n        log_scale = log_scale.reshape(log_scale.shape[:-1] + x2.shape[-self.event_dim:])\n\n        log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)\n        self._cached_log_scale = log_scale\n\n        y1 = x1\n        y2 = torch.exp(log_scale) * x2 + mean\n        return torch.cat([y1, y2], dim=self.dim)\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x. Uses a previously cached inverse if available, otherwise\n        performs the inversion afresh.\n        """"""\n        y1, y2 = y.split([self.split_dim, y.size(self.dim) - self.split_dim], dim=self.dim)\n        x1 = y1\n\n        # Now that we can split on an arbitrary dimension, we have do a bit of reshaping...\n        mean, log_scale = self.nn(x1.reshape(x1.shape[:-self.event_dim] + (-1,)))\n        mean = mean.reshape(mean.shape[:-1] + y2.shape[-self.event_dim:])\n        log_scale = log_scale.reshape(log_scale.shape[:-1] + y2.shape[-self.event_dim:])\n\n        log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)\n        self._cached_log_scale = log_scale\n\n        x2 = (y2 - mean) * torch.exp(-log_scale)\n        return torch.cat([x1, x2], dim=self.dim)\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log jacobian\n        """"""\n        x_old, y_old = self._cached_x_y\n        if self._cached_log_scale is not None and x is x_old and y is y_old:\n            log_scale = self._cached_log_scale\n        else:\n            x1, x2 = x.split([self.split_dim, x.size(self.dim) - self.split_dim], dim=self.dim)\n            _, log_scale = self.nn(x1.reshape(x1.shape[:-self.event_dim] + (-1,)))\n            log_scale = log_scale.reshape(log_scale.shape[:-1] + x2.shape[-self.event_dim:])\n            log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)\n        return _sum_rightmost(log_scale, self.event_dim)\n\n\n@copy_docs_from(ConditionalTransformModule)\nclass ConditionalAffineCoupling(ConditionalTransformModule):\n    r""""""\n    An implementation of the affine coupling layer of RealNVP (Dinh et al., 2017)\n    that conditions on an additional context variable and uses the bijective\n    transform,\n\n        :math:`\\mathbf{y}_{1:d} = \\mathbf{x}_{1:d}`\n        :math:`\\mathbf{y}_{(d+1):D} = \\mu + \\sigma\\odot\\mathbf{x}_{(d+1):D}`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    e.g. :math:`\\mathbf{x}_{1:d}` represents the first :math:`d` elements of the\n    inputs, and :math:`\\mu,\\sigma` are shift and translation parameters calculated\n    as the output of a function input :math:`\\mathbf{x}_{1:d}` and a context\n    variable :math:`\\mathbf{z}\\in\\mathbb{R}^M`.\n\n    That is, the first :math:`d` components remain unchanged, and the subsequent\n    :math:`D-d` are shifted and translated by a function of the previous components.\n\n    Together with :class:`~pyro.distributions.ConditionalTransformedDistribution`\n    this provides a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> from pyro.nn import ConditionalDenseNN\n    >>> input_dim = 10\n    >>> split_dim = 6\n    >>> context_dim = 4\n    >>> batch_size = 3\n    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n    >>> param_dims = [input_dim-split_dim, input_dim-split_dim]\n    >>> hypernet = ConditionalDenseNN(split_dim, context_dim, [10*input_dim],\n    ... param_dims)\n    >>> transform = ConditionalAffineCoupling(split_dim, hypernet)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> z = torch.rand(batch_size, context_dim)\n    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,\n    ... [transform]).condition(z)\n    >>> flow_dist.sample(sample_shape=torch.Size([batch_size]))  # doctest: +SKIP\n\n    The inverse of the Bijector is required when, e.g., scoring the log density of a\n    sample with :class:`~pyro.distributions.ConditionalTransformedDistribution`.\n    This implementation caches the inverse of the Bijector when its forward\n    operation is called, e.g., when sampling from\n    :class:`~pyro.distributions.ConditionalTransformedDistribution`. However, if the\n    cached value isn\'t available, either because it was overwritten during sampling\n    a new value or an arbitary value is being scored, it will calculate it manually.\n\n    This is an operation that scales as O(1), i.e. constant in the input dimension.\n    So in general, it is cheap to sample *and* score (an arbitrary value) from\n    :class:`~pyro.distributions.transforms.ConditionalAffineCoupling`.\n\n    :param split_dim: Zero-indexed dimension :math:`d` upon which to perform input/\n        output split for transformation.\n    :type split_dim: int\n    :param hypernet: A neural network whose forward call returns a real-valued mean\n        and logit-scale as a tuple. The input should have final dimension split_dim\n        and the output final dimension input_dim-split_dim for each member of the\n        tuple. The network also inputs a context variable as a keyword argument in\n        order to condition the output upon it.\n    :type hypernet: callable\n    :param log_scale_min_clip: The minimum value for clipping the log(scale) from\n        the NN\n    :type log_scale_min_clip: float\n    :param log_scale_max_clip: The maximum value for clipping the log(scale) from\n        the NN\n    :type log_scale_max_clip: float\n\n    References:\n\n    Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using\n    Real NVP. ICLR 2017.\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, split_dim, hypernet, **kwargs):\n        super().__init__()\n        self.split_dim = split_dim\n        self.nn = hypernet\n        self.kwargs = kwargs\n\n    def condition(self, context):\n        cond_nn = partial(self.nn, context=context)\n        return AffineCoupling(self.split_dim, cond_nn, **self.kwargs)\n\n\ndef affine_coupling(input_dim, hidden_dims=None, split_dim=None, dim=-1, **kwargs):\n    """"""\n    A helper function to create an\n    :class:`~pyro.distributions.transforms.AffineCoupling` object that takes care of\n    constructing a dense network with the correct input/output dimensions.\n\n    :param input_dim: Dimension(s) of input variable to permute. Note that when\n        `dim < -1` this must be a tuple corresponding to the event shape.\n    :type input_dim: int\n    :param hidden_dims: The desired hidden dimensions of the dense network. Defaults\n        to using [10*input_dim]\n    :type hidden_dims: list[int]\n    :param split_dim: The dimension to split the input on for the coupling\n        transform. Defaults to using input_dim // 2\n    :type split_dim: int\n    :param dim: the tensor dimension on which to split. This value must be negative\n        and defines the event dim as `abs(dim)`.\n    :type dim: int\n    :param log_scale_min_clip: The minimum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_min_clip: float\n    :param log_scale_max_clip: The maximum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_max_clip: float\n\n    """"""\n    if not isinstance(input_dim, int):\n        if len(input_dim) != -dim:\n            raise ValueError(\'event shape {} must have same length as event_dim {}\'.format(input_dim, -dim))\n        event_shape = input_dim\n        extra_dims = reduce(operator.mul, event_shape[(dim + 1):], 1)\n    else:\n        event_shape = [input_dim]\n        extra_dims = 1\n    event_shape = list(event_shape)\n\n    if split_dim is None:\n        split_dim = event_shape[dim] // 2\n    if hidden_dims is None:\n        hidden_dims = [10 * event_shape[dim] * extra_dims]\n\n    hypernet = DenseNN(split_dim * extra_dims,\n                       hidden_dims,\n                       [(event_shape[dim] - split_dim) * extra_dims,\n                        (event_shape[dim] - split_dim) * extra_dims])\n    return AffineCoupling(split_dim, hypernet, dim=dim, **kwargs)\n\n\ndef conditional_affine_coupling(input_dim, context_dim, hidden_dims=None, split_dim=None, dim=-1, **kwargs):\n    """"""\n    A helper function to create an\n    :class:`~pyro.distributions.transforms.ConditionalAffineCoupling` object that\n    takes care of constructing a dense network with the correct input/output\n    dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param context_dim: Dimension of context variable\n    :type context_dim: int\n    :param hidden_dims: The desired hidden dimensions of the dense network. Defaults\n        to using [10*input_dim]\n    :type hidden_dims: list[int]\n    :param split_dim: The dimension to split the input on for the coupling\n        transform. Defaults to using input_dim // 2\n    :type split_dim: int\n    :param dim: the tensor dimension on which to split. This value must be negative\n        and defines the event dim as `abs(dim)`.\n    :type dim: int\n    :param log_scale_min_clip: The minimum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_min_clip: float\n    :param log_scale_max_clip: The maximum value for clipping the log(scale) from\n        the autoregressive NN\n    :type log_scale_max_clip: float\n\n    """"""\n    if not isinstance(input_dim, int):\n        if len(input_dim) != -dim:\n            raise ValueError(\'event shape {} must have same length as event_dim {}\'.format(input_dim, -dim))\n        event_shape = input_dim\n        extra_dims = reduce(operator.mul, event_shape[(dim + 1):], 1)\n    else:\n        event_shape = [input_dim]\n        extra_dims = 1\n    event_shape = list(event_shape)\n\n    if split_dim is None:\n        split_dim = event_shape[dim] // 2\n    if hidden_dims is None:\n        hidden_dims = [10 * event_shape[dim] * extra_dims]\n\n    nn = ConditionalDenseNN(split_dim * extra_dims, context_dim, hidden_dims,\n                            [(event_shape[dim] - split_dim) * extra_dims, (event_shape[dim] - split_dim) * extra_dims])\n    return ConditionalAffineCoupling(split_dim, nn, dim=dim, **kwargs)\n'"
pyro/distributions/transforms/basic.py,5,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import TanhTransform, Transform\n\n# TODO: Move upstream\n\n\nclass ELUTransform(Transform):\n    r""""""\n    Bijective transform via the mapping :math:`y = \\text{ELU}(x)`.\n    """"""\n    domain = constraints.real\n    codomain = constraints.positive\n    bijective = True\n    sign = +1\n\n    def __eq__(self, other):\n        return isinstance(other, ELUTransform)\n\n    def _call(self, x):\n        return F.elu(x)\n\n    def _inverse(self, y, eps=1e-8):\n        return torch.max(y, torch.zeros_like(y)) + torch.min(torch.log1p(y + eps), torch.zeros_like(y))\n\n    def log_abs_det_jacobian(self, x, y):\n        return -F.relu(-x)\n\n\ndef elu():\n    """"""\n    A helper function to create an\n    :class:`~pyro.distributions.transform.ELUTransform` object for consistency with\n    other helpers.\n    """"""\n    return ELUTransform()\n\n# TODO: Move upstream\n\n\nclass LeakyReLUTransform(Transform):\n    r""""""\n    Bijective transform via the mapping :math:`y = \\text{LeakyReLU}(x)`.\n    """"""\n    domain = constraints.real\n    codomain = constraints.positive\n    bijective = True\n    sign = +1\n\n    def __eq__(self, other):\n        return isinstance(other, LeakyReLUTransform)\n\n    def _call(self, x):\n        return F.leaky_relu(x)\n\n    def _inverse(self, y):\n        return F.leaky_relu(y, negative_slope=100.0)\n\n    def log_abs_det_jacobian(self, x, y):\n        return torch.where(x >= 0., torch.zeros_like(x), torch.ones_like(x) * math.log(0.01))\n\n\ndef leaky_relu():\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.LeakyReLUTransform` object for\n    consistency with other helpers.\n    """"""\n    return LeakyReLUTransform()\n\n\ndef tanh():\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.TanhTransform` object for consistency\n    with other helpers.\n    """"""\n    return TanhTransform()\n'"
pyro/distributions/transforms/batchnorm.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.util import copy_docs_from\n\n\n@copy_docs_from(TransformModule)\nclass BatchNorm(TransformModule):\n    r""""""\n    A type of batch normalization that can be used to stabilize training in\n    normalizing flows. The inverse operation is defined as\n\n        :math:`x = (y - \\hat{\\mu}) \\oslash \\sqrt{\\hat{\\sigma^2}} \\otimes \\gamma + \\beta`\n\n    that is, the standard batch norm equation, where :math:`x` is the input,\n    :math:`y` is the output, :math:`\\gamma,\\beta` are learnable parameters, and\n    :math:`\\hat{\\mu}`/:math:`\\hat{\\sigma^2}` are smoothed running averages of\n    the sample mean and variance, respectively. The constraint :math:`\\gamma>0` is\n    enforced to ease calculation of the log-det-Jacobian term.\n\n    This is an element-wise transform, and when applied to a vector, learns two\n    parameters (:math:`\\gamma,\\beta`) for each dimension of the input.\n\n    When the module is set to training mode, the moving averages of the sample mean\n    and variance are updated every time the inverse operator is called, e.g., when a\n    normalizing flow scores a minibatch with the `log_prob` method.\n\n    Also, when the module is set to training mode, the sample mean and variance on\n    the current minibatch are used in place of the smoothed averages,\n    :math:`\\hat{\\mu}` and :math:`\\hat{\\sigma^2}`, for the inverse operator. For\n    this reason it is not the case that :math:`x=g(g^{-1}(x))` during training,\n    i.e., that the inverse operation is the inverse of the forward one.\n\n    Example usage:\n\n    >>> from pyro.nn import AutoRegressiveNN\n    >>> from pyro.distributions.transforms import AffineAutoregressive\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> iafs = [AffineAutoregressive(AutoRegressiveNN(10, [40])) for _ in range(2)]\n    >>> bn = BatchNorm(10)\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [iafs[0], bn, iafs[1]])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    :param input_dim: the dimension of the input\n    :type input_dim: int\n    :param momentum: momentum parameter for updating moving averages\n    :type momentum: float\n    :param epsilon: small number to add to variances to ensure numerical stability\n    :type epsilon: float\n\n    References:\n\n    [1] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep\n    Network Training by Reducing Internal Covariate Shift. In International\n    Conference on Machine Learning, 2015. https://arxiv.org/abs/1502.03167\n\n    [2] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation\n    using Real NVP. In International Conference on Learning Representations, 2017.\n    https://arxiv.org/abs/1605.08803\n\n    [3] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive\n    Flow for Density Estimation. In Neural Information Processing Systems, 2017.\n    https://arxiv.org/abs/1705.07057\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 0\n\n    def __init__(self, input_dim, momentum=0.1, epsilon=1e-5):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.gamma = nn.Parameter(torch.ones(input_dim))\n        self.beta = nn.Parameter(torch.zeros(input_dim))\n        self.momentum = momentum\n        self.epsilon = epsilon\n\n        self.register_buffer(\'moving_mean\', torch.zeros(input_dim))\n        self.register_buffer(\'moving_variance\', torch.ones(input_dim))\n\n    @property\n    def constrained_gamma(self):\n        return F.relu(self.gamma) + 1e-6\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        # Enforcing the constraint that gamma is positive\n        return (x - self.beta) / self.constrained_gamma * \\\n            torch.sqrt(self.moving_variance + self.epsilon) + self.moving_mean\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x.\n        """"""\n        # During training, keep smoothed average of sample mean and variance\n        if self.training:\n            mean, var = y.mean(0), y.var(0)\n\n            with torch.no_grad():\n                # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`\n                self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\n                self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)\n\n        # During test time, use smoothed averages rather than the sample ones\n        else:\n            mean, var = self.moving_mean, self.moving_variance\n\n        return (y - mean) * self.constrained_gamma / torch.sqrt(var + self.epsilon) + self.beta\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian, dx/dy\n        """"""\n        if self.training:\n            var = torch.var(y, dim=0, keepdim=True)\n        else:\n            # NOTE: You wouldn\'t typically run this function in eval mode, but included for gradient tests\n            var = self.moving_variance\n        return (-self.constrained_gamma.log() + 0.5 * torch.log(var + self.epsilon))\n\n\ndef batchnorm(input_dim, **kwargs):\n    """"""\n    A helper function to create a :class:`~pyro.distributions.transforms.BatchNorm`\n    object for consistency with other helpers.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param momentum: momentum parameter for updating moving averages\n    :type momentum: float\n    :param epsilon: small number to add to variances to ensure numerical stability\n    :type epsilon: float\n\n    """"""\n    bn = BatchNorm(input_dim, **kwargs)\n    return bn\n'"
pyro/distributions/transforms/block_autoregressive.py,25,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n# This implementation is adapted in part from https://github.com/nicola-decao/BNAF under the MIT license.\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.transforms.neural_autoregressive import ELUTransform, LeakyReLUTransform, TanhTransform\nfrom pyro.distributions.util import copy_docs_from\n\neps = 1e-8\n\n\ndef log_matrix_product(A, B):\n    """"""\n    Computes the matrix products of two matrices in log-space, returning the result\n    in log-space. This is useful for calculating the vector chain rule for Jacobian\n    terms.\n    """"""\n    return torch.logsumexp(A.unsqueeze(-1) + B.unsqueeze(-3), dim=-2)\n\n\n@copy_docs_from(TransformModule)\nclass BlockAutoregressive(TransformModule):\n    r""""""\n    An implementation of Block Neural Autoregressive Flow (block-NAF)\n    (De Cao et al., 2019) bijective transform. Block-NAF uses a similar\n    transformation to deep dense NAF, building the autoregressive NN into the\n    structure of the transform, in a sense.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` this provides\n    a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> naf = BlockAutoregressive(input_dim=10)\n    >>> pyro.module(""my_naf"", naf)  # doctest: +SKIP\n    >>> naf_dist = dist.TransformedDistribution(base_dist, [naf])\n    >>> naf_dist.sample()  # doctest: +SKIP\n\n    The inverse operation is not implemented. This would require numerical\n    inversion, e.g., using a root finding method - a possibility for a future\n    implementation.\n\n    :param input_dim: The dimensionality of the input and output variables.\n    :type input_dim: int\n    :param hidden_factors: Hidden layer i has hidden_factors[i] hidden units per\n        input dimension. This corresponds to both :math:`a` and :math:`b` in De Cao\n        et al. (2019). The elements of hidden_factors must be integers.\n    :type hidden_factors: list\n    :param activation: Activation function to use. One of \'ELU\', \'LeakyReLU\',\n        \'sigmoid\', or \'tanh\'.\n    :type activation: string\n    :param residual: Type of residual connections to use. Choices are ""None"",\n        ""normal"" for :math:`\\mathbf{y}+f(\\mathbf{y})`, and ""gated"" for\n        :math:`\\alpha\\mathbf{y} + (1 - \\alpha\\mathbf{y})` for learnable\n        parameter :math:`\\alpha`.\n    :type residual: string\n\n    References:\n\n    [1] Nicola De Cao, Ivan Titov, Wilker Aziz. Block Neural Autoregressive Flow.\n    [arXiv:1904.04676]\n\n    """"""\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n    autoregressive = True\n\n    def __init__(self, input_dim, hidden_factors=[8, 8], activation=\'tanh\', residual=None):\n        super().__init__(cache_size=1)\n\n        if any([h < 1 for h in hidden_factors]):\n            raise ValueError(\'Hidden factors, {}, must all be >= 1\'.format(hidden_factors))\n\n        if residual not in [None, \'normal\', \'gated\']:\n            raise ValueError(\'Invalid value {} for keyword argument ""residual""\'.format(residual))\n\n        # Mix in activation function methods\n        name_to_mixin = {\n            \'ELU\': ELUTransform,\n            \'LeakyReLU\': LeakyReLUTransform,\n            \'sigmoid\': torch.distributions.transforms.SigmoidTransform,\n            \'tanh\': TanhTransform}\n        if activation not in name_to_mixin:\n            raise ValueError(\'Invalid activation function ""{}""\'.format(activation))\n        self.T = name_to_mixin[activation]()\n\n        # Initialize modules for each layer in transform\n        self.residual = residual\n        self.input_dim = input_dim\n        self.layers = nn.ModuleList([MaskedBlockLinear(input_dim, input_dim * hidden_factors[0], input_dim)])\n        for idx in range(1, len(hidden_factors)):\n            self.layers.append(MaskedBlockLinear(\n                input_dim * hidden_factors[idx - 1], input_dim * hidden_factors[idx], input_dim))\n        self.layers.append(MaskedBlockLinear(input_dim * hidden_factors[-1], input_dim, input_dim))\n        self._cached_logDetJ = None\n\n        if residual == \'gated\':\n            self.gate = torch.nn.Parameter(torch.nn.init.normal_(torch.Tensor(1)))\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        y = x\n        for idx in range(len(self.layers)):\n            pre_activation, dy_dx = self.layers[idx](y.unsqueeze(-1))\n\n            if idx == 0:\n                y = self.T(pre_activation)\n                J_act = self.T.log_abs_det_jacobian((pre_activation).view(\n                    *(list(x.size()) + [-1, 1])), y.view(*(list(x.size()) + [-1, 1])))\n                logDetJ = dy_dx + J_act\n\n            elif idx < len(self.layers) - 1:\n                y = self.T(pre_activation)\n                J_act = self.T.log_abs_det_jacobian((pre_activation).view(\n                    *(list(x.size()) + [-1, 1])), y.view(*(list(x.size()) + [-1, 1])))\n                logDetJ = log_matrix_product(dy_dx, logDetJ) + J_act\n\n            else:\n                y = pre_activation\n                logDetJ = log_matrix_product(dy_dx, logDetJ)\n\n        self._cached_logDetJ = logDetJ.squeeze(-1).squeeze(-1)\n\n        if self.residual == \'normal\':\n            y = y + x\n            self._cached_logDetJ = F.softplus(self._cached_logDetJ)\n        elif self.residual == \'gated\':\n            y = self.gate.sigmoid() * x + (1. - self.gate.sigmoid()) * y\n            term1 = torch.log(self.gate.sigmoid() + eps)\n            log1p_gate = torch.log1p(eps - self.gate.sigmoid())\n            log_gate = torch.log(self.gate.sigmoid() + eps)\n            term2 = F.softplus(log1p_gate - log_gate + self._cached_logDetJ)\n            self._cached_logDetJ = term1 + term2\n\n        return y\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x. As noted above, this implementation is incapable of\n        inverting arbitrary values `y`; rather it assumes `y` is the result of a\n        previously computed application of the bijector to some `x` (which was\n        cached on the forward call)\n        """"""\n\n        raise KeyError(""BlockAutoregressive object expected to find key in intermediates cache but didn\'t"")\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log jacobian\n        """"""\n        x_old, y_old = self._cached_x_y\n        if x is not x_old or y is not y_old:\n            # This call to the parent class Transform will update the cache\n            # as well as calling self._call and recalculating y and log_detJ\n            self(x)\n\n        return self._cached_logDetJ.sum(-1)\n\n\nclass MaskedBlockLinear(torch.nn.Module):\n    """"""\n    Module that implements a linear layer with block matrices with positive diagonal\n    blocks. Moreover, it uses Weight Normalization\n    (https://arxiv.org/abs/1602.07868) for stability.\n    """"""\n\n    def __init__(self, in_features, out_features, dim, bias=True):\n        super().__init__()\n        self.in_features, self.out_features, self.dim = in_features, out_features, dim\n\n        weight = torch.zeros(out_features, in_features)\n\n        # Fill in non-zero entries of block weight matrix, going from top\n        # to bottom.\n        for i in range(dim):\n            weight[i * out_features // dim:(i + 1) * out_features // dim,\n                   0:(i + 1) * in_features // dim] = torch.nn.init.xavier_uniform_(\n                torch.Tensor(out_features // dim, (i + 1) * in_features // dim))\n\n        self._weight = torch.nn.Parameter(weight)\n        self._diag_weight = torch.nn.Parameter(torch.nn.init.uniform_(torch.Tensor(out_features, 1)).log())\n\n        self.bias = torch.nn.Parameter(\n            torch.nn.init.uniform_(torch.Tensor(out_features),\n                                   -1 / math.sqrt(out_features),\n                                   1 / math.sqrt(out_features))) if bias else 0\n\n        # Diagonal block mask\n        mask_d = torch.eye(dim).unsqueeze(-1).repeat(1, out_features // dim,\n                                                     in_features // dim).view(out_features, in_features)\n        self.register_buffer(\'mask_d\', mask_d)\n\n        # Off-diagonal block mask for lower triangular weight matrix\n        mask_o = torch.tril(torch.ones(dim, dim), diagonal=-1).unsqueeze(-1)\n        mask_o = mask_o.repeat(1, out_features // dim, in_features // dim).view(out_features, in_features)\n        self.register_buffer(\'mask_o\', mask_o)\n\n    def get_weights(self):\n        """"""\n        Computes the weight matrix using masks and weight normalization.\n        It also compute the log diagonal blocks of it.\n        """"""\n\n        # Form block weight matrix, making sure it\'s positive on diagonal!\n        w = torch.exp(self._weight) * self.mask_d + self._weight * self.mask_o\n\n        # Sum is taken over columns, i.e. one norm per row\n        w_squared_norm = (w ** 2).sum(-1, keepdim=True)\n\n        # Effect of multiplication and division is that each row is normalized and rescaled\n        w = self._diag_weight.exp() * w / (w_squared_norm.sqrt() + eps)\n\n        # Taking the effect of weight normalization into account in calculating the log-gradient is straightforward!\n        # Instead of differentiating, e.g. d(W_1x)/dx, we have d(g_1W_1/(W_1^TW_1)^0.5x)/dx, roughly speaking, and\n        # taking the log gives the right hand side below:\n        wpl = self._diag_weight + self._weight - 0.5 * torch.log(w_squared_norm + eps)\n\n        return w, wpl[self.mask_d.bool()].view(self.dim, self.out_features // self.dim, self.in_features // self.dim)\n\n    def forward(self, x):\n        w, wpl = self.get_weights()\n        return (torch.matmul(w, x) + self.bias.unsqueeze(-1)).squeeze(-1), wpl\n\n\ndef block_autoregressive(input_dim, **kwargs):\n    r""""""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.BlockAutoregressive` object for\n    consistency with other helpers.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param hidden_factors: Hidden layer i has hidden_factors[i] hidden units per\n        input dimension. This corresponds to both :math:`a` and :math:`b` in De Cao\n        et al. (2019). The elements of hidden_factors must be integers.\n    :type hidden_factors: list\n    :param activation: Activation function to use. One of \'ELU\', \'LeakyReLU\',\n        \'sigmoid\', or \'tanh\'.\n    :type activation: string\n    :param residual: Type of residual connections to use. Choices are ""None"",\n        ""normal"" for :math:`\\mathbf{y}+f(\\mathbf{y})`, and ""gated"" for\n        :math:`\\alpha\\mathbf{y} + (1 - \\alpha\\mathbf{y})` for learnable\n        parameter :math:`\\alpha`.\n    :type residual: string\n\n    """"""\n\n    return BlockAutoregressive(input_dim, **kwargs)\n'"
pyro/distributions/transforms/cholesky.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import Transform\n\nfrom pyro.distributions.constraints import corr_cholesky_constraint\n\n\ndef _vector_to_l_cholesky(z):\n    D = (1.0 + math.sqrt(1.0 + 8.0 * z.shape[-1])) / 2.0\n    if D % 1 != 0:\n        raise ValueError(""Correlation matrix transformation requires d choose 2 inputs"")\n    D = int(D)\n    x = torch.zeros(z.shape[:-1] + (D, D), dtype=z.dtype, device=z.device)\n\n    x[..., 0, 0] = 1\n    x[..., 1:, 0] = z[..., :(D - 1)]\n    i = D - 1\n    last_squared_x = torch.zeros(z.shape[:-1] + (D,), dtype=z.dtype, device=z.device)\n    for j in range(1, D):\n        distance_to_copy = D - 1 - j\n        last_squared_x = last_squared_x[..., 1:] + x[..., j:, (j - 1)].clone()**2\n        x[..., j, j] = (1 - last_squared_x[..., 0]).sqrt()\n        x[..., (j + 1):, j] = z[..., i:(i + distance_to_copy)] * (1 - last_squared_x[..., 1:]).sqrt()\n        i += distance_to_copy\n    return x\n\n\nclass CorrLCholeskyTransform(Transform):\n    """"""\n    Transforms a vector into the cholesky factor of a correlation matrix.\n\n    The input should have shape `[batch_shape] + [d * (d-1)/2]`. The output will\n    have shape `[batch_shape] + [d, d]`.\n\n    References:\n\n    [1] Cholesky Factors of Correlation Matrices. Stan Reference Manual v2.18,\n    Section 10.12.\n\n    """"""\n    domain = constraints.real\n    codomain = corr_cholesky_constraint\n    bijective = True\n    sign = +1\n    event_dim = 1\n\n    def __eq__(self, other):\n        return isinstance(other, CorrLCholeskyTransform)\n\n    def _call(self, x):\n        z = x.tanh()\n        return _vector_to_l_cholesky(z)\n\n    def _inverse(self, y):\n        if (y.shape[-2] != y.shape[-1]):\n            raise ValueError(""A matrix that isn\'t square can\'t be a Cholesky factor of a correlation matrix"")\n        D = y.shape[-1]\n\n        z_tri = torch.zeros(y.shape[:-2] + (D - 2, D - 2), dtype=y.dtype, device=y.device)\n        z_stack = [\n            y[..., 1:, 0]\n        ]\n\n        for i in range(2, D):\n            z_tri[..., i - 2, 0:(i - 1)] = y[..., i, 1:i] / (1 - y[..., i, 0:(i - 1)].pow(2).cumsum(-1)).sqrt()\n        for j in range(D - 2):\n            z_stack.append(z_tri[..., j:, j])\n\n        z = torch.cat(z_stack, -1)\n        return torch.log1p((2 * z) / (1 - z)) / 2\n\n    def log_abs_det_jacobian(self, x, y):\n        # Note dependence on pytorch 1.0.1 for batched tril\n        tanpart = x.cosh().log().sum(-1).mul(-2)\n        matpart = (1 - y.pow(2).cumsum(-1).tril(diagonal=-2)).log().div(2).sum(-1).sum(-1)\n        return tanpart + matpart\n'"
pyro/distributions/transforms/discrete_cosine.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import Transform\n\nfrom pyro.ops.tensor_utils import dct, idct\n\n\nclass DiscreteCosineTransform(Transform):\n    """"""\n    Discrete Cosine Transform of type-II.\n\n    This uses :func:`~pyro.ops.tensor_utils.dct` and\n    :func:`~pyro.ops.tensor_utils.idct` to compute\n    orthonormal DCT and inverse DCT transforms. The jacobian is 1.\n\n    :param int dim: Dimension along which to transform. Must be negative.\n        This is an absolute dim counting from the right.\n    :param float smooth: Smoothing parameter. When 0, this transforms white\n        noise to white noise; when 1 this transforms Brownian noise to to white\n        noise; when -1 this transforms violet noise to white noise; etc. Any\n        real number is allowed. https://en.wikipedia.org/wiki/Colors_of_noise.\n    """"""\n    domain = constraints.real_vector\n    codomain = constraints.real_vector\n    bijective = True\n\n    def __init__(self, dim=-1, smooth=0., cache_size=0):\n        assert isinstance(dim, int) and dim < 0\n        self.event_dim = -dim\n        self.smooth = float(smooth)\n        self._weight_cache = None\n        super().__init__(cache_size=cache_size)\n\n    def __eq__(self, other):\n        return (type(self) == type(other) and self.event_dim == other.event_dim\n                and self.smooth == other.smooth)\n\n    @torch.no_grad()\n    def _weight(self, y):\n        size = y.size(-1)\n        if self._weight_cache is None or self._weight_cache.size(-1) != size:\n            # Weight by frequency**smooth, where the DCT-II frequencies are:\n            freq = torch.linspace(0.5, size - 0.5, size, dtype=y.dtype, device=y.device)\n            w = freq.pow_(self.smooth)\n            w /= w.log().mean().exp()  # Ensure |jacobian| = 1.\n            self._weight_cache = w\n        return self._weight_cache\n\n    def _call(self, x):\n        dim = -self.event_dim\n        if dim != -1:\n            x = x.transpose(dim, -1)\n        y = dct(x)\n        if self.smooth:\n            y = y * self._weight(y)\n        if dim != -1:\n            y = y.transpose(dim, -1)\n        return y\n\n    def _inverse(self, y):\n        dim = -self.event_dim\n        if dim != -1:\n            y = y.transpose(dim, -1)\n        if self.smooth:\n            y = y / self._weight(y)\n        x = idct(y)\n        if dim != -1:\n            x = x.transpose(dim, -1)\n        return x\n\n    def log_abs_det_jacobian(self, x, y):\n        return x.new_zeros(x.shape[:-self.event_dim])\n\n    def with_cache(self, cache_size=1):\n        if self._cache_size == cache_size:\n            return self\n        return DiscreteCosineTransform(-self.event_dim, self.smooth, cache_size=cache_size)\n'"
pyro/distributions/transforms/generalized_channel_permute.py,21,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import Transform, constraints\n\nfrom pyro.distributions.conditional import ConditionalTransformModule\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import DenseNN\n\n\n@copy_docs_from(Transform)\nclass ConditionedGeneralizedChannelPermute(Transform):\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 3\n\n    def __init__(self, permutation=None, LU=None):\n        super(ConditionedGeneralizedChannelPermute, self).__init__(cache_size=1)\n\n        self.permutation = permutation\n        self.LU = LU\n\n    @property\n    def U_diag(self):\n        return self.LU.diag()\n\n    @property\n    def L(self):\n        return self.LU.tril(diagonal=-1) + torch.eye(self.LU.size(-1), dtype=self.LU.dtype, device=self.LU.device)\n\n    @property\n    def U(self):\n        return self.LU.triu()\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n\n        """"""\n        NOTE: As is the case for other conditional transforms, the batch dim of the\n        context variable (reflected in the initial dimensions of filters in this\n        case), if this is a conditional transform, must broadcast over the batch dim\n        of the input variable.\n\n        Also, the reason the following line uses matrix multiplication rather than\n        F.conv2d is so we can perform multiple convolutions when the filters\n        ""kernel"" has batch dimensions\n        """"""\n        filters = (self.permutation @ self.L @ self.U)[..., None, None]\n        y = (filters * x.unsqueeze(-4)).sum(-3)\n        return y\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x.\n        """"""\n\n        """"""\n        NOTE: This method is equivalent to the following two lines. Using\n        Tensor.inverse() would be numerically unstable, however.\n\n        filters = (self.permutation @ self.L @ self.U).inverse()[..., None, None]\n        x = F.conv2d(y.view(-1, *y.shape[-3:]), filters)\n        return x.view_as(y)\n\n        """"""\n\n        # Do a matrix vector product over the channel dimension\n        # in order to apply inverse permutation matrix\n        y_flat = y.flatten(start_dim=-2)\n        LUx = (y_flat.unsqueeze(-3) * self.permutation.T.unsqueeze(-1)).sum(-2)\n\n        # Solve L(Ux) = P^1y\n        Ux, _ = torch.triangular_solve(LUx, self.L, upper=False)\n\n        # Solve Ux = (PL)^-1y\n        x, _ = torch.triangular_solve(Ux, self.U)\n\n        # Unflatten x (works when context variable has batch dim)\n        return x.reshape(x.shape[:-1] + y.shape[-2:])\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian, i.e.\n        log(abs(det(dy/dx))).\n        """"""\n\n        h, w = x.shape[-2:]\n        log_det = h * w * self.U_diag.abs().log().sum()\n        return log_det * torch.ones(x.size()[:-3], dtype=x.dtype, layout=x.layout, device=x.device)\n\n\n@copy_docs_from(ConditionedGeneralizedChannelPermute)\nclass GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformModule):\n    r""""""\n    A bijection that generalizes a permutation on the channels of a batch of 2D\n    image in :math:`[\\ldots,C,H,W]` format. Specifically this transform performs\n    the operation,\n\n        :math:`\\mathbf{y} = \\text{torch.nn.functional.conv2d}(\\mathbf{x}, W)`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    and :math:`W\\sim C\\times C\\times 1\\times 1` is the filter matrix for a 1x1\n    convolution with :math:`C` input and output channels.\n\n    Ignoring the final two dimensions, :math:`W` is restricted to be the matrix\n    product,\n\n        :math:`W = PLU`\n\n    where :math:`P\\sim C\\times C` is a permutation matrix on the channel\n    dimensions, :math:`L\\sim C\\times C` is a lower triangular matrix with ones on\n    the diagonal, and :math:`U\\sim C\\times C` is an upper triangular matrix.\n    :math:`W` is initialized to a random orthogonal matrix. Then, :math:`P` is fixed\n    and the learnable parameters set to :math:`L,U`.\n\n    The input :math:`\\mathbf{x}` and output :math:`\\mathbf{y}` both have shape\n    `[...,C,H,W]`, where `C` is the number of channels set at initialization.\n\n    This operation was introduced in [1] for Glow normalizing flow, and is also\n    known as 1x1 invertible convolution. It appears in other notable work such as\n    [2,3], and corresponds to the class `tfp.bijectors.MatvecLU` of TensorFlow\n    Probability.\n\n    Example usage:\n\n    >>> channels = 3\n    >>> base_dist = dist.Normal(torch.zeros(channels, 32, 32),\n    ... torch.ones(channels, 32, 32))\n    >>> inv_conv = GeneralizedChannelPermute(channels=channels)\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [inv_conv])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    :param channels: Number of channel dimensions in the input.\n    :type channels: int\n\n    [1] Diederik P. Kingma, Prafulla Dhariwal. Glow: Generative Flow with Invertible\n    1x1 Convolutions. [arXiv:1807.03039]\n\n    [2] Ryan Prenger, Rafael Valle, Bryan Catanzaro. WaveGlow: A Flow-based\n    Generative Network for Speech Synthesis. [arXiv:1811.00002]\n\n    [3] Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural Spline\n    Flows. [arXiv:1906.04032]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 3\n\n    def __init__(self, channels=3, permutation=None):\n        super(GeneralizedChannelPermute, self).__init__()\n        self.__delattr__(\'permutation\')\n\n        # Sample a random orthogonal matrix\n        W, _ = torch.qr(torch.randn(channels, channels))\n\n        # Construct the partially pivoted LU-form and the pivots\n        LU, pivots = W.lu()\n\n        # Convert the pivots into the permutation matrix\n        if permutation is None:\n            P, _, _ = torch.lu_unpack(LU, pivots)\n        else:\n            if len(permutation) != channels:\n                raise ValueError(\n                    \'Keyword argument ""permutation"" expected to have {} elements but {} found.\'.format(\n                        channels, len(permutation)))\n            P = torch.eye(channels, channels)[permutation.type(dtype=torch.int64)]\n\n        # We register the permutation matrix so that the model can be serialized\n        self.register_buffer(\'permutation\', P)\n\n        # NOTE: For this implementation I have chosen to store the parameters densely, rather than\n        # storing L, U, and s separately\n        self.LU = torch.nn.Parameter(LU)\n\n\n@copy_docs_from(ConditionalTransformModule)\nclass ConditionalGeneralizedChannelPermute(ConditionalTransformModule):\n    r""""""\n    A bijection that generalizes a permutation on the channels of a batch of 2D\n    image in :math:`[\\ldots,C,H,W]` format conditioning on an additional context\n    variable. Specifically this transform performs the operation,\n\n        :math:`\\mathbf{y} = \\text{torch.nn.functional.conv2d}(\\mathbf{x}, W)`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    and :math:`W\\sim C\\times C\\times 1\\times 1` is the filter matrix for a 1x1\n    convolution with :math:`C` input and output channels.\n\n    Ignoring the final two dimensions, :math:`W` is restricted to be the matrix\n    product,\n\n        :math:`W = PLU`\n\n    where :math:`P\\sim C\\times C` is a permutation matrix on the channel\n    dimensions, and  :math:`LU\\sim C\\times C` is an invertible product of a lower\n    triangular and an upper triangular matrix that is the output of an NN with\n    input :math:`z\\in\\mathbb{R}^{M}` representing the context variable to\n    condition on.\n\n    The input :math:`\\mathbf{x}` and output :math:`\\mathbf{y}` both have shape\n    `[...,C,H,W]`, where `C` is the number of channels set at initialization.\n\n    This operation was introduced in [1] for Glow normalizing flow, and is also\n    known as 1x1 invertible convolution. It appears in other notable work such as\n    [2,3], and corresponds to the class `tfp.bijectors.MatvecLU` of TensorFlow\n    Probability.\n\n    Example usage:\n\n    >>> from pyro.nn.dense_nn import DenseNN\n    >>> context_dim = 5\n    >>> batch_size = 3\n    >>> channels = 3\n    >>> base_dist = dist.Normal(torch.zeros(channels, 32, 32),\n    ... torch.ones(channels, 32, 32))\n    >>> hidden_dims = [context_dim*10, context_dim*10]\n    >>> nn = DenseNN(context_dim, hidden_dims, param_dims=[channels*channels])\n    >>> transform = ConditionalGeneralizedChannelPermute(nn, channels=channels)\n    >>> z = torch.rand(batch_size, context_dim)\n    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,\n    ... [transform]).condition(z)\n    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP\n\n    :param nn: a function inputting the context variable and outputting\n        real-valued parameters of dimension :math:`C^2`.\n    :param channels: Number of channel dimensions in the input.\n    :type channels: int\n\n    [1] Diederik P. Kingma, Prafulla Dhariwal. Glow: Generative Flow with Invertible\n    1x1 Convolutions. [arXiv:1807.03039]\n\n    [2] Ryan Prenger, Rafael Valle, Bryan Catanzaro. WaveGlow: A Flow-based\n    Generative Network for Speech Synthesis. [arXiv:1811.00002]\n\n    [3] Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural Spline\n    Flows. [arXiv:1906.04032]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 3\n\n    def __init__(self, nn, channels=3, permutation=None):\n        super().__init__()\n        self.nn = nn\n        self.channels = channels\n        if permutation is None:\n            permutation = torch.randperm(channels, device=\'cpu\').to(torch.Tensor().device)\n        P = torch.eye(len(permutation), len(permutation))[permutation.type(dtype=torch.int64)]\n        self.register_buffer(\'permutation\', P)\n\n    def condition(self, context):\n        LU = self.nn(context)\n        LU = LU.view(LU.shape[:-1] + (self.channels, self.channels))\n        return ConditionedGeneralizedChannelPermute(self.permutation, LU)\n\n\ndef generalized_channel_permute(**kwargs):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.GeneralizedChannelPermute` object for\n    consistency with other helpers.\n\n    :param channels: Number of channel dimensions in the input.\n    :type channels: int\n\n    """"""\n\n    return GeneralizedChannelPermute(**kwargs)\n\n\ndef conditional_generalized_channel_permute(context_dim, channels=3, hidden_dims=None):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.ConditionalGeneralizedChannelPermute`\n    object for consistency with other helpers.\n\n    :param channels: Number of channel dimensions in the input.\n    :type channels: int\n\n    """"""\n\n    if hidden_dims is None:\n        hidden_dims = [channels * 10, channels * 10]\n    nn = DenseNN(context_dim, hidden_dims, param_dims=[channels * channels])\n    return ConditionalGeneralizedChannelPermute(nn, channels)\n'"
pyro/distributions/transforms/haar.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import Transform\n\nfrom pyro.ops.tensor_utils import haar_transform, inverse_haar_transform\n\n\nclass HaarTransform(Transform):\n    """"""\n    Discrete Haar transform.\n\n    This uses :func:`~pyro.ops.tensor_utils.haar_transform` and\n    :func:`~pyro.ops.tensor_utils.inverse_haar_transform` to compute\n    (orthonormal) Haar and inverse Haar transforms. The jacobian is 1.\n    For sequences with length `T` not a power of two, this implementation\n    is equivalent to a block-structured Haar transform in which block\n    sizes decrease by factors of one half from left to right.\n\n    :param int dim: Dimension along which to transform. Must be negative.\n        This is an absolute dim counting from the right.\n    :param bool flip: Whether to flip the time axis before applying the\n        Haar transform. Defaults to false.\n    """"""\n    domain = constraints.real_vector\n    codomain = constraints.real_vector\n    bijective = True\n\n    def __init__(self, dim=-1, flip=False, cache_size=0):\n        assert isinstance(dim, int) and dim < 0\n        self.event_dim = -dim\n        self.flip = flip\n        super().__init__(cache_size=cache_size)\n\n    def __eq__(self, other):\n        return (type(self) == type(other) and self.event_dim == other.event_dim and\n                self.flip == other.flip)\n\n    def _call(self, x):\n        dim = -self.event_dim\n        if dim != -1:\n            x = x.transpose(dim, -1)\n        if self.flip:\n            x = x.flip(-1)\n        y = haar_transform(x)\n        if dim != -1:\n            y = y.transpose(dim, -1)\n        return y\n\n    def _inverse(self, y):\n        dim = -self.event_dim\n        if dim != -1:\n            y = y.transpose(dim, -1)\n        x = inverse_haar_transform(y)\n        if self.flip:\n            x = x.flip(-1)\n        if dim != -1:\n            x = x.transpose(dim, -1)\n        return x\n\n    def log_abs_det_jacobian(self, x, y):\n        return x.new_zeros(x.shape[:-self.event_dim])\n\n    def with_cache(self, cache_size=1):\n        if self._cache_size == cache_size:\n            return self\n        return HaarTransform(-self.event_dim, flip=self.flip, cache_size=cache_size)\n'"
pyro/distributions/transforms/householder.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport warnings\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import Transform, constraints\n\nfrom pyro.distributions.conditional import ConditionalTransformModule\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import DenseNN\n\n\nclass ConditionedHouseholder(Transform):\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n    volume_preserving = True\n\n    def __init__(self, u_unnormed=None):\n        super().__init__(cache_size=1)\n        self.u_unnormed = u_unnormed\n\n    # Construct normalized vectors for Householder transform\n    def u(self):\n        norm = torch.norm(self.u_unnormed, p=2, dim=-1, keepdim=True)\n        return torch.div(self.u_unnormed, norm)\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n\n        y = x\n        u = self.u()\n        for idx in range(u.size(-2)):\n            projection = (u[..., idx, :] * y).sum(dim=-1, keepdim=True) * u[..., idx, :]\n            y = y - 2. * projection\n        return y\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x. The Householder transformation, H, is ""involutory,"" i.e.\n        H^2 = I. If you reflect a point around a plane, then the same operation will\n        reflect it back\n        """"""\n\n        x = y\n        u = self.u()\n        for jdx in reversed(range(u.size(-2))):\n            # NOTE: Need to apply transforms in reverse order from forward operation!\n            projection = (u[..., jdx, :] * x).sum(dim=-1, keepdim=True) * u[..., jdx, :]\n            x = x - 2. * projection\n        return x\n\n    def log_abs_det_jacobian(self, x, y):\n        r""""""\n        Calculates the elementwise determinant of the log jacobian. Householder flow\n        is measure preserving, so :math:`\\log(|detJ|) = 0`\n        """"""\n\n        return torch.zeros(x.size()[:-1], dtype=x.dtype, layout=x.layout, device=x.device)\n\n\n@copy_docs_from(TransformModule)\nclass Householder(ConditionedHouseholder, TransformModule):\n    r""""""\n    Represents multiple applications of the Householder bijective transformation. A\n    single Householder transformation takes the form,\n\n        :math:`\\mathbf{y} = (I - 2*\\frac{\\mathbf{u}\\mathbf{u}^T}{||\\mathbf{u}||^2})\\mathbf{x}`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    and the learnable parameters are :math:`\\mathbf{u}\\in\\mathbb{R}^D` for input\n    dimension :math:`D`.\n\n    The transformation represents the reflection of :math:`\\mathbf{x}` through the\n    plane passing through the origin with normal :math:`\\mathbf{u}`.\n\n    :math:`D` applications of this transformation are able to transform standard\n    i.i.d. standard Gaussian noise into a Gaussian variable with an arbitrary\n    covariance matrix. With :math:`K<D` transformations, one is able to approximate\n    a full-rank Gaussian distribution using a linear transformation of rank\n    :math:`K`.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` this provides\n    a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> transform = Householder(10, count_transforms=5)\n    >>> pyro.module(""my_transform"", p) # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    :param input_dim: the dimension of the input (and output) variable.\n    :type input_dim: int\n    :param count_transforms: number of applications of Householder transformation to\n        apply.\n    :type count_transforms: int\n\n    References:\n\n    [1] Jakub M. Tomczak, Max Welling. Improving Variational Auto-Encoders using\n    Householder Flow. [arXiv:1611.09630]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n    volume_preserving = True\n\n    def __init__(self, input_dim, count_transforms=1):\n        super().__init__()\n\n        self.input_dim = input_dim\n        if count_transforms < 1:\n            raise ValueError(\'Number of Householder transforms, {}, is less than 1!\'.format(count_transforms))\n        elif count_transforms > input_dim:\n            warnings.warn(\n                ""Number of Householder transforms, {}, is greater than input dimension {}, which is an \\\nover-parametrization!"".format(count_transforms, input_dim))\n        self.u_unnormed = nn.Parameter(torch.Tensor(count_transforms, input_dim))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.u_unnormed.size(-1))\n        self.u_unnormed.data.uniform_(-stdv, stdv)\n\n\n@copy_docs_from(ConditionalTransformModule)\nclass ConditionalHouseholder(ConditionalTransformModule):\n    r""""""\n    Represents multiple applications of the Householder bijective transformation\n    conditioning on an additional context. A single Householder transformation takes\n    the form,\n\n        :math:`\\mathbf{y} = (I - 2*\\frac{\\mathbf{u}\\mathbf{u}^T}{||\\mathbf{u}||^2})\\mathbf{x}`\n\n    where :math:`\\mathbf{x}` are the inputs with dimension :math:`D`,\n    :math:`\\mathbf{y}` are the outputs, and :math:`\\mathbf{u}\\in\\mathbb{R}^D`\n    is the output of a function, e.g. a NN, with input :math:`z\\in\\mathbb{R}^{M}`\n    representing the context variable to condition on.\n\n    The transformation represents the reflection of :math:`\\mathbf{x}` through the\n    plane passing through the origin with normal :math:`\\mathbf{u}`.\n\n    :math:`D` applications of this transformation are able to transform standard\n    i.i.d. standard Gaussian noise into a Gaussian variable with an arbitrary\n    covariance matrix. With :math:`K<D` transformations, one is able to approximate\n    a full-rank Gaussian distribution using a linear transformation of rank\n    :math:`K`.\n\n    Together with :class:`~pyro.distributions.ConditionalTransformedDistribution`\n    this provides a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> from pyro.nn.dense_nn import DenseNN\n    >>> input_dim = 10\n    >>> context_dim = 5\n    >>> batch_size = 3\n    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n    >>> param_dims = [input_dim]\n    >>> hypernet = DenseNN(context_dim, [50, 50], param_dims)\n    >>> transform = ConditionalHouseholder(input_dim, hypernet)\n    >>> z = torch.rand(batch_size, context_dim)\n    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,\n    ... [transform]).condition(z)\n    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP\n\n    :param input_dim: the dimension of the input (and output) variable.\n    :type input_dim: int\n    :param nn: a function inputting the context variable and outputting a triplet of\n        real-valued parameters of dimensions :math:`(1, D, D)`.\n    :type nn: callable\n    :param count_transforms: number of applications of Householder transformation to\n        apply.\n    :type count_transforms: int\n\n    References:\n\n    [1] Jakub M. Tomczak, Max Welling. Improving Variational Auto-Encoders using\n    Householder Flow. [arXiv:1611.09630]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, input_dim, nn, count_transforms=1):\n        super().__init__()\n        self.nn = nn\n        self.input_dim = input_dim\n        if count_transforms < 1:\n            raise ValueError(\'Number of Householder transforms, {}, is less than 1!\'.format(count_transforms))\n        elif count_transforms > input_dim:\n            warnings.warn(\n                ""Number of Householder transforms, {}, is greater than input dimension {}, which is an \\\nover-parametrization!"".format(count_transforms, input_dim))\n        self.count_transforms = count_transforms\n\n    def condition(self, context):\n        # u_unnormed ~ (count_transforms, input_dim)\n        # Hence, input_dim must divide\n        u_unnormed = self.nn(context)\n        if self.count_transforms == 1:\n            u_unnormed = u_unnormed.unsqueeze(-2)\n        else:\n            u_unnormed = torch.stack(u_unnormed, dim=-2)\n        return ConditionedHouseholder(u_unnormed)\n\n\ndef householder(input_dim, count_transforms=None):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.Householder` object for consistency with\n    other helpers.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param count_transforms: number of applications of Householder transformation to\n        apply.\n    :type count_transforms: int\n\n    """"""\n\n    if count_transforms is None:\n        count_transforms = input_dim // 2 + 1\n    return Householder(input_dim, count_transforms=count_transforms)\n\n\ndef conditional_householder(input_dim, context_dim, hidden_dims=None, count_transforms=1):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.ConditionalHouseholder` object that takes\n    care of constructing a dense network with the correct input/output dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param context_dim: Dimension of context variable\n    :type context_dim: int\n    :param hidden_dims: The desired hidden dimensions of the dense network. Defaults\n        to using [input_dim * 10, input_dim * 10]\n    :type hidden_dims: list[int]\n\n    """"""\n\n    if hidden_dims is None:\n        hidden_dims = [input_dim * 10, input_dim * 10]\n    nn = DenseNN(context_dim, hidden_dims, param_dims=[input_dim] * count_transforms)\n    return ConditionalHouseholder(input_dim, nn, count_transforms)\n'"
pyro/distributions/transforms/lower_cholesky_affine.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import Transform\n\nfrom pyro.distributions.util import copy_docs_from\n\n\n@copy_docs_from(Transform)\nclass LowerCholeskyAffine(Transform):\n    """"""\n    A bijection of the form,\n\n        :math:`\\\\mathbf{y} = \\\\mathbf{L} \\\\mathbf{x} + \\\\mathbf{r}`\n\n    where `\\\\mathbf{L}` is a lower triangular matrix and `\\\\mathbf{r}` is a vector.\n\n    :param loc: the fixed D-dimensional vector to shift the input by.\n    :type loc: torch.tensor\n    :param scale_tril: the D x D lower triangular matrix used in the transformation.\n    :type scale_tril: torch.tensor\n\n    """"""\n    codomain = constraints.real_vector\n    bijective = True\n    event_dim = 1\n    volume_preserving = False\n\n    def __init__(self, loc, scale_tril, cache_size=0):\n        super().__init__(cache_size=cache_size)\n        self.loc = loc\n        self.scale_tril = scale_tril\n        assert loc.size(-1) == scale_tril.size(-1) == scale_tril.size(-2), \\\n            ""loc and scale_tril must be of size D and D x D, respectively (instead: {}, {})"".format(loc.shape,\n                                                                                                    scale_tril.shape)\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        return torch.matmul(self.scale_tril, x.unsqueeze(-1)).squeeze(-1) + self.loc\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x.\n        """"""\n        return torch.triangular_solve((y - self.loc).unsqueeze(-1), self.scale_tril,\n                                      upper=False, transpose=False)[0].squeeze(-1)\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian, i.e.\n        log(abs(dy/dx)).\n        """"""\n        return torch.ones(x.size()[:-1], dtype=x.dtype, layout=x.layout, device=x.device) * \\\n            self.scale_tril.diag().log().sum()\n\n    def with_cache(self, cache_size=1):\n        if self._cache_size == cache_size:\n            return self\n        return LowerCholeskyAffine(self.loc, self.scale_tril, cache_size=cache_size)\n'"
pyro/distributions/transforms/neural_autoregressive.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import SigmoidTransform, TanhTransform\n\nfrom pyro.distributions.conditional import ConditionalTransformModule\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.transforms.basic import ELUTransform, LeakyReLUTransform\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import AutoRegressiveNN, ConditionalAutoRegressiveNN\n\neps = 1e-8\n\n\n@copy_docs_from(TransformModule)\nclass NeuralAutoregressive(TransformModule):\n    r""""""\n    An implementation of the deep Neural Autoregressive Flow (NAF) bijective\n    transform of the ""IAF flavour"" that can be used for sampling and scoring samples\n    drawn from it (but not arbitrary ones).\n\n    Example usage:\n\n    >>> from pyro.nn import AutoRegressiveNN\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> arn = AutoRegressiveNN(10, [40], param_dims=[16]*3)\n    >>> transform = NeuralAutoregressive(arn, hidden_units=16)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    The inverse operation is not implemented. This would require numerical\n    inversion, e.g., using a root finding method - a possibility for a future\n    implementation.\n\n    :param autoregressive_nn: an autoregressive neural network whose forward call\n        returns a tuple of three real-valued tensors, whose last dimension is the\n        input dimension, and whose penultimate dimension is equal to hidden_units.\n    :type autoregressive_nn: nn.Module\n    :param hidden_units: the number of hidden units to use in the NAF transformation\n        (see Eq (8) in reference)\n    :type hidden_units: int\n    :param activation: Activation function to use. One of \'ELU\', \'LeakyReLU\',\n        \'sigmoid\', or \'tanh\'.\n    :type activation: string\n\n    Reference:\n\n    [1] Chin-Wei Huang, David Krueger, Alexandre Lacoste, Aaron Courville. Neural\n    Autoregressive Flows. [arXiv:1804.00779]\n\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n    eps = 1e-8\n    autoregressive = True\n\n    def __init__(self, autoregressive_nn, hidden_units=16, activation=\'sigmoid\'):\n        super().__init__(cache_size=1)\n\n        # Create the intermediate transform used\n        name_to_mixin = {\n            \'ELU\': ELUTransform,\n            \'LeakyReLU\': LeakyReLUTransform,\n            \'sigmoid\': SigmoidTransform,\n            \'tanh\': TanhTransform}\n        if activation not in name_to_mixin:\n            raise ValueError(\'Invalid activation function ""{}""\'.format(activation))\n        self.T = name_to_mixin[activation]()\n\n        self.arn = autoregressive_nn\n        self.hidden_units = hidden_units\n        self.logsoftmax = nn.LogSoftmax(dim=-2)\n        self._cached_log_df_inv_dx = None\n        self._cached_A = None\n        self._cached_W_pre = None\n        self._cached_C = None\n        self._cached_T_C = None\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        # A, W, b ~ batch_shape x hidden_units x event_shape\n        A, W_pre, b = self.arn(x)\n        T = self.T\n\n        # Divide the autoregressive output into the component activations\n        A = F.softplus(A)\n        C = A * x.unsqueeze(-2) + b\n        W = F.softmax(W_pre, dim=-2)\n        T_C = T(C)\n        D = (W * T_C).sum(dim=-2)\n        y = T.inv(D)\n\n        self._cached_log_df_inv_dx = T.inv.log_abs_det_jacobian(D, y)\n        self._cached_A = A\n        self._cached_W_pre = W_pre\n        self._cached_C = C\n        self._cached_T_C = T_C\n\n        return y\n\n    # This method returns log(abs(det(dy/dx)), which is equal to -log(abs(det(dx/dy))\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian\n        """"""\n\n        A = self._cached_A\n        W_pre = self._cached_W_pre\n        C = self._cached_C\n        T_C = self._cached_T_C\n        T = self.T\n\n        log_dydD = self._cached_log_df_inv_dx\n        log_dDdx = torch.logsumexp(torch.log(A + self.eps) + self.logsoftmax(W_pre) +\n                                   T.log_abs_det_jacobian(C, T_C), dim=-2)\n        log_det = log_dydD + log_dDdx\n        return log_det.sum(-1)\n\n\n@copy_docs_from(ConditionalTransformModule)\nclass ConditionalNeuralAutoregressive(ConditionalTransformModule):\n    r""""""\n    An implementation of the deep Neural Autoregressive Flow (NAF) bijective\n    transform of the ""IAF flavour"" conditioning on an additiona context variable\n    that can be used for sampling and scoring samples drawn from it (but not\n    arbitrary ones).\n\n    Example usage:\n\n    >>> from pyro.nn import ConditionalAutoRegressiveNN\n    >>> input_dim = 10\n    >>> context_dim = 5\n    >>> batch_size = 3\n    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n    >>> arn = ConditionalAutoRegressiveNN(input_dim, context_dim, [40],\n    ... param_dims=[16]*3)\n    >>> transform = ConditionalNeuralAutoregressive(arn, hidden_units=16)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> z = torch.rand(batch_size, context_dim)\n    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,\n    ... [transform]).condition(z)\n    >>> flow_dist.sample(sample_shape=torch.Size([batch_size]))  # doctest: +SKIP\n\n    The inverse operation is not implemented. This would require numerical\n    inversion, e.g., using a root finding method - a possibility for a future\n    implementation.\n\n    :param autoregressive_nn: an autoregressive neural network whose forward call\n        returns a tuple of three real-valued tensors, whose last dimension is the\n        input dimension, and whose penultimate dimension is equal to hidden_units.\n    :type autoregressive_nn: nn.Module\n    :param hidden_units: the number of hidden units to use in the NAF transformation\n        (see Eq (8) in reference)\n    :type hidden_units: int\n    :param activation: Activation function to use. One of \'ELU\', \'LeakyReLU\',\n        \'sigmoid\', or \'tanh\'.\n    :type activation: string\n\n    Reference:\n\n    [1] Chin-Wei Huang, David Krueger, Alexandre Lacoste, Aaron Courville. Neural\n    Autoregressive Flows. [arXiv:1804.00779]\n\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, autoregressive_nn, **kwargs):\n        super().__init__()\n        self.nn = autoregressive_nn\n        self.kwargs = kwargs\n\n    def condition(self, context):\n        """"""\n        Conditions on a context variable, returning a non-conditional transform of\n        of type :class:`~pyro.distributions.transforms.NeuralAutoregressive`.\n        """"""\n\n        # Note that nn.condition doesn\'t copy the weights of the ConditionalAutoregressiveNN\n        cond_nn = partial(self.nn, context=context)\n        cond_nn.permutation = cond_nn.func.permutation\n        cond_nn.get_permutation = cond_nn.func.get_permutation\n        return NeuralAutoregressive(cond_nn, **self.kwargs)\n\n\ndef neural_autoregressive(input_dim, hidden_dims=None, activation=\'sigmoid\', width=16):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.NeuralAutoregressive` object that takes\n    care of constructing an autoregressive network with the correct input/output\n    dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param hidden_dims: The desired hidden dimensions of the autoregressive network.\n        Defaults to using [3*input_dim + 1]\n    :type hidden_dims: list[int]\n    :param activation: Activation function to use. One of \'ELU\', \'LeakyReLU\',\n        \'sigmoid\', or \'tanh\'.\n    :type activation: string\n    :param width: The width of the ""multilayer perceptron"" in the transform (see\n        paper). Defaults to 16\n    :type width: int\n\n    """"""\n\n    if hidden_dims is None:\n        hidden_dims = [3 * input_dim + 1]\n    arn = AutoRegressiveNN(input_dim, hidden_dims, param_dims=[width] * 3)\n    return NeuralAutoregressive(arn, hidden_units=width, activation=activation)\n\n\ndef conditional_neural_autoregressive(input_dim, context_dim, hidden_dims=None, activation=\'sigmoid\', width=16):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.ConditionalNeuralAutoregressive` object\n    that takes care of constructing an autoregressive network with the correct\n    input/output dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param context_dim: Dimension of context variable\n    :type context_dim: int\n    :param hidden_dims: The desired hidden dimensions of the autoregressive network.\n        Defaults to using [3*input_dim + 1]\n    :type hidden_dims: list[int]\n    :param activation: Activation function to use. One of \'ELU\', \'LeakyReLU\',\n        \'sigmoid\', or \'tanh\'.\n    :type activation: string\n    :param width: The width of the ""multilayer perceptron"" in the transform (see\n        paper). Defaults to 16\n    :type width: int\n\n    """"""\n\n    if hidden_dims is None:\n        hidden_dims = [3 * input_dim + 1]\n    arn = ConditionalAutoRegressiveNN(input_dim, context_dim, hidden_dims, param_dims=[width] * 3)\n    return ConditionalNeuralAutoregressive(arn, hidden_units=width, activation=activation)\n'"
pyro/distributions/transforms/permute.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import Transform\nfrom torch.distributions.utils import lazy_property\n\nfrom pyro.distributions.util import copy_docs_from\n\n\n@copy_docs_from(Transform)\nclass Permute(Transform):\n    r""""""\n    A bijection that reorders the input dimensions, that is, multiplies the input by\n    a permutation matrix. This is useful in between\n    :class:`~pyro.distributions.transforms.AffineAutoregressive` transforms to\n    increase the flexibility of the resulting distribution and stabilize learning.\n    Whilst not being an autoregressive transform, the log absolute determinate of\n    the Jacobian is easily calculable as 0. Note that reordering the input dimension\n    between two layers of\n    :class:`~pyro.distributions.transforms.AffineAutoregressive` is not equivalent\n    to reordering the dimension inside the MADE networks that those IAFs use; using\n    a :class:`~pyro.distributions.transforms.Permute` transform results in a\n    distribution with more flexibility.\n\n    Example usage:\n\n    >>> from pyro.nn import AutoRegressiveNN\n    >>> from pyro.distributions.transforms import AffineAutoregressive, Permute\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> iaf1 = AffineAutoregressive(AutoRegressiveNN(10, [40]))\n    >>> ff = Permute(torch.randperm(10, dtype=torch.long))\n    >>> iaf2 = AffineAutoregressive(AutoRegressiveNN(10, [40]))\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [iaf1, ff, iaf2])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    :param permutation: a permutation ordering that is applied to the inputs.\n    :type permutation: torch.LongTensor\n    :param dim: the tensor dimension to permute. This value must be negative and\n        defines the event dim as `abs(dim)`.\n    :type dim: int\n\n    """"""\n\n    codomain = constraints.real\n    bijective = True\n    volume_preserving = True\n\n    def __init__(self, permutation, *, dim=-1, cache_size=1):\n        super().__init__(cache_size=cache_size)\n\n        if dim >= 0:\n            raise ValueError(""\'dim\' keyword argument must be negative"")\n\n        self.permutation = permutation\n        self.dim = dim\n        self.event_dim = -dim\n\n    @lazy_property\n    def inv_permutation(self):\n        result = torch.empty_like(self.permutation, dtype=torch.long)\n        result[self.permutation] = torch.arange(self.permutation.size(0),\n                                                dtype=torch.long,\n                                                device=self.permutation.device)\n        return result\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n\n        return x.index_select(self.dim, self.permutation)\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x.\n        """"""\n        return y.index_select(self.dim, self.inv_permutation)\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian, i.e.\n        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n        transform is not autoregressive, so the log Jacobian is not the sum of the\n        previous expression. However, it turns out it\'s always 0 (since the\n        determinant is -1 or +1), and so returning a vector of zeros works.\n        """"""\n\n        return torch.zeros(x.size()[:-self.event_dim], dtype=x.dtype, layout=x.layout, device=x.device)\n\n    def with_cache(self, cache_size=1):\n        if self._cache_size == cache_size:\n            return self\n        return Permute(self.permutation, cache_size=cache_size)\n\n\ndef permute(input_dim, permutation=None, dim=-1):\n    """"""\n    A helper function to create a :class:`~pyro.distributions.transforms.Permute`\n    object for consistency with other helpers.\n\n    :param input_dim: Dimension(s) of input variable to permute. Note that when\n        `dim < -1` this must be a tuple corresponding to the event shape.\n    :type input_dim: int\n    :param permutation: Torch tensor of integer indices representing permutation.\n        Defaults to a random permutation.\n    :type permutation: torch.LongTensor\n    :param dim: the tensor dimension to permute. This value must be negative and\n        defines the event dim as `abs(dim)`.\n    :type dim: int\n\n    """"""\n    if dim < -1 or not isinstance(input_dim, int):\n        if len(input_dim) != -dim:\n            raise ValueError(\'event shape {} must have same length as event_dim {}\'.format(input_dim, -dim))\n        input_dim = input_dim[dim]\n\n    if permutation is None:\n        permutation = torch.randperm(input_dim)\n    return Permute(permutation, dim=dim)\n'"
pyro/distributions/transforms/planar.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Transform, constraints\n\nfrom pyro.distributions.conditional import ConditionalTransformModule\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import DenseNN\n\n\n@copy_docs_from(Transform)\nclass ConditionedPlanar(Transform):\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, bias=None, u=None, w=None):\n        super().__init__(cache_size=1)\n        self.bias = bias\n        self.u = u\n        self.w = w\n        self._cached_logDetJ = None\n\n    # This method ensures that torch(u_hat, w) > -1, required for invertibility\n    def u_hat(self, u, w):\n        alpha = torch.matmul(u.unsqueeze(-2), w.unsqueeze(-1)).squeeze(-1)\n        a_prime = -1 + F.softplus(alpha)\n        return u + (a_prime - alpha) * w.div(w.pow(2).sum(dim=-1, keepdim=True))\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n        Invokes the bijection x => y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n\n        # x ~ (batch_size, dim_size, 1)\n        # w ~ (batch_size, 1, dim_size)\n        # bias ~ (batch_size, 1)\n        act = torch.tanh(torch.matmul(self.w.unsqueeze(-2), x.unsqueeze(-1)).squeeze(-1) + self.bias)\n        u_hat = self.u_hat(self.u, self.w)\n        y = x + u_hat * act\n\n        psi_z = (1. - act.pow(2)) * self.w\n        self._cached_logDetJ = torch.log(\n            torch.abs(1 + torch.matmul(psi_z.unsqueeze(-2), u_hat.unsqueeze(-1)).squeeze(-1).squeeze(-1)))\n\n        return y\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n        Inverts y => x. As noted above, this implementation is incapable of\n        inverting arbitrary values `y`; rather it assumes `y` is the result of a\n        previously computed application of the bijector to some `x` (which was\n        cached on the forward call)\n        """"""\n\n        raise KeyError(""ConditionedPlanar object expected to find key in intermediates cache but didn\'t"")\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian\n        """"""\n        x_old, y_old = self._cached_x_y\n        if x is not x_old or y is not y_old:\n            # This call to the parent class Transform will update the cache\n            # as well as calling self._call and recalculating y and log_detJ\n            self(x)\n\n        return self._cached_logDetJ\n\n\n@copy_docs_from(ConditionedPlanar)\nclass Planar(ConditionedPlanar, TransformModule):\n    r""""""\n    A \'planar\' bijective transform with equation,\n\n        :math:`\\mathbf{y} = \\mathbf{x} + \\mathbf{u}\\tanh(\\mathbf{w}^T\\mathbf{z}+b)`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    and the learnable parameters are :math:`b\\in\\mathbb{R}`,\n    :math:`\\mathbf{u}\\in\\mathbb{R}^D`, :math:`\\mathbf{w}\\in\\mathbb{R}^D` for\n    input dimension :math:`D`. For this to be an invertible transformation, the\n    condition :math:`\\mathbf{w}^T\\mathbf{u}>-1` is enforced.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` this provides\n    a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> transform = Planar(10)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    The inverse of this transform does not possess an analytical solution and is\n    left unimplemented. However, the inverse is cached when the forward operation is\n    called during sampling, and so samples drawn using the planar transform can be\n    scored.\n\n    :param input_dim: the dimension of the input (and output) variable.\n    :type input_dim: int\n\n    References:\n\n    [1] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with\n    Normalizing Flows. [arXiv:1505.05770]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, input_dim):\n        super().__init__()\n\n        self.bias = nn.Parameter(torch.Tensor(1,))\n        self.u = nn.Parameter(torch.Tensor(input_dim,))\n        self.w = nn.Parameter(torch.Tensor(input_dim,))\n        self.input_dim = input_dim\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.u.size(0))\n        self.w.data.uniform_(-stdv, stdv)\n        self.u.data.uniform_(-stdv, stdv)\n        self.bias.data.zero_()\n\n\n@copy_docs_from(ConditionalTransformModule)\nclass ConditionalPlanar(ConditionalTransformModule):\n    r""""""\n    A conditional \'planar\' bijective transform using the equation,\n\n        :math:`\\mathbf{y} = \\mathbf{x} + \\mathbf{u}\\tanh(\\mathbf{w}^T\\mathbf{z}+b)`\n\n    where :math:`\\mathbf{x}` are the inputs with dimension :math:`D`,\n    :math:`\\mathbf{y}` are the outputs, and the pseudo-parameters\n    :math:`b\\in\\mathbb{R}`, :math:`\\mathbf{u}\\in\\mathbb{R}^D`, and\n    :math:`\\mathbf{w}\\in\\mathbb{R}^D` are the output of a function, e.g. a NN,\n    with input :math:`z\\in\\mathbb{R}^{M}` representing the context variable to\n    condition on. For this to be an invertible transformation, the condition\n    :math:`\\mathbf{w}^T\\mathbf{u}>-1` is enforced.\n\n    Together with :class:`~pyro.distributions.ConditionalTransformedDistribution`\n    this provides a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> from pyro.nn.dense_nn import DenseNN\n    >>> input_dim = 10\n    >>> context_dim = 5\n    >>> batch_size = 3\n    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n    >>> param_dims = [1, input_dim, input_dim]\n    >>> hypernet = DenseNN(context_dim, [50, 50], param_dims)\n    >>> transform = ConditionalPlanar(hypernet)\n    >>> z = torch.rand(batch_size, context_dim)\n    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,\n    ... [transform]).condition(z)\n    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP\n\n    The inverse of this transform does not possess an analytical solution and is\n    left unimplemented. However, the inverse is cached when the forward operation is\n    called during sampling, and so samples drawn using the planar transform can be\n    scored.\n\n    :param nn: a function inputting the context variable and outputting a triplet of\n        real-valued parameters of dimensions :math:`(1, D, D)`.\n    :type nn: callable\n\n    References:\n    [1] Variational Inference with Normalizing Flows [arXiv:1505.05770]\n    Danilo Jimenez Rezende, Shakir Mohamed\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, nn):\n        super().__init__()\n        self.nn = nn\n\n    def condition(self, context):\n        bias, u, w = self.nn(context)\n        return ConditionedPlanar(bias, u, w)\n\n\ndef planar(input_dim):\n    """"""\n    A helper function to create a :class:`~pyro.distributions.transforms.Planar`\n    object for consistency with other helpers.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n\n    """"""\n\n    return Planar(input_dim)\n\n\ndef conditional_planar(input_dim, context_dim, hidden_dims=None):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.ConditionalPlanar` object that takes care\n    of constructing a dense network with the correct input/output dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param context_dim: Dimension of context variable\n    :type context_dim: int\n    :param hidden_dims: The desired hidden dimensions of the dense network. Defaults\n        to using [input_dim * 10, input_dim * 10]\n    :type hidden_dims: list[int]\n\n    """"""\n\n    if hidden_dims is None:\n        hidden_dims = [input_dim * 10, input_dim * 10]\n    nn = DenseNN(context_dim, hidden_dims, param_dims=[1, input_dim, input_dim])\n    return ConditionalPlanar(nn)\n'"
pyro/distributions/transforms/polynomial.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import AutoRegressiveNN\n\n\n@copy_docs_from(TransformModule)\nclass Polynomial(TransformModule):\n    r""""""\n    An autoregressive bijective transform as described in Jaini et al. (2019)\n    applying following equation element-wise,\n\n        :math:`y_n = c_n + \\int^{x_n}_0\\sum^K_{k=1}\\left(\\sum^R_{r=0}a^{(n)}_{r,k}u^r\\right)du`\n\n    where :math:`x_n` is the :math:`n`th input, :math:`y_n` is the :math:`n`th\n    output, and :math:`c_n\\in\\mathbb{R}`,\n    :math:`\\left\\{a^{(n)}_{r,k}\\in\\mathbb{R}\\right\\}` are learnable parameters\n    that are the output of an autoregressive NN inputting\n    :math:`x_{\\prec n}={x_1,x_2,\\ldots,x_{n-1}}`.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` this provides\n    a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> from pyro.nn import AutoRegressiveNN\n    >>> input_dim = 10\n    >>> count_degree = 4\n    >>> count_sum = 3\n    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n    >>> param_dims = [(count_degree + 1)*count_sum]\n    >>> arn = AutoRegressiveNN(input_dim, [input_dim*10], param_dims)\n    >>> transform = Polynomial(arn, input_dim=input_dim, count_degree=count_degree,\n    ... count_sum=count_sum)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    The inverse of this transform does not possess an analytical solution and is\n    left unimplemented. However, the inverse is cached when the forward operation is\n    called during sampling, and so samples drawn using a polynomial transform can be\n    scored.\n\n    :param autoregressive_nn: an autoregressive neural network whose forward call\n        returns a tensor of real-valued\n        numbers of size (batch_size, (count_degree+1)*count_sum, input_dim)\n    :type autoregressive_nn: nn.Module\n    :param count_degree: The degree of the polynomial to use for each element-wise\n        transformation.\n    :type count_degree: int\n    :param count_sum: The number of polynomials to sum in each element-wise\n        transformation.\n    :type count_sum: int\n\n    References:\n\n    [1] Priyank Jaini, Kira A. Shelby, Yaoliang Yu. Sum-of-squares polynomial flow.\n    [arXiv:1905.02325]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n    autoregressive = True\n\n    def __init__(self, autoregressive_nn, input_dim, count_degree, count_sum):\n        super().__init__(cache_size=1)\n\n        self.arn = autoregressive_nn\n        self.input_dim = input_dim\n        self.count_degree = count_degree\n        self.count_sum = count_sum\n        self._cached_logDetJ = None\n\n        self.c = nn.Parameter(torch.Tensor(input_dim))\n        self.reset_parameters()\n\n        # Vector of powers of input dimension\n        powers = torch.arange(1, count_degree + 2, dtype=torch.get_default_dtype())\n        self.register_buffer(\'powers\', powers)\n\n        # Build mask of constants\n        mask = self.powers + torch.arange(count_degree + 1).unsqueeze(-1).type_as(powers)\n        power_mask = mask\n        mask = mask.reciprocal()\n\n        self.register_buffer(\'power_mask\', power_mask)\n        self.register_buffer(\'mask\', mask)\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.c.size(0))\n        self.c.data.uniform_(-stdv, stdv)\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        # Calculate the polynomial coefficients\n        # ~ (batch_size, count_sum, count_degree+1, input_dim)\n        A = self.arn(x).view(-1, self.count_sum, self.count_degree + 1, self.input_dim)\n\n        # Take cross product of coefficients across degree dim\n        # ~ (batch_size, count_sum, count_degree+1, count_degree+1, input_dim)\n        coefs = A.unsqueeze(-2) * A.unsqueeze(-3)\n\n        # Calculate output as sum-of-squares polynomial\n        x_view = x.view(-1, 1, 1, self.input_dim)\n        x_pow_matrix = x_view.pow(self.power_mask.unsqueeze(-1)).unsqueeze(-4)\n\n        # Eq (8) from the paper, expanding the squared term and integrating\n        # NOTE: The view_as is necessary because the batch dimensions were collapsed previously\n        y = self.c + (coefs * x_pow_matrix * self.mask.unsqueeze(-1)).sum((1, 2, 3)).view_as(x)\n\n        # log(|det(J)|) is calculated by the fundamental theorem of calculus, i.e. remove the constant\n        # term and the integral from eq (8) (the equation for this isn\'t given in the paper)\n        x_pow_matrix = x_view.pow(self.power_mask.unsqueeze(-1) - 1).unsqueeze(-4)\n        self._cached_logDetJ = torch.log((coefs * x_pow_matrix).sum((1, 2, 3)).view_as(x) + 1e-8).sum(-1)\n\n        return y\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x. As noted above, this implementation is incapable of\n        inverting arbitrary values `y`; rather it assumes `y` is the result of a\n        previously computed application of the bijector to some `x` (which was\n        cached on the forward call)\n        """"""\n\n        raise KeyError(""Polynomial object expected to find key in intermediates cache but didn\'t"")\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian\n        """"""\n        x_old, y_old = self._cached_x_y\n        if x is not x_old or y is not y_old:\n            # This call to the parent class Transform will update the cache\n            # as well as calling self._call and recalculating y and log_detJ\n            self(x)\n\n        return self._cached_logDetJ\n\n\ndef polynomial(input_dim, hidden_dims=None):\n    """"""\n    A helper function to create a :class:`~pyro.distributions.transforms.Polynomial`\n    object that takes care of constructing an autoregressive network with the\n    correct input/output dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param hidden_dims: The desired hidden dimensions of of the autoregressive\n        network. Defaults to using [input_dim * 10]\n\n    """"""\n\n    count_degree = 4\n    count_sum = 3\n    if hidden_dims is None:\n        hidden_dims = [input_dim * 10]\n    arn = AutoRegressiveNN(input_dim, hidden_dims, param_dims=[(count_degree + 1) * count_sum])\n    return Polynomial(arn, input_dim=input_dim, count_degree=count_degree, count_sum=count_sum)\n'"
pyro/distributions/transforms/radial.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Transform, constraints\n\nfrom pyro.distributions.conditional import ConditionalTransformModule\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import DenseNN\n\n\n@copy_docs_from(Transform)\nclass ConditionedRadial(Transform):\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, x0=None, alpha_prime=None, beta_prime=None):\n        super().__init__(cache_size=1)\n        self.x0 = x0\n        self.alpha_prime = alpha_prime\n        self.beta_prime = beta_prime\n        self._cached_logDetJ = None\n\n    # This method ensures that torch(u_hat, w) > -1, required for invertibility\n    def u_hat(self, u, w):\n        alpha = torch.matmul(u.unsqueeze(-2), w.unsqueeze(-1)).squeeze(-1)\n        a_prime = -1 + F.softplus(alpha)\n        return u + (a_prime - alpha) * w.div(w.pow(2).sum(dim=-1, keepdim=True))\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from the base distribution (or the output\n        of a previous transform)\n        """"""\n        # Ensure invertibility using approach in appendix A.2\n        alpha = F.softplus(self.alpha_prime)\n        beta = -alpha + F.softplus(self.beta_prime)\n\n        # Compute y and logDet using Equation 14.\n        diff = x - self.x0\n        r = diff.norm(dim=-1, keepdim=True)\n        h = (alpha + r).reciprocal()\n        h_prime = - (h ** 2)\n        beta_h = beta * h\n\n        self._cached_logDetJ = ((self.x0.size(-1) - 1) * torch.log1p(beta_h) +\n                                torch.log1p(beta_h + beta * h_prime * r)).sum(-1)\n        return x + beta_h * diff\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n        Inverts y => x. As noted above, this implementation is incapable of\n        inverting arbitrary values `y`; rather it assumes `y` is the result of a\n        previously computed application of the bijector to some `x` (which was\n        cached on the forward call)\n        """"""\n\n        raise KeyError(""ConditionedRadial object expected to find key in intermediates cache but didn\'t"")\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian\n        """"""\n        x_old, y_old = self._cached_x_y\n        if x is not x_old or y is not y_old:\n            # This call to the parent class Transform will update the cache\n            # as well as calling self._call and recalculating y and log_detJ\n            self(x)\n\n        return self._cached_logDetJ\n\n\n@copy_docs_from(ConditionedRadial)\nclass Radial(ConditionedRadial, TransformModule):\n    r""""""\n    A \'radial\' bijective transform using the equation,\n\n        :math:`\\mathbf{y} = \\mathbf{x} + \\beta h(\\alpha,r)(\\mathbf{x} - \\mathbf{x}_0)`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    and the learnable parameters are :math:`\\alpha\\in\\mathbb{R}^+`,\n    :math:`\\beta\\in\\mathbb{R}`, :math:`\\mathbf{x}_0\\in\\mathbb{R}^D`, for input\n    dimension :math:`D`, :math:`r=||\\mathbf{x}-\\mathbf{x}_0||_2`,\n    :math:`h(\\alpha,r)=1/(\\alpha+r)`. For this to be an invertible transformation,\n    the condition :math:`\\beta>-\\alpha` is enforced.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` this provides\n    a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> transform = Radial(10)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    The inverse of this transform does not possess an analytical solution and is\n    left unimplemented. However, the inverse is cached when the forward operation is\n    called during sampling, and so samples drawn using the radial transform can be\n    scored.\n\n    :param input_dim: the dimension of the input (and output) variable.\n    :type input_dim: int\n\n    References:\n\n    [1] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with\n    Normalizing Flows. [arXiv:1505.05770]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, input_dim):\n        super().__init__()\n\n        self.x0 = nn.Parameter(torch.Tensor(input_dim,))\n        self.alpha_prime = nn.Parameter(torch.Tensor(1,))\n        self.beta_prime = nn.Parameter(torch.Tensor(1,))\n        self.input_dim = input_dim\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.x0.size(0))\n        self.alpha_prime.data.uniform_(-stdv, stdv)\n        self.beta_prime.data.uniform_(-stdv, stdv)\n        self.x0.data.uniform_(-stdv, stdv)\n\n\n@copy_docs_from(ConditionalTransformModule)\nclass ConditionalRadial(ConditionalTransformModule):\n    r""""""\n    A conditional \'radial\' bijective transform context using the equation,\n\n        :math:`\\mathbf{y} = \\mathbf{x} + \\beta h(\\alpha,r)(\\mathbf{x} - \\mathbf{x}_0)`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    and :math:`\\alpha\\in\\mathbb{R}^+`, :math:`\\beta\\in\\mathbb{R}`,\n    and :math:`\\mathbf{x}_0\\in\\mathbb{R}^D`, are the output of a function, e.g. a NN,\n    with input :math:`z\\in\\mathbb{R}^{M}` representing the context variable to\n    condition on. The input dimension is :math:`D`,\n    :math:`r=||\\mathbf{x}-\\mathbf{x}_0||_2`, and :math:`h(\\alpha,r)=1/(\\alpha+r)`.\n    For this to be an invertible transformation, the condition :math:`\\beta>-\\alpha`\n    is enforced.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` this provides\n    a way to create richer variational approximations.\n\n    Example usage:\n\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> transform = Radial(10)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    The inverse of this transform does not possess an analytical solution and is\n    left unimplemented. However, the inverse is cached when the forward operation is\n    called during sampling, and so samples drawn using the radial transform can be\n    scored.\n\n    :param input_dim: the dimension of the input (and output) variable.\n    :type input_dim: int\n\n    References:\n\n    [1] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with\n    Normalizing Flows. [arXiv:1505.05770]\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, nn):\n        super().__init__()\n        self.nn = nn\n\n    def condition(self, context):\n        x0, alpha_prime, beta_prime = self.nn(context)\n        return ConditionedRadial(x0, alpha_prime, beta_prime)\n\n\ndef radial(input_dim):\n    """"""\n    A helper function to create a :class:`~pyro.distributions.transforms.Radial`\n    object for consistency with other helpers.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n\n    """"""\n\n    return Radial(input_dim)\n\n\ndef conditional_radial(input_dim, context_dim, hidden_dims=None):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.ConditionalRadial` object that takes care\n    of constructing a dense network with the correct input/output dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param context_dim: Dimension of context variable\n    :type context_dim: int\n    :param hidden_dims: The desired hidden dimensions of the dense network. Defaults\n        to using [input_dim * 10, input_dim * 10]\n    :type hidden_dims: list[int]\n\n\n    """"""\n\n    if hidden_dims is None:\n        hidden_dims = [input_dim * 10, input_dim * 10]\n    nn = DenseNN(context_dim, hidden_dims, param_dims=[input_dim, 1, 1])\n    return ConditionalRadial(nn)\n'"
pyro/distributions/transforms/spline.py,22,"b'# Copyright Contributors to the Pyro project.\n# Copyright (c) 2020 Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie\n# Copyright (c) 2019 Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios\n# Copyright (c) 2019 Tony Duan\n# SPDX-License-Identifier: MIT\n\n# This implementation is adapted in part from:\n# * https://github.com/tonyduan/normalizing-flows/blob/master/nf/flows.py; and,\n# * https://github.com/hmdolatabadi/LRS_NF/blob/master/nde/transforms/nonlinearities.py,\n# under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Transform, constraints\n\nfrom pyro.distributions.conditional import ConditionalTransformModule\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.util import copy_docs_from\nfrom pyro.nn import DenseNN\n\n\ndef _searchsorted(sorted_sequence, values):\n    """"""\n    Searches for which bin an input belongs to (in a way that is parallelizable and\n    amenable to autodiff)\n\n    TODO: Replace with torch.searchsorted once it is released\n    """"""\n    return torch.sum(\n        values[..., None] >= sorted_sequence,\n        dim=-1\n    ) - 1\n\n\ndef _select_bins(x, idx):\n    """"""\n    Performs gather to select the bin in the correct way on batched inputs\n    """"""\n    idx = idx.clamp(min=0, max=x.size(-1) - 1)\n\n    """"""\n    Broadcast dimensions of idx over x\n\n    idx ~ (batch_dims, input_dim, 1)\n    x ~ (context_batch_dims, input_dim, count_bins)\n\n    Note that by convention, the context variable batch dimensions must broadcast\n    over the input batch dimensions.\n    """"""\n    if len(idx.shape) >= len(x.shape):\n        x = x.reshape((1,) * (len(idx.shape) - len(x.shape)) + x.shape)\n        x = x.expand(idx.shape[:-2] + (-1,) * 2)\n\n    return x.gather(-1, idx).squeeze(-1)\n\n\ndef _calculate_knots(lengths, lower, upper):\n    """"""\n    Given a tensor of unscaled bin lengths that sum to 1, plus the lower and upper\n    limits, returns the shifted and scaled lengths plus knot positions\n    """"""\n\n    # Cumulative widths gives x (y for inverse) position of knots\n    knots = torch.cumsum(lengths, dim=-1)\n\n    # Pad left of last dimension with 1 zero to compensate for dim lost to cumsum\n    knots = F.pad(knots, pad=(1, 0), mode=\'constant\', value=0.0)\n\n    # Translate [0,1] knot points to [-B, B]\n    knots = (upper - lower) * knots + lower\n\n    # Convert the knot points back to lengths\n    # NOTE: Are following two lines a necessary fix for accumulation (round-off) error?\n    knots[..., 0] = lower\n    knots[..., -1] = upper\n    lengths = knots[..., 1:] - knots[..., :-1]\n\n    return lengths, knots\n\n\ndef _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas,\n                               inverse=False,\n                               bound=3.,\n                               min_bin_width=1e-3,\n                               min_bin_height=1e-3,\n                               min_derivative=1e-3,\n                               min_lambda=0.025,\n                               eps=1e-6):\n    """"""\n    Calculating a monotonic rational spline (linear for now) or its inverse, plus\n    the log(abs(detJ)) required for normalizing flows.\n    NOTE: I omit the docstring with parameter descriptions for this method since it\n    is not considered ""public"" yet!\n    """"""\n\n    # Ensure bound is positive\n    # NOTE: For simplicity, we apply the identity function outside [-B, B] X [-B, B] rather than allowing arbitrary\n    # corners to the bounding box. If you want a different bounding box you can apply an affine transform before and\n    # after the input\n    assert bound > 0.0\n\n    num_bins = widths.shape[-1]\n    if min_bin_width * num_bins > 1.0:\n        raise ValueError(\'Minimal bin width too large for the number of bins\')\n    if min_bin_height * num_bins > 1.0:\n        raise ValueError(\'Minimal bin height too large for the number of bins\')\n\n    # inputs, inside_interval_mask, outside_interval_mask ~ (batch_dim, input_dim)\n    left, right = -bound, bound\n    bottom, top = -bound, bound\n    inside_interval_mask = (inputs >= left) & (inputs <= right)\n    outside_interval_mask = ~inside_interval_mask\n\n    # outputs, logabsdet ~ (batch_dim, input_dim)\n    outputs = torch.zeros_like(inputs)\n    logabsdet = torch.zeros_like(inputs)\n\n    # For numerical stability, put lower/upper limits on parameters. E.g .give every bin min_bin_width,\n    # then add width fraction of remaining length\n    # NOTE: Do this here rather than higher up because we want everything to ensure numerical\n    # stability within this function\n    widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths\n    heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights\n    derivatives = min_derivative + derivatives\n    lambdas = (1 - 2 * min_lambda) * lambdas + min_lambda\n\n    # Cumulative widths are x (y for inverse) position of knots\n    # Similarly, cumulative heights are y (x for inverse) position of knots\n    widths, cumwidths = _calculate_knots(widths, left, right)\n    heights, cumheights = _calculate_knots(heights, bottom, top)\n\n    # Pad left and right derivatives with fixed values at first and last knots\n    # These are 1 since the function is the identity outside the bounding box and the derivative is continuous\n    # NOTE: Not sure why this is 1.0 - min_derivative rather than 1.0. I\'ve copied this from original implementation\n    derivatives = F.pad(derivatives, pad=(1, 1), mode=\'constant\', value=1.0 - min_derivative)\n\n    # Get the index of the bin that each input is in\n    # bin_idx ~ (batch_dim, input_dim, 1)\n    bin_idx = _searchsorted(cumheights + eps if inverse else cumwidths + eps, inputs)[..., None]\n\n    # Select the value for the relevant bin for the variables used in the main calculation\n    input_widths = _select_bins(widths, bin_idx)\n    input_cumwidths = _select_bins(cumwidths, bin_idx)\n    input_cumheights = _select_bins(cumheights, bin_idx)\n    input_delta = _select_bins(heights / widths, bin_idx)\n    input_derivatives = _select_bins(derivatives, bin_idx)\n    input_derivatives_plus_one = _select_bins(derivatives[..., 1:], bin_idx)\n    input_heights = _select_bins(heights, bin_idx)\n    input_lambdas = _select_bins(lambdas, bin_idx)\n\n    # The weight, w_a, at the left-hand-side of each bin\n    # We are free to choose w_a, so set it to 1\n    wa = 1.0\n\n    # The weight, w_b, at the right-hand-side of each bin\n    # This turns out to be a multiple of the w_a\n    # TODO: Should this be done in log space for numerical stability?\n    wb = torch.sqrt(input_derivatives / input_derivatives_plus_one) * wa\n\n    # The weight, w_c, at the division point of each bin\n    # Recall that each bin is divided into two parts so we have enough d.o.f. to fit spline\n    wc = (input_lambdas * wa * input_derivatives + (1 - input_lambdas) * wb * input_derivatives_plus_one) / input_delta\n\n    # Calculate y coords of bins\n    ya = input_cumheights\n    yb = input_heights + input_cumheights\n    yc = ((1.0 - input_lambdas) * wa * ya + input_lambdas * wb * yb) / ((1.0 - input_lambdas) * wa + input_lambdas * wb)\n\n    # The core monotonic rational spline equation\n    if inverse:\n        numerator = (input_lambdas * wa * (ya - inputs)) * (inputs <= yc).float() \\\n            + ((wc - input_lambdas * wb) * inputs + input_lambdas * wb * yb - wc * yc) * (inputs > yc).float()\n\n        denominator = ((wc - wa) * inputs + wa * ya - wc * yc) * (inputs <= yc).float()\\\n            + ((wc - wb) * inputs + wb * yb - wc * yc) * (inputs > yc).float()\n\n        theta = numerator / denominator\n\n        outputs = theta * input_widths + input_cumwidths\n\n        derivative_numerator = (wa * wc * input_lambdas * (yc - ya) * (inputs <= yc).float()\n                                + wb * wc * (1 - input_lambdas) * (yb - yc) * (inputs > yc).float()) * input_widths\n\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(torch.abs(denominator))\n\n    else:\n        theta = (inputs - input_cumwidths) / input_widths\n\n        numerator = (wa * ya * (input_lambdas - theta) + wc * yc * theta) * (theta <= input_lambdas).float()\\\n            + (wc * yc * (1 - theta) + wb * yb * (theta - input_lambdas)) * (theta > input_lambdas).float()\n\n        denominator = (wa * (input_lambdas - theta) + wc * theta) * (theta <= input_lambdas).float()\\\n            + (wc * (1 - theta) + wb * (theta - input_lambdas)) * (theta > input_lambdas).float()\n\n        outputs = numerator / denominator\n\n        derivative_numerator = (wa * wc * input_lambdas * (yc - ya) * (theta <= input_lambdas).float() +\n                                wb * wc * (1 - input_lambdas) * (yb - yc) * (theta > input_lambdas).float()) \\\n            / input_widths\n\n        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(torch.abs(denominator))\n\n    # Apply the identity function outside the bounding box\n    outputs[outside_interval_mask] = inputs[outside_interval_mask]\n    logabsdet[outside_interval_mask] = 0.0\n    return outputs, logabsdet\n\n\n@copy_docs_from(Transform)\nclass ConditionedSpline(Transform):\n    """"""\n    Helper class to manage learnable splines. One could imagine this as a standard\n    layer in PyTorch...\n    """"""\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 0\n\n    def __init__(self, widths=None, heights=None, derivatives=None, lambdas=None, bound=3.0, order=\'linear\'):\n        super().__init__(cache_size=1)\n\n        self.order = order\n        self.bound = bound\n        self.widths = widths\n        self.heights = heights\n        self.derivatives = derivatives\n        self.lambdas = lambdas\n\n    def _call(self, x):\n        y, log_detJ = self.spline_op(x)\n        self._cache_log_detJ = log_detJ\n        return y\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n\n        Inverts y => x. Uses a previously cached inverse if available,\n        otherwise performs the inversion afresh.\n        """"""\n        x, log_detJ = self.spline_op(y, inverse=True)\n        self._cache_log_detJ = -log_detJ\n        return x\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log jacobian\n        """"""\n        x_old, y_old = self._cached_x_y\n        if x is not x_old or y is not y_old:\n            # This call to the parent class Transform will update the cache\n            # as well as calling self._call and recalculating y and log_detJ\n            self(x)\n\n        return self._cache_log_detJ\n\n    def spline_op(self, x, **kwargs):\n        y, log_detJ = _monotonic_rational_spline(\n            x,\n            self.widths,\n            self.heights,\n            self.derivatives,\n            self.lambdas,\n            bound=self.bound,\n            **kwargs)\n        return y, log_detJ\n\n\n@copy_docs_from(ConditionedSpline)\nclass Spline(ConditionedSpline, TransformModule):\n    r""""""\n    An implementation of the element-wise rational spline bijections of linear and\n    quadratic order (Durkan et al., 2019; Dolatabadi et al., 2020). Rational splines\n    are functions that are comprised of segments that are the ratio of two\n    polynomials. For instance, for the :math:`d`-th dimension and the :math:`k`-th\n    segment on the spline, the function will take the form,\n\n        :math:`y_d = \\frac{\\alpha^{(k)}(x_d)}{\\beta^{(k)}(x_d)},`\n\n    where :math:`\\alpha^{(k)}` and :math:`\\beta^{(k)}` are two polynomials of\n    order :math:`d`. For :math:`d=1`, we say that the spline is linear, and for\n    :math:`d=2`, quadratic. The spline is constructed on the specified bounding box,\n    :math:`[-K,K]\\times[-K,K]`, with the identity function used elsewhere.\n\n    Rational splines offer an excellent combination of functional flexibility whilst\n    maintaining a numerically stable inverse that is of the same computational and\n    space complexities as the forward operation. This element-wise transform permits\n    the accurate represention of complex univariate distributions.\n\n    Example usage:\n\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> transform = Spline(10, count_bins=4, bound=3.)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n\n    :param input_dim: Dimension of the input vector. This is required so we know how\n        many parameters to store.\n    :type input_dim: int\n    :param count_bins: The number of segments comprising the spline.\n    :type count_bins: int\n    :param bound: The quantity :math:`K` determining the bounding box,\n        :math:`[-K,K]\\times[-K,K]`, of the spline.\n    :type bound: float\n    :param order: One of [\'linear\', \'quadratic\'] specifying the order of the spline.\n    :type order: string\n\n    References:\n\n    Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural\n    Spline Flows. NeurIPS 2019.\n\n    Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie. Invertible Generative\n    Modeling using Linear Rational Splines. AISTATS 2020.\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 0\n\n    def __init__(self, input_dim, count_bins=8, bound=3., order=\'linear\'):\n        super(Spline, self).__init__()\n\n        self.input_dim = input_dim\n        self.count_bins = count_bins\n        self.bound = bound\n        self.order = order\n\n        self.unnormalized_widths = nn.Parameter(torch.randn(self.input_dim, self.count_bins))\n        self.unnormalized_heights = nn.Parameter(torch.randn(self.input_dim, self.count_bins))\n        self.unnormalized_derivatives = nn.Parameter(torch.randn(self.input_dim, self.count_bins - 1))\n\n        # Rational linear splines have additional lambda parameters\n        if self.order == ""linear"":\n            self.unnormalized_lambdas = nn.Parameter(torch.rand(self.input_dim, self.count_bins))\n            self.lambdas = torch.sigmoid(self.unnormalized_lambdas)\n        elif self.order == ""quadratic"":\n            self.lambdas = None\n            raise ValueError(""Monotonic rational quadratic splines not yet implemented!"")\n        else:\n            raise ValueError(\n                ""Keyword argument \'order\' must be one of [\'linear\', \'quadratic\'], but \'{}\' was found!"".format(\n                    self.order))\n\n        self.widths = F.softmax(self.unnormalized_widths, dim=-1)\n        self.heights = F.softmax(self.unnormalized_heights, dim=-1)\n        self.derivatives = F.softplus(self.unnormalized_derivatives)\n        self._cache_log_detJ = None\n\n\n@copy_docs_from(ConditionalTransformModule)\nclass ConditionalSpline(ConditionalTransformModule):\n    r""""""\n    An implementation of the element-wise rational spline bijections of linear and\n    quadratic order (Durkan et al., 2019; Dolatabadi et al., 2020) conditioning on\n    an additional context variable.\n\n    Rational splines are functions that are comprised of segments that are the ratio\n    of two polynomials. For instance, for the :math:`d`-th dimension and the\n    :math:`k`-th segment on the spline, the function will take the form,\n\n        :math:`y_d = \\frac{\\alpha^{(k)}(x_d)}{\\beta^{(k)}(x_d)},`\n\n    where :math:`\\alpha^{(k)}` and :math:`\\beta^{(k)}` are two polynomials of\n    order :math:`d` whose parameters are the output of a function, e.g. a NN, with\n    input :math:`z\\\\in\\\\mathbb{R}^{M}` representing the context variable to\n    condition on.. For :math:`d=1`, we say that the spline is linear, and for\n    :math:`d=2`, quadratic. The spline is constructed on the specified bounding box,\n    :math:`[-K,K]\\times[-K,K]`, with the identity function used elsewhere.\n\n    Rational splines offer an excellent combination of functional flexibility whilst\n    maintaining a numerically stable inverse that is of the same computational and\n    space complexities as the forward operation. This element-wise transform permits\n    the accurate represention of complex univariate distributions.\n\n    Example usage:\n\n    >>> from pyro.nn.dense_nn import DenseNN\n    >>> input_dim = 10\n    >>> context_dim = 5\n    >>> batch_size = 3\n    >>> count_bins = 8\n    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n    >>> param_dims = [input_dim * count_bins, input_dim * count_bins,\n    ... input_dim * (count_bins - 1), input_dim * count_bins]\n    >>> hypernet = DenseNN(context_dim, [50, 50], param_dims)\n    >>> transform = ConditionalSpline(hypernet, input_dim, count_bins)\n    >>> z = torch.rand(batch_size, context_dim)\n    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,\n    ... [transform]).condition(z)\n    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP\n\n    :param input_dim: Dimension of the input vector. This is required so we know how\n        many parameters to store.\n    :type input_dim: int\n    :param count_bins: The number of segments comprising the spline.\n    :type count_bins: int\n    :param bound: The quantity :math:`K` determining the bounding box,\n        :math:`[-K,K]\\times[-K,K]`, of the spline.\n    :type bound: float\n    :param order: One of [\'linear\', \'quadratic\'] specifying the order of the spline.\n    :type order: string\n\n    References:\n\n    Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural\n    Spline Flows. NeurIPS 2019.\n\n    Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie. Invertible Generative\n    Modeling using Linear Rational Splines. AISTATS 2020.\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 0\n\n    def __init__(self, nn, input_dim, count_bins, bound=3.0, order=\'linear\'):\n        super().__init__()\n\n        self.nn = nn\n        self.input_dim = input_dim\n        self.count_bins = count_bins\n        self.bound = bound\n        self.order = order\n\n    def condition(self, context):\n        # Rational linear splines have additional lambda parameters\n        if self.order == ""linear"":\n            w, h, d, l = self.nn(context)\n            l = torch.sigmoid(l.reshape(l.shape[:-1] + (self.input_dim, self.count_bins)))\n        elif self.order == ""quadratic"":\n            w, h, d = self.nn(context)\n            l = None\n            raise ValueError(""Monotonic rational quadratic splines not yet implemented!"")\n        else:\n            raise ValueError(\n                ""Keyword argument \'order\' must be one of [\'linear\', \'quadratic\'], but \'{}\' was found!"".format(\n                    self.order))\n\n        w = F.softmax(w.reshape(w.shape[:-1] + (self.input_dim, self.count_bins)), dim=-1)\n        h = F.softmax(h.reshape(h.shape[:-1] + (self.input_dim, self.count_bins)), dim=-1)\n        d = F.softplus(d.reshape(d.shape[:-1] + (self.input_dim, self.count_bins - 1)))\n        return ConditionedSpline(w, h, d, l, bound=self.bound, order=self.order)\n\n\ndef spline(input_dim, **kwargs):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.Spline` object for consistency with\n    other helpers.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n\n    """"""\n\n    # TODO: A useful heuristic for choosing number of bins from input\n    # dimension like: count_bins=min(5, math.log(input_dim))?\n    return Spline(input_dim, **kwargs)\n\n\ndef conditional_spline(input_dim, context_dim, hidden_dims=None, count_bins=8, bound=3.0):\n    """"""\n    A helper function to create a\n    :class:`~pyro.distributions.transforms.ConditionalSpline` object that takes care\n    of constructing a dense network with the correct input/output dimensions.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param context_dim: Dimension of context variable\n    :type context_dim: int\n    :param hidden_dims: The desired hidden dimensions of the dense network. Defaults\n        to using [input_dim * 10, input_dim * 10]\n    :type hidden_dims: list[int]\n    :param count_bins: The number of segments comprising the spline.\n    :type count_bins: int\n    :param bound: The quantity :math:`K` determining the bounding box,\n        :math:`[-K,K]\\times[-K,K]`, of the spline.\n    :type bound: float\n\n    """"""\n\n    if hidden_dims is None:\n        hidden_dims = [input_dim * 10, input_dim * 10]\n\n    nn = DenseNN(context_dim,\n                 hidden_dims,\n                 param_dims=[input_dim * count_bins,\n                             input_dim * count_bins,\n                             input_dim * (count_bins - 1),\n                             input_dim * count_bins])\n    return ConditionalSpline(nn, input_dim, count_bins, bound=bound)\n'"
pyro/distributions/transforms/sylvester.py,21,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import constraints\n\nfrom pyro.distributions.torch_transform import TransformModule\nfrom pyro.distributions.transforms.householder import Householder\nfrom pyro.distributions.util import copy_docs_from\n\n\n@copy_docs_from(TransformModule)\nclass Sylvester(Householder):\n    r""""""\n    An implementation of the Sylvester bijective transform of the Householder\n    variety (Van den Berg Et Al., 2018),\n\n        :math:`\\mathbf{y} = \\mathbf{x} + QR\\tanh(SQ^T\\mathbf{x}+\\mathbf{b})`\n\n    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,\n    :math:`R,S\\sim D\\times D` are upper triangular matrices for input dimension\n    :math:`D`, :math:`Q\\sim D\\times D` is an orthogonal matrix, and\n    :math:`\\mathbf{b}\\sim D` is learnable bias term.\n\n    The Sylvester transform is a generalization of\n    :class:`~pyro.distributions.transforms.Planar`. In the Householder type of the\n    Sylvester transform, the orthogonality of :math:`Q` is enforced by representing\n    it as the product of Householder transformations.\n\n    Together with :class:`~pyro.distributions.TransformedDistribution` it provides a\n    way to create richer variational approximations.\n\n    Example usage:\n\n    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n    >>> transform = Sylvester(10, count_transforms=4)\n    >>> pyro.module(""my_transform"", transform)  # doctest: +SKIP\n    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])\n    >>> flow_dist.sample()  # doctest: +SKIP\n        tensor([-0.4071, -0.5030,  0.7924, -0.2366, -0.2387, -0.1417,  0.0868,\n                0.1389, -0.4629,  0.0986])\n\n    The inverse of this transform does not possess an analytical solution and is\n    left unimplemented. However, the inverse is cached when the forward operation is\n    called during sampling, and so samples drawn using the Sylvester transform can\n    be scored.\n\n    References:\n\n    [1] Rianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak, Max Welling.\n    Sylvester Normalizing Flows for Variational Inference. UAI 2018.\n\n    """"""\n\n    domain = constraints.real\n    codomain = constraints.real\n    bijective = True\n    event_dim = 1\n\n    def __init__(self, input_dim, count_transforms=1):\n        super().__init__(input_dim, count_transforms)\n\n        # Create parameters for Sylvester transform\n        self.R_dense = nn.Parameter(torch.Tensor(input_dim, input_dim))\n        self.S_dense = nn.Parameter(torch.Tensor(input_dim, input_dim))\n        self.R_diag = nn.Parameter(torch.Tensor(input_dim))\n        self.S_diag = nn.Parameter(torch.Tensor(input_dim))\n        self.b = nn.Parameter(torch.Tensor(input_dim))\n\n        # Register masks and indices\n        triangular_mask = torch.triu(torch.ones(input_dim, input_dim), diagonal=1)\n        self.register_buffer(\'triangular_mask\', triangular_mask)\n\n        self._cached_logDetJ = None\n        self.tanh = nn.Tanh()\n        self.reset_parameters2()\n\n    # Derivative of hyperbolic tan\n    def dtanh_dx(self, x):\n        return 1. - self.tanh(x).pow(2)\n\n    # Construct upper diagonal R matrix\n    def R(self):\n        return self.R_dense * self.triangular_mask + torch.diag(self.tanh(self.R_diag))\n\n    # Construct upper diagonal S matrix\n    def S(self):\n        return self.S_dense * self.triangular_mask + torch.diag(self.tanh(self.S_diag))\n\n    # Construct orthonomal matrix using Householder flow\n    def Q(self, x):\n        u = self.u()\n        partial_Q = torch.eye(self.input_dim, dtype=x.dtype, layout=x.layout,\n                              device=x.device) - 2. * torch.ger(u[0], u[0])\n\n        for idx in range(1, self.u_unnormed.size(-2)):\n            partial_Q = torch.matmul(partial_Q, torch.eye(self.input_dim) - 2. * torch.ger(u[idx], u[idx]))\n\n        return partial_Q\n\n    # Self.u_unnormed is initialized in parent class\n    def reset_parameters2(self):\n        for v in [self.b, self.R_diag, self.S_diag, self.R_dense, self.S_dense]:\n            v.data.uniform_(-0.01, 0.01)\n\n    def _call(self, x):\n        """"""\n        :param x: the input into the bijection\n        :type x: torch.Tensor\n\n        Invokes the bijection x=>y; in the prototypical context of a\n        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n        the base distribution (or the output of a previous transform)\n        """"""\n        Q = self.Q(x)\n        R = self.R()\n        S = self.S()\n\n        A = torch.matmul(Q, R)\n        B = torch.matmul(S, Q.t())\n\n        preactivation = torch.matmul(x, B) + self.b\n        y = x + torch.matmul(self.tanh(preactivation), A)\n\n        self._cached_logDetJ = torch.log1p(self.dtanh_dx(preactivation) * R.diagonal() * S.diagonal() + 1e-8).sum(-1)\n        return y\n\n    def _inverse(self, y):\n        """"""\n        :param y: the output of the bijection\n        :type y: torch.Tensor\n        Inverts y => x. As noted above, this implementation is incapable of\n        inverting arbitrary values `y`; rather it assumes `y` is the result of a\n        previously computed application of the bijector to some `x` (which was\n        cached on the forward call)\n        """"""\n\n        raise KeyError(""Sylvester object expected to find key in intermediates cache but didn\'t"")\n\n    def log_abs_det_jacobian(self, x, y):\n        """"""\n        Calculates the elementwise determinant of the log Jacobian\n        """"""\n        x_old, y_old = self._cached_x_y\n        if x is not x_old or y is not y_old:\n            # This call to the parent class Transform will update the cache\n            # as well as calling self._call and recalculating y and log_detJ\n            self(x)\n\n        return self._cached_logDetJ\n\n\ndef sylvester(input_dim, count_transforms=None):\n    """"""\n    A helper function to create a :class:`~pyro.distributions.transforms.Sylvester`\n    object for consistency with other helpers.\n\n    :param input_dim: Dimension of input variable\n    :type input_dim: int\n    :param count_transforms: Number of Sylvester operations to apply. Defaults to\n        input_dim // 2 + 1. :type count_transforms: int\n\n    """"""\n\n    if count_transforms is None:\n        count_transforms = input_dim // 2 + 1\n    return Sylvester(input_dim, count_transforms=count_transforms)\n'"
pyro/distributions/transforms/utils.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n\ndef clamp_preserve_gradients(x, min, max):\n    # This helper function clamps gradients but still passes through the gradient in clamped regions\n    return x + (x.clamp(min, max) - x).detach()\n'"
pyro/infer/autoguide/__init__.py,0,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.infer.autoguide.guides import (AutoCallable, AutoContinuous, AutoDelta, AutoDiagonalNormal,\n                                         AutoDiscreteParallel, AutoGuide, AutoGuideList, AutoIAFNormal,\n                                         AutoLaplaceApproximation, AutoLowRankMultivariateNormal,\n                                         AutoMultivariateNormal, AutoNormal, AutoNormalizingFlow)\nfrom pyro.infer.autoguide.initialization import (init_to_feasible, init_to_generated, init_to_mean, init_to_median,\n                                                 init_to_sample, init_to_uniform, init_to_value)\nfrom pyro.infer.autoguide.utils import mean_field_entropy\n\n__all__ = [\n    'AutoCallable',\n    'AutoContinuous',\n    'AutoDelta',\n    'AutoDiagonalNormal',\n    'AutoDiscreteParallel',\n    'AutoGuide',\n    'AutoGuideList',\n    'AutoIAFNormal',\n    'AutoLaplaceApproximation',\n    'AutoLowRankMultivariateNormal',\n    'AutoMultivariateNormal',\n    'AutoNormal',\n    'AutoNormalizingFlow',\n    'init_to_feasible',\n    'init_to_generated',\n    'init_to_mean',\n    'init_to_median',\n    'init_to_sample',\n    'init_to_uniform',\n    'init_to_value',\n    'mean_field_entropy',\n]\n"""
pyro/infer/autoguide/guides.py,17,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThe :mod:`pyro.infer.autoguide` module provides algorithms to automatically\ngenerate guides from simple models, for use in :class:`~pyro.infer.svi.SVI`.\nFor example to generate a mean field Gaussian guide::\n\n    def model():\n        ...\n\n    guide = AutoDiagonalNormal(model)  # a mean field guide\n    svi = SVI(model, guide, Adam({\'lr\': 1e-3}), Trace_ELBO())\n\nAutomatic guides can also be combined using :func:`pyro.poutine.block` and\n:class:`AutoGuideList`.\n""""""\nimport functools\nimport operator\nimport warnings\nimport weakref\nfrom contextlib import ExitStack  # python 3\n\nimport torch\nfrom torch import nn\nfrom torch.distributions import biject_to, constraints\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.distributions.transforms import affine_autoregressive, iterated\nfrom pyro.distributions.util import broadcast_shape, eye_like, sum_rightmost\nfrom pyro.infer.autoguide.initialization import InitMessenger, init_to_feasible, init_to_median\nfrom pyro.infer.autoguide.utils import _product\nfrom pyro.infer.enum import config_enumerate\nfrom pyro.nn import PyroModule, PyroParam\nfrom pyro.ops.hessian import hessian\nfrom pyro.ops.tensor_utils import periodic_repeat\nfrom pyro.poutine.util import site_is_subsample\n\n\ndef _deep_setattr(obj, key, val):\n    """"""\n    Set an attribute `key` on the object. If any of the prefix attributes do\n    not exist, they are set to :class:`~pyro.nn.PyroModule`.\n    """"""\n\n    def _getattr(obj, attr):\n        obj_next = getattr(obj, attr, None)\n        if obj_next is not None:\n            return obj_next\n        setattr(obj, attr, PyroModule())\n        return getattr(obj, attr)\n\n    lpart, _, rpart = key.rpartition(""."")\n    # Recursive getattr while setting any prefix attributes to PyroModule\n    if lpart:\n        obj = functools.reduce(_getattr, [obj] + lpart.split(\'.\'))\n    setattr(obj, rpart, val)\n\n\ndef _deep_getattr(obj, key):\n    for part in key.split("".""):\n        obj = getattr(obj, part)\n    return obj\n\n\ndef prototype_hide_fn(msg):\n    # Record only stochastic sites in the prototype_trace.\n    return msg[""type""] != ""sample"" or msg[""is_observed""] or site_is_subsample(msg)\n\n\nclass AutoGuide(PyroModule):\n    """"""\n    Base class for automatic guides.\n\n    Derived classes must implement the :meth:`forward` method, with the\n    same ``*args, **kwargs`` as the base ``model``.\n\n    Auto guides can be used individually or combined in an\n    :class:`AutoGuideList` object.\n\n    :param callable model: A pyro model.\n    :param callable create_plates: An optional function inputing the same\n        ``*args,**kwargs`` as ``model()`` and returning a :class:`pyro.plate`\n        or iterable of plates. Plates not returned will be created\n        automatically as usual. This is useful for data subsampling.\n    """"""\n\n    def __init__(self, model, *, create_plates=None):\n        super().__init__(name=type(self).__name__)\n        self.master = None\n        # Do not register model as submodule\n        self._model = (model,)\n        self.create_plates = create_plates\n        self.prototype_trace = None\n        self._prototype_frames = {}\n\n    @property\n    def model(self):\n        return self._model[0]\n\n    def _update_master(self, master_ref):\n        self.master = master_ref\n\n    def call(self, *args, **kwargs):\n        """"""\n        Method that calls :meth:`forward` and returns parameter values of the\n        guide as a `tuple` instead of a `dict`, which is a requirement for\n        JIT tracing. Unlike :meth:`forward`, this method can be traced by\n        :func:`torch.jit.trace_module`.\n\n        .. warning::\n            This method may be removed once PyTorch JIT tracer starts accepting\n            `dict` as valid return types. See\n            `issue <https://github.com/pytorch/pytorch/issues/27743>_`.\n        """"""\n        result = self(*args, **kwargs)\n        return tuple(v for _, v in sorted(result.items()))\n\n    def sample_latent(*args, **kwargs):\n        """"""\n        Samples an encoded latent given the same ``*args, **kwargs`` as the\n        base ``model``.\n        """"""\n        pass\n\n    def __setattr__(self, name, value):\n        if isinstance(value, AutoGuide):\n            master_ref = self if self.master is None else self.master\n            value._update_master(weakref.ref(master_ref))\n        super().__setattr__(name, value)\n\n    def _create_plates(self, *args, **kwargs):\n        if self.master is None:\n            if self.create_plates is None:\n                self.plates = {}\n            else:\n                plates = self.create_plates(*args, **kwargs)\n                if isinstance(plates, pyro.plate):\n                    plates = [plates]\n                assert all(isinstance(p, pyro.plate) for p in plates), \\\n                    ""create_plates() returned a non-plate""\n                self.plates = {p.name: p for p in plates}\n            for name, frame in sorted(self._prototype_frames.items()):\n                if name not in self.plates:\n                    self.plates[name] = pyro.plate(name, frame.size, dim=frame.dim)\n        else:\n            assert self.create_plates is None, ""Cannot pass create_plates() to non-master guide""\n            self.plates = self.master().plates\n        return self.plates\n\n    def _setup_prototype(self, *args, **kwargs):\n        # run the model so we can inspect its structure\n        model = poutine.block(self.model, prototype_hide_fn)\n        self.prototype_trace = poutine.block(poutine.trace(model).get_trace)(*args, **kwargs)\n        if self.master is not None:\n            self.master()._check_prototype(self.prototype_trace)\n\n        self._prototype_frames = {}\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            for frame in site[""cond_indep_stack""]:\n                if frame.vectorized:\n                    self._prototype_frames[frame.name] = frame\n                else:\n                    raise NotImplementedError(""AutoGuide does not support sequential pyro.plate"")\n\n    def median(self, *args, **kwargs):\n        """"""\n        Returns the posterior median value of each latent variable.\n\n        :return: A dict mapping sample site name to median tensor.\n        :rtype: dict\n        """"""\n        raise NotImplementedError\n\n\nclass AutoGuideList(AutoGuide, nn.ModuleList):\n    """"""\n    Container class to combine multiple automatic guides.\n\n    Example usage::\n\n        guide = AutoGuideList(my_model)\n        guide.add(AutoDiagonalNormal(poutine.block(model, hide=[""assignment""])))\n        guide.add(AutoDiscreteParallel(poutine.block(model, expose=[""assignment""])))\n        svi = SVI(model, guide, optim, Trace_ELBO())\n\n    :param callable model: a Pyro model\n    """"""\n\n    def _check_prototype(self, part_trace):\n        for name, part_site in part_trace.nodes.items():\n            if part_site[""type""] != ""sample"":\n                continue\n            self_site = self.prototype_trace.nodes[name]\n            assert part_site[""fn""].batch_shape == self_site[""fn""].batch_shape\n            assert part_site[""fn""].event_shape == self_site[""fn""].event_shape\n            assert part_site[""value""].shape == self_site[""value""].shape\n\n    def _update_master(self, master_ref):\n        self.master = master_ref\n        for submodule in self:\n            submodule._update_master(master_ref)\n\n    def append(self, part):\n        """"""\n        Add an automatic guide for part of the model. The guide should\n        have been created by blocking the model to restrict to a subset of\n        sample sites. No two parts should operate on any one sample site.\n\n        :param part: a partial guide to add\n        :type part: AutoGuide or callable\n        """"""\n        if not isinstance(part, AutoGuide):\n            part = AutoCallable(self.model, part)\n        if part.master is not None:\n            raise RuntimeError(""The module `{}` is already added."".format(self._pyro_name))\n        setattr(self, str(len(self)), part)\n\n    def add(self, part):\n        """"""Deprecated alias for :meth:`append`.""""""\n        warnings.warn(""The method `.add` has been deprecated in favor of `.append`."", DeprecationWarning)\n        self.append(part)\n\n    def forward(self, *args, **kwargs):\n        """"""\n        A composite guide with the same ``*args, **kwargs`` as the base ``model``.\n\n        :return: A dict mapping sample site name to sampled value.\n        :rtype: dict\n        """"""\n        # if we\'ve never run the model before, do so now so we can inspect the model structure\n        if self.prototype_trace is None:\n            self._setup_prototype(*args, **kwargs)\n\n        # create all plates\n        self._create_plates(*args, **kwargs)\n\n        # run slave guides\n        result = {}\n        for part in self:\n            result.update(part(*args, **kwargs))\n        return result\n\n    def median(self, *args, **kwargs):\n        """"""\n        Returns the posterior median value of each latent variable.\n\n        :return: A dict mapping sample site name to median tensor.\n        :rtype: dict\n        """"""\n        result = {}\n        for part in self:\n            result.update(part.median(*args, **kwargs))\n        return result\n\n\nclass AutoCallable(AutoGuide):\n    """"""\n    :class:`AutoGuide` wrapper for simple callable guides.\n\n    This is used internally for composing autoguides with custom user-defined\n    guides that are simple callables, e.g.::\n\n        def my_local_guide(*args, **kwargs):\n            ...\n\n        guide = AutoGuideList(model)\n        guide.add(AutoDelta(poutine.block(model, expose=[\'my_global_param\']))\n        guide.add(my_local_guide)  # automatically wrapped in an AutoCallable\n\n    To specify a median callable, you can instead::\n\n        def my_local_median(*args, **kwargs)\n            ...\n\n        guide.add(AutoCallable(model, my_local_guide, my_local_median))\n\n    For more complex guides that need e.g. access to plates, users should\n    instead subclass ``AutoGuide``.\n\n    :param callable model: a Pyro model\n    :param callable guide: a Pyro guide (typically over only part of the model)\n    :param callable median: an optional callable returning a dict mapping\n        sample site name to computed median tensor.\n    """"""\n\n    def __init__(self, model, guide, median=lambda *args, **kwargs: {}):\n        super().__init__(model)\n        self._guide = guide\n        self.median = median\n\n    def forward(self, *args, **kwargs):\n        result = self._guide(*args, **kwargs)\n        return {} if result is None else result\n\n\nclass AutoDelta(AutoGuide):\n    """"""\n    This implementation of :class:`AutoGuide` uses Delta distributions to\n    construct a MAP guide over the entire latent space. The guide does not\n    depend on the model\'s ``*args, **kwargs``.\n\n    .. note:: This class does MAP inference in constrained space.\n\n    Usage::\n\n        guide = AutoDelta(model)\n        svi = SVI(model, guide, ...)\n\n    Latent variables are initialized using ``init_loc_fn()``. To change the\n    default behavior, create a custom ``init_loc_fn()`` as described in\n    :ref:`autoguide-initialization` , for example::\n\n        def my_init_fn(site):\n            if site[""name""] == ""level"":\n                return torch.tensor([-1., 0., 1.])\n            if site[""name""] == ""concentration"":\n                return torch.ones(k)\n            return init_to_sample(site)\n\n    :param callable model: A Pyro model.\n    :param callable init_loc_fn: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n    :param callable create_plates: An optional function inputing the same\n        ``*args,**kwargs`` as ``model()`` and returning a :class:`pyro.plate`\n        or iterable of plates. Plates not returned will be created\n        automatically as usual. This is useful for data subsampling.\n    """"""\n    def __init__(self, model, init_loc_fn=init_to_median, *,\n                 create_plates=None):\n        self.init_loc_fn = init_loc_fn\n        model = InitMessenger(self.init_loc_fn)(model)\n        super().__init__(model, create_plates=create_plates)\n\n    def _setup_prototype(self, *args, **kwargs):\n        super()._setup_prototype(*args, **kwargs)\n\n        # Initialize guide params\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            value = site[""value""].detach()\n            event_dim = site[""fn""].event_dim\n\n            # If subsampling, repeat init_value to full size.\n            for frame in site[""cond_indep_stack""]:\n                full_size = getattr(frame, ""full_size"", frame.size)\n                if full_size != frame.size:\n                    dim = frame.dim - event_dim\n                    value = periodic_repeat(value, full_size, dim).contiguous()\n\n            value = PyroParam(value, site[""fn""].support, event_dim)\n            _deep_setattr(self, name, value)\n\n    def forward(self, *args, **kwargs):\n        """"""\n        An automatic guide with the same ``*args, **kwargs`` as the base ``model``.\n\n        :return: A dict mapping sample site name to sampled value.\n        :rtype: dict\n        """"""\n        # if we\'ve never run the model before, do so now so we can inspect the model structure\n        if self.prototype_trace is None:\n            self._setup_prototype(*args, **kwargs)\n\n        plates = self._create_plates(*args, **kwargs)\n        result = {}\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            with ExitStack() as stack:\n                for frame in site[""cond_indep_stack""]:\n                    if frame.vectorized:\n                        stack.enter_context(plates[frame.name])\n                attr_get = operator.attrgetter(name)\n                result[name] = pyro.sample(name, dist.Delta(attr_get(self),\n                                                            event_dim=site[""fn""].event_dim))\n        return result\n\n    def median(self, *args, **kwargs):\n        """"""\n        Returns the posterior median value of each latent variable.\n\n        :return: A dict mapping sample site name to median tensor.\n        :rtype: dict\n        """"""\n        return self(*args, **kwargs)\n\n\nclass AutoNormal(AutoGuide):\n    """"""This implementation of :class:`AutoGuide` uses Normal(0, 1) distributions\n    to construct a guide over the entire latent space. The guide does not\n    depend on the model\'s ``*args, **kwargs``.\n\n    It should be equivalent to :class: `AutoDiagonalNormal` , but with\n    more convenient site names and with better support for\n    :class:`~pyro.infer.trace_mean_field_elbo.TraceMeanField_ELBO` .\n\n    In :class:`AutoDiagonalNormal` , if your model has N named\n    parameters with dimensions k_i and sum k_i = D, you get a single\n    vector of length D for your mean, and a single vector of length D\n    for sigmas.  This guide gives you N distinct normals that you can\n    call by name.\n\n    Usage::\n\n        guide = AutoNormal(model)\n        svi = SVI(model, guide, ...)\n\n    :param callable model: A Pyro model.\n    :param callable init_loc_fn: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n    :param float init_scale: Initial scale for the standard deviation of each\n        (unconstrained transformed) latent variable.\n    :param callable create_plates: An optional function inputing the same\n        ``*args,**kwargs`` as ``model()`` and returning a :class:`pyro.plate`\n        or iterable of plates. Plates not returned will be created\n        automatically as usual. This is useful for data subsampling.\n    """"""\n    def __init__(self, model, *,\n                 init_loc_fn=init_to_feasible,\n                 init_scale=0.1,\n                 create_plates=None):\n        self.init_loc_fn = init_loc_fn\n\n        if not isinstance(init_scale, float) or not (init_scale > 0):\n            raise ValueError(""Expected init_scale > 0. but got {}"".format(init_scale))\n        self._init_scale = init_scale\n\n        model = InitMessenger(self.init_loc_fn)(model)\n        super().__init__(model, create_plates=create_plates)\n\n    def _setup_prototype(self, *args, **kwargs):\n        super()._setup_prototype(*args, **kwargs)\n\n        self._event_dims = {}\n        self._cond_indep_stacks = {}\n        self.locs = PyroModule()\n        self.scales = PyroModule()\n\n        # Initialize guide params\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            # Collect unconstrained event_dims, which may differ from constrained event_dims.\n            init_loc = biject_to(site[""fn""].support).inv(site[""value""].detach()).detach()\n            event_dim = site[""fn""].event_dim + init_loc.dim() - site[""value""].dim()\n            self._event_dims[name] = event_dim\n\n            # Collect independence contexts.\n            self._cond_indep_stacks[name] = site[""cond_indep_stack""]\n\n            # If subsampling, repeat init_value to full size.\n            for frame in site[""cond_indep_stack""]:\n                full_size = getattr(frame, ""full_size"", frame.size)\n                if full_size != frame.size:\n                    dim = frame.dim - event_dim\n                    init_loc = periodic_repeat(init_loc, full_size, dim).contiguous()\n            init_scale = torch.full_like(init_loc, self._init_scale)\n\n            _deep_setattr(self.locs, name, PyroParam(init_loc, constraints.real, event_dim))\n            _deep_setattr(self.scales, name, PyroParam(init_scale, constraints.positive, event_dim))\n\n    def _get_loc_and_scale(self, name):\n        site_loc = _deep_getattr(self.locs, name)\n        site_scale = _deep_getattr(self.scales, name)\n        return site_loc, site_scale\n\n    def forward(self, *args, **kwargs):\n        """"""\n        An automatic guide with the same ``*args, **kwargs`` as the base ``model``.\n\n        :return: A dict mapping sample site name to sampled value.\n        :rtype: dict\n        """"""\n        # if we\'ve never run the model before, do so now so we can inspect the model structure\n        if self.prototype_trace is None:\n            self._setup_prototype(*args, **kwargs)\n\n        plates = self._create_plates(*args, **kwargs)\n        result = {}\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            transform = biject_to(site[""fn""].support)\n\n            with ExitStack() as stack:\n                for frame in site[""cond_indep_stack""]:\n                    if frame.vectorized:\n                        stack.enter_context(plates[frame.name])\n\n                site_loc, site_scale = self._get_loc_and_scale(name)\n                unconstrained_latent = pyro.sample(\n                    name + ""_unconstrained"",\n                    dist.Normal(\n                        site_loc, site_scale,\n                    ).to_event(self._event_dims[name]),\n                    infer={""is_auxiliary"": True}\n                )\n\n                value = transform(unconstrained_latent)\n                log_density = transform.inv.log_abs_det_jacobian(value, unconstrained_latent)\n                log_density = sum_rightmost(log_density, log_density.dim() - value.dim() + site[""fn""].event_dim)\n                delta_dist = dist.Delta(value, log_density=log_density,\n                                        event_dim=site[""fn""].event_dim)\n\n                result[name] = pyro.sample(name, delta_dist)\n\n        return result\n\n    def median(self, *args, **kwargs):\n        """"""\n        Returns the posterior median value of each latent variable.\n\n        :return: A dict mapping sample site name to median tensor.\n        :rtype: dict\n        """"""\n        medians = {}\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            site_loc, _ = self._get_loc_and_scale(name)\n            median = biject_to(site[""fn""].support)(site_loc)\n            if median is site_loc:\n                median = median.clone()\n            medians[name] = median\n\n        return medians\n\n    def quantiles(self, quantiles, *args, **kwargs):\n        """"""\n        Returns posterior quantiles each latent variable. Example::\n\n            print(guide.quantiles([0.05, 0.5, 0.95]))\n\n        :param quantiles: A list of requested quantiles between 0 and 1.\n        :type quantiles: torch.Tensor or list\n        :return: A dict mapping sample site name to a list of quantile values.\n        :rtype: dict\n        """"""\n        results = {}\n\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            site_loc, site_scale = self._get_loc_and_scale(name)\n\n            site_quantiles = torch.tensor(quantiles, dtype=site_loc.dtype, device=site_loc.device)\n            site_quantiles_values = dist.Normal(site_loc, site_scale).icdf(site_quantiles)\n            constrained_site_quantiles = biject_to(site[""fn""].support)(site_quantiles_values)\n            results[name] = constrained_site_quantiles\n\n        return results\n\n\nclass AutoContinuous(AutoGuide):\n    """"""\n    Base class for implementations of continuous-valued Automatic\n    Differentiation Variational Inference [1].\n\n    This uses :mod:`torch.distributions.transforms` to transform each\n    constrained latent variable to an unconstrained space, then concatenate all\n    variables into a single unconstrained latent variable.  Each derived class\n    implements a :meth:`get_posterior` method returning a distribution over\n    this single unconstrained latent variable.\n\n    Assumes model structure and latent dimension are fixed, and all latent\n    variables are continuous.\n\n    :param callable model: a Pyro model\n\n    Reference:\n\n    [1] `Automatic Differentiation Variational Inference`,\n        Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M.\n        Blei\n\n    :param callable model: A Pyro model.\n    :param callable init_loc_fn: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n    """"""\n    def __init__(self, model, init_loc_fn=init_to_median):\n        model = InitMessenger(init_loc_fn)(model)\n        super().__init__(model)\n\n    def _setup_prototype(self, *args, **kwargs):\n        super()._setup_prototype(*args, **kwargs)\n        self._unconstrained_shapes = {}\n        self._cond_indep_stacks = {}\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            # Collect the shapes of unconstrained values.\n            # These may differ from the shapes of constrained values.\n            self._unconstrained_shapes[name] = biject_to(site[""fn""].support).inv(site[""value""]).shape\n\n            # Collect independence contexts.\n            self._cond_indep_stacks[name] = site[""cond_indep_stack""]\n\n        self.latent_dim = sum(_product(shape) for shape in self._unconstrained_shapes.values())\n        if self.latent_dim == 0:\n            raise RuntimeError(\'{} found no latent variables; Use an empty guide instead\'.format(type(self).__name__))\n\n    def _init_loc(self):\n        """"""\n        Creates an initial latent vector using a per-site init function.\n        """"""\n        parts = []\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            constrained_value = site[""value""].detach()\n            unconstrained_value = biject_to(site[""fn""].support).inv(constrained_value)\n            parts.append(unconstrained_value.reshape(-1))\n        latent = torch.cat(parts)\n        assert latent.size() == (self.latent_dim,)\n        return latent\n\n    def get_base_dist(self):\n        """"""\n        Returns the base distribution of the posterior when reparameterized\n        as a :class:`~pyro.distributions.TransformedDistribution`. This\n        should not depend on the model\'s `*args, **kwargs`.\n\n        .. code-block:: python\n\n          posterior = TransformedDistribution(self.get_base_dist(), self.get_transform(*args, **kwargs))\n\n        :return: :class:`~pyro.distributions.TorchDistribution` instance representing the base distribution.\n        """"""\n        raise NotImplementedError\n\n    def get_transform(self, *args, **kwargs):\n        """"""\n        Returns the transform applied to the base distribution when the posterior\n        is reparameterized as a :class:`~pyro.distributions.TransformedDistribution`.\n        This may depend on the model\'s `*args, **kwargs`.\n\n        .. code-block:: python\n\n          posterior = TransformedDistribution(self.get_base_dist(), self.get_transform(*args, **kwargs))\n\n        :return: a :class:`~torch.distributions.Transform` instance.\n        """"""\n        raise NotImplementedError\n\n    def get_posterior(self, *args, **kwargs):\n        """"""\n        Returns the posterior distribution.\n        """"""\n        base_dist = self.get_base_dist()\n        transform = self.get_transform(*args, **kwargs)\n        return dist.TransformedDistribution(base_dist, transform)\n\n    def sample_latent(self, *args, **kwargs):\n        """"""\n        Samples an encoded latent given the same ``*args, **kwargs`` as the\n        base ``model``.\n        """"""\n        pos_dist = self.get_posterior(*args, **kwargs)\n        return pyro.sample(""_{}_latent"".format(self._pyro_name), pos_dist, infer={""is_auxiliary"": True})\n\n    def _unpack_latent(self, latent):\n        """"""\n        Unpacks a packed latent tensor, iterating over tuples of the form::\n\n            (site, unconstrained_value)\n        """"""\n        batch_shape = latent.shape[:-1]  # for plates outside of _setup_prototype, e.g. parallel particles\n        pos = 0\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            constrained_shape = site[""value""].shape\n            unconstrained_shape = self._unconstrained_shapes[name]\n            size = _product(unconstrained_shape)\n            event_dim = site[""fn""].event_dim + len(unconstrained_shape) - len(constrained_shape)\n            unconstrained_shape = broadcast_shape(unconstrained_shape,\n                                                  batch_shape + (1,) * event_dim)\n            unconstrained_value = latent[..., pos:pos + size].view(unconstrained_shape)\n            yield site, unconstrained_value\n            pos += size\n        if not torch._C._get_tracing_state():\n            assert pos == latent.size(-1)\n\n    def forward(self, *args, **kwargs):\n        """"""\n        An automatic guide with the same ``*args, **kwargs`` as the base ``model``.\n\n        :return: A dict mapping sample site name to sampled value.\n        :rtype: dict\n        """"""\n        # if we\'ve never run the model before, do so now so we can inspect the model structure\n        if self.prototype_trace is None:\n            self._setup_prototype(*args, **kwargs)\n\n        latent = self.sample_latent(*args, **kwargs)\n        plates = self._create_plates(*args, **kwargs)\n\n        # unpack continuous latent samples\n        result = {}\n        for site, unconstrained_value in self._unpack_latent(latent):\n            name = site[""name""]\n            transform = biject_to(site[""fn""].support)\n            value = transform(unconstrained_value)\n            log_density = transform.inv.log_abs_det_jacobian(value, unconstrained_value)\n            log_density = sum_rightmost(log_density, log_density.dim() - value.dim() + site[""fn""].event_dim)\n            delta_dist = dist.Delta(value, log_density=log_density, event_dim=site[""fn""].event_dim)\n\n            with ExitStack() as stack:\n                for frame in self._cond_indep_stacks[name]:\n                    stack.enter_context(plates[frame.name])\n                result[name] = pyro.sample(name, delta_dist)\n\n        return result\n\n    def _loc_scale(self, *args, **kwargs):\n        """"""\n        :returns: a tuple ``(loc, scale)`` used by :meth:`median` and\n            :meth:`quantiles`\n        """"""\n        raise NotImplementedError\n\n    def median(self, *args, **kwargs):\n        """"""\n        Returns the posterior median value of each latent variable.\n\n        :return: A dict mapping sample site name to median tensor.\n        :rtype: dict\n        """"""\n        loc, _ = self._loc_scale(*args, **kwargs)\n        return {site[""name""]: biject_to(site[""fn""].support)(unconstrained_value)\n                for site, unconstrained_value in self._unpack_latent(loc)}\n\n    def quantiles(self, quantiles, *args, **kwargs):\n        """"""\n        Returns posterior quantiles each latent variable. Example::\n\n            print(guide.quantiles([0.05, 0.5, 0.95]))\n\n        :param quantiles: A list of requested quantiles between 0 and 1.\n        :type quantiles: torch.Tensor or list\n        :return: A dict mapping sample site name to a list of quantile values.\n        :rtype: dict\n        """"""\n        loc, scale = self._loc_scale(*args, **kwargs)\n        quantiles = torch.tensor(quantiles, dtype=loc.dtype, device=loc.device).unsqueeze(-1)\n        latents = dist.Normal(loc, scale).icdf(quantiles)\n        result = {}\n        for latent in latents:\n            for site, unconstrained_value in self._unpack_latent(latent):\n                result.setdefault(site[""name""], []).append(biject_to(site[""fn""].support)(unconstrained_value))\n        return result\n\n\nclass AutoMultivariateNormal(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a Cholesky\n    factorization of a Multivariate Normal distribution to construct a guide\n    over the entire latent space. The guide does not depend on the model\'s\n    ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoMultivariateNormal(model)\n        svi = SVI(model, guide, ...)\n\n    By default the mean vector is initialized by ``init_loc_fn()`` and the\n    Cholesky factor is initialized to the identity times a small factor.\n\n    :param callable model: A generative model.\n    :param callable init_loc_fn: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n    :param float init_scale: Initial scale for the standard deviation of each\n        (unconstrained transformed) latent variable.\n    """"""\n\n    def __init__(self, model, init_loc_fn=init_to_median, init_scale=0.1):\n        if not isinstance(init_scale, float) or not (init_scale > 0):\n            raise ValueError(""Expected init_scale > 0. but got {}"".format(init_scale))\n        self._init_scale = init_scale\n        super().__init__(model, init_loc_fn=init_loc_fn)\n\n    def _setup_prototype(self, *args, **kwargs):\n        super()._setup_prototype(*args, **kwargs)\n        # Initialize guide params\n        self.loc = nn.Parameter(self._init_loc())\n        self.scale_tril = PyroParam(eye_like(self.loc, self.latent_dim) * self._init_scale,\n                                    constraints.lower_cholesky)\n\n    def get_base_dist(self):\n        return dist.Normal(torch.zeros_like(self.loc), torch.zeros_like(self.loc)).to_event(1)\n\n    def get_transform(self, *args, **kwargs):\n        return dist.transforms.LowerCholeskyAffine(self.loc, scale_tril=self.scale_tril)\n\n    def get_posterior(self, *args, **kwargs):\n        """"""\n        Returns a MultivariateNormal posterior distribution.\n        """"""\n        return dist.MultivariateNormal(self.loc, scale_tril=self.scale_tril)\n\n    def _loc_scale(self, *args, **kwargs):\n        return self.loc, self.scale_tril.diag()\n\n\nclass AutoDiagonalNormal(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a Normal distribution\n    with a diagonal covariance matrix to construct a guide over the entire\n    latent space. The guide does not depend on the model\'s ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoDiagonalNormal(model)\n        svi = SVI(model, guide, ...)\n\n    By default the mean vector is initialized to zero and the scale is\n    initialized to the identity times a small factor.\n\n    :param callable model: A generative model.\n    :param callable init_loc_fn: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n    :param float init_scale: Initial scale for the standard deviation of each\n        (unconstrained transformed) latent variable.\n    """"""\n\n    def __init__(self, model, init_loc_fn=init_to_median, init_scale=0.1):\n        if not isinstance(init_scale, float) or not (init_scale > 0):\n            raise ValueError(""Expected init_scale > 0. but got {}"".format(init_scale))\n        self._init_scale = init_scale\n        super().__init__(model, init_loc_fn=init_loc_fn)\n\n    def _setup_prototype(self, *args, **kwargs):\n        super()._setup_prototype(*args, **kwargs)\n        # Initialize guide params\n        self.loc = nn.Parameter(self._init_loc())\n        self.scale = PyroParam(self.loc.new_full((self.latent_dim,), self._init_scale),\n                               constraints.positive)\n\n    def get_base_dist(self):\n        return dist.Normal(torch.zeros_like(self.loc), torch.zeros_like(self.loc)).to_event(1)\n\n    def get_transform(self, *args, **kwargs):\n        return dist.transforms.AffineTransform(self.loc, self.scale)\n\n    def get_posterior(self, *args, **kwargs):\n        """"""\n        Returns a diagonal Normal posterior distribution.\n        """"""\n        return dist.Normal(self.loc, self.scale).to_event(1)\n\n    def _loc_scale(self, *args, **kwargs):\n        return self.loc, self.scale\n\n\nclass AutoLowRankMultivariateNormal(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a low rank plus\n    diagonal Multivariate Normal distribution to construct a guide\n    over the entire latent space. The guide does not depend on the model\'s\n    ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoLowRankMultivariateNormal(model, rank=10)\n        svi = SVI(model, guide, ...)\n\n    By default the ``cov_diag`` is initialized to a small constant and the\n    ``cov_factor`` is initialized randomly such that on average\n    ``cov_factor.matmul(cov_factor.t())`` has the same scale as ``cov_diag``.\n\n    :param callable model: A generative model.\n    :param rank: The rank of the low-rank part of the covariance matrix.\n        Defaults to approximately ``sqrt(latent dim)``.\n    :type rank: int or None\n    :param callable init_loc_fn: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n    :param float init_scale: Approximate initial scale for the standard\n        deviation of each (unconstrained transformed) latent variable.\n    """"""\n\n    def __init__(self, model, init_loc_fn=init_to_median, init_scale=0.1, rank=None):\n        if not isinstance(init_scale, float) or not (init_scale > 0):\n            raise ValueError(""Expected init_scale > 0. but got {}"".format(init_scale))\n        if not (rank is None or isinstance(rank, int) and rank > 0):\n            raise ValueError(""Expected rank > 0 but got {}"".format(rank))\n        self._init_scale = init_scale\n        self.rank = rank\n        super().__init__(model, init_loc_fn=init_loc_fn)\n\n    def _setup_prototype(self, *args, **kwargs):\n        super()._setup_prototype(*args, **kwargs)\n        # Initialize guide params\n        self.loc = nn.Parameter(self._init_loc())\n        if self.rank is None:\n            self.rank = int(round(self.latent_dim ** 0.5))\n        self.scale = PyroParam(\n            self.loc.new_full((self.latent_dim,), 0.5 ** 0.5 * self._init_scale),\n            constraint=constraints.positive)\n        self.cov_factor = nn.Parameter(\n            self.loc.new_empty(self.latent_dim, self.rank).normal_(0, 1 / self.rank ** 0.5))\n\n    def get_posterior(self, *args, **kwargs):\n        """"""\n        Returns a LowRankMultivariateNormal posterior distribution.\n        """"""\n        scale = self.scale\n        cov_factor = self.cov_factor * scale.unsqueeze(-1)\n        cov_diag = scale * scale\n        return dist.LowRankMultivariateNormal(self.loc, cov_factor, cov_diag)\n\n    def _loc_scale(self, *args, **kwargs):\n        scale = self.scale * (self.cov_factor.pow(2).sum(-1) + 1).sqrt()\n        return self.loc, scale\n\n\nclass AutoNormalizingFlow(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a Diagonal Normal\n    distribution transformed via a sequence of bijective transforms\n    (e.g. various :mod:`~pyro.distributions.TransformModule` subclasses)\n    to construct a guide over the entire latent space. The guide does not\n    depend on the model\'s ``*args, **kwargs``.\n\n    Usage::\n\n        transform_init = partial(iterated, block_autoregressive,\n                                 repeats=2)\n        guide = AutoNormalizingFlow(model, transform_init)\n        svi = SVI(model, guide, ...)\n\n    :param callable model: a generative model\n    :param init_transform_fn: a callable which when provided with the latent\n        dimension returns an instance of :class:`~torch.distributions.Transform`\n        , or :class:`~pyro.distributions.TransformModule` if the transform has\n        trainable params.\n    """"""\n\n    def __init__(self, model, init_transform_fn):\n        super().__init__(model, init_loc_fn=init_to_feasible)\n        self._init_transform_fn = init_transform_fn\n        self.transform = None\n        self._prototype_tensor = torch.tensor(0.)\n\n    def get_base_dist(self):\n        loc = self._prototype_tensor.new_zeros(1)\n        scale = self._prototype_tensor.new_ones(1)\n        return dist.Normal(loc, scale).expand([self.latent_dim]).to_event(1)\n\n    def get_transform(self, *args, **kwargs):\n        return self.transform\n\n    def get_posterior(self, *args, **kwargs):\n        if self.transform is None:\n            self.transform = self._init_transform_fn(self.latent_dim)\n            # Update prototype tensor in case transform parameters\n            # device/dtype is not the same as default tensor type.\n            for _, p in self.named_pyro_params():\n                self._prototype_tensor = p\n                break\n        return super().get_posterior(*args, **kwargs)\n\n\nclass AutoIAFNormal(AutoNormalizingFlow):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a Diagonal Normal\n    distribution transformed via a :class:`~pyro.distributions.transforms.AffineAutoregressive`\n    to construct a guide over the entire latent space. The guide does not depend on the model\'s\n    ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoIAFNormal(model, hidden_dim=latent_dim)\n        svi = SVI(model, guide, ...)\n\n    :param callable model: a generative model\n    :param int hidden_dim: number of hidden dimensions in the IAF\n    :param callable init_loc_fn: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n\n        .. warning::\n\n            This argument is only to preserve backwards compatibility\n            and has no effect in practice.\n\n    :param int num_transforms: number of :class:`~pyro.distributions.transforms.AffineAutoregressive`\n        transforms to use in sequence.\n    :param init_transform_kwargs: other keyword arguments taken by\n        :func:`~pyro.distributions.transforms.affine_autoregressive`.\n    """"""\n\n    def __init__(self, model, hidden_dim=None, init_loc_fn=None, num_transforms=1, **init_transform_kwargs):\n        if init_loc_fn:\n            warnings.warn(""The `init_loc_fn` argument to AutoIAFNormal is not used in practice. ""\n                          ""Please consider removing, as this may be removed in a future release."",\n                          category=FutureWarning)\n        super().__init__(model,\n                         init_transform_fn=functools.partial(iterated, num_transforms,\n                                                             affine_autoregressive,\n                                                             hidden_dims=hidden_dim,\n                                                             **init_transform_kwargs))\n\n\nclass AutoLaplaceApproximation(AutoContinuous):\n    r""""""\n    Laplace approximation (quadratic approximation) approximates the posterior\n    :math:`\\log p(z | x)` by a multivariate normal distribution in the\n    unconstrained space. Under the hood, it uses Delta distributions to\n    construct a MAP guide over the entire (unconstrained) latent space. Its\n    covariance is given by the inverse of the hessian of :math:`-\\log p(x, z)`\n    at the MAP point of `z`.\n\n    Usage::\n\n        delta_guide = AutoLaplaceApproximation(model)\n        svi = SVI(model, delta_guide, ...)\n        # ...then train the delta_guide...\n        guide = delta_guide.laplace_approximation()\n\n    By default the mean vector is initialized to an empirical prior median.\n\n    :param callable model: a generative model\n    :param callable init_loc_fn: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n    """"""\n    def _setup_prototype(self, *args, **kwargs):\n        super()._setup_prototype(*args, **kwargs)\n        # Initialize guide params\n        self.loc = nn.Parameter(self._init_loc())\n\n    def get_posterior(self, *args, **kwargs):\n        """"""\n        Returns a Delta posterior distribution for MAP inference.\n        """"""\n        return dist.Delta(self.loc).to_event(1)\n\n    def laplace_approximation(self, *args, **kwargs):\n        """"""\n        Returns a :class:`AutoMultivariateNormal` instance whose posterior\'s `loc` and\n        `scale_tril` are given by Laplace approximation.\n        """"""\n        guide_trace = poutine.trace(self).get_trace(*args, **kwargs)\n        model_trace = poutine.trace(\n            poutine.replay(self.model, trace=guide_trace)).get_trace(*args, **kwargs)\n        loss = guide_trace.log_prob_sum() - model_trace.log_prob_sum()\n\n        H = hessian(loss, self.loc)\n        cov = H.inverse()\n        loc = self.loc\n        scale_tril = cov.cholesky()\n\n        gaussian_guide = AutoMultivariateNormal(self.model)\n        gaussian_guide._setup_prototype(*args, **kwargs)\n        # Set loc, scale_tril parameters as computed above.\n        gaussian_guide.loc = loc\n        gaussian_guide.scale_tril = scale_tril\n        return gaussian_guide\n\n\nclass AutoDiscreteParallel(AutoGuide):\n    """"""\n    A discrete mean-field guide that learns a latent discrete distribution for\n    each discrete site in the model.\n    """"""\n    def _setup_prototype(self, *args, **kwargs):\n        # run the model so we can inspect its structure\n        model = poutine.block(config_enumerate(self.model), prototype_hide_fn)\n        self.prototype_trace = poutine.block(poutine.trace(model).get_trace)(*args, **kwargs)\n        if self.master is not None:\n            self.master()._check_prototype(self.prototype_trace)\n\n        self._discrete_sites = []\n        self._cond_indep_stacks = {}\n        self._prototype_frames = {}\n        for name, site in self.prototype_trace.iter_stochastic_nodes():\n            if site[""infer""].get(""enumerate"") != ""parallel"":\n                raise NotImplementedError(\'Expected sample site ""{}"" to be discrete and \'\n                                          \'configured for parallel enumeration\'.format(name))\n\n            # collect discrete sample sites\n            fn = site[""fn""]\n            Dist = type(fn)\n            if Dist in (dist.Bernoulli, dist.Categorical, dist.OneHotCategorical):\n                params = [(""probs"", fn.probs.detach().clone(), fn.arg_constraints[""probs""])]\n            else:\n                raise NotImplementedError(""{} is not supported"".format(Dist.__name__))\n            self._discrete_sites.append((site, Dist, params))\n\n            # collect independence contexts\n            self._cond_indep_stacks[name] = site[""cond_indep_stack""]\n            for frame in site[""cond_indep_stack""]:\n                if frame.vectorized:\n                    self._prototype_frames[frame.name] = frame\n                else:\n                    raise NotImplementedError(""AutoDiscreteParallel does not support sequential pyro.plate"")\n        # Initialize guide params\n        for site, Dist, param_spec in self._discrete_sites:\n            name = site[""name""]\n            for param_name, param_init, param_constraint in param_spec:\n                _deep_setattr(self, ""{}_{}"".format(name, param_name),\n                              PyroParam(param_init, constraint=param_constraint))\n\n    def forward(self, *args, **kwargs):\n        """"""\n        An automatic guide with the same ``*args, **kwargs`` as the base ``model``.\n\n        :return: A dict mapping sample site name to sampled value.\n        :rtype: dict\n        """"""\n        # if we\'ve never run the model before, do so now so we can inspect the model structure\n        if self.prototype_trace is None:\n            self._setup_prototype(*args, **kwargs)\n\n        plates = self._create_plates(*args, **kwargs)\n\n        # enumerate discrete latent samples\n        result = {}\n        for site, Dist, param_spec in self._discrete_sites:\n            name = site[""name""]\n            dist_params = {\n                param_name: operator.attrgetter(""{}_{}"".format(name, param_name))(self)\n                for param_name, param_init, param_constraint in param_spec\n            }\n            discrete_dist = Dist(**dist_params)\n\n            with ExitStack() as stack:\n                for frame in self._cond_indep_stacks[name]:\n                    stack.enter_context(plates[frame.name])\n                result[name] = pyro.sample(name, discrete_dist, infer={""enumerate"": ""parallel""})\n\n        return result\n'"
pyro/infer/autoguide/initialization.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nr""""""\nThe pyro.infer.autoguide.initialization module contains initialization functions for\nautomatic guides.\n\nThe standard interface for initialization is a function that inputs a Pyro\ntrace ``site`` dict and returns an appropriately sized ``value`` to serve\nas an initial constrained value for a guide estimate.\n""""""\nimport functools\n\nimport torch\nfrom torch.distributions import transform_to\n\nfrom pyro.distributions.torch import Independent\nfrom pyro.distributions.torch_distribution import MaskedDistribution\nfrom pyro.infer.util import is_validation_enabled\nfrom pyro.poutine.messenger import Messenger\nfrom pyro.util import torch_isnan\n\n\n# TODO: move this file out of `autoguide` in a minor release\n\ndef _is_multivariate(d):\n    while isinstance(d, (Independent, MaskedDistribution)):\n        d = d.base_dist\n    return any(size > 1 for size in d.event_shape)\n\n\ndef init_to_feasible(site=None):\n    """"""\n    Initialize to an arbitrary feasible point, ignoring distribution\n    parameters.\n    """"""\n    if site is None:\n        return init_to_feasible\n\n    value = site[""fn""].sample().detach()\n    t = transform_to(site[""fn""].support)\n    return t(torch.zeros_like(t.inv(value)))\n\n\ndef init_to_sample(site=None):\n    """"""\n    Initialize to a random sample from the prior.\n    """"""\n    if site is None:\n        return init_to_sample\n\n    return site[""fn""].sample().detach()\n\n\ndef init_to_median(site=None, num_samples=15):\n    """"""\n    Initialize to the prior median; fallback to a feasible point if median is\n    undefined.\n    """"""\n    if site is None:\n        return functools.partial(init_to_median, num_samples=num_samples)\n\n    # The median undefined for multivariate distributions.\n    if _is_multivariate(site[""fn""]):\n        return init_to_feasible(site)\n    try:\n        # Try to compute empirical median.\n        samples = site[""fn""].sample(sample_shape=(num_samples,))\n        value = samples.median(dim=0)[0]\n        if torch_isnan(value):\n            raise ValueError\n        if hasattr(site[""fn""], ""_validate_sample""):\n            site[""fn""]._validate_sample(value)\n        return value\n    except (RuntimeError, ValueError):\n        # Fall back to feasible point.\n        return init_to_feasible(site)\n\n\ndef init_to_mean(site=None):\n    """"""\n    Initialize to the prior mean; fallback to median if mean is undefined.\n    """"""\n    if site is None:\n        return init_to_mean\n\n    try:\n        # Try .mean() method.\n        value = site[""fn""].mean.detach()\n        if torch_isnan(value):\n            raise ValueError\n        if hasattr(site[""fn""], ""_validate_sample""):\n            site[""fn""]._validate_sample(value)\n        return value\n    except (NotImplementedError, ValueError):\n        # Fall back to a median.\n        # This is required for distributions with infinite variance, e.g. Cauchy.\n        return init_to_median(site)\n\n\ndef init_to_uniform(site=None, radius=2):\n    """"""\n    Initialize to a random point in the area ``(-radius, radius)`` of\n    unconstrained domain.\n\n    :param float radius: specifies the range to draw an initial point in the unconstrained domain.\n    """"""\n    if site is None:\n        return functools.partial(init_to_uniform, radius=radius)\n\n    value = site[""fn""].sample().detach()\n    t = transform_to(site[""fn""].support)\n    return t(torch.rand_like(t.inv(value)) * (2 * radius) - radius)\n\n\ndef init_to_value(site=None, values={}):\n    """"""\n    Initialize to the value specified in ``values``. We defer to\n    :func:`init_to_uniform` strategy for sites which do not appear in ``values``.\n\n    :param dict values: dictionary of initial values keyed by site name.\n    """"""\n    if site is None:\n        return functools.partial(init_to_value, values=values)\n\n    if site[""name""] in values:\n        return values[site[""name""]]\n    else:\n        return init_to_uniform(site)\n\n\nclass _InitToGenerated:\n    def __init__(self, generate):\n        self.generate = generate\n        self._init = None\n        self._seen = set()\n\n    def __call__(self, site):\n        if self._init is None or site[""name""] in self._seen:\n            self._init = self.generate()\n            self._seen = {site[""name""]}\n        return self._init(site)\n\n\ndef init_to_generated(site=None, generate=lambda: init_to_uniform):\n    """"""\n    Initialize to another initialization strategy returned by the callback\n    ``generate`` which is called once per model execution.\n\n    This is like :func:`init_to_value` but can produce different (e.g. random)\n    values once per model execution. For example to generate values and return\n    ``init_to_value`` you could define::\n\n        def generate():\n            values = {""x"": torch.randn(100), ""y"": torch.rand(5)}\n            return init_to_value(values=values)\n\n        my_init_fn = init_to_generated(generate=generate)\n\n    :param callable generate: A callable returning another initialization\n        function, e.g. returning an ``init_to_value(values={...})`` populated\n        with a dictionary of random samples.\n    """"""\n    init = _InitToGenerated(generate)\n    return init if site is None else init(site)\n\n\nclass InitMessenger(Messenger):\n    """"""\n    Initializes a site by replacing ``.sample()`` calls with values\n    drawn from an initialization strategy. This is mainly for internal use by\n    autoguide classes.\n\n    :param callable init_fn: An initialization function.\n    """"""\n    def __init__(self, init_fn):\n        self.init_fn = init_fn\n        super().__init__()\n\n    def _pyro_sample(self, msg):\n        if msg[""done""] or msg[""is_observed""] or type(msg[""fn""]).__name__ == ""_Subsample"":\n            return\n        with torch.no_grad():\n            value = self.init_fn(msg)\n        if is_validation_enabled() and msg[""value""] is not None:\n            if not isinstance(value, type(msg[""value""])):\n                raise ValueError(\n                    ""{} provided invalid type for site {}:\\nexpected {}\\nactual {}""\n                    .format(self.init_fn, msg[""name""], type(msg[""value""]), type(value)))\n            if value.shape != msg[""value""].shape:\n                raise ValueError(\n                    ""{} provided invalid shape for site {}:\\nexpected {}\\nactual {}""\n                    .format(self.init_fn, msg[""name""], msg[""value""].shape, value.shape))\n        msg[""value""] = value\n        msg[""done""] = True\n'"
pyro/infer/autoguide/utils.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro import poutine\n\n\ndef _product(shape):\n    """"""\n    Computes the product of the dimensions of a given shape tensor\n    """"""\n    result = 1\n    for size in shape:\n        result *= size\n    return result\n\n\ndef mean_field_entropy(model, args, whitelist=None):\n    """"""Computes the entropy of a model, assuming\n    that the model is fully mean-field (i.e. all sample sites\n    in the model are independent).\n\n    The entropy is simply the sum of the entropies at the\n    individual sites. If `whitelist` is not `None`, only sites\n    listed in `whitelist` will have their entropies included\n    in the sum. If `whitelist` is `None`, all non-subsample\n    sites are included.\n    """"""\n    trace = poutine.trace(model).get_trace(*args)\n    entropy = 0.\n    for name, site in trace.nodes.items():\n        if site[""type""] == ""sample"":\n            if not poutine.util.site_is_subsample(site):\n                if whitelist is None or name in whitelist:\n                    entropy += site[""fn""].entropy()\n    return entropy\n'"
pyro/infer/mcmc/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.infer.mcmc.adaptation import ArrowheadMassMatrix, BlockMassMatrix\nfrom pyro.infer.mcmc.hmc import HMC\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.infer.mcmc.nuts import NUTS\n\n__all__ = [\n    ""ArrowheadMassMatrix"",\n    ""BlockMassMatrix"",\n    ""HMC"",\n    ""MCMC"",\n    ""NUTS"",\n]\n'"
pyro/infer/mcmc/adaptation.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nfrom collections import namedtuple\n\nimport torch\n\nimport pyro\nfrom pyro.ops.arrowhead import SymmArrowhead, sqrt, triu_gram, triu_inverse, triu_matvecmul\nfrom pyro.ops.dual_averaging import DualAveraging\nfrom pyro.ops.welford import WelfordCovariance, WelfordArrowheadCovariance\n\nadapt_window = namedtuple(""adapt_window"", [""start"", ""end""])\n\n\nclass WarmupAdapter:\n    r""""""\n    Adapts tunable parameters, namely step size and mass matrix, during the\n    warmup phase. This class provides lookup properties to read the latest\n    values of ``step_size`` and ``inverse_mass_matrix``. These values are\n    periodically updated when adaptation is engaged.\n    """"""\n\n    def __init__(self,\n                 step_size=1,\n                 adapt_step_size=False,\n                 target_accept_prob=0.8,\n                 adapt_mass_matrix=False,\n                 dense_mass=False):\n        self.adapt_step_size = adapt_step_size\n        self.adapt_mass_matrix = adapt_mass_matrix\n        self.target_accept_prob = target_accept_prob\n        self.dense_mass = dense_mass\n        self.step_size = 1 if step_size is None else step_size\n        self._init_step_size = self.step_size\n        self._adaptation_disabled = not (adapt_step_size or adapt_mass_matrix)\n        if adapt_step_size:\n            self._step_size_adapt_scheme = DualAveraging()\n        self._mass_matrix_adapter = BlockMassMatrix()\n\n        # We separate warmup_steps into windows:\n        #   start_buffer + window 1 + window 2 + window 3 + ... + end_buffer\n        # where the length of each window will be doubled for the next window.\n        # We won\'t adapt mass matrix during start and end buffers; and mass\n        # matrix will be updated at the end of each window. This is helpful\n        # for dealing with the intense computation of sampling momentum from the\n        # inverse of mass matrix.\n        self._adapt_start_buffer = 75  # from Stan\n        self._adapt_end_buffer = 50  # from Stan\n        self._adapt_initial_window = 25  # from Stan\n\n        # configured later on setup\n        self._warmup_steps = None\n        self._adaptation_schedule = []\n\n    def _build_adaptation_schedule(self):\n        adaptation_schedule = []\n        # from Stan, for small warmup_steps < 20\n        if self._warmup_steps < 20:\n            adaptation_schedule.append(adapt_window(0, self._warmup_steps - 1))\n            return adaptation_schedule\n\n        start_buffer_size = self._adapt_start_buffer\n        end_buffer_size = self._adapt_end_buffer\n        init_window_size = self._adapt_initial_window\n        if (self._adapt_start_buffer + self._adapt_end_buffer\n                + self._adapt_initial_window > self._warmup_steps):\n            start_buffer_size = int(0.15 * self._warmup_steps)\n            end_buffer_size = int(0.1 * self._warmup_steps)\n            init_window_size = self._warmup_steps - start_buffer_size - end_buffer_size\n        adaptation_schedule.append(adapt_window(start=0, end=start_buffer_size - 1))\n        end_window_start = self._warmup_steps - end_buffer_size\n\n        next_window_size = init_window_size\n        next_window_start = start_buffer_size\n        while next_window_start < end_window_start:\n            cur_window_start, cur_window_size = next_window_start, next_window_size\n            # Ensure that slow adaptation windows are monotonically increasing\n            if 3 * cur_window_size <= end_window_start - cur_window_start:\n                next_window_size = 2 * cur_window_size\n            else:\n                cur_window_size = end_window_start - cur_window_start\n            next_window_start = cur_window_start + cur_window_size\n            adaptation_schedule.append(adapt_window(cur_window_start, next_window_start - 1))\n        adaptation_schedule.append(adapt_window(end_window_start,\n                                                self._warmup_steps - 1))\n        return adaptation_schedule\n\n    def reset_step_size_adaptation(self, z):\n        r""""""\n        Finds a reasonable step size and resets step size adaptation scheme.\n        """"""\n        if self._find_reasonable_step_size is not None:\n            with pyro.validation_enabled(False):\n                self.step_size = self._find_reasonable_step_size(z)\n        self._step_size_adapt_scheme.prox_center = math.log(10 * self.step_size)\n        self._step_size_adapt_scheme.reset()\n\n    def _update_step_size(self, accept_prob):\n        # calculate a statistic for Dual Averaging scheme\n        H = self.target_accept_prob - accept_prob\n        self._step_size_adapt_scheme.step(H)\n        log_step_size, _ = self._step_size_adapt_scheme.get_state()\n        self.step_size = math.exp(log_step_size)\n\n    def _end_adaptation(self):\n        if self.adapt_step_size:\n            _, log_step_size_avg = self._step_size_adapt_scheme.get_state()\n            self.step_size = math.exp(log_step_size_avg)\n\n    def configure(self, warmup_steps, initial_step_size=None, mass_matrix_shape=None,\n                  find_reasonable_step_size_fn=None, options={}):\n        r""""""\n        Model specific properties that are specified when the HMC kernel is setup.\n\n        :param warmup_steps: Number of warmup steps that the sampler is initialized with.\n        :param initial_step_size: Step size to use to initialize the Dual Averaging scheme.\n        :param mass_matrix_shape: Shape of the mass matrix.\n        :param find_reasonable_step_size_fn: A callable to find reasonable step size when\n            mass matrix is changed.\n        :param dict options: A dict which maps `dtype`, `device` to the corresponding default\n            tensor options. This is used to construct initial mass matrix in `mass_matrix_adapter`.\n        """"""\n        self._warmup_steps = warmup_steps\n        self.step_size = initial_step_size if initial_step_size is not None else self._init_step_size\n        if find_reasonable_step_size_fn is not None:\n            self._find_reasonable_step_size = find_reasonable_step_size_fn\n        if mass_matrix_shape is None or self.step_size is None:\n            raise ValueError(""Incomplete configuration - step size and inverse mass matrix ""\n                             ""need to be initialized."")\n        self.mass_matrix_adapter.configure(mass_matrix_shape, self.adapt_mass_matrix, options=options)\n        if not self._adaptation_disabled:\n            self._adaptation_schedule = self._build_adaptation_schedule()\n        self._current_window = 0  # starting window index\n        if self.adapt_step_size:\n            self._step_size_adapt_scheme.reset()\n\n    def step(self, t, z, accept_prob, z_grad=None):\n        r""""""\n        Called at each step during the warmup phase to learn tunable\n        parameters.\n\n        :param int t: time step, beginning at 0.\n        :param dict z: latent variables.\n        :param float accept_prob: acceptance probability of the proposal.\n        """"""\n        if t >= self._warmup_steps or self._adaptation_disabled:\n            return\n        window = self._adaptation_schedule[self._current_window]\n        num_windows = len(self._adaptation_schedule)\n        mass_matrix_adaptation_phase = self.adapt_mass_matrix and \\\n            (0 < self._current_window < num_windows - 1)\n        if self.adapt_step_size:\n            self._update_step_size(accept_prob.item())\n        if mass_matrix_adaptation_phase:\n            self.mass_matrix_adapter.update(z, z_grad)\n\n        if t == window.end:\n            if self._current_window == num_windows - 1:\n                self._current_window += 1\n                self._end_adaptation()\n                return\n\n            if self._current_window == 0:\n                self._current_window += 1\n                return\n\n            if mass_matrix_adaptation_phase:\n                self.mass_matrix_adapter.end_adaptation()\n                if self.adapt_step_size:\n                    self.reset_step_size_adaptation(z)\n\n            self._current_window += 1\n\n    @property\n    def adaptation_schedule(self):\n        return self._adaptation_schedule\n\n    @property\n    def mass_matrix_adapter(self):\n        return self._mass_matrix_adapter\n\n    @mass_matrix_adapter.setter\n    def mass_matrix_adapter(self, value):\n        self._mass_matrix_adapter = value\n\n\n# this works for diagonal matrix `x`\ndef _matvecmul(x, y):\n    return x.mul(y) if x.dim() == 1 else x.matmul(y)\n\n\ndef _cholesky(x):\n    return x.sqrt() if x.dim() == 1 else x.cholesky()\n\n\ndef _transpose(x):\n    return x if x.dim() == 1 else x.t()\n\n\ndef _triu_inverse(x):\n    if x.dim() == 1:\n        return x.reciprocal()\n    else:\n        identity = torch.eye(x.size(-1), dtype=x.dtype, device=x.device)\n        return torch.triangular_solve(identity, x, upper=True)[0]\n\n\nclass BlockMassMatrix:\n    """"""\n    EXPERIMENTAL This class is used to adapt (inverse) mass matrix and provide\n    useful methods to calculate algebraic terms which involves the mass matrix.\n\n    The mass matrix will have block structure, which can be specified by\n    using the method :meth:`configure` with the corresponding structured\n    `mass_matrix_shape` arg.\n\n    :param float init_scale: initial scale to construct the initial mass matrix.\n    """"""\n    def __init__(self, init_scale=1.):\n        # TODO: we might allow users specify the initial mass matrix in the constructor.\n        self._init_scale = init_scale\n        self._adapt_scheme = {}\n        self._inverse_mass_matrix = {}\n        # NB: those sqrt matrices are upper triangular\n        self._mass_matrix_sqrt = {}\n        self._mass_matrix_sqrt_inverse = {}\n        self._mass_matrix_size = {}\n\n    @property\n    def mass_matrix_size(self):\n        """"""\n        A dict that maps site names to the size of the corresponding mass matrix.\n        """"""\n        return self._mass_matrix_size\n\n    @property\n    def inverse_mass_matrix(self):\n        return self._inverse_mass_matrix\n\n    @inverse_mass_matrix.setter\n    def inverse_mass_matrix(self, value):\n        for site_names, inverse_mass_matrix in value.items():\n            if site_names in self._adapt_scheme:\n                self._adapt_scheme[site_names].reset()\n            mass_matrix_sqrt_inverse = _transpose(_cholesky(inverse_mass_matrix))\n            mass_matrix_sqrt = _triu_inverse(mass_matrix_sqrt_inverse)\n            self._inverse_mass_matrix[site_names] = inverse_mass_matrix\n            self._mass_matrix_sqrt[site_names] = mass_matrix_sqrt\n            self._mass_matrix_sqrt_inverse[site_names] = mass_matrix_sqrt_inverse\n\n    def configure(self, mass_matrix_shape, adapt_mass_matrix=True, options={}):\n        """"""\n        Sets up an initial mass matrix.\n\n        :param dict mass_matrix_shape: a dict that maps tuples of site names to the shape of\n            the corresponding mass matrix. Each tuple of site names corresponds to a block.\n        :param bool adapt_mass_matrix: a flag to decide whether an adaptation scheme will be used.\n        :param dict options: tensor options to construct the initial mass matrix.\n        """"""\n        inverse_mass_matrix = {}\n        for site_names, shape in mass_matrix_shape.items():\n            self._mass_matrix_size[site_names] = shape[0]\n            diagonal = len(shape) == 1\n            inverse_mass_matrix[site_names] = torch.full(shape, self._init_scale, **options) \\\n                if diagonal else torch.eye(*shape, **options) * self._init_scale\n            if adapt_mass_matrix:\n                adapt_scheme = WelfordCovariance(diagonal=diagonal)\n                self._adapt_scheme[site_names] = adapt_scheme\n\n        self.inverse_mass_matrix = inverse_mass_matrix\n\n    def update(self, z, z_grad):\n        """"""\n        Updates the adaptation scheme using the new sample `z` or its grad `z_grad`.\n\n        :param dict z: the current value.\n        :param dict z_grad: grad of the current value.\n        """"""\n        for site_names, adapt_scheme in self._adapt_scheme.items():\n            z_flat = torch.cat([z[name].detach().reshape(-1) for name in site_names])\n            adapt_scheme.update(z_flat)\n\n    def end_adaptation(self):\n        """"""\n        Updates the current mass matrix using the adaptation scheme.\n        """"""\n        inverse_mass_matrix = {}\n        for site_names, adapt_scheme in self._adapt_scheme.items():\n            inverse_mass_matrix[site_names] = adapt_scheme.get_covariance(regularize=True)\n        self.inverse_mass_matrix = inverse_mass_matrix\n\n    def kinetic_grad(self, r):\n        """"""\n        Computes the gradient of kinetic energy w.r.t. the momentum `r`.\n        It is equivalent to compute velocity given the momentum `r`.\n\n        :param dict r: a dictionary maps site names to a tensor momentum.\n        :returns: a dictionary maps site names to the corresponding gradient\n        """"""\n        v = {}\n        for site_names, inverse_mass_matrix in self._inverse_mass_matrix.items():\n            r_flat = torch.cat([r[site_name].reshape(-1) for site_name in site_names])\n            v_flat = _matvecmul(inverse_mass_matrix, r_flat)\n\n            # unpacking\n            pos = 0\n            for site_name in site_names:\n                next_pos = pos + r[site_name].numel()\n                v[site_name] = v_flat[pos:next_pos].reshape(r[site_name].shape)\n                pos = next_pos\n        return v\n\n    def scale(self, r_unscaled, r_prototype):\n        """"""\n        Computes `M^{1/2} @ r_unscaled`.\n\n        Note that `r` is generated from a gaussian with scale `mass_matrix_sqrt`.\n        This method will scale it.\n\n        :param dict r_unscaled: a dictionary maps site names to a tensor momentum.\n        :param dict r_prototype: a dictionary mapes site names to prototype momentum.\n            Those prototype values are used to get shapes of the scaled version.\n        :returns: a dictionary maps site names to the corresponding tensor\n        """"""\n        s = {}\n        for site_names, mass_matrix_sqrt in self._mass_matrix_sqrt.items():\n            r_flat = _matvecmul(mass_matrix_sqrt, r_unscaled[site_names])\n\n            # unpacking\n            pos = 0\n            for site_name in site_names:\n                next_pos = pos + r_prototype[site_name].numel()\n                s[site_name] = r_flat[pos:next_pos].reshape(r_prototype[site_name].shape)\n                pos = next_pos\n        return s\n\n    def unscale(self, r):\n        """"""\n        Computes `inv(M^{1/2}) @ r`.\n\n        Note that `r` is generated from a gaussian with scale `mass_matrix_sqrt`.\n        This method will unscale it.\n\n        :param dict r: a dictionary maps site names to a tensor momentum.\n        :returns: a dictionary maps site names to the corresponding tensor\n        """"""\n        u = {}\n        for site_names, mass_matrix_sqrt_inverse in self._mass_matrix_sqrt_inverse.items():\n            r_flat = torch.cat([r[site_name].reshape(-1) for site_name in site_names])\n            u[site_names] = _matvecmul(mass_matrix_sqrt_inverse, r_flat)\n        return u\n\n\nclass ArrowheadMassMatrix:\n    """"""\n    EXPERIMENTAL This class is used to adapt (inverse) mass matrix and provide useful\n    methods to calculate algebraic terms which involves the mass matrix.\n\n    The mass matrix will have arrowhead structure, with the head including all\n    dense sites specified in the argument `full_mass` of the HMC/NUTS kernels.\n\n    :param float init_scale: initial scale to construct the initial mass matrix.\n    """"""\n    def __init__(self, init_scale=1.):\n        self._init_scale = init_scale\n        self._adapt_scheme = {}\n        self._mass_matrix = {}\n        # NB: like BlockMassMatrix, those sqrt matrices are upper triangular\n        self._mass_matrix_sqrt = {}\n        self._mass_matrix_sqrt_inverse = {}\n        self._mass_matrix_size = {}\n\n    @property\n    def mass_matrix_size(self):\n        """"""\n        A dict that maps site names to the size of the corresponding mass matrix.\n        """"""\n        return self._mass_matrix_size\n\n    @property\n    def inverse_mass_matrix(self):\n        # NB: this computation is O(N^2 x head_size)\n        # however, HMC/NUTS kernel does not require us computing inverse_mass_matrix;\n        # so all linear algebra cost in HMC/NUTS is still O(N x head_size^2);\n        # we still expose this property for testing and for backward compatibility\n        inverse_mass_matrix = {}\n        for site_names, sqrt_inverse in self._mass_matrix_sqrt_inverse.items():\n            inverse_mass_matrix[site_names] = triu_gram(sqrt_inverse)\n        return inverse_mass_matrix\n\n    @property\n    def mass_matrix(self):\n        return self._mass_matrix\n\n    @mass_matrix.setter\n    def mass_matrix(self, value):\n        for site_names, mass_matrix in value.items():\n            # XXX: consider to add a try/except here:\n            # if mass_matrix is not positive definite, we won\'t reset adapt_scheme\n            self._adapt_scheme[site_names].reset()\n            mass_matrix_sqrt = sqrt(mass_matrix)\n            mass_matrix_sqrt_inverse = triu_inverse(mass_matrix_sqrt)\n            self._mass_matrix[site_names] = mass_matrix\n            self._mass_matrix_sqrt[site_names] = mass_matrix_sqrt\n            self._mass_matrix_sqrt_inverse[site_names] = mass_matrix_sqrt_inverse\n\n    def configure(self, mass_matrix_shape, adapt_mass_matrix=True, options={}):\n        """"""\n        Sets up an initial mass matrix.\n\n        :param dict mass_matrix_shape: a dict that maps tuples of site names to the shape of\n            the corresponding mass matrix. Each tuple of site names corresponds to a block.\n        :param bool adapt_mass_matrix: a flag to decide whether an adaptation scheme will be used.\n        :param dict options: tensor options to construct the initial mass matrix.\n        """"""\n        mass_matrix = {}\n        dense_sites = ()\n        dense_size = 0\n        diag_sites = ()\n        diag_size = 0\n        for site_names, shape in mass_matrix_shape.items():\n            if len(shape) == 2:\n                dense_sites = dense_sites + site_names\n                dense_size = dense_size + shape[0]\n            else:\n                diag_sites = diag_sites + site_names\n                diag_size = diag_size + shape[0]\n\n        size = dense_size + diag_size\n        head_size = dense_size\n        all_sites = dense_sites + diag_sites\n        self._mass_matrix_size[all_sites] = size\n        top = torch.eye(head_size, size, **options) * self._init_scale\n        bottom_diag = torch.full((size - head_size,), self._init_scale, **options)\n        mass_matrix[all_sites] = SymmArrowhead(top, bottom_diag)\n        if adapt_mass_matrix:\n            adapt_scheme = WelfordArrowheadCovariance(head_size=head_size)\n            self._adapt_scheme[all_sites] = adapt_scheme\n\n        self.mass_matrix = mass_matrix\n\n    def update(self, z, z_grad):\n        """"""\n        Updates the adaptation scheme using the new sample `z` or its grad `z_grad`.\n\n        :param dict z: the current value.\n        :param dict z_grad: grad of the current value.\n        """"""\n        for site_names, adapt_scheme in self._adapt_scheme.items():\n            z_grad_flat = torch.cat([z_grad[name].reshape(-1) for name in site_names])\n            adapt_scheme.update(z_grad_flat)\n\n    def end_adaptation(self):\n        """"""\n        Updates the current mass matrix using the adaptation scheme.\n        """"""\n        mass_matrix = {}\n        for site_names, adapt_scheme in self._adapt_scheme.items():\n            top, bottom_diag = adapt_scheme.get_covariance(regularize=True)\n            mass_matrix[site_names] = SymmArrowhead(top, bottom_diag)\n        self.mass_matrix = mass_matrix\n\n    def kinetic_grad(self, r):\n        """"""\n        Computes the gradient of kinetic energy w.r.t. the momentum `r`.\n        It is equivalent to compute velocity given the momentum `r`.\n\n        :param dict r: a dictionary maps site names to a tensor momentum.\n        :returns: a dictionary maps site names to the corresponding gradient\n        """"""\n        v = {}\n        for site_names, mass_matrix_sqrt_inverse in self._mass_matrix_sqrt_inverse.items():\n            r_flat = torch.cat([r[site_name].reshape(-1) for site_name in site_names])\n            # NB: using inverse_mass_matrix as in BlockMassMatrix will cost\n            # O(N^2 x head_size) operators and O(N^2) memory requirement;\n            # here, we will leverage mass_matrix_sqrt_inverse to reduce the cost to\n            # O(N x head_size^2) operators and O(N x head_size) memory requirement.\n            r_unscaled = triu_matvecmul(mass_matrix_sqrt_inverse, r_flat)\n            v_flat = triu_matvecmul(mass_matrix_sqrt_inverse, r_unscaled, transpose=True)\n\n            # unpacking\n            pos = 0\n            for site_name in site_names:\n                next_pos = pos + r[site_name].numel()\n                v[site_name] = v_flat[pos:next_pos].reshape(r[site_name].shape)\n                pos = next_pos\n        return v\n\n    def scale(self, r_unscaled, r_prototype):\n        """"""\n        Computes `M^{1/2} @ r_unscaled`.\n\n        Note that `r` is generated from a gaussian with scale `mass_matrix_sqrt`.\n        This method will scale it.\n\n        :param dict r_unscaled: a dictionary maps site names to a tensor momentum.\n        :param dict r_prototype: a dictionary mapes site names to prototype momentum.\n            Those prototype values are used to get shapes of the scaled version.\n        :returns: a dictionary maps site names to the corresponding tensor\n        """"""\n        s = {}\n        for site_names, mass_matrix_sqrt in self._mass_matrix_sqrt.items():\n            r_flat = triu_matvecmul(mass_matrix_sqrt, r_unscaled[site_names])\n\n            # unpacking\n            pos = 0\n            for site_name in site_names:\n                next_pos = pos + r_prototype[site_name].numel()\n                s[site_name] = r_flat[pos:next_pos].reshape(r_prototype[site_name].shape)\n                pos = next_pos\n        return s\n\n    def unscale(self, r):\n        """"""\n        Computes `inv(M^{1/2}) @ r`.\n\n        Note that `r` is generated from a gaussian with scale `mass_matrix_sqrt`.\n        This method will unscale it.\n\n        :param dict r: a dictionary maps site names to a tensor momentum.\n        :returns: a dictionary maps site names to the corresponding tensor\n        """"""\n        u = {}\n        for site_names, mass_matrix_sqrt_inverse in self._mass_matrix_sqrt_inverse.items():\n            r_flat = torch.cat([r[site_name].reshape(-1) for site_name in site_names])\n            u[site_names] = triu_matvecmul(mass_matrix_sqrt_inverse, r_flat)\n        return u\n'"
pyro/infer/mcmc/api.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis module offers a modified interface for MCMC inference with the following objectives:\n  - making MCMC independent of Pyro specific trace data structure, to facilitate\n    integration with other PyTorch based libraries.\n  - bringing the interface closer to that of NumPyro to make it easier to write\n    code that works with different backends.\n  - minimal memory consumption with multiprocessing and CUDA.\n""""""\n\nimport json\nimport logging\nimport queue\nimport signal\nimport threading\nimport warnings\nfrom collections import OrderedDict\n\nimport torch\nimport torch.multiprocessing as mp\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.infer.mcmc.hmc import HMC\nfrom pyro.infer.mcmc.logger import DIAGNOSTIC_MSG, ProgressBar, TqdmHandler, initialize_logger\nfrom pyro.infer.mcmc.nuts import NUTS\nfrom pyro.infer.mcmc.util import diagnostics, initialize_model, print_summary\nfrom pyro.util import optional\n\nMAX_SEED = 2**32 - 1\n\n\ndef logger_thread(log_queue, warmup_steps, num_samples, num_chains, disable_progbar=False):\n    """"""\n    Logging thread that asynchronously consumes logging events from `log_queue`,\n    and handles them appropriately.\n    """"""\n    progress_bars = ProgressBar(warmup_steps, num_samples, disable=disable_progbar, num_bars=num_chains)\n    logger = logging.getLogger(__name__)\n    logger.propagate = False\n    logger.addHandler(TqdmHandler())\n    num_samples = [0] * num_chains\n    try:\n        while True:\n            try:\n                record = log_queue.get(timeout=1)\n            except queue.Empty:\n                continue\n            if record is None:\n                break\n            metadata, msg = record.getMessage().split(""]"", 1)\n            _, msg_type, logger_id = metadata[1:].split()\n            if msg_type == DIAGNOSTIC_MSG:\n                pbar_pos = int(logger_id.split("":"")[-1])\n                num_samples[pbar_pos] += 1\n                if num_samples[pbar_pos] == warmup_steps:\n                    progress_bars.set_description(""Sample [{}]"".format(pbar_pos + 1), pos=pbar_pos)\n                diagnostics = json.loads(msg, object_pairs_hook=OrderedDict)\n                progress_bars.set_postfix(diagnostics, pos=pbar_pos, refresh=False)\n                progress_bars.update(pos=pbar_pos)\n            else:\n                logger.handle(record)\n    finally:\n        progress_bars.close()\n\n\nclass _Worker:\n    def __init__(self, chain_id, result_queue, log_queue, event, kernel, num_samples,\n                 warmup_steps, initial_params=None, hook=None):\n        self.chain_id = chain_id\n        self.kernel = kernel\n        if initial_params is not None:\n            self.kernel.initial_params = initial_params\n        self.num_samples = num_samples\n        self.warmup_steps = warmup_steps\n        self.rng_seed = (torch.initial_seed() + chain_id) % MAX_SEED\n        self.log_queue = log_queue\n        self.result_queue = result_queue\n        self.default_tensor_type = torch.Tensor().type()\n        self.hook = hook\n        self.event = event\n\n    def run(self, *args, **kwargs):\n        pyro.set_rng_seed(self.rng_seed)\n        torch.set_default_tensor_type(self.default_tensor_type)\n        # XXX we clone CUDA tensor args to resolve the issue ""Invalid device pointer""\n        # at https://github.com/pytorch/pytorch/issues/10375\n        args = [arg.clone().detach() if (torch.is_tensor(arg) and arg.is_cuda) else arg for arg in args]\n        kwargs = kwargs\n        logger = logging.getLogger(""pyro.infer.mcmc"")\n        logger_id = ""CHAIN:{}"".format(self.chain_id)\n        log_queue = self.log_queue\n        logger = initialize_logger(logger, logger_id, None, log_queue)\n        logging_hook = _add_logging_hook(logger, None, self.hook)\n\n        try:\n            for sample in _gen_samples(self.kernel, self.warmup_steps, self.num_samples, logging_hook,\n                                       None, *args, **kwargs):\n                self.result_queue.put_nowait((self.chain_id, sample))\n                self.event.wait()\n                self.event.clear()\n            self.result_queue.put_nowait((self.chain_id, None))\n        except Exception as e:\n            logger.exception(e)\n            self.result_queue.put_nowait((self.chain_id, e))\n\n\ndef _gen_samples(kernel, warmup_steps, num_samples, hook, chain_id, *args, **kwargs):\n    kernel.setup(warmup_steps, *args, **kwargs)\n    params = kernel.initial_params\n    # yield structure (key, value.shape) of params\n    yield {k: v.shape for k, v in params.items()}\n    for i in range(warmup_steps):\n        params = kernel.sample(params)\n        hook(kernel, params, \'Warmup [{}]\'.format(chain_id) if chain_id is not None else \'Warmup\', i)\n    for i in range(num_samples):\n        params = kernel.sample(params)\n        hook(kernel, params, \'Sample [{}]\'.format(chain_id) if chain_id is not None else \'Sample\', i)\n        yield torch.cat([params[site].reshape(-1) for site in sorted(params)]) if params else torch.tensor([])\n    yield kernel.diagnostics()\n    kernel.cleanup()\n\n\ndef _add_logging_hook(logger, progress_bar=None, hook=None):\n    def _add_logging(kernel, params, stage, i):\n        diagnostics = json.dumps(kernel.logging())\n        logger.info(diagnostics, extra={""msg_type"": DIAGNOSTIC_MSG})\n        if progress_bar:\n            progress_bar.set_description(stage, refresh=False)\n        if hook:\n            hook(kernel, params, stage, i)\n\n    return _add_logging\n\n\nclass _UnarySampler:\n    """"""\n    Single process runner class optimized for the case chains are drawn sequentially.\n    """"""\n\n    def __init__(self, kernel, num_samples, warmup_steps, num_chains, disable_progbar, initial_params=None, hook=None):\n        self.kernel = kernel\n        self.initial_params = initial_params\n        self.warmup_steps = warmup_steps\n        self.num_samples = num_samples\n        self.num_chains = num_chains\n        self.logger = None\n        self.disable_progbar = disable_progbar\n        self.hook = hook\n        super().__init__()\n\n    def terminate(self, *args, **kwargs):\n        pass\n\n    def run(self, *args, **kwargs):\n        logger = logging.getLogger(""pyro.infer.mcmc"")\n        for i in range(self.num_chains):\n            if self.initial_params is not None:\n                initial_params = {k: v[i] for k, v in self.initial_params.items()}\n                self.kernel.initial_params = initial_params\n\n            progress_bar = ProgressBar(self.warmup_steps, self.num_samples, disable=self.disable_progbar)\n            logger = initialize_logger(logger, """", progress_bar)\n            hook_w_logging = _add_logging_hook(logger, progress_bar, self.hook)\n            for sample in _gen_samples(self.kernel, self.warmup_steps, self.num_samples, hook_w_logging,\n                                       i if self.num_chains > 1 else None,\n                                       *args, **kwargs):\n                yield sample, i  # sample, chain_id\n            self.kernel.cleanup()\n            progress_bar.close()\n\n\nclass _MultiSampler:\n    """"""\n    Parallel runner class for running MCMC chains in parallel. This uses the\n    `torch.multiprocessing` module (itself a light wrapper over the python\n    `multiprocessing` module) to spin up parallel workers.\n    """"""\n    def __init__(self, kernel, num_samples, warmup_steps, num_chains, mp_context,\n                 disable_progbar, initial_params=None, hook=None):\n        self.kernel = kernel\n        self.warmup_steps = warmup_steps\n        self.num_chains = num_chains\n        self.hook = hook\n        self.workers = []\n        self.ctx = mp\n        if mp_context:\n            self.ctx = mp.get_context(mp_context)\n        self.result_queue = self.ctx.Queue()\n        self.log_queue = self.ctx.Queue()\n        self.logger = initialize_logger(logging.getLogger(""pyro.infer.mcmc""),\n                                        ""MAIN"", log_queue=self.log_queue)\n        self.num_samples = num_samples\n        self.initial_params = initial_params\n        self.log_thread = threading.Thread(target=logger_thread,\n                                           args=(self.log_queue, self.warmup_steps, self.num_samples,\n                                                 self.num_chains, disable_progbar))\n        self.log_thread.daemon = True\n        self.log_thread.start()\n        self.events = [self.ctx.Event() for _ in range(num_chains)]\n\n    def init_workers(self, *args, **kwargs):\n        self.workers = []\n        for i in range(self.num_chains):\n            init_params = {k: v[i] for k, v in self.initial_params.items()} if self.initial_params is not None else None\n            worker = _Worker(i, self.result_queue, self.log_queue, self.events[i], self.kernel,\n                             self.num_samples, self.warmup_steps, initial_params=init_params, hook=self.hook)\n            worker.daemon = True\n            self.workers.append(self.ctx.Process(name=str(i), target=worker.run,\n                                                 args=args, kwargs=kwargs))\n\n    def terminate(self, terminate_workers=False):\n        if self.log_thread.is_alive():\n            self.log_queue.put_nowait(None)\n            self.log_thread.join(timeout=1)\n        # Only kill workers if exception is raised. worker processes are daemon\n        # processes that will otherwise be terminated with the main process.\n        # Note that it is important to not\n        if terminate_workers:\n            for w in self.workers:\n                if w.is_alive():\n                    w.terminate()\n\n    def run(self, *args, **kwargs):\n        # Ignore sigint in worker processes; they will be shut down\n        # when the main process terminates.\n        sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n        self.init_workers(*args, **kwargs)\n        # restore original handler\n        signal.signal(signal.SIGINT, sigint_handler)\n        active_workers = self.num_chains\n        exc_raised = True\n        try:\n            for w in self.workers:\n                w.start()\n            while active_workers:\n                try:\n                    chain_id, val = self.result_queue.get(timeout=5)\n                except queue.Empty:\n                    continue\n                if isinstance(val, Exception):\n                    # Exception trace is already logged by worker.\n                    raise val\n                if val is not None:\n                    yield val, chain_id\n                    self.events[chain_id].set()\n                else:\n                    active_workers -= 1\n            exc_raised = False\n        finally:\n            self.terminate(terminate_workers=exc_raised)\n\n\nclass MCMC:\n    """"""\n    Wrapper class for Markov Chain Monte Carlo algorithms. Specific MCMC algorithms\n    are TraceKernel instances and need to be supplied as a ``kernel`` argument\n    to the constructor.\n\n    .. note:: The case of `num_chains > 1` uses python multiprocessing to\n        run parallel chains in multiple processes. This goes with the usual\n        caveats around multiprocessing in python, e.g. the model used to\n        initialize the ``kernel`` must be serializable via `pickle`, and the\n        performance / constraints will be platform dependent (e.g. only\n        the ""spawn"" context is available in Windows). This has also not\n        been extensively tested on the Windows platform.\n\n    :param kernel: An instance of the ``TraceKernel`` class, which when\n        given an execution trace returns another sample trace from the target\n        (posterior) distribution.\n    :param int num_samples: The number of samples that need to be generated,\n        excluding the samples discarded during the warmup phase.\n    :param int warmup_steps: Number of warmup iterations. The samples generated\n        during the warmup phase are discarded. If not provided, default is\n        is the same as `num_samples`.\n    :param int num_chains: Number of MCMC chains to run in parallel. Depending on\n        whether `num_chains` is 1 or more than 1, this class internally dispatches\n        to either `_UnarySampler` or `_MultiSampler`.\n    :param dict initial_params: dict containing initial tensors in unconstrained\n        space to initiate the markov chain. The leading dimension\'s size must match\n        that of `num_chains`. If not specified, parameter values will be sampled from\n        the prior.\n    :param hook_fn: Python callable that takes in `(kernel, samples, stage, i)`\n        as arguments. stage is either `sample` or `warmup` and i refers to the\n        i\'th sample for the given stage. This can be used to implement additional\n        logging, or more generally, run arbitrary code per generated sample.\n    :param str mp_context: Multiprocessing context to use when `num_chains > 1`.\n        Only applicable for Python 3.5 and above. Use `mp_context=""spawn""` for\n        CUDA.\n    :param bool disable_progbar: Disable progress bar and diagnostics update.\n    :param bool disable_validation: Disables distribution validation check.\n        Defaults to ``True``, disabling validation, since divergent transitions\n        will lead to exceptions. Switch to ``False`` to enable validation, or\n        to ``None`` to preserve existing global values.\n    :param dict transforms: dictionary that specifies a transform for a sample site\n        with constrained support to unconstrained space.\n    """"""\n    def __init__(self, kernel, num_samples, warmup_steps=None, initial_params=None,\n                 num_chains=1, hook_fn=None, mp_context=None, disable_progbar=False,\n                 disable_validation=True, transforms=None):\n        self.warmup_steps = num_samples if warmup_steps is None else warmup_steps  # Stan\n        self.num_samples = num_samples\n        self.kernel = kernel\n        self.transforms = transforms\n        self.disable_validation = disable_validation\n        self._samples = None\n        self._args = None\n        self._kwargs = None\n        if isinstance(self.kernel, (HMC, NUTS)) and self.kernel.potential_fn is not None:\n            if initial_params is None:\n                raise ValueError(""Must provide valid initial parameters to begin sampling""\n                                 "" when using `potential_fn` in HMC/NUTS kernel."")\n        parallel = False\n        if num_chains > 1:\n            # check that initial_params is different for each chain\n            if initial_params:\n                for v in initial_params.values():\n                    if v.shape[0] != num_chains:\n                        raise ValueError(""The leading dimension of tensors in `initial_params` ""\n                                         ""must match the number of chains."")\n                # FIXME: probably we want to use ""spawn"" method by default to avoid the error\n                # CUDA initialization error https://github.com/pytorch/pytorch/issues/2517\n                # even that we run MCMC in CPU.\n                if mp_context is None:\n                    # change multiprocessing context to \'spawn\' for CUDA tensors.\n                    if list(initial_params.values())[0].is_cuda:\n                        mp_context = ""spawn""\n\n            # verify num_chains is compatible with available CPU.\n            available_cpu = max(mp.cpu_count() - 1, 1)  # reserving 1 for the main process.\n            if num_chains <= available_cpu:\n                parallel = True\n            else:\n                warnings.warn(""num_chains={} is more than available_cpu={}. ""\n                              ""Chains will be drawn sequentially.""\n                              .format(num_chains, available_cpu))\n        else:\n            if initial_params:\n                initial_params = {k: v.unsqueeze(0) for k, v in initial_params.items()}\n\n        self.num_chains = num_chains\n        self._diagnostics = [None] * num_chains\n\n        if parallel:\n            self.sampler = _MultiSampler(kernel, num_samples, self.warmup_steps, num_chains, mp_context,\n                                         disable_progbar, initial_params=initial_params, hook=hook_fn)\n        else:\n            self.sampler = _UnarySampler(kernel, num_samples, self.warmup_steps, num_chains, disable_progbar,\n                                         initial_params=initial_params, hook=hook_fn)\n\n    @poutine.block\n    def run(self, *args, **kwargs):\n        """"""\n        Run MCMC to generate samples and populate `self._samples`.\n\n        Example usage:\n\n        .. code-block:: python\n\n            def model(data):\n                ...\n\n            nuts_kernel = NUTS(model)\n            mcmc = MCMC(nuts_kernel, num_samples=500)\n            mcmc.run(data)\n            samples = mcmc.get_samples()\n\n        :param args: optional arguments taken by\n            :meth:`MCMCKernel.setup <pyro.infer.mcmc.mcmc_kernel.MCMCKernel.setup>`.\n        :param kwargs: optional keywords arguments taken by\n            :meth:`MCMCKernel.setup <pyro.infer.mcmc.mcmc_kernel.MCMCKernel.setup>`.\n        """"""\n        self._args, self._kwargs = args, kwargs\n        num_samples = [0] * self.num_chains\n        z_flat_acc = [[] for _ in range(self.num_chains)]\n        with optional(pyro.validation_enabled(not self.disable_validation),\n                      self.disable_validation is not None):\n            for x, chain_id in self.sampler.run(*args, **kwargs):\n                if num_samples[chain_id] == 0:\n                    num_samples[chain_id] += 1\n                    z_structure = x\n                elif num_samples[chain_id] == self.num_samples + 1:\n                    self._diagnostics[chain_id] = x\n                else:\n                    num_samples[chain_id] += 1\n                    if self.num_chains > 1:\n                        x_cloned = x.clone()\n                        del x\n                    else:\n                        x_cloned = x\n                    z_flat_acc[chain_id].append(x_cloned)\n\n        z_flat_acc = torch.stack([torch.stack(l) for l in z_flat_acc])\n\n        # unpack latent\n        pos = 0\n        z_acc = z_structure.copy()\n        for k in sorted(z_structure):\n            shape = z_structure[k]\n            next_pos = pos + shape.numel()\n            z_acc[k] = z_flat_acc[:, :, pos:next_pos].reshape(\n                (self.num_chains, self.num_samples) + shape)\n            pos = next_pos\n        assert pos == z_flat_acc.shape[-1]\n\n        # If transforms is not explicitly provided, infer automatically using\n        # model args, kwargs.\n        if self.transforms is None:\n            # Use `kernel.transforms` when available\n            if hasattr(self.kernel, \'transforms\') and self.kernel.transforms is not None:\n                self.transforms = self.kernel.transforms\n            # Else, get transforms from model (e.g. in multiprocessing).\n            elif self.kernel.model:\n                _, _, self.transforms, _ = initialize_model(self.kernel.model,\n                                                            model_args=args,\n                                                            model_kwargs=kwargs,\n                                                            initial_params={})\n            # Assign default value\n            else:\n                self.transforms = {}\n\n        # transform samples back to constrained space\n        for name, transform in self.transforms.items():\n            z_acc[name] = transform.inv(z_acc[name])\n        self._samples = z_acc\n\n        # terminate the sampler (shut down worker processes)\n        self.sampler.terminate(True)\n\n    def get_samples(self, num_samples=None, group_by_chain=False):\n        """"""\n        Get samples from the MCMC run, potentially resampling with replacement.\n\n        :param int num_samples: Number of samples to return. If `None`, all the samples\n            from an MCMC chain are returned in their original ordering.\n        :param bool group_by_chain: Whether to preserve the chain dimension. If True,\n            all samples will have num_chains as the size of their leading dimension.\n        :return: dictionary of samples keyed by site name.\n        """"""\n        samples = self._samples\n        if num_samples is None:\n            # reshape to collapse chain dim when group_by_chain=False\n            if not group_by_chain:\n                samples = {k: v.reshape((-1,) + v.shape[2:]) for k, v in samples.items()}\n        else:\n            if not samples:\n                raise ValueError(""No samples found from MCMC run."")\n            if group_by_chain:\n                batch_dim = 1\n            else:\n                samples = {k: v.reshape((-1,) + v.shape[2:]) for k, v in samples.items()}\n                batch_dim = 0\n            sample_tensor = list(samples.values())[0]\n            batch_size, device = sample_tensor.shape[batch_dim], sample_tensor.device\n            idxs = torch.randint(0, batch_size, size=(num_samples,), device=device)\n            samples = {k: v.index_select(batch_dim, idxs) for k, v in samples.items()}\n        return samples\n\n    def diagnostics(self):\n        """"""\n        Gets some diagnostics statistics such as effective sample size, split\n        Gelman-Rubin, or divergent transitions from the sampler.\n        """"""\n        diag = diagnostics(self._samples)\n        for diag_name in self._diagnostics[0]:\n            diag[diag_name] = {\'chain {}\'.format(i): self._diagnostics[i][diag_name]\n                               for i in range(self.num_chains)}\n        return diag\n\n    def summary(self, prob=0.9):\n        """"""\n        Prints a summary table displaying diagnostics of samples obtained from\n        posterior. The diagnostics displayed are mean, standard deviation, median,\n        the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,\n        :func:`~pyro.ops.stats.split_gelman_rubin`.\n\n        :param float prob: the probability mass of samples within the credibility interval.\n        """"""\n        print_summary(self._samples, prob=prob)\n        if \'divergences\' in self._diagnostics[0]:\n            print(""Number of divergences: {}"".format(\n                sum([len(self._diagnostics[i][\'divergences\']) for i in range(self.num_chains)])))\n'"
pyro/infer/mcmc/hmc.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nfrom collections import OrderedDict\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions.util import scalar_like\nfrom pyro.distributions.testing.fakes import NonreparameterizedNormal\n\nfrom pyro.infer.autoguide import init_to_uniform\nfrom pyro.infer.mcmc.adaptation import WarmupAdapter\nfrom pyro.infer.mcmc.mcmc_kernel import MCMCKernel\nfrom pyro.infer.mcmc.util import initialize_model\nfrom pyro.ops.integrator import potential_grad, velocity_verlet\nfrom pyro.util import optional, torch_isnan\n\n\nclass HMC(MCMCKernel):\n    r""""""\n    Simple Hamiltonian Monte Carlo kernel, where ``step_size`` and ``num_steps``\n    need to be explicitly specified by the user.\n\n    **References**\n\n    [1] `MCMC Using Hamiltonian Dynamics`,\n    Radford M. Neal\n\n    :param model: Python callable containing Pyro primitives.\n    :param potential_fn: Python callable calculating potential energy with input\n        is a dict of real support parameters.\n    :param float step_size: Determines the size of a single step taken by the\n        verlet integrator while computing the trajectory using Hamiltonian\n        dynamics. If not specified, it will be set to 1.\n    :param float trajectory_length: Length of a MCMC trajectory. If not\n        specified, it will be set to ``step_size x num_steps``. In case\n        ``num_steps`` is not specified, it will be set to :math:`2\\pi`.\n    :param int num_steps: The number of discrete steps over which to simulate\n        Hamiltonian dynamics. The state at the end of the trajectory is\n        returned as the proposal. This value is always equal to\n        ``int(trajectory_length / step_size)``.\n    :param bool adapt_step_size: A flag to decide if we want to adapt step_size\n        during warm-up phase using Dual Averaging scheme.\n    :param bool adapt_mass_matrix: A flag to decide if we want to adapt mass\n        matrix during warm-up phase using Welford scheme.\n    :param bool full_mass: A flag to decide if mass matrix is dense or diagonal.\n    :param dict transforms: Optional dictionary that specifies a transform\n        for a sample site with constrained support to unconstrained space. The\n        transform should be invertible, and implement `log_abs_det_jacobian`.\n        If not specified and the model has sites with constrained support,\n        automatic transformations will be applied, as specified in\n        :mod:`torch.distributions.constraint_registry`.\n    :param int max_plate_nesting: Optional bound on max number of nested\n        :func:`pyro.plate` contexts. This is required if model contains\n        discrete sample sites that can be enumerated over in parallel.\n    :param bool jit_compile: Optional parameter denoting whether to use\n        the PyTorch JIT to trace the log density computation, and use this\n        optimized executable trace in the integrator.\n    :param dict jit_options: A dictionary contains optional arguments for\n        :func:`torch.jit.trace` function.\n    :param bool ignore_jit_warnings: Flag to ignore warnings from the JIT\n        tracer when ``jit_compile=True``. Default is False.\n    :param float target_accept_prob: Increasing this value will lead to a smaller\n        step size, hence the sampling will be slower and more robust. Default to 0.8.\n    :param callable init_strategy: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n\n    .. note:: Internally, the mass matrix will be ordered according to the order\n        of the names of latent variables, not the order of their appearance in\n        the model.\n\n    Example:\n\n        >>> true_coefs = torch.tensor([1., 2., 3.])\n        >>> data = torch.randn(2000, 3)\n        >>> dim = 3\n        >>> labels = dist.Bernoulli(logits=(true_coefs * data).sum(-1)).sample()\n        >>>\n        >>> def model(data):\n        ...     coefs_mean = torch.zeros(dim)\n        ...     coefs = pyro.sample(\'beta\', dist.Normal(coefs_mean, torch.ones(3)))\n        ...     y = pyro.sample(\'y\', dist.Bernoulli(logits=(coefs * data).sum(-1)), obs=labels)\n        ...     return y\n        >>>\n        >>> hmc_kernel = HMC(model, step_size=0.0855, num_steps=4)\n        >>> mcmc = MCMC(hmc_kernel, num_samples=500, warmup_steps=100)\n        >>> mcmc.run(data)\n        >>> mcmc.get_samples()[\'beta\'].mean(0)  # doctest: +SKIP\n        tensor([ 0.9819,  1.9258,  2.9737])\n    """"""\n\n    def __init__(self,\n                 model=None,\n                 potential_fn=None,\n                 step_size=1,\n                 trajectory_length=None,\n                 num_steps=None,\n                 adapt_step_size=True,\n                 adapt_mass_matrix=True,\n                 full_mass=False,\n                 transforms=None,\n                 max_plate_nesting=None,\n                 jit_compile=False,\n                 jit_options=None,\n                 ignore_jit_warnings=False,\n                 target_accept_prob=0.8,\n                 init_strategy=init_to_uniform):\n        if not ((model is None) ^ (potential_fn is None)):\n            raise ValueError(""Only one of `model` or `potential_fn` must be specified."")\n        # NB: deprecating args - model, transforms\n        self.model = model\n        self.transforms = transforms\n        self._max_plate_nesting = max_plate_nesting\n        self._jit_compile = jit_compile\n        self._jit_options = jit_options\n        self._ignore_jit_warnings = ignore_jit_warnings\n        self._init_strategy = init_strategy\n\n        self.potential_fn = potential_fn\n        if trajectory_length is not None:\n            self.trajectory_length = trajectory_length\n        elif num_steps is not None:\n            self.trajectory_length = step_size * num_steps\n        else:\n            self.trajectory_length = 2 * math.pi  # from Stan\n        # The following parameter is used in find_reasonable_step_size method.\n        # In NUTS paper, this threshold is set to a fixed log(0.5).\n        # After https://github.com/stan-dev/stan/pull/356, it is set to a fixed log(0.8).\n        self._direction_threshold = math.log(0.8)  # from Stan\n        self._max_sliced_energy = 1000\n        self._reset()\n        self._adapter = WarmupAdapter(step_size,\n                                      adapt_step_size=adapt_step_size,\n                                      adapt_mass_matrix=adapt_mass_matrix,\n                                      target_accept_prob=target_accept_prob,\n                                      dense_mass=full_mass)\n        super().__init__()\n\n    def _kinetic_energy(self, r_unscaled):\n        energy = 0.\n        for site_names, value in r_unscaled.items():\n            energy = energy + value.dot(value)\n        return 0.5 * energy\n\n    def _reset(self):\n        self._t = 0\n        self._accept_cnt = 0\n        self._mean_accept_prob = 0.\n        self._divergences = []\n        self._prototype_trace = None\n        self._initial_params = None\n        self._z_last = None\n        self._potential_energy_last = None\n        self._z_grads_last = None\n        self._warmup_steps = None\n\n    def _find_reasonable_step_size(self, z):\n        step_size = self.step_size\n\n        # We are going to find a step_size which make accept_prob (Metropolis correction)\n        # near the target_accept_prob. If accept_prob:=exp(-delta_energy) is small,\n        # then we have to decrease step_size; otherwise, increase step_size.\n        potential_energy = self.potential_fn(z)\n        r, r_unscaled = self._sample_r(name=""r_presample_0"")\n        energy_current = self._kinetic_energy(r_unscaled) + potential_energy\n        # This is required so as to avoid issues with autograd when model\n        # contains transforms with cache_size > 0 (https://github.com/pyro-ppl/pyro/issues/2292)\n        z = {k: v.clone() for k, v in z.items()}\n        z_new, r_new, z_grads_new, potential_energy_new = velocity_verlet(\n            z, r, self.potential_fn, self.mass_matrix_adapter.kinetic_grad, step_size)\n        r_new_unscaled = self.mass_matrix_adapter.unscale(r_new)\n        energy_new = self._kinetic_energy(r_new_unscaled) + potential_energy_new\n        delta_energy = energy_new - energy_current\n        # direction=1 means keep increasing step_size, otherwise decreasing step_size.\n        # Note that the direction is -1 if delta_energy is `NaN` which may be the\n        # case for a diverging trajectory (e.g. in the case of evaluating log prob\n        # of a value simulated using a large step size for a constrained sample site).\n        direction = 1 if self._direction_threshold < -delta_energy else -1\n\n        # define scale for step_size: 2 for increasing, 1/2 for decreasing\n        step_size_scale = 2 ** direction\n        direction_new = direction\n        # keep scale step_size until accept_prob crosses its target\n        # TODO: make thresholds for too small step_size or too large step_size\n        t = 0\n        while direction_new == direction:\n            t += 1\n            step_size = step_size_scale * step_size\n            r, r_unscaled = self._sample_r(name=""r_presample_{}"".format(t))\n            energy_current = self._kinetic_energy(r_unscaled) + potential_energy\n            z_new, r_new, z_grads_new, potential_energy_new = velocity_verlet(\n                z, r, self.potential_fn, self.mass_matrix_adapter.kinetic_grad, step_size)\n            r_new_unscaled = self.mass_matrix_adapter.unscale(r_new)\n            energy_new = self._kinetic_energy(r_new_unscaled) + potential_energy_new\n            delta_energy = energy_new - energy_current\n            direction_new = 1 if self._direction_threshold < -delta_energy else -1\n        return step_size\n\n    def _sample_r(self, name):\n        r_unscaled = {}\n        options = {""dtype"": self._potential_energy_last.dtype,\n                   ""device"": self._potential_energy_last.device}\n        for site_names, size in self.mass_matrix_adapter.mass_matrix_size.items():\n            # we want to sample from Normal distribution using `sample` method rather than\n            # `rsample` method because the former is a bit faster\n            r_unscaled[site_names] = pyro.sample(\n                ""{}_{}"".format(name, site_names),\n                NonreparameterizedNormal(torch.zeros(size, **options), torch.ones(size, **options)))\n\n        r = self.mass_matrix_adapter.scale(r_unscaled, r_prototype=self.initial_params)\n        return r, r_unscaled\n\n    @property\n    def mass_matrix_adapter(self):\n        return self._adapter.mass_matrix_adapter\n\n    @mass_matrix_adapter.setter\n    def mass_matrix_adapter(self, value):\n        self._adapter.mass_matrix_adapter = value\n\n    @property\n    def inverse_mass_matrix(self):\n        return self.mass_matrix_adapter.inverse_mass_matrix\n\n    @property\n    def step_size(self):\n        return self._adapter.step_size\n\n    @property\n    def num_steps(self):\n        return max(1, int(self.trajectory_length / self.step_size))\n\n    @property\n    def initial_params(self):\n        return self._initial_params\n\n    @initial_params.setter\n    def initial_params(self, params):\n        self._initial_params = params\n\n    def _initialize_model_properties(self, model_args, model_kwargs):\n        init_params, potential_fn, transforms, trace = initialize_model(\n            self.model,\n            model_args,\n            model_kwargs,\n            transforms=self.transforms,\n            max_plate_nesting=self._max_plate_nesting,\n            jit_compile=self._jit_compile,\n            jit_options=self._jit_options,\n            skip_jit_warnings=self._ignore_jit_warnings,\n            init_strategy=self._init_strategy,\n            initial_params=self._initial_params,\n        )\n        self.potential_fn = potential_fn\n        self.transforms = transforms\n        self._initial_params = init_params\n        self._prototype_trace = trace\n\n    def _initialize_adapter(self):\n        if self._adapter.dense_mass is False:\n            dense_sites_list = []\n        elif self._adapter.dense_mass is True:\n            dense_sites_list = [tuple(sorted(self.initial_params))]\n        else:\n            msg = ""full_mass should be a list of tuples of site names.""\n            dense_sites_list = self._adapter.dense_mass\n            assert isinstance(dense_sites_list, list), msg\n            for dense_sites in dense_sites_list:\n                assert dense_sites and isinstance(dense_sites, tuple), msg\n                for name in dense_sites:\n                    assert isinstance(name, str) and name in self.initial_params, msg\n        dense_sites_set = set().union(*dense_sites_list)\n        diag_sites = tuple(sorted([name for name in self.initial_params\n                                   if name not in dense_sites_set]))\n        assert len(diag_sites) + sum([len(sites) for sites in dense_sites_list]) == len(self.initial_params), \\\n            ""Site names specified in full_mass are duplicated.""\n\n        mass_matrix_shape = OrderedDict()\n        for dense_sites in dense_sites_list:\n            size = sum([self.initial_params[site].numel() for site in dense_sites])\n            mass_matrix_shape[dense_sites] = (size, size)\n\n        if diag_sites:\n            size = sum([self.initial_params[site].numel() for site in diag_sites])\n            mass_matrix_shape[diag_sites] = (size,)\n\n        options = {""dtype"": self._potential_energy_last.dtype,\n                   ""device"": self._potential_energy_last.device}\n        self._adapter.configure(self._warmup_steps,\n                                mass_matrix_shape=mass_matrix_shape,\n                                find_reasonable_step_size_fn=self._find_reasonable_step_size,\n                                options=options)\n\n        if self._adapter.adapt_step_size:\n            self._adapter.reset_step_size_adaptation(self._initial_params)\n\n    def setup(self, warmup_steps, *args, **kwargs):\n        self._warmup_steps = warmup_steps\n        if self.model is not None:\n            self._initialize_model_properties(args, kwargs)\n        if self.initial_params:\n            z = {k: v.detach() for k, v in self.initial_params.items()}\n            z_grads, potential_energy = potential_grad(self.potential_fn, z)\n        else:\n            z_grads, potential_energy = {}, self.potential_fn(self.initial_params)\n        self._cache(self.initial_params, potential_energy, z_grads)\n        if self.initial_params:\n            self._initialize_adapter()\n\n    def cleanup(self):\n        self._reset()\n\n    def _cache(self, z, potential_energy, z_grads=None):\n        self._z_last = z\n        self._potential_energy_last = potential_energy\n        self._z_grads_last = z_grads\n\n    def clear_cache(self):\n        self._z_last = None\n        self._potential_energy_last = None\n        self._z_grads_last = None\n\n    def _fetch_from_cache(self):\n        return self._z_last, self._potential_energy_last, self._z_grads_last\n\n    def sample(self, params):\n        z, potential_energy, z_grads = self._fetch_from_cache()\n        # recompute PE when cache is cleared\n        if z is None:\n            z = params\n            z_grads, potential_energy = potential_grad(self.potential_fn, z)\n            self._cache(z, potential_energy, z_grads)\n        # return early if no sample sites\n        elif len(z) == 0:\n            self._t += 1\n            self._mean_accept_prob = 1.\n            if self._t > self._warmup_steps:\n                self._accept_cnt += 1\n            return params\n        r, r_unscaled = self._sample_r(name=""r_t={}"".format(self._t))\n        energy_current = self._kinetic_energy(r_unscaled) + potential_energy\n\n        # Temporarily disable distributions args checking as\n        # NaNs are expected during step size adaptation\n        with optional(pyro.validation_enabled(False), self._t < self._warmup_steps):\n            z_new, r_new, z_grads_new, potential_energy_new = velocity_verlet(\n                z, r, self.potential_fn, self.mass_matrix_adapter.kinetic_grad,\n                self.step_size, self.num_steps, z_grads=z_grads)\n            # apply Metropolis correction.\n            r_new_unscaled = self.mass_matrix_adapter.unscale(r_new)\n            energy_proposal = self._kinetic_energy(r_new_unscaled) + potential_energy_new\n        delta_energy = energy_proposal - energy_current\n        # handle the NaN case which may be the case for a diverging trajectory\n        # when using a large step size.\n        delta_energy = scalar_like(delta_energy, float(""inf"")) if torch_isnan(delta_energy) else delta_energy\n        if delta_energy > self._max_sliced_energy and self._t >= self._warmup_steps:\n            self._divergences.append(self._t - self._warmup_steps)\n\n        accept_prob = (-delta_energy).exp().clamp(max=1.)\n        rand = pyro.sample(""rand_t={}"".format(self._t), dist.Uniform(scalar_like(accept_prob, 0.),\n                                                                     scalar_like(accept_prob, 1.)))\n        accepted = False\n        if rand < accept_prob:\n            accepted = True\n            z = z_new\n            z_grads = z_grads_new\n            self._cache(z, potential_energy_new, z_grads)\n\n        self._t += 1\n        if self._t > self._warmup_steps:\n            n = self._t - self._warmup_steps\n            if accepted:\n                self._accept_cnt += 1\n        else:\n            n = self._t\n            self._adapter.step(self._t, z, accept_prob, z_grads)\n\n        self._mean_accept_prob += (accept_prob.item() - self._mean_accept_prob) / n\n        return z.copy()\n\n    def logging(self):\n        return OrderedDict([\n            (""step size"", ""{:.2e}"".format(self.step_size)),\n            (""acc. prob"", ""{:.3f}"".format(self._mean_accept_prob))\n        ])\n\n    def diagnostics(self):\n        return {""divergences"": self._divergences,\n                ""acceptance rate"": self._accept_cnt / (self._t - self._warmup_steps)}\n'"
pyro/infer/mcmc/logger.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport json\nimport logging\nimport os\nimport sys\nfrom collections import OrderedDict\n\nfrom tqdm import tqdm\nfrom tqdm.auto import tqdm as tqdm_nb\n\ntry:\n    get_ipython\n    ipython_env = True\nexcept NameError:\n    ipython_env = False\n\n# Identifiers to distinguish between diagnostic messages for progress bars\n# vs. logging output. Useful when using QueueHandler in multiprocessing.\nLOG_MSG = ""LOG""\nDIAGNOSTIC_MSG = ""DIAGNOSTICS""\n\n\n# Following compatibility code is for Python 2 (available in Python 3.2+).\n# Source: https://github.com/python/cpython/blob/master/Lib/logging/handlers.py\n#\n# Copyright 2001-2016 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\nclass ProgressBar:\n    """"""\n    Initialize progress bars using :class:`~tqdm.tqdm`.\n\n    :param int warmup_steps: Number of warmup steps.\n    :param int num_samples: Number of MCMC samples.\n    :param int min_width: Minimum column width of the bar.\n    :param int max_width: Maximum column width of the bar.\n    :param bool disable: Disable progress bar.\n    :param int num_bars: Number of progress bars to initialize.\n        If multiple bars are initialized, they need to be separately\n        updated via the ``pos`` kwarg.\n    """"""\n    def __init__(self, warmup_steps, num_samples, min_width=80, max_width=120,\n                 disable=False, num_bars=1):\n        total_steps = warmup_steps + num_samples\n        # Disable progress bar in ""CI""\n        # (see https://github.com/travis-ci/travis-ci/issues/1337).\n        disable = disable or ""CI"" in os.environ or ""PYTEST_XDIST_WORKER"" in os.environ\n        bar_format = ""{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}, {rate_fmt}{postfix}]""\n        pbar_cls = tqdm_nb if num_bars > 1 and ipython_env else tqdm\n        self.progress_bars = []\n        for i in range(num_bars):\n            description = ""Warmup"" if num_bars == 1 else ""Warmup [{}]"".format(i + 1)\n            pbar = pbar_cls(total=total_steps, desc=description, bar_format=bar_format,\n                            position=i, file=sys.stderr, disable=disable)\n            # Assume reasonable values when terminal width not available\n            if getattr(pbar, ""ncols"", None) is not None:\n                pbar.ncols = max(min_width, pbar.ncols)\n                pbar.ncols = min(max_width, pbar.ncols)\n            self.progress_bars.append(pbar)\n        self.disable = disable\n        self.ipython_env = ipython_env\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc):\n        self.close()\n        return False\n\n    def set_description(self, *args, **kwargs):\n        pos = kwargs.pop(""pos"", 0)\n        if not self.disable:\n            self.progress_bars[pos].set_description(*args, **kwargs)\n\n    def set_postfix(self, *args, **kwargs):\n        pos = kwargs.pop(""pos"", 0)\n        if not self.disable:\n            self.progress_bars[pos].set_postfix(*args, **kwargs)\n\n    def update(self, *args, **kwargs):\n        pos = kwargs.pop(""pos"", 0)\n        if not self.disable:\n            self.progress_bars[pos].update(*args, **kwargs)\n\n    def close(self):\n        for pbar in self.progress_bars:\n            pbar.close()\n        # Required to not overwrite multiple progress bars on exit.\n        if not self.ipython_env and not self.disable:\n            sys.stderr.write(""\\n"" * len(self.progress_bars))\n\n\nclass QueueHandler(logging.Handler):\n    """"""\n    This handler sends events to a queue. Typically, it would be used together\n    with a multiprocessing Queue to centralise logging to file in one process\n    (in a multi-process application), so as to avoid file write contention\n    between processes.\n\n    This code is new in Python 3.2, but this class can be copy pasted into\n    user code for use with earlier Python versions.\n    """"""\n\n    def __init__(self, queue):\n        """"""\n        Initialise an instance, using the passed queue.\n        """"""\n        logging.Handler.__init__(self)\n        self.queue = queue\n\n    def enqueue(self, record):\n        """"""\n        Enqueue a record.\n\n        The base implementation uses put_nowait. You may want to override\n        this method if you want to use blocking, timeouts or custom queue\n        implementations.\n        """"""\n        self.queue.put_nowait(record)\n\n    def prepare(self, record):\n        """"""\n        Prepares a record for queuing. The object returned by this method is\n        enqueued.\n\n        The base implementation formats the record to merge the message\n        and arguments, and removes unpickleable items from the record\n        in-place.\n\n        You might want to override this method if you want to convert\n        the record to a dict or JSON string, or send a modified copy\n        of the record while leaving the original intact.\n        """"""\n        record.msg = self.format(record)\n        record.args = None\n        record.exc_info = None\n        return record\n\n    def emit(self, record):\n        """"""\n        Emit a record.\n\n        Writes the LogRecord to the queue, preparing it for pickling first.\n        """"""\n        try:\n            self.enqueue(self.prepare(record))\n        except Exception:\n            self.handleError(record)\n\n\nclass TqdmHandler(logging.StreamHandler):\n    """"""\n    Handler that synchronizes the log output with the\n    :class:`~tqdm.tqdm` progress bar.\n    """"""\n    def emit(self, record):\n        try:\n            msg = self.format(record)\n            self.flush()\n            tqdm.write(msg, file=sys.stderr)\n        except (KeyboardInterrupt, SystemExit) as e:\n            raise e\n        except Exception:\n            self.handleError(record)\n\n\nclass MCMCLoggingHandler(logging.Handler):\n    """"""\n    Main logging handler used by :class:`~pyro.infer.mcmc`,\n    to handle both progress bar updates and regular `logging`\n    messages.\n\n    :param log_handler: default log handler for logging\n        output.\n    :param progress_bar: If provided, diagnostic information\n        is updated using the bar.\n    """"""\n    def __init__(self, log_handler, progress_bar=None):\n        logging.Handler.__init__(self)\n        self.log_handler = log_handler\n        self.progress_bar = progress_bar\n\n    def emit(self, record):\n        try:\n            if self.progress_bar and record.msg_type == DIAGNOSTIC_MSG:\n                diagnostics = json.loads(record.getMessage(),\n                                         object_pairs_hook=OrderedDict)\n                self.progress_bar.set_postfix(diagnostics, refresh=False)\n                self.progress_bar.update()\n            else:\n                self.log_handler.handle(record)\n        except (KeyboardInterrupt, SystemExit) as e:\n            raise e\n        except Exception:\n            self.handleError(record)\n\n\nclass MetadataFilter(logging.Filter):\n    """"""\n    Adds auxiliary information to log records, like `logger_id` and\n    `msg_type`.\n    """"""\n    def __init__(self, logger_id):\n        self.logger_id = logger_id\n        super().__init__()\n\n    def filter(self, record):\n        record.logger_id = self.logger_id\n        if not getattr(record, ""msg_type"", None):\n            record.msg_type = LOG_MSG\n        return True\n\n\ndef initialize_logger(logger, logger_id, progress_bar=None, log_queue=None):\n    """"""\n    Initialize logger for the :class:`pyro.infer.mcmc` module.\n\n    :param logger: logger instance.\n    :param str logger_id: identifier for the log record,\n        e.g. chain id in case of multiple samplers.\n    :param progress_bar: a :class:`tqdm.tqdm` instance.\n    """"""\n    # Reset handler with new `progress_bar`.\n    logger.handlers = []\n    logger.propagate = False\n    if log_queue:\n        handler = QueueHandler(log_queue)\n        format = ""[%(levelname)s %(msg_type)s %(logger_id)s]%(message)s""\n        progress_bar = None\n    elif progress_bar:\n        format = ""%(levelname).1s \\t %(message)s""\n        handler = TqdmHandler()\n    else:\n        raise ValueError(""Logger cannot be initialized without a ""\n                         ""valid handler."")\n    handler.setFormatter(logging.Formatter(format))\n    logging_handler = MCMCLoggingHandler(handler, progress_bar)\n    logging_handler.addFilter(MetadataFilter(logger_id))\n    logger.addHandler(logging_handler)\n    return logger\n'"
pyro/infer/mcmc/mcmc_kernel.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABCMeta, abstractmethod\n\n\nclass MCMCKernel(object, metaclass=ABCMeta):\n\n    def setup(self, warmup_steps, *args, **kwargs):\n        r""""""\n        Optional method to set up any state required at the start of the\n        simulation run.\n\n        :param int warmup_steps: Number of warmup iterations.\n        :param \\*args: Algorithm specific positional arguments.\n        :param \\*\\*kwargs: Algorithm specific keyword arguments.\n        """"""\n        pass\n\n    def cleanup(self):\n        """"""\n        Optional method to clean up any residual state on termination.\n        """"""\n        pass\n\n    def logging(self):\n        """"""\n        Relevant logging information to be printed at regular intervals\n        of the MCMC run. Returns `None` by default.\n\n        :return: String containing the diagnostic summary. e.g. acceptance rate\n        :rtype: string\n        """"""\n        return None\n\n    def diagnostics(self):\n        """"""\n        Returns a dict of useful diagnostics after finishing sampling process.\n        """"""\n        # NB: should be not None for multiprocessing works\n        return {}\n\n    def end_warmup(self):\n        """"""\n        Optional method to tell kernel that warm-up phase has been finished.\n        """"""\n        pass\n\n    @property\n    def initial_params(self):\n        """"""\n        Returns a dict of initial params (by default, from the prior) to initiate the MCMC run.\n\n        :return: dict of parameter values keyed by their name.\n        """"""\n        raise NotImplementedError\n\n    @initial_params.setter\n    def initial_params(self, params):\n        """"""\n        Sets the parameters to initiate the MCMC run. Note that the parameters must\n        have unconstrained support.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def sample(self, params):\n        """"""\n        Samples parameters from the posterior distribution, when given existing parameters.\n\n        :param dict params: Current parameter values.\n        :param int time_step: Current time step.\n        :return: New parameters from the posterior distribution.\n        """"""\n        raise NotImplementedError\n\n    def __call__(self, params):\n        """"""\n        Alias for MCMCKernel.sample() method.\n        """"""\n        return self.sample(params)\n'"
pyro/infer/mcmc/nuts.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions.util import scalar_like\nfrom pyro.infer.autoguide import init_to_uniform\nfrom pyro.infer.mcmc.hmc import HMC\nfrom pyro.ops.integrator import potential_grad, velocity_verlet\nfrom pyro.util import optional, torch_isnan\n\n\ndef _logaddexp(x, y):\n    minval, maxval = (x, y) if x < y else (y, x)\n    return (minval - maxval).exp().log1p() + maxval\n\n\n# sum_accept_probs and num_proposals are used to calculate\n# the statistic accept_prob for Dual Averaging scheme;\n# z_left_grads and z_right_grads are kept to avoid recalculating\n# grads at left and right leaves;\n# r_sum is used to check turning condition;\n# z_proposal_pe and z_proposal_grads are used to cache the\n#   potential energy and potential energy gradient values for\n#   the proposal trace.\n# weight is the number of valid points in case we use slice sampling\n#   and is the log sum of (unnormalized) probabilites of valid points\n#   when we use multinomial sampling\n_TreeInfo = namedtuple(""TreeInfo"", [""z_left"", ""r_left"", ""r_left_unscaled"", ""z_left_grads"",\n                                    ""z_right"", ""r_right"", ""r_right_unscaled"", ""z_right_grads"",\n                                    ""z_proposal"", ""z_proposal_pe"", ""z_proposal_grads"",\n                                    ""r_sum"", ""weight"", ""turning"", ""diverging"",\n                                    ""sum_accept_probs"", ""num_proposals""])\n\n\nclass NUTS(HMC):\n    """"""\n    No-U-Turn Sampler kernel, which provides an efficient and convenient way\n    to run Hamiltonian Monte Carlo. The number of steps taken by the\n    integrator is dynamically adjusted on each call to ``sample`` to ensure\n    an optimal length for the Hamiltonian trajectory [1]. As such, the samples\n    generated will typically have lower autocorrelation than those generated\n    by the :class:`~pyro.infer.mcmc.HMC` kernel. Optionally, the NUTS kernel\n    also provides the ability to adapt step size during the warmup phase.\n\n    Refer to the `baseball example <https://github.com/pyro-ppl/pyro/blob/dev/examples/baseball.py>`_\n    to see how to do Bayesian inference in Pyro using NUTS.\n\n    **References**\n\n    [1] `The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo`,\n        Matthew D. Hoffman, and Andrew Gelman.\n    [2] `A Conceptual Introduction to Hamiltonian Monte Carlo`,\n        Michael Betancourt\n    [3] `Slice Sampling`,\n        Radford M. Neal\n\n    :param model: Python callable containing Pyro primitives.\n    :param potential_fn: Python callable calculating potential energy with input\n        is a dict of real support parameters.\n    :param float step_size: Determines the size of a single step taken by the\n        verlet integrator while computing the trajectory using Hamiltonian\n        dynamics. If not specified, it will be set to 1.\n    :param bool adapt_step_size: A flag to decide if we want to adapt step_size\n        during warm-up phase using Dual Averaging scheme.\n    :param bool adapt_mass_matrix: A flag to decide if we want to adapt mass\n        matrix during warm-up phase using Welford scheme.\n    :param bool full_mass: A flag to decide if mass matrix is dense or diagonal.\n    :param bool use_multinomial_sampling: A flag to decide if we want to sample\n        candidates along its trajectory using ""multinomial sampling"" or using\n        ""slice sampling"". Slice sampling is used in the original NUTS paper [1],\n        while multinomial sampling is suggested in [2]. By default, this flag is\n        set to True. If it is set to `False`, NUTS uses slice sampling.\n    :param dict transforms: Optional dictionary that specifies a transform\n        for a sample site with constrained support to unconstrained space. The\n        transform should be invertible, and implement `log_abs_det_jacobian`.\n        If not specified and the model has sites with constrained support,\n        automatic transformations will be applied, as specified in\n        :mod:`torch.distributions.constraint_registry`.\n    :param int max_plate_nesting: Optional bound on max number of nested\n        :func:`pyro.plate` contexts. This is required if model contains\n        discrete sample sites that can be enumerated over in parallel.\n    :param bool jit_compile: Optional parameter denoting whether to use\n        the PyTorch JIT to trace the log density computation, and use this\n        optimized executable trace in the integrator.\n    :param dict jit_options: A dictionary contains optional arguments for\n        :func:`torch.jit.trace` function.\n    :param bool ignore_jit_warnings: Flag to ignore warnings from the JIT\n        tracer when ``jit_compile=True``. Default is False.\n    :param float target_accept_prob: Target acceptance probability of step size\n        adaptation scheme. Increasing this value will lead to a smaller step size,\n        so the sampling will be slower but more robust. Default to 0.8.\n    :param int max_tree_depth: Max depth of the binary tree created during the doubling\n        scheme of NUTS sampler. Default to 10.\n    :param callable init_strategy: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n\n    Example:\n\n        >>> true_coefs = torch.tensor([1., 2., 3.])\n        >>> data = torch.randn(2000, 3)\n        >>> dim = 3\n        >>> labels = dist.Bernoulli(logits=(true_coefs * data).sum(-1)).sample()\n        >>>\n        >>> def model(data):\n        ...     coefs_mean = torch.zeros(dim)\n        ...     coefs = pyro.sample(\'beta\', dist.Normal(coefs_mean, torch.ones(3)))\n        ...     y = pyro.sample(\'y\', dist.Bernoulli(logits=(coefs * data).sum(-1)), obs=labels)\n        ...     return y\n        >>>\n        >>> nuts_kernel = NUTS(model, adapt_step_size=True)\n        >>> mcmc = MCMC(nuts_kernel, num_samples=500, warmup_steps=300)\n        >>> mcmc.run(data)\n        >>> mcmc.get_samples()[\'beta\'].mean(0)  # doctest: +SKIP\n        tensor([ 0.9221,  1.9464,  2.9228])\n    """"""\n\n    def __init__(self,\n                 model=None,\n                 potential_fn=None,\n                 step_size=1,\n                 adapt_step_size=True,\n                 adapt_mass_matrix=True,\n                 full_mass=False,\n                 use_multinomial_sampling=True,\n                 transforms=None,\n                 max_plate_nesting=None,\n                 jit_compile=False,\n                 jit_options=None,\n                 ignore_jit_warnings=False,\n                 target_accept_prob=0.8,\n                 max_tree_depth=10,\n                 init_strategy=init_to_uniform):\n        super().__init__(model,\n                         potential_fn,\n                         step_size,\n                         adapt_step_size=adapt_step_size,\n                         adapt_mass_matrix=adapt_mass_matrix,\n                         full_mass=full_mass,\n                         transforms=transforms,\n                         max_plate_nesting=max_plate_nesting,\n                         jit_compile=jit_compile,\n                         jit_options=jit_options,\n                         ignore_jit_warnings=ignore_jit_warnings,\n                         target_accept_prob=target_accept_prob,\n                         init_strategy=init_strategy)\n        self.use_multinomial_sampling = use_multinomial_sampling\n        self._max_tree_depth = max_tree_depth\n        # There are three conditions to stop doubling process:\n        #     + Tree is becoming too big.\n        #     + The trajectory is making a U-turn.\n        #     + The probability of the states becoming negligible: p(z, r) << u,\n        # here u is the ""slice"" variable introduced at the `self.sample(...)` method.\n        # Denote E_p = -log p(z, r), E_u = -log u, the third condition is equivalent to\n        #     sliced_energy := E_p - E_u > some constant =: max_sliced_energy.\n        # This also suggests the notion ""diverging"" in the implemenation:\n        #     when the energy E_p diverges from E_u too much, we stop doubling.\n        # Here, as suggested in [1], we set dE_max = 1000.\n        self._max_sliced_energy = 1000\n\n    def _is_turning(self, r_left_unscaled, r_right_unscaled, r_sum):\n        # We follow the strategy in Section A.4.2 of [2] for this implementation.\n        left_angle = 0.\n        right_angle = 0.\n        for site_names, value in r_sum.items():\n            rho = value - (r_left_unscaled[site_names] + r_right_unscaled[site_names]) / 2\n            left_angle += r_left_unscaled[site_names].dot(rho)\n            right_angle += r_right_unscaled[site_names].dot(rho)\n\n        return (left_angle <= 0) or (right_angle <= 0)\n\n    def _build_basetree(self, z, r, z_grads, log_slice, direction, energy_current):\n        step_size = self.step_size if direction == 1 else -self.step_size\n        z_new, r_new, z_grads, potential_energy = velocity_verlet(\n            z, r, self.potential_fn, self.mass_matrix_adapter.kinetic_grad, step_size, z_grads=z_grads)\n        r_new_unscaled = self.mass_matrix_adapter.unscale(r_new)\n        energy_new = potential_energy + self._kinetic_energy(r_new_unscaled)\n        # handle the NaN case\n        energy_new = scalar_like(energy_new, float(""inf"")) if torch_isnan(energy_new) else energy_new\n        sliced_energy = energy_new + log_slice\n        diverging = (sliced_energy > self._max_sliced_energy)\n        delta_energy = energy_new - energy_current\n        accept_prob = (-delta_energy).exp().clamp(max=1.0)\n\n        if self.use_multinomial_sampling:\n            tree_weight = -sliced_energy\n        else:\n            # As a part of the slice sampling process (see below), along the trajectory\n            #   we eliminate states which p(z, r) < u, or dE > 0.\n            # Due to this elimination (and stop doubling conditions),\n            #   the weight of binary tree might not equal to 2^tree_depth.\n            tree_weight = scalar_like(sliced_energy, 1. if sliced_energy <= 0 else 0.)\n\n        r_sum = r_new_unscaled\n        return _TreeInfo(z_new, r_new, r_new_unscaled, z_grads, z_new, r_new, r_new_unscaled, z_grads,\n                         z_new, potential_energy, z_grads, r_sum, tree_weight, False, diverging, accept_prob, 1)\n\n    def _build_tree(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current):\n        if tree_depth == 0:\n            return self._build_basetree(z, r, z_grads, log_slice, direction, energy_current)\n\n        # build the first half of tree\n        half_tree = self._build_tree(z, r, z_grads, log_slice,\n                                     direction, tree_depth-1, energy_current)\n        z_proposal = half_tree.z_proposal\n        z_proposal_pe = half_tree.z_proposal_pe\n        z_proposal_grads = half_tree.z_proposal_grads\n\n        # Check conditions to stop doubling. If we meet that condition,\n        #     there is no need to build the other tree.\n        if half_tree.turning or half_tree.diverging:\n            return half_tree\n\n        # Else, build remaining half of tree.\n        # If we are going to the right, start from the right leaf of the first half.\n        if direction == 1:\n            z = half_tree.z_right\n            r = half_tree.r_right\n            z_grads = half_tree.z_right_grads\n        else:  # otherwise, start from the left leaf of the first half\n            z = half_tree.z_left\n            r = half_tree.r_left\n            z_grads = half_tree.z_left_grads\n        other_half_tree = self._build_tree(z, r, z_grads, log_slice,\n                                           direction, tree_depth-1, energy_current)\n\n        if self.use_multinomial_sampling:\n            tree_weight = _logaddexp(half_tree.weight, other_half_tree.weight)\n        else:\n            tree_weight = half_tree.weight + other_half_tree.weight\n        sum_accept_probs = half_tree.sum_accept_probs + other_half_tree.sum_accept_probs\n        num_proposals = half_tree.num_proposals + other_half_tree.num_proposals\n        r_sum = {site_names: half_tree.r_sum[site_names] + other_half_tree.r_sum[site_names]\n                 for site_names in self.inverse_mass_matrix}\n\n        # The probability of that proposal belongs to which half of tree\n        #     is computed based on the weights of each half.\n        if self.use_multinomial_sampling:\n            other_half_tree_prob = (other_half_tree.weight - tree_weight).exp()\n        else:\n            # For the special case that the weights of each half are both 0,\n            #   we choose the proposal from the first half\n            #   (any is fine, because the probability of picking it at the end is 0!).\n            other_half_tree_prob = (other_half_tree.weight / tree_weight if tree_weight > 0\n                                    else scalar_like(tree_weight, 0.))\n        is_other_half_tree = pyro.sample(""is_other_half_tree"",\n                                         dist.Bernoulli(probs=other_half_tree_prob))\n\n        if is_other_half_tree == 1:\n            z_proposal = other_half_tree.z_proposal\n            z_proposal_pe = other_half_tree.z_proposal_pe\n            z_proposal_grads = other_half_tree.z_proposal_grads\n\n        # leaves of the full tree are determined by the direction\n        if direction == 1:\n            z_left = half_tree.z_left\n            r_left = half_tree.r_left\n            r_left_unscaled = half_tree.r_left_unscaled\n            z_left_grads = half_tree.z_left_grads\n            z_right = other_half_tree.z_right\n            r_right = other_half_tree.r_right\n            r_right_unscaled = other_half_tree.r_right_unscaled\n            z_right_grads = other_half_tree.z_right_grads\n        else:\n            z_left = other_half_tree.z_left\n            r_left = other_half_tree.r_left\n            r_left_unscaled = other_half_tree.r_left_unscaled\n            z_left_grads = other_half_tree.z_left_grads\n            z_right = half_tree.z_right\n            r_right = half_tree.r_right\n            r_right_unscaled = half_tree.r_right_unscaled\n            z_right_grads = half_tree.z_right_grads\n\n        # We already check if first half tree is turning. Now, we check\n        #     if the other half tree or full tree are turning.\n        turning = other_half_tree.turning or self._is_turning(r_left_unscaled, r_right_unscaled, r_sum)\n\n        # The divergence is checked by the second half tree (the first half is already checked).\n        diverging = other_half_tree.diverging\n\n        return _TreeInfo(z_left, r_left, r_left_unscaled, z_left_grads, z_right, r_right, r_right_unscaled,\n                         z_right_grads, z_proposal, z_proposal_pe, z_proposal_grads, r_sum, tree_weight,\n                         turning, diverging, sum_accept_probs, num_proposals)\n\n    def sample(self, params):\n        z, potential_energy, z_grads = self._fetch_from_cache()\n        # recompute PE when cache is cleared\n        if z is None:\n            z = params\n            z_grads, potential_energy = potential_grad(self.potential_fn, z)\n            self._cache(z, potential_energy, z_grads)\n        # return early if no sample sites\n        elif len(z) == 0:\n            self._t += 1\n            self._mean_accept_prob = 1.\n            if self._t > self._warmup_steps:\n                self._accept_cnt += 1\n            return z\n        r, r_unscaled = self._sample_r(name=""r_t={}"".format(self._t))\n        energy_current = self._kinetic_energy(r_unscaled) + potential_energy\n\n        # Ideally, following a symplectic integrator trajectory, the energy is constant.\n        # In that case, we can sample the proposal uniformly, and there is no need to use ""slice"".\n        # However, it is not the case for real situation: there are errors during the computation.\n        # To deal with that problem, as in [1], we introduce an auxiliary ""slice"" variable (denoted\n        # by u).\n        # The sampling process goes as follows:\n        #   first sampling u from initial state (z_0, r_0) according to\n        #     u ~ Uniform(0, p(z_0, r_0)),\n        #   then sampling state (z, r) from the integrator trajectory according to\n        #     (z, r) ~ Uniform({(z\', r\') in trajectory | p(z\', r\') >= u}).\n        #\n        # For more information about slice sampling method, see [3].\n        # For another version of NUTS which uses multinomial sampling instead of slice sampling,\n        # see [2].\n\n        if self.use_multinomial_sampling:\n            log_slice = -energy_current\n        else:\n            # Rather than sampling the slice variable from `Uniform(0, exp(-energy))`, we can\n            # sample log_slice directly using `energy`, so as to avoid potential underflow or\n            # overflow issues ([2]).\n            slice_exp_term = pyro.sample(""slicevar_exp_t={}"".format(self._t),\n                                         dist.Exponential(scalar_like(energy_current, 1.)))\n            log_slice = -energy_current - slice_exp_term\n\n        z_left = z_right = z\n        r_left = r_right = r\n        r_left_unscaled = r_right_unscaled = r_unscaled\n        z_left_grads = z_right_grads = z_grads\n        accepted = False\n        r_sum = r_unscaled\n        sum_accept_probs = 0.\n        num_proposals = 0\n        tree_weight = scalar_like(energy_current, 0. if self.use_multinomial_sampling else 1.)\n\n        # Temporarily disable distributions args checking as\n        # NaNs are expected during step size adaptation.\n        with optional(pyro.validation_enabled(False), self._t < self._warmup_steps):\n            # doubling process, stop when turning or diverging\n            tree_depth = 0\n            while tree_depth < self._max_tree_depth:\n                direction = pyro.sample(""direction_t={}_treedepth={}"".format(self._t, tree_depth),\n                                        dist.Bernoulli(probs=scalar_like(tree_weight, 0.5)))\n                direction = int(direction.item())\n                if direction == 1:  # go to the right, start from the right leaf of current tree\n                    new_tree = self._build_tree(z_right, r_right, z_right_grads, log_slice,\n                                                direction, tree_depth, energy_current)\n                    # update leaf for the next doubling process\n                    z_right = new_tree.z_right\n                    r_right = new_tree.r_right\n                    r_right_unscaled = new_tree.r_right_unscaled\n                    z_right_grads = new_tree.z_right_grads\n                else:  # go the the left, start from the left leaf of current tree\n                    new_tree = self._build_tree(z_left, r_left, z_left_grads, log_slice,\n                                                direction, tree_depth, energy_current)\n                    z_left = new_tree.z_left\n                    r_left = new_tree.r_left\n                    r_left_unscaled = new_tree.r_left_unscaled\n                    z_left_grads = new_tree.z_left_grads\n\n                sum_accept_probs = sum_accept_probs + new_tree.sum_accept_probs\n                num_proposals = num_proposals + new_tree.num_proposals\n\n                # stop doubling\n                if new_tree.diverging:\n                    if self._t >= self._warmup_steps:\n                        self._divergences.append(self._t - self._warmup_steps)\n                    break\n\n                if new_tree.turning:\n                    break\n\n                tree_depth += 1\n\n                if self.use_multinomial_sampling:\n                    new_tree_prob = (new_tree.weight - tree_weight).exp()\n                else:\n                    new_tree_prob = new_tree.weight / tree_weight\n                rand = pyro.sample(""rand_t={}_treedepth={}"".format(self._t, tree_depth),\n                                   dist.Uniform(scalar_like(new_tree_prob, 0.),\n                                                scalar_like(new_tree_prob, 1.)))\n                if rand < new_tree_prob:\n                    accepted = True\n                    z = new_tree.z_proposal\n                    z_grads = new_tree.z_proposal_grads\n                    self._cache(z, new_tree.z_proposal_pe, z_grads)\n\n                r_sum = {site_names: r_sum[site_names] + new_tree.r_sum[site_names]\n                         for site_names in r_unscaled}\n                if self._is_turning(r_left_unscaled, r_right_unscaled, r_sum):  # stop doubling\n                    break\n                else:  # update tree_weight\n                    if self.use_multinomial_sampling:\n                        tree_weight = _logaddexp(tree_weight, new_tree.weight)\n                    else:\n                        tree_weight = tree_weight + new_tree.weight\n\n        accept_prob = sum_accept_probs / num_proposals\n\n        self._t += 1\n        if self._t > self._warmup_steps:\n            n = self._t - self._warmup_steps\n            if accepted:\n                self._accept_cnt += 1\n        else:\n            n = self._t\n            self._adapter.step(self._t, z, accept_prob, z_grads)\n        self._mean_accept_prob += (accept_prob.item() - self._mean_accept_prob) / n\n\n        return z.copy()\n'"
pyro/infer/mcmc/util.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport warnings\nfrom collections import OrderedDict, defaultdict\nfrom functools import partial, reduce\nfrom itertools import product\nimport traceback as tb\n\nimport torch\nfrom torch.distributions import biject_to\nfrom opt_einsum import shared_intermediates\n\nimport pyro\nimport pyro.poutine as poutine\nfrom pyro.distributions.util import broadcast_shape, logsumexp\nfrom pyro.infer import config_enumerate\nfrom pyro.infer.autoguide.initialization import InitMessenger, init_to_uniform\nfrom pyro.infer.util import is_validation_enabled\nfrom pyro.ops import stats\nfrom pyro.ops.contract import contract_to_tensor\nfrom pyro.ops.integrator import potential_grad\nfrom pyro.poutine.subsample_messenger import _Subsample\nfrom pyro.poutine.util import prune_subsample_sites\nfrom pyro.util import check_site_shape, ignore_jit_warnings\n\n\nclass TraceTreeEvaluator:\n    """"""\n    Computes the log probability density of a trace (of a model with\n    tree structure) that possibly contains discrete sample sites\n    enumerated in parallel. This will be deprecated in favor of\n    :class:`~pyro.infer.mcmc.util.EinsumTraceProbEvaluator`.\n\n    :param model_trace: execution trace from a static model.\n    :param bool has_enumerable_sites: whether the trace contains any\n        discrete enumerable sites.\n    :param int max_plate_nesting: Optional bound on max number of nested\n        :func:`pyro.plate` contexts.\n    """"""\n    def __init__(self,\n                 model_trace,\n                 has_enumerable_sites=False,\n                 max_plate_nesting=None):\n        self.has_enumerable_sites = has_enumerable_sites\n        self.max_plate_nesting = max_plate_nesting\n        # To be populated using the model trace once.\n        self._log_probs = defaultdict(list)\n        self._log_prob_shapes = defaultdict(tuple)\n        self._children = defaultdict(list)\n        self._enum_dims = {}\n        self._plate_dims = {}\n        self._parse_model_structure(model_trace)\n\n    def _parse_model_structure(self, model_trace):\n        if not self.has_enumerable_sites:\n            return\n        if self.max_plate_nesting is None:\n            raise ValueError(""Finite value required for `max_plate_nesting` when model ""\n                             ""has discrete (enumerable) sites."")\n        self._compute_log_prob_terms(model_trace)\n        # 1. Infer model structure - compute parent-child relationship.\n        sorted_ordinals = sorted(self._log_probs.keys())\n        for i, child_node in enumerate(sorted_ordinals):\n            for j in range(i-1, -1, -1):\n                cur_node = sorted_ordinals[j]\n                if cur_node < child_node:\n                    self._children[cur_node].append(child_node)\n                    break  # at most 1 parent.\n        # 2. Populate `plate_dims` and `enum_dims` to be evaluated/\n        #    enumerated out at each ordinal.\n        self._populate_cache(frozenset(), frozenset(), set())\n\n    def _populate_cache(self, ordinal, parent_ordinal, parent_enum_dims):\n        """"""\n        For each ordinal, populate the `plate` and `enum` dims to be\n        evaluated or enumerated out.\n        """"""\n        log_prob_shape = self._log_prob_shapes[ordinal]\n        plate_dims = sorted([frame.dim for frame in ordinal - parent_ordinal])\n        enum_dims = set((i for i in range(-len(log_prob_shape), -self.max_plate_nesting)\n                         if log_prob_shape[i] > 1))\n        self._plate_dims[ordinal] = plate_dims\n        self._enum_dims[ordinal] = set(enum_dims - parent_enum_dims)\n        for c in self._children[ordinal]:\n            self._populate_cache(c, ordinal, enum_dims)\n\n    def _compute_log_prob_terms(self, model_trace):\n        """"""\n        Computes the conditional probabilities for each of the sites\n        in the model trace, and stores the result in `self._log_probs`.\n        """"""\n        model_trace.compute_log_prob()\n        self._log_probs = defaultdict(list)\n        ordering = {name: frozenset(site[""cond_indep_stack""])\n                    for name, site in model_trace.nodes.items()\n                    if site[""type""] == ""sample""}\n        # Collect log prob terms per independence context.\n        for name, site in model_trace.nodes.items():\n            if site[""type""] == ""sample"":\n                if is_validation_enabled():\n                    check_site_shape(site, self.max_plate_nesting)\n                self._log_probs[ordering[name]].append(site[""log_prob""])\n        if not self._log_prob_shapes:\n            for ordinal, log_prob in self._log_probs.items():\n                self._log_prob_shapes[ordinal] = broadcast_shape(*(t.shape for t in self._log_probs[ordinal]))\n\n    def _reduce(self, ordinal, agg_log_prob=torch.tensor(0.)):\n        """"""\n        Reduce the log prob terms for the given ordinal:\n          - taking log_sum_exp of factors in enum dims (i.e.\n            adding up the probability terms).\n          - summing up the dims within `max_plate_nesting`.\n            (i.e. multiplying probs within independent batches).\n\n        :param ordinal: node (ordinal)\n        :param torch.Tensor agg_log_prob: aggregated `log_prob`\n            terms from the downstream nodes.\n        :return: `log_prob` with marginalized `plate` and `enum`\n            dims.\n        """"""\n        log_prob = sum(self._log_probs[ordinal]) + agg_log_prob\n        for enum_dim in self._enum_dims[ordinal]:\n            log_prob = logsumexp(log_prob, dim=enum_dim, keepdim=True)\n        for marginal_dim in self._plate_dims[ordinal]:\n            log_prob = log_prob.sum(dim=marginal_dim, keepdim=True)\n        return log_prob\n\n    def _aggregate_log_probs(self, ordinal):\n        """"""\n        Aggregate the `log_prob` terms using depth first search.\n        """"""\n        if not self._children[ordinal]:\n            return self._reduce(ordinal)\n        agg_log_prob = sum(map(self._aggregate_log_probs, self._children[ordinal]))\n        return self._reduce(ordinal, agg_log_prob)\n\n    def log_prob(self, model_trace):\n        """"""\n        Returns the log pdf of `model_trace` by appropriately handling\n        enumerated log prob factors.\n\n        :return: log pdf of the trace.\n        """"""\n        with shared_intermediates():\n            if not self.has_enumerable_sites:\n                return model_trace.log_prob_sum()\n            self._compute_log_prob_terms(model_trace)\n            return self._aggregate_log_probs(ordinal=frozenset()).sum()\n\n\nclass TraceEinsumEvaluator:\n    """"""\n    Computes the log probability density of a trace (of a model with\n    tree structure) that possibly contains discrete sample sites\n    enumerated in parallel. This uses optimized `einsum` operations\n    to marginalize out the the enumerated dimensions in the trace\n    via :class:`~pyro.ops.contract.contract_to_tensor`.\n\n    :param model_trace: execution trace from a static model.\n    :param bool has_enumerable_sites: whether the trace contains any\n        discrete enumerable sites.\n    :param int max_plate_nesting: Optional bound on max number of nested\n        :func:`pyro.plate` contexts.\n    """"""\n    def __init__(self,\n                 model_trace,\n                 has_enumerable_sites=False,\n                 max_plate_nesting=None):\n        self.has_enumerable_sites = has_enumerable_sites\n        self.max_plate_nesting = max_plate_nesting\n        # To be populated using the model trace once.\n        self._enum_dims = set()\n        self.ordering = {}\n        self._populate_cache(model_trace)\n\n    def _populate_cache(self, model_trace):\n        """"""\n        Populate the ordinals (set of ``CondIndepStack`` frames)\n        and enum_dims for each sample site.\n        """"""\n        if not self.has_enumerable_sites:\n            return\n        if self.max_plate_nesting is None:\n            raise ValueError(""Finite value required for `max_plate_nesting` when model ""\n                             ""has discrete (enumerable) sites."")\n        model_trace.compute_log_prob()\n        model_trace.pack_tensors()\n        for name, site in model_trace.nodes.items():\n            if site[""type""] == ""sample"" and not isinstance(site[""fn""], _Subsample):\n                if is_validation_enabled():\n                    check_site_shape(site, self.max_plate_nesting)\n                self.ordering[name] = frozenset(model_trace.plate_to_symbol[f.name]\n                                                for f in site[""cond_indep_stack""]\n                                                if f.vectorized)\n        self._enum_dims = set(model_trace.symbol_to_dim) - set(model_trace.plate_to_symbol.values())\n\n    def _get_log_factors(self, model_trace):\n        """"""\n        Aggregates the `log_prob` terms into a list for each\n        ordinal.\n        """"""\n        model_trace.compute_log_prob()\n        model_trace.pack_tensors()\n        log_probs = OrderedDict()\n        # Collect log prob terms per independence context.\n        for name, site in model_trace.nodes.items():\n            if site[""type""] == ""sample"" and not isinstance(site[""fn""], _Subsample):\n                if is_validation_enabled():\n                    check_site_shape(site, self.max_plate_nesting)\n                log_probs.setdefault(self.ordering[name], []).append(site[""packed""][""log_prob""])\n        return log_probs\n\n    def log_prob(self, model_trace):\n        """"""\n        Returns the log pdf of `model_trace` by appropriately handling\n        enumerated log prob factors.\n\n        :return: log pdf of the trace.\n        """"""\n        if not self.has_enumerable_sites:\n            return model_trace.log_prob_sum()\n        log_probs = self._get_log_factors(model_trace)\n        with shared_intermediates() as cache:\n            return contract_to_tensor(log_probs, self._enum_dims, cache=cache)\n\n\ndef _guess_max_plate_nesting(model, args, kwargs):\n    """"""\n    Guesses max_plate_nesting by running the model once\n    without enumeration. This optimistically assumes static model\n    structure.\n    """"""\n    with poutine.block():\n        model_trace = poutine.trace(model).get_trace(*args, **kwargs)\n    sites = [site for site in model_trace.nodes.values()\n             if site[""type""] == ""sample""]\n\n    dims = [frame.dim\n            for site in sites\n            for frame in site[""cond_indep_stack""]\n            if frame.vectorized]\n    max_plate_nesting = -min(dims) if dims else 0\n    return max_plate_nesting\n\n\nclass _PEMaker:\n    def __init__(self, model, model_args, model_kwargs, trace_prob_evaluator, transforms):\n        self.model = model\n        self.model_args = model_args\n        self.model_kwargs = model_kwargs\n        self.trace_prob_evaluator = trace_prob_evaluator\n        self.transforms = transforms\n        self._compiled_fn = None\n\n    def _potential_fn(self, params):\n        params_constrained = {k: self.transforms[k].inv(v) for k, v in params.items()}\n        cond_model = poutine.condition(self.model, params_constrained)\n        model_trace = poutine.trace(cond_model).get_trace(*self.model_args,\n                                                          **self.model_kwargs)\n        log_joint = self.trace_prob_evaluator.log_prob(model_trace)\n        for name, t in self.transforms.items():\n            log_joint = log_joint - torch.sum(\n                t.log_abs_det_jacobian(params_constrained[name], params[name]))\n        return -log_joint\n\n    def _potential_fn_jit(self, skip_jit_warnings, jit_options, params):\n        if not params:\n            return self._potential_fn(params)\n        names, vals = zip(*sorted(params.items()))\n\n        if self._compiled_fn:\n            return self._compiled_fn(*vals)\n\n        with pyro.validation_enabled(False):\n            tmp = []\n            for _, v in pyro.get_param_store().named_parameters():\n                if v.requires_grad:\n                    v.requires_grad_(False)\n                    tmp.append(v)\n\n            def _pe_jit(*zi):\n                params = dict(zip(names, zi))\n                return self._potential_fn(params)\n\n            if skip_jit_warnings:\n                _pe_jit = ignore_jit_warnings()(_pe_jit)\n            self._compiled_fn = torch.jit.trace(_pe_jit, vals, **jit_options)\n\n            result = self._compiled_fn(*vals)\n            for v in tmp:\n                v.requires_grad_(True)\n            return result\n\n    def get_potential_fn(self, jit_compile=False, skip_jit_warnings=True, jit_options=None):\n        if jit_compile:\n            jit_options = {""check_trace"": False} if jit_options is None else jit_options\n            return partial(self._potential_fn_jit, skip_jit_warnings, jit_options)\n        return self._potential_fn\n\n\ndef _find_valid_initial_params(model, model_args, model_kwargs, transforms, potential_fn,\n                               prototype_params, max_tries_initial_params=100, num_chains=1,\n                               init_strategy=init_to_uniform, trace=None):\n    params = prototype_params\n\n    # For empty models, exit early\n    if not params:\n        return params\n\n    params_per_chain = defaultdict(list)\n    num_found = 0\n    model = InitMessenger(init_strategy)(model)\n    for attempt in range(num_chains * max_tries_initial_params):\n        if trace is None:\n            trace = poutine.trace(model).get_trace(*model_args, **model_kwargs)\n        samples = {name: trace.nodes[name][""value""].detach() for name in params}\n        params = {k: transforms[k](v) for k, v in samples.items()}\n        pe_grad, pe = potential_grad(potential_fn, params)\n\n        if torch.isfinite(pe) and all(map(torch.all, map(torch.isfinite, pe_grad.values()))):\n            for k, v in params.items():\n                params_per_chain[k].append(v)\n            num_found += 1\n            if num_found == num_chains:\n                if num_chains == 1:\n                    return {k: v[0] for k, v in params_per_chain.items()}\n                else:\n                    return {k: torch.stack(v) for k, v in params_per_chain.items()}\n        trace = None\n    raise ValueError(""Model specification seems incorrect - cannot find valid initial params."")\n\n\ndef initialize_model(model, model_args=(), model_kwargs={}, transforms=None, max_plate_nesting=None,\n                     jit_compile=False, jit_options=None, skip_jit_warnings=False, num_chains=1,\n                     init_strategy=init_to_uniform, initial_params=None):\n    """"""\n    Given a Python callable with Pyro primitives, generates the following model-specific\n    properties needed for inference using HMC/NUTS kernels:\n\n    - initial parameters to be sampled using a HMC kernel,\n    - a potential function whose input is a dict of parameters in unconstrained space,\n    - transforms to transform latent sites of `model` to unconstrained space,\n    - a prototype trace to be used in MCMC to consume traces from sampled parameters.\n\n    :param model: a Pyro model which contains Pyro primitives.\n    :param tuple model_args: optional args taken by `model`.\n    :param dict model_kwargs: optional kwargs taken by `model`.\n    :param dict transforms: Optional dictionary that specifies a transform\n        for a sample site with constrained support to unconstrained space. The\n        transform should be invertible, and implement `log_abs_det_jacobian`.\n        If not specified and the model has sites with constrained support,\n        automatic transformations will be applied, as specified in\n        :mod:`torch.distributions.constraint_registry`.\n    :param int max_plate_nesting: Optional bound on max number of nested\n        :func:`pyro.plate` contexts. This is required if model contains\n        discrete sample sites that can be enumerated over in parallel.\n    :param bool jit_compile: Optional parameter denoting whether to use\n        the PyTorch JIT to trace the log density computation, and use this\n        optimized executable trace in the integrator.\n    :param dict jit_options: A dictionary contains optional arguments for\n        :func:`torch.jit.trace` function.\n    :param bool ignore_jit_warnings: Flag to ignore warnings from the JIT\n        tracer when ``jit_compile=True``. Default is False.\n    :param int num_chains: Number of parallel chains. If `num_chains > 1`,\n        the returned `initial_params` will be a list with `num_chains` elements.\n    :param callable init_strategy: A per-site initialization function.\n        See :ref:`autoguide-initialization` section for available functions.\n    :param dict initial_params: dict containing initial tensors in unconstrained\n        space to initiate the markov chain.\n    :returns: a tuple of (`initial_params`, `potential_fn`, `transforms`, `prototype_trace`)\n    """"""\n    # XXX `transforms` domains are sites\' supports\n    # FIXME: find a good pattern to deal with `transforms` arg\n    if transforms is None:\n        automatic_transform_enabled = True\n        transforms = {}\n    else:\n        automatic_transform_enabled = False\n    if max_plate_nesting is None:\n        max_plate_nesting = _guess_max_plate_nesting(model, model_args, model_kwargs)\n    # Wrap model in `poutine.enum` to enumerate over discrete latent sites.\n    # No-op if model does not have any discrete latents.\n    model = poutine.enum(config_enumerate(model),\n                         first_available_dim=-1 - max_plate_nesting)\n    prototype_model = poutine.trace(InitMessenger(init_strategy)(model))\n    model_trace = prototype_model.get_trace(*model_args, **model_kwargs)\n    has_enumerable_sites = False\n    prototype_samples = {}\n    for name, node in model_trace.iter_stochastic_nodes():\n        fn = node[""fn""]\n        if isinstance(fn, _Subsample):\n            if fn.subsample_size is not None and fn.subsample_size < fn.size:\n                raise NotImplementedError(""HMC/NUTS does not support model with subsample sites."")\n            continue\n        if node[""fn""].has_enumerate_support:\n            has_enumerable_sites = True\n            continue\n        # we need to detach here because this sample can be a leaf variable,\n        # so we can\'t change its requires_grad flag to calculate its grad in\n        # velocity_verlet\n        prototype_samples[name] = node[""value""].detach()\n        if automatic_transform_enabled:\n            transforms[name] = biject_to(node[""fn""].support).inv\n\n    trace_prob_evaluator = TraceEinsumEvaluator(model_trace,\n                                                has_enumerable_sites,\n                                                max_plate_nesting)\n\n    pe_maker = _PEMaker(model, model_args, model_kwargs, trace_prob_evaluator, transforms)\n\n    if initial_params is None:\n        prototype_params = {k: transforms[k](v) for k, v in prototype_samples.items()}\n        # Note that we deliberately do not exercise jit compilation here so as to\n        # enable potential_fn to be picklable (a torch._C.Function cannot be pickled).\n        # We pass model_trace merely for computational savings.\n        initial_params = _find_valid_initial_params(model, model_args, model_kwargs, transforms,\n                                                    pe_maker.get_potential_fn(), prototype_params,\n                                                    num_chains=num_chains, init_strategy=init_strategy,\n                                                    trace=model_trace)\n    potential_fn = pe_maker.get_potential_fn(jit_compile, skip_jit_warnings, jit_options)\n    return initial_params, potential_fn, transforms, model_trace\n\n\ndef _safe(fn):\n    """"""\n    Safe version of utilities in the :mod:`pyro.ops.stats` module. Wrapped\n    functions return `NaN` tensors instead of throwing exceptions.\n\n    :param fn: stats function from :mod:`pyro.ops.stats` module.\n    """"""\n    @functools.wraps(fn)\n    def wrapped(sample, *args, **kwargs):\n        try:\n            val = fn(sample, *args, **kwargs)\n        except Exception:\n            warnings.warn(tb.format_exc())\n            val = torch.full(sample.shape[2:], float(""nan""),\n                             dtype=sample.dtype, device=sample.device)\n        return val\n\n    return wrapped\n\n\ndef diagnostics(samples, group_by_chain=True):\n    """"""\n    Gets diagnostics statistics such as effective sample size and\n    split Gelman-Rubin using the samples drawn from the posterior\n    distribution.\n\n    :param dict samples: dictionary of samples keyed by site name.\n    :param bool group_by_chain: If True, each variable in `samples`\n        will be treated as having shape `num_chains x num_samples x sample_shape`.\n        Otherwise, the corresponding shape will be `num_samples x sample_shape`\n        (i.e. without chain dimension).\n    :return: dictionary of diagnostic stats for each sample site.\n    """"""\n    diagnostics = {}\n    for site, support in samples.items():\n        if not group_by_chain:\n            support = support.unsqueeze(0)\n        site_stats = OrderedDict()\n        site_stats[""n_eff""] = _safe(stats.effective_sample_size)(support)\n        site_stats[""r_hat""] = stats.split_gelman_rubin(support)\n        diagnostics[site] = site_stats\n    return diagnostics\n\n\ndef summary(samples, prob=0.9, group_by_chain=True):\n    """"""\n    Returns a summary table displaying diagnostics of ``samples`` from the\n    posterior. The diagnostics displayed are mean, standard deviation, median,\n    the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,\n    :func:`~pyro.ops.stats.split_gelman_rubin`.\n\n    :param dict samples: dictionary of samples keyed by site name.\n    :param float prob: the probability mass of samples within the credibility interval.\n    :param bool group_by_chain: If True, each variable in `samples`\n        will be treated as having shape `num_chains x num_samples x sample_shape`.\n        Otherwise, the corresponding shape will be `num_samples x sample_shape`\n        (i.e. without chain dimension).\n    """"""\n    if not group_by_chain:\n        samples = {k: v.unsqueeze(0) for k, v in samples.items()}\n\n    summary_dict = {}\n    for name, value in samples.items():\n        value_flat = torch.reshape(value, (-1,) + value.shape[2:])\n        mean = value_flat.mean(dim=0)\n        std = value_flat.std(dim=0)\n        median = value_flat.median(dim=0)[0]\n        hpdi = stats.hpdi(value_flat, prob=prob)\n        n_eff = _safe(stats.effective_sample_size)(value)\n        r_hat = stats.split_gelman_rubin(value)\n        hpd_lower = \'{:.1f}%\'.format(50 * (1 - prob))\n        hpd_upper = \'{:.1f}%\'.format(50 * (1 + prob))\n        summary_dict[name] = OrderedDict([(""mean"", mean), (""std"", std), (""median"", median),\n                                          (hpd_lower, hpdi[0]), (hpd_upper, hpdi[1]),\n                                          (""n_eff"", n_eff), (""r_hat"", r_hat)])\n    return summary_dict\n\n\ndef print_summary(samples, prob=0.9, group_by_chain=True):\n    """"""\n    Prints a summary table displaying diagnostics of ``samples`` from the\n    posterior. The diagnostics displayed are mean, standard deviation, median,\n    the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,\n    :func:`~pyro.ops.stats.split_gelman_rubin`.\n\n    :param dict samples: dictionary of samples keyed by site name.\n    :param float prob: the probability mass of samples within the credibility interval.\n    :param bool group_by_chain: If True, each variable in `samples`\n        will be treated as having shape `num_chains x num_samples x sample_shape`.\n        Otherwise, the corresponding shape will be `num_samples x sample_shape`\n        (i.e. without chain dimension).\n    """"""\n    summary_dict = summary(samples, prob, group_by_chain)\n\n    row_names = {k: k + \'[\' + \',\'.join(map(lambda x: str(x - 1), v.shape[2:])) + \']\'\n                 for k, v in samples.items()}\n    max_len = max(max(map(lambda x: len(x), row_names.values())), 10)\n    name_format = \'{:>\' + str(max_len) + \'}\'\n    header_format = name_format + \' {:>9}\' * 7\n    columns = [\'\'] + list(list(summary_dict.values())[0].keys())\n\n    print()\n    print(header_format.format(*columns))\n\n    row_format = name_format + \' {:>9.2f}\' * 7\n    for name, stats_dict in summary_dict.items():\n        shape = stats_dict[""mean""].shape\n        if len(shape) == 0:\n            print(row_format.format(name, *stats_dict.values()))\n        else:\n            for idx in product(*map(range, shape)):\n                idx_str = \'[{}]\'.format(\',\'.join(map(str, idx)))\n                print(row_format.format(name + idx_str, *[v[idx] for v in stats_dict.values()]))\n    print()\n\n\ndef _predictive_sequential(model, posterior_samples, model_args, model_kwargs,\n                           num_samples, sample_sites, return_trace=False):\n    collected = []\n    samples = [{k: v[i] for k, v in posterior_samples.items()} for i in range(num_samples)]\n    for i in range(num_samples):\n        trace = poutine.trace(poutine.condition(model, samples[i])).get_trace(*model_args, **model_kwargs)\n        if return_trace:\n            collected.append(trace)\n        else:\n            collected.append({site: trace.nodes[site][\'value\'] for site in sample_sites})\n\n    return collected if return_trace else {site: torch.stack([s[site] for s in collected])\n                                           for site in sample_sites}\n\n\ndef predictive(model, posterior_samples, *args, **kwargs):\n    """"""\n    .. warning::\n        This function is deprecated and will be removed in a future release.\n        Use the :class:`~pyro.infer.predictive.Predictive` class instead.\n\n    Run model by sampling latent parameters from `posterior_samples`, and return\n    values at sample sites from the forward run. By default, only sites not contained in\n    `posterior_samples` are returned. This can be modified by changing the `return_sites`\n    keyword argument.\n\n    :param model: Python callable containing Pyro primitives.\n    :param dict posterior_samples: dictionary of samples from the posterior.\n    :param args: model arguments.\n    :param kwargs: model kwargs; and other keyword arguments (see below).\n\n    :Keyword Arguments:\n        * **num_samples** (``int``) - number of samples to draw from the predictive distribution.\n          This argument has no effect if ``posterior_samples`` is non-empty, in which case, the\n          leading dimension size of samples in ``posterior_samples`` is used.\n        * **return_sites** (``list``) - sites to return; by default only sample sites not present\n          in `posterior_samples` are returned.\n        * **return_trace** (``bool``) - whether to return the full trace. Note that this is vectorized\n          over `num_samples`.\n        * **parallel** (``bool``) - predict in parallel by wrapping the existing model\n          in an outermost `plate` messenger. Note that this requires that the model has\n          all batch dims correctly annotated via :class:`~pyro.plate`. Default is `False`.\n\n    :return: dict of samples from the predictive distribution, or a single vectorized\n        `trace` (if `return_trace=True`).\n    """"""\n    warnings.warn(\'The `mcmc.predictive` function is deprecated and will be removed in \'\n                  \'a future release. Use the `pyro.infer.Predictive` class instead.\',\n                  FutureWarning)\n    num_samples = kwargs.pop(\'num_samples\', None)\n    return_sites = kwargs.pop(\'return_sites\', None)\n    return_trace = kwargs.pop(\'return_trace\', False)\n    parallel = kwargs.pop(\'parallel\', False)\n\n    max_plate_nesting = _guess_max_plate_nesting(model, args, kwargs)\n    model_trace = prune_subsample_sites(poutine.trace(model).get_trace(*args, **kwargs))\n    reshaped_samples = {}\n\n    for name, sample in posterior_samples.items():\n\n        batch_size, sample_shape = sample.shape[0], sample.shape[1:]\n\n        if num_samples is None:\n            num_samples = batch_size\n\n        elif num_samples != batch_size:\n            warnings.warn(""Sample\'s leading dimension size {} is different from the ""\n                          ""provided {} num_samples argument. Defaulting to {}.""\n                          .format(batch_size, num_samples, batch_size), UserWarning)\n            num_samples = batch_size\n\n        sample = sample.reshape((num_samples,) + (1,) * (max_plate_nesting - len(sample_shape)) + sample_shape)\n        reshaped_samples[name] = sample\n\n    if num_samples is None:\n        raise ValueError(""No sample sites in model to infer `num_samples`."")\n\n    return_site_shapes = {}\n    for site in model_trace.stochastic_nodes + model_trace.observation_nodes:\n        site_shape = (num_samples,) + model_trace.nodes[site][\'value\'].shape\n        if return_sites:\n            if site in return_sites:\n                return_site_shapes[site] = site_shape\n        else:\n            if site not in reshaped_samples:\n                return_site_shapes[site] = site_shape\n\n    if not parallel:\n        return _predictive_sequential(model, posterior_samples, args, kwargs, num_samples,\n                                      return_site_shapes.keys(), return_trace)\n\n    def _vectorized_fn(fn):\n        """"""\n        Wraps a callable inside an outermost :class:`~pyro.plate` to parallelize\n        sampling from the posterior predictive.\n\n        :param fn: arbitrary callable containing Pyro primitives.\n        :return: wrapped callable.\n        """"""\n\n        def wrapped_fn(*args, **kwargs):\n            with pyro.plate(""_num_predictive_samples"", num_samples, dim=-max_plate_nesting-1):\n                return fn(*args, **kwargs)\n\n        return wrapped_fn\n\n    trace = poutine.trace(poutine.condition(_vectorized_fn(model), reshaped_samples))\\\n        .get_trace(*args, **kwargs)\n\n    if return_trace:\n        return trace\n\n    predictions = {}\n    for site, shape in return_site_shapes.items():\n        value = trace.nodes[site][\'value\']\n        if value.numel() < reduce((lambda x, y: x * y), shape):\n            predictions[site] = value.expand(shape)\n        else:\n            predictions[site] = value.reshape(shape)\n\n    return predictions\n'"
pyro/infer/reparam/__init__.py,0,"b'# Copyright Contributors to the Pyro project.\n# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom .conjugate import ConjugateReparam\nfrom .discrete_cosine import DiscreteCosineReparam\nfrom .haar import HaarReparam\nfrom .hmm import LinearHMMReparam\nfrom .loc_scale import LocScaleReparam\nfrom .neutra import NeuTraReparam\nfrom .split import SplitReparam\nfrom .stable import LatentStableReparam, StableReparam, SymmetricStableReparam\nfrom .studentt import StudentTReparam\nfrom .transform import TransformReparam\nfrom .unit_jacobian import UnitJacobianReparam\n\n__all__ = [\n    ""ConjugateReparam"",\n    ""DiscreteCosineReparam"",\n    ""HaarReparam"",\n    ""LatentStableReparam"",\n    ""LinearHMMReparam"",\n    ""LocScaleReparam"",\n    ""NeuTraReparam"",\n    ""SplitReparam"",\n    ""StableReparam"",\n    ""StudentTReparam"",\n    ""SymmetricStableReparam"",\n    ""TransformReparam"",\n    ""UnitJacobianReparam"",\n]\n'"
pyro/infer/reparam/conjugate.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom .reparam import Reparam\n\n\nclass ConjugateReparam(Reparam):\n    """"""\n    EXPERIMENTAL Reparameterize to a conjugate updated distribution.\n\n    This updates a prior distribution ``fn`` using the\n    :meth:`~pyro.distributions.Distribution.conjugate_update`\n    method.  The guide may be either a distribution object or a callable\n    inputting model ``*args,**kwargs`` and returning a distribution object. The\n    guide may be approximate or learned.\n\n    For example consider the model and naive variational guide::\n\n        total = torch.tensor(10.)\n        count = torch.tensor(2.)\n\n        def model():\n            prob = pyro.sample(""prob"", dist.Beta(0.5, 1.5))\n            pyro.sample(""count"", dist.Binomial(total, prob), obs=count)\n\n        guide = AutoDiagonalNormal(model)  # learns the posterior over prob\n\n    Instead of using this learned guide, we can hand-compute the conjugate\n    posterior distribution over ""prob"", and then use a simpler guide during\n    inference, in this case an empty guide::\n\n        reparam_model = poutine.reparam(model, {\n            ""prob"": ConjugateReparam(dist.Beta(1 + count, 1 + total - count))\n        })\n\n        def reparam_guide():\n            pass  # nothing remains to be modeled!\n\n    :param guide: A likelihood distribution or a callable returning a\n        guide distribution. Only a few distributions are supported, depending\n        on the prior distribution\'s\n        :meth:`~pyro.distributions.Distribution.conjugate_update`\n        implementation.\n    :type guide: ~pyro.distributions.Distribution or callable\n    """"""\n    def __init__(self, guide):\n        self.guide = guide\n\n    def __call__(self, name, fn, obs):\n        assert obs is None, ""PosteriorReparam does not support observe statements""\n\n        # Compute a guide distribution, either static or dependent.\n        guide_dist = self.guide\n        if not isinstance(guide_dist, dist.Distribution):\n            args, kwargs = self.args_kwargs\n            guide_dist = guide_dist(*args, **kwargs)\n        assert isinstance(guide_dist, dist.Distribution)\n\n        # Draw a sample from the updated distribution.\n        fn, log_normalizer = fn.conjugate_update(guide_dist)\n        assert isinstance(guide_dist, dist.Distribution)\n        if not fn.has_rsample:\n            # Note supporting non-reparameterized sites would require more delicate\n            # handling of traced sites than the crude _do_not_trace flag below.\n            raise NotImplementedError(""ConjugateReparam inference supports only reparameterized ""\n                                      ""distributions, but got {}"".format(type(fn)))\n        value = pyro.sample(""{}_updated"".format(name), fn,\n                            infer={""is_auxiliary"": True, ""_do_not_trace"": True})\n\n        # Compute importance weight. Let p(z) be the original fn, q(z|x) be\n        # the guide, and u(z) be the conjugate_updated distribution. Then\n        #   normalizer = p(z) q(z|x) / u(z).\n        # Since we\'ve sampled from u(z) instead of p(z), we\n        # need an importance weight\n        #   p(z) / u(z) = normalizer / q(z|x)                          (Eqn 1)\n        # Note that q(z|x) is often approximate; in the exact case\n        #   q(z|x) = p(x|z) / integral p(x|z) dz\n        # so this site and the downstream likelihood site will have combined density\n        #   (p(z) / u(z)) p(x|z) = (normalizer / q(z|x)) p(x|z)\n        #                        = normalizer integral p(x|z) dz\n        # Hence in the exact case, downstream probability does not depend on the sampled z,\n        # permitting this reparameterizer to be used in HMC.\n        log_density = log_normalizer - guide_dist.log_prob(value)  # By Eqn 1.\n\n        # Return an importance-weighted point estimate.\n        new_fn = dist.Delta(value, log_density=log_density, event_dim=fn.event_dim)\n        return new_fn, value\n'"
pyro/infer/reparam/discrete_cosine.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.distributions.transforms.discrete_cosine import DiscreteCosineTransform\n\nfrom .unit_jacobian import UnitJacobianReparam\n\n\nclass DiscreteCosineReparam(UnitJacobianReparam):\n    """"""\n    Discrete Cosine reparameterizer, using a\n    :class:`~pyro.distributions.transforms.DiscreteCosineTransform` .\n\n    This is useful for sequential models where coupling along a time-like axis\n    (e.g. a banded precision matrix) introduces long-range correlation. This\n    reparameterizes to a frequency-domain representation where posterior\n    covariance should be closer to diagonal, thereby improving the accuracy of\n    diagonal guides in SVI and improving the effectiveness of a diagonal mass\n    matrix in HMC.\n\n    When reparameterizing variables that are approximately continuous along the\n    time dimension, set ``smooth=1``. For variables that are approximately\n    continuously differentiable along the time axis, set ``smooth=2``.\n\n    This reparameterization works only for latent variables, not likelihoods.\n\n    :param int dim: Dimension along which to transform. Must be negative.\n        This is an absolute dim counting from the right.\n    :param float smooth: Smoothing parameter. When 0, this transforms white\n        noise to white noise; when 1 this transforms Brownian noise to to white\n        noise; when -1 this transforms violet noise to white noise; etc. Any\n        real number is allowed. https://en.wikipedia.org/wiki/Colors_of_noise.\n    """"""\n    def __init__(self, dim=-1, smooth=0.):\n        transform = DiscreteCosineTransform(dim=dim, smooth=smooth, cache_size=1)\n        super().__init__(transform, suffix=""dct"")\n'"
pyro/infer/reparam/haar.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.distributions.transforms.haar import HaarTransform\n\nfrom .unit_jacobian import UnitJacobianReparam\n\n\nclass HaarReparam(UnitJacobianReparam):\n    """"""\n    Haar wavelet reparameterizer, using a\n    :class:`~pyro.distributions.transforms.HaarTransform`.\n\n    This is useful for sequential models where coupling along a time-like axis\n    (e.g. a banded precision matrix) introduces long-range correlation. This\n    reparameterizes to a frequency-domain representation where posterior\n    covariance should be closer to diagonal, thereby improving the accuracy of\n    diagonal guides in SVI and improving the effectiveness of a diagonal mass\n    matrix in HMC.\n\n    This reparameterization works only for latent variables, not likelihoods.\n\n    :param int dim: Dimension along which to transform. Must be negative.\n        This is an absolute dim counting from the right.\n    :param bool flip: Whether to flip the time axis before applying the\n        Haar transform. Defaults to false.\n    """"""\n    def __init__(self, dim=-1, flip=False):\n        transform = HaarTransform(dim=dim, flip=flip, cache_size=1)\n        super().__init__(transform, suffix=""haar"")\n'"
pyro/infer/reparam/hmm.py,3,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro.distributions as dist\n\nfrom .reparam import Reparam\n\n\nclass LinearHMMReparam(Reparam):\n    """"""\n    Auxiliary variable reparameterizer for\n    :class:`~pyro.distributions.LinearHMM` random variables.\n\n    This defers to component reparameterizers to create auxiliary random\n    variables conditioned on which the process becomes a\n    :class:`~pyro.distributions.GaussianHMM` . If the ``observation_dist`` is a\n    :class:`~pyro.distributions.TransformedDistribution` this reorders those\n    transforms so that the result is a\n    :class:`~pyro.distributions.TransformedDistribution` of\n    :class:`~pyro.distributions.GaussianHMM` .\n\n    This is useful for training the parameters of a\n    :class:`~pyro.distributions.LinearHMM` distribution, whose\n    :meth:`~pyro.distributions.LinearHMM.log_prob` method is undefined.  To\n    perform inference in the presence of non-Gaussian factors such as\n    :meth:`~pyro.distributions.Stable`, :meth:`~pyro.distributions.StudentT` or\n    :meth:`~pyro.distributions.LogNormal` , configure with\n    :class:`~pyro.infer.reparam.studentt.StudentTReparam` ,\n    :class:`~pyro.infer.reparam.stable.StableReparam` ,\n    :class:`~pyro.infer.reparam.stable.SymmetricStableReparam` , etc.  component\n    reparameterizers for ``init``, ``trans``, and ``scale``. For example::\n\n        hmm = LinearHMM(\n            init_dist=Stable(1,0,1,0).expand([2]).to_event(1),\n            trans_matrix=torch.eye(2),\n            trans_dist=MultivariateNormal(torch.zeros(2), torch.eye(2)),\n            obs_matrix=torch.eye(2),\n            obs_dist=TransformedDistribution(\n                Stable(1.5,-0.5,1.0).expand([2]).to_event(1),\n                ExpTransform()))\n\n        rep = LinearHMMReparam(init=SymmetricStableReparam(),\n                               obs=StableReparam())\n\n        with poutine.reparam(config={""hmm"": rep}):\n            pyro.sample(""hmm"", hmm, obs=data)\n\n    :param init: Optional reparameterizer for the initial distribution.\n    :type init: ~pyro.infer.reparam.reparam.Reparam\n    :param trans: Optional reparameterizer for the transition distribution.\n    :type trans: ~pyro.infer.reparam.reparam.Reparam\n    :param obs: Optional reparameterizer for the observation distribution.\n    :type obs: ~pyro.infer.reparam.reparam.Reparam\n    """"""\n    def __init__(self, init=None, trans=None, obs=None):\n        assert init is None or isinstance(init, Reparam)\n        assert trans is None or isinstance(trans, Reparam)\n        assert obs is None or isinstance(obs, Reparam)\n        self.init = init\n        self.trans = trans\n        self.obs = obs\n\n    def __call__(self, name, fn, obs):\n        fn, event_dim = self._unwrap(fn)\n        assert isinstance(fn, (dist.LinearHMM, dist.IndependentHMM))\n        if fn.duration is None:\n            raise ValueError(""LinearHMMReparam requires duration to be specified ""\n                             ""on targeted LinearHMM distributions"")\n\n        # Unwrap IndependentHMM.\n        if isinstance(fn, dist.IndependentHMM):\n            if obs is not None:\n                obs = obs.transpose(-1, -2).unsqueeze(-1)\n            hmm, obs = self(name, fn.base_dist.to_event(1), obs)\n            hmm = dist.IndependentHMM(hmm.to_event(-1))\n            if obs is not None:\n                obs = obs.squeeze(-1).transpose(-1, -2)\n            return hmm, obs\n\n        # Reparameterize the initial distribution as conditionally Gaussian.\n        init_dist = fn.initial_dist\n        if self.init is not None:\n            init_dist, _ = self.init(""{}_init"".format(name),\n                                     self._wrap(init_dist, event_dim - 1), None)\n            init_dist = init_dist.to_event(1 - init_dist.event_dim)\n\n        # Reparameterize the transition distribution as conditionally Gaussian.\n        trans_dist = fn.transition_dist\n        if self.trans is not None:\n            if trans_dist.batch_shape[-1] != fn.duration:\n                trans_dist = trans_dist.expand(trans_dist.batch_shape[:-1] + (fn.duration,))\n            trans_dist, _ = self.trans(""{}_trans"".format(name),\n                                       self._wrap(trans_dist, event_dim), None)\n            trans_dist = trans_dist.to_event(1 - trans_dist.event_dim)\n\n        # Reparameterize the observation distribution as conditionally Gaussian.\n        obs_dist = fn.observation_dist\n        if self.obs is not None:\n            if obs_dist.batch_shape[-1] != fn.duration:\n                obs_dist = obs_dist.expand(obs_dist.batch_shape[:-1] + (fn.duration,))\n            obs_dist, obs = self.obs(""{}_obs"".format(name),\n                                     self._wrap(obs_dist, event_dim), obs)\n            obs_dist = obs_dist.to_event(1 - obs_dist.event_dim)\n\n        # Reparameterize the entire HMM as conditionally Gaussian.\n        hmm = dist.GaussianHMM(init_dist, fn.transition_matrix, trans_dist,\n                               fn.observation_matrix, obs_dist, duration=fn.duration)\n        hmm = self._wrap(hmm, event_dim)\n\n        # Apply any observation transforms.\n        if fn.transforms:\n            hmm = dist.TransformedDistribution(hmm, fn.transforms)\n\n        return hmm, obs\n'"
pyro/infer/reparam/loc_scale.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions.util import is_identically_one, is_validation_enabled\n\nfrom .reparam import Reparam\n\n\nclass LocScaleReparam(Reparam):\n    """"""\n    Generic decentering reparameterizer [1] for latent variables parameterized\n    by ``loc`` and ``scale`` (and possibly additional ``shape_params``).\n\n    This reparameterization works only for latent variables, not likelihoods.\n\n    [1] Maria I. Gorinova, Dave Moore, Matthew D. Hoffman (2019)\n        ""Automatic Reparameterisation of Probabilistic Programs""\n        https://arxiv.org/pdf/1906.03028.pdf\n\n    :param float centered: optional centered parameter. If None (default) learn\n        a per-site per-element centering parameter in ``[0,1]``. If 0, fully\n        decenter the distribution; if 1, preserve the centered distribution\n        unchanged.\n    :param shape_params: list of additional parameter names to copy unchanged from\n        the centered to decentered distribution.\n    :type shape_params: tuple or list\n    """"""\n    def __init__(self, centered=None, shape_params=()):\n        assert centered is None or isinstance(centered, (float, torch.Tensor))\n        assert isinstance(shape_params, (tuple, list))\n        assert all(isinstance(name, str) for name in shape_params)\n        if is_validation_enabled():\n            if isinstance(centered, float):\n                assert 0 <= centered and centered <= 1\n            elif isinstance(centered, torch.Tensor):\n                assert (0 <= centered).all()\n                assert (centered <= 1).all()\n            else:\n                assert centered is None\n        self.centered = centered\n        self.shape_params = shape_params\n\n    def __call__(self, name, fn, obs):\n        assert obs is None, ""LocScaleReparam does not support observe statements""\n        centered = self.centered\n        if is_identically_one(centered):\n            return name, fn, obs\n        event_shape = fn.event_shape\n        fn, event_dim = self._unwrap(fn)\n\n        # Apply a partial decentering transform.\n        params = {key: getattr(fn, key) for key in self.shape_params}\n        if self.centered is None:\n            centered = pyro.param(""{}_centered"".format(name),\n                                  lambda: fn.loc.new_full(event_shape, 0.5),\n                                  constraint=constraints.unit_interval)\n        params[""loc""] = fn.loc * centered\n        params[""scale""] = fn.scale ** centered\n        decentered_fn = type(fn)(**params)\n\n        # Draw decentered noise.\n        decentered_value = pyro.sample(""{}_decentered"".format(name),\n                                       self._wrap(decentered_fn, event_dim))\n\n        # Differentiably transform.\n        delta = decentered_value - centered * fn.loc\n        value = fn.loc + fn.scale.pow(1 - centered) * delta\n\n        # Simulate a pyro.deterministic() site.\n        new_fn = dist.Delta(value, event_dim=event_dim).mask(False)\n        return new_fn, value\n'"
pyro/infer/reparam/neutra.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch.distributions import biject_to\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.distributions.util import sum_rightmost\nfrom pyro.infer.autoguide.guides import AutoContinuous\nfrom .reparam import Reparam\n\n\nclass NeuTraReparam(Reparam):\n    """"""\n    Neural Transport reparameterizer [1] of multiple latent variables.\n\n    This uses a trained :class:`~pyro.infer.autoguide.AutoContinuous`\n    guide to alter the geometry of a model, typically for use e.g. in MCMC.\n    Example usage::\n\n        # Step 1. Train a guide\n        guide = AutoIAFNormal(model)\n        svi = SVI(model, guide, ...)\n        # ...train the guide...\n\n        # Step 2. Use trained guide in NeuTra MCMC\n        neutra = NeuTraReparam(guide)\n        model = poutine.reparam(model, config=lambda _: neutra)\n        nuts = NUTS(model)\n        # ...now use the model in HMC or NUTS...\n\n    This reparameterization works only for latent variables, not likelihoods.\n    Note that all sites must share a single common :class:`NeuTraReparam`\n    instance, and that the model must have static structure.\n\n    [1] Hoffman, M. et al. (2019)\n        ""NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport""\n        https://arxiv.org/abs/1903.03704\n\n    :param ~pyro.infer.autoguide.AutoContinuous guide: A trained guide.\n    """"""\n    def __init__(self, guide):\n        if not isinstance(guide, AutoContinuous):\n            raise TypeError(""NeuTraReparam expected an AutoContinuous guide, but got {}""\n                            .format(type(guide)))\n        self.guide = guide\n        self.transform = None\n        self.x_unconstrained = []\n\n    def _reparam_config(self, site):\n        if site[""name""] in self.guide.prototype_trace:\n            return self\n\n    def reparam(self, fn=None):\n        return poutine.reparam(fn, config=self._reparam_config)\n\n    def __call__(self, name, fn, obs):\n        if name not in self.guide.prototype_trace.nodes:\n            return fn, obs\n        assert obs is None, ""NeuTraReparam does not support observe statements""\n        log_density = 0.\n        if not self.x_unconstrained:  # On first sample site.\n            # Sample a shared latent.\n            try:\n                self.transform = self.guide.get_transform()\n            except (NotImplementedError, TypeError):\n                raise ValueError(""NeuTraReparam only supports guides that implement ""\n                                 ""`get_transform` method that does not depend on the ""\n                                 ""model\'s `*args, **kwargs`"")\n\n            z_unconstrained = pyro.sample(""{}_shared_latent"".format(name),\n                                          self.guide.get_base_dist().mask(False))\n\n            # Differentiably transform.\n            x_unconstrained = self.transform(z_unconstrained)\n            log_density = self.transform.log_abs_det_jacobian(z_unconstrained, x_unconstrained)\n            self.x_unconstrained = list(reversed(list(self.guide._unpack_latent(x_unconstrained))))\n\n        # Extract a single site\'s value from the shared latent.\n        site, unconstrained_value = self.x_unconstrained.pop()\n        assert name == site[""name""], ""model structure changed""\n        transform = biject_to(fn.support)\n        value = transform(unconstrained_value)\n        logdet = transform.log_abs_det_jacobian(unconstrained_value, value)\n        logdet = sum_rightmost(logdet, logdet.dim() - value.dim() + fn.event_dim)\n        log_density = log_density + fn.log_prob(value) + logdet\n        new_fn = dist.Delta(value, log_density, event_dim=fn.event_dim)\n        return new_fn, value\n\n    def transform_sample(self, latent):\n        """"""\n        Given latent samples from the warped posterior (with a possible batch dimension),\n        return a `dict` of samples from the latent sites in the model.\n\n        :param latent: sample from the warped posterior (possibly batched). Note that the\n            batch dimension must not collide with plate dimensions in the model, i.e.\n            any batch dims `d < - max_plate_nesting`.\n        :return: a `dict` of samples keyed by latent sites in the model.\n        :rtype: dict\n        """"""\n        x_unconstrained = self.transform(latent)\n        transformed_samples = {}\n        for site, value in self.guide._unpack_latent(x_unconstrained):\n            transform = biject_to(site[""fn""].support)\n            x_constrained = transform(value)\n            transformed_samples[site[""name""]] = x_constrained\n        return transformed_samples\n'"
pyro/infer/reparam/reparam.py,2,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABC, abstractmethod\n\nimport torch\n\n\nclass Reparam(ABC):\n    """"""\n    Base class for reparameterizers.\n    """"""\n    @abstractmethod\n    def __call__(self, name, fn, obs):\n        """"""\n        :param str name: A sample site name.\n        :param ~pyro.distributions.TorchDistribution fn: A distribution.\n        :param ~torch.Tensor obs: Observed value or None.\n        :return: A pair (``new_fn``, ``value``).\n        """"""\n        return fn, obs\n\n    def _unwrap(self, fn):\n        """"""\n        Unwrap Independent distributions.\n        """"""\n        event_dim = fn.event_dim\n        while isinstance(fn, torch.distributions.Independent):\n            fn = fn.base_dist\n        return fn, event_dim\n\n    def _wrap(self, fn, event_dim):\n        """"""\n        Wrap in Independent distributions.\n        """"""\n        if fn.event_dim < event_dim:\n            fn = fn.to_event(event_dim - fn.event_dim)\n        assert fn.event_dim == event_dim\n        return fn\n'"
pyro/infer/reparam/split.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom .reparam import Reparam\n\n\nclass SplitReparam(Reparam):\n    """"""\n    Reparameterizer to split a random variable along a dimension, similar to\n    :func:`torch.split`.\n\n    This is useful for treating different parts of a tensor with different\n    reparameterizers or inference methods. For example when performing HMC\n    inference on a time series, you can first apply\n    :class:`~pyro.infer.reparam.discrete_cosine.DiscreteCosineReparam` or\n    :class:`~pyro.infer.reparam.haar.HaarReparam`, then apply\n    :class:`SplitReparam` to split into low-frequency and high-frequency\n    components, and finally add the low-frequency components to the\n    ``full_mass`` matrix together with globals.\n\n    :param sections: Size of a single chunk or list of sizes for\n        each chunk.\n    :type: list(int)\n    :param int dim: Dimension along which to split. Defaults to -1.\n    """"""\n    def __init__(self, sections, dim):\n        assert isinstance(dim, int) and dim < 0\n        assert isinstance(sections, list)\n        assert all(isinstance(size, int) for size in sections)\n        self.event_dim = -dim\n        self.sections = sections\n\n    def __call__(self, name, fn, obs):\n        assert fn.event_dim >= self.event_dim\n        assert obs is None, ""SplitReparam does not support observe statements""\n\n        # Draw independent parts.\n        dim = fn.event_dim - self.event_dim\n        left_shape = fn.event_shape[:dim]\n        right_shape = fn.event_shape[1 + dim:]\n        parts = []\n        for i, size in enumerate(self.sections):\n            event_shape = left_shape + (size,) + right_shape\n            parts.append(pyro.sample(\n                ""{}_split_{}"".format(name, i),\n                dist.ImproperUniform(fn.support, fn.batch_shape, event_shape)))\n        value = torch.cat(parts, dim=-self.event_dim)\n\n        # Combine parts.\n        log_prob = fn.log_prob(value)\n        new_fn = dist.Delta(value, event_dim=fn.event_dim, log_density=log_prob)\n        return new_fn, value\n'"
pyro/infer/reparam/stable.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions.stable import _standard_stable, _unsafe_standard_stable\nfrom pyro.infer.util import is_validation_enabled\n\nfrom .reparam import Reparam\n\n\nclass LatentStableReparam(Reparam):\n    """"""\n    Auxiliary variable reparameterizer for\n    :class:`~pyro.distributions.Stable` latent variables.\n\n    This is useful in inference of latent :class:`~pyro.distributions.Stable`\n    variables because the :meth:`~pyro.distributions.Stable.log_prob` is not\n    implemented.\n\n    This uses the Chambers-Mallows-Stuck method [1], creating a pair of\n    parameter-free auxiliary distributions (``Uniform(-pi/2,pi/2)`` and\n    ``Exponential(1)``) with well-defined ``.log_prob()`` methods, thereby\n    permitting use of reparameterized stable distributions in likelihood-based\n    inference algorithms like SVI and MCMC.\n\n    This reparameterization works only for latent variables, not likelihoods.\n    For likelihood-compatible reparameterization see\n    :class:`SymmetricStableReparam` or :class:`StableReparam` .\n\n    [1] J.P. Nolan (2017).\n        Stable Distributions: Models for Heavy Tailed Data.\n        http://fs2.american.edu/jpnolan/www/stable/chap1.pdf\n    """"""\n    def __call__(self, name, fn, obs):\n        fn, event_dim = self._unwrap(fn)\n        assert isinstance(fn, dist.Stable) and fn.coords == ""S0""\n        assert obs is None, ""LatentStableReparam does not support observe statements""\n\n        # Draw parameter-free noise.\n        proto = fn.stability\n        half_pi = proto.new_tensor(math.pi / 2)\n        one = proto.new_ones(proto.shape)\n        u = pyro.sample(""{}_uniform"".format(name),\n                        self._wrap(dist.Uniform(-half_pi, half_pi).expand(proto.shape), event_dim))\n        e = pyro.sample(""{}_exponential"".format(name),\n                        self._wrap(dist.Exponential(one), event_dim))\n\n        # Differentiably transform.\n        x = _standard_stable(fn.stability, fn.skew, u, e, coords=""S0"")\n        value = fn.loc + fn.scale * x\n\n        # Simulate a pyro.deterministic() site.\n        new_fn = dist.Delta(value, event_dim=event_dim).mask(False)\n        return new_fn, value\n\n\nclass SymmetricStableReparam(Reparam):\n    """"""\n    Auxiliary variable reparameterizer for symmetric\n    :class:`~pyro.distributions.Stable` random variables (i.e. those for which\n    ``skew=0``).\n\n    This is useful in inference of symmetric\n    :class:`~pyro.distributions.Stable` variables because the\n    :meth:`~pyro.distributions.Stable.log_prob` is not implemented.\n\n    This reparameterizes a symmetric :class:`~pyro.distributions.Stable` random\n    variable as a totally-skewed (``skew=1``)\n    :class:`~pyro.distributions.Stable` scale mixture of\n    :class:`~pyro.distributions.Normal` random variables. See Proposition 3. of\n    [1] (but note we differ since :class:`Stable` uses Nolan\'s continuous S0\n    parameterization).\n\n    [1] Alvaro Cartea and Sam Howison (2009)\n        ""Option Pricing with Levy-Stable Processes""\n        https://pdfs.semanticscholar.org/4d66/c91b136b2a38117dd16c2693679f5341c616.pdf\n    """"""\n    def __call__(self, name, fn, obs):\n        fn, event_dim = self._unwrap(fn)\n        assert isinstance(fn, dist.Stable) and fn.coords == ""S0""\n        if is_validation_enabled():\n            if not (fn.skew == 0).all():\n                raise ValueError(""SymmetricStableReparam found nonzero skew"")\n            if not (fn.stability < 2).all():\n                raise ValueError(""SymmetricStableReparam found stability >= 2"")\n\n        # Draw parameter-free noise.\n        proto = fn.stability\n        half_pi = proto.new_tensor(math.pi / 2)\n        one = proto.new_ones(proto.shape)\n        u = pyro.sample(""{}_uniform"".format(name),\n                        self._wrap(dist.Uniform(-half_pi, half_pi).expand(proto.shape), event_dim))\n        e = pyro.sample(""{}_exponential"".format(name),\n                        self._wrap(dist.Exponential(one), event_dim))\n\n        # Differentiably transform to scale drawn from a totally-skewed stable variable.\n        a = fn.stability\n        z = _unsafe_standard_stable(a / 2, 1, u, e, coords=""S"")\n        assert (z >= 0).all()\n        scale = fn.scale * (math.pi / 4 * a).cos().pow(a.reciprocal()) * z.sqrt()\n        scale = scale.clamp(min=torch.finfo(scale.dtype).tiny)\n\n        # Construct a scaled Gaussian, using Stable(2,0,s,m) == Normal(m,s*sqrt(2)).\n        new_fn = self._wrap(dist.Normal(fn.loc, scale * (2 ** 0.5)), event_dim)\n        return new_fn, obs\n\n\nclass StableReparam(Reparam):\n    """"""\n    Auxiliary variable reparameterizer for arbitrary\n    :class:`~pyro.distributions.Stable` random variables.\n\n    This is useful in inference of non-symmetric\n    :class:`~pyro.distributions.Stable` variables because the\n    :meth:`~pyro.distributions.Stable.log_prob` is not implemented.\n\n    This reparameterizes a :class:`~pyro.distributions.Stable` random variable\n    as sum of two other stable random variables, one symmetric and the other\n    totally skewed (applying Property 2.3.a of [1]). The totally skewed\n    variable is sampled as in :class:`LatentStableReparam` , and the symmetric\n    variable is decomposed as in :class:`SymmetricStableReparam` .\n\n    [1] V. M. Zolotarev (1986)\n        ""One-dimensional stable distributions""\n    """"""\n\n    def __call__(self, name, fn, obs):\n        fn, event_dim = self._unwrap(fn)\n        assert isinstance(fn, dist.Stable) and fn.coords == ""S0""\n\n        # Strategy: Let X ~ S0(a,b,s,m) be the stable variable of interest.\n        # 1. WLOG scale and shift so s=1 and m=0, additionally shifting to convert\n        #    from Zolotarev\'s S parameterization to Nolan\'s S0 parameterization.\n        # 2. Decompose X = S + T, where\n        #    S ~ S(a,0,...,0) is symmetric and\n        #    T ~ S(a,sgn(b),...,0) is totally skewed.\n        # 3. Decompose S = G * sqrt(Z) via the symmetric strategy, where\n        #    Z ~ S(a/2,1,...,0) is totally-skewed and\n        #    G ~ Normal(0,1) is Gaussian.\n        # 4. Defer the totally-skewed Z and T to the Chambers-Mallows-Stuck\n        #    strategy: Z = f(Unif,Exp), T = f(Unif,Exp).\n        #\n        # To derive the parameters of S and T, we solve the equations\n        #\n        #   T.stability = a            S.stability = a\n        #   T.skew = sgn(b)            S.skew = 0\n        #   T.loc = 0                  S.loc = 0\n        #\n        #   s = (S.scale**a + T.scale**a)**(1/a) = 1       # by step 1.\n        #\n        #       S.skew * S.scale**a + T.skew * T.scale**a\n        #   b = ----------------------------------------- = sgn(b) * T.scale**a\n        #                S.scale**a + T.scale**a\n        # yielding\n        #\n        #   T.scale = |b| ** (1/a)     S.scale = (1 - |b|) ** (1/a)\n\n        # Draw parameter-free noise.\n        proto = fn.stability\n        half_pi = proto.new_tensor(math.pi / 2)\n        one = proto.new_ones(proto.shape)\n        zu = pyro.sample(""{}_z_uniform"".format(name),\n                         self._wrap(dist.Uniform(-half_pi, half_pi).expand(proto.shape), event_dim))\n        ze = pyro.sample(""{}_z_exponential"".format(name),\n                         self._wrap(dist.Exponential(one), event_dim))\n        tu = pyro.sample(""{}_t_uniform"".format(name),\n                         self._wrap(dist.Uniform(-half_pi, half_pi).expand(proto.shape), event_dim))\n        te = pyro.sample(""{}_t_exponential"".format(name),\n                         self._wrap(dist.Exponential(one), event_dim))\n\n        # Differentiably transform.\n        a = fn.stability\n        z = _unsafe_standard_stable(a / 2, 1, zu, ze, coords=""S"")\n        t = _standard_stable(a, one, tu, te, coords=""S0"")\n        a_inv = a.reciprocal()\n        eps = torch.finfo(a.dtype).eps\n        skew_abs = fn.skew.abs().clamp(min=eps, max=1 - eps)\n        t_scale = skew_abs.pow(a_inv)\n        s_scale = (1 - skew_abs).pow(a_inv)\n        shift = _safe_shift(a, fn.skew, t_scale, skew_abs)\n        loc = fn.loc + fn.scale * (fn.skew.sign() * t_scale * t + shift)\n        scale = fn.scale * s_scale * z.sqrt() * (math.pi / 4 * a).cos().pow(a_inv)\n        scale = scale.clamp(min=torch.finfo(scale.dtype).tiny)\n\n        # Construct a scaled Gaussian, using Stable(2,0,s,m) == Normal(m,s*sqrt(2)).\n        new_fn = self._wrap(dist.Normal(loc, scale * (2 ** 0.5)), event_dim)\n        return new_fn, obs\n\n\ndef _unsafe_shift(a, skew, t_scale):\n    # At a=1 the lhs has a root and the rhs has an asymptote.\n    return (skew.sign() * t_scale - skew) * (math.pi / 2 * a).tan()\n\n\ndef _safe_shift(a, skew, t_scale, skew_abs):\n    radius = 0.005\n    hole = 1.0\n    with torch.no_grad():\n        near_hole = (a - hole).abs() <= radius\n    if not near_hole.any():\n        return _unsafe_shift(a, skew, t_scale)\n\n    # Avoid the hole at a=1 by interpolating between points on either side.\n    a_ = a.unsqueeze(-1).expand(a.shape + (2,)).contiguous()\n    with torch.no_grad():\n        lb, ub = a_.data.unbind(-1)\n        lb[near_hole] = hole - radius\n        ub[near_hole] = hole + radius\n        # We don\'t need to backprop through weights, since we\'ve pretended\n        # a_ is reparametrized, even though we\'ve clamped some values.\n        weights = (a_ - a.unsqueeze(-1)).abs_().mul_(-1 / (2 * radius)).add_(1)\n        weights[~near_hole] = 0.5\n    skew_ = skew.unsqueeze(-1)\n    skew_abs_ = skew_abs.unsqueeze(-1)\n    t_scale_ = skew_abs_.pow(a_.reciprocal())\n    pairs = _unsafe_shift(a_, skew_, t_scale_)\n    return (pairs * weights).sum(-1)\n'"
pyro/infer/reparam/studentt.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom .reparam import Reparam\n\n\nclass StudentTReparam(Reparam):\n    """"""\n    Auxiliary variable reparameterizer for\n    :class:`~pyro.distributions.StudentT` random variables.\n\n    This is useful in combination with\n    :class:`~pyro.infer.reparam.hmm.LinearHMMReparam` because it allows\n    StudentT processes to be treated as conditionally Gaussian processes,\n    permitting cheap inference via :class:`~pyro.distributions.GaussianHMM` .\n\n    This reparameterizes a :class:`~pyro.distributions.StudentT` by introducing\n    an auxiliary :class:`~pyro.distributions.Gamma` variable conditioned on\n    which the result is :class:`~pyro.distributions.Normal` .\n    """"""\n    def __call__(self, name, fn, obs):\n        fn, event_dim = self._unwrap(fn)\n        assert isinstance(fn, dist.StudentT)\n\n        # Draw a sample that depends only on df.\n        half_df = fn.df * 0.5\n        gamma = pyro.sample(""{}_gamma"".format(name),\n                            self._wrap(dist.Gamma(half_df, half_df), event_dim))\n\n        # Construct a scaled Normal.\n        loc = fn.loc\n        scale = fn.scale * gamma.rsqrt()\n        new_fn = self._wrap(dist.Normal(loc, scale), event_dim)\n        return new_fn, obs\n'"
pyro/infer/reparam/transform.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom .reparam import Reparam\n\n\nclass TransformReparam(Reparam):\n    """"""\n    Reparameterizer for\n    :class:`pyro.distributions.torch.TransformedDistribution` latent variables.\n\n    This is useful for transformed distributions with complex,\n    geometry-changing transforms, where the posterior has simple shape in\n    the space of ``base_dist``.\n\n    This reparameterization works only for latent variables, not likelihoods.\n    """"""\n    def __call__(self, name, fn, obs):\n        assert obs is None, ""TransformReparam does not support observe statements""\n        assert isinstance(fn, dist.TransformedDistribution)\n\n        # Draw noise from the base distribution.\n        x = pyro.sample(""{}_base"".format(name), fn.base_dist)\n\n        # Differentiably transform.\n        for t in fn.transforms:\n            x = t(x)\n\n        # Simulate a pyro.deterministic() site.\n        new_fn = dist.Delta(x, event_dim=fn.event_dim)\n        return new_fn, x\n'"
pyro/infer/reparam/unit_jacobian.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch.distributions import biject_to\nfrom torch.distributions.transforms import ComposeTransform\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom .reparam import Reparam\n\n\n# TODO Replace with .with_cache() once the following is released:\n# https://github.com/probtorch/pytorch/pull/153\ndef _with_cache(t):\n    return t.with_cache() if hasattr(t, ""with_cache"") else t\n\n\nclass UnitJacobianReparam(Reparam):\n    """"""\n    Reparameterizer for :class:`~torch.distributions.transforms.Transform`\n    objects whose Jacobian determinant is one.\n\n    :param transform: A transform whose Jacobian has determinant 1.\n    :type transform: ~torch.distributions.transforms.Transform\n    :param str suffix: A suffix to append to the transformed site.\n    """"""\n    def __init__(self, transform, suffix=""transformed""):\n        self.transform = _with_cache(transform)\n        self.suffix = suffix\n\n    def __call__(self, name, fn, obs):\n        assert obs is None, ""TransformReparam does not support observe statements""\n        assert fn.event_dim >= self.transform.event_dim, (\n            ""Cannot transform along batch dimension; ""\n            ""try converting a batch dimension to an event dimension"")\n\n        # Draw noise from the base distribution.\n        transform = ComposeTransform([_with_cache(biject_to(fn.support).inv),\n                                      self.transform])\n        x_trans = pyro.sample(""{}_{}"".format(name, self.suffix),\n                              dist.TransformedDistribution(fn, transform))\n\n        # Differentiably transform.\n        x = transform.inv(x_trans)  # should be free due to transform cache\n\n        # Simulate a pyro.deterministic() site.\n        new_fn = dist.Delta(x, event_dim=fn.event_dim)\n        return new_fn, x\n'"
pyro/ops/einsum/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport opt_einsum\n\nfrom pyro.util import ignore_jit_warnings\n\n_PATH_CACHE = {}\n\n\ndef contract_expression(equation, *shapes, **kwargs):\n    """"""\n    Wrapper around :func:`opt_einsum.contract_expression` that optionally uses\n    Pyro\'s cheap optimizer and optionally caches contraction paths.\n\n    :param bool cache_path: whether to cache the contraction path.\n        Defaults to True.\n    """"""\n    # memoize the contraction path\n    cache_path = kwargs.pop(\'cache_path\', True)\n    if cache_path:\n        kwargs_key = tuple(kwargs.items())\n        key = equation, shapes, kwargs_key\n        if key in _PATH_CACHE:\n            return _PATH_CACHE[key]\n\n    expr = opt_einsum.contract_expression(equation, *shapes, **kwargs)\n    if cache_path:\n        _PATH_CACHE[key] = expr\n    return expr\n\n\ndef contract(equation, *operands, **kwargs):\n    """"""\n    Wrapper around :func:`opt_einsum.contract` that optionally uses Pyro\'s\n    cheap optimizer and optionally caches contraction paths.\n\n    :param bool cache_path: whether to cache the contraction path.\n        Defaults to True.\n    """"""\n    backend = kwargs.pop(\'backend\', \'numpy\')\n    out = kwargs.pop(\'out\', None)\n    shapes = [tuple(t.shape) for t in operands]\n    with ignore_jit_warnings():\n        expr = contract_expression(equation, *shapes)\n        return expr(*operands, backend=backend, out=out)\n\n\n__all__ = [\'contract\', \'contract_expression\']\n'"
pyro/ops/einsum/adjoint.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport weakref\nfrom abc import ABCMeta, abstractmethod\n\nimport torch\n\nfrom pyro.ops import packed\nfrom pyro.util import jit_iter\n\nSAMPLE_SYMBOL = "" ""  # must be unique and precede alphanumeric characters\n\n\nclass Backward(object, metaclass=ABCMeta):\n    is_leaf = False\n\n    def __call__(self):\n        """"""\n        Performs entire backward pass in depth-first order.\n        """"""\n        message = None\n        stack = [(self, message)]\n        while stack:\n            bwd, message = stack.pop()\n            stack.extend(bwd.process(message))\n\n    @abstractmethod\n    def process(self, message):\n        raise NotImplementedError\n\n\nclass _LeafBackward(Backward):\n    is_leaf = True\n\n    def __init__(self, target):\n        self.target = weakref.ref(target)\n\n    def process(self, message):\n        target = self.target()\n        assert message is not target, \'memory leak\'\n        target._pyro_backward_result = message\n        return ()\n\n\ndef require_backward(tensor):\n    """"""\n    Marks a tensor as a leaf in the adjoint graph.\n    """"""\n    tensor._pyro_backward = _LeafBackward(tensor)\n\n\nclass _TransposeBackward(Backward):\n    def __init__(self, a, axes):\n        self.a = a\n        self.axes = axes\n\n    def process(self, message):\n        if message is None:\n            yield self.a._pyro_backward, None\n        else:\n            inv_axes = [None] * len(self.axes)\n            for i, j in enumerate(self.axes):\n                inv_axes[j] = i\n            yield self.a._pyro_backward, message.permute(inv_axes)\n\n\n# this requires https://github.com/dgasmith/opt_einsum/pull/74\ndef transpose(a, axes):\n    result = a.permute(axes)\n    if hasattr(a, \'_pyro_backward\'):\n        result._pyro_backward = _TransposeBackward(a, axes)\n        result._pyro_name = getattr(a, \'_pyro_name\', \'?\') + ""\'""\n    return result\n\n\ndef einsum_backward_sample(operands, sample1, sample2):\n    """"""\n    Cuts down samples to pass on to subsequent steps.\n    This is used in various ``_EinsumBackward.__call__()`` methods.\n    This assumes all operands have a ``._pyro_dims`` attribute set.\n    """"""\n    # Combine upstream sample with sample at this site.\n    if sample1 is None:\n        sample = sample2\n    elif sample2 is None:\n        sample = sample1\n    else:\n        # Slice sample1 down based on choices in sample2.\n        assert set(sample1._pyro_sample_dims).isdisjoint(sample2._pyro_sample_dims)\n        sample_dims = sample1._pyro_sample_dims + sample2._pyro_sample_dims\n        for dim, index in zip(sample2._pyro_sample_dims, jit_iter(sample2)):\n            if dim in sample1._pyro_dims:\n                index._pyro_dims = sample2._pyro_dims[1:]\n                sample1 = packed.gather(sample1, index, dim)\n\n        # Concatenate the two samples.\n        parts = packed.broadcast_all(sample1, sample2)\n        sample = torch.cat(parts)\n        sample._pyro_dims = parts[0]._pyro_dims\n        sample._pyro_sample_dims = sample_dims\n        assert sample.dim() == len(sample._pyro_dims)\n        if not torch._C._get_tracing_state():\n            assert sample.size(0) == len(sample._pyro_sample_dims)\n\n    # Select sample dimensions to pass on to downstream sites.\n    for x in operands:\n        if not hasattr(x, \'_pyro_backward\'):\n            continue\n        if sample is None:\n            yield x._pyro_backward, None\n            continue\n        x_sample_dims = set(x._pyro_dims) & set(sample._pyro_sample_dims)\n        if not x_sample_dims:\n            yield x._pyro_backward, None\n            continue\n        if x_sample_dims == set(sample._pyro_sample_dims):\n            yield x._pyro_backward, sample\n            continue\n        x_sample_dims = \'\'.join(sorted(x_sample_dims))\n        x_sample = sample[[sample._pyro_sample_dims.index(dim)\n                           for dim in x_sample_dims]]\n        x_sample._pyro_dims = sample._pyro_dims\n        x_sample._pyro_sample_dims = x_sample_dims\n        assert x_sample.dim() == len(x_sample._pyro_dims)\n        if not torch._C._get_tracing_state():\n            assert x_sample.size(0) == len(x_sample._pyro_sample_dims)\n        yield x._pyro_backward, x_sample\n\n\ndef unflatten(flat_sample, output_dims, contract_dims, contract_shape):\n    """"""\n    Unpack a collection of indices that have been packed into a 64-bit\n    tensor, via modular arithmetic.\n    """"""\n    assert contract_dims\n    sample = flat_sample.unsqueeze(0)\n    if len(contract_dims) > 1:\n        slices = [None] * len(contract_dims)\n        for i, size in reversed(list(enumerate(contract_shape))):\n            slices[i] = sample % size\n            sample = sample // size\n        sample = torch.cat(slices)\n    sample._pyro_dims = SAMPLE_SYMBOL + output_dims\n    sample._pyro_sample_dims = contract_dims\n    assert sample.dim() == len(sample._pyro_dims)\n    if not torch._C._get_tracing_state():\n        assert sample.size(0) == len(sample._pyro_sample_dims)\n    return sample\n'"
pyro/ops/einsum/torch_log.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.ops.einsum.util import Tensordot\nfrom pyro.ops.special import safe_log\n\n\ndef transpose(a, axes):\n    return a.permute(axes)\n\n\ndef einsum(equation, *operands):\n    """"""\n    Log-sum-exp implementation of einsum.\n    """"""\n    # rename symbols to support PyTorch 0.4.1 and earlier,\n    # which allow only symbols a-z.\n    symbols = sorted(set(equation) - set(\',->\'))\n    rename = dict(zip(symbols, \'abcdefghijklmnopqrstuvwxyz\'))\n    equation = \'\'.join(rename.get(s, s) for s in equation)\n\n    inputs, output = equation.split(\'->\')\n    if inputs == output:\n        return operands[0][...]  # create a new object\n    inputs = inputs.split(\',\')\n\n    shifts = []\n    exp_operands = []\n    for dims, operand in zip(inputs, operands):\n        shift = operand\n        for i, dim in enumerate(dims):\n            if dim not in output:\n                shift = shift.max(i, keepdim=True)[0]\n        # avoid nan due to -inf - -inf\n        shift = shift.clamp(min=torch.finfo(shift.dtype).min)\n        exp_operands.append((operand - shift).exp())\n\n        # permute shift to match output\n        shift = shift.reshape(torch.Size(size for size, dim in zip(operand.shape, dims)\n                                         if dim in output))\n        if shift.dim():\n            shift = shift.reshape((1,) * (len(output) - shift.dim()) + shift.shape)\n            dims = [dim for dim in dims if dim in output]\n            dims = [dim for dim in output if dim not in dims] + dims\n            shift = shift.permute(*(dims.index(dim) for dim in output))\n        shifts.append(shift)\n\n    result = safe_log(torch.einsum(equation, exp_operands))\n    return sum(shifts + [result])\n\n\ntensordot = Tensordot(einsum)\n\n__all__ = [""transpose"", ""einsum"", ""tensordot""]\n'"
pyro/ops/einsum/torch_map.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport operator\n\nfrom functools import reduce\n\nfrom pyro.ops import packed\nfrom pyro.ops.einsum.adjoint import Backward, einsum_backward_sample, transpose, unflatten\nfrom pyro.ops.einsum.util import Tensordot\n\n\nclass _EinsumBackward(Backward):\n    def __init__(self, operands, argmax):\n        self.operands = operands\n        self.argmax = argmax\n\n    def process(self, message):\n        sample1 = self.argmax\n        sample2 = message\n        return einsum_backward_sample(self.operands, sample1, sample2)\n\n\ndef einsum(equation, *operands):\n    """"""\n    Forward-max-sum backward-argmax implementation of einsum.\n    This assumes all operands have a ``._pyro_dims`` attribute set.\n    """"""\n    equation = packed.rename_equation(equation, *operands)\n    inputs, output = equation.split(\'->\')\n    any_requires_backward = any(hasattr(x, \'_pyro_backward\') for x in operands)\n\n    contract_dims = \'\'.join(sorted(set().union(*(x._pyro_dims for x in operands)) - set(output)))\n    dims = output + contract_dims\n    result = reduce(operator.add, packed.broadcast_all(*operands, dims=dims))\n    argmax = None  # work around lack of pytorch support for zero-sized tensors\n    if contract_dims:\n        output_shape = result.shape[:len(output)]\n        contract_shape = result.shape[len(output):]\n        result, argmax = result.reshape(output_shape + (-1,)).max(-1)\n        if any_requires_backward:\n            argmax = unflatten(argmax, output, contract_dims, contract_shape)\n    elif result is operands[0]:\n        result = result[...]  # create a new object\n    result._pyro_dims = output\n    assert result.dim() == len(result._pyro_dims)\n\n    if any_requires_backward:\n        result._pyro_backward = _EinsumBackward(operands, argmax)\n    return result\n\n\ntensordot = Tensordot(einsum)\n\n__all__ = [""transpose"", ""einsum"", ""tensordot""]\n'"
pyro/ops/einsum/torch_marginal.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro.ops.einsum.torch_log\nfrom pyro.ops.einsum.adjoint import Backward, transpose\nfrom pyro.ops.einsum.util import Tensordot\n\n\nclass _EinsumBackward(Backward):\n    def __init__(self, equation, operands):\n        self.equation = equation\n        self.operands = operands\n\n    def process(self, message):\n        # Create extended lists of inputs and operands.\n        operands = list(self.operands)\n        inputs, output = self.equation.split(\'->\')\n        inputs = inputs.split(\',\')\n        if message is not None:\n            assert message.dim() == len(output)\n            inputs.append(output)\n            operands.append(message)\n\n        # Aggregate all messages and pass backward.\n        for i, operand in enumerate(self.operands):\n            if not hasattr(operand, ""_pyro_backward""):\n                continue\n            output_i = inputs[i]\n            inputs_i = list(inputs)\n            operands_i = list(operands)\n            if not operand._pyro_backward.is_leaf:\n                del inputs_i[i]\n                del operands_i[i]\n            if operands_i:\n                inputs_i = \',\'.join(inputs_i)\n                output_i = \'\'.join(dim for dim in output_i if dim in inputs_i)\n                equation = inputs_i + \'->\' + output_i\n                message_i = pyro.ops.einsum.torch_log.einsum(equation, *operands_i)\n                if output_i != inputs[i]:\n                    for pos, dim in enumerate(inputs[i]):\n                        if dim not in output_i:\n                            message_i = message_i.unsqueeze(pos)\n                    message_i = message_i.expand_as(operands[i])\n            else:\n                message_i = None\n            yield operand._pyro_backward, message_i\n\n\ndef einsum(equation, *operands):\n    """"""\n    Forward-log-sum-product-exp backward-marginal implementation of einsum.\n    """"""\n    result = pyro.ops.einsum.torch_log.einsum(equation, *operands)\n\n    if any(hasattr(x, \'_pyro_backward\') for x in operands):\n        result._pyro_backward = _EinsumBackward(equation, operands)\n    return result\n\n\ntensordot = Tensordot(einsum)\n\n__all__ = [""transpose"", ""einsum"", ""tensordot""]\n'"
pyro/ops/einsum/torch_sample.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport operator\n\nfrom functools import reduce\n\nimport pyro.distributions as dist\nimport pyro.ops.einsum.torch_log\nfrom pyro.ops import packed\nfrom pyro.ops.einsum.adjoint import Backward, einsum_backward_sample, transpose, unflatten\nfrom pyro.ops.einsum.util import Tensordot\nfrom pyro.util import jit_iter\n\n\nclass _EinsumBackward(Backward):\n    def __init__(self, output, operands):\n        self.output = output\n        self.operands = operands\n\n    def process(self, message):\n        output = self.output\n        operands = list(self.operands)\n        contract_dims = \'\'.join(sorted(set().union(*(x._pyro_dims for x in operands)) - set(output)))\n        batch_dims = output\n\n        # Slice down operands before combining terms.\n        sample2 = message\n        if sample2 is not None:\n            for dim, index in zip(sample2._pyro_sample_dims, jit_iter(sample2)):\n                batch_dims = batch_dims.replace(dim, \'\')\n                for i, x in enumerate(operands):\n                    if dim in x._pyro_dims:\n                        index._pyro_dims = sample2._pyro_dims[1:]\n                        x = packed.gather(x, index, dim)\n                    operands[i] = x\n\n        # Combine terms.\n        dims = batch_dims + contract_dims\n        logits = reduce(operator.add, packed.broadcast_all(*operands, dims=dims))\n\n        # Sample.\n        sample1 = None  # work around lack of pytorch support for zero-sized tensors\n        if contract_dims:\n            output_shape = logits.shape[:len(batch_dims)]\n            contract_shape = logits.shape[len(batch_dims):]\n            flat_logits = logits.reshape(output_shape + (-1,))\n            flat_sample = dist.Categorical(logits=flat_logits).sample()\n            sample1 = unflatten(flat_sample, batch_dims, contract_dims, contract_shape)\n\n        # Cut down samples to pass on to subsequent steps.\n        return einsum_backward_sample(self.operands, sample1, sample2)\n\n\ndef einsum(equation, *operands):\n    """"""\n    Forward-log-sum-product-exp backward-sample-exp implementation of einsum.\n    This assumes all operands have a ``._pyro_dims`` attribute set.\n    """"""\n    equation = packed.rename_equation(equation, *operands)\n    inputs, output = equation.split(\'->\')\n    result = pyro.ops.einsum.torch_log.einsum(equation, *operands)\n    result._pyro_dims = output\n    assert result.dim() == len(result._pyro_dims)\n\n    if any(hasattr(x, \'_pyro_backward\') for x in operands):\n        result._pyro_backward = _EinsumBackward(output, operands)\n    return result\n\n\ntensordot = Tensordot(einsum)\n\n__all__ = [""transpose"", ""einsum"", ""tensordot""]\n'"
pyro/ops/einsum/util.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0 AND MIT\n\nEINSUM_SYMBOLS_BASE = \'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\'\n\n\nclass Tensordot:\n    """"""\n    Creates a tensordot implementation from an einsum implementation.\n    """"""\n    def __init__(self, einsum):\n        self.einsum = einsum\n\n    # Copyright (c) 2014 Daniel Smith\n    # SPDX-License-Identifier: MIT\n    # This function is copied and adapted from:\n    # https://github.com/dgasmith/opt_einsum/blob/a6dd686/opt_einsum/backends/torch.py\n    def __call__(self, x, y, axes=2):\n        xnd = x.ndimension()\n        ynd = y.ndimension()\n\n        # convert int argument to (list[int], list[int])\n        if isinstance(axes, int):\n            axes = range(xnd - axes, xnd), range(axes)\n\n        # convert (int, int) to (list[int], list[int])\n        if isinstance(axes[0], int):\n            axes = (axes[0],), axes[1]\n        if isinstance(axes[1], int):\n            axes = axes[0], (axes[1],)\n\n        # initialize empty indices\n        x_ix = [None] * xnd\n        y_ix = [None] * ynd\n        out_ix = []\n\n        # fill in repeated indices\n        available_ix = iter(EINSUM_SYMBOLS_BASE)\n        for ax1, ax2 in zip(*axes):\n            repeat = next(available_ix)\n            x_ix[ax1] = repeat\n            y_ix[ax2] = repeat\n\n        # fill in the rest, and maintain output order\n        for i in range(xnd):\n            if x_ix[i] is None:\n                leave = next(available_ix)\n                x_ix[i] = leave\n                out_ix.append(leave)\n        for i in range(ynd):\n            if y_ix[i] is None:\n                leave = next(available_ix)\n                y_ix[i] = leave\n                out_ix.append(leave)\n\n        # form full string and contract!\n        einsum_str = ""{},{}->{}"".format(*map("""".join, (x_ix, y_ix, out_ix)))\n        return self.einsum(einsum_str, x, y)\n'"
tests/contrib/autoguide/test_inference.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nfrom functools import partial\n\nimport numpy as np\nimport pytest\nimport torch\nfrom torch.distributions import biject_to, constraints\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim as optim\nfrom pyro.distributions.transforms import iterated, block_autoregressive\nfrom pyro.infer.autoguide import (AutoDiagonalNormal, AutoIAFNormal, AutoLaplaceApproximation,\n                                  AutoLowRankMultivariateNormal, AutoMultivariateNormal)\nfrom pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\nfrom pyro.infer.autoguide.guides import AutoNormalizingFlow\nfrom tests.common import assert_equal\nfrom tests.integration_tests.test_conjugate_gaussian_models import GaussianChain\n\nlogger = logging.getLogger(__name__)\npytestmark = pytest.mark.stage(""integration"", ""integration_batch_2"")\n\n\n# conjugate model to test AutoGuide logic from end-to-end (this has a non-mean-field posterior)\nclass AutoGaussianChain(GaussianChain):\n\n    # this is gross but we need to convert between different posterior factorizations\n    def compute_target(self, N):\n        self.target_auto_mus = torch.zeros(N + 1)\n        self.target_auto_diag_cov = torch.zeros(N + 1)\n        self.target_auto_mus[-1] = self.target_mus[N].item()\n        self.target_auto_diag_cov[-1] = 1.0 / self.lambda_posts[-1].item()\n        for n in range(N - 1, 0, -1):\n            self.target_auto_mus[n] += self.target_mus[n].item()\n            self.target_auto_mus[n] += self.target_kappas[n].item() * self.target_auto_mus[n + 1]\n            self.target_auto_diag_cov[n] += 1.0 / self.lambda_posts[n].item()\n            self.target_auto_diag_cov[n] += (self.target_kappas[n].item() ** 2) * self.target_auto_diag_cov[n + 1]\n\n    def test_multivariatate_normal_auto(self):\n        self.do_test_auto(3, reparameterized=True, n_steps=10001)\n\n    def do_test_auto(self, N, reparameterized, n_steps):\n        logger.debug(""\\nGoing to do AutoGaussianChain test..."")\n        pyro.clear_param_store()\n        self.setUp()\n        self.setup_chain(N)\n        self.compute_target(N)\n        self.guide = AutoMultivariateNormal(self.model)\n        logger.debug(""target auto_loc: {}""\n                     .format(self.target_auto_mus[1:].detach().cpu().numpy()))\n        logger.debug(""target auto_diag_cov: {}""\n                     .format(self.target_auto_diag_cov[1:].detach().cpu().numpy()))\n\n        # TODO speed up with parallel num_particles > 1\n        adam = optim.Adam({""lr"": .001, ""betas"": (0.95, 0.999)})\n        svi = SVI(self.model, self.guide, adam, loss=Trace_ELBO())\n\n        for k in range(n_steps):\n            loss = svi.step(reparameterized)\n            assert np.isfinite(loss), loss\n\n            if k % 1000 == 0 and k > 0 or k == n_steps - 1:\n                logger.debug(""[step {}] guide mean parameter: {}""\n                             .format(k, self.guide.loc.detach().cpu().numpy()))\n                L = self.guide.scale_tril\n                diag_cov = torch.mm(L, L.t()).diag()\n                logger.debug(""[step {}] auto_diag_cov: {}""\n                             .format(k, diag_cov.detach().cpu().numpy()))\n\n        assert_equal(self.guide.loc.detach(), self.target_auto_mus[1:], prec=0.05,\n                     msg=""guide mean off"")\n        assert_equal(diag_cov, self.target_auto_diag_cov[1:], prec=0.07,\n                     msg=""guide covariance off"")\n\n\n@pytest.mark.parametrize(\'auto_class\', [AutoDiagonalNormal, AutoMultivariateNormal,\n                                        AutoLowRankMultivariateNormal, AutoLaplaceApproximation])\n@pytest.mark.parametrize(\'Elbo\', [Trace_ELBO, TraceMeanField_ELBO])\ndef test_auto_diagonal_gaussians(auto_class, Elbo):\n    n_steps = 3501 if auto_class == AutoDiagonalNormal else 6001\n\n    def model():\n        pyro.sample(""x"", dist.Normal(-0.2, 1.2))\n        pyro.sample(""y"", dist.Normal(0.2, 0.7))\n\n    if auto_class is AutoLowRankMultivariateNormal:\n        guide = auto_class(model, rank=1)\n    else:\n        guide = auto_class(model)\n    adam = optim.Adam({""lr"": .001, ""betas"": (0.95, 0.999)})\n    svi = SVI(model, guide, adam, loss=Elbo())\n\n    for k in range(n_steps):\n        loss = svi.step()\n        assert np.isfinite(loss), loss\n\n    if auto_class is AutoLaplaceApproximation:\n        guide = guide.laplace_approximation()\n\n    loc, scale = guide._loc_scale()\n\n    expected_loc = torch.tensor([-0.2, 0.2])\n    assert_equal(loc.detach(), expected_loc, prec=0.05,\n                 msg=""\\n"".join([""Incorrect guide loc. Expected:"",\n                                str(expected_loc.cpu().numpy()),\n                                ""Actual:"",\n                                str(loc.detach().cpu().numpy())]))\n    expected_scale = torch.tensor([1.2, 0.7])\n    assert_equal(scale.detach(), expected_scale, prec=0.08,\n                 msg=""\\n"".join([""Incorrect guide scale. Expected:"",\n                                str(expected_scale.cpu().numpy()),\n                                ""Actual:"",\n                                str(scale.detach().cpu().numpy())]))\n\n\n@pytest.mark.parametrize(\'auto_class\', [AutoDiagonalNormal, AutoMultivariateNormal,\n                                        AutoLowRankMultivariateNormal, AutoLaplaceApproximation])\ndef test_auto_transform(auto_class):\n    n_steps = 3500\n\n    def model():\n        pyro.sample(""x"", dist.LogNormal(0.2, 0.7))\n\n    if auto_class is AutoLowRankMultivariateNormal:\n        guide = auto_class(model, rank=1)\n    else:\n        guide = auto_class(model)\n    adam = optim.Adam({""lr"": .001, ""betas"": (0.90, 0.999)})\n    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n\n    for k in range(n_steps):\n        loss = svi.step()\n        assert np.isfinite(loss), loss\n\n    if auto_class is AutoLaplaceApproximation:\n        guide = guide.laplace_approximation()\n\n    loc, scale = guide._loc_scale()\n    assert_equal(loc.detach(), torch.tensor([0.2]), prec=0.04,\n                 msg=""guide mean off"")\n    assert_equal(scale.detach(), torch.tensor([0.7]), prec=0.04,\n                 msg=""guide covariance off"")\n\n\n@pytest.mark.parametrize(\'auto_class\', [\n    AutoDiagonalNormal,\n    AutoIAFNormal,\n    AutoMultivariateNormal,\n    AutoLowRankMultivariateNormal,\n    AutoLaplaceApproximation,\n    lambda m: AutoNormalizingFlow(m, partial(iterated, 2, block_autoregressive)),\n])\n@pytest.mark.parametrize(\'Elbo\', [Trace_ELBO, TraceMeanField_ELBO])\ndef test_auto_dirichlet(auto_class, Elbo):\n    num_steps = 2000\n    prior = torch.tensor([0.5, 1.0, 1.5, 3.0])\n    data = torch.tensor([0] * 4 + [1] * 2 + [2] * 5).long()\n    posterior = torch.tensor([4.5, 3.0, 6.5, 3.0])\n\n    def model(data):\n        p = pyro.sample(""p"", dist.Dirichlet(prior))\n        with pyro.plate(""data_plate""):\n            pyro.sample(""data"", dist.Categorical(p).expand_by(data.shape), obs=data)\n\n    guide = auto_class(model)\n    svi = SVI(model, guide, optim.Adam({""lr"": .003}), loss=Elbo())\n\n    for _ in range(num_steps):\n        loss = svi.step(data)\n        assert np.isfinite(loss), loss\n\n    expected_mean = posterior / posterior.sum()\n    if isinstance(guide, (AutoIAFNormal, AutoNormalizingFlow)):\n        loc = guide.transform(torch.zeros(guide.latent_dim))\n    else:\n        loc = guide.loc\n    actual_mean = biject_to(constraints.simplex)(loc)\n    assert_equal(actual_mean, expected_mean, prec=0.2, msg=\'\'.join([\n        \'\\nexpected {}\'.format(expected_mean.detach().cpu().numpy()),\n        \'\\n  actual {}\'.format(actual_mean.detach().cpu().numpy())]))\n'"
tests/contrib/autoguide/test_mean_field_entropy.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport scipy.special as sc\nimport pytest\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.autoguide.utils import mean_field_entropy\nfrom tests.common import assert_equal\n\n\ndef mean_field_guide(batch_tensor, design):\n    # A batched variable\n    w_p = pyro.param(""w_p"", 0.2*torch.ones(batch_tensor.shape))\n    u_p = pyro.param(""u_p"", 0.5*torch.ones(batch_tensor.shape))\n    pyro.sample(""w"", dist.Bernoulli(w_p))\n    pyro.sample(""u"", dist.Bernoulli(u_p))\n\n\ndef h(p):\n    return -(sc.xlogy(p, p) + sc.xlog1py(1 - p, -p))\n\n\n@pytest.mark.parametrize(""guide,args,expected_entropy"", [\n    (mean_field_guide, (torch.Tensor([0.]), None), torch.Tensor([h(0.2) + h(0.5)])),\n    (mean_field_guide, (torch.eye(2), None), (h(0.2) + h(0.5))*torch.ones(2, 2))\n])\ndef test_guide_entropy(guide, args, expected_entropy):\n    assert_equal(mean_field_entropy(guide, args), expected_entropy)\n'"
tests/contrib/autoname/test_named.py,15,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.contrib.autoname import named\n\n\ndef get_sample_names(tr):\n    return set([name for name, site in tr.nodes.items()\n                if site[""type""] == ""sample"" and not site[""is_observed""]])\n\n\ndef get_observe_names(tr):\n    return set([name for name, site in tr.nodes.items()\n                if site[""type""] == ""sample"" and site[""is_observed""]])\n\n\ndef get_param_names(tr):\n    return set([name for name, site in tr.nodes.items() if site[""type""] == ""param""])\n\n\ndef test_named_object():\n    pyro.clear_param_store()\n\n    def model():\n        latent = named.Object(""latent"")\n        loc = latent.loc.param_(torch.zeros(1))\n        foo = latent.foo.sample_(dist.Normal(loc, torch.ones(1)))\n        latent.bar.sample_(dist.Normal(loc, torch.ones(1)), obs=foo)\n        latent.x.z.sample_(dist.Normal(loc, torch.ones(1)))\n\n    tr = poutine.trace(model).get_trace()\n    assert get_sample_names(tr) == set([""latent.foo"", ""latent.x.z""])\n    assert get_observe_names(tr) == set([""latent.bar""])\n    assert get_param_names(tr) == set([""latent.loc""])\n\n\ndef test_named_list():\n    pyro.clear_param_store()\n\n    def model():\n        latent = named.List(""latent"")\n        loc = latent.add().param_(torch.zeros(1))\n        foo = latent.add().sample_(dist.Normal(loc, torch.ones(1)))\n        latent.add().sample_(dist.Normal(loc, torch.ones(1)), obs=foo)\n        latent.add().z.sample_(dist.Normal(loc, torch.ones(1)))\n\n    tr = poutine.trace(model).get_trace()\n    assert get_sample_names(tr) == set([""latent[1]"", ""latent[3].z""])\n    assert get_observe_names(tr) == set([""latent[2]""])\n    assert get_param_names(tr) == set([""latent[0]""])\n\n\ndef test_named_dict():\n    pyro.clear_param_store()\n\n    def model():\n        latent = named.Dict(""latent"")\n        loc = latent[""loc""].param_(torch.zeros(1))\n        foo = latent[""foo""].sample_(dist.Normal(loc, torch.ones(1)))\n        latent[""bar""].sample_(dist.Normal(loc, torch.ones(1)), obs=foo)\n        latent[""x""].z.sample_(dist.Normal(loc, torch.ones(1)))\n\n    tr = poutine.trace(model).get_trace()\n    assert get_sample_names(tr) == set([""latent[\'foo\']"", ""latent[\'x\'].z""])\n    assert get_observe_names(tr) == set([""latent[\'bar\']""])\n    assert get_param_names(tr) == set([""latent[\'loc\']""])\n\n\ndef test_nested():\n    pyro.clear_param_store()\n\n    def model():\n        latent = named.Object(""latent"")\n        latent.list = named.List()\n        loc = latent.list.add().loc.param_(torch.zeros(1))\n        latent.dict = named.Dict()\n        foo = latent.dict[""foo""].foo.sample_(dist.Normal(loc, torch.ones(1)))\n        latent.object.bar.sample_(dist.Normal(loc, torch.ones(1)), obs=foo)\n\n    tr = poutine.trace(model).get_trace()\n    assert get_sample_names(tr) == set([""latent.dict[\'foo\'].foo""])\n    assert get_observe_names(tr) == set([""latent.object.bar""])\n    assert get_param_names(tr) == set([""latent.list[0].loc""])\n\n\ndef test_eval_str():\n    state = named.Object(""state"")\n    state.x = 0\n    state.ys = named.List()\n    state.ys.add().foo = 1\n    state.zs = named.Dict()\n    state.zs[42].bar = 2\n\n    assert state is eval(str(state))\n    assert state.x is eval(str(state.x))\n    assert state.ys is eval(str(state.ys))\n    assert state.ys[0] is eval(str(state.ys[0]))\n    assert state.ys[0].foo is eval(str(state.ys[0].foo))\n    assert state.zs is eval(str(state.zs))\n    assert state.zs[42] is eval(str(state.zs[42]))\n    assert state.zs[42].bar is eval(str(state.zs[42].bar))\n'"
tests/contrib/autoname/test_scoping.py,1,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\nimport torch\n\nimport pyro\nimport pyro.distributions.torch as dist\nimport pyro.poutine as poutine\nfrom pyro.contrib.autoname import scope, name_count\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_multi_nested():\n\n    @scope\n    def model1(r=True):\n        model2()\n        model2()\n        with scope(prefix=""inter""):\n            model2()\n            if r:\n                model1(r=False)\n        model2()\n\n    @scope\n    def model2():\n        return pyro.sample(""y"", dist.Normal(0.0, 1.0))\n\n    true_samples = [""model1/model2/y"",\n                    ""model1/model2__1/y"",\n                    ""model1/inter/model2/y"",\n                    ""model1/inter/model1/model2/y"",\n                    ""model1/inter/model1/model2__1/y"",\n                    ""model1/inter/model1/inter/model2/y"",\n                    ""model1/inter/model1/model2__2/y"",\n                    ""model1/model2__2/y""]\n\n    tr = poutine.trace(name_count(model1)).get_trace(r=True)\n\n    samples = [name for name, node in tr.nodes.items()\n               if node[""type""] == ""sample""]\n    logger.debug(samples)\n    assert true_samples == samples\n\n\ndef test_recur_multi():\n\n    @scope(inner=True)\n    def model1(r=True):\n        model2()\n        with scope(prefix=""inter""):\n            model2()\n            if r:\n                model1(r=False)\n        model2()\n\n    @scope(inner=True)\n    def model2():\n        return pyro.sample(""y"", dist.Normal(0.0, 1.0))\n\n    true_samples = [""model1/model2/y"",\n                    ""model1/inter/model2/y"",\n                    ""model1/inter/model1/model2/y"",\n                    ""model1/inter/model1/inter/model2/y"",\n                    ""model1/inter/model1/model2/y__1"",\n                    ""model1/model2/y__1""]\n\n    tr = poutine.trace(name_count(model1)).get_trace()\n\n    samples = [name for name, node in tr.nodes.items()\n               if node[""type""] == ""sample""]\n    logger.debug(samples)\n    assert true_samples == samples\n\n\ndef test_only_withs():\n\n    def model1():\n        with scope(prefix=""a""):\n            with scope(prefix=""b""):\n                pyro.sample(""x"", dist.Bernoulli(0.5))\n\n    tr1 = poutine.trace(name_count(model1)).get_trace()\n    assert ""a/b/x"" in tr1.nodes\n\n    tr2 = poutine.trace(name_count(scope(prefix=""model1"")(model1))).get_trace()\n    assert ""model1/a/b/x"" in tr2.nodes\n\n\ndef test_mutual_recur():\n\n    @scope\n    def model1(n):\n        pyro.sample(""a"", dist.Bernoulli(0.5))\n        if n <= 0:\n            return\n        else:\n            return model2(n-1)\n\n    @scope\n    def model2(n):\n        pyro.sample(""b"", dist.Bernoulli(0.5))\n        if n <= 0:\n            return\n        else:\n            model1(n)\n\n    names = set([""_INPUT"", ""_RETURN"",\n                 ""model2/b"", ""model2/model1/a"", ""model2/model1/model2/b""])\n    tr_names = set([name for name in poutine.trace(name_count(model2)).get_trace(1)])\n    assert names == tr_names\n\n\ndef test_simple_recur():\n\n    @scope\n    def geometric(p):\n        x = pyro.sample(""x"", dist.Bernoulli(p))\n        if x.item() == 1.0:\n            # model1()\n            return x + geometric(p)\n        else:\n            return x\n\n    prev_name = ""x""\n    for name, node in poutine.trace(name_count(geometric)).get_trace(0.9).nodes.items():\n        if node[""type""] == ""sample"":\n            logger.debug(name)\n            assert name == ""geometric/"" + prev_name\n            prev_name = ""geometric/"" + prev_name\n\n\ndef test_basic_scope():\n\n    @scope\n    def f1():\n        return pyro.sample(""x"", dist.Bernoulli(0.5))\n\n    @scope\n    def f2():\n        f1()\n        return pyro.sample(""y"", dist.Bernoulli(0.5))\n\n    tr1 = poutine.trace(f1).get_trace()\n    assert ""f1/x"" in tr1.nodes\n\n    tr2 = poutine.trace(f2).get_trace()\n    assert ""f2/f1/x"" in tr2.nodes\n    assert ""f2/y"" in tr2.nodes\n\n\ndef test_nested_traces():\n\n    @scope\n    def f1():\n        return pyro.sample(""x"", dist.Bernoulli(0.5))\n\n    @scope\n    def f2():\n        f1()\n        f1()\n        f1()\n        return pyro.sample(""y"", dist.Bernoulli(0.5))\n\n    expected_names = [""f2/f1/x"", ""f2/f1__1/x"", ""f2/f1__2/x"", ""f2/y""]\n    tr2 = poutine.trace(name_count(name_count(f2))).get_trace()\n    actual_names = [name for name, node in tr2.nodes.items()\n                    if node[\'type\'] == ""sample""]\n    assert expected_names == actual_names\n\n\ndef test_no_param():\n\n    pyro.clear_param_store()\n\n    @scope\n    def model():\n        a = pyro.param(""a"", torch.tensor(0.5))\n        return pyro.sample(""b"", dist.Bernoulli(a))\n\n    expected_names = [""a"", ""model/b""]\n    tr = poutine.trace(model).get_trace()\n    actual_names = [name for name, node in tr.nodes.items()\n                    if node[\'type\'] in (\'param\', \'sample\')]\n\n    assert expected_names == actual_names\n'"
tests/contrib/bnn/test_hidden_layer.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\n\nimport pytest\nfrom pyro.contrib.bnn import HiddenLayer\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize(""non_linearity"", [F.relu])\n@pytest.mark.parametrize(""include_hidden_bias"", [False, True])\ndef test_hidden_layer_rsample(non_linearity, include_hidden_bias, B=2, D=3, H=4, N=900000):\n    X = torch.randn(B, D)\n    A_mean = torch.rand(D, H)\n    A_scale = 0.3 * torch.exp(0.3 * torch.rand(D, H))\n\n    # test naive weight space sampling against sampling in pre-activation space\n    dist1 = HiddenLayer(X=X, A_mean=A_mean, A_scale=A_scale, non_linearity=non_linearity,\n                        include_hidden_bias=include_hidden_bias, weight_space_sampling=True)\n    dist2 = HiddenLayer(X=X, A_mean=A_mean, A_scale=A_scale, non_linearity=non_linearity,\n                        include_hidden_bias=include_hidden_bias, weight_space_sampling=False)\n\n    out1 = dist1.rsample(sample_shape=(N,))\n    out1_mean, out1_var = out1.mean(0), out1.var(0)\n    out2 = dist2.rsample(sample_shape=(N,))\n    out2_mean, out2_var = out2.mean(0), out2.var(0)\n\n    assert_equal(out1_mean, out2_mean, prec=0.003)\n    assert_equal(out1_var, out2_var, prec=0.003)\n    return\n\n\n@pytest.mark.parametrize(""non_linearity"", [F.relu])\n@pytest.mark.parametrize(""include_hidden_bias"", [True, False])\ndef test_hidden_layer_log_prob(non_linearity, include_hidden_bias, B=2, D=3, H=2):\n    X = torch.randn(B, D)\n    A_mean = torch.rand(D, H)\n    A_scale = 0.3 * torch.exp(0.3 * torch.rand(D, H))\n    dist = HiddenLayer(X=X, A_mean=A_mean, A_scale=A_scale,\n                       non_linearity=non_linearity, include_hidden_bias=include_hidden_bias)\n\n    A_dist = Normal(A_mean, A_scale)\n    A_prior = Normal(torch.zeros(D, H), torch.ones(D, H))\n    kl = torch.distributions.kl.kl_divergence(A_dist, A_prior).sum()\n    assert_equal(kl, dist.KL, prec=0.01)\n'"
tests/contrib/cevae/test_cevae.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport io\nimport warnings\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.cevae import CEVAE, DistributionNet\nfrom tests.common import assert_close\n\nDIST_NETS = [cls.__name__.lower()[:-3]\n             for cls in DistributionNet.__subclasses__()]\n\n\ndef generate_data(num_data, feature_dim):\n    z = dist.Bernoulli(0.5).sample([num_data])\n    x = dist.Normal(z, 5 * z + 3 * (1 - z)).sample([feature_dim]).t()\n    t = dist.Bernoulli(0.75 * z + 0.25 * (1 - z)).sample()\n    y = dist.Bernoulli(logits=3 * (z + 2 * (2 * t - 2))).sample()\n    return x, t, y\n\n\n@pytest.mark.parametrize(""num_data"", [1, 100, 200])\n@pytest.mark.parametrize(""feature_dim"", [1, 2])\n@pytest.mark.parametrize(""outcome_dist"", DIST_NETS)\ndef test_smoke(num_data, feature_dim, outcome_dist):\n    x, t, y = generate_data(num_data, feature_dim)\n    if outcome_dist == ""exponential"":\n        y.clamp_(min=1e-20)\n    cevae = CEVAE(feature_dim, outcome_dist)\n    cevae.fit(x, t, y, num_epochs=2)\n    ite = cevae.ite(x)\n    assert ite.shape == (num_data,)\n\n\n@pytest.mark.parametrize(""feature_dim"", [1, 2])\n@pytest.mark.parametrize(""outcome_dist"", DIST_NETS)\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""python"", ""jit""])\ndef test_serialization(jit, feature_dim, outcome_dist):\n    x, t, y = generate_data(num_data=32, feature_dim=feature_dim)\n    if outcome_dist == ""exponential"":\n        y.clamp_(min=1e-20)\n    cevae = CEVAE(feature_dim, outcome_dist=outcome_dist, num_samples=1000, hidden_dim=32)\n    cevae.fit(x, t, y, num_epochs=4, batch_size=8)\n    pyro.set_rng_seed(0)\n    expected_ite = cevae.ite(x)\n\n    if jit:\n        traced_cevae = cevae.to_script_module()\n        f = io.BytesIO()\n        torch.jit.save(traced_cevae, f)\n        f.seek(0)\n        loaded_cevae = torch.jit.load(f)\n    else:\n        f = io.BytesIO()\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", category=UserWarning)\n            torch.save(cevae, f)\n        f.seek(0)\n        loaded_cevae = torch.load(f)\n\n    pyro.set_rng_seed(0)\n    actual_ite = loaded_cevae.ite(x)\n    assert_close(actual_ite, expected_ite, atol=0.1)\n'"
tests/contrib/easyguide/test_easyguide.py,20,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport io\nimport warnings\n\nimport pytest\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.easyguide import EasyGuide, easy_guide\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.infer.autoguide.initialization import init_to_mean, init_to_median\nfrom pyro.optim import Adam\nfrom pyro.util import ignore_jit_warnings\n\n\n# The model from tutorial/source/easyguide.ipynb\ndef model(batch, subsample, full_size):\n    with ignore_jit_warnings():\n        num_time_steps = len(batch)\n    result = [None] * num_time_steps\n    drift = pyro.sample(""drift"", dist.LogNormal(-1, 0.5))\n    with pyro.plate(""data"", full_size, subsample=subsample):\n        z = 0.\n        for t in range(num_time_steps):\n            z = pyro.sample(""state_{}"".format(t),\n                            dist.Normal(z, drift))\n            result[t] = pyro.sample(""obs_{}"".format(t), dist.Bernoulli(logits=z),\n                                    obs=batch[t])\n    return torch.stack(result)\n\n\ndef check_guide(guide):\n    full_size = 50\n    batch_size = 20\n    num_time_steps = 8\n    pyro.set_rng_seed(123456789)\n    data = model([None] * num_time_steps, torch.arange(full_size), full_size)\n    assert data.shape == (num_time_steps, full_size)\n\n    pyro.get_param_store().clear()\n    pyro.set_rng_seed(123456789)\n    svi = SVI(model, guide, Adam({""lr"": 0.02}), Trace_ELBO())\n    for epoch in range(2):\n        beg = 0\n        while beg < full_size:\n            end = min(full_size, beg + batch_size)\n            subsample = torch.arange(beg, end)\n            batch = data[:, beg:end]\n            beg = end\n            svi.step(batch, subsample, full_size=full_size)\n\n\n@pytest.mark.parametrize(""init_fn"", [None, init_to_mean, init_to_median])\ndef test_delta_smoke(init_fn):\n\n    @easy_guide(model)\n    def guide(self, batch, subsample, full_size):\n        self.map_estimate(""drift"")\n        with self.plate(""data"", full_size, subsample=subsample):\n            self.group(match=""state_[0-9]*"").map_estimate()\n\n    if init_fn is not None:\n        guide.init = init_fn\n\n    check_guide(guide)\n\n\nclass PickleGuide(EasyGuide):\n    def __init__(self, model):\n        super().__init__(model)\n        self.init = init_to_median\n\n    def guide(self, batch, subsample, full_size):\n        self.map_estimate(""drift"")\n        with self.plate(""data"", full_size, subsample=subsample):\n            self.group(match=""state_[0-9]*"").map_estimate()\n\n\ndef test_serialize():\n    guide = PickleGuide(model)\n    check_guide(guide)\n\n    # Work around https://github.com/pytorch/pytorch/issues/27972\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=UserWarning)\n        f = io.BytesIO()\n        torch.save(guide, f)\n        f.seek(0)\n        actual = torch.load(f)\n\n    assert type(actual) == type(guide)\n    assert dir(actual) == dir(guide)\n    check_guide(guide)\n    check_guide(actual)\n\n\n@pytest.mark.parametrize(""init_fn"", [None, init_to_mean, init_to_median])\ndef test_subsample_smoke(init_fn):\n    rank = 2\n\n    @easy_guide(model)\n    def guide(self, batch, subsample, full_size):\n        self.map_estimate(""drift"")\n        group = self.group(match=""state_[0-9]*"")\n        cov_diag = pyro.param(""state_cov_diag"",\n                              lambda: torch.full(group.event_shape, 0.01),\n                              constraint=constraints.positive)\n        cov_factor = pyro.param(""state_cov_factor"",\n                                lambda: torch.randn(group.event_shape + (rank,)) * 0.01)\n        with self.plate(""data"", full_size, subsample=subsample):\n            loc = pyro.param(""state_loc"",\n                             lambda: torch.full((full_size,) + group.event_shape, 0.5),\n                             event_dim=1)\n            group.sample(""states"", dist.LowRankMultivariateNormal(loc, cov_factor, cov_diag))\n\n    if init_fn is not None:\n        guide.init = init_fn\n\n    check_guide(guide)\n\n\n@pytest.mark.parametrize(""init_fn"", [None, init_to_mean, init_to_median])\ndef test_amortized_smoke(init_fn):\n    rank = 2\n\n    @easy_guide(model)\n    def guide(self, batch, subsample, full_size):\n        num_time_steps, batch_size = batch.shape\n        self.map_estimate(""drift"")\n\n        group = self.group(match=""state_[0-9]*"")\n        cov_diag = pyro.param(""state_cov_diag"",\n                              lambda: torch.full(group.event_shape, 0.01),\n                              constraint=constraints.positive)\n        cov_factor = pyro.param(""state_cov_factor"",\n                                lambda: torch.randn(group.event_shape + (rank,)) * 0.01)\n\n        if not hasattr(self, ""nn""):\n            self.nn = torch.nn.Linear(group.event_shape.numel(), group.event_shape.numel())\n            self.nn.weight.data.fill_(1.0 / num_time_steps)\n            self.nn.bias.data.fill_(-0.5)\n        pyro.module(""state_nn"", self.nn)\n        with self.plate(""data"", full_size, subsample=subsample):\n            loc = self.nn(batch.t())\n            group.sample(""states"", dist.LowRankMultivariateNormal(loc, cov_factor, cov_diag))\n\n    if init_fn is not None:\n        guide.init = init_fn\n\n    check_guide(guide)\n\n\ndef test_overlapping_plates_ok():\n\n    def model(batch, subsample, full_size):\n        # This is ok because the shared plate is left of the nonshared plate.\n        with pyro.plate(""shared"", full_size, subsample=subsample, dim=-2):\n            x = pyro.sample(""x"", dist.Normal(0, 1))\n            with pyro.plate(""nonshared"", 2, dim=-1):\n                y = pyro.sample(""y"", dist.Normal(0, 1))\n            xy = x + y.sum(-1, keepdim=True)\n            return pyro.sample(""z"", dist.Normal(xy, 1),\n                               obs=batch)\n\n    @easy_guide(model)\n    def guide(self, batch, subsample, full_size):\n        with self.plate(""shared"", full_size, subsample=subsample, dim=-2):\n            group = self.group(match=""x|y"")\n            loc = pyro.param(""guide_loc"",\n                             torch.zeros((full_size, 1) + group.event_shape),\n                             event_dim=1)\n            scale = pyro.param(""guide_scale"",\n                               torch.ones((full_size, 1) + group.event_shape),\n                               constraint=constraints.positive,\n                               event_dim=1)\n            group.sample(""xy"", dist.Normal(loc, scale).to_event(1))\n\n    # Generate data.\n    full_size = 5\n    batch_size = 2\n    data = model(None, torch.arange(full_size), full_size)\n    assert data.shape == (full_size, 1)\n\n    # Train for one epoch.\n    pyro.get_param_store().clear()\n    svi = SVI(model, guide, Adam({""lr"": 0.02}), Trace_ELBO())\n    beg = 0\n    while beg < full_size:\n        end = min(full_size, beg + batch_size)\n        subsample = torch.arange(beg, end)\n        batch = data[beg:end]\n        beg = end\n        svi.step(batch, subsample, full_size=full_size)\n\n\ndef test_overlapping_plates_error():\n\n    def model(batch, subsample, full_size):\n        # This is an error because the shared plate is right of the nonshared plate.\n        with pyro.plate(""shared"", full_size, subsample=subsample, dim=-1):\n            x = pyro.sample(""x"", dist.Normal(0, 1))\n            with pyro.plate(""nonshared"", 2, dim=-2):\n                y = pyro.sample(""y"", dist.Normal(0, 1))\n            xy = x + y.sum(-2)\n            return pyro.sample(""z"", dist.Normal(xy, 1),\n                               obs=batch)\n\n    @easy_guide(model)\n    def guide(self, batch, subsample, full_size):\n        with self.plate(""shared"", full_size, subsample=subsample, dim=-1):\n            group = self.group(match=""x|y"")\n            loc = pyro.param(""guide_loc"",\n                             torch.zeros((full_size,) + group.event_shape),\n                             event_dim=1)\n            scale = pyro.param(""guide_scale"",\n                               torch.ones((full_size,) + group.event_shape),\n                               constraint=constraints.positive,\n                               event_dim=1)\n            group.sample(""xy"", dist.Normal(loc, scale).to_event(1))\n\n    # Generate data.\n    full_size = 5\n    batch_size = 2\n    data = model(None, torch.arange(full_size), full_size)\n    assert data.shape == (full_size,)\n\n    # Train for one epoch.\n    pyro.get_param_store().clear()\n    svi = SVI(model, guide, Adam({""lr"": 0.02}), Trace_ELBO())\n    beg = 0\n    end = min(full_size, beg + batch_size)\n    subsample = torch.arange(beg, end)\n    batch = data[beg:end]\n    beg = end\n    with pytest.raises(ValueError, match=""Group expects all per-site plates""):\n        svi.step(batch, subsample, full_size=full_size)\n'"
tests/contrib/epidemiology/__init__.py,0,b''
tests/contrib/epidemiology/test_distributions.py,15,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\nfrom torch.distributions.transforms import SigmoidTransform\n\nimport pyro.distributions as dist\nfrom pyro.contrib.epidemiology import beta_binomial_dist, binomial_dist, infection_dist\nfrom tests.common import assert_close\n\n\ndef assert_dist_close(d1, d2):\n    x = torch.arange(float(200))\n    p1 = d1.log_prob(x).exp()\n    p2 = d2.log_prob(x).exp()\n\n    assert (p1.sum() - 1).abs() < 1e-3, ""incomplete mass""\n    assert (p2.sum() - 1).abs() < 1e-3, ""incomplete mass""\n\n    mean1 = (p1 * x).sum()\n    mean2 = (p2 * x).sum()\n    assert_close(mean1, mean2, rtol=0.05)\n\n    max_prob = torch.max(p1.max(), p2.max())\n    assert (p1 - p2).abs().max() / max_prob < 0.05\n\n\n@pytest.mark.parametrize(""R0,I"", [\n    (1., 1),\n    (1., 10),\n    (10., 1),\n    (5., 5),\n])\ndef test_binomial_vs_poisson(R0, I):\n    R0 = torch.tensor(R0)\n    I = torch.tensor(I)\n\n    d1 = infection_dist(individual_rate=R0, num_infectious=I)\n    d2 = infection_dist(individual_rate=R0, num_infectious=I,\n                        num_susceptible=1000., population=1000.)\n\n    assert isinstance(d1, dist.Poisson)\n    assert isinstance(d2, dist.Binomial)\n    assert_dist_close(d1, d2)\n\n\n@pytest.mark.parametrize(""R0,I,k"", [\n    (1., 1., 0.5),\n    (1., 1., 1.),\n    (1., 1., 2.),\n    (1., 10., 0.5),\n    (1., 10., 1.),\n    (1., 10., 2.),\n    (10., 1., 0.5),\n    (10., 1., 1.),\n    (10., 1., 2.),\n    (5., 5, 0.5),\n    (5., 5, 1.),\n    (5., 5, 2.),\n])\ndef test_beta_binomial_vs_negative_binomial(R0, I, k):\n    R0 = torch.tensor(R0)\n    I = torch.tensor(I)\n\n    d1 = infection_dist(individual_rate=R0, num_infectious=I, concentration=k)\n    d2 = infection_dist(individual_rate=R0, num_infectious=I, concentration=k,\n                        num_susceptible=1000., population=1000.)\n\n    assert isinstance(d1, dist.NegativeBinomial)\n    assert isinstance(d2, dist.BetaBinomial)\n    assert_dist_close(d1, d2)\n\n\n@pytest.mark.parametrize(""R0,I"", [\n    (1., 1.),\n    (1., 10.),\n    (10., 1.),\n    (5., 5.),\n])\ndef test_beta_binomial_vs_binomial(R0, I):\n    R0 = torch.tensor(R0)\n    I = torch.tensor(I)\n\n    d1 = infection_dist(individual_rate=R0, num_infectious=I,\n                        num_susceptible=20., population=30.)\n    d2 = infection_dist(individual_rate=R0, num_infectious=I,\n                        num_susceptible=20., population=30.,\n                        concentration=200.)\n\n    assert isinstance(d1, dist.Binomial)\n    assert isinstance(d2, dist.BetaBinomial)\n    assert_dist_close(d1, d2)\n\n\n@pytest.mark.parametrize(""R0,I"", [\n    (1., 1.),\n    (1., 10.),\n    (10., 1.),\n    (5., 5.),\n])\ndef test_negative_binomial_vs_poisson(R0, I):\n    R0 = torch.tensor(R0)\n    I = torch.tensor(I)\n\n    d1 = infection_dist(individual_rate=R0, num_infectious=I)\n    d2 = infection_dist(individual_rate=R0, num_infectious=I,\n                        concentration=200.)\n\n    assert isinstance(d1, dist.Poisson)\n    assert isinstance(d2, dist.NegativeBinomial)\n    assert_dist_close(d1, d2)\n\n\n@pytest.mark.parametrize(""overdispersion"", [0.01, 0.03, 0.1, 0.3, 1.0, 1.5])\n@pytest.mark.parametrize(""probs"", [0.01, 0.03, 0.1, 0.3, 0.7, 0.9, 0.97, 0.99])\ndef test_overdispersed_bound(probs, overdispersion):\n    total_count = torch.tensor([1, 2, 5, 10, 20, 50, 1e2, 1e3, 1e5, 1e6, 1e7, 1e8])\n    d = binomial_dist(total_count, probs, overdispersion=overdispersion)\n    relative_error = d.variance.sqrt() / (probs * (1 - probs) * total_count)\n\n    # Check bound is valid.\n    assert (relative_error >= overdispersion).all()\n\n    # Check bound is tight.\n    assert relative_error[-1] / overdispersion < 1.05\n\n\n@pytest.mark.parametrize(""overdispersion"", [0.05, 0.1, 0.2, 0.3])\n@pytest.mark.parametrize(""probs"", [0.1, 0.2, 0.5, 0.8, 0.9])\ndef test_overdispersed_asymptote(probs, overdispersion):\n    total_count = 100000\n\n    # Check binomial_dist converges in distribution to LogitNormal.\n    d1 = binomial_dist(total_count, probs)\n    d2 = dist.TransformedDistribution(\n        dist.Normal(math.log(probs / (1 - probs)), overdispersion),\n        SigmoidTransform())\n\n    # CRPS is equivalent to the Cramer-von Mises test.\n    # https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion\n    k = torch.arange(0., total_count + 1.)\n    cdf1 = d1.log_prob(k).exp().cumsum(-1)\n    cdf2 = d2.cdf(k / total_count)\n    crps = (cdf1 - cdf2).pow(2).mean()\n    assert crps < 0.02\n\n\n@pytest.mark.parametrize(""total_count"", [1, 2, 5, 10, 20, 50])\n@pytest.mark.parametrize(""concentration1"", [0.2, 1.0, 5.])\n@pytest.mark.parametrize(""concentration0"", [0.2, 1.0, 5.])\ndef test_beta_binomial(concentration1, concentration0, total_count):\n    # For small overdispersion, beta_binomial_dist is close to BetaBinomial.\n    d1 = dist.BetaBinomial(concentration1, concentration0, total_count)\n    d2 = beta_binomial_dist(concentration1, concentration0, total_count,\n                            overdispersion=0.01)\n\n    # CRPS is equivalent to the Cramer-von Mises test.\n    # https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion\n    k = torch.arange(0., total_count + 1.)\n    cdf1 = d1.log_prob(k).exp().cumsum(-1)\n    cdf2 = d2.log_prob(k).exp().cumsum(-1)\n    crps = (cdf1 - cdf2).pow(2).mean()\n    assert crps < 0.01\n\n\n@pytest.mark.parametrize(""overdispersion"", [0.05, 0.1, 0.2, 0.5, 1.0])\n@pytest.mark.parametrize(""total_count"", [1, 2, 5, 10, 20, 50])\n@pytest.mark.parametrize(""probs"", [0.1, 0.2, 0.5, 0.8, 0.9])\ndef test_overdispersed_beta_binomial(probs, total_count, overdispersion):\n    # For high concentraion, beta_binomial_dist is close to binomial_dist.\n    concentration = 100.  # very little uncertainty\n    concentration1 = concentration * probs\n    concentration0 = concentration * (1 - probs)\n    d1 = binomial_dist(total_count, probs, overdispersion=overdispersion)\n    d2 = beta_binomial_dist(concentration1, concentration0, total_count,\n                            overdispersion=overdispersion)\n\n    # CRPS is equivalent to the Cramer-von Mises test.\n    # https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion\n    k = torch.arange(0., total_count + 1.)\n    cdf1 = d1.log_prob(k).exp().cumsum(-1)\n    cdf2 = d2.log_prob(k).exp().cumsum(-1)\n    crps = (cdf1 - cdf2).pow(2).mean()\n    assert crps < 0.01\n'"
tests/contrib/epidemiology/test_models.py,5,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport math\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.contrib.epidemiology import (HeterogeneousSIRModel, OverdispersedSEIRModel, OverdispersedSIRModel,\n                                       RegionalSIRModel, SimpleSEIRModel, SimpleSIRModel, SparseSIRModel,\n                                       SuperspreadingSEIRModel, SuperspreadingSIRModel, UnknownStartSIRModel)\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.parametrize(""duration"", [3, 7])\n@pytest.mark.parametrize(""forecast"", [0, 7])\n@pytest.mark.parametrize(""options"", [\n    {},\n    {""haar"": True},\n    {""haar_full_mass"": 2},\n    {""num_quant_bins"": 2},\n    {""num_quant_bins"": 8},\n    {""num_quant_bins"": 12},\n    {""num_quant_bins"": 16},\n    {""arrowhead_mass"": True},\n], ids=str)\ndef test_simple_sir_smoke(duration, forecast, options):\n    population = 100\n    recovery_time = 7.0\n\n    # Generate data.\n    model = SimpleSIRModel(population, recovery_time, [None] * duration)\n    assert model.full_mass == [(""R0"", ""rho"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n\n    # Infer.\n    model = SimpleSIRModel(population, recovery_time, data)\n    num_samples = 5\n    model.fit(warmup_steps=1, num_samples=num_samples, max_tree_depth=2, **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n\n\n@pytest.mark.parametrize(""duration"", [3, 7])\n@pytest.mark.parametrize(""forecast"", [0, 7])\n@pytest.mark.parametrize(""options"", [\n    {},\n    {""haar"": True},\n    {""haar_full_mass"": 2},\n    {""num_quant_bins"": 8},\n], ids=str)\ndef test_simple_seir_smoke(duration, forecast, options):\n    population = 100\n    incubation_time = 2.0\n    recovery_time = 7.0\n\n    # Generate data.\n    model = SimpleSEIRModel(population, incubation_time, recovery_time,\n                            [None] * duration)\n    assert model.full_mass == [(""R0"", ""rho"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n\n    # Infer.\n    model = SimpleSEIRModel(population, incubation_time, recovery_time, data)\n    num_samples = 5\n    model.fit(warmup_steps=2, num_samples=num_samples, max_tree_depth=2,\n              **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""E""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n\n\n@pytest.mark.parametrize(""duration"", [3])\n@pytest.mark.parametrize(""forecast"", [7])\n@pytest.mark.parametrize(""options"", [{}, {""haar_full_mass"": 2}], ids=str)\ndef test_overdispersed_sir_smoke(duration, forecast, options):\n    population = 100\n    recovery_time = 7.0\n\n    # Generate data.\n    model = OverdispersedSIRModel(population, recovery_time, [None] * duration)\n    assert model.full_mass == [(""R0"", ""rho"", ""od"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n\n    # Infer.\n    model = OverdispersedSIRModel(population, recovery_time, data)\n    num_samples = 5\n    model.fit(warmup_steps=1, num_samples=num_samples, max_tree_depth=2, **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n\n\n@pytest.mark.parametrize(""duration"", [3])\n@pytest.mark.parametrize(""forecast"", [7])\n@pytest.mark.parametrize(""options"", [{}, {""haar_full_mass"": 2}], ids=str)\ndef test_overdispersed_seir_smoke(duration, forecast, options):\n    population = 100\n    incubation_time = 2.0\n    recovery_time = 7.0\n\n    # Generate data.\n    model = OverdispersedSEIRModel(population, incubation_time, recovery_time,\n                                   [None] * duration)\n    assert model.full_mass == [(""R0"", ""rho"", ""od"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n\n    # Infer.\n    model = OverdispersedSEIRModel(population, incubation_time, recovery_time, data)\n    num_samples = 5\n    model.fit(warmup_steps=2, num_samples=num_samples, max_tree_depth=2,\n              **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""E""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n\n\n@pytest.mark.parametrize(""duration"", [3, 7])\n@pytest.mark.parametrize(""forecast"", [0, 7])\n@pytest.mark.parametrize(""options"", [\n    {},\n    {""haar"": True},\n    {""haar_full_mass"": 2},\n    {""num_quant_bins"": 8},\n], ids=str)\ndef test_superspreading_sir_smoke(duration, forecast, options):\n    population = 100\n    recovery_time = 7.0\n\n    # Generate data.\n    model = SuperspreadingSIRModel(population, recovery_time, [None] * duration)\n    assert model.full_mass == [(""R0"", ""k"", ""rho"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5, ""k"": 1.0})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n\n    # Infer.\n    model = SuperspreadingSIRModel(population, recovery_time, data)\n    num_samples = 5\n    model.fit(warmup_steps=1, num_samples=num_samples, max_tree_depth=2, **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n\n\n@pytest.mark.parametrize(""duration"", [3, 7])\n@pytest.mark.parametrize(""forecast"", [0, 7])\n@pytest.mark.parametrize(""options"", [\n    {},\n    {""haar"": True},\n    {""haar_full_mass"": 2},\n    {""num_quant_bins"": 8},\n], ids=str)\ndef test_superspreading_seir_smoke(duration, forecast, options):\n    population = 100\n    incubation_time = 2.0\n    recovery_time = 7.0\n\n    # Generate data.\n    model = SuperspreadingSEIRModel(\n        population, incubation_time, recovery_time, [None] * duration)\n    assert model.full_mass == [(""R0"", ""k"", ""rho"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5, ""k"": 1.0})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n\n    # Infer.\n    model = SuperspreadingSEIRModel(\n        population, incubation_time, recovery_time, data)\n    num_samples = 5\n    model.fit(warmup_steps=2, num_samples=num_samples, max_tree_depth=2,\n              **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""E""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n\n\n@pytest.mark.parametrize(""duration"", [3, 7])\n@pytest.mark.parametrize(""forecast"", [0, 7])\ndef test_coalescent_likelihood_smoke(duration, forecast):\n    population = 100\n    incubation_time = 2.0\n    recovery_time = 7.0\n\n    # Generate data.\n    model = SuperspreadingSEIRModel(\n        population, incubation_time, recovery_time, [None] * duration)\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5, ""k"": 1.0})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n    leaf_times = torch.rand(5).pow(0.5) * duration\n    coal_times = dist.CoalescentTimes(leaf_times).sample()\n    coal_times = coal_times[..., torch.randperm(coal_times.size(-1))]\n\n    # Infer.\n    model = SuperspreadingSEIRModel(\n        population, incubation_time, recovery_time, data,\n        leaf_times=leaf_times, coal_times=coal_times)\n    num_samples = 5\n    model.fit(warmup_steps=2, num_samples=num_samples, max_tree_depth=2)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""E""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n\n\n@pytest.mark.parametrize(""duration"", [3, 7])\n@pytest.mark.parametrize(""forecast"", [0, 7])\n@pytest.mark.parametrize(""options"", [\n    {""num_quant_bins"": 2},\n    {""haar_full_mass"": 2},\n], ids=str)\ndef test_heterogeneous_sir_smoke(duration, forecast, options):\n    population = 100\n    recovery_time = 7.0\n\n    # Generate data.\n    model = HeterogeneousSIRModel(population, recovery_time, [None] * duration)\n    assert model.full_mass == [(""R0"", ""rho0"", ""rho1"", ""rho2"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n\n    # Infer.\n    model = HeterogeneousSIRModel(population, recovery_time, data)\n    num_samples = 5\n    model.fit(warmup_steps=1, num_samples=num_samples, max_tree_depth=2, **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n    assert samples[""beta""].shape == (num_samples, duration + forecast)\n\n\n@pytest.mark.parametrize(""duration"", [4, 12])\n@pytest.mark.parametrize(""forecast"", [7])\n@pytest.mark.parametrize(""options"", [\n    {},\n    {""haar"": True},\n    {""haar_full_mass"": 3},\n    {""num_quant_bins"": 8},\n], ids=str)\ndef test_sparse_smoke(duration, forecast, options):\n    population = 100\n    recovery_time = 7.0\n\n    # Generate data.\n    data = [None] * duration\n    mask = torch.arange(duration) % 4 == 3\n    model = SparseSIRModel(population, recovery_time, data, mask)\n    assert model.full_mass == [(""R0"", ""rho"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5})[""obs""]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n    assert (data[1:] >= data[:-1]).all()\n    data[~mask] = math.nan\n    logger.info(""data:\\n{}"".format(data))\n\n    # Infer.\n    model = SparseSIRModel(population, recovery_time, data, mask)\n    num_samples = 5\n    model.fit(warmup_steps=1, num_samples=num_samples, max_tree_depth=2, **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast)\n    assert samples[""I""].shape == (num_samples, duration + forecast)\n    assert samples[""O""].shape == (num_samples, duration + forecast)\n    assert (samples[""O""][..., 1:] >= samples[""O""][..., :-1]).all()\n    for O in samples[""O""]:\n        logger.info(""imputed:\\n{}"".format(O))\n        assert (O[:duration][mask] == data[mask]).all()\n\n\n@pytest.mark.parametrize(""pre_obs_window"", [6])\n@pytest.mark.parametrize(""duration"", [8])\n@pytest.mark.parametrize(""forecast"", [0, 7])\n@pytest.mark.parametrize(""options"", [\n    {},\n    {""haar"": True},\n    {""haar_full_mass"": 4},\n    {""num_quant_bins"": 8},\n], ids=str)\ndef test_unknown_start_smoke(duration, pre_obs_window, forecast, options):\n    population = 100\n    recovery_time = 7.0\n\n    # Generate data.\n    data = [None] * duration\n    model = UnknownStartSIRModel(population, recovery_time, pre_obs_window, data)\n    assert model.full_mass == [(""R0"", ""rho0"", ""rho1"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho0"": 0.1, ""rho1"": 0.5})[""obs""]\n        assert len(data) == pre_obs_window + duration\n        data = data[pre_obs_window:]\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n    logger.info(""data:\\n{}"".format(data))\n\n    # Infer.\n    model = UnknownStartSIRModel(population, recovery_time, pre_obs_window, data)\n    num_samples = 5\n    model.fit(warmup_steps=1, num_samples=num_samples, max_tree_depth=2, **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, pre_obs_window + duration + forecast)\n    assert samples[""I""].shape == (num_samples, pre_obs_window + duration + forecast)\n\n    # Check time of first infection.\n    t = samples[""first_infection""]\n    logger.info(""first_infection:\\n{}"".format(t))\n    assert t.shape == (num_samples,)\n    assert (0 <= t).all()\n    assert (t < pre_obs_window + duration).all()\n    for I, ti in zip(samples[""I""], t):\n        assert (I[:ti] == 0).all()\n        assert I[ti] > 0\n\n\n@pytest.mark.parametrize(""duration"", [3, 7])\n@pytest.mark.parametrize(""forecast"", [0, 7])\n@pytest.mark.parametrize(""options"", [\n    {},\n    {""haar"": True},\n    {""haar_full_mass"": 2},\n    {""num_quant_bins"": 8},\n], ids=str)\ndef test_regional_smoke(duration, forecast, options):\n    num_regions = 6\n    coupling = torch.eye(num_regions).clamp(min=0.1)\n    population = torch.tensor([2., 3., 4., 10., 100., 1000.])\n    recovery_time = 7.0\n\n    # Generate data.\n    model = RegionalSIRModel(population, coupling, recovery_time,\n                             data=[None] * duration)\n    assert model.full_mass == [(""R0"", ""rho_c1"", ""rho_c0"", ""rho"")]\n    for attempt in range(100):\n        data = model.generate({""R0"": 1.5, ""rho"": 0.5})[""obs""]\n        assert data.shape == (duration, num_regions)\n        if data.sum():\n            break\n    assert data.sum() > 0, ""failed to generate positive data""\n\n    # Infer.\n    model = RegionalSIRModel(population, coupling, recovery_time, data)\n    num_samples = 5\n    model.fit(warmup_steps=1, num_samples=num_samples, max_tree_depth=2, **options)\n\n    # Predict and forecast.\n    samples = model.predict(forecast=forecast)\n    assert samples[""S""].shape == (num_samples, duration + forecast, num_regions)\n    assert samples[""I""].shape == (num_samples, duration + forecast, num_regions)\n'"
tests/contrib/epidemiology/test_quant.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\nimport torch\n\nfrom pyro.contrib.epidemiology.util import compute_bin_probs\n\n\n@pytest.mark.parametrize(""num_quant_bins"", [2, 4, 8, 12, 16])\ndef test_quantization_scheme(num_quant_bins, num_samples=1000 * 1000):\n    min, max = 0, 7\n    probs = torch.zeros(max + 1)\n\n    x = torch.linspace(-0.5, max + 0.5, num_samples)\n    bin_probs = compute_bin_probs(x - x.floor(), num_quant_bins=num_quant_bins)\n    x_floor = x.floor()\n\n    q_min = 1 - num_quant_bins // 2\n    q_max = 1 + num_quant_bins // 2\n\n    for k, q in enumerate(range(q_min, q_max)):\n        y = (x_floor + q).long()\n        y = torch.max(y, 2 * min - 1 - y)\n        y = torch.min(y, 2 * max + 1 - y)\n        probs.scatter_add_(0, y, bin_probs[:, k] / num_samples)\n\n    max_deviation = (probs - 1.0 / (max + 1.0)).abs().max().item()\n    assert max_deviation < 1.0e-4\n'"
tests/contrib/epidemiology/test_util.py,9,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.contrib.epidemiology.util import cat2, clamp\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize(""min"", [None, 0., (), (2,)], ids=str)\n@pytest.mark.parametrize(""max"", [None, 1., (), (2,)], ids=str)\n@pytest.mark.parametrize(""shape"", [(2,), (3, 2)], ids=str)\ndef test_clamp(shape, min, max):\n    tensor = torch.randn(shape)\n    if isinstance(min, tuple):\n        min = torch.zeros(min)\n    if isinstance(max, tuple):\n        max = torch.ones(max)\n\n    actual = clamp(tensor, min=min, max=max)\n\n    expected = tensor\n    if min is not None:\n        min = torch.as_tensor(min).expand_as(tensor)\n        expected = torch.max(min, expected)\n    if max is not None:\n        max = torch.as_tensor(max).expand_as(tensor)\n        expected = torch.min(max, expected)\n\n    assert_equal(actual, expected)\n\n\n@pytest.mark.parametrize(""shape"", [(), (2,), (2, 3), (2, 3, 4)], ids=str)\ndef test_cat2_scalar(shape):\n    tensor = torch.randn(shape)\n    for dim in range(-len(shape), 0):\n        expected_shape = list(shape)\n        expected_shape[dim] += 1\n        expected_shape = torch.Size(expected_shape)\n        assert cat2(tensor, 0, dim=dim).shape == expected_shape\n        assert cat2(0, tensor, dim=dim).shape == expected_shape\n        assert (cat2(tensor, 0, dim=dim).unbind(dim)[-1] == 0).all()\n        assert (cat2(0, tensor, dim=dim).unbind(dim)[0] == 0).all()\n'"
tests/contrib/forecast/__init__.py,0,b''
tests/contrib/forecast/test_evaluate.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.forecast import Forecaster, ForecastingModel, HMCForecaster, backtest\nfrom pyro.contrib.forecast.evaluate import DEFAULT_METRICS\nfrom pyro.util import optional\n\n\nclass Model(ForecastingModel):\n    def model(self, zero_data, covariates):\n        loc = zero_data[..., :1, :]\n        scale = pyro.sample(""scale"", dist.LogNormal(loc, 1).to_event(1))\n\n        with self.time_plate:\n            jumps = pyro.sample(""jumps"", dist.Normal(0, scale).to_event(1))\n        prediction = jumps.cumsum(-2)\n\n        noise_dist = dist.Normal(zero_data, 1)\n        self.predict(noise_dist, prediction)\n\n\nWINDOWS = [\n    (None, 1, None, 1, 8),\n    (None, 10, None, 10, 1),\n    (10, 1, None, 3, 5),\n    (None, 5, 10, 1, 5),\n    (7, 1, 7, 1, 7),\n    (14, 1, 7, 1, 1),\n]\n\n\n@pytest.mark.parametrize(""train_window,min_train_window,test_window,min_test_window,stride"", WINDOWS)\n@pytest.mark.parametrize(""warm_start"", [False, True], ids=[""cold"", ""warm""])\ndef test_simple(train_window, min_train_window, test_window, min_test_window, stride, warm_start):\n    duration = 30\n    obs_dim = 2\n    covariates = torch.zeros(duration, 0)\n    data = torch.randn(duration, obs_dim) + 4\n    forecaster_options = {""num_steps"": 2, ""warm_start"": warm_start}\n\n    expect_error = (warm_start and train_window is not None)\n    with optional(pytest.raises(ValueError), expect_error):\n        windows = backtest(data, covariates, Model,\n                           train_window=train_window,\n                           min_train_window=min_train_window,\n                           test_window=test_window,\n                           min_test_window=min_test_window,\n                           stride=stride,\n                           forecaster_options=forecaster_options)\n    if not expect_error:\n        assert any(window[""t0""] == 0 for window in windows)\n        if stride == 1:\n            assert any(window[""t2""] == duration for window in windows)\n        for window in windows:\n            assert window[""train_walltime""] >= 0\n            assert window[""test_walltime""] >= 0\n            for name in DEFAULT_METRICS:\n                assert name in window\n                assert 0 < window[name] < math.inf\n\n\n@pytest.mark.parametrize(""train_window,min_train_window,test_window,min_test_window,stride"", WINDOWS)\n@pytest.mark.parametrize(""engine"", [""svi"", ""hmc""])\ndef test_poisson(train_window, min_train_window, test_window, min_test_window, stride, engine):\n    duration = 30\n    obs_dim = 2\n    covariates = torch.zeros(duration, 0)\n    rate = torch.randn(duration, obs_dim) + 4\n    counts = dist.Poisson(rate).sample()\n\n    # Transform count data to log domain.\n    data = counts.log1p()\n\n    def transform(pred, truth):\n        pred = dist.Poisson(pred.clamp(min=1e-4).expm1()).sample()\n        truth = truth.expm1()\n        return pred, truth\n\n    if engine == ""svi"":\n        forecaster_fn = Forecaster\n        forecaster_options = {""num_steps"": 2}\n    else:\n        forecaster_fn = HMCForecaster\n        forecaster_options = {""num_warmup"": 1, ""num_samples"": 1}\n\n    windows = backtest(data, covariates, Model,\n                       forecaster_fn=forecaster_fn,\n                       transform=transform,\n                       train_window=train_window,\n                       min_train_window=min_train_window,\n                       test_window=test_window,\n                       min_test_window=min_test_window,\n                       stride=stride,\n                       forecaster_options=forecaster_options)\n\n    assert any(window[""t0""] == 0 for window in windows)\n    if stride == 1:\n        assert any(window[""t0""] == 0 for window in windows)\n        assert any(window[""t2""] == duration for window in windows)\n    for name in DEFAULT_METRICS:\n        for window in windows:\n            assert name in window\n            assert 0 < window[name] < math.inf\n\n\ndef test_custom_warm_start():\n    duration = 30\n    obs_dim = 2\n    covariates = torch.zeros(duration, 0)\n    data = torch.randn(duration, obs_dim) + 4\n    min_train_window = 10\n\n    def forecaster_options(t0, t1, t2):\n        if t1 == min_train_window:\n            return {""num_steps"": 2, ""warm_start"": True}\n        else:\n            return {""num_steps"": 0, ""warm_start"": True}\n\n    backtest(data, covariates, Model,\n             min_train_window=min_train_window,\n             test_window=10,\n             forecaster_options=forecaster_options)\n'"
tests/contrib/forecast/test_forecaster.py,15,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.contrib.forecast import Forecaster, ForecastingModel, HMCForecaster\nfrom pyro.infer.autoguide import AutoDelta\nfrom pyro.infer.reparam import LinearHMMReparam, StableReparam\nfrom pyro.optim import Adam\n\n\nclass Model0(ForecastingModel):\n    def model(self, zero_data, covariates):\n        with pyro.plate_stack(""batch"", zero_data.shape[:-2], rightmost_dim=-2):\n            loc = zero_data[..., :1, 0]\n            scale = pyro.sample(""scale"", dist.LogNormal(loc, 1))\n\n            with self.time_plate:\n                jumps = pyro.sample(""jumps"", dist.Normal(0, scale))\n            prediction = jumps.cumsum(-1).unsqueeze(-1) + zero_data\n\n            noise_dist = dist.Laplace(zero_data, 1)\n            self.predict(noise_dist, prediction)\n\n\nclass Model1(ForecastingModel):\n    def model(self, zero_data, covariates):\n        with pyro.plate_stack(""batch"", zero_data.shape[:-2], rightmost_dim=-2):\n            loc = zero_data[..., :1, :]\n            scale = pyro.sample(""scale"", dist.LogNormal(loc, 1).to_event(1))\n\n            with self.time_plate:\n                jumps = pyro.sample(""jumps"", dist.Normal(0, scale).to_event(1))\n            prediction = jumps.cumsum(-2)\n\n            noise_dist = dist.Laplace(0, 1)\n            self.predict(noise_dist, prediction)\n\n\nclass Model2(ForecastingModel):\n    def model(self, zero_data, covariates):\n        with pyro.plate_stack(""batch"", zero_data.shape[:-2], rightmost_dim=-2):\n            loc = zero_data[..., :1, :]\n            scale = pyro.sample(""scale"", dist.LogNormal(loc, 1).to_event(1))\n\n            with self.time_plate:\n                jumps = pyro.sample(""jumps"", dist.Normal(0, scale).to_event(1))\n            prediction = jumps.cumsum(-2)\n\n            scale_tril = torch.eye(zero_data.size(-1))\n            noise_dist = dist.MultivariateNormal(zero_data, scale_tril=scale_tril)\n            self.predict(noise_dist, prediction)\n\n\nclass Model3(ForecastingModel):\n    def model(self, zero_data, covariates):\n        with pyro.plate_stack(""batch"", zero_data.shape[:-2], rightmost_dim=-2):\n            loc = zero_data[..., :1, :]\n            scale = pyro.sample(""scale"", dist.LogNormal(loc, 1).to_event(1))\n\n            with self.time_plate:\n                jumps = pyro.sample(""jumps"", dist.Normal(0, scale).to_event(1))\n            prediction = jumps.cumsum(-2)\n\n            duration, obs_dim = zero_data.shape[-2:]\n            noise_dist = dist.GaussianHMM(\n                dist.Normal(0, 1).expand([obs_dim]).to_event(1),\n                torch.eye(obs_dim),\n                dist.Normal(0, 1).expand([obs_dim]).to_event(1),\n                torch.eye(obs_dim),\n                dist.Normal(0, 1).expand([obs_dim]).to_event(1),\n                duration=duration,\n            )\n            self.predict(noise_dist, prediction)\n\n\nclass Model4(ForecastingModel):\n    def model(self, zero_data, covariates):\n        with pyro.plate_stack(""batch"", zero_data.shape[:-2], rightmost_dim=-2):\n            loc = zero_data[..., :1, :]\n            scale = pyro.sample(""scale"", dist.LogNormal(loc, 1).to_event(1))\n\n            with self.time_plate:\n                jumps = pyro.sample(""jumps"", dist.Normal(0, scale).to_event(1))\n            prediction = jumps.cumsum(-2)\n\n            duration, obs_dim = zero_data.shape[-2:]\n            noise_dist = dist.LinearHMM(\n                dist.Stable(1.9, 0).expand([obs_dim]).to_event(1),\n                torch.eye(obs_dim),\n                dist.Stable(1.9, 0).expand([obs_dim]).to_event(1),\n                torch.eye(obs_dim),\n                dist.Stable(1.9, 0).expand([obs_dim]).to_event(1),\n                duration=duration,\n            )\n            rep = StableReparam()\n            with poutine.reparam(config={""residual"": LinearHMMReparam(rep, rep, rep)}):\n                self.predict(noise_dist, prediction)\n\n\n@pytest.mark.parametrize(""t_obs"", [1, 7])\n@pytest.mark.parametrize(""t_forecast"", [1, 3])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""cov_dim"", [0, 1, 6])\n@pytest.mark.parametrize(""obs_dim"", [1, 2])\n@pytest.mark.parametrize(""dct_gradients"", [False, True])\n@pytest.mark.parametrize(""Model"", [Model0, Model1, Model2, Model3, Model4])\n@pytest.mark.parametrize(""engine"", [""svi"", ""hmc""])\ndef test_smoke(Model, batch_shape, t_obs, t_forecast, obs_dim, cov_dim, dct_gradients, engine):\n    model = Model()\n    data = torch.randn(batch_shape + (t_obs, obs_dim))\n    covariates = torch.randn(batch_shape + (t_obs + t_forecast, cov_dim))\n\n    if engine == ""svi"":\n        forecaster = Forecaster(model, data, covariates[..., :t_obs, :],\n                                num_steps=2, log_every=1, dct_gradients=dct_gradients)\n    else:\n        if dct_gradients is True:\n            pytest.skip(""Duplicated test."")\n        forecaster = HMCForecaster(model, data, covariates[..., :t_obs, :], max_tree_depth=1,\n                                   num_warmup=1, num_samples=1, jit_compile=False)\n\n    num_samples = 5\n    samples = forecaster(data, covariates, num_samples)\n    assert samples.shape == (num_samples,) + batch_shape + (t_forecast, obs_dim,)\n    samples = forecaster(data, covariates, num_samples, batch_size=2)\n    assert samples.shape == (num_samples,) + batch_shape + (t_forecast, obs_dim,)\n\n\n@pytest.mark.parametrize(""subsample_aware"", [False, True])\ndef test_svi_custom_smoke(subsample_aware):\n    t_obs = 5\n    t_forecast = 4\n    cov_dim = 3\n    obs_dim = 2\n\n    model = Model0()\n    data = torch.randn(t_obs, obs_dim)\n    covariates = torch.randn(t_obs + t_forecast, cov_dim)\n    guide = AutoDelta(model)\n    optim = Adam({})\n\n    Forecaster(model, data, covariates[..., :t_obs, :],\n               guide=guide, optim=optim, subsample_aware=subsample_aware,\n               num_steps=2, log_every=1)\n\n\nclass SubsampleModel3(ForecastingModel):\n    def model(self, zero_data, covariates):\n        with pyro.plate(""batch"", len(zero_data), dim=-2):\n            zero_data = pyro.subsample(zero_data, event_dim=1)\n            covariates = pyro.subsample(covariates, event_dim=1)\n\n            loc = zero_data[..., :1, :]\n            scale = pyro.sample(""scale"", dist.LogNormal(loc, 1).to_event(1))\n\n            with self.time_plate:\n                jumps = pyro.sample(""jumps"", dist.Normal(0, scale).to_event(1))\n            prediction = jumps.cumsum(-2)\n\n            duration, obs_dim = zero_data.shape[-2:]\n            noise_dist = dist.GaussianHMM(\n                dist.Normal(0, 1).expand([obs_dim]).to_event(1),\n                torch.eye(obs_dim),\n                dist.Normal(0, 1).expand([obs_dim]).to_event(1),\n                torch.eye(obs_dim),\n                dist.Normal(0, 1).expand([obs_dim]).to_event(1),\n                duration=duration,\n            )\n            self.predict(noise_dist, prediction)\n\n\nclass SubsampleModel4(ForecastingModel):\n    def model(self, zero_data, covariates):\n        with pyro.plate(""batch"", len(zero_data), dim=-2):\n            zero_data = pyro.subsample(zero_data, event_dim=1)\n            covariates = pyro.subsample(covariates, event_dim=1)\n\n            loc = zero_data[..., :1, :]\n            scale = pyro.sample(""scale"", dist.LogNormal(loc, 1).to_event(1))\n\n            with self.time_plate:\n                jumps = pyro.sample(""jumps"", dist.Normal(0, scale).to_event(1))\n            prediction = jumps.cumsum(-2)\n\n            duration, obs_dim = zero_data.shape[-2:]\n            noise_dist = dist.LinearHMM(\n                dist.Stable(1.9, 0).expand([obs_dim]).to_event(1),\n                torch.eye(obs_dim),\n                dist.Stable(1.9, 0).expand([obs_dim]).to_event(1),\n                torch.eye(obs_dim),\n                dist.Stable(1.9, 0).expand([obs_dim]).to_event(1),\n                duration=duration,\n            )\n            rep = StableReparam()\n            with poutine.reparam(config={""residual"": LinearHMMReparam(rep, rep, rep)}):\n                self.predict(noise_dist, prediction)\n\n\n@pytest.mark.parametrize(""t_obs"", [1, 7])\n@pytest.mark.parametrize(""t_forecast"", [1, 3])\n@pytest.mark.parametrize(""cov_dim"", [0, 6])\n@pytest.mark.parametrize(""obs_dim"", [1, 2])\n@pytest.mark.parametrize(""Model"", [SubsampleModel3, SubsampleModel4])\ndef test_subsample_smoke(Model, t_obs, t_forecast, obs_dim, cov_dim):\n    batch_shape = (4,)\n    model = Model()\n    data = torch.randn(batch_shape + (t_obs, obs_dim))\n    covariates = torch.randn(batch_shape + (t_obs + t_forecast, cov_dim))\n\n    def create_plates(zero_data, covariates):\n        size = len(zero_data)\n        return pyro.plate(""batch"", size, subsample_size=2, dim=-2)\n\n    forecaster = Forecaster(model, data, covariates[..., :t_obs, :],\n                            num_steps=2, log_every=1, create_plates=create_plates)\n\n    num_samples = 5\n    samples = forecaster(data, covariates, num_samples)\n    assert samples.shape == (num_samples,) + batch_shape + (t_forecast, obs_dim,)\n    samples = forecaster(data, covariates, num_samples, batch_size=2)\n    assert samples.shape == (num_samples,) + batch_shape + (t_forecast, obs_dim,)\n'"
tests/contrib/forecast/test_util.py,5,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.distributions import transform_to\n\nimport pyro.distributions as dist\nfrom pyro.contrib.forecast.util import UNIVARIATE_DISTS, UNIVARIATE_TRANSFORMS, prefix_condition, reshape_batch\nfrom tests.ops.gaussian import random_mvn\n\nDISTS = [\n    dist.Bernoulli,\n    dist.Beta,\n    dist.BetaBinomial,\n    dist.Cauchy,\n    dist.Dirichlet,\n    dist.DirichletMultinomial,\n    dist.Exponential,\n    dist.FoldedDistribution,\n    dist.Gamma,\n    dist.GammaPoisson,\n    dist.GaussianHMM,\n    dist.Geometric,\n    dist.IndependentHMM,\n    dist.InverseGamma,\n    dist.Laplace,\n    dist.LinearHMM,\n    dist.LogNormal,\n    dist.MultivariateNormal,\n    dist.NegativeBinomial,\n    dist.Normal,\n    dist.StudentT,\n    dist.ZeroInflatedPoisson,\n    dist.ZeroInflatedNegativeBinomial,\n]\n\n\ndef random_dist(Dist, shape, transform=None):\n    if Dist is dist.FoldedDistribution:\n        return Dist(random_dist(dist.Normal, shape))\n    elif Dist in (dist.GaussianHMM, dist.LinearHMM):\n        batch_shape, duration, obs_dim = shape[:-2], shape[-2], shape[-1]\n        hidden_dim = obs_dim + 1\n        init_dist = random_dist(dist.Normal, batch_shape + (hidden_dim,)).to_event(1)\n        trans_mat = torch.randn(batch_shape + (duration, hidden_dim, hidden_dim))\n        trans_dist = random_dist(dist.Normal, batch_shape + (duration, hidden_dim)).to_event(1)\n        obs_mat = torch.randn(batch_shape + (duration, hidden_dim, obs_dim))\n        obs_dist = random_dist(dist.Normal, batch_shape + (duration, obs_dim)).to_event(1)\n        if Dist is dist.LinearHMM and transform is not None:\n            obs_dist = dist.TransformedDistribution(obs_dist, transform)\n        return Dist(init_dist, trans_mat, trans_dist, obs_mat, obs_dist,\n                    duration=duration)\n    elif Dist is dist.IndependentHMM:\n        batch_shape, duration, obs_dim = shape[:-2], shape[-2], shape[-1]\n        base_shape = batch_shape + (obs_dim, duration, 1)\n        base_dist = random_dist(dist.GaussianHMM, base_shape)\n        return Dist(base_dist)\n    elif Dist is dist.MultivariateNormal:\n        return random_mvn(shape[:-1], shape[-1])\n    else:\n        params = {\n            name: transform_to(Dist.arg_constraints[name])(torch.rand(shape) - 0.5)\n            for name in UNIVARIATE_DISTS[Dist]}\n        return Dist(**params)\n\n\n@pytest.mark.parametrize(""dim"", [1, 7])\n@pytest.mark.parametrize(""t,f"", [(1, 1), (2, 1), (3, 2)])\n@pytest.mark.parametrize(""batch_shape"", [(), (6,), (5, 4)])\n@pytest.mark.parametrize(""Dist"", DISTS)\ndef test_prefix_condition(Dist, batch_shape, t, f, dim):\n    if Dist is dist.LinearHMM:\n        pytest.xfail(reason=""not implemented"")\n    duration = t + f\n    d = random_dist(Dist, batch_shape + (duration, dim))\n    d = d.to_event(2 - d.event_dim)\n    data = d.sample()\n    expected = d.log_prob(data)\n    d2 = prefix_condition(d, data[..., :t, :])\n    actual = d2.log_prob(data[..., t:, :])\n    actual.shape == expected.shape\n\n\n@pytest.mark.parametrize(""dim"", [1, 7])\n@pytest.mark.parametrize(""duration"", [1, 2, 3])\n@pytest.mark.parametrize(""batch_shape"", [(), (6,), (5, 4)])\n@pytest.mark.parametrize(""Dist"", DISTS)\ndef test_reshape_batch(Dist, batch_shape, duration, dim):\n    d = random_dist(Dist, batch_shape + (duration, dim))\n    d = d.to_event(2 - d.event_dim)\n    assert d.batch_shape == batch_shape\n    assert d.event_shape == (duration, dim)\n\n    actual = reshape_batch(d, batch_shape + (1,))\n    assert type(actual) is type(d)\n    assert actual.batch_shape == batch_shape + (1,)\n    assert actual.event_shape == (duration, dim)\n\n\n@pytest.mark.parametrize(""dim"", [1, 7])\n@pytest.mark.parametrize(""duration"", [1, 2, 3])\n@pytest.mark.parametrize(""batch_shape"", [(), (6,), (5, 4)])\n@pytest.mark.parametrize(""transform"", list(UNIVARIATE_TRANSFORMS.keys()))\ndef test_reshape_transform_batch(transform, batch_shape, duration, dim):\n    params = {p: torch.rand(batch_shape + (duration, dim))\n              for p in UNIVARIATE_TRANSFORMS[transform]}\n    t = transform(**params)\n    d = random_dist(dist.LinearHMM, batch_shape + (duration, dim), transform=t)\n    d = d.to_event(2 - d.event_dim)\n    assert d.batch_shape == batch_shape\n    assert d.event_shape == (duration, dim)\n\n    actual = reshape_batch(d, batch_shape + (1,))\n    assert type(actual) is type(d)\n    assert actual.batch_shape == batch_shape + (1,)\n    assert actual.event_shape == (duration, dim)\n\n    # test if we have reshape transforms correctly\n    assert actual.rsample().shape == actual.shape()\n'"
tests/contrib/gp/__init__.py,0,b''
tests/contrib/gp/test_conditional.py,12,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\n\nimport pytest\nimport torch\n\nimport pyro\nfrom pyro.contrib.gp.kernels import Matern52, WhiteNoise\nfrom pyro.contrib.gp.util import conditional\nfrom tests.common import assert_equal\n\nT = namedtuple(""TestConditional"", [""Xnew"", ""X"", ""kernel"", ""f_loc"", ""f_scale_tril"",\n                                   ""loc"", ""cov""])\n\nXnew = torch.tensor([[2., 3.], [4., 6.]])\nX = torch.tensor([[1., 5.], [2., 1.], [3., 2.]])\nkernel = Matern52(input_dim=2)\nKff = kernel(X) + torch.eye(3) * 1e-6\nLff = Kff.cholesky()\npyro.set_rng_seed(123)\nf_loc = torch.rand(3)\nf_scale_tril = torch.rand(3, 3).tril(-1) + torch.rand(3).exp().diag()\nf_cov = f_scale_tril.matmul(f_scale_tril.t())\n\nTEST_CASES = [\n    T(\n        Xnew, X, kernel, torch.zeros(3), Lff, torch.zeros(2), None\n    ),\n    T(\n        Xnew, X, kernel, torch.zeros(3), None, torch.zeros(2), None\n    ),\n    T(\n        Xnew, X, kernel, f_loc, Lff, None, kernel(Xnew)\n    ),\n    T(\n        X, X, kernel, f_loc, f_scale_tril, f_loc, f_cov\n    ),\n    T(\n        X, X, kernel, f_loc, None, f_loc, torch.zeros(3, 3)\n    ),\n    T(\n        Xnew, X, WhiteNoise(input_dim=2), f_loc, f_scale_tril, torch.zeros(2), torch.eye(2)\n    ),\n    T(\n        Xnew, X, WhiteNoise(input_dim=2), f_loc, None, torch.zeros(2), torch.eye(2)\n    ),\n]\n\nTEST_IDS = [str(i) for i in range(len(TEST_CASES))]\n\n\n@pytest.mark.parametrize(""Xnew, X, kernel, f_loc, f_scale_tril, loc, cov"",\n                         TEST_CASES, ids=TEST_IDS)\ndef test_conditional(Xnew, X, kernel, f_loc, f_scale_tril, loc, cov):\n    loc0, cov0 = conditional(Xnew, X, kernel, f_loc, f_scale_tril, full_cov=True)\n    loc1, var1 = conditional(Xnew, X, kernel, f_loc, f_scale_tril, full_cov=False)\n\n    if loc is not None:\n        assert_equal(loc0, loc)\n        assert_equal(loc1, loc)\n    n = cov0.shape[-1]\n    var0 = torch.stack([mat.diag() for mat in cov0.view(-1, n, n)]).reshape(cov0.shape[:-1])\n    assert_equal(var0, var1)\n    if cov is not None:\n        assert_equal(cov0, cov)\n\n\n@pytest.mark.parametrize(""Xnew, X, kernel, f_loc, f_scale_tril, loc, cov"",\n                         TEST_CASES, ids=TEST_IDS)\ndef test_conditional_whiten(Xnew, X, kernel, f_loc, f_scale_tril, loc, cov):\n    if f_scale_tril is None:\n        return\n\n    loc0, cov0 = conditional(Xnew, X, kernel, f_loc, f_scale_tril, full_cov=True,\n                             whiten=False)\n    Kff = kernel(X) + torch.eye(3) * 1e-6\n    Lff = Kff.cholesky()\n    whiten_f_loc = Lff.inverse().matmul(f_loc)\n    whiten_f_scale_tril = Lff.inverse().matmul(f_scale_tril)\n    loc1, cov1 = conditional(Xnew, X, kernel, whiten_f_loc, whiten_f_scale_tril,\n                             full_cov=True, whiten=True)\n\n    assert_equal(loc0, loc1)\n    assert_equal(cov0, cov1)\n'"
tests/contrib/gp/test_kernels.py,16,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\n\nimport pytest\nimport torch\n\nfrom pyro.contrib.gp.kernels import (RBF, Brownian, Constant, Coregionalize, Cosine, Exponent,\n                                     Exponential, Linear, Matern32, Matern52, Periodic,\n                                     Polynomial, Product, RationalQuadratic, Sum,\n                                     VerticalScaling, Warping, WhiteNoise)\nfrom tests.common import assert_equal\n\nT = namedtuple(""TestGPKernel"", [""kernel"", ""X"", ""Z"", ""K_sum""])\n\nvariance = torch.tensor([3.0])\nlengthscale = torch.tensor([2.0, 1.0, 2.0])\nX = torch.tensor([[1.0, 0.0, 1.0], [2.0, 1.0, 3.0]])\nZ = torch.tensor([[4.0, 5.0, 6.0], [3.0, 1.0, 7.0], [3.0, 1.0, 2.0]])\n\nTEST_CASES = [\n    T(\n        Constant(3, variance),\n        X=X, Z=Z, K_sum=18\n    ),\n    T(\n        Brownian(1, variance),\n        # only work on 1D input\n        X=X[:, 0], Z=Z[:, 0], K_sum=27\n    ),\n    T(\n        Cosine(3, variance, lengthscale),\n        X=X, Z=Z, K_sum=-0.193233\n    ),\n    T(\n        Linear(3, variance),\n        X=X, Z=Z, K_sum=291\n    ),\n    T(\n        Exponential(3, variance, lengthscale),\n        X=X, Z=Z, K_sum=2.685679\n    ),\n    T(\n        Matern32(3, variance, lengthscale),\n        X=X, Z=Z, K_sum=3.229314\n    ),\n    T(\n        Matern52(3, variance, lengthscale),\n        X=X, Z=Z, K_sum=3.391847\n    ),\n    T(\n        Periodic(3, variance, lengthscale, period=torch.ones(1)),\n        X=X, Z=Z, K_sum=18\n    ),\n    T(\n        Polynomial(3, variance, degree=2),\n        X=X, Z=Z, K_sum=7017\n    ),\n    T(\n        RationalQuadratic(3, variance, lengthscale, scale_mixture=torch.ones(1)),\n        X=X, Z=Z, K_sum=5.684670\n    ),\n    T(\n        RBF(3, variance, lengthscale),\n        X=X, Z=Z, K_sum=3.681117\n    ),\n    T(\n        WhiteNoise(3, variance, lengthscale),\n        X=X, Z=Z, K_sum=0\n    ),\n    T(\n        WhiteNoise(3, variance, lengthscale),\n        X=X, Z=None, K_sum=6\n    ),\n    T(\n        Coregionalize(3, components=torch.eye(3, 3)),\n        X=torch.tensor([[1., 0., 0.],\n                        [0.5, 0., 0.5]]),\n        Z=torch.tensor([[1., 0., 0.],\n                        [0., 1., 0.]]),\n        K_sum=2.25,\n    ),\n    T(\n        Coregionalize(3, rank=2),\n        X=torch.tensor([[1., 0., 0.],\n                        [0.5, 0., 0.5]]),\n        Z=torch.tensor([[1., 0., 0.],\n                        [0., 1., 0.]]),\n        K_sum=None,  # kernel is randomly initialized\n    ),\n    T(\n        Coregionalize(3),\n        X=torch.tensor([[1., 0., 0.],\n                        [0.5, 0., 0.5]]),\n        Z=torch.tensor([[1., 0., 0.],\n                        [0., 1., 0.]]),\n        K_sum=None,  # kernel is randomly initialized\n    ),\n    T(\n        Coregionalize(3, rank=2, diagonal=0.01 * torch.ones(3)),\n        X=torch.tensor([[1., 0., 0.],\n                        [0.5, 0., 0.5]]),\n        Z=torch.tensor([[1., 0., 0.],\n                        [0., 1., 0.]]),\n        K_sum=None,  # kernel is randomly initialized\n    ),\n]\n\nTEST_IDS = [t[0].__class__.__name__ for t in TEST_CASES]\n\n\n@pytest.mark.parametrize(""kernel, X, Z, K_sum"", TEST_CASES, ids=TEST_IDS)\ndef test_kernel_forward(kernel, X, Z, K_sum):\n    K = kernel(X, Z)\n    assert K.shape == (X.shape[0], (X if Z is None else Z).shape[0])\n    if K_sum is not None:\n        assert_equal(K.sum().item(), K_sum)\n    assert_equal(kernel(X).diag(), kernel(X, diag=True))\n    if not isinstance(kernel, WhiteNoise):  # WhiteNoise avoids computing a delta function by assuming X != Z\n        assert_equal(kernel(X), kernel(X, X))\n    if Z is not None:\n        assert_equal(kernel(X, Z), kernel(Z, X).t())\n\n\ndef test_combination():\n    k0 = TEST_CASES[0][0]\n    k5 = TEST_CASES[5][0]   # TEST_CASES[1] is Brownian, only work for 1D\n    k2 = TEST_CASES[2][0]\n    k3 = TEST_CASES[3][0]\n    k4 = TEST_CASES[4][0]\n\n    k = Sum(Product(Product(Sum(Sum(k0, k5), k2), 2), k3), Sum(k4, 1))\n\n    K = 2 * (k0(X, Z) + k5(X, Z) + k2(X, Z)) * k3(X, Z) + (k4(X, Z) + 1)\n\n    assert_equal(K.data, k(X, Z).data)\n\n\ndef test_active_dims_overlap_ok():\n    k1 = Matern52(2, variance, lengthscale[0], active_dims=[0, 1])\n    k2 = Matern32(2, variance, lengthscale[0], active_dims=[1, 2])\n    Sum(k1, k2)\n\n\ndef test_active_dims_disjoint_ok():\n    k1 = Matern52(2, variance, lengthscale[0], active_dims=[0, 1])\n    k2 = Matern32(1, variance, lengthscale[0], active_dims=[2])\n    Sum(k1, k2)\n\n\ndef test_transforming():\n    k = TEST_CASES[6][0]\n\n    def vscaling_fn(x):\n        return x.sum(dim=1)\n\n    def iwarping_fn(x):\n        return x**2\n\n    owarping_coef = [2, 0, 1, 3, 0]\n\n    K = k(X, Z)\n    K_iwarp = k(iwarping_fn(X), iwarping_fn(Z))\n    K_owarp = 2 + K ** 2 + 3 * K ** 3\n    K_vscale = vscaling_fn(X).unsqueeze(1) * K * vscaling_fn(Z).unsqueeze(0)\n\n    assert_equal(K_iwarp.data, Warping(k, iwarping_fn=iwarping_fn)(X, Z).data)\n    assert_equal(K_owarp.data, Warping(k, owarping_coef=owarping_coef)(X, Z).data)\n    assert_equal(K_vscale.data, VerticalScaling(k, vscaling_fn=vscaling_fn)(X, Z).data)\n    assert_equal(K.exp().data, Exponent(k)(X, Z).data)\n'"
tests/contrib/gp/test_likelihoods.py,15,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\n\nimport pytest\nimport torch\n\nfrom pyro.contrib.gp.kernels import RBF\nfrom pyro.contrib.gp.likelihoods import Binary, MultiClass, Poisson\nfrom pyro.contrib.gp.models import VariationalGP, VariationalSparseGP\nfrom pyro.contrib.gp.util import train\n\n\nT = namedtuple(""TestGPLikelihood"", [""model_class"", ""X"", ""y"", ""kernel"", ""likelihood""])\n\nX = torch.tensor([[1.0, 5.0, 3.0], [4.0, 3.0, 7.0], [3.0, 4.0, 6.0]])\nkernel = RBF(input_dim=3, variance=torch.tensor(1.), lengthscale=torch.tensor(3.))\nnoise = torch.tensor(1e-6)\ny_binary1D = torch.tensor([0.0, 1.0, 0.0])\ny_binary2D = torch.tensor([[0.0, 1.0, 1.0], [1.0, 0.0, 1.0]])\nbinary_likelihood = Binary()\ny_count1D = torch.tensor([0.0, 1.0, 4.0])\ny_count2D = torch.tensor([[5.0, 9.0, 3.0], [4.0, 0.0, 1.0]])\npoisson_likelihood = Poisson()\ny_multiclass1D = torch.tensor([2.0, 0.0, 1.0])\ny_multiclass2D = torch.tensor([[2.0, 1.0, 1.0], [0.0, 2.0, 1.0]])\nmulticlass_likelihood = MultiClass(num_classes=3)\n\nTEST_CASES = [\n    T(\n        VariationalGP,\n        X, y_binary1D, kernel, binary_likelihood\n    ),\n    T(\n        VariationalGP,\n        X, y_binary2D, kernel, binary_likelihood\n    ),\n    T(\n        VariationalGP,\n        X, y_multiclass1D, kernel, multiclass_likelihood\n    ),\n    T(\n        VariationalGP,\n        X, y_multiclass2D, kernel, multiclass_likelihood\n    ),\n    T(\n        VariationalGP,\n        X, y_count1D, kernel, poisson_likelihood\n    ),\n    T(\n        VariationalGP,\n        X, y_count2D, kernel, poisson_likelihood\n    ),\n    T(\n        VariationalSparseGP,\n        X, y_binary1D, kernel, binary_likelihood\n    ),\n    T(\n        VariationalSparseGP,\n        X, y_binary2D, kernel, binary_likelihood\n    ),\n    T(\n        VariationalSparseGP,\n        X, y_multiclass1D, kernel, multiclass_likelihood\n    ),\n    T(\n        VariationalSparseGP,\n        X, y_multiclass2D, kernel, multiclass_likelihood\n    ),\n    T(\n        VariationalSparseGP,\n        X, y_count1D, kernel, poisson_likelihood\n    ),\n    T(\n        VariationalSparseGP,\n        X, y_count2D, kernel, poisson_likelihood\n    ),\n]\n\nTEST_IDS = [""_"".join([t[0].__name__, t[4].__class__.__name__.split(""."")[-1],\n                      str(t[2].dim()) + ""D""])\n            for t in TEST_CASES]\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", TEST_CASES, ids=TEST_IDS)\ndef test_inference(model_class, X, y, kernel, likelihood):\n    if isinstance(likelihood, MultiClass):\n        latent_shape = y.shape[:-1] + (likelihood.num_classes,)\n    else:\n        latent_shape = y.shape[:-1]\n    if model_class is VariationalSparseGP:\n        gp = model_class(X, y, kernel, X, likelihood, latent_shape=latent_shape)\n    else:\n        gp = model_class(X, y, kernel, likelihood, latent_shape=latent_shape)\n\n    train(gp, num_steps=1)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", TEST_CASES, ids=TEST_IDS)\ndef test_inference_with_empty_latent_shape(model_class, X, y, kernel, likelihood):\n    if isinstance(likelihood, MultiClass):\n        latent_shape = torch.Size([likelihood.num_classes])\n    else:\n        latent_shape = torch.Size([])\n    if model_class is VariationalSparseGP:\n        gp = model_class(X, y, kernel, X, likelihood, latent_shape=latent_shape)\n    else:\n        gp = model_class(X, y, kernel, likelihood, latent_shape=latent_shape)\n\n    train(gp, num_steps=1)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", TEST_CASES, ids=TEST_IDS)\ndef test_forward(model_class, X, y, kernel, likelihood):\n    if isinstance(likelihood, MultiClass):\n        latent_shape = y.shape[:-1] + (likelihood.num_classes,)\n    else:\n        latent_shape = y.shape[:-1]\n    if model_class is VariationalSparseGP:\n        gp = model_class(X, y, kernel, X, likelihood, latent_shape=latent_shape)\n    else:\n        gp = model_class(X, y, kernel, likelihood, latent_shape=latent_shape)\n\n    Xnew_shape = (X.shape[0] * 2,) + X.shape[1:]\n    Xnew = torch.rand(Xnew_shape, dtype=X.dtype, device=X.device)\n    f_loc, f_var = gp(Xnew)\n    ynew = gp.likelihood(f_loc, f_var)\n\n    assert ynew.shape == y.shape[:-1] + (Xnew.shape[0],)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", TEST_CASES, ids=TEST_IDS)\ndef test_forward_with_empty_latent_shape(model_class, X, y, kernel, likelihood):\n    if isinstance(likelihood, MultiClass):\n        latent_shape = torch.Size([likelihood.num_classes])\n    else:\n        latent_shape = torch.Size([])\n    if model_class is VariationalSparseGP:\n        gp = model_class(X, y, kernel, X, likelihood, latent_shape=latent_shape)\n    else:\n        gp = model_class(X, y, kernel, likelihood, latent_shape=latent_shape)\n\n    Xnew_shape = (X.shape[0] * 2,) + X.shape[1:]\n    Xnew = torch.rand(Xnew_shape, dtype=X.dtype, device=X.device)\n    f_loc, f_var = gp(Xnew)\n    ynew = gp.likelihood(f_loc, f_var)\n\n    assert ynew.shape == (Xnew.shape[0],)\n'"
tests/contrib/gp/test_models.py,55,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nfrom collections import namedtuple\n\nimport pytest\nimport torch\n\nimport pyro.distributions as dist\nfrom pyro.contrib.gp.kernels import Cosine, Matern32, RBF, WhiteNoise\nfrom pyro.contrib.gp.likelihoods import Gaussian\nfrom pyro.contrib.gp.models import (GPLVM, GPRegression, SparseGPRegression,\n                                    VariationalGP, VariationalSparseGP)\nfrom pyro.contrib.gp.util import train\nfrom pyro.infer.mcmc.hmc import HMC\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.nn.module import PyroSample\nfrom tests.common import assert_equal\n\nlogger = logging.getLogger(__name__)\n\nT = namedtuple(""TestGPModel"", [""model_class"", ""X"", ""y"", ""kernel"", ""likelihood""])\n\nX = torch.tensor([[1., 5., 3.], [4., 3., 7.]])\ny1D = torch.tensor([2., 1.])\ny2D = torch.tensor([[1., 2.], [3., 3.], [1., 4.], [-1., 1.]])\nnoise = torch.tensor(1e-7)\n\n\ndef _kernel():\n    return RBF(input_dim=3, variance=torch.tensor(3.), lengthscale=torch.tensor(2.))\n\n\ndef _likelihood():\n    return Gaussian(torch.tensor(1e-7))\n\n\ndef _TEST_CASES():\n    TEST_CASES = [\n        T(\n            GPRegression,\n            X, y1D, _kernel(), noise\n        ),\n        T(\n            GPRegression,\n            X, y2D, _kernel(), noise\n        ),\n        T(\n            SparseGPRegression,\n            X, y1D, _kernel(), noise\n        ),\n        T(\n            SparseGPRegression,\n            X, y2D, _kernel(), noise\n        ),\n        T(\n            VariationalGP,\n            X, y1D, _kernel(), _likelihood()\n        ),\n        T(\n            VariationalGP,\n            X, y2D, _kernel(), _likelihood()\n        ),\n        T(\n            VariationalSparseGP,\n            X, y1D, _kernel(), _likelihood()\n        ),\n        T(\n            VariationalSparseGP,\n            X, y2D, _kernel(), _likelihood()\n        ),\n    ]\n\n    return TEST_CASES\n\n\nTEST_IDS = [t[0].__name__ + ""_y{}D"".format(str(t[2].dim()))\n            for t in _TEST_CASES()]\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", _TEST_CASES(), ids=TEST_IDS)\ndef test_model(model_class, X, y, kernel, likelihood):\n    if model_class is SparseGPRegression or model_class is VariationalSparseGP:\n        gp = model_class(X, None, kernel, X, likelihood)\n    else:\n        gp = model_class(X, None, kernel, likelihood)\n\n    loc, var = gp.model()\n    if model_class is VariationalGP or model_class is VariationalSparseGP:\n        assert_equal(loc.norm().item(), 0)\n        assert_equal(var, torch.ones(var.shape[-1]).expand(var.shape))\n    else:\n        assert_equal(loc.norm().item(), 0)\n        assert_equal(var, kernel(X).diag())\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", _TEST_CASES(), ids=TEST_IDS)\ndef test_forward(model_class, X, y, kernel, likelihood):\n    if model_class is SparseGPRegression or model_class is VariationalSparseGP:\n        gp = model_class(X, y, kernel, X, likelihood)\n    else:\n        gp = model_class(X, y, kernel, likelihood)\n\n    # test shape\n    Xnew = torch.tensor([[2.0, 3.0, 1.0]])\n    loc0, cov0 = gp(Xnew, full_cov=True)\n    loc1, var1 = gp(Xnew, full_cov=False)\n    assert loc0.dim() == y.dim()\n    assert loc0.shape[-1] == Xnew.shape[0]\n    # test latent shape\n    assert loc0.shape[:-1] == y.shape[:-1]\n    assert cov0.shape[:-2] == y.shape[:-1]\n    assert cov0.shape[-1] == cov0.shape[-2]\n    assert cov0.shape[-1] == Xnew.shape[0]\n    assert_equal(loc0, loc1)\n    n = Xnew.shape[0]\n    cov0_diag = torch.stack([mat.diag() for mat in cov0.view(-1, n, n)]).reshape(var1.shape)\n    assert_equal(cov0_diag, var1)\n\n    # test trivial forward: Xnew = X\n    loc, cov = gp(X, full_cov=True)\n    if model_class is VariationalGP or model_class is VariationalSparseGP:\n        assert_equal(loc.norm().item(), 0)\n        assert_equal(cov, torch.eye(cov.shape[-1]).expand(cov.shape))\n    else:\n        assert_equal(loc, y)\n        assert_equal(cov.norm().item(), 0)\n\n    # test same input forward: Xnew[0,:] = Xnew[1,:] = ...\n    Xnew = torch.tensor([[2.0, 3.0, 1.0]]).expand(10, 3)\n    loc, cov = gp(Xnew, full_cov=True)\n    loc_diff = loc - loc[..., :1].expand(y.shape[:-1] + (10,))\n    assert_equal(loc_diff.norm().item(), 0)\n    cov_diff = cov - cov[..., :1, :1].expand(y.shape[:-1] + (10, 10))\n    assert_equal(cov_diff.norm().item(), 0)\n\n    # test noise kernel forward: kernel = WhiteNoise\n    gp.kernel = WhiteNoise(input_dim=3, variance=torch.tensor(10.))\n    loc, cov = gp(X, full_cov=True)\n    assert_equal(loc.norm().item(), 0)\n    assert_equal(cov, torch.eye(cov.shape[-1]).expand(cov.shape) * 10)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", _TEST_CASES(), ids=TEST_IDS)\ndef test_forward_with_empty_latent_shape(model_class, X, y, kernel, likelihood):\n    # regression models don\'t use latent_shape, no need for test\n    if model_class is GPRegression or model_class is SparseGPRegression:\n        return\n    elif model_class is VariationalGP:\n        gp = model_class(X, y, kernel, likelihood, latent_shape=torch.Size([]))\n    else:  # model_class is VariationalSparseGP\n        gp = model_class(X, y, kernel, X, likelihood, latent_shape=torch.Size([]))\n\n    # test shape\n    Xnew = torch.tensor([[2.0, 3.0, 1.0]])\n    loc0, cov0 = gp(Xnew, full_cov=True)\n    loc1, var1 = gp(Xnew, full_cov=False)\n    assert loc0.shape[-1] == Xnew.shape[0]\n    assert cov0.shape[-1] == cov0.shape[-2]\n    assert cov0.shape[-1] == Xnew.shape[0]\n    # test latent shape\n    assert loc0.shape[:-1] == torch.Size([])\n    assert cov0.shape[:-2] == torch.Size([])\n    assert_equal(loc0, loc1)\n    assert_equal(cov0.diag(), var1)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", _TEST_CASES(), ids=TEST_IDS)\n@pytest.mark.init(rng_seed=0)\ndef test_inference(model_class, X, y, kernel, likelihood):\n    # skip variational GP models because variance/lengthscale highly\n    # depend on variational parameters\n    if model_class is VariationalGP or model_class is VariationalSparseGP:\n        return\n    elif model_class is GPRegression:\n        gp = model_class(X, y, RBF(input_dim=3), likelihood)\n    else:  # model_class is SparseGPRegression\n        gp = model_class(X, y, RBF(input_dim=3), X, likelihood)\n        # fix inducing points because variance/lengthscale highly depend on it\n        gp.Xu.requires_grad_(False)\n\n    generator = dist.MultivariateNormal(torch.zeros(X.shape[0]), kernel(X))\n    target_y = generator(sample_shape=torch.Size([1000])).detach()\n    gp.set_data(X, target_y)\n\n    train(gp)\n\n    y_cov = gp.kernel(X)\n    target_y_cov = kernel(X)\n    assert_equal(y_cov, target_y_cov, prec=0.15)\n\n\n@pytest.mark.init(rng_seed=0)\ndef test_inference_sgpr():\n    N = 1000\n    X = dist.Uniform(torch.zeros(N), torch.ones(N)*5).sample()\n    y = 0.5 * torch.sin(3*X) + dist.Normal(torch.zeros(N), torch.ones(N)*0.5).sample()\n    kernel = RBF(input_dim=1)\n    Xu = torch.arange(0., 5.5, 0.5)\n\n    sgpr = SparseGPRegression(X, y, kernel, Xu)\n    train(sgpr)\n\n    Xnew = torch.arange(0., 5.05, 0.05)\n    loc, var = sgpr(Xnew, full_cov=False)\n    target = 0.5 * torch.sin(3*Xnew)\n\n    assert_equal((loc - target).abs().mean().item(), 0, prec=0.07)\n\n\n@pytest.mark.init(rng_seed=0)\ndef test_inference_vsgp():\n    N = 1000\n    X = dist.Uniform(torch.zeros(N), torch.ones(N)*5).sample()\n    y = 0.5 * torch.sin(3*X) + dist.Normal(torch.zeros(N), torch.ones(N)*0.5).sample()\n    kernel = RBF(input_dim=1)\n    Xu = torch.arange(0., 5.5, 0.5)\n\n    vsgp = VariationalSparseGP(X, y, kernel, Xu, Gaussian())\n    optimizer = torch.optim.Adam(vsgp.parameters(), lr=0.03)\n    train(vsgp, optimizer)\n\n    Xnew = torch.arange(0., 5.05, 0.05)\n    loc, var = vsgp(Xnew, full_cov=False)\n    target = 0.5 * torch.sin(3*Xnew)\n\n    assert_equal((loc - target).abs().mean().item(), 0, prec=0.06)\n\n\n@pytest.mark.init(rng_seed=0)\ndef test_inference_whiten_vsgp():\n    N = 1000\n    X = dist.Uniform(torch.zeros(N), torch.ones(N)*5).sample()\n    y = 0.5 * torch.sin(3*X) + dist.Normal(torch.zeros(N), torch.ones(N)*0.5).sample()\n    kernel = RBF(input_dim=1)\n    Xu = torch.arange(0., 5.5, 0.5)\n\n    vsgp = VariationalSparseGP(X, y, kernel, Xu, Gaussian(), whiten=True)\n    train(vsgp)\n\n    Xnew = torch.arange(0., 5.05, 0.05)\n    loc, var = vsgp(Xnew, full_cov=False)\n    target = 0.5 * torch.sin(3*Xnew)\n\n    assert_equal((loc - target).abs().mean().item(), 0, prec=0.07)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", _TEST_CASES(), ids=TEST_IDS)\ndef test_inference_with_empty_latent_shape(model_class, X, y, kernel, likelihood):\n    # regression models don\'t use latent_shape (default=torch.Size([]))\n    if model_class is GPRegression or model_class is SparseGPRegression:\n        return\n    elif model_class is VariationalGP:\n        gp = model_class(X, y, kernel, likelihood, latent_shape=torch.Size([]))\n    else:  # model_class is SparseVariationalGP\n        gp = model_class(X, y, kernel, X.clone(), likelihood, latent_shape=torch.Size([]))\n\n    train(gp, num_steps=1)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", _TEST_CASES(), ids=TEST_IDS)\ndef test_inference_with_whiten(model_class, X, y, kernel, likelihood):\n    # regression models don\'t use whiten\n    if model_class is GPRegression or model_class is SparseGPRegression:\n        return\n    elif model_class is VariationalGP:\n        gp = model_class(X, y, kernel, likelihood, whiten=True)\n    else:  # model_class is SparseVariationalGP\n        gp = model_class(X, y, kernel, X.clone(), likelihood, whiten=True)\n\n    train(gp, num_steps=1)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", _TEST_CASES(), ids=TEST_IDS)\ndef test_hmc(model_class, X, y, kernel, likelihood):\n    if model_class is SparseGPRegression or model_class is VariationalSparseGP:\n        gp = model_class(X, y, kernel, X.clone(), likelihood)\n    else:\n        gp = model_class(X, y, kernel, likelihood)\n\n    kernel.variance = PyroSample(dist.Uniform(torch.tensor(0.5), torch.tensor(1.5)))\n    kernel.lengthscale = PyroSample(dist.Uniform(torch.tensor(1.0), torch.tensor(3.0)))\n\n    hmc_kernel = HMC(gp.model, step_size=1)\n    mcmc = MCMC(hmc_kernel, num_samples=10)\n    mcmc.run()\n\n    for name, param in mcmc.get_samples().items():\n        param_mean = torch.mean(param, 0)\n        logger.info(""Posterior mean - {}"".format(name))\n        logger.info(param_mean)\n\n\ndef test_inference_deepGP():\n    gp1 = GPRegression(X, None, RBF(input_dim=3, variance=torch.tensor(3.),\n                                    lengthscale=torch.tensor(2.)))\n    Z, _ = gp1.model()\n    gp2 = VariationalSparseGP(Z, y2D, Matern32(input_dim=3), Z.clone(),\n                              Gaussian(torch.tensor(1e-6)))\n\n    class DeepGP(torch.nn.Module):\n        def __init__(self, gp1, gp2):\n            super().__init__()\n            self.gp1 = gp1\n            self.gp2 = gp2\n\n        def model(self):\n            Z, _ = self.gp1.model()\n            self.gp2.set_data(Z, y2D)\n            self.gp2.model()\n\n        def guide(self):\n            self.gp1.guide()\n            self.gp2.guide()\n\n    deepgp = DeepGP(gp1, gp2)\n    train(deepgp, num_steps=1)\n\n\n@pytest.mark.parametrize(""model_class, X, y, kernel, likelihood"", _TEST_CASES(), ids=TEST_IDS)\ndef test_gplvm(model_class, X, y, kernel, likelihood):\n    if model_class is SparseGPRegression or model_class is VariationalSparseGP:\n        gp = model_class(X, y, kernel, X.clone(), likelihood)\n    else:\n        gp = model_class(X, y, kernel, likelihood)\n\n    gplvm = GPLVM(gp)\n    # test inference\n    train(gplvm, num_steps=1)\n    # test forward\n    gplvm(Xnew=X)\n\n\ndef _pre_test_mean_function():\n    def f(x):\n        return 2 * x + 3 + 5 * torch.sin(7 * x)\n\n    X = torch.arange(100, dtype=torch.Tensor().dtype)\n    y = f(X)\n    Xnew = torch.arange(100, 150, dtype=torch.Tensor().dtype)\n    ynew = f(Xnew)\n\n    kernel = Cosine(input_dim=1)\n\n    class Trend(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.a = torch.nn.Parameter(torch.tensor(0.))\n            self.b = torch.nn.Parameter(torch.tensor(1.))\n\n        def forward(self, x):\n            return self.a * x + self.b\n\n    trend = Trend()\n    return X, y, Xnew, ynew, kernel, trend\n\n\ndef _mape(y_true, y_pred):\n    return ((y_pred - y_true) / y_true).abs().mean()\n\n\ndef _post_test_mean_function(gpmodule, Xnew, y_true):\n    assert_equal(gpmodule.mean_function.a.item(), 2, prec=0.03)\n    assert_equal(gpmodule.mean_function.b.item(), 3, prec=0.03)\n\n    y_pred, _ = gpmodule(Xnew)\n    assert_equal(_mape(y_true, y_pred).item(), 0, prec=0.02)\n\n\ndef test_mean_function_GPR():\n    X, y, Xnew, ynew, kernel, mean_fn = _pre_test_mean_function()\n    gpmodule = GPRegression(X, y, kernel, mean_function=mean_fn)\n    train(gpmodule)\n    _post_test_mean_function(gpmodule, Xnew, ynew)\n\n\ndef test_mean_function_SGPR():\n    X, y, Xnew, ynew, kernel, mean_fn = _pre_test_mean_function()\n    Xu = X[::20].clone()\n    gpmodule = SparseGPRegression(X, y, kernel, Xu, mean_function=mean_fn)\n    train(gpmodule)\n    _post_test_mean_function(gpmodule, Xnew, ynew)\n\n\ndef test_mean_function_SGPR_DTC():\n    X, y, Xnew, ynew, kernel, mean_fn = _pre_test_mean_function()\n    Xu = X[::20].clone()\n    gpmodule = SparseGPRegression(X, y, kernel, Xu, mean_function=mean_fn, approx=""DTC"")\n    train(gpmodule)\n    _post_test_mean_function(gpmodule, Xnew, ynew)\n\n\ndef test_mean_function_SGPR_FITC():\n    X, y, Xnew, ynew, kernel, mean_fn = _pre_test_mean_function()\n    Xu = X[::20].clone()\n    gpmodule = SparseGPRegression(X, y, kernel, Xu, mean_function=mean_fn, approx=""FITC"")\n    train(gpmodule)\n    _post_test_mean_function(gpmodule, Xnew, ynew)\n\n\ndef test_mean_function_VGP():\n    X, y, Xnew, ynew, kernel, mean_fn = _pre_test_mean_function()\n    likelihood = Gaussian()\n    gpmodule = VariationalGP(X, y, kernel, likelihood, mean_function=mean_fn)\n    train(gpmodule)\n    _post_test_mean_function(gpmodule, Xnew, ynew)\n\n\ndef test_mean_function_VGP_whiten():\n    X, y, Xnew, ynew, kernel, mean_fn = _pre_test_mean_function()\n    likelihood = Gaussian()\n    gpmodule = VariationalGP(X, y, kernel, likelihood, mean_function=mean_fn,\n                             whiten=True)\n    optimizer = torch.optim.Adam(gpmodule.parameters(), lr=0.1)\n    train(gpmodule, optimizer)\n    _post_test_mean_function(gpmodule, Xnew, ynew)\n\n\ndef test_mean_function_VSGP():\n    X, y, Xnew, ynew, kernel, mean_fn = _pre_test_mean_function()\n    Xu = X[::20].clone()\n    likelihood = Gaussian()\n    gpmodule = VariationalSparseGP(X, y, kernel, Xu, likelihood, mean_function=mean_fn)\n    optimizer = torch.optim.Adam(gpmodule.parameters(), lr=0.02)\n    train(gpmodule, optimizer)\n    _post_test_mean_function(gpmodule, Xnew, ynew)\n\n\ndef test_mean_function_VSGP_whiten():\n    X, y, Xnew, ynew, kernel, mean_fn = _pre_test_mean_function()\n    Xu = X[::20].clone()\n    likelihood = Gaussian()\n    gpmodule = VariationalSparseGP(X, y, kernel, Xu, likelihood, mean_function=mean_fn,\n                                   whiten=True)\n    optimizer = torch.optim.Adam(gpmodule.parameters(), lr=0.1)\n    train(gpmodule, optimizer)\n    _post_test_mean_function(gpmodule, Xnew, ynew)\n'"
tests/contrib/gp/test_parameterized.py,15,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.nn import Parameter\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.gp.parameterized import Parameterized\nfrom pyro.nn.module import PyroParam, PyroSample\nfrom tests.common import assert_equal\n\n\ndef test_parameterized():\n    class Linear(Parameterized):\n        def __init__(self):\n            super().__init__()\n            self._pyro_name = ""Linear""\n            self.a = PyroParam(torch.tensor(1.), constraints.positive)\n            self.b = PyroSample(dist.Normal(0, 1))\n            self.c = PyroSample(dist.Normal(0, 1))\n            self.d = PyroSample(dist.Normal(0, 4).expand([1]).to_event())\n            self.e = PyroSample(dist.LogNormal(0, 1))\n            self.f = PyroSample(dist.MultivariateNormal(torch.zeros(2), torch.eye(2)))\n            self.g = PyroSample(dist.Exponential(1))\n\n        def forward(self, x):\n            return self.a * x + self.b + self.c + self.d + self.e + self.f + self.g + self.e\n\n    linear = Linear()\n    linear.autoguide(""c"", dist.Normal)\n    linear.autoguide(""d"", dist.MultivariateNormal)\n    linear.autoguide(""e"", dist.Normal)\n\n    assert set(dict(linear.named_parameters()).keys()) == {\n        ""a_unconstrained"", ""b_map"", ""c_loc"", ""c_scale_unconstrained"",\n        ""d_loc"", ""d_scale_tril_unconstrained"",\n        ""e_loc"", ""e_scale_unconstrained"", ""f_map"", ""g_map_unconstrained""\n    }\n\n    def model(x):\n        linear.mode = ""model""\n        return linear(x)\n\n    def guide(x):\n        linear.mode = ""guide""\n        return linear(x)\n\n    model_trace = pyro.poutine.trace(model).get_trace(torch.tensor(5.))\n    guide_trace = pyro.poutine.trace(guide).get_trace(torch.tensor(5.))\n    for p in [""b"", ""c"", ""d""]:\n        assert ""Linear.{}"".format(p) in model_trace.nodes\n        assert ""Linear.{}"".format(p) in guide_trace.nodes\n\n    assert isinstance(guide_trace.nodes[""Linear.b""][""fn""], dist.Delta)\n    c_dist = guide_trace.nodes[""Linear.c""][""fn""]\n    assert isinstance(getattr(c_dist, ""base_dist"", c_dist), dist.Normal)\n    d_dist = guide_trace.nodes[""Linear.d""][""fn""]\n    assert isinstance(getattr(d_dist, ""base_dist"", d_dist), dist.MultivariateNormal)\n\n\ndef test_nested_parameterized():\n    class Linear(Parameterized):\n        def __init__(self, a):\n            super().__init__()\n            self.a = Parameter(a)\n\n        def forward(self, x):\n            return self.a * x\n\n    class Quadratic(Parameterized):\n        def __init__(self, linear1, linear2, a):\n            super().__init__()\n            self._pyro_name = ""Quadratic""\n            self.linear1 = linear1\n            self.linear2 = linear2\n            self.a = Parameter(a)\n\n        def forward(self, x):\n            return self.linear1(x) * x + self.linear2(self.a)\n\n    linear1 = Linear(torch.tensor(1.))\n    linear1.a = PyroSample(dist.Normal(0, 1))\n    linear2 = Linear(torch.tensor(1.))\n    linear2.a = PyroSample(dist.Normal(0, 1))\n    q = Quadratic(linear1, linear2, torch.tensor(2.))\n    q.a = PyroSample(dist.Cauchy(0, 1))\n\n    def model(x):\n        q.set_mode(""model"")\n        return q(x)\n\n    trace = pyro.poutine.trace(model).get_trace(torch.tensor(5.))\n    assert ""Quadratic.a"" in trace.nodes\n    assert ""Quadratic.linear1.a"" in trace.nodes\n    assert ""Quadratic.linear2.a"" in trace.nodes\n\n\ndef test_inference():\n    class Linear(Parameterized):\n        def __init__(self, a):\n            super().__init__()\n            self.a = Parameter(a)\n\n        def forward(self, x):\n            return self.a * x\n\n    target_a = torch.tensor(2.)\n    x_train = torch.rand(100)\n    y_train = target_a * x_train + torch.rand(100) * 0.001\n    linear = Linear(torch.tensor(1.))\n    linear.a = PyroSample(dist.Normal(0, 10))\n    linear.autoguide(""a"", dist.Normal)\n\n    def model(x, y):\n        linear.set_mode(""model"")\n        mu = linear(x)\n        with pyro.plate(""plate""):\n            return pyro.sample(""y"", dist.Normal(mu, 0.1), obs=y)\n\n    def guide(x, y):\n        linear.set_mode(""guide"")\n        linear._load_pyro_samples()\n\n    loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n    optimizer = torch.optim.Adam(linear.parameters(), lr=0.5)\n\n    def closure():\n        optimizer.zero_grad()\n        loss = loss_fn(model, guide, x_train, y_train)\n        loss.backward()\n        return loss\n\n    for i in range(200):\n        optimizer.step(closure)\n\n    linear.mode = ""guide""\n    assert_equal(linear.a, target_a, prec=0.05)\n'"
tests/contrib/oed/test_ewma.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\n\nimport pytest\nfrom pyro.contrib.oed.eig import EwmaLog\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize(""alpha"", [0.5, 0.9, 0.99])\ndef test_ewma(alpha, NS=10000, D=1):\n    ewma_log = EwmaLog(alpha=alpha)\n    sigma = torch.tensor(1.0, requires_grad=True)\n\n    for k in range(1000):\n        exponent = torch.randn(NS, D) * sigma\n        s, _ = torch.max(exponent, dim=0)\n        log_eT = s + ewma_log((exponent - s).exp().mean(dim=0), s)\n        log_eT.backward()\n        sigma_grad = sigma.grad.clone().cpu().numpy()\n        sigma.grad.zero_()\n        if k % 100 == 0:\n            error = math.fabs(sigma_grad - 1.0)\n            assert error < 0.07\n\n\ndef test_ewma_log():\n    ewma_log = EwmaLog(alpha=0.5)\n    input1 = torch.tensor(2.)\n    ewma_log(input1, torch.tensor(0.))\n    assert_equal(ewma_log.ewma, input1)\n    input2 = torch.tensor(3.)\n    ewma_log(input2, torch.tensor(0.))\n    assert_equal(ewma_log.ewma, torch.tensor(8./3))\n\n\ndef test_ewma_log_with_s():\n    ewma_log = EwmaLog(alpha=0.5)\n    input1 = torch.tensor(-1.)\n    s1 = torch.tensor(210.)\n    ewma_log(input1, s1)\n    assert_equal(ewma_log.ewma, input1)\n    input2 = torch.tensor(-1.)\n    s2 = torch.tensor(210.5)\n    ewma_log(input2, s2)\n    true_ewma = (1./3)*(torch.exp(s1 - s2)*input1 + 2*input2)\n    assert_equal(ewma_log.ewma, true_ewma)\n'"
tests/contrib/oed/test_finite_spaces_eig.py,14,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport pytest\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim as optim\nfrom pyro.contrib.oed.eig import (\n    nmc_eig, posterior_eig, marginal_eig, marginal_likelihood_eig, vnmc_eig, lfire_eig,\n    donsker_varadhan_eig)\nfrom pyro.contrib.util import iter_plates_to_shape\n\nfrom tests.common import assert_equal\n\ntry:\n    from contextlib import ExitStack  # python 3\nexcept ImportError:\n    from contextlib2 import ExitStack  # python 2\n\n\n@pytest.fixture\ndef finite_space_model():\n    def model(design):\n        batch_shape = design.shape\n        with ExitStack() as stack:\n            for plate in iter_plates_to_shape(batch_shape):\n                stack.enter_context(plate)\n            theta = pyro.sample(""theta"", dist.Bernoulli(.4).expand(batch_shape))\n            y = pyro.sample(""y"", dist.Bernoulli((design + theta) / 2.))\n            return y\n    return model\n\n\n@pytest.fixture\ndef one_point_design():\n    return torch.tensor(.5)\n\n\n@pytest.fixture\ndef true_eig():\n    return torch.tensor(0.12580366909478014)\n\n\ndef posterior_guide(y_dict, design, observation_labels, target_labels):\n\n    y = torch.cat(list(y_dict.values()), dim=-1)\n    a, b = pyro.param(""a"", torch.tensor(0.)), pyro.param(""b"", torch.tensor(0.))\n    pyro.sample(""theta"", dist.Bernoulli(logits=a + b*y))\n\n\ndef marginal_guide(design, observation_labels, target_labels):\n\n    logit_p = pyro.param(""logit_p"", torch.tensor(0.))\n    pyro.sample(""y"", dist.Bernoulli(logits=logit_p))\n\n\ndef likelihood_guide(theta_dict, design, observation_labels, target_labels):\n\n    theta = torch.cat(list(theta_dict.values()), dim=-1)\n    a, b = pyro.param(""a"", torch.tensor(0.)), pyro.param(""b"", torch.tensor(0.))\n    pyro.sample(""y"", dist.Bernoulli(logits=a + b*theta))\n\n\ndef make_lfire_classifier(n_theta_samples):\n    def lfire_classifier(design, trace, observation_labels, target_labels):\n        y_dict = {l: trace.nodes[l][""value""] for l in observation_labels}\n        y = torch.cat(list(y_dict.values()), dim=-1)\n        a, b = pyro.param(""a"", torch.zeros(n_theta_samples)), pyro.param(""b"", torch.zeros(n_theta_samples))\n\n        return a + b*y\n\n    return lfire_classifier\n\n\ndef dv_critic(design, trace, observation_labels, target_labels):\n    y_dict = {l: trace.nodes[l][""value""] for l in observation_labels}\n    y = torch.cat(list(y_dict.values()), dim=-1)\n    theta_dict = {l: trace.nodes[l][""value""] for l in target_labels}\n    theta = torch.cat(list(theta_dict.values()), dim=-1)\n\n    w_y = pyro.param(""w_y"", torch.tensor(0.))\n    w_theta = pyro.param(""w_theta"", torch.tensor(0.))\n    w_ytheta = pyro.param(""w_ytheta"", torch.tensor(0.))\n\n    return y*w_y + theta*w_theta + y*theta*w_ytheta\n\n\n########################################################################################################################\n# TESTS\n########################################################################################################################\n\n\ndef test_posterior_finite_space_model(finite_space_model, one_point_design, true_eig):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # Pre-train (large learning rate)\n    posterior_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=10,\n                  num_steps=250, guide=posterior_guide,\n                  optim=optim.Adam({""lr"": 0.1}))\n    # Finesse (small learning rate)\n    estimated_eig = posterior_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=10,\n                                  num_steps=250, guide=posterior_guide,\n                                  optim=optim.Adam({""lr"": 0.01}), final_num_samples=1000)\n    assert_equal(estimated_eig, true_eig, prec=1e-2)\n\n\ndef test_marginal_finite_space_model(finite_space_model, one_point_design, true_eig):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # Pre-train (large learning rate)\n    marginal_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=10,\n                 num_steps=250, guide=marginal_guide,\n                 optim=optim.Adam({""lr"": 0.1}))\n    # Finesse (small learning rate)\n    estimated_eig = marginal_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=10,\n                                 num_steps=250, guide=marginal_guide,\n                                 optim=optim.Adam({""lr"": 0.01}), final_num_samples=1000)\n    assert_equal(estimated_eig, true_eig, prec=1e-2)\n\n\ndef test_marginal_likelihood_finite_space_model(finite_space_model, one_point_design, true_eig):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # Pre-train (large learning rate)\n    marginal_likelihood_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=10,\n                            num_steps=250, marginal_guide=marginal_guide, cond_guide=likelihood_guide,\n                            optim=optim.Adam({""lr"": 0.1}))\n    # Finesse (small learning rate)\n    estimated_eig = marginal_likelihood_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=10,\n                                            num_steps=250, marginal_guide=marginal_guide, cond_guide=likelihood_guide,\n                                            optim=optim.Adam({""lr"": 0.01}), final_num_samples=1000)\n    assert_equal(estimated_eig, true_eig, prec=1e-2)\n\n\n@pytest.mark.xfail(reason=""Bernoullis are not reparametrizable and current VNMC implementation ""\n                          ""assumes reparametrization"")\ndef test_vnmc_finite_space_model(finite_space_model, one_point_design, true_eig):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # Pre-train (large learning rate)\n    vnmc_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=[9, 3],\n             num_steps=250, guide=posterior_guide,\n             optim=optim.Adam({""lr"": 0.1}))\n    # Finesse (small learning rate)\n    estimated_eig = vnmc_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=[9, 3],\n                             num_steps=250, guide=posterior_guide,\n                             optim=optim.Adam({""lr"": 0.01}), final_num_samples=[1000, 100])\n    assert_equal(estimated_eig, true_eig, prec=1e-2)\n\n\ndef test_nmc_eig_finite_space_model(finite_space_model, one_point_design, true_eig):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    estimated_eig = nmc_eig(finite_space_model, one_point_design, ""y"", ""theta"", M=40, N=40 * 40)\n    assert_equal(estimated_eig, true_eig, prec=1e-2)\n\n\ndef test_lfire_finite_space_model(finite_space_model, one_point_design, true_eig):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    estimated_eig = lfire_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_y_samples=5,\n                              num_theta_samples=50, num_steps=1000, classifier=make_lfire_classifier(50),\n                              optim=optim.Adam({""lr"": 0.0025}), final_num_samples=500)\n    assert_equal(estimated_eig, true_eig, prec=1e-2)\n\n\ndef test_dv_finite_space_model(finite_space_model, one_point_design, true_eig):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    donsker_varadhan_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=100,\n                         num_steps=250, T=dv_critic, optim=optim.Adam({""lr"": 0.1}))\n    estimated_eig = donsker_varadhan_eig(finite_space_model, one_point_design, ""y"", ""theta"", num_samples=100,\n                                         num_steps=250, T=dv_critic, optim=optim.Adam({""lr"": 0.01}),\n                                         final_num_samples=2000)\n    assert_equal(estimated_eig, true_eig, prec=1e-2)\n'"
tests/contrib/oed/test_glmm.py,51,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.distributions.transforms import AffineTransform, SigmoidTransform\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.contrib.oed.glmm import (\n    known_covariance_linear_model, group_linear_model, zero_mean_unit_obs_sd_lm,\n    normal_inverse_gamma_linear_model, logistic_regression_model, sigmoid_model\n)\nfrom tests.common import assert_equal\n\n\ndef lm_2p_10_10_1(design):\n    w = pyro.sample(""w"", dist.Normal(torch.tensor(0.),\n                                     torch.tensor([10., 10.])).to_event(1))\n    mean = torch.matmul(design, w.unsqueeze(-1)).squeeze(-1)\n    y = pyro.sample(""y"", dist.Normal(mean, torch.tensor(1.)).to_event(1))\n    return y\n\n\ndef lm_2p_10_10_1_w12(design):\n    w1 = pyro.sample(""w1"", dist.Normal(torch.tensor([0.]),\n                                       torch.tensor(10.)).to_event(1))\n    w2 = pyro.sample(""w2"", dist.Normal(torch.tensor([0.]),\n                                       torch.tensor(10.)).to_event(1))\n    w = torch.cat([w1, w2], dim=-1)\n    mean = torch.matmul(design, w.unsqueeze(-1)).squeeze(-1)\n    y = pyro.sample(""y"", dist.Normal(mean, torch.tensor(1.)).to_event(1))\n    return y\n\n\ndef nz_lm_2p_10_10_1(design):\n    w = pyro.sample(""w"", dist.Normal(torch.tensor([1., -1.]),\n                                     torch.tensor([10., 10.])).to_event(1))\n    mean = torch.matmul(design, w.unsqueeze(-1)).squeeze(-1)\n    y = pyro.sample(""y"", dist.Normal(mean, torch.tensor(1.)).to_event(1))\n    return y\n\n\ndef normal_inv_gamma_2_2_10_10(design):\n    tau = pyro.sample(""tau"", dist.Gamma(torch.tensor(2.), torch.tensor(2.)))\n    obs_sd = 1./torch.sqrt(tau)\n    w = pyro.sample(""w"", dist.Normal(torch.tensor([1., -1.]),\n                                     obs_sd*torch.tensor([10., 10.])).to_event(1))\n    mean = torch.matmul(design, w.unsqueeze(-1)).squeeze(-1)\n    y = pyro.sample(""y"", dist.Normal(mean, torch.tensor(1.)).to_event(1))\n    return y\n\n\ndef lr_10_10(design):\n    w = pyro.sample(""w"", dist.Normal(torch.tensor([1., -1.]),\n                                     torch.tensor([10., 10.])).to_event(1))\n    mean = torch.matmul(design, w.unsqueeze(-1)).squeeze(-1)\n    y = pyro.sample(""y"", dist.Bernoulli(logits=mean).to_event(1))\n    return y\n\n\ndef sigmoid_example(design):\n    n = design.shape[-2]\n    random_effect_k = pyro.sample(""k"", dist.Gamma(2.*torch.ones(n), torch.tensor(2.)))\n    random_effect_offset = pyro.sample(""w2"", dist.Normal(torch.tensor(0.), torch.ones(n)))\n    w1 = pyro.sample(""w1"", dist.Normal(torch.tensor([1., -1.]),\n                                       torch.tensor([10., 10.])).to_event(1))\n    mean = torch.matmul(design[..., :-2], w1.unsqueeze(-1)).squeeze(-1)\n    offset_mean = mean + random_effect_offset\n\n    base_dist = dist.Normal(offset_mean, torch.tensor(1.)).to_event(1)\n    transforms = [\n        AffineTransform(loc=torch.tensor(0.), scale=random_effect_k),\n        SigmoidTransform()\n    ]\n    response_dist = dist.TransformedDistribution(base_dist, transforms)\n    y = pyro.sample(""y"", response_dist)\n    return y\n\n\n@pytest.mark.parametrize(""model1,model2,design"", [\n    (\n        zero_mean_unit_obs_sd_lm(torch.tensor([10., 10.]))[0],\n        lm_2p_10_10_1,\n        torch.tensor([[1., -1.]])\n    ),\n    (\n        lm_2p_10_10_1,\n        zero_mean_unit_obs_sd_lm(torch.tensor([10., 10.]))[0],\n        torch.tensor([[100., -100.]])\n    ),\n    (\n        group_linear_model(torch.tensor(0.), torch.tensor([10.]), torch.tensor(0.),\n                           torch.tensor([10.]), torch.tensor(1.)),\n        lm_2p_10_10_1_w12,\n        torch.tensor([[-1.5, 0.5], [1.5, 0.]])\n    ),\n    (\n        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),\n        nz_lm_2p_10_10_1,\n        torch.tensor([[-1., 0.5], [2.5, -2.]])\n    ),\n    (\n        normal_inverse_gamma_linear_model(torch.tensor([1., -1.]), torch.tensor(.1),\n                                          torch.tensor(2.), torch.tensor(2.)),\n        normal_inv_gamma_2_2_10_10,\n        torch.tensor([[1., -0.5], [1.5, 2.]])\n    ),\n    (\n        logistic_regression_model(torch.tensor([1., -1.]), torch.tensor(10.)),\n        lr_10_10,\n        torch.tensor([[6., -1.5], [.5, 0.]])\n    ),\n    (\n        sigmoid_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]),\n                      torch.tensor(0.), torch.tensor([1., 1.]),\n                      torch.tensor(1.),\n                      torch.tensor(2.), torch.tensor(2.), torch.eye(2)),\n        sigmoid_example,\n        torch.cat([torch.tensor([[1., 1.], [.5, -2.5]]), torch.eye(2)], dim=-1)\n    )\n])\ndef test_log_prob_matches(model1, model2, design):\n    trace = poutine.trace(model1).get_trace(design)\n    trace.compute_log_prob()\n    ks = [k for k in trace.nodes.keys() if not k.startswith(""_"")]\n    data = {l: trace.nodes[l][""value""] for l in ks}\n    lp = {l: trace.nodes[l][""log_prob""] for l in ks}\n    cond_model = pyro.condition(model2, data=data)\n    cond_trace = poutine.trace(cond_model).get_trace(design)\n    cond_trace.compute_log_prob()\n    assert trace.nodes.keys() == cond_trace.nodes.keys()\n    lp2 = {l: trace.nodes[l][""log_prob""] for l in ks}\n    for l in lp.keys():\n        assert_equal(lp[l], lp2[l])\n'"
tests/contrib/oed/test_linear_models_eig.py,21,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport pytest\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.optim as optim\nfrom pyro.infer import Trace_ELBO\nfrom pyro.contrib.oed.glmm import known_covariance_linear_model\nfrom pyro.contrib.oed.util import linear_model_ground_truth\nfrom pyro.contrib.oed.eig import (\n    nmc_eig, posterior_eig, marginal_eig, marginal_likelihood_eig, vnmc_eig, laplace_eig, lfire_eig,\n    donsker_varadhan_eig)\nfrom pyro.contrib.util import rmv, rvv\nfrom pyro.contrib.oed.glmm.guides import LinearModelLaplaceGuide\nfrom tests.common import assert_equal\n\n\n@pytest.fixture\ndef linear_model():\n    return known_covariance_linear_model(coef_means=torch.tensor(0.),\n                                         coef_sds=torch.tensor([1., 1.5]),\n                                         observation_sd=torch.tensor(1.))\n\n\n@pytest.fixture\ndef one_point_design():\n    X = torch.zeros(3, 2)\n    X[0, 0] = X[1, 1] = X[2, 1] = 1.\n    return X\n\n\ndef posterior_guide(y_dict, design, observation_labels, target_labels):\n\n    y = torch.cat(list(y_dict.values()), dim=-1)\n    A = pyro.param(""A"", torch.zeros(2, 3))\n    scale_tril = pyro.param(""scale_tril"", torch.tensor([[1., 0.], [0., 1.5]]),\n                            constraint=torch.distributions.constraints.lower_cholesky)\n    mu = rmv(A, y)\n    pyro.sample(""w"", dist.MultivariateNormal(mu, scale_tril=scale_tril))\n\n\ndef marginal_guide(design, observation_labels, target_labels):\n\n    mu = pyro.param(""mu"", torch.zeros(3))\n    scale_tril = pyro.param(""scale_tril"", torch.eye(3),\n                            constraint=torch.distributions.constraints.lower_cholesky)\n    pyro.sample(""y"", dist.MultivariateNormal(mu, scale_tril))\n\n\ndef likelihood_guide(theta_dict, design, observation_labels, target_labels):\n\n    theta = torch.cat(list(theta_dict.values()), dim=-1)\n    centre = rmv(design, theta)\n\n    # Need to avoid name collision here\n    mu = pyro.param(""mu_l"", torch.zeros(3))\n    scale_tril = pyro.param(""scale_tril_l"", torch.eye(3),\n                            constraint=torch.distributions.constraints.lower_cholesky)\n\n    pyro.sample(""y"", dist.MultivariateNormal(centre + mu, scale_tril=scale_tril))\n\n\n# The guide includes some features of the Laplace approximation that would be tiresome to copy across\nlaplace_guide = LinearModelLaplaceGuide(tuple(), {""w"": 2})\n\n\ndef make_lfire_classifier(n_theta_samples):\n    def lfire_classifier(design, trace, observation_labels, target_labels):\n        y_dict = {l: trace.nodes[l][""value""] for l in observation_labels}\n        y = torch.cat(list(y_dict.values()), dim=-1)\n\n        quadratic_coef = pyro.param(""quadratic_coef"", torch.zeros(n_theta_samples, 3, 3))\n        linear_coef = pyro.param(""linear_coef"", torch.zeros(n_theta_samples, 3))\n        bias = pyro.param(""bias"", torch.zeros(n_theta_samples))\n\n        y_quadratic = y.unsqueeze(-1) * y.unsqueeze(-2)\n        return (quadratic_coef * y_quadratic).sum(-1).sum(-1) + (linear_coef * y).sum(-1) + bias\n\n    return lfire_classifier\n\n\ndef dv_critic(design, trace, observation_labels, target_labels):\n    y_dict = {l: trace.nodes[l][""value""] for l in observation_labels}\n    theta_dict = {l: trace.nodes[l][""value""] for l in target_labels}\n    x = torch.cat(list(theta_dict.values()) + list(y_dict.values()), dim=-1)\n\n    B = pyro.param(""B"", torch.zeros(5, 5))\n    return rvv(x, rmv(B, x))\n\n\n########################################################################################################################\n# TESTS\n########################################################################################################################\n\n\ndef test_posterior_linear_model(linear_model, one_point_design):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # Pre-train (large learning rate)\n    posterior_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=10,\n                  num_steps=250, guide=posterior_guide,\n                  optim=optim.Adam({""lr"": 0.1}))\n    # Finesse (small learning rate)\n    estimated_eig = posterior_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=10,\n                                  num_steps=250, guide=posterior_guide,\n                                  optim=optim.Adam({""lr"": 0.01}), final_num_samples=500)\n    expected_eig = linear_model_ground_truth(linear_model, one_point_design, ""y"", ""w"")\n    assert_equal(estimated_eig, expected_eig, prec=5e-2)\n\n\ndef test_marginal_linear_model(linear_model, one_point_design):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # Pre-train (large learning rate)\n    marginal_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=10,\n                 num_steps=250, guide=marginal_guide,\n                 optim=optim.Adam({""lr"": 0.1}))\n    # Finesse (small learning rate)\n    estimated_eig = marginal_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=10,\n                                 num_steps=250, guide=marginal_guide,\n                                 optim=optim.Adam({""lr"": 0.01}), final_num_samples=500)\n    expected_eig = linear_model_ground_truth(linear_model, one_point_design, ""y"", ""w"")\n    assert_equal(estimated_eig, expected_eig, prec=5e-2)\n\n\ndef test_marginal_likelihood_linear_model(linear_model, one_point_design):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # Pre-train (large learning rate)\n    marginal_likelihood_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=10,\n                            num_steps=250, marginal_guide=marginal_guide, cond_guide=likelihood_guide,\n                            optim=optim.Adam({""lr"": 0.1}))\n    # Finesse (small learning rate)\n    estimated_eig = marginal_likelihood_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=10,\n                                            num_steps=250, marginal_guide=marginal_guide, cond_guide=likelihood_guide,\n                                            optim=optim.Adam({""lr"": 0.01}), final_num_samples=500)\n    expected_eig = linear_model_ground_truth(linear_model, one_point_design, ""y"", ""w"")\n    assert_equal(estimated_eig, expected_eig, prec=5e-2)\n\n\ndef test_vnmc_linear_model(linear_model, one_point_design):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # Pre-train (large learning rate)\n    vnmc_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=[9, 3],\n             num_steps=250, guide=posterior_guide,\n             optim=optim.Adam({""lr"": 0.1}))\n    # Finesse (small learning rate)\n    estimated_eig = vnmc_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=[9, 3],\n                             num_steps=250, guide=posterior_guide,\n                             optim=optim.Adam({""lr"": 0.01}), final_num_samples=[500, 100])\n    expected_eig = linear_model_ground_truth(linear_model, one_point_design, ""y"", ""w"")\n    assert_equal(estimated_eig, expected_eig, prec=5e-2)\n\n\ndef test_nmc_eig_linear_model(linear_model, one_point_design):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    estimated_eig = nmc_eig(linear_model, one_point_design, ""y"", ""w"", M=60, N=60 * 60)\n    expected_eig = linear_model_ground_truth(linear_model, one_point_design, ""y"", ""w"")\n    assert_equal(estimated_eig, expected_eig, prec=5e-2)\n\n\ndef test_laplace_linear_model(linear_model, one_point_design):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    # You can use 1 final sample here because linear models have a posterior entropy that is independent of `y`\n    estimated_eig = laplace_eig(linear_model, one_point_design, ""y"", ""w"",\n                                guide=laplace_guide, num_steps=250, final_num_samples=1,\n                                optim=optim.Adam({""lr"": 0.05}),\n                                loss=Trace_ELBO().differentiable_loss)\n    expected_eig = linear_model_ground_truth(linear_model, one_point_design, ""y"", ""w"")\n    assert_equal(estimated_eig, expected_eig, prec=5e-2)\n\n\ndef test_lfire_linear_model(linear_model, one_point_design):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    estimated_eig = lfire_eig(linear_model, one_point_design, ""y"", ""w"", num_y_samples=2, num_theta_samples=50,\n                              num_steps=1200, classifier=make_lfire_classifier(50), optim=optim.Adam({""lr"": 0.0025}),\n                              final_num_samples=100)\n    expected_eig = linear_model_ground_truth(linear_model, one_point_design, ""y"", ""w"")\n    assert_equal(estimated_eig, expected_eig, prec=5e-2)\n\n\ndef test_dv_linear_model(linear_model, one_point_design):\n    pyro.set_rng_seed(42)\n    pyro.clear_param_store()\n    donsker_varadhan_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=100, num_steps=500, T=dv_critic,\n                         optim=optim.Adam({""lr"": 0.1}))\n    estimated_eig = donsker_varadhan_eig(linear_model, one_point_design, ""y"", ""w"", num_samples=100,\n                                         num_steps=650, T=dv_critic, optim=optim.Adam({""lr"": 0.001}),\n                                         final_num_samples=2000)\n    expected_eig = linear_model_ground_truth(linear_model, one_point_design, ""y"", ""w"")\n    assert_equal(estimated_eig, expected_eig, prec=5e-2)\n'"
tests/contrib/oed/test_xexpx.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.contrib.oed.eig import xexpx\n\n\n@pytest.mark.parametrize(""argument,output"", [\n    (torch.tensor([float(\'-inf\')]), torch.tensor([0.])),\n    (torch.tensor([0.]), torch.tensor([0.])),\n    (torch.tensor([1.]), torch.exp(torch.tensor([1.])))\n])\ndef test_xexpx(argument, output):\n    assert xexpx(argument) == output\n'"
tests/contrib/randomvariable/test_random_variable.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch.tensor as tt\nfrom pyro.distributions import Uniform\n\nN_SAMPLES = 100\n\n\ndef test_add():\n    X = Uniform(0, 1).rv  # (0, 1)\n    X = X + 1  # (1, 2)\n    X = 1 + X  # (2, 3)\n    X += 1  # (3, 4)\n    x = X.dist.sample([N_SAMPLES])\n    assert ((3 <= x) & (x <= 4)).all().item()\n\n\ndef test_subtract():\n    X = Uniform(0, 1).rv  # (0, 1)\n    X = 1 - X  # (0, 1)\n    X = X - 1  # (-1, 0)\n    X -= 1  # (-2, -1)\n    x = X.dist.sample([N_SAMPLES])\n    assert ((-2 <= x) & (x <= -1)).all().item()\n\n\ndef test_multiply_divide():\n    X = Uniform(0, 1).rv  # (0, 1)\n    X *= 4  # (0, 4)\n    X /= 2  # (0, 2)\n    x = X.dist.sample([N_SAMPLES])\n    assert ((0 <= x) & (x <= 2)).all().item()\n\n\ndef test_abs():\n    X = Uniform(0, 1).rv  # (0, 1)\n    X = 2*(X - 0.5)  # (-1, 1)\n    X = abs(X)  # (0, 1)\n    x = X.dist.sample([N_SAMPLES])\n    assert ((0 <= x) & (x <= 1)).all().item()\n\n\ndef test_neg():\n    X = Uniform(0, 1).rv  # (0, 1)\n    X = -X  # (-1, 0)\n    x = X.dist.sample([N_SAMPLES])\n    assert ((-1 <= x) & (x <= 0)).all().item()\n\n\ndef test_pow():\n    X = Uniform(0, 1).rv  # (0, 1)\n    X = X**2  # (0, 1)\n    x = X.dist.sample([N_SAMPLES])\n    assert ((0 <= x) & (x <= 1)).all().item()\n\n\ndef test_tensor_ops():\n    pi = 3.141592654\n    X = Uniform(0, 1).expand([5, 5]).rv\n    a = tt([[1, 2, 3, 4, 5]])\n    b = a.T\n    X = abs(pi*(-X + a - 3*b))\n    x = X.dist.sample()\n    assert x.shape == (5, 5)\n    assert (x >= 0).all().item()\n\n\ndef test_chaining():\n    X = (\n        Uniform(0, 1).rv  # (0, 1)\n        .add(1)  # (1, 2)\n        .pow(2)  # (1, 4)\n        .mul(2)  # (2, 8)\n        .sub(5)  # (-3, 3)\n        .tanh()  # (-1, 1); more like (-0.995, +0.995)\n        .exp()  # (1/e, e)\n    )\n    x = X.dist.sample([N_SAMPLES])\n    assert ((1/math.e <= x) & (x <= math.e)).all().item()\n'"
tests/contrib/timeseries/test_gp.py,27,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport torch\n\nfrom tests.common import assert_equal\nimport pyro\nfrom pyro.contrib.timeseries import (IndependentMaternGP, LinearlyCoupledMaternGP, GenericLGSSM,\n                                     GenericLGSSMWithGPNoiseModel, DependentMaternGP)\nfrom pyro.ops.tensor_utils import block_diag_embed\nimport pytest\n\n\n@pytest.mark.parametrize('model,obs_dim,nu_statedim', [('ssmgp', 3, 1.5), ('ssmgp', 2, 2.5),\n                                                       ('lcmgp', 3, 1.5), ('lcmgp', 2, 2.5),\n                                                       ('imgp', 1, 0.5), ('imgp', 2, 0.5),\n                                                       ('imgp', 1, 1.5), ('imgp', 3, 1.5),\n                                                       ('imgp', 1, 2.5), ('imgp', 3, 2.5),\n                                                       ('dmgp', 1, 1.5), ('dmgp', 2, 1.5),\n                                                       ('dmgp', 3, 1.5),\n                                                       ('glgssm', 1, 3), ('glgssm', 3, 1)])\n@pytest.mark.parametrize('T', [11, 37])\ndef test_timeseries_models(model, nu_statedim, obs_dim, T):\n    torch.set_default_tensor_type('torch.DoubleTensor')\n    dt = 0.1 + torch.rand(1).item()\n\n    if model == 'lcmgp':\n        num_gps = 2\n        gp = LinearlyCoupledMaternGP(nu=nu_statedim, obs_dim=obs_dim, dt=dt, num_gps=num_gps,\n                                     length_scale_init=0.5 + torch.rand(num_gps),\n                                     kernel_scale_init=0.5 + torch.rand(num_gps),\n                                     obs_noise_scale_init=0.5 + torch.rand(obs_dim))\n    elif model == 'imgp':\n        gp = IndependentMaternGP(nu=nu_statedim, obs_dim=obs_dim, dt=dt,\n                                 length_scale_init=0.5 + torch.rand(obs_dim),\n                                 kernel_scale_init=0.5 + torch.rand(obs_dim),\n                                 obs_noise_scale_init=0.5 + torch.rand(obs_dim))\n    elif model == 'glgssm':\n        gp = GenericLGSSM(state_dim=nu_statedim, obs_dim=obs_dim,\n                          obs_noise_scale_init=0.5 + torch.rand(obs_dim))\n    elif model == 'ssmgp':\n        state_dim = {0.5: 4, 1.5: 3, 2.5: 2}[nu_statedim]\n        gp = GenericLGSSMWithGPNoiseModel(nu=nu_statedim, state_dim=state_dim, obs_dim=obs_dim,\n                                          obs_noise_scale_init=0.5 + torch.rand(obs_dim))\n    elif model == 'dmgp':\n        linearly_coupled = bool(torch.rand(1).item() > 0.5)\n        gp = DependentMaternGP(nu=nu_statedim, obs_dim=obs_dim, dt=dt, linearly_coupled=linearly_coupled,\n                               length_scale_init=0.5 + torch.rand(obs_dim))\n\n    targets = torch.randn(T, obs_dim)\n    gp_log_prob = gp.log_prob(targets)\n    if model == 'imgp':\n        assert gp_log_prob.shape == (obs_dim,)\n    else:\n        assert gp_log_prob.dim() == 0\n\n    # compare matern log probs to vanilla GP result via multivariate normal\n    if model == 'imgp':\n        times = dt * torch.arange(T).double()\n        for dim in range(obs_dim):\n            lengthscale = gp.kernel.length_scale[dim]\n            variance = gp.kernel.kernel_scale.pow(2)[dim]\n            obs_noise = gp.obs_noise_scale.pow(2)[dim]\n\n            kernel = {0.5: pyro.contrib.gp.kernels.Exponential,\n                      1.5: pyro.contrib.gp.kernels.Matern32,\n                      2.5: pyro.contrib.gp.kernels.Matern52}[nu_statedim]\n            kernel = kernel(input_dim=1, lengthscale=lengthscale, variance=variance)\n            # XXX kernel(times) loads old parameters from param store\n            kernel = kernel.forward(times) + obs_noise * torch.eye(T)\n\n            mvn = torch.distributions.MultivariateNormal(torch.zeros(T), kernel)\n            mvn_log_prob = mvn.log_prob(targets[:, dim])\n            assert_equal(mvn_log_prob, gp_log_prob[dim], prec=1e-4)\n\n    for S in [1, 5]:\n        if model in ['imgp', 'lcmgp', 'dmgp', 'lcdgp']:\n            dts = torch.rand(S).cumsum(dim=-1)\n            predictive = gp.forecast(targets, dts)\n        else:\n            predictive = gp.forecast(targets, S)\n        assert predictive.loc.shape == (S, obs_dim)\n        if model == 'imgp':\n            assert predictive.scale.shape == (S, obs_dim)\n            # assert monotonic increase of predictive noise\n            if S > 1:\n                delta = predictive.scale[1:S, :] - predictive.scale[0:S-1, :]\n                assert (delta > 0.0).sum() == (S - 1) * obs_dim\n        else:\n            assert predictive.covariance_matrix.shape == (S, obs_dim, obs_dim)\n            # assert monotonic increase of predictive noise\n            if S > 1:\n                dets = predictive.covariance_matrix.det()\n                delta = dets[1:S] - dets[0:S-1]\n                assert (delta > 0.0).sum() == (S - 1)\n\n    if model in ['imgp', 'lcmgp', 'dmgp', 'lcdgp']:\n        # the distant future\n        dts = torch.tensor([500.0])\n        predictive = gp.forecast(targets, dts)\n        # assert mean reverting behavior for GP models\n        assert_equal(predictive.loc, torch.zeros(1, obs_dim))\n\n\n@pytest.mark.parametrize('obs_dim', [1, 3])\ndef test_dependent_matern_gp(obs_dim):\n    dt = 0.5 + torch.rand(1).item()\n    gp = DependentMaternGP(nu=1.5, obs_dim=obs_dim, dt=dt,\n                           length_scale_init=0.5 + torch.rand(obs_dim))\n\n    # make sure stationary covariance matrix satisfies the relevant\n    # matrix riccati equation\n    lengthscale = gp.kernel.length_scale.unsqueeze(-1).unsqueeze(-1)\n    F = torch.tensor([[0.0, 1.0], [0.0, 0.0]])\n    mask1 = torch.tensor([[0.0, 0.0], [-3.0, 0.0]])\n    mask2 = torch.tensor([[0.0, 0.0], [0.0, -math.sqrt(12.0)]])\n    F = block_diag_embed(F + mask1 / lengthscale.pow(2.0) + mask2 / lengthscale)\n\n    stat_cov = gp._stationary_covariance()\n    wiener_cov = gp._get_wiener_cov()\n    wiener_cov *= torch.tensor([[0.0, 0.0], [0.0, 1.0]]).repeat(obs_dim, obs_dim)\n\n    expected_zero = torch.matmul(F, stat_cov) + torch.matmul(stat_cov, F.transpose(-1, -2)) + wiener_cov\n    assert_equal(expected_zero, torch.zeros(gp.full_state_dim, gp.full_state_dim))\n"""
tests/contrib/timeseries/test_lgssm.py,23,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom tests.common import assert_equal\nfrom pyro.contrib.timeseries import GenericLGSSM, GenericLGSSMWithGPNoiseModel\nimport pytest\n\n\n@pytest.mark.parametrize('model_class', ['lgssm', 'lgssmgp'])\n@pytest.mark.parametrize('state_dim', [2, 3])\n@pytest.mark.parametrize('obs_dim', [2, 4])\n@pytest.mark.parametrize('T', [11, 17])\ndef test_generic_lgssm_forecast(model_class, state_dim, obs_dim, T):\n    torch.set_default_tensor_type('torch.DoubleTensor')\n\n    if model_class == 'lgssm':\n        model = GenericLGSSM(state_dim=state_dim, obs_dim=obs_dim,\n                             obs_noise_scale_init=0.1 + torch.rand(obs_dim))\n    elif model_class == 'lgssmgp':\n        model = GenericLGSSMWithGPNoiseModel(state_dim=state_dim, obs_dim=obs_dim, nu=1.5,\n                                             obs_noise_scale_init=0.1 + torch.rand(obs_dim))\n        # with these hyperparameters we essentially turn off the GP contributions\n        model.kernel.length_scale = 1.0e-6 * torch.ones(obs_dim)\n        model.kernel.kernel_scale = 1.0e-6 * torch.ones(obs_dim)\n\n    targets = torch.randn(T, obs_dim)\n    filtering_state = model._filter(targets)\n\n    actual_loc, actual_cov = model._forecast(3, filtering_state, include_observation_noise=False)\n\n    obs_matrix = model.obs_matrix if model_class == 'lgssm' else model.z_obs_matrix\n    trans_matrix = model.trans_matrix if model_class == 'lgssm' else model.z_trans_matrix\n    trans_matrix_sq = torch.mm(trans_matrix, trans_matrix)\n    trans_matrix_cubed = torch.mm(trans_matrix_sq, trans_matrix)\n\n    trans_obs = torch.mm(trans_matrix, obs_matrix)\n    trans_trans_obs = torch.mm(trans_matrix_sq, obs_matrix)\n    trans_trans_trans_obs = torch.mm(trans_matrix_cubed, obs_matrix)\n\n    # we only compute contributions for the state space portion for lgssmgp\n    fs_loc = filtering_state.loc if model_class == 'lgssm' else filtering_state.loc[-state_dim:]\n\n    predicted_mean1 = torch.mm(fs_loc.unsqueeze(-2), trans_obs).squeeze(-2)\n    predicted_mean2 = torch.mm(fs_loc.unsqueeze(-2), trans_trans_obs).squeeze(-2)\n    predicted_mean3 = torch.mm(fs_loc.unsqueeze(-2), trans_trans_trans_obs).squeeze(-2)\n\n    # check predicted means for 3 timesteps\n    assert_equal(actual_loc[0], predicted_mean1)\n    assert_equal(actual_loc[1], predicted_mean2)\n    assert_equal(actual_loc[2], predicted_mean3)\n\n    # check predicted covariances for 3 timesteps\n    fs_covar, process_covar = None, None\n    if model_class == 'lgssm':\n        process_covar = model._get_trans_dist().covariance_matrix\n        fs_covar = filtering_state.covariance_matrix\n    elif model_class == 'lgssmgp':\n        # we only compute contributions for the state space portion\n        process_covar = model.trans_noise_scale_sq.diag_embed()\n        fs_covar = filtering_state.covariance_matrix[-state_dim:, -state_dim:]\n\n    predicted_covar1 = torch.mm(trans_obs.t(), torch.mm(fs_covar, trans_obs)) + \\\n        torch.mm(obs_matrix.t(), torch.mm(process_covar, obs_matrix))\n\n    predicted_covar2 = torch.mm(trans_trans_obs.t(), torch.mm(fs_covar, trans_trans_obs)) + \\\n        torch.mm(trans_obs.t(), torch.mm(process_covar, trans_obs)) + \\\n        torch.mm(obs_matrix.t(), torch.mm(process_covar, obs_matrix))\n\n    predicted_covar3 = torch.mm(trans_trans_trans_obs.t(), torch.mm(fs_covar, trans_trans_trans_obs)) + \\\n        torch.mm(trans_trans_obs.t(), torch.mm(process_covar, trans_trans_obs)) + \\\n        torch.mm(trans_obs.t(), torch.mm(process_covar, trans_obs)) + \\\n        torch.mm(obs_matrix.t(), torch.mm(process_covar, obs_matrix))\n\n    assert_equal(actual_cov[0], predicted_covar1)\n    assert_equal(actual_cov[1], predicted_covar2)\n    assert_equal(actual_cov[2], predicted_covar3)\n"""
tests/contrib/tracking/__init__.py,0,b''
tests/contrib/tracking/test_assignment.py,35,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nimport logging\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.tracking.assignment import MarginalAssignment, MarginalAssignmentPersistent, MarginalAssignmentSparse\nfrom tests.common import assert_equal\n\nINF = float('inf')\nlogger = logging.getLogger(__name__)\n\n\ndef assert_finite(tensor, name):\n    assert ((tensor - tensor) == 0).all(), 'bad {}: {}'.format(tensor, name)\n\n\ndef logit(p):\n    return p.log() - (-p).log1p()\n\n\ndef dense_to_sparse(assign_logits):\n    num_detections, num_objects = assign_logits.shape\n    edges = assign_logits.new_tensor([[j, i] for j in range(num_detections) for i in range(num_objects)],\n                                     dtype=torch.long).t()\n    assign_logits = assign_logits[edges[0], edges[1]]\n    return edges, assign_logits\n\n\ndef sparse_to_dense(num_objects, num_detections, edges, assign_logits):\n    result = assign_logits.new_empty(num_detections, num_objects).fill_(-INF)\n    result[edges[0], edges[1]] = assign_logits\n    return result\n\n\ndef test_dense_smoke():\n    num_objects = 4\n    num_detections = 2\n    pyro.set_rng_seed(0)\n    exists_logits = torch.zeros(num_objects)\n    assign_logits = logit(torch.tensor([\n        [0.5, 0.5, 0.0, 0.0],\n        [0.0, 0.5, 0.5, 0.5],\n    ]))\n    assert assign_logits.shape == (num_detections, num_objects)\n\n    solver = MarginalAssignment(exists_logits, assign_logits, bp_iters=5)\n\n    assert solver.exists_dist.batch_shape == (num_objects,)\n    assert solver.exists_dist.event_shape == ()\n    assert solver.assign_dist.batch_shape == (num_detections,)\n    assert solver.assign_dist.event_shape == ()\n    assert solver.assign_dist.probs.shape[-1] == num_objects + 1  # true + spurious\n\n    # test dense matches sparse\n    edges, assign_logits = dense_to_sparse(assign_logits)\n    other = MarginalAssignmentSparse(num_objects, num_detections, edges, exists_logits, assign_logits, bp_iters=5)\n    assert_equal(other.exists_dist.probs, solver.exists_dist.probs, prec=1e-3)\n    assert_equal(other.assign_dist.probs, solver.assign_dist.probs, prec=1e-3)\n\n\ndef test_sparse_smoke():\n    num_objects = 4\n    num_detections = 2\n    pyro.set_rng_seed(0)\n    exists_logits = torch.zeros(num_objects)\n    edges = exists_logits.new_tensor([\n        [0, 0, 1, 0, 1, 0],\n        [0, 1, 1, 2, 2, 3],\n    ], dtype=torch.long)\n    assign_logits = logit(torch.tensor([0.99, 0.8, 0.2, 0.2, 0.8, 0.9]))\n    assert assign_logits.shape == edges.shape[1:]\n\n    solver = MarginalAssignmentSparse(num_objects, num_detections, edges,\n                                      exists_logits, assign_logits, bp_iters=5)\n\n    assert solver.exists_dist.batch_shape == (num_objects,)\n    assert solver.exists_dist.event_shape == ()\n    assert solver.assign_dist.batch_shape == (num_detections,)\n    assert solver.assign_dist.event_shape == ()\n    assert solver.assign_dist.probs.shape[-1] == num_objects + 1  # true + spurious\n\n    # test dense matches sparse\n    assign_logits = sparse_to_dense(num_objects, num_detections, edges, assign_logits)\n    other = MarginalAssignment(exists_logits, assign_logits, bp_iters=5)\n    assert_equal(other.exists_dist.probs, solver.exists_dist.probs, prec=1e-3)\n    assert_equal(other.assign_dist.probs, solver.assign_dist.probs, prec=1e-3)\n\n\ndef test_sparse_grid_smoke():\n\n    def my_existence_prior(ox, oy):\n        return -0.5\n\n    def my_assign_prior(ox, oy, dx, dy):\n        return 0.0\n\n    num_detections = 3 * 3\n    detections = [[0, 1, 2],\n                  [3, 4, 5],\n                  [6, 7, 8]]\n    num_objects = 2 * 2\n    objects = [[0, 1],\n               [2, 3]]\n    edges = []\n    edge_coords = []\n    for x in range(2):\n        for y in range(2):\n            object_id = objects[x][y]\n            for dx in [0, 1]:\n                for dy in [0, 1]:\n                    detection_id = detections[x + dx][y + dy]\n                    edges.append((detection_id, object_id))\n                    edge_coords.append((x, y, x + dx, y + dy))\n\n    exists_logits = torch.empty(num_objects)\n    edges = exists_logits.new_tensor(edges, dtype=torch.long).t()\n    assert edges.shape == (2, 4 * 4)\n\n    for x in range(2):\n        for y in range(2):\n            object_id = objects[x][y]\n            exists_logits[object_id] = my_existence_prior(x, y)\n    assign_logits = exists_logits.new_tensor([my_assign_prior(ox, oy, dx, dy)\n                                              for ox, oy, dx, dy in edge_coords])\n    assign = MarginalAssignmentSparse(num_objects, num_detections, edges,\n                                      exists_logits, assign_logits, bp_iters=10)\n    assert isinstance(assign.assign_dist, dist.Categorical)\n\n\n@pytest.mark.parametrize('bp_iters', [None, 10], ids=['enum', 'bp'])\ndef test_persistent_smoke(bp_iters):\n    exists_logits = torch.tensor([-1., -1., -2.], requires_grad=True)\n    assign_logits = torch.tensor([[[-1., -INF, -INF],\n                                   [-2., -2., -INF]],\n                                  [[-1., -2., -3.],\n                                   [-2., -2., -1.]],\n                                  [[-1., -2., -3.],\n                                   [-2., -2., -1.]],\n                                  [[-1., -1., 1.],\n                                   [1., 1., -1.]]], requires_grad=True)\n\n    assignment = MarginalAssignmentPersistent(exists_logits, assign_logits, bp_iters=bp_iters)\n    assert assignment.num_frames == 4\n    assert assignment.num_detections == 2\n    assert assignment.num_objects == 3\n\n    assign_dist = assignment.assign_dist\n    exists_dist = assignment.exists_dist\n    assert_finite(exists_dist.probs, 'exists_probs')\n    assert_finite(assign_dist.probs, 'assign_probs')\n\n    for exists in exists_dist.enumerate_support():\n        log_prob = exists_dist.log_prob(exists).sum()\n        e_grad, a_grad = grad(log_prob, [exists_logits, assign_logits], create_graph=True)\n        assert_finite(e_grad, 'dexists_probs/dexists_logits')\n        assert_finite(a_grad, 'dexists_probs/dassign_logits')\n\n    for assign in assign_dist.enumerate_support():\n        log_prob = assign_dist.log_prob(assign).sum()\n        e_grad, a_grad = grad(log_prob, [exists_logits, assign_logits], create_graph=True)\n        assert_finite(e_grad, 'dassign_probs/dexists_logits')\n        assert_finite(a_grad, 'dassign_probs/dassign_logits')\n\n\n@pytest.mark.parametrize('e', [-1., 0., 1.])\n@pytest.mark.parametrize('a', [-1., 0., 1.])\ndef test_flat_exact_1_1(e, a):\n    exists_logits = torch.tensor([e])\n    assign_logits = torch.tensor([[a]])\n    expected = MarginalAssignment(exists_logits, assign_logits, None)\n    actual = MarginalAssignment(exists_logits, assign_logits, 10)\n    assert_equal(expected.exists_dist.probs, actual.exists_dist.probs)\n    assert_equal(expected.assign_dist.probs, actual.assign_dist.probs)\n\n\n@pytest.mark.parametrize('e', [-1., 0., 1.])\n@pytest.mark.parametrize('a11', [-1., 0., 1.])\n@pytest.mark.parametrize('a21', [-1., 0., 1.])\ndef test_flat_exact_2_1(e, a11, a21):\n    exists_logits = torch.tensor([e])\n    assign_logits = torch.tensor([[a11], [a21]])\n    expected = MarginalAssignment(exists_logits, assign_logits, None)\n    actual = MarginalAssignment(exists_logits, assign_logits, 10)\n    assert_equal(expected.exists_dist.probs, actual.exists_dist.probs)\n    assert_equal(expected.assign_dist.probs, actual.assign_dist.probs)\n\n\n@pytest.mark.parametrize('e1', [-1., 0., 1.])\n@pytest.mark.parametrize('e2', [-1., 0., 1.])\n@pytest.mark.parametrize('a11', [-1., 0., 1.])\n@pytest.mark.parametrize('a12', [-1., 0., 1.])\ndef test_flat_exact_1_2(e1, e2, a11, a12):\n    exists_logits = torch.tensor([e1, e2])\n    assign_logits = torch.tensor([[a11, a12]])\n    expected = MarginalAssignment(exists_logits, assign_logits, None)\n    actual = MarginalAssignment(exists_logits, assign_logits, 10)\n    assert_equal(expected.exists_dist.probs, actual.exists_dist.probs)\n    assert_equal(expected.assign_dist.probs, actual.assign_dist.probs)\n\n\n@pytest.mark.parametrize('e1', [-1., 1.])\n@pytest.mark.parametrize('e2', [-1., 1.])\n@pytest.mark.parametrize('a11', [-1., 1.])\n@pytest.mark.parametrize('a12', [-1., 1.])\n@pytest.mark.parametrize('a22', [-1., 1.])\ndef test_flat_exact_2_2(e1, e2, a11, a12, a22):\n    a21 = -INF\n    exists_logits = torch.tensor([e1, e2])\n    assign_logits = torch.tensor([[a11, a12], [a21, a22]])\n    expected = MarginalAssignment(exists_logits, assign_logits, None)\n    actual = MarginalAssignment(exists_logits, assign_logits, 10)\n    assert_equal(expected.exists_dist.probs, actual.exists_dist.probs)\n    assert_equal(expected.assign_dist.probs, actual.assign_dist.probs)\n\n\n@pytest.mark.parametrize('num_detections', [1, 2, 3, 4])\n@pytest.mark.parametrize('num_objects', [1, 2, 3, 4])\ndef test_flat_bp_vs_exact(num_objects, num_detections):\n    exists_logits = -2 * torch.rand(num_objects)\n    assign_logits = -2 * torch.rand(num_detections, num_objects)\n    expected = MarginalAssignment(exists_logits, assign_logits, None)\n    actual = MarginalAssignment(exists_logits, assign_logits, 10)\n    # these should only approximately agree\n    assert_equal(expected.exists_dist.probs, actual.exists_dist.probs, prec=0.01)\n    assert_equal(expected.assign_dist.probs, actual.assign_dist.probs, prec=0.01)\n\n\n@pytest.mark.parametrize('num_frames', [1, 2, 3, 4])\n@pytest.mark.parametrize('num_objects', [1, 2, 3, 4])\n@pytest.mark.parametrize('bp_iters', [None, 30], ids=['enum', 'bp'])\ndef test_flat_vs_persistent(num_objects, num_frames, bp_iters):\n    exists_logits = -2 * torch.rand(num_objects)\n    assign_logits = -2 * torch.rand(num_frames, num_objects)\n    flat = MarginalAssignment(exists_logits, assign_logits, bp_iters)\n    full = MarginalAssignmentPersistent(exists_logits, assign_logits.unsqueeze(1), bp_iters)\n    assert_equal(flat.exists_dist.probs, full.exists_dist.probs)\n    assert_equal(flat.assign_dist.probs, full.assign_dist.probs.squeeze(1))\n\n\n@pytest.mark.parametrize('num_detections', [1, 2, 3, 4])\n@pytest.mark.parametrize('num_frames', [1, 2, 3, 4])\n@pytest.mark.parametrize('num_objects', [1, 2, 3, 4])\ndef test_persistent_bp_vs_exact(num_objects, num_frames, num_detections):\n    exists_logits = -2 * torch.rand(num_objects)\n    assign_logits = 2 * torch.rand(num_frames, num_detections, num_objects) - 1\n    expected = MarginalAssignmentPersistent(exists_logits, assign_logits, None)\n    actual = MarginalAssignmentPersistent(exists_logits, assign_logits, 30)\n    # these should only approximately agree\n    assert_equal(expected.exists_dist.probs, actual.exists_dist.probs, prec=0.05)\n    assert_equal(expected.assign_dist.probs, actual.assign_dist.probs, prec=0.05)\n\n\n@pytest.mark.parametrize('e1', [-1., 1.])\n@pytest.mark.parametrize('e2', [-1., 1.])\n@pytest.mark.parametrize('e3', [-1., 1.])\n@pytest.mark.parametrize('bp_iters, bp_momentum', [(3, 0.), (30, 0.5)], ids=['momentum', 'none'])\ndef test_persistent_exact_5_4_3(e1, e2, e3, bp_iters, bp_momentum):\n    exists_logits = torch.tensor([e1, e2, e3])\n    assign_logits = 2 * torch.rand(5, 4, 3) - 1\n    # this has tree-shaped connectivity and should lead to exact inference\n    mask = torch.tensor([[[1, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]],\n                         [[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]],\n                         [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]],\n                         [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]],\n                         [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]]], dtype=torch.bool)\n    assign_logits[~mask] = -INF\n    expected = MarginalAssignmentPersistent(exists_logits, assign_logits, None)\n    actual = MarginalAssignmentPersistent(exists_logits, assign_logits, bp_iters, bp_momentum)\n    assert_equal(expected.exists_dist.probs, actual.exists_dist.probs)\n    assert_equal(expected.assign_dist.probs, actual.assign_dist.probs)\n    logger.debug(actual.exists_dist.probs)\n    logger.debug(actual.assign_dist.probs)\n\n\n@pytest.mark.parametrize('num_detections', [1, 2, 3])\n@pytest.mark.parametrize('num_frames', [1, 2, 3])\n@pytest.mark.parametrize('num_objects', [1, 2])\n@pytest.mark.parametrize('bp_iters', [None, 30], ids=['enum', 'bp'])\ndef test_persistent_independent_subproblems(num_objects, num_frames, num_detections, bp_iters):\n    # solve a random assignment problem\n    exists_logits_1 = -2 * torch.rand(num_objects)\n    assign_logits_1 = 2 * torch.rand(num_frames, num_detections, num_objects) - 1\n    assignment_1 = MarginalAssignmentPersistent(exists_logits_1, assign_logits_1, bp_iters)\n    exists_probs_1 = assignment_1.exists_dist.probs\n    assign_probs_1 = assignment_1.assign_dist.probs\n\n    # solve another random assignment problem\n    exists_logits_2 = -2 * torch.rand(num_objects)\n    assign_logits_2 = 2 * torch.rand(num_frames, num_detections, num_objects) - 1\n    assignment_2 = MarginalAssignmentPersistent(exists_logits_2, assign_logits_2, bp_iters)\n    exists_probs_2 = assignment_2.exists_dist.probs\n    assign_probs_2 = assignment_2.assign_dist.probs\n\n    # solve a unioned assignment problem\n    exists_logits = torch.cat([exists_logits_1, exists_logits_2])\n    assign_logits = torch.full((num_frames, num_detections * 2, num_objects * 2), -INF)\n    assign_logits[:, :num_detections, :num_objects] = assign_logits_1\n    assign_logits[:, num_detections:, num_objects:] = assign_logits_2\n    assignment = MarginalAssignmentPersistent(exists_logits, assign_logits, bp_iters)\n    exists_probs = assignment.exists_dist.probs\n    assign_probs = assignment.assign_dist.probs\n\n    # check agreement\n    assert_equal(exists_probs_1, exists_probs[:num_objects])\n    assert_equal(exists_probs_2, exists_probs[num_objects:])\n    assert_equal(assign_probs_1[:, :, :-1], assign_probs[:, :num_detections, :num_objects])\n    assert_equal(assign_probs_1[:, :, -1], assign_probs[:, :num_detections, -1])\n    assert_equal(assign_probs_2[:, :, :-1], assign_probs[:, num_detections:, num_objects:-1])\n    assert_equal(assign_probs_2[:, :, -1], assign_probs[:, num_detections:, -1])\n"""
tests/contrib/tracking/test_distributions.py,6,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.contrib.tracking.distributions import EKFDistribution\nfrom pyro.contrib.tracking.dynamic_models import NcpContinuous, NcvContinuous\n\nimport pytest\n\n\n@pytest.mark.parametrize('Model', [NcpContinuous, NcvContinuous])\n@pytest.mark.parametrize('dim', [2, 3])\n@pytest.mark.parametrize('time', [2, 3])\ndef test_EKFDistribution_smoke(Model, dim, time):\n    x0 = torch.rand(2*dim)\n    ys = torch.randn(time, dim)\n    P0 = torch.eye(2*dim).requires_grad_()\n    R = torch.eye(dim).requires_grad_()\n    model = Model(2*dim, 2.0)\n    dist = EKFDistribution(x0, P0, model, R, time_steps=time)\n    log_prob = dist.log_prob(ys)\n    assert log_prob.shape == torch.Size()\n    dP0, dR = torch.autograd.grad(log_prob, [P0, R])\n    assert dP0.shape == P0.shape\n    assert dR.shape == R.shape\n"""
tests/contrib/tracking/test_dynamic_models.py,19,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.contrib.tracking.dynamic_models import (NcpContinuous, NcvContinuous,\n                                                  NcvDiscrete, NcpDiscrete)\nfrom tests.common import assert_equal, assert_not_equal\n\n\ndef assert_cov_validity(cov, eigenvalue_lbnd=0., condition_number_ubnd=1e6):\n    '''\n    cov: covariance matrix\n    eigenvalue_lbnd: eigenvalues should be at least this much greater than\n      zero. Must be strictly positive.\n    condition_number_ubnd: inclusive upper bound on matrix condition\n      number. Must be greater or equal to 1.0.\n    '''\n    assert eigenvalue_lbnd >= 0.0, \\\n        'Covariance eigenvalue lower bound must be > 0.0!'\n    assert condition_number_ubnd >= 1.0, \\\n        'Covariance condition number bound must be >= 1.0!'\n\n    # Symmetry\n    assert (cov.t() == cov).all(), 'Covariance must be symmetric!'\n    # Precompute eigenvalues for subsequent tests.\n    ws, _ = torch.symeig(cov)  # The eigenvalues of cov\n    w_min = torch.min(ws)\n    w_max = torch.max(ws)\n\n    # Strict positivity\n    assert w_min > 0.0, 'Covariance must be strictly positive!'\n\n    # Eigenvalue lower bound\n    assert w_min >= eigenvalue_lbnd, \\\n        'Covariance eigenvalues must be >= lower bound!'\n\n    # Condition number upper bound\n    assert w_max/w_min <= condition_number_ubnd, \\\n        'Condition number must be <= upper bound!'\n\n\ndef test_NcpContinuous():\n    framerate = 100  # Hz\n    dt = 1.0 / framerate\n    d = 3\n    ncp = NcpContinuous(dimension=d, sv2=2.0)\n    assert ncp.dimension == d\n    assert ncp.dimension_pv == 2*d\n    assert ncp.num_process_noise_parameters == 1\n\n    x = torch.rand(d)\n    y = ncp(x, dt)\n    assert_equal(y, x)\n\n    dx = ncp.geodesic_difference(x, y)\n    assert_equal(dx, torch.zeros(d))\n\n    x_pv = ncp.mean2pv(x)\n    assert len(x_pv) == 6\n    assert_equal(x, x_pv[:d])\n    assert_equal(torch.zeros(d), x_pv[d:])\n\n    P = torch.eye(d)\n    P_pv = ncp.cov2pv(P)\n    assert P_pv.shape == (2*d, 2*d)\n    P_pv_ref = torch.zeros((2*d, 2*d))\n    P_pv_ref[:d, :d] = P\n    assert_equal(P_pv_ref, P_pv)\n\n    Q = ncp.process_noise_cov(dt)\n    Q1 = ncp.process_noise_cov(dt)  # Test caching.\n    assert_equal(Q, Q1)\n    assert Q1.shape == (d, d)\n    assert_cov_validity(Q1)\n\n    dx = ncp.process_noise_dist(dt).sample()\n    assert dx.shape == (ncp.dimension,)\n\n\ndef test_NcvContinuous():\n    framerate = 100  # Hz\n    dt = 1.0/framerate\n    d = 6\n    ncv = NcvContinuous(dimension=d, sa2=2.0)\n    assert ncv.dimension == d\n    assert ncv.dimension_pv == d\n    assert ncv.num_process_noise_parameters == 1\n\n    x = torch.rand(d)\n    y = ncv(x, dt)\n    assert_equal(y[0], x[0] + dt*x[d//2])\n\n    dx = ncv.geodesic_difference(x, y)\n    assert_not_equal(dx, torch.zeros(d))\n\n    x_pv = ncv.mean2pv(x)\n    assert len(x_pv) == d\n    assert_equal(x, x_pv)\n\n    P = torch.eye(d)\n    P_pv = ncv.cov2pv(P)\n    assert P_pv.shape == (d, d)\n    assert_equal(P, P_pv)\n\n    Q = ncv.process_noise_cov(dt)\n    Q1 = ncv.process_noise_cov(dt)  # Test caching.\n    assert_equal(Q, Q1)\n    assert Q1.shape == (d, d)\n    assert_cov_validity(Q1)\n\n    dx = ncv.process_noise_dist(dt).sample()\n    assert dx.shape == (ncv.dimension,)\n\n\ndef test_NcpDiscrete():\n    framerate = 100  # Hz\n    dt = 1.0/framerate\n    d = 3\n    ncp = NcpDiscrete(dimension=d, sv2=2.0)\n    assert ncp.dimension == d\n    assert ncp.dimension_pv == 2*d\n    assert ncp.num_process_noise_parameters == 1\n\n    x = torch.rand(d)\n    y = ncp(x, dt)\n    assert_equal(y, x)\n\n    dx = ncp.geodesic_difference(x, y)\n    assert_equal(dx, torch.zeros(d))\n\n    x_pv = ncp.mean2pv(x)\n    assert len(x_pv) == 6\n    assert_equal(x, x_pv[:d])\n    assert_equal(torch.zeros(d), x_pv[d:])\n\n    P = torch.eye(d)\n    P_pv = ncp.cov2pv(P)\n    assert P_pv.shape == (2*d, 2*d)\n    P_pv_ref = torch.zeros((2*d, 2*d))\n    P_pv_ref[:d, :d] = P\n    assert_equal(P_pv_ref, P_pv)\n\n    Q = ncp.process_noise_cov(dt)\n    Q1 = ncp.process_noise_cov(dt)  # Test caching.\n    assert_equal(Q, Q1)\n    assert Q1.shape == (d, d)\n    assert_cov_validity(Q1)\n\n    dx = ncp.process_noise_dist(dt).sample()\n    assert dx.shape == (ncp.dimension,)\n\n\ndef test_NcvDiscrete():\n    framerate = 100  # Hz\n    dt = 1.0/framerate\n    dt = 100\n    d = 6\n    ncv = NcvDiscrete(dimension=d, sa2=2.0)\n    assert ncv.dimension == d\n    assert ncv.dimension_pv == d\n    assert ncv.num_process_noise_parameters == 1\n\n    x = torch.rand(d)\n    y = ncv(x, dt)\n    assert_equal(y[0], x[0] + dt*x[d//2])\n\n    dx = ncv.geodesic_difference(x, y)\n    assert_not_equal(dx, torch.zeros(d))\n\n    x_pv = ncv.mean2pv(x)\n    assert len(x_pv) == d\n    assert_equal(x, x_pv)\n\n    P = torch.eye(d)\n    P_pv = ncv.cov2pv(P)\n    assert P_pv.shape == (d, d)\n    assert_equal(P, P_pv)\n\n    Q = ncv.process_noise_cov(dt)\n    Q1 = ncv.process_noise_cov(dt)  # Test caching.\n    assert_equal(Q, Q1)\n    assert Q1.shape == (d, d)\n    # Q has rank `dimension/2`, so it is not a valid cov matrix\n"""
tests/contrib/tracking/test_ekf.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nfrom pyro.contrib.tracking.extended_kalman_filter import EKFState\nfrom pyro.contrib.tracking.dynamic_models import NcpContinuous, NcvContinuous\nfrom pyro.contrib.tracking.measurements import PositionMeasurement\n\nfrom tests.common import assert_equal, assert_not_equal\n\n\ndef test_EKFState_with_NcpContinuous():\n    d = 3\n    ncp = NcpContinuous(dimension=d, sv2=2.0)\n    x = torch.rand(d)\n    P = torch.eye(d)\n    t = 0.0\n    dt = 2.0\n    ekf_state = EKFState(dynamic_model=ncp, mean=x, cov=P, time=t)\n\n    assert ekf_state.dynamic_model.__class__ == NcpContinuous\n    assert ekf_state.dimension == d\n    assert ekf_state.dimension_pv == 2*d\n\n    assert_equal(x, ekf_state.mean, prec=1e-5)\n    assert_equal(P, ekf_state.cov, prec=1e-5)\n    assert_equal(x, ekf_state.mean_pv[:d], prec=1e-5)\n    assert_equal(P, ekf_state.cov_pv[:d, :d], prec=1e-5)\n    assert_equal(t, ekf_state.time, prec=1e-5)\n\n    ekf_state1 = EKFState(ncp, 2*x, 2*P, t)\n    ekf_state2 = ekf_state1.predict(dt)\n    assert ekf_state2.dynamic_model.__class__ == NcpContinuous\n\n    measurement = PositionMeasurement(\n        mean=torch.rand(d),\n        cov=torch.eye(d),\n        time=t + dt)\n    log_likelihood = ekf_state2.log_likelihood_of_update(measurement)\n    assert (log_likelihood < 0.).all()\n    ekf_state3, (dz, S) = ekf_state2.update(measurement)\n    assert dz.shape == (measurement.dimension,)\n    assert S.shape == (measurement.dimension, measurement.dimension)\n    assert_not_equal(ekf_state3.mean, ekf_state2.mean, prec=1e-5)\n\n\ndef test_EKFState_with_NcvContinuous():\n    d = 6\n    ncv = NcvContinuous(dimension=d, sa2=2.0)\n    x = torch.rand(d)\n    P = torch.eye(d)\n    t = 0.0\n    dt = 2.0\n    ekf_state = EKFState(\n        dynamic_model=ncv, mean=x, cov=P, time=t)\n\n    assert ekf_state.dynamic_model.__class__ == NcvContinuous\n    assert ekf_state.dimension == d\n    assert ekf_state.dimension_pv == d\n\n    assert_equal(x, ekf_state.mean, prec=1e-5)\n    assert_equal(P, ekf_state.cov, prec=1e-5)\n    assert_equal(x, ekf_state.mean_pv, prec=1e-5)\n    assert_equal(P, ekf_state.cov_pv, prec=1e-5)\n    assert_equal(t, ekf_state.time, prec=1e-5)\n\n    ekf_state1 = EKFState(ncv, 2*x, 2*P, t)\n    ekf_state2 = ekf_state1.predict(dt)\n    assert ekf_state2.dynamic_model.__class__ == NcvContinuous\n\n    measurement = PositionMeasurement(\n        mean=torch.rand(d),\n        cov=torch.eye(d),\n        time=t + dt)\n    log_likelihood = ekf_state2.log_likelihood_of_update(measurement)\n    assert (log_likelihood < 0.).all()\n    ekf_state3, (dz, S) = ekf_state2.update(measurement)\n    assert dz.shape == (measurement.dimension,)\n    assert S.shape == (measurement.dimension, measurement.dimension)\n    assert_not_equal(ekf_state3.mean, ekf_state2.mean, prec=1e-5)\n'"
tests/contrib/tracking/test_em.py,16,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport math\n\nimport pytest\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.contrib.tracking.assignment import MarginalAssignment\nfrom pyro.infer import SVI, TraceEnum_ELBO\nfrom pyro.optim import Adam\nfrom pyro.optim.multi import MixedMultiOptimizer, Newton\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef make_args():\n    args = type('Args', (), {})  # A fake ArgumentParser.parse_args()\n    args.max_num_objects = 4\n    args.num_real_detections = 13\n    args.num_fake_detections = 3\n    args.expected_num_objects = 2\n    args.init_noise_scale = 0.1\n\n    # TODO Is it correct to detach gradients of assignments?\n    # Detaching is indeed required for the Hessian to be block-diagonal,\n    # but it is unclear whether convergence would be faster if we applied\n    # a blockwise method (Newton) to the full Hessian, without detaching.\n    args.assignment_grad = False\n\n    return args\n\n\ndef model(detections, args):\n    noise_scale = pyro.param('noise_scale')\n    objects = pyro.param('objects_loc').squeeze(-1)\n    num_detections, = detections.shape\n    max_num_objects, = objects.shape\n\n    # Existence part.\n    p_exists = args.expected_num_objects / max_num_objects\n    with pyro.plate('objects_plate', max_num_objects):\n        exists = pyro.sample('exists', dist.Bernoulli(p_exists))\n        with poutine.mask(mask=exists.bool()):\n            pyro.sample('objects', dist.Normal(0., 1.), obs=objects)\n\n    # Assignment part.\n    p_fake = args.num_fake_detections / num_detections\n    with pyro.plate('detections_plate', num_detections):\n        assign_probs = torch.empty(max_num_objects + 1)\n        assign_probs[:-1] = (1 - p_fake) / max_num_objects\n        assign_probs[-1] = p_fake\n        assign = pyro.sample('assign', dist.Categorical(logits=assign_probs))\n        is_fake = (assign == assign.shape[-1] - 1)\n        objects_plus_bogus = torch.zeros(max_num_objects + 1)\n        objects_plus_bogus[:max_num_objects] = objects\n        real_dist = dist.Normal(objects_plus_bogus[assign], noise_scale)\n        fake_dist = dist.Normal(0., 1.)\n        pyro.sample('detections', dist.MaskedMixture(is_fake, real_dist, fake_dist),\n                    obs=detections)\n\n\n# This should match detection_model's existence part.\ndef compute_exists_logits(objects, args):\n    p_exists = args.expected_num_objects / args.max_num_objects\n    real_part = dist.Normal(0., 1.).log_prob(objects)\n    real_part = real_part + math.log(p_exists)\n    spurious_part = torch.full(real_part.shape, math.log(1 - p_exists))\n    return real_part - spurious_part\n\n\n# This should match detection_model's assignment part.\ndef compute_assign_logits(objects, detections, noise_scale, args):\n    num_detections = len(detections)\n    p_fake = args.num_fake_detections / num_detections\n    real_part = dist.Normal(objects, noise_scale).log_prob(detections)\n    real_part = real_part + math.log((1 - p_fake) / args.max_num_objects)\n    fake_part = dist.Normal(0., 1.).log_prob(detections)\n    fake_part = fake_part + math.log(p_fake)\n    return real_part - fake_part\n\n\ndef guide(detections, args):\n    noise_scale = pyro.param('noise_scale')  # trained by SVI\n    objects = pyro.param('objects_loc').squeeze(-1)  # trained by M-step of EM\n    num_detections, = detections.shape\n    max_num_objects, = objects.shape\n\n    with torch.set_grad_enabled(args.assignment_grad):\n        # Evaluate log likelihoods. TODO make this more pyronic.\n        exists_logits = compute_exists_logits(objects, args)\n        assign_logits = compute_assign_logits(objects, detections.unsqueeze(-1), noise_scale, args)\n        assert exists_logits.shape == (max_num_objects,)\n        assert assign_logits.shape == (num_detections, max_num_objects)\n\n        # Compute soft assignments.\n        assignment = MarginalAssignment(exists_logits, assign_logits, bp_iters=10)\n\n    with pyro.plate('objects_plate', max_num_objects):\n        pyro.sample('exists', assignment.exists_dist,\n                    infer={'enumerate': 'parallel'})\n    with pyro.plate('detections_plate', num_detections):\n        pyro.sample('assign', assignment.assign_dist,\n                    infer={'enumerate': 'parallel'})\n\n\ndef generate_data(args):\n    num_objects = args.expected_num_objects\n    true_objects = torch.randn(num_objects)\n    true_assign = dist.Categorical(torch.ones(args.num_real_detections, num_objects)).sample()\n    real_detections = true_objects[true_assign]\n    real_detections = real_detections + args.init_noise_scale * torch.randn(real_detections.shape)\n    fake_detections = torch.randn(args.num_fake_detections)\n    detections = torch.cat([real_detections, fake_detections])\n    assert detections.shape == (args.num_real_detections + args.num_fake_detections,)\n    return detections\n\n\n@pytest.mark.parametrize('assignment_grad', [False, True])\ndef test_em(assignment_grad):\n    args = make_args()\n    args.assignment_grad = assignment_grad\n    detections = generate_data(args)\n\n    pyro.clear_param_store()\n    pyro.param('noise_scale', torch.tensor(args.init_noise_scale),\n               constraint=constraints.positive)\n    pyro.param('objects_loc', torch.randn(args.max_num_objects, 1))\n\n    # Learn object_loc via EM algorithm.\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    newton = Newton(trust_radii={'objects_loc': 1.0})\n    for step in range(10):\n        # Detach previous iterations.\n        objects_loc = pyro.param('objects_loc').detach_().requires_grad_()\n        loss = elbo.differentiable_loss(model, guide, detections, args)  # E-step\n        newton.step(loss, {'objects_loc': objects_loc})  # M-step\n        logger.debug('step {}, loss = {}'.format(step, loss.item()))\n\n\n@pytest.mark.parametrize('assignment_grad', [False, True])\ndef test_em_nested_in_svi(assignment_grad):\n    args = make_args()\n    args.assignment_grad = assignment_grad\n    detections = generate_data(args)\n\n    pyro.clear_param_store()\n    pyro.param('noise_scale', torch.tensor(args.init_noise_scale),\n               constraint=constraints.positive)\n    pyro.param('objects_loc', torch.randn(args.max_num_objects, 1))\n\n    # Learn object_loc via EM and noise_scale via SVI.\n    optim = Adam({'lr': 0.1})\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    newton = Newton(trust_radii={'objects_loc': 1.0})\n    svi = SVI(poutine.block(model, hide=['objects_loc']),\n              poutine.block(guide, hide=['objects_loc']), optim, elbo)\n    for svi_step in range(50):\n        for em_step in range(2):\n            objects_loc = pyro.param('objects_loc').detach_().requires_grad_()\n            assert pyro.param('objects_loc').grad_fn is None\n            loss = elbo.differentiable_loss(model, guide, detections, args)  # E-step\n            updated = newton.get_step(loss, {'objects_loc': objects_loc})  # M-step\n            assert updated['objects_loc'].grad_fn is not None\n            pyro.get_param_store()['objects_loc'] = updated['objects_loc']\n            assert pyro.param('objects_loc').grad_fn is not None\n        loss = svi.step(detections, args)\n        logger.debug('step {: >2d}, loss = {:0.6f}, noise_scale = {:0.6f}'.format(\n            svi_step, loss, pyro.param('noise_scale').item()))\n\n\ndef test_svi_multi():\n    args = make_args()\n    args.assignment_grad = True\n    detections = generate_data(args)\n\n    pyro.clear_param_store()\n    pyro.param('noise_scale', torch.tensor(args.init_noise_scale),\n               constraint=constraints.positive)\n    pyro.param('objects_loc', torch.randn(args.max_num_objects, 1))\n\n    # Learn object_loc via Newton and noise_scale via Adam.\n    elbo = TraceEnum_ELBO(max_plate_nesting=2)\n    adam = Adam({'lr': 0.1})\n    newton = Newton(trust_radii={'objects_loc': 1.0})\n    optim = MixedMultiOptimizer([(['noise_scale'], adam),\n                                 (['objects_loc'], newton)])\n    for svi_step in range(50):\n        with poutine.trace(param_only=True) as param_capture:\n            loss = elbo.differentiable_loss(model, guide, detections, args)\n        params = {name: pyro.param(name).unconstrained()\n                  for name in param_capture.trace.nodes.keys()}\n        optim.step(loss, params)\n        logger.debug('step {: >2d}, loss = {:0.6f}, noise_scale = {:0.6f}'.format(\n            svi_step, loss.item(), pyro.param('noise_scale').item()))\n"""
tests/contrib/tracking/test_hashing.py,26,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\n\nimport pytest\nimport torch\n\nfrom pyro.contrib.tracking.hashing import LSH, ApproxSet, merge_points\nfrom tests.common import assert_equal\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.parametrize('scale', [-1., 0., -1 * torch.ones(2, 2)])\ndef test_lsh_init(scale):\n    with pytest.raises(ValueError):\n        LSH(scale)\n\n\n@pytest.mark.parametrize('scale', [0.1, 1, 10, 100])\ndef test_lsh_add(scale):\n    lsh = LSH(scale)\n    a = torch.rand(10)\n    lsh.add('a', a)\n    assert lsh._hash_to_key[lsh._key_to_hash['a']] == {'a'}\n\n\n@pytest.mark.parametrize('scale', [0.1, 1, 10, 100])\ndef test_lsh_hash_nearby(scale):\n    k = 5\n    lsh = LSH(scale)\n    a = -2 * scale + torch.rand(k) * scale * 0.49\n    b = -1 * scale + torch.rand(k) * scale * 0.49\n    c = torch.rand(k) * scale * 0.49\n    d = scale + torch.rand(k) * scale * 0.49\n    e = 2 * scale + torch.rand(k) * scale * 0.49\n    f = 4 * scale + torch.rand(k) * scale * 0.49\n\n    assert_equal(lsh._hash(a), (-2,) * k)\n    assert_equal(lsh._hash(b), (-1,) * k)\n    assert_equal(lsh._hash(c), (0,) * k)\n    assert_equal(lsh._hash(d), (1,) * k)\n    assert_equal(lsh._hash(e), (2,) * k)\n    assert_equal(lsh._hash(f), (4,) * k)\n\n    lsh.add('a', a)\n    lsh.add('b', b)\n    lsh.add('c', c)\n    lsh.add('d', d)\n    lsh.add('e', e)\n    lsh.add('f', f)\n\n    assert lsh.nearby('a') == {'b'}\n    assert lsh.nearby('b') == {'a', 'c'}\n    assert lsh.nearby('c') == {'b', 'd'}\n    assert lsh.nearby('d') == {'c', 'e'}\n    assert lsh.nearby('e') == {'d'}\n    assert lsh.nearby('f') == set()\n\n\ndef test_lsh_overwrite():\n    lsh = LSH(1)\n    a = torch.zeros(2)\n    b = torch.ones(2)\n    lsh.add('a', a)\n    lsh.add('b', b)\n    assert lsh.nearby('a') == {'b'}\n    b = torch.ones(2) * 4\n    lsh.add('b', b)\n    assert lsh.nearby('a') == set()\n\n\ndef test_lsh_remove():\n    lsh = LSH(1)\n    a = torch.zeros(2)\n    b = torch.ones(2)\n    lsh.add('a', a)\n    lsh.add('b', b)\n    assert lsh.nearby('a') == {'b'}\n    lsh.remove('b')\n    assert lsh.nearby('a') == set()\n\n\n@pytest.mark.parametrize('scale', [-1., 0., -1 * torch.ones(2, 2)])\ndef test_aps_init(scale):\n    with pytest.raises(ValueError):\n        ApproxSet(scale)\n\n\n@pytest.mark.parametrize('scale', [0.1, 1, 10, 100])\ndef test_aps_hash(scale):\n    k = 10\n    aps = ApproxSet(scale)\n    a = -2 * scale + torch.rand(k) * scale * 0.49\n    b = -1 * scale + torch.rand(k) * scale * 0.49\n    c = torch.rand(k) * scale * 0.49\n    d = scale + torch.rand(k) * scale * 0.49\n    e = 2 * scale + torch.rand(k) * scale * 0.49\n    f = 4 * scale + torch.rand(k) * scale * 0.49\n\n    assert_equal(aps._hash(a), (-2,) * k)\n    assert_equal(aps._hash(b), (-1,) * k)\n    assert_equal(aps._hash(c), (0,) * k)\n    assert_equal(aps._hash(d), (1,) * k)\n    assert_equal(aps._hash(e), (2,) * k)\n    assert_equal(aps._hash(f), (4,) * k)\n\n\n@pytest.mark.parametrize('scale', [0.1, 1, 10, 100])\ndef test_aps_try_add(scale):\n    k = 10\n    aps = ApproxSet(scale)\n    a = torch.rand(k) * scale * 0.49\n    b = torch.rand(k) * scale * 0.49\n    c = scale + torch.rand(k) * scale * 0.49\n    d = scale + torch.rand(k) * scale * 0.49\n\n    assert_equal(aps.try_add(a), True)\n    assert_equal(aps.try_add(b), False)\n    assert_equal(aps.try_add(c), True)\n    assert_equal(aps.try_add(d), False)\n\n\ndef test_merge_points_small():\n    points = torch.tensor([\n        [0., 0.],\n        [0., 1.],\n        [2., 0.],\n        [2., 0.5],\n        [2., 1.0],\n    ])\n    merged_points, groups = merge_points(points, radius=1.0)\n\n    assert len(merged_points) == 3\n    assert set(map(frozenset, groups)) == set(map(frozenset, [[0], [1], [2, 3, 4]]))\n    assert_equal(merged_points[0], points[0])\n    assert_equal(merged_points[1], points[1])\n    assert merged_points[2, 0] == 2\n    assert 0.325 <= merged_points[2, 1] <= 0.625\n\n\n@pytest.mark.parametrize('radius', [0.01, 0.1, 1., 10., 100.])\n@pytest.mark.parametrize('dim', [1, 2, 3])\ndef test_merge_points_large(dim, radius):\n    points = 10 * torch.randn(200, dim)\n    merged_points, groups = merge_points(points, radius)\n    logger.debug('merged {} -> {}'.format(len(points), len(merged_points)))\n\n    assert merged_points.dim() == 2\n    assert merged_points.shape[-1] == dim\n    assert len(groups) == len(merged_points)\n    assert sum(len(g) for g in groups) == len(points)\n    assert set(sum(groups, ())) == set(range(len(points)))\n    d2 = (merged_points.unsqueeze(-2) - merged_points.unsqueeze(-3)).pow(2).sum(-1)\n    assert d2.min() < radius ** 2\n"""
tests/contrib/tracking/test_measurements.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom pyro.contrib.tracking.measurements import PositionMeasurement\n\n\ndef test_PositionMeasurement():\n    dimension = 3\n    time = 0.232\n    frame_num = 5\n    measurement = PositionMeasurement(\n        mean=torch.rand(dimension),\n        cov=torch.eye(dimension), time=time, frame_num=frame_num)\n    assert measurement.dimension == dimension\n    x = torch.rand(2*dimension)\n    assert measurement(x).shape == (dimension,)\n    assert measurement.mean.shape == (dimension,)\n    assert measurement.cov.shape == (dimension, dimension)\n    assert measurement.time == time\n    assert measurement.frame_num == frame_num\n    assert measurement.geodesic_difference(\n        torch.rand(dimension), torch.rand(dimension)).shape \\\n        == (dimension,)\n    assert measurement.jacobian().shape == (dimension, 2*dimension)\n'"
tests/infer/mcmc/__init__.py,0,b''
tests/infer/mcmc/test_adaptation.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nfrom pyro.infer.mcmc.adaptation import (\n    ArrowheadMassMatrix,\n    BlockMassMatrix,\n    WarmupAdapter,\n    adapt_window,\n)\nfrom tests.common import assert_close, assert_equal\n\n\n@pytest.mark.parametrize(""adapt_step_size, adapt_mass, warmup_steps, expected"", [\n    (False, False, 100, []),\n    (False, True, 50, [(0, 6), (7, 44), (45, 49)]),\n    (True, False, 150, [(0, 74), (75, 99), (100, 149)]),\n    (True, True, 200, [(0, 74), (75, 99), (100, 149), (150, 199)]),\n    (True, True, 280, [(0, 74), (75, 99), (100, 229), (230, 279)]),\n    (True, True, 18, [(0, 17)]),\n])\ndef test_adaptation_schedule(adapt_step_size, adapt_mass, warmup_steps, expected):\n    adapter = WarmupAdapter(0.1,\n                            adapt_step_size=adapt_step_size,\n                            adapt_mass_matrix=adapt_mass)\n    adapter.configure(warmup_steps, mass_matrix_shape={""z"": (5, 5)})\n    expected_schedule = [adapt_window(i, j) for i, j in expected]\n    assert_equal(adapter.adaptation_schedule, expected_schedule, prec=0)\n\n\n@pytest.mark.parametrize(""diagonal"", [True, False])\ndef test_arrowhead_mass_matrix(diagonal):\n    shape = (2, 3)\n    num_samples = 1000\n\n    size = shape[0] * shape[1]\n    block_adapter = BlockMassMatrix()\n    arrowhead_adapter = ArrowheadMassMatrix()\n    mass_matrix_shape = (size,) if diagonal else (size, size)\n    block_adapter.configure({(""z"",): mass_matrix_shape})\n    arrowhead_adapter.configure({(""z"",): mass_matrix_shape})\n\n    cov = torch.randn(size, size)\n    cov = torch.mm(cov, cov.t())\n    if diagonal:\n        cov = cov.diag().diag()\n    z_dist = torch.distributions.MultivariateNormal(torch.zeros(size), covariance_matrix=cov)\n    g_dist = torch.distributions.MultivariateNormal(torch.zeros(size), precision_matrix=cov)\n    z_samples = z_dist.sample((num_samples,)).reshape((num_samples,) + shape)\n    g_samples = g_dist.sample((num_samples,)).reshape((num_samples,) + shape)\n\n    for i in range(num_samples):\n        block_adapter.update({""z"": z_samples[i]}, {""z"": g_samples[i]})\n        arrowhead_adapter.update({""z"": z_samples[i]}, {""z"": g_samples[i]})\n    block_adapter.end_adaptation()\n    arrowhead_adapter.end_adaptation()\n\n    assert_close(arrowhead_adapter.inverse_mass_matrix[(\'z\',)],\n                 block_adapter.inverse_mass_matrix[(\'z\',)],\n                 atol=0.3, rtol=0.3)\n'"
tests/infer/mcmc/test_hmc.py,34,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport os\nfrom collections import namedtuple\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.mcmc import NUTS\nfrom pyro.infer.mcmc.hmc import HMC\nfrom pyro.infer.mcmc.api import MCMC\nfrom tests.common import assert_equal, assert_close\n\nlogger = logging.getLogger(__name__)\n\n\ndef mark_jit(*args, **kwargs):\n    jit_markers = kwargs.pop(""marks"", [])\n    jit_markers += [\n        pytest.mark.skipif(\'CI\' in os.environ,\n                           reason=\'to reduce running time on CI\')\n    ]\n    kwargs[""marks""] = jit_markers\n    return pytest.param(*args, **kwargs)\n\n\ndef jit_idfn(param):\n    return ""JIT={}"".format(param)\n\n\nclass GaussianChain:\n\n    def __init__(self, dim, chain_len, num_obs):\n        self.dim = dim\n        self.chain_len = chain_len\n        self.num_obs = num_obs\n        self.loc_0 = torch.zeros(self.dim)\n        self.lambda_prec = torch.ones(self.dim)\n\n    def model(self, data):\n        loc = self.loc_0\n        lambda_prec = self.lambda_prec\n        for i in range(1, self.chain_len + 1):\n            loc = pyro.sample(\'loc_{}\'.format(i),\n                              dist.Normal(loc=loc, scale=lambda_prec))\n        pyro.sample(\'obs\', dist.Normal(loc, lambda_prec), obs=data)\n\n    @property\n    def data(self):\n        return torch.ones(self.num_obs, self.dim)\n\n    def id_fn(self):\n        return \'dim={}_chain-len={}_num_obs={}\'.format(self.dim, self.chain_len, self.num_obs)\n\n\ndef rmse(t1, t2):\n    return (t1 - t2).pow(2).mean().sqrt()\n\n\nT = namedtuple(\'TestExample\', [\n    \'fixture\',\n    \'num_samples\',\n    \'warmup_steps\',\n    \'hmc_params\',\n    \'expected_means\',\n    \'expected_precs\',\n    \'mean_tol\',\n    \'std_tol\'])\n\nTEST_CASES = [\n    T(\n        GaussianChain(dim=10, chain_len=3, num_obs=1),\n        num_samples=800,\n        warmup_steps=200,\n        hmc_params={\'step_size\': 0.5,\n                    \'num_steps\': 4},\n        expected_means=[0.25, 0.50, 0.75],\n        expected_precs=[1.33, 1, 1.33],\n        mean_tol=0.08,\n        std_tol=0.08,\n    ),\n    T(\n        GaussianChain(dim=10, chain_len=4, num_obs=1),\n        num_samples=1600,\n        warmup_steps=300,\n        hmc_params={\'step_size\': 0.46,\n                    \'num_steps\': 5},\n        expected_means=[0.20, 0.40, 0.60, 0.80],\n        expected_precs=[1.25, 0.83, 0.83, 1.25],\n        mean_tol=0.08,\n        std_tol=0.08,\n    ),\n    T(\n        GaussianChain(dim=5, chain_len=2, num_obs=100),\n        num_samples=2000,\n        warmup_steps=1000,\n        hmc_params={\'num_steps\': 15, \'step_size\': 0.7},\n        expected_means=[0.5, 1.0],\n        expected_precs=[2.0, 100],\n        mean_tol=0.08,\n        std_tol=0.08,\n    ),\n    T(\n        GaussianChain(dim=5, chain_len=9, num_obs=1),\n        num_samples=3000,\n        warmup_steps=500,\n        hmc_params={\'step_size\': 0.2,\n                    \'num_steps\': 15},\n        expected_means=[0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90],\n        expected_precs=[1.11, 0.63, 0.48, 0.42, 0.4, 0.42, 0.48, 0.63, 1.11],\n        mean_tol=0.11,\n        std_tol=0.11,\n    )\n]\n\nTEST_IDS = [t[0].id_fn() if type(t).__name__ == \'TestExample\'\n            else t[0][0].id_fn() for t in TEST_CASES]\n\n\n@pytest.mark.parametrize(\n    \'fixture, num_samples, warmup_steps, hmc_params, expected_means, expected_precs, mean_tol, std_tol\',\n    TEST_CASES,\n    ids=TEST_IDS)\n@pytest.mark.skip(reason=\'Slow test (https://github.com/pytorch/pytorch/issues/12190)\')\n@pytest.mark.disable_validation()\ndef test_hmc_conjugate_gaussian(fixture,\n                                num_samples,\n                                warmup_steps,\n                                hmc_params,\n                                expected_means,\n                                expected_precs,\n                                mean_tol,\n                                std_tol):\n    pyro.get_param_store().clear()\n    hmc_kernel = HMC(fixture.model, **hmc_params)\n    samples = MCMC(hmc_kernel, num_samples, warmup_steps).run(fixture.data)\n    for i in range(1, fixture.chain_len + 1):\n        param_name = \'loc_\' + str(i)\n        marginal = samples[param_name]\n        latent_loc = marginal.mean(0)\n        latent_std = marginal.var(0).sqrt()\n        expected_mean = torch.ones(fixture.dim) * expected_means[i - 1]\n        expected_std = 1 / torch.sqrt(torch.ones(fixture.dim) * expected_precs[i - 1])\n\n        # Actual vs expected posterior means for the latents\n        logger.debug(\'Posterior mean (actual) - {}\'.format(param_name))\n        logger.debug(latent_loc)\n        logger.debug(\'Posterior mean (expected) - {}\'.format(param_name))\n        logger.debug(expected_mean)\n        assert_equal(rmse(latent_loc, expected_mean).item(), 0.0, prec=mean_tol)\n\n        # Actual vs expected posterior precisions for the latents\n        logger.debug(\'Posterior std (actual) - {}\'.format(param_name))\n        logger.debug(latent_std)\n        logger.debug(\'Posterior std (expected) - {}\'.format(param_name))\n        logger.debug(expected_std)\n        assert_equal(rmse(latent_std, expected_std).item(), 0.0, prec=std_tol)\n\n\n@pytest.mark.parametrize(\n    ""step_size, trajectory_length, num_steps, adapt_step_size, adapt_mass_matrix, full_mass"",\n    [\n        (0.0855, None, 4, False, False, False),\n        (0.0855, None, 4, False, True, False),\n        (None, 1, None, True, False, False),\n        (None, 1, None, True, True, False),\n        (None, 1, None, True, True, True),\n    ]\n)\ndef test_logistic_regression(step_size, trajectory_length, num_steps,\n                             adapt_step_size, adapt_mass_matrix, full_mass):\n    dim = 3\n    data = torch.randn(2000, dim)\n    true_coefs = torch.arange(1., dim + 1.)\n    labels = dist.Bernoulli(logits=(true_coefs * data).sum(-1)).sample()\n\n    def model(data):\n        coefs_mean = pyro.param(\'coefs_mean\', torch.zeros(dim))\n        coefs = pyro.sample(\'beta\', dist.Normal(coefs_mean, torch.ones(dim)))\n        y = pyro.sample(\'y\', dist.Bernoulli(logits=(coefs * data).sum(-1)), obs=labels)\n        return y\n\n    hmc_kernel = HMC(model, step_size=step_size, trajectory_length=trajectory_length,\n                     num_steps=num_steps, adapt_step_size=adapt_step_size,\n                     adapt_mass_matrix=adapt_mass_matrix, full_mass=full_mass)\n    mcmc = MCMC(hmc_kernel, num_samples=500, warmup_steps=100, disable_progbar=True)\n    mcmc.run(data)\n    samples = mcmc.get_samples()[\'beta\']\n    assert_equal(rmse(true_coefs, samples.mean(0)).item(), 0.0, prec=0.1)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\ndef test_dirichlet_categorical(jit):\n    def model(data):\n        concentration = torch.tensor([1.0, 1.0, 1.0])\n        p_latent = pyro.sample(\'p_latent\', dist.Dirichlet(concentration))\n        pyro.sample(""obs"", dist.Categorical(p_latent), obs=data)\n        return p_latent\n\n    true_probs = torch.tensor([0.1, 0.6, 0.3])\n    data = dist.Categorical(true_probs).sample(sample_shape=(torch.Size((2000,))))\n    hmc_kernel = HMC(model, trajectory_length=1, jit_compile=jit, ignore_jit_warnings=True)\n    mcmc = MCMC(hmc_kernel, num_samples=200, warmup_steps=100)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[\'p_latent\'].mean(0), true_probs, prec=0.02)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\ndef test_beta_bernoulli(jit):\n    def model(data):\n        alpha = torch.tensor([1.1, 1.1])\n        beta = torch.tensor([1.1, 1.1])\n        p_latent = pyro.sample(\'p_latent\', dist.Beta(alpha, beta))\n        with pyro.plate(""data"", data.shape[0], dim=-2):\n            pyro.sample(\'obs\', dist.Bernoulli(p_latent), obs=data)\n        return p_latent\n\n    true_probs = torch.tensor([0.9, 0.1])\n    data = dist.Bernoulli(true_probs).sample(sample_shape=(torch.Size((1000,))))\n    hmc_kernel = HMC(model, trajectory_length=1, max_plate_nesting=2,\n                     jit_compile=jit, ignore_jit_warnings=True)\n    mcmc = MCMC(hmc_kernel, num_samples=800, warmup_steps=500)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[\'p_latent\'].mean(0), true_probs, prec=0.05)\n\n\ndef test_gamma_normal():\n    def model(data):\n        rate = torch.tensor([1.0, 1.0])\n        concentration = torch.tensor([1.0, 1.0])\n        p_latent = pyro.sample(\'p_latent\', dist.Gamma(rate, concentration))\n        pyro.sample(""obs"", dist.Normal(3, p_latent), obs=data)\n        return p_latent\n\n    true_std = torch.tensor([0.5, 2])\n    data = dist.Normal(3, true_std).sample(sample_shape=(torch.Size((2000,))))\n    hmc_kernel = HMC(model, num_steps=15, step_size=0.01, adapt_step_size=True)\n    mcmc = MCMC(hmc_kernel, num_samples=200, warmup_steps=200)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[\'p_latent\'].mean(0), true_std, prec=0.05)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\ndef test_bernoulli_latent_model(jit):\n    def model(data):\n        y_prob = pyro.sample(""y_prob"", dist.Beta(1.0, 1.0))\n        y = pyro.sample(""y"", dist.Bernoulli(y_prob))\n        with pyro.plate(""data"", data.shape[0]):\n            z = pyro.sample(""z"", dist.Bernoulli(0.65 * y + 0.1))\n            pyro.sample(""obs"", dist.Normal(2. * z, 1.), obs=data)\n        pyro.sample(""nuisance"", dist.Bernoulli(0.3))\n\n    N = 2000\n    y_prob = torch.tensor(0.3)\n    y = dist.Bernoulli(y_prob).sample(torch.Size((N,)))\n    z = dist.Bernoulli(0.65 * y + 0.1).sample()\n    data = dist.Normal(2. * z, 1.0).sample()\n    hmc_kernel = HMC(model, trajectory_length=1, max_plate_nesting=1,\n                     jit_compile=jit, ignore_jit_warnings=True)\n    mcmc = MCMC(hmc_kernel, num_samples=600, warmup_steps=200)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[\'y_prob\'].mean(0), y_prob, prec=0.06)\n\n\n@pytest.mark.parametrize(""kernel"", [HMC, NUTS])\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\n@pytest.mark.skipif(""CUDA_TEST"" in os.environ, reason=""https://github.com/pytorch/pytorch/issues/22811"")\ndef test_unnormalized_normal(kernel, jit):\n    true_mean, true_std = torch.tensor(5.), torch.tensor(1.)\n    init_params = {""z"": torch.tensor(0.)}\n\n    def potential_energy(params):\n        return 0.5 * torch.sum(((params[""z""] - true_mean) / true_std) ** 2)\n\n    potential_fn = potential_energy if not jit else torch.jit.trace(potential_energy, init_params)\n    hmc_kernel = kernel(model=None, potential_fn=potential_fn)\n\n    samples = init_params\n    warmup_steps = 400\n    hmc_kernel.initial_params = samples\n    hmc_kernel.setup(warmup_steps)\n\n    for i in range(warmup_steps):\n        samples = hmc_kernel(samples)\n\n    posterior = []\n    for i in range(2000):\n        hmc_kernel.clear_cache()\n        samples = hmc_kernel(samples)\n        posterior.append(samples)\n\n    posterior = torch.stack([sample[""z""] for sample in posterior])\n    assert_close(torch.mean(posterior), true_mean, rtol=0.05)\n    assert_close(torch.std(posterior), true_std, rtol=0.05)\n\n\n@pytest.mark.parametrize(\'jit\', [False, mark_jit(True)], ids=jit_idfn)\n@pytest.mark.parametrize(\'op\', [torch.inverse, torch.cholesky])\ndef test_singular_matrix_catch(jit, op):\n    def potential_energy(z):\n        return op(z[\'cov\']).sum()\n\n    init_params = {\'cov\': torch.eye(3)}\n    potential_fn = potential_energy if not jit else torch.jit.trace(potential_energy, init_params)\n    hmc_kernel = HMC(potential_fn=potential_fn, adapt_step_size=False,\n                     num_steps=10, step_size=1e-20)\n    hmc_kernel.initial_params = init_params\n    hmc_kernel.setup(warmup_steps=0)\n    # setup an invalid cache to trigger singular error for torch.inverse\n    hmc_kernel._cache({\'cov\': torch.ones(3, 3)}, torch.tensor(0.), {\'cov\': torch.zeros(3, 3)})\n\n    samples = init_params\n    for i in range(10):\n        samples = hmc_kernel.sample(samples)\n'"
tests/infer/mcmc/test_mcmc_api.py,13,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nfrom functools import partial\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.mcmc import HMC, NUTS\nfrom pyro.infer.mcmc.api import MCMC, _UnarySampler, _MultiSampler\nfrom pyro.infer.mcmc.mcmc_kernel import MCMCKernel\nfrom pyro.infer.mcmc.util import initialize_model\nfrom pyro.util import optional\nfrom tests.common import assert_close\n\n\nclass PriorKernel(MCMCKernel):\n    """"""\n    Disregards the value of the current trace (or observed data) and\n    samples a value from the model\'s prior.\n    """"""\n    def __init__(self, model):\n        self.model = model\n        self.data = None\n        self._initial_params = None\n        self._prototype_trace = None\n        self.transforms = None\n\n    def setup(self, warmup_steps, data):\n        self.data = data\n        init_params, potential_fn, transforms, model_trace = initialize_model(self.model,\n                                                                              model_args=(data,))\n        if self._initial_params is None:\n            self._initial_params = init_params\n        if self.transforms is None:\n            self.transforms = transforms\n        self._prototype_trace = model_trace\n\n    def diagnostics(self):\n        return {\'dummy_key\': \'dummy_value\'}\n\n    @property\n    def initial_params(self):\n        return self._initial_params\n\n    @initial_params.setter\n    def initial_params(self, params):\n        self._initial_params = params\n\n    def cleanup(self):\n        self.data = None\n\n    def sample_params(self):\n        trace = poutine.trace(self.model).get_trace(self.data)\n        return {k: v[""value""] for k, v in trace.iter_stochastic_nodes()}\n\n    def sample(self, params):\n        new_params = self.sample_params()\n        assert params.keys() == new_params.keys()\n        for k, v in params.items():\n            assert new_params[k].shape == v.shape\n        return new_params\n\n\ndef normal_normal_model(data):\n    x = torch.tensor([0.0])\n    y = pyro.sample(\'y\', dist.Normal(x, torch.ones(data.shape)))\n    pyro.sample(\'obs\', dist.Normal(y, torch.tensor([1.0])), obs=data)\n    return y\n\n\n@pytest.mark.parametrize(\'num_draws\', [None, 1800, 2200])\n@pytest.mark.parametrize(\'group_by_chain\', [False, True])\n@pytest.mark.parametrize(\'num_chains\', [1, 2])\n@pytest.mark.filterwarnings(""ignore:num_chains"")\ndef test_mcmc_interface(num_draws, group_by_chain, num_chains):\n    num_samples = 2000\n    data = torch.tensor([1.0])\n    initial_params, _, transforms, _ = initialize_model(normal_normal_model, model_args=(data,),\n                                                        num_chains=num_chains)\n    kernel = PriorKernel(normal_normal_model)\n    mcmc = MCMC(kernel=kernel, num_samples=num_samples, warmup_steps=100, num_chains=num_chains,\n                mp_context=""spawn"", initial_params=initial_params, transforms=transforms)\n    mcmc.run(data)\n    samples = mcmc.get_samples(num_draws, group_by_chain=group_by_chain)\n    # test sample shape\n    expected_samples = num_draws if num_draws is not None else num_samples\n    if group_by_chain:\n        expected_shape = (mcmc.num_chains, expected_samples, 1)\n    elif num_draws is not None:\n        # FIXME: what is the expected behavior of num_draw is not None and group_by_chain=False?\n        expected_shape = (expected_samples, 1)\n    else:\n        expected_shape = (mcmc.num_chains * expected_samples, 1)\n    assert samples[\'y\'].shape == expected_shape\n\n    # test sample stats\n    if group_by_chain:\n        samples = {k: v.reshape((-1,) + v.shape[2:]) for k, v in samples.items()}\n    sample_mean = samples[\'y\'].mean()\n    sample_std = samples[\'y\'].std()\n    assert_close(sample_mean, torch.tensor(0.0), atol=0.1)\n    assert_close(sample_std, torch.tensor(1.0), atol=0.1)\n\n\n@pytest.mark.parametrize(""num_chains, cpu_count"", [\n    (1, 2),\n    (2, 1),\n    (2, 2),\n    (2, 3),\n])\n@pytest.mark.parametrize(""default_init_params"", [True, False])\ndef test_num_chains(num_chains, cpu_count, default_init_params,\n                    monkeypatch):\n    monkeypatch.setattr(torch.multiprocessing, \'cpu_count\', lambda: cpu_count)\n    data = torch.tensor([1.0])\n    initial_params, _, transforms, _ = initialize_model(normal_normal_model,\n                                                        model_args=(data,),\n                                                        num_chains=num_chains)\n    if default_init_params:\n        initial_params = None\n    kernel = PriorKernel(normal_normal_model)\n    available_cpu = max(1, cpu_count-1)\n    mp_context = ""spawn""\n    with optional(pytest.warns(UserWarning), available_cpu < num_chains):\n        mcmc = MCMC(kernel, num_samples=10, warmup_steps=10, num_chains=num_chains,\n                    initial_params=initial_params, transforms=transforms, mp_context=mp_context)\n    mcmc.run(data)\n    assert mcmc.num_chains == num_chains\n    if mcmc.num_chains == 1 or available_cpu < num_chains:\n        assert isinstance(mcmc.sampler, _UnarySampler)\n    else:\n        assert isinstance(mcmc.sampler, _MultiSampler)\n\n\ndef _empty_model():\n    return torch.tensor(1)\n\n\ndef _hook(iters, kernel, samples, stage, i):\n    assert samples == {}\n    iters.append((stage, i))\n\n\n@pytest.mark.parametrize(""kernel, model"", [\n    (HMC, _empty_model),\n    (NUTS, _empty_model),\n])\n@pytest.mark.parametrize(""jit"", [False, True])\n@pytest.mark.parametrize(""num_chains"", [\n    1,\n    2\n])\n@pytest.mark.filterwarnings(""ignore:num_chains"")\ndef test_null_model_with_hook(kernel, model, jit, num_chains):\n    num_warmup, num_samples = 10, 10\n    initial_params, potential_fn, transforms, _ = initialize_model(model,\n                                                                   num_chains=num_chains)\n\n    iters = []\n    hook = partial(_hook, iters)\n\n    mp_context = ""spawn"" if ""CUDA_TEST"" in os.environ else None\n\n    kern = kernel(potential_fn=potential_fn, transforms=transforms, jit_compile=jit)\n    mcmc = MCMC(kern, num_samples=num_samples, warmup_steps=num_warmup,\n                num_chains=num_chains, initial_params=initial_params, hook_fn=hook, mp_context=mp_context)\n    mcmc.run()\n    samples = mcmc.get_samples()\n    assert samples == {}\n    if num_chains == 1:\n        expected = [(""Warmup"", i) for i in range(num_warmup)] + [(""Sample"", i) for i in range(num_samples)]\n        assert iters == expected\n\n\n@pytest.mark.parametrize(""num_chains"", [\n    1,\n    2\n])\n@pytest.mark.filterwarnings(""ignore:num_chains"")\ndef test_mcmc_diagnostics(num_chains):\n    data = torch.tensor([2.0]).repeat(3)\n    initial_params, _, transforms, _ = initialize_model(normal_normal_model,\n                                                        model_args=(data,),\n                                                        num_chains=num_chains)\n    kernel = PriorKernel(normal_normal_model)\n    mcmc = MCMC(kernel, num_samples=10, warmup_steps=10, num_chains=num_chains, mp_context=""spawn"",\n                initial_params=initial_params, transforms=transforms)\n    mcmc.run(data)\n    if not torch.backends.mkl.is_available():\n        pytest.skip()\n    diagnostics = mcmc.diagnostics()\n    assert diagnostics[""y""][""n_eff""].shape == data.shape\n    assert diagnostics[""y""][""r_hat""].shape == data.shape\n    assert diagnostics[""dummy_key""] == {\'chain {}\'.format(i): \'dummy_value\'\n                                        for i in range(num_chains)}\n\n\n@pytest.mark.filterwarnings(""ignore:num_chains"")\ndef test_sequential_consistent(monkeypatch):\n    # test if there is no stuff left from the previous chain\n    monkeypatch.setattr(torch.multiprocessing, \'cpu_count\', lambda: 1)\n\n    class FirstKernel(NUTS):\n        def setup(self, warmup_steps, *args, **kwargs):\n            self._chain_id = 0 if \'_chain_id\' not in self.__dict__ else 1\n            pyro.set_rng_seed(self._chain_id)\n            super().setup(warmup_steps, *args, **kwargs)\n\n    class SecondKernel(NUTS):\n        def setup(self, warmup_steps, *args, **kwargs):\n            self._chain_id = 1 if \'_chain_id\' not in self.__dict__ else 0\n            pyro.set_rng_seed(self._chain_id)\n            super().setup(warmup_steps, *args, **kwargs)\n\n    data = torch.tensor([1.0])\n    kernel = FirstKernel(normal_normal_model)\n    mcmc = MCMC(kernel, num_samples=100, warmup_steps=100, num_chains=2)\n    mcmc.run(data)\n    samples1 = mcmc.get_samples(group_by_chain=True)\n\n    kernel = SecondKernel(normal_normal_model)\n    mcmc = MCMC(kernel, num_samples=100, warmup_steps=100, num_chains=2)\n    mcmc.run(data)\n    samples2 = mcmc.get_samples(group_by_chain=True)\n\n    assert_close(samples1[""y""][0], samples2[""y""][1])\n    assert_close(samples1[""y""][1], samples2[""y""][0])\n'"
tests/infer/mcmc/test_mcmc_util.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import Predictive\nfrom pyro.infer.autoguide import (init_to_feasible, init_to_generated, init_to_mean, init_to_median, init_to_sample,\n                                  init_to_uniform, init_to_value)\nfrom pyro.infer.mcmc import NUTS\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.infer.mcmc.util import initialize_model\nfrom pyro.util import optional\nfrom tests.common import assert_close\n\n\ndef beta_bernoulli():\n    N = 1000\n    true_probs = torch.tensor([0.2, 0.3, 0.4, 0.8, 0.5])\n    data = dist.Bernoulli(true_probs).sample([N])\n\n    def model(data=None):\n        with pyro.plate(""num_components"", 5):\n            beta = pyro.sample(""beta"", dist.Beta(1., 1.))\n            with pyro.plate(""data"", N):\n                pyro.sample(""obs"", dist.Bernoulli(beta), obs=data)\n\n    return model, data, true_probs\n\n\n@pytest.mark.parametrize(""num_samples"", [100, 200, None])\n@pytest.mark.parametrize(""parallel"", [False, True])\ndef test_predictive(num_samples, parallel):\n    model, data, true_probs = beta_bernoulli()\n    init_params, potential_fn, transforms, _ = initialize_model(model,\n                                                                model_args=(data,))\n    nuts_kernel = NUTS(potential_fn=potential_fn, transforms=transforms)\n    mcmc = MCMC(nuts_kernel,\n                100,\n                initial_params=init_params,\n                warmup_steps=100)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    with optional(pytest.warns(UserWarning), num_samples not in (None, 100)):\n        predictive = Predictive(model, samples,\n                                num_samples=num_samples,\n                                return_sites=[""beta"", ""obs""],\n                                parallel=parallel)\n        predictive_samples = predictive()\n\n    # check shapes\n    assert predictive_samples[""beta""].shape == (100, 1, 5)\n    assert predictive_samples[""obs""].shape == (100, 1000, 5)\n\n    # check sample mean\n    assert_close(predictive_samples[""obs""].reshape([-1, 5]).mean(0), true_probs, rtol=0.1)\n\n\ndef model_with_param():\n    x = pyro.param(""x"", torch.tensor(1.))\n    pyro.sample(""y"", dist.Normal(x, 1))\n\n\n@pytest.mark.parametrize(""jit_compile"", [False, True])\n@pytest.mark.parametrize(""num_chains"", [1, 2])\n@pytest.mark.filterwarnings(""ignore:num_chains"")\ndef test_model_with_param(jit_compile, num_chains):\n    kernel = NUTS(model_with_param, jit_compile=jit_compile, ignore_jit_warnings=True)\n    mcmc = MCMC(kernel, 10, num_chains=num_chains, mp_context=""spawn"")\n    mcmc.run()\n\n\n@pytest.mark.parametrize(""subsample_size"", [10, 5])\ndef test_model_with_subsample(subsample_size):\n    size = 10\n\n    def model():\n        with pyro.plate(""J"", size, subsample_size=subsample_size):\n            pyro.sample(""x"", dist.Normal(0, 1))\n\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, 10)\n    if subsample_size < size:\n        with pytest.raises(RuntimeError, match=""subsample""):\n            mcmc.run()\n    else:\n        mcmc.run()\n\n\ndef test_init_to_value():\n    def model():\n        pyro.sample(""x"", dist.LogNormal(0, 1))\n\n    value = torch.randn(()).exp() * 10\n    kernel = NUTS(model, init_strategy=partial(init_to_value, values={""x"": value}))\n    kernel.setup(warmup_steps=10)\n    assert_close(value, kernel.initial_params[\'x\'].exp())\n\n\n@pytest.mark.parametrize(""init_strategy"", [\n    init_to_feasible,\n    init_to_mean,\n    init_to_median,\n    init_to_sample,\n    init_to_uniform,\n    init_to_value,\n    init_to_feasible(),\n    init_to_mean(),\n    init_to_median(num_samples=4),\n    init_to_sample(),\n    init_to_uniform(radius=0.1),\n    init_to_value(values={""x"": torch.tensor(3.)}),\n    init_to_generated(\n        generate=lambda: init_to_value(values={""x"": torch.rand(())})),\n], ids=str)\ndef test_init_strategy_smoke(init_strategy):\n    def model():\n        pyro.sample(""x"", dist.LogNormal(0, 1))\n\n    kernel = NUTS(model, init_strategy=init_strategy)\n    kernel.setup(warmup_steps=10)\n'"
tests/infer/mcmc/test_nuts.py,56,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport os\nfrom collections import namedtuple\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.autoguide import AutoDelta\nfrom pyro.contrib.conjugate.infer import BetaBinomialPair, collapse_conjugate, GammaPoissonPair, posterior_replay\nfrom pyro.infer import TraceEnum_ELBO, SVI\nfrom pyro.infer.mcmc import ArrowheadMassMatrix, MCMC, NUTS\nimport pyro.optim as optim\nimport pyro.poutine as poutine\nfrom pyro.util import ignore_jit_warnings\nfrom tests.common import assert_close, assert_equal\n\nfrom .test_hmc import GaussianChain, rmse\n\nlogger = logging.getLogger(__name__)\n\n\nT = namedtuple(\'TestExample\', [\n    \'fixture\',\n    \'num_samples\',\n    \'warmup_steps\',\n    \'expected_means\',\n    \'expected_precs\',\n    \'mean_tol\',\n    \'std_tol\'])\n\nTEST_CASES = [\n    T(\n        GaussianChain(dim=10, chain_len=3, num_obs=1),\n        num_samples=800,\n        warmup_steps=200,\n        expected_means=[0.25, 0.50, 0.75],\n        expected_precs=[1.33, 1, 1.33],\n        mean_tol=0.09,\n        std_tol=0.09,\n    ),\n    T(\n        GaussianChain(dim=10, chain_len=4, num_obs=1),\n        num_samples=1600,\n        warmup_steps=200,\n        expected_means=[0.20, 0.40, 0.60, 0.80],\n        expected_precs=[1.25, 0.83, 0.83, 1.25],\n        mean_tol=0.07,\n        std_tol=0.06,\n    ),\n    T(\n        GaussianChain(dim=5, chain_len=2, num_obs=10000),\n        num_samples=800,\n        warmup_steps=200,\n        expected_means=[0.5, 1.0],\n        expected_precs=[2.0, 10000],\n        mean_tol=0.05,\n        std_tol=0.05,\n    ),\n    T(\n        GaussianChain(dim=5, chain_len=9, num_obs=1),\n        num_samples=1400,\n        warmup_steps=200,\n        expected_means=[0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90],\n        expected_precs=[1.11, 0.63, 0.48, 0.42, 0.4, 0.42, 0.48, 0.63, 1.11],\n        mean_tol=0.08,\n        std_tol=0.08,\n    )\n]\n\n\nTEST_IDS = [t[0].id_fn() if type(t).__name__ == \'TestExample\'\n            else t[0][0].id_fn() for t in TEST_CASES]\n\n\ndef mark_jit(*args, **kwargs):\n    jit_markers = kwargs.pop(""marks"", [])\n    jit_markers += [\n        pytest.mark.skipif(\'CI\' in os.environ,\n                           reason=\'to reduce running time on CI\')\n    ]\n    kwargs[""marks""] = jit_markers\n    return pytest.param(*args, **kwargs)\n\n\ndef jit_idfn(param):\n    return ""JIT={}"".format(param)\n\n\n@pytest.mark.parametrize(\n    \'fixture, num_samples, warmup_steps, expected_means, expected_precs, mean_tol, std_tol\',\n    TEST_CASES,\n    ids=TEST_IDS)\n@pytest.mark.skip(reason=\'Slow test (https://github.com/pytorch/pytorch/issues/12190)\')\n@pytest.mark.disable_validation()\ndef test_nuts_conjugate_gaussian(fixture,\n                                 num_samples,\n                                 warmup_steps,\n                                 expected_means,\n                                 expected_precs,\n                                 mean_tol,\n                                 std_tol):\n    pyro.get_param_store().clear()\n    nuts_kernel = NUTS(fixture.model)\n    mcmc = MCMC(nuts_kernel, num_samples, warmup_steps)\n    mcmc.run(fixture.data)\n    samples = mcmc.get_samples()\n    for i in range(1, fixture.chain_len + 1):\n        param_name = \'loc_\' + str(i)\n        latent = samples[param_name]\n        latent_loc = latent.mean(0)\n        latent_std = latent.std(0)\n        expected_mean = torch.ones(fixture.dim) * expected_means[i - 1]\n        expected_std = 1 / torch.sqrt(torch.ones(fixture.dim) * expected_precs[i - 1])\n\n        # Actual vs expected posterior means for the latents\n        logger.debug(\'Posterior mean (actual) - {}\'.format(param_name))\n        logger.debug(latent_loc)\n        logger.debug(\'Posterior mean (expected) - {}\'.format(param_name))\n        logger.debug(expected_mean)\n        assert_equal(rmse(latent_loc, expected_mean).item(), 0.0, prec=mean_tol)\n\n        # Actual vs expected posterior precisions for the latents\n        logger.debug(\'Posterior std (actual) - {}\'.format(param_name))\n        logger.debug(latent_std)\n        logger.debug(\'Posterior std (expected) - {}\'.format(param_name))\n        logger.debug(expected_std)\n        assert_equal(rmse(latent_std, expected_std).item(), 0.0, prec=std_tol)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\n@pytest.mark.parametrize(""use_multinomial_sampling"", [True, False])\ndef test_logistic_regression(jit, use_multinomial_sampling):\n    dim = 3\n    data = torch.randn(2000, dim)\n    true_coefs = torch.arange(1., dim + 1.)\n    labels = dist.Bernoulli(logits=(true_coefs * data).sum(-1)).sample()\n\n    def model(data):\n        coefs_mean = torch.zeros(dim)\n        coefs = pyro.sample(\'beta\', dist.Normal(coefs_mean, torch.ones(dim)))\n        y = pyro.sample(\'y\', dist.Bernoulli(logits=(coefs * data).sum(-1)), obs=labels)\n        return y\n\n    nuts_kernel = NUTS(model,\n                       use_multinomial_sampling=use_multinomial_sampling,\n                       jit_compile=jit,\n                       ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel, num_samples=500, warmup_steps=100)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(rmse(true_coefs, samples[""beta""].mean(0)).item(), 0.0, prec=0.1)\n\n\n@pytest.mark.parametrize(\n    ""step_size, adapt_step_size, adapt_mass_matrix, full_mass"",\n    [\n        (0.1, False, False, False),\n        (0.5, False, True, False),\n        (None, True, False, False),\n        (None, True, True, False),\n        (None, True, True, True),\n    ]\n)\ndef test_beta_bernoulli(step_size, adapt_step_size, adapt_mass_matrix, full_mass):\n    def model(data):\n        alpha = torch.tensor([1.1, 1.1])\n        beta = torch.tensor([1.1, 1.1])\n        p_latent = pyro.sample(""p_latent"", dist.Beta(alpha, beta))\n        pyro.sample(""obs"", dist.Bernoulli(p_latent), obs=data)\n        return p_latent\n\n    true_probs = torch.tensor([0.9, 0.1])\n    data = dist.Bernoulli(true_probs).sample(sample_shape=(torch.Size((1000,))))\n    nuts_kernel = NUTS(model, step_size=step_size, adapt_step_size=adapt_step_size,\n                       adapt_mass_matrix=adapt_mass_matrix, full_mass=full_mass)\n    mcmc = MCMC(nuts_kernel, num_samples=400, warmup_steps=200)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[""p_latent""].mean(0), true_probs, prec=0.02)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\n@pytest.mark.parametrize(""use_multinomial_sampling"", [True, False])\ndef test_gamma_normal(jit, use_multinomial_sampling):\n    def model(data):\n        rate = torch.tensor([1.0, 1.0])\n        concentration = torch.tensor([1.0, 1.0])\n        p_latent = pyro.sample(\'p_latent\', dist.Gamma(rate, concentration))\n        pyro.sample(""obs"", dist.Normal(3, p_latent), obs=data)\n        return p_latent\n\n    true_std = torch.tensor([0.5, 2])\n    data = dist.Normal(3, true_std).sample(sample_shape=(torch.Size((2000,))))\n    nuts_kernel = NUTS(model,\n                       use_multinomial_sampling=use_multinomial_sampling,\n                       jit_compile=jit,\n                       ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel, num_samples=200, warmup_steps=100)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[""p_latent""].mean(0), true_std, prec=0.05)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\ndef test_dirichlet_categorical(jit):\n    def model(data):\n        concentration = torch.tensor([1.0, 1.0, 1.0])\n        p_latent = pyro.sample(\'p_latent\', dist.Dirichlet(concentration))\n        pyro.sample(""obs"", dist.Categorical(p_latent), obs=data)\n        return p_latent\n\n    true_probs = torch.tensor([0.1, 0.6, 0.3])\n    data = dist.Categorical(true_probs).sample(sample_shape=(torch.Size((2000,))))\n    nuts_kernel = NUTS(model, jit_compile=jit, ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel, num_samples=200, warmup_steps=100)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    posterior = samples[""p_latent""]\n    assert_equal(posterior.mean(0), true_probs, prec=0.02)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\ndef test_gamma_beta(jit):\n    def model(data):\n        alpha_prior = pyro.sample(\'alpha\', dist.Gamma(concentration=1., rate=1.))\n        beta_prior = pyro.sample(\'beta\', dist.Gamma(concentration=1., rate=1.))\n        pyro.sample(\'x\', dist.Beta(concentration1=alpha_prior, concentration0=beta_prior), obs=data)\n\n    true_alpha = torch.tensor(5.)\n    true_beta = torch.tensor(1.)\n    data = dist.Beta(concentration1=true_alpha, concentration0=true_beta).sample(torch.Size((5000,)))\n    nuts_kernel = NUTS(model, jit_compile=jit, ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel, num_samples=500, warmup_steps=200)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[""alpha""].mean(0), true_alpha, prec=0.08)\n    assert_equal(samples[""beta""].mean(0), true_beta, prec=0.05)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\ndef test_gaussian_mixture_model(jit):\n    K, N = 3, 1000\n\n    def gmm(data):\n        mix_proportions = pyro.sample(""phi"", dist.Dirichlet(torch.ones(K)))\n        with pyro.plate(""num_clusters"", K):\n            cluster_means = pyro.sample(""cluster_means"", dist.Normal(torch.arange(float(K)), 1.))\n        with pyro.plate(""data"", data.shape[0]):\n            assignments = pyro.sample(""assignments"", dist.Categorical(mix_proportions))\n            pyro.sample(""obs"", dist.Normal(cluster_means[assignments], 1.), obs=data)\n        return cluster_means\n\n    true_cluster_means = torch.tensor([1., 5., 10.])\n    true_mix_proportions = torch.tensor([0.1, 0.3, 0.6])\n    cluster_assignments = dist.Categorical(true_mix_proportions).sample(torch.Size((N,)))\n    data = dist.Normal(true_cluster_means[cluster_assignments], 1.0).sample()\n    nuts_kernel = NUTS(gmm, max_plate_nesting=1, jit_compile=jit, ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel, num_samples=300, warmup_steps=100)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[""phi""].mean(0).sort()[0], true_mix_proportions, prec=0.05)\n    assert_equal(samples[""cluster_means""].mean(0).sort()[0], true_cluster_means, prec=0.2)\n\n\n@pytest.mark.parametrize(""jit"", [False, mark_jit(True)], ids=jit_idfn)\ndef test_bernoulli_latent_model(jit):\n    @poutine.broadcast\n    def model(data):\n        y_prob = pyro.sample(""y_prob"", dist.Beta(1., 1.))\n        with pyro.plate(""data"", data.shape[0]):\n            y = pyro.sample(""y"", dist.Bernoulli(y_prob))\n            z = pyro.sample(""z"", dist.Bernoulli(0.65 * y + 0.1))\n            pyro.sample(""obs"", dist.Normal(2. * z, 1.), obs=data)\n\n    N = 2000\n    y_prob = torch.tensor(0.3)\n    y = dist.Bernoulli(y_prob).sample(torch.Size((N,)))\n    z = dist.Bernoulli(0.65 * y + 0.1).sample()\n    data = dist.Normal(2. * z, 1.0).sample()\n    nuts_kernel = NUTS(model, max_plate_nesting=1, jit_compile=jit, ignore_jit_warnings=True)\n    mcmc = MCMC(nuts_kernel, num_samples=600, warmup_steps=200)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    assert_equal(samples[""y_prob""].mean(0), y_prob, prec=0.05)\n\n\n@pytest.mark.parametrize(""num_steps"", [2, 3, 30])\ndef test_gaussian_hmm(num_steps):\n    dim = 4\n\n    def model(data):\n        initialize = pyro.sample(""initialize"", dist.Dirichlet(torch.ones(dim)))\n        with pyro.plate(""states"", dim):\n            transition = pyro.sample(""transition"", dist.Dirichlet(torch.ones(dim, dim)))\n            emission_loc = pyro.sample(""emission_loc"", dist.Normal(torch.zeros(dim), torch.ones(dim)))\n            emission_scale = pyro.sample(""emission_scale"", dist.LogNormal(torch.zeros(dim), torch.ones(dim)))\n        x = None\n        with ignore_jit_warnings([(""Iterating over a tensor"", RuntimeWarning)]):\n            for t, y in pyro.markov(enumerate(data)):\n                x = pyro.sample(""x_{}"".format(t),\n                                dist.Categorical(initialize if x is None else transition[x]),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""y_{}"".format(t), dist.Normal(emission_loc[x], emission_scale[x]), obs=y)\n\n    def _get_initial_trace():\n        guide = AutoDelta(poutine.block(model, expose_fn=lambda msg: not msg[""name""].startswith(""x"") and\n                                        not msg[""name""].startswith(""y"")))\n        elbo = TraceEnum_ELBO(max_plate_nesting=1)\n        svi = SVI(model, guide, optim.Adam({""lr"": .01}), elbo)\n        for _ in range(100):\n            svi.step(data)\n        return poutine.trace(guide).get_trace(data)\n\n    def _generate_data():\n        transition_probs = torch.rand(dim, dim)\n        emissions_loc = torch.arange(dim, dtype=torch.Tensor().dtype)\n        emissions_scale = 1.\n        state = torch.tensor(1)\n        obs = [dist.Normal(emissions_loc[state], emissions_scale).sample()]\n        for _ in range(num_steps):\n            state = dist.Categorical(transition_probs[state]).sample()\n            obs.append(dist.Normal(emissions_loc[state], emissions_scale).sample())\n        return torch.stack(obs)\n\n    data = _generate_data()\n    nuts_kernel = NUTS(model, max_plate_nesting=1, jit_compile=True, ignore_jit_warnings=True)\n    if num_steps == 30:\n        nuts_kernel.initial_trace = _get_initial_trace()\n    mcmc = MCMC(nuts_kernel, num_samples=5, warmup_steps=5)\n    mcmc.run(data)\n\n\n@pytest.mark.parametrize(""hyperpriors"", [False, True])\ndef test_beta_binomial(hyperpriors):\n    def model(data):\n        with pyro.plate(""plate_0"", data.shape[-1]):\n            alpha = pyro.sample(""alpha"", dist.HalfCauchy(1.)) if hyperpriors else torch.tensor([1., 1.])\n            beta = pyro.sample(""beta"", dist.HalfCauchy(1.)) if hyperpriors else torch.tensor([1., 1.])\n            beta_binom = BetaBinomialPair()\n            with pyro.plate(""plate_1"", data.shape[-2]):\n                probs = pyro.sample(""probs"", beta_binom.latent(alpha, beta))\n                with pyro.plate(""data"", data.shape[0]):\n                    pyro.sample(""binomial"", beta_binom.conditional(probs=probs, total_count=total_count), obs=data)\n\n    true_probs = torch.tensor([[0.7, 0.4], [0.6, 0.4]])\n    total_count = torch.tensor([[1000, 600], [400, 800]])\n    num_samples = 80\n    data = dist.Binomial(total_count=total_count, probs=true_probs).sample(sample_shape=(torch.Size((10,))))\n    hmc_kernel = NUTS(collapse_conjugate(model), jit_compile=True, ignore_jit_warnings=True)\n    mcmc = MCMC(hmc_kernel, num_samples=num_samples, warmup_steps=50)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    posterior = posterior_replay(model, samples, data, num_samples=num_samples)\n    assert_equal(posterior[""probs""].mean(0), true_probs, prec=0.05)\n\n\n@pytest.mark.parametrize(""hyperpriors"", [False, True])\ndef test_gamma_poisson(hyperpriors):\n    def model(data):\n        with pyro.plate(""latent_dim"", data.shape[1]):\n            alpha = pyro.sample(""alpha"", dist.HalfCauchy(1.)) if hyperpriors else torch.tensor([1., 1.])\n            beta = pyro.sample(""beta"", dist.HalfCauchy(1.)) if hyperpriors else torch.tensor([1., 1.])\n            gamma_poisson = GammaPoissonPair()\n            rate = pyro.sample(""rate"", gamma_poisson.latent(alpha, beta))\n            with pyro.plate(""data"", data.shape[0]):\n                pyro.sample(""obs"", gamma_poisson.conditional(rate), obs=data)\n\n    true_rate = torch.tensor([3., 10.])\n    num_samples = 100\n    data = dist.Poisson(rate=true_rate).sample(sample_shape=(torch.Size((100,))))\n    hmc_kernel = NUTS(collapse_conjugate(model), jit_compile=True, ignore_jit_warnings=True)\n    mcmc = MCMC(hmc_kernel, num_samples=num_samples, warmup_steps=50)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    posterior = posterior_replay(model, samples, data, num_samples=num_samples)\n    assert_equal(posterior[""rate""].mean(0), true_rate, prec=0.3)\n\n\ndef test_structured_mass():\n    def model(cov):\n        w = pyro.sample(""w"", dist.Normal(0, 1000).expand([2]).to_event(1))\n        x = pyro.sample(""x"", dist.Normal(0, 1000).expand([1]).to_event(1))\n        y = pyro.sample(""y"", dist.Normal(0, 1000).expand([1]).to_event(1))\n        z = pyro.sample(""z"", dist.Normal(0, 1000).expand([1]).to_event(1))\n        wxyz = torch.cat([w, x, y, z])\n        pyro.sample(""obs"", dist.MultivariateNormal(torch.zeros(5), cov), obs=wxyz)\n\n    w_cov = torch.tensor([[1.5, 0.5], [0.5, 1.5]])\n    xy_cov = torch.tensor([[2., 1.], [1., 3.]])\n    z_var = torch.tensor([2.5])\n    cov = torch.zeros(5, 5)\n    cov[:2, :2] = w_cov\n    cov[2:4, 2:4] = xy_cov\n    cov[4, 4] = z_var\n\n    # smoke tests\n    for dense_mass in [True, False]:\n        kernel = NUTS(model, jit_compile=True, ignore_jit_warnings=True, full_mass=dense_mass)\n        mcmc = MCMC(kernel, num_samples=1, warmup_steps=1)\n        mcmc.run(cov)\n        assert kernel.inverse_mass_matrix[(""w"", ""x"", ""y"", ""z"")].dim() == 1 + int(dense_mass)\n\n    kernel = NUTS(model, jit_compile=True, ignore_jit_warnings=True, full_mass=[(""w"",), (""x"", ""y"")])\n    mcmc = MCMC(kernel, num_samples=1, warmup_steps=1000)\n    mcmc.run(cov)\n    assert_close(kernel.inverse_mass_matrix[(""w"",)], w_cov, atol=0.5, rtol=0.5)\n    assert_close(kernel.inverse_mass_matrix[(""x"", ""y"")], xy_cov, atol=0.5, rtol=0.5)\n    assert_close(kernel.inverse_mass_matrix[(""z"",)], z_var, atol=0.5, rtol=0.5)\n\n\ndef test_arrowhead_mass():\n    def model(prec):\n        w = pyro.sample(""w"", dist.Normal(0, 1000).expand([2]).to_event(1))\n        x = pyro.sample(""x"", dist.Normal(0, 1000).expand([1]).to_event(1))\n        y = pyro.sample(""y"", dist.Normal(0, 1000).expand([1]).to_event(1))\n        z = pyro.sample(""z"", dist.Normal(0, 1000).expand([2]).to_event(1))\n        wyxz = torch.cat([w, y, x, z])\n        pyro.sample(""obs"", dist.MultivariateNormal(torch.zeros(6), precision_matrix=prec), obs=wyxz)\n\n    A = torch.randn(6, 12)\n    prec = A @ A.t() * 0.1\n\n    # smoke tests\n    for dense_mass in [True, False]:\n        kernel = NUTS(model, jit_compile=True, ignore_jit_warnings=True, full_mass=dense_mass)\n        mcmc = MCMC(kernel, num_samples=1, warmup_steps=1)\n        mcmc.run(prec)\n        assert kernel.inverse_mass_matrix[(""w"", ""x"", ""y"", ""z"")].dim() == 1 + int(dense_mass)\n\n    kernel = NUTS(model, jit_compile=True, ignore_jit_warnings=True, full_mass=[(""w"",), (""y"", ""x"")])\n    kernel.mass_matrix_adapter = ArrowheadMassMatrix()\n    mcmc = MCMC(kernel, num_samples=1, warmup_steps=1000)\n    mcmc.run(prec)\n    assert (""w"", ""y"", ""x"", ""z"") in kernel.inverse_mass_matrix\n    mass_matrix = kernel.mass_matrix_adapter.mass_matrix[(""w"", ""y"", ""x"", ""z"")]\n    assert mass_matrix.top.shape == (4, 6)\n    assert mass_matrix.bottom_diag.shape == (2,)\n    assert_close(mass_matrix.top, prec[:4], atol=0.2, rtol=0.2)\n    assert_close(mass_matrix.bottom_diag, prec.diag()[4:], atol=0.2, rtol=0.2)\n\n\ndef test_dirichlet_categorical_grad_adapt():\n    def model(data):\n        concentration = torch.tensor([1.0, 1.0, 1.0])\n        p_latent = pyro.sample(\'p_latent\', dist.Dirichlet(concentration))\n        pyro.sample(""obs"", dist.Categorical(p_latent), obs=data)\n        return p_latent\n\n    true_probs = torch.tensor([0.1, 0.6, 0.3])\n    data = dist.Categorical(true_probs).sample(sample_shape=(torch.Size((2000,))))\n    nuts_kernel = NUTS(model, jit_compile=True, ignore_jit_warnings=True)\n    nuts_kernel.mass_matrix_adapter = ArrowheadMassMatrix()\n    mcmc = MCMC(nuts_kernel, num_samples=200, warmup_steps=100)\n    mcmc.run(data)\n    samples = mcmc.get_samples()\n    posterior = samples[""p_latent""]\n    assert_equal(posterior.mean(0), true_probs, prec=0.02)\n'"
tests/infer/mcmc/test_valid_models.py,47,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport io\nimport logging\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer import config_enumerate\nfrom pyro.infer.mcmc import HMC, NUTS\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.infer.mcmc.util import TraceEinsumEvaluator, TraceTreeEvaluator, initialize_model\nfrom pyro.infer.reparam import LatentStableReparam\nfrom pyro.poutine.subsample_messenger import _Subsample\nfrom tests.common import assert_close, assert_equal, xfail_param\n\nlogger = logging.getLogger(__name__)\n\n\ndef assert_ok(mcmc_kernel):\n    """"""\n    Assert that inference works without warnings or errors.\n    """"""\n    MCMC(mcmc_kernel, num_samples=10, warmup_steps=10).run()\n\n\ndef assert_error(mcmc_kernel):\n    """"""\n    Assert that inference fails with an error.\n    """"""\n    with pytest.raises(ValueError):\n        MCMC(mcmc_kernel, num_samples=10, warmup_steps=10).run()\n\n\ndef print_debug_info(model_trace):\n    model_trace.compute_log_prob()\n    for name, site in model_trace.nodes.items():\n        if site[""type""] == ""sample"" and not isinstance(site[""fn""], _Subsample):\n            logger.debug(""prob( {} ):\\n {}"".format(name, site[""log_prob""].exp()))\n\n\n@pytest.mark.parametrize(""kernel, kwargs"", [\n    (HMC, {""adapt_step_size"": True, ""num_steps"": 3}),\n    (NUTS, {""adapt_step_size"": True}),\n])\ndef test_model_error_stray_batch_dims(kernel, kwargs):\n\n    def gmm():\n        data = torch.tensor([0., 0., 3., 3., 3., 5., 5.])\n        mix_proportions = pyro.sample(""phi"", dist.Dirichlet(torch.ones(3)))\n        cluster_means = pyro.sample(""cluster_means"", dist.Normal(torch.arange(3.), 1.))\n        with pyro.plate(""data"", data.shape[0]):\n            assignments = pyro.sample(""assignments"", dist.Categorical(mix_proportions))\n            pyro.sample(""obs"", dist.Normal(cluster_means[assignments], 1.), obs=data)\n        return cluster_means\n\n    mcmc_kernel = kernel(gmm, **kwargs)\n    # Error due to non finite value for `max_plate_nesting`.\n    assert_error(mcmc_kernel)\n    # Error due to batch dims not inside plate.\n    mcmc_kernel = kernel(gmm, max_plate_nesting=1, **kwargs)\n    assert_error(mcmc_kernel)\n\n\n@pytest.mark.parametrize(""kernel, kwargs"", [\n    (HMC, {""adapt_step_size"": True, ""num_steps"": 3}),\n    (NUTS, {""adapt_step_size"": True}),\n])\ndef test_model_error_enum_dim_clash(kernel, kwargs):\n\n    def gmm():\n        data = torch.tensor([0., 0., 3., 3., 3., 5., 5.])\n        with pyro.plate(""num_clusters"", 3):\n            mix_proportions = pyro.sample(""phi"", dist.Dirichlet(torch.tensor(1.)))\n            cluster_means = pyro.sample(""cluster_means"", dist.Normal(torch.arange(3.), 1.))\n        with pyro.plate(""data"", data.shape[0]):\n            assignments = pyro.sample(""assignments"", dist.Categorical(mix_proportions))\n            pyro.sample(""obs"", dist.Normal(cluster_means[assignments], 1.), obs=data)\n        return cluster_means\n\n    mcmc_kernel = kernel(gmm, max_plate_nesting=0, **kwargs)\n    assert_error(mcmc_kernel)\n\n\ndef test_log_prob_eval_iterates_in_correct_order():\n    @poutine.enum(first_available_dim=-5)\n    @config_enumerate\n    @poutine.condition(data={""p"": torch.tensor(0.4)})\n    def model():\n        outer = pyro.plate(""outer"", 3, dim=-1)\n        inner1 = pyro.plate(""inner1"", 4, dim=-3)\n        inner2 = pyro.plate(""inner2"", 5, dim=-2)\n        inner3 = pyro.plate(""inner3"", 6, dim=-4)\n\n        p = pyro.sample(""p"", dist.Uniform(0., 1.))\n        y = pyro.sample(""y"", dist.Bernoulli(p))\n        q = 0.5 + 0.25 * y\n        with outer, inner2:\n            z0 = pyro.sample(""z0"", dist.Bernoulli(q))\n            pyro.sample(""obs0"", dist.Normal(2 * z0 - 1, 1.), obs=torch.ones(5, 3))\n        with outer:\n            v = pyro.sample(""v"", dist.Bernoulli(q))\n            r = 0.4 + 0.1 * v\n            with inner1, inner3:\n                z1 = pyro.sample(""z1"", dist.Bernoulli(r))\n                pyro.sample(""obs1"", dist.Normal(2 * z1 - 1, 1.), obs=torch.ones(6, 4, 1, 3))\n            with inner2:\n                z2 = pyro.sample(""z2"", dist.Bernoulli(r))\n                pyro.sample(""obs2"", dist.Normal(2 * z2 - 1, 1.), obs=torch.ones(5, 3))\n\n    model_trace = poutine.trace(model).get_trace()\n    trace_prob_evaluator = TraceTreeEvaluator(model_trace, True, 4)\n    trace_prob_evaluator.log_prob(model_trace)\n    plate_dims, enum_dims = [], []\n    for key in reversed(sorted(trace_prob_evaluator._log_probs.keys(), key=lambda x: (len(x), x))):\n        plate_dims.append(trace_prob_evaluator._plate_dims[key])\n        enum_dims.append(trace_prob_evaluator._enum_dims[key])\n    # The reduction operation returns a singleton with dimensions preserved.\n    assert not any(i != 1 for i in trace_prob_evaluator._aggregate_log_probs(frozenset()).shape)\n    assert plate_dims == [[-4, -3], [-2], [-1], []]\n    assert enum_dims, [[-8], [-9, -6], [-7], [-5]]\n\n\n@pytest.mark.parametrize(""Eval"", [TraceTreeEvaluator, TraceEinsumEvaluator])\ndef test_all_discrete_sites_log_prob(Eval):\n    p = 0.3\n\n    @poutine.enum(first_available_dim=-4)\n    @config_enumerate\n    def model():\n        d = dist.Bernoulli(p)\n        context1 = pyro.plate(""outer"", 2, dim=-1)\n        context2 = pyro.plate(""inner1"", 1, dim=-2)\n        context3 = pyro.plate(""inner2"", 1, dim=-3)\n        pyro.sample(""w"", d)\n        with context1:\n            pyro.sample(""x"", d)\n        with context1, context2:\n            pyro.sample(""y"", d)\n        with context1, context3:\n            pyro.sample(""z"", d)\n\n    model_trace = poutine.trace(model).get_trace()\n    print_debug_info(model_trace)\n    trace_prob_evaluator = Eval(model_trace, True, 3)\n    # all discrete sites enumerated out.\n    assert_equal(trace_prob_evaluator.log_prob(model_trace), torch.tensor(0.))\n\n\n@pytest.mark.parametrize(""Eval"", [TraceTreeEvaluator,\n                                  xfail_param(TraceEinsumEvaluator, reason=""TODO: Debug this failure case."")])\ndef test_enumeration_in_tree(Eval):\n    @poutine.enum(first_available_dim=-5)\n    @config_enumerate\n    @poutine.condition(data={""sample1"": torch.tensor(0.),\n                             ""sample2"": torch.tensor(1.),\n                             ""sample3"": torch.tensor(2.)})\n    def model():\n        outer = pyro.plate(""outer"", 2, dim=-1)\n        inner1 = pyro.plate(""inner1"", 2, dim=-3)\n        inner2 = pyro.plate(""inner2"", 3, dim=-2)\n        inner3 = pyro.plate(""inner3"", 2, dim=-4)\n\n        d = dist.Bernoulli(0.3)\n        n = dist.Normal(0., 1.)\n        pyro.sample(""y"", d)\n        pyro.sample(""sample1"", n)\n        with outer, inner2:\n            pyro.sample(""z0"", d)\n            pyro.sample(""sample2"", n)\n        with outer:\n            pyro.sample(""z1"", d)\n            pyro.sample(""sample3"", n)\n            with inner1, inner3:\n                pyro.sample(""z2"", d)\n            with inner2:\n                pyro.sample(""z3"", d)\n\n    model_trace = poutine.trace(model).get_trace()\n    print_debug_info(model_trace)\n    trace_prob_evaluator = Eval(model_trace, True, 4)\n    # p_n(0.) * p_n(2.)^2 * p_n(1.)^6\n    assert_equal(trace_prob_evaluator.log_prob(model_trace), torch.tensor(-15.2704), prec=1e-4)\n\n\n@pytest.mark.xfail(reason=""Enumeration currently does not work for general DAGs"")\n@pytest.mark.parametrize(""Eval"", [TraceTreeEvaluator, TraceEinsumEvaluator])\ndef test_enumeration_in_dag(Eval):\n    p = 0.3\n\n    @poutine.enum(first_available_dim=-3)\n    @config_enumerate\n    @poutine.condition(data={""b"": torch.tensor(0.4), ""c"": torch.tensor(0.4)})\n    def model():\n        d = dist.Bernoulli(p)\n        context1 = pyro.plate(""outer"", 3, dim=-1)\n        context2 = pyro.plate(""inner"", 2, dim=-2)\n        pyro.sample(""w"", d)\n        pyro.sample(""b"", dist.Beta(1.1, 1.1))\n        with context1:\n            pyro.sample(""x"", d)\n        with context2:\n            pyro.sample(""c"", dist.Beta(1.1, 1.1))\n            pyro.sample(""y"", d)\n        with context1, context2:\n            pyro.sample(""z"", d)\n\n    model_trace = poutine.trace(model).get_trace()\n    print_debug_info(model_trace)\n    trace_prob_evaluator = Eval(model_trace, True, 2)\n    assert_equal(trace_prob_evaluator.log_prob(model_trace), torch.tensor(0.16196))  # p_beta(0.3)^3\n\n\n@pytest.mark.parametrize(""data, expected_log_prob"", [\n    (torch.tensor([1.]), torch.tensor(-1.3434)),\n    (torch.tensor([0.]), torch.tensor(-1.4189)),\n    (torch.tensor([1., 0., 0.]), torch.tensor(-4.1813)),\n])\n@pytest.mark.parametrize(""Eval"", [TraceTreeEvaluator, TraceEinsumEvaluator])\ndef test_enum_log_prob_continuous_observed(data, expected_log_prob, Eval):\n\n    @poutine.enum(first_available_dim=-2)\n    @config_enumerate\n    @poutine.condition(data={""p"": torch.tensor(0.4)})\n    def model(data):\n        p = pyro.sample(""p"", dist.Uniform(0., 1.))\n        y = pyro.sample(""y"", dist.Bernoulli(p))\n        q = 0.5 + 0.25 * y\n        with pyro.plate(""data"", len(data)):\n            z = pyro.sample(""z"", dist.Bernoulli(q))\n            mean = 2 * z - 1\n            pyro.sample(""obs"", dist.Normal(mean, 1.), obs=data)\n\n    model_trace = poutine.trace(model).get_trace(data)\n    print_debug_info(model_trace)\n    trace_prob_evaluator = Eval(model_trace, True, 1)\n    assert_equal(trace_prob_evaluator.log_prob(model_trace),\n                 expected_log_prob,\n                 prec=1e-3)\n\n\n@pytest.mark.parametrize(""data, expected_log_prob"", [\n    (torch.tensor([1.]), torch.tensor(-3.5237)),\n    (torch.tensor([0.]), torch.tensor(-3.7091)),\n    (torch.tensor([1., 1.]), torch.tensor(-3.9699)),\n    (torch.tensor([1., 0., 0.]), torch.tensor(-5.3357)),\n])\n@pytest.mark.parametrize(""Eval"", [TraceTreeEvaluator, TraceEinsumEvaluator])\ndef test_enum_log_prob_continuous_sampled(data, expected_log_prob, Eval):\n\n    @poutine.enum(first_available_dim=-2)\n    @config_enumerate\n    @poutine.condition(data={""p"": torch.tensor(0.4),\n                             ""n"": torch.tensor([[1.], [-1.]])})\n    def model(data):\n        p = pyro.sample(""p"", dist.Uniform(0., 1.))\n        y = pyro.sample(""y"", dist.Bernoulli(p))\n        mean = 2 * y - 1\n        n = pyro.sample(""n"", dist.Normal(mean, 1.))\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", dist.Bernoulli(torch.sigmoid(n)), obs=data)\n\n    model_trace = poutine.trace(model).get_trace(data)\n    print_debug_info(model_trace)\n    trace_prob_evaluator = Eval(model_trace, True, 1)\n    assert_equal(trace_prob_evaluator.log_prob(model_trace),\n                 expected_log_prob,\n                 prec=1e-3)\n\n\n@pytest.mark.parametrize(""data, expected_log_prob"", [\n    (torch.tensor([1.]), torch.tensor(-0.5108)),\n    (torch.tensor([1., 1.]), torch.tensor(-0.9808)),\n    (torch.tensor([1., 0., 0.]), torch.tensor(-2.3671)),\n])\n@pytest.mark.parametrize(""Eval"", [TraceTreeEvaluator, TraceEinsumEvaluator])\ndef test_enum_log_prob_discrete_observed(data, expected_log_prob, Eval):\n\n    @poutine.enum(first_available_dim=-2)\n    @config_enumerate\n    @poutine.condition(data={""p"": torch.tensor(0.4)})\n    def model(data):\n        p = pyro.sample(""p"", dist.Uniform(0., 1.))\n        y = pyro.sample(""y"", dist.Bernoulli(p))\n        q = 0.25 * y + 0.5\n        with pyro.plate(""data"", len(data)):\n            pyro.sample(""obs"", dist.Bernoulli(q), obs=data)\n\n    model_trace = poutine.trace(model).get_trace(data)\n    print_debug_info(model_trace)\n    trace_prob_evaluator = Eval(model_trace, True, 1)\n    assert_equal(trace_prob_evaluator.log_prob(model_trace),\n                 expected_log_prob,\n                 prec=1e-3)\n\n\n@pytest.mark.parametrize(""data, expected_log_prob"", [\n    (torch.tensor([1.]), torch.tensor(-1.15)),\n    (torch.tensor([0.]), torch.tensor(-1.46)),\n    (torch.tensor([1., 1.]), torch.tensor(-2.1998)),\n])\n@pytest.mark.parametrize(""Eval"", [TraceTreeEvaluator, TraceEinsumEvaluator])\ndef test_enum_log_prob_multiple_plate(data, expected_log_prob, Eval):\n\n    @poutine.enum(first_available_dim=-2)\n    @config_enumerate\n    @poutine.condition(data={""p"": torch.tensor(0.4)})\n    def model(data):\n        p = pyro.sample(""p"", dist.Beta(1.1, 1.1))\n        y = pyro.sample(""y"", dist.Bernoulli(p))\n        q = 0.5 + 0.25 * y\n        r = 0.4 + 0.2 * y\n        with pyro.plate(""data1"", len(data)):\n            pyro.sample(""obs1"", dist.Bernoulli(q), obs=data)\n        with pyro.plate(""data2"", len(data)):\n            pyro.sample(""obs2"", dist.Bernoulli(r), obs=data)\n\n    model_trace = poutine.trace(model).get_trace(data)\n    print_debug_info(model_trace)\n    trace_prob_evaluator = Eval(model_trace, True, 1)\n    assert_equal(trace_prob_evaluator.log_prob(model_trace),\n                 expected_log_prob,\n                 prec=1e-3)\n\n\n@pytest.mark.parametrize(""data, expected_log_prob"", [\n    (torch.tensor([1.]), torch.tensor(-1.5478)),\n    (torch.tensor([0.]), torch.tensor(-1.4189)),\n    (torch.tensor([1., 0., 0.]), torch.tensor(-4.3857)),\n])\n@pytest.mark.parametrize(""Eval"", [TraceTreeEvaluator, TraceEinsumEvaluator])\ndef test_enum_log_prob_nested_plate(data, expected_log_prob, Eval):\n\n    @poutine.enum(first_available_dim=-3)\n    @config_enumerate\n    @poutine.condition(data={""p"": torch.tensor(0.4)})\n    def model(data):\n        p = pyro.sample(""p"", dist.Uniform(0., 1.))\n        y = pyro.sample(""y"", dist.Bernoulli(p))\n        q = 0.5 + 0.25 * y\n        with pyro.plate(""intermediate"", 1, dim=-2):\n            v = pyro.sample(""v"", dist.Bernoulli(q))\n            with pyro.plate(""data"", len(data), dim=-1):\n                r = 0.4 + 0.1 * v\n                z = pyro.sample(""z"", dist.Bernoulli(r))\n                pyro.sample(""obs"", dist.Normal(2 * z - 1, 1.), obs=data)\n\n    model_trace = poutine.trace(model).get_trace(data)\n    print_debug_info(model_trace)\n    trace_prob_evaluator = Eval(model_trace, True, 2)\n    assert_equal(trace_prob_evaluator.log_prob(model_trace),\n                 expected_log_prob,\n                 prec=1e-3)\n\n\ndef _beta_bernoulli(data):\n    alpha = torch.tensor([1.1, 1.1])\n    beta = torch.tensor([1.1, 1.1])\n    p_latent = pyro.sample(\'p_latent\', dist.Beta(alpha, beta))\n    with pyro.plate(\'data\', data.shape[0], dim=-2):\n        pyro.sample(\'obs\', dist.Bernoulli(p_latent), obs=data)\n    return p_latent\n\n\n@pytest.mark.parametrize(\'jit\', [False, True])\ndef test_potential_fn_pickling(jit):\n    data = dist.Bernoulli(torch.tensor([0.8, 0.2])).sample(sample_shape=(torch.Size((1000,))))\n    _, potential_fn, _, _ = initialize_model(_beta_bernoulli, (data,), jit_compile=jit,\n                                             skip_jit_warnings=True)\n    test_data = {\'p_latent\': torch.tensor([0.2, 0.6])}\n    buffer = io.BytesIO()\n    torch.save(potential_fn, buffer)\n    buffer.seek(0)\n    deser_potential_fn = torch.load(buffer)\n    assert_close(deser_potential_fn(test_data), potential_fn(test_data))\n\n\n@pytest.mark.parametrize(""kernel, kwargs"", [\n    (HMC, {""adapt_step_size"": True, ""num_steps"": 3}),\n    (NUTS, {""adapt_step_size"": True}),\n])\ndef test_reparam_stable(kernel, kwargs):\n\n    @poutine.reparam(config={""z"": LatentStableReparam()})\n    def model():\n        stability = pyro.sample(""stability"", dist.Uniform(0., 2.))\n        skew = pyro.sample(""skew"", dist.Uniform(-1., 1.))\n        y = pyro.sample(""z"", dist.Stable(stability, skew))\n        pyro.sample(""x"", dist.Poisson(y.abs()), obs=torch.tensor(1.))\n\n    mcmc_kernel = kernel(model, max_plate_nesting=0, **kwargs)\n    assert_ok(mcmc_kernel)\n'"
tests/infer/reparam/__init__.py,0,b''
tests/infer/reparam/test_conjugate.py,19,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.infer import Predictive, Trace_ELBO\nfrom pyro.infer.autoguide import AutoDiagonalNormal\nfrom pyro.infer.mcmc.api import MCMC\nfrom pyro.infer.mcmc.hmc import HMC\nfrom pyro.infer.reparam import ConjugateReparam, LinearHMMReparam, StableReparam\nfrom tests.common import assert_close\nfrom tests.ops.gaussian import random_mvn\n\n\ndef test_beta_binomial_static_sample():\n    total = 10\n    counts = dist.Binomial(total, 0.3).sample()\n    concentration1 = torch.tensor(0.5)\n    concentration0 = torch.tensor(1.5)\n\n    prior = dist.Beta(concentration1, concentration0)\n    likelihood = dist.Beta(1 + counts, 1 + total - counts)\n    posterior = dist.Beta(concentration1 + counts, concentration0 + total - counts)\n\n    def model():\n        prob = pyro.sample(""prob"", prior)\n        pyro.sample(""counts"", dist.Binomial(total, prob), obs=counts)\n\n    reparam_model = poutine.reparam(model, {""prob"": ConjugateReparam(likelihood)})\n\n    with poutine.trace() as tr, pyro.plate(""particles"", 10000):\n        reparam_model()\n    samples = tr.trace.nodes[""prob""][""value""]\n\n    assert_close(samples.mean(), posterior.mean, atol=0.01)\n    assert_close(samples.std(), posterior.variance.sqrt(), atol=0.01)\n\n\ndef test_beta_binomial_dependent_sample():\n    total = 10\n    counts = dist.Binomial(total, 0.3).sample()\n    concentration1 = torch.tensor(0.5)\n    concentration0 = torch.tensor(1.5)\n\n    prior = dist.Beta(concentration1, concentration0)\n    posterior = dist.Beta(concentration1 + counts, concentration0 + total - counts)\n\n    def model(counts):\n        prob = pyro.sample(""prob"", prior)\n        pyro.sample(""counts"", dist.Binomial(total, prob), obs=counts)\n\n    reparam_model = poutine.reparam(model, {\n        ""prob"": ConjugateReparam(lambda counts: dist.Beta(1 + counts, 1 + total - counts)),\n    })\n\n    with poutine.trace() as tr, pyro.plate(""particles"", 10000):\n        reparam_model(counts)\n    samples = tr.trace.nodes[""prob""][""value""]\n\n    assert_close(samples.mean(), posterior.mean, atol=0.01)\n    assert_close(samples.std(), posterior.variance.sqrt(), atol=0.01)\n\n\ndef test_beta_binomial_elbo():\n    total = 10\n    counts = dist.Binomial(total, 0.3).sample()\n    concentration1 = torch.tensor(0.5, requires_grad=True)\n    concentration0 = torch.tensor(1.5, requires_grad=True)\n\n    prior = dist.Beta(concentration1, concentration0)\n    likelihood = dist.Beta(1 + counts, 1 + total - counts)\n    posterior = dist.Beta(concentration1 + counts, concentration0 + total - counts)\n\n    def model():\n        prob = pyro.sample(""prob"", prior)\n        pyro.sample(""counts"", dist.Binomial(total, prob), obs=counts)\n\n    def guide():\n        pyro.sample(""prob"", posterior)\n\n    reparam_model = poutine.reparam(model, {""prob"": ConjugateReparam(likelihood)})\n\n    def reparam_guide():\n        pass\n\n    elbo = Trace_ELBO(num_particles=10000, vectorize_particles=True, max_plate_nesting=0)\n    expected_loss = elbo.differentiable_loss(model, guide)\n    actual_loss = elbo.differentiable_loss(reparam_model, reparam_guide)\n    assert_close(actual_loss, expected_loss, atol=0.01)\n\n    params = [concentration1, concentration0]\n    expected_grads = torch.autograd.grad(expected_loss, params, retain_graph=True)\n    actual_grads = torch.autograd.grad(actual_loss, params, retain_graph=True)\n    for a, e in zip(actual_grads, expected_grads):\n        assert_close(a, e, rtol=0.01)\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""hidden_dim,obs_dim"", [(1, 1), (3, 2)], ids=str)\n@pytest.mark.parametrize(""num_steps"", range(1, 6))\ndef test_gaussian_hmm_elbo(batch_shape, num_steps, hidden_dim, obs_dim):\n    init_dist = random_mvn(batch_shape, hidden_dim)\n    trans_mat = torch.randn(batch_shape + (num_steps, hidden_dim, hidden_dim), requires_grad=True)\n    trans_dist = random_mvn(batch_shape + (num_steps,), hidden_dim)\n    obs_mat = torch.randn(batch_shape + (num_steps, hidden_dim, obs_dim), requires_grad=True)\n    obs_dist = random_mvn(batch_shape + (num_steps,), obs_dim)\n\n    data = obs_dist.sample()\n    assert data.shape == batch_shape + (num_steps, obs_dim)\n    prior = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n    likelihood = dist.Normal(data, 1).to_event(2)\n    posterior, log_normalizer = prior.conjugate_update(likelihood)\n\n    def model(data):\n        with pyro.plate_stack(""plates"", batch_shape):\n            z = pyro.sample(""z"", prior)\n            pyro.sample(""x"", dist.Normal(z, 1).to_event(2), obs=data)\n\n    def guide(data):\n        with pyro.plate_stack(""plates"", batch_shape):\n            pyro.sample(""z"", posterior)\n\n    reparam_model = poutine.reparam(model, {""z"": ConjugateReparam(likelihood)})\n\n    def reparam_guide(data):\n        pass\n\n    elbo = Trace_ELBO(num_particles=1000, vectorize_particles=True)\n    expected_loss = elbo.differentiable_loss(model, guide, data)\n    actual_loss = elbo.differentiable_loss(reparam_model, reparam_guide, data)\n    assert_close(actual_loss, expected_loss, atol=0.01)\n\n    params = [trans_mat, obs_mat]\n    expected_grads = torch.autograd.grad(expected_loss, params, retain_graph=True)\n    actual_grads = torch.autograd.grad(actual_loss, params, retain_graph=True)\n    for a, e in zip(actual_grads, expected_grads):\n        assert_close(a, e, rtol=0.01)\n\n\ndef random_stable(shape):\n    stability = dist.Uniform(1.4, 1.9).sample(shape)\n    skew = dist.Uniform(-1, 1).sample(shape)\n    scale = torch.rand(shape).exp()\n    loc = torch.randn(shape)\n    return dist.Stable(stability, skew, scale, loc)\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""hidden_dim,obs_dim"", [(1, 1), (2, 3)], ids=str)\n@pytest.mark.parametrize(""num_steps"", range(1, 6))\ndef test_stable_hmm_smoke(batch_shape, num_steps, hidden_dim, obs_dim):\n    init_dist = random_stable(batch_shape + (hidden_dim,)).to_event(1)\n    trans_mat = torch.randn(batch_shape + (num_steps, hidden_dim, hidden_dim), requires_grad=True)\n    trans_dist = random_stable(batch_shape + (num_steps, hidden_dim)).to_event(1)\n    obs_mat = torch.randn(batch_shape + (num_steps, hidden_dim, obs_dim), requires_grad=True)\n    obs_dist = random_stable(batch_shape + (num_steps, obs_dim)).to_event(1)\n    data = obs_dist.sample()\n    assert data.shape == batch_shape + (num_steps, obs_dim)\n\n    def model(data):\n        hmm = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist, duration=num_steps)\n        with pyro.plate_stack(""plates"", batch_shape):\n            z = pyro.sample(""z"", hmm)\n            pyro.sample(""x"", dist.Normal(z, 1).to_event(2), obs=data)\n\n    # Test that we can combine these two reparameterizers.\n    reparam_model = poutine.reparam(model, {\n        ""z"": LinearHMMReparam(StableReparam(), StableReparam(), StableReparam()),\n    })\n    reparam_model = poutine.reparam(reparam_model, {\n        ""z"": ConjugateReparam(dist.Normal(data, 1).to_event(2)),\n    })\n    reparam_guide = AutoDiagonalNormal(reparam_model)  # Models auxiliary variables.\n\n    # Smoke test only.\n    elbo = Trace_ELBO(num_particles=5, vectorize_particles=True)\n    loss = elbo.differentiable_loss(reparam_model, reparam_guide, data)\n    params = [trans_mat, obs_mat]\n    torch.autograd.grad(loss, params, retain_graph=True)\n\n\ndef test_beta_binomial_hmc():\n    num_samples = 1000\n    total = 10\n    counts = dist.Binomial(total, 0.3).sample()\n    concentration1 = torch.tensor(0.5)\n    concentration0 = torch.tensor(1.5)\n\n    prior = dist.Beta(concentration1, concentration0)\n    likelihood = dist.Beta(1 + counts, 1 + total - counts)\n    posterior = dist.Beta(concentration1 + counts, concentration0 + total - counts)\n\n    def model():\n        prob = pyro.sample(""prob"", prior)\n        pyro.sample(""counts"", dist.Binomial(total, prob), obs=counts)\n\n    reparam_model = poutine.reparam(model, {""prob"": ConjugateReparam(likelihood)})\n\n    kernel = HMC(reparam_model)\n    samples = MCMC(kernel, num_samples, warmup_steps=0).run()\n    pred = Predictive(reparam_model, samples, num_samples=num_samples)\n    trace = pred.get_vectorized_trace()\n    samples = trace.nodes[""prob""][""value""]\n\n    assert_close(samples.mean(), posterior.mean, atol=0.01)\n    assert_close(samples.std(), posterior.variance.sqrt(), atol=0.01)\n'"
tests/infer/reparam/test_discrete_cosine.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.reparam import DiscreteCosineReparam\nfrom tests.common import assert_close\n\n\n# Test helper to extract central moments from samples.\ndef get_moments(x):\n    n = x.size(0)\n    x = x.reshape(n, -1)\n    mean = x.mean(0)\n    x = x - mean\n    std = (x * x).mean(0).sqrt()\n    x = x / std\n    corr = (x.unsqueeze(-1) * x.unsqueeze(-2)).mean(0).reshape(-1)\n    return torch.cat([mean, std, corr])\n\n\n@pytest.mark.parametrize(""smooth"", [0., 0.5, 1.0, 2.0])\n@pytest.mark.parametrize(""shape,dim"", [\n    ((6,), -1),\n    ((2, 5,), -1),\n    ((4, 2), -2),\n    ((2, 3, 1), -2),\n], ids=str)\ndef test_normal(shape, dim, smooth):\n    loc = torch.empty(shape).uniform_(-1., 1.).requires_grad_()\n    scale = torch.empty(shape).uniform_(0.5, 1.5).requires_grad_()\n\n    def model():\n        with pyro.plate_stack(""plates"", shape[:dim]):\n            with pyro.plate(""particles"", 10000):\n                pyro.sample(""x"", dist.Normal(loc, scale).expand(shape).to_event(-dim))\n\n    value = poutine.trace(model).get_trace().nodes[""x""][""value""]\n    expected_probe = get_moments(value)\n\n    rep = DiscreteCosineReparam(dim=dim, smooth=smooth)\n    reparam_model = poutine.reparam(model, {""x"": rep})\n    trace = poutine.trace(reparam_model).get_trace()\n    assert isinstance(trace.nodes[""x_dct""][""fn""], dist.TransformedDistribution)\n    assert isinstance(trace.nodes[""x""][""fn""], dist.Delta)\n    value = trace.nodes[""x""][""value""]\n    actual_probe = get_moments(value)\n    assert_close(actual_probe, expected_probe, atol=0.1)\n\n    for actual_m, expected_m in zip(actual_probe[:10], expected_probe[:10]):\n        expected_grads = grad(expected_m.sum(), [loc, scale], retain_graph=True)\n        actual_grads = grad(actual_m.sum(), [loc, scale], retain_graph=True)\n        assert_close(actual_grads[0], expected_grads[0], atol=0.05)\n        assert_close(actual_grads[1], expected_grads[1], atol=0.05)\n\n\n@pytest.mark.parametrize(""smooth"", [0., 0.5, 1.0, 2.0])\n@pytest.mark.parametrize(""shape,dim"", [\n    ((6,), -1),\n    ((2, 5,), -1),\n    ((4, 2), -2),\n    ((2, 3, 1), -2),\n], ids=str)\ndef test_uniform(shape, dim, smooth):\n\n    def model():\n        with pyro.plate_stack(""plates"", shape[:dim]):\n            with pyro.plate(""particles"", 10000):\n                pyro.sample(""x"", dist.Uniform(0, 1).expand(shape).to_event(-dim))\n\n    value = poutine.trace(model).get_trace().nodes[""x""][""value""]\n    expected_probe = get_moments(value)\n\n    reparam_model = poutine.reparam(model, {""x"": DiscreteCosineReparam(dim=dim, smooth=smooth)})\n    trace = poutine.trace(reparam_model).get_trace()\n    assert isinstance(trace.nodes[""x_dct""][""fn""], dist.TransformedDistribution)\n    assert isinstance(trace.nodes[""x""][""fn""], dist.Delta)\n    value = trace.nodes[""x""][""value""]\n    actual_probe = get_moments(value)\n    assert_close(actual_probe, expected_probe, atol=0.1)\n'"
tests/infer/reparam/test_haar.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.reparam import HaarReparam\nfrom tests.common import assert_close\n\n\n# Test helper to extract central moments from samples.\ndef get_moments(x):\n    n = x.size(0)\n    x = x.reshape(n, -1)\n    mean = x.mean(0)\n    x = x - mean\n    std = (x * x).mean(0).sqrt()\n    x = x / std\n    corr = (x.unsqueeze(-1) * x.unsqueeze(-2)).mean(0).reshape(-1)\n    return torch.cat([mean, std, corr])\n\n\n@pytest.mark.parametrize(""flip"", [False, True])\n@pytest.mark.parametrize(""shape,dim"", [\n    ((6,), -1),\n    ((2, 5,), -1),\n    ((4, 2), -2),\n    ((2, 3, 1), -2),\n], ids=str)\ndef test_normal(shape, dim, flip):\n    loc = torch.empty(shape).uniform_(-1., 1.).requires_grad_()\n    scale = torch.empty(shape).uniform_(0.5, 1.5).requires_grad_()\n\n    def model():\n        with pyro.plate_stack(""plates"", shape[:dim]):\n            with pyro.plate(""particles"", 10000):\n                pyro.sample(""x"", dist.Normal(loc, scale).expand(shape).to_event(-dim))\n\n    value = poutine.trace(model).get_trace().nodes[""x""][""value""]\n    expected_probe = get_moments(value)\n\n    rep = HaarReparam(dim=dim, flip=flip)\n    reparam_model = poutine.reparam(model, {""x"": rep})\n    trace = poutine.trace(reparam_model).get_trace()\n    assert isinstance(trace.nodes[""x_haar""][""fn""], dist.TransformedDistribution)\n    assert isinstance(trace.nodes[""x""][""fn""], dist.Delta)\n    value = trace.nodes[""x""][""value""]\n    actual_probe = get_moments(value)\n    assert_close(actual_probe, expected_probe, atol=0.1)\n\n    for actual_m, expected_m in zip(actual_probe[:10], expected_probe[:10]):\n        expected_grads = grad(expected_m.sum(), [loc, scale], retain_graph=True)\n        actual_grads = grad(actual_m.sum(), [loc, scale], retain_graph=True)\n        assert_close(actual_grads[0], expected_grads[0], atol=0.05)\n        assert_close(actual_grads[1], expected_grads[1], atol=0.05)\n\n\n@pytest.mark.parametrize(""flip"", [False, True])\n@pytest.mark.parametrize(""shape,dim"", [\n    ((6,), -1),\n    ((2, 5,), -1),\n    ((4, 2), -2),\n    ((2, 3, 1), -2),\n], ids=str)\ndef test_uniform(shape, dim, flip):\n\n    def model():\n        with pyro.plate_stack(""plates"", shape[:dim]):\n            with pyro.plate(""particles"", 10000):\n                pyro.sample(""x"", dist.Uniform(0, 1).expand(shape).to_event(-dim))\n\n    value = poutine.trace(model).get_trace().nodes[""x""][""value""]\n    expected_probe = get_moments(value)\n\n    reparam_model = poutine.reparam(model, {""x"": HaarReparam(dim=dim, flip=flip)})\n    trace = poutine.trace(reparam_model).get_trace()\n    assert isinstance(trace.nodes[""x_haar""][""fn""], dist.TransformedDistribution)\n    assert isinstance(trace.nodes[""x""][""fn""], dist.Delta)\n    value = trace.nodes[""x""][""value""]\n    actual_probe = get_moments(value)\n    assert_close(actual_probe, expected_probe, atol=0.1)\n'"
tests/infer/reparam/test_hmm.py,22,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.reparam import LinearHMMReparam, StableReparam, StudentTReparam, SymmetricStableReparam\nfrom tests.common import assert_close\nfrom tests.ops.gaussian import random_mvn\n\n\ndef random_studentt(shape):\n    df = torch.rand(shape).exp()\n    loc = torch.randn(shape)\n    scale = torch.rand(shape).exp()\n    return dist.StudentT(df, loc, scale)\n\n\ndef random_stable(shape, stability, skew=None):\n    if skew is None:\n        skew = dist.Uniform(-1, 1).sample(shape)\n    scale = torch.rand(shape).exp()\n    loc = torch.randn(shape)\n    return dist.Stable(stability, skew, scale, loc)\n\n\n@pytest.mark.parametrize(""duration"", [1, 2, 3, 4, 5, 6])\n@pytest.mark.parametrize(""obs_dim"", [1, 2])\n@pytest.mark.parametrize(""hidden_dim"", [1, 3])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (2, 3)], ids=str)\ndef test_transformed_hmm_shape(batch_shape, duration, hidden_dim, obs_dim):\n    init_dist = random_mvn(batch_shape, hidden_dim)\n    trans_mat = torch.randn(batch_shape + (duration, hidden_dim, hidden_dim))\n    trans_dist = random_mvn(batch_shape + (duration,), hidden_dim)\n    obs_mat = torch.randn(batch_shape + (duration, hidden_dim, obs_dim))\n    obs_dist = dist.LogNormal(torch.randn(batch_shape + (duration, obs_dim)),\n                              torch.rand(batch_shape + (duration, obs_dim)).exp()).to_event(1)\n    hmm = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist, duration=duration)\n\n    def model(data=None):\n        with pyro.plate_stack(""plates"", batch_shape):\n            return pyro.sample(""x"", hmm, obs=data)\n\n    data = model()\n    with poutine.trace() as tr:\n        with poutine.reparam(config={""x"": LinearHMMReparam()}):\n            model(data)\n    fn = tr.trace.nodes[""x""][""fn""]\n    assert isinstance(fn, dist.TransformedDistribution)\n    assert isinstance(fn.base_dist, dist.GaussianHMM)\n    tr.trace.compute_log_prob()  # smoke test only\n\n\n@pytest.mark.parametrize(""duration"", [1, 2, 3, 4, 5, 6])\n@pytest.mark.parametrize(""obs_dim"", [1, 2])\n@pytest.mark.parametrize(""hidden_dim"", [1, 3])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (2, 3)], ids=str)\ndef test_studentt_hmm_shape(batch_shape, duration, hidden_dim, obs_dim):\n    init_dist = random_studentt(batch_shape + (hidden_dim,)).to_event(1)\n    trans_mat = torch.randn(batch_shape + (duration, hidden_dim, hidden_dim))\n    trans_dist = random_studentt(batch_shape + (duration, hidden_dim)).to_event(1)\n    obs_mat = torch.randn(batch_shape + (duration, hidden_dim, obs_dim))\n    obs_dist = random_studentt(batch_shape + (duration, obs_dim)).to_event(1)\n    hmm = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist, duration=duration)\n\n    def model(data=None):\n        with pyro.plate_stack(""plates"", batch_shape):\n            return pyro.sample(""x"", hmm, obs=data)\n\n    data = model()\n    rep = StudentTReparam()\n    with poutine.trace() as tr:\n        with poutine.reparam(config={""x"": LinearHMMReparam(rep, rep, rep)}):\n            model(data)\n\n    assert isinstance(tr.trace.nodes[""x""][""fn""], dist.GaussianHMM)\n    assert tr.trace.nodes[""x_init_gamma""][""fn""].event_shape == (hidden_dim,)\n    assert tr.trace.nodes[""x_trans_gamma""][""fn""].event_shape == (duration, hidden_dim)\n    assert tr.trace.nodes[""x_obs_gamma""][""fn""].event_shape == (duration, obs_dim)\n    tr.trace.compute_log_prob()  # smoke test only\n\n\n@pytest.mark.parametrize(""duration"", [1, 2, 3, 4, 5, 6])\n@pytest.mark.parametrize(""obs_dim"", [1, 2])\n@pytest.mark.parametrize(""hidden_dim"", [1, 3])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (2, 3)], ids=str)\n@pytest.mark.parametrize(""skew"", [0, None], ids=[""symmetric"", ""skewed""])\ndef test_stable_hmm_shape(skew, batch_shape, duration, hidden_dim, obs_dim):\n    stability = dist.Uniform(0.5, 2).sample(batch_shape)\n    init_dist = random_stable(batch_shape + (hidden_dim,),\n                              stability.unsqueeze(-1), skew=skew).to_event(1)\n    trans_mat = torch.randn(batch_shape + (duration, hidden_dim, hidden_dim))\n    trans_dist = random_stable(batch_shape + (duration, hidden_dim),\n                               stability.unsqueeze(-1).unsqueeze(-1), skew=skew).to_event(1)\n    obs_mat = torch.randn(batch_shape + (duration, hidden_dim, obs_dim))\n    obs_dist = random_stable(batch_shape + (duration, obs_dim),\n                             stability.unsqueeze(-1).unsqueeze(-1), skew=skew).to_event(1)\n    hmm = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist, duration=duration)\n    assert hmm.batch_shape == batch_shape\n    assert hmm.event_shape == (duration, obs_dim)\n\n    def model(data=None):\n        with pyro.plate_stack(""plates"", batch_shape):\n            return pyro.sample(""x"", hmm, obs=data)\n\n    data = torch.randn(duration, obs_dim)\n    rep = SymmetricStableReparam() if skew == 0 else StableReparam()\n    with poutine.trace() as tr:\n        with poutine.reparam(config={""x"": LinearHMMReparam(rep, rep, rep)}):\n            model(data)\n    assert isinstance(tr.trace.nodes[""x""][""fn""], dist.GaussianHMM)\n    tr.trace.compute_log_prob()  # smoke test only\n\n\n@pytest.mark.parametrize(""duration"", [1, 2, 3, 4, 5, 6])\n@pytest.mark.parametrize(""obs_dim"", [1, 2])\n@pytest.mark.parametrize(""hidden_dim"", [1, 3])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (2, 3)], ids=str)\n@pytest.mark.parametrize(""skew"", [0, None], ids=[""symmetric"", ""skewed""])\ndef test_independent_hmm_shape(skew, batch_shape, duration, hidden_dim, obs_dim):\n    base_batch_shape = batch_shape + (obs_dim,)\n    stability = dist.Uniform(0.5, 2).sample(base_batch_shape)\n    init_dist = random_stable(base_batch_shape + (hidden_dim,),\n                              stability.unsqueeze(-1), skew=skew).to_event(1)\n    trans_mat = torch.randn(base_batch_shape + (duration, hidden_dim, hidden_dim))\n    trans_dist = random_stable(base_batch_shape + (duration, hidden_dim),\n                               stability.unsqueeze(-1).unsqueeze(-1), skew=skew).to_event(1)\n    obs_mat = torch.randn(base_batch_shape + (duration, hidden_dim, 1))\n    obs_dist = random_stable(base_batch_shape + (duration, 1),\n                             stability.unsqueeze(-1).unsqueeze(-1), skew=skew).to_event(1)\n    hmm = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist, duration=duration)\n    assert hmm.batch_shape == base_batch_shape\n    assert hmm.event_shape == (duration, 1)\n\n    hmm = dist.IndependentHMM(hmm)\n    assert hmm.batch_shape == batch_shape\n    assert hmm.event_shape == (duration, obs_dim)\n\n    def model(data=None):\n        with pyro.plate_stack(""plates"", batch_shape):\n            return pyro.sample(""x"", hmm, obs=data)\n\n    data = torch.randn(duration, obs_dim)\n    rep = SymmetricStableReparam() if skew == 0 else StableReparam()\n    with poutine.trace() as tr:\n        with poutine.reparam(config={""x"": LinearHMMReparam(rep, rep, rep)}):\n            model(data)\n    assert isinstance(tr.trace.nodes[""x""][""fn""], dist.IndependentHMM)\n    tr.trace.compute_log_prob()  # smoke test only\n\n\n# Test helper to extract a few fractional moments from joint samples.\n# This uses fractional moments because Stable variance is infinite.\ndef get_hmm_moments(samples):\n    loc = samples.median(0).values\n    delta = samples - loc\n    cov = (delta.unsqueeze(-1) * delta.unsqueeze(-2)).sqrt().mean(0)\n    scale = cov.diagonal(dim1=-2, dim2=-1)\n    sigma = scale.sqrt()\n    corr = cov / (sigma.unsqueeze(-1) * sigma.unsqueeze(-2))\n    return loc, scale, corr\n\n\n@pytest.mark.parametrize(""duration"", [1, 2, 3])\n@pytest.mark.parametrize(""obs_dim"", [1, 2])\n@pytest.mark.parametrize(""hidden_dim"", [1, 2])\n@pytest.mark.parametrize(""stability"", [1.9, 1.6])\n@pytest.mark.parametrize(""skew"", [0, None], ids=[""symmetric"", ""skewed""])\ndef test_stable_hmm_distribution(stability, skew, duration, hidden_dim, obs_dim):\n    init_dist = random_stable((hidden_dim,), stability, skew=skew).to_event(1)\n    trans_mat = torch.randn(duration, hidden_dim, hidden_dim)\n    trans_dist = random_stable((duration, hidden_dim), stability, skew=skew).to_event(1)\n    obs_mat = torch.randn(duration, hidden_dim, obs_dim)\n    obs_dist = random_stable((duration, obs_dim), stability, skew=skew).to_event(1)\n    hmm = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist, duration=duration)\n\n    num_samples = 200000\n    expected_samples = hmm.sample([num_samples]).reshape(num_samples, duration * obs_dim)\n    expected_loc, expected_scale, expected_corr = get_hmm_moments(expected_samples)\n\n    rep = SymmetricStableReparam() if skew == 0 else StableReparam()\n    with pyro.plate(""samples"", num_samples):\n        with poutine.reparam(config={""x"": LinearHMMReparam(rep, rep, rep)}):\n            actual_samples = pyro.sample(""x"", hmm).reshape(num_samples, duration * obs_dim)\n    actual_loc, actual_scale, actual_corr = get_hmm_moments(actual_samples)\n\n    assert_close(actual_loc, expected_loc, atol=0.05, rtol=0.05)\n    assert_close(actual_scale, expected_scale, atol=0.05, rtol=0.05)\n    assert_close(actual_corr, expected_corr, atol=0.01)\n\n\n@pytest.mark.parametrize(""duration"", [1, 2, 3, 4, 5, 6])\n@pytest.mark.parametrize(""obs_dim"", [1, 2])\n@pytest.mark.parametrize(""hidden_dim"", [1, 3])\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (2, 3)], ids=str)\ndef test_stable_hmm_shape_error(batch_shape, duration, hidden_dim, obs_dim):\n    stability = dist.Uniform(0.5, 2).sample(batch_shape)\n    init_dist = random_stable(batch_shape + (hidden_dim,),\n                              stability.unsqueeze(-1)).to_event(1)\n    trans_mat = torch.randn(batch_shape + (1, hidden_dim, hidden_dim))\n    trans_dist = random_stable(batch_shape + (1, hidden_dim),\n                               stability.unsqueeze(-1).unsqueeze(-1)).to_event(1)\n    obs_mat = torch.randn(batch_shape + (1, hidden_dim, obs_dim))\n    obs_dist = random_stable(batch_shape + (1, obs_dim),\n                             stability.unsqueeze(-1).unsqueeze(-1)).to_event(1)\n    hmm = dist.LinearHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n    assert hmm.batch_shape == batch_shape\n    assert hmm.event_shape == (1, obs_dim)\n\n    def model(data=None):\n        with pyro.plate_stack(""plates"", batch_shape):\n            return pyro.sample(""x"", hmm, obs=data)\n\n    data = torch.randn(duration, obs_dim)\n    rep = StableReparam()\n    with poutine.reparam(config={""x"": LinearHMMReparam(rep, rep, rep)}):\n        with pytest.raises(ValueError):\n            model(data)\n'"
tests/infer/reparam/test_loc_scale.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.reparam import LocScaleReparam\nfrom tests.common import assert_close\n\n\n# Test helper to extract a few central moments from samples.\ndef get_moments(x):\n    m1 = x.mean(0)\n    x = x - m1\n    xx = x * x\n    xxx = x * xx\n    xxxx = xx * xx\n    m2 = xx.mean(0)\n    m3 = xxx.mean(0) / m2 ** 1.5\n    m4 = xxxx.mean(0) / m2 ** 2\n    return torch.stack([m1, m2, m3, m4])\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""centered"", [0., 0.6, 1., torch.tensor(0.4), None])\n@pytest.mark.parametrize(""dist_type"", [""Normal"", ""StudentT""])\ndef test_normal(dist_type, centered, shape):\n    loc = torch.empty(shape).uniform_(-1., 1.).requires_grad_()\n    scale = torch.empty(shape).uniform_(0.5, 1.5).requires_grad_()\n    if isinstance(centered, torch.Tensor):\n        centered = centered.expand(shape)\n\n    def model():\n        with pyro.plate_stack(""plates"", shape):\n            with pyro.plate(""particles"", 200000):\n                if ""dist_type"" == ""Normal"":\n                    pyro.sample(""x"", dist.Normal(loc, scale))\n                else:\n                    pyro.sample(""x"", dist.StudentT(10.0, loc, scale))\n\n    value = poutine.trace(model).get_trace().nodes[""x""][""value""]\n    expected_probe = get_moments(value)\n\n    if ""dist_type"" == ""Normal"":\n        reparam = LocScaleReparam()\n    else:\n        reparam = LocScaleReparam(shape_params=[""df""])\n    reparam_model = poutine.reparam(model, {""x"": reparam})\n    value = poutine.trace(reparam_model).get_trace().nodes[""x""][""value""]\n    actual_probe = get_moments(value)\n    assert_close(actual_probe, expected_probe, atol=0.1)\n\n    for actual_m, expected_m in zip(actual_probe, expected_probe):\n        expected_grads = grad(expected_m.sum(), [loc, scale], retain_graph=True)\n        actual_grads = grad(actual_m.sum(), [loc, scale], retain_graph=True)\n        assert_close(actual_grads[0], expected_grads[0], atol=0.05)\n        assert_close(actual_grads[1], expected_grads[1], atol=0.05)\n'"
tests/infer/reparam/test_neutra.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import optim\nfrom pyro.distributions.transforms import ComposeTransform\nfrom pyro.infer import MCMC, NUTS, SVI, Trace_ELBO\nfrom pyro.infer.autoguide import AutoIAFNormal\nfrom pyro.infer.mcmc.util import initialize_model\nfrom pyro.infer.reparam import NeuTraReparam\nfrom tests.common import assert_close, xfail_param\n\n\ndef neals_funnel(dim):\n    y = pyro.sample(\'y\', dist.Normal(0, 3))\n    with pyro.plate(\'D\', dim):\n        pyro.sample(\'x\', dist.Normal(0, torch.exp(y / 2)))\n\n\ndef dirichlet_categorical(data):\n    concentration = torch.tensor([1.0, 1.0, 1.0])\n    p_latent = pyro.sample(\'p\', dist.Dirichlet(concentration))\n    with pyro.plate(\'N\', data.shape[0]):\n        pyro.sample(\'obs\', dist.Categorical(p_latent), obs=data)\n    return p_latent\n\n\n@pytest.mark.parametrize(\'jit\', [\n    False,\n    xfail_param(True, reason=""https://github.com/pyro-ppl/pyro/issues/2292""),\n])\ndef test_neals_funnel_smoke(jit):\n    dim = 10\n\n    guide = AutoIAFNormal(neals_funnel)\n    svi = SVI(neals_funnel, guide,  optim.Adam({""lr"": 1e-10}), Trace_ELBO())\n    for _ in range(1000):\n        svi.step(dim)\n\n    neutra = NeuTraReparam(guide.requires_grad_(False))\n    model = neutra.reparam(neals_funnel)\n    nuts = NUTS(model, jit_compile=jit)\n    mcmc = MCMC(nuts, num_samples=50, warmup_steps=50)\n    mcmc.run(dim)\n    samples = mcmc.get_samples()\n    # XXX: `MCMC.get_samples` adds a leftmost batch dim to all sites, not uniformly at -max_plate_nesting-1;\n    # hence the unsqueeze\n    transformed_samples = neutra.transform_sample(samples[\'y_shared_latent\'].unsqueeze(-2))\n    assert \'x\' in transformed_samples\n    assert \'y\' in transformed_samples\n\n\n@pytest.mark.parametrize(\'model, kwargs\', [\n    (neals_funnel, {\'dim\': 10}),\n    (dirichlet_categorical, {\'data\': torch.ones(10,)})\n])\ndef test_reparam_log_joint(model, kwargs):\n    guide = AutoIAFNormal(model)\n    guide(**kwargs)\n    neutra = NeuTraReparam(guide)\n    reparam_model = neutra.reparam(model)\n    _, pe_fn, transforms, _ = initialize_model(model, model_kwargs=kwargs)\n    init_params, pe_fn_neutra, _, _ = initialize_model(reparam_model, model_kwargs=kwargs)\n    latent_x = list(init_params.values())[0]\n    transformed_params = neutra.transform_sample(latent_x)\n    pe_transformed = pe_fn_neutra(init_params)\n    neutra_transform = ComposeTransform(guide.get_posterior(**kwargs).transforms)\n    latent_y = neutra_transform(latent_x)\n    log_det_jacobian = neutra_transform.log_abs_det_jacobian(latent_x, latent_y)\n    pe = pe_fn({k: transforms[k](v) for k, v in transformed_params.items()})\n    assert_close(pe_transformed, pe - log_det_jacobian)\n'"
tests/infer/reparam/test_split.py,3,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.reparam import SplitReparam\nfrom tests.common import assert_close\n\n\n@pytest.mark.parametrize(""event_shape,splits,dim"", [\n    ((6,), [2, 1, 3], -1),\n    ((2, 5,), [2, 3], -1),\n    ((4, 2), [1, 3], -2),\n    ((2, 3, 1), [1, 2], -2),\n], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\ndef test_normal(batch_shape, event_shape, splits, dim):\n    shape = batch_shape + event_shape\n    loc = torch.empty(shape).uniform_(-1., 1.).requires_grad_()\n    scale = torch.empty(shape).uniform_(0.5, 1.5).requires_grad_()\n\n    def model():\n        with pyro.plate_stack(""plates"", batch_shape):\n            pyro.sample(""x"",\n                        dist.Normal(loc, scale)\n                            .to_event(len(event_shape)))\n\n    # Run without reparam.\n    trace = poutine.trace(model).get_trace()\n    expected_value = trace.nodes[""x""][""value""]\n    expected_log_prob = trace.log_prob_sum()\n    expected_grads = grad(expected_log_prob, [loc, scale], create_graph=True)\n\n    # Run with reparam.\n    split_values = {\n        ""x_split_{}"".format(i): xi\n        for i, xi in enumerate(expected_value.split(splits, dim))}\n    rep = SplitReparam(splits, dim)\n    reparam_model = poutine.reparam(model, {""x"": rep})\n    reparam_model = poutine.condition(reparam_model, split_values)\n    trace = poutine.trace(reparam_model).get_trace()\n    assert all(name in trace.nodes for name in split_values)\n    assert isinstance(trace.nodes[""x""][""fn""], dist.Delta)\n    assert trace.nodes[""x""][""fn""].batch_shape == batch_shape\n    assert trace.nodes[""x""][""fn""].event_shape == event_shape\n\n    # Check values.\n    actual_value = trace.nodes[""x""][""value""]\n    assert_close(actual_value, expected_value, atol=0.1)\n\n    # Check log prob.\n    actual_log_prob = trace.log_prob_sum()\n    assert_close(actual_log_prob, expected_log_prob)\n    actual_grads = grad(actual_log_prob, [loc, scale], create_graph=True)\n    assert_close(actual_grads, expected_grads)\n'"
tests/infer/reparam/test_stable.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom scipy.stats import ks_2samp\nfrom torch.autograd import grad\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.distributions.torch_distribution import MaskedDistribution\nfrom pyro.infer import Trace_ELBO\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.infer.reparam import LatentStableReparam, StableReparam, SymmetricStableReparam\nfrom tests.common import assert_close\n\n\n# Test helper to extract a few absolute moments from univariate samples.\n# This uses abs moments because Stable variance is infinite.\ndef get_moments(x):\n    points = torch.tensor([-4., -1., 0., 1., 4.])\n    points = points.reshape((-1,) + (1,) * x.dim())\n    return torch.cat([x.mean(0, keepdim=True), (x - points).abs().mean(1)])\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (2, 3)], ids=str)\n@pytest.mark.parametrize(""Reparam"", [LatentStableReparam, StableReparam])\ndef test_stable(Reparam, shape):\n    stability = torch.empty(shape).uniform_(1.5, 2.).requires_grad_()\n    skew = torch.empty(shape).uniform_(-0.5, 0.5).requires_grad_()\n    # test edge case when skew is 0\n    if skew.dim() > 0 and skew.shape[-1] > 0:\n        skew.data[..., 0] = 0.\n    scale = torch.empty(shape).uniform_(0.5, 1.0).requires_grad_()\n    loc = torch.empty(shape).uniform_(-1., 1.).requires_grad_()\n    params = [stability, skew, scale, loc]\n\n    def model():\n        with pyro.plate_stack(""plates"", shape):\n            with pyro.plate(""particles"", 100000):\n                return pyro.sample(""x"", dist.Stable(stability, skew, scale, loc))\n\n    value = model()\n    expected_moments = get_moments(value)\n\n    reparam_model = poutine.reparam(model, {""x"": Reparam()})\n    trace = poutine.trace(reparam_model).get_trace()\n    if Reparam is LatentStableReparam:\n        assert isinstance(trace.nodes[""x""][""fn""], MaskedDistribution)\n        assert isinstance(trace.nodes[""x""][""fn""].base_dist, dist.Delta)\n    else:\n        assert isinstance(trace.nodes[""x""][""fn""], dist.Normal)\n    trace.compute_log_prob()  # smoke test only\n    value = trace.nodes[""x""][""value""]\n    actual_moments = get_moments(value)\n    assert_close(actual_moments, expected_moments, atol=0.05)\n\n    for actual_m, expected_m in zip(actual_moments, expected_moments):\n        expected_grads = grad(expected_m.sum(), params, retain_graph=True)\n        actual_grads = grad(actual_m.sum(), params, retain_graph=True)\n        assert_close(actual_grads[0], expected_grads[0], atol=0.2)\n        assert_close(actual_grads[1][skew != 0], expected_grads[1][skew != 0], atol=0.1)\n        assert_close(actual_grads[1][skew == 0], expected_grads[1][skew == 0], atol=0.3)\n        assert_close(actual_grads[2], expected_grads[2], atol=0.1)\n        assert_close(actual_grads[3], expected_grads[3], atol=0.1)\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (2, 3)], ids=str)\ndef test_symmetric_stable(shape):\n    stability = torch.empty(shape).uniform_(1.6, 1.9).requires_grad_()\n    scale = torch.empty(shape).uniform_(0.5, 1.0).requires_grad_()\n    loc = torch.empty(shape).uniform_(-1., 1.).requires_grad_()\n    params = [stability, scale, loc]\n\n    def model():\n        with pyro.plate_stack(""plates"", shape):\n            with pyro.plate(""particles"", 200000):\n                return pyro.sample(""x"", dist.Stable(stability, 0, scale, loc))\n\n    value = model()\n    expected_moments = get_moments(value)\n\n    reparam_model = poutine.reparam(model, {""x"": SymmetricStableReparam()})\n    trace = poutine.trace(reparam_model).get_trace()\n    assert isinstance(trace.nodes[""x""][""fn""], dist.Normal)\n    trace.compute_log_prob()  # smoke test only\n    value = trace.nodes[""x""][""value""]\n    actual_moments = get_moments(value)\n    assert_close(actual_moments, expected_moments, atol=0.05)\n\n    for actual_m, expected_m in zip(actual_moments, expected_moments):\n        expected_grads = grad(expected_m.sum(), params, retain_graph=True)\n        actual_grads = grad(actual_m.sum(), params, retain_graph=True)\n        assert_close(actual_grads[0], expected_grads[0], atol=0.2)\n        assert_close(actual_grads[1], expected_grads[1], atol=0.1)\n        assert_close(actual_grads[2], expected_grads[2], atol=0.1)\n\n\n@pytest.mark.parametrize(""skew"", [-1.0, -0.5, 0.0, 0.5, 1.0])\n@pytest.mark.parametrize(""stability"", [0.1, 0.4, 0.8, 0.99, 1.0, 1.01, 1.3, 1.7, 2.0])\n@pytest.mark.parametrize(""Reparam"", [LatentStableReparam, SymmetricStableReparam, StableReparam])\ndef test_distribution(stability, skew, Reparam):\n    if Reparam is SymmetricStableReparam and (skew != 0 or stability == 2):\n        pytest.skip()\n    if stability == 2 and skew in (-1, 1):\n        pytest.skip()\n\n    def model():\n        with pyro.plate(""particles"", 20000):\n            return pyro.sample(""x"", dist.Stable(stability, skew))\n\n    expected = model()\n    with poutine.reparam(config={""x"": Reparam()}):\n        actual = model()\n    assert ks_2samp(expected, actual).pvalue > 0.05\n\n\n@pytest.mark.parametrize(""subsample"", [False, True], ids=[""full"", ""subsample""])\n@pytest.mark.parametrize(""Reparam"", [LatentStableReparam, SymmetricStableReparam, StableReparam])\ndef test_subsample_smoke(Reparam, subsample):\n    def model():\n        with pyro.plate(""plate"", 10):\n            with poutine.reparam(config={""x"": Reparam()}):\n                return pyro.sample(""x"", dist.Stable(1.5, 0))\n\n    def create_plates():\n        return pyro.plate(""plate"", 10, subsample_size=3)\n\n    guide = AutoNormal(model, create_plates=create_plates if subsample else None)\n    Trace_ELBO().loss(model, guide)  # smoke test\n'"
tests/infer/reparam/test_studentt.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom scipy.stats import ks_2samp\nfrom torch.autograd import grad\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.infer.reparam import StudentTReparam\nfrom tests.common import assert_close\n\n\n# Test helper to extract a few absolute moments from univariate samples.\n# This uses abs moments because StudentT variance may be infinite.\ndef get_moments(x):\n    points = torch.tensor([-4., -1., 0., 1., 4.])\n    points = points.reshape((-1,) + (1,) * x.dim())\n    return torch.cat([x.mean(0, keepdim=True), (x - points).abs().mean(1)])\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (2, 3)], ids=str)\ndef test_moments(shape):\n    df = torch.empty(shape).uniform_(1.8, 5).requires_grad_()\n    loc = torch.empty(shape).uniform_(-1., 1.).requires_grad_()\n    scale = torch.empty(shape).uniform_(0.5, 1.0).requires_grad_()\n    params = [df, loc, scale]\n\n    def model():\n        with pyro.plate_stack(""plates"", shape):\n            with pyro.plate(""particles"", 100000):\n                return pyro.sample(""x"", dist.StudentT(df, loc, scale))\n\n    value = model()\n    expected_moments = get_moments(value)\n\n    reparam_model = poutine.reparam(model, {""x"": StudentTReparam()})\n    trace = poutine.trace(reparam_model).get_trace()\n    assert isinstance(trace.nodes[""x""][""fn""], dist.Normal)\n    trace.compute_log_prob()  # smoke test only\n    value = trace.nodes[""x""][""value""]\n    actual_moments = get_moments(value)\n    assert_close(actual_moments, expected_moments, atol=0.05)\n\n    for actual_m, expected_m in zip(actual_moments, expected_moments):\n        expected_grads = grad(expected_m.sum(), params, retain_graph=True)\n        actual_grads = grad(actual_m.sum(), params, retain_graph=True)\n        assert_close(actual_grads[0], expected_grads[0], atol=0.2)\n        assert_close(actual_grads[1], expected_grads[1], atol=0.1)\n        assert_close(actual_grads[2], expected_grads[2], atol=0.1)\n\n\n@pytest.mark.parametrize(""df"", [0.5, 1.0, 1.5, 2.0, 3.0])\n@pytest.mark.parametrize(""scale"", [0.1, 1.0, 2.0])\n@pytest.mark.parametrize(""loc"", [0.0, 1.234])\ndef test_distribution(df, loc, scale):\n\n    def model():\n        with pyro.plate(""particles"", 20000):\n            return pyro.sample(""x"", dist.StudentT(df, loc, scale))\n\n    expected = model()\n    with poutine.reparam(config={""x"": StudentTReparam()}):\n        actual = model()\n    assert ks_2samp(expected, actual).pvalue > 0.05\n'"
tests/infer/reparam/test_transform.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.distributions.transforms import AffineTransform, ExpTransform\nfrom pyro.infer.reparam import TransformReparam\nfrom tests.common import assert_close\n\n\n# Test helper to extract a few log central moments from samples.\ndef get_moments(x):\n    assert (x > 0).all()\n    x = x.log()\n    m1 = x.mean(0)\n    x = x - m1\n    xx = x * x\n    xxx = x * xx\n    xxxx = xx * xx\n    m2 = xx.mean(0)\n    m3 = xxx.mean(0) / m2 ** 1.5\n    m4 = xxxx.mean(0) / m2 ** 2\n    return torch.stack([m1, m2, m3, m4])\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (2, 3)], ids=str)\ndef test_log_normal(shape):\n    loc = torch.empty(shape).uniform_(-1, 1)\n    scale = torch.empty(shape).uniform_(0.5, 1.5)\n\n    def model():\n        with pyro.plate_stack(""plates"", shape):\n            with pyro.plate(""particles"", 200000):\n                return pyro.sample(""x"",\n                                   dist.TransformedDistribution(\n                                       dist.Normal(torch.zeros_like(loc),\n                                                   torch.ones_like(scale)),\n                                       [AffineTransform(loc, scale),\n                                        ExpTransform()]))\n\n    with poutine.trace() as tr:\n        value = model()\n    assert isinstance(tr.trace.nodes[""x""][""fn""], dist.TransformedDistribution)\n    expected_moments = get_moments(value)\n\n    with poutine.reparam(config={""x"": TransformReparam()}):\n        with poutine.trace() as tr:\n            value = model()\n    assert isinstance(tr.trace.nodes[""x""][""fn""], dist.Delta)\n    actual_moments = get_moments(value)\n    assert_close(actual_moments, expected_moments, atol=0.05)\n'"
tests/infer/reparam/test_unit_jacobian.py,5,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nimport torch\nfrom torch.autograd import grad\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.distributions.transforms import Permute\nfrom pyro.infer.reparam import UnitJacobianReparam\nfrom tests.common import assert_close\n\n\n# Test helper to extract central moments from samples.\ndef get_moments(x):\n    n = x.size(0)\n    x = x.reshape(n, -1)\n    mean = x.mean(0)\n    x = x - mean\n    std = (x * x).mean(0).sqrt()\n    x = x / std\n    corr = (x.unsqueeze(-1) * x.unsqueeze(-2)).mean(0).reshape(-1)\n    return torch.cat([mean, std, corr])\n\n\n@pytest.mark.parametrize(""shape"", [(6,), (4, 5), (2, 1, 3)], ids=str)\ndef test_normal(shape):\n    loc = torch.empty(shape).uniform_(-1., 1.).requires_grad_()\n    scale = torch.empty(shape).uniform_(0.5, 1.5).requires_grad_()\n\n    def model():\n        with pyro.plate_stack(""plates"", shape[:-1]):\n            with pyro.plate(""particles"", 10000):\n                pyro.sample(""x"", dist.Normal(loc, scale).expand(shape).to_event(1))\n\n    value = poutine.trace(model).get_trace().nodes[""x""][""value""]\n    expected_probe = get_moments(value)\n\n    transform = Permute(torch.randperm(shape[-1]))\n    rep = UnitJacobianReparam(transform)\n    reparam_model = poutine.reparam(model, {""x"": rep})\n    trace = poutine.trace(reparam_model).get_trace()\n    assert isinstance(trace.nodes[""x_transformed""][""fn""], dist.TransformedDistribution)\n    assert isinstance(trace.nodes[""x""][""fn""], dist.Delta)\n    value = trace.nodes[""x""][""value""]\n    actual_probe = get_moments(value)\n    assert_close(actual_probe, expected_probe, atol=0.1)\n\n    for actual_m, expected_m in zip(actual_probe[:10], expected_probe[:10]):\n        expected_grads = grad(expected_m.sum(), [loc, scale], retain_graph=True)\n        actual_grads = grad(actual_m.sum(), [loc, scale], retain_graph=True)\n        assert_close(actual_grads[0], expected_grads[0], atol=0.05)\n        assert_close(actual_grads[1], expected_grads[1], atol=0.05)\n'"
tests/ops/einsum/conftest.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.nodeid.startswith(""tests/ops/einsum""):\n            if ""stage"" not in item.keywords:\n                item.add_marker(pytest.mark.stage(""unit""))\n            if ""init"" not in item.keywords:\n                item.add_marker(pytest.mark.init(rng_seed=123))\n'"
tests/ops/einsum/test_adjoint.py,3,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\n\nimport pytest\nimport torch\n\nfrom pyro.ops.einsum import contract\nfrom pyro.ops.einsum.adjoint import require_backward\nfrom tests.common import assert_equal\n\nEQUATIONS = [\n    '->',\n    'w->',\n    ',w->',\n    'w,w->',\n    'w,x->',\n    'w,wx,x->',\n    'w,wx,xy,yz->',\n    'wx,xy,yz,zw->',\n    'i->i',\n    'wi->i',\n    'i,wi->i',\n    'wi,wi->i',\n    'wi,xi->i',\n    'wi,wxi,xi->i',\n    'wi,wxi,xyi,yzi->i',\n    'wxi,xyi,yzi,zwi->i',\n    'ij->ij',\n    'iwj->ij',\n    'ij,iwj->ij',\n    'iwj,iwj->ij',\n    'iwj,ixj->ij',\n    'iwj,iwxj,ixj->ij',\n    'iwj,iwxj,ixyj,iyzj->ij',\n    'iwxj,ixyj,iyzj,izwj->ij',\n    'ij->ji',\n    'iwj->ji',\n    'ji,iwj->ji',\n    'iwj,iwj->ji',\n    'iwj,ixj->ji',\n    'iwj,iwxj,ixj->ji',\n    'iwj,iwxj,ixyj,iyzj->ji',\n    'iwxj,ixyj,iyzj,izwj->ji',\n]\n\n\n@pytest.mark.parametrize('equation', EQUATIONS)\n@pytest.mark.parametrize('backend', ['map', 'sample', 'marginal'])\ndef test_shape(backend, equation):\n    backend = 'pyro.ops.einsum.torch_{}'.format(backend)\n    inputs, output = equation.split('->')\n    inputs = inputs.split(',')\n    symbols = sorted(set(equation) - set(',->'))\n    sizes = dict(zip(symbols, itertools.count(2)))\n    input_shapes = [torch.Size(sizes[dim] for dim in dims)\n                    for dims in inputs]\n    operands = [torch.randn(shape) for shape in input_shapes]\n    for input_, x in zip(inputs, operands):\n        x._pyro_dims = input_\n\n    # check forward pass\n    for x in operands:\n        require_backward(x)\n    expected = contract(equation, *operands, backend='pyro.ops.einsum.torch_log')\n    actual = contract(equation, *operands, backend=backend)\n    if backend.endswith('map'):\n        assert actual.dtype == expected.dtype\n        assert actual.shape == expected.shape\n    else:\n        assert_equal(actual, expected)\n\n    # check backward pass\n    actual._pyro_backward()\n    for input_, x in zip(inputs, operands):\n        backward_result = x._pyro_backward_result\n        if backend.endswith('marginal'):\n            assert backward_result.shape == x.shape\n        else:\n            contract_dims = set(input_) - set(output)\n            if contract_dims:\n                assert backward_result.size(0) == len(contract_dims)\n                assert set(backward_result._pyro_dims[1:]) == set(output)\n                for sample, dim in zip(backward_result, backward_result._pyro_sample_dims):\n                    assert sample.min() >= 0\n                    assert sample.max() < sizes[dim]\n            else:\n                assert backward_result is None\n\n\n@pytest.mark.parametrize('equation', EQUATIONS)\ndef test_marginal(equation):\n    inputs, output = equation.split('->')\n    inputs = inputs.split(',')\n    operands = [torch.randn(torch.Size((2,) * len(input_)))\n                for input_ in inputs]\n    for input_, x in zip(inputs, operands):\n        x._pyro_dims = input_\n\n    # check forward pass\n    for x in operands:\n        require_backward(x)\n    actual = contract(equation, *operands, backend='pyro.ops.einsum.torch_marginal')\n    expected = contract(equation, *operands,\n                        backend='pyro.ops.einsum.torch_log')\n    assert_equal(expected, actual)\n\n    # check backward pass\n    actual._pyro_backward()\n    for input_, operand in zip(inputs, operands):\n        marginal_equation = ','.join(inputs) + '->' + input_\n        expected = contract(marginal_equation, *operands,\n                            backend='pyro.ops.einsum.torch_log')\n        actual = operand._pyro_backward_result\n        assert_equal(expected, actual)\n"""
tests/ops/einsum/test_torch_log.py,2,"b""# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\n\nimport pytest\nimport torch\n\nfrom pyro.infer.util import torch_exp\nfrom pyro.ops.einsum import contract\nfrom tests.common import assert_equal\n\n\n@pytest.mark.parametrize('min_size', [1, 2])\n@pytest.mark.parametrize('equation', [\n    ',ab->ab',\n    'ab,,bc->a',\n    'ab,,bc->b',\n    'ab,,bc->c',\n    'ab,,bc->ac',\n    'ab,,b,bc->ac',\n    'a,ab->ab',\n    'ab,b,bc->a',\n    'ab,b,bc->b',\n    'ab,b,bc->c',\n    'ab,b,bc->ac',\n    'ab,bc->ac',\n    'ab,bc,cd->',\n    'ab,bc,cd->a',\n    'ab,bc,cd->b',\n    'ab,bc,cd->c',\n    'ab,bc,cd->d',\n    'ab,bc,cd->ac',\n    'ab,bc,cd->ad',\n    'ab,bc,cd->bc',\n    'a,a,ab,b,b,b,b->a',\n])\n@pytest.mark.parametrize('infinite', [False, True], ids=['finite', 'infinite'])\ndef test_einsum(equation, min_size, infinite):\n    inputs, output = equation.split('->')\n    inputs = inputs.split(',')\n    symbols = sorted(set(equation) - set(',->'))\n    sizes = dict(zip(symbols, itertools.count(min_size)))\n    shapes = [torch.Size(tuple(sizes[dim] for dim in dims))\n              for dims in inputs]\n    operands = [torch.full(shape, -float('inf')) if infinite else torch.randn(shape)\n                for shape in shapes]\n\n    expected = contract(equation, *(torch_exp(x) for x in operands), backend='torch').log()\n    actual = contract(equation, *operands, backend='pyro.ops.einsum.torch_log')\n    assert_equal(actual, expected)\n"""
pyro/contrib/gp/kernels/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.gp.kernels.brownian import Brownian\nfrom pyro.contrib.gp.kernels.coregionalize import Coregionalize\nfrom pyro.contrib.gp.kernels.dot_product import DotProduct, Linear, Polynomial\nfrom pyro.contrib.gp.kernels.isotropic import (RBF, Exponential, Isotropy, Matern32, Matern52,\n                                               RationalQuadratic)\nfrom pyro.contrib.gp.kernels.kernel import (Combination, Exponent, Kernel, Product, Sum,\n                                            Transforming, VerticalScaling, Warping)\nfrom pyro.contrib.gp.kernels.periodic import Cosine, Periodic\nfrom pyro.contrib.gp.kernels.static import Constant, WhiteNoise\n\n__all__ = [\n    ""Kernel"",\n    ""Brownian"",\n    ""Combination"",\n    ""Constant"",\n    ""Coregionalize"",\n    ""Cosine"",\n    ""DotProduct"",\n    ""Exponent"",\n    ""Exponential"",\n    ""Isotropy"",\n    ""Linear"",\n    ""Matern32"",\n    ""Matern52"",\n    ""Periodic"",\n    ""Polynomial"",\n    ""Product"",\n    ""RBF"",\n    ""RationalQuadratic"",\n    ""Sum"",\n    ""Transforming"",\n    ""VerticalScaling"",\n    ""Warping"",\n    ""WhiteNoise"",\n]\n\n# Create sphinx documentation.\n__doc__ = \'\\n\\n\'.join([\n\n    \'\'\'\n    {0}\n    ----------------------------------------------------------------\n    .. autoclass:: pyro.contrib.gp.kernels.{0}\n        :members:\n        :undoc-members:\n        :special-members: __call__\n        :show-inheritance:\n        :member-order: bysource\n    \'\'\'.format(_name)\n    for _name in __all__\n])\n'"
pyro/contrib/gp/kernels/brownian.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.contrib.gp.kernels.kernel import Kernel\nfrom pyro.nn.module import PyroParam\n\n\nclass Brownian(Kernel):\n    r""""""\n    This kernel correponds to a two-sided Brownion motion (Wiener process):\n\n        :math:`k(x,z)=\\begin{cases}\\sigma^2\\min(|x|,|z|),& \\text{if } x\\cdot z\\ge 0\\\\\n        0, & \\text{otherwise}. \\end{cases}`\n\n    Note that the input dimension of this kernel must be 1.\n\n    Reference:\n\n    [1] `Theory and Statistical Applications of Stochastic Processes`,\n    Yuliya Mishura, Georgiy Shevchenko\n    """"""\n\n    def __init__(self, input_dim, variance=None, active_dims=None):\n        if input_dim != 1:\n            raise ValueError(""Input dimensional for Brownian kernel must be 1."")\n        super().__init__(input_dim, active_dims)\n\n        variance = torch.tensor(1.) if variance is None else variance\n        self.variance = PyroParam(variance, constraints.positive)\n\n    def forward(self, X, Z=None, diag=False):\n        if Z is None:\n            Z = X\n        X = self._slice_input(X)\n        if diag:\n            return self.variance * X.abs().squeeze(1)\n\n        Z = self._slice_input(Z)\n        if X.size(1) != Z.size(1):\n            raise ValueError(""Inputs must have the same number of features."")\n\n        Zt = Z.t()\n        return torch.where(X.sign() == Zt.sign(),\n                           self.variance * torch.min(X.abs(), Zt.abs()),\n                           X.data.new_zeros(X.size(0), Z.size(0)))\n'"
pyro/contrib/gp/kernels/coregionalize.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.nn import Parameter\n\nfrom pyro.contrib.gp.kernels.kernel import Kernel\nfrom pyro.nn.module import PyroParam\n\n\nclass Coregionalize(Kernel):\n    r""""""\n    A kernel for the linear model of coregionalization\n    :math:`k(x,z) = x^T (W W^T + D) z` where :math:`W` is an\n    ``input_dim``-by-``rank`` matrix and typically ``rank < input_dim``,\n    and ``D`` is a diagonal matrix.\n\n    This generalizes the\n    :class:`~pyro.contrib.gp.kernels.dot_product.Linear` kernel to multiple\n    features with a low-rank-plus-diagonal weight matrix. The typical use case\n    is for modeling correlations among outputs of a multi-output GP, where\n    outputs are coded as distinct data points with one-hot coded features\n    denoting which output each datapoint represents.\n\n    If only ``rank`` is specified, the kernel ``(W W^T + D)`` will be\n    randomly initialized to a matrix with expected value the identity matrix.\n\n    References:\n\n    [1] Mauricio A. Alvarez, Lorenzo Rosasco, Neil D. Lawrence (2012)\n        Kernels for Vector-Valued Functions: a Review\n\n    :param int input_dim: Number of feature dimensions of inputs.\n    :param int rank: Optional rank. This is only used if ``components`` is\n        unspecified. If neigher ``rank`` nor ``components`` is specified,\n        then ``rank`` defaults to ``input_dim``.\n    :param torch.Tensor components: An optional ``(input_dim, rank)`` shaped\n        matrix that maps features to ``rank``-many components. If unspecified,\n        this will be randomly initialized.\n    :param torch.Tensor diagonal: An optional vector of length ``input_dim``.\n        If unspecified, this will be set to constant ``0.5``.\n    :param list active_dims: List of feature dimensions of the input which the\n        kernel acts on.\n    :param str name: Name of the kernel.\n    """"""\n\n    def __init__(self, input_dim, rank=None, components=None, diagonal=None, active_dims=None):\n        super().__init__(input_dim, active_dims)\n\n        # Add a low-rank kernel with expected value torch.eye(input_dim, input_dim) / 2.\n        if components is None:\n            rank = input_dim if rank is None else rank\n            components = torch.randn(input_dim, rank) * (0.5 / rank) ** 0.5\n        else:\n            rank = components.size(-1)\n        if components.shape != (input_dim, rank):\n            raise ValueError(""Expected components.shape == ({},rank), actual {}""\n                             .format(input_dim, components.shape))\n        self.components = Parameter(components)\n\n        # Add a diagonal component initialized to torch.eye(input_dim, input_dim) / 2,\n        # such that the total kernel has expected value the identity matrix.\n        diagonal = components.new_ones(input_dim) * 0.5 if diagonal is None else diagonal\n        if diagonal.shape != (input_dim,):\n            raise ValueError(""Expected diagonal.shape == ({},), actual {}""\n                             .format(input_dim, diagonal.shape))\n        self.diagonal = PyroParam(diagonal, constraints.positive)\n\n    def forward(self, X, Z=None, diag=False):\n        X = self._slice_input(X)\n        Xc = X.matmul(self.components)\n\n        if diag:\n            return (Xc ** 2).sum(-1) + (X ** 2).mv(self.diagonal)\n\n        if Z is None:\n            Z = X\n            Zc = Xc\n        else:\n            Z = self._slice_input(Z)\n            Zc = Z.matmul(self.components)\n\n        return Xc.matmul(Zc.t()) + (X * self.diagonal).matmul(Z.t())\n'"
pyro/contrib/gp/kernels/dot_product.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.contrib.gp.kernels.kernel import Kernel\nfrom pyro.nn.module import PyroParam\n\n\nclass DotProduct(Kernel):\n    r""""""\n    Base class for kernels which are functions of :math:`x \\cdot z`.\n    """"""\n\n    def __init__(self, input_dim, variance=None, active_dims=None):\n        super().__init__(input_dim, active_dims)\n\n        variance = torch.tensor(1.) if variance is None else variance\n        self.variance = PyroParam(variance, constraints.positive)\n\n    def _dot_product(self, X, Z=None, diag=False):\n        r""""""\n        Returns :math:`X \\cdot Z`.\n        """"""\n        if Z is None:\n            Z = X\n        X = self._slice_input(X)\n        if diag:\n            return (X ** 2).sum(-1)\n\n        Z = self._slice_input(Z)\n        if X.size(1) != Z.size(1):\n            raise ValueError(""Inputs must have the same number of features."")\n\n        return X.matmul(Z.t())\n\n\nclass Linear(DotProduct):\n    r""""""\n    Implementation of Linear kernel:\n\n        :math:`k(x, z) = \\sigma^2 x \\cdot z.`\n\n    Doing Gaussian Process regression with linear kernel is equivalent to doing a\n    linear regression.\n\n    .. note:: Here we implement the homogeneous version. To use the inhomogeneous\n        version, consider using :class:`Polynomial` kernel with ``degree=1`` or making\n        a :class:`.Sum` with a :class:`.Constant` kernel.\n    """"""\n\n    def __init__(self, input_dim, variance=None, active_dims=None):\n        super().__init__(input_dim, variance, active_dims)\n\n    def forward(self, X, Z=None, diag=False):\n        return self.variance * self._dot_product(X, Z, diag)\n\n\nclass Polynomial(DotProduct):\n    r""""""\n    Implementation of Polynomial kernel:\n\n        :math:`k(x, z) = \\sigma^2(\\text{bias} + x \\cdot z)^d.`\n\n    :param torch.Tensor bias: Bias parameter of this kernel. Should be positive.\n    :param int degree: Degree :math:`d` of the polynomial.\n    """"""\n\n    def __init__(self, input_dim, variance=None, bias=None, degree=1, active_dims=None):\n        super().__init__(input_dim, variance, active_dims)\n\n        bias = torch.tensor(1.) if bias is None else bias\n        self.bias = PyroParam(bias, constraints.positive)\n\n        if not isinstance(degree, int) or degree < 1:\n            raise ValueError(""Degree for Polynomial kernel should be a positive integer."")\n        self.degree = degree\n\n    def forward(self, X, Z=None, diag=False):\n        return self.variance * ((self.bias + self._dot_product(X, Z, diag)) ** self.degree)\n'"
pyro/contrib/gp/kernels/isotropic.py,11,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.contrib.gp.kernels.kernel import Kernel\nfrom pyro.nn.module import PyroParam\n\n\ndef _torch_sqrt(x, eps=1e-12):\n    """"""\n    A convenient function to avoid the NaN gradient issue of :func:`torch.sqrt`\n    at 0.\n    """"""\n    # Ref: https://github.com/pytorch/pytorch/issues/2421\n    return (x + eps).sqrt()\n\n\nclass Isotropy(Kernel):\n    """"""\n    Base class for a family of isotropic covariance kernels which are functions of the\n    distance :math:`|x-z|/l`, where :math:`l` is the length-scale parameter.\n\n    By default, the parameter ``lengthscale`` has size 1. To use the isotropic version\n    (different lengthscale for each dimension), make sure that ``lengthscale`` has size\n    equal to ``input_dim``.\n\n    :param torch.Tensor lengthscale: Length-scale parameter of this kernel.\n    """"""\n    def __init__(self, input_dim, variance=None, lengthscale=None, active_dims=None):\n        super().__init__(input_dim, active_dims)\n\n        variance = torch.tensor(1.) if variance is None else variance\n        self.variance = PyroParam(variance, constraints.positive)\n\n        lengthscale = torch.tensor(1.) if lengthscale is None else lengthscale\n        self.lengthscale = PyroParam(lengthscale, constraints.positive)\n\n    def _square_scaled_dist(self, X, Z=None):\n        r""""""\n        Returns :math:`\\|\\frac{X-Z}{l}\\|^2`.\n        """"""\n        if Z is None:\n            Z = X\n        X = self._slice_input(X)\n        Z = self._slice_input(Z)\n        if X.size(1) != Z.size(1):\n            raise ValueError(""Inputs must have the same number of features."")\n\n        scaled_X = X / self.lengthscale\n        scaled_Z = Z / self.lengthscale\n        X2 = (scaled_X ** 2).sum(1, keepdim=True)\n        Z2 = (scaled_Z ** 2).sum(1, keepdim=True)\n        XZ = scaled_X.matmul(scaled_Z.t())\n        r2 = X2 - 2 * XZ + Z2.t()\n        return r2.clamp(min=0)\n\n    def _scaled_dist(self, X, Z=None):\n        r""""""\n        Returns :math:`\\|\\frac{X-Z}{l}\\|`.\n        """"""\n        return _torch_sqrt(self._square_scaled_dist(X, Z))\n\n    def _diag(self, X):\n        """"""\n        Calculates the diagonal part of covariance matrix on active features.\n        """"""\n        return self.variance.expand(X.size(0))\n\n\nclass RBF(Isotropy):\n    r""""""\n    Implementation of Radial Basis Function kernel:\n\n        :math:`k(x,z) = \\sigma^2\\exp\\left(-0.5 \\times \\frac{|x-z|^2}{l^2}\\right).`\n\n    .. note:: This kernel also has name `Squared Exponential` in literature.\n    """"""\n    def __init__(self, input_dim, variance=None, lengthscale=None, active_dims=None):\n        super().__init__(input_dim, variance, lengthscale, active_dims)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self._diag(X)\n\n        r2 = self._square_scaled_dist(X, Z)\n        return self.variance * torch.exp(-0.5 * r2)\n\n\nclass RationalQuadratic(Isotropy):\n    r""""""\n    Implementation of RationalQuadratic kernel:\n\n        :math:`k(x, z) = \\sigma^2 \\left(1 + 0.5 \\times \\frac{|x-z|^2}{\\alpha l^2}\n        \\right)^{-\\alpha}.`\n\n    :param torch.Tensor scale_mixture: Scale mixture (:math:`\\alpha`) parameter of this\n        kernel. Should have size 1.\n    """"""\n    def __init__(self, input_dim, variance=None, lengthscale=None, scale_mixture=None,\n                 active_dims=None):\n        super().__init__(input_dim, variance, lengthscale, active_dims)\n\n        if scale_mixture is None:\n            scale_mixture = torch.tensor(1.)\n        self.scale_mixture = PyroParam(scale_mixture, constraints.positive)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self._diag(X)\n\n        r2 = self._square_scaled_dist(X, Z)\n        return self.variance * (1 + (0.5 / self.scale_mixture) * r2).pow(-self.scale_mixture)\n\n\nclass Exponential(Isotropy):\n    r""""""\n    Implementation of Exponential kernel:\n\n        :math:`k(x, z) = \\sigma^2\\exp\\left(-\\frac{|x-z|}{l}\\right).`\n    """"""\n    def __init__(self, input_dim, variance=None, lengthscale=None, active_dims=None):\n        super().__init__(input_dim, variance, lengthscale, active_dims)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self._diag(X)\n\n        r = self._scaled_dist(X, Z)\n        return self.variance * torch.exp(-r)\n\n\nclass Matern32(Isotropy):\n    r""""""\n    Implementation of Matern32 kernel:\n\n        :math:`k(x, z) = \\sigma^2\\left(1 + \\sqrt{3} \\times \\frac{|x-z|}{l}\\right)\n        \\exp\\left(-\\sqrt{3} \\times \\frac{|x-z|}{l}\\right).`\n    """"""\n    def __init__(self, input_dim, variance=None, lengthscale=None, active_dims=None):\n        super().__init__(input_dim, variance, lengthscale, active_dims)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self._diag(X)\n\n        r = self._scaled_dist(X, Z)\n        sqrt3_r = 3**0.5 * r\n        return self.variance * (1 + sqrt3_r) * torch.exp(-sqrt3_r)\n\n\nclass Matern52(Isotropy):\n    r""""""\n    Implementation of Matern52 kernel:\n\n        :math:`k(x,z)=\\sigma^2\\left(1+\\sqrt{5}\\times\\frac{|x-z|}{l}+\\frac{5}{3}\\times\n        \\frac{|x-z|^2}{l^2}\\right)\\exp\\left(-\\sqrt{5} \\times \\frac{|x-z|}{l}\\right).`\n    """"""\n    def __init__(self, input_dim, variance=None, lengthscale=None, active_dims=None):\n        super().__init__(input_dim, variance, lengthscale, active_dims)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self._diag(X)\n\n        r2 = self._square_scaled_dist(X, Z)\n        r = _torch_sqrt(r2)\n        sqrt5_r = 5**0.5 * r\n        return self.variance * (1 + sqrt5_r + (5/3) * r2) * torch.exp(-sqrt5_r)\n'"
pyro/contrib/gp/kernels/kernel.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numbers\n\nfrom pyro.contrib.gp.parameterized import Parameterized\n\n\nclass Kernel(Parameterized):\n    """"""\n    Base class for kernels used in this Gaussian Process module.\n\n    Every inherited class should implement a :meth:`forward` pass which takes inputs\n    :math:`X`, :math:`Z` and returns their covariance matrix.\n\n    To construct a new kernel from the old ones, we can use methods :meth:`add`,\n    :meth:`mul`, :meth:`exp`, :meth:`warp`, :meth:`vertical_scale`.\n\n    References:\n\n    [1] `Gaussian Processes for Machine Learning`,\n    Carl E. Rasmussen, Christopher K. I. Williams\n\n    :param int input_dim: Number of feature dimensions of inputs.\n    :param torch.Tensor variance: Variance parameter of this kernel.\n    :param list active_dims: List of feature dimensions of the input which the kernel\n        acts on.\n    """"""\n\n    def __init__(self, input_dim, active_dims=None):\n        super().__init__()\n\n        if active_dims is None:\n            active_dims = list(range(input_dim))\n        elif input_dim != len(active_dims):\n            raise ValueError(""Input size and the length of active dimensionals should be equal."")\n        self.input_dim = input_dim\n        self.active_dims = active_dims\n\n    def forward(self, X, Z=None, diag=False):\n        r""""""\n        Calculates covariance matrix of inputs on active dimensionals.\n\n        :param torch.Tensor X: A 2D tensor with shape :math:`N \\times input\\_dim`.\n        :param torch.Tensor Z: An (optional) 2D tensor with shape\n            :math:`M \\times input\\_dim`.\n        :param bool diag: A flag to decide if we want to return full covariance matrix\n            or just its diagonal part.\n        :returns: covariance matrix of :math:`X` and :math:`Z` with shape\n            :math:`N \\times M`\n        :rtype: torch.Tensor\n        """"""\n        raise NotImplementedError\n\n    def _slice_input(self, X):\n        r""""""\n        Slices :math:`X` according to ``self.active_dims``. If ``X`` is 1D then returns\n        a 2D tensor with shape :math:`N \\times 1`.\n\n        :param torch.Tensor X: A 1D or 2D input tensor.\n        :returns: a 2D slice of :math:`X`\n        :rtype: torch.Tensor\n        """"""\n        if X.dim() == 2:\n            return X[:, self.active_dims]\n        elif X.dim() == 1:\n            return X.unsqueeze(1)\n        else:\n            raise ValueError(""Input X must be either 1 or 2 dimensional."")\n\n\nclass Combination(Kernel):\n    """"""\n    Base class for kernels derived from a combination of kernels.\n\n    :param Kernel kern0: First kernel to combine.\n    :param kern1: Second kernel to combine.\n    :type kern1: Kernel or numbers.Number\n    """"""\n    def __init__(self, kern0, kern1):\n        if not isinstance(kern0, Kernel):\n            raise TypeError(""The first component of a combined kernel must be a ""\n                            ""Kernel instance."")\n        if not (isinstance(kern1, Kernel) or isinstance(kern1, numbers.Number)):\n            raise TypeError(""The second component of a combined kernel must be a ""\n                            ""Kernel instance or a number."")\n\n        active_dims = set(kern0.active_dims)\n        if isinstance(kern1, Kernel):\n            active_dims |= set(kern1.active_dims)\n        active_dims = sorted(active_dims)\n        input_dim = len(active_dims)\n        super().__init__(input_dim, active_dims)\n\n        self.kern0 = kern0\n        self.kern1 = kern1\n\n\nclass Sum(Combination):\n    """"""\n    Returns a new kernel which acts like a sum/direct sum of two kernels.\n    The second kernel can be a constant.\n    """"""\n    def forward(self, X, Z=None, diag=False):\n        if isinstance(self.kern1, Kernel):\n            return self.kern0(X, Z, diag=diag) + self.kern1(X, Z, diag=diag)\n        else:  # constant\n            return self.kern0(X, Z, diag=diag) + self.kern1\n\n\nclass Product(Combination):\n    """"""\n    Returns a new kernel which acts like a product/tensor product of two kernels.\n    The second kernel can be a constant.\n    """"""\n    def forward(self, X, Z=None, diag=False):\n        if isinstance(self.kern1, Kernel):\n            return self.kern0(X, Z, diag=diag) * self.kern1(X, Z, diag=diag)\n        else:  # constant\n            return self.kern0(X, Z, diag=diag) * self.kern1\n\n\nclass Transforming(Kernel):\n    """"""\n    Base class for kernels derived from a kernel by some transforms such as warping,\n    exponent, vertical scaling.\n\n    :param Kernel kern: The original kernel.\n    """"""\n    def __init__(self, kern):\n        super().__init__(kern.input_dim, kern.active_dims)\n\n        self.kern = kern\n\n\nclass Exponent(Transforming):\n    r""""""\n    Creates a new kernel according to\n\n        :math:`k_{new}(x, z) = \\exp(k(x, z)).`\n    """"""\n    def forward(self, X, Z=None, diag=False):\n        return self.kern(X, Z, diag=diag).exp()\n\n\nclass VerticalScaling(Transforming):\n    """"""\n    Creates a new kernel according to\n\n        :math:`k_{new}(x, z) = f(x)k(x, z)f(z),`\n\n    where :math:`f` is a function.\n\n    :param callable vscaling_fn: A vertical scaling function :math:`f`.\n    """"""\n    def __init__(self, kern, vscaling_fn):\n        super().__init__(kern)\n\n        self.vscaling_fn = vscaling_fn\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self.vscaling_fn(X) * self.kern(X, Z, diag=diag) * self.vscaling_fn(X)\n        elif Z is None:\n            vscaled_X = self.vscaling_fn(X).unsqueeze(1)\n            return vscaled_X * self.kern(X, Z, diag=diag) * vscaled_X.t()\n        else:\n            return (self.vscaling_fn(X).unsqueeze(1) * self.kern(X, Z, diag=diag) *\n                    self.vscaling_fn(Z).unsqueeze(0))\n\n\ndef _Horner_evaluate(x, coef):\n    """"""\n    Evaluates the value of a polynomial according to Horner\'s method.\n    """"""\n    # https://en.wikipedia.org/wiki/Horner%27s_method\n    n = len(coef) - 1\n    b = coef[n]\n    for i in range(n-1, -1, -1):\n        b = coef[i] + b * x\n    return b\n\n\nclass Warping(Transforming):\n    """"""\n    Creates a new kernel according to\n\n        :math:`k_{new}(x, z) = q(k(f(x), f(z))),`\n\n    where :math:`f` is an function and :math:`q` is a polynomial with non-negative\n    coefficients ``owarping_coef``.\n\n    We can take advantage of :math:`f` to combine a Gaussian Process kernel with a deep\n    learning architecture. For example:\n\n        >>> linear = torch.nn.Linear(10, 3)\n        >>> # register its parameters to Pyro\'s ParamStore and wrap it by lambda\n        >>> # to call the primitive pyro.module each time we use the linear function\n        >>> pyro_linear_fn = lambda x: pyro.module(""linear"", linear)(x)\n        >>> kernel = gp.kernels.Matern52(input_dim=3, lengthscale=torch.ones(3))\n        >>> warped_kernel = gp.kernels.Warping(kernel, pyro_linear_fn)\n\n    Reference:\n\n    [1] `Deep Kernel Learning`,\n    Andrew G. Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing\n\n    :param callable iwarping_fn: An input warping function :math:`f`.\n    :param list owarping_coef: A list of coefficients of the output warping polynomial.\n        These coefficients must be non-negative.\n    """"""\n    def __init__(self, kern, iwarping_fn=None, owarping_coef=None):\n        super().__init__(kern)\n\n        self.iwarping_fn = iwarping_fn\n\n        if owarping_coef is not None:\n            for coef in owarping_coef:\n                if not isinstance(coef, int) and coef < 0:\n                    raise ValueError(""Coefficients of the polynomial must be a ""\n                                     ""non-negative integer."")\n            if len(owarping_coef) < 2 and sum(owarping_coef) == 0:\n                raise ValueError(""The ouput warping polynomial should have a degree ""\n                                 ""of at least 1."")\n        self.owarping_coef = owarping_coef\n\n    def forward(self, X, Z=None, diag=False):\n        if self.iwarping_fn is None:\n            K_iwarp = self.kern(X, Z, diag=diag)\n        elif Z is None:\n            K_iwarp = self.kern(self.iwarping_fn(X), None, diag=diag)\n        else:\n            K_iwarp = self.kern(self.iwarping_fn(X), self.iwarping_fn(Z), diag=diag)\n\n        if self.owarping_coef is None:\n            return K_iwarp\n        else:\n            return _Horner_evaluate(K_iwarp, self.owarping_coef)\n'"
pyro/contrib/gp/kernels/periodic.py,10,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.contrib.gp.kernels.isotropic import Isotropy\nfrom pyro.contrib.gp.kernels.kernel import Kernel\nfrom pyro.nn.module import PyroParam\n\n\nclass Cosine(Isotropy):\n    r""""""\n    Implementation of Cosine kernel:\n\n        :math:`k(x,z) = \\sigma^2 \\cos\\left(\\frac{|x-z|}{l}\\right).`\n\n    :param torch.Tensor lengthscale: Length-scale parameter of this kernel.\n    """"""\n    def __init__(self, input_dim, variance=None, lengthscale=None, active_dims=None):\n        super().__init__(input_dim, variance, lengthscale, active_dims)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self._diag(X)\n\n        r = self._scaled_dist(X, Z)\n        return self.variance * torch.cos(r)\n\n\nclass Periodic(Kernel):\n    r""""""\n    Implementation of Periodic kernel:\n\n        :math:`k(x,z)=\\sigma^2\\exp\\left(-2\\times\\frac{\\sin^2(\\pi(x-z)/p)}{l^2}\\right),`\n\n    where :math:`p` is the ``period`` parameter.\n\n    References:\n\n    [1] `Introduction to Gaussian processes`,\n    David J.C. MacKay\n\n    :param torch.Tensor lengthscale: Length scale parameter of this kernel.\n    :param torch.Tensor period: Period parameter of this kernel.\n    """"""\n    def __init__(self, input_dim, variance=None, lengthscale=None, period=None, active_dims=None):\n        super().__init__(input_dim, active_dims)\n\n        variance = torch.tensor(1.) if variance is None else variance\n        self.variance = PyroParam(variance, constraints.positive)\n\n        lengthscale = torch.tensor(1.) if lengthscale is None else lengthscale\n        self.lengthscale = PyroParam(lengthscale, constraints.positive)\n\n        period = torch.tensor(1.) if period is None else period\n        self.period = PyroParam(period, constraints.positive)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self.variance.expand(X.size(0))\n\n        if Z is None:\n            Z = X\n        X = self._slice_input(X)\n        Z = self._slice_input(Z)\n        if X.size(1) != Z.size(1):\n            raise ValueError(""Inputs must have the same number of features."")\n\n        d = X.unsqueeze(1) - Z.unsqueeze(0)\n        scaled_sin = torch.sin(math.pi * d / self.period) / self.lengthscale\n        return self.variance * torch.exp(-2 * (scaled_sin ** 2).sum(-1))\n'"
pyro/contrib/gp/kernels/static.py,3,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nfrom pyro.contrib.gp.kernels.kernel import Kernel\nfrom pyro.nn.module import PyroParam\n\n\nclass Constant(Kernel):\n    r""""""\n    Implementation of Constant kernel:\n\n        :math:`k(x, z) = \\sigma^2.`\n    """"""\n    def __init__(self, input_dim, variance=None, active_dims=None):\n        super().__init__(input_dim, active_dims)\n\n        variance = torch.tensor(1.) if variance is None else variance\n        self.variance = PyroParam(variance, constraints.positive)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self.variance.expand(X.size(0))\n\n        if Z is None:\n            Z = X\n        return self.variance.expand(X.size(0), Z.size(0))\n\n\nclass WhiteNoise(Kernel):\n    r""""""\n    Implementation of WhiteNoise kernel:\n\n        :math:`k(x, z) = \\sigma^2 \\delta(x, z),`\n\n    where :math:`\\delta` is a Dirac delta function.\n    """"""\n    def __init__(self, input_dim, variance=None, active_dims=None):\n        super().__init__(input_dim, active_dims)\n\n        variance = torch.tensor(1.) if variance is None else variance\n        self.variance = PyroParam(variance, constraints.positive)\n\n    def forward(self, X, Z=None, diag=False):\n        if diag:\n            return self.variance.expand(X.size(0))\n\n        if Z is None:\n            return self.variance.expand(X.size(0)).diag()\n        else:\n            return X.data.new_zeros(X.size(0), Z.size(0))\n'"
pyro/contrib/gp/likelihoods/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.gp.likelihoods.binary import Binary\nfrom pyro.contrib.gp.likelihoods.gaussian import Gaussian\nfrom pyro.contrib.gp.likelihoods.likelihood import Likelihood\nfrom pyro.contrib.gp.likelihoods.multi_class import MultiClass\nfrom pyro.contrib.gp.likelihoods.poisson import Poisson\n\n__all__ = [\n    ""Likelihood"",\n    ""Binary"",\n    ""Gaussian"",\n    ""MultiClass"",\n    ""Poisson"",\n]\n\n\n# Create sphinx documentation.\n__doc__ = \'\\n\\n\'.join([\n\n    \'\'\'\n    {0}\n    ----------------------------------------------------------------\n    .. autoclass:: pyro.contrib.gp.likelihoods.{0}\n        :members:\n        :undoc-members:\n        :special-members: __call__\n        :show-inheritance:\n        :member-order: bysource\n    \'\'\'.format(_name)\n    for _name in __all__\n])\n'"
pyro/contrib/gp/likelihoods/binary.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom pyro.contrib.gp.likelihoods.likelihood import Likelihood\n\n\nclass Binary(Likelihood):\n    """"""\n    Implementation of Binary likelihood, which is used for binary classification\n    problems.\n\n    Binary likelihood uses :class:`~pyro.distributions.Bernoulli` distribution,\n    so the output of ``response_function`` should be in range :math:`(0,1)`. By\n    default, we use `sigmoid` function.\n\n    :param callable response_function: A mapping to correct domain for Binary\n        likelihood.\n    """"""\n    def __init__(self, response_function=None):\n        super().__init__()\n        self.response_function = torch.sigmoid if response_function is None else response_function\n\n    def forward(self, f_loc, f_var, y=None):\n        r""""""\n        Samples :math:`y` given :math:`f_{loc}`, :math:`f_{var}` according to\n\n            .. math:: f & \\sim \\mathbb{Normal}(f_{loc}, f_{var}),\\\\\n                y & \\sim \\mathbb{Bernoulli}(f).\n\n        .. note:: The log likelihood is estimated using Monte Carlo with 1 sample of\n            :math:`f`.\n\n        :param torch.Tensor f_loc: Mean of latent function output.\n        :param torch.Tensor f_var: Variance of latent function output.\n        :param torch.Tensor y: Training output tensor.\n        :returns: a tensor sampled from likelihood\n        :rtype: torch.Tensor\n        """"""\n        # calculates Monte Carlo estimate for E_q(f) [logp(y | f)]\n        f = dist.Normal(f_loc, f_var.sqrt())()\n        if self.response_function is torch.sigmoid:\n            y_dist = dist.Bernoulli(logits=f)\n        else:\n            f_res = self.response_function(f)\n            y_dist = dist.Bernoulli(f_res)\n        if y is not None:\n            y_dist = y_dist.expand_by(y.shape[:-f.dim()]).to_event(y.dim())\n        return pyro.sample(self._pyro_get_fullname(""y""), y_dist, obs=y)\n'"
pyro/contrib/gp/likelihoods/gaussian.py,7,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom pyro.contrib.gp.likelihoods.likelihood import Likelihood\nfrom pyro.nn.module import PyroParam\n\n\nclass Gaussian(Likelihood):\n    """"""\n    Implementation of Gaussian likelihood, which is used for regression problems.\n\n    Gaussian likelihood uses :class:`~pyro.distributions.Normal` distribution.\n\n    :param torch.Tensor variance: A variance parameter, which plays the role of\n        ``noise`` in regression problems.\n    """"""\n    def __init__(self, variance=None):\n        super().__init__()\n\n        variance = torch.tensor(1.) if variance is None else variance\n        self.variance = PyroParam(variance, constraints.positive)\n\n    def forward(self, f_loc, f_var, y=None):\n        r""""""\n        Samples :math:`y` given :math:`f_{loc}`, :math:`f_{var}` according to\n\n            .. math:: y \\sim \\mathbb{Normal}(f_{loc}, f_{var} + \\epsilon),\n\n        where :math:`\\epsilon` is the ``variance`` parameter of this likelihood.\n\n        :param torch.Tensor f_loc: Mean of latent function output.\n        :param torch.Tensor f_var: Variance of latent function output.\n        :param torch.Tensor y: Training output tensor.\n        :returns: a tensor sampled from likelihood\n        :rtype: torch.Tensor\n        """"""\n        y_var = f_var + self.variance\n\n        y_dist = dist.Normal(f_loc, y_var.sqrt())\n        if y is not None:\n            y_dist = y_dist.expand_by(y.shape[:-f_loc.dim()]).to_event(y.dim())\n        return pyro.sample(self._pyro_get_fullname(""y""), y_dist, obs=y)\n'"
pyro/contrib/gp/likelihoods/likelihood.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.gp.parameterized import Parameterized\n\n\nclass Likelihood(Parameterized):\n    """"""\n    Base class for likelihoods used in Gaussian Process.\n\n    Every inherited class should implement a forward pass which\n    takes an input :math:`f` and returns a sample :math:`y`.\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, f_loc, f_var, y=None):\n        """"""\n        Samples :math:`y` given :math:`f_{loc}`, :math:`f_{var}`.\n\n        :param torch.Tensor f_loc: Mean of latent function output.\n        :param torch.Tensor f_var: Variance of latent function output.\n        :param torch.Tensor y: Training output tensor.\n        :returns: a tensor sampled from likelihood\n        :rtype: torch.Tensor\n        """"""\n        raise NotImplementedError\n'"
pyro/contrib/gp/likelihoods/multi_class.py,5,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch.nn.functional as F\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom pyro.contrib.gp.likelihoods.likelihood import Likelihood\n\n\ndef _softmax(x):\n    return F.softmax(x, dim=-1)\n\n\nclass MultiClass(Likelihood):\n    """"""\n    Implementation of MultiClass likelihood, which is used for multi-class\n    classification problems.\n\n    MultiClass likelihood uses :class:`~pyro.distributions.Categorical`\n    distribution, so ``response_function`` should normalize its input\'s rightmost axis.\n    By default, we use `softmax` function.\n\n    :param int num_classes: Number of classes for prediction.\n    :param callable response_function: A mapping to correct domain for MultiClass\n        likelihood.\n    """"""\n    def __init__(self, num_classes, response_function=None):\n        super().__init__()\n        self.num_classes = num_classes\n        self.response_function = _softmax if response_function is None else response_function\n\n    def forward(self, f_loc, f_var, y=None):\n        r""""""\n        Samples :math:`y` given :math:`f_{loc}`, :math:`f_{var}` according to\n\n            .. math:: f & \\sim \\mathbb{Normal}(f_{loc}, f_{var}),\\\\\n                y & \\sim \\mathbb{Categorical}(f).\n\n        .. note:: The log likelihood is estimated using Monte Carlo with 1 sample of\n            :math:`f`.\n\n        :param torch.Tensor f_loc: Mean of latent function output.\n        :param torch.Tensor f_var: Variance of latent function output.\n        :param torch.Tensor y: Training output tensor.\n        :returns: a tensor sampled from likelihood\n        :rtype: torch.Tensor\n        """"""\n        # calculates Monte Carlo estimate for E_q(f) [logp(y | f)]\n        f = dist.Normal(f_loc, f_var.sqrt())()\n        if f.dim() < 2:\n            raise ValueError(""Latent function output should have at least 2 ""\n                             ""dimensions: one for number of classes and one for ""\n                             ""number of data."")\n\n        # swap class dimension and data dimension\n        f_swap = f.transpose(-2, -1)  # -> num_data x num_classes\n        if f_swap.size(-1) != self.num_classes:\n            raise ValueError(""Number of Gaussian processes should be equal to the ""\n                             ""number of classes. Expected {} but got {}.""\n                             .format(self.num_classes, f_swap.size(-1)))\n        if self.response_function is _softmax:\n            y_dist = dist.Categorical(logits=f_swap)\n        else:\n            f_res = self.response_function(f_swap)\n            y_dist = dist.Categorical(f_res)\n        if y is not None:\n            y_dist = y_dist.expand_by(y.shape[:-f.dim() + 1]).to_event(y.dim())\n        return pyro.sample(self._pyro_get_fullname(""y""), y_dist, obs=y)\n'"
pyro/contrib/gp/likelihoods/poisson.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom pyro.contrib.gp.likelihoods.likelihood import Likelihood\n\n\nclass Poisson(Likelihood):\n    """"""\n    Implementation of Poisson likelihood, which is used for count data.\n\n    Poisson likelihood uses the :class:`~pyro.distributions.Poisson`\n    distribution, so the output of ``response_function`` should be positive.\n    By default, we use :func:`torch.exp` as response function, corresponding\n    to a log-Gaussian Cox process.\n\n    :param callable response_function: A mapping to positive real numbers.\n    """"""\n    def __init__(self, response_function=None):\n        super().__init__()\n        self.response_function = torch.exp if response_function is None else response_function\n\n    def forward(self, f_loc, f_var, y=None):\n        r""""""\n        Samples :math:`y` given :math:`f_{loc}`, :math:`f_{var}` according to\n\n            .. math:: f & \\sim \\mathbb{Normal}(f_{loc}, f_{var}),\\\\\n                y & \\sim \\mathbb{Poisson}(\\exp(f)).\n\n        .. note:: The log likelihood is estimated using Monte Carlo with 1 sample of\n            :math:`f`.\n\n        :param torch.Tensor f_loc: Mean of latent function output.\n        :param torch.Tensor f_var: Variance of latent function output.\n        :param torch.Tensor y: Training output tensor.\n        :returns: a tensor sampled from likelihood\n        :rtype: torch.Tensor\n        """"""\n        # calculates Monte Carlo estimate for E_q(f) [logp(y | f)]\n        f = dist.Normal(f_loc, f_var.sqrt())()\n        f_res = self.response_function(f)\n\n        y_dist = dist.Poisson(f_res)\n        if y is not None:\n            y_dist = y_dist.expand_by(y.shape[:-f_res.dim()]).to_event(y.dim())\n        return pyro.sample(self._pyro_get_fullname(""y""), y_dist, obs=y)\n'"
pyro/contrib/gp/models/__init__.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.gp.models.gplvm import GPLVM\nfrom pyro.contrib.gp.models.gpr import GPRegression\nfrom pyro.contrib.gp.models.model import GPModel\nfrom pyro.contrib.gp.models.sgpr import SparseGPRegression\nfrom pyro.contrib.gp.models.vgp import VariationalGP\nfrom pyro.contrib.gp.models.vsgp import VariationalSparseGP\n\n__all__ = [\n    ""GPLVM"",\n    ""GPModel"",\n    ""GPRegression"",\n    ""SparseGPRegression"",\n    ""VariationalGP"",\n    ""VariationalSparseGP"",\n]\n'"
pyro/contrib/gp/models/gplvm.py,4,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro.distributions as dist\nfrom pyro.contrib.gp.parameterized import Parameterized\nfrom pyro.nn.module import PyroSample, pyro_method\n\n\nclass GPLVM(Parameterized):\n    """"""\n    Gaussian Process Latent Variable Model (GPLVM) model.\n\n    GPLVM is a Gaussian Process model with its train input data is a latent variable.\n    This model is useful for dimensional reduction of high dimensional data. Assume the\n    mapping from low dimensional latent variable to is a Gaussian Process instance.\n    Then the high dimensional data will play the role of train output ``y`` and our\n    target is to learn latent inputs which best explain ``y``. For the purpose of\n    dimensional reduction, latent inputs should have lower dimensions than ``y``.\n\n    We follows reference [1] to put a unit Gaussian prior to the input and approximate\n    its posterior by a multivariate normal distribution with two variational\n    parameters: ``X_loc`` and ``X_scale_tril``.\n\n    For example, we can do dimensional reduction on Iris dataset as follows:\n\n        >>> # With y as the 2D Iris data of shape 150x4 and we want to reduce its dimension\n        >>> # to a tensor X of shape 150x2, we will use GPLVM.\n\n        .. doctest::\n           :hide:\n\n            >>> # Simulating iris data.\n            >>> y = torch.stack([dist.Normal(4.8, 0.1).sample((150,)),\n            ...                  dist.Normal(3.2, 0.3).sample((150,)),\n            ...                  dist.Normal(1.5, 0.4).sample((150,)),\n            ...                  dist.Exponential(0.5).sample((150,))])\n\n        >>> # First, define the initial values for X parameter:\n        >>> X_init = torch.zeros(150, 2)\n        >>> # Then, define a Gaussian Process model with input X_init and output y:\n        >>> kernel = gp.kernels.RBF(input_dim=2, lengthscale=torch.ones(2))\n        >>> Xu = torch.zeros(20, 2)  # initial inducing inputs of sparse model\n        >>> gpmodule = gp.models.SparseGPRegression(X_init, y, kernel, Xu)\n        >>> # Finally, wrap gpmodule by GPLVM, optimize, and get the ""learned"" mean of X:\n        >>> gplvm = gp.models.GPLVM(gpmodule)\n        >>> gp.util.train(gplvm)  # doctest: +SKIP\n        >>> X = gplvm.X\n\n    Reference:\n\n    [1] Bayesian Gaussian Process Latent Variable Model\n    Michalis K. Titsias, Neil D. Lawrence\n\n    :param ~pyro.contrib.gp.models.model.GPModel base_model: A Pyro Gaussian Process\n        model object. Note that ``base_model.X`` will be the initial value for the\n        variational parameter ``X_loc``.\n    """"""\n    def __init__(self, base_model):\n        super().__init__()\n        if base_model.X.dim() != 2:\n            raise ValueError(""GPLVM model only works with 2D latent X, but got ""\n                             ""X.dim() = {}."".format(base_model.X.dim()))\n        self.base_model = base_model\n\n        self.X = PyroSample(dist.Normal(base_model.X.new_zeros(base_model.X.shape), 1.).to_event())\n        self.autoguide(""X"", dist.Normal)\n        self.X_loc.data = base_model.X\n\n    @pyro_method\n    def model(self):\n        self.mode = ""model""\n        # X is sampled from prior will be put into base_model\n        self.base_model.set_data(self.X, self.base_model.y)\n        self.base_model.model()\n\n    @pyro_method\n    def guide(self):\n        self.mode = ""guide""\n        # X is sampled from guide will be put into base_model\n        self.base_model.set_data(self.X, self.base_model.y)\n        self.base_model.guide()\n\n    def forward(self, **kwargs):\n        """"""\n        Forward method has the same signal as its ``base_model``. Note that the train\n        input data of ``base_model`` is sampled from GPLVM.\n        """"""\n        self.mode = ""guide""\n        self.base_model.set_data(self.X, self.base_model.y)\n        return self.base_model(**kwargs)\n'"
pyro/contrib/gp/models/gpr.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nimport torch.distributions as torchdist\nfrom torch.distributions import constraints\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.gp.models.model import GPModel\nfrom pyro.contrib.gp.util import conditional\nfrom pyro.nn.module import PyroParam, pyro_method\nfrom pyro.util import warn_if_nan\n\n\nclass GPRegression(GPModel):\n    r""""""\n    Gaussian Process Regression model.\n\n    The core of a Gaussian Process is a covariance function :math:`k` which governs\n    the similarity between input points. Given :math:`k`, we can establish a\n    distribution over functions :math:`f` by a multivarite normal distribution\n\n    .. math:: p(f(X)) = \\mathcal{N}(0, k(X, X)),\n\n    where :math:`X` is any set of input points and :math:`k(X, X)` is a covariance\n    matrix whose entries are outputs :math:`k(x, z)` of :math:`k` over input pairs\n    :math:`(x, z)`. This distribution is usually denoted by\n\n    .. math:: f \\sim \\mathcal{GP}(0, k).\n\n    .. note:: Generally, beside a covariance matrix :math:`k`, a Gaussian Process can\n        also be specified by a mean function :math:`m` (which is a zero-value function\n        by default). In that case, its distribution will be\n\n        .. math:: p(f(X)) = \\mathcal{N}(m(X), k(X, X)).\n\n    Given inputs :math:`X` and their noisy observations :math:`y`, the Gaussian Process\n    Regression model takes the form\n\n    .. math::\n        f &\\sim \\mathcal{GP}(0, k(X, X)),\\\\\n        y & \\sim f + \\epsilon,\n\n    where :math:`\\epsilon` is Gaussian noise.\n\n    .. note:: This model has :math:`\\mathcal{O}(N^3)` complexity for training,\n        :math:`\\mathcal{O}(N^3)` complexity for testing. Here, :math:`N` is the number\n        of train inputs.\n\n    Reference:\n\n    [1] `Gaussian Processes for Machine Learning`,\n    Carl E. Rasmussen, Christopher K. I. Williams\n\n    :param torch.Tensor X: A input data for training. Its first dimension is the number\n        of data points.\n    :param torch.Tensor y: An output data for training. Its last dimension is the\n        number of data points.\n    :param ~pyro.contrib.gp.kernels.kernel.Kernel kernel: A Pyro kernel object, which\n        is the covariance function :math:`k`.\n    :param torch.Tensor noise: Variance of Gaussian noise of this model.\n    :param callable mean_function: An optional mean function :math:`m` of this Gaussian\n        process. By default, we use zero mean.\n    :param float jitter: A small positive term which is added into the diagonal part of\n        a covariance matrix to help stablize its Cholesky decomposition.\n    """"""\n    def __init__(self, X, y, kernel, noise=None, mean_function=None, jitter=1e-6):\n        super().__init__(X, y, kernel, mean_function, jitter)\n\n        noise = self.X.new_tensor(1.) if noise is None else noise\n        self.noise = PyroParam(noise, constraints.positive)\n\n    @pyro_method\n    def model(self):\n        self.set_mode(""model"")\n\n        N = self.X.size(0)\n        Kff = self.kernel(self.X)\n        Kff.view(-1)[::N + 1] += self.jitter + self.noise  # add noise to diagonal\n        Lff = Kff.cholesky()\n\n        zero_loc = self.X.new_zeros(self.X.size(0))\n        f_loc = zero_loc + self.mean_function(self.X)\n        if self.y is None:\n            f_var = Lff.pow(2).sum(dim=-1)\n            return f_loc, f_var\n        else:\n            return pyro.sample(self._pyro_get_fullname(""y""),\n                               dist.MultivariateNormal(f_loc, scale_tril=Lff)\n                                   .expand_by(self.y.shape[:-1])\n                                   .to_event(self.y.dim() - 1),\n                               obs=self.y)\n\n    @pyro_method\n    def guide(self):\n        self.set_mode(""guide"")\n        self._load_pyro_samples()\n\n    def forward(self, Xnew, full_cov=False, noiseless=True):\n        r""""""\n        Computes the mean and covariance matrix (or variance) of Gaussian Process\n        posterior on a test input data :math:`X_{new}`:\n\n        .. math:: p(f^* \\mid X_{new}, X, y, k, \\epsilon) = \\mathcal{N}(loc, cov).\n\n        .. note:: The noise parameter ``noise`` (:math:`\\epsilon`) together with\n            kernel\'s parameters have been learned from a training procedure (MCMC or\n            SVI).\n\n        :param torch.Tensor Xnew: A input data for testing. Note that\n            ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``.\n        :param bool full_cov: A flag to decide if we want to predict full covariance\n            matrix or just variance.\n        :param bool noiseless: A flag to decide if we want to include noise in the\n            prediction output or not.\n        :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))`\n        :rtype: tuple(torch.Tensor, torch.Tensor)\n        """"""\n        self._check_Xnew_shape(Xnew)\n        self.set_mode(""guide"")\n\n        N = self.X.size(0)\n        Kff = self.kernel(self.X).contiguous()\n        Kff.view(-1)[::N + 1] += self.jitter + self.noise  # add noise to the diagonal\n        Lff = Kff.cholesky()\n\n        y_residual = self.y - self.mean_function(self.X)\n        loc, cov = conditional(Xnew, self.X, self.kernel, y_residual, None, Lff,\n                               full_cov, jitter=self.jitter)\n\n        if full_cov and not noiseless:\n            M = Xnew.size(0)\n            cov = cov.contiguous()\n            cov.view(-1, M * M)[:, ::M + 1] += self.noise  # add noise to the diagonal\n        if not full_cov and not noiseless:\n            cov = cov + self.noise\n\n        return loc + self.mean_function(Xnew), cov\n\n    def iter_sample(self, noiseless=True):\n        r""""""\n        Iteratively constructs a sample from the Gaussian Process posterior.\n\n        Recall that at test input points :math:`X_{new}`, the posterior is\n        multivariate Gaussian distributed with mean and covariance matrix\n        given by :func:`forward`.\n\n        This method samples lazily from this multivariate Gaussian. The advantage\n        of this approach is that later query points can depend upon earlier ones.\n        Particularly useful when the querying is to be done by an optimisation\n        routine.\n\n        .. note:: The noise parameter ``noise`` (:math:`\\epsilon`) together with\n            kernel\'s parameters have been learned from a training procedure (MCMC or\n            SVI).\n\n        :param bool noiseless: A flag to decide if we want to add sampling noise\n            to the samples beyond the noise inherent in the GP posterior.\n        :returns: sampler\n        :rtype: function\n        """"""\n        noise = self.noise.detach()\n        X = self.X.clone().detach()\n        y = self.y.clone().detach()\n        N = X.size(0)\n        Kff = self.kernel(X).contiguous()\n        Kff.view(-1)[::N + 1] += noise  # add noise to the diagonal\n\n        outside_vars = {""X"": X, ""y"": y, ""N"": N, ""Kff"": Kff}\n\n        def sample_next(xnew, outside_vars):\n            """"""Repeatedly samples from the Gaussian process posterior,\n            conditioning on previously sampled values.\n            """"""\n            warn_if_nan(xnew)\n\n            # Variables from outer scope\n            X, y, Kff = outside_vars[""X""], outside_vars[""y""], outside_vars[""Kff""]\n\n            # Compute Cholesky decomposition of kernel matrix\n            Lff = Kff.cholesky()\n            y_residual = y - self.mean_function(X)\n\n            # Compute conditional mean and variance\n            loc, cov = conditional(xnew, X, self.kernel, y_residual, None, Lff,\n                                   False, jitter=self.jitter)\n            if not noiseless:\n                cov = cov + noise\n\n            ynew = torchdist.Normal(loc + self.mean_function(xnew), cov.sqrt()).rsample()\n\n            # Update kernel matrix\n            N = outside_vars[""N""]\n            Kffnew = Kff.new_empty(N+1, N+1)\n            Kffnew[:N, :N] = Kff\n            cross = self.kernel(X, xnew).squeeze()\n            end = self.kernel(xnew, xnew).squeeze()\n            Kffnew[N, :N] = cross\n            Kffnew[:N, N] = cross\n            # No noise, just jitter for numerical stability\n            Kffnew[N, N] = end + self.jitter\n            # Heuristic to avoid adding degenerate points\n            if Kffnew.logdet() > -15.:\n                outside_vars[""Kff""] = Kffnew\n                outside_vars[""N""] += 1\n                outside_vars[""X""] = torch.cat((X, xnew))\n                outside_vars[""y""] = torch.cat((y, ynew))\n\n            return ynew\n\n        return lambda xnew: sample_next(xnew, outside_vars)\n'"
pyro/contrib/gp/models/model.py,20,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyro.contrib.gp.parameterized import Parameterized\n\n\ndef _zero_mean_function(x):\n    return 0\n\n\nclass GPModel(Parameterized):\n    r""""""\n    Base class for Gaussian Process models.\n\n    The core of a Gaussian Process is a covariance function :math:`k` which governs\n    the similarity between input points. Given :math:`k`, we can establish a\n    distribution over functions :math:`f` by a multivarite normal distribution\n\n    .. math:: p(f(X)) = \\mathcal{N}(0, k(X, X)),\n\n    where :math:`X` is any set of input points and :math:`k(X, X)` is a covariance\n    matrix whose entries are outputs :math:`k(x, z)` of :math:`k` over input pairs\n    :math:`(x, z)`. This distribution is usually denoted by\n\n    .. math:: f \\sim \\mathcal{GP}(0, k).\n\n    .. note:: Generally, beside a covariance matrix :math:`k`, a Gaussian Process can\n        also be specified by a mean function :math:`m` (which is a zero-value function\n        by default). In that case, its distribution will be\n\n        .. math:: p(f(X)) = \\mathcal{N}(m(X), k(X, X)).\n\n    Gaussian Process models are :class:`~pyro.contrib.gp.util.Parameterized`\n    subclasses. So its parameters can be learned, set priors, or fixed by using\n    corresponding methods from :class:`~pyro.contrib.gp.util.Parameterized`. A typical\n    way to define a Gaussian Process model is\n\n        >>> X = torch.tensor([[1., 5, 3], [4, 3, 7]])\n        >>> y = torch.tensor([2., 1])\n        >>> kernel = gp.kernels.RBF(input_dim=3)\n        >>> kernel.variance = pyro.nn.PyroSample(dist.Uniform(torch.tensor(0.5), torch.tensor(1.5)))\n        >>> kernel.lengthscale = pyro.nn.PyroSample(dist.Uniform(torch.tensor(1.0), torch.tensor(3.0)))\n        >>> gpr = gp.models.GPRegression(X, y, kernel)\n\n    There are two ways to train a Gaussian Process model:\n\n    + Using an MCMC algorithm (in module :mod:`pyro.infer.mcmc`) on :meth:`model` to\n      get posterior samples for the Gaussian Process\'s parameters. For example:\n\n        >>> hmc_kernel = HMC(gpr.model)\n        >>> mcmc = MCMC(hmc_kernel, num_samples=10)\n        >>> mcmc.run()\n        >>> ls_name = ""kernel.lengthscale""\n        >>> posterior_ls = mcmc.get_samples()[ls_name]\n\n    + Using a variational inference on the pair :meth:`model`, :meth:`guide`:\n\n        >>> optimizer = torch.optim.Adam(gpr.parameters(), lr=0.01)\n        >>> loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss\n        >>>\n        >>> for i in range(1000):\n        ...     svi.step()  # doctest: +SKIP\n        ...     optimizer.zero_grad()\n        ...     loss = loss_fn(gpr.model, gpr.guide)  # doctest: +SKIP\n        ...     loss.backward()  # doctest: +SKIP\n        ...     optimizer.step()\n\n    To give a prediction on new dataset, simply use :meth:`forward` like any PyTorch\n    :class:`torch.nn.Module`:\n\n        >>> Xnew = torch.tensor([[2., 3, 1]])\n        >>> f_loc, f_cov = gpr(Xnew, full_cov=True)\n\n    Reference:\n\n    [1] `Gaussian Processes for Machine Learning`,\n    Carl E. Rasmussen, Christopher K. I. Williams\n\n    :param torch.Tensor X: A input data for training. Its first dimension is the number\n        of data points.\n    :param torch.Tensor y: An output data for training. Its last dimension is the\n        number of data points.\n    :param ~pyro.contrib.gp.kernels.kernel.Kernel kernel: A Pyro kernel object, which\n        is the covariance function :math:`k`.\n    :param callable mean_function: An optional mean function :math:`m` of this Gaussian\n        process. By default, we use zero mean.\n    :param float jitter: A small positive term which is added into the diagonal part of\n        a covariance matrix to help stablize its Cholesky decomposition.\n    """"""\n    def __init__(self, X, y, kernel, mean_function=None, jitter=1e-6):\n        super().__init__()\n        self.set_data(X, y)\n        self.kernel = kernel\n        self.mean_function = (mean_function if mean_function is not None else\n                              _zero_mean_function)\n        self.jitter = jitter\n\n    def model(self):\n        """"""\n        A ""model"" stochastic function. If ``self.y`` is ``None``, this method returns\n        mean and variance of the Gaussian Process prior.\n        """"""\n        raise NotImplementedError\n\n    def guide(self):\n        """"""\n        A ""guide"" stochastic function to be used in variational inference methods. It\n        also gives posterior information to the method :meth:`forward` for prediction.\n        """"""\n        raise NotImplementedError\n\n    def forward(self, Xnew, full_cov=False):\n        r""""""\n        Computes the mean and covariance matrix (or variance) of Gaussian Process\n        posterior on a test input data :math:`X_{new}`:\n\n        .. math:: p(f^* \\mid X_{new}, X, y, k, \\theta),\n\n        where :math:`\\theta` are parameters of this model.\n\n        .. note:: Model\'s parameters :math:`\\theta` together with kernel\'s parameters\n            have been learned from a training procedure (MCMC or SVI).\n\n        :param torch.Tensor Xnew: A input data for testing. Note that\n            ``Xnew.shape[1:]`` must be the same as ``X.shape[1:]``.\n        :param bool full_cov: A flag to decide if we want to predict full covariance\n            matrix or just variance.\n        :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))`\n        :rtype: tuple(torch.Tensor, torch.Tensor)\n        """"""\n        raise NotImplementedError\n\n    def set_data(self, X, y=None):\n        """"""\n        Sets data for Gaussian Process models.\n\n        Some examples to utilize this method are:\n\n        .. doctest::\n           :hide:\n\n            >>> X = torch.tensor([[1., 5, 3], [4, 3, 7]])\n            >>> y = torch.tensor([2., 1])\n            >>> kernel = gp.kernels.RBF(input_dim=3)\n            >>> kernel.variance = pyro.nn.PyroSample(dist.Uniform(torch.tensor(0.5), torch.tensor(1.5)))\n            >>> kernel.lengthscale = pyro.nn.PyroSample(dist.Uniform(torch.tensor(1.0), torch.tensor(3.0)))\n\n        + Batch training on a sparse variational model:\n\n            >>> Xu = torch.tensor([[1., 0, 2]])  # inducing input\n            >>> likelihood = gp.likelihoods.Gaussian()\n            >>> vsgp = gp.models.VariationalSparseGP(X, y, kernel, Xu, likelihood)\n            >>> optimizer = torch.optim.Adam(vsgp.parameters(), lr=0.01)\n            >>> loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss\n            >>> batched_X, batched_y = X.split(split_size=10), y.split(split_size=10)\n            >>> for Xi, yi in zip(batched_X, batched_y):\n            ...     optimizer.zero_grad()\n            ...     vsgp.set_data(Xi, yi)\n            ...     svi.step()  # doctest: +SKIP\n            ...     loss = loss_fn(vsgp.model, vsgp.guide)  # doctest: +SKIP\n            ...     loss.backward()  # doctest: +SKIP\n            ...     optimizer.step()\n\n        + Making a two-layer Gaussian Process stochastic function:\n\n            >>> gpr1 = gp.models.GPRegression(X, None, kernel)\n            >>> Z, _ = gpr1.model()\n            >>> gpr2 = gp.models.GPRegression(Z, y, kernel)\n            >>> def two_layer_model():\n            ...     Z, _ = gpr1.model()\n            ...     gpr2.set_data(Z, y)\n            ...     return gpr2.model()\n\n        References:\n\n        [1] `Scalable Variational Gaussian Process Classification`,\n        James Hensman, Alexander G. de G. Matthews, Zoubin Ghahramani\n\n        [2] `Deep Gaussian Processes`,\n        Andreas C. Damianou, Neil D. Lawrence\n\n        :param torch.Tensor X: A input data for training. Its first dimension is the\n            number of data points.\n        :param torch.Tensor y: An output data for training. Its last dimension is the\n            number of data points.\n        """"""\n        if y is not None and X.size(0) != y.size(-1):\n            raise ValueError(""Expected the number of input data points equal to the ""\n                             ""number of output data points, but got {} and {}.""\n                             .format(X.size(0), y.size(-1)))\n        self.X = X\n        self.y = y\n\n    def _check_Xnew_shape(self, Xnew):\n        """"""\n        Checks the correction of the shape of new data.\n\n        :param torch.Tensor Xnew: A input data for testing. Note that\n            ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``.\n        """"""\n        if Xnew.dim() != self.X.dim():\n            raise ValueError(""Train data and test data should have the same ""\n                             ""number of dimensions, but got {} and {}.""\n                             .format(self.X.dim(), Xnew.dim()))\n        if self.X.shape[1:] != Xnew.shape[1:]:\n            raise ValueError(""Train data and test data should have the same ""\n                             ""shape of features, but got {} and {}.""\n                             .format(self.X.shape[1:], Xnew.shape[1:]))\n'"
pyro/contrib/gp/models/sgpr.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.nn import Parameter\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.gp.models.model import GPModel\nfrom pyro.nn.module import PyroParam, pyro_method\n\n\nclass SparseGPRegression(GPModel):\n    u""""""\n    Sparse Gaussian Process Regression model.\n\n    In :class:`.GPRegression` model, when the number of input data :math:`X` is large,\n    the covariance matrix :math:`k(X, X)` will require a lot of computational steps to\n    compute its inverse (for log likelihood and for prediction). By introducing an\n    additional inducing-input parameter :math:`X_u`, we can reduce computational cost\n    by approximate :math:`k(X, X)` by a low-rank Nymstr\\u00F6m approximation :math:`Q`\n    (see reference [1]), where\n\n    .. math:: Q = k(X, X_u) k(X,X)^{-1} k(X_u, X).\n\n    Given inputs :math:`X`, their noisy observations :math:`y`, and the inducing-input\n    parameters :math:`X_u`, the model takes the form:\n\n    .. math::\n        u & \\\\sim \\\\mathcal{GP}(0, k(X_u, X_u)),\\\\\\\\\n        f & \\\\sim q(f \\\\mid X, X_u) = \\\\mathbb{E}_{p(u)}q(f\\\\mid X, X_u, u),\\\\\\\\\n        y & \\\\sim f + \\\\epsilon,\n\n    where :math:`\\\\epsilon` is Gaussian noise and the conditional distribution\n    :math:`q(f\\\\mid X, X_u, u)` is an approximation of\n\n    .. math:: p(f\\\\mid X, X_u, u) = \\\\mathcal{N}(m, k(X, X) - Q),\n\n    whose terms :math:`m` and :math:`k(X, X) - Q` is derived from the joint\n    multivariate normal distribution:\n\n    .. math:: [f, u] \\\\sim \\\\mathcal{GP}(0, k([X, X_u], [X, X_u])).\n\n    This class implements three approximation methods:\n\n    + Deterministic Training Conditional (DTC):\n\n        .. math:: q(f\\\\mid X, X_u, u) = \\\\mathcal{N}(m, 0),\n\n      which in turns will imply\n\n        .. math:: f \\\\sim \\\\mathcal{N}(0, Q).\n\n    + Fully Independent Training Conditional (FITC):\n\n        .. math:: q(f\\\\mid X, X_u, u) = \\\\mathcal{N}(m, diag(k(X, X) - Q)),\n\n      which in turns will correct the diagonal part of the approximation in DTC:\n\n        .. math:: f \\\\sim \\\\mathcal{N}(0, Q + diag(k(X, X) - Q)).\n\n    + Variational Free Energy (VFE), which is similar to DTC but has an additional\n      `trace_term` in the model\'s log likelihood. This additional term makes ""VFE""\n      equivalent to the variational approach in :class:`.SparseVariationalGP`\n      (see reference [2]).\n\n    .. note:: This model has :math:`\\\\mathcal{O}(NM^2)` complexity for training,\n        :math:`\\\\mathcal{O}(NM^2)` complexity for testing. Here, :math:`N` is the number\n        of train inputs, :math:`M` is the number of inducing inputs.\n\n    References:\n\n    [1] `A Unifying View of Sparse Approximate Gaussian Process Regression`,\n    Joaquin Qui\\u00F1onero-Candela, Carl E. Rasmussen\n\n    [2] `Variational learning of inducing variables in sparse Gaussian processes`,\n    Michalis Titsias\n\n    :param torch.Tensor X: A input data for training. Its first dimension is the number\n        of data points.\n    :param torch.Tensor y: An output data for training. Its last dimension is the\n        number of data points.\n    :param ~pyro.contrib.gp.kernels.kernel.Kernel kernel: A Pyro kernel object, which\n        is the covariance function :math:`k`.\n    :param torch.Tensor Xu: Initial values for inducing points, which are parameters\n        of our model.\n    :param torch.Tensor noise: Variance of Gaussian noise of this model.\n    :param callable mean_function: An optional mean function :math:`m` of this Gaussian\n        process. By default, we use zero mean.\n    :param str approx: One of approximation methods: ""DTC"", ""FITC"", and ""VFE""\n        (default).\n    :param float jitter: A small positive term which is added into the diagonal part of\n        a covariance matrix to help stablize its Cholesky decomposition.\n    :param str name: Name of this model.\n    """"""\n    def __init__(self, X, y, kernel, Xu, noise=None, mean_function=None, approx=None, jitter=1e-6):\n        super().__init__(X, y, kernel, mean_function, jitter)\n\n        self.Xu = Parameter(Xu)\n\n        noise = self.X.new_tensor(1.) if noise is None else noise\n        self.noise = PyroParam(noise, constraints.positive)\n\n        if approx is None:\n            self.approx = ""VFE""\n        elif approx in [""DTC"", ""FITC"", ""VFE""]:\n            self.approx = approx\n        else:\n            raise ValueError(""The sparse approximation method should be one of ""\n                             ""\'DTC\', \'FITC\', \'VFE\'."")\n\n    @pyro_method\n    def model(self):\n        self.set_mode(""model"")\n\n        # W = (inv(Luu) @ Kuf).T\n        # Qff = Kfu @ inv(Kuu) @ Kuf = W @ W.T\n        # Fomulas for each approximation method are\n        # DTC:  y_cov = Qff + noise,                   trace_term = 0\n        # FITC: y_cov = Qff + diag(Kff - Qff) + noise, trace_term = 0\n        # VFE:  y_cov = Qff + noise,                   trace_term = tr(Kff-Qff) / noise\n        # y_cov = W @ W.T + D\n        # trace_term is added into log_prob\n\n        N = self.X.size(0)\n        M = self.Xu.size(0)\n        Kuu = self.kernel(self.Xu).contiguous()\n        Kuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal\n        Luu = Kuu.cholesky()\n        Kuf = self.kernel(self.Xu, self.X)\n        W = Kuf.triangular_solve(Luu, upper=False)[0].t()\n\n        D = self.noise.expand(N)\n        if self.approx == ""FITC"" or self.approx == ""VFE"":\n            Kffdiag = self.kernel(self.X, diag=True)\n            Qffdiag = W.pow(2).sum(dim=-1)\n            if self.approx == ""FITC"":\n                D = D + Kffdiag - Qffdiag\n            else:  # approx = ""VFE""\n                trace_term = (Kffdiag - Qffdiag).sum() / self.noise\n                trace_term = trace_term.clamp(min=0)\n\n        zero_loc = self.X.new_zeros(N)\n        f_loc = zero_loc + self.mean_function(self.X)\n        if self.y is None:\n            f_var = D + W.pow(2).sum(dim=-1)\n            return f_loc, f_var\n        else:\n            if self.approx == ""VFE"":\n                pyro.factor(self._pyro_get_fullname(""trace_term""), -trace_term / 2.)\n\n            return pyro.sample(self._pyro_get_fullname(""y""),\n                               dist.LowRankMultivariateNormal(f_loc, W, D)\n                                   .expand_by(self.y.shape[:-1])\n                                   .to_event(self.y.dim() - 1),\n                               obs=self.y)\n\n    @pyro_method\n    def guide(self):\n        self.set_mode(""guide"")\n        self._load_pyro_samples()\n\n    def forward(self, Xnew, full_cov=False, noiseless=True):\n        r""""""\n        Computes the mean and covariance matrix (or variance) of Gaussian Process\n        posterior on a test input data :math:`X_{new}`:\n\n        .. math:: p(f^* \\mid X_{new}, X, y, k, X_u, \\epsilon) = \\mathcal{N}(loc, cov).\n\n        .. note:: The noise parameter ``noise`` (:math:`\\epsilon`), the inducing-point\n            parameter ``Xu``, together with kernel\'s parameters have been learned from\n            a training procedure (MCMC or SVI).\n\n        :param torch.Tensor Xnew: A input data for testing. Note that\n            ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``.\n        :param bool full_cov: A flag to decide if we want to predict full covariance\n            matrix or just variance.\n        :param bool noiseless: A flag to decide if we want to include noise in the\n            prediction output or not.\n        :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))`\n        :rtype: tuple(torch.Tensor, torch.Tensor)\n        """"""\n        self._check_Xnew_shape(Xnew)\n        self.set_mode(""guide"")\n\n        # W = inv(Luu) @ Kuf\n        # Ws = inv(Luu) @ Kus\n        # D as in self.model()\n        # K = I + W @ inv(D) @ W.T = L @ L.T\n        # S = inv[Kuu + Kuf @ inv(D) @ Kfu]\n        #   = inv(Luu).T @ inv[I + inv(Luu)@ Kuf @ inv(D)@ Kfu @ inv(Luu).T] @ inv(Luu)\n        #   = inv(Luu).T @ inv[I + W @ inv(D) @ W.T] @ inv(Luu)\n        #   = inv(Luu).T @ inv(K) @ inv(Luu)\n        #   = inv(Luu).T @ inv(L).T @ inv(L) @ inv(Luu)\n        # loc = Ksu @ S @ Kuf @ inv(D) @ y = Ws.T @ inv(L).T @ inv(L) @ W @ inv(D) @ y\n        # cov = Kss - Ksu @ inv(Kuu) @ Kus + Ksu @ S @ Kus\n        #     = kss - Ksu @ inv(Kuu) @ Kus + Ws.T @ inv(L).T @ inv(L) @ Ws\n\n        N = self.X.size(0)\n        M = self.Xu.size(0)\n\n        # TODO: cache these calculations to get faster inference\n\n        Kuu = self.kernel(self.Xu).contiguous()\n        Kuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal\n        Luu = Kuu.cholesky()\n\n        Kuf = self.kernel(self.Xu, self.X)\n\n        W = Kuf.triangular_solve(Luu, upper=False)[0]\n        D = self.noise.expand(N)\n        if self.approx == ""FITC"":\n            Kffdiag = self.kernel(self.X, diag=True)\n            Qffdiag = W.pow(2).sum(dim=0)\n            D = D + Kffdiag - Qffdiag\n\n        W_Dinv = W / D\n        K = W_Dinv.matmul(W.t()).contiguous()\n        K.view(-1)[::M + 1] += 1  # add identity matrix to K\n        L = K.cholesky()\n\n        # get y_residual and convert it into 2D tensor for packing\n        y_residual = self.y - self.mean_function(self.X)\n        y_2D = y_residual.reshape(-1, N).t()\n        W_Dinv_y = W_Dinv.matmul(y_2D)\n\n        # End caching ----------\n\n        Kus = self.kernel(self.Xu, Xnew)\n        Ws = Kus.triangular_solve(Luu, upper=False)[0]\n        pack = torch.cat((W_Dinv_y, Ws), dim=1)\n        Linv_pack = pack.triangular_solve(L, upper=False)[0]\n        # unpack\n        Linv_W_Dinv_y = Linv_pack[:, :W_Dinv_y.shape[1]]\n        Linv_Ws = Linv_pack[:, W_Dinv_y.shape[1]:]\n\n        C = Xnew.size(0)\n        loc_shape = self.y.shape[:-1] + (C,)\n        loc = Linv_W_Dinv_y.t().matmul(Linv_Ws).reshape(loc_shape)\n\n        if full_cov:\n            Kss = self.kernel(Xnew).contiguous()\n            if not noiseless:\n                Kss.view(-1)[::C + 1] += self.noise  # add noise to the diagonal\n            Qss = Ws.t().matmul(Ws)\n            cov = Kss - Qss + Linv_Ws.t().matmul(Linv_Ws)\n            cov_shape = self.y.shape[:-1] + (C, C)\n            cov = cov.expand(cov_shape)\n        else:\n            Kssdiag = self.kernel(Xnew, diag=True)\n            if not noiseless:\n                Kssdiag = Kssdiag + self.noise\n            Qssdiag = Ws.pow(2).sum(dim=0)\n            cov = Kssdiag - Qssdiag + Linv_Ws.pow(2).sum(dim=0)\n            cov_shape = self.y.shape[:-1] + (C,)\n            cov = cov.expand(cov_shape)\n\n        return loc + self.mean_function(Xnew), cov\n'"
pyro/contrib/gp/models/vgp.py,8,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.nn import Parameter\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.gp.models.model import GPModel\nfrom pyro.contrib.gp.util import conditional\nfrom pyro.distributions.util import eye_like\nfrom pyro.nn.module import PyroParam, pyro_method\n\n\nclass VariationalGP(GPModel):\n    r""""""\n    Variational Gaussian Process model.\n\n    This model deals with both Gaussian and non-Gaussian likelihoods. Given inputs\\\n    :math:`X` and their noisy observations :math:`y`, the model takes the form\n\n    .. math::\n        f &\\sim \\mathcal{GP}(0, k(X, X)),\\\\\n        y & \\sim p(y) = p(y \\mid f) p(f),\n\n    where :math:`p(y \\mid f)` is the likelihood.\n\n    We will use a variational approach in this model by approximating :math:`q(f)` to\n    the posterior :math:`p(f\\mid y)`. Precisely, :math:`q(f)` will be a multivariate\n    normal distribution with two parameters ``f_loc`` and ``f_scale_tril``, which will\n    be learned during a variational inference process.\n\n    .. note:: This model can be seen as a special version of\n        :class:`.SparseVariationalGP` model with :math:`X_u = X`.\n\n    .. note:: This model has :math:`\\mathcal{O}(N^3)` complexity for training,\n        :math:`\\mathcal{O}(N^3)` complexity for testing. Here, :math:`N` is the number\n        of train inputs. Size of variational parameters is :math:`\\mathcal{O}(N^2)`.\n\n    :param torch.Tensor X: A input data for training. Its first dimension is the number\n        of data points.\n    :param torch.Tensor y: An output data for training. Its last dimension is the\n        number of data points.\n    :param ~pyro.contrib.gp.kernels.kernel.Kernel kernel: A Pyro kernel object, which\n        is the covariance function :math:`k`.\n    :param ~pyro.contrib.gp.likelihoods.likelihood Likelihood likelihood: A likelihood\n        object.\n    :param callable mean_function: An optional mean function :math:`m` of this Gaussian\n        process. By default, we use zero mean.\n    :param torch.Size latent_shape: Shape for latent processes (`batch_shape` of\n        :math:`q(f)`). By default, it equals to output batch shape ``y.shape[:-1]``.\n        For the multi-class classification problems, ``latent_shape[-1]`` should\n        corresponse to the number of classes.\n    :param bool whiten: A flag to tell if variational parameters ``f_loc`` and\n        ``f_scale_tril`` are transformed by the inverse of ``Lff``, where ``Lff`` is\n        the lower triangular decomposition of :math:`kernel(X, X)`. Enable this flag\n        will help optimization.\n    :param float jitter: A small positive term which is added into the diagonal part of\n        a covariance matrix to help stablize its Cholesky decomposition.\n    """"""\n    def __init__(self, X, y, kernel, likelihood, mean_function=None,\n                 latent_shape=None, whiten=False, jitter=1e-6):\n        super().__init__(X, y, kernel, mean_function, jitter)\n\n        self.likelihood = likelihood\n\n        y_batch_shape = self.y.shape[:-1] if self.y is not None else torch.Size([])\n        self.latent_shape = latent_shape if latent_shape is not None else y_batch_shape\n\n        N = self.X.size(0)\n        f_loc = self.X.new_zeros(self.latent_shape + (N,))\n        self.f_loc = Parameter(f_loc)\n\n        identity = eye_like(self.X, N)\n        f_scale_tril = identity.repeat(self.latent_shape + (1, 1))\n        self.f_scale_tril = PyroParam(f_scale_tril, constraints.lower_cholesky)\n\n        self.whiten = whiten\n        self._sample_latent = True\n\n    @pyro_method\n    def model(self):\n        self.set_mode(""model"")\n\n        N = self.X.size(0)\n        Kff = self.kernel(self.X).contiguous()\n        Kff.view(-1)[::N + 1] += self.jitter  # add jitter to the diagonal\n        Lff = Kff.cholesky()\n\n        zero_loc = self.X.new_zeros(self.f_loc.shape)\n        if self.whiten:\n            identity = eye_like(self.X, N)\n            pyro.sample(self._pyro_get_fullname(""f""),\n                        dist.MultivariateNormal(zero_loc, scale_tril=identity)\n                            .to_event(zero_loc.dim() - 1))\n            f_scale_tril = Lff.matmul(self.f_scale_tril)\n            f_loc = Lff.matmul(self.f_loc.unsqueeze(-1)).squeeze(-1)\n        else:\n            pyro.sample(self._pyro_get_fullname(""f""),\n                        dist.MultivariateNormal(zero_loc, scale_tril=Lff)\n                            .to_event(zero_loc.dim() - 1))\n            f_scale_tril = self.f_scale_tril\n            f_loc = self.f_loc\n\n        f_loc = f_loc + self.mean_function(self.X)\n        f_var = f_scale_tril.pow(2).sum(dim=-1)\n        if self.y is None:\n            return f_loc, f_var\n        else:\n            return self.likelihood(f_loc, f_var, self.y)\n\n    @pyro_method\n    def guide(self):\n        self.set_mode(""guide"")\n        self._load_pyro_samples()\n\n        pyro.sample(self._pyro_get_fullname(""f""),\n                    dist.MultivariateNormal(self.f_loc, scale_tril=self.f_scale_tril)\n                        .to_event(self.f_loc.dim()-1))\n\n    def forward(self, Xnew, full_cov=False):\n        r""""""\n        Computes the mean and covariance matrix (or variance) of Gaussian Process\n        posterior on a test input data :math:`X_{new}`:\n\n        .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale\\_tril})\n            = \\mathcal{N}(loc, cov).\n\n        .. note:: Variational parameters ``f_loc``, ``f_scale_tril``, together with\n            kernel\'s parameters have been learned from a training procedure (MCMC or\n            SVI).\n\n        :param torch.Tensor Xnew: A input data for testing. Note that\n            ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``.\n        :param bool full_cov: A flag to decide if we want to predict full covariance\n            matrix or just variance.\n        :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))`\n        :rtype: tuple(torch.Tensor, torch.Tensor)\n        """"""\n        self._check_Xnew_shape(Xnew)\n        self.set_mode(""guide"")\n\n        loc, cov = conditional(Xnew, self.X, self.kernel, self.f_loc, self.f_scale_tril,\n                               full_cov=full_cov, whiten=self.whiten, jitter=self.jitter)\n        return loc + self.mean_function(Xnew), cov\n'"
pyro/contrib/gp/models/vsgp.py,9,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.nn import Parameter\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.poutine as poutine\nfrom pyro.contrib.gp.models.model import GPModel\nfrom pyro.contrib.gp.util import conditional\nfrom pyro.distributions.util import eye_like\nfrom pyro.nn.module import PyroParam, pyro_method\n\n\nclass VariationalSparseGP(GPModel):\n    r""""""\n    Variational Sparse Gaussian Process model.\n\n    In :class:`.VariationalGP` model, when the number of input data :math:`X` is large,\n    the covariance matrix :math:`k(X, X)` will require a lot of computational steps to\n    compute its inverse (for log likelihood and for prediction). This model introduces\n    an additional inducing-input parameter :math:`X_u` to solve that problem. Given\n    inputs :math:`X`, their noisy observations :math:`y`, and the inducing-input\n    parameters :math:`X_u`, the model takes the form:\n\n    .. math::\n        [f, u] &\\sim \\mathcal{GP}(0, k([X, X_u], [X, X_u])),\\\\\n        y & \\sim p(y) = p(y \\mid f) p(f),\n\n    where :math:`p(y \\mid f)` is the likelihood.\n\n    We will use a variational approach in this model by approximating :math:`q(f,u)`\n    to the posterior :math:`p(f,u \\mid y)`. Precisely, :math:`q(f) = p(f\\mid u)q(u)`,\n    where :math:`q(u)` is a multivariate normal distribution with two parameters\n    ``u_loc`` and ``u_scale_tril``, which will be learned during a variational\n    inference process.\n\n    .. note:: This model can be learned using MCMC method as in reference [2]. See also\n        :class:`.GPModel`.\n\n    .. note:: This model has :math:`\\mathcal{O}(NM^2)` complexity for training,\n        :math:`\\mathcal{O}(M^3)` complexity for testing. Here, :math:`N` is the number\n        of train inputs, :math:`M` is the number of inducing inputs. Size of\n        variational parameters is :math:`\\mathcal{O}(M^2)`.\n\n    References:\n\n    [1] `Scalable variational Gaussian process classification`,\n    James Hensman, Alexander G. de G. Matthews, Zoubin Ghahramani\n\n    [2] `MCMC for Variationally Sparse Gaussian Processes`,\n    James Hensman, Alexander G. de G. Matthews, Maurizio Filippone, Zoubin Ghahramani\n\n    :param torch.Tensor X: A input data for training. Its first dimension is the number\n        of data points.\n    :param torch.Tensor y: An output data for training. Its last dimension is the\n        number of data points.\n    :param ~pyro.contrib.gp.kernels.kernel.Kernel kernel: A Pyro kernel object, which\n        is the covariance function :math:`k`.\n    :param torch.Tensor Xu: Initial values for inducing points, which are parameters\n        of our model.\n    :param ~pyro.contrib.gp.likelihoods.likelihood Likelihood likelihood: A likelihood\n        object.\n    :param callable mean_function: An optional mean function :math:`m` of this Gaussian\n        process. By default, we use zero mean.\n    :param torch.Size latent_shape: Shape for latent processes (`batch_shape` of\n        :math:`q(u)`). By default, it equals to output batch shape ``y.shape[:-1]``.\n        For the multi-class classification problems, ``latent_shape[-1]`` should\n        corresponse to the number of classes.\n    :param int num_data: The size of full training dataset. It is useful for training\n        this model with mini-batch.\n    :param bool whiten: A flag to tell if variational parameters ``u_loc`` and\n        ``u_scale_tril`` are transformed by the inverse of ``Luu``, where ``Luu`` is\n        the lower triangular decomposition of :math:`kernel(X_u, X_u)`. Enable this\n        flag will help optimization.\n    :param float jitter: A small positive term which is added into the diagonal part of\n        a covariance matrix to help stablize its Cholesky decomposition.\n    """"""\n    def __init__(self, X, y, kernel, Xu, likelihood, mean_function=None,\n                 latent_shape=None, num_data=None, whiten=False, jitter=1e-6):\n        super().__init__(X, y, kernel, mean_function, jitter)\n\n        self.likelihood = likelihood\n        self.Xu = Parameter(Xu)\n\n        y_batch_shape = self.y.shape[:-1] if self.y is not None else torch.Size([])\n        self.latent_shape = latent_shape if latent_shape is not None else y_batch_shape\n\n        M = self.Xu.size(0)\n        u_loc = self.Xu.new_zeros(self.latent_shape + (M,))\n        self.u_loc = Parameter(u_loc)\n\n        identity = eye_like(self.Xu, M)\n        u_scale_tril = identity.repeat(self.latent_shape + (1, 1))\n        self.u_scale_tril = PyroParam(u_scale_tril, constraints.lower_cholesky)\n\n        self.num_data = num_data if num_data is not None else self.X.size(0)\n        self.whiten = whiten\n        self._sample_latent = True\n\n    @pyro_method\n    def model(self):\n        self.set_mode(""model"")\n\n        M = self.Xu.size(0)\n        Kuu = self.kernel(self.Xu).contiguous()\n        Kuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal\n        Luu = Kuu.cholesky()\n\n        zero_loc = self.Xu.new_zeros(self.u_loc.shape)\n        if self.whiten:\n            identity = eye_like(self.Xu, M)\n            pyro.sample(self._pyro_get_fullname(""u""),\n                        dist.MultivariateNormal(zero_loc, scale_tril=identity)\n                            .to_event(zero_loc.dim() - 1))\n        else:\n            pyro.sample(self._pyro_get_fullname(""u""),\n                        dist.MultivariateNormal(zero_loc, scale_tril=Luu)\n                            .to_event(zero_loc.dim() - 1))\n\n        f_loc, f_var = conditional(self.X, self.Xu, self.kernel, self.u_loc, self.u_scale_tril,\n                                   Luu, full_cov=False, whiten=self.whiten, jitter=self.jitter)\n\n        f_loc = f_loc + self.mean_function(self.X)\n        if self.y is None:\n            return f_loc, f_var\n        else:\n            # we would like to load likelihood\'s parameters outside poutine.scale context\n            self.likelihood._load_pyro_samples()\n            with poutine.scale(scale=self.num_data / self.X.size(0)):\n                return self.likelihood(f_loc, f_var, self.y)\n\n    @pyro_method\n    def guide(self):\n        self.set_mode(""guide"")\n        self._load_pyro_samples()\n\n        pyro.sample(self._pyro_get_fullname(""u""),\n                    dist.MultivariateNormal(self.u_loc, scale_tril=self.u_scale_tril)\n                        .to_event(self.u_loc.dim()-1))\n\n    def forward(self, Xnew, full_cov=False):\n        r""""""\n        Computes the mean and covariance matrix (or variance) of Gaussian Process\n        posterior on a test input data :math:`X_{new}`:\n\n        .. math:: p(f^* \\mid X_{new}, X, y, k, X_u, u_{loc}, u_{scale\\_tril})\n            = \\mathcal{N}(loc, cov).\n\n        .. note:: Variational parameters ``u_loc``, ``u_scale_tril``, the\n            inducing-point parameter ``Xu``, together with kernel\'s parameters have\n            been learned from a training procedure (MCMC or SVI).\n\n        :param torch.Tensor Xnew: A input data for testing. Note that\n            ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``.\n        :param bool full_cov: A flag to decide if we want to predict full covariance\n            matrix or just variance.\n        :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))`\n        :rtype: tuple(torch.Tensor, torch.Tensor)\n        """"""\n        self._check_Xnew_shape(Xnew)\n        self.set_mode(""guide"")\n\n        loc, cov = conditional(Xnew, self.Xu, self.kernel, self.u_loc, self.u_scale_tril,\n                               full_cov=full_cov, whiten=self.whiten, jitter=self.jitter)\n        return loc + self.mean_function(Xnew), cov\n'"
pyro/contrib/oed/glmm/__init__.py,6,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\n.. warning :: This module will eventually be deprecated in favor of `brmp <https://github.com/pyro-ppl/brmp/>`_\n\nThe :mod:`pyro.contrib.oed.glmm` module provides models and guides for\ngeneralised linear mixed models (GLMM). It also includes the\nNormal-inverse-gamma family.\n\nTo create a classical Bayesian linear model, use::\n\n    from pyro.contrib.oed.glmm import known_covariance_linear_model\n\n    # Note: coef is a p-vector, observation_sd is a scalar\n    # Here, p=1 (one feature)\n    model = known_covariance_linear_model(coef_mean=torch.tensor([0.]),\n                                          coef_sd=torch.tensor([10.]),\n                                          observation_sd=torch.tensor(2.))\n\n    # An n x p design tensor\n    # Here, n=2 (two observations)\n    design = torch.tensor(torch.tensor([[1.], [-1.]]))\n\n    model(design)\n\nA non-linear link function may be introduced, for instance::\n\n    from pyro.contrib.oed.glmm import logistic_regression_model\n\n    # No observation_sd is needed for logistic models\n    model = logistic_regression_model(coef_mean=torch.tensor([0.]),\n                                      coef_sd=torch.tensor([10.]))\n\nRandom effects may be incorporated as regular Bayesian regression coefficients.\nFor random effects with a shared covariance matrix, see :meth:`pyro.contrib.oed.glmm.lmer_model`.\n""""""\n\nfrom pyro.contrib.oed.glmm.glmm import *  # noqa: F403,F401\nfrom pyro.contrib.oed.glmm import guides  # noqa: F401\n'"
pyro/contrib/oed/glmm/glmm.py,42,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\nfrom collections import OrderedDict\nfrom functools import partial\nfrom contextlib import ExitStack\n\nimport torch\nfrom torch.nn.functional import softplus\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import AffineTransform, SigmoidTransform\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.contrib.util import rmv, iter_plates_to_shape\n\n# TODO read from torch float spec\nepsilon = torch.tensor(2**-24)\n\n\ndef known_covariance_linear_model(coef_means, coef_sds, observation_sd,\n                                  coef_labels=""w"", observation_label=""y""):\n\n    if not isinstance(coef_means, list):\n        coef_means = [coef_means]\n    if not isinstance(coef_sds, list):\n        coef_sds = [coef_sds]\n    if not isinstance(coef_labels, list):\n        coef_labels = [coef_labels]\n\n    model = partial(bayesian_linear_model,\n                    w_means=OrderedDict([(label, mean) for label, mean in zip(coef_labels, coef_means)]),\n                    w_sqrtlambdas=OrderedDict([\n                        (label, 1./(observation_sd*sd)) for label, sd in zip(coef_labels, coef_sds)]),\n                    obs_sd=observation_sd,\n                    response_label=observation_label)\n    # For computing the true EIG\n    model.obs_sd = observation_sd\n    model.w_sds = OrderedDict([(label, sd) for label, sd in zip(coef_labels, coef_sds)])\n    model.w_sizes = OrderedDict([(label, sd.shape[-1]) for label, sd in zip(coef_labels, coef_sds)])\n    model.observation_label = observation_label\n    model.coef_labels = coef_labels\n    return model\n\n\ndef normal_guide(observation_sd, coef_shape, coef_label=""w""):\n    return partial(normal_inv_gamma_family_guide,\n                   obs_sd=observation_sd,\n                   w_sizes={coef_label: coef_shape})\n\n\ndef group_linear_model(coef1_mean, coef1_sd, coef2_mean, coef2_sd, observation_sd,\n                       coef1_label=""w1"", coef2_label=""w2"", observation_label=""y""):\n    model = partial(\n        bayesian_linear_model,\n        w_means=OrderedDict([(coef1_label, coef1_mean), (coef2_label, coef2_mean)]),\n        w_sqrtlambdas=OrderedDict([(coef1_label, 1./(observation_sd*coef1_sd)),\n                                   (coef2_label, 1./(observation_sd*coef2_sd))]),\n        obs_sd=observation_sd,\n        response_label=observation_label)\n    model.obs_sd = observation_sd\n    model.w_sds = OrderedDict([(coef1_label, coef1_sd), (coef2_label, coef2_sd)])\n    return model\n\n\ndef group_normal_guide(observation_sd, coef1_shape, coef2_shape,\n                       coef1_label=""w1"", coef2_label=""w2""):\n    return partial(\n        normal_inv_gamma_family_guide, w_sizes=OrderedDict([(coef1_label, coef1_shape), (coef2_label, coef2_shape)]),\n        obs_sd=observation_sd)\n\n\ndef zero_mean_unit_obs_sd_lm(coef_sd, coef_label=""w""):\n    model = known_covariance_linear_model(torch.tensor(0.), coef_sd, torch.tensor(1.), coef_labels=coef_label)\n    guide = normal_guide(torch.tensor(1.), coef_sd.shape, coef_label=coef_label)\n    return model, guide\n\n\ndef normal_inverse_gamma_linear_model(coef_mean, coef_sqrtlambda, alpha,\n                                      beta, coef_label=""w"",\n                                      observation_label=""y""):\n    return partial(bayesian_linear_model,\n                   w_means={coef_label: coef_mean},\n                   w_sqrtlambdas={coef_label: coef_sqrtlambda},\n                   alpha_0=alpha, beta_0=beta,\n                   response_label=observation_label)\n\n\ndef normal_inverse_gamma_guide(coef_shape, coef_label=""w"", **kwargs):\n    return partial(normal_inv_gamma_family_guide, obs_sd=None, w_sizes={coef_label: coef_shape}, **kwargs)\n\n\ndef logistic_regression_model(coef_mean, coef_sd, coef_label=""w"", observation_label=""y""):\n    return partial(bayesian_linear_model,\n                   w_means={coef_label: coef_mean},\n                   w_sqrtlambdas={coef_label: 1./coef_sd},\n                   obs_sd=torch.tensor(1.),\n                   response=""bernoulli"",\n                   response_label=observation_label)\n\n\ndef lmer_model(fixed_effects_sd, n_groups, random_effects_alpha, random_effects_beta,\n               fixed_effects_label=""w"", random_effects_label=""u"", observation_label=""y"",\n               response=""normal""):\n    return partial(bayesian_linear_model,\n                   w_means={fixed_effects_label: torch.tensor(0.)},\n                   w_sqrtlambdas={fixed_effects_label: 1./fixed_effects_sd},\n                   obs_sd=torch.tensor(1.),\n                   re_group_sizes={random_effects_label: n_groups},\n                   re_alphas={random_effects_label: random_effects_alpha},\n                   re_betas={random_effects_label: random_effects_beta},\n                   response=response,\n                   response_label=observation_label)\n\n\ndef sigmoid_model(coef1_mean, coef1_sd, coef2_mean, coef2_sd, observation_sd,\n                  sigmoid_alpha, sigmoid_beta, sigmoid_design,\n                  coef1_label=""w1"", coef2_label=""w2"", observation_label=""y"",\n                  sigmoid_label=""k""):\n\n    def model(design):\n        batch_shape = design.shape[:-2]\n        k_shape = batch_shape + (sigmoid_design.shape[-1],)\n        k = pyro.sample(sigmoid_label,\n                        dist.Gamma(sigmoid_alpha.expand(k_shape),\n                                   sigmoid_beta.expand(k_shape)).to_event(1))\n        k_assigned = rmv(sigmoid_design, k)\n\n        return bayesian_linear_model(\n            design,\n            w_means=OrderedDict([(coef1_label, coef1_mean), (coef2_label, coef2_mean)]),\n            w_sqrtlambdas={coef1_label: 1./(observation_sd*coef1_sd), coef2_label: 1./(observation_sd*coef2_sd)},\n            obs_sd=observation_sd,\n            response=""sigmoid"",\n            response_label=observation_label,\n            k=k_assigned\n            )\n\n    return model\n\n\ndef bayesian_linear_model(design, w_means={}, w_sqrtlambdas={}, re_group_sizes={},\n                          re_alphas={}, re_betas={}, obs_sd=None,\n                          alpha_0=None, beta_0=None, response=""normal"",\n                          response_label=""y"", k=None):\n    """"""\n    A pyro model for Bayesian linear regression.\n\n    If :param:`response` is `""normal""` this corresponds to a linear regression\n    model\n\n        :math:`Y = Xw + \\\\epsilon`\n\n    with `\\\\epsilon`` i.i.d. zero-mean Gaussian. The observation standard deviation\n    (:param:`obs_sd`) may be known or unknown. If unknown, it is assumed to follow an\n    inverse Gamma distribution with parameters :param:`alpha_0` and :param:`beta_0`.\n\n    If the response type is `""bernoulli""` we instead have :math:`Y \\\\sim Bernoulli(p)`\n    with\n\n        :math:`logit(p) = Xw`\n\n    Given parameter groups in :param:`w_means` and :param:`w_sqrtlambda`, the fixed effects\n    regression coefficient is taken to be Gaussian with mean `w_mean` and standard deviation\n    given by\n\n        :math:`\\\\sigma / \\\\sqrt{\\\\lambda}`\n\n    corresponding to the normal inverse Gamma family.\n\n    The random effects coefficient is constructed as follows. For each random effect\n    group, standard deviations for that group are sampled from a normal inverse Gamma\n    distribution. For each group, a random effect coefficient is then sampled from a zero\n    mean Gaussian with those standard deviations.\n\n    :param torch.Tensor design: a tensor with last two dimensions `n` and `p`\n            corresponding to observations and features respectively.\n    :param OrderedDict w_means: map from variable names to tensors of fixed effect means.\n    :param OrderedDict w_sqrtlambdas: map from variable names to tensors of square root\n        :math:`\\\\lambda` values for fixed effects.\n    :param OrderedDict re_group_sizes: map from variable names to int representing the\n        group size\n    :param OrderedDict re_alphas: map from variable names to `torch.Tensor`, the tensor\n        consists of Gamma dist :math:`\\\\alpha` values\n    :param OrderedDict re_betas: map from variable names to `torch.Tensor`, the tensor\n        consists of Gamma dist :math:`\\\\beta` values\n    :param torch.Tensor obs_sd: the observation standard deviation (if assumed known).\n        This is still relevant in the case of Bernoulli observations when coefficeints\n        are sampled using `w_sqrtlambdas`.\n    :param torch.Tensor alpha_0: Gamma :math:`\\\\alpha` parameter for unknown observation\n        covariance.\n    :param torch.Tensor beta_0: Gamma :math:`\\\\beta` parameter for unknown observation\n        covariance.\n    :param str response: Emission distribution. May be `""normal""` or `""bernoulli""`.\n    :param str response_label: Variable label for response.\n    :param torch.Tensor k: Only used for a sigmoid response. The slope of the sigmoid\n        transformation.\n    """"""\n    # design is size batch x n x p\n    # tau is size batch\n    batch_shape = design.shape[:-2]\n    with ExitStack() as stack:\n        for plate in iter_plates_to_shape(batch_shape):\n            stack.enter_context(plate)\n\n        if obs_sd is None:\n            # First, sample tau (observation precision)\n            tau_prior = dist.Gamma(alpha_0.unsqueeze(-1),\n                                   beta_0.unsqueeze(-1)).to_event(1)\n            tau = pyro.sample(""tau"", tau_prior)\n            obs_sd = 1./torch.sqrt(tau)\n\n        elif alpha_0 is not None or beta_0 is not None:\n            warnings.warn(""Values of `alpha_0` and `beta_0` unused becased""\n                          ""`obs_sd` was specified already."")\n\n        obs_sd = obs_sd.expand(batch_shape + (1,))\n\n        # Build the regression coefficient\n        w = []\n        # Allow different names for different coefficient groups\n        # Process fixed effects\n        for name, w_sqrtlambda in w_sqrtlambdas.items():\n            w_mean = w_means[name]\n            # Place a normal prior on the regression coefficient\n            w_prior = dist.Normal(w_mean, obs_sd / w_sqrtlambda).to_event(1)\n            w.append(pyro.sample(name, w_prior))\n        # Process random effects\n        for name, group_size in re_group_sizes.items():\n            # Sample `G` once for this group\n            alpha, beta = re_alphas[name], re_betas[name]\n            G_prior = dist.Gamma(alpha, beta).to_event(1)\n            G = 1./torch.sqrt(pyro.sample(""G_"" + name, G_prior))\n            # Repeat `G` for each group\n            repeat_shape = tuple(1 for _ in batch_shape) + (group_size,)\n            u_prior = dist.Normal(torch.tensor(0.), G.repeat(repeat_shape)).to_event(1)\n            w.append(pyro.sample(name, u_prior))\n        # Regression coefficient `w` is batch x p\n        w = broadcast_cat(w)\n\n        # Run the regressor forward conditioned on inputs\n        prediction_mean = rmv(design, w)\n        if response == ""normal"":\n            # y is an n-vector: hence use .to_event(1)\n            return pyro.sample(response_label, dist.Normal(prediction_mean, obs_sd).to_event(1))\n        elif response == ""bernoulli"":\n            return pyro.sample(response_label, dist.Bernoulli(logits=prediction_mean).to_event(1))\n        elif response == ""sigmoid"":\n            base_dist = dist.Normal(prediction_mean, obs_sd).to_event(1)\n            # You can add loc via the linear model itself\n            k = k.expand(prediction_mean.shape)\n            transforms = [AffineTransform(loc=torch.tensor(0.), scale=k), SigmoidTransform()]\n            response_dist = dist.TransformedDistribution(base_dist, transforms)\n            return pyro.sample(response_label, response_dist)\n        else:\n            raise ValueError(""Unknown response distribution: \'{}\'"".format(response))\n\n\n# TODO replace this guide with one allowing correlation between\n# regression coefficients (necessary!)\ndef normal_inv_gamma_family_guide(design, obs_sd, w_sizes, mf=False):\n    """"""Normal inverse Gamma family guide.\n\n    If `obs_sd` is known, this is a multivariate Normal family with separate\n    parameters for each batch. `w` is sampled from a Gaussian with mean `mw_param` and\n    covariance matrix derived from  `obs_sd * lambda_param` and the two parameters `mw_param` and `lambda_param`\n    are learned.\n\n    If `obs_sd=None`, this is a four-parameter family. The observation precision\n    `tau` is sampled from a Gamma distribution with parameters `alpha`, `beta`\n    (separate for each batch). We let `obs_sd = 1./torch.sqrt(tau)` and then\n    proceed as above.\n\n    :param torch.Tensor design: a tensor with last two dimensions `n` and `p`\n        corresponding to observations and features respectively.\n    :param torch.Tensor obs_sd: observation standard deviation, or `None` to use\n        inverse Gamma\n    :param OrderedDict w_sizes: map from variable names to torch.Size\n    """"""\n    # design is size batch x n x p\n    # tau is size batch\n    tau_shape = design.shape[:-2]\n    with ExitStack() as stack:\n        for plate in iter_plates_to_shape(tau_shape):\n            stack.enter_context(plate)\n\n        if obs_sd is None:\n            # First, sample tau (observation precision)\n            alpha = softplus(pyro.param(""invsoftplus_alpha"", 20.*torch.ones(tau_shape)))\n            beta = softplus(pyro.param(""invsoftplus_beta"", 20.*torch.ones(tau_shape)))\n            # Global variable\n            tau_prior = dist.Gamma(alpha, beta)\n            tau = pyro.sample(""tau"", tau_prior)\n            obs_sd = 1./torch.sqrt(tau)\n\n        # response will be shape batch x n\n        obs_sd = obs_sd.expand(tau_shape).unsqueeze(-1)\n\n        for name, size in w_sizes.items():\n            w_shape = tau_shape + size\n            # Set up mu and lambda\n            mw_param = pyro.param(""{}_guide_mean"".format(name),\n                                  torch.zeros(w_shape))\n            scale_tril = pyro.param(\n                ""{}_guide_scale_tril"".format(name),\n                torch.eye(*size).expand(tau_shape + size + size),\n                constraint=constraints.lower_cholesky)\n            # guide distributions for w\n            if mf:\n                w_dist = dist.MultivariateNormal(mw_param, scale_tril=scale_tril)\n            else:\n                w_dist = dist.MultivariateNormal(mw_param, scale_tril=obs_sd.unsqueeze(-1) * scale_tril)\n            pyro.sample(name, w_dist)\n\n\ndef group_assignment_matrix(design):\n    """"""Converts a one-dimensional tensor listing group sizes into a\n    two-dimensional binary tensor of indicator variables.\n\n    :return: A :math:`n \\times p` binary matrix where :math:`p` is\n        the length of `design` and :math:`n` is its sum. There are\n        :math:`n_i` ones in the :math:`i`th column.\n    :rtype: torch.tensor\n\n    """"""\n    n, p = int(torch.sum(design)), int(design.shape[0])\n    X = torch.zeros(n, p)\n    t = 0\n    for col, i in enumerate(design):\n        i = int(i)\n        if i > 0:\n            X[t:t+i, col] = 1.\n        t += i\n    if t < n:\n        X[t:, -1] = 1.\n    return X\n\n\ndef rf_group_assignments(n, random_intercept=True):\n    assert n % 2 == 0\n    n_designs = n//2 + 1\n    participant_matrix = torch.eye(n)\n    Xs = []\n    for i in range(n_designs):\n        X1 = group_assignment_matrix(torch.tensor([i, n//2 - i]))\n        X2 = group_assignment_matrix(torch.tensor([n//2 - i, i]))\n        X = torch.cat([X1, X2], dim=-2)\n        Xs.append(X)\n    X = torch.stack(Xs, dim=0)\n    if random_intercept:\n        X = torch.cat([X, participant_matrix.expand(n_designs, n, n)], dim=-1)\n    return X, participant_matrix\n\n\ndef analytic_posterior_cov(prior_cov, x, obs_sd):\n    """"""\n    Given a prior covariance matrix and a design matrix `x`,\n    returns the covariance of the posterior under a Bayesian\n    linear regression model with design `x` and observation\n    noise `obs_sd`.\n    """"""\n    # Use some kernel trick magic\n    p = prior_cov.shape[-1]\n    SigmaXX = prior_cov.mm(x.t().mm(x))\n    posterior_cov = prior_cov - torch.inverse(\n        SigmaXX + (obs_sd**2)*torch.eye(p)).mm(SigmaXX.mm(prior_cov))\n    return posterior_cov\n\n\ndef broadcast_cat(ws):\n    target = torch.broadcast_tensors(*(w[..., 0] for w in ws))[0].shape\n    expanded = [w.expand(target + (w.shape[-1],)) for w in ws]\n    return torch.cat(expanded, dim=-1)\n'"
pyro/contrib/oed/glmm/guides.py,20,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom torch import nn\n\nfrom contextlib import ExitStack\n\nimport pyro\nimport pyro.distributions as dist\nfrom pyro import poutine\nfrom pyro.contrib.util import (\n    tensor_to_dict, rmv, rvv, rtril, lexpand, iter_plates_to_shape\n)\nfrom pyro.ops.linalg import rinverse\n\n\nclass LinearModelPosteriorGuide(nn.Module):\n\n    def __init__(self, d, w_sizes, y_sizes, regressor_init=0., scale_tril_init=3., use_softplus=True, **kwargs):\n        """"""\n        Guide for linear models. No amortisation happens over designs.\n        Amortisation over data is taken care of by analytic formulae for\n        linear models (heavy use of truth).\n\n        :param tuple d: the shape by which to expand the guide parameters, e.g. `(num_batches, num_designs)`.\n        :param dict w_sizes: map from variable string names to int, indicating the dimension of each\n                             weight vector in the linear model.\n        :param float regressor_init: initial value for the regressor matrix used to learn the posterior mean.\n        :param float scale_tril_init: initial value for posterior `scale_tril` parameter.\n        :param bool use_softplus: whether to transform the regressor by a softplus transform: useful if the\n                                  regressor should be nonnegative but close to zero.\n        """"""\n        super().__init__()\n        # Represent each parameter group as independent Gaussian\n        # Making a weak mean-field assumption\n        # To avoid this- combine labels\n        if not isinstance(d, (tuple, list, torch.Tensor)):\n            d = (d,)\n        self.regressor = nn.ParameterDict({l: nn.Parameter(\n                regressor_init * torch.ones(*(d + (p, sum(y_sizes.values()))))) for l, p in w_sizes.items()})\n        self.scale_tril = nn.ParameterDict({l: nn.Parameter(\n                scale_tril_init * lexpand(torch.eye(p), *d)) for l, p in w_sizes.items()})\n        self.w_sizes = w_sizes\n        self.use_softplus = use_softplus\n        self.softplus = nn.Softplus()\n\n    def get_params(self, y_dict, design, target_labels):\n\n        y = torch.cat(list(y_dict.values()), dim=-1)\n        return self.linear_model_formula(y, design, target_labels)\n\n    def linear_model_formula(self, y, design, target_labels):\n\n        if self.use_softplus:\n            mu = {l: rmv(self.softplus(self.regressor[l]), y) for l in target_labels}\n        else:\n            mu = {l: rmv(self.regressor[l], y) for l in target_labels}\n        scale_tril = {l: rtril(self.scale_tril[l]) for l in target_labels}\n\n        return mu, scale_tril\n\n    def forward(self, y_dict, design, observation_labels, target_labels):\n\n        pyro.module(""posterior_guide"", self)\n\n        # Returns two dicts from labels -> tensors\n        mu, scale_tril = self.get_params(y_dict, design, target_labels)\n\n        for l in target_labels:\n            w_dist = dist.MultivariateNormal(mu[l], scale_tril=scale_tril[l])\n            pyro.sample(l, w_dist)\n\n\nclass LinearModelLaplaceGuide(nn.Module):\n    """"""\n    Laplace approximation for a (G)LM.\n\n    :param tuple d: the shape by which to expand the guide parameters, e.g. `(num_batches, num_designs)`.\n    :param dict w_sizes: map from variable string names to int, indicating the dimension of each\n                         weight vector in the linear model.\n    :param str tau_label: the label used for inverse variance parameter sample site, or `None` to indicate a\n                          fixed variance.\n    :param float init_value: initial value for the posterior mean parameters.\n    """"""\n    def __init__(self, d, w_sizes, tau_label=None, init_value=0.1, **kwargs):\n        super().__init__()\n        # start in train mode\n        self.train()\n        if not isinstance(d, (tuple, list, torch.Tensor)):\n            d = (d,)\n        self.means = nn.ParameterDict()\n        if tau_label is not None:\n            w_sizes[tau_label] = 1\n        for l, mu_l in tensor_to_dict(w_sizes, init_value*torch.ones(*(d + (sum(w_sizes.values()), )))).items():\n            self.means[l] = nn.Parameter(mu_l)\n        self.scale_trils = {}\n        self.w_sizes = w_sizes\n\n    @staticmethod\n    def _hessian_diag(y, x, event_shape):\n        batch_shape = x.shape[:-len(event_shape)]\n        assert tuple(x.shape) == tuple(batch_shape) + tuple(event_shape)\n\n        dy = torch.autograd.grad(y, [x, ], create_graph=True)[0]\n        H = []\n\n        # collapse independent dimensions into a single one,\n        # and dependent dimensions into another single one\n        batch_size = 1\n        for batch_shape_dim in batch_shape:\n            batch_size *= batch_shape_dim\n\n        event_size = 1\n        for event_shape_dim in event_shape:\n            event_size *= event_shape_dim\n\n        flat_dy = dy.reshape(batch_size, event_size)\n\n        # loop over dependent part\n        for i in range(flat_dy.shape[-1]):\n            dyi = flat_dy.index_select(-1, torch.tensor([i]))\n            Hi = torch.autograd.grad([dyi], [x, ], grad_outputs=[torch.ones_like(dyi)], retain_graph=True)[0]\n            H.append(Hi)\n        H = torch.stack(H, -1).reshape(*(x.shape + event_shape))\n        return H\n\n    def finalize(self, loss, target_labels):\n        """"""\n        Compute the Hessian of the parameters wrt ``loss``\n\n        :param torch.Tensor loss: the output of evaluating a loss function such as\n                                  `pyro.infer.Trace_ELBO().differentiable_loss` on the model, guide and design.\n        :param list target_labels: list indicating the sample sites that are targets, i.e. for which information gain\n                                   should be measured.\n        """"""\n        # set self.training = False\n        self.eval()\n        for l, mu_l in self.means.items():\n            if l not in target_labels:\n                continue\n            hess_l = self._hessian_diag(loss, mu_l, event_shape=(self.w_sizes[l],))\n            cov_l = rinverse(hess_l)\n            self.scale_trils[l] = cov_l.cholesky(upper=False)\n\n    def forward(self, design, target_labels=None):\n        """"""\n        Sample the posterior.\n\n        :param torch.Tensor design: tensor of possible designs.\n        :param list target_labels: list indicating the sample sites that are targets, i.e. for which information gain\n                                   should be measured.\n        """"""\n        if target_labels is None:\n            target_labels = list(self.means.keys())\n\n        pyro.module(""laplace_guide"", self)\n        with ExitStack() as stack:\n            for plate in iter_plates_to_shape(design.shape[:-2]):\n                stack.enter_context(plate)\n\n            if self.training:\n                # MAP via Delta guide\n                for l in target_labels:\n                    w_dist = dist.Delta(self.means[l]).to_event(1)\n                    pyro.sample(l, w_dist)\n            else:\n                # Laplace approximation via MVN with hessian\n                for l in target_labels:\n                    w_dist = dist.MultivariateNormal(self.means[l], scale_tril=self.scale_trils[l])\n                    pyro.sample(l, w_dist)\n\n\nclass SigmoidGuide(LinearModelPosteriorGuide):\n\n    def __init__(self, d, n, w_sizes, **kwargs):\n        super().__init__(d, w_sizes, **kwargs)\n        self.inverse_sigmoid_scale = nn.Parameter(torch.ones(n))\n        self.h1_weight = nn.Parameter(torch.ones(n))\n        self.h1_bias = nn.Parameter(torch.zeros(n))\n\n    def get_params(self, y_dict, design, target_labels):\n\n        y = torch.cat(list(y_dict.values()), dim=-1)\n\n        # Approx invert transformation on y in expectation\n        y, y1m = y.clamp(1e-35, 1), (1.-y).clamp(1e-35, 1)\n        logited = y.log() - y1m.log()\n        y_trans = logited/.1\n        y_trans = y_trans * self.inverse_sigmoid_scale\n        hidden = self.softplus(y_trans)\n        y_trans = y_trans + hidden * self.h1_weight + self.h1_bias\n\n        return self.linear_model_formula(y_trans, design, target_labels)\n\n\nclass NormalInverseGammaGuide(LinearModelPosteriorGuide):\n\n    def __init__(self, d, w_sizes, mf=False, tau_label=""tau"", alpha_init=100.,\n                 b0_init=100., **kwargs):\n        super().__init__(d, w_sizes, **kwargs)\n        self.alpha = nn.Parameter(alpha_init*torch.ones(d))\n        self.b0 = nn.Parameter(b0_init*torch.ones(d))\n        self.mf = mf\n        self.tau_label = tau_label\n\n    def get_params(self, y_dict, design, target_labels):\n\n        y = torch.cat(list(y_dict.values()), dim=-1)\n\n        coefficient_labels = [label for label in target_labels if label != self.tau_label]\n        mu, scale_tril = self.linear_model_formula(y, design, coefficient_labels)\n        mu_vec = torch.cat(list(mu.values()), dim=-1)\n\n        yty = rvv(y, y)\n        ytxmu = rvv(y, rmv(design, mu_vec))\n        beta = self.b0 + .5*(yty - ytxmu)\n\n        return mu, scale_tril, self.alpha, beta\n\n    def forward(self, y_dict, design, observation_labels, target_labels):\n\n        pyro.module(""ba_guide"", self)\n\n        mu, scale_tril, alpha, beta = self.get_params(y_dict, design, target_labels)\n\n        if self.tau_label in target_labels:\n            tau_dist = dist.Gamma(alpha, beta)\n            tau = pyro.sample(self.tau_label, tau_dist)\n            obs_sd = 1./tau.sqrt().unsqueeze(-1).unsqueeze(-1)\n\n        for label in target_labels:\n            if label != self.tau_label:\n                if self.mf:\n                    w_dist = dist.MultivariateNormal(mu[label],\n                                                     scale_tril=scale_tril[label])\n                else:\n                    w_dist = dist.MultivariateNormal(mu[label],\n                                                     scale_tril=scale_tril[label]*obs_sd)\n                pyro.sample(label, w_dist)\n\n\nclass GuideDV(nn.Module):\n    """"""A Donsker-Varadhan `T` family based on a guide family via\n    the relation `T = log p(theta) - log q(theta | y, d)`\n    """"""\n    def __init__(self, guide):\n        super().__init__()\n        self.guide = guide\n\n    def forward(self, design, trace, observation_labels, target_labels):\n\n        trace.compute_log_prob()\n        prior_lp = sum(trace.nodes[l][""log_prob""] for l in target_labels)\n        y_dict = {l: trace.nodes[l][""value""] for l in observation_labels}\n        theta_dict = {l: trace.nodes[l][""value""] for l in target_labels}\n\n        conditional_guide = pyro.condition(self.guide, data=theta_dict)\n        cond_trace = poutine.trace(conditional_guide).get_trace(\n                y_dict, design, observation_labels, target_labels)\n        cond_trace.compute_log_prob()\n\n        posterior_lp = sum(cond_trace.nodes[l][""log_prob""] for l in target_labels)\n\n        return posterior_lp - prior_lp\n'"
