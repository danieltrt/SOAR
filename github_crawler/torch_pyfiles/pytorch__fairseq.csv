file_path,api_count,code
eval_lm.py,0,"b""#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq_cli.eval_lm import cli_main\n\n\nif __name__ == '__main__':\n    cli_main()\n"""
generate.py,0,"b""#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq_cli.generate import cli_main\n\n\nif __name__ == '__main__':\n    cli_main()\n"""
hubconf.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\n\nfrom fairseq.hub_utils import BPEHubInterface as bpe  # noqa\nfrom fairseq.hub_utils import TokenizerHubInterface as tokenizer  # noqa\nfrom fairseq.models import MODEL_REGISTRY\n\n\ndependencies = [\n    'numpy',\n    'regex',\n    'requests',\n    'torch',\n]\n\n\n# torch.hub doesn't build Cython components, so if they are not found then try\n# to build them here\ntry:\n    import fairseq.data.token_block_utils_fast\nexcept (ImportError, ModuleNotFoundError):\n    try:\n        import cython\n        import os\n        from setuptools import sandbox\n        sandbox.run_setup(\n            os.path.join(os.path.dirname(__file__), 'setup.py'),\n            ['build_ext', '--inplace'],\n        )\n    except (ImportError, ModuleNotFoundError):\n        print(\n            'Unable to build Cython components. Please make sure Cython is '\n            'installed if the torch.hub model you are loading depends on it.'\n        )\n\n\nfor _model_type, _cls in MODEL_REGISTRY.items():\n    for model_name in _cls.hub_models().keys():\n        globals()[model_name] = functools.partial(\n            _cls.from_pretrained,\n            model_name,\n        )\n    # to simplify the interface we only expose named models\n    # globals()[_model_type] = _cls.from_pretrained\n"""
interactive.py,0,"b""#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq_cli.interactive import cli_main\n\n\nif __name__ == '__main__':\n    cli_main()\n"""
preprocess.py,0,"b""#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq_cli.preprocess import cli_main\n\n\nif __name__ == '__main__':\n    cli_main()\n"""
score.py,0,"b""#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq_cli.score import cli_main\n\n\nif __name__ == '__main__':\n    cli_main()\n"""
setup.py,2,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nfrom setuptools import setup, find_packages, Extension\nimport sys\n\n\nif sys.version_info < (3, 6):\n    sys.exit(\'Sorry, Python >= 3.6 is required for fairseq.\')\n\n\nwith open(\'README.md\') as f:\n    readme = f.read()\n\n\nif sys.platform == \'darwin\':\n    extra_compile_args = [\'-stdlib=libc++\', \'-O3\']\nelse:\n    extra_compile_args = [\'-std=c++11\', \'-O3\']\n\n\nclass NumpyExtension(Extension):\n    """"""Source: https://stackoverflow.com/a/54128391""""""\n\n    def __init__(self, *args, **kwargs):\n        self.__include_dirs = []\n        super().__init__(*args, **kwargs)\n\n    @property\n    def include_dirs(self):\n        import numpy\n        return self.__include_dirs + [numpy.get_include()]\n\n    @include_dirs.setter\n    def include_dirs(self, dirs):\n        self.__include_dirs = dirs\n\n\nextensions = [\n    Extension(\n        \'fairseq.libbleu\',\n        sources=[\n            \'fairseq/clib/libbleu/libbleu.cpp\',\n            \'fairseq/clib/libbleu/module.cpp\',\n        ],\n        extra_compile_args=extra_compile_args,\n    ),\n    NumpyExtension(\n        \'fairseq.data.data_utils_fast\',\n        sources=[\'fairseq/data/data_utils_fast.pyx\'],\n        language=\'c++\',\n        extra_compile_args=extra_compile_args,\n    ),\n    NumpyExtension(\n        \'fairseq.data.token_block_utils_fast\',\n        sources=[\'fairseq/data/token_block_utils_fast.pyx\'],\n        language=\'c++\',\n        extra_compile_args=extra_compile_args,\n    ),\n]\n\n\ncmdclass = {}\n\n\ntry:\n    # torch is not available when generating docs\n    from torch.utils import cpp_extension\n    extensions.extend([\n        cpp_extension.CppExtension(\n            \'fairseq.libnat\',\n            sources=[\n                \'fairseq/clib/libnat/edit_dist.cpp\',\n            ],\n        )\n    ])\n\n    if \'CUDA_HOME\' in os.environ:\n        extensions.extend([\n            cpp_extension.CppExtension(\n                \'fairseq.libnat_cuda\',\n                sources=[\n                    \'fairseq/clib/libnat_cuda/edit_dist.cu\',\n                    \'fairseq/clib/libnat_cuda/binding.cpp\'\n                ],\n            )])\n    cmdclass[\'build_ext\'] = cpp_extension.BuildExtension\n\nexcept ImportError:\n    pass\n\n\nif \'READTHEDOCS\' in os.environ:\n    # don\'t build extensions when generating docs\n    extensions = []\n    if \'build_ext\' in cmdclass:\n        del cmdclass[\'build_ext\']\n\n    # use CPU build of PyTorch\n    dependency_links = [\n        \'https://download.pytorch.org/whl/cpu/torch-1.3.0%2Bcpu-cp36-cp36m-linux_x86_64.whl\'\n    ]\nelse:\n    dependency_links = []\n\n\nif \'clean\' in sys.argv[1:]:\n    # Source: https://bit.ly/2NLVsgE\n    print(""deleting Cython files..."")\n    import subprocess\n    subprocess.run([\'rm -f fairseq/*.so fairseq/**/*.so fairseq/*.pyd fairseq/**/*.pyd\'], shell=True)\n\n\nsetup(\n    name=\'fairseq\',\n    version=\'0.9.0\',\n    description=\'Facebook AI Research Sequence-to-Sequence Toolkit\',\n    url=\'https://github.com/pytorch/fairseq\',\n    classifiers=[\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    ],\n    long_description=readme,\n    long_description_content_type=\'text/markdown\',\n    setup_requires=[\n        \'cython\',\n        \'numpy\',\n        \'setuptools>=18.0\',\n    ],\n    install_requires=[\n        \'cffi\',\n        \'cython\',\n        \'numpy\',\n        \'regex\',\n        \'sacrebleu\',\n        \'torch\',\n        \'tqdm\',\n    ],\n    dependency_links=dependency_links,\n    packages=find_packages(exclude=[\'scripts\', \'tests\']),\n    ext_modules=extensions,\n    test_suite=\'tests\',\n    entry_points={\n        \'console_scripts\': [\n            \'fairseq-eval-lm = fairseq_cli.eval_lm:cli_main\',\n            \'fairseq-generate = fairseq_cli.generate:cli_main\',\n            \'fairseq-interactive = fairseq_cli.interactive:cli_main\',\n            \'fairseq-preprocess = fairseq_cli.preprocess:cli_main\',\n            \'fairseq-score = fairseq_cli.score:cli_main\',\n            \'fairseq-train = fairseq_cli.train:cli_main\',\n            \'fairseq-validate = fairseq_cli.validate:cli_main\',\n        ],\n    },\n    cmdclass=cmdclass,\n    zip_safe=False,\n)\n'"
train.py,0,"b""#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq_cli.train import cli_main\n\n\nif __name__ == '__main__':\n    cli_main()\n"""
validate.py,0,"b""#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq_cli.validate import cli_main\n\n\nif __name__ == '__main__':\n    cli_main()\n"""
docs/conf.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# fairseq documentation build configuration file, created by\n# sphinx-quickstart on Fri Aug 17 21:45:30 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\n\n# source code directory, relative to this file, for sphinx-autobuild\nsys.path.insert(0, os.path.abspath(\'..\'))\n\nsource_suffix = [\'.rst\']\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.napoleon\',\n    \'sphinxarg.ext\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'fairseq\'\ncopyright = \'2019, Facebook AI Research (FAIR)\'\nauthor = \'Facebook AI Research (FAIR)\'\n\ngithub_doc_root = \'https://github.com/pytorch/fairseq/tree/master/docs/\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.9.0\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.9.0\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\nhighlight_language = \'python\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_context = {\n    \'css_files\': [\n        \'_static/theme_overrides.css\',  # override wide tables in RTD theme\n    ],\n}\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\n#html_sidebars = {\n#    \'**\': [\n#        \'about.html\',\n#        \'navigation.html\',\n#        \'relations.html\',  # needs \'show_related\': True theme option to display\n#        \'searchbox.html\',\n#        \'donate.html\',\n#    ]\n#}\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'python\': (\'https://docs.python.org/\', None),\n    \'torch\': (\'https://pytorch.org/docs/master/\', None),\n}\n'"
examples/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n__version__ = '0.9.0'\n\nimport examples.noisychannel  # noqa\n"""
fairseq/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n__all__ = ['pdb']\n__version__ = '0.9.0'\n\nimport sys\n\n# backwards compatibility to support `from fairseq.meters import AverageMeter`\nfrom fairseq.logging import meters, metrics, progress_bar  # noqa\nsys.modules['fairseq.meters'] = meters\nsys.modules['fairseq.metrics'] = metrics\nsys.modules['fairseq.progress_bar'] = progress_bar\n\nimport fairseq.criterions  # noqa\nimport fairseq.models  # noqa\nimport fairseq.modules  # noqa\nimport fairseq.optim  # noqa\nimport fairseq.optim.lr_scheduler  # noqa\nimport fairseq.pdb  # noqa\nimport fairseq.tasks  # noqa\n\nimport fairseq.benchmark  # noqa\nimport fairseq.model_parallel  # noqa\n"""
fairseq/binarizer.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nfrom collections import Counter\n\nfrom fairseq.tokenizer import tokenize_line\nimport torch\n\n\ndef safe_readline(f):\n    pos = f.tell()\n    while True:\n        try:\n            return f.readline()\n        except UnicodeDecodeError:\n            pos -= 1\n            f.seek(pos)  # search where this character begins\n\n\nclass Binarizer:\n    @staticmethod\n    def binarize(\n        filename,\n        dict,\n        consumer,\n        tokenize=tokenize_line,\n        append_eos=True,\n        reverse_order=False,\n        offset=0,\n        end=-1,\n        already_numberized=False,\n    ):\n        nseq, ntok = 0, 0\n        replaced = Counter()\n\n        def replaced_consumer(word, idx):\n            if idx == dict.unk_index and word != dict.unk_word:\n                replaced.update([word])\n\n        with open(filename, ""r"", encoding=""utf-8"") as f:\n            f.seek(offset)\n            # next(f) breaks f.tell(), hence readline() must be used\n            line = safe_readline(f)\n            while line:\n                if end > 0 and f.tell() > end:\n                    break\n                if already_numberized:\n                    id_strings = line.strip().split()\n                    id_list = [int(id_string) for id_string in id_strings]\n                    if reverse_order:\n                        id_list.reverse()\n                    if append_eos:\n                        id_list.append(dict.eos())\n                    ids = torch.IntTensor(id_list)\n                else:\n                    ids = dict.encode_line(\n                        line=line,\n                        line_tokenizer=tokenize,\n                        add_if_not_exist=False,\n                        consumer=replaced_consumer,\n                        append_eos=append_eos,\n                        reverse_order=reverse_order,\n                    )\n                nseq += 1\n                ntok += len(ids)\n                consumer(ids)\n                line = f.readline()\n        return {\n            ""nseq"": nseq,\n            ""nunk"": sum(replaced.values()),\n            ""ntok"": ntok,\n            ""replaced"": replaced,\n        }\n\n    @staticmethod\n    def binarize_alignments(filename, alignment_parser, consumer, offset=0, end=-1):\n        nseq = 0\n\n        with open(filename, ""r"") as f:\n            f.seek(offset)\n            line = safe_readline(f)\n            while line:\n                if end > 0 and f.tell() > end:\n                    break\n                ids = alignment_parser(line)\n                nseq += 1\n                consumer(ids)\n                line = f.readline()\n        return {""nseq"": nseq}\n\n    @staticmethod\n    def find_offsets(filename, num_chunks):\n        with open(filename, ""r"", encoding=""utf-8"") as f:\n            size = os.fstat(f.fileno()).st_size\n            chunk_size = size // num_chunks\n            offsets = [0 for _ in range(num_chunks + 1)]\n            for i in range(1, num_chunks):\n                f.seek(chunk_size * i)\n                safe_readline(f)\n                offsets[i] = f.tell()\n            return offsets\n'"
fairseq/bleu.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport ctypes\nimport math\nimport torch\n\ntry:\n    from fairseq import libbleu\nexcept ImportError as e:\n    import sys\n    sys.stderr.write(\'ERROR: missing libbleu.so. run `pip install --editable .`\\n\')\n    raise e\n\n\nC = ctypes.cdll.LoadLibrary(libbleu.__file__)\n\n\nclass BleuStat(ctypes.Structure):\n    _fields_ = [\n        (\'reflen\', ctypes.c_size_t),\n        (\'predlen\', ctypes.c_size_t),\n        (\'match1\', ctypes.c_size_t),\n        (\'count1\', ctypes.c_size_t),\n        (\'match2\', ctypes.c_size_t),\n        (\'count2\', ctypes.c_size_t),\n        (\'match3\', ctypes.c_size_t),\n        (\'count3\', ctypes.c_size_t),\n        (\'match4\', ctypes.c_size_t),\n        (\'count4\', ctypes.c_size_t),\n    ]\n\n\nclass SacrebleuScorer(object):\n    def __init__(self):\n        import sacrebleu\n        self.sacrebleu = sacrebleu\n        self.reset()\n\n    def reset(self, one_init=False):\n        if one_init:\n            raise NotImplementedError\n        self.ref = []\n        self.sys = []\n\n    def add_string(self, ref, pred):\n        self.ref.append(ref)\n        self.sys.append(pred)\n\n    def score(self, order=4):\n        return self.result_string(order).score\n\n    def result_string(self, order=4):\n        if order != 4:\n            raise NotImplementedError\n        return self.sacrebleu.corpus_bleu(self.sys, [self.ref])\n\n\nclass Scorer(object):\n    def __init__(self, pad, eos, unk):\n        self.stat = BleuStat()\n        self.pad = pad\n        self.eos = eos\n        self.unk = unk\n        self.reset()\n\n    def reset(self, one_init=False):\n        if one_init:\n            C.bleu_one_init(ctypes.byref(self.stat))\n        else:\n            C.bleu_zero_init(ctypes.byref(self.stat))\n\n    def add(self, ref, pred):\n        if not isinstance(ref, torch.IntTensor):\n            raise TypeError(\'ref must be a torch.IntTensor (got {})\'\n                            .format(type(ref)))\n        if not isinstance(pred, torch.IntTensor):\n            raise TypeError(\'pred must be a torch.IntTensor(got {})\'\n                            .format(type(pred)))\n\n        # don\'t match unknown words\n        rref = ref.clone()\n        assert not rref.lt(0).any()\n        rref[rref.eq(self.unk)] = -999\n\n        rref = rref.contiguous().view(-1)\n        pred = pred.contiguous().view(-1)\n\n        C.bleu_add(\n            ctypes.byref(self.stat),\n            ctypes.c_size_t(rref.size(0)),\n            ctypes.c_void_p(rref.data_ptr()),\n            ctypes.c_size_t(pred.size(0)),\n            ctypes.c_void_p(pred.data_ptr()),\n            ctypes.c_int(self.pad),\n            ctypes.c_int(self.eos))\n\n    def score(self, order=4):\n        psum = sum(math.log(p) if p > 0 else float(\'-Inf\')\n                   for p in self.precision()[:order])\n        return self.brevity() * math.exp(psum / order) * 100\n\n    def precision(self):\n        def ratio(a, b):\n            return a / b if b > 0 else 0\n\n        return [\n            ratio(self.stat.match1, self.stat.count1),\n            ratio(self.stat.match2, self.stat.count2),\n            ratio(self.stat.match3, self.stat.count3),\n            ratio(self.stat.match4, self.stat.count4),\n        ]\n\n    def brevity(self):\n        r = self.stat.reflen / self.stat.predlen\n        return min(1, math.exp(1 - r))\n\n    def result_string(self, order=4):\n        assert order <= 4, ""BLEU scores for order > 4 aren\'t supported""\n        fmt = \'BLEU{} = {:2.2f}, {:2.1f}\'\n        for _ in range(1, order):\n            fmt += \'/{:2.1f}\'\n        fmt += \' (BP={:.3f}, ratio={:.3f}, syslen={}, reflen={})\'\n        bleup = [p * 100 for p in self.precision()[:order]]\n        return fmt.format(order, self.score(order=order), *bleup,\n                          self.brevity(), self.stat.predlen/self.stat.reflen,\n                          self.stat.predlen, self.stat.reflen)\n'"
fairseq/checkpoint_utils.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport logging\nimport os\nimport re\nimport traceback\nfrom collections import OrderedDict\nfrom typing import Union\n\nimport torch\nfrom fairseq.file_io import PathManager\nfrom fairseq.models import FairseqDecoder, FairseqEncoder\nfrom torch.serialization import default_restore_location\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef save_checkpoint(args, trainer, epoch_itr, val_loss):\n    from fairseq import distributed_utils, meters\n\n    # only one worker should attempt to create the required dir\n    if args.distributed_rank == 0:\n        os.makedirs(args.save_dir, exist_ok=True)\n\n    prev_best = getattr(save_checkpoint, ""best"", val_loss)\n    if val_loss is not None:\n        best_function = max if args.maximize_best_checkpoint_metric else min\n        save_checkpoint.best = best_function(val_loss, prev_best)\n\n    if args.no_save or not trainer.is_data_parallel_master:\n        return\n\n    def is_better(a, b):\n        return a >= b if args.maximize_best_checkpoint_metric else a <= b\n\n    write_timer = meters.StopwatchMeter()\n    write_timer.start()\n\n    epoch = epoch_itr.epoch\n    end_of_epoch = epoch_itr.end_of_epoch()\n    updates = trainer.get_num_updates()\n\n    suffix = getattr(args, ""checkpoint_suffix"", """")\n    checkpoint_conds = collections.OrderedDict()\n    checkpoint_conds[""checkpoint{}{}.pt"".format(epoch, suffix)] = (\n        end_of_epoch\n        and not args.no_epoch_checkpoints\n        and epoch % args.save_interval == 0\n    )\n    checkpoint_conds[""checkpoint_{}_{}{}.pt"".format(epoch, updates, suffix)] = (\n        not end_of_epoch\n        and args.save_interval_updates > 0\n        and updates % args.save_interval_updates == 0\n    )\n    checkpoint_conds[""checkpoint_best{}.pt"".format(suffix)] = val_loss is not None and (\n        not hasattr(save_checkpoint, ""best"")\n        or is_better(val_loss, save_checkpoint.best)\n    )\n    if val_loss is not None and args.keep_best_checkpoints > 0:\n        checkpoint_conds[""checkpoint.best_{}_{:.2f}.pt"".format(\n            args.best_checkpoint_metric, val_loss)] = (\n            not hasattr(save_checkpoint, ""best"")\n            or is_better(val_loss, save_checkpoint.best)\n        )\n    checkpoint_conds[""checkpoint_last{}.pt"".format(suffix)] = not args.no_last_checkpoints\n\n    extra_state = {""train_iterator"": epoch_itr.state_dict(), ""val_loss"": val_loss}\n    if hasattr(save_checkpoint, ""best""):\n        extra_state.update({""best"": save_checkpoint.best})\n\n    checkpoints = [\n        os.path.join(args.save_dir, fn) for fn, cond in checkpoint_conds.items() if cond\n    ]\n    if len(checkpoints) > 0:\n        trainer.save_checkpoint(checkpoints[0], extra_state)\n        for cp in checkpoints[1:]:\n            PathManager.copy(checkpoints[0], cp, overwrite=True)\n\n        write_timer.stop()\n        logger.info(\n            ""saved checkpoint {} (epoch {} @ {} updates, score {}) (writing took {} seconds)"".format(\n                checkpoints[0], epoch, updates, val_loss, write_timer.sum\n            )\n        )\n\n    if not end_of_epoch and args.keep_interval_updates > 0:\n        # remove old checkpoints; checkpoints are sorted in descending order\n        checkpoints = checkpoint_paths(\n            args.save_dir, pattern=r""checkpoint_\\d+_(\\d+)\\.pt""\n        )\n        for old_chk in checkpoints[args.keep_interval_updates :]:\n            if os.path.lexists(old_chk):\n                os.remove(old_chk)\n\n    if args.keep_last_epochs > 0:\n        # remove old epoch checkpoints; checkpoints are sorted in descending order\n        checkpoints = checkpoint_paths(args.save_dir, pattern=r""checkpoint(\\d+)\\.pt"")\n        for old_chk in checkpoints[args.keep_last_epochs :]:\n            if os.path.lexists(old_chk):\n                os.remove(old_chk)\n\n    if args.keep_best_checkpoints > 0:\n        # only keep the best N checkpoints according to validation metric\n        checkpoints = checkpoint_paths(\n            args.save_dir, pattern=r""checkpoint\\.best_{}_(\\d+\\.?\\d*)\\.pt"".format(args.best_checkpoint_metric))\n        if not args.maximize_best_checkpoint_metric:\n            checkpoints = checkpoints[::-1]\n        for old_chk in checkpoints[args.keep_best_checkpoints:]:\n            if os.path.lexists(old_chk):\n                os.remove(old_chk)\n\n\ndef load_checkpoint(args, trainer, **passthrough_args):\n    """"""\n    Load a checkpoint and restore the training iterator.\n\n    *passthrough_args* will be passed through to\n    ``trainer.get_train_iterator``.\n    """"""\n\n    suffix = getattr(args, ""checkpoint_suffix"", """")\n    if args.restore_file == ""checkpoint_last.pt"":\n        checkpoint_path = os.path.join(args.save_dir, ""checkpoint_last{}.pt"".format(suffix))\n    else:\n        checkpoint_path = args.restore_file\n\n    extra_state = trainer.load_checkpoint(\n        checkpoint_path,\n        args.reset_optimizer,\n        args.reset_lr_scheduler,\n        eval(args.optimizer_overrides),\n        reset_meters=args.reset_meters,\n    )\n\n    if (\n        extra_state is not None\n        and ""best"" in extra_state\n        and not args.reset_optimizer\n        and not args.reset_meters\n    ):\n        save_checkpoint.best = extra_state[""best""]\n\n    if extra_state is not None and not args.reset_dataloader:\n        # restore iterator from checkpoint\n        itr_state = extra_state[""train_iterator""]\n        epoch_itr = trainer.get_train_iterator(\n            epoch=itr_state[""epoch""], load_dataset=True, **passthrough_args\n        )\n        epoch_itr.load_state_dict(itr_state)\n    else:\n        epoch_itr = trainer.get_train_iterator(\n            epoch=1, load_dataset=True, **passthrough_args\n        )\n\n    trainer.lr_step(epoch_itr.epoch)\n\n    return extra_state, epoch_itr\n\n\ndef load_checkpoint_to_cpu(path, arg_overrides=None):\n    """"""Loads a checkpoint to CPU (with upgrading for backward compatibility).""""""\n    with PathManager.open(path, ""rb"") as f:\n        state = torch.load(\n            f, map_location=lambda s, l: default_restore_location(s, ""cpu"")\n        )\n\n    args = state[""args""]\n    if arg_overrides is not None:\n        for arg_name, arg_val in arg_overrides.items():\n            setattr(args, arg_name, arg_val)\n    state = _upgrade_state_dict(state)\n    return state\n\n\ndef load_model_ensemble(filenames, arg_overrides=None, task=None, strict=True, suffix=\'\'):\n    """"""Loads an ensemble of models.\n\n    Args:\n        filenames (List[str]): checkpoint files to load\n        arg_overrides (Dict[str,Any], optional): override model args that\n            were used during model training\n        task (fairseq.tasks.FairseqTask, optional): task to use for loading\n    """"""\n    ensemble, args, _task = load_model_ensemble_and_task(\n        filenames, arg_overrides, task, strict, suffix,\n    )\n    return ensemble, args\n\n\ndef load_model_ensemble_and_task(filenames, arg_overrides=None, task=None, strict=True, suffix=\'\'):\n    from fairseq import tasks\n\n    ensemble = []\n    for filename in filenames:\n        filename = filename.replace("".pt"", suffix + "".pt"")\n        if not PathManager.exists(filename):\n            raise IOError(""Model file not found: {}"".format(filename))\n        state = load_checkpoint_to_cpu(filename, arg_overrides)\n\n        args = state[""args""]\n        if task is None:\n            task = tasks.setup_task(args)\n\n        # build model for ensemble\n        model = task.build_model(args)\n        model.load_state_dict(state[""model""], strict=strict, args=args)\n        ensemble.append(model)\n    return ensemble, args, task\n\n\ndef checkpoint_paths(path, pattern=r""checkpoint(\\d+)\\.pt""):\n    """"""Retrieves all checkpoints found in `path` directory.\n\n    Checkpoints are identified by matching filename to the specified pattern. If\n    the pattern contains groups, the result will be sorted by the first group in\n    descending order.\n    """"""\n    pt_regexp = re.compile(pattern)\n    files = os.listdir(path)\n\n    entries = []\n    for i, f in enumerate(files):\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            idx = float(m.group(1)) if len(m.groups()) > 0 else i\n            entries.append((idx, m.group(0)))\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)]\n\n\ndef torch_persistent_save(*args, **kwargs):\n    for i in range(3):\n        try:\n            return torch.save(*args, **kwargs)\n        except Exception:\n            if i == 2:\n                logger.error(traceback.format_exc())\n\n\ndef save_state(\n    filename,\n    args,\n    model_state_dict,\n    criterion,\n    optimizer,\n    lr_scheduler,\n    num_updates,\n    optim_history=None,\n    extra_state=None,\n):\n    from fairseq import utils\n\n    if optim_history is None:\n        optim_history = []\n    if extra_state is None:\n        extra_state = {}\n    state_dict = {\n        ""args"": args,\n        ""model"": model_state_dict or {},\n        ""optimizer_history"": optim_history\n        + [\n            {\n                ""criterion_name"": criterion.__class__.__name__,\n                ""optimizer_name"": optimizer.__class__.__name__,\n                ""lr_scheduler_state"": lr_scheduler.state_dict(),\n                ""num_updates"": num_updates,\n            }\n        ],\n        ""extra_state"": extra_state,\n    }\n    if utils.has_parameters(criterion):\n        state_dict[""criterion""] = criterion.state_dict()\n    if not args.no_save_optimizer_state:\n        state_dict[""last_optimizer_state""] = optimizer.state_dict()\n\n    # convert all state to CPU\n    state_dict = utils.move_to_cpu(state_dict)\n\n    with PathManager.open(filename, ""wb"") as f:\n        torch_persistent_save(state_dict, f)\n\n\ndef _upgrade_state_dict(state):\n    """"""Helper for upgrading old model checkpoints.""""""\n    from fairseq import models, registry, tasks\n\n    # add optimizer_history\n    if ""optimizer_history"" not in state:\n        state[""optimizer_history""] = [\n            {""criterion_name"": ""CrossEntropyCriterion"", ""best_loss"": state[""best_loss""]}\n        ]\n        state[""last_optimizer_state""] = state[""optimizer""]\n        del state[""optimizer""]\n        del state[""best_loss""]\n    # move extra_state into sub-dictionary\n    if ""epoch"" in state and ""extra_state"" not in state:\n        state[""extra_state""] = {\n            ""epoch"": state[""epoch""],\n            ""batch_offset"": state[""batch_offset""],\n            ""val_loss"": state[""val_loss""],\n        }\n        del state[""epoch""]\n        del state[""batch_offset""]\n        del state[""val_loss""]\n    # reduce optimizer history\'s memory usage (only keep the last state)\n    if ""optimizer"" in state[""optimizer_history""][-1]:\n        state[""last_optimizer_state""] = state[""optimizer_history""][-1][""optimizer""]\n        for optim_hist in state[""optimizer_history""]:\n            del optim_hist[""optimizer""]\n    # record the optimizer class name\n    if ""optimizer_name"" not in state[""optimizer_history""][-1]:\n        state[""optimizer_history""][-1][""optimizer_name""] = ""FairseqNAG""\n    # move best_loss into lr_scheduler_state\n    if ""lr_scheduler_state"" not in state[""optimizer_history""][-1]:\n        state[""optimizer_history""][-1][""lr_scheduler_state""] = {\n            ""best"": state[""optimizer_history""][-1][""best_loss""]\n        }\n        del state[""optimizer_history""][-1][""best_loss""]\n    # keep track of number of updates\n    if ""num_updates"" not in state[""optimizer_history""][-1]:\n        state[""optimizer_history""][-1][""num_updates""] = 0\n    # old model checkpoints may not have separate source/target positions\n    if hasattr(state[""args""], ""max_positions"") and not hasattr(\n        state[""args""], ""max_source_positions""\n    ):\n        state[""args""].max_source_positions = state[""args""].max_positions\n        state[""args""].max_target_positions = state[""args""].max_positions\n    # use stateful training data iterator\n    if ""train_iterator"" not in state[""extra_state""]:\n        state[""extra_state""][""train_iterator""] = {\n            ""epoch"": state[""extra_state""][""epoch""],\n            ""iterations_in_epoch"": state[""extra_state""].get(""batch_offset"", 0),\n        }\n    # default to translation task\n    if not hasattr(state[""args""], ""task""):\n        state[""args""].task = ""translation""\n    # --raw-text and --lazy-load are deprecated\n    if getattr(state[""args""], ""raw_text"", False):\n        state[""args""].dataset_impl = ""raw""\n    elif getattr(state[""args""], ""lazy_load"", False):\n        state[""args""].dataset_impl = ""lazy""\n    # epochs start at 1\n    if state[""extra_state""][""train_iterator""] is not None:\n        state[""extra_state""][""train_iterator""][""epoch""] = max(\n            state[""extra_state""][""train_iterator""].get(""epoch"", 1),\n            1,\n        )\n\n    # set any missing default values in the task, model or other registries\n    registry.set_defaults(state[""args""], tasks.TASK_REGISTRY[state[""args""].task])\n    registry.set_defaults(state[""args""], models.ARCH_MODEL_REGISTRY[state[""args""].arch])\n    for registry_name, REGISTRY in registry.REGISTRIES.items():\n        choice = getattr(state[""args""], registry_name, None)\n        if choice is not None:\n            cls = REGISTRY[""registry""][choice]\n            registry.set_defaults(state[""args""], cls)\n\n    return state\n\n\ndef prune_state_dict(state_dict, args):\n    """"""Prune the given state_dict if desired for LayerDrop\n    (https://arxiv.org/abs/1909.11556).\n\n    Training with LayerDrop allows models to be robust to pruning at inference\n    time. This function prunes state_dict to allow smaller models to be loaded\n    from a larger model and re-maps the existing state_dict for this to occur.\n\n    It\'s called by functions that load models from checkpoints and does not\n    need to be called directly.\n    """"""\n    if not args or args.arch == ""ptt_transformer"":\n        # args should not be none, but don\'t crash if it is.\n        return state_dict\n\n    encoder_layers_to_keep = (\n        args.encoder_layers_to_keep if ""encoder_layers_to_keep"" in vars(args) else None\n    )\n    decoder_layers_to_keep = (\n        args.decoder_layers_to_keep if ""decoder_layers_to_keep"" in vars(args) else None\n    )\n\n    if not encoder_layers_to_keep and not decoder_layers_to_keep:\n        return state_dict\n\n    # apply pruning\n    logger.info(\n        ""Pruning model to specified layer configuration - this works best if the model was trained with LayerDrop""\n    )\n\n    def create_pruning_pass(layers_to_keep, layer_name):\n        keep_layers = sorted(\n            [int(layer_string) for layer_string in layers_to_keep.split("","")]\n        )\n        mapping_dict = {}\n        for i in range(len(keep_layers)):\n            mapping_dict[str(keep_layers[i])] = str(i)\n\n        regex = re.compile(r""^{layer}.*\\.layers\\.(\\d+)"".format(layer=layer_name))\n        return {""substitution_regex"": regex, ""mapping_dict"": mapping_dict}\n\n    pruning_passes = []\n    if encoder_layers_to_keep:\n        pruning_passes.append(create_pruning_pass(encoder_layers_to_keep, ""encoder""))\n    if decoder_layers_to_keep:\n        pruning_passes.append(create_pruning_pass(decoder_layers_to_keep, ""decoder""))\n\n    new_state_dict = {}\n    for layer_name in state_dict.keys():\n        match = re.search(r""\\.layers\\.(\\d+)\\."", layer_name)\n        # if layer has no number in it, it is a supporting layer, such as an\n        # embedding\n        if not match:\n            new_state_dict[layer_name] = state_dict[layer_name]\n            continue\n\n        # otherwise, layer should be pruned.\n        original_layer_number = match.group(1)\n        # figure out which mapping dict to replace from\n        for pruning_pass in pruning_passes:\n            if original_layer_number in pruning_pass[""mapping_dict""] and pruning_pass[\n                ""substitution_regex""\n            ].search(layer_name):\n                new_layer_number = pruning_pass[""mapping_dict""][original_layer_number]\n                substitution_match = pruning_pass[""substitution_regex""].search(\n                    layer_name\n                )\n                new_state_key = (\n                    layer_name[: substitution_match.start(1)]\n                    + new_layer_number\n                    + layer_name[substitution_match.end(1) :]\n                )\n                new_state_dict[new_state_key] = state_dict[layer_name]\n\n    # Since layers are now pruned, *_layers_to_keep are no longer needed.\n    # This is more of ""It would make it work fix"" rather than a proper fix.\n    if ""encoder_layers_to_keep"" in vars(args):\n        args.encoder_layers_to_keep = None\n    if ""decoder_layers_to_keep"" in vars(args):\n        args.decoder_layers_to_keep = None\n\n    return new_state_dict\n\n\ndef load_pretrained_component_from_model(\n    component: Union[FairseqEncoder, FairseqDecoder], checkpoint: str\n):\n    """"""\n    Load a pretrained FairseqEncoder or FairseqDecoder from checkpoint into the\n    provided `component` object. If state_dict fails to load, there may be a\n    mismatch in the architecture of the corresponding `component` found in the\n    `checkpoint` file.\n    """"""\n    if not PathManager.exists(checkpoint):\n        raise IOError(""Model file not found: {}"".format(checkpoint))\n    state = load_checkpoint_to_cpu(checkpoint)\n    if isinstance(component, FairseqEncoder):\n        component_type = ""encoder""\n    elif isinstance(component, FairseqDecoder):\n        component_type = ""decoder""\n    else:\n        raise ValueError(\n            ""component to load must be either a FairseqEncoder or ""\n            ""FairseqDecoder. Loading other component types are not supported.""\n        )\n    component_state_dict = OrderedDict()\n    for key in state[""model""].keys():\n        if key.startswith(component_type):\n            # encoder.input_layers.0.0.weight --> input_layers.0.0.weight\n            component_subkey = key[len(component_type) + 1 :]\n            component_state_dict[component_subkey] = state[""model""][key]\n    component.load_state_dict(component_state_dict, strict=True)\n    return component\n\n\ndef verify_checkpoint_directory(save_dir: str) -> None:\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n    temp_file_path = os.path.join(save_dir, ""dummy"")\n    try:\n        with open(temp_file_path, ""w""):\n            pass\n    except OSError as e:\n        logger.warning(""Unable to access checkpoint save directory: {}"".format(save_dir))\n        raise e\n    else:\n        os.remove(temp_file_path)\n'"
fairseq/distributed_utils.py,24,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\nimport pickle\nimport random\nimport socket\nimport struct\nimport subprocess\nimport warnings\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Mapping\n\nimport torch\nimport torch.distributed as dist\n\nfrom fairseq import utils\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef is_master(args):\n    return args.distributed_rank == 0\n\n\ndef infer_init_method(args):\n    if args.distributed_init_method is not None or getattr(args, \'tpu\', False):\n        return\n\n    # support torch.distributed.launch\n    if all(key in os.environ for key in [\n        \'MASTER_ADDR\', \'MASTER_PORT\', \'WORLD_SIZE\', \'RANK\'\n    ]):\n        args.distributed_init_method = \'env://\'\n        args.distributed_world_size = int(os.environ[\'WORLD_SIZE\'])\n        args.distributed_rank = int(os.environ[\'RANK\'])\n\n    # we can determine the init method automatically for Slurm\n    elif args.distributed_port > 0:\n        node_list = os.environ.get(\'SLURM_STEP_NODELIST\')\n        if node_list is None:\n            node_list = os.environ.get(\'SLURM_JOB_NODELIST\')\n        if node_list is not None:\n            try:\n                hostnames = subprocess.check_output([\'scontrol\', \'show\', \'hostnames\', node_list])\n                args.distributed_init_method = \'tcp://{host}:{port}\'.format(\n                    host=hostnames.split()[0].decode(\'utf-8\'),\n                    port=args.distributed_port,\n                )\n                nnodes = int(os.environ.get(\'SLURM_NNODES\'))\n                ntasks_per_node = os.environ.get(\'SLURM_NTASKS_PER_NODE\')\n                if ntasks_per_node is not None:\n                    ntasks_per_node = int(ntasks_per_node)\n                else:\n                    ntasks = int(os.environ.get(\'SLURM_NTASKS\'))\n                    nnodes = int(os.environ.get(\'SLURM_NNODES\'))\n                    assert ntasks % nnodes == 0\n                    ntasks_per_node = int(ntasks / nnodes)\n                if ntasks_per_node == 1:\n                    assert args.distributed_world_size % nnodes == 0\n                    gpus_per_node = args.distributed_world_size // nnodes\n                    node_id = int(os.environ.get(\'SLURM_NODEID\'))\n                    args.distributed_rank = node_id * gpus_per_node\n                else:\n                    assert ntasks_per_node == args.distributed_world_size // nnodes\n                    args.distributed_no_spawn = True\n                    args.distributed_rank = int(os.environ.get(\'SLURM_PROCID\'))\n                    args.device_id = int(os.environ.get(\'SLURM_LOCALID\'))\n            except subprocess.CalledProcessError as e:  # scontrol failed\n                raise e\n            except FileNotFoundError:  # Slurm is not installed\n                pass\n\n\ndef distributed_init(args):\n    if args.distributed_world_size == 1:\n        raise ValueError(\'Cannot initialize distributed with distributed_world_size=1\')\n\n    if not getattr(args, \'tpu\', False):\n        if torch.distributed.is_initialized():\n            warnings.warn(\'Distributed is already initialized, cannot initialize twice!\')\n        else:\n            logger.info(\'distributed init (rank {}): {}\'.format(\n                args.distributed_rank, args.distributed_init_method,\n            ))\n            dist.init_process_group(\n                backend=args.distributed_backend,\n                init_method=args.distributed_init_method,\n                world_size=args.distributed_world_size,\n                rank=args.distributed_rank,\n            )\n            logger.info(\'initialized host {} as rank {}\'.format(\n                socket.gethostname(), args.distributed_rank,\n            ))\n\n            # perform a dummy all-reduce to initialize the NCCL communicator\n            if torch.cuda.is_available():\n                dist.all_reduce(torch.zeros(1).cuda())\n\n        args.distributed_rank = torch.distributed.get_rank()\n    else:\n        import torch_xla.core.xla_model as xm\n        assert xm.xrt_world_size() == args.distributed_world_size\n        args.device_id = xm.get_local_ordinal()\n        args.distributed_rank = xm.get_ordinal()\n        xm.rendezvous(\'distributed_init\')  # wait for all workers\n        xm.mark_step()\n\n    if is_master(args):\n        logging.getLogger().setLevel(logging.INFO)\n    else:\n        logging.getLogger().setLevel(logging.WARNING)\n\n    if args.model_parallel_size > 1:\n        try:\n            from fairseq.model_parallel.megatron.mpu import (\n                get_model_parallel_rank,\n                initialize_model_parallel,\n                model_parallel_cuda_manual_seed,\n            )\n        except ImportError:\n            raise ImportError(\n                \'\\n\\nPlease install the megatron submodule:\'\n                \'\\n\\n  git submodule update --init \'\n                \'fairseq/model_parallel/megatron\'\n            )\n        initialize_model_parallel(args.model_parallel_size)\n        model_parallel_cuda_manual_seed(args.seed)\n        model_part_number = get_model_parallel_rank()\n        args.checkpoint_suffix += \'-model_part-{0}\'.format(model_part_number)\n    return args.distributed_rank\n\n\ndef _distributed_main(i, main, args, kwargs):\n    args.device_id = i\n    if torch.cuda.is_available() and not args.cpu:\n        torch.cuda.set_device(args.device_id)\n    if args.distributed_rank is None:  # torch.multiprocessing.spawn\n        args.distributed_rank = kwargs.get(\'start_rank\', 0) + i\n\n    args.distributed_rank = distributed_init(args)\n    main(args, **kwargs)\n\n\ndef call_main(args, main, **kwargs):\n    if args.distributed_init_method is None:\n        infer_init_method(args)\n\n    if args.distributed_init_method is not None:\n        # distributed main\n        if torch.cuda.device_count() > 1 and not args.distributed_no_spawn:\n            start_rank = args.distributed_rank\n            args.distributed_rank = None  # assign automatically\n            kwargs[\'start_rank\'] = start_rank\n            torch.multiprocessing.spawn(\n                fn=_distributed_main,\n                args=(main, args, kwargs),\n                nprocs=torch.cuda.device_count(),\n            )\n        else:\n            _distributed_main(args.device_id, main, args, kwargs)\n    elif args.distributed_world_size > 1:\n        # fallback for single node with multiple GPUs\n        assert args.distributed_world_size <= torch.cuda.device_count()\n        port = random.randint(10000, 20000)\n        args.distributed_init_method = \'tcp://localhost:{port}\'.format(port=port)\n        args.distributed_rank = None  # set based on device id\n        torch.multiprocessing.spawn(\n            fn=_distributed_main,\n            args=(main, args, kwargs),\n            nprocs=args.distributed_world_size,\n        )\n    else:\n        # single GPU main\n        main(args, **kwargs)\n\n\ndef get_rank():\n    return dist.get_rank()\n\n\ndef get_world_size():\n    return dist.get_world_size()\n\n\ndef get_default_group():\n    return dist.group.WORLD\n\n\ndef all_reduce(tensor, group=None):\n    if isinstance(group, tuple) and group[0] == \'tpu\':\n        import torch_xla.core.xla_model as xm\n        return xm.all_reduce(\'sum\', [tensor], groups=group[1])\n    else:\n        if group is None:\n            group = get_default_group()\n        return dist.all_reduce(tensor, group=group)\n\n\ndef all_gather_list(data, group=None, max_size=16384):\n    """"""Gathers arbitrary data from all nodes into a list.\n\n    Similar to :func:`~torch.distributed.all_gather` but for arbitrary Python\n    data. Note that *data* must be picklable.\n\n    Args:\n        data (Any): data from the local worker to be gathered on other workers\n        group (optional): group of the collective\n        max_size (int, optional): maximum size of the data to be gathered\n            across workers\n    """"""\n    rank = get_rank()\n    world_size = get_world_size()\n\n    buffer_size = max_size * world_size\n    if not hasattr(all_gather_list, \'_buffer\') or \\\n            all_gather_list._buffer.numel() < buffer_size:\n        all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)\n        all_gather_list._cpu_buffer = torch.ByteTensor(max_size).pin_memory()\n    buffer = all_gather_list._buffer\n    buffer.zero_()\n    cpu_buffer = all_gather_list._cpu_buffer\n\n    data = utils.move_to_cpu(data)\n    enc = pickle.dumps(data)\n    enc_size = len(enc)\n    header_size = 4  # size of header that contains the length of the encoded data\n    size = header_size + enc_size\n    if size > max_size:\n        raise ValueError(\'encoded data size ({}) exceeds max_size ({})\'.format(size, max_size))\n\n    header = struct.pack("">I"", enc_size)\n    cpu_buffer[:size] = torch.ByteTensor(list(header + enc))\n    start = rank * max_size\n    buffer[start:start + size].copy_(cpu_buffer[:size])\n\n    all_reduce(buffer, group=group)\n\n    buffer = buffer.cpu()\n    try:\n        result = []\n        for i in range(world_size):\n            out_buffer = buffer[i * max_size:(i + 1) * max_size]\n            enc_size, = struct.unpack("">I"", bytes(out_buffer[:header_size].tolist()))\n            if enc_size > 0:\n                result.append(pickle.loads(bytes(out_buffer[header_size:header_size + enc_size].tolist())))\n        return result\n    except pickle.UnpicklingError:\n        raise Exception(\n            \'Unable to unpickle data from other workers. all_gather_list requires all \'\n            \'workers to enter the function together, so this error usually indicates \'\n            \'that the workers have fallen out of sync somehow. Workers can fall out of \'\n            \'sync if one of them runs out of memory, or if there are other conditions \'\n            \'in your training script that can cause one worker to finish an epoch \'\n            \'while other workers are still iterating over their portions of the data. \'\n            \'Try rerunning with --ddp-backend=no_c10d and see if that helps.\'\n        )\n\n\ndef all_reduce_dict(\n    data: Mapping[str, Any],\n    device,\n    group=None,\n) -> Dict[str, Any]:\n    """"""\n    AllReduce a dictionary of values across workers. We separately\n    reduce items that are already on the device and items on CPU for\n    better performance.\n\n    Args:\n        data (Mapping[str, Any]): dictionary of data to all-reduce, but\n            cannot be a nested dictionary\n        device (torch.device): device for the reduction\n        group (optional): group of the collective\n    """"""\n    data_keys = list(data.keys())\n\n    # We want to separately reduce items that are already on the\n    # device and items on CPU for performance reasons.\n    cpu_data = OrderedDict()\n    device_data = OrderedDict()\n    for k in data_keys:\n        t = data[k]\n        if not torch.is_tensor(t):\n            cpu_data[k] = torch.tensor(t, dtype=torch.double)\n        elif t.device.type != device.type:\n            cpu_data[k] = t.to(dtype=torch.double)\n        else:\n            device_data[k] = t.to(dtype=torch.double)\n\n    def _all_reduce_dict(data: OrderedDict):\n        if len(data) == 0:\n            return data\n        buf = torch.stack(list(data.values())).to(device=device)\n        all_reduce(buf, group=group)\n        return {k: buf[i] for i, k in enumerate(data)}\n\n    cpu_data = _all_reduce_dict(cpu_data)\n    device_data = _all_reduce_dict(device_data)\n\n    def get_from_stack(key):\n        if key in cpu_data:\n            return cpu_data[key]\n        elif key in device_data:\n            return device_data[key]\n        raise KeyError\n\n    return OrderedDict([(key, get_from_stack(key)) for key in data_keys])\n'"
fairseq/file_io.py,0,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport shutil\nfrom typing import List, Optional\n\n\ntry:\n    from fvcore.common.file_io import PathManager as FVCorePathManager\n\nexcept ImportError:\n    FVCorePathManager = None\n\n\nclass PathManager:\n    """"""\n    Wrapper for insulating OSS I/O (using Python builtin operations) from\n    fvcore\'s PathManager abstraction (for transparently handling various\n    internal backends).\n    """"""\n\n    @staticmethod\n    def open(\n        path: str,\n        mode: str = ""r"",\n        buffering: int = -1,\n        encoding: Optional[str] = None,\n        errors: Optional[str] = None,\n        newline: Optional[str] = None,\n    ):\n        if FVCorePathManager:\n            return FVCorePathManager.open(\n                path=path,\n                mode=mode,\n                buffering=buffering,\n                encoding=encoding,\n                errors=errors,\n                newline=newline,\n            )\n        return open(\n            path,\n            mode=mode,\n            buffering=buffering,\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n        )\n\n    @staticmethod\n    def copy(src_path: str, dst_path: str, overwrite: bool = False) -> bool:\n        if FVCorePathManager:\n            return FVCorePathManager.copy(\n                src_path=src_path, dst_path=dst_path, overwrite=overwrite\n            )\n        return shutil.copyfile(src_path, dst_path)\n\n    @staticmethod\n    def get_local_path(path: str, **kwargs) -> str:\n        if FVCorePathManager:\n            return FVCorePathManager.get_local_path(path, **kwargs)\n        return path\n\n    @staticmethod\n    def exists(path: str) -> bool:\n        if FVCorePathManager:\n            return FVCorePathManager.exists(path)\n        return os.path.exists(path)\n\n    @staticmethod\n    def isfile(path: str) -> bool:\n        if FVCorePathManager:\n            return FVCorePathManager.isfile(path)\n        return os.path.isfile(path)\n\n    @staticmethod\n    def ls(path: str) -> List[str]:\n        if FVCorePathManager:\n            return FVCorePathManager.ls(path)\n        return os.listdir(path)\n\n    @staticmethod\n    def mkdirs(path: str) -> None:\n        if FVCorePathManager:\n            return FVCorePathManager.mkdirs(path)\n        os.makedirs(path, exist_ok=True)\n\n    @staticmethod\n    def rm(path: str) -> None:\n        if FVCorePathManager:\n            return FVCorePathManager.rm(path)\n        os.remove(path)\n\n    @staticmethod\n    def register_handler(handler) -> None:\n        if FVCorePathManager:\n            return FVCorePathManager.register_handler(handler=handler)\n'"
fairseq/file_utils.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from `AllenNLP <https://github.com/allenai/allennlp>`_.\nand `huggingface <https://github.com/huggingface>`_.\n""""""\n\nimport fnmatch\nfrom functools import wraps, partial\nfrom hashlib import sha256\nfrom io import open\nimport json\nimport logging\nimport os\nimport shutil\nimport tarfile\nimport tempfile\n\n\ntry:\n    from torch.hub import _get_torch_home\n    torch_cache_home = _get_torch_home()\nexcept ImportError:\n    torch_cache_home = os.path.expanduser(\n        os.getenv(\'TORCH_HOME\', os.path.join(\n            os.getenv(\'XDG_CACHE_HOME\', \'~/.cache\'), \'torch\')))\ndefault_cache_path = os.path.join(torch_cache_home, \'pytorch_fairseq\')\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_FAIRSEQ_CACHE = Path(\n        os.getenv(\'PYTORCH_FAIRSEQ_CACHE\', default_cache_path))\nexcept (AttributeError, ImportError):\n    PYTORCH_FAIRSEQ_CACHE = os.getenv(\n        \'PYTORCH_FAIRSEQ_CACHE\', default_cache_path)\n\nCONFIG_NAME = ""config.json""\nWEIGHTS_NAME = ""pytorch_model.bin""\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef load_archive_file(archive_file):\n    # redirect to the cache, if necessary\n    try:\n        resolved_archive_file = cached_path(archive_file, cache_dir=None)\n    except EnvironmentError:\n        logger.info(\n            ""Archive name \'{}\' was not found in archive name list. ""\n            ""We assumed \'{}\' was a path or URL but couldn\'t find any file ""\n            ""associated to this path or URL."".format(\n                archive_file,\n                archive_file,\n            )\n        )\n        return None\n\n    if resolved_archive_file == archive_file:\n        logger.info(""loading archive file {}"".format(archive_file))\n    else:\n        logger.info(""loading archive file {} from cache at {}"".format(\n            archive_file, resolved_archive_file))\n\n    # Extract archive to temp dir and replace .tar.bz2 if necessary\n    tempdir = None\n    if not os.path.isdir(resolved_archive_file):\n        tempdir = tempfile.mkdtemp()\n        logger.info(""extracting archive file {} to temp dir {}"".format(\n            resolved_archive_file, tempdir))\n        ext = os.path.splitext(archive_file)[1][1:]\n        with tarfile.open(resolved_archive_file, \'r:\' + ext) as archive:\n            top_dir = os.path.commonprefix(archive.getnames())\n            archive.extractall(tempdir)\n        os.remove(resolved_archive_file)\n        shutil.move(os.path.join(tempdir, top_dir), resolved_archive_file)\n        shutil.rmtree(tempdir)\n\n    return resolved_archive_file\n\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the URL\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(\'utf-8\')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\'utf-8\')\n        etag_hash = sha256(etag_bytes)\n        filename += \'.\' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_FAIRSEQ_CACHE\n    if isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + \'.json\'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[\'url\']\n    etag = metadata[\'etag\']\n\n    return url, etag\n\n\ndef cached_path(url_or_filename, cache_dir=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_FAIRSEQ_CACHE\n    if isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\'http\', \'https\', \'s3\'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \'\':\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        from botocore.exceptions import ClientError\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    """"""Check ETag on S3 object.""""""\n    import boto3\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    """"""Pull a file directly from S3.""""""\n    import boto3\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef request_wrap_timeout(func, url):\n    import requests\n    for attempt, timeout in enumerate([10, 20, 40, 60, 60]):\n        try:\n            return func(timeout=timeout)\n        except requests.exceptions.Timeout as e:\n            logger.warning(""Request for %s timed-out (attempt %d). Retrying with a timeout of %d secs"",\n                           url, attempt, timeout, exc_info=e)\n            continue\n    raise RuntimeError(f""Unable to fetch file {url}"")\n\n\ndef http_get(url, temp_file):\n    import requests\n    from tqdm import tqdm\n\n    req = request_wrap_timeout(partial(requests.get, url, stream=True), url)\n    content_length = req.headers.get(\'Content-Length\')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk:  # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_FAIRSEQ_CACHE\n    if isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url)\n    else:\n        try:\n            import requests\n            response = request_wrap_timeout(partial(requests.head, url, allow_redirects=True), url)\n            if response.status_code != 200:\n                etag = None\n            else:\n                etag = response.headers.get(""ETag"")\n        except EnvironmentError:\n            etag = None\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    # If we don\'t have a connection (etag is None) and can\'t identify the file\n    # try to get the last downloaded one\n    if not os.path.exists(cache_path) and etag is None:\n        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + \'.*\')\n        matching_files = list(filter(lambda s: not s.endswith(\'.json\'), matching_files))\n        if matching_files:\n            cache_path = os.path.join(cache_dir, matching_files[-1])\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, \'wb\') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {\'url\': url, \'etag\': etag}\n            meta_path = cache_path + \'.json\'\n            with open(meta_path, \'w\') as meta_file:\n                output_string = json.dumps(meta)\n                meta_file.write(output_string)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename):\n    \'\'\'\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \'\'\'\n    collection = set()\n    with open(filename, \'r\', encoding=\'utf-8\') as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path, dot=True, lower=True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n'"
fairseq/hub_utils.py,9,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport copy\nimport logging\nimport os\nfrom typing import List, Dict, Iterator, Tuple, Any\n\nimport torch\nfrom torch import nn\n\nfrom fairseq import utils\nfrom fairseq.data import encoders\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef from_pretrained(\n    model_name_or_path,\n    checkpoint_file=\'model.pt\',\n    data_name_or_path=\'.\',\n    archive_map=None,\n    **kwargs\n):\n    from fairseq import checkpoint_utils, file_utils\n\n    if archive_map is not None:\n        if model_name_or_path in archive_map:\n            model_name_or_path = archive_map[model_name_or_path]\n        if data_name_or_path is not None and data_name_or_path in archive_map:\n            data_name_or_path = archive_map[data_name_or_path]\n\n        # allow archive_map to set default arg_overrides (e.g., tokenizer, bpe)\n        # for each model\n        if isinstance(model_name_or_path, dict):\n            for k, v in model_name_or_path.items():\n                if k == \'checkpoint_file\':\n                    checkpoint_file = v\n                elif (\n                    k != \'path\'\n                    # only set kwargs that don\'t already have overrides\n                    and k not in kwargs\n                ):\n                    kwargs[k] = v\n            model_name_or_path = model_name_or_path[\'path\']\n\n    model_path = file_utils.load_archive_file(model_name_or_path)\n\n    # convenience hack for loading data and BPE codes from model archive\n    if data_name_or_path.startswith(\'.\'):\n        kwargs[\'data\'] = os.path.abspath(os.path.join(model_path, data_name_or_path))\n    else:\n        kwargs[\'data\'] = file_utils.load_archive_file(data_name_or_path)\n    for file, arg in {\n        \'code\': \'bpe_codes\',\n        \'bpecodes\': \'bpe_codes\',\n        \'sentencepiece.bpe.model\': \'sentencepiece_vocab\',\n    }.items():\n        path = os.path.join(model_path, file)\n        if os.path.exists(path):\n            kwargs[arg] = path\n\n    if \'user_dir\' in kwargs:\n        utils.import_user_module(argparse.Namespace(user_dir=kwargs[\'user_dir\']))\n\n    models, args, task = checkpoint_utils.load_model_ensemble_and_task(\n        [os.path.join(model_path, cpt) for cpt in checkpoint_file.split(os.pathsep)],\n        arg_overrides=kwargs,\n    )\n\n    return {\n        \'args\': args,\n        \'task\': task,\n        \'models\': models,\n    }\n\n\nclass GeneratorHubInterface(nn.Module):\n    """"""\n    PyTorch Hub interface for generating sequences from a pre-trained\n    translation or language model.\n    """"""\n\n    def __init__(self, args, task, models):\n        super().__init__()\n        self.args = args\n        self.task = task\n        self.models = nn.ModuleList(models)\n        self.src_dict = task.source_dictionary\n        self.tgt_dict = task.target_dictionary\n\n        # optimize model for generation\n        for model in self.models:\n            model.make_generation_fast_(\n                beamable_mm_beam_size=(\n                    None if getattr(args, \'no_beamable_mm\', False)\n                    else getattr(args, \'beam\', 5)\n                ),\n                need_attn=getattr(args, \'print_alignment\', False),\n            )\n\n        # Load alignment dictionary for unknown word replacement\n        # (None if no unknown word replacement, empty if no path to align dictionary)\n        self.align_dict = utils.load_align_dict(getattr(args, \'replace_unk\', None))\n\n        self.tokenizer = encoders.build_tokenizer(args)\n        self.bpe = encoders.build_bpe(args)\n\n        self.max_positions = utils.resolve_max_positions(\n            self.task.max_positions(), *[model.max_positions() for model in models]\n        )\n\n        # this is useful for determining the device\n        self.register_buffer(\'_float_tensor\', torch.tensor([0], dtype=torch.float))\n\n    @property\n    def device(self):\n        return self._float_tensor.device\n\n    def translate(self, sentences: List[str], beam: int = 5, verbose: bool = False, **kwargs) -> List[str]:\n        return self.sample(sentences, beam, verbose, **kwargs)\n\n    def sample(self, sentences: List[str], beam: int = 1, verbose: bool = False, **kwargs) -> List[str]:\n        if isinstance(sentences, str):\n            return self.sample([sentences], beam=beam, verbose=verbose, **kwargs)[0]\n        tokenized_sentences = [self.encode(sentence) for sentence in sentences]\n        batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\n        return [self.decode(hypos[0][\'tokens\']) for hypos in batched_hypos]\n\n    def score(self, sentences: List[str], **kwargs):\n        if isinstance(sentences, str):\n            return self.score([sentences], **kwargs)[0]\n        # NOTE: this doesn\'t support translation tasks currently\n        tokenized_sentences = [self.encode(sentence) for sentence in sentences]\n        return [hypos[0] for hypos in self.generate(tokenized_sentences, score_reference=True, **kwargs)]\n\n    def generate(\n        self,\n        tokenized_sentences: List[torch.LongTensor],\n        beam: int = 5,\n        verbose: bool = False,\n        skip_invalid_size_inputs=False,\n        inference_step_args=None,\n        **kwargs\n    ) -> List[List[Dict[str, torch.Tensor]]]:\n        if torch.is_tensor(tokenized_sentences) and tokenized_sentences.dim() == 1:\n            return self.generate(\n                tokenized_sentences.unsqueeze(0), beam=beam, verbose=verbose, **kwargs\n            )[0]\n\n        # build generator using current args as well as any kwargs\n        gen_args = copy.copy(self.args)\n        gen_args.beam = beam\n        for k, v in kwargs.items():\n            setattr(gen_args, k, v)\n        generator = self.task.build_generator(self.models, gen_args)\n\n        inference_step_args = inference_step_args or {}\n        results = []\n        for batch in self._build_batches(tokenized_sentences, skip_invalid_size_inputs):\n            batch = utils.apply_to_sample(lambda t: t.to(self.device), batch)\n            translations = self.task.inference_step(\n                generator, self.models, batch, **inference_step_args\n            )\n            for id, hypos in zip(batch[""id""].tolist(), translations):\n                results.append((id, hypos))\n\n        # sort output to match input order\n        outputs = [hypos for _, hypos in sorted(results, key=lambda x: x[0])]\n\n        if verbose:\n\n            def getarg(name, default):\n                return getattr(gen_args, name, getattr(self.args, name, default))\n\n            for source_tokens, target_hypotheses in zip(tokenized_sentences, outputs):\n                src_str_with_unk = self.string(source_tokens)\n                logger.info(\'S\\t{}\'.format(src_str_with_unk))\n                for hypo in target_hypotheses:\n                    hypo_str = self.decode(hypo[\'tokens\'])\n                    logger.info(\'H\\t{}\\t{}\'.format(hypo[\'score\'], hypo_str))\n                    logger.info(\'P\\t{}\'.format(\n                        \' \'.join(map(lambda x: \'{:.4f}\'.format(x), hypo[\'positional_scores\'].tolist()))\n                    ))\n                    if hypo[\'alignment\'] is not None and getarg(\'print_alignment\', False):\n                        logger.info(\'A\\t{}\'.format(\n                            \' \'.join(map(lambda x: str(utils.item(x)), hypo[\'alignment\'].int().cpu()))\n                        ))\n        return outputs\n\n    def encode(self, sentence: str) -> torch.LongTensor:\n        sentence = self.tokenize(sentence)\n        sentence = self.apply_bpe(sentence)\n        return self.binarize(sentence)\n\n    def decode(self, tokens: torch.LongTensor) -> str:\n        sentence = self.string(tokens)\n        sentence = self.remove_bpe(sentence)\n        return self.detokenize(sentence)\n\n    def tokenize(self, sentence: str) -> str:\n        if self.tokenizer is not None:\n            sentence = self.tokenizer.encode(sentence)\n        return sentence\n\n    def detokenize(self, sentence: str) -> str:\n        if self.tokenizer is not None:\n            sentence = self.tokenizer.decode(sentence)\n        return sentence\n\n    def apply_bpe(self, sentence: str) -> str:\n        if self.bpe is not None:\n            sentence = self.bpe.encode(sentence)\n        return sentence\n\n    def remove_bpe(self, sentence: str) -> str:\n        if self.bpe is not None:\n            sentence = self.bpe.decode(sentence)\n        return sentence\n\n    def binarize(self, sentence: str) -> torch.LongTensor:\n        return self.src_dict.encode_line(sentence, add_if_not_exist=False).long()\n\n    def string(self, tokens: torch.LongTensor) -> str:\n        return self.tgt_dict.string(tokens)\n\n    def _build_batches(\n        self, tokens: List[List[int]], skip_invalid_size_inputs: bool\n    ) -> Iterator[Dict[str, Any]]:\n        lengths = torch.LongTensor([t.numel() for t in tokens])\n        batch_iterator = self.task.get_batch_iterator(\n            dataset=self.task.build_dataset_for_inference(tokens, lengths),\n            max_tokens=self.args.max_tokens,\n            max_sentences=self.args.max_sentences,\n            max_positions=self.max_positions,\n            ignore_invalid_inputs=skip_invalid_size_inputs,\n        ).next_epoch_itr(shuffle=False)\n        return batch_iterator\n\n\nclass BPEHubInterface(object):\n    """"""PyTorch Hub interface for Byte-Pair Encoding (BPE).""""""\n\n    def __init__(self, bpe, **kwargs):\n        super().__init__()\n        args = argparse.Namespace(bpe=bpe, **kwargs)\n        self.bpe = encoders.build_bpe(args)\n        assert self.bpe is not None\n\n    def encode(self, sentence: str) -> str:\n        return self.bpe.encode(sentence)\n\n    def decode(self, sentence: str) -> str:\n        return self.bpe.decode(sentence)\n\n\nclass TokenizerHubInterface(object):\n    """"""PyTorch Hub interface for tokenization.""""""\n\n    def __init__(self, tokenizer, **kwargs):\n        super().__init__()\n        args = argparse.Namespace(tokenizer=tokenizer, **kwargs)\n        self.tokenizer = encoders.build_tokenizer(args)\n        assert self.tokenizer is not None\n\n    def encode(self, sentence: str) -> str:\n        return self.tokenizer.encode(sentence)\n\n    def decode(self, sentence: str) -> str:\n        return self.tokenizer.decode(sentence)\n'"
fairseq/incremental_decoding_utils.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, Optional\nimport uuid\n\nfrom torch import Tensor\n\n\nclass FairseqIncrementalState(object):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.init_incremental_state()\n\n    def init_incremental_state(self):\n        self._incremental_state_id = str(uuid.uuid4())\n\n    def _get_full_incremental_state_key(self, key: str) -> str:\n        return ""{}.{}"".format(self._incremental_state_id, key)\n\n    def get_incremental_state(\n        self,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]],\n        key: str,\n    ) -> Optional[Dict[str, Optional[Tensor]]]:\n        """"""Helper for getting incremental state for an nn.Module.""""""\n        full_key = self._get_full_incremental_state_key(key)\n        if incremental_state is None or full_key not in incremental_state:\n            return None\n        return incremental_state[full_key]\n\n    def set_incremental_state(\n        self,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]],\n        key: str,\n        value: Dict[str, Optional[Tensor]],\n    ) -> Optional[Dict[str, Dict[str, Optional[Tensor]]]]:\n        """"""Helper for setting incremental state for an nn.Module.""""""\n        if incremental_state is not None:\n            full_key = self._get_full_incremental_state_key(key)\n            incremental_state[full_key] = value\n        return incremental_state\n\n\ndef with_incremental_state(cls):\n    cls.__bases__ = (FairseqIncrementalState,) + tuple(b for b in cls.__bases__ if b != FairseqIncrementalState)\n    return cls\n'"
fairseq/iterative_refinement_generator.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import namedtuple\n\nimport torch\nimport numpy as np\n\nfrom fairseq import utils\n\n\nDecoderOut = namedtuple(\'IterativeRefinementDecoderOut\', [\n    \'output_tokens\',\n    \'output_scores\',\n    \'attn\',\n    \'step\',\n    \'max_step\',\n    \'history\'\n])\n\n\nclass IterativeRefinementGenerator(object):\n    def __init__(\n        self,\n        tgt_dict,\n        models=None,\n        eos_penalty=0.0,\n        max_iter=10,\n        max_ratio=2,\n        beam_size=1,\n        decoding_format=None,\n        retain_dropout=False,\n        adaptive=True,\n        retain_history=False,\n        reranking=False,\n    ):\n        """"""\n        Generates translations based on iterative refinement.\n\n        Args:\n            tgt_dict: target dictionary\n            eos_penalty: if > 0.0, it penalized early-stopping in decoding\n            max_iter: maximum number of refinement iterations\n            max_ratio: generate sequences of maximum length ax, where x is the source length\n            decoding_format: decoding mode in {\'unigram\', \'ensemble\', \'vote\', \'dp\', \'bs\'}\n            retain_dropout: retaining dropout in the inference\n            adaptive: decoding with early stop\n        """"""\n        self.bos = tgt_dict.bos()\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.eos_penalty = eos_penalty\n        self.max_iter = max_iter\n        self.max_ratio = max_ratio\n        self.beam_size = beam_size\n        self.reranking = reranking\n        self.decoding_format = decoding_format\n        self.retain_dropout = retain_dropout\n        self.retain_history = retain_history\n        self.adaptive = adaptive\n        self.models = models\n\n    def generate_batched_itr(\n        self,\n        data_itr,\n        maxlen_a=None,\n        maxlen_b=None,\n        cuda=False,\n        timer=None,\n        prefix_size=0,\n    ):\n        """"""Iterate over a batched dataset and yield individual translations.\n\n        Args:\n            maxlen_a/b: generate sequences of maximum length ax + b,\n                where x is the source sentence length.\n            cuda: use GPU for generation\n            timer: StopwatchMeter for timing generations.\n        """"""\n\n        for sample in data_itr:\n            if ""net_input"" not in sample:\n                continue\n            if timer is not None:\n                timer.start()\n            with torch.no_grad():\n                hypos = self.generate(\n                    self.models,\n                    sample,\n                    prefix_tokens=sample[""target""][:, :prefix_size]\n                    if prefix_size > 0\n                    else None,\n                )\n            if timer is not None:\n                timer.stop(sample[""ntokens""])\n            for i, id in enumerate(sample[""id""]):\n                # remove padding\n                src = utils.strip_pad(sample[""net_input""][""src_tokens""][i, :], self.pad)\n                ref = utils.strip_pad(sample[""target""][i, :], self.pad)\n                yield id, src, ref, hypos[i]\n\n\n    @torch.no_grad()\n    def generate(self, models, sample, prefix_tokens=None):\n\n        # TODO: iterative refinement generator does not support ensemble for now.\n        if not self.retain_dropout:\n            for model in models:\n                model.eval()\n\n        model, reranker = models[0], None\n        if self.reranking:\n            assert len(models) > 1, ""Assuming the last checkpoint is the reranker""\n            assert self.beam_size > 1, ""Reranking requires multiple translation for each example""\n\n            reranker = models[-1]\n            models = models[:-1]\n\n        if len(models) > 1 and hasattr(model, \'enable_ensemble\'):\n            assert model.allow_ensemble, ""{} does not support ensembling"".format(model.__class__.__name__)\n            model.enable_ensemble(models)\n\n        # TODO: better encoder inputs?\n        src_tokens = sample[""net_input""][""src_tokens""]\n        src_lengths = sample[""net_input""][""src_lengths""]\n        bsz, src_len = src_tokens.size()\n\n        # initialize\n        encoder_out = model.forward_encoder([src_tokens, src_lengths])\n        prev_decoder_out = model.initialize_output_tokens(encoder_out, src_tokens)\n\n        if self.beam_size > 1:\n            assert model.allow_length_beam, \\\n                ""{} does not support decoding with length beam."".format(model.__class__.__name__)\n\n            # regenerate data based on length-beam\n            length_beam_order = utils.new_arange(src_tokens, self.beam_size, bsz).t().reshape(-1)\n            encoder_out = model.encoder.reorder_encoder_out(encoder_out, length_beam_order)\n            prev_decoder_out = model.regenerate_length_beam(prev_decoder_out, self.beam_size)\n            bsz = bsz * self.beam_size\n\n        sent_idxs = torch.arange(bsz)\n        prev_output_tokens = prev_decoder_out.output_tokens.clone()\n\n        if self.retain_history:\n            prev_decoder_out = prev_decoder_out._replace(history=[prev_output_tokens])\n\n        finalized = [[] for _ in range(bsz)]\n\n        def is_a_loop(x, y, s, a):\n            b, l_x, l_y = x.size(0), x.size(1), y.size(1)\n            if l_x > l_y:\n                y = torch.cat([y, x.new_zeros(b, l_x - l_y).fill_(self.pad)], 1)\n                s = torch.cat([s, s.new_zeros(b, l_x - l_y)], 1)\n                if a is not None:\n                    a = torch.cat([a, a.new_zeros(b, l_x - l_y, a.size(2))], 1)\n            elif l_x < l_y:\n                x = torch.cat([x, y.new_zeros(b, l_y - l_x).fill_(self.pad)], 1)\n            return (x == y).all(1), y, s, a\n\n        def finalized_hypos(step, prev_out_token, prev_out_score, prev_out_attn):\n            cutoff = prev_out_token.ne(self.pad)\n            tokens = prev_out_token[cutoff]\n            if prev_out_score is None:\n                scores, score = None, None\n            else:\n                scores = prev_out_score[cutoff]\n                score = scores.mean()\n\n            if prev_out_attn is None:\n                hypo_attn, alignment = None, None\n            else:\n                hypo_attn = prev_out_attn[cutoff]\n                alignment = hypo_attn.max(dim=1)[1]\n            return {\n                ""steps"": step,\n                ""tokens"": tokens,\n                ""positional_scores"": scores,\n                ""score"": score,\n                ""hypo_attn"": hypo_attn,\n                ""alignment"": alignment,\n            }\n\n        for step in range(self.max_iter + 1):\n\n            decoder_options = {\n                ""eos_penalty"": self.eos_penalty,\n                ""max_ratio"": self.max_ratio,\n                ""decoding_format"": self.decoding_format,\n            }\n            prev_decoder_out = prev_decoder_out._replace(\n                step=step,\n                max_step=self.max_iter + 1,\n            )\n\n            decoder_out = model.forward_decoder(\n                prev_decoder_out, encoder_out, **decoder_options\n            )\n\n            if self.adaptive:\n                # terminate if there is a loop\n                terminated, out_tokens, out_scores, out_attn = is_a_loop(\n                    prev_output_tokens, decoder_out.output_tokens, decoder_out.output_scores, decoder_out.attn\n                )\n                decoder_out = decoder_out._replace(\n                    output_tokens=out_tokens,\n                    output_scores=out_scores,\n                    attn=out_attn,\n                )\n\n            else:\n                terminated = decoder_out.output_tokens.new_zeros(decoder_out.output_tokens.size(0)).bool()\n\n            if step == self.max_iter:  # reach last iteration, terminate\n                terminated.fill_(1)\n\n            # collect finalized sentences\n            finalized_idxs = sent_idxs[terminated]\n            finalized_tokens = decoder_out.output_tokens[terminated]\n            finalized_scores = decoder_out.output_scores[terminated]\n            finalized_attn = (\n                None if (decoder_out.attn is None or decoder_out.attn.size(0) == 0) else decoder_out.attn[terminated]\n            )\n\n            if self.retain_history:\n                finalized_history_tokens = [h[terminated] for h in decoder_out.history]\n\n            for i in range(finalized_idxs.size(0)):\n                finalized[finalized_idxs[i]] = [\n                    finalized_hypos(\n                        step,\n                        finalized_tokens[i],\n                        finalized_scores[i],\n                        None if finalized_attn is None else finalized_attn[i],\n                    )\n                ]\n\n                if self.retain_history:\n                    finalized[finalized_idxs[i]][0][\'history\'] = []\n                    for j in range(len(finalized_history_tokens)):\n                        finalized[finalized_idxs[i]][0][\'history\'].append(\n                            finalized_hypos(\n                                step,\n                                finalized_history_tokens[j][i],\n                                None, None\n                            )\n                        )\n\n            # check if all terminated\n            if terminated.sum() == terminated.size(0):\n                break\n\n            # for next step\n            not_terminated = ~terminated\n            prev_decoder_out = decoder_out._replace(\n                output_tokens=decoder_out.output_tokens[not_terminated],\n                output_scores=decoder_out.output_scores[not_terminated],\n                attn=decoder_out.attn[not_terminated]\n                if (decoder_out.attn is not None and decoder_out.attn.size(0) > 0)\n                else None,\n                history=[h[not_terminated] for h in decoder_out.history]\n                if decoder_out.history is not None\n                else None,\n            )\n            encoder_out = model.encoder.reorder_encoder_out(encoder_out, not_terminated.nonzero().squeeze())\n            sent_idxs = sent_idxs[not_terminated]\n            prev_output_tokens = prev_decoder_out.output_tokens.clone()\n\n        if self.beam_size > 1:\n            if reranker is not None:\n                finalized = self.rerank(\n                    reranker, finalized, [src_tokens, src_lengths], self.beam_size\n                )\n\n            # aggregate information from length beam\n            finalized = [\n                finalized[np.argmax(\n                    [finalized[self.beam_size * i + j][0][\'score\'] for j in range(self.beam_size)]\n                    ) + self.beam_size * i] for i in range(len(finalized) // self.beam_size)\n                ]\n\n        return finalized\n\n    def rerank(self, reranker, finalized, encoder_input, beam_size):\n\n        def rebuild_batch(finalized):\n            finalized_tokens = [f[0][\'tokens\'] for f in finalized]\n            finalized_maxlen = max(f.size(0) for f in finalized_tokens)\n            final_output_tokens = finalized_tokens[0].new_zeros(len(finalized_tokens), finalized_maxlen).fill_(self.pad)\n            for i, f in enumerate(finalized_tokens):\n                final_output_tokens[i, :f.size(0)] = f\n            return final_output_tokens\n\n        final_output_tokens = rebuild_batch(finalized)\n        final_output_tokens[:, 0] = self.eos  # autoregressive model assumes starting with EOS\n\n        reranker_encoder_out = reranker.encoder(*encoder_input)\n        length_beam_order = utils.new_arange(\n            final_output_tokens, beam_size, reranker_encoder_out.encoder_out.size(1)).t().reshape(-1)\n        reranker_encoder_out = reranker.encoder.reorder_encoder_out(reranker_encoder_out, length_beam_order)\n        reranking_scores = reranker.get_normalized_probs(\n            reranker.decoder(final_output_tokens[:, :-1], reranker_encoder_out), True, None)\n        reranking_scores = reranking_scores.gather(2, final_output_tokens[:, 1:, None])\n        reranking_masks = final_output_tokens[:, 1:].ne(self.pad)\n        reranking_scores = reranking_scores[:, :, 0].masked_fill_(~reranking_masks, 0).sum(1)\n        reranking_scores = reranking_scores / reranking_masks.sum(1).type_as(reranking_scores)\n\n        for i in range(len(finalized)):\n            finalized[i][0][\'score\'] = reranking_scores[i]\n\n        return finalized\n'"
fairseq/legacy_distributed_data_parallel.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nA modified version of the legacy DistributedDataParallel module that uses c10d\ncommunication primitives. This version is simpler than the latest PyTorch\nversion and is useful for debugging. Notably it does not overlap gradient\ncommunication with the backward pass, which makes it slower but more robust\nthan the PyTorch version.\n\nThis version also supports the *no_sync* context manager, which allows faster\ntraining with `--update-freq`.\n""""""\n\nfrom contextlib import contextmanager\nimport copy\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nfrom . import distributed_utils\n\n\nclass LegacyDistributedDataParallel(nn.Module):\n    """"""Implements distributed data parallelism at the module level.\n\n    A simplified version of :class:`torch.nn.parallel.DistributedDataParallel`.\n    This version uses a c10d process group for communication and does not\n    broadcast buffers.\n\n    Args:\n        module (~torch.nn.Module): module to be parallelized\n        world_size (int): number of parallel workers\n        process_group (optional): the c10d process group to be used for\n            distributed data all-reduction. If None, the default process group\n            will be used.\n        buffer_size (int, optional): number of elements to buffer before\n            performing all-reduce (default: 256M).\n    """"""\n\n    def __init__(self, module, world_size, process_group=None, buffer_size=2**28):\n        super().__init__()\n\n        self.module = module\n        self.world_size = world_size\n        self.process_group = process_group\n\n        # Never use a bigger buffer than the number of model params\n        self.buffer_size = min(buffer_size, sum(p.numel() for p in module.parameters()))\n        self.buffer = None\n\n        # Flag used by the NCCL backend to make sure we only reduce gradients\n        # one time in the execution engine\n        self.need_reduction = False\n\n        # We can also forcibly accumulate grads locally and only do the\n        # all-reduce at some later time\n        self.accumulate_grads = False\n\n        # For NCCL backend, since every single NCCL call is asynchoronous, we\n        # therefore directly enqueue all the NCCL reduction calls to the\n        # default CUDA stream without spawning up other reduction threads.\n        # This achieves the best performance.\n        self._register_grad_hook()\n\n    def __getstate__(self):\n        attrs = copy.copy(self.__dict__)\n        return attrs\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        self._register_grad_hook()\n\n    @contextmanager\n    def no_sync(self):\n        """"""A context manager to disable gradient synchronization.""""""\n        old_accumulate_grads = self.accumulate_grads\n        self.accumulate_grads = True\n        yield\n        self.accumulate_grads = old_accumulate_grads\n\n    def forward(self, *inputs, **kwargs):\n        return self.module(*inputs, **kwargs)\n\n    def _register_grad_hook(self):\n        """"""\n        This function registers the callback all-reduction function for the\n        NCCL backend. All gradients will be all reduced in one single step.\n        The NCCL reduction will directly be enqueued into the default CUDA\n        stream. Therefore, no synchronization is needed.\n        """"""\n\n        def all_reduce(params):\n            buffer = self.buffer\n            nonzero_buffer = False\n            if len(params) > 1:\n                offset = 0\n                for p in params:\n                    sz = p.numel()\n                    if p.grad is not None:\n                        buffer[offset:offset+sz].copy_(p.grad.data.view(-1))\n                        nonzero_buffer = True\n                    else:\n                        buffer[offset:offset+sz].zero_()\n                    offset += sz\n            else:\n                # we only have a single grad to all-reduce\n                p = params[0]\n                if p.grad is not None:\n                    buffer = p.grad.data\n                    nonzero_buffer = True\n                elif p.numel() <= self.buffer.numel():\n                    buffer = buffer[:p.numel()]\n                    buffer.zero_()\n                else:\n                    buffer = torch.zeros_like(p)\n\n            if nonzero_buffer:\n                buffer.div_(self.world_size)\n\n            distributed_utils.all_reduce(buffer, self.process_group)\n\n            # copy all-reduced grads back into their original place\n            offset = 0\n            for p in params:\n                sz = p.numel()\n                if p.grad is not None:\n                    p.grad.data.copy_(buffer[offset:offset+sz].view_as(p))\n                else:\n                    p.grad = buffer[offset:offset+sz].view_as(p).clone()\n                offset += sz\n\n        def reduction_fn():\n            # This function only needs to be called once\n            if not self.need_reduction or self.accumulate_grads:\n                return\n            self.need_reduction = False\n\n            if self.buffer is None:\n                self.buffer = next(self.module.parameters()).new(self.buffer_size)\n\n            # All-reduce the gradients in buckets\n            offset = 0\n            buffered_params = []\n            for param in self.module.parameters():\n                if not param.requires_grad:\n                    continue\n                if param.grad is None:\n                    param.grad = torch.zeros_like(param)\n                if param.grad.requires_grad:\n                    raise RuntimeError(""DistributedDataParallel only works ""\n                                       ""with gradients that don\'t require ""\n                                       ""grad"")\n                sz = param.numel()\n                if sz > self.buffer.numel():\n                    # all-reduce big params directly\n                    all_reduce([param])\n                else:\n                    if offset + sz > self.buffer.numel():\n                        all_reduce(buffered_params)\n                        offset = 0\n                        buffered_params.clear()\n                    buffered_params.append(param)\n                    offset += sz\n\n            if len(buffered_params) > 0:\n                all_reduce(buffered_params)\n\n        # Now register the reduction hook on the parameters\n        for p in self.module.parameters():\n\n            def allreduce_hook(*unused):\n                self.need_reduction = True\n                Variable._execution_engine.queue_callback(reduction_fn)\n\n            if p.requires_grad:\n                p.register_hook(allreduce_hook)\n'"
fairseq/nan_detector.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport torch\n\nlogger = logging.getLogger(__name__)\n\n\nclass NanDetector:\n    """"""\n        Detects the first NaN or Inf in forward and/or backward pass and logs, together with the module name\n    """"""\n\n    def __init__(self, model, forward=True, backward=True):\n        self.bhooks = []\n        self.fhooks = []\n        self.forward = forward\n        self.backward = backward\n        self.reset()\n\n        for name, mod in model.named_modules():\n            mod.__module_name = name\n            self.add_hooks(mod)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        self.close()\n\n    def add_hooks(self, module):\n        if self.forward:\n            self.fhooks.append(module.register_forward_hook(self.fhook_fn))\n        if self.backward:\n            self.bhooks.append(module.register_backward_hook(self.bhook_fn))\n\n    def reset(self):\n        self.has_printed_f = False\n        self.has_printed_b = False\n\n    def _detect(self, tensor, name, backward):\n        err = None\n        if (\n            tensor.numel() >= 2\n        ):  # single value tensors (like the loss) will not provide much info\n            with torch.no_grad():\n                if torch.isnan(tensor).any():\n                    err = ""NaN""\n                elif torch.isinf(tensor).any():\n                    err = ""Inf""\n        if err is not None:\n            err = f""{err} detected in output of {name}, shape: {tensor.shape}, {\'backward\' if backward else \'forward\'}""\n        return err\n\n    def _apply(self, module, inp, x, backward):\n        if torch.is_tensor(x):\n            if isinstance(inp, tuple) and len(inp) > 0:\n                inp = inp[0]\n            err = self._detect(x, module.__module_name, backward)\n            if err is not None:\n                if torch.is_tensor(inp) and not backward:\n                    err += (\n                        f"" input max: {inp.max().item()}, input min: {inp.min().item()}""\n                    )\n\n                has_printed_attr = \'has_printed_b\' if backward else \'has_printed_f\'\n                logger.warning(err)\n                setattr(self, has_printed_attr, True)\n        elif isinstance(x, dict):\n            for v in x.values():\n                self._apply(module, inp, v, backward)\n        elif isinstance(x, list) or isinstance(x, tuple):\n            for v in x:\n                self._apply(module, inp, v, backward)\n\n    def fhook_fn(self, module, inp, output):\n        if not self.has_printed_f:\n            self._apply(module, inp, output, backward=False)\n\n    def bhook_fn(self, module, inp, output):\n        if not self.has_printed_b:\n            self._apply(module, inp, output, backward=True)\n\n    def close(self):\n        for hook in self.fhooks + self.bhooks:\n            hook.remove()\n'"
fairseq/options.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport sys\nfrom typing import Callable, List, Optional\n\nimport torch\n\nfrom fairseq import utils\nfrom fairseq.data.indexed_dataset import get_available_dataset_impl\n\n\ndef get_preprocessing_parser(default_task=""translation""):\n    parser = get_parser(""Preprocessing"", default_task)\n    add_preprocess_args(parser)\n    return parser\n\n\ndef get_training_parser(default_task=""translation""):\n    parser = get_parser(""Trainer"", default_task)\n    add_dataset_args(parser, train=True)\n    add_distributed_training_args(parser)\n    add_model_args(parser)\n    add_optimization_args(parser)\n    add_checkpoint_args(parser)\n    return parser\n\n\ndef get_generation_parser(interactive=False, default_task=""translation""):\n    parser = get_parser(""Generation"", default_task)\n    add_dataset_args(parser, gen=True)\n    add_generation_args(parser)\n    if interactive:\n        add_interactive_args(parser)\n    return parser\n\n\ndef get_interactive_generation_parser(default_task=""translation""):\n    return get_generation_parser(interactive=True, default_task=default_task)\n\n\ndef get_eval_lm_parser(default_task=""language_modeling""):\n    parser = get_parser(""Evaluate Language Model"", default_task)\n    add_dataset_args(parser, gen=True)\n    add_distributed_training_args(parser, default_world_size=1)\n    add_eval_lm_args(parser)\n    return parser\n\n\ndef get_validation_parser(default_task=None):\n    parser = get_parser(""Validation"", default_task)\n    add_dataset_args(parser, train=True)\n    add_distributed_training_args(parser, default_world_size=1)\n    group = parser.add_argument_group(""Evaluation"")\n    add_common_eval_args(group)\n    return parser\n\n\ndef eval_str_list(x, type=float):\n    if x is None:\n        return None\n    if isinstance(x, str):\n        x = eval(x)\n    try:\n        return list(map(type, x))\n    except TypeError:\n        return [type(x)]\n\n\ndef eval_bool(x, default=False):\n    if x is None:\n        return default\n    try:\n        return bool(eval(x))\n    except TypeError:\n        return default\n\n\ndef parse_args_and_arch(\n    parser: argparse.ArgumentParser,\n    input_args: List[str] = None,\n    parse_known: bool = False,\n    suppress_defaults: bool = False,\n    modify_parser: Optional[Callable[[argparse.ArgumentParser], None]] = None,\n):\n    """"""\n    Args:\n        parser (ArgumentParser): the parser\n        input_args (List[str]): strings to parse, defaults to sys.argv\n        parse_known (bool): only parse known arguments, similar to\n            `ArgumentParser.parse_known_args`\n        suppress_defaults (bool): parse while ignoring all default values\n        modify_parser (Optional[Callable[[ArgumentParser], None]]):\n            function to modify the parser, e.g., to set default values\n    """"""\n    if suppress_defaults:\n        # Parse args without any default values. This requires us to parse\n        # twice, once to identify all the necessary task/model args, and a second\n        # time with all defaults set to None.\n        args = parse_args_and_arch(\n            parser,\n            input_args=input_args,\n            parse_known=parse_known,\n            suppress_defaults=False,\n        )\n        suppressed_parser = argparse.ArgumentParser(add_help=False, parents=[parser])\n        suppressed_parser.set_defaults(**{k: None for k, v in vars(args).items()})\n        args = suppressed_parser.parse_args(input_args)\n        return argparse.Namespace(\n            **{k: v for k, v in vars(args).items() if v is not None}\n        )\n\n    from fairseq.models import ARCH_MODEL_REGISTRY, ARCH_CONFIG_REGISTRY\n\n    # Before creating the true parser, we need to import optional user module\n    # in order to eagerly import custom tasks, optimizers, architectures, etc.\n    usr_parser = argparse.ArgumentParser(add_help=False, allow_abbrev=False)\n    usr_parser.add_argument(""--user-dir"", default=None)\n    usr_args, _ = usr_parser.parse_known_args(input_args)\n    utils.import_user_module(usr_args)\n\n    if modify_parser is not None:\n        modify_parser(parser)\n\n    # The parser doesn\'t know about model/criterion/optimizer-specific args, so\n    # we parse twice. First we parse the model/criterion/optimizer, then we\n    # parse a second time after adding the *-specific arguments.\n    # If input_args is given, we will parse those args instead of sys.argv.\n    args, _ = parser.parse_known_args(input_args)\n\n    # Add model-specific args to parser.\n    if hasattr(args, ""arch""):\n        model_specific_group = parser.add_argument_group(\n            ""Model-specific configuration"",\n            # Only include attributes which are explicitly given as command-line\n            # arguments or which have default values.\n            argument_default=argparse.SUPPRESS,\n        )\n        ARCH_MODEL_REGISTRY[args.arch].add_args(model_specific_group)\n\n    # Add *-specific args to parser.\n    from fairseq.registry import REGISTRIES\n\n    for registry_name, REGISTRY in REGISTRIES.items():\n        choice = getattr(args, registry_name, None)\n        if choice is not None:\n            cls = REGISTRY[""registry""][choice]\n            if hasattr(cls, ""add_args""):\n                cls.add_args(parser)\n    if hasattr(args, ""task""):\n        from fairseq.tasks import TASK_REGISTRY\n\n        TASK_REGISTRY[args.task].add_args(parser)\n    if getattr(args, ""use_bmuf"", False):\n        # hack to support extra args for block distributed data parallelism\n        from fairseq.optim.bmuf import FairseqBMUF\n\n        FairseqBMUF.add_args(parser)\n\n    # Modify the parser a second time, since defaults may have been reset\n    if modify_parser is not None:\n        modify_parser(parser)\n\n    # Parse a second time.\n    if parse_known:\n        args, extra = parser.parse_known_args(input_args)\n    else:\n        args = parser.parse_args(input_args)\n        extra = None\n\n    # Post-process args.\n    if hasattr(args, ""max_sentences_valid"") and args.max_sentences_valid is None:\n        args.max_sentences_valid = args.max_sentences\n    if hasattr(args, ""max_tokens_valid"") and args.max_tokens_valid is None:\n        args.max_tokens_valid = args.max_tokens\n    if getattr(args, ""memory_efficient_fp16"", False):\n        args.fp16 = True\n    if getattr(args, ""memory_efficient_bf16"", False):\n        args.bf16 = True\n    args.tpu = getattr(args, ""tpu"", False)\n    args.bf16 = getattr(args, ""bf16"", False)\n    if args.bf16:\n        args.tpu = True\n    if args.tpu and args.fp16:\n        raise ValueError(""Cannot combine --fp16 and --tpu, use --bf16 on TPUs"")\n\n    # Apply architecture configuration.\n    if hasattr(args, ""arch""):\n        ARCH_CONFIG_REGISTRY[args.arch](args)\n\n    if parse_known:\n        return args, extra\n    else:\n        return args\n\n\ndef get_parser(desc, default_task=""translation""):\n    # Before creating the true parser, we need to import optional user module\n    # in order to eagerly import custom tasks, optimizers, architectures, etc.\n    usr_parser = argparse.ArgumentParser(add_help=False, allow_abbrev=False)\n    usr_parser.add_argument(""--user-dir"", default=None)\n    usr_args, _ = usr_parser.parse_known_args()\n    utils.import_user_module(usr_args)\n\n    parser = argparse.ArgumentParser(allow_abbrev=False)\n    # fmt: off\n    parser.add_argument(\'--no-progress-bar\', action=\'store_true\', help=\'disable progress bar\')\n    parser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                        help=\'log progress every N batches (when progress bar is disabled)\')\n    parser.add_argument(\'--log-format\', default=None, help=\'log format to use\',\n                        choices=[\'json\', \'none\', \'simple\', \'tqdm\'])\n    parser.add_argument(\'--tensorboard-logdir\', metavar=\'DIR\', default=\'\',\n                        help=\'path to save logs for tensorboard, should match --logdir \'\n                             \'of running tensorboard (default: no tensorboard logging)\')\n    parser.add_argument(\'--seed\', default=1, type=int, metavar=\'N\',\n                        help=\'pseudo random number generator seed\')\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'use CPU instead of CUDA\')\n    parser.add_argument(\'--tpu\', action=\'store_true\', help=\'use TPU instead of CUDA\')\n    parser.add_argument(\'--bf16\', action=\'store_true\', help=\'use bfloat16; implies --tpu\')\n    parser.add_argument(\'--fp16\', action=\'store_true\', help=\'use FP16\')\n    parser.add_argument(\'--memory-efficient-bf16\', action=\'store_true\',\n                        help=\'use a memory-efficient version of BF16 training; implies --bf16\')\n    parser.add_argument(\'--memory-efficient-fp16\', action=\'store_true\',\n                        help=\'use a memory-efficient version of FP16 training; implies --fp16\')\n    parser.add_argument(\'--fp16-no-flatten-grads\', action=\'store_true\',\n                        help=\'don\\\'t flatten FP16 grads tensor\')\n    parser.add_argument(\'--fp16-init-scale\', default=2 ** 7, type=int,\n                        help=\'default FP16 loss scale\')\n    parser.add_argument(\'--fp16-scale-window\', type=int,\n                        help=\'number of updates before increasing loss scale\')\n    parser.add_argument(\'--fp16-scale-tolerance\', default=0.0, type=float,\n                        help=\'pct of updates that can overflow before decreasing the loss scale\')\n    parser.add_argument(\'--min-loss-scale\', default=1e-4, type=float, metavar=\'D\',\n                        help=\'minimum FP16 loss scale, after which training is stopped\')\n    parser.add_argument(\'--threshold-loss-scale\', type=float,\n                        help=\'threshold FP16 loss scale from below\')\n    parser.add_argument(\'--user-dir\', default=None,\n                        help=\'path to a python module containing custom extensions (tasks and/or architectures)\')\n    parser.add_argument(\'--empty-cache-freq\', default=0, type=int,\n                        help=\'how often to clear the PyTorch CUDA cache (0 to disable)\')\n    parser.add_argument(\'--all-gather-list-size\', default=16384, type=int,\n                        help=\'number of bytes reserved for gathering stats from workers\')\n    parser.add_argument(\'--model-parallel-size\', type=int, metavar=\'N\',\n                        default=1,\n                        help=\'total number of GPUs to parallelize model over\')\n    parser.add_argument(\'--checkpoint-suffix\', default=\'\',\n                        help=\'suffix to add to the checkpoint file name\')\n    parser.add_argument(\'--quantization-config-path\', default=None,\n                        help=\'path to quantization config file\')\n\n    from fairseq.registry import REGISTRIES\n    for registry_name, REGISTRY in REGISTRIES.items():\n        parser.add_argument(\n            \'--\' + registry_name.replace(\'_\', \'-\'),\n            default=REGISTRY[\'default\'],\n            choices=REGISTRY[\'registry\'].keys(),\n        )\n\n    # Task definitions can be found under fairseq/tasks/\n    from fairseq.tasks import TASK_REGISTRY\n    parser.add_argument(\'--task\', metavar=\'TASK\', default=default_task,\n                        choices=TASK_REGISTRY.keys(),\n                        help=\'task\')\n    # fmt: on\n    return parser\n\n\ndef add_preprocess_args(parser):\n    group = parser.add_argument_group(""Preprocessing"")\n    # fmt: off\n    group.add_argument(""-s"", ""--source-lang"", default=None, metavar=""SRC"",\n                       help=""source language"")\n    group.add_argument(""-t"", ""--target-lang"", default=None, metavar=""TARGET"",\n                       help=""target language"")\n    group.add_argument(""--trainpref"", metavar=""FP"", default=None,\n                       help=""train file prefix"")\n    group.add_argument(""--validpref"", metavar=""FP"", default=None,\n                       help=""comma separated, valid file prefixes"")\n    group.add_argument(""--testpref"", metavar=""FP"", default=None,\n                       help=""comma separated, test file prefixes"")\n    group.add_argument(""--align-suffix"", metavar=""FP"", default=None,\n                       help=""alignment file suffix"")\n    group.add_argument(""--destdir"", metavar=""DIR"", default=""data-bin"",\n                       help=""destination dir"")\n    group.add_argument(""--thresholdtgt"", metavar=""N"", default=0, type=int,\n                       help=""map words appearing less than threshold times to unknown"")\n    group.add_argument(""--thresholdsrc"", metavar=""N"", default=0, type=int,\n                       help=""map words appearing less than threshold times to unknown"")\n    group.add_argument(""--tgtdict"", metavar=""FP"",\n                       help=""reuse given target dictionary"")\n    group.add_argument(""--srcdict"", metavar=""FP"",\n                       help=""reuse given source dictionary"")\n    group.add_argument(""--nwordstgt"", metavar=""N"", default=-1, type=int,\n                       help=""number of target words to retain"")\n    group.add_argument(""--nwordssrc"", metavar=""N"", default=-1, type=int,\n                       help=""number of source words to retain"")\n    group.add_argument(""--alignfile"", metavar=""ALIGN"", default=None,\n                       help=""an alignment file (optional)"")\n    parser.add_argument(\'--dataset-impl\', metavar=\'FORMAT\', default=\'mmap\',\n                        choices=get_available_dataset_impl(),\n                        help=\'output dataset implementation\')\n    group.add_argument(""--joined-dictionary"", action=""store_true"",\n                       help=""Generate joined dictionary"")\n    group.add_argument(""--only-source"", action=""store_true"",\n                       help=""Only process the source language"")\n    group.add_argument(""--padding-factor"", metavar=""N"", default=8, type=int,\n                       help=""Pad dictionary size to be multiple of N"")\n    group.add_argument(""--workers"", metavar=""N"", default=1, type=int,\n                       help=""number of parallel workers"")\n    # fmt: on\n    return parser\n\n\ndef add_dataset_args(parser, train=False, gen=False):\n    group = parser.add_argument_group(""Dataset and data loading"")\n    # fmt: off\n    group.add_argument(\'--num-workers\', default=1, type=int, metavar=\'N\',\n                       help=\'how many subprocesses to use for data loading\')\n    group.add_argument(\'--skip-invalid-size-inputs-valid-test\', action=\'store_true\',\n                       help=\'ignore too long or too short lines in valid and test set\')\n    group.add_argument(\'--max-tokens\', type=int, metavar=\'N\',\n                       help=\'maximum number of tokens in a batch\')\n    group.add_argument(\'--max-sentences\', \'--batch-size\', type=int, metavar=\'N\',\n                       help=\'maximum number of sentences in a batch\')\n    group.add_argument(\'--required-batch-size-multiple\', default=8, type=int, metavar=\'N\',\n                       help=\'batch size will be a multiplier of this value\')\n    parser.add_argument(\'--dataset-impl\', metavar=\'FORMAT\',\n                        choices=get_available_dataset_impl(),\n                        help=\'output dataset implementation\')\n    group.add_argument(\'--data-buffer-size\', default=2, type=int, metavar=\'N\',\n                        help=\'Number of batches to preload\')\n    if train:\n        group.add_argument(\'--train-subset\', default=\'train\', metavar=\'SPLIT\',\n                           help=\'data subset to use for training (e.g. train, valid, test)\')\n        group.add_argument(\'--valid-subset\', default=\'valid\', metavar=\'SPLIT\',\n                           help=\'comma separated list of data subsets to use for validation\'\n                                \' (e.g. train, valid, test)\')\n        group.add_argument(\'--validate-interval\', type=int, default=1, metavar=\'N\',\n                           help=\'validate every N epochs\')\n        group.add_argument(\'--fixed-validation-seed\', default=None, type=int, metavar=\'N\',\n                           help=\'specified random seed for validation\')\n        group.add_argument(\'--disable-validation\', action=\'store_true\',\n                           help=\'disable validation\')\n        group.add_argument(\'--max-tokens-valid\', type=int, metavar=\'N\',\n                           help=\'maximum number of tokens in a validation batch\'\n                                \' (defaults to --max-tokens)\')\n        group.add_argument(\'--max-sentences-valid\', type=int, metavar=\'N\',\n                           help=\'maximum number of sentences in a validation batch\'\n                                \' (defaults to --max-sentences)\')\n        group.add_argument(\'--curriculum\', default=0, type=int, metavar=\'N\',\n                           help=\'don\\\'t shuffle batches for first N epochs\')\n    if gen:\n        group.add_argument(\'--gen-subset\', default=\'test\', metavar=\'SPLIT\',\n                           help=\'data subset to generate (train, valid, test)\')\n        group.add_argument(\'--num-shards\', default=1, type=int, metavar=\'N\',\n                           help=\'shard generation over N shards\')\n        group.add_argument(\'--shard-id\', default=0, type=int, metavar=\'ID\',\n                           help=\'id of the shard to generate (id < num_shards)\')\n    # fmt: on\n    return group\n\n\ndef add_distributed_training_args(parser, default_world_size=None):\n    group = parser.add_argument_group(""Distributed training"")\n    # fmt: off\n    if default_world_size is None:\n        default_world_size = max(1, torch.cuda.device_count())\n    group.add_argument(\'--distributed-world-size\', type=int, metavar=\'N\',\n                       default=default_world_size,\n                       help=\'total number of GPUs across all nodes (default: all visible GPUs)\')\n    group.add_argument(\'--distributed-rank\', default=0, type=int,\n                       help=\'rank of the current worker\')\n    group.add_argument(\'--distributed-backend\', default=\'nccl\', type=str,\n                       help=\'distributed backend\')\n    group.add_argument(\'--distributed-init-method\', default=None, type=str,\n                       help=\'typically tcp://hostname:port that will be used to \'\n                            \'establish initial connetion\')\n    group.add_argument(\'--distributed-port\', default=-1, type=int,\n                       help=\'port number (not required if using --distributed-init-method)\')\n    group.add_argument(\'--device-id\', \'--local_rank\', default=0, type=int,\n                       help=\'which GPU to use (usually configured automatically)\')\n    group.add_argument(\'--distributed-no-spawn\', action=\'store_true\',\n                       help=\'do not spawn multiple processes even if multiple GPUs are visible\')\n    # ""c10d"" is PyTorch\'s DDP implementation and provides the fastest\n    # training. ""no_c10d"" is a more robust, but slightly slower DDP\n    # implementation. Try this if you get warning messages about\n    # inconsistent gradients between workers, or if some of your model\n    # parameters are not always used.\n    group.add_argument(\'--ddp-backend\', default=\'c10d\', type=str,\n                       choices=[\'c10d\', \'no_c10d\'],\n                       help=\'DistributedDataParallel backend\')\n    group.add_argument(\'--bucket-cap-mb\', default=25, type=int, metavar=\'MB\',\n                       help=\'bucket size for reduction\')\n    group.add_argument(\'--fix-batches-to-gpus\', action=\'store_true\',\n                       help=\'don\\\'t shuffle batches between GPUs; this reduces overall \'\n                            \'randomness and may affect precision but avoids the cost of \'\n                            \'re-reading the data\')\n    group.add_argument(\'--find-unused-parameters\', default=False, action=\'store_true\',\n                       help=\'disable unused parameter detection (not applicable to \'\n                       \'no_c10d ddp-backend\')\n    group.add_argument(\'--fast-stat-sync\', default=False, action=\'store_true\',\n                       help=\'[deprecated] this is now defined per Criterion\')\n    group.add_argument(\'--broadcast-buffers\', default=False, action=\'store_true\',\n                       help=\'Copy non-trainable parameters between GPUs, such as \'\n                      \'batchnorm population statistics\')\n\n    group.add_argument(\'--distributed-wrapper\', default=\'DDP\', type=str,\n                       choices=[\'DDP\', \'SlowMo\'],\n                       help=\'DistributedDataParallel backend\')\n    # Add arguments for SlowMo - these will be used when SlowMo is enabled via above\n    group.add_argument(\'--slowmo-momentum\', default=None, type=float,\n                       help=\'SlowMo momentum term; by default use 0.0 for 16 GPUs, \'\n                            \'0.2 for 32 GPUs; 0.5 for 64 GPUs, 0.6 for > 64 GPUs\')\n    group.add_argument(\'--slowmo-algorithm\', default=\'LocalSGD\', choices=[\'LocalSGD\', \'SGP\'],\n                       help=\'whether to use LocalSGD or SGP\')\n    group.add_argument(\'--localsgd-frequency\', default=3, type=int,\n                       help=\'Local SGD allreduce frequency\')\n    group.add_argument(\'--nprocs-per-node\', type=int, metavar=\'N\',\n                       default=max(1, torch.cuda.device_count()),\n                       help=\'number of GPUs in each node. An allreduce operation across GPUs in \'\n                            \'a node is very fast. Hence, we do allreduce across GPUs in a node, \'\n                            \'and gossip across different nodes\')\n    # fmt: on\n    return group\n\n\ndef add_optimization_args(parser):\n    group = parser.add_argument_group(""Optimization"")\n    # fmt: off\n    group.add_argument(\'--max-epoch\', \'--me\', default=0, type=int, metavar=\'N\',\n                       help=\'force stop training at specified epoch\')\n    group.add_argument(\'--max-update\', \'--mu\', default=0, type=int, metavar=\'N\',\n                       help=\'force stop training at specified update\')\n    group.add_argument(\'--clip-norm\', default=25, type=float, metavar=\'NORM\',\n                       help=\'clip threshold of gradients\')\n    group.add_argument(\'--sentence-avg\', action=\'store_true\',\n                       help=\'normalize gradients by the number of sentences in a batch\'\n                            \' (default is to normalize by number of tokens)\')\n    group.add_argument(\'--update-freq\', default=\'1\', metavar=\'N1,N2,...,N_K\',\n                       type=lambda uf: eval_str_list(uf, type=int),\n                       help=\'update parameters every N_i batches, when in epoch i\')\n    group.add_argument(\'--lr\', \'--learning-rate\', default=\'0.25\', type=eval_str_list,\n                       metavar=\'LR_1,LR_2,...,LR_N\',\n                       help=\'learning rate for the first N epochs; all epochs >N using LR_N\'\n                            \' (note: this may be interpreted differently depending on --lr-scheduler)\')\n    group.add_argument(\'--min-lr\', default=-1, type=float, metavar=\'LR\',\n                       help=\'stop training when the learning rate reaches this minimum\')\n    group.add_argument(\'--use-bmuf\', default=False, action=\'store_true\',\n                       help=\'specify global optimizer for syncing models on different GPUs/shards\')\n    # fmt: on\n    return group\n\n\ndef add_checkpoint_args(parser):\n    group = parser.add_argument_group(""Checkpointing"")\n    # fmt: off\n    group.add_argument(\'--save-dir\', metavar=\'DIR\', default=\'checkpoints\',\n                       help=\'path to save checkpoints\')\n    group.add_argument(\'--restore-file\', default=\'checkpoint_last.pt\',\n                       help=\'filename from which to load checkpoint \'\n                            \'(default: <save-dir>/checkpoint_last.pt\')\n    group.add_argument(\'--reset-dataloader\', action=\'store_true\',\n                       help=\'if set, does not reload dataloader state from the checkpoint\')\n    group.add_argument(\'--reset-lr-scheduler\', action=\'store_true\',\n                       help=\'if set, does not load lr scheduler state from the checkpoint\')\n    group.add_argument(\'--reset-meters\', action=\'store_true\',\n                       help=\'if set, does not load meters from the checkpoint\')\n    group.add_argument(\'--reset-optimizer\', action=\'store_true\',\n                       help=\'if set, does not load optimizer state from the checkpoint\')\n    group.add_argument(\'--optimizer-overrides\', default=""{}"", type=str, metavar=\'DICT\',\n                       help=\'a dictionary used to override optimizer args when loading a checkpoint\')\n    group.add_argument(\'--save-interval\', type=int, default=1, metavar=\'N\',\n                       help=\'save a checkpoint every N epochs\')\n    group.add_argument(\'--save-interval-updates\', type=int, default=0, metavar=\'N\',\n                       help=\'save a checkpoint (and validate) every N updates\')\n    group.add_argument(\'--keep-interval-updates\', type=int, default=-1, metavar=\'N\',\n                       help=\'keep the last N checkpoints saved with --save-interval-updates\')\n    group.add_argument(\'--keep-last-epochs\', type=int, default=-1, metavar=\'N\',\n                       help=\'keep last N epoch checkpoints\')\n    group.add_argument(\'--keep-best-checkpoints\', type=int, default=-1, metavar=\'N\',\n                       help=\'keep best N checkpoints based on scores\')\n    group.add_argument(\'--no-save\', action=\'store_true\',\n                       help=\'don\\\'t save models or checkpoints\')\n    group.add_argument(\'--no-epoch-checkpoints\', action=\'store_true\',\n                       help=\'only store last and best checkpoints\')\n    group.add_argument(\'--no-last-checkpoints\', action=\'store_true\',\n                       help=\'don\\\'t store last checkpoints\')\n    group.add_argument(\'--no-save-optimizer-state\', action=\'store_true\',\n                       help=\'don\\\'t save optimizer-state as part of checkpoint\')\n    group.add_argument(\'--best-checkpoint-metric\', type=str, default=\'loss\',\n                       help=\'metric to use for saving ""best"" checkpoints\')\n    group.add_argument(\'--maximize-best-checkpoint-metric\', action=\'store_true\',\n                       help=\'select the largest metric value for saving ""best"" checkpoints\')\n    group.add_argument(\'--patience\', type=int, default=-1, metavar=\'N\',\n                       help=(\'early stop training if valid performance doesn\\\'t \'\n                             \'improve for N consecutive validation runs; note \'\n                             \'that this is influenced by --validate-interval\'))\n    # fmt: on\n    return group\n\n\ndef add_common_eval_args(group):\n    # fmt: off\n    group.add_argument(\'--path\', metavar=\'FILE\',\n                       help=\'path(s) to model file(s), colon separated\')\n    group.add_argument(\'--remove-bpe\', nargs=\'?\', const=\'@@ \', default=None,\n                       help=\'remove BPE tokens before scoring (can be set to sentencepiece)\')\n    group.add_argument(\'--quiet\', action=\'store_true\',\n                       help=\'only print final scores\')\n    group.add_argument(\'--model-overrides\', default=""{}"", type=str, metavar=\'DICT\',\n                       help=\'a dictionary used to override model args at generation \'\n                            \'that were used during model training\')\n    group.add_argument(\'--results-path\', metavar=\'RESDIR\', type=str, default=None,\n                       help=\'path to save eval results (optional)""\')\n    # fmt: on\n\n\ndef add_eval_lm_args(parser):\n    group = parser.add_argument_group(""LM Evaluation"")\n    add_common_eval_args(group)\n    # fmt: off\n    group.add_argument(\'--output-word-probs\', action=\'store_true\',\n                       help=\'if set, outputs words and their predicted log probabilities to standard output\')\n    group.add_argument(\'--output-word-stats\', action=\'store_true\',\n                       help=\'if set, outputs word statistics such as word count, average probability, etc\')\n    group.add_argument(\'--context-window\', default=0, type=int, metavar=\'N\',\n                       help=\'ensures that every evaluated token has access to a context of at least this size,\'\n                            \' if possible\')\n    group.add_argument(\'--softmax-batch\', default=sys.maxsize, type=int, metavar=\'N\',\n                       help=\'if BxT is more than this, will batch the softmax over vocab to this amount of tokens\'\n                            \' in order to fit into GPU memory\')\n    # fmt: on\n\n\ndef add_generation_args(parser):\n    group = parser.add_argument_group(""Generation"")\n    add_common_eval_args(group)\n    # fmt: off\n    group.add_argument(\'--beam\', default=5, type=int, metavar=\'N\',\n                       help=\'beam size\')\n    group.add_argument(\'--nbest\', default=1, type=int, metavar=\'N\',\n                       help=\'number of hypotheses to output\')\n    group.add_argument(\'--max-len-a\', default=0, type=float, metavar=\'N\',\n                       help=(\'generate sequences of maximum length ax + b, \'\n                             \'where x is the source length\'))\n    group.add_argument(\'--max-len-b\', default=200, type=int, metavar=\'N\',\n                       help=(\'generate sequences of maximum length ax + b, \'\n                             \'where x is the source length\'))\n    group.add_argument(\'--min-len\', default=1, type=float, metavar=\'N\',\n                       help=(\'minimum generation length\'))\n    group.add_argument(\'--match-source-len\', default=False, action=\'store_true\',\n                       help=(\'generations should match the source length\'))\n    group.add_argument(\'--no-early-stop\', action=\'store_true\',\n                       help=\'deprecated\')\n    group.add_argument(\'--unnormalized\', action=\'store_true\',\n                       help=\'compare unnormalized hypothesis scores\')\n    group.add_argument(\'--no-beamable-mm\', action=\'store_true\',\n                       help=\'don\\\'t use BeamableMM in attention layers\')\n    group.add_argument(\'--lenpen\', default=1, type=float,\n                       help=\'length penalty: <1.0 favors shorter, >1.0 favors longer sentences\')\n    group.add_argument(\'--unkpen\', default=0, type=float,\n                       help=\'unknown word penalty: <0 produces more unks, >0 produces fewer\')\n    group.add_argument(\'--replace-unk\', nargs=\'?\', const=True, default=None,\n                       help=\'perform unknown replacement (optionally with alignment dictionary)\')\n    group.add_argument(\'--sacrebleu\', action=\'store_true\',\n                       help=\'score with sacrebleu\')\n    group.add_argument(\'--score-reference\', action=\'store_true\',\n                       help=\'just score the reference translation\')\n    group.add_argument(\'--prefix-size\', default=0, type=int, metavar=\'PS\',\n                       help=\'initialize generation by target prefix of given length\')\n    group.add_argument(\'--no-repeat-ngram-size\', default=0, type=int, metavar=\'N\',\n                       help=\'ngram blocking such that this size ngram cannot be repeated in the generation\')\n    group.add_argument(\'--sampling\', action=\'store_true\',\n                       help=\'sample hypotheses instead of using beam search\')\n    group.add_argument(\'--sampling-topk\', default=-1, type=int, metavar=\'PS\',\n                       help=\'sample from top K likely next words instead of all words\')\n    group.add_argument(\'--sampling-topp\', default=-1.0, type=float, metavar=\'PS\',\n                       help=\'sample from the smallest set whose cumulative probability mass exceeds p for next words\')\n    group.add_argument(\'--temperature\', default=1., type=float, metavar=\'N\',\n                       help=\'temperature for generation\')\n    group.add_argument(\'--diverse-beam-groups\', default=-1, type=int, metavar=\'N\',\n                       help=\'number of groups for Diverse Beam Search\')\n    group.add_argument(\'--diverse-beam-strength\', default=0.5, type=float, metavar=\'N\',\n                       help=\'strength of diversity penalty for Diverse Beam Search\')\n    group.add_argument(\'--diversity-rate\', default=-1.0, type=float, metavar=\'N\',\n                       help=\'strength of diversity penalty for Diverse Siblings Search\')\n    group.add_argument(\'--print-alignment\', action=\'store_true\',\n                       help=\'if set, uses attention feedback to compute and print alignment to source tokens\')\n    group.add_argument(\'--print-step\', action=\'store_true\')\n\n    # arguments for iterative refinement generator\n    group.add_argument(\'--iter-decode-eos-penalty\', default=0.0, type=float, metavar=\'N\',\n                       help=\'if > 0.0, it penalized early-stopping in decoding.\')\n    group.add_argument(\'--iter-decode-max-iter\', default=10, type=int, metavar=\'N\',\n                       help=\'maximum iterations for iterative refinement.\')\n    group.add_argument(\'--iter-decode-force-max-iter\', action=\'store_true\',\n                       help=\'if set, run exact the maximum number of iterations without early stop\')\n    group.add_argument(\'--iter-decode-with-beam\', default=1, type=int, metavar=\'N\',\n                       help=\'if > 1, model will generate translations varying by the lengths.\')\n    group.add_argument(\'--iter-decode-with-external-reranker\', action=\'store_true\',\n                       help=\'if set, the last checkpoint are assumed to be a reranker to rescore the translations\'),\n    group.add_argument(\'--retain-iter-history\', action=\'store_true\',\n                       help=\'if set, decoding returns the whole history of iterative refinement\')\n\n    # special decoding format for advanced decoding.\n    group.add_argument(\'--decoding-format\', default=None, type=str, choices=[\'unigram\', \'ensemble\', \'vote\', \'dp\', \'bs\'])\n    # fmt: on\n    return group\n\n\ndef add_interactive_args(parser):\n    group = parser.add_argument_group(""Interactive"")\n    # fmt: off\n    group.add_argument(\'--buffer-size\', default=0, type=int, metavar=\'N\',\n                       help=\'read this many sentences into a buffer before processing them\')\n    group.add_argument(\'--input\', default=\'-\', type=str, metavar=\'FILE\',\n                       help=\'file to read from; use - for stdin\')\n    # fmt: on\n\n\ndef add_model_args(parser):\n    group = parser.add_argument_group(""Model configuration"")\n    # fmt: off\n\n    # Model definitions can be found under fairseq/models/\n    #\n    # The model architecture can be specified in several ways.\n    # In increasing order of priority:\n    # 1) model defaults (lowest priority)\n    # 2) --arch argument\n    # 3) --encoder/decoder-* arguments (highest priority)\n    from fairseq.models import ARCH_MODEL_REGISTRY\n    group.add_argument(\'--arch\', \'-a\', default=\'fconv\', metavar=\'ARCH\',\n                       choices=ARCH_MODEL_REGISTRY.keys(),\n                       help=\'Model Architecture\')\n    # fmt: on\n    return group\n'"
fairseq/pdb.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport multiprocessing\nimport os\nimport pdb\nimport sys\n\n\n__all__ = [\'set_trace\']\n\n\n_stdin = [None]\n_stdin_lock = multiprocessing.Lock()\ntry:\n    _stdin_fd = sys.stdin.fileno()\nexcept Exception:\n    _stdin_fd = None\n\n\nclass MultiprocessingPdb(pdb.Pdb):\n    """"""A Pdb wrapper that works in a multiprocessing environment.\n\n    Usage: `from fairseq import pdb; pdb.set_trace()`\n    """"""\n\n    def __init__(self):\n        pdb.Pdb.__init__(self, nosigint=True)\n\n    def _cmdloop(self):\n        stdin_bak = sys.stdin\n        with _stdin_lock:\n            try:\n                if _stdin_fd is not None:\n                    if not _stdin[0]:\n                        _stdin[0] = os.fdopen(_stdin_fd)\n                    sys.stdin = _stdin[0]\n                self.cmdloop()\n            finally:\n                sys.stdin = stdin_bak\n\n\ndef set_trace():\n    pdb = MultiprocessingPdb()\n    pdb.set_trace(sys._getframe().f_back)\n'"
fairseq/quantization_utils.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nfrom fairseq.modules.quantization import pq, quantization_options, scalar\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef quantize_model_scalar(model, args):\n    quant_noise_scalar = getattr(args, \'quant_noise_scalar\', 0)\n    if quant_noise_scalar > 0:\n        # quantize_model edits the model in place\n        scalar.quantize_model_(model, p=quant_noise_scalar, bits=8, update_step=1000)\n    return model\n\n\nclass Quantizer(object):\n\n    def __init__(self, config_path, max_epoch, max_update):\n        try:\n            import yaml\n        except ImportError:\n            raise ImportError(\'Please install yaml with: pip install yaml\')\n\n        # parse config\n        if config_path:\n            with open(config_path) as config_file:\n                config = quantization_options.parse_config_yaml(\n                    yaml.safe_load(config_file)\n                )\n        else:\n            config = quantization_options.parse_config_yaml({})\n\n        self.n_centroids_config = config[""n_centroids""]\n        self.block_sizes_config = config[""block_sizes""]\n        self.layers_to_quantize = config[""layers_to_quantize""]\n\n        # We assume that training will run for a fixed number of epochs\n        # (or updates) and that we should train for equal durations\n        # between iterations of PQ.\n        num_iterations = len(self.layers_to_quantize)\n        if max_epoch > 0:\n            assert max_epoch % num_iterations == 0, (\n                \'for iterative PQ, --max-epoch (={}) must be evenly divisible by \'\n                \'len(layers_to_quantize) (={})\'.format(max_epoch, num_iterations)\n            )\n            self.epoch_schedule = max_epoch // num_iterations\n        else:\n            self.epoch_schedule = None\n        if max_update > 0:\n            assert max_update % num_iterations == 0, (\n                \'for iterative PQ, --max-update (={}) must be evenly divisible by \'\n                \'len(layers_to_quantize) (={})\'.format(max_update, num_iterations)\n            )\n            self.update_schedule = max_update // num_iterations\n        else:\n            self.update_schedule = None\n        assert (self.epoch_schedule is not None) ^ (self.update_schedule is not None), \\\n            \'for iterative PQ, cannot specify both --max-update and --max-epoch\'\n\n        # 0 is a special value for quantization step, which will force\n        # the first call to begin_epoch() to call step()\n        self.quantization_step = 0\n\n    def set_trainer(self, trainer):\n        self.trainer = trainer\n        self.size_tracker = pq.SizeTracker(self.trainer.get_model())\n\n    def step(self):\n        """"""Move to the next stage of quantization.""""""\n        if self.quantization_step >= len(self.layers_to_quantize):\n            # Maybe we just finished the last training step or we loaded\n            # a checkpoint for an iterative PQ model which previously\n            # finished training. Either way, don\'t quantize again.\n            return\n\n        logger.info(\n            \'quantizing model (step={}; layers_to_quantize[step]={})\'.format(\n                self.quantization_step, self.layers_to_quantize[self.quantization_step]\n            )\n        )\n        quantized_layers = pq.quantize_model_(\n            self.trainer.get_model(),\n            self.size_tracker,\n            self.layers_to_quantize,\n            self.block_sizes_config,\n            self.n_centroids_config,\n            step=self.quantization_step,\n        )\n        logger.info(\'quantized layers: {}\'.format(quantized_layers))\n        logger.info(self.size_tracker)\n\n        self.quantization_step += 1\n\n        # reintialize the Trainer since model parameters have changed\n        self.trainer.reinitialize()\n\n    def begin_epoch(self, epoch):\n        """"""Called at the beginning of each epoch (epochs start at 1).""""""\n        if (\n            (\n                self.epoch_schedule is not None\n                and epoch > 0\n                and (epoch - 1) % self.epoch_schedule == 0\n            )\n            # we always step once in the beginning, even if using\n            # update-based quantization\n            or self.quantization_step == 0\n        ):\n            self.step()\n\n    def step_update(self, num_updates):\n        """"""Called at the end of each step.""""""\n        if (\n            self.update_schedule is not None\n            and num_updates > 0\n            and num_updates % self.update_schedule == 0\n        ):\n            self.step()\n\n    def state_dict(self):\n        return {\n            \'n_centroids_config\': self.n_centroids_config,\n            \'block_sizes_config\': self.block_sizes_config,\n            \'layers_to_quantize\': self.layers_to_quantize,\n            \'epoch_schedule\': self.epoch_schedule,\n            \'update_schedule\': self.update_schedule,\n            \'quantization_step\': self.quantization_step,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.n_centroids_config = state_dict[\'n_centroids_config\']\n        self.block_sizes_config = state_dict[\'block_sizes_config\']\n        self.layers_to_quantize = state_dict[\'layers_to_quantize\']\n        self.epoch_schedule = state_dict[\'epoch_schedule\']\n        self.update_schedule = state_dict[\'update_schedule\']\n        self.quantization_step = state_dict[\'quantization_step\']\n'"
fairseq/registry.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\n\n\nREGISTRIES = {}\n\n\ndef setup_registry(\n    registry_name: str,\n    base_class=None,\n    default=None,\n):\n    assert registry_name.startswith(\'--\')\n    registry_name = registry_name[2:].replace(\'-\', \'_\')\n\n    REGISTRY = {}\n    REGISTRY_CLASS_NAMES = set()\n\n    # maintain a registry of all registries\n    if registry_name in REGISTRIES:\n        return  # registry already exists\n    REGISTRIES[registry_name] = {\n        \'registry\': REGISTRY,\n        \'default\': default,\n    }\n\n    def build_x(args, *extra_args, **extra_kwargs):\n        choice = getattr(args, registry_name, None)\n        if choice is None:\n            return None\n        cls = REGISTRY[choice]\n        if hasattr(cls, \'build_\' + registry_name):\n            builder = getattr(cls, \'build_\' + registry_name)\n        else:\n            builder = cls\n        set_defaults(args, cls)\n        return builder(args, *extra_args, **extra_kwargs)\n\n    def register_x(name):\n\n        def register_x_cls(cls):\n            if name in REGISTRY:\n                raise ValueError(\'Cannot register duplicate {} ({})\'.format(registry_name, name))\n            if cls.__name__ in REGISTRY_CLASS_NAMES:\n                raise ValueError(\n                    \'Cannot register {} with duplicate class name ({})\'.format(\n                        registry_name, cls.__name__,\n                    )\n                )\n            if base_class is not None and not issubclass(cls, base_class):\n                raise ValueError(\'{} must extend {}\'.format(cls.__name__, base_class.__name__))\n            REGISTRY[name] = cls\n            REGISTRY_CLASS_NAMES.add(cls.__name__)\n            return cls\n\n        return register_x_cls\n\n    return build_x, register_x, REGISTRY\n\n\ndef set_defaults(args, cls):\n    """"""Helper to set default arguments based on *add_args*.""""""\n    if not hasattr(cls, \'add_args\'):\n        return\n    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS, allow_abbrev=False)\n    cls.add_args(parser)\n    # copied from argparse.py:\n    defaults = argparse.Namespace()\n    for action in parser._actions:\n        if action.dest is not argparse.SUPPRESS:\n            if not hasattr(defaults, action.dest):\n                if action.default is not argparse.SUPPRESS:\n                    setattr(defaults, action.dest, action.default)\n    for key, default_value in vars(defaults).items():\n        if not hasattr(args, key):\n            setattr(args, key, default_value)\n'"
fairseq/search.py,33,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Optional, List\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass Search(nn.Module):\n    def __init__(self, tgt_dict):\n        super().__init__()\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.src_lengths = torch.tensor(-1)\n\n    def step(self, step, lprobs, scores):\n        """"""Take a single search step.\n\n        Args:\n            step: the current search step, starting at 0\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model\'s log-probabilities over the vocabulary at the current step\n            scores: (bsz x input_beam_size x step)\n                the historical model scores of each hypothesis up to this point\n\n        Return: A tuple of (scores, indices, beams) where:\n            scores: (bsz x output_beam_size)\n                the scores of the chosen elements; output_beam_size can be\n                larger than input_beam_size, e.g., we may return\n                2*input_beam_size to account for EOS\n            indices: (bsz x output_beam_size)\n                the indices of the chosen elements\n            beams: (bsz x output_beam_size)\n                the hypothesis ids of the chosen elements, in the range [0, input_beam_size)\n        """"""\n        raise NotImplementedError\n\n    @torch.jit.export\n    def set_src_lengths(self, src_lengths):\n        self.src_lengths = src_lengths\n\n\nclass BeamSearch(Search):\n    def __init__(self, tgt_dict):\n        super().__init__(tgt_dict)\n\n    @torch.jit.export\n    def step(self, step: int, lprobs, scores: Optional[Tensor]):\n        bsz, beam_size, vocab_size = lprobs.size()\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n        else:\n            # make probs contain cumulative scores for each hypothesis\n            assert scores is not None\n            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n\n        top_prediction = torch.topk(\n            lprobs.view(bsz, -1),\n            k=min(\n                # Take the best 2 x beam_size predictions. We\'ll choose the first\n                # beam_size of these which don\'t predict eos to continue with.\n                beam_size * 2,\n                lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad\n            ),\n        )\n        scores_buf = top_prediction[0]\n        indices_buf = top_prediction[1]\n        if torch.__version__ < \'1.6.0\':\n            beams_buf = torch.div(indices_buf, vocab_size)\n        else:\n            beams_buf = torch.floor_divide(indices_buf, vocab_size)\n        indices_buf = indices_buf.fmod(vocab_size)\n        return scores_buf, indices_buf, beams_buf\n\n\nclass LengthConstrainedBeamSearch(Search):\n    def __init__(self, tgt_dict, min_len_a, min_len_b, max_len_a, max_len_b):\n        super().__init__(tgt_dict)\n        self.min_len_a = min_len_a\n        self.min_len_b = min_len_b\n        self.max_len_a = max_len_a\n        self.max_len_b = max_len_b\n        self.beam = BeamSearch(tgt_dict)\n\n    def step(self, step: int, lprobs, scores):\n        min_lens = self.min_len_a * self.src_lengths + self.min_len_b\n        max_lens = self.max_len_a * self.src_lengths + self.max_len_b\n        lprobs[step < min_lens, :, self.eos] = -math.inf\n        lprobs[step >= max_lens, :, self.eos] = 0\n        return self.beam.step(step, lprobs, scores)\n\n\nclass DiverseBeamSearch(Search):\n    """"""Diverse Beam Search.\n\n    See ""Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence\n    Models"" for details.\n\n    We only implement the Hamming Diversity penalty here, which performed best\n    in the original paper.\n    """"""\n\n    def __init__(self, tgt_dict, num_groups, diversity_strength):\n        super().__init__(tgt_dict)\n        self.num_groups = num_groups\n        self.diversity_strength = -diversity_strength\n        self.beam = BeamSearch(tgt_dict)\n\n    @torch.jit.export\n    def step(self, step: int, lprobs, scores):\n        bsz, beam_size, vocab_size = lprobs.size()\n        if beam_size % self.num_groups != 0:\n            raise ValueError(\n                ""DiverseBeamSearch requires --beam to be divisible by the number of groups""\n            )\n\n        # initialize diversity penalty\n        diversity_buf = torch.zeros(lprobs[:, 0, :].size()).to(lprobs)\n\n        scores_G, indices_G, beams_G = [], [], []\n        for g in range(self.num_groups):\n            lprobs_g = lprobs[:, g :: self.num_groups, :]\n            scores_g = scores[:, g :: self.num_groups, :] if step > 0 else None\n\n            # apply diversity penalty\n            if g > 0:\n                lprobs_g = torch.add(\n                    lprobs_g, self.diversity_strength, diversity_buf.unsqueeze(1)\n                )\n            else:\n                lprobs_g = lprobs_g.contiguous()\n\n            scores_buf, indices_buf, beams_buf = self.beam.step(\n                step, lprobs_g, scores_g\n            )\n            beams_buf.mul_(self.num_groups).add_(g)\n\n            scores_G.append(scores_buf.clone())\n            indices_G.append(indices_buf.clone())\n            beams_G.append(beams_buf.clone())\n\n            # update diversity penalty\n            diversity_buf.scatter_add_(\n                1, indices_buf, torch.ones(indices_buf.size()).to(diversity_buf)\n            )\n\n        # interleave results from different groups\n        scores_buf = torch.stack(scores_G, dim=2).view(bsz, -1)\n        indices_buf = torch.stack(indices_G, dim=2).view(bsz, -1)\n        beams_buf = torch.stack(beams_G, dim=2).view(bsz, -1)\n        return scores_buf, indices_buf, beams_buf\n\n\nclass Sampling(Search):\n    sampling_topk: int\n    sampling_topp: float\n\n    def __init__(self, tgt_dict, sampling_topk=-1, sampling_topp=-1.0):\n        super().__init__(tgt_dict)\n        self.sampling_topk = sampling_topk\n        self.sampling_topp = sampling_topp\n\n    def _sample_topp(self, lprobs):\n        """"""Sample among the smallest set of elements whose cumulative probability mass exceeds p.\n\n        See `""The Curious Case of Neural Text Degeneration""\n        (Holtzman et al., 2019) <https://arxiv.org/abs/1904.09751>`_.\n\n        Args:\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model\'s log-probabilities over the vocabulary at the current step\n\n        Return: A tuple of (trimed_probs, truncated_indices) where:\n            trimed_probs: (bsz x input_beam_size x ?)\n                the model\'s probabilities over the elements selected to sample from. The\n                width of the third dimension is determined by top-P.\n            truncated_indices: (bsz x input_beam_size x ?)\n                the indices of the chosen elements.\n        """"""\n        probs = lprobs.exp_()\n\n        # sort the last dimension (vocab dimension) in descending order\n        sorted_probs, sorted_indices = probs.sort(descending=True)\n\n        # compute a mask to indicate the words to be included in the top-P set.\n        cumsum_probs = sorted_probs.cumsum(dim=2)\n        mask = cumsum_probs.lt(self.sampling_topp)\n\n        # note that mask was computed by \'lt\'. One more word needs to be included\n        # so that the cumulative probability mass can exceed p.\n        cumsum_mask = mask.cumsum(dim=2)\n        last_included = cumsum_mask[:, :, -1:]\n        last_included.clamp_(0, mask.size()[2] - 1)\n        mask = mask.scatter_(2, last_included, 1)\n\n        # truncate unnecessary dims.\n        max_dim = last_included.max()\n        truncated_mask = mask[:, :, : max_dim + 1]\n        truncated_probs = sorted_probs[:, :, : max_dim + 1]\n        truncated_indices = sorted_indices[:, :, : max_dim + 1]\n\n        # trim the words that are not in top-P by setting their probabilities\n        # to 0, so that they would not be sampled later.\n        trim_mask = ~truncated_mask\n        trimed_probs = truncated_probs.masked_fill_(trim_mask, 0)\n        return trimed_probs, truncated_indices\n\n    @torch.jit.export\n    def step(self, step: int, lprobs, scores):\n        bsz, beam_size, vocab_size = lprobs.size()\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n\n        if self.sampling_topp > 0:\n            # only sample from the smallest set of words whose cumulative probability mass exceeds p\n            probs, top_indices = self._sample_topp(lprobs)\n        elif self.sampling_topk > 0:\n            # only sample from top-k candidates\n            lprobs, top_indices = lprobs.topk(self.sampling_topk)\n            probs = lprobs.exp_()\n        else:\n            probs = lprobs.exp_()\n\n            # dummy data to be consistent with true branch for type check\n            top_indices = torch.empty(0).to(probs)\n        # sample\n        if step == 0:\n            indices_buf = torch.multinomial(\n                probs.view(bsz, -1), beam_size, replacement=True,\n            ).view(bsz, beam_size)\n        else:\n            indices_buf = torch.multinomial(\n                probs.view(bsz * beam_size, -1),\n                1,\n                replacement=True,\n            ).view(bsz, beam_size)\n\n        if step == 0:\n            # expand to beam size\n            probs = probs.expand(bsz, beam_size, -1)\n\n        # gather scores\n        scores_buf = torch.gather(\n            probs, dim=2, index=indices_buf.unsqueeze(-1)\n        )\n        scores_buf = scores_buf.log_().view(bsz, -1)\n\n        # remap indices if using top-k or top-P sampling\n        if self.sampling_topk > 0 or self.sampling_topp > 0:\n            indices_buf = torch.gather(\n                top_indices.expand(bsz, beam_size, -1),\n                dim=2,\n                index=indices_buf.unsqueeze(-1),\n            ).squeeze(2)\n\n        if step == 0:\n            beams_buf = indices_buf.new_zeros(bsz, beam_size)\n        else:\n            beams_buf = torch.arange(0, beam_size).to(indices_buf).repeat(bsz, 1)\n            # make scores cumulative\n            scores_buf.add_(\n                torch.gather(scores[:, :, step - 1], dim=1, index=beams_buf)\n            )\n\n        return scores_buf, indices_buf, beams_buf\n\n\nclass DiverseSiblingsSearch(Search):\n    """"""\n    Beam search with diverse siblings.\n\n    See ""A Simple, Fast Diverse Decoding Algorithm for Neural Generation"" for details.\n    https://arxiv.org/abs/1611.08562\n\n    1/ Calculate hypotheses for each beam\n    2/ Intra-sibling ordering\n    3/ Rewrite scores\n    4/ Choose top K hypotheses\n\n    if diversity_rate == 0 is equivalent to BeamSearch\n    """"""\n\n    def __init__(self, tgt_dict, diversity_rate):\n        super().__init__(tgt_dict)\n        self.diversity_rate = diversity_rate\n        self.beam = BeamSearch(tgt_dict)\n\n    def step(self, step: int, lprobs, scores):\n        bsz, beam_size, vocab_size = lprobs.size()\n        k = min(\n            # Take the best 2 x beam_size predictions. We\'ll choose the first\n            # beam_size of these which don\'t predict eos to continue with.\n            beam_size * 2,\n            lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad\n        )\n        s_list: List[Tensor]\n        i_list: List[Tensor]\n        s_list = [torch.empty(0).to(lprobs) for i in range(beam_size)]\n        i_list = [torch.LongTensor().to(device=lprobs.device) for i in range(beam_size)]\n        sibling_score = torch.arange(1, k + 1).to(lprobs) * self.diversity_rate\n\n        if step == 0:\n            return self.beam.step(step, lprobs, scores)\n        lprobs.add_(scores[:, :, step - 1].unsqueeze(-1))\n\n        # 1/ Calculate hypotheses for each beam\n        for i in range(beam_size):\n            torch.topk(lprobs[:, i, :].view(bsz, -1), k, out=(s_list[i], i_list[i]))\n            i_list[i].fmod_(vocab_size)\n\n            # 2/ Intra-sibling ordering by default from topk + 3/ Rewrite scores\n            s_list[i].sub_(sibling_score)\n\n        # 4/ Choose top K hypotheses\n        indices = torch.stack(i_list, dim=1).view(bsz, -1)\n\n        final_scores = torch.empty(0).to(lprobs)\n        final_indices = torch.LongTensor().to(device=lprobs.device)\n        final_beams = torch.LongTensor().to(device=lprobs.device)\n        (final_scores, final_indices) = torch.topk(\n            torch.stack(s_list, dim=1).view(bsz, -1),\n            k,\n        )\n\n        final_beams = final_indices // k\n\n        for i in range(bsz):\n            final_indices[i] = indices[i][final_indices[i]]\n\n        return final_scores, final_indices, final_beams\n'"
fairseq/sequence_generator.py,56,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Dict, List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom fairseq import search, utils\nfrom fairseq.data import data_utils\nfrom fairseq.models import FairseqIncrementalDecoder\nfrom fairseq.models.fairseq_encoder import EncoderOut\nfrom torch import Tensor\n\n\nclass SequenceGenerator(nn.Module):\n    def __init__(\n        self,\n        models,\n        tgt_dict,\n        beam_size=1,\n        max_len_a=0,\n        max_len_b=200,\n        min_len=1,\n        normalize_scores=True,\n        len_penalty=1.0,\n        unk_penalty=0.0,\n        retain_dropout=False,\n        temperature=1.0,\n        match_source_len=False,\n        no_repeat_ngram_size=0,\n        search_strategy=None,\n        eos=None,\n    ):\n        """"""Generates translations of a given source sentence.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models,\n                currently support fairseq.models.TransformerModel for scripting\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            normalize_scores (bool, optional): normalize scores by the length\n                of the output (default: True)\n            len_penalty (float, optional): length penalty, where <1.0 favors\n                shorter, >1.0 favors longer sentences (default: 1.0)\n            unk_penalty (float, optional): unknown word penalty, where <0\n                produces more unks, >0 produces fewer (default: 0.0)\n            retain_dropout (bool, optional): use dropout when generating\n                (default: False)\n            temperature (float, optional): temperature, where values\n                >1.0 produce more uniform samples and values <1.0 produce\n                sharper samples (default: 1.0)\n            match_source_len (bool, optional): outputs should match the source\n                length (default: False)\n        """"""\n        super().__init__()\n        if isinstance(models, EnsembleModel):\n            self.model = models\n        else:\n            self.model = EnsembleModel(models)\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos() if eos is None else eos\n        self.vocab_size = len(tgt_dict)\n        self.beam_size = beam_size\n        # the max beam size is the dictionary size - 1, since we never select pad\n        self.beam_size = min(beam_size, self.vocab_size - 1)\n        self.max_len_a = max_len_a\n        self.max_len_b = max_len_b\n        self.min_len = min_len\n\n        self.normalize_scores = normalize_scores\n        self.len_penalty = len_penalty\n        self.unk_penalty = unk_penalty\n        self.retain_dropout = retain_dropout\n        self.temperature = temperature\n        self.match_source_len = match_source_len\n        self.no_repeat_ngram_size = no_repeat_ngram_size\n        assert temperature > 0, ""--temperature must be greater than 0""\n\n        self.search = (\n            search.BeamSearch(tgt_dict) if search_strategy is None else search_strategy\n        )\n        if not self.retain_dropout:\n            self.model.eval()\n\n    def cuda(self):\n        self.model.cuda()\n        return self\n\n    @torch.no_grad()\n    def forward(\n        self,\n        sample: Dict[str, Dict[str, Tensor]],\n        prefix_tokens: Optional[Tensor] = None,\n        bos_token: Optional[int] = None,\n    ):\n        """"""Generate a batch of translations.\n\n        Args:\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n            bos_token (int, optional): beginning of sentence token\n                (default: self.eos)\n        """"""\n        self.model.reset_incremental_state()\n        return self._generate(sample, prefix_tokens, bos_token)\n\n    # TODO(myleott): unused, deprecate after pytorch-translate migration\n    def generate_batched_itr(self, data_itr, beam_size=None, cuda=False, timer=None):\n        """"""Iterate over a batched dataset and yield individual translations.\n        Args:\n            cuda (bool, optional): use GPU for generation\n            timer (StopwatchMeter, optional): time generations\n        """"""\n        for sample in data_itr:\n            s = utils.move_to_cuda(sample) if cuda else sample\n            if ""net_input"" not in s:\n                continue\n            input = s[""net_input""]\n            # model.forward normally channels prev_output_tokens into the decoder\n            # separately, but SequenceGenerator directly calls model.encoder\n            encoder_input = {\n                k: v for k, v in input.items() if k != ""prev_output_tokens""\n            }\n            if timer is not None:\n                timer.start()\n            with torch.no_grad():\n                hypos = self.generate(encoder_input)\n            if timer is not None:\n                timer.stop(sum(len(h[0][""tokens""]) for h in hypos))\n            for i, id in enumerate(s[""id""].data):\n                # remove padding\n                src = utils.strip_pad(input[""src_tokens""].data[i, :], self.pad)\n                ref = (\n                    utils.strip_pad(s[""target""].data[i, :], self.pad)\n                    if s[""target""] is not None\n                    else None\n                )\n                yield id, src, ref, hypos[i]\n\n    @torch.no_grad()\n    def generate(self, models, sample: Dict[str, Dict[str, Tensor]], **kwargs):\n        """"""Generate translations. Match the api of other fairseq generators.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n            bos_token (int, optional): beginning of sentence token\n                (default: self.eos)\n        """"""\n        self.model.reset_incremental_state()\n        return self._generate(sample, **kwargs)\n\n    def _generate(\n        self,\n        sample: Dict[str, Dict[str, Tensor]],\n        prefix_tokens: Optional[Tensor] = None,\n        bos_token: Optional[int] = None,\n    ):\n        net_input = sample[""net_input""]\n        src_tokens = net_input[""src_tokens""]\n        # length of the source text being the character length except EndOfSentence and pad\n        src_lengths = (\n            (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n        )\n        # bsz: total number of sentences in beam\n        input_size = src_tokens.size()\n        bsz, src_len = input_size[0], input_size[1]\n        beam_size = self.beam_size\n\n        max_len: int = -1\n        if self.match_source_len:\n            max_len = src_lengths.max().item()\n        else:\n            max_len = min(\n                int(self.max_len_a * src_len + self.max_len_b),\n                # exclude the EOS marker\n                self.model.max_decoder_positions() - 1,\n            )\n        assert (\n            self.min_len <= max_len\n        ), ""min_len cannot be larger than max_len, please adjust these!""\n        # compute the encoder output for each beam\n        encoder_outs = self.model.forward_encoder(net_input)\n\n        # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores\n        new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n        new_order = new_order.to(src_tokens.device).long()\n        encoder_outs = self.model.reorder_encoder_out(encoder_outs, new_order)\n        # ensure encoder_outs is a List.\n        assert encoder_outs is not None\n\n        # initialize buffers\n        scores = (\n            torch.zeros(bsz * beam_size, max_len + 1).to(src_tokens).float()\n        )  # +1 for eos; pad is never choosed for scoring\n        tokens = (\n            torch.zeros(bsz * beam_size, max_len + 2)\n            .to(src_tokens)\n            .long()\n            .fill_(self.pad)\n        )  # +2 for eos and pad\n        tokens[:, 0] = self.eos if bos_token is None else bos_token\n        attn: Optional[Tensor] = None\n\n        # The blacklist indicates candidates that should be ignored.\n        # For example, suppose we\'re sampling and have already finalized 2/5\n        # samples. Then the blacklist would mark 2 positions as being ignored,\n        # so that we only finalize the remaining 3 samples.\n        blacklist = (\n            torch.zeros(bsz, beam_size).to(src_tokens).eq(-1)\n        )  # forward and backward-compatible False mask\n\n        # list of completed sentences\n        finalized = torch.jit.annotate(\n            List[List[Dict[str, Tensor]]],\n            [torch.jit.annotate(List[Dict[str, Tensor]], []) for i in range(bsz)],\n        )  # contains lists of dictionaries of infomation about the hypothesis being finalized at each step\n\n        finished = [\n            False for i in range(bsz)\n        ]  # a boolean array indicating if the sentence at the index is finished or not\n        num_remaining_sent = bsz  # number of sentences remaining\n\n        # number of candidate hypos per step\n        cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n\n        # offset arrays for converting between different indexing schemes\n        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n        cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n\n        reorder_state: Optional[Tensor] = None\n        batch_idxs: Optional[Tensor] = None\n        for step in range(max_len + 1):  # one extra step for EOS marker\n            # reorder decoder internal states based on the prev choice of beams\n            # print(f\'step: {step}\')\n            if reorder_state is not None:\n                if batch_idxs is not None:\n                    # update beam indices to take into account removed sentences\n                    corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(\n                        batch_idxs\n                    )\n                    reorder_state.view(-1, beam_size).add_(\n                        corr.unsqueeze(-1) * beam_size\n                    )\n                self.model.reorder_incremental_state(reorder_state)\n                encoder_outs = self.model.reorder_encoder_out(\n                    encoder_outs, reorder_state\n                )\n\n            lprobs, avg_attn_scores = self.model.forward_decoder(\n                tokens[:, : step + 1], encoder_outs, self.temperature\n            )\n            lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)\n\n            lprobs[:, self.pad] = -math.inf  # never select pad\n            lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty\n\n            # handle max length constraint\n            if step >= max_len:\n                lprobs[:, : self.eos] = -math.inf\n                lprobs[:, self.eos + 1 :] = -math.inf\n\n            # handle prefix tokens (possibly with different lengths)\n            if (\n                prefix_tokens is not None\n                and step < prefix_tokens.size(1)\n                and step < max_len\n            ):\n                lprobs, tokens, scores = self._prefix_tokens(\n                    step, lprobs, scores, tokens, prefix_tokens, beam_size\n                )\n            elif step < self.min_len:\n                # minimum length constraint (does not apply if using prefix_tokens)\n                lprobs[:, self.eos] = -math.inf\n\n            # Record attention scores, only support avg_attn_scores is a Tensor\n            if avg_attn_scores is not None:\n                if attn is None:\n                    attn = torch.empty(\n                        bsz * beam_size, avg_attn_scores.size(1), max_len + 2\n                    ).to(scores)\n                attn[:, :, step + 1].copy_(avg_attn_scores)\n\n            scores = scores.type_as(lprobs)\n            eos_bbsz_idx = torch.empty(0).to(\n                tokens\n            )  # indices of hypothesis ending with eos (finished sentences)\n            eos_scores = torch.empty(0).to(\n                scores\n            )  # scores of hypothesis ending with eos (finished sentences)\n\n            self.search.set_src_lengths(src_lengths)\n\n            if self.no_repeat_ngram_size > 0:\n                lprobs = self._no_repeat_ngram(tokens, lprobs, bsz, beam_size, step)\n\n            cand_scores, cand_indices, cand_beams = self.search.step(\n                step,\n                lprobs.view(bsz, -1, self.vocab_size),\n                scores.view(bsz, beam_size, -1)[:, :, :step],\n            )\n\n            # cand_bbsz_idx contains beam indices for the top candidate\n            # hypotheses, with a range of values: [0, bsz*beam_size),\n            # and dimensions: [bsz, cand_size]\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n\n            # finalize hypotheses that end in eos\n            eos_mask = cand_indices.eq(self.eos) & cand_scores.ne(-math.inf)\n            eos_mask[:, :beam_size][blacklist] = torch.tensor(0).to(eos_mask)\n\n            # only consider eos when it\'s among the top beam_size indices\n            eos_bbsz_idx = torch.masked_select(\n                cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size]\n            )\n\n            finalized_sents: List[int] = []\n            if eos_bbsz_idx.numel() > 0:\n                eos_scores = torch.masked_select(\n                    cand_scores[:, :beam_size], mask=eos_mask[:, :beam_size]\n                )\n                finalized_sents = self.finalize_hypos(\n                    step,\n                    eos_bbsz_idx,\n                    eos_scores,\n                    tokens,\n                    scores,\n                    finalized,\n                    finished,\n                    beam_size,\n                    attn,\n                    src_lengths,\n                    max_len,\n                )\n                num_remaining_sent -= len(finalized_sents)\n\n            assert num_remaining_sent >= 0\n            if num_remaining_sent == 0:\n                break\n            assert step < max_len\n\n            if len(finalized_sents) > 0:\n                new_bsz = bsz - len(finalized_sents)\n\n                # construct batch_idxs which holds indices of batches to keep for the next pass\n                batch_mask = torch.ones(bsz).to(cand_indices)\n                batch_mask[\n                    torch.tensor(finalized_sents).to(cand_indices)\n                ] = torch.tensor(0).to(batch_mask)\n                batch_idxs = batch_mask.nonzero().squeeze(-1)\n\n                eos_mask = eos_mask[batch_idxs]\n                cand_beams = cand_beams[batch_idxs]\n                bbsz_offsets.resize_(new_bsz, 1)\n                cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n                cand_scores = cand_scores[batch_idxs]\n                cand_indices = cand_indices[batch_idxs]\n\n                if prefix_tokens is not None:\n                    prefix_tokens = prefix_tokens[batch_idxs]\n                src_lengths = src_lengths[batch_idxs]\n                blacklist = blacklist[batch_idxs]\n\n                scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n                tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n                if attn is not None:\n                    attn = attn.view(bsz, -1)[batch_idxs].view(\n                        new_bsz * beam_size, attn.size(1), -1\n                    )\n                bsz = new_bsz\n            else:\n                batch_idxs = None\n            # set active_mask so that values > cand_size indicate eos hypos\n            # and values < cand_size indicate candidate active hypos.\n            # After, the min values per row are the top candidate active hypos\n\n            # Rewrite the operator since the element wise or is not supported in torchscript.\n\n            eos_mask[:, :beam_size] = ~((~blacklist) & (~eos_mask[:, :beam_size]))\n            active_mask = torch.add(\n                eos_mask.type_as(cand_offsets) * cand_size,\n                cand_offsets[: eos_mask.size(1)],\n            )\n\n            # get the top beam_size active hypotheses, which are just the hypos\n            # with the smallest values in active_mask\n            new_blacklist, active_hypos = torch.topk(\n                active_mask, k=beam_size, dim=1, largest=False\n            )\n\n            # update blacklist to ignore any finalized hypos\n            blacklist = new_blacklist.ge(cand_size)[:, :beam_size]\n            assert (~blacklist).any(dim=1).all()\n\n            active_bbsz_idx = torch.gather(cand_bbsz_idx, dim=1, index=active_hypos)\n            active_scores = torch.gather(cand_scores, dim=1, index=active_hypos)\n\n            active_bbsz_idx = active_bbsz_idx.view(-1)\n            active_scores = active_scores.view(-1)\n\n            # copy tokens and scores for active hypotheses\n            tokens[:, : step + 1] = torch.index_select(\n                tokens[:, : step + 1], dim=0, index=active_bbsz_idx\n            )\n            tokens.view(bsz, beam_size, -1)[:, :, step + 1] = torch.gather(\n                cand_indices, dim=1, index=active_hypos\n            )\n            if step > 0:\n                scores[:, :step] = torch.index_select(\n                    scores[:, :step], dim=0, index=active_bbsz_idx\n                )\n            scores.view(bsz, beam_size, -1)[:, :, step] = torch.gather(\n                cand_scores, dim=1, index=active_hypos\n            )\n\n            # copy attention for active hypotheses\n            if attn is not None:\n                attn[:, :, : step + 2] = torch.index_select(\n                    attn[:, :, : step + 2], dim=0, index=active_bbsz_idx\n                )\n\n            # reorder incremental state in decoder\n            reorder_state = active_bbsz_idx\n\n        # sort by score descending\n        for sent in range(len(finalized)):\n            # make into beam container\n            BCList = [\n                BeamContainer(elem[""score""].item(), elem) for elem in finalized[sent]\n            ]\n            BCList.sort()\n            BCList.reverse()\n            finalized[sent] = torch.jit.annotate(\n                List[Dict[str, Tensor]], [x.elem for x in BCList]\n            )\n\n        return finalized\n\n    def _prefix_tokens(\n        self, step: int, lprobs, scores, tokens, prefix_tokens, beam_size: int\n    ):\n        """"""Handle prefix tokens""""""\n        prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n        prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n        prefix_mask = prefix_toks.ne(self.pad)\n        lprobs[prefix_mask] = torch.tensor(-math.inf).to(lprobs)\n        lprobs[prefix_mask] = lprobs[prefix_mask].scatter(\n            -1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lprobs[prefix_mask]\n        )\n        # if prefix includes eos, then we should make sure tokens and\n        # scores are the same across all beams\n        eos_mask = prefix_toks.eq(self.eos)\n        if eos_mask.any():\n            # validate that the first beam matches the prefix\n            first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[\n                :, 0, 1 : step + 1\n            ]\n            eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n            target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n            assert (first_beam == target_prefix).all()\n\n            # copy tokens, scores and lprobs from the first beam to all beams\n            tokens = self.replicate_first_beam(tokens, eos_mask_batch_dim, beam_size)\n            scores = self.replicate_first_beam(scores, eos_mask_batch_dim, beam_size)\n            lprobs = self.replicate_first_beam(lprobs, eos_mask_batch_dim, beam_size)\n        return lprobs, tokens, scores\n\n    def replicate_first_beam(self, tensor, mask, beam_size: int):\n        tensor = tensor.view(-1, beam_size, tensor.size(-1))\n        tensor[mask] = tensor[mask][:, :1, :]\n        return tensor.view(-1, tensor.size(-1))\n\n    def finalize_hypos(\n        self,\n        step: int,\n        bbsz_idx,\n        eos_scores,\n        tokens,\n        scores,\n        finalized: List[List[Dict[str, Tensor]]],\n        finished: List[bool],\n        beam_size: int,\n        attn: Optional[Tensor],\n        src_lengths,\n        max_len: int,\n    ):\n        """"""Finalize hypothesis, store finalized information in `finalized`, and change `finished` accordingly.\n        Returns number of sentences being finalized.\n        Args:\n            bbsz_idx (Tensor):\n        """"""\n        assert bbsz_idx.numel() == eos_scores.numel()\n\n        # clone relevant token and attention tensors\n        tokens_clone = tokens.index_select(0, bbsz_idx)[\n            :, 1 : step + 2\n        ]  # skip the first index, which is EOS\n\n        tokens_clone[:, step] = self.eos\n        attn_clone = (\n            attn.index_select(0, bbsz_idx)[:, :, 1 : step + 2]\n            if attn is not None\n            else None\n        )\n\n        # compute scores per token position\n        pos_scores = scores.index_select(0, bbsz_idx)[:, : step + 1]\n        pos_scores[:, step] = eos_scores\n        # convert from cumulative to per-position scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n\n        # normalize sentence-level scores\n        if self.normalize_scores:\n            eos_scores /= (step + 1) ** self.len_penalty\n\n        cum_unfin: List[int] = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n\n        # set() is not supported in script export\n        sents_seen: Dict[str, Optional[Tensor]] = {}\n        for i in range(bbsz_idx.size()[0]):\n            idx = bbsz_idx[i]\n            score = eos_scores[i]\n            unfin_idx = idx // beam_size\n            sent = unfin_idx + cum_unfin[unfin_idx]\n            # Cannot create dict for key type \'(int, int)\' in torchscript.\n            # The workaround is to cast int to string\n            seen = str(sent.item()) + ""_"" + str(unfin_idx.item())\n            if seen not in sents_seen:\n                sents_seen[seen] = None\n\n            if self.match_source_len and step > src_lengths[unfin_idx]:\n                score = torch.tensor(-math.inf).to(score)\n\n            if len(finalized[sent]) < beam_size:\n                if attn_clone is not None:\n                    # remove padding tokens from attn scores\n                    hypo_attn = attn_clone[i]\n                else:\n                    hypo_attn = torch.empty(0)\n                finalized[sent].append(\n                    {\n                        ""tokens"": tokens_clone[i],\n                        ""score"": score,\n                        ""attention"": hypo_attn,  # src_len x tgt_len\n                        ""alignment"": torch.empty(0),\n                        ""positional_scores"": pos_scores[i],\n                    }\n                )\n\n        newly_finished: List[int] = []\n        for seen in sents_seen.keys():\n            # check termination conditions for this sentence\n            sent: int = int(float(seen.split(""_"")[0]))\n            unfin_idx: int = int(float(seen.split(""_"")[1]))\n            if not finished[sent] and self.is_finished(\n                step, unfin_idx, max_len, len(finalized[sent]), beam_size\n            ):\n                finished[sent] = True\n                newly_finished.append(unfin_idx)\n        return newly_finished\n\n    def is_finished(\n        self,\n        step: int,\n        unfin_idx: int,\n        max_len: int,\n        finalized_sent_len: int,\n        beam_size: int,\n    ):\n        """"""\n        Check whether we\'ve finished generation for a given sentence, by\n        comparing the worst score among finalized hypotheses to the best\n        possible score among unfinalized hypotheses.\n        """"""\n        assert finalized_sent_len <= beam_size\n        if finalized_sent_len == beam_size or step == max_len:\n            return True\n        return False\n\n    def calculate_banned_tokens(\n        self,\n        tokens,\n        step: int,\n        gen_ngrams: List[Dict[str, List[int]]],\n        no_repeat_ngram_size: int,\n        bbsz_idx: int,\n    ):\n        tokens_list: List[int] = tokens[\n            bbsz_idx, step + 2 - no_repeat_ngram_size : step + 1\n        ].tolist()\n        # before decoding the next token, prevent decoding of ngrams that have already appeared\n        ngram_index = "","".join([str(x) for x in tokens_list])\n        return gen_ngrams[bbsz_idx].get(ngram_index, torch.jit.annotate(List[int], []))\n\n    def transpose_list(self, l: List[List[int]]):\n        # GeneratorExp aren\'t supported in TS so ignoring the lint\n        min_len = min([len(x) for x in l])  # noqa\n        l2 = [[row[i] for row in l] for i in range(min_len)]\n        return l2\n\n    def _no_repeat_ngram(self, tokens, lprobs, bsz: int, beam_size: int, step: int):\n        # for each beam and batch sentence, generate a list of previous ngrams\n        gen_ngrams: List[Dict[str, List[int]]] = [\n            torch.jit.annotate(Dict[str, List[int]], {})\n            for bbsz_idx in range(bsz * beam_size)\n        ]\n        cpu_tokens = tokens.cpu()\n        for bbsz_idx in range(bsz * beam_size):\n            gen_tokens: List[int] = cpu_tokens[bbsz_idx].tolist()\n            for ngram in self.transpose_list(\n                [gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]\n            ):\n                key = "","".join([str(x) for x in ngram[:-1]])\n                gen_ngrams[bbsz_idx][key] = gen_ngrams[bbsz_idx].get(\n                    key, torch.jit.annotate(List[int], [])\n                ) + [ngram[-1]]\n\n        if step + 2 - self.no_repeat_ngram_size >= 0:\n            # no banned tokens if we haven\'t generated no_repeat_ngram_size tokens yet\n            banned_tokens = [\n                self.calculate_banned_tokens(\n                    tokens, step, gen_ngrams, self.no_repeat_ngram_size, bbsz_idx\n                )\n                for bbsz_idx in range(bsz * beam_size)\n            ]\n        else:\n            banned_tokens = [\n                torch.jit.annotate(List[int], []) for bbsz_idx in range(bsz * beam_size)\n            ]\n        for bbsz_idx in range(bsz * beam_size):\n            lprobs[bbsz_idx][\n                torch.tensor(banned_tokens[bbsz_idx]).long()\n            ] = torch.tensor(-math.inf, dtype=torch.float)\n        return lprobs\n\n\nclass EnsembleModel(nn.Module):\n    """"""A wrapper around an ensemble of models.""""""\n\n    incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]]\n\n    def __init__(self, models):\n        super().__init__()\n        self.models_size = len(models)\n        # method \'__len__\' is not supported in ModuleList for torch script\n        self.single_model = models[0]\n        self.models = nn.ModuleList(models)\n\n        self.incremental_states = torch.jit.annotate(\n            List[Dict[str, Dict[str, Optional[Tensor]]]],\n            [\n                torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {})\n                for i in range(self.models_size)\n            ],\n        )\n        self.has_incremental: bool = False\n        if all(\n            hasattr(m, ""decoder"") and isinstance(m.decoder, FairseqIncrementalDecoder)\n            for m in models\n        ):\n            self.has_incremental = True\n\n    def forward(self):\n        pass\n\n    def reset_incremental_state(self):\n        if self.has_incremental_states():\n            self.incremental_states = torch.jit.annotate(\n                List[Dict[str, Dict[str, Optional[Tensor]]]],\n                [\n                    torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {})\n                    for i in range(self.models_size)\n                ],\n            )\n        return\n\n    def has_encoder(self):\n        return hasattr(self.single_model, ""encoder"")\n\n    def has_incremental_states(self):\n        return self.has_incremental\n\n    def max_decoder_positions(self):\n        return min([m.max_decoder_positions() for m in self.models])\n\n    @torch.jit.export\n    def forward_encoder(self, net_input: Dict[str, Tensor]):\n        if not self.has_encoder():\n            return None\n        return [\n            model.encoder.forward_torchscript(net_input)\n            for model in self.models\n        ]\n\n    @torch.jit.export\n    def forward_decoder(\n        self, tokens, encoder_outs: List[EncoderOut], temperature: float = 1.0\n    ):\n        log_probs = []\n        avg_attn: Optional[Tensor] = None\n        encoder_out: Optional[EncoderOut] = None\n        for i, model in enumerate(self.models):\n            if self.has_encoder():\n                encoder_out = encoder_outs[i]\n            # decode each model\n            if self.has_incremental_states():\n                decoder_out = model.decoder.forward(\n                    tokens,\n                    encoder_out=encoder_out,\n                    incremental_state=self.incremental_states[i],\n                )\n            else:\n                decoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)\n\n            attn: Optional[Tensor] = None\n            decoder_len = len(decoder_out)\n            if decoder_len > 1 and decoder_out[1] is not None:\n                if isinstance(decoder_out[1], Tensor):\n                    attn = decoder_out[1]\n                else:\n                    attn_holder = decoder_out[1][""attn""]\n                    if isinstance(attn_holder, Tensor):\n                        attn = attn_holder\n                    elif attn_holder is not None:\n                        attn = attn_holder[0]\n                if attn is not None:\n                    attn = attn[:, -1, :]\n\n            decoder_out_tuple = (\n                decoder_out[0][:, -1:, :].div_(temperature),\n                None if decoder_len <= 1 else decoder_out[1],\n            )\n\n            probs = model.get_normalized_probs(\n                decoder_out_tuple, log_probs=True, sample=None\n            )\n            probs = probs[:, -1, :]\n            if self.models_size == 1:\n                return probs, attn\n\n            log_probs.append(probs)\n            if attn is not None:\n                if avg_attn is None:\n                    avg_attn = attn\n                else:\n                    avg_attn.add_(attn)\n        avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(\n            self.models_size\n        )\n        if avg_attn is not None:\n            avg_attn.div_(self.models_size)\n        return avg_probs, avg_attn\n\n    @torch.jit.export\n    def reorder_encoder_out(self, encoder_outs: Optional[List[EncoderOut]], new_order):\n        """"""\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        """"""\n        new_outs: List[EncoderOut] = []\n        if not self.has_encoder():\n            return new_outs\n        for i, model in enumerate(self.models):\n            assert encoder_outs is not None\n            new_outs.append(\n                model.encoder.reorder_encoder_out(encoder_outs[i], new_order)\n            )\n        return new_outs\n\n    @torch.jit.export\n    def reorder_incremental_state(self, new_order):\n        if not self.has_incremental_states():\n            return\n        for i, model in enumerate(self.models):\n            model.decoder.reorder_incremental_state(\n                self.incremental_states[i], new_order\n            )\n\n\nclass SequenceGeneratorWithAlignment(SequenceGenerator):\n    def __init__(self, models, tgt_dict, left_pad_target=False, **kwargs):\n        """"""Generates translations of a given source sentence.\n\n        Produces alignments following ""Jointly Learning to Align and\n        Translate with Transformer Models"" (Garg et al., EMNLP 2019).\n\n        Args:\n            left_pad_target (bool, optional): Whether or not the\n                hypothesis should be left padded or not when they are\n                teacher forced for generating alignments.\n        """"""\n        super().__init__(EnsembleModelWithAlignment(models), tgt_dict, **kwargs)\n        self.left_pad_target = left_pad_target\n\n    @torch.no_grad()\n    def generate(self, models, sample, **kwargs):\n        self.model.reset_incremental_state()\n        finalized = super()._generate(sample, **kwargs)\n\n        src_tokens = sample[""net_input""][""src_tokens""]\n        bsz = src_tokens.shape[0]\n        beam_size = self.beam_size\n        src_tokens, src_lengths, prev_output_tokens, tgt_tokens = self._prepare_batch_for_alignment(\n            sample, finalized\n        )\n        if any(getattr(m, ""full_context_alignment"", False) for m in self.model.models):\n            attn = self.model.forward_align(src_tokens, src_lengths, prev_output_tokens)\n        else:\n            attn = [\n                finalized[i // beam_size][i % beam_size][""attention""].transpose(1, 0)\n                for i in range(bsz * beam_size)\n            ]\n\n        if src_tokens.device != ""cpu"":\n            src_tokens = src_tokens.to(\'cpu\')\n            tgt_tokens = tgt_tokens.to(\'cpu\')\n            attn = [i.to(\'cpu\') for i in attn]\n\n        # Process the attn matrix to extract hard alignments.\n        for i in range(bsz * beam_size):\n            alignment = utils.extract_hard_alignment(\n                attn[i], src_tokens[i], tgt_tokens[i], self.pad, self.eos\n            )\n            finalized[i // beam_size][i % beam_size][""alignment""] = alignment\n        return finalized\n\n    def _prepare_batch_for_alignment(self, sample, hypothesis):\n        src_tokens = sample[""net_input""][""src_tokens""]\n        bsz = src_tokens.shape[0]\n        src_tokens = (\n            src_tokens[:, None, :]\n            .expand(-1, self.beam_size, -1)\n            .contiguous()\n            .view(bsz * self.beam_size, -1)\n        )\n        src_lengths = sample[""net_input""][""src_lengths""]\n        src_lengths = (\n            src_lengths[:, None]\n            .expand(-1, self.beam_size)\n            .contiguous()\n            .view(bsz * self.beam_size)\n        )\n        prev_output_tokens = data_utils.collate_tokens(\n            [beam[""tokens""] for example in hypothesis for beam in example],\n            self.pad,\n            self.eos,\n            self.left_pad_target,\n            move_eos_to_beginning=True,\n        )\n        tgt_tokens = data_utils.collate_tokens(\n            [beam[""tokens""] for example in hypothesis for beam in example],\n            self.pad,\n            self.eos,\n            self.left_pad_target,\n            move_eos_to_beginning=False,\n        )\n        return src_tokens, src_lengths, prev_output_tokens, tgt_tokens\n\n\nclass EnsembleModelWithAlignment(EnsembleModel):\n    """"""A wrapper around an ensemble of models.""""""\n\n    def __init__(self, models):\n        super().__init__(models)\n\n    def forward_align(self, src_tokens, src_lengths, prev_output_tokens):\n        avg_attn = None\n        for model in self.models:\n            decoder_out = model(src_tokens, src_lengths, prev_output_tokens)\n            attn = decoder_out[1][""attn""]\n            if avg_attn is None:\n                avg_attn = attn\n            else:\n                avg_attn.add_(attn)\n        if len(self.models) > 1:\n            avg_attn.div_(len(self.models))\n        return avg_attn\n\n\n@torch.jit.script\nclass BeamContainer(object):\n    def __init__(self, score: float, elem: Dict[str, Tensor]):\n        self.score = score\n        self.elem = elem\n\n    def __lt__(self, other):\n        # type: (BeamContainer) -> bool\n        # Due to https://github.com/pytorch/pytorch/issues/20388,\n        # this has to use old style type annotations\n        # Match original behavior of sorted function when two scores are equal.\n        return self.score <= other.score\n'"
fairseq/sequence_scorer.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport sys\n\nfrom fairseq import utils\n\n\nclass SequenceScorer(object):\n    """"""Scores the target for a given source sentence.""""""\n\n    def __init__(self, tgt_dict, softmax_batch=None, compute_alignment=False, eos=None):\n        self.pad = tgt_dict.pad()\n        self.eos = tgt_dict.eos() if eos is None else eos\n        self.softmax_batch = softmax_batch or sys.maxsize\n        assert self.softmax_batch > 0\n        self.compute_alignment = compute_alignment\n\n    @torch.no_grad()\n    def generate(self, models, sample, **kwargs):\n        """"""Score a batch of translations.""""""\n        net_input = sample[\'net_input\']\n\n        def batch_for_softmax(dec_out, target):\n            # assumes decoder_out[0] is the only thing needed (may not be correct for future models!)\n            first, rest = dec_out[0], dec_out[1:]\n            bsz, tsz, dim = first.shape\n            if bsz * tsz < self.softmax_batch:\n                yield dec_out, target, True\n            else:\n                flat = first.contiguous().view(1, -1, dim)\n                flat_tgt = target.contiguous().view(flat.shape[:-1])\n                s = 0\n                while s < flat.size(1):\n                    e = s + self.softmax_batch\n                    yield (flat[:, s:e],) + rest, flat_tgt[:, s:e], False\n                    s = e\n\n        def gather_target_probs(probs, target):\n            probs = probs.gather(\n                dim=2,\n                index=target.unsqueeze(-1),\n            )\n            return probs\n\n        orig_target = sample[\'target\']\n\n        # compute scores for each model in the ensemble\n        avg_probs = None\n        avg_attn = None\n        for model in models:\n            model.eval()\n            decoder_out = model(**net_input)\n            attn = decoder_out[1] if len(decoder_out) > 1 else None\n            if type(attn) is dict:\n                attn = attn.get(\'attn\', None)\n\n            batched = batch_for_softmax(decoder_out, orig_target)\n            probs, idx = None, 0\n            for bd, tgt, is_single in batched:\n                sample[\'target\'] = tgt\n                curr_prob = model.get_normalized_probs(bd, log_probs=len(models) == 1, sample=sample).data\n                if is_single:\n                    probs = gather_target_probs(curr_prob, orig_target)\n                else:\n                    if probs is None:\n                        probs = curr_prob.new(orig_target.numel())\n                    step = curr_prob.size(0) * curr_prob.size(1)\n                    end = step + idx\n                    tgt_probs = gather_target_probs(curr_prob.view(tgt.shape + (curr_prob.size(-1),)), tgt)\n                    probs[idx:end] = tgt_probs.view(-1)\n                    idx = end\n                sample[\'target\'] = orig_target\n\n            probs = probs.view(sample[\'target\'].shape)\n\n            if avg_probs is None:\n                avg_probs = probs\n            else:\n                avg_probs.add_(probs)\n            if attn is not None and torch.is_tensor(attn):\n                attn = attn.data\n                if avg_attn is None:\n                    avg_attn = attn\n                else:\n                    avg_attn.add_(attn)\n        if len(models) > 1:\n            avg_probs.div_(len(models))\n            avg_probs.log_()\n            if avg_attn is not None:\n                avg_attn.div_(len(models))\n\n        bsz = avg_probs.size(0)\n        hypos = []\n        start_idxs = sample[\'start_indices\'] if \'start_indices\' in sample else [0] * bsz\n        for i in range(bsz):\n            # remove padding from ref\n            ref = utils.strip_pad(sample[\'target\'][i, start_idxs[i]:], self.pad) \\\n                if sample[\'target\'] is not None else None\n            tgt_len = ref.numel()\n            avg_probs_i = avg_probs[i][start_idxs[i]:start_idxs[i] + tgt_len]\n            score_i = avg_probs_i.sum() / tgt_len\n            if avg_attn is not None:\n                avg_attn_i = avg_attn[i]\n                if self.compute_alignment:\n                    alignment = utils.extract_hard_alignment(\n                        avg_attn_i,\n                        sample[\'net_input\'][\'src_tokens\'][i],\n                        sample[\'target\'][i],\n                        self.pad,\n                        self.eos,\n                    )\n                else:\n                    alignment = None\n            else:\n                avg_attn_i = alignment = None\n            hypos.append([{\n                \'tokens\': ref,\n                \'score\': score_i,\n                \'attention\': avg_attn_i,\n                \'alignment\': alignment,\n                \'positional_scores\': avg_probs_i,\n            }])\n        return hypos\n'"
fairseq/tokenizer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport re\n\nSPACE_NORMALIZER = re.compile(r""\\s+"")\n\n\ndef tokenize_line(line):\n    line = SPACE_NORMALIZER.sub("" "", line)\n    line = line.strip()\n    return line.split()\n'"
fairseq/trainer.py,26,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nTrain a network across multiple GPUs.\n""""""\n\nimport contextlib\nfrom itertools import chain\nimport logging\nimport sys\nfrom typing import Any, Dict, List\n\nimport torch\n\nfrom fairseq import checkpoint_utils, distributed_utils, models, optim, utils\nfrom fairseq.file_io import PathManager\nfrom fairseq.logging import meters, metrics\nfrom fairseq.nan_detector import NanDetector\nfrom fairseq.optim import lr_scheduler\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Trainer(object):\n    """"""Main class for data parallel training.\n\n    This class supports synchronous distributed data parallel training,\n    where multiple workers each have a full model replica and gradients\n    are accumulated across workers before each update. We use\n    :class:`~torch.nn.parallel.DistributedDataParallel` to handle\n    communication of the gradients across workers.\n    """"""\n\n    def __init__(self, args, task, model, criterion, quantizer=None):\n        self.args = args\n        self.task = task\n\n        # catalog shared parameters\n        shared_params = _catalog_shared_params(model)\n\n        self.tpu = getattr(args, \'tpu\', False)\n        self.cuda = torch.cuda.is_available() and not args.cpu and not self.tpu\n        if self.cuda:\n            self.device = torch.device(\'cuda\')\n        elif self.tpu:\n            self.device = utils.get_tpu_device(args)\n        else:\n            self.device = torch.device(\'cpu\')\n\n        # copy model and criterion to current device/dtype\n        self._criterion = criterion\n        self._model = model\n        if self.tpu:\n            import torch_xla.core.xla_model as xm\n            self._model = xm.send_cpu_data_to_device(self._model, self.device)\n        if args.fp16:\n            self._criterion = self._criterion.half()\n            self._model = self._model.half()\n        elif args.bf16:\n            self._criterion = self._criterion.to(dtype=torch.bfloat16)\n            self._model = self._model.to(dtype=torch.bfloat16)\n        self._criterion = self._criterion.to(device=self.device)\n        self._model = self._model.to(device=self.device)\n\n        # check that shared parameters are preserved after device transfer\n        for shared_param in shared_params:\n            ref = _get_module_by_path(self._model, shared_param[0])\n            for path in shared_param[1:]:\n                logger.info(\n                    \'detected shared parameter: {} <- {}\'.format(shared_param[0], path)\n                )\n                _set_module_by_path(self._model, path, ref)\n\n        self._dummy_batch = ""DUMMY""  # indicates we don\'t have a dummy batch at first\n        self._lr_scheduler = None\n        self._num_updates = 0\n        self._num_xla_compiles = 0  # for TPUs\n        self._optim_history = None\n        self._optimizer = None\n        self._warn_once = set()\n        self._wrapped_criterion = None\n        self._wrapped_model = None\n\n        # TODO(myleott): support tpu\n        if self.cuda and self.data_parallel_world_size > 1:\n            self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)\n        else:\n            self._grad_norm_buf = None\n\n        self.quantizer = quantizer\n        if self.quantizer is not None:\n            self.quantizer.set_trainer(self)\n\n        metrics.log_start_time(""wall"", priority=790, round=0)\n\n    def reinitialize(self):\n        """"""Reinitialize the Trainer, typically after model params change.""""""\n        self._lr_scheduler = None\n        self._optimizer = None\n        self._wrapped_criterion = None\n        self._wrapped_model = None\n\n    @property\n    def data_parallel_world_size(self):\n        return self.args.distributed_world_size\n\n    @property\n    def data_parallel_process_group(self):\n        if self.tpu:\n            return (\'tpu\', None)\n        else:\n            return None\n\n    @property\n    def data_parallel_rank(self):\n        return self.args.distributed_rank\n\n    @property\n    def is_data_parallel_master(self):\n        return distributed_utils.is_master(self.args)\n\n    @property\n    def criterion(self):\n        if self._wrapped_criterion is None:\n            if (\n                utils.has_parameters(self._criterion)\n                and self.data_parallel_world_size > 1\n                and not self.args.use_bmuf\n                and not self.tpu\n            ):\n                self._wrapped_criterion = models.DistributedFairseqModel(\n                    self.args, self._criterion,\n                    process_group=self.data_parallel_process_group\n                )\n            else:\n                self._wrapped_criterion = self._criterion\n        return self._wrapped_criterion\n\n    @property\n    def model(self):\n        if self._wrapped_model is None:\n            if (\n                self.data_parallel_world_size > 1\n                and not self.args.use_bmuf\n                and not self.tpu\n            ):\n                self._wrapped_model = models.DistributedFairseqModel(\n                    self.args, self._model,\n                    process_group=self.data_parallel_process_group\n                )\n            else:\n                self._wrapped_model = self._model\n        return self._wrapped_model\n\n    @property\n    def optimizer(self):\n        if self._optimizer is None:\n            self._build_optimizer()\n        return self._optimizer\n\n    @property\n    def lr_scheduler(self):\n        if self._lr_scheduler is None:\n            self._build_optimizer()  # this will initialize self._lr_scheduler\n        return self._lr_scheduler\n\n    def _build_optimizer(self):\n        params = list(\n            filter(\n                lambda p: p.requires_grad,\n                chain(self.model.parameters(), self.criterion.parameters()),\n            )\n        )\n\n        if self.args.fp16 or self.args.bf16:\n            if self.cuda and torch.cuda.get_device_capability(0)[0] < 7:\n                logger.info(\n                    ""NOTE: your device does NOT support faster training with --fp16, ""\n                    ""please switch to FP32 which is likely to be faster""\n                )\n            if self.args.memory_efficient_fp16 or self.args.memory_efficient_bf16:\n                self._optimizer = optim.MemoryEfficientFP16Optimizer.build_optimizer(\n                    self.args, params\n                )\n            else:\n                self._optimizer = optim.FP16Optimizer.build_optimizer(self.args, params)\n        else:\n            if self.cuda and torch.cuda.get_device_capability(0)[0] >= 7:\n                logger.info(""NOTE: your device may support faster training with --fp16"")\n            self._optimizer = optim.build_optimizer(self.args, params)\n\n        if self.args.use_bmuf:\n            self._optimizer = optim.FairseqBMUF(self.args, self._optimizer)\n\n        # We should initialize the learning rate scheduler immediately after\n        # building the optimizer, so that the initial learning rate is set.\n        self._lr_scheduler = lr_scheduler.build_lr_scheduler(self.args, self.optimizer)\n        self._lr_scheduler.step_update(0)\n\n    def save_checkpoint(self, filename, extra_state):\n        """"""Save all training state in a checkpoint file.""""""\n        if self.is_data_parallel_master:  # only save one checkpoint\n            extra_state[""metrics""] = metrics.state_dict()\n            checkpoint_utils.save_state(\n                filename,\n                self.args,\n                self.get_model().state_dict(),\n                self.get_criterion(),\n                self.optimizer,\n                self.lr_scheduler,\n                self.get_num_updates(),\n                self._optim_history,\n                extra_state,\n            )\n\n    def load_checkpoint(\n        self,\n        filename,\n        reset_optimizer=False,\n        reset_lr_scheduler=False,\n        optimizer_overrides=None,\n        reset_meters=False,\n    ):\n        """"""Load all training state from a checkpoint file.""""""\n        extra_state, self._optim_history, last_optim_state = None, [], None\n\n        bexists = PathManager.isfile(filename)\n        if bexists:\n            state = checkpoint_utils.load_checkpoint_to_cpu(filename)\n\n            # load model parameters\n            try:\n                self.get_model().load_state_dict(\n                    state[""model""], strict=True, args=self.args\n                )\n                if utils.has_parameters(self.get_criterion()):\n                    self.get_criterion().load_state_dict(\n                        state[""criterion""], strict=True\n                    )\n            except Exception:\n                raise Exception(\n                    ""Cannot load model parameters from checkpoint {}; ""\n                    ""please ensure that the architectures match."".format(filename)\n                )\n\n            extra_state = state[""extra_state""]\n            self._optim_history = state[""optimizer_history""]\n            last_optim_state = state.get(""last_optimizer_state"", None)\n\n        if last_optim_state is not None and not reset_optimizer:\n            # rebuild optimizer after loading model, since params may have changed\n            self._build_optimizer()\n\n            # only reload optimizer and lr_scheduler if they match\n            last_optim = self._optim_history[-1]\n            assert (\n                last_optim[""criterion_name""] == self.get_criterion().__class__.__name__\n            ), ""Criterion does not match; please reset the optimizer (--reset-optimizer).""\n            assert (\n                last_optim[""optimizer_name""] == self.optimizer.__class__.__name__\n            ), ""Optimizer does not match; please reset the optimizer (--reset-optimizer).""\n\n            if not reset_lr_scheduler:\n                self.lr_scheduler.load_state_dict(last_optim[""lr_scheduler_state""])\n            self.optimizer.load_state_dict(last_optim_state, optimizer_overrides)\n\n            self.set_num_updates(last_optim[""num_updates""])\n\n        if extra_state is not None:\n            epoch = extra_state[""train_iterator""][""epoch""]\n            logger.info(\n                ""loaded checkpoint {} (epoch {} @ {} updates)"".format(\n                    filename, epoch, self.get_num_updates()\n                )\n            )\n\n            self.lr_step(epoch)\n\n            if ""metrics"" in extra_state and not reset_meters:\n                metrics.load_state_dict(extra_state[""metrics""])\n\n                # reset TimeMeters, since their start times don\'t make sense anymore\n                for meter in metrics.get_meters(""default""):\n                    if isinstance(meter, meters.TimeMeter):\n                        meter.reset()\n        else:\n            logger.info(""no existing checkpoint found {}"".format(filename))\n\n        return extra_state\n\n    def get_train_iterator(\n        self,\n        epoch,\n        combine=True,\n        load_dataset=True,\n        data_selector=None,\n        shard_batch_itr=True,\n    ):\n        """"""Return an EpochBatchIterator over the training set for a given epoch.""""""\n        if load_dataset:\n            logger.info(""loading train data for epoch {}"".format(epoch))\n            self.task.load_dataset(\n                self.args.train_subset,\n                epoch=epoch,\n                combine=combine,\n                data_selector=data_selector,\n            )\n        return self.task.get_batch_iterator(\n            dataset=self.task.dataset(self.args.train_subset),\n            max_tokens=self.args.max_tokens,\n            max_sentences=self.args.max_sentences,\n            max_positions=utils.resolve_max_positions(\n                self.task.max_positions(),\n                self.model.max_positions(),\n                self.args.max_tokens,\n            ),\n            ignore_invalid_inputs=True,\n            required_batch_size_multiple=self.args.required_batch_size_multiple,\n            seed=self.args.seed,\n            num_shards=self.data_parallel_world_size if shard_batch_itr else 1,\n            shard_id=self.data_parallel_rank if shard_batch_itr else 0,\n            num_workers=self.args.num_workers,\n            epoch=epoch\n        )\n\n    def get_valid_iterator(\n        self,\n        subset,\n    ):\n        """"""Return an EpochBatchIterator over given validation subset for a given epoch.""""""\n        return self.task.get_batch_iterator(\n            dataset=self.task.dataset(subset),\n            max_tokens=self.args.max_tokens_valid,\n            max_sentences=self.args.max_sentences_valid,\n            max_positions=utils.resolve_max_positions(\n                self.task.max_positions(),\n                self.model.max_positions(),\n            ),\n            ignore_invalid_inputs=self.args.skip_invalid_size_inputs_valid_test,\n            required_batch_size_multiple=self.args.required_batch_size_multiple,\n            seed=self.args.seed,\n            num_shards=self.data_parallel_world_size,\n            shard_id=self.data_parallel_rank,\n            num_workers=self.args.num_workers\n        )\n\n    def begin_epoch(self, epoch):\n        """"""Called at the beginning of each epoch.""""""\n        if self.quantizer is not None:\n            self.quantizer.begin_epoch(epoch)\n\n        # task specific setup per epoch\n        self.task.begin_epoch(epoch, self.get_model())\n\n    @metrics.aggregate(""train"")\n    def train_step(self, samples, raise_oom=False):\n        """"""Do forward, backward and parameter update.""""""\n        if self._dummy_batch == ""DUMMY"":\n            self._dummy_batch = samples[0]\n\n        self._set_seed()\n        self.model.train()\n        self.criterion.train()\n        self.zero_grad()\n\n        metrics.log_start_time(""train_wall"", priority=800, round=0)\n\n        # forward and backward pass\n        logging_outputs, sample_size, ooms = [], 0, 0\n        for i, sample in enumerate(samples):\n            sample = self._prepare_sample(sample)\n            if sample is None:\n                # when sample is None, run forward/backward on a dummy batch\n                # and ignore the resulting gradients\n                sample = self._prepare_sample(self._dummy_batch)\n                is_dummy_batch = True\n            else:\n                is_dummy_batch = False\n\n            def maybe_no_sync():\n                """"""\n                Whenever *samples* contains more than one mini-batch, we\n                want to accumulate gradients locally and only call\n                all-reduce in the last backwards pass.\n                """"""\n                if (\n                    self.data_parallel_world_size > 1\n                    and hasattr(self.model, ""no_sync"")\n                    and i < len(samples) - 1\n                ):\n                    return self.model.no_sync()\n                else:\n                    return contextlib.ExitStack()  # dummy contextmanager\n\n            try:\n                with maybe_no_sync():\n                    # forward and backward\n                    loss, sample_size_i, logging_output = self.task.train_step(\n                        sample=sample,\n                        model=self.model,\n                        criterion=self.criterion,\n                        optimizer=self.optimizer,\n                        update_num=self.get_num_updates(),\n                        ignore_grad=is_dummy_batch,\n                    )\n                    del loss\n\n                logging_outputs.append(logging_output)\n                sample_size += sample_size_i\n\n                # emptying the CUDA cache after the first step can\n                # reduce the chance of OOM\n                if self.cuda and self.get_num_updates() == 0:\n                    torch.cuda.empty_cache()\n            except RuntimeError as e:\n                if ""out of memory"" in str(e):\n                    self._log_oom(e)\n                    if raise_oom:\n                        raise e\n                    logger.warning(\n                        ""attempting to recover from OOM in forward/backward pass""\n                    )\n                    ooms += 1\n                    self.zero_grad()\n                else:\n                    raise e\n\n            if self.tpu and i < len(samples) - 1:\n                # tpu-comment: every XLA operation before marking step is\n                # appended to the IR graph, and processing too many batches\n                # before marking step can lead to OOM errors.\n                # To handle gradient accumulation use case, we explicitly\n                # mark step here for every forward pass without a backward pass\n                import torch_xla.core.xla_model as xm\n                xm.mark_step()\n\n        if is_dummy_batch:\n            if torch.is_tensor(sample_size):\n                sample_size.zero_()\n            else:\n                sample_size *= 0.\n\n        if torch.is_tensor(sample_size):\n            sample_size = sample_size.float()\n        else:\n            sample_size = float(sample_size)\n\n        # gather logging outputs from all replicas\n        if self._sync_stats():\n            logging_outputs, (sample_size, ooms) = self._aggregate_logging_outputs(\n                logging_outputs, sample_size, ooms, ignore=is_dummy_batch,\n            )\n\n        overflow = False\n        try:\n            if self.tpu and self.data_parallel_world_size > 1:\n                import torch_xla.core.xla_model as xm\n                gradients = xm._fetch_gradients(self.optimizer.optimizer)\n                xm.all_reduce(\'sum\', gradients, scale=1.0 / self.data_parallel_world_size)\n\n            # multiply gradients by (# GPUs / sample_size) since DDP\n            # already normalizes by the number of GPUs. Thus we get\n            # (sum_of_gradients / sample_size).\n            if not self.args.use_bmuf:\n                self.optimizer.multiply_grads(self.data_parallel_world_size / sample_size)\n            elif sample_size > 0:  # BMUF needs to check sample size\n                num = self.data_parallel_world_size if self._sync_stats() else 1\n                self.optimizer.multiply_grads(num / sample_size)\n\n            # clip grads\n            grad_norm = self.clip_grad_norm(self.args.clip_norm)\n\n            # check that grad norms are consistent across workers\n            if (\n                not self.args.use_bmuf\n                and self.args.distributed_wrapper != \'SlowMo\'\n                and not self.tpu\n            ):\n                self._check_grad_norms(grad_norm)\n\n            # take an optimization step\n            self.optimizer.step()\n        except FloatingPointError:\n            # re-run the forward and backward pass with hooks attached to print out where it fails\n            with NanDetector(self.model):\n                self.task.train_step(\n                    sample, self.model, self.criterion, self.optimizer, self.get_num_updates(),\n                    ignore_grad=False\n                )\n            raise\n        except OverflowError as e:\n            overflow = True\n            logger.info(""NOTE: overflow detected, "" + str(e))\n            grad_norm = torch.tensor(0.).cuda()\n            self.zero_grad()\n        except RuntimeError as e:\n            if ""out of memory"" in str(e):\n                self._log_oom(e)\n                logger.error(""OOM during optimization, irrecoverable"")\n            raise e\n\n        # Some distributed wrappers (e.g., SlowMo) need access to the optimizer after the step\n        if hasattr(self.model, \'perform_additional_optimizer_actions\'):\n            if hasattr(self.optimizer, \'fp32_params\'):\n                self.model.perform_additional_optimizer_actions(self.optimizer.optimizer, self.optimizer.fp32_params)\n            else:\n                self.model.perform_additional_optimizer_actions(self.optimizer.optimizer)\n\n        if not overflow or self.args.distributed_wrapper == \'SlowMo\':\n            self.set_num_updates(self.get_num_updates() + 1)\n\n            if self.tpu:\n                # mark step on TPUs\n                import torch_xla.core.xla_model as xm\n                xm.mark_step()\n\n                # only log stats every log_interval steps\n                # this causes wps to be misreported when log_interval > 1\n                logging_output = {}\n                if self.get_num_updates() % self.args.log_interval == 0:\n                    logging_output = self._reduce_and_log_stats(\n                        logging_outputs, sample_size, grad_norm,\n                    )\n\n                # log whenever there\'s an XLA compilation, since these\n                # slow down training and may indicate opportunities for\n                # optimization\n                self._check_xla_compilation()\n            else:\n                # log stats\n                logging_output = self._reduce_and_log_stats(\n                    logging_outputs, sample_size, grad_norm,\n                )\n\n                # clear CUDA cache to reduce memory fragmentation\n                if (\n                    self.cuda\n                    and self.args.empty_cache_freq > 0\n                    and (\n                        (self.get_num_updates() + self.args.empty_cache_freq - 1)\n                        % self.args.empty_cache_freq\n                    ) == 0\n                ):\n                    torch.cuda.empty_cache()\n\n        if self.args.fp16:\n            metrics.log_scalar(""loss_scale"", self.optimizer.scaler.loss_scale, priority=700, round=0)\n\n        metrics.log_stop_time(""train_wall"")\n\n        return logging_output\n\n    @metrics.aggregate(""valid"")\n    def valid_step(self, sample, raise_oom=False):\n        """"""Do forward pass in evaluation mode.""""""\n        if self._dummy_batch == ""DUMMY"":\n            self._dummy_batch = sample\n        if self.tpu:\n            import torch_xla.core.xla_model as xm\n            xm.rendezvous(\'valid_step\')  # wait for all workers\n            xm.mark_step()\n\n        with torch.no_grad():\n            self.model.eval()\n            self.criterion.eval()\n\n            sample = self._prepare_sample(sample)\n            if sample is None:\n                sample = self._prepare_sample(self._dummy_batch)\n                is_dummy_batch = True\n            else:\n                is_dummy_batch = False\n\n            try:\n                _loss, sample_size, logging_output = self.task.valid_step(\n                    sample, self.model, self.criterion\n                )\n            except RuntimeError as e:\n                if ""out of memory"" in str(e):\n                    self._log_oom(e)\n                    if not raise_oom:\n                        logger.warning(\n                            ""ran out of memory in validation step, retrying batch""\n                        )\n                        for p in self.model.parameters():\n                            if p.grad is not None:\n                                p.grad = None  # free some memory\n                        if self.cuda:\n                            torch.cuda.empty_cache()\n                        return self.valid_step(sample, raise_oom=True)\n                raise e\n\n            logging_outputs = [logging_output]\n            if is_dummy_batch:\n                if torch.is_tensor(sample_size):\n                    sample_size.zero_()\n                else:\n                    sample_size *= 0.\n\n        # gather logging outputs from all replicas\n        if self.data_parallel_world_size > 1:\n            logging_outputs, (sample_size, ) = self._aggregate_logging_outputs(\n                logging_outputs, sample_size, ignore=is_dummy_batch,\n            )\n\n        # log validation stats\n        logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\n\n        return logging_output\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def lr_step(self, epoch, val_loss=None):\n        """"""Adjust the learning rate at the end of the epoch.""""""\n        self.lr_scheduler.step(epoch, val_loss)\n        # prefer updating the LR based on the number of steps\n        return self.lr_step_update()\n\n    def lr_step_update(self):\n        """"""Update the learning rate after each update.""""""\n        new_lr = self.lr_scheduler.step_update(self.get_num_updates())\n        metrics.log_scalar(""lr"", new_lr, weight=0, priority=300)\n        return new_lr\n\n    def get_lr(self):\n        """"""Get the current learning rate.""""""\n        return self.optimizer.get_lr()\n\n    def get_model(self):\n        """"""Get the (non-wrapped) model instance.""""""\n        return self._model\n\n    def get_criterion(self):\n        """"""Get the (non-wrapped) criterion instance.""""""\n        return self._criterion\n\n    def get_meter(self, name):\n        """"""[deprecated] Get a specific meter by name.""""""\n        from fairseq import meters\n\n        if \'get_meter\' not in self._warn_once:\n            self._warn_once.add(\'get_meter\')\n            utils.deprecation_warning(\n                \'Trainer.get_meter is deprecated. Please use fairseq.metrics instead.\'\n            )\n\n        train_meters = metrics.get_meters(""train"")\n        if train_meters is None:\n            train_meters = {}\n\n        if name == ""train_loss"" and ""loss"" in train_meters:\n            return train_meters[""loss""]\n        elif name == ""train_nll_loss"":\n            # support for legacy train.py, which assumed this meter is\n            # always initialized\n            m = train_meters.get(""nll_loss"", None)\n            return m or meters.AverageMeter()\n        elif name == ""wall"":\n            # support for legacy train.py, which assumed this meter is\n            # always initialized\n            m = metrics.get_meter(""default"", ""wall"")\n            return m or meters.TimeMeter()\n        elif name == ""wps"":\n            m = metrics.get_meter(""train"", ""wps"")\n            return m or meters.TimeMeter()\n        elif name in {""valid_loss"", ""valid_nll_loss""}:\n            # support for legacy train.py, which assumed these meters\n            # are always initialized\n            k = name[len(""valid_""):]\n            m = metrics.get_meter(""valid"", k)\n            return m or meters.AverageMeter()\n        elif name == ""oom"":\n            return meters.AverageMeter()\n        elif name in train_meters:\n            return train_meters[name]\n        return None\n\n    def get_num_updates(self):\n        """"""Get the number of parameters updates.""""""\n        return self._num_updates\n\n    def set_num_updates(self, num_updates):\n        """"""Set the number of parameters updates.""""""\n        self._num_updates = num_updates\n        self.lr_step_update()\n        if self.quantizer:\n            self.quantizer.step_update(self._num_updates)\n        metrics.log_scalar(""num_updates"", self._num_updates, weight=0, priority=200)\n\n    def clip_grad_norm(self, clip_norm):\n        return self.optimizer.clip_grad_norm(clip_norm, aggregate_norm_fn=None)\n\n    def _prepare_sample(self, sample):\n        if sample == ""DUMMY"":\n            raise Exception(\n                ""Trying to use an uninitialized \'dummy\' batch. This usually indicates ""\n                ""that the total number of batches is smaller than the number of ""\n                ""participating GPUs. Try reducing the batch size or using fewer GPUs.""\n            )\n\n        if sample is None or len(sample) == 0:\n            return None\n\n        if self.cuda:\n            sample = utils.move_to_cuda(sample)\n\n        def apply_half(t):\n            if t.dtype is torch.float32:\n                return t.half()\n            return t\n\n        def apply_bfloat16(t):\n            if t.dtype is torch.float32:\n                return t.to(dtype=torch.bfloat16)\n            return t\n\n        if self.args.fp16:\n            sample = utils.apply_to_sample(apply_half, sample)\n\n        if self.args.bf16:\n            sample = utils.apply_to_sample(apply_bfloat16, sample)\n\n        return sample\n\n    def _set_seed(self):\n        # Set seed based on args.seed and the update number so that we get\n        # reproducible results when resuming from checkpoints\n        seed = self.args.seed + self.get_num_updates()\n        utils.set_torch_seed(seed)\n\n    def _sync_stats(self):\n        # Return True if it\'s using multiple GPUs and DDP or multiple GPUs with\n        # BMUF and it\'s a bmuf sync with warmup iterations completed before.\n        if self.data_parallel_world_size == 1:\n            return False\n        elif self.args.use_bmuf:\n            return (\n                (self.get_num_updates() + 1) % self.args.global_sync_iter == 0\n                and (self.get_num_updates() + 1) > self.args.warmup_iterations\n            )\n        else:\n            return True\n\n    def _log_oom(self, exc):\n        msg = ""OOM: Ran out of memory with exception: {}"".format(exc)\n        logger.warning(msg)\n        if torch.cuda.is_available() and hasattr(torch.cuda, ""memory_summary""):\n            for device_idx in range(torch.cuda.device_count()):\n                logger.warning(torch.cuda.memory_summary(device=device_idx))\n        sys.stderr.flush()\n\n    def _aggregate_logging_outputs(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        if self.task.__class__.logging_outputs_can_be_summed(self.get_criterion()):\n            return self._fast_stat_sync_sum(\n                logging_outputs, *extra_stats_to_sum, ignore=ignore\n            )\n        else:\n            return self._all_gather_list_sync(\n                logging_outputs, *extra_stats_to_sum, ignore=ignore\n            )\n\n    def _all_gather_list_sync(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        """"""\n        Sync logging outputs across workers. all_gather_list_sync is\n        suitable when logging outputs are complex types.\n        """"""\n        if self.tpu:\n            raise NotImplementedError\n        if ignore:\n            logging_outputs = []\n        results = list(zip(\n            *distributed_utils.all_gather_list(\n                [logging_outputs] + list(extra_stats_to_sum),\n                max_size=getattr(self.args, \'all_gather_list_size\', 16384),\n                group=self.data_parallel_process_group,\n            )\n        ))\n        logging_outputs, extra_stats_to_sum = results[0], results[1:]\n        logging_outputs = list(chain.from_iterable(logging_outputs))\n        extra_stats_to_sum = [sum(s) for s in extra_stats_to_sum]\n        return logging_outputs, extra_stats_to_sum\n\n    def _fast_stat_sync_sum(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        """"""\n        Sync logging outputs across workers. fast_stat_sync_sum is\n        faster than all_gather_list_sync, but is only suitable when\n        logging outputs are scalars and can be summed. Note that\n        *logging_outputs* cannot contain any nested dicts/lists.\n        """"""\n        data = {}\n        for i, stat in enumerate(extra_stats_to_sum):\n            data[\'extra_stats_\' + str(i)] = stat\n        if len(logging_outputs) > 0:\n            log_keys = list(logging_outputs[0].keys())\n            for k in log_keys:\n                if not ignore:\n                    v = sum(log[k] for log in logging_outputs if k in log)\n                else:\n                    v = logging_outputs[0][k]\n                    v = torch.zeros_like(v) if torch.is_tensor(v) else 0\n                data[\'logging_outputs_\' + k] = v\n        else:\n            log_keys = None\n\n        data = distributed_utils.all_reduce_dict(\n            data,\n            device=self.device,\n            group=self.data_parallel_process_group\n        )\n\n        extra_stats_to_sum = [\n            data[\'extra_stats_\' + str(i)] for i in range(len(extra_stats_to_sum))\n        ]\n        if log_keys is not None:\n            logging_outputs = [{k: data[\'logging_outputs_\' + k] for k in log_keys}]\n        else:\n            logging_outputs = []\n        return logging_outputs, extra_stats_to_sum\n\n    def _is_grad_norms_consistent(self, grad_norm_buf):\n        """"""check whether a given tensor (shape (N,)) is consistent """"""\n        """"""consistent means all the values are diff within a tolerate range""""""\n        diff = grad_norm_buf - grad_norm_buf[0]\n        max_abs_diff = torch.max(torch.abs(diff)).item()\n        first_grad_norm = grad_norm_buf[0].item()\n        # TODO: make 1e-6 a configurable value\n        return max_abs_diff / (first_grad_norm + 1e-6) < 1e-6\n\n    def _check_grad_norms(self, grad_norm):\n        """"""Check that grad norms are consistent across workers.""""""\n        if self._grad_norm_buf is not None:\n            self._grad_norm_buf.zero_()\n            self._grad_norm_buf[self.data_parallel_rank] = grad_norm\n            distributed_utils.all_reduce(\n                self._grad_norm_buf,\n                group=self.data_parallel_process_group\n            )\n\n            if not self._is_grad_norms_consistent(self._grad_norm_buf):\n                pretty_detail = ""\\n"".join(\n                    ""rank {:3d} = {:.8f}"".format(r, n)\n                    for r, n in enumerate(self._grad_norm_buf.tolist())\n                )\n                error_detail = ""grad_norm across the workers:\\n{}\\n"".format(pretty_detail)\n                raise RuntimeError(\n                    ""Fatal error: gradients are inconsistent between workers. ""\n                    ""Try --ddp-backend=no_c10d. ""\n                    ""Or are you mixing up different generation of GPUs in training?""\n                    + ""\\n""\n                    + ""-"" * 80\n                    + ""\\n{}\\n"".format(error_detail)\n                    + ""-"" * 80\n                )\n\n    def _reduce_and_log_stats(self, logging_outputs, sample_size, grad_norm=None):\n        if grad_norm is not None:\n            metrics.log_speed(""ups"", 1., priority=100, round=2)\n            metrics.log_scalar(""gnorm"", grad_norm, priority=400, round=3)\n            if self.args.clip_norm > 0:\n                metrics.log_scalar(\n                    ""clip"",\n                    torch.where(\n                        grad_norm > self.args.clip_norm,\n                        grad_norm.new_tensor(100),\n                        grad_norm.new_tensor(0),\n                    ),\n                    priority=500,\n                    round=1,\n                )\n\n        with metrics.aggregate() as agg:\n            if logging_outputs is not None:\n                self.task.reduce_metrics(logging_outputs, self.get_criterion())\n                del logging_outputs\n\n            # extra warning for criterions that don\'t properly log a loss value\n            if ""loss"" not in agg:\n                if ""loss"" not in self._warn_once:\n                    self._warn_once.add(""loss"")\n                    logger.warning(\n                        ""Criterion.reduce_metrics did not log a \'loss\' value, ""\n                        ""which may break some functionality""\n                    )\n                metrics.log_scalar(""loss"", -1)\n\n            # support legacy interface\n            if self.tpu:\n                logging_output = {}\n            else:\n                logging_output = agg.get_smoothed_values()\n                logging_output[""sample_size""] = sample_size\n                for key_to_delete in [""ppl"", ""wps"", ""wpb"", ""bsz""]:\n                    if key_to_delete in logging_output:\n                        del logging_output[key_to_delete]\n            return logging_output\n\n    def _check_xla_compilation(self, message=None):\n        import torch_xla.debug.metrics as met\n        compile_stats = met.metric_data(""CompileTime"")\n        if compile_stats is None:\n            return\n        num_xla_compiles = compile_stats[0]\n        if num_xla_compiles > self._num_xla_compiles:\n            if message is None:\n                message = (\n                    ""too many of these can lead to slow training, ""\n                    ""but we expect a few in the beginning""\n                )\n            logging.info(""NOTE: XLA compilation detected; {}"".format(message))\n        self._num_xla_compiles = num_xla_compiles\n\n\ndef _catalog_shared_params(module, memo=None, prefix=\'\'):\n    if memo is None:\n        first_call = True\n        memo = {}\n    else:\n        first_call = False\n    for name, param in module._parameters.items():\n        param_prefix = prefix + (\'.\' if prefix else \'\') + name\n        if param not in memo:\n            memo[param] = []\n        memo[param].append(param_prefix)\n    for name, m in module._modules.items():\n        if m is None:\n            continue\n        submodule_prefix = prefix + (\'.\' if prefix else \'\') + name\n        _catalog_shared_params(m, memo, submodule_prefix)\n    if first_call:\n        return [x for x in memo.values() if len(x) > 1]\n\n\ndef _get_module_by_path(module, path):\n    path = path.split(\'.\')\n    for name in path:\n        module = getattr(module, name)\n    return module\n\n\ndef _set_module_by_path(module, path, value):\n    path = path.split(\'.\')\n    for name in path[:-1]:\n        module = getattr(module, name)\n    setattr(module, path[-1], value)\n'"
fairseq/utils.py,29,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nimport copy\nimport importlib.util\nimport logging\nimport math\nimport os\nimport sys\nimport warnings\nfrom collections import defaultdict\nfrom itertools import accumulate\nfrom typing import Callable, Dict, List, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fairseq.logging.meters import safe_round\nfrom fairseq.modules import gelu, gelu_accurate\nfrom fairseq.modules.multihead_attention import MultiheadAttention\nfrom torch import Tensor\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef split_paths(paths: str) -> List[str]:\n    return paths.split(os.pathsep) if ""://"" not in paths else paths.split(""|"")\n\n\ndef load_ensemble_for_inference(filenames, task, model_arg_overrides=None):\n    from fairseq import checkpoint_utils\n\n    deprecation_warning(\n        ""utils.load_ensemble_for_inference is deprecated. ""\n        ""Please use checkpoint_utils.load_model_ensemble instead.""\n    )\n    return checkpoint_utils.load_model_ensemble(\n        filenames, arg_overrides=model_arg_overrides, task=task\n    )\n\n\ndef apply_to_sample(f, sample):\n    if hasattr(sample, \'__len__\') and len(sample) == 0:\n        return {}\n\n    def _apply(x):\n        if torch.is_tensor(x):\n            return f(x)\n        elif isinstance(x, dict):\n            return {key: _apply(value) for key, value in x.items()}\n        elif isinstance(x, list):\n            return [_apply(x) for x in x]\n        elif isinstance(x, tuple):\n            return tuple(_apply(x) for x in x)\n        elif isinstance(x, set):\n            return {_apply(x) for x in x}\n        else:\n            return x\n\n    return _apply(sample)\n\n\ndef move_to_cuda(sample):\n    def _move_to_cuda(tensor):\n        return tensor.cuda()\n\n    return apply_to_sample(_move_to_cuda, sample)\n\n\ndef move_to_cpu(sample):\n    def _move_to_cpu(tensor):\n        # PyTorch has poor support for half tensors (float16) on CPU.\n        # Move any such tensors to float32.\n        if tensor.dtype in {torch.bfloat16, torch.float16}:\n            tensor = tensor.to(dtype=torch.float32)\n        return tensor.cpu()\n\n    return apply_to_sample(_move_to_cpu, sample)\n\n\ndef get_incremental_state(\n    module: MultiheadAttention,\n    incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]],\n    key: str,\n) -> Optional[Dict[str, Optional[Tensor]]]:\n    """"""Helper for getting incremental state for an nn.Module.""""""\n    return module.get_incremental_state(incremental_state, key)\n\n\ndef set_incremental_state(\n    module: MultiheadAttention,\n    incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]],\n    key: str,\n    value: Dict[str, Optional[Tensor]],\n) -> Optional[Dict[str, Dict[str, Optional[Tensor]]]]:\n    """"""Helper for setting incremental state for an nn.Module.""""""\n    if incremental_state is not None:\n        result = module.set_incremental_state(incremental_state, key, value)\n        if result is not None:\n            incremental_state = result\n    return incremental_state\n\n\ndef load_align_dict(replace_unk):\n    if replace_unk is None:\n        align_dict = None\n    elif isinstance(replace_unk, str) and len(replace_unk) > 0:\n        # Load alignment dictionary for unknown word replacement if it was passed as an argument.\n        align_dict = {}\n        with open(replace_unk, ""r"") as f:\n            for line in f:\n                cols = line.split()\n                align_dict[cols[0]] = cols[1]\n    else:\n        # No alignment dictionary provided but we still want to perform unknown word replacement by copying the\n        # original source word.\n        align_dict = {}\n    return align_dict\n\n\ndef print_embed_overlap(embed_dict, vocab_dict):\n    embed_keys = set(embed_dict.keys())\n    vocab_keys = set(vocab_dict.symbols)\n    overlap = len(embed_keys & vocab_keys)\n    logger.info(""found {}/{} types in embedding file"".format(overlap, len(vocab_dict)))\n\n\ndef parse_embedding(embed_path):\n    """"""Parse embedding text file into a dictionary of word and embedding tensors.\n\n    The first line can have vocabulary size and dimension. The following lines\n    should contain word and embedding separated by spaces.\n\n    Example:\n        2 5\n        the -0.0230 -0.0264  0.0287  0.0171  0.1403\n        at -0.0395 -0.1286  0.0275  0.0254 -0.0932\n    """"""\n    embed_dict = {}\n    with open(embed_path) as f_embed:\n        next(f_embed)  # skip header\n        for line in f_embed:\n            pieces = line.rstrip().split("" "")\n            embed_dict[pieces[0]] = torch.Tensor(\n                [float(weight) for weight in pieces[1:]]\n            )\n    return embed_dict\n\n\ndef load_embedding(embed_dict, vocab, embedding):\n    for idx in range(len(vocab)):\n        token = vocab[idx]\n        if token in embed_dict:\n            embedding.weight.data[idx] = embed_dict[token]\n    return embedding\n\n\ndef replace_unk(hypo_str, src_str, alignment, align_dict, unk):\n    from fairseq import tokenizer\n\n    # Tokens are strings here\n    hypo_tokens = tokenizer.tokenize_line(hypo_str)\n    # TODO: Very rare cases where the replacement is \'<eos>\' should be handled gracefully\n    src_tokens = tokenizer.tokenize_line(src_str) + [""<eos>""]\n    for i, ht in enumerate(hypo_tokens):\n        if ht == unk:\n            src_token = src_tokens[alignment[i]]\n            # Either take the corresponding value in the aligned dictionary or just copy the original value.\n            hypo_tokens[i] = align_dict.get(src_token, src_token)\n    return "" "".join(hypo_tokens)\n\n\ndef post_process_prediction(\n    hypo_tokens, src_str, alignment, align_dict, tgt_dict, remove_bpe=None, extra_symbols_to_ignore=None\n):\n    hypo_str = tgt_dict.string(hypo_tokens, remove_bpe, extra_symbols_to_ignore=extra_symbols_to_ignore)\n    if align_dict is not None:\n        hypo_str = replace_unk(\n            hypo_str, src_str, alignment, align_dict, tgt_dict.unk_string()\n        )\n    if align_dict is not None or remove_bpe is not None:\n        # Convert back to tokens for evaluating with unk replacement or without BPE\n        # Note that the dictionary can be modified inside the method.\n        hypo_tokens = tgt_dict.encode_line(hypo_str, add_if_not_exist=True)\n    return hypo_tokens, hypo_str, alignment\n\n\ndef make_positions(tensor, padding_idx: int, onnx_trace: bool = False):\n    """"""Replace non-padding symbols with their position numbers.\n\n    Position numbers begin at padding_idx+1. Padding symbols are ignored.\n    """"""\n    # The series of casts and type-conversions here are carefully\n    # balanced to both work with ONNX export and XLA. In particular XLA\n    # prefers ints, cumsum defaults to output longs, and ONNX doesn\'t know\n    # how to handle the dtype kwarg in cumsum.\n    mask = tensor.ne(padding_idx).int()\n    return (torch.cumsum(mask, dim=1).type_as(mask) * mask).long() + padding_idx\n\n\ndef strip_pad(tensor, pad):\n    return tensor[tensor.ne(pad)]\n\n\ndef buffered_arange(max):\n    if not hasattr(buffered_arange, ""buf""):\n        buffered_arange.buf = torch.LongTensor()\n    if max > buffered_arange.buf.numel():\n        buffered_arange.buf.resize_(max)\n        torch.arange(max, out=buffered_arange.buf)\n    return buffered_arange.buf[:max]\n\n\ndef convert_padding_direction(\n    src_tokens, padding_idx, right_to_left: bool = False, left_to_right: bool = False\n):\n    assert right_to_left ^ left_to_right\n    pad_mask = src_tokens.eq(padding_idx)\n    if not pad_mask.any():\n        # no padding, return early\n        return src_tokens\n    if left_to_right and not pad_mask[:, 0].any():\n        # already right padded\n        return src_tokens\n    if right_to_left and not pad_mask[:, -1].any():\n        # already left padded\n        return src_tokens\n    max_len = src_tokens.size(1)\n    buffered = torch.empty(0).long()\n    if max_len > 0:\n        torch.arange(max_len, out=buffered)\n    range = buffered.type_as(src_tokens).expand_as(src_tokens)\n    num_pads = pad_mask.long().sum(dim=1, keepdim=True)\n    if right_to_left:\n        index = torch.remainder(range - num_pads, max_len)\n    else:\n        index = torch.remainder(range + num_pads, max_len)\n    return src_tokens.gather(1, index)\n\n\ndef item(tensor):\n    if hasattr(tensor, ""item""):\n        return tensor.item()\n    if hasattr(tensor, ""__getitem__""):\n        return tensor[0]\n    return tensor\n\n\ndef clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:\n    if isinstance(params, torch.Tensor):\n        params = [params]\n    params = list(params)\n    grads = [p.grad.detach() for p in filter(lambda p: p.grad is not None, params)]\n    if len(grads) == 0:\n        if len(params) > 0:\n            return params[0].new_tensor(0.)\n        else:\n            return torch.tensor(0.)\n\n    if len(grads) == 1:\n        total_norm = torch.norm(grads[0])\n    else:\n        total_norm = torch.norm(torch.stack([torch.norm(g) for g in grads]))\n\n    if aggregate_norm_fn is not None:\n        total_norm = aggregate_norm_fn(total_norm)\n\n    if max_norm > 0:\n        max_norm = float(max_norm)\n        clip_coef = (max_norm / (total_norm + 1e-6)).clamp_(max=1)\n        for g in grads:\n            g.mul_(clip_coef)\n    return total_norm\n\n\ndef fill_with_neg_inf(t):\n    """"""FP16-compatible function that fills a tensor with -inf.""""""\n    return t.float().fill_(float(""-inf"")).type_as(t)\n\n\ndef _match_types(arg1, arg2):\n    """"""Convert the numerical argument to the same type as the other argument""""""\n\n    def upgrade(arg_number, arg_structure):\n        if isinstance(arg_structure, tuple):\n            return tuple([arg_number] * len(arg_structure))\n        elif isinstance(arg_structure, dict):\n            arg = copy.deepcopy(arg_structure)\n            for k in arg:\n                arg[k] = upgrade(arg_number, arg_structure[k])\n            return arg\n        else:\n            return arg_number\n\n    if isinstance(arg1, float) or isinstance(arg1, int):\n        return upgrade(arg1, arg2), arg2\n    elif isinstance(arg2, float) or isinstance(arg2, int):\n        return arg1, upgrade(arg2, arg1)\n\n    return arg1, arg2\n\n\ndef resolve_max_positions(*args):\n    """"""Resolve max position constraints from multiple sources.""""""\n\n    def map_value_update(d1, d2):\n        updated_value = copy.deepcopy(d1)\n        for key in d2:\n            if key not in updated_value:\n                updated_value[key] = d2[key]\n            else:\n                updated_value[key] = min(d1[key], d2[key])\n        return updated_value\n\n    def nullsafe_min(l):\n        minim = None\n        for item in l:\n            if minim is None:\n                minim = item\n            elif item is not None and item < minim:\n                minim = item\n        return minim\n\n    max_positions = None\n    for arg in args:\n        if max_positions is None:\n            max_positions = arg\n        elif arg is not None:\n            max_positions, arg = _match_types(max_positions, arg)\n            if isinstance(arg, float) or isinstance(arg, int):\n                max_positions = min(max_positions, arg)\n            elif isinstance(arg, dict):\n                max_positions = map_value_update(max_positions, arg)\n            else:\n                max_positions = tuple(map(nullsafe_min, zip(max_positions, arg)))\n\n    return max_positions\n\n\ndef import_user_module(args):\n    module_path = getattr(args, ""user_dir"", None)\n    if module_path is not None:\n        module_path = os.path.abspath(args.user_dir)\n        if not os.path.exists(module_path):\n            fairseq_rel_path = os.path.join(\n                os.path.dirname(__file__), "".."", args.user_dir\n            )\n            if os.path.exists(fairseq_rel_path):\n                module_path = fairseq_rel_path\n        module_parent, module_name = os.path.split(module_path)\n\n        if module_name not in sys.modules:\n            sys.path.insert(0, module_parent)\n            importlib.import_module(module_name)\n\n\ndef softmax(x, dim: int, onnx_trace: bool = False):\n    if onnx_trace:\n        return F.softmax(x.float(), dim=dim)\n    else:\n        return F.softmax(x, dim=dim, dtype=torch.float32)\n\n\ndef log_softmax(x, dim: int, onnx_trace: bool = False):\n    if onnx_trace:\n        return F.log_softmax(x.float(), dim=dim)\n    else:\n        return F.log_softmax(x, dim=dim, dtype=torch.float32)\n\n\ndef get_perplexity(loss, round=2, base=2):\n    if loss is None:\n        return 0.\n    try:\n        return safe_round(base ** loss, round)\n    except OverflowError:\n        return float(\'inf\')\n\n\ndef deprecation_warning(message, stacklevel=3):\n    # don\'t use DeprecationWarning, since it\'s ignored by default\n    warnings.warn(message, stacklevel=stacklevel)\n\n\ndef get_activation_fn(activation: str) -> Callable:\n    """""" Returns the activation function corresponding to `activation` """"""\n    if activation == ""relu"":\n        return F.relu\n    elif activation == ""gelu"":\n        return gelu\n    elif activation == ""gelu_fast"":\n        deprecation_warning(\n            ""--activation-fn=gelu_fast has been renamed to gelu_accurate""\n        )\n        return gelu_accurate\n    elif activation == ""gelu_accurate"":\n        return gelu_accurate\n    elif activation == ""tanh"":\n        return torch.tanh\n    elif activation == ""linear"":\n        return lambda x: x\n    else:\n        raise RuntimeError(""--activation-fn {} not supported"".format(activation))\n\n\ndef get_available_activation_fns() -> List:\n    return [\n        ""relu"",\n        ""gelu"",\n        ""gelu_fast"",  # deprecated\n        ""gelu_accurate"",\n        ""tanh"",\n        ""linear"",\n    ]\n\n\n@contextlib.contextmanager\ndef eval(model):\n    is_training = model.training\n    model.eval()\n    yield\n    model.train(is_training)\n\n\ndef has_parameters(module):\n    try:\n        next(module.parameters())\n        return True\n    except StopIteration:\n        return False\n\n\ndef set_torch_seed(seed):\n    # Set seed based on args.seed and the update number so that we get\n    # reproducible results when resuming from checkpoints\n    assert isinstance(seed, int)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\n@contextlib.contextmanager\ndef with_torch_seed(seed):\n    assert isinstance(seed, int)\n    rng_state = torch.get_rng_state()\n    cuda_rng_state = torch.cuda.get_rng_state()\n    set_torch_seed(seed)\n    yield\n    torch.set_rng_state(rng_state)\n    torch.cuda.set_rng_state(cuda_rng_state)\n\n\ndef parse_alignment(line):\n    """"""\n    Parses a single line from the alingment file.\n\n    Args:\n        line (str): String containing the alignment of the format:\n            <src_idx_1>-<tgt_idx_1> <src_idx_2>-<tgt_idx_2> ..\n            <src_idx_m>-<tgt_idx_m>. All indices are 0 indexed.\n\n    Returns:\n        torch.IntTensor: packed alignments of shape (2 * m).\n    """"""\n    alignments = line.strip().split()\n    parsed_alignment = torch.IntTensor(2 * len(alignments))\n    for idx, alignment in enumerate(alignments):\n        src_idx, tgt_idx = alignment.split(""-"")\n        parsed_alignment[2 * idx] = int(src_idx)\n        parsed_alignment[2 * idx + 1] = int(tgt_idx)\n    return parsed_alignment\n\n\ndef get_token_to_word_mapping(tokens, exclude_list):\n    n = len(tokens)\n    word_start = [int(token not in exclude_list) for token in tokens]\n    word_idx = list(accumulate(word_start))\n    token_to_word = {i: word_idx[i] for i in range(n)}\n    return token_to_word\n\n\ndef extract_hard_alignment(attn, src_sent, tgt_sent, pad, eos):\n    tgt_valid = ((tgt_sent != pad) & (tgt_sent != eos)).nonzero().squeeze(dim=-1)\n    src_invalid = ((src_sent == pad) | (src_sent == eos)).nonzero().squeeze(dim=-1)\n    src_token_to_word = get_token_to_word_mapping(src_sent, [eos, pad])\n    tgt_token_to_word = get_token_to_word_mapping(tgt_sent, [eos, pad])\n    alignment = []\n    if len(tgt_valid) != 0 and len(src_invalid) < len(src_sent):\n        attn_valid = attn[tgt_valid]\n        attn_valid[:, src_invalid] = float(""-inf"")\n        _, src_indices = attn_valid.max(dim=1)\n        for tgt_idx, src_idx in zip(tgt_valid, src_indices):\n            alignment.append(\n                (\n                    src_token_to_word[src_idx.item()] - 1,\n                    tgt_token_to_word[tgt_idx.item()] - 1,\n                )\n            )\n    return alignment\n\n\ndef new_arange(x, *size):\n    """"""\n    Return a Tensor of `size` filled with a range function on the device of x.\n    If size is empty, using the size of the variable x.\n    """"""\n    if len(size) == 0:\n        size = x.size()\n    return torch.arange(size[-1], device=x.device).expand(*size).contiguous()\n\n\ndef get_tpu_device(args):\n    import torch_xla.core.xla_model as xm\n    return xm.xla_device()\n'"
fairseq_cli/__init__.py,0,b''
fairseq_cli/eval_lm.py,3,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nEvaluate the perplexity of a trained language model.\n""""""\n\nimport logging\nimport math\nimport os\n\nimport torch\n\nfrom fairseq import checkpoint_utils, options, tasks, utils\nfrom fairseq.data import LMContextWindowDataset\nfrom fairseq.logging import progress_bar\nfrom fairseq.logging.meters import StopwatchMeter, TimeMeter\nfrom fairseq.sequence_scorer import SequenceScorer\nfrom fairseq.options import add_distributed_training_args\nfrom fairseq import distributed_utils\n\n\nlogging.basicConfig(\n    format=\'%(asctime)s | %(levelname)s | %(name)s | %(message)s\',\n    datefmt=\'%Y-%m-%d %H:%M:%S\',\n    level=logging.INFO,\n)\nlogger = logging.getLogger(\'fairseq_cli.eval_lm\')\n\n\nclass WordStat(object):\n    def __init__(self, word, is_bpe):\n        self.word = word\n        self.is_bpe = is_bpe\n        self.log_prob = 0\n        self.next_word_prob = 0\n        self.count = 0\n        self.missing_next_words = 0\n\n    def add(self, log_prob, next_word_prob):\n        """""" increments counters for the sum of log probs of current word and next\n            word (given context ending at current word). Since the next word might be at the end of the example,\n            or it might be not counted because it is not an ending subword unit,\n            also keeps track of how many of those we have seen """"""\n        if next_word_prob is not None:\n            self.next_word_prob += next_word_prob\n        else:\n            self.missing_next_words += 1\n        self.log_prob += log_prob\n        self.count += 1\n\n    def __str__(self):\n        return \'{}\\t{}\\t{}\\t{}\\t{}\\t{}\'.format(self.word, self.count, self.log_prob, self.is_bpe,\n                                               self.next_word_prob, self.count - self.missing_next_words)\n\n\ndef main(parsed_args, **unused_kwargs):\n    assert parsed_args.path is not None, \'--path required for evaluation!\'\n\n    if torch.cuda.is_available() and not parsed_args.cpu:\n        torch.cuda.set_device(parsed_args.device_id)\n\n    utils.import_user_module(parsed_args)\n\n    logger.info(parsed_args)\n\n    use_cuda = torch.cuda.is_available() and not parsed_args.cpu\n\n    task = tasks.setup_task(parsed_args)\n\n    # Load ensemble\n    logger.info(\'loading model(s) from {}\'.format(parsed_args.path))\n    models, args = checkpoint_utils.load_model_ensemble(\n        parsed_args.path.split(os.pathsep),\n        arg_overrides=eval(parsed_args.model_overrides),\n        task=task,\n        suffix=getattr(parsed_args, ""checkpoint_suffix"", """"),\n    )\n\n    for arg in vars(parsed_args).keys():\n        if arg not in {\n            \'self_target\', \'future_target\', \'past_target\', \'tokens_per_sample\',\n            \'output_size_dictionary\', \'add_bos_token\',\n        }:\n            setattr(args, arg, getattr(parsed_args, arg))\n\n    # reduce tokens per sample by the required context window size\n    args.tokens_per_sample -= args.context_window\n    task = tasks.setup_task(args)\n\n    # Load dataset splits\n    task.load_dataset(args.gen_subset)\n    dataset = task.dataset(args.gen_subset)\n    if args.context_window > 0:\n        dataset = LMContextWindowDataset(\n            dataset=dataset,\n            tokens_per_sample=args.tokens_per_sample,\n            context_window=args.context_window,\n            pad_idx=task.source_dictionary.pad(),\n        )\n    logger.info(\'{} {} {} examples\'.format(args.data, args.gen_subset, len(dataset)))\n\n    # Optimize ensemble for generation and set the source and dest dicts on the model (required by scorer)\n    for model in models:\n        model.make_generation_fast_()\n        if args.fp16:\n            model.half()\n        if use_cuda:\n            model.cuda()\n\n    assert len(models) > 0\n\n    logger.info(\'num. model params: {}\'.format(sum(p.numel() for p in models[0].parameters())))\n\n    itr = task.get_batch_iterator(\n        dataset=dataset,\n        max_tokens=args.max_tokens or 36000,\n        max_sentences=args.max_sentences,\n        max_positions=utils.resolve_max_positions(*[\n            model.max_positions() for model in models\n        ]),\n        ignore_invalid_inputs=True,\n        num_shards=args.num_shards,\n        shard_id=args.shard_id,\n        num_workers=args.num_workers,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=args.log_format,\n        log_interval=args.log_interval,\n        default_log_format=(\'tqdm\' if not args.no_progress_bar else \'none\'),\n    )\n\n    gen_timer = StopwatchMeter()\n    scorer = SequenceScorer(task.target_dictionary, args.softmax_batch)\n\n    score_sum = 0.\n    count = 0\n\n    if args.remove_bpe is not None:\n        if args.remove_bpe == \'sentencepiece\':\n            raise NotImplementedError\n        else:\n            bpe_cont = args.remove_bpe.rstrip()\n            bpe_toks = {\n                i\n                for i in range(len(task.source_dictionary))\n                if task.source_dictionary[i].endswith(bpe_cont)\n            }\n        bpe_len = len(bpe_cont)\n    else:\n        bpe_toks = None\n        bpe_len = 0\n\n    word_stats = dict()\n\n    wps_meter = TimeMeter()\n\n    for sample in progress:\n        if \'net_input\' not in sample:\n            continue\n\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n\n        gen_timer.start()\n        hypos = scorer.generate(models, sample)\n        gen_timer.stop(sample[\'ntokens\'])\n\n        for i, hypos_i in enumerate(hypos):\n            hypo = hypos_i[0]\n            sample_id = sample[\'id\'][i]\n\n            tokens = hypo[\'tokens\']\n            tgt_len = tokens.numel()\n            pos_scores = hypo[\'positional_scores\'].float()\n\n            if args.add_bos_token:\n                assert hypo[\'tokens\'][0].item() == task.target_dictionary.bos()\n                tokens = tokens[1:]\n                pos_scores = pos_scores[1:]\n\n            skipped_toks = 0\n            if bpe_toks is not None:\n                for i in range(tgt_len - 1):\n                    if tokens[i].item() in bpe_toks:\n                        skipped_toks += 1\n                        pos_scores[i + 1] += pos_scores[i]\n                        pos_scores[i] = 0\n\n            inf_scores = pos_scores.eq(float(\'inf\')) | pos_scores.eq(float(\'-inf\'))\n            if inf_scores.any():\n                logger.info(\n                    \'skipping tokens with inf scores:\',\n                    task.target_dictionary.string(tokens[inf_scores.nonzero()])\n                )\n                pos_scores = pos_scores[(~inf_scores).nonzero()]\n            score_sum += pos_scores.sum().cpu()\n            count += pos_scores.numel() - skipped_toks\n\n            if args.output_word_probs or args.output_word_stats:\n                w = \'\'\n                word_prob = []\n                is_bpe = False\n                for i in range(len(tokens)):\n                    w_ind = tokens[i].item()\n                    w += task.source_dictionary[w_ind]\n                    if bpe_toks is not None and w_ind in bpe_toks:\n                        w = w[:-bpe_len]\n                        is_bpe = True\n                    else:\n                        word_prob.append((w, pos_scores[i].item()))\n\n                        next_prob = None\n                        ind = i + 1\n                        while ind < len(tokens):\n                            if pos_scores[ind].item() != 0:\n                                next_prob = pos_scores[ind]\n                                break\n                            ind += 1\n\n                        word_stats.setdefault(w, WordStat(w, is_bpe)).add(pos_scores[i].item(), next_prob)\n                        is_bpe = False\n                        w = \'\'\n                if args.output_word_probs:\n                    logger.info(\n                        str(int(sample_id)) + "" ""\n                        + (\'\\t\'.join(\'{} [{:2f}]\'.format(x[0], x[1]) for x in word_prob))\n                    )\n\n        wps_meter.update(sample[\'ntokens\'])\n        progress.log({\'wps\': round(wps_meter.avg)})\n\n    avg_nll_loss = -score_sum / count / math.log(2)  # convert to base 2\n    logger.info(\'Evaluated {} tokens in {:.1f}s ({:.2f} tokens/s)\'.format(\n        gen_timer.n, gen_timer.sum, 1. / gen_timer.avg\n    ))\n    logger.info(\'Loss (base 2): {:.4f}, Perplexity: {:.2f}\'.format(\n        avg_nll_loss, 2**avg_nll_loss\n    ))\n\n    if args.output_word_stats:\n        for ws in sorted(word_stats.values(), key=lambda x: x.count, reverse=True):\n            logger.info(ws)\n\n\ndef cli_main():\n    parser = options.get_eval_lm_parser()\n    args = options.parse_args_and_arch(parser)\n    distributed_utils.call_main(args, main)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
fairseq_cli/generate.py,1,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nTranslate pre-processed data with a trained model.\n""""""\n\nimport logging\nimport math\nimport os\nimport sys\n\nimport torch\n\nfrom fairseq import bleu, checkpoint_utils, options, tasks, utils\nfrom fairseq.logging import progress_bar\nfrom fairseq.logging.meters import StopwatchMeter, TimeMeter\nfrom fairseq.data import encoders\n\n\ndef main(args):\n    assert args.path is not None, \'--path required for generation!\'\n    assert not args.sampling or args.nbest == args.beam, \\\n        \'--sampling requires --nbest to be equal to --beam\'\n    assert args.replace_unk is None or args.dataset_impl == \'raw\', \\\n        \'--replace-unk requires a raw text dataset (--dataset-impl=raw)\'\n\n    if args.results_path is not None:\n        os.makedirs(args.results_path, exist_ok=True)\n        output_path = os.path.join(args.results_path, \'generate-{}.txt\'.format(args.gen_subset))\n        with open(output_path, \'w\', buffering=1) as h:\n            return _main(args, h)\n    else:\n        return _main(args, sys.stdout)\n\n\ndef _main(args, output_file):\n    logging.basicConfig(\n        format=\'%(asctime)s | %(levelname)s | %(name)s | %(message)s\',\n        datefmt=\'%Y-%m-%d %H:%M:%S\',\n        level=logging.INFO,\n        stream=output_file,\n    )\n    logger = logging.getLogger(\'fairseq_cli.generate\')\n\n    utils.import_user_module(args)\n\n    if args.max_tokens is None and args.max_sentences is None:\n        args.max_tokens = 12000\n    logger.info(args)\n\n    use_cuda = torch.cuda.is_available() and not args.cpu\n\n    # Load dataset splits\n    task = tasks.setup_task(args)\n    task.load_dataset(args.gen_subset)\n\n    # Set dictionaries\n    try:\n        src_dict = getattr(task, \'source_dictionary\', None)\n    except NotImplementedError:\n        src_dict = None\n    tgt_dict = task.target_dictionary\n\n    # Load ensemble\n    logger.info(\'loading model(s) from {}\'.format(args.path))\n    models, _model_args = checkpoint_utils.load_model_ensemble(\n        utils.split_paths(args.path),\n        arg_overrides=eval(args.model_overrides),\n        task=task,\n    )\n\n    # Optimize ensemble for generation\n    for model in models:\n        model.make_generation_fast_(\n            beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,\n            need_attn=args.print_alignment,\n        )\n        if args.fp16:\n            model.half()\n        if use_cuda:\n            model.cuda()\n\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(args.replace_unk)\n\n    # Load dataset (possibly sharded)\n    itr = task.get_batch_iterator(\n        dataset=task.dataset(args.gen_subset),\n        max_tokens=args.max_tokens,\n        max_sentences=args.max_sentences,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(),\n            *[model.max_positions() for model in models]\n        ),\n        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=args.required_batch_size_multiple,\n        num_shards=args.num_shards,\n        shard_id=args.shard_id,\n        num_workers=args.num_workers,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=args.log_format,\n        log_interval=args.log_interval,\n        default_log_format=(\'tqdm\' if not args.no_progress_bar else \'none\'),\n    )\n\n    # Initialize generator\n    gen_timer = StopwatchMeter()\n    generator = task.build_generator(models, args)\n\n    # Handle tokenization and BPE\n    tokenizer = encoders.build_tokenizer(args)\n    bpe = encoders.build_bpe(args)\n\n    def decode_fn(x):\n        if bpe is not None:\n            x = bpe.decode(x)\n        if tokenizer is not None:\n            x = tokenizer.decode(x)\n        return x\n\n    # Generate and compute BLEU score\n    if args.sacrebleu:\n        scorer = bleu.SacrebleuScorer()\n    else:\n        scorer = bleu.Scorer(tgt_dict.pad(), tgt_dict.eos(), tgt_dict.unk())\n    num_sentences = 0\n    has_target = True\n    wps_meter = TimeMeter()\n    for sample in progress:\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n        if \'net_input\' not in sample:\n            continue\n\n        prefix_tokens = None\n        if args.prefix_size > 0:\n            prefix_tokens = sample[\'target\'][:, :args.prefix_size]\n\n        gen_timer.start()\n        hypos = task.inference_step(generator, models, sample, prefix_tokens)\n        num_generated_tokens = sum(len(h[0][\'tokens\']) for h in hypos)\n        gen_timer.stop(num_generated_tokens)\n\n        for i, sample_id in enumerate(sample[\'id\'].tolist()):\n            has_target = sample[\'target\'] is not None\n\n            # Remove padding\n            src_tokens = utils.strip_pad(sample[\'net_input\'][\'src_tokens\'][i, :], tgt_dict.pad())\n            target_tokens = None\n            if has_target:\n                target_tokens = utils.strip_pad(sample[\'target\'][i, :], tgt_dict.pad()).int().cpu()\n\n            # Either retrieve the original sentences or regenerate them from tokens.\n            if align_dict is not None:\n                src_str = task.dataset(args.gen_subset).src.get_original_text(sample_id)\n                target_str = task.dataset(args.gen_subset).tgt.get_original_text(sample_id)\n            else:\n                if src_dict is not None:\n                    src_str = src_dict.string(src_tokens, args.remove_bpe)\n                else:\n                    src_str = """"\n                if has_target:\n                    target_str = tgt_dict.string(\n                        target_tokens,\n                        args.remove_bpe,\n                        escape_unk=True,\n                        extra_symbols_to_ignore={\n                            generator.eos,\n                        }\n                    )\n\n            src_str = decode_fn(src_str)\n            if has_target:\n                target_str = decode_fn(target_str)\n\n            if not args.quiet:\n                if src_dict is not None:\n                    print(\'S-{}\\t{}\'.format(sample_id, src_str), file=output_file)\n                if has_target:\n                    print(\'T-{}\\t{}\'.format(sample_id, target_str), file=output_file)\n\n            # Process top predictions\n            for j, hypo in enumerate(hypos[i][:args.nbest]):\n                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                    hypo_tokens=hypo[\'tokens\'].int().cpu(),\n                    src_str=src_str,\n                    alignment=hypo[\'alignment\'],\n                    align_dict=align_dict,\n                    tgt_dict=tgt_dict,\n                    remove_bpe=args.remove_bpe,\n                    extra_symbols_to_ignore={\n                        generator.eos,\n                    }\n                )\n                detok_hypo_str = decode_fn(hypo_str)\n                if not args.quiet:\n                    score = hypo[\'score\'] / math.log(2)  # convert to base 2\n                    # original hypothesis (after tokenization and BPE)\n                    print(\'H-{}\\t{}\\t{}\'.format(sample_id, score, hypo_str), file=output_file)\n                    # detokenized hypothesis\n                    print(\'D-{}\\t{}\\t{}\'.format(sample_id, score, detok_hypo_str), file=output_file)\n                    print(\'P-{}\\t{}\'.format(\n                        sample_id,\n                        \' \'.join(map(\n                            lambda x: \'{:.4f}\'.format(x),\n                            # convert from base e to base 2\n                            hypo[\'positional_scores\'].div_(math.log(2)).tolist(),\n                        ))\n                    ), file=output_file)\n\n                    if args.print_alignment:\n                        print(\'A-{}\\t{}\'.format(\n                            sample_id,\n                            \' \'.join([\'{}-{}\'.format(src_idx, tgt_idx) for src_idx, tgt_idx in alignment])\n                        ), file=output_file)\n\n                    if args.print_step:\n                        print(\'I-{}\\t{}\'.format(sample_id, hypo[\'steps\']), file=output_file)\n\n                    if getattr(args, \'retain_iter_history\', False):\n                        for step, h in enumerate(hypo[\'history\']):\n                            _, h_str, _ = utils.post_process_prediction(\n                                hypo_tokens=h[\'tokens\'].int().cpu(),\n                                src_str=src_str,\n                                alignment=None,\n                                align_dict=None,\n                                tgt_dict=tgt_dict,\n                                remove_bpe=None,\n                            )\n                            print(\'E-{}_{}\\t{}\'.format(sample_id, step, h_str), file=output_file)\n\n                # Score only the top hypothesis\n                if has_target and j == 0:\n                    if align_dict is not None or args.remove_bpe is not None:\n                        # Convert back to tokens for evaluation with unk replacement and/or without BPE\n                        target_tokens = tgt_dict.encode_line(target_str, add_if_not_exist=True)\n                        hypo_tokens = tgt_dict.encode_line(detok_hypo_str, add_if_not_exist=True)\n                    if hasattr(scorer, \'add_string\'):\n                        scorer.add_string(target_str, detok_hypo_str)\n                    else:\n                        scorer.add(target_tokens, hypo_tokens)\n\n        wps_meter.update(num_generated_tokens)\n        progress.log({\'wps\': round(wps_meter.avg)})\n        num_sentences += sample[\'nsentences\']\n\n    logger.info(\'NOTE: hypothesis and token scores are output in base 2\')\n    logger.info(\'Translated {} sentences ({} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)\'.format(\n        num_sentences, gen_timer.n, gen_timer.sum, num_sentences / gen_timer.sum, 1. / gen_timer.avg))\n    if has_target:\n        if args.bpe and not args.sacrebleu:\n            if args.remove_bpe:\n                logger.warning(""BLEU score is being computed by splitting detokenized string on spaces, this is probably not what you want. Use --sacrebleu for standard 13a BLEU tokenization"")\n            else:\n                logger.warning(""If you are using BPE on the target side, the BLEU score is computed on BPE tokens, not on proper words.  Use --sacrebleu for standard 13a BLEU tokenization"")\n        logger.info(\'Generate {} with beam={}: {}\'.format(args.gen_subset, args.beam, scorer.result_string()))\n\n    return scorer\n\n\ndef cli_main():\n    parser = options.get_generation_parser()\n    args = options.parse_args_and_arch(parser)\n    main(args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
fairseq_cli/interactive.py,1,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nTranslate raw text with a trained model. Batches data on-the-fly.\n""""""\n\nfrom collections import namedtuple\nimport fileinput\nimport logging\nimport math\nimport sys\nimport os\n\nimport torch\n\nfrom fairseq import checkpoint_utils, options, tasks, utils\nfrom fairseq.data import encoders\n\n\nlogging.basicConfig(\n    format=\'%(asctime)s | %(levelname)s | %(name)s | %(message)s\',\n    datefmt=\'%Y-%m-%d %H:%M:%S\',\n    level=logging.INFO,\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\'fairseq_cli.interactive\')\n\n\nBatch = namedtuple(\'Batch\', \'ids src_tokens src_lengths\')\nTranslation = namedtuple(\'Translation\', \'src_str hypos pos_scores alignments\')\n\n\ndef buffered_read(input, buffer_size):\n    buffer = []\n    with fileinput.input(files=[input], openhook=fileinput.hook_encoded(""utf-8"")) as h:\n        for src_str in h:\n            buffer.append(src_str.strip())\n            if len(buffer) >= buffer_size:\n                yield buffer\n                buffer = []\n\n    if len(buffer) > 0:\n        yield buffer\n\n\ndef make_batches(lines, args, task, max_positions, encode_fn):\n    tokens = [\n        task.source_dictionary.encode_line(\n            encode_fn(src_str), add_if_not_exist=False\n        ).long()\n        for src_str in lines\n    ]\n    lengths = [t.numel() for t in tokens]\n    itr = task.get_batch_iterator(\n        dataset=task.build_dataset_for_inference(tokens, lengths),\n        max_tokens=args.max_tokens,\n        max_sentences=args.max_sentences,\n        max_positions=max_positions,\n        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test\n    ).next_epoch_itr(shuffle=False)\n    for batch in itr:\n        yield Batch(\n            ids=batch[\'id\'],\n            src_tokens=batch[\'net_input\'][\'src_tokens\'], src_lengths=batch[\'net_input\'][\'src_lengths\'],\n        )\n\n\ndef main(args):\n    utils.import_user_module(args)\n\n    if args.buffer_size < 1:\n        args.buffer_size = 1\n    if args.max_tokens is None and args.max_sentences is None:\n        args.max_sentences = 1\n\n    assert not args.sampling or args.nbest == args.beam, \\\n        \'--sampling requires --nbest to be equal to --beam\'\n    assert not args.max_sentences or args.max_sentences <= args.buffer_size, \\\n        \'--max-sentences/--batch-size cannot be larger than --buffer-size\'\n\n    logger.info(args)\n\n    use_cuda = torch.cuda.is_available() and not args.cpu\n\n    # Setup task, e.g., translation\n    task = tasks.setup_task(args)\n\n    # Load ensemble\n    logger.info(\'loading model(s) from {}\'.format(args.path))\n    models, _model_args = checkpoint_utils.load_model_ensemble(\n        args.path.split(os.pathsep),\n        arg_overrides=eval(args.model_overrides),\n        task=task,\n    )\n\n    # Set dictionaries\n    src_dict = task.source_dictionary\n    tgt_dict = task.target_dictionary\n\n    # Optimize ensemble for generation\n    for model in models:\n        model.make_generation_fast_(\n            beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,\n            need_attn=args.print_alignment,\n        )\n        if args.fp16:\n            model.half()\n        if use_cuda:\n            model.cuda()\n\n    # Initialize generator\n    generator = task.build_generator(models, args)\n\n    # Handle tokenization and BPE\n    tokenizer = encoders.build_tokenizer(args)\n    bpe = encoders.build_bpe(args)\n\n    def encode_fn(x):\n        if tokenizer is not None:\n            x = tokenizer.encode(x)\n        if bpe is not None:\n            x = bpe.encode(x)\n        return x\n\n    def decode_fn(x):\n        if bpe is not None:\n            x = bpe.decode(x)\n        if tokenizer is not None:\n            x = tokenizer.decode(x)\n        return x\n\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(args.replace_unk)\n\n    max_positions = utils.resolve_max_positions(\n        task.max_positions(),\n        *[model.max_positions() for model in models]\n    )\n\n    if args.buffer_size > 1:\n        logger.info(\'Sentence buffer size: %s\', args.buffer_size)\n    logger.info(\'NOTE: hypothesis and token scores are output in base 2\')\n    logger.info(\'Type the input sentence and press return:\')\n    start_id = 0\n    for inputs in buffered_read(args.input, args.buffer_size):\n        results = []\n        for batch in make_batches(inputs, args, task, max_positions, encode_fn):\n            src_tokens = batch.src_tokens\n            src_lengths = batch.src_lengths\n            if use_cuda:\n                src_tokens = src_tokens.cuda()\n                src_lengths = src_lengths.cuda()\n\n            sample = {\n                \'net_input\': {\n                    \'src_tokens\': src_tokens,\n                    \'src_lengths\': src_lengths,\n                },\n            }\n            translations = task.inference_step(generator, models, sample)\n            for i, (id, hypos) in enumerate(zip(batch.ids.tolist(), translations)):\n                src_tokens_i = utils.strip_pad(src_tokens[i], tgt_dict.pad())\n                results.append((start_id + id, src_tokens_i, hypos))\n\n        # sort output to match input order\n        for id, src_tokens, hypos in sorted(results, key=lambda x: x[0]):\n            if src_dict is not None:\n                src_str = src_dict.string(src_tokens, args.remove_bpe)\n                print(\'S-{}\\t{}\'.format(id, src_str))\n\n            # Process top predictions\n            for hypo in hypos[:min(len(hypos), args.nbest)]:\n                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                    hypo_tokens=hypo[\'tokens\'].int().cpu(),\n                    src_str=src_str,\n                    alignment=hypo[\'alignment\'],\n                    align_dict=align_dict,\n                    tgt_dict=tgt_dict,\n                    remove_bpe=args.remove_bpe,\n                )\n                detok_hypo_str = decode_fn(hypo_str)\n                score = hypo[\'score\'] / math.log(2)  # convert to base 2\n                # original hypothesis (after tokenization and BPE)\n                print(\'H-{}\\t{}\\t{}\'.format(id, score, hypo_str))\n                # detokenized hypothesis\n                print(\'D-{}\\t{}\\t{}\'.format(id, score, detok_hypo_str))\n                print(\'P-{}\\t{}\'.format(\n                    id,\n                    \' \'.join(map(\n                        lambda x: \'{:.4f}\'.format(x),\n                        # convert from base e to base 2\n                        hypo[\'positional_scores\'].div_(math.log(2)).tolist(),\n                    ))\n                ))\n                if args.print_alignment:\n                    alignment_str = "" "".join([""{}-{}"".format(src, tgt) for src, tgt in alignment])\n                    print(\'A-{}\\t{}\'.format(\n                        id,\n                        alignment_str\n                    ))\n\n        # update running id counter\n        start_id += len(inputs)\n\n\ndef cli_main():\n    parser = options.get_generation_parser(interactive=True)\n    args = options.parse_args_and_arch(parser)\n    main(args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
fairseq_cli/preprocess.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nData pre-processing: build vocabularies and binarize training data.\n""""""\n\nfrom collections import Counter\nfrom itertools import zip_longest\nimport logging\nfrom multiprocessing import Pool\nimport os\nimport shutil\nimport sys\n\nfrom fairseq import options, tasks, utils\nfrom fairseq.data import indexed_dataset\nfrom fairseq.binarizer import Binarizer\n\n\nlogging.basicConfig(\n    format=\'%(asctime)s | %(levelname)s | %(name)s | %(message)s\',\n    datefmt=\'%Y-%m-%d %H:%M:%S\',\n    level=logging.INFO,\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\'fairseq_cli.preprocess\')\n\n\ndef main(args):\n    utils.import_user_module(args)\n\n    os.makedirs(args.destdir, exist_ok=True)\n\n    logger.addHandler(logging.FileHandler(\n        filename=os.path.join(args.destdir, \'preprocess.log\'),\n    ))\n    logger.info(args)\n\n    task = tasks.get_task(args.task)\n\n    def train_path(lang):\n        return ""{}{}"".format(args.trainpref, (""."" + lang) if lang else """")\n\n    def file_name(prefix, lang):\n        fname = prefix\n        if lang is not None:\n            fname += "".{lang}"".format(lang=lang)\n        return fname\n\n    def dest_path(prefix, lang):\n        return os.path.join(args.destdir, file_name(prefix, lang))\n\n    def dict_path(lang):\n        return dest_path(""dict"", lang) + "".txt""\n\n    def build_dictionary(filenames, src=False, tgt=False):\n        assert src ^ tgt\n        return task.build_dictionary(\n            filenames,\n            workers=args.workers,\n            threshold=args.thresholdsrc if src else args.thresholdtgt,\n            nwords=args.nwordssrc if src else args.nwordstgt,\n            padding_factor=args.padding_factor,\n        )\n\n    target = not args.only_source\n\n    if not args.srcdict and os.path.exists(dict_path(args.source_lang)):\n        raise FileExistsError(dict_path(args.source_lang))\n    if target and not args.tgtdict and os.path.exists(dict_path(args.target_lang)):\n        raise FileExistsError(dict_path(args.target_lang))\n\n    if args.joined_dictionary:\n        assert not args.srcdict or not args.tgtdict, \\\n            ""cannot use both --srcdict and --tgtdict with --joined-dictionary""\n\n        if args.srcdict:\n            src_dict = task.load_dictionary(args.srcdict)\n        elif args.tgtdict:\n            src_dict = task.load_dictionary(args.tgtdict)\n        else:\n            assert args.trainpref, ""--trainpref must be set if --srcdict is not specified""\n            src_dict = build_dictionary(\n                {train_path(lang) for lang in [args.source_lang, args.target_lang]}, src=True\n            )\n        tgt_dict = src_dict\n    else:\n        if args.srcdict:\n            src_dict = task.load_dictionary(args.srcdict)\n        else:\n            assert args.trainpref, ""--trainpref must be set if --srcdict is not specified""\n            src_dict = build_dictionary([train_path(args.source_lang)], src=True)\n\n        if target:\n            if args.tgtdict:\n                tgt_dict = task.load_dictionary(args.tgtdict)\n            else:\n                assert args.trainpref, ""--trainpref must be set if --tgtdict is not specified""\n                tgt_dict = build_dictionary([train_path(args.target_lang)], tgt=True)\n        else:\n            tgt_dict = None\n\n    src_dict.save(dict_path(args.source_lang))\n    if target and tgt_dict is not None:\n        tgt_dict.save(dict_path(args.target_lang))\n\n    def make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers):\n        logger.info(""[{}] Dictionary: {} types"".format(lang, len(vocab) - 1))\n        n_seq_tok = [0, 0]\n        replaced = Counter()\n\n        def merge_result(worker_result):\n            replaced.update(worker_result[""replaced""])\n            n_seq_tok[0] += worker_result[""nseq""]\n            n_seq_tok[1] += worker_result[""ntok""]\n\n        input_file = ""{}{}"".format(\n            input_prefix, (""."" + lang) if lang is not None else """"\n        )\n        offsets = Binarizer.find_offsets(input_file, num_workers)\n        pool = None\n        if num_workers > 1:\n            pool = Pool(processes=num_workers - 1)\n            for worker_id in range(1, num_workers):\n                prefix = ""{}{}"".format(output_prefix, worker_id)\n                pool.apply_async(\n                    binarize,\n                    (\n                        args,\n                        input_file,\n                        vocab,\n                        prefix,\n                        lang,\n                        offsets[worker_id],\n                        offsets[worker_id + 1]\n                    ),\n                    callback=merge_result\n                )\n            pool.close()\n\n        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, ""bin""),\n                                          impl=args.dataset_impl, vocab_size=len(vocab))\n        merge_result(\n            Binarizer.binarize(\n                input_file, vocab, lambda t: ds.add_item(t),\n                offset=0, end=offsets[1]\n            )\n        )\n        if num_workers > 1:\n            pool.join()\n            for worker_id in range(1, num_workers):\n                prefix = ""{}{}"".format(output_prefix, worker_id)\n                temp_file_path = dataset_dest_prefix(args, prefix, lang)\n                ds.merge_file_(temp_file_path)\n                os.remove(indexed_dataset.data_file_path(temp_file_path))\n                os.remove(indexed_dataset.index_file_path(temp_file_path))\n\n        ds.finalize(dataset_dest_file(args, output_prefix, lang, ""idx""))\n\n        logger.info(\n            ""[{}] {}: {} sents, {} tokens, {:.3}% replaced by {}"".format(\n                lang,\n                input_file,\n                n_seq_tok[0],\n                n_seq_tok[1],\n                100 * sum(replaced.values()) / n_seq_tok[1],\n                vocab.unk_word,\n            )\n        )\n\n    def make_binary_alignment_dataset(input_prefix, output_prefix, num_workers):\n        nseq = [0]\n\n        def merge_result(worker_result):\n            nseq[0] += worker_result[\'nseq\']\n\n        input_file = input_prefix\n        offsets = Binarizer.find_offsets(input_file, num_workers)\n        pool = None\n        if num_workers > 1:\n            pool = Pool(processes=num_workers - 1)\n            for worker_id in range(1, num_workers):\n                prefix = ""{}{}"".format(output_prefix, worker_id)\n                pool.apply_async(\n                    binarize_alignments,\n                    (\n                        args,\n                        input_file,\n                        utils.parse_alignment,\n                        prefix,\n                        offsets[worker_id],\n                        offsets[worker_id + 1]\n                    ),\n                    callback=merge_result\n                )\n            pool.close()\n\n        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, None, ""bin""),\n                                          impl=args.dataset_impl)\n\n        merge_result(\n            Binarizer.binarize_alignments(\n                input_file, utils.parse_alignment, lambda t: ds.add_item(t),\n                offset=0, end=offsets[1]\n            )\n        )\n        if num_workers > 1:\n            pool.join()\n            for worker_id in range(1, num_workers):\n                prefix = ""{}{}"".format(output_prefix, worker_id)\n                temp_file_path = dataset_dest_prefix(args, prefix, None)\n                ds.merge_file_(temp_file_path)\n                os.remove(indexed_dataset.data_file_path(temp_file_path))\n                os.remove(indexed_dataset.index_file_path(temp_file_path))\n\n        ds.finalize(dataset_dest_file(args, output_prefix, None, ""idx""))\n\n        logger.info(\n            ""[alignments] {}: parsed {} alignments"".format(\n                input_file,\n                nseq[0]\n            )\n        )\n\n    def make_dataset(vocab, input_prefix, output_prefix, lang, num_workers=1):\n        if args.dataset_impl == ""raw"":\n            # Copy original text file to destination folder\n            output_text_file = dest_path(\n                output_prefix + "".{}-{}"".format(args.source_lang, args.target_lang),\n                lang,\n            )\n            shutil.copyfile(file_name(input_prefix, lang), output_text_file)\n        else:\n            make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers)\n\n    def make_all(lang, vocab):\n        if args.trainpref:\n            make_dataset(vocab, args.trainpref, ""train"", lang, num_workers=args.workers)\n        if args.validpref:\n            for k, validpref in enumerate(args.validpref.split("","")):\n                outprefix = ""valid{}"".format(k) if k > 0 else ""valid""\n                make_dataset(vocab, validpref, outprefix, lang, num_workers=args.workers)\n        if args.testpref:\n            for k, testpref in enumerate(args.testpref.split("","")):\n                outprefix = ""test{}"".format(k) if k > 0 else ""test""\n                make_dataset(vocab, testpref, outprefix, lang, num_workers=args.workers)\n\n    def make_all_alignments():\n        if args.trainpref and os.path.exists(args.trainpref + ""."" + args.align_suffix):\n            make_binary_alignment_dataset(args.trainpref + ""."" + args.align_suffix, ""train.align"", num_workers=args.workers)\n        if args.validpref and os.path.exists(args.validpref + ""."" + args.align_suffix):\n            make_binary_alignment_dataset(args.validpref + ""."" + args.align_suffix, ""valid.align"", num_workers=args.workers)\n        if args.testpref and os.path.exists(args.testpref + ""."" + args.align_suffix):\n            make_binary_alignment_dataset(args.testpref + ""."" + args.align_suffix, ""test.align"", num_workers=args.workers)\n\n    make_all(args.source_lang, src_dict)\n    if target:\n        make_all(args.target_lang, tgt_dict)\n    if args.align_suffix:\n        make_all_alignments()\n\n    logger.info(""Wrote preprocessed data to {}"".format(args.destdir))\n\n    if args.alignfile:\n        assert args.trainpref, ""--trainpref must be set if --alignfile is specified""\n        src_file_name = train_path(args.source_lang)\n        tgt_file_name = train_path(args.target_lang)\n        freq_map = {}\n        with open(args.alignfile, ""r"", encoding=\'utf-8\') as align_file:\n            with open(src_file_name, ""r"", encoding=\'utf-8\') as src_file:\n                with open(tgt_file_name, ""r"", encoding=\'utf-8\') as tgt_file:\n                    for a, s, t in zip_longest(align_file, src_file, tgt_file):\n                        si = src_dict.encode_line(s, add_if_not_exist=False)\n                        ti = tgt_dict.encode_line(t, add_if_not_exist=False)\n                        ai = list(map(lambda x: tuple(x.split(""-"")), a.split()))\n                        for sai, tai in ai:\n                            srcidx = si[int(sai)]\n                            tgtidx = ti[int(tai)]\n                            if srcidx != src_dict.unk() and tgtidx != tgt_dict.unk():\n                                assert srcidx != src_dict.pad()\n                                assert srcidx != src_dict.eos()\n                                assert tgtidx != tgt_dict.pad()\n                                assert tgtidx != tgt_dict.eos()\n\n                                if srcidx not in freq_map:\n                                    freq_map[srcidx] = {}\n                                if tgtidx not in freq_map[srcidx]:\n                                    freq_map[srcidx][tgtidx] = 1\n                                else:\n                                    freq_map[srcidx][tgtidx] += 1\n\n        align_dict = {}\n        for srcidx in freq_map.keys():\n            align_dict[srcidx] = max(freq_map[srcidx], key=freq_map[srcidx].get)\n\n        with open(\n                os.path.join(\n                    args.destdir,\n                    ""alignment.{}-{}.txt"".format(args.source_lang, args.target_lang),\n                ),\n                ""w"", encoding=\'utf-8\'\n        ) as f:\n            for k, v in align_dict.items():\n                print(""{} {}"".format(src_dict[k], tgt_dict[v]), file=f)\n\n\ndef binarize(args, filename, vocab, output_prefix, lang, offset, end, append_eos=True):\n    ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, ""bin""),\n                                      impl=args.dataset_impl, vocab_size=len(vocab))\n\n    def consumer(tensor):\n        ds.add_item(tensor)\n\n    res = Binarizer.binarize(filename, vocab, consumer, append_eos=append_eos,\n                             offset=offset, end=end)\n    ds.finalize(dataset_dest_file(args, output_prefix, lang, ""idx""))\n    return res\n\n\ndef binarize_alignments(args, filename, parse_alignment, output_prefix, offset, end):\n    ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, None, ""bin""),\n                                      impl=args.dataset_impl, vocab_size=None)\n\n    def consumer(tensor):\n        ds.add_item(tensor)\n\n    res = Binarizer.binarize_alignments(filename, parse_alignment, consumer, offset=offset,\n                                        end=end)\n    ds.finalize(dataset_dest_file(args, output_prefix, None, ""idx""))\n    return res\n\n\ndef dataset_dest_prefix(args, output_prefix, lang):\n    base = ""{}/{}"".format(args.destdir, output_prefix)\n    if lang is not None:\n        lang_part = "".{}-{}.{}"".format(args.source_lang, args.target_lang, lang)\n    elif args.only_source:\n        lang_part = """"\n    else:\n        lang_part = "".{}-{}"".format(args.source_lang, args.target_lang)\n\n    return ""{}{}"".format(base, lang_part)\n\n\ndef dataset_dest_file(args, output_prefix, lang, extension):\n    base = dataset_dest_prefix(args, output_prefix, lang)\n    return ""{}.{}"".format(base, extension)\n\n\ndef get_offsets(input_file, num_workers):\n    return Binarizer.find_offsets(input_file, num_workers)\n\n\ndef cli_main():\n    parser = options.get_preprocessing_parser()\n    args = parser.parse_args()\n    main(args)\n\n\nif __name__ == ""__main__"":\n    cli_main()\n'"
fairseq_cli/score.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nBLEU scoring of generated translations against reference translations.\n""""""\n\nimport argparse\nimport os\nimport sys\n\nfrom fairseq import bleu\nfrom fairseq.data import dictionary\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\'Command-line script for BLEU scoring.\')\n    # fmt: off\n    parser.add_argument(\'-s\', \'--sys\', default=\'-\', help=\'system output\')\n    parser.add_argument(\'-r\', \'--ref\', required=True, help=\'references\')\n    parser.add_argument(\'-o\', \'--order\', default=4, metavar=\'N\',\n                        type=int, help=\'consider ngrams up to this order\')\n    parser.add_argument(\'--ignore-case\', action=\'store_true\',\n                        help=\'case-insensitive scoring\')\n    parser.add_argument(\'--sacrebleu\', action=\'store_true\',\n                        help=\'score with sacrebleu\')\n    parser.add_argument(\'--sentence-bleu\', action=\'store_true\',\n                        help=\'report sentence-level BLEUs (i.e., with +1 smoothing)\')\n    # fmt: on\n    return parser\n\n\ndef cli_main():\n    parser = get_parser()\n    args = parser.parse_args()\n    print(args)\n\n    assert args.sys == \'-\' or os.path.exists(args.sys), \\\n        ""System output file {} does not exist"".format(args.sys)\n    assert os.path.exists(args.ref), \\\n        ""Reference file {} does not exist"".format(args.ref)\n\n    dict = dictionary.Dictionary()\n\n    def readlines(fd):\n        for line in fd.readlines():\n            if args.ignore_case:\n                yield line.lower()\n            else:\n                yield line\n\n    if args.sacrebleu:\n        import sacrebleu\n\n        def score(fdsys):\n            with open(args.ref) as fdref:\n                print(sacrebleu.corpus_bleu(fdsys, [fdref]))\n    elif args.sentence_bleu:\n        def score(fdsys):\n            with open(args.ref) as fdref:\n                scorer = bleu.Scorer(dict.pad(), dict.eos(), dict.unk())\n                for i, (sys_tok, ref_tok) in enumerate(zip(readlines(fdsys), readlines(fdref))):\n                    scorer.reset(one_init=True)\n                    sys_tok = dict.encode_line(sys_tok)\n                    ref_tok = dict.encode_line(ref_tok)\n                    scorer.add(ref_tok, sys_tok)\n                    print(i, scorer.result_string(args.order))\n    else:\n        def score(fdsys):\n            with open(args.ref) as fdref:\n                scorer = bleu.Scorer(dict.pad(), dict.eos(), dict.unk())\n                for sys_tok, ref_tok in zip(readlines(fdsys), readlines(fdref)):\n                    sys_tok = dict.encode_line(sys_tok)\n                    ref_tok = dict.encode_line(ref_tok)\n                    scorer.add(ref_tok, sys_tok)\n                print(scorer.result_string(args.order))\n\n    if args.sys == \'-\':\n        score(sys.stdin)\n    else:\n        with open(args.sys, \'r\') as f:\n            score(f)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
fairseq_cli/train.py,9,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nTrain a new model on one or across multiple GPUs.\n""""""\n\nimport logging\nimport math\nimport os\nimport random\nimport sys\n\nimport numpy as np\nimport torch\n\nfrom fairseq import (\n    checkpoint_utils,\n    distributed_utils,\n    options,\n    quantization_utils,\n    tasks,\n    utils,\n)\nfrom fairseq.data import iterators\nfrom fairseq.logging import meters, metrics, progress_bar\nfrom fairseq.trainer import Trainer\nfrom fairseq.model_parallel.megatron_trainer import MegatronTrainer\n\n\nlogging.basicConfig(\n    format=\'%(asctime)s | %(levelname)s | %(name)s | %(message)s\',\n    datefmt=\'%Y-%m-%d %H:%M:%S\',\n    level=logging.INFO,\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\'fairseq_cli.train\')\n\n\ndef main(args, init_distributed=False):\n    utils.import_user_module(args)\n\n    assert args.max_tokens is not None or args.max_sentences is not None, \\\n        \'Must specify batch size either with --max-tokens or --max-sentences\'\n    metrics.reset()\n\n    # Initialize CUDA and distributed training\n    if torch.cuda.is_available() and not args.cpu and not getattr(args, \'tpu\', False):\n        torch.cuda.set_device(args.device_id)\n    np.random.seed(args.seed)\n    utils.set_torch_seed(args.seed)\n    if init_distributed:\n        args.distributed_rank = distributed_utils.distributed_init(args)\n\n    if distributed_utils.is_master(args):\n        checkpoint_utils.verify_checkpoint_directory(args.save_dir)\n\n    # Print args\n    logger.info(args)\n\n    # Setup task, e.g., translation, language modeling, etc.\n    task = tasks.setup_task(args)\n\n    # Load valid dataset (we load training data below, based on the latest checkpoint)\n    for valid_sub_split in args.valid_subset.split(\',\'):\n        task.load_dataset(valid_sub_split, combine=False, epoch=1)\n\n    # Build model and criterion\n    model = task.build_model(args)\n    criterion = task.build_criterion(args)\n    logger.info(model)\n    logger.info(\'model {}, criterion {}\'.format(args.arch, criterion.__class__.__name__))\n    logger.info(\'num. model params: {} (num. trained: {})\'.format(\n        sum(p.numel() for p in model.parameters()),\n        sum(p.numel() for p in model.parameters() if p.requires_grad),\n    ))\n\n    # (optionally) Configure quantization\n    if args.quantization_config_path is not None:\n        quantizer = quantization_utils.Quantizer(\n            config_path=args.quantization_config_path,\n            max_epoch=args.max_epoch,\n            max_update=args.max_update,\n        )\n    else:\n        quantizer = None\n\n    # Build trainer\n    if args.model_parallel_size == 1:\n        trainer = Trainer(args, task, model, criterion, quantizer)\n    else:\n        trainer = MegatronTrainer(args, task, model, criterion)\n\n    logger.info(\'training on {} devices (GPUs/TPUs)\'.format(args.distributed_world_size))\n    logger.info(\'max tokens per GPU = {} and max sentences per GPU = {}\'.format(\n        args.max_tokens,\n        args.max_sentences,\n    ))\n\n    # Load the latest checkpoint if one is available and restore the\n    # corresponding train iterator\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\n    if args.tpu:\n        import torch_xla.core.xla_model as xm\n        xm.rendezvous(\'load_checkpoint\')  # wait for all workers\n        xm.mark_step()\n\n    # Train until the learning rate gets too small\n    max_epoch = args.max_epoch or math.inf\n    lr = trainer.get_lr()\n    train_meter = meters.StopwatchMeter()\n    train_meter.start()\n    while (\n        lr > args.min_lr\n        and epoch_itr.next_epoch_idx <= max_epoch\n    ):\n        # train for one epoch\n        valid_losses, should_stop = train(args, trainer, task, epoch_itr)\n        if should_stop:\n            break\n\n        # only use first validation loss to update the learning rate\n        lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n\n        epoch_itr = trainer.get_train_iterator(\n            epoch_itr.next_epoch_idx,\n            # sharded data: get train iterator for next epoch\n            load_dataset=(os.pathsep in getattr(args, \'data\', \'\')),\n        )\n    train_meter.stop()\n    logger.info(\'done training in {:.1f} seconds\'.format(train_meter.sum))\n\n\ndef should_stop_early(args, valid_loss):\n    # skip check if no validation was done in the current epoch\n    if valid_loss is None:\n        return False\n    if args.patience <= 0:\n        return False\n\n    def is_better(a, b):\n        return a > b if args.maximize_best_checkpoint_metric else a < b\n\n    prev_best = getattr(should_stop_early, \'best\', None)\n    if prev_best is None or is_better(valid_loss, prev_best):\n        should_stop_early.best = valid_loss\n        should_stop_early.num_runs = 0\n        return False\n    else:\n        should_stop_early.num_runs += 1\n        if should_stop_early.num_runs >= args.patience:\n            logger.info(\'early stop since valid performance hasn\\\'t improved for last {} runs\'.format(args.patience))\n            return True\n        else:\n            return False\n\n\ndef tpu_data_loader(args, itr):\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    xm.rendezvous(\'tpu_data_loader\')  # wait for all workers\n    xm.mark_step()\n    device = utils.get_tpu_device(args)\n    return iterators.CountingIterator(\n        pl.ParallelLoader(itr, [device]).per_device_loader(device),\n        start=getattr(itr, \'n\', 0),\n        total=len(itr),\n    )\n\n\n@metrics.aggregate(\'train\')\ndef train(args, trainer, task, epoch_itr):\n    """"""Train the model for one epoch and return validation losses.""""""\n    # Initialize data iterator\n    itr = epoch_itr.next_epoch_itr(\n        fix_batches_to_gpus=args.fix_batches_to_gpus,\n        shuffle=(epoch_itr.next_epoch_idx > args.curriculum),\n    )\n    update_freq = (\n        args.update_freq[epoch_itr.epoch - 1]\n        if epoch_itr.epoch <= len(args.update_freq)\n        else args.update_freq[-1]\n    )\n    itr = iterators.GroupedIterator(itr, update_freq)\n    if getattr(args, \'tpu\', False):\n        itr = tpu_data_loader(args, itr)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=args.log_format,\n        log_interval=args.log_interval,\n        epoch=epoch_itr.epoch,\n        tensorboard_logdir=(\n            args.tensorboard_logdir if distributed_utils.is_master(args) else None\n        ),\n        default_log_format=(\'tqdm\' if not args.no_progress_bar else \'simple\'),\n    )\n\n    trainer.begin_epoch(epoch_itr.epoch)\n\n    valid_subsets = args.valid_subset.split(\',\')\n    should_stop = False\n    for samples in progress:\n        with metrics.aggregate(\'train_inner\'):\n            log_output = trainer.train_step(samples)\n            if log_output is None:  # OOM, overflow, ...\n                continue\n\n        # log mid-epoch stats\n        num_updates = trainer.get_num_updates()\n        if num_updates % args.log_interval == 0:\n            stats = get_training_stats(metrics.get_smoothed_values(\'train_inner\'))\n            progress.log(stats, tag=\'train_inner\', step=num_updates)\n\n            # reset mid-epoch stats after each log interval\n            # the end-of-epoch stats will still be preserved\n            metrics.reset_meters(\'train_inner\')\n\n        end_of_epoch = not itr.has_next()\n        valid_losses, should_stop = validate_and_save(\n            args, trainer, task, epoch_itr, valid_subsets, end_of_epoch\n        )\n        if should_stop:\n            break\n\n    # log end-of-epoch stats\n    stats = get_training_stats(metrics.get_smoothed_values(\'train\'))\n    progress.print(stats, tag=\'train\', step=num_updates)\n\n    # reset epoch-level meters\n    metrics.reset_meters(\'train\')\n    return valid_losses, should_stop\n\n\ndef validate_and_save(args, trainer, task, epoch_itr, valid_subsets, end_of_epoch):\n    num_updates = trainer.get_num_updates()\n    do_save = (\n        (\n            args.save_interval_updates > 0\n            and num_updates > 0\n            and num_updates % args.save_interval_updates == 0\n        )\n        or (end_of_epoch and epoch_itr.epoch % args.save_interval == 0)\n    )\n    do_validate = (\n        (\n            (not end_of_epoch and do_save)  # validate during mid-epoch saves\n            or (end_of_epoch and epoch_itr.epoch % args.validate_interval == 0)\n        )\n        and not args.disable_validation\n    )\n\n    # Validate\n    valid_losses = [None]\n    if do_validate:\n        valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets)\n\n    # Stopping conditions\n    max_update = args.max_update or math.inf\n    should_stop = (\n        should_stop_early(args, valid_losses[0])\n        or trainer.get_num_updates() >= max_update\n    )\n\n    # Save checkpoint\n    if do_save or should_stop:\n        checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])\n\n    return valid_losses, should_stop\n\n\ndef get_training_stats(stats):\n    stats[\'wall\'] = round(metrics.get_meter(\'default\', \'wall\').elapsed_time, 0)\n    return stats\n\n\ndef validate(args, trainer, task, epoch_itr, subsets):\n    """"""Evaluate the model on the validation set(s) and return the losses.""""""\n\n    if args.fixed_validation_seed is not None:\n        # set fixed seed for every validation\n        utils.set_torch_seed(args.fixed_validation_seed)\n\n    valid_losses = []\n    for subset in subsets:\n        # Initialize data iterator\n        itr = trainer.get_valid_iterator(subset).next_epoch_itr(shuffle=False)\n        if getattr(args, \'tpu\', False):\n            itr = tpu_data_loader(args, itr)\n        progress = progress_bar.progress_bar(\n            itr,\n            log_format=args.log_format,\n            log_interval=args.log_interval,\n            epoch=epoch_itr.epoch,\n            prefix=f""valid on \'{subset}\' subset"",\n            tensorboard_logdir=(\n                args.tensorboard_logdir if distributed_utils.is_master(args) else None\n            ),\n            default_log_format=(\'tqdm\' if not args.no_progress_bar else \'simple\'),\n        )\n\n        # create a new root metrics aggregator so validation metrics\n        # don\'t pollute other aggregators (e.g., train meters)\n        with metrics.aggregate(new_root=True) as agg:\n            for sample in progress:\n                trainer.valid_step(sample)\n\n        # log validation stats\n        stats = get_valid_stats(args, trainer, agg.get_smoothed_values())\n        progress.print(stats, tag=subset, step=trainer.get_num_updates())\n\n        valid_losses.append(stats[args.best_checkpoint_metric])\n    return valid_losses\n\n\ndef get_valid_stats(args, trainer, stats):\n    stats[\'num_updates\'] = trainer.get_num_updates()\n    if hasattr(checkpoint_utils.save_checkpoint, \'best\'):\n        key = \'best_{0}\'.format(args.best_checkpoint_metric)\n        best_function = max if args.maximize_best_checkpoint_metric else min\n        stats[key] = best_function(\n            checkpoint_utils.save_checkpoint.best,\n            stats[args.best_checkpoint_metric],\n        )\n    return stats\n\n\ndef distributed_main(i, args, start_rank=0):\n    args.device_id = i\n    if args.distributed_rank is None:  # torch.multiprocessing.spawn\n        args.distributed_rank = start_rank + i\n    main(args, init_distributed=True)\n\n\ndef cli_main(modify_parser=None):\n    parser = options.get_training_parser()\n    args = options.parse_args_and_arch(parser, modify_parser=modify_parser)\n\n    if args.distributed_init_method is None:\n        distributed_utils.infer_init_method(args)\n\n    if args.distributed_init_method is not None:\n        # distributed training\n        if torch.cuda.device_count() > 1 and not args.distributed_no_spawn:\n            start_rank = args.distributed_rank\n            args.distributed_rank = None  # assign automatically\n            torch.multiprocessing.spawn(\n                fn=distributed_main,\n                args=(args, start_rank),\n                nprocs=torch.cuda.device_count(),\n            )\n        else:\n            distributed_main(args.device_id, args)\n    elif args.distributed_world_size > 1:\n        if not getattr(args, \'tpu\', False):\n            # fallback for single node with multiple GPUs\n            assert args.distributed_world_size <= torch.cuda.device_count()\n            port = random.randint(10000, 20000)\n            args.distributed_init_method = \'tcp://localhost:{port}\'.format(port=port)\n            args.distributed_rank = None  # set based on device id\n            torch.multiprocessing.spawn(\n                fn=distributed_main,\n                args=(args, ),\n                nprocs=args.distributed_world_size,\n            )\n        else:\n            import torch_xla.distributed.xla_multiprocessing as xmp\n            torch.multiprocessing.set_sharing_strategy(\'file_system\')\n            xmp.spawn(\n                fn=distributed_main,\n                args=(args, ),\n                nprocs=8,  # use all 8 TPU cores\n            )\n    else:\n        # single GPU training\n        main(args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
fairseq_cli/validate.py,2,"b'#!/usr/bin/env python3 -u\n#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom itertools import chain\nimport logging\nimport sys\n\nimport torch\n\nfrom fairseq import checkpoint_utils, distributed_utils, options, utils\nfrom fairseq.logging import metrics, progress_bar\n\n\nlogging.basicConfig(\n    format=\'%(asctime)s | %(levelname)s | %(name)s | %(message)s\',\n    datefmt=\'%Y-%m-%d %H:%M:%S\',\n    level=logging.INFO,\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\'fairseq_cli.validate\')\n\n\ndef main(args, override_args=None):\n    utils.import_user_module(args)\n\n    assert args.max_tokens is not None or args.max_sentences is not None, \\\n        \'Must specify batch size either with --max-tokens or --max-sentences\'\n\n    use_fp16 = args.fp16\n    use_cuda = torch.cuda.is_available() and not args.cpu\n\n    if use_cuda:\n        torch.cuda.set_device(args.device_id)\n\n    if override_args is not None:\n        overrides = vars(override_args)\n        overrides.update(eval(getattr(override_args, \'model_overrides\', \'{}\')))\n    else:\n        overrides = None\n\n    # Load ensemble\n    logger.info(\'loading model(s) from {}\'.format(args.path))\n    models, model_args, task = checkpoint_utils.load_model_ensemble_and_task(\n        [args.path],\n        arg_overrides=overrides,\n        suffix=getattr(args, ""checkpoint_suffix"", """"),\n    )\n    model = models[0]\n\n    # Move models to GPU\n    for model in models:\n        if use_fp16:\n            model.half()\n        if use_cuda:\n            model.cuda()\n\n    # Print args\n    logger.info(model_args)\n\n    # Build criterion\n    criterion = task.build_criterion(model_args)\n    criterion.eval()\n\n    for subset in args.valid_subset.split(\',\'):\n        try:\n            task.load_dataset(subset, combine=False, epoch=1)\n            dataset = task.dataset(subset)\n        except KeyError:\n            raise Exception(\'Cannot find dataset: \' + subset)\n\n        # Initialize data iterator\n        itr = task.get_batch_iterator(\n            dataset=dataset,\n            max_tokens=args.max_tokens,\n            max_sentences=args.max_sentences,\n            max_positions=utils.resolve_max_positions(\n                task.max_positions(),\n                *[m.max_positions() for m in models],\n            ),\n            ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n            required_batch_size_multiple=args.required_batch_size_multiple,\n            seed=args.seed,\n            num_shards=args.distributed_world_size,\n            shard_id=args.distributed_rank,\n            num_workers=args.num_workers,\n        ).next_epoch_itr(shuffle=False)\n        progress = progress_bar.progress_bar(\n            itr,\n            log_format=args.log_format,\n            log_interval=args.log_interval,\n            prefix=f""valid on \'{subset}\' subset"",\n            default_log_format=(\'tqdm\' if not args.no_progress_bar else \'simple\'),\n        )\n\n        log_outputs = []\n        for i, sample in enumerate(progress):\n            sample = utils.move_to_cuda(sample) if use_cuda else sample\n            _loss, _sample_size, log_output = task.valid_step(sample, model, criterion)\n            progress.log(log_output, step=i)\n            log_outputs.append(log_output)\n\n        if args.distributed_world_size > 1:\n            log_outputs = distributed_utils.all_gather_list(\n                log_outputs,\n                max_size=getattr(args, \'all_gather_list_size\', 16384),\n            )\n            log_outputs = list(chain.from_iterable(log_outputs))\n\n        with metrics.aggregate() as agg:\n            task.reduce_metrics(log_outputs, criterion)\n            log_output = agg.get_smoothed_values()\n\n        progress.print(log_output, tag=subset, step=i)\n\n\ndef cli_main():\n    parser = options.get_validation_parser()\n    args = options.parse_args_and_arch(parser)\n\n    # only override args that are explicitly given on the command line\n    override_parser = options.get_validation_parser()\n    override_args = options.parse_args_and_arch(override_parser, suppress_defaults=True)\n\n    distributed_utils.call_main(args, main, override_args=override_args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
scripts/__init__.py,0,b''
scripts/average_checkpoints.py,4,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport collections\nimport torch\nimport os\nimport re\n\nfrom fairseq.file_io import PathManager\n\n\ndef average_checkpoints(inputs):\n    """"""Loads checkpoints from inputs and returns a model with averaged weights.\n\n    Args:\n      inputs: An iterable of string paths of checkpoints to load from.\n\n    Returns:\n      A dict of string keys mapping to various values. The \'model\' key\n      from the returned dict should correspond to an OrderedDict mapping\n      string parameter names to torch Tensors.\n    """"""\n    params_dict = collections.OrderedDict()\n    params_keys = None\n    new_state = None\n    num_models = len(inputs)\n\n    for fpath in inputs:\n        with PathManager.open(fpath, \'rb\') as f:\n            state = torch.load(\n                f,\n                map_location=(\n                    lambda s, _: torch.serialization.default_restore_location(s, \'cpu\')\n                ),\n            )\n        # Copies over the settings from the first checkpoint\n        if new_state is None:\n            new_state = state\n\n        model_params = state[\'model\']\n\n        model_params_keys = list(model_params.keys())\n        if params_keys is None:\n            params_keys = model_params_keys\n        elif params_keys != model_params_keys:\n            raise KeyError(\n                \'For checkpoint {}, expected list of params: {}, \'\n                \'but found: {}\'.format(f, params_keys, model_params_keys)\n            )\n\n        for k in params_keys:\n            p = model_params[k]\n            if isinstance(p, torch.HalfTensor):\n                p = p.float()\n            if k not in params_dict:\n                params_dict[k] = p.clone()\n                # NOTE: clone() is needed in case of p is a shared parameter\n            else:\n                params_dict[k] += p\n\n    averaged_params = collections.OrderedDict()\n    for k, v in params_dict.items():\n        averaged_params[k] = v\n        if averaged_params[k].is_floating_point():\n            averaged_params[k].div_(num_models)\n        else:\n            averaged_params[k] //= num_models\n    new_state[\'model\'] = averaged_params\n    return new_state\n\n\ndef last_n_checkpoints(paths, n, update_based, upper_bound=None):\n    assert len(paths) == 1\n    path = paths[0]\n    if update_based:\n        pt_regexp = re.compile(r\'checkpoint_\\d+_(\\d+)\\.pt\')\n    else:\n        pt_regexp = re.compile(r\'checkpoint(\\d+)\\.pt\')\n    files = PathManager.ls(path)\n\n    entries = []\n    for f in files:\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            sort_key = int(m.group(1))\n            if upper_bound is None or sort_key <= upper_bound:\n                entries.append((sort_key, m.group(0)))\n    if len(entries) < n:\n        raise Exception(\'Found {} checkpoint files but need at least {}\', len(entries), n)\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)[:n]]\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\'Tool to average the params of input checkpoints to \'\n                    \'produce a new checkpoint\',\n    )\n    # fmt: off\n    parser.add_argument(\'--inputs\', required=True, nargs=\'+\',\n                        help=\'Input checkpoint file paths.\')\n    parser.add_argument(\'--output\', required=True, metavar=\'FILE\',\n                        help=\'Write the new checkpoint containing the averaged weights to this path.\')\n    num_group = parser.add_mutually_exclusive_group()\n    num_group.add_argument(\'--num-epoch-checkpoints\', type=int,\n                           help=\'if set, will try to find checkpoints with names checkpoint_xx.pt in the path specified by input, \'\n                           \'and average last this many of them.\')\n    num_group.add_argument(\'--num-update-checkpoints\', type=int,\n                           help=\'if set, will try to find checkpoints with names checkpoint_ee_xx.pt in the path specified by input, \'\n                           \'and average last this many of them.\')\n    parser.add_argument(\'--checkpoint-upper-bound\', type=int,\n                        help=\'when using --num-epoch-checkpoints, this will set an upper bound on which epoch to use, \'\n                        \'when using --num-update-checkpoints, this will set an upper bound on which update to use\'\n                        \'e.g., with --num-epoch-checkpoints=10 --checkpoint-upper-bound=50, checkpoints 41-50 would be averaged.\'\n                        \'e.g., with --num-update-checkpoints=10 --checkpoint-upper-bound=50000, checkpoints 40500-50000 would be averaged assuming --save-interval-updates 500\'\n                        )\n    # fmt: on\n    args = parser.parse_args()\n    print(args)\n\n    num = None\n    is_update_based = False\n    if args.num_update_checkpoints is not None:\n        num = args.num_update_checkpoints\n        is_update_based = True\n    elif args.num_epoch_checkpoints is not None:\n        num = args.num_epoch_checkpoints\n\n    assert args.checkpoint_upper_bound is None or (args.num_epoch_checkpoints is not None or args.num_update_checkpoints is not None), \\\n        \'--checkpoint-upper-bound requires --num-epoch-checkpoints or --num-update-checkpoints\'\n    assert args.num_epoch_checkpoints is None or args.num_update_checkpoints is None, \\\n        \'Cannot combine --num-epoch-checkpoints and --num-update-checkpoints\'\n\n    if num is not None:\n        args.inputs = last_n_checkpoints(\n            args.inputs, num, is_update_based, upper_bound=args.checkpoint_upper_bound,\n        )\n        print(\'averaging checkpoints: \', args.inputs)\n\n    new_state = average_checkpoints(args.inputs)\n    with PathManager.open(args.output, \'wb\') as f:\n        torch.save(new_state, f)\n    print(\'Finished writing averaged checkpoint to {}.\'.format(args.output))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/build_sym_alignment.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nUse this script in order to build symmetric alignments for your translation\ndataset.\nThis script depends on fast_align and mosesdecoder tools. You will need to\nbuild those before running the script.\nfast_align:\n    github: http://github.com/clab/fast_align\n    instructions: follow the instructions in README.md\nmosesdecoder:\n    github: http://github.com/moses-smt/mosesdecoder\n    instructions: http://www.statmt.org/moses/?n=Development.GetStarted\nThe script produces the following files under --output_dir:\n    text.joined - concatenation of lines from the source_file and the\n    target_file.\n    align.forward - forward pass of fast_align.\n    align.backward - backward pass of fast_align.\n    aligned.sym_heuristic - symmetrized alignment.\n""""""\n\nimport argparse\nimport os\nfrom itertools import zip_longest\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'symmetric alignment builer\')\n    # fmt: off\n    parser.add_argument(\'--fast_align_dir\',\n                        help=\'path to fast_align build directory\')\n    parser.add_argument(\'--mosesdecoder_dir\',\n                        help=\'path to mosesdecoder root directory\')\n    parser.add_argument(\'--sym_heuristic\',\n                        help=\'heuristic to use for symmetrization\',\n                        default=\'grow-diag-final-and\')\n    parser.add_argument(\'--source_file\',\n                        help=\'path to a file with sentences \'\n                             \'in the source language\')\n    parser.add_argument(\'--target_file\',\n                        help=\'path to a file with sentences \'\n                             \'in the target language\')\n    parser.add_argument(\'--output_dir\',\n                        help=\'output directory\')\n    # fmt: on\n    args = parser.parse_args()\n\n    fast_align_bin = os.path.join(args.fast_align_dir, \'fast_align\')\n    symal_bin = os.path.join(args.mosesdecoder_dir, \'bin\', \'symal\')\n    sym_fast_align_bin = os.path.join(\n        args.mosesdecoder_dir, \'scripts\', \'ems\',\n        \'support\', \'symmetrize-fast-align.perl\')\n\n    # create joined file\n    joined_file = os.path.join(args.output_dir, \'text.joined\')\n    with open(args.source_file, \'r\', encoding=\'utf-8\') as src, open(args.target_file, \'r\', encoding=\'utf-8\') as tgt:\n        with open(joined_file, \'w\', encoding=\'utf-8\') as joined:\n            for s, t in zip_longest(src, tgt):\n                print(\'{} ||| {}\'.format(s.strip(), t.strip()), file=joined)\n\n    bwd_align_file = os.path.join(args.output_dir, \'align.backward\')\n\n    # run forward alignment\n    fwd_align_file = os.path.join(args.output_dir, \'align.forward\')\n    fwd_fast_align_cmd = \'{FASTALIGN} -i {JOINED} -d -o -v > {FWD}\'.format(\n        FASTALIGN=fast_align_bin,\n        JOINED=joined_file,\n        FWD=fwd_align_file)\n    assert os.system(fwd_fast_align_cmd) == 0\n\n    # run backward alignment\n    bwd_align_file = os.path.join(args.output_dir, \'align.backward\')\n    bwd_fast_align_cmd = \'{FASTALIGN} -i {JOINED} -d -o -v -r > {BWD}\'.format(\n        FASTALIGN=fast_align_bin,\n        JOINED=joined_file,\n        BWD=bwd_align_file)\n    assert os.system(bwd_fast_align_cmd) == 0\n\n    # run symmetrization\n    sym_out_file = os.path.join(args.output_dir, \'aligned\')\n    sym_cmd = \'{SYMFASTALIGN} {FWD} {BWD} {SRC} {TGT} {OUT} {HEURISTIC} {SYMAL}\'.format(\n        SYMFASTALIGN=sym_fast_align_bin,\n        FWD=fwd_align_file,\n        BWD=bwd_align_file,\n        SRC=args.source_file,\n        TGT=args.target_file,\n        OUT=sym_out_file,\n        HEURISTIC=args.sym_heuristic,\n        SYMAL=symal_bin\n    )\n    assert os.system(sym_cmd) == 0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/compare_namespaces.py,0,"b'#!/usr/bin/env python\n""""""Helper script to compare two argparse.Namespace objects.""""""\n\nfrom argparse import Namespace  # noqa\n\n\ndef main():\n\n    ns1 = eval(input(\'Namespace 1: \'))\n    ns2 = eval(input(\'Namespace 2: \'))\n\n    def keys(ns):\n        ks = set()\n        for k in dir(ns):\n            if not k.startswith(\'_\'):\n                ks.add(k)\n        return ks\n\n    k1 = keys(ns1)\n    k2 = keys(ns2)\n\n    def print_keys(ks, ns1, ns2=None):\n        for k in ks:\n            if ns2 is None:\n                print(\'{}\\t{}\'.format(k, getattr(ns1, k, None)))\n            else:\n                print(\'{}\\t{}\\t{}\'.format(k, getattr(ns1, k, None), getattr(ns2, k, None)))\n\n    print(\'Keys unique to namespace 1:\')\n    print_keys(k1 - k2, ns1)\n    print()\n\n    print(\'Keys unique to namespace 2:\')\n    print_keys(k2 - k1, ns2)\n    print()\n\n    print(\'Overlapping keys with different values:\')\n    ks = [k for k in k1 & k2 if getattr(ns1, k, \'None\') != getattr(ns2, k, \'None\')]\n    print_keys(ks, ns1, ns2)\n    print()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/count_docs.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nCount the number of documents and average number of lines and tokens per\ndocument in a large file. Documents should be separated by a single empty line.\n""""""\n\nimport argparse\nimport gzip\nimport sys\n\nimport numpy as np\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'input\')\n    parser.add_argument(\'--gzip\', action=\'store_true\')\n    args = parser.parse_args()\n\n    def gopen():\n        if args.gzip:\n            return gzip.open(args.input, \'r\')\n        else:\n            return open(args.input, \'r\', encoding=\'utf-8\')\n\n    num_lines = []\n    num_toks = []\n    with gopen() as h:\n        num_docs = 1\n        num_lines_in_doc = 0\n        num_toks_in_doc = 0\n        for i, line in enumerate(h):\n            if len(line.strip()) == 0:  # empty line indicates new document\n                num_docs += 1\n                num_lines.append(num_lines_in_doc)\n                num_toks.append(num_toks_in_doc)\n                num_lines_in_doc = 0\n                num_toks_in_doc = 0\n            else:\n                num_lines_in_doc += 1\n                num_toks_in_doc += len(line.rstrip().split())\n            if i % 1000000 == 0:\n                print(i, file=sys.stderr, end="""", flush=True)\n            elif i % 100000 == 0:\n                print(""."", file=sys.stderr, end="""", flush=True)\n        print(file=sys.stderr, flush=True)\n\n    print(""found {} docs"".format(num_docs))\n    print(""average num lines per doc: {}"".format(np.mean(num_lines)))\n    print(""average num toks per doc: {}"".format(np.mean(num_toks)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/read_binarized.py,0,"b""#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\n\nfrom fairseq.data import data_utils, Dictionary, indexed_dataset\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description='writes text from binarized file to stdout')\n    # fmt: off\n    parser.add_argument('--dataset-impl', help='dataset implementation',\n                        choices=indexed_dataset.get_available_dataset_impl())\n    parser.add_argument('--dict', metavar='FP', help='dictionary containing known words', default=None)\n    parser.add_argument('--input', metavar='FP', required=True, help='binarized file to read')\n    # fmt: on\n\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    dictionary = Dictionary.load(args.dict) if args.dict is not None else None\n    dataset = data_utils.load_indexed_dataset(\n        args.input,\n        dictionary,\n        dataset_impl=args.dataset_impl,\n        default='lazy',\n    )\n\n    for tensor_line in dataset:\n        if dictionary is None:\n            line = ' '.join([str(int(x)) for x in tensor_line])\n        else:\n            line = dictionary.string(tensor_line)\n\n        print(line)\n\n\nif __name__ == '__main__':\n    main()\n"""
scripts/rm_pt.py,0,"b""#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport os\nimport re\nimport shutil\nimport sys\n\n\npt_regexp = re.compile(r'checkpoint(\\d+|_\\d+_\\d+|_[a-z]+)\\.pt')\npt_regexp_epoch_based = re.compile(r'checkpoint(\\d+)\\.pt')\npt_regexp_update_based = re.compile(r'checkpoint_\\d+_(\\d+)\\.pt')\n\n\ndef parse_checkpoints(files):\n    entries = []\n    for f in files:\n        m = pt_regexp_epoch_based.fullmatch(f)\n        if m is not None:\n            entries.append((int(m.group(1)), m.group(0)))\n        else:\n            m = pt_regexp_update_based.fullmatch(f)\n            if m is not None:\n                entries.append((int(m.group(1)), m.group(0)))\n    return entries\n\n\ndef last_n_checkpoints(files, n):\n    entries = parse_checkpoints(files)\n    return [x[1] for x in sorted(entries, reverse=True)[:n]]\n\n\ndef every_n_checkpoints(files, n):\n    entries = parse_checkpoints(files)\n    return [x[1] for x in sorted(sorted(entries)[::-n])]\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=(\n            'Recursively delete checkpoint files from `root_dir`, '\n            'but preserve checkpoint_best.pt and checkpoint_last.pt'\n        )\n    )\n    parser.add_argument('root_dirs', nargs='*')\n    parser.add_argument('--save-last', type=int, default=0, help='number of last checkpoints to save')\n    parser.add_argument('--save-every', type=int, default=0, help='interval of checkpoints to save')\n    parser.add_argument('--preserve-test', action='store_true',\n                        help='preserve checkpoints in dirs that start with test_ prefix (default: delete them)')\n    parser.add_argument('--delete-best', action='store_true', help='delete checkpoint_best.pt')\n    parser.add_argument('--delete-last', action='store_true', help='delete checkpoint_last.pt')\n    parser.add_argument('--no-dereference', action='store_true', help='don\\'t dereference symlinks')\n    args = parser.parse_args()\n\n    files_to_desymlink = []\n    files_to_preserve = []\n    files_to_delete = []\n    for root_dir in args.root_dirs:\n        for root, _subdirs, files in os.walk(root_dir):\n            if args.save_last > 0:\n                to_save = last_n_checkpoints(files, args.save_last)\n            else:\n                to_save = []\n            if args.save_every > 0:\n                to_save += every_n_checkpoints(files, args.save_every)\n            for file in files:\n                if not pt_regexp.fullmatch(file):\n                    continue\n                full_path = os.path.join(root, file)\n                if (\n                    (\n                        not os.path.basename(root).startswith('test_')\n                        or args.preserve_test\n                    )\n                    and (\n                        (file == 'checkpoint_last.pt' and not args.delete_last)\n                        or (file == 'checkpoint_best.pt' and not args.delete_best)\n                        or file in to_save\n                    )\n                ):\n                    if os.path.islink(full_path) and not args.no_dereference:\n                        files_to_desymlink.append(full_path)\n                    else:\n                        files_to_preserve.append(full_path)\n                else:\n                    files_to_delete.append(full_path)\n\n    if len(files_to_desymlink) == 0 and len(files_to_delete) == 0:\n        print('Nothing to do.')\n        sys.exit(0)\n\n    files_to_desymlink = sorted(files_to_desymlink)\n    files_to_preserve = sorted(files_to_preserve)\n    files_to_delete = sorted(files_to_delete)\n\n    print('Operations to perform (in order):')\n    if len(files_to_desymlink) > 0:\n        for file in files_to_desymlink:\n            print(' - preserve (and dereference symlink): ' + file)\n    if len(files_to_preserve) > 0:\n        for file in files_to_preserve:\n            print(' - preserve: ' + file)\n    if len(files_to_delete) > 0:\n        for file in files_to_delete:\n            print(' - delete: ' + file)\n    while True:\n        resp = input('Continue? (Y/N): ')\n        if resp.strip().lower() == 'y':\n            break\n        elif resp.strip().lower() == 'n':\n            sys.exit(0)\n\n    print('Executing...')\n    if len(files_to_desymlink) > 0:\n        for file in files_to_desymlink:\n            realpath = os.path.realpath(file)\n            print('rm ' + file)\n            os.remove(file)\n            print('cp {} {}'.format(realpath, file))\n            shutil.copyfile(realpath, file)\n    if len(files_to_delete) > 0:\n        for file in files_to_delete:\n            print('rm ' + file)\n            os.remove(file)\n\n\nif __name__ == '__main__':\n    main()\n"""
scripts/shard_docs.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nSplit a large file into shards while respecting document boundaries. Documents\nshould be separated by a single empty line.\n""""""\n\nimport argparse\nimport contextlib\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'input\')\n    parser.add_argument(\'--num-shards\', type=int)\n    args = parser.parse_args()\n\n    assert args.num_shards is not None and args.num_shards > 1\n\n    with open(args.input, \'r\', encoding=\'utf-8\') as h:\n        with contextlib.ExitStack() as stack:\n            outputs = [\n                stack.enter_context(open(args.input + "".shard"" + str(i), ""w"", encoding=""utf-8""))\n                for i in range(args.num_shards)\n            ]\n\n            doc = []\n            first_doc = [True]*args.num_shards\n\n            def output_doc(i):\n                if not first_doc[i]:\n                    outputs[i].write(""\\n"")\n                first_doc[i] = False\n                for line in doc:\n                    outputs[i].write(line)\n                doc.clear()\n\n            num_docs = 0\n            for line in h:\n                if line.strip() == """":  # empty line indicates new document\n                    output_doc(num_docs % args.num_shards)\n                    num_docs += 1\n                else:\n                    doc.append(line)\n            output_doc(num_docs % args.num_shards)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/split_train_valid_docs.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nSplit a large file into a train and valid set while respecting document\nboundaries. Documents should be separated by a single empty line.\n""""""\n\nimport argparse\nimport random\nimport sys\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'input\')\n    parser.add_argument(\'sample_output\', help=\'train output file\')\n    parser.add_argument(\'remainder_output\', help=\'valid output file\')\n    parser.add_argument(\'-k\', type=int, help=""remainder size"")\n    parser.add_argument(\'--lines\', action=\'store_true\',\n                        help=\'split lines instead of docs\')\n    args = parser.parse_args()\n\n    assert args.k is not None\n\n    sample = []\n    remainder = []\n    num_docs = [0]\n\n    def update_sample(doc):\n        if len(sample) < args.k:\n            sample.append(doc.copy())\n        else:\n            i = num_docs[0]\n            j = random.randrange(i + 1)\n            if j < args.k:\n                remainder.append(sample[j])\n                sample[j] = doc.copy()\n            else:\n                remainder.append(doc.copy())\n        num_docs[0] += 1\n        doc.clear()\n\n    with open(args.input, \'r\', encoding=\'utf-8\') as h:\n        doc = []\n        for i, line in enumerate(h):\n            if line.strip() == """":  # empty line indicates new document\n                update_sample(doc)\n            else:\n                doc.append(line)\n            if args.lines:\n                update_sample(doc)\n            if i % 1000000 == 0:\n                print(i, file=sys.stderr, end="""", flush=True)\n            elif i % 100000 == 0:\n                print(""."", file=sys.stderr, end="""", flush=True)\n        if len(doc) > 0:\n            update_sample(doc)\n    print(file=sys.stderr, flush=True)\n\n    assert len(sample) == args.k\n\n    with open(args.sample_output, \'w\', encoding=\'utf-8\') as out:\n        first = True\n        for doc in sample:\n            if not first and not args.lines:\n                out.write(""\\n"")\n            first = False\n            for line in doc:\n                out.write(line)\n\n    with open(args.remainder_output, \'w\', encoding=\'utf-8\') as out:\n        first = True\n        for doc in remainder:\n            if not first and not args.lines:\n                out.write(""\\n"")\n            first = False\n            for line in doc:\n                out.write(line)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/spm_decode.py,0,"b'#!/usr/bin/env python\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\n\nimport sentencepiece as spm\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--model"", required=True,\n                        help=""sentencepiece model to use for decoding"")\n    parser.add_argument(""--input"", required=True, help=""input file to decode"")\n    parser.add_argument(""--input_format"", choices=[""piece"", ""id""], default=""piece"")\n    args = parser.parse_args()\n\n    sp = spm.SentencePieceProcessor()\n    sp.Load(args.model)\n\n    if args.input_format == ""piece"":\n        def decode(l):\n            return """".join(sp.DecodePieces(l))\n    elif args.input_format == ""id"":\n        def decode(l):\n            return """".join(sp.DecodeIds(l))\n    else:\n        raise NotImplementedError\n\n    def tok2int(tok):\n        # remap reference-side <unk> (represented as <<unk>>) to 0\n        return int(tok) if tok != ""<<unk>>"" else 0\n\n    with open(args.input, ""r"", encoding=""utf-8"") as h:\n        for line in h:\n            if args.input_format == ""id"":\n                print(decode(list(map(tok2int, line.rstrip().split()))))\n            elif args.input_format == ""piece"":\n                print(decode(line.rstrip().split()))\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/spm_encode.py,0,"b'#!/usr/bin/env python\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport argparse\nimport contextlib\nimport sys\n\nimport sentencepiece as spm\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--model"", required=True,\n                        help=""sentencepiece model to use for encoding"")\n    parser.add_argument(""--inputs"", nargs=""+"", default=[\'-\'],\n                        help=""input files to filter/encode"")\n    parser.add_argument(""--outputs"", nargs=""+"", default=[\'-\'],\n                        help=""path to save encoded outputs"")\n    parser.add_argument(""--output_format"", choices=[""piece"", ""id""], default=""piece"")\n    parser.add_argument(""--min-len"", type=int, metavar=""N"",\n                        help=""filter sentence pairs with fewer than N tokens"")\n    parser.add_argument(""--max-len"", type=int, metavar=""N"",\n                        help=""filter sentence pairs with more than N tokens"")\n    args = parser.parse_args()\n\n    assert len(args.inputs) == len(args.outputs), \\\n        ""number of input and output paths should match""\n\n    sp = spm.SentencePieceProcessor()\n    sp.Load(args.model)\n\n    if args.output_format == ""piece"":\n        def encode(l):\n            return sp.EncodeAsPieces(l)\n    elif args.output_format == ""id"":\n        def encode(l):\n            return list(map(str, sp.EncodeAsIds(l)))\n    else:\n        raise NotImplementedError\n\n    if args.min_len is not None or args.max_len is not None:\n        def valid(line):\n            return (\n                (args.min_len is None or len(line) >= args.min_len)\n                and (args.max_len is None or len(line) <= args.max_len)\n            )\n    else:\n        def valid(lines):\n            return True\n\n    with contextlib.ExitStack() as stack:\n        inputs = [\n            stack.enter_context(open(input, ""r"", encoding=""utf-8"")) \\\n                if input != ""-"" else sys.stdin\n            for input in args.inputs\n        ]\n        outputs = [\n            stack.enter_context(open(output, ""w"", encoding=""utf-8"")) \\\n                if output != ""-"" else sys.stdout\n            for output in args.outputs\n        ]\n\n        stats = {\n            ""num_empty"": 0,\n            ""num_filtered"": 0,\n        }\n\n        def encode_line(line):\n            line = line.strip()\n            if len(line) > 0:\n                line = encode(line)\n                if valid(line):\n                    return line\n                else:\n                    stats[""num_filtered""] += 1\n            else:\n                stats[""num_empty""] += 1\n            return None\n\n        for i, lines in enumerate(zip(*inputs), start=1):\n            enc_lines = list(map(encode_line, lines))\n            if not any(enc_line is None for enc_line in enc_lines):\n                for enc_line, output_h in zip(enc_lines, outputs):\n                    print("" "".join(enc_line), file=output_h)\n            if i % 10000 == 0:\n                print(""processed {} lines"".format(i), file=sys.stderr)\n\n        print(""skipped {} empty lines"".format(stats[""num_empty""]), file=sys.stderr)\n        print(""filtered {} lines"".format(stats[""num_filtered""]), file=sys.stderr)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/spm_train.py,0,"b'#!/usr/bin/env python\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport sys\n\nimport sentencepiece as spm\n\n\nif __name__ == ""__main__"":\n    spm.SentencePieceTrainer.Train("" "".join(sys.argv[1:]))\n'"
tests/__init__.py,0,b''
tests/test_average_checkpoints.py,15,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport os\nimport tempfile\nimport unittest\nimport shutil\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\n\nfrom scripts.average_checkpoints import average_checkpoints\n\n\nclass ModelWithSharedParameter(nn.Module):\n    def __init__(self):\n        super(ModelWithSharedParameter, self).__init__()\n        self.embedding = nn.Embedding(1000, 200)\n        self.FC1 = nn.Linear(200, 200)\n        self.FC2 = nn.Linear(200, 200)\n        # tie weight in FC2 to FC1\n        self.FC2.weight = nn.Parameter(self.FC1.weight)\n        self.FC2.bias = nn.Parameter(self.FC1.bias)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, input):\n        return self.FC2(self.ReLU(self.FC1(input))) + self.FC1(input)\n\n\nclass TestAverageCheckpoints(unittest.TestCase):\n    def test_average_checkpoints(self):\n        params_0 = collections.OrderedDict(\n            [\n                (\'a\', torch.DoubleTensor([100.0])),\n                (\'b\', torch.FloatTensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])),\n                (\'c\', torch.IntTensor([7, 8, 9])),\n            ]\n        )\n        params_1 = collections.OrderedDict(\n            [\n                (\'a\', torch.DoubleTensor([1.0])),\n                (\'b\', torch.FloatTensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]])),\n                (\'c\', torch.IntTensor([2, 2, 2])),\n            ]\n        )\n        params_avg = collections.OrderedDict(\n            [\n                (\'a\', torch.DoubleTensor([50.5])),\n                (\'b\', torch.FloatTensor([[1.0, 1.5, 2.0], [2.5, 3.0, 3.5]])),\n                # We expect truncation for integer division\n                (\'c\', torch.IntTensor([4, 5, 5])),\n            ]\n        )\n\n        fd_0, path_0 = tempfile.mkstemp()\n        fd_1, path_1 = tempfile.mkstemp()\n        torch.save(collections.OrderedDict([(\'model\', params_0)]), path_0)\n        torch.save(collections.OrderedDict([(\'model\', params_1)]), path_1)\n\n        output = average_checkpoints([path_0, path_1])[\'model\']\n\n        os.close(fd_0)\n        os.remove(path_0)\n        os.close(fd_1)\n        os.remove(path_1)\n\n        for (k_expected, v_expected), (k_out, v_out) in zip(\n                params_avg.items(), output.items()):\n            self.assertEqual(\n                k_expected, k_out, \'Key mismatch - expected {} but found {}. \'\n                \'(Expected list of keys: {} vs actual list of keys: {})\'.format(\n                    k_expected, k_out, params_avg.keys(), output.keys()\n                )\n            )\n            np.testing.assert_allclose(\n                v_expected.numpy(),\n                v_out.numpy(),\n                err_msg=\'Tensor value mismatch for key {}\'.format(k_expected)\n            )\n\n    def test_average_checkpoints_with_shared_parameters(self):\n\n        def _construct_model_with_shared_parameters(path, value):\n            m = ModelWithSharedParameter()\n            nn.init.constant_(m.FC1.weight, value)\n            torch.save(\n                {\'model\': m.state_dict()},\n                path\n            )\n            return m\n\n        tmpdir = tempfile.mkdtemp()\n        paths = []\n        path = os.path.join(tmpdir, ""m1.pt"")\n        m1 = _construct_model_with_shared_parameters(path, 1.0)\n        paths.append(path)\n\n        path = os.path.join(tmpdir, ""m2.pt"")\n        m2 = _construct_model_with_shared_parameters(path, 2.0)\n        paths.append(path)\n\n        path = os.path.join(tmpdir, ""m3.pt"")\n        m3 = _construct_model_with_shared_parameters(path, 3.0)\n        paths.append(path)\n\n        new_model = average_checkpoints(paths)\n        self.assertTrue(\n            torch.equal(\n                new_model[\'model\'][\'embedding.weight\'],\n                (m1.embedding.weight +\n                 m2.embedding.weight +\n                 m3.embedding.weight) / 3.0\n            )\n        )\n\n        self.assertTrue(\n            torch.equal(\n                new_model[\'model\'][\'FC1.weight\'],\n                (m1.FC1.weight +\n                 m2.FC1.weight +\n                 m3.FC1.weight) / 3.0\n            )\n        )\n\n        self.assertTrue(\n            torch.equal(\n                new_model[\'model\'][\'FC2.weight\'],\n                (m1.FC2.weight +\n                 m2.FC2.weight +\n                 m3.FC2.weight) / 3.0\n            )\n        )\n        shutil.rmtree(tmpdir)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_backtranslation_dataset.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\n\nfrom fairseq.data import (\n    BacktranslationDataset,\n    LanguagePairDataset,\n    TransformEosDataset,\n)\nfrom fairseq.sequence_generator import SequenceGenerator\n\nimport tests.utils as test_utils\n\n\nclass TestBacktranslationDataset(unittest.TestCase):\n\n    def setUp(self):\n        self.tgt_dict, self.w1, self.w2, self.src_tokens, self.src_lengths, self.model = (\n            test_utils.sequence_generator_setup()\n        )\n\n        dummy_src_samples = self.src_tokens\n\n        self.tgt_dataset = test_utils.TestDataset(data=dummy_src_samples)\n        self.cuda = torch.cuda.is_available()\n\n    def _backtranslation_dataset_helper(\n        self, remove_eos_from_input_src, remove_eos_from_output_src,\n    ):\n        tgt_dataset = LanguagePairDataset(\n            src=self.tgt_dataset,\n            src_sizes=self.tgt_dataset.sizes,\n            src_dict=self.tgt_dict,\n            tgt=None,\n            tgt_sizes=None,\n            tgt_dict=None,\n        )\n\n        generator = SequenceGenerator(\n            [self.model],\n            tgt_dict=self.tgt_dict,\n            max_len_a=0,\n            max_len_b=200,\n            beam_size=2,\n            unk_penalty=0,\n        )\n\n        backtranslation_dataset = BacktranslationDataset(\n            tgt_dataset=TransformEosDataset(\n                dataset=tgt_dataset,\n                eos=self.tgt_dict.eos(),\n                # remove eos from the input src\n                remove_eos_from_src=remove_eos_from_input_src,\n            ),\n            src_dict=self.tgt_dict,\n            backtranslation_fn=(\n                lambda sample: generator.generate([self.model], sample)\n            ),\n            output_collater=TransformEosDataset(\n                dataset=tgt_dataset,\n                eos=self.tgt_dict.eos(),\n                # if we remove eos from the input src, then we need to add it\n                # back to the output tgt\n                append_eos_to_tgt=remove_eos_from_input_src,\n                remove_eos_from_src=remove_eos_from_output_src,\n            ).collater,\n            cuda=self.cuda,\n        )\n        dataloader = torch.utils.data.DataLoader(\n            backtranslation_dataset,\n            batch_size=2,\n            collate_fn=backtranslation_dataset.collater,\n        )\n        backtranslation_batch_result = next(iter(dataloader))\n\n        eos, pad, w1, w2 = self.tgt_dict.eos(), self.tgt_dict.pad(), self.w1, self.w2\n\n        # Note that we sort by src_lengths and add left padding, so actually\n        # ids will look like: [1, 0]\n        expected_src = torch.LongTensor([[w1, w2, w1, eos], [pad, pad, w1, eos]])\n        if remove_eos_from_output_src:\n            expected_src = expected_src[:, :-1]\n        expected_tgt = torch.LongTensor([[w1, w2, eos], [w1, w2, eos]])\n        generated_src = backtranslation_batch_result[""net_input""][""src_tokens""]\n        tgt_tokens = backtranslation_batch_result[""target""]\n\n        self.assertTensorEqual(expected_src, generated_src)\n        self.assertTensorEqual(expected_tgt, tgt_tokens)\n\n    def test_backtranslation_dataset_no_eos_in_output_src(self):\n        self._backtranslation_dataset_helper(\n            remove_eos_from_input_src=False, remove_eos_from_output_src=True,\n        )\n\n    def test_backtranslation_dataset_with_eos_in_output_src(self):\n        self._backtranslation_dataset_helper(\n            remove_eos_from_input_src=False, remove_eos_from_output_src=False,\n        )\n\n    def test_backtranslation_dataset_no_eos_in_input_src(self):\n        self._backtranslation_dataset_helper(\n            remove_eos_from_input_src=True, remove_eos_from_output_src=False,\n        )\n\n    def assertTensorEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_binaries.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nfrom io import StringIO\nimport logging\nimport os\nimport random\nimport tempfile\nimport unittest\n\nimport torch\n\nfrom fairseq import options\nfrom fairseq_cli import train\nfrom fairseq_cli import eval_lm\nfrom fairseq_cli import validate\nfrom tests.utils import (\n    create_dummy_data,\n    preprocess_lm_data,\n    preprocess_translation_data,\n    train_translation_model,\n    generate_main,\n)\n\n\nclass TestTranslation(unittest.TestCase):\n\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def test_fconv(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_fconv\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'fconv_iwslt_de_en\')\n                generate_main(data_dir)\n\n    def test_raw(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_fconv_raw\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir, [\'--dataset-impl\', \'raw\'])\n                train_translation_model(data_dir, \'fconv_iwslt_de_en\', [\'--dataset-impl\', \'raw\'])\n                generate_main(data_dir, [\'--dataset-impl\', \'raw\'])\n\n    def test_update_freq(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_update_freq\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'fconv_iwslt_de_en\', [\'--update-freq\', \'3\'])\n                generate_main(data_dir)\n\n    def test_max_positions(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_max_positions\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                with self.assertRaises(Exception) as context:\n                    train_translation_model(\n                        data_dir, \'fconv_iwslt_de_en\', [\'--max-target-positions\', \'5\'],\n                    )\n                self.assertTrue(\n                    \'skip this example with --skip-invalid-size-inputs-valid-test\' in str(context.exception)\n                )\n                train_translation_model(\n                    data_dir, \'fconv_iwslt_de_en\',\n                    [\'--max-target-positions\', \'5\', \'--skip-invalid-size-inputs-valid-test\'],\n                )\n                with self.assertRaises(Exception) as context:\n                    generate_main(data_dir)\n                generate_main(data_dir, [\'--skip-invalid-size-inputs-valid-test\'])\n\n    def test_generation(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_sampling\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'fconv_iwslt_de_en\')\n                generate_main(data_dir, [\n                    \'--sampling\',\n                    \'--temperature\', \'2\',\n                    \'--beam\', \'2\',\n                    \'--nbest\', \'2\',\n                ])\n                generate_main(data_dir, [\n                    \'--sampling\',\n                    \'--sampling-topk\', \'3\',\n                    \'--beam\', \'2\',\n                    \'--nbest\', \'2\',\n                ])\n                generate_main(data_dir, [\n                    \'--sampling\',\n                    \'--sampling-topp\', \'0.2\',\n                    \'--beam\', \'2\',\n                    \'--nbest\', \'2\',\n                ])\n                generate_main(data_dir, [\n                    \'--diversity-rate\', \'0.5\',\n                    \'--beam\', \'6\',\n                ])\n                with self.assertRaises(ValueError):\n                    generate_main(data_dir, [\n                        \'--diverse-beam-groups\', \'4\',\n                        \'--match-source-len\',\n                    ])\n                generate_main(data_dir, [\'--prefix-size\', \'2\'])\n\n    def test_eval_bleu(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_eval_bleu\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'fconv_iwslt_de_en\', [\n                    \'--eval-bleu\',\n                    \'--eval-bleu-print-samples\',\n                    \'--eval-bleu-remove-bpe\',\n                    \'--eval-bleu-detok\', \'space\',\n                    \'--eval-bleu-args\', \'{""beam"": 4, ""min_len"": 10}\',\n                ])\n\n    def test_lstm(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_lstm\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'lstm_wiseman_iwslt_de_en\', [\n                    \'--encoder-layers\', \'2\',\n                    \'--decoder-layers\', \'2\',\n                    \'--encoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                    \'--decoder-out-embed-dim\', \'8\',\n                ])\n                generate_main(data_dir)\n\n    def test_lstm_bidirectional(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_lstm_bidirectional\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'lstm\', [\n                    \'--encoder-layers\', \'2\',\n                    \'--encoder-bidirectional\',\n                    \'--encoder-hidden-size\', \'16\',\n                    \'--encoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                    \'--decoder-out-embed-dim\', \'8\',\n                    \'--decoder-layers\', \'2\',\n                ])\n                generate_main(data_dir)\n\n    def test_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_transformer\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'transformer_iwslt_de_en\', [\n                    \'--encoder-layers\', \'2\',\n                    \'--decoder-layers\', \'2\',\n                    \'--encoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                ], run_validation=True)\n                generate_main(data_dir)\n\n    def test_multilingual_transformer(self):\n        # test with all combinations of encoder/decoder lang tokens\n        encoder_langtok_flags = [[], [\'--encoder-langtok\', \'src\'], [\'--encoder-langtok\', \'tgt\']]\n        decoder_langtok_flags = [[], [\'--decoder-langtok\']]\n        with contextlib.redirect_stdout(StringIO()):\n            for i in range(len(encoder_langtok_flags)):\n                for j in range(len(decoder_langtok_flags)):\n                    enc_ltok_flag = encoder_langtok_flags[i]\n                    dec_ltok_flag = decoder_langtok_flags[j]\n                    with tempfile.TemporaryDirectory(f\'test_multilingual_transformer_{i}_{j}\') as data_dir:\n                        create_dummy_data(data_dir)\n                        preprocess_translation_data(data_dir)\n                        train_translation_model(\n                            data_dir,\n                            arch=\'multilingual_transformer\',\n                            task=\'multilingual_translation\',\n                            extra_flags=[\n                                \'--encoder-layers\', \'2\',\n                                \'--decoder-layers\', \'2\',\n                                \'--encoder-embed-dim\', \'8\',\n                                \'--decoder-embed-dim\', \'8\',\n                            ] + enc_ltok_flag + dec_ltok_flag,\n                            lang_flags=[\'--lang-pairs\', \'in-out,out-in\'],\n                            run_validation=True,\n                            extra_valid_flags=enc_ltok_flag + dec_ltok_flag,\n                        )\n                        generate_main(\n                            data_dir,\n                            extra_flags=[\n                                \'--task\', \'multilingual_translation\',\n                                \'--lang-pairs\', \'in-out,out-in\',\n                                \'--source-lang\', \'in\',\n                                \'--target-lang\', \'out\',\n                            ] + enc_ltok_flag + dec_ltok_flag,\n                        )\n\n    def test_transformer_cross_self_attention(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_transformer_cross_self_attention\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'transformer_iwslt_de_en\', [\n                    \'--encoder-layers\', \'2\',\n                    \'--decoder-layers\', \'2\',\n                    \'--encoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                    \'--no-cross-attention\',\n                    \'--cross-self-attention\',\n                ], run_validation=True)\n                generate_main(data_dir, extra_flags=[])\n\n    def test_lightconv(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_lightconv\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'lightconv_iwslt_de_en\', [\n                    \'--encoder-conv-type\', \'lightweight\',\n                    \'--decoder-conv-type\', \'lightweight\',\n                    \'--encoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                ])\n                generate_main(data_dir)\n\n    def test_dynamicconv(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_dynamicconv\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'lightconv_iwslt_de_en\', [\n                    \'--encoder-conv-type\', \'dynamic\',\n                    \'--decoder-conv-type\', \'dynamic\',\n                    \'--encoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                ])\n                generate_main(data_dir)\n\n    def test_cmlm_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_cmlm_transformer\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir, [\'--joined-dictionary\'])\n                train_translation_model(data_dir, \'cmlm_transformer\', [\n                    \'--apply-bert-init\',\n                    \'--criterion\', \'nat_loss\',\n                    \'--noise\', \'full_mask\',\n                    \'--pred-length-offset\',\n                    \'--length-loss-factor\', \'0.1\'\n                ], task=\'translation_lev\')\n                generate_main(data_dir, [\n                    \'--task\', \'translation_lev\',\n                    \'--iter-decode-max-iter\', \'9\',\n                    \'--iter-decode-eos-penalty\', \'0\',\n                    \'--print-step\',\n                ])\n\n    def test_nonautoregressive_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_nonautoregressive_transformer\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir, [\'--joined-dictionary\'])\n                train_translation_model(data_dir, \'nonautoregressive_transformer\', [\n                    \'--apply-bert-init\', \'--src-embedding-copy\', \'--criterion\',\n                    \'nat_loss\', \'--noise\', \'full_mask\', \'--pred-length-offset\',\n                    \'--length-loss-factor\', \'0.1\'\n                ], task=\'translation_lev\')\n                generate_main(data_dir, [\n                    \'--task\', \'translation_lev\',\n                    \'--iter-decode-max-iter\', \'0\',\n                    \'--iter-decode-eos-penalty\', \'0\',\n                    \'--print-step\',\n                ])\n\n    # def test_nat_crf_transformer(self):\n    #     with contextlib.redirect_stdout(StringIO()):\n    #         with tempfile.TemporaryDirectory(\'test_nat_crf_transformer\') as data_dir:\n    #             create_dummy_data(data_dir)\n    #             preprocess_translation_data(data_dir, [\'--joined-dictionary\'])\n    #             train_translation_model(data_dir, \'nacrf_transformer\', [\n    #                 \'--apply-bert-init\', \'--criterion\',\n    #                 \'nat_loss\', \'--noise\', \'full_mask\', \'--pred-length-offset\',\n    #                 \'--length-loss-factor\', \'0.1\',\n    #                 \'--word-ins-loss-factor\', \'0.5\',\n    #                 \'--crf-lowrank-approx\', \'1\',\n    #                 \'--crf-beam-approx\', \'1\'\n    #             ], task=\'translation_lev\')\n    #             generate_main(data_dir, [\n    #                 \'--task\', \'translation_lev\',\n    #                 \'--iter-decode-max-iter\', \'0\',\n    #                 \'--iter-decode-eos-penalty\', \'0\',\n    #                 \'--print-step\',\n    #             ])\n\n    def test_iterative_nonautoregressive_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_iterative_nonautoregressive_transformer\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir, [\'--joined-dictionary\'])\n                train_translation_model(data_dir, \'iterative_nonautoregressive_transformer\', [\n                    \'--apply-bert-init\', \'--src-embedding-copy\', \'--criterion\',\n                    \'nat_loss\', \'--noise\', \'full_mask\', \'--stochastic-approx\',\n                    \'--dae-ratio\', \'0.5\', \'--train-step\', \'3\'\n                ], task=\'translation_lev\')\n                generate_main(data_dir, [\n                    \'--task\', \'translation_lev\',\n                    \'--iter-decode-max-iter\', \'9\',\n                    \'--iter-decode-eos-penalty\', \'0\',\n                    \'--print-step\',\n                ])\n\n    def test_insertion_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_insertion_transformer\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir, [\'--joined-dictionary\'])\n                train_translation_model(data_dir, \'insertion_transformer\', [\n                    \'--apply-bert-init\', \'--criterion\', \'nat_loss\', \'--noise\',\n                    \'random_mask\'\n                ], task=\'translation_lev\')\n                generate_main(data_dir, [\n                    \'--task\', \'translation_lev\',\n                    \'--iter-decode-max-iter\', \'9\',\n                    \'--iter-decode-eos-penalty\', \'0\',\n                    \'--print-step\',\n                ])\n\n    def test_mixture_of_experts(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_moe\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, \'transformer_iwslt_de_en\', [\n                    \'--task\', \'translation_moe\',\n                    \'--user-dir\', \'examples/translation_moe/src\',\n                    \'--method\', \'hMoElp\',\n                    \'--mean-pool-gating-network\',\n                    \'--num-experts\', \'3\',\n                    \'--encoder-layers\', \'2\',\n                    \'--decoder-layers\', \'2\',\n                    \'--encoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                ])\n                generate_main(data_dir, [\n                    \'--task\', \'translation_moe\',\n                    \'--user-dir\', \'examples/translation_moe/src\',\n                    \'--method\', \'hMoElp\',\n                    \'--mean-pool-gating-network\',\n                    \'--num-experts\', \'3\',\n                    \'--gen-expert\', \'0\'\n                ])\n\n    def test_alignment(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_alignment\') as data_dir:\n                create_dummy_data(data_dir, alignment=True)\n                preprocess_translation_data(data_dir, [\'--align-suffix\', \'align\'])\n                train_translation_model(\n                    data_dir,\n                    \'transformer_align\',\n                    [\n                        \'--encoder-layers\', \'2\',\n                        \'--decoder-layers\', \'2\',\n                        \'--encoder-embed-dim\', \'8\',\n                        \'--decoder-embed-dim\', \'8\',\n                        \'--load-alignments\',\n                        \'--alignment-layer\', \'1\',\n                        \'--criterion\', \'label_smoothed_cross_entropy_with_alignment\'\n                    ],\n                    run_validation=True,\n                )\n                generate_main(data_dir)\n\n\nclass TestStories(unittest.TestCase):\n\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def test_fconv_self_att_wp(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_fconv_self_att_wp\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                config = [\n                    \'--encoder-layers\', \'[(128, 3)] * 2\',\n                    \'--decoder-layers\', \'[(128, 3)] * 2\',\n                    \'--decoder-attention\', \'True\',\n                    \'--encoder-attention\', \'False\',\n                    \'--gated-attention\', \'True\',\n                    \'--self-attention\', \'True\',\n                    \'--project-input\', \'True\',\n                    \'--encoder-embed-dim\', \'8\',\n                    \'--decoder-embed-dim\', \'8\',\n                    \'--decoder-out-embed-dim\', \'8\',\n                    \'--multihead-self-attention-nheads\', \'2\'\n                ]\n                train_translation_model(data_dir, \'fconv_self_att_wp\', config)\n                generate_main(data_dir)\n\n                # fusion model\n                os.rename(os.path.join(data_dir, \'checkpoint_last.pt\'), os.path.join(data_dir, \'pretrained.pt\'))\n                config.extend([\n                    \'--pretrained\', \'True\',\n                    \'--pretrained-checkpoint\', os.path.join(data_dir, \'pretrained.pt\'),\n                    \'--save-dir\', os.path.join(data_dir, \'fusion_model\'),\n                ])\n                train_translation_model(data_dir, \'fconv_self_att_wp\', config)\n\n\nclass TestLanguageModeling(unittest.TestCase):\n\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def test_fconv_lm(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_fconv_lm\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                train_language_model(data_dir, \'fconv_lm\', [\n                    \'--decoder-layers\', \'[(850, 3)] * 2 + [(1024,4)]\',\n                    \'--decoder-embed-dim\', \'280\',\n                    \'--optimizer\', \'nag\',\n                    \'--lr\', \'0.1\',\n                ])\n                eval_lm_main(data_dir)\n                generate_main(data_dir, [\n                    \'--task\', \'language_modeling\',\n                    \'--sample-break-mode\', \'eos\',\n                    \'--tokens-per-sample\', \'500\',\n                ])\n\n    def test_transformer_lm(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_transformer_lm\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                train_language_model(\n                    data_dir, \'transformer_lm\', [\'--add-bos-token\'], run_validation=True,\n                )\n                eval_lm_main(data_dir)\n                generate_main(data_dir, [\n                    \'--task\', \'language_modeling\',\n                    \'--sample-break-mode\', \'eos\',\n                    \'--tokens-per-sample\', \'500\',\n                ])\n\n    def test_lightconv_lm(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_lightconv_lm\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                train_language_model(\n                    data_dir, \'lightconv_lm\', [\'--add-bos-token\'], run_validation=True,\n                )\n                eval_lm_main(data_dir)\n                generate_main(data_dir, [\n                    \'--task\', \'language_modeling\',\n                    \'--sample-break-mode\', \'eos\',\n                    \'--tokens-per-sample\', \'500\',\n                ])\n\n    def test_lstm_lm(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_lstm_lm\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                train_language_model(\n                    data_dir, \'lstm_lm\', [\'--add-bos-token\'], run_validation=True,\n                )\n                eval_lm_main(data_dir)\n                generate_main(data_dir, [\n                    \'--task\', \'language_modeling\',\n                    \'--sample-break-mode\', \'eos\',\n                    \'--tokens-per-sample\', \'500\',\n                ])\n\n    def test_lstm_lm_residuals(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_lstm_lm_residuals\') as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                train_language_model(\n                    data_dir, \'lstm_lm\', [\'--add-bos-token\', \'--residuals\'], run_validation=True,\n                )\n                eval_lm_main(data_dir)\n                generate_main(data_dir, [\n                    \'--task\', \'language_modeling\',\n                    \'--sample-break-mode\', \'eos\',\n                    \'--tokens-per-sample\', \'500\',\n                ])\n\nclass TestMaskedLanguageModel(unittest.TestCase):\n\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def test_legacy_masked_lm(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_legacy_mlm"") as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                train_legacy_masked_language_model(data_dir, ""masked_lm"")\n\n    def test_roberta_masked_lm(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_roberta_mlm"") as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                train_masked_lm(data_dir, ""roberta_base"")\n\n    def test_roberta_sentence_prediction(self):\n        num_classes = 3\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_roberta_head"") as data_dir:\n                create_dummy_roberta_head_data(data_dir, num_classes=num_classes)\n                preprocess_lm_data(os.path.join(data_dir, \'input0\'))\n                preprocess_lm_data(os.path.join(data_dir, \'label\'))\n                train_roberta_head(data_dir, ""roberta_base"", num_classes=num_classes)\n\n    def test_roberta_regression_single(self):\n        num_classes = 1\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_roberta_regression_single"") as data_dir:\n                create_dummy_roberta_head_data(data_dir, num_classes=num_classes, regression=True)\n                preprocess_lm_data(os.path.join(data_dir, \'input0\'))\n                train_roberta_head(data_dir, ""roberta_base"", num_classes=num_classes, extra_flags=[\'--regression-target\'])\n\n    def test_roberta_regression_multiple(self):\n        num_classes = 3\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_roberta_regression_multiple"") as data_dir:\n                create_dummy_roberta_head_data(data_dir, num_classes=num_classes, regression=True)\n                preprocess_lm_data(os.path.join(data_dir, \'input0\'))\n                train_roberta_head(data_dir, ""roberta_base"", num_classes=num_classes, extra_flags=[\'--regression-target\'])\n\n    def _test_pretrained_masked_lm_for_translation(self, learned_pos_emb, encoder_only):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_mlm"") as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                train_legacy_masked_language_model(\n                    data_dir,\n                    arch=""masked_lm"",\n                    extra_args=(\'--encoder-learned-pos\',) if learned_pos_emb else ()\n                )\n                with tempfile.TemporaryDirectory(\n                    ""test_mlm_translation""\n                ) as translation_dir:\n                    create_dummy_data(translation_dir)\n                    preprocess_translation_data(\n                        translation_dir, extra_flags=[""--joined-dictionary""]\n                    )\n                    # Train transformer with data_dir/checkpoint_last.pt\n                    train_translation_model(\n                        translation_dir,\n                        arch=""transformer_from_pretrained_xlm"",\n                        extra_flags=[\n                            ""--decoder-layers"",\n                            ""1"",\n                            ""--decoder-embed-dim"",\n                            ""32"",\n                            ""--decoder-attention-heads"",\n                            ""1"",\n                            ""--decoder-ffn-embed-dim"",\n                            ""32"",\n                            ""--encoder-layers"",\n                            ""1"",\n                            ""--encoder-embed-dim"",\n                            ""32"",\n                            ""--encoder-attention-heads"",\n                            ""1"",\n                            ""--encoder-ffn-embed-dim"",\n                            ""32"",\n                            ""--pretrained-xlm-checkpoint"",\n                            ""{}/checkpoint_last.pt"".format(data_dir),\n                            ""--activation-fn"",\n                            ""gelu"",\n                            ""--max-source-positions"",\n                            ""500"",\n                            ""--max-target-positions"",\n                            ""500"",\n                        ] + (\n                            [""--encoder-learned-pos"", ""--decoder-learned-pos""]\n                            if learned_pos_emb else []\n                        ) + ([\'--init-encoder-only\'] if encoder_only else []),\n                        task=""translation_from_pretrained_xlm"",\n                    )\n\n    def test_pretrained_masked_lm_for_translation_learned_pos_emb(self):\n        self._test_pretrained_masked_lm_for_translation(True, False)\n\n    def test_pretrained_masked_lm_for_translation_sinusoidal_pos_emb(self):\n        self._test_pretrained_masked_lm_for_translation(False, False)\n\n    def test_pretrained_masked_lm_for_translation_encoder_only(self):\n        self._test_pretrained_masked_lm_for_translation(True, True)\n\n\ndef train_legacy_masked_language_model(data_dir, arch, extra_args=()):\n    train_parser = options.get_training_parser()\n    # TODO: langs should be in and out right?\n    train_args = options.parse_args_and_arch(\n        train_parser,\n        [\n            ""--task"",\n            ""cross_lingual_lm"",\n            data_dir,\n            ""--arch"",\n            arch,\n            # Optimizer args\n            ""--optimizer"",\n            ""adam"",\n            ""--lr-scheduler"",\n            ""reduce_lr_on_plateau"",\n            ""--lr-shrink"",\n            ""0.5"",\n            ""--lr"",\n            ""0.0001"",\n            ""--min-lr"",\n            ""1e-09"",\n            # dropout, attention args\n            ""--dropout"",\n            ""0.1"",\n            ""--attention-dropout"",\n            ""0.1"",\n            # MLM args\n            ""--criterion"",\n            ""legacy_masked_lm_loss"",\n            ""--masked-lm-only"",\n            ""--monolingual-langs"",\n            ""in,out"",\n            ""--num-segment"",\n            ""5"",\n            # Transformer args: use a small transformer model for fast training\n            ""--encoder-layers"",\n            ""1"",\n            ""--encoder-embed-dim"",\n            ""32"",\n            ""--encoder-attention-heads"",\n            ""1"",\n            ""--encoder-ffn-embed-dim"",\n            ""32"",\n            # Other training args\n            ""--max-tokens"",\n            ""500"",\n            ""--tokens-per-sample"",\n            ""500"",\n            ""--save-dir"",\n            data_dir,\n            ""--max-epoch"",\n            ""1"",\n            ""--no-progress-bar"",\n            ""--distributed-world-size"",\n            ""1"",\n            ""--dataset-impl"",\n            ""raw"",\n        ] + list(extra_args),\n    )\n    train.main(train_args)\n\n\nclass TestOptimizers(unittest.TestCase):\n\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def test_optimizers(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\'test_optimizers\') as data_dir:\n                # Use just a bit of data and tiny model to keep this test runtime reasonable\n                create_dummy_data(data_dir, num_examples=10, maxlen=5)\n                preprocess_translation_data(data_dir)\n                optimizers = [\'adafactor\', \'adam\', \'nag\', \'adagrad\', \'sgd\', \'adadelta\']\n                last_checkpoint = os.path.join(data_dir, \'checkpoint_last.pt\')\n                for optimizer in optimizers:\n                    if os.path.exists(last_checkpoint):\n                        os.remove(last_checkpoint)\n                    train_translation_model(data_dir, \'lstm\', [\n                        \'--required-batch-size-multiple\', \'1\',\n                        \'--encoder-layers\', \'1\',\n                        \'--encoder-hidden-size\', \'32\',\n                        \'--decoder-layers\', \'1\',\n                        \'--optimizer\', optimizer,\n                    ])\n                    generate_main(data_dir)\n\n\ndef create_dummy_roberta_head_data(data_dir, num_examples=100, maxlen=10, num_classes=2, regression=False):\n    input_dir = \'input0\'\n    def _create_dummy_data(filename):\n        random_data = torch.rand(num_examples * maxlen)\n        input_data = 97 + torch.floor(26 * random_data).int()\n        if regression:\n            output_data = torch.rand((num_examples, num_classes))\n        else:\n            output_data = 1 + torch.floor(num_classes * torch.rand(num_examples)).int()\n        with open(os.path.join(data_dir, input_dir, filename+\'.out\'), \'w\') as f_in:\n            label_filename = filename+\'.label\' if regression else filename+\'.out\'\n            with open(os.path.join(data_dir, \'label\', label_filename), \'w\') as f_out:\n                offset = 0\n                for i in range(num_examples):\n                    # write example input\n                    ex_len = random.randint(1, maxlen)\n                    ex_str = \' \'.join(map(chr, input_data[offset:offset+ex_len]))\n                    print(ex_str, file=f_in)\n                    # write example label\n                    if regression:\n                        class_str = \' \'.join(map(str, output_data[i].numpy()))\n                        print(class_str, file=f_out)\n                    else:\n                        class_str = \'class{}\'.format(output_data[i])\n                        print(class_str, file=f_out)\n                    offset += ex_len\n\n    os.mkdir(os.path.join(data_dir, input_dir))\n    os.mkdir(os.path.join(data_dir, \'label\'))\n    _create_dummy_data(\'train\')\n    _create_dummy_data(\'valid\')\n    _create_dummy_data(\'test\')\n\n\ndef train_masked_lm(data_dir, arch, extra_flags=None):\n    train_parser = options.get_training_parser()\n    train_args = options.parse_args_and_arch(\n        train_parser,\n        [\n            \'--task\', \'masked_lm\',\n            data_dir,\n            \'--arch\', arch,\n            \'--optimizer\', \'adam\',\n            \'--lr\', \'0.0001\',\n            \'--criterion\', \'masked_lm\',\n            \'--max-sentences\', \'500\',\n            \'--save-dir\', data_dir,\n            \'--max-epoch\', \'1\',\n            \'--no-progress-bar\',\n            \'--distributed-world-size\', \'1\',\n            \'--ddp-backend\', \'no_c10d\',\n            \'--num-workers\', 0,\n        ] + (extra_flags or []),\n    )\n    train.main(train_args)\n\n\ndef train_roberta_head(data_dir, arch, num_classes=2, extra_flags=None):\n    train_parser = options.get_training_parser()\n    train_args = options.parse_args_and_arch(\n        train_parser,\n        [\n            \'--task\', \'sentence_prediction\',\n            data_dir,\n            \'--arch\', arch,\n            \'--num-classes\', str(num_classes),\n            \'--optimizer\', \'adam\',\n            \'--lr\', \'0.0001\',\n            \'--criterion\', \'sentence_prediction\',\n            \'--max-tokens\', \'500\',\n            \'--max-positions\', \'500\',\n            \'--max-sentences\', \'500\',\n            \'--save-dir\', data_dir,\n            \'--max-epoch\', \'1\',\n            \'--no-progress-bar\',\n            \'--distributed-world-size\', \'1\',\n            \'--ddp-backend\', \'no_c10d\',\n            \'--num-workers\', 0,\n        ] + (extra_flags or []),\n    )\n    train.main(train_args)\n\n\ndef train_language_model(data_dir, arch, extra_flags=None, run_validation=False):\n    train_parser = options.get_training_parser()\n    train_args = options.parse_args_and_arch(\n        train_parser,\n        [\n            \'--task\', \'language_modeling\',\n            data_dir,\n            \'--arch\', arch,\n            \'--optimizer\', \'adam\',\n            \'--lr\', \'0.0001\',\n            \'--criterion\', \'adaptive_loss\',\n            \'--adaptive-softmax-cutoff\', \'5,10,15\',\n            \'--max-tokens\', \'500\',\n            \'--tokens-per-sample\', \'500\',\n            \'--save-dir\', data_dir,\n            \'--max-epoch\', \'1\',\n            \'--no-progress-bar\',\n            \'--distributed-world-size\', \'1\',\n            \'--ddp-backend\', \'no_c10d\',\n        ] + (extra_flags or []),\n    )\n    train.main(train_args)\n\n    if run_validation:\n        # test validation\n        validate_parser = options.get_validation_parser()\n        validate_args = options.parse_args_and_arch(\n            validate_parser,\n            [\n                \'--task\', \'language_modeling\',\n                data_dir,\n                \'--path\', os.path.join(data_dir, \'checkpoint_last.pt\'),\n                \'--valid-subset\', \'valid\',\n                \'--max-tokens\', \'500\',\n                \'--no-progress-bar\',\n            ]\n        )\n        validate.main(validate_args)\n\n\ndef eval_lm_main(data_dir):\n    eval_lm_parser = options.get_eval_lm_parser()\n    eval_lm_args = options.parse_args_and_arch(\n        eval_lm_parser,\n        [\n            data_dir,\n            \'--path\', os.path.join(data_dir, \'checkpoint_last.pt\'),\n            \'--no-progress-bar\',\n        ],\n    )\n    eval_lm.main(eval_lm_args)\n\n\ndef train_masked_language_model(data_dir, arch, extra_args=()):\n    train_parser = options.get_training_parser()\n    # TODO: langs should be in and out right?\n    train_args = options.parse_args_and_arch(\n        train_parser,\n        [\n            ""--task"",\n            ""cross_lingual_lm"",\n            data_dir,\n            ""--arch"",\n            arch,\n            # Optimizer args\n            ""--optimizer"",\n            ""adam"",\n            ""--lr-scheduler"",\n            ""reduce_lr_on_plateau"",\n            ""--lr-shrink"",\n            ""0.5"",\n            ""--lr"",\n            ""0.0001"",\n            ""--min-lr"",\n            ""1e-09"",\n            # dropout, attention args\n            ""--dropout"",\n            ""0.1"",\n            ""--attention-dropout"",\n            ""0.1"",\n            # MLM args\n            ""--criterion"",\n            ""masked_lm_loss"",\n            ""--masked-lm-only"",\n            ""--monolingual-langs"",\n            ""in,out"",\n            ""--num-segment"",\n            ""5"",\n            # Transformer args: use a small transformer model for fast training\n            ""--encoder-layers"",\n            ""1"",\n            ""--encoder-embed-dim"",\n            ""32"",\n            ""--encoder-attention-heads"",\n            ""1"",\n            ""--encoder-ffn-embed-dim"",\n            ""32"",\n            # Other training args\n            ""--max-tokens"",\n            ""500"",\n            ""--tokens-per-sample"",\n            ""500"",\n            ""--save-dir"",\n            data_dir,\n            ""--max-epoch"",\n            ""1"",\n            ""--no-progress-bar"",\n            ""--distributed-world-size"",\n            ""1"",\n            ""--dataset-impl"",\n            ""raw"",\n        ] + list(extra_args),\n    )\n    train.main(train_args)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_bmuf.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nfrom multiprocessing import Manager\nimport random\nimport unittest\n\nimport torch\nimport torch.nn as nn\n\nfrom fairseq import distributed_utils, optim\n\n\nclass Model(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        return output\n\n\ndef setup_model_loss_criterion(args, rank, is_cuda):\n    """"""\n    setup model, criterion and optimizer based on input args\n    """"""\n    args.distributed_rank = rank\n    if args.distributed_world_size > 1:\n        distributed_utils.distributed_init(args)\n    torch.manual_seed(1)\n    model = Model(args.input_size, args.nb_classes)\n    loss_fn = nn.CrossEntropyLoss()\n    if is_cuda:\n        model = model.cuda()\n        loss_fn = loss_fn.cuda()\n\n    optimizer = optim.sgd.SGD(args, model.parameters())\n    optimizer = optim.FairseqBMUF(args, optimizer)\n\n    return model, loss_fn, optimizer\n\n\ndef train_step(input, target, model, loss_fn, optimizer, **unused):\n    """"""Do forward, backward and parameter update.""""""\n    model.train()\n    output = model(input)\n    loss = loss_fn(output, target)\n    optimizer.backward(loss)\n    optimizer.step()\n\n\ndef single_gpu_training(args, rank, iterations, shared_results):\n\n    is_cuda = torch.cuda.is_available()\n    if is_cuda:\n        torch.cuda.set_device(rank)\n\n    model, loss_fn, optimizer = setup_model_loss_criterion(args, rank, is_cuda)\n\n    for _ in range(iterations):\n        input = torch.randn(1, args.input_size)\n        target = torch.empty(args.batch_size, dtype=torch.long).random_(args.nb_classes)\n\n        if is_cuda:\n            input = input.cuda()\n            target = target.cuda()\n        train_step(input, target, model, loss_fn, optimizer)\n\n    results = []\n    for param in model.parameters():\n        if len(results) == 0:\n            results = param.flatten().cpu().data\n        else:\n            results = torch.cat((results, param.flatten().cpu().data), 0)\n\n    shared_results[rank] = results\n\n\ndef setup_args():\n    args = argparse.Namespace()\n    args.global_sync_iter = 20\n    args.block_momentum = 0.875\n    args.block_lr = 0.5\n    args.input_size = 5\n    args.nb_classes = 2\n    args.batch_size = 1\n    args.lr = [1e-3]\n    args.momentum = 0\n    args.weight_decay = 0\n    args.warmup_iterations = 0\n    args.use_nbm = True\n    args.average_sync = True\n    args.global_sync_iter = 1\n    args.model_parallel_size = 1\n    args.distributed_backend = ""gloo""\n\n    args.distributed_world_size = 2\n    port = random.randint(10000, 20000)\n    args.distributed_init_method = ""tcp://localhost:{port}"".format(port=port)\n    args.distributed_init_host = ""localhost""\n    args.distributed_port = port + 1\n    args.local_world_size = args.distributed_world_size\n    return args\n\n\n@unittest.skipIf(torch.cuda.device_count() < 2, ""test requires 2 GPUs"")\nclass TestBMUF(unittest.TestCase):\n    def bmuf_process(self, args, iterations):\n        processes = []\n        results = Manager().dict()\n        ctx = torch.multiprocessing.get_context(""spawn"")\n        for rank in range(args.distributed_world_size):\n            p = ctx.Process(\n                target=single_gpu_training, args=(args, rank, iterations, results)\n            )\n            p.start()\n            processes.append(p)\n\n        for p in processes:\n            p.join()\n        return results\n\n    def test_bmuf_sync(self):\n        # Train model for 1 iteration and do bmuf sync without doing warmup\n        args = setup_args()\n        iterations = 1\n        results = self.bmuf_process(args, iterations)\n        # Make sure params in both machines are same\n        assert len(results) == 2\n        self.assertAlmostEqual(results[0], results[1])\n\n    def test_warmup_sync(self):\n        # Train model for 20 iteration and do warmup sync without doing bmuf sync\n        args = setup_args()\n        args.warmup_iterations = 20\n        iterations = 20\n        results = self.bmuf_process(args, iterations)\n        # Make sure params in both machines are same\n        assert len(results) == 2\n        self.assertAlmostEqual(results[0], results[1])\n\n    def test_warmup_sync_bmuf_sync(self):\n        # Train model for 25 iteration and do warmup sync after 20 iteration\n        # and bmuf sync after 25 iteration\n        args = setup_args()\n        args.warmup_iterations = 20\n        args.global_sync_iter = 5\n        iterations = 25\n        results = self.bmuf_process(args, iterations)\n        # Make sure params in both machines are same\n        assert len(results) == 2\n        self.assertAlmostEqual(results[0], results[1])\n\n    def test_single_gpu_bmuf(self):\n        # Train model for 5 iterations and use GPU 1\n        args = setup_args()\n        args.distributed_world_size = 1\n        args.warmup_iterations = 5\n        iterations = 20\n        results = self.bmuf_process(args, iterations)\n        assert len(results) == 1\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertLess((t1 - t2).abs().max(), 1e-4)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_character_token_embedder.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport unittest\n\nfrom fairseq.data import Dictionary\nfrom fairseq.modules import CharacterTokenEmbedder\n\n\nclass TestCharacterTokenEmbedder(unittest.TestCase):\n    def test_character_token_embedder(self):\n        vocab = Dictionary()\n        vocab.add_symbol(\'hello\')\n        vocab.add_symbol(\'there\')\n\n        embedder = CharacterTokenEmbedder(vocab, [(2, 16), (4, 32), (8, 64), (16, 2)], 64, 5, 2)\n\n        test_sents = [[\'hello\', \'unk\', \'there\'], [\'there\'], [\'hello\', \'there\']]\n        max_len = max(len(s) for s in test_sents)\n        input = torch.LongTensor(len(test_sents), max_len + 2).fill_(vocab.pad())\n        for i in range(len(test_sents)):\n            input[i][0] = vocab.eos()\n            for j in range(len(test_sents[i])):\n                input[i][j + 1] = vocab.index(test_sents[i][j])\n            input[i][j + 2] = vocab.eos()\n        embs = embedder(input)\n\n        assert embs.size() == (len(test_sents), max_len + 2, 5)\n        self.assertAlmostEqual(embs[0][0], embs[1][0])\n        self.assertAlmostEqual(embs[0][0], embs[0][-1])\n        self.assertAlmostEqual(embs[0][1], embs[2][1])\n        self.assertAlmostEqual(embs[0][3], embs[1][1])\n\n        embs.sum().backward()\n        assert embedder.char_embeddings.weight.grad is not None\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertLess((t1 - t2).abs().max(), 1e-6)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_concat_dataset.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\nfrom fairseq.data import LanguagePairDataset, TokenBlockDataset\nfrom fairseq.data.concat_dataset import ConcatDataset\nfrom tests.test_train import mock_dict\n\n\nclass TestConcatDataset(unittest.TestCase):\n    def setUp(self):\n        d = mock_dict()\n        tokens_1 = torch.LongTensor([1]).view(1, -1)\n        tokens_ds1 = TokenBlockDataset(\n            tokens_1,\n            sizes=[tokens_1.size(-1)],\n            block_size=1,\n            pad=0,\n            eos=1,\n            include_targets=False,\n        )\n        self.dataset_1 = LanguagePairDataset(\n            tokens_ds1, tokens_ds1.sizes, d, shuffle=False\n        )\n        tokens_2 = torch.LongTensor([2]).view(1, -1)\n        tokens_ds2 = TokenBlockDataset(\n            tokens_2,\n            sizes=[tokens_2.size(-1)],\n            block_size=1,\n            pad=0,\n            eos=1,\n            include_targets=False,\n        )\n        self.dataset_2 = LanguagePairDataset(\n            tokens_ds2, tokens_ds2.sizes, d, shuffle=False\n        )\n\n    def test_concat_dataset_basics(self):\n        d = ConcatDataset(\n            [self.dataset_1, self.dataset_2]\n        )\n        assert(len(d) == 2)\n        assert(d[0]['source'][0] == 1)\n        assert(d[1]['source'][0] == 2)\n\n        d = ConcatDataset(\n            [self.dataset_1, self.dataset_2], sample_ratios=[1, 2]\n        )\n        assert(len(d) == 3)\n        assert(d[0]['source'][0] == 1)\n        assert(d[1]['source'][0] == 2)\n        assert(d[2]['source'][0] == 2)\n\n        d = ConcatDataset(\n            [self.dataset_1, self.dataset_2], sample_ratios=[2, 1]\n        )\n        assert(len(d) == 3)\n        assert(d[0]['source'][0] == 1)\n        assert(d[1]['source'][0] == 1)\n        assert(d[2]['source'][0] == 2)\n"""
tests/test_convtbc.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport unittest\nfrom fairseq.modules import ConvTBC\nimport torch.nn as nn\n\n\nclass TestConvTBC(unittest.TestCase):\n\n    def test_convtbc(self):\n        # ksz, in_channels, out_channels\n        conv_tbc = ConvTBC(4, 5, kernel_size=3, padding=1)\n        # out_channels, in_channels, ksz\n        conv1d = nn.Conv1d(4, 5, kernel_size=3, padding=1)\n\n        conv_tbc.weight.data.copy_(conv1d.weight.data.transpose(0, 2))\n        conv_tbc.bias.data.copy_(conv1d.bias.data)\n\n        input_tbc = torch.randn(7, 2, 4, requires_grad=True)\n        input1d = input_tbc.data.transpose(0, 1).transpose(1, 2)\n        input1d.requires_grad = True\n\n        output_tbc = conv_tbc(input_tbc)\n        output1d = conv1d(input1d)\n\n        self.assertAlmostEqual(output_tbc.data.transpose(0, 1).transpose(1, 2), output1d.data)\n\n        grad_tbc = torch.randn(output_tbc.size())\n        grad1d = grad_tbc.transpose(0, 1).transpose(1, 2).contiguous()\n\n        output_tbc.backward(grad_tbc)\n        output1d.backward(grad1d)\n\n        self.assertAlmostEqual(conv_tbc.weight.grad.data.transpose(0, 2), conv1d.weight.grad.data)\n        self.assertAlmostEqual(conv_tbc.bias.grad.data, conv1d.bias.grad.data)\n        self.assertAlmostEqual(input_tbc.grad.data.transpose(0, 1).transpose(1, 2), input1d.grad.data)\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertLess((t1 - t2).abs().max(), 1e-4)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_dictionary.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport io\nimport tempfile\nimport unittest\n\nimport torch\n\nfrom fairseq.data import Dictionary\n\n\nclass TestDictionary(unittest.TestCase):\n\n    def test_finalize(self):\n        txt = [\n            \'A B C D\',\n            \'B C D\',\n            \'C D\',\n            \'D\',\n        ]\n        ref_ids1 = list(map(torch.IntTensor, [\n            [4, 5, 6, 7, 2],\n            [5, 6, 7, 2],\n            [6, 7, 2],\n            [7, 2],\n        ]))\n        ref_ids2 = list(map(torch.IntTensor, [\n            [7, 6, 5, 4, 2],\n            [6, 5, 4, 2],\n            [5, 4, 2],\n            [4, 2],\n        ]))\n\n        # build dictionary\n        d = Dictionary()\n        for line in txt:\n            d.encode_line(line, add_if_not_exist=True)\n\n        def get_ids(dictionary):\n            ids = []\n            for line in txt:\n                ids.append(dictionary.encode_line(line, add_if_not_exist=False))\n            return ids\n\n        def assertMatch(ids, ref_ids):\n            for toks, ref_toks in zip(ids, ref_ids):\n                self.assertEqual(toks.size(), ref_toks.size())\n                self.assertEqual(0, (toks != ref_toks).sum().item())\n\n        ids = get_ids(d)\n        assertMatch(ids, ref_ids1)\n\n        # check finalized dictionary\n        d.finalize()\n        finalized_ids = get_ids(d)\n        assertMatch(finalized_ids, ref_ids2)\n\n        # write to disk and reload\n        with tempfile.NamedTemporaryFile(mode=\'w\') as tmp_dict:\n            d.save(tmp_dict.name)\n            d = Dictionary.load(tmp_dict.name)\n            reload_ids = get_ids(d)\n            assertMatch(reload_ids, ref_ids2)\n            assertMatch(finalized_ids, reload_ids)\n\n    def test_overwrite(self):\n        # for example, Camembert overwrites <unk>, <s> and </s>\n        dict_file = io.StringIO(\n            ""<unk> 999 #fairseq:overwrite\\n""\n            ""<s> 999 #fairseq:overwrite\\n""\n            ""</s> 999 #fairseq:overwrite\\n""\n            "", 999\\n""\n            ""\xe2\x96\x81de 999\\n""\n        )\n        d = Dictionary()\n        d.add_from_file(dict_file)\n        self.assertEqual(d.index(\'<pad>\'), 1)\n        self.assertEqual(d.index(\'foo\'), 3)\n        self.assertEqual(d.index(\'<unk>\'), 4)\n        self.assertEqual(d.index(\'<s>\'), 5)\n        self.assertEqual(d.index(\'</s>\'), 6)\n        self.assertEqual(d.index(\',\'), 7)\n        self.assertEqual(d.index(\'\xe2\x96\x81de\'), 8)\n\n    def test_no_overwrite(self):\n        # for example, Camembert overwrites <unk>, <s> and </s>\n        dict_file = io.StringIO(\n            ""<unk> 999\\n""\n            ""<s> 999\\n""\n            ""</s> 999\\n""\n            "", 999\\n""\n            ""\xe2\x96\x81de 999\\n""\n        )\n        d = Dictionary()\n        with self.assertRaisesRegex(RuntimeError, \'Duplicate\'):\n            d.add_from_file(dict_file)\n\n    def test_space(self):\n        # for example, character models treat space as a symbol\n        dict_file = io.StringIO(\n            ""  999\\n""\n            ""a 999\\n""\n            ""b 999\\n""\n        )\n        d = Dictionary()\n        d.add_from_file(dict_file)\n        self.assertEqual(d.index(\' \'), 4)\n        self.assertEqual(d.index(\'a\'), 5)\n        self.assertEqual(d.index(\'b\'), 6)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_export.py,9,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport tempfile\nimport unittest\n\nimport torch\nfrom fairseq.data.dictionary import Dictionary\nfrom fairseq.models.transformer import TransformerModel\nfrom fairseq.modules import multihead_attention, sinusoidal_positional_embedding\nfrom fairseq.tasks.fairseq_task import FairseqTask\n\n\nDEFAULT_TEST_VOCAB_SIZE = 100\n\n\nclass DummyTask(FairseqTask):\n    def __init__(self, args):\n        super().__init__(args)\n        self.dictionary = get_dummy_dictionary()\n        if getattr(self.args, ""ctc"", False):\n            self.dictionary.add_symbol(""<ctc_blank>"")\n        self.src_dict = self.dictionary\n        self.tgt_dict = self.dictionary\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\ndef get_dummy_dictionary(vocab_size=DEFAULT_TEST_VOCAB_SIZE):\n    dummy_dict = Dictionary()\n    # add dummy symbol to satisfy vocab size\n    for id, _ in enumerate(range(vocab_size)):\n        dummy_dict.add_symbol(""{}"".format(id), 1000)\n    return dummy_dict\n\n\ndef get_dummy_task_and_parser():\n    """"""\n    Return a dummy task and argument parser, which can be used to\n    create a model/criterion.\n    """"""\n    parser = argparse.ArgumentParser(\n        description=""test_dummy_s2s_task"", argument_default=argparse.SUPPRESS\n    )\n    DummyTask.add_args(parser)\n    args = parser.parse_args([])\n    task = DummyTask.setup_task(args)\n    return task, parser\n\n\ndef _test_save_and_load(scripted_module):\n    with tempfile.NamedTemporaryFile() as f:\n        scripted_module.save(f.name)\n        torch.jit.load(f.name)\n\n\nclass TestExportModels(unittest.TestCase):\n    def test_export_multihead_attention(self):\n        module = multihead_attention.MultiheadAttention(embed_dim=8, num_heads=2)\n        scripted = torch.jit.script(module)\n        _test_save_and_load(scripted)\n\n    def test_incremental_state_multihead_attention(self):\n        module1 = multihead_attention.MultiheadAttention(embed_dim=8, num_heads=2)\n        module1 = torch.jit.script(module1)\n        module2 = multihead_attention.MultiheadAttention(embed_dim=8, num_heads=2)\n        module2 = torch.jit.script(module2)\n\n        state = {}\n        state = module1.set_incremental_state(state, ""key"", {""a"": torch.tensor([1])})\n        state = module2.set_incremental_state(state, ""key"", {""a"": torch.tensor([2])})\n        v1 = module1.get_incremental_state(state, ""key"")[""a""]\n        v2 = module2.get_incremental_state(state, ""key"")[""a""]\n\n        self.assertEqual(v1, 1)\n        self.assertEqual(v2, 2)\n\n    def test_positional_embedding(self):\n        module = sinusoidal_positional_embedding.SinusoidalPositionalEmbedding(\n            embedding_dim=8, padding_idx=1\n        )\n        scripted = torch.jit.script(module)\n        _test_save_and_load(scripted)\n\n    @unittest.skipIf(\n        torch.__version__ < ""1.6.0"", ""Targeting OSS scriptability for the 1.6 release""\n    )\n    def test_export_transformer(self):\n        task, parser = get_dummy_task_and_parser()\n        TransformerModel.add_args(parser)\n        args = parser.parse_args([])\n        model = TransformerModel.build_model(args, task)\n        scripted = torch.jit.script(model)\n        _test_save_and_load(scripted)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_file_io.py,0,"b'# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport sys\nimport tempfile\nimport os\nimport shutil\n\nfrom typing import Optional\n\nimport unittest\nfrom unittest.mock import MagicMock\n\n\nclass TestFileIO(unittest.TestCase):\n\n    _tmpdir: Optional[str] = None\n    _tmpfile: Optional[str] = None\n    _tmpfile_contents = ""Hello, World""\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls._tmpdir = tempfile.mkdtemp()\n        with open(os.path.join(cls._tmpdir, ""test.txt""), ""w"") as f:\n            cls._tmpfile = f.name\n            f.write(cls._tmpfile_contents)\n            f.flush()\n\n    @classmethod\n    def tearDownClass(cls) -> None:\n        # Cleanup temp working dir.\n        if cls._tmpdir is not None:\n            shutil.rmtree(cls._tmpdir)  # type: ignore\n\n    def test_file_io(self):\n        from fairseq.file_io import PathManager\n        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:\n            s = f.read()\n        self.assertEqual(s, self._tmpfile_contents)\n\n    def test_file_io_oss(self):\n        # Mock fvcore to simulate oss environment.\n        sys.modules[\'fvcore\'] = MagicMock()\n        from fairseq.file_io import PathManager\n        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:\n            s = f.read()\n        self.assertEqual(s, self._tmpfile_contents)\n'"
tests/test_iterators.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nfrom fairseq.data import iterators\n\n\nclass TestIterators(unittest.TestCase):\n\n    def test_counting_iterator(self, ref=None, itr=None):\n        if ref is None:\n            assert itr is None\n            ref = list(range(10))\n            itr = iterators.CountingIterator(ref)\n        else:\n            assert len(ref) == 10\n            assert itr is not None\n        self.assertTrue(itr.has_next())\n        self.assertEqual(itr.n, 0)\n        self.assertEqual(next(itr), ref[0])\n        self.assertEqual(itr.n, 1)\n        self.assertEqual(next(itr), ref[1])\n        self.assertEqual(itr.n, 2)\n        itr.skip(3)\n        self.assertEqual(itr.n, 5)\n        self.assertEqual(next(itr), ref[5])\n        itr.skip(3)\n        self.assertEqual(itr.n, 9)\n        self.assertEqual(next(itr), ref[9])\n        self.assertFalse(itr.has_next())\n\n    def test_grouped_iterator(self):\n        # test correctness\n        x = list(range(10))\n        itr = iterators.GroupedIterator(x, 1)\n        self.assertEqual(list(itr), [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]])\n        itr = iterators.GroupedIterator(x, 4)\n        self.assertEqual(list(itr), [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9]])\n        itr = iterators.GroupedIterator(x, 5)\n        self.assertEqual(list(itr), [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n\n        # test CountingIterator functionality\n        x = list(range(30))\n        ref = list(iterators.GroupedIterator(x, 3))\n        itr = iterators.GroupedIterator(x, 3)\n        self.test_counting_iterator(ref, itr)\n\n    def test_sharded_iterator(self):\n        # test correctness\n        x = list(range(10))\n        itr = iterators.ShardedIterator(x, num_shards=1, shard_id=0)\n        self.assertEqual(list(itr), x)\n        itr = iterators.ShardedIterator(x, num_shards=2, shard_id=0)\n        self.assertEqual(list(itr), [0, 2, 4, 6, 8])\n        itr = iterators.ShardedIterator(x, num_shards=2, shard_id=1)\n        self.assertEqual(list(itr), [1, 3, 5, 7, 9])\n        itr = iterators.ShardedIterator(x, num_shards=3, shard_id=0)\n        self.assertEqual(list(itr), [0, 3, 6, 9])\n        itr = iterators.ShardedIterator(x, num_shards=3, shard_id=1)\n        self.assertEqual(list(itr), [1, 4, 7, None])\n        itr = iterators.ShardedIterator(x, num_shards=3, shard_id=2)\n        self.assertEqual(list(itr), [2, 5, 8, None])\n\n        # test CountingIterator functionality\n        x = list(range(30))\n        ref = list(iterators.ShardedIterator(x, num_shards=3, shard_id=0))\n        itr = iterators.ShardedIterator(x, num_shards=3, shard_id=0)\n        self.test_counting_iterator(ref, itr)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_label_smoothing.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport copy\nimport unittest\n\nimport torch\n\nfrom fairseq.criterions.cross_entropy import CrossEntropyCriterion\nfrom fairseq.criterions.label_smoothed_cross_entropy import LabelSmoothedCrossEntropyCriterion\n\nimport tests.utils as test_utils\n\n\nclass TestLabelSmoothing(unittest.TestCase):\n\n    def setUp(self):\n        # build dictionary\n        self.d = test_utils.dummy_dictionary(3)\n        vocab = len(self.d)\n        self.assertEqual(vocab, 4 + 3)  # 4 special + 3 tokens\n        self.assertEqual(self.d.pad(), 1)\n        self.assertEqual(self.d.eos(), 2)\n        self.assertEqual(self.d.unk(), 3)\n        pad, eos, unk, w1, w2, w3 = 1, 2, 3, 4, 5, 6  # noqa: F841\n\n        # build dataset\n        self.data = [\n            # the first batch item has padding\n            {\'source\': torch.LongTensor([w1, eos]), \'target\': torch.LongTensor([w1, eos])},\n            {\'source\': torch.LongTensor([w1, eos]), \'target\': torch.LongTensor([w1, w1, eos])},\n        ]\n        self.sample = next(test_utils.dummy_dataloader(self.data))\n\n        # build model\n        self.args = argparse.Namespace()\n        self.args.sentence_avg = False\n        self.args.probs = torch.FloatTensor([\n            #      pad   eos  unk   w1   w2   w3\n            [0.05, 0.05, 0.1, 0.05, 0.3, 0.4, 0.05],\n            [0.05, 0.10, 0.2, 0.05, 0.2, 0.3, 0.10],\n            [0.05, 0.15, 0.3, 0.05, 0.1, 0.2, 0.15],\n        ]).unsqueeze(0).expand(2, 3, 7)  # add batch dimension\n        self.task = test_utils.TestTranslationTask.setup_task(self.args, self.d, self.d)\n        self.model = self.task.build_model(self.args)\n\n    def test_nll_loss(self):\n        self.args.label_smoothing = 0.1\n        nll_crit = CrossEntropyCriterion.build_criterion(self.args, self.task)\n        smooth_crit = LabelSmoothedCrossEntropyCriterion.build_criterion(self.args, self.task)\n        nll_loss, nll_sample_size, nll_logging_output = nll_crit(self.model, self.sample)\n        smooth_loss, smooth_sample_size, smooth_logging_output = smooth_crit(self.model, self.sample)\n        self.assertLess(abs(nll_loss - nll_logging_output[\'loss\']), 1e-6)\n        self.assertLess(abs(nll_loss - smooth_logging_output[\'nll_loss\']), 1e-6)\n\n    def test_padding(self):\n        self.args.label_smoothing = 0.1\n        crit = LabelSmoothedCrossEntropyCriterion.build_criterion(self.args, self.task)\n        loss, _, logging_output = crit(self.model, self.sample)\n\n        def get_one_no_padding(idx):\n            # create a new sample with just a single batch item so that there\'s\n            # no padding\n            sample1 = next(test_utils.dummy_dataloader([self.data[idx]]))\n            args1 = copy.copy(self.args)\n            args1.probs = args1.probs[idx, :, :].unsqueeze(0)\n            model1 = self.task.build_model(args1)\n            loss1, _, _ = crit(model1, sample1)\n            return loss1\n\n        loss1 = get_one_no_padding(0)\n        loss2 = get_one_no_padding(1)\n        self.assertAlmostEqual(loss, loss1 + loss2)\n\n    def test_reduction(self):\n        self.args.label_smoothing = 0.1\n        crit = LabelSmoothedCrossEntropyCriterion.build_criterion(self.args, self.task)\n        loss, _, logging_output = crit(self.model, self.sample, reduce=True)\n        unreduced_loss, _, _ = crit(self.model, self.sample, reduce=False)\n        self.assertAlmostEqual(loss, unreduced_loss.sum())\n\n    def test_zero_eps(self):\n        self.args.label_smoothing = 0.0\n        nll_crit = CrossEntropyCriterion.build_criterion(self.args, self.task)\n        smooth_crit = LabelSmoothedCrossEntropyCriterion.build_criterion(self.args, self.task)\n        nll_loss, nll_sample_size, nll_logging_output = nll_crit(self.model, self.sample)\n        smooth_loss, smooth_sample_size, smooth_logging_output = smooth_crit(self.model, self.sample)\n        self.assertAlmostEqual(nll_loss, smooth_loss)\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertLess((t1 - t2).abs().max(), 1e-6)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_lstm_jitable.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport tempfile\nimport unittest\n\nimport torch\nfrom fairseq.data.dictionary import Dictionary\nfrom fairseq.models.lstm import LSTMModel\nfrom fairseq.tasks.fairseq_task import FairseqTask\n\n\nDEFAULT_TEST_VOCAB_SIZE = 100\n\n\nclass DummyTask(FairseqTask):\n    def __init__(self, args):\n        super().__init__(args)\n        self.dictionary = get_dummy_dictionary()\n        if getattr(self.args, ""ctc"", False):\n            self.dictionary.add_symbol(""<ctc_blank>"")\n        self.src_dict = self.dictionary\n        self.tgt_dict = self.dictionary\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\ndef get_dummy_dictionary(vocab_size=DEFAULT_TEST_VOCAB_SIZE):\n    dummy_dict = Dictionary()\n    # add dummy symbol to satisfy vocab size\n    for id, _ in enumerate(range(vocab_size)):\n        dummy_dict.add_symbol(""{}"".format(id), 1000)\n    return dummy_dict\n\n\ndef get_dummy_task_and_parser():\n    """"""\n    to build a fariseq model, we need some dummy parse and task. This function\n    is used to create dummy task and parser to faciliate model/criterion test\n\n    Note: we use FbSpeechRecognitionTask as the dummy task. You may want\n    to use other task by providing another function\n    """"""\n    parser = argparse.ArgumentParser(\n        description=""test_dummy_s2s_task"", argument_default=argparse.SUPPRESS\n    )\n    DummyTask.add_args(parser)\n    args = parser.parse_args([])\n    task = DummyTask.setup_task(args)\n    return task, parser\n\n\nclass TestJitLSTMModel(unittest.TestCase):\n    def _test_save_and_load(self, scripted_module):\n        with tempfile.NamedTemporaryFile() as f:\n            scripted_module.save(f.name)\n            torch.jit.load(f.name)\n\n    def assertTensorEqual(self, t1, t2):\n        t1 = t1[~torch.isnan(t1)]  # can cause size mismatch errors if there are NaNs\n        t2 = t2[~torch.isnan(t2)]\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n    def test_jit_and_export_lstm(self):\n        task, parser = get_dummy_task_and_parser()\n        LSTMModel.add_args(parser)\n        args = parser.parse_args([])\n        args.criterion = """"\n        model = LSTMModel.build_model(args, task)\n        scripted_model = torch.jit.script(model)\n        self._test_save_and_load(scripted_model)\n\n    def test_assert_jit_vs_nonjit_(self):\n        task, parser = get_dummy_task_and_parser()\n        LSTMModel.add_args(parser)\n        args = parser.parse_args([])\n        args.criterion = """"\n        model = LSTMModel.build_model(args, task)\n        model.eval()\n        scripted_model = torch.jit.script(model)\n        scripted_model.eval()\n        idx = len(task.source_dictionary)\n        iter = 100\n        # Inject random input and check output\n        seq_len_tensor = torch.randint(1, 10, (iter, ))\n        num_samples_tensor = torch.randint(1, 10, (iter, ))\n        for i in range(iter):\n            seq_len = seq_len_tensor[i]\n            num_samples = num_samples_tensor[i]\n            src_token = torch.randint(0, idx, (num_samples, seq_len)),\n            src_lengths = torch.randint(1, seq_len+1, (num_samples,))\n            src_lengths, _ = torch.sort(src_lengths, descending=True)\n            # Force the first sample to have seq_len\n            src_lengths[0] = seq_len\n            prev_output_token = torch.randint(0, idx, (num_samples, 1)),\n            result = model(src_token[0], src_lengths, prev_output_token[0], None)\n            scripted_result = scripted_model(src_token[0], src_lengths, prev_output_token[0], None)\n            self.assertTensorEqual(result[0], scripted_result[0])\n            self.assertTensorEqual(result[1], scripted_result[1])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_memory_efficient_fp16.py,6,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport logging\nimport unittest\n\nimport torch\n\nfrom fairseq.optim.adam import FairseqAdam\nfrom fairseq.optim.fp16_optimizer import MemoryEfficientFP16Optimizer\n\n\n@unittest.skipIf(not torch.cuda.is_available(), 'test requires a GPU')\nclass TestMemoryEfficientFP16(unittest.TestCase):\n\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def test_load_state_dict(self):\n        # define simple FP16 model\n        model = torch.nn.Linear(5, 5).cuda().half()\n        params = list(model.parameters())\n\n        # initialize memory efficient FP16 optimizer\n        optimizer = FairseqAdam(\n            argparse.Namespace(\n                lr=[0.00001],\n                adam_betas='(0.9, 0.999)',\n                adam_eps=1e-8,\n                weight_decay=0.0,\n            ),\n            params,\n        )\n        me_optimizer = MemoryEfficientFP16Optimizer(\n            argparse.Namespace(\n                fp16_init_scale=1,\n                fp16_scale_window=1,\n                fp16_scale_tolerance=1,\n                threshold_loss_scale=1,\n                min_loss_scale=1e-4,\n            ),\n            params,\n            optimizer,\n        )\n\n        # optimizer state is created in the first step\n        loss = model(torch.rand(5).cuda().half()).sum()\n        me_optimizer.backward(loss)\n        me_optimizer.step()\n\n        # reload state\n        state = me_optimizer.state_dict()\n        me_optimizer.load_state_dict(state)\n        for k, v in me_optimizer.optimizer.state.items():\n            self.assertTrue(k.dtype == torch.float16)\n            for v_i in v.values():\n                if torch.is_tensor(v_i):\n                    self.assertTrue(v_i.dtype == torch.float32)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_metrics.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\nimport uuid\n\nfrom fairseq import metrics\n\n\nclass TestMetrics(unittest.TestCase):\n\n    def test_nesting(self):\n        with metrics.aggregate() as a:\n            metrics.log_scalar('loss', 1)\n            with metrics.aggregate() as b:\n                metrics.log_scalar('loss', 2)\n\n        self.assertEqual(a.get_smoothed_values()['loss'], 1.5)\n        self.assertEqual(b.get_smoothed_values()['loss'], 2)\n\n    def test_new_root(self):\n        with metrics.aggregate() as a:\n            metrics.log_scalar('loss', 1)\n            with metrics.aggregate(new_root=True) as b:\n                metrics.log_scalar('loss', 2)\n\n        self.assertEqual(a.get_smoothed_values()['loss'], 1)\n        self.assertEqual(b.get_smoothed_values()['loss'], 2)\n\n    def test_nested_new_root(self):\n        with metrics.aggregate() as layer1:\n            metrics.log_scalar('loss', 1)\n            with metrics.aggregate(new_root=True) as layer2:\n                metrics.log_scalar('loss', 2)\n                with metrics.aggregate() as layer3:\n                    metrics.log_scalar('loss', 3)\n                    with metrics.aggregate(new_root=True) as layer4:\n                        metrics.log_scalar('loss', 4)\n            metrics.log_scalar('loss', 1.5)\n\n        self.assertEqual(layer4.get_smoothed_values()['loss'], 4)\n        self.assertEqual(layer3.get_smoothed_values()['loss'], 3)\n        self.assertEqual(layer2.get_smoothed_values()['loss'], 2.5)\n        self.assertEqual(layer1.get_smoothed_values()['loss'], 1.25)\n\n    def test_named(self):\n        name = str(uuid.uuid4())\n        metrics.reset_meters(name)\n\n        with metrics.aggregate(name):\n            metrics.log_scalar('loss', 1)\n\n        metrics.log_scalar('loss', 3)\n\n        with metrics.aggregate(name):\n            metrics.log_scalar('loss', 2)\n\n        self.assertEqual(metrics.get_smoothed_values(name)['loss'], 1.5)\n\n    def test_nested_duplicate_names(self):\n        name = str(uuid.uuid4())\n        metrics.reset_meters(name)\n\n        with metrics.aggregate(name):\n            metrics.log_scalar('loss', 1)\n            with metrics.aggregate() as other:\n                with metrics.aggregate(name):\n                    metrics.log_scalar('loss', 2)\n            metrics.log_scalar('loss', 6)\n\n        self.assertEqual(metrics.get_smoothed_values(name)['loss'], 3)\n        self.assertEqual(other.get_smoothed_values()['loss'], 2)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_multi_corpus_sampled_dataset.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch\nfrom fairseq.data import LanguagePairDataset, TokenBlockDataset\nfrom fairseq.data.multi_corpus_sampled_dataset import MultiCorpusSampledDataset\nfrom tests.test_train import mock_dict\n\n\nclass TestMultiCorpusSampledDataset(unittest.TestCase):\n    def setUp(self):\n        d = mock_dict()\n        tokens_1 = torch.LongTensor([1]).view(1, -1)\n        tokens_ds1 = TokenBlockDataset(\n            tokens_1,\n            sizes=[tokens_1.size(-1)],\n            block_size=1,\n            pad=0,\n            eos=1,\n            include_targets=False,\n        )\n        self.dataset_1 = LanguagePairDataset(\n            tokens_ds1, tokens_ds1.sizes, d, shuffle=False\n        )\n        tokens_2 = torch.LongTensor([2]).view(1, -1)\n        tokens_ds2 = TokenBlockDataset(\n            tokens_2,\n            sizes=[tokens_2.size(-1)],\n            block_size=1,\n            pad=0,\n            eos=1,\n            include_targets=False,\n        )\n        self.dataset_2 = LanguagePairDataset(\n            tokens_ds2, tokens_ds2.sizes, d, shuffle=False\n        )\n\n    def _test_sample_helper(\n        self,\n        expected_sample_from_first_ds_percentage,\n        num_samples=1000,\n        sampling_func=None,\n    ):\n        # To make sure test is not flaky\n        np.random.seed(0)\n        if sampling_func is None:\n            m = MultiCorpusSampledDataset(\n                OrderedDict({0: self.dataset_1, 1: self.dataset_2}),\n            )\n        else:\n            m = MultiCorpusSampledDataset(\n                OrderedDict({0: self.dataset_1, 1: self.dataset_2}),\n                sampling_func=sampling_func,\n            )\n        m.ordered_indices()\n        count_sample_from_first_dataset = 0\n        for _ in range(num_samples):\n            if m.collater([m[0], m[1]])[""net_input""][""src_tokens""][0] == 1:\n                count_sample_from_first_dataset += 1\n        sample_from_first_ds_percentage = (\n            1.0 * count_sample_from_first_dataset / num_samples\n        )\n        self.assertLess(\n            abs(\n                sample_from_first_ds_percentage\n                - expected_sample_from_first_ds_percentage\n            ),\n            0.01,\n        )\n\n    def test_multi_corpus_sampled_dataset_uniform_sample(self):\n        self._test_sample_helper(expected_sample_from_first_ds_percentage=0.5)\n\n    def test_multi_corpus_sampled_dataset_weighted_sample(self):\n        def naive_weighted_sample(weights):\n            def f(l):\n                v = np.random.random()\n                agg = 0\n                for i, weight in enumerate(weights):\n                    agg += weight\n                    if agg > v:\n                        return i\n\n            return f\n\n        self._test_sample_helper(\n            expected_sample_from_first_ds_percentage=0.9,\n            sampling_func=naive_weighted_sample(weights=[0.9, 0.1]),\n        )\n'"
tests/test_multihead_attention.py,8,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport unittest\nfrom fairseq.modules.multihead_attention import MultiheadAttention\n\n\nclass TestMultiheadAttention(unittest.TestCase):\n    def test_append_prev_key_padding_mask(self):\n        bsz = 1\n        src_len = 4\n\n        cases = [\n            # no padding mask\n            (None, None, None),\n            # current padding mask only\n            (\n                torch.tensor([[1]]).bool(),\n                None,\n                torch.tensor([[0, 0, 0, 1]]).bool(),\n            ),\n            # previous padding mask only\n            (\n                None,\n                torch.tensor([[0, 1, 0]]).bool(),\n                torch.tensor([[0, 1, 0, 0]]).bool(),\n            ),\n            # both padding masks\n            (\n                torch.tensor([[1]]).bool(),\n                torch.tensor([[0, 1, 0]]).bool(),\n                torch.tensor([[0, 1, 0, 1]]).bool(),\n            ),\n        ]\n        for c in cases:\n            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n                c[0],\n                c[1],\n                batch_size=bsz,\n                src_len=src_len,\n                static_kv=False,\n            )\n\n            if key_padding_mask is not None:\n                self.assertTrue(\n                    torch.all(torch.eq(key_padding_mask, c[2])),\n                    f'Unexpected resultant key padding mask: {key_padding_mask}'\n                    f' given current: {c[0]} and previous: {c[1]}',\n                )\n                self.assertEqual(key_padding_mask.size(0), bsz)\n                self.assertEqual(key_padding_mask.size(1), src_len)\n            else:\n                self.assertIsNone(c[2])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_noising.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\nfrom typing import Dict, List\n\nimport tests.utils as test_utils\nimport torch\nfrom fairseq import utils\nfrom fairseq.data import (\n    Dictionary,\n    LanguagePairDataset,\n    TransformEosDataset,\n    data_utils,\n    noising,\n)\n\n\nclass TestDataNoising(unittest.TestCase):\n    def _get_test_data_with_bpe_cont_marker(self, append_eos=True):\n        """"""\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: BPE vocab with continuation markers as suffixes to denote\n                non-end of word tokens. This is the standard BPE format used in\n                fairseq\'s preprocessing.\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        """"""\n        vocab = Dictionary()\n        vocab.add_symbol(""he@@"")\n        vocab.add_symbol(""llo"")\n        vocab.add_symbol(""how"")\n        vocab.add_symbol(""are"")\n        vocab.add_symbol(""y@@"")\n        vocab.add_symbol(""ou"")\n        vocab.add_symbol(""n@@"")\n        vocab.add_symbol(""ew"")\n        vocab.add_symbol(""or@@"")\n        vocab.add_symbol(""k"")\n\n        src_tokens = [\n            [""he@@"", ""llo"", ""n@@"", ""ew"", ""y@@"", ""or@@"", ""k""],\n            [""how"", ""are"", ""y@@"", ""ou""],\n        ]\n        x, src_lengths = x, src_lengths = self._convert_src_tokens_to_tensor(\n            vocab=vocab, src_tokens=src_tokens, append_eos=append_eos\n        )\n        return vocab, x, src_lengths\n\n    def _get_test_data_with_bpe_end_marker(self, append_eos=True):\n        """"""\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: BPE vocab with end-of-word markers as suffixes to denote\n                tokens at the end of a word. This is an alternative to fairseq\'s\n                standard preprocessing framework and is not generally supported\n                within fairseq.\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        """"""\n        vocab = Dictionary()\n        vocab.add_symbol(""he"")\n        vocab.add_symbol(""llo_EOW"")\n        vocab.add_symbol(""how_EOW"")\n        vocab.add_symbol(""are_EOW"")\n        vocab.add_symbol(""y"")\n        vocab.add_symbol(""ou_EOW"")\n        vocab.add_symbol(""n"")\n        vocab.add_symbol(""ew_EOW"")\n        vocab.add_symbol(""or"")\n        vocab.add_symbol(""k_EOW"")\n\n        src_tokens = [\n            [""he"", ""llo_EOW"", ""n"", ""ew_EOW"", ""y"", ""or"", ""k_EOW""],\n            [""how_EOW"", ""are_EOW"", ""y"", ""ou_EOW""],\n        ]\n        x, src_lengths = x, src_lengths = self._convert_src_tokens_to_tensor(\n            vocab=vocab, src_tokens=src_tokens, append_eos=append_eos\n        )\n        return vocab, x, src_lengths\n\n    def _get_test_data_with_word_vocab(self, append_eos=True):\n        """"""\n        Args:\n            append_eos: if True, each input sentence in the source tokens tensor\n                will have an EOS appended to the end.\n\n        Returns:\n            vocabs: word vocab\n            x: input tensor containing numberized source tokens, with EOS at the\n                end if append_eos is true\n            src_lengths: and source lengths.\n        """"""\n        vocab = Dictionary()\n\n        vocab.add_symbol(""hello"")\n        vocab.add_symbol(""how"")\n        vocab.add_symbol(""are"")\n        vocab.add_symbol(""you"")\n        vocab.add_symbol(""new"")\n        vocab.add_symbol(""york"")\n        src_tokens = [\n            [""hello"", ""new"", ""york"", ""you""],\n            [""how"", ""are"", ""you"", ""new"", ""york""],\n        ]\n        x, src_lengths = self._convert_src_tokens_to_tensor(\n            vocab=vocab, src_tokens=src_tokens, append_eos=append_eos\n        )\n        return vocab, x, src_lengths\n\n    def _convert_src_tokens_to_tensor(\n        self, vocab: Dictionary, src_tokens: List[List[str]], append_eos: bool\n    ):\n        src_len = [len(x) for x in src_tokens]\n        # If we have to append EOS, we include EOS in counting src length\n        if append_eos:\n            src_len = [length + 1 for length in src_len]\n\n        x = torch.LongTensor(len(src_tokens), max(src_len)).fill_(vocab.pad())\n        for i in range(len(src_tokens)):\n            for j in range(len(src_tokens[i])):\n                x[i][j] = vocab.index(src_tokens[i][j])\n            if append_eos:\n                x[i][j + 1] = vocab.eos()\n\n        x = x.transpose(1, 0)\n        return x, torch.LongTensor(src_len)\n\n    def assert_eos_at_end(self, x, x_len, eos):\n        """"""Asserts last token of every sentence in x is EOS """"""\n        for i in range(len(x_len)):\n            self.assertEqual(\n                x[x_len[i] - 1][i],\n                eos,\n                (\n                    ""Expected eos (token id {eos}) at the end of sentence {i} ""\n                    ""but got {other} instead""\n                ).format(i=i, eos=eos, other=x[i][-1]),\n            )\n\n    def assert_word_dropout_correct(self, x, x_noised, x_len, l_noised):\n        # Expect only the first word (2 bpe tokens) of the first example\n        # was dropped out\n        self.assertEqual(x_len[0] - 2, l_noised[0])\n        for i in range(l_noised[0]):\n            self.assertEqual(x_noised[i][0], x[i + 2][0])\n\n    def test_word_dropout_with_eos(self):\n        vocab, x, x_len = self._get_test_data_with_bpe_cont_marker(append_eos=True)\n\n        with data_utils.numpy_seed(1234):\n            noising_gen = noising.WordDropout(vocab)\n            x_noised, l_noised = noising_gen.noising(x, x_len, 0.2)\n            self.assert_word_dropout_correct(\n                x=x, x_noised=x_noised, x_len=x_len, l_noised=l_noised\n            )\n            self.assert_eos_at_end(x=x_noised, x_len=l_noised, eos=vocab.eos())\n\n    def assert_word_blanking_correct(self, x, x_noised, x_len, l_noised, unk):\n        # Expect only the first word (2 bpe tokens) of the first example\n        # was blanked out\n        self.assertEqual(x_len[0], l_noised[0])\n        for i in range(l_noised[0]):\n            if i < 2:\n                self.assertEqual(x_noised[i][0], unk)\n            else:\n                self.assertEqual(x_noised[i][0], x[i][0])\n\n    def test_word_blank_with_eos(self):\n        vocab, x, x_len = self._get_test_data_with_bpe_cont_marker(append_eos=True)\n\n        with data_utils.numpy_seed(1234):\n            noising_gen = noising.WordDropout(vocab)\n            x_noised, l_noised = noising_gen.noising(x, x_len, 0.2, vocab.unk())\n            self.assert_word_blanking_correct(\n                x=x, x_noised=x_noised, x_len=x_len, l_noised=l_noised, unk=vocab.unk()\n            )\n            self.assert_eos_at_end(x=x_noised, x_len=l_noised, eos=vocab.eos())\n\n    def generate_unchanged_shuffle_map(self, length):\n        return {i: i for i in range(length)}\n\n    def assert_word_shuffle_matches_expected(\n        self,\n        x,\n        x_len,\n        max_shuffle_distance: int,\n        vocab: Dictionary,\n        expected_shufle_maps: List[Dict[int, int]],\n        expect_eos_at_end: bool,\n        bpe_end_marker=None,\n    ):\n        """"""\n        This verifies that with a given x, x_len, max_shuffle_distance, and\n        vocab, we get the expected shuffle result.\n\n        Args:\n            x: Tensor of shape (T x B) = (sequence_length, batch_size)\n            x_len: Tensor of length B = batch_size\n            max_shuffle_distance: arg to pass to noising\n            expected_shuffle_maps: List[mapping] where mapping is a\n                Dict[old_index, new_index], mapping x\'s elements from their\n                old positions in x to their new positions in x.\n            expect_eos_at_end: if True, check the output to make sure there is\n                an EOS at the end.\n            bpe_end_marker: str denoting the BPE end token. If this is not None, we\n                set the BPE cont token to None in the noising classes.\n        """"""\n        bpe_cont_marker = None\n        if bpe_end_marker is None:\n            bpe_cont_marker = ""@@""\n\n        with data_utils.numpy_seed(1234):\n            word_shuffle = noising.WordShuffle(\n                vocab, bpe_cont_marker=bpe_cont_marker, bpe_end_marker=bpe_end_marker\n            )\n            x_noised, l_noised = word_shuffle.noising(\n                x, x_len, max_shuffle_distance=max_shuffle_distance\n            )\n\n        # For every example, we have a different expected shuffle map. We check\n        # that each example is shuffled as expected according to each\n        # corresponding shuffle map.\n        for i in range(len(expected_shufle_maps)):\n            shuffle_map = expected_shufle_maps[i]\n            for k, v in shuffle_map.items():\n                self.assertEqual(x[k][i], x_noised[v][i])\n\n        # Shuffling should not affect the length of each example\n        for pre_shuffle_length, post_shuffle_length in zip(x_len, l_noised):\n            self.assertEqual(pre_shuffle_length, post_shuffle_length)\n        if expect_eos_at_end:\n            self.assert_eos_at_end(x=x_noised, x_len=l_noised, eos=vocab.eos())\n\n    def test_word_shuffle_with_eos(self):\n        vocab, x, x_len = self._get_test_data_with_bpe_cont_marker(append_eos=True)\n\n        # Assert word shuffle with max shuffle distance 0 causes input to be\n        # unchanged\n        self.assert_word_shuffle_matches_expected(\n            x=x,\n            x_len=x_len,\n            max_shuffle_distance=0,\n            vocab=vocab,\n            expected_shufle_maps=[\n                self.generate_unchanged_shuffle_map(example_len)\n                for example_len in x_len\n            ],\n            expect_eos_at_end=True,\n        )\n\n        # Assert word shuffle with max shuffle distance 3 matches our expected\n        # shuffle order\n        self.assert_word_shuffle_matches_expected(\n            x=x,\n            x_len=x_len,\n            vocab=vocab,\n            max_shuffle_distance=3,\n            expected_shufle_maps=[\n                self.generate_unchanged_shuffle_map(x_len[0]),\n                {0: 0, 1: 3, 2: 1, 3: 2},\n            ],\n            expect_eos_at_end=True,\n        )\n\n    def test_word_shuffle_with_eos_nonbpe(self):\n        """"""The purpose of this is to test shuffling logic with word vocabs""""""\n        vocab, x, x_len = self._get_test_data_with_word_vocab(append_eos=True)\n\n        # Assert word shuffle with max shuffle distance 0 causes input to be\n        # unchanged\n        self.assert_word_shuffle_matches_expected(\n            x=x,\n            x_len=x_len,\n            max_shuffle_distance=0,\n            vocab=vocab,\n            expected_shufle_maps=[\n                self.generate_unchanged_shuffle_map(example_len)\n                for example_len in x_len\n            ],\n            expect_eos_at_end=True,\n        )\n\n        # Assert word shuffle with max shuffle distance 3 matches our expected\n        # shuffle order\n        self.assert_word_shuffle_matches_expected(\n            x=x,\n            x_len=x_len,\n            vocab=vocab,\n            max_shuffle_distance=3,\n            expected_shufle_maps=[\n                {0: 0, 1: 1, 2: 3, 3: 2},\n                {0: 0, 1: 2, 2: 1, 3: 3, 4: 4},\n            ],\n            expect_eos_at_end=True,\n        )\n\n    def test_word_shuffle_without_eos(self):\n        """"""Same result as word shuffle with eos except no EOS at end""""""\n        vocab, x, x_len = self._get_test_data_with_bpe_cont_marker(append_eos=False)\n\n        # Assert word shuffle with max shuffle distance 0 causes input to be\n        # unchanged\n        self.assert_word_shuffle_matches_expected(\n            x=x,\n            x_len=x_len,\n            max_shuffle_distance=0,\n            vocab=vocab,\n            expected_shufle_maps=[\n                self.generate_unchanged_shuffle_map(example_len)\n                for example_len in x_len\n            ],\n            expect_eos_at_end=False,\n        )\n\n        # Assert word shuffle with max shuffle distance 3 matches our expected\n        # shuffle order\n        self.assert_word_shuffle_matches_expected(\n            x=x,\n            x_len=x_len,\n            vocab=vocab,\n            max_shuffle_distance=3,\n            expected_shufle_maps=[\n                self.generate_unchanged_shuffle_map(x_len[0]),\n                {0: 0, 1: 3, 2: 1, 3: 2},\n            ],\n            expect_eos_at_end=False,\n        )\n\n    def test_word_shuffle_without_eos_with_bpe_end_marker(self):\n        """"""Same result as word shuffle without eos except using BPE end token""""""\n        vocab, x, x_len = self._get_test_data_with_bpe_end_marker(append_eos=False)\n\n        # Assert word shuffle with max shuffle distance 0 causes input to be\n        # unchanged\n        self.assert_word_shuffle_matches_expected(\n            x=x,\n            x_len=x_len,\n            max_shuffle_distance=0,\n            vocab=vocab,\n            expected_shufle_maps=[\n                self.generate_unchanged_shuffle_map(example_len)\n                for example_len in x_len\n            ],\n            expect_eos_at_end=False,\n            bpe_end_marker=""_EOW"",\n        )\n\n        # Assert word shuffle with max shuffle distance 3 matches our expected\n        # shuffle order\n        self.assert_word_shuffle_matches_expected(\n            x=x,\n            x_len=x_len,\n            vocab=vocab,\n            max_shuffle_distance=3,\n            expected_shufle_maps=[\n                self.generate_unchanged_shuffle_map(x_len[0]),\n                {0: 0, 1: 3, 2: 1, 3: 2},\n            ],\n            expect_eos_at_end=False,\n            bpe_end_marker=""_EOW"",\n        )\n\n    def assert_no_eos_at_end(self, x, x_len, eos):\n        """"""Asserts that the last token of each sentence in x is not EOS """"""\n        for i in range(len(x_len)):\n            self.assertNotEqual(\n                x[x_len[i] - 1][i],\n                eos,\n                ""Expected no eos (token id {eos}) at the end of sentence {i}."".format(\n                    eos=eos, i=i\n                ),\n            )\n\n    def test_word_dropout_without_eos(self):\n        """"""Same result as word dropout with eos except no EOS at end""""""\n        vocab, x, x_len = self._get_test_data_with_bpe_cont_marker(append_eos=False)\n\n        with data_utils.numpy_seed(1234):\n            noising_gen = noising.WordDropout(vocab)\n            x_noised, l_noised = noising_gen.noising(x, x_len, 0.2)\n            self.assert_word_dropout_correct(\n                x=x, x_noised=x_noised, x_len=x_len, l_noised=l_noised\n            )\n            self.assert_no_eos_at_end(x=x_noised, x_len=l_noised, eos=vocab.eos())\n\n    def test_word_blank_without_eos(self):\n        """"""Same result as word blank with eos except no EOS at end""""""\n        vocab, x, x_len = self._get_test_data_with_bpe_cont_marker(append_eos=False)\n\n        with data_utils.numpy_seed(1234):\n            noising_gen = noising.WordDropout(vocab)\n            x_noised, l_noised = noising_gen.noising(x, x_len, 0.2, vocab.unk())\n            self.assert_word_blanking_correct(\n                x=x, x_noised=x_noised, x_len=x_len, l_noised=l_noised, unk=vocab.unk()\n            )\n            self.assert_no_eos_at_end(x=x_noised, x_len=l_noised, eos=vocab.eos())\n\n    def _get_noising_dataset_batch(\n        self, src_tokens_no_pad, src_dict, append_eos_to_tgt=False,\n    ):\n        """"""\n        Constructs a NoisingDataset and the corresponding\n        ``LanguagePairDataset(NoisingDataset(src), src)``. If\n        *append_eos_to_tgt* is True, wrap the source dataset in\n        :class:`TransformEosDataset` to append EOS to the clean source when\n        using it as the target.\n        """"""\n        src_dataset = test_utils.TestDataset(data=src_tokens_no_pad)\n\n        noising_dataset = noising.NoisingDataset(\n            src_dataset=src_dataset,\n            src_dict=src_dict,\n            seed=1234,\n            max_word_shuffle_distance=3,\n            word_dropout_prob=0.2,\n            word_blanking_prob=0.2,\n            noising_class=noising.UnsupervisedMTNoising,\n        )\n        tgt = src_dataset\n        language_pair_dataset = LanguagePairDataset(\n            src=noising_dataset, tgt=tgt, src_sizes=None, src_dict=src_dict\n        )\n        language_pair_dataset = TransformEosDataset(\n            language_pair_dataset, src_dict.eos(),\n            append_eos_to_tgt=append_eos_to_tgt,\n        )\n\n        dataloader = torch.utils.data.DataLoader(\n            dataset=language_pair_dataset,\n            batch_size=2,\n            collate_fn=language_pair_dataset.collater,\n        )\n        denoising_batch_result = next(iter(dataloader))\n        return denoising_batch_result\n\n    def test_noising_dataset_with_eos(self):\n        src_dict, src_tokens, _ = self._get_test_data_with_bpe_cont_marker(\n            append_eos=True\n        )\n\n        # Format data for src_dataset\n        src_tokens = torch.t(src_tokens)\n        src_tokens_no_pad = []\n        for src_sentence in src_tokens:\n            src_tokens_no_pad.append(\n                utils.strip_pad(tensor=src_sentence, pad=src_dict.pad())\n            )\n        denoising_batch_result = self._get_noising_dataset_batch(\n            src_tokens_no_pad=src_tokens_no_pad, src_dict=src_dict\n        )\n\n        eos, pad = src_dict.eos(), src_dict.pad()\n\n        # Generated noisy source as source\n        expected_src = torch.LongTensor(\n            [[4, 5, 10, 11, 8, 12, 13, eos], [pad, pad, pad, 6, 8, 9, 7, eos]]\n        )\n        # Original clean source as target (right-padded)\n        expected_tgt = torch.LongTensor(\n            [[4, 5, 10, 11, 8, 12, 13, eos], [6, 7, 8, 9, eos, pad, pad, pad]]\n        )\n        generated_src = denoising_batch_result[""net_input""][""src_tokens""]\n        tgt_tokens = denoising_batch_result[""target""]\n\n        self.assertTensorEqual(expected_src, generated_src)\n        self.assertTensorEqual(expected_tgt, tgt_tokens)\n\n    def test_noising_dataset_without_eos(self):\n        """"""\n        Similar to test noising dataset with eos except that we have to set\n        *append_eos_to_tgt* to ``True``.\n        """"""\n\n        src_dict, src_tokens, _ = self._get_test_data_with_bpe_cont_marker(\n            append_eos=False\n        )\n\n        # Format data for src_dataset\n        src_tokens = torch.t(src_tokens)\n        src_tokens_no_pad = []\n        for src_sentence in src_tokens:\n            src_tokens_no_pad.append(\n                utils.strip_pad(tensor=src_sentence, pad=src_dict.pad())\n            )\n        denoising_batch_result = self._get_noising_dataset_batch(\n            src_tokens_no_pad=src_tokens_no_pad,\n            src_dict=src_dict,\n            append_eos_to_tgt=True,\n        )\n\n        eos, pad = src_dict.eos(), src_dict.pad()\n\n        # Generated noisy source as source\n        expected_src = torch.LongTensor(\n            [[4, 5, 10, 11, 8, 12, 13], [pad, pad, pad, 6, 8, 9, 7]]\n        )\n        # Original clean source as target (right-padded)\n        expected_tgt = torch.LongTensor(\n            [[4, 5, 10, 11, 8, 12, 13, eos], [6, 7, 8, 9, eos, pad, pad, pad]]\n        )\n\n        generated_src = denoising_batch_result[""net_input""][""src_tokens""]\n        tgt_tokens = denoising_batch_result[""target""]\n\n        self.assertTensorEqual(expected_src, generated_src)\n        self.assertTensorEqual(expected_tgt, tgt_tokens)\n\n    def assertTensorEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_reproducibility.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nfrom io import StringIO\nimport json\nimport os\nimport tempfile\nimport unittest\n\nimport torch\n\nfrom . import test_binaries\n\n\nclass TestReproducibility(unittest.TestCase):\n\n    def _test_reproducibility(\n        self,\n        name,\n        extra_flags=None,\n        delta=0.0001,\n        resume_checkpoint='checkpoint1.pt',\n        max_epoch=3,\n    ):\n        def get_last_log_stats_containing_string(log_records, search_string):\n            for log_record in logs.records[::-1]:\n                if search_string in log_record.msg:\n                    return json.loads(log_record.msg)\n\n        if extra_flags is None:\n            extra_flags = []\n\n        with tempfile.TemporaryDirectory(name) as data_dir:\n            with self.assertLogs() as logs:\n                test_binaries.create_dummy_data(data_dir)\n                test_binaries.preprocess_translation_data(data_dir)\n\n            # train epochs 1 and 2 together\n            with self.assertLogs() as logs:\n                test_binaries.train_translation_model(\n                    data_dir, 'fconv_iwslt_de_en', [\n                        '--dropout', '0.0',\n                        '--log-format', 'json',\n                        '--log-interval', '1',\n                        '--max-epoch', str(max_epoch),\n                    ] + extra_flags,\n                )\n            train_log = get_last_log_stats_containing_string(logs.records, 'train_loss')\n            valid_log = get_last_log_stats_containing_string(logs.records, 'valid_loss')\n\n            # train epoch 2, resuming from previous checkpoint 1\n            os.rename(\n                os.path.join(data_dir, resume_checkpoint),\n                os.path.join(data_dir, 'checkpoint_last.pt'),\n            )\n            with self.assertLogs() as logs:\n                test_binaries.train_translation_model(\n                    data_dir, 'fconv_iwslt_de_en', [\n                        '--dropout', '0.0',\n                        '--log-format', 'json',\n                        '--log-interval', '1',\n                        '--max-epoch', str(max_epoch),\n                    ] + extra_flags,\n                )\n            train_res_log = get_last_log_stats_containing_string(logs.records, 'train_loss')\n            valid_res_log = get_last_log_stats_containing_string(logs.records, 'valid_loss')\n\n            for k in ['train_loss', 'train_ppl', 'train_num_updates', 'train_gnorm']:\n                self.assertAlmostEqual(float(train_log[k]), float(train_res_log[k]), delta=delta)\n            for k in ['valid_loss', 'valid_ppl', 'valid_num_updates', 'valid_best_loss']:\n                self.assertAlmostEqual(float(valid_log[k]), float(valid_res_log[k]), delta=delta)\n\n    def test_reproducibility(self):\n        self._test_reproducibility('test_reproducibility')\n\n    @unittest.skipIf(not torch.cuda.is_available(), 'test requires a GPU')\n    def test_reproducibility_fp16(self):\n        self._test_reproducibility('test_reproducibility_fp16', [\n            '--fp16',\n            '--fp16-init-scale', '4096',\n        ], delta=0.011)\n\n    @unittest.skipIf(not torch.cuda.is_available(), 'test requires a GPU')\n    def test_reproducibility_memory_efficient_fp16(self):\n        self._test_reproducibility('test_reproducibility_memory_efficient_fp16', [\n            '--memory-efficient-fp16',\n            '--fp16-init-scale', '4096',\n        ])\n\n    def test_mid_epoch_reproducibility(self):\n        self._test_reproducibility(\n            'test_mid_epoch_reproducibility',\n            ['--save-interval-updates', '3'],\n            resume_checkpoint='checkpoint_1_3.pt',\n            max_epoch=1,\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_resampling_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport unittest\n\nimport numpy as np\n\nfrom fairseq.data import ListDataset, ResamplingDataset\n\n\nclass TestResamplingDataset(unittest.TestCase):\n    def setUp(self):\n        self.strings = [""ab"", ""c"", ""def"", ""ghij""]\n        self.weights = [4.0, 2.0, 7.0, 1.5]\n        self.size_ratio = 2\n        self.dataset = ListDataset(\n            self.strings, np.array([len(s) for s in self.strings])\n        )\n\n    def _test_common(self, resampling_dataset, iters):\n        assert len(self.dataset) == len(self.strings) == len(self.weights)\n        assert len(resampling_dataset) == self.size_ratio * len(self.strings)\n\n        results = {""ordered_by_size"": True, ""max_distribution_diff"": 0.0}\n\n        totalfreqs = 0\n        freqs = collections.defaultdict(int)\n\n        for epoch_num in range(iters):\n            resampling_dataset.set_epoch(epoch_num)\n\n            indices = resampling_dataset.ordered_indices()\n            assert len(indices) == len(resampling_dataset)\n\n            prev_size = -1\n\n            for i in indices:\n                cur_size = resampling_dataset.size(i)\n                # Make sure indices map to same sequences within an epoch\n                assert resampling_dataset[i] == resampling_dataset[i]\n\n                # Make sure length of sequence is correct\n                assert cur_size == len(resampling_dataset[i])\n\n                freqs[resampling_dataset[i]] += 1\n                totalfreqs += 1\n\n                if prev_size > cur_size:\n                    results[""ordered_by_size""] = False\n\n                prev_size = cur_size\n\n        assert set(freqs.keys()) == set(self.strings)\n        for s, weight in zip(self.strings, self.weights):\n            freq = freqs[s] / totalfreqs\n            expected_freq = weight / sum(self.weights)\n            results[""max_distribution_diff""] = max(\n                results[""max_distribution_diff""], abs(expected_freq - freq)\n            )\n\n        return results\n\n    def test_resampling_dataset_batch_by_size_false(self):\n        resampling_dataset = ResamplingDataset(\n            self.dataset,\n            self.weights,\n            size_ratio=self.size_ratio,\n            batch_by_size=False,\n            seed=0,\n        )\n\n        results = self._test_common(resampling_dataset, iters=1000)\n\n        # For batch_by_size = False, the batches should be returned in\n        # arbitrary order of size.\n        assert not results[""ordered_by_size""]\n\n        # Allow tolerance in distribution error of 2%.\n        assert results[""max_distribution_diff""] < 0.02\n\n    def test_resampling_dataset_batch_by_size_true(self):\n        resampling_dataset = ResamplingDataset(\n            self.dataset,\n            self.weights,\n            size_ratio=self.size_ratio,\n            batch_by_size=True,\n            seed=0,\n        )\n\n        results = self._test_common(resampling_dataset, iters=1000)\n\n        # For batch_by_size = True, the batches should be returned in\n        # increasing order of size.\n        assert results[""ordered_by_size""]\n\n        # Allow tolerance in distribution error of 2%.\n        assert results[""max_distribution_diff""] < 0.02\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_sequence_generator.py,30,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport tempfile\nimport unittest\n\nimport tests.utils as test_utils\nimport torch\nfrom fairseq import search\nfrom fairseq.data.dictionary import Dictionary\n\nfrom fairseq.models.transformer import TransformerModel\nfrom fairseq.sequence_generator import SequenceGenerator, EnsembleModel\nfrom fairseq.tasks.fairseq_task import FairseqTask\n\n\nDEFAULT_TEST_VOCAB_SIZE = 100\n\n\nclass DummyTask(FairseqTask):\n    def __init__(self, args):\n        super().__init__(args)\n        self.dictionary = get_dummy_dictionary()\n        if getattr(self.args, ""ctc"", False):\n            self.dictionary.add_symbol(""<ctc_blank>"")\n        self.src_dict = self.dictionary\n        self.tgt_dict = self.dictionary\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\ndef get_dummy_dictionary(vocab_size=DEFAULT_TEST_VOCAB_SIZE):\n    dummy_dict = Dictionary()\n    # add dummy symbol to satisfy vocab size\n    for id, _ in enumerate(range(vocab_size)):\n        dummy_dict.add_symbol(""{}"".format(id), 1000)\n    return dummy_dict\n\n\ndef get_dummy_task_and_parser():\n    """"""\n    to build a fariseq model, we need some dummy parse and task. This function\n    is used to create dummy task and parser to faciliate model/criterion test\n\n    Note: we use FbSpeechRecognitionTask as the dummy task. You may want\n    to use other task by providing another function\n    """"""\n    parser = argparse.ArgumentParser(\n        description=""test_dummy_s2s_task"", argument_default=argparse.SUPPRESS\n    )\n    DummyTask.add_args(parser)\n    args = parser.parse_args([])\n    task = DummyTask.setup_task(args)\n    return task, parser\n\n\nclass TestJitSequenceGeneratorBase(unittest.TestCase):\n    def setUp(self):\n        self.task, self.parser = get_dummy_task_and_parser()\n        eos = self.task.tgt_dict.eos()\n        src_tokens = torch.randint(3, 50, (2, 10)).long()\n        src_tokens = torch.cat((src_tokens, torch.LongTensor([[eos], [eos]])), -1)\n        src_lengths = torch.LongTensor([2, 10])\n        self.sample = {\n            ""net_input"": {""src_tokens"": src_tokens, ""src_lengths"": src_lengths}\n        }\n        TransformerModel.add_args(self.parser)\n        args = self.parser.parse_args([])\n        args.encoder_layers = 2\n        args.decoder_layers = 1\n        self.transformer_model = TransformerModel.build_model(args, self.task)\n\n    def assertOutputEqual(self, hypo, pos_probs):\n        pos_scores = torch.FloatTensor(pos_probs).log()\n        self.assertTensorSizeEqual(hypo[""positional_scores""], pos_scores)\n        self.assertTensorSizeEqual(pos_scores.numel(), hypo[""tokens""].numel())\n\n    def assertTensorSizeEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertLess((t1 - t2).abs().max(), 1e-4)\n\n    def assertTensorEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n    def assertHypoEqual(self, h1, h2):\n        ""Check two hypos are equal""\n        self.assertTensorEqual(h1[""tokens""], h2[""tokens""])\n        self.assertAlmostEqual(h1[""positional_scores""], h2[""positional_scores""])\n        self.assertLess(abs(h1[""score""] - h2[""score""]), 1e-6)\n        self.assertAlmostEqual(h1[""attention""], h2[""attention""])\n\n    def _test_save_and_load(self, scripted_module):\n        with tempfile.NamedTemporaryFile() as f:\n            scripted_module.save(f.name)\n            torch.jit.load(f.name)\n\n\nclass TestJitSequeneceGenerator(TestJitSequenceGeneratorBase):\n\n    @unittest.skipIf(\n        torch.__version__ < ""1.6.0"", ""Targeting OSS scriptability for the 1.6 release""\n    )\n    def test_export_transformer(self):\n        model = self.transformer_model\n        torch.jit.script(model)\n\n    @unittest.skipIf(\n        torch.__version__ < ""1.6.0"", ""Targeting OSS scriptability for the 1.6 release""\n    )\n    def test_ensemble_sequence_generator(self):\n        model = self.transformer_model\n        generator = SequenceGenerator(\n            [model], self.task.tgt_dict, beam_size=2, no_repeat_ngram_size=2\n        )\n        scripted_model = torch.jit.script(generator)\n        self._test_save_and_load(scripted_model)\n\n\nclass TestJitEnsemble(TestJitSequenceGeneratorBase):\n\n    @unittest.skipIf(\n        torch.__version__ < ""1.6.0"", ""Targeting OSS scriptability for the 1.6 release""\n    )\n    def test_export_ensemble_model(self):\n        model = self.transformer_model\n        ensemble_models = EnsembleModel([model])\n        torch.jit.script(ensemble_models)\n\n\nclass TestExportSearch(unittest.TestCase):\n    def setUp(self):\n        task, _ = get_dummy_task_and_parser()\n        self.tgt_dict = task.tgt_dict\n        self.min_top1_prob = 0.4\n\n    def test_export_diverse_bs(self):\n        search_strategy = search.DiverseBeamSearch(\n            self.tgt_dict, num_groups=2, diversity_strength=0.0\n        )\n        torch.jit.script(search_strategy)\n\n    def test_export_sampling(self):\n        low_sampling_topp = self.min_top1_prob / 2.0\n        search_strategy = search.Sampling(\n            self.tgt_dict, sampling_topp=low_sampling_topp\n        )\n        torch.jit.script(search_strategy)\n\n    def test_export_diverse_siblings_search(self):\n        search_strategy = search.DiverseSiblingsSearch(\n            self.tgt_dict, diversity_rate=0.5\n        )\n        torch.jit.script(search_strategy)\n\n\nclass TestSequenceGeneratorBase(unittest.TestCase):\n    def assertHypoTokens(self, hypo, tokens):\n        self.assertTensorEqual(hypo[""tokens""], torch.LongTensor(tokens))\n\n    def assertHypoScore(self, hypo, pos_probs, normalized=True, lenpen=1.0):\n        pos_scores = torch.FloatTensor(pos_probs).log()\n        self.assertAlmostEqual(hypo[""positional_scores""], pos_scores)\n        self.assertEqual(pos_scores.numel(), hypo[""tokens""].numel())\n        score = pos_scores.sum()\n        if normalized:\n            score /= pos_scores.numel() ** lenpen\n        self.assertLess(abs(score - hypo[""score""]), 1e-6)\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertLess((t1 - t2).abs().max(), 1e-4)\n\n    def assertTensorEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n\nclass TestSequeneceGenerator(TestSequenceGeneratorBase):\n    def setUp(self):\n        self.tgt_dict, self.w1, self.w2, src_tokens, src_lengths, self.model = (\n            test_utils.sequence_generator_setup()\n        )\n        self.sample = {\n            ""net_input"": {""src_tokens"": src_tokens, ""src_lengths"": src_lengths}\n        }\n\n    def test_with_normalization(self):\n        generator = SequenceGenerator([self.model], self.tgt_dict, beam_size=2)\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 1.0])\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w2, w1, w2, eos])\n        self.assertHypoScore(hypos[0][1], [0.1, 0.9, 0.9, 1.0])\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, w1, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.4, 1.0])\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][1], [0.7, 0.4, 0.6])\n\n    def test_without_normalization(self):\n        # Sentence 1: unchanged from the normalized case\n        # Sentence 2: beams swap order\n        generator = SequenceGenerator(\n            [self.model], self.tgt_dict, beam_size=2, normalize_scores=False\n        )\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 1.0], normalized=False)\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w2, w1, w2, eos])\n        self.assertHypoScore(hypos[0][1], [0.1, 0.9, 0.9, 1.0], normalized=False)\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.6], normalized=False)\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w2, w1, eos])\n        self.assertHypoScore(hypos[1][1], [0.7, 0.4, 0.4, 1.0], normalized=False)\n\n    def test_with_lenpen_favoring_short_hypos(self):\n        lenpen = 0.6\n        generator = SequenceGenerator(\n            [self.model], self.tgt_dict, beam_size=2, len_penalty=lenpen\n        )\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 1.0], lenpen=lenpen)\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w2, w1, w2, eos])\n        self.assertHypoScore(hypos[0][1], [0.1, 0.9, 0.9, 1.0], lenpen=lenpen)\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.6], lenpen=lenpen)\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w2, w1, eos])\n        self.assertHypoScore(hypos[1][1], [0.7, 0.4, 0.4, 1.0], lenpen=lenpen)\n\n    def test_with_lenpen_favoring_long_hypos(self):\n        lenpen = 5.0\n        generator = SequenceGenerator(\n            [self.model], self.tgt_dict, beam_size=2, len_penalty=lenpen\n        )\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w2, w1, w2, eos])\n        self.assertHypoScore(hypos[0][0], [0.1, 0.9, 0.9, 1.0], lenpen=lenpen)\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w1, eos])\n        self.assertHypoScore(hypos[0][1], [0.9, 1.0], lenpen=lenpen)\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, w1, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.4, 1.0], lenpen=lenpen)\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][1], [0.7, 0.4, 0.6], lenpen=lenpen)\n\n    def test_maxlen(self):\n        generator = SequenceGenerator([self.model], self.tgt_dict, beam_size=2, max_len_b=2)\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 1.0])\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w2, w2, eos])\n        self.assertHypoScore(hypos[0][1], [0.1, 0.1, 0.6])\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.6])\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w2, w2, eos])\n        self.assertHypoScore(hypos[1][1], [0.3, 0.9, 0.01])\n\n    def test_encoder_with_different_output_len(self):\n        args = self.model.encoder.args\n        task = test_utils.TestTranslationTask.setup_task(args, self.tgt_dict, self.tgt_dict)\n        reshaping_model = test_utils.TestReshapingModel.build_model(args, task)\n        generator = SequenceGenerator([reshaping_model], self.tgt_dict, beam_size=2, max_len_b=2)\n        hypos = generator.forward(self.sample)\n        for sent in [0, 1]:\n            for beam in [0, 1]:\n                assert hypos[sent][beam][\'attention\'] is not None\n\n    def test_generation_with_additional_input(self):\n        args = self.model.encoder.args\n        task = test_utils.TestTranslationTask.setup_task(args, self.tgt_dict, self.tgt_dict)\n        add_input_model = test_utils.TestAdditionalInputModel.build_model(args, task)\n        generator = SequenceGenerator([add_input_model], self.tgt_dict, beam_size=2)\n        sample = self.sample.copy()\n        sample[\'net_input\'][\'fancy_other_input\'] = sample[\'net_input\'][\'src_tokens\']\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 1.0])\n\n\nclass TestDiverseBeamSearch(TestSequenceGeneratorBase):\n\n    def setUp(self):\n        # construct dummy dictionary\n        d = test_utils.dummy_dictionary(vocab_size=2)\n        self.assertEqual(d.pad(), 1)\n        self.assertEqual(d.eos(), 2)\n        self.assertEqual(d.unk(), 3)\n        self.eos = d.eos()\n        self.w1 = 4\n        self.w2 = 5\n\n        # construct source data\n        self.src_tokens = torch.LongTensor([\n            [self.w1, self.w2, self.eos],\n            [self.w1, self.w2, self.eos],\n        ])\n        self.src_lengths = torch.LongTensor([2, 2])\n\n        args = argparse.Namespace()\n        unk = 0.\n        args.beam_probs = [\n            # step 0:\n            torch.FloatTensor([\n                # eos      w1   w2\n                # sentence 1:\n                [0.0, unk, 0.9, 0.1],  # beam 1\n                [0.0, unk, 0.9, 0.1],  # beam 2\n                # sentence 2:\n                [0.0, unk, 0.7, 0.3],\n                [0.0, unk, 0.7, 0.3],\n            ]),\n            # step 1:\n            torch.FloatTensor([\n                # eos      w1   w2\n                # sentence 1:\n                [0.0, unk, 0.6, 0.4],\n                [0.0, unk, 0.6, 0.4],\n                # sentence 2:\n                [0.25, unk, 0.35, 0.4],\n                [0.25, unk, 0.35, 0.4],\n            ]),\n            # step 2:\n            torch.FloatTensor([\n                # eos      w1   w2\n                # sentence 1:\n                [1.0, unk, 0.0, 0.0],\n                [1.0, unk, 0.0, 0.0],\n                # sentence 2:\n                [0.9, unk, 0.1, 0.0],\n                [0.9, unk, 0.1, 0.0],\n            ]),\n        ]\n\n        task = test_utils.TestTranslationTask.setup_task(args, d, d)\n        self.model = task.build_model(args)\n        self.tgt_dict = task.target_dictionary\n\n    def test_diverse_beam_search(self):\n        search_strategy = search.DiverseBeamSearch(self.tgt_dict, num_groups=2, diversity_strength=0.)\n        generator = SequenceGenerator(\n            [self.model], self.tgt_dict, beam_size=2, search_strategy=search_strategy,\n        )\n        sample = {\'net_input\': {\'src_tokens\': self.src_tokens, \'src_lengths\': self.src_lengths}}\n        hypos = generator.forward(sample)\n        eos, w1, w2 = self.eos, self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 0.6, 1.0])\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w1, w1, eos])\n        self.assertHypoScore(hypos[0][1], [0.9, 0.6, 1.0])\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.9])\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][1], [0.7, 0.4, 0.9])\n\n\nclass TestDiverseSiblingsSearch(TestDiverseBeamSearch):\n    def assertHypoScore(\n        self, hypo, pos_probs, sibling_rank, diversity_rate, normalized=True, lenpen=1.0\n    ):\n        pos_scores = torch.FloatTensor(pos_probs).log()\n        pos_scores.sub_(torch.Tensor(sibling_rank) * diversity_rate)\n        self.assertAlmostEqual(hypo[""positional_scores""], pos_scores)\n        self.assertEqual(pos_scores.numel(), hypo[""tokens""].numel())\n        score = pos_scores.sum()\n        if normalized:\n            score /= pos_scores.numel() ** lenpen\n        self.assertLess(abs(score - hypo[""score""]), 1e-6)\n\n    def test_diverse_beam_search(self):\n        search_strategy = search.DiverseSiblingsSearch(\n            self.tgt_dict, diversity_rate=0.5\n        )\n        generator = SequenceGenerator(\n            [self.model], self.tgt_dict, beam_size=2, search_strategy=search_strategy\n        )\n        sample = {\n            ""net_input"": {\n                ""src_tokens"": self.src_tokens,\n                ""src_lengths"": self.src_lengths,\n            }\n        }\n        hypos = generator.forward(sample)\n        eos, w1, w2 = self.eos, self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 0.6, 1.0], [0, 1, 1], 0.5)\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w1, w2, eos])\n        self.assertHypoScore(hypos[0][1], [0.9, 0.4, 1.0], [0, 2, 1], 0.5)\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.9], [0, 1, 1], 0.5)\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w1, eos])\n        self.assertHypoScore(hypos[1][1], [0.7, 0.35, 0.9], [0, 2, 1], 0.5)\n\n\nclass TestTopPSamplingSearch(TestSequenceGeneratorBase):\n\n    def setUp(self):\n        # construct dummy dictionary\n        d = test_utils.dummy_dictionary(vocab_size=2)\n        self.assertEqual(d.pad(), 1)\n        self.assertEqual(d.eos(), 2)\n        self.assertEqual(d.unk(), 3)\n        self.eos = d.eos()\n        self.w1 = 4\n        self.w2 = 5\n\n        # construct source data\n        self.src_tokens = torch.LongTensor([\n            [self.w1, self.w2, self.eos],\n            [self.w1, self.w2, self.eos],\n        ])\n        self.src_lengths = torch.LongTensor([2, 2])\n\n        args = argparse.Namespace()\n        unk = 0.\n        # The minimal probability of top 2 tokens.\n        self.min_top2_prob = 0.75\n        # The minimal probability of the top 1 token.\n        self.min_top1_prob = 0.4\n\n        w1_prob = self.min_top1_prob\n        w2_prob = self.min_top2_prob - self.min_top1_prob\n        eos_prob = 1 - self.min_top2_prob\n\n        args.beam_probs = [\n            # step 0:\n            torch.FloatTensor([\n                # eos      w1   w2\n                [0.0, unk, 1.0, 0.0],\n                [0.0, unk, 1.0, 0.0],\n                [0.0, unk, 1.0, 0.0],\n                [0.0, unk, 1.0, 0.0],\n            ]),\n            # step 1:\n            torch.FloatTensor([\n                # eos           w1       w2\n                [eos_prob, unk, w1_prob, w2_prob],\n                [eos_prob, unk, w1_prob, w2_prob],\n                [eos_prob, unk, w1_prob, w2_prob],\n                [eos_prob, unk, w1_prob, w2_prob],\n            ]),\n            # step 2:\n            torch.FloatTensor([\n                # eos      w1   w2\n                [1.0, unk, 0.0, 0.0],\n                [1.0, unk, 0.0, 0.0],\n                [1.0, unk, 0.0, 0.0],\n                [1.0, unk, 0.0, 0.0],\n            ]),\n        ]\n\n        task = test_utils.TestTranslationTask.setup_task(args, d, d)\n        self.model = task.build_model(args)\n        self.tgt_dict = task.target_dictionary\n\n    def test_topp_sampling_search_low_prob(self):\n        # Given a prob low enough to top-P sampling, we expect only the top\n        # 1 token to be sampled, which always results in the same output.\n        low_sampling_topp = self.min_top1_prob/2.0\n        search_strategy = search.Sampling(self.tgt_dict, sampling_topp=low_sampling_topp)\n        generator = SequenceGenerator(\n            [self.model], self.tgt_dict, beam_size=2, search_strategy=search_strategy)\n        sample = {\n            \'net_input\': {\n                \'src_tokens\': self.src_tokens,\n                \'src_lengths\': self.src_lengths\n            }\n        }\n        hypos = generator.forward(sample)\n        eos, w1 = self.eos, self.w1\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, w1, eos])\n        self.assertHypoScore(hypos[0][0], [1.0, 0.4, 1.0])\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w1, w1, eos])\n        self.assertHypoScore(hypos[0][1], [1.0, 0.4, 1.0])\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w1, eos])\n        self.assertHypoScore(hypos[1][0], [1.0, 0.4, 1.0])\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w1, eos])\n        self.assertHypoScore(hypos[1][1], [1.0, 0.4, 1.0])\n\n    def test_topp_sampling_search_high_prob(self):\n        # Given a prob high enough to top-P sampling, any of the top 2\n        # tokens could be sampled. This can cause different outputs.\n        high_sampling_topp = (self.min_top1_prob+self.min_top2_prob)/2.0\n        search_strategy = search.Sampling(self.tgt_dict, sampling_topp=high_sampling_topp)\n        generator = SequenceGenerator(\n            [self.model], self.tgt_dict, beam_size=2, search_strategy=search_strategy)\n        sample = {\n            \'net_input\': {\n                \'src_tokens\': self.src_tokens,\n                \'src_lengths\': self.src_lengths\n            }\n        }\n        hypos = generator.forward(sample)\n        eos, w1, w2 = self.eos, self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertTrue(self.hypoTokens(hypos[0][0], [w1, w1, eos]) or\n                        self.hypoTokens(hypos[0][0], [w1, w2, eos]))\n        self.assertTrue(self.hypoScore(hypos[0][0], [1.0, 0.4, 1.0]) or\n                        self.hypoScore(hypos[0][0], [1.0, 0.35, 1.0]))\n\n        # sentence 1, beam 2\n        self.assertTrue(self.hypoTokens(hypos[0][1], [w1, w1, eos]) or\n                        self.hypoTokens(hypos[0][1], [w1, w2, eos]))\n        self.assertTrue(self.hypoScore(hypos[0][1], [1.0, 0.4, 1.0]) or\n                        self.hypoScore(hypos[0][1], [1.0, 0.35, 1.0]))\n\n        # sentence 2, beam 1\n        self.assertTrue(self.hypoTokens(hypos[1][0], [w1, w1, eos]) or\n                        self.hypoTokens(hypos[1][0], [w1, w2, eos]))\n        self.assertTrue(self.hypoScore(hypos[1][0], [1.0, 0.4, 1.0]) or\n                        self.hypoScore(hypos[1][0], [1.0, 0.35, 1.0]))\n\n        # sentence 2, beam 2\n        self.assertTrue(self.hypoTokens(hypos[1][1], [w1, w1, eos]) or\n                        self.hypoTokens(hypos[1][1], [w1, w2, eos]))\n        self.assertTrue(self.hypoScore(hypos[1][1], [1.0, 0.4, 1.0]) or\n                        self.hypoScore(hypos[1][1], [1.0, 0.35, 1.0]))\n\n    def hypoTokens(self, hypo, tokens):\n        return self.tensorEqual(hypo[\'tokens\'], torch.LongTensor(tokens))\n\n    def hypoScore(self, hypo, pos_probs, normalized=True, lenpen=1.):\n        pos_scores = torch.FloatTensor(pos_probs).log()\n        if not self.almostEqual(hypo[\'positional_scores\'], pos_scores):\n            return False\n        if pos_scores.numel() != hypo[\'tokens\'].numel():\n            return False\n        score = pos_scores.sum()\n        if normalized:\n            score /= pos_scores.numel() ** lenpen\n        return abs(score - hypo[\'score\']) < 1e-6\n\n    def almostEqual(self, t1, t2):\n        return t1.size() == t2.size() and (t1 - t2).abs().max() < 1e-4\n\n    def tensorEqual(self, t1, t2):\n        return t1.size() == t2.size() and t1.ne(t2).long().sum() == 0\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_sequence_scorer.py,12,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport unittest\n\nimport torch\n\nfrom fairseq.sequence_scorer import SequenceScorer\n\nimport tests.utils as test_utils\n\n\nclass TestSequenceScorer(unittest.TestCase):\n\n    def test_sequence_scorer(self):\n        # construct dummy dictionary\n        d = test_utils.dummy_dictionary(vocab_size=2)\n        self.assertEqual(d.pad(), 1)\n        self.assertEqual(d.eos(), 2)\n        self.assertEqual(d.unk(), 3)\n        eos = d.eos()\n        w1 = 4\n        w2 = 5\n\n        # construct dataloader\n        data = [\n            {\n                \'source\': torch.LongTensor([w1, w2, eos]),\n                \'target\': torch.LongTensor([w1, w2, w1, eos]),\n            },\n            {\n                \'source\': torch.LongTensor([w2, eos]),\n                \'target\': torch.LongTensor([w2, w1, eos]),\n            },\n            {\n                \'source\': torch.LongTensor([w2, eos]),\n                \'target\': torch.LongTensor([w2, eos]),\n            },\n        ]\n        data_itr = test_utils.dummy_dataloader(data)\n\n        # specify expected output probabilities\n        args = argparse.Namespace()\n        unk = 0.\n        args.beam_probs = [\n            # step 0:\n            torch.FloatTensor([\n                # eos      w1   w2\n                [0.0, unk, 0.6, 0.4],  # sentence 1\n                [0.0, unk, 0.4, 0.6],  # sentence 2\n                [0.0, unk, 0.7, 0.3],  # sentence 3\n            ]),\n            # step 1:\n            torch.FloatTensor([\n                # eos      w1   w2\n                [0.0, unk, 0.2, 0.7],  # sentence 1\n                [0.0, unk, 0.8, 0.2],  # sentence 2\n                [0.7, unk, 0.1, 0.2],  # sentence 3\n            ]),\n            # step 2:\n            torch.FloatTensor([\n                # eos       w1    w2\n                [0.10, unk, 0.50, 0.4],  # sentence 1\n                [0.15, unk, 0.15, 0.7],  # sentence 2\n                [0.00, unk, 0.00, 0.0],  # sentence 3\n            ]),\n            # step 3:\n            torch.FloatTensor([\n                # eos      w1    w2\n                [0.9, unk, 0.05, 0.05],  # sentence 1\n                [0.0, unk, 0.00, 0.0],  # sentence 2\n                [0.0, unk, 0.00, 0.0],  # sentence 3\n            ]),\n        ]\n        expected_scores = [\n            [0.6, 0.7, 0.5, 0.9],  # sentence 1\n            [0.6, 0.8, 0.15],  # sentence 2\n            [0.3, 0.7],  # sentence 3\n        ]\n\n        task = test_utils.TestTranslationTask.setup_task(args, d, d)\n        model = task.build_model(args)\n        scorer = SequenceScorer(task.target_dictionary)\n        for sample in data_itr:\n            hypos = task.inference_step(scorer, [model], sample)\n            for id, hypos_id in zip(sample[\'id\'].tolist(), hypos):\n                self.assertHypoTokens(hypos_id[0], data[id][\'target\'])\n                self.assertHypoScore(hypos_id[0], expected_scores[id])\n\n    def assertHypoTokens(self, hypo, tokens):\n        self.assertTensorEqual(hypo[\'tokens\'], torch.LongTensor(tokens))\n\n    def assertHypoScore(self, hypo, pos_probs, normalized=True, lenpen=1.):\n        pos_scores = torch.FloatTensor(pos_probs).log()\n        self.assertAlmostEqual(hypo[\'positional_scores\'], pos_scores)\n        self.assertEqual(pos_scores.numel(), hypo[\'tokens\'].numel())\n        score = pos_scores.sum()\n        if normalized:\n            score /= pos_scores.numel()**lenpen\n        self.assertLess(abs(score - hypo[\'score\']), 1e-6)\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertLess((t1 - t2).abs().max(), 1e-4)\n\n    def assertTensorEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_sparse_multihead_attention.py,5,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport unittest\nfrom fairseq.modules.sparse_multihead_attention import SparseMultiheadAttention\n\n\nclass TestSparseMultiheadAttention(unittest.TestCase):\n    def test_sparse_multihead_attention(self):\n        attn_weights = torch.randn(1, 8, 8)\n        bidirectional_sparse_mask = torch.tensor([\n                [0, 0, 0, 0, 0, float('-inf'), float('-inf'), 0],\n                [0, 0, 0, 0, 0, float('-inf'), float('-inf'), 0],\n                [0, 0, 0, 0, 0, float('-inf'), float('-inf'), 0],\n                [0, 0, 0, 0, 0, float('-inf'), float('-inf'), 0],\n                [float('-inf'), float('-inf'), float('-inf'), 0, 0, 0, 0, 0],\n                [float('-inf'), float('-inf'), float('-inf'), 0, 0, 0, 0, 0],\n                [float('-inf'), float('-inf'), float('-inf'), 0, 0, 0, 0, 0],\n                [float('-inf'), float('-inf'), float('-inf'), 0, 0, 0, 0, 0]\n            ])\n\n        bidirectional_attention = SparseMultiheadAttention(16, 1, stride=4, expressivity=1, is_bidirectional=True)\n        bidirectional_attention_sparse_mask = bidirectional_attention.buffered_sparse_mask(attn_weights, 8, 8)\n        torch.all(torch.eq(bidirectional_attention_sparse_mask, bidirectional_sparse_mask))\n\n        sparse_mask = torch.tensor([\n                [0, float('-inf'), float('-inf'), float('-inf'), float('-inf'), float('-inf'),\n                 float('-inf'), float('-inf')],\n                [0, 0, float('-inf'), float('-inf'), float('-inf'), float('-inf'), float('-inf'), float('-inf')],\n                [0, 0, 0, float('-inf'), float('-inf'), float('-inf'), float('-inf'), float('-inf')],\n                [0, 0, 0, 0, float('-inf'), float('-inf'), float('-inf'), float('-inf')],\n                [0, 0, 0, 0, 0, float('-inf'), float('-inf'), float('-inf')],\n                [float('-inf'), float('-inf'), float('-inf'), 0, 0, 0, float('-inf'), float('-inf')],\n                [float('-inf'), float('-inf'), float('-inf'), 0, 0, 0, 0, float('-inf')],\n                [float('-inf'), float('-inf'), float('-inf'), 0, 0, 0, 0, 0],\n            ])\n\n        attention = SparseMultiheadAttention(16, 1, stride=4, expressivity=1, is_bidirectional=False)\n        attention_sparse_mask = attention.buffered_sparse_mask(attn_weights, 8, 8)\n\n        torch.all(torch.eq(attention_sparse_mask, sparse_mask))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_token_block_dataset.py,16,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\n\nfrom fairseq.data import TokenBlockDataset\n\nimport tests.utils as test_utils\n\n\nclass TestTokenBlockDataset(unittest.TestCase):\n\n    def _build_dataset(self, data, **kwargs):\n        sizes = [len(x) for x in data]\n        underlying_ds = test_utils.TestDataset(data)\n        return TokenBlockDataset(underlying_ds, sizes, **kwargs)\n\n    def test_eos_break_mode(self):\n        data = [\n            torch.tensor([5, 4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([1], dtype=torch.long),\n            torch.tensor([8, 7, 6, 1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(data, block_size=None, pad=0, eos=1, break_mode=\'eos\')\n        self.assertEqual(ds[0].tolist(), [5, 4, 3, 2, 1])\n        self.assertEqual(ds[1].tolist(), [1])\n        self.assertEqual(ds[2].tolist(), [8, 7, 6, 1])\n\n        data = [\n            torch.tensor([5, 4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([8, 7, 6, 1], dtype=torch.long),\n            torch.tensor([1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(data, block_size=None, pad=0, eos=1, break_mode=\'eos\')\n        self.assertEqual(ds[0].tolist(), [5, 4, 3, 2, 1])\n        self.assertEqual(ds[1].tolist(), [8, 7, 6, 1])\n        self.assertEqual(ds[2].tolist(), [1])\n\n    def test_block_break_mode(self):\n        data = [\n            torch.tensor([5, 4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([8, 7, 6, 1], dtype=torch.long),\n            torch.tensor([9, 1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(data, block_size=3, pad=0, eos=1, break_mode=\'none\')\n        self.assertEqual(ds[0].tolist(), [5, 4, 3])\n        self.assertEqual(ds[1].tolist(), [2, 1, 8])\n        self.assertEqual(ds[2].tolist(), [7, 6, 1])\n        self.assertEqual(ds[3].tolist(), [9, 1])\n\n    def test_complete_break_mode(self):\n        data = [\n            torch.tensor([5, 4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([8, 7, 6, 1], dtype=torch.long),\n            torch.tensor([9, 1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(data, block_size=6, pad=0, eos=1, break_mode=\'complete\')\n        self.assertEqual(ds[0].tolist(), [5, 4, 3, 2, 1])\n        self.assertEqual(ds[1].tolist(), [8, 7, 6, 1, 9, 1])\n\n        data = [\n            torch.tensor([4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([5, 1], dtype=torch.long),\n            torch.tensor([1], dtype=torch.long),\n            torch.tensor([6, 1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(data, block_size=3, pad=0, eos=1, break_mode=\'complete\')\n        self.assertEqual(ds[0].tolist(), [4, 3, 2, 1])\n        self.assertEqual(ds[1].tolist(), [5, 1, 1])\n        self.assertEqual(ds[2].tolist(), [6, 1])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_train.py,1,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nfrom io import StringIO\nimport unittest\nfrom unittest.mock import MagicMock, patch\n\nimport torch\n\nfrom fairseq import data, checkpoint_utils\n\n\ndef mock_trainer(epoch, num_updates, iterations_in_epoch):\n    trainer = MagicMock()\n    trainer.load_checkpoint.return_value = {\n        'train_iterator': {\n            'epoch': epoch,\n            'iterations_in_epoch': iterations_in_epoch,\n            'shuffle': False,\n        },\n    }\n    trainer.get_num_updates.return_value = num_updates\n    return trainer\n\n\ndef mock_dict():\n    d = MagicMock()\n    d.pad.return_value = 1\n    d.eos.return_value = 2\n    d.unk.return_value = 3\n    return d\n\n\ndef get_trainer_and_epoch_itr(epoch, epoch_size, num_updates, iterations_in_epoch):\n    tokens = torch.LongTensor(list(range(epoch_size))).view(1, -1)\n    tokens_ds = data.TokenBlockDataset(\n        tokens, sizes=[tokens.size(-1)], block_size=1, pad=0, eos=1, include_targets=False,\n    )\n    trainer = mock_trainer(epoch, num_updates, iterations_in_epoch)\n    dataset = data.LanguagePairDataset(tokens_ds, tokens_ds.sizes, mock_dict(), shuffle=False)\n    epoch_itr = data.EpochBatchIterator(\n        dataset=dataset,\n        collate_fn=dataset.collater,\n        batch_sampler=[[i] for i in range(epoch_size)],\n    )\n    return trainer, epoch_itr\n\n\nclass TestLoadCheckpoint(unittest.TestCase):\n\n    def setUp(self):\n        self.args_mock = MagicMock()\n        self.args_mock.optimizer_overrides = '{}'\n        self.args_mock.reset_dataloader = False\n        self.args_mock.reset_meters = False\n        self.args_mock.reset_optimizer = False\n        self.patches = {\n            'os.makedirs': MagicMock(),\n            'os.path.join': MagicMock(),\n            'os.path.isfile': MagicMock(return_value=True),\n            'os.path.isabs': MagicMock(return_value=False),\n        }\n        self.applied_patches = [patch(p, d) for p, d in self.patches.items()]\n        [p.start() for p in self.applied_patches]\n\n    def test_load_partial_checkpoint(self):\n        with contextlib.redirect_stdout(StringIO()):\n            trainer, epoch_itr = get_trainer_and_epoch_itr(2, 150, 200, 50)\n            trainer.get_train_iterator = MagicMock(return_value=epoch_itr)\n\n            _, epoch_itr = checkpoint_utils.load_checkpoint(self.args_mock, trainer)\n\n            self.assertEqual(epoch_itr.epoch, 2)\n            self.assertEqual(epoch_itr.iterations_in_epoch, 50)\n\n            itr = epoch_itr.next_epoch_itr(shuffle=False)\n            self.assertEqual(epoch_itr.epoch, 2)\n            self.assertEqual(epoch_itr.iterations_in_epoch, 50)\n\n            self.assertEqual(next(itr)['net_input']['src_tokens'][0].item(), 50)\n            self.assertEqual(epoch_itr.iterations_in_epoch, 51)\n\n            for _ in range(150 - 52):\n                next(itr)\n            self.assertEqual(epoch_itr.iterations_in_epoch, 149)\n            self.assertTrue(itr.has_next())\n            next(itr)\n            self.assertFalse(itr.has_next())\n\n            itr = epoch_itr.next_epoch_itr(shuffle=False)\n            self.assertTrue(itr.has_next())\n            self.assertEqual(epoch_itr.epoch, 3)\n            self.assertEqual(epoch_itr.iterations_in_epoch, 0)\n\n    def test_load_full_checkpoint(self):\n        with contextlib.redirect_stdout(StringIO()):\n            trainer, epoch_itr = get_trainer_and_epoch_itr(2, 150, 300, 150)\n            trainer.get_train_iterator = MagicMock(return_value=epoch_itr)\n\n            _, epoch_itr = checkpoint_utils.load_checkpoint(self.args_mock, trainer)\n            itr = epoch_itr.next_epoch_itr(shuffle=False)\n\n            self.assertEqual(epoch_itr.epoch, 3)\n            self.assertEqual(epoch_itr.iterations_in_epoch, 0)\n            self.assertEqual(next(itr)['net_input']['src_tokens'][0].item(), 0)\n\n    def test_load_no_checkpoint(self):\n        with contextlib.redirect_stdout(StringIO()):\n            trainer, epoch_itr = get_trainer_and_epoch_itr(1, 150, 0, 0)\n            trainer.get_train_iterator = MagicMock(return_value=epoch_itr)\n            self.patches['os.path.isfile'].return_value = False\n\n            _, epoch_itr = checkpoint_utils.load_checkpoint(self.args_mock, trainer)\n            itr = epoch_itr.next_epoch_itr(shuffle=False)\n\n            self.assertEqual(epoch_itr.epoch, 1)\n            self.assertEqual(epoch_itr.iterations_in_epoch, 0)\n            self.assertEqual(next(itr)['net_input']['src_tokens'][0].item(), 0)\n\n    def tearDown(self):\n        patch.stopall()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_utils.py,13,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\n\nfrom fairseq import utils\n\n\nclass TestUtils(unittest.TestCase):\n\n    def test_convert_padding_direction(self):\n        pad = 1\n        left_pad = torch.LongTensor([\n            [2, 3, 4, 5, 6],\n            [1, 7, 8, 9, 10],\n            [1, 1, 1, 11, 12],\n        ])\n        right_pad = torch.LongTensor([\n            [2, 3, 4, 5, 6],\n            [7, 8, 9, 10, 1],\n            [11, 12, 1, 1, 1],\n        ])\n\n        self.assertAlmostEqual(\n            right_pad,\n            utils.convert_padding_direction(\n                left_pad,\n                pad,\n                left_to_right=True,\n            ),\n        )\n        self.assertAlmostEqual(\n            left_pad,\n            utils.convert_padding_direction(\n                right_pad,\n                pad,\n                right_to_left=True,\n            ),\n        )\n\n    def test_make_positions(self):\n        pad = 1\n        left_pad_input = torch.LongTensor([\n            [9, 9, 9, 9, 9],\n            [1, 9, 9, 9, 9],\n            [1, 1, 1, 9, 9],\n        ])\n        left_pad_output = torch.LongTensor([\n            [2, 3, 4, 5, 6],\n            [1, 2, 3, 4, 5],\n            [1, 1, 1, 2, 3],\n        ])\n        right_pad_input = torch.LongTensor([\n            [9, 9, 9, 9, 9],\n            [9, 9, 9, 9, 1],\n            [9, 9, 1, 1, 1],\n        ])\n        right_pad_output = torch.LongTensor([\n            [2, 3, 4, 5, 6],\n            [2, 3, 4, 5, 1],\n            [2, 3, 1, 1, 1],\n        ])\n\n        self.assertAlmostEqual(\n            left_pad_output,\n            utils.make_positions(left_pad_input, pad),\n        )\n        self.assertAlmostEqual(\n            right_pad_output,\n            utils.make_positions(right_pad_input, pad),\n        )\n\n    def test_clip_grad_norm_(self):\n        params = torch.nn.Parameter(torch.zeros(5)).requires_grad_(False)\n        grad_norm = utils.clip_grad_norm_(params, 1.0)\n        self.assertTrue(torch.is_tensor(grad_norm))\n        self.assertEqual(grad_norm, 0.0)\n\n        params = [torch.nn.Parameter(torch.zeros(5)) for i in range(3)]\n        for p in params:\n            p.grad = torch.full((5,), fill_value=2)\n        grad_norm = utils.clip_grad_norm_(params, 1.0)\n        exp_grad_norm = torch.full((15,), fill_value=2).norm()\n        self.assertTrue(torch.is_tensor(grad_norm))\n        self.assertEqual(grad_norm, exp_grad_norm)\n\n        grad_norm = utils.clip_grad_norm_(params, 1.0)\n        self.assertAlmostEqual(grad_norm, torch.tensor(1.0))\n\n    def test_resolve_max_positions_with_tuple(self):\n        resolved = utils.resolve_max_positions(None, (2000, 100, 2000), 12000)\n        self.assertEqual(resolved, (2000, 100, 2000))\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertLess(utils.item((t1 - t2).abs().max()), 1e-4)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/utils.py,16,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport os\nimport random\nimport sys\nimport torch\nimport torch.nn.functional as F\n\nfrom io import StringIO\nfrom fairseq import options, utils\nfrom fairseq.data import Dictionary\nfrom fairseq.data.language_pair_dataset import collate\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqEncoderDecoderModel,\n    FairseqIncrementalDecoder,\n)\nfrom fairseq.models.fairseq_encoder import EncoderOut\nfrom fairseq.tasks import FairseqTask\nfrom fairseq_cli import (\n    generate,\n    interactive,\n    preprocess,\n    train,\n    validate,\n)\n\n\ndef dummy_dictionary(vocab_size, prefix=\'token_\'):\n    d = Dictionary()\n    for i in range(vocab_size):\n        token = prefix + str(i)\n        d.add_symbol(token)\n    d.finalize(padding_factor=1)  # don\'t add extra padding symbols\n    return d\n\n\ndef dummy_dataloader(\n    samples,\n    padding_idx=1,\n    eos_idx=2,\n    batch_size=None,\n):\n    if batch_size is None:\n        batch_size = len(samples)\n\n    # add any missing data to samples\n    for i, sample in enumerate(samples):\n        if \'id\' not in sample:\n            sample[\'id\'] = i\n\n    # create dataloader\n    dataset = TestDataset(samples)\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        collate_fn=(lambda samples: collate(samples, padding_idx, eos_idx)),\n    )\n    return iter(dataloader)\n\n\ndef sequence_generator_setup():\n    # construct dummy dictionary\n    d = dummy_dictionary(vocab_size=2)\n\n    eos = d.eos()\n    w1 = 4\n    w2 = 5\n\n    # construct source data\n    src_tokens = torch.LongTensor([[w1, w2, eos], [w1, w2, eos]])\n    src_lengths = torch.LongTensor([2, 2])\n\n    args = argparse.Namespace()\n    unk = 0.\n    args.beam_probs = [\n        # step 0:\n        torch.FloatTensor([\n            # eos      w1   w2\n            # sentence 1:\n            [0.0, unk, 0.9, 0.1],  # beam 1\n            [0.0, unk, 0.9, 0.1],  # beam 2\n            # sentence 2:\n            [0.0, unk, 0.7, 0.3],\n            [0.0, unk, 0.7, 0.3],\n        ]),\n        # step 1:\n        torch.FloatTensor([\n            # eos      w1   w2       prefix\n            # sentence 1:\n            [1.0, unk, 0.0, 0.0],  # w1: 0.9  (emit: w1 <eos>: 0.9*1.0)\n            [0.0, unk, 0.9, 0.1],  # w2: 0.1\n            # sentence 2:\n            [0.25, unk, 0.35, 0.4],  # w1: 0.7  (don\'t emit: w1 <eos>: 0.7*0.25)\n            [0.00, unk, 0.10, 0.9],  # w2: 0.3\n        ]),\n        # step 2:\n        torch.FloatTensor([\n            # eos      w1   w2       prefix\n            # sentence 1:\n            [0.0, unk, 0.1, 0.9],  # w2 w1: 0.1*0.9\n            [0.6, unk, 0.2, 0.2],  # w2 w2: 0.1*0.1  (emit: w2 w2 <eos>: 0.1*0.1*0.6)\n            # sentence 2:\n            [0.60, unk, 0.4, 0.00],  # w1 w2: 0.7*0.4  (emit: w1 w2 <eos>: 0.7*0.4*0.6)\n            [0.01, unk, 0.0, 0.99],  # w2 w2: 0.3*0.9\n        ]),\n        # step 3:\n        torch.FloatTensor([\n            # eos      w1   w2       prefix\n            # sentence 1:\n            [1.0, unk, 0.0, 0.0],  # w2 w1 w2: 0.1*0.9*0.9  (emit: w2 w1 w2 <eos>: 0.1*0.9*0.9*1.0)\n            [1.0, unk, 0.0, 0.0],  # w2 w1 w1: 0.1*0.9*0.1  (emit: w2 w1 w1 <eos>: 0.1*0.9*0.1*1.0)\n            # sentence 2:\n            [0.1, unk, 0.5, 0.4],  # w2 w2 w2: 0.3*0.9*0.99  (emit: w2 w2 w2 <eos>: 0.3*0.9*0.99*0.1)\n            [1.0, unk, 0.0, 0.0],  # w1 w2 w1: 0.7*0.4*0.4  (emit: w1 w2 w1 <eos>: 0.7*0.4*0.4*1.0)\n        ]),\n    ]\n\n    task = TestTranslationTask.setup_task(args, d, d)\n    model = task.build_model(args)\n    tgt_dict = task.target_dictionary\n\n    return tgt_dict, w1, w2, src_tokens, src_lengths, model\n\n\ndef create_dummy_data(data_dir, num_examples=100, maxlen=20, alignment=False):\n    def _create_dummy_data(filename):\n        data = torch.rand(num_examples * maxlen)\n        data = 97 + torch.floor(26 * data).int()\n        with open(os.path.join(data_dir, filename), \'w\') as h:\n            offset = 0\n            for _ in range(num_examples):\n                ex_len = random.randint(1, maxlen)\n                ex_str = \' \'.join(map(chr, data[offset:offset+ex_len]))\n                print(ex_str, file=h)\n                offset += ex_len\n\n    def _create_dummy_alignment_data(filename_src, filename_tgt, filename):\n        with open(os.path.join(data_dir, filename_src), \'r\') as src_f, \\\n             open(os.path.join(data_dir, filename_tgt), \'r\') as tgt_f, \\\n             open(os.path.join(data_dir, filename), \'w\') as h:\n            for src, tgt in zip(src_f, tgt_f):\n                src_len = len(src.split())\n                tgt_len = len(tgt.split())\n                avg_len = (src_len + tgt_len) // 2\n                num_alignments = random.randint(avg_len // 2, 2 * avg_len)\n                src_indices = torch.floor(torch.rand(num_alignments) * src_len).int()\n                tgt_indices = torch.floor(torch.rand(num_alignments) * tgt_len).int()\n                ex_str = \' \'.join([""{}-{}"".format(src, tgt) for src, tgt in zip(src_indices, tgt_indices)])\n                print(ex_str, file=h)\n\n    _create_dummy_data(\'train.in\')\n    _create_dummy_data(\'train.out\')\n    _create_dummy_data(\'valid.in\')\n    _create_dummy_data(\'valid.out\')\n    _create_dummy_data(\'test.in\')\n    _create_dummy_data(\'test.out\')\n\n    if alignment:\n        _create_dummy_alignment_data(\'train.in\', \'train.out\', \'train.align\')\n        _create_dummy_alignment_data(\'valid.in\', \'valid.out\', \'valid.align\')\n        _create_dummy_alignment_data(\'test.in\', \'test.out\', \'test.align\')\n\n\ndef preprocess_lm_data(data_dir):\n    preprocess_parser = options.get_preprocessing_parser()\n    preprocess_args = preprocess_parser.parse_args([\n        \'--only-source\',\n        \'--trainpref\', os.path.join(data_dir, \'train.out\'),\n        \'--validpref\', os.path.join(data_dir, \'valid.out\'),\n        \'--testpref\', os.path.join(data_dir, \'test.out\'),\n        \'--destdir\', data_dir,\n    ])\n    preprocess.main(preprocess_args)\n\n\ndef preprocess_translation_data(data_dir, extra_flags=None):\n    preprocess_parser = options.get_preprocessing_parser()\n    preprocess_args = preprocess_parser.parse_args(\n        [\n            \'--source-lang\', \'in\',\n            \'--target-lang\', \'out\',\n            \'--trainpref\', os.path.join(data_dir, \'train\'),\n            \'--validpref\', os.path.join(data_dir, \'valid\'),\n            \'--testpref\', os.path.join(data_dir, \'test\'),\n            \'--thresholdtgt\', \'0\',\n            \'--thresholdsrc\', \'0\',\n            \'--destdir\', data_dir,\n        ] + (extra_flags or []),\n    )\n    preprocess.main(preprocess_args)\n\n\ndef train_translation_model(data_dir, arch, extra_flags=None, task=\'translation\', run_validation=False,\n                            lang_flags=None, extra_valid_flags=None):\n    if lang_flags is None:\n        lang_flags = [\n            \'--source-lang\', \'in\',\n            \'--target-lang\', \'out\',\n        ]\n    train_parser = options.get_training_parser()\n    train_args = options.parse_args_and_arch(\n        train_parser,\n        [\n            \'--task\', task,\n            data_dir,\n            \'--save-dir\', data_dir,\n            \'--arch\', arch,\n            \'--lr\', \'0.05\',\n            \'--max-tokens\', \'500\',\n            \'--max-epoch\', \'1\',\n            \'--no-progress-bar\',\n            \'--distributed-world-size\', \'1\',\n            \'--num-workers\', 0,\n        ] + lang_flags + (extra_flags or []),\n    )\n    train.main(train_args)\n\n    if run_validation:\n        # test validation\n        validate_parser = options.get_validation_parser()\n        validate_args = options.parse_args_and_arch(\n            validate_parser,\n            [\n                \'--task\', task,\n                data_dir,\n                \'--path\', os.path.join(data_dir, \'checkpoint_last.pt\'),\n                \'--valid-subset\', \'valid\',\n                \'--max-tokens\', \'500\',\n                \'--no-progress-bar\',\n            ] + lang_flags + (extra_valid_flags or [])\n        )\n        validate.main(validate_args)\n\n\ndef generate_main(data_dir, extra_flags=None):\n    if extra_flags is None:\n        extra_flags = [\n            \'--print-alignment\',\n        ]\n    generate_parser = options.get_generation_parser()\n    generate_args = options.parse_args_and_arch(\n        generate_parser,\n        [\n            data_dir,\n            \'--path\', os.path.join(data_dir, \'checkpoint_last.pt\'),\n            \'--beam\', \'3\',\n            \'--batch-size\', \'64\',\n            \'--max-len-b\', \'5\',\n            \'--gen-subset\', \'valid\',\n            \'--no-progress-bar\',\n        ] + (extra_flags or []),\n    )\n\n    # evaluate model in batch mode\n    generate.main(generate_args)\n\n    # evaluate model interactively\n    generate_args.buffer_size = 0\n    generate_args.input = \'-\'\n    generate_args.max_sentences = None\n    orig_stdin = sys.stdin\n    sys.stdin = StringIO(\'h e l l o\\n\')\n    interactive.main(generate_args)\n    sys.stdin = orig_stdin\n\n\nclass TestDataset(torch.utils.data.Dataset):\n\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n        self.sizes = None\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return len(self.data)\n\n\nclass TestTranslationTask(FairseqTask):\n\n    def __init__(self, args, src_dict, tgt_dict, model):\n        super().__init__(args)\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n        self.model = model\n\n    @classmethod\n    def setup_task(cls, args, src_dict=None, tgt_dict=None, model=None):\n        return cls(args, src_dict, tgt_dict, model)\n\n    def build_model(self, args):\n        return TestModel.build_model(args, self)\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.tgt_dict\n\n\nclass TestModel(FairseqEncoderDecoderModel):\n    def __init__(self, encoder, decoder):\n        super().__init__(encoder, decoder)\n\n    @classmethod\n    def build_model(cls, args, task):\n        encoder = TestEncoder(args, task.source_dictionary)\n        decoder = TestIncrementalDecoder(args, task.target_dictionary)\n        return cls(encoder, decoder)\n\n\nclass TestEncoder(FairseqEncoder):\n    def __init__(self, args, dictionary):\n        super().__init__(dictionary)\n        self.args = args\n\n    def forward(self, src_tokens, src_lengths=None, **kwargs):\n        return EncoderOut(\n            encoder_out=src_tokens,\n            encoder_padding_mask=None,\n            encoder_embedding=None,\n            encoder_states=None,\n            src_tokens=None,\n            src_lengths=None,\n        )\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        return EncoderOut(\n            encoder_out=encoder_out.encoder_out.index_select(0, new_order),\n            encoder_padding_mask=None,\n            encoder_embedding=None,\n            encoder_states=None,\n            src_tokens=None,\n            src_lengths=None,\n        )\n\n\nclass TestIncrementalDecoder(FairseqIncrementalDecoder):\n    def __init__(self, args, dictionary):\n        super().__init__(dictionary)\n        assert hasattr(args, \'beam_probs\') or hasattr(args, \'probs\')\n        args.max_decoder_positions = getattr(args, \'max_decoder_positions\', 100)\n        self.args = args\n\n    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None):\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n        bbsz = prev_output_tokens.size(0)\n        vocab = len(self.dictionary)\n        src_len = encoder_out.encoder_out.size(1)\n        tgt_len = prev_output_tokens.size(1)\n\n        # determine number of steps\n        if incremental_state is not None:\n            # cache step number\n            step = utils.get_incremental_state(self, incremental_state, \'step\')\n            if step is None:\n                step = 0\n            utils.set_incremental_state(self, incremental_state, \'step\', step + 1)\n            steps = [step]\n        else:\n            steps = list(range(tgt_len))\n\n        # define output in terms of raw probs\n        if hasattr(self.args, \'probs\'):\n            assert self.args.probs.dim() == 3, \\\n                \'expected probs to have size bsz*steps*vocab\'\n            probs = self.args.probs.index_select(1, torch.LongTensor(steps))\n        else:\n            probs = torch.FloatTensor(bbsz, len(steps), vocab).zero_()\n            for i, step in enumerate(steps):\n                # args.beam_probs gives the probability for every vocab element,\n                # starting with eos, then unknown, and then the rest of the vocab\n                if step < len(self.args.beam_probs):\n                    probs[:, i, self.dictionary.eos():] = self.args.beam_probs[step]\n                else:\n                    probs[:, i, self.dictionary.eos()] = 1.0\n\n        # random attention\n        attn = torch.rand(bbsz, tgt_len, src_len)\n\n        dev = prev_output_tokens.device\n        return probs.to(dev), {""attn"": [attn.to(dev)]}\n\n    def get_normalized_probs(self, net_output, log_probs, _):\n        # the decoder returns probabilities directly\n        probs = net_output[0]\n        if log_probs:\n            return probs.log()\n        else:\n            return probs\n\n    def max_positions(self):\n        return self.args.max_decoder_positions\n\n\nclass TestReshapingEncoder(FairseqEncoder):\n    def __init__(self, args, dictionary):\n        super().__init__(dictionary)\n        self.args = args\n\n    def forward(self, src_tokens, src_lengths=None, **kwargs):\n        b_sz, t_sz = src_tokens.shape\n        padding_needed = t_sz % 2\n        x = src_tokens\n        if padding_needed > 0:\n            padding_needed = 2 - padding_needed\n            x = F.pad(x, (0, padding_needed))\n\n        return EncoderOut(\n            encoder_out=x.view(b_sz, -1, 2),\n            encoder_padding_mask=None,\n            encoder_embedding=None,\n            encoder_states=None,\n            src_tokens=None,\n            src_lengths=None,\n        )\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        return EncoderOut(\n            encoder_out=encoder_out.encoder_out.index_select(0, new_order),\n            encoder_padding_mask=None,\n            encoder_embedding=None,\n            encoder_states=None,\n            src_tokens=None,\n            src_lengths=None,\n        )\n\n\nclass TestReshapingModel(FairseqEncoderDecoderModel):\n    def __init__(self, encoder, decoder):\n        super().__init__(encoder, decoder)\n\n    @classmethod\n    def build_model(cls, args, task):\n        encoder = TestReshapingEncoder(args, task.source_dictionary)\n        decoder = TestIncrementalDecoder(args, task.target_dictionary)\n        return cls(encoder, decoder)\n\n\nclass TestAdditionalInputEncoder(FairseqEncoder):\n    def __init__(self, args, dictionary):\n        super().__init__(dictionary)\n        self.args = args\n\n    def forward(self, src_tokens, src_lengths=None, **kwargs):\n        assert \'fancy_other_input\' in kwargs\n        assert kwargs[\'fancy_other_input\'] is not None\n        return EncoderOut(\n            encoder_out=src_tokens,\n            encoder_padding_mask=None,\n            encoder_embedding=None,\n            encoder_states=None,\n            src_tokens=None,\n            src_lengths=None,\n        )\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        return EncoderOut(\n            encoder_out=encoder_out.encoder_out.index_select(0, new_order),\n            encoder_padding_mask=None,\n            encoder_embedding=None,\n            encoder_states=None,\n            src_tokens=None,\n            src_lengths=None,\n        )\n\n\nclass TestAdditionalInputModel(FairseqEncoderDecoderModel):\n    def __init__(self, encoder, decoder):\n        super().__init__(encoder, decoder)\n\n    @classmethod\n    def build_model(cls, args, task):\n        encoder = TestAdditionalInputEncoder(args, task.source_dictionary)\n        decoder = TestIncrementalDecoder(args, task.target_dictionary)\n        return cls(encoder, decoder)\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n        encoder_out = self.encoder(\n            src_tokens, src_lengths=src_lengths, **kwargs)\n        decoder_out = self.decoder(\n            prev_output_tokens, encoder_out=encoder_out, **kwargs)\n        return decoder_out\n'"
examples/backtranslation/deduplicate_lines.py,0,"b'#!/usr/bin/python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport fileinput\nimport hashlib\nfrom multiprocessing import Pool\nimport sys\n\n\ndef get_hashes_and_lines(raw_line):\n    hash = hashlib.md5(raw_line).hexdigest()\n    return hash, raw_line\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--workers\', type=int, default=10)\n    parser.add_argument(\'files\', nargs=\'*\', help=\'input files\')\n    args = parser.parse_args()\n\n    seen = set()\n    with fileinput.input(args.files, mode=\'rb\') as h:\n        pool = Pool(args.workers)\n        results = pool.imap_unordered(get_hashes_and_lines, h, 1000)\n        for i, (hash, raw_line) in enumerate(results):\n            if hash not in seen:\n                seen.add(hash)\n                sys.stdout.buffer.write(raw_line)\n            if i % 1000000 == 0:\n                print(i, file=sys.stderr, end="""", flush=True)\n            elif i % 100000 == 0:\n                print(""."", file=sys.stderr, end="""", flush=True)\n    print(file=sys.stderr, flush=True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/backtranslation/extract_bt_data.py,0,"b""#!/usr/bin/env python\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport fileinput\n\nfrom tqdm import tqdm\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=(\n        'Extract back-translations from the stdout of fairseq-generate. '\n        'If there are multiply hypotheses for a source, we only keep the first one. '\n    ))\n    parser.add_argument('--output', required=True, help='output prefix')\n    parser.add_argument('--srclang', required=True, help='source language (extracted from H-* lines)')\n    parser.add_argument('--tgtlang', required=True, help='target language (extracted from S-* lines)')\n    parser.add_argument('--minlen', type=int, help='min length filter')\n    parser.add_argument('--maxlen', type=int, help='max length filter')\n    parser.add_argument('--ratio', type=float, help='ratio filter')\n    parser.add_argument('files', nargs='*', help='input files')\n    args = parser.parse_args()\n\n    def validate(src, tgt):\n        srclen = len(src.split(' ')) if src != '' else 0\n        tgtlen = len(tgt.split(' ')) if tgt != '' else 0\n        if (\n            (args.minlen is not None and (srclen < args.minlen or tgtlen < args.minlen))\n            or (args.maxlen is not None and (srclen > args.maxlen or tgtlen > args.maxlen))\n            or (args.ratio is not None and (max(srclen, tgtlen) / float(min(srclen, tgtlen)) > args.ratio))\n        ):\n            return False\n        return True\n\n    def safe_index(toks, index, default):\n        try:\n            return toks[index]\n        except IndexError:\n            return default\n\n    with open(args.output + '.' + args.srclang, 'w') as src_h, \\\n            open(args.output + '.' + args.tgtlang, 'w') as tgt_h:\n        for line in tqdm(fileinput.input(args.files)):\n            if line.startswith('S-'):\n                tgt = safe_index(line.rstrip().split('\\t'), 1, '')\n            elif line.startswith('H-'):\n                if tgt is not None:\n                    src = safe_index(line.rstrip().split('\\t'), 2, '')\n                    if validate(src, tgt):\n                        print(src, file=src_h)\n                        print(tgt, file=tgt_h)\n                    tgt = None\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/byte_level_bpe/get_bitext.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport os.path as op\nimport argparse\nimport os\nfrom multiprocessing import cpu_count\nfrom collections import namedtuple\nfrom typing import Optional, List\n\nimport sentencepiece as sp\n\nfrom fairseq.data.encoders.moses_tokenizer import MosesTokenizer\nfrom fairseq.data.encoders.byte_utils import byte_encode\nfrom fairseq.data.encoders.sentencepiece_bpe import SentencepieceBPE\nfrom fairseq.data.encoders.characters import Characters\nfrom fairseq.data.encoders.byte_bpe import ByteBPE\nfrom fairseq.data.encoders.bytes import Bytes\n\n\nSPLITS = [\'train\', \'valid\', \'test\']\n\n\ndef _convert_xml(in_path: str, out_path: str):\n    with open(in_path) as f, open(out_path, \'w\') as f_o:\n        for s in f:\n            ss = s.strip()\n            if not ss.startswith(\'<seg\'):\n                continue\n            ss = ss.replace(\'</seg>\', \'\').split(\'"">\')\n            assert len(ss) == 2\n            f_o.write(ss[1].strip() + \'\\n\')\n\n\ndef _convert_train(in_path: str, out_path: str):\n    with open(in_path) as f, open(out_path, \'w\') as f_o:\n        for s in f:\n            ss = s.strip()\n            if ss.startswith(\'<\'):\n                continue\n            f_o.write(ss.strip() + \'\\n\')\n\n\ndef _get_bytes(in_path: str, out_path: str):\n    with open(in_path) as f, open(out_path, \'w\') as f_o:\n        for s in f:\n            f_o.write(Bytes.encode(s.strip()) + \'\\n\')\n\n\ndef _get_chars(in_path: str, out_path: str):\n    with open(in_path) as f, open(out_path, \'w\') as f_o:\n        for s in f:\n            f_o.write(Characters.encode(s.strip()) + \'\\n\')\n\n\ndef pretokenize(in_path: str, out_path: str, src: str, tgt: str):\n    Args = namedtuple(\'Args\', [\'moses_source_lang\', \'moses_target_lang\',\n                               \'moses_no_dash_splits\', \'moses_no_escape\'])\n    args = Args(moses_source_lang=src, moses_target_lang=tgt,\n                moses_no_dash_splits=False, moses_no_escape=False)\n    pretokenizer = MosesTokenizer(args)\n    with open(in_path) as f, open(out_path, \'w\') as f_o:\n        for s in f:\n            f_o.write(pretokenizer.encode(s.strip()) + \'\\n\')\n\n\ndef _convert_to_bchar(in_path_prefix: str, src: str, tgt: str, out_path: str):\n    with open(out_path, \'w\') as f_o:\n        for lang in [src, tgt]:\n            with open(f\'{in_path_prefix}.{lang}\') as f:\n                for s in f:\n                    f_o.write(byte_encode(s.strip()) + \'\\n\')\n\n\ndef _get_bpe(in_path: str, model_prefix: str, vocab_size: int):\n    arguments = [\n        f\'--input={in_path}\', f\'--model_prefix={model_prefix}\',\n        f\'--model_type=bpe\', f\'--vocab_size={vocab_size}\',\n        \'--character_coverage=1.0\', \'--normalization_rule_name=identity\',\n        f\'--num_threads={cpu_count()}\'\n    ]\n    sp.SentencePieceTrainer.Train(\' \'.join(arguments))\n\n\ndef _apply_bbpe(model_path: str, in_path: str, out_path: str):\n    Args = namedtuple(\'Args\', [\'sentencepiece_model_path\'])\n    args = Args(sentencepiece_model_path=model_path)\n    tokenizer = ByteBPE(args)\n    with open(in_path) as f, open(out_path, \'w\') as f_o:\n        for s in f:\n            f_o.write(tokenizer.encode(s.strip()) + \'\\n\')\n\n\ndef _apply_bpe(model_path: str, in_path: str, out_path: str):\n    Args = namedtuple(\'Args\', [\'sentencepiece_vocab\'])\n    args = Args(sentencepiece_vocab=model_path)\n    tokenizer = SentencepieceBPE(args)\n    with open(in_path) as f, open(out_path, \'w\') as f_o:\n        for s in f:\n            f_o.write(tokenizer.encode(s.strip()) + \'\\n\')\n\n\ndef _concat_files(in_paths: List[str], out_path: str):\n    with open(out_path, \'w\') as f_o:\n        for p in in_paths:\n            with open(p) as f:\n                for r in f:\n                    f_o.write(r)\n\n\ndef preprocess_iwslt17(root: str, src: str, tgt: str, bpe_size: Optional[int],\n                       need_chars: bool, bbpe_size: Optional[int],\n                       need_bytes: bool):\n    # extract bitext\n    in_root = op.join(root, f\'{src}-{tgt}\')\n    for lang in [src, tgt]:\n        _convert_train(\n            op.join(in_root, f\'train.tags.{src}-{tgt}.{lang}\'),\n            op.join(root, f\'train.{lang}\')\n        )\n        _convert_xml(\n            op.join(in_root, f\'IWSLT17.TED.dev2010.{src}-{tgt}.{lang}.xml\'),\n            op.join(root, f\'valid.{lang}\')\n        )\n        _convert_xml(\n            op.join(in_root, f\'IWSLT17.TED.tst2015.{src}-{tgt}.{lang}.xml\'),\n            op.join(root, f\'test.{lang}\')\n        )\n    # pre-tokenize\n    for lang in [src, tgt]:\n        for split in SPLITS:\n            pretokenize(op.join(root, f\'{split}.{lang}\'),\n                        op.join(root, f\'{split}.moses.{lang}\'), src, tgt)\n    # tokenize with BPE vocabulary\n    if bpe_size is not None:\n        # learn vocabulary\n        concated_train_path = op.join(root, \'train.all\')\n        _concat_files(\n            [op.join(root, \'train.moses.fr\'), op.join(root, \'train.moses.en\')],\n            concated_train_path\n        )\n        bpe_model_prefix = op.join(root, f\'spm_bpe{bpe_size}\')\n        _get_bpe(concated_train_path, bpe_model_prefix, bpe_size)\n        os.remove(concated_train_path)\n        # apply\n        for lang in [src, tgt]:\n            for split in SPLITS:\n                _apply_bpe(\n                    bpe_model_prefix + \'.model\',\n                    op.join(root, f\'{split}.moses.{lang}\'),\n                    op.join(root, f\'{split}.moses.bpe{bpe_size}.{lang}\')\n                )\n    # tokenize with bytes vocabulary\n    if need_bytes:\n        for lang in [src, tgt]:\n            for split in SPLITS:\n                _get_bytes(op.join(root, f\'{split}.moses.{lang}\'),\n                           op.join(root, f\'{split}.moses.bytes.{lang}\'))\n    # tokenize with characters vocabulary\n    if need_chars:\n        for lang in [src, tgt]:\n            for split in SPLITS:\n                _get_chars(op.join(root, f\'{split}.moses.{lang}\'),\n                           op.join(root, f\'{split}.moses.chars.{lang}\'))\n    # tokenize with byte-level BPE vocabulary\n    if bbpe_size is not None:\n        # learn vocabulary\n        bchar_path = op.join(root, \'train.bchar\')\n        _convert_to_bchar(op.join(root, \'train.moses\'), src, tgt, bchar_path)\n        bbpe_model_prefix = op.join(root, f\'spm_bbpe{bbpe_size}\')\n        _get_bpe(bchar_path, bbpe_model_prefix, bbpe_size)\n        os.remove(bchar_path)\n        # apply\n        for lang in [src, tgt]:\n            for split in SPLITS:\n                _apply_bbpe(\n                    bbpe_model_prefix + \'.model\',\n                    op.join(root, f\'{split}.moses.{lang}\'),\n                    op.join(root, f\'{split}.moses.bbpe{bbpe_size}.{lang}\')\n                )\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--root\', type=str, default=\'data\')\n    parser.add_argument(\'--bpe-vocab\', default=None, type=int,\n                        help=\'Generate tokenized bitext with BPE of size K.\'\n                             \'Default to None (disabled).\')\n    parser.add_argument(\'--bbpe-vocab\', default=None, type=int,\n                        help=\'Generate tokenized bitext with BBPE of size K.\'\n                             \'Default to None (disabled).\')\n    parser.add_argument(\'--byte-vocab\', action=\'store_true\',\n                        help=\'Generate tokenized bitext with bytes vocabulary\')\n    parser.add_argument(\'--char-vocab\', action=\'store_true\',\n                        help=\'Generate tokenized bitext with chars vocabulary\')\n    args = parser.parse_args()\n\n    preprocess_iwslt17(args.root, \'fr\', \'en\', args.bpe_vocab, args.char_vocab,\n                       args.bbpe_vocab, args.byte_vocab)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/byte_level_bpe/gru_transformer.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.transformer import TransformerModel, TransformerEncoder\n\n\n@register_model(""gru_transformer"")\nclass GRUTransformerModel(TransformerModel):\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return GRUTransformerEncoder(args, src_dict, embed_tokens)\n\n\nclass GRUTransformerEncoder(TransformerEncoder):\n    def __init__(self, args, dictionary, embed_tokens):\n        super().__init__(args, dictionary, embed_tokens)\n        self.emb_ctx = nn.GRU(input_size=embed_tokens.embedding_dim,\n                              hidden_size=embed_tokens.embedding_dim // 2,\n                              num_layers=1, bidirectional=True)\n\n    def forward_embedding(self, src_tokens):\n        # embed tokens and positions\n        x = embed = self.embed_scale * self.embed_tokens(src_tokens)\n        if self.embed_positions is not None:\n            x = embed + self.embed_positions(src_tokens)\n\n        # contextualize embeddings\n        x = x.transpose(0, 1)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x, _ = self.emb_ctx.forward(x)\n        x = x.transpose(0, 1)\n\n        if self.layernorm_embedding is not None:\n            x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        return x, embed\n\n\n@register_model_architecture(""gru_transformer"", ""gru_transformer"")\ndef gru_transformer_base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.0)\n    args.activation_fn = getattr(args, ""activation_fn"", ""relu"")\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.adaptive_input = getattr(args, ""adaptive_input"", False)\n    args.no_cross_attention = getattr(args, ""no_cross_attention"", False)\n    args.cross_self_attention = getattr(args, ""cross_self_attention"", False)\n    args.layer_wise_attention = getattr(args, ""layer_wise_attention"", False)\n\n    args.decoder_output_dim = getattr(\n        args, ""decoder_output_dim"", args.decoder_embed_dim\n    )\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n\n    args.no_scale_embedding = getattr(args, ""no_scale_embedding"", False)\n    args.layernorm_embedding = getattr(args, ""layernorm_embedding"", False)\n\n\n@register_model_architecture(""gru_transformer"", ""gru_transformer_big"")\ndef gru_transformer_big(args):\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 1024)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 4096)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 16)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", 1024)\n    args.decoder_ffn_embed_dim = getattr(args, ""decoder_ffn_embed_dim"", 4096)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 16)\n    args.dropout = getattr(args, ""dropout"", 0.3)\n    gru_transformer_base_architecture(args)\n'"
examples/megatron_11b/detok.py,0,"b""#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport fileinput\nimport sacremoses\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('files', nargs='*', help='input files')\n    args = parser.parse_args()\n\n    detok = sacremoses.MosesDetokenizer()\n\n    for line in fileinput.input(args.files, openhook=fileinput.hook_compressed):\n        print(detok.detokenize(line.strip().split(' ')).replace(' @', '').replace('@ ', '').replace(' =', '=').replace('= ', '=').replace(' \xe2\x80\x93 ', '\xe2\x80\x93'))\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/noisychannel/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .rerank_options import *  # noqa\n'"
examples/noisychannel/rerank.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom multiprocessing import Pool\n\nimport numpy as np\n\nfrom fairseq import bleu, options\nfrom fairseq.data import dictionary\n\nfrom . import (\n    rerank_generate,\n    rerank_score_bw,\n    rerank_score_lm,\n    rerank_options,\n    rerank_utils,\n)\n\n\ndef score_target_hypo(args, a, b, c, lenpen, target_outfile, hypo_outfile, write_hypos, normalize):\n\n    print(""lenpen"", lenpen, ""weight1"", a, ""weight2"", b, ""weight3"", c)\n    gen_output_lst, bitext1_lst, bitext2_lst, lm_res_lst = load_score_files(args)\n    dict = dictionary.Dictionary()\n    scorer = bleu.Scorer(dict.pad(), dict.eos(), dict.unk())\n\n    ordered_hypos = {}\n    ordered_targets = {}\n\n    for shard_id in range(len(bitext1_lst)):\n        bitext1 = bitext1_lst[shard_id]\n        bitext2 = bitext2_lst[shard_id]\n        gen_output = gen_output_lst[shard_id]\n        lm_res = lm_res_lst[shard_id]\n\n        total = len(bitext1.rescore_source.keys())\n        source_lst = []\n        hypo_lst = []\n        score_lst = []\n        reference_lst = []\n        j = 1\n        best_score = -math.inf\n\n        for i in range(total):\n            # length is measured in terms of words, not bpe tokens, since models may not share the same bpe\n            target_len = len(bitext1.rescore_hypo[i].split())\n\n            if lm_res is not None:\n                lm_score = lm_res.score[i]\n            else:\n                lm_score = 0\n\n            if bitext2 is not None:\n                bitext2_score = bitext2.rescore_score[i]\n                bitext2_backwards = bitext2.backwards\n            else:\n                bitext2_score = None\n                bitext2_backwards = None\n\n            score = rerank_utils.get_score(a, b, c, target_len,\n                                           bitext1.rescore_score[i], bitext2_score, lm_score=lm_score,\n                                           lenpen=lenpen, src_len=bitext1.source_lengths[i],\n                                           tgt_len=bitext1.target_lengths[i], bitext1_backwards=bitext1.backwards,\n                                           bitext2_backwards=bitext2_backwards, normalize=normalize)\n\n            if score > best_score:\n                best_score = score\n                best_hypo = bitext1.rescore_hypo[i]\n\n            if j == gen_output.num_hypos[i] or j == args.num_rescore:\n                j = 1\n                hypo_lst.append(best_hypo)\n                score_lst.append(best_score)\n                source_lst.append(bitext1.rescore_source[i])\n                reference_lst.append(bitext1.rescore_target[i])\n\n                best_score = -math.inf\n                best_hypo = """"\n            else:\n                j += 1\n\n        gen_keys = list(sorted(gen_output.no_bpe_target.keys()))\n\n        for key in range(len(gen_keys)):\n            if args.prefix_len is None:\n                assert hypo_lst[key] in gen_output.no_bpe_hypo[gen_keys[key]], (\n                    ""pred and rescore hypo mismatch: i: "" + str(key) + "", ""\n                    + str(hypo_lst[key]) + str(gen_keys[key])\n                    + str(gen_output.no_bpe_hypo[key])\n                )\n                sys_tok = dict.encode_line(hypo_lst[key])\n                ref_tok = dict.encode_line(gen_output.no_bpe_target[gen_keys[key]])\n                scorer.add(ref_tok, sys_tok)\n\n            else:\n                full_hypo = rerank_utils.get_full_from_prefix(hypo_lst[key], gen_output.no_bpe_hypo[gen_keys[key]])\n                sys_tok = dict.encode_line(full_hypo)\n                ref_tok = dict.encode_line(gen_output.no_bpe_target[gen_keys[key]])\n                scorer.add(ref_tok, sys_tok)\n\n        # if only one set of hyper parameters is provided, write the predictions to a file\n        if write_hypos:\n            # recover the orinal ids from n best list generation\n            for key in range(len(gen_output.no_bpe_target)):\n                if args.prefix_len is None:\n                    assert hypo_lst[key] in gen_output.no_bpe_hypo[gen_keys[key]], \\\n                        ""pred and rescore hypo mismatch:""+""i:""+str(key)+str(hypo_lst[key]) + str(gen_output.no_bpe_hypo[key])\n                    ordered_hypos[gen_keys[key]] = hypo_lst[key]\n                    ordered_targets[gen_keys[key]] = gen_output.no_bpe_target[gen_keys[key]]\n\n                else:\n                    full_hypo = rerank_utils.get_full_from_prefix(hypo_lst[key], gen_output.no_bpe_hypo[gen_keys[key]])\n                    ordered_hypos[gen_keys[key]] = full_hypo\n                    ordered_targets[gen_keys[key]] = gen_output.no_bpe_target[gen_keys[key]]\n\n    # write the hypos in the original order from nbest list generation\n    if args.num_shards == (len(bitext1_lst)):\n        with open(target_outfile, \'w\') as t:\n            with open(hypo_outfile, \'w\') as h:\n                for key in range(len(ordered_hypos)):\n                    t.write(ordered_targets[key])\n                    h.write(ordered_hypos[key])\n\n    res = scorer.result_string(4)\n    if write_hypos:\n        print(res)\n    score = rerank_utils.parse_bleu_scoring(res)\n    return score\n\n\ndef match_target_hypo(args, target_outfile, hypo_outfile):\n    """"""combine scores from the LM and bitext models, and write the top scoring hypothesis to a file""""""\n    if len(args.weight1) == 1:\n        res = score_target_hypo(args, args.weight1[0], args.weight2[0],\n                                args.weight3[0], args.lenpen[0], target_outfile,\n                                hypo_outfile, True, args.normalize)\n        rerank_scores = [res]\n    else:\n        print(""launching pool"")\n        with Pool(32) as p:\n            rerank_scores = p.starmap(score_target_hypo,\n                                      [(args, args.weight1[i], args.weight2[i], args.weight3[i],\n                                        args.lenpen[i], target_outfile, hypo_outfile,\n                                        False, args.normalize) for i in range(len(args.weight1))])\n\n    if len(rerank_scores) > 1:\n        best_index = np.argmax(rerank_scores)\n        best_score = rerank_scores[best_index]\n        print(""best score"", best_score)\n        print(""best lenpen"", args.lenpen[best_index])\n        print(""best weight1"", args.weight1[best_index])\n        print(""best weight2"", args.weight2[best_index])\n        print(""best weight3"", args.weight3[best_index])\n        return args.lenpen[best_index], args.weight1[best_index], \\\n            args.weight2[best_index], args.weight3[best_index], best_score\n\n    else:\n        return args.lenpen[0], args.weight1[0], args.weight2[0], args.weight3[0], rerank_scores[0]\n\n\ndef load_score_files(args):\n    if args.all_shards:\n        shard_ids = list(range(args.num_shards))\n    else:\n        shard_ids = [args.shard_id]\n\n    gen_output_lst = []\n    bitext1_lst = []\n    bitext2_lst = []\n    lm_res1_lst = []\n\n    for shard_id in shard_ids:\n        using_nbest = args.nbest_list is not None\n        pre_gen, left_to_right_preprocessed_dir, right_to_left_preprocessed_dir, \\\n            backwards_preprocessed_dir, lm_preprocessed_dir = \\\n            rerank_utils.get_directories(args.data_dir_name, args.num_rescore, args.gen_subset,\n                                         args.gen_model_name, shard_id, args.num_shards, args.sampling,\n                                         args.prefix_len, args.target_prefix_frac, args.source_prefix_frac)\n\n        rerank1_is_gen = args.gen_model == args.score_model1 and args.source_prefix_frac is None\n        rerank2_is_gen = args.gen_model == args.score_model2 and args.source_prefix_frac is None\n\n        score1_file = rerank_utils.rescore_file_name(pre_gen, args.prefix_len, args.model1_name,\n                                                     target_prefix_frac=args.target_prefix_frac,\n                                                     source_prefix_frac=args.source_prefix_frac,\n                                                     backwards=args.backwards1)\n        if args.score_model2 is not None:\n            score2_file = rerank_utils.rescore_file_name(pre_gen, args.prefix_len, args.model2_name,\n                                                         target_prefix_frac=args.target_prefix_frac,\n                                                         source_prefix_frac=args.source_prefix_frac,\n                                                         backwards=args.backwards2)\n        if args.language_model is not None:\n            lm_score_file = rerank_utils.rescore_file_name(pre_gen, args.prefix_len, args.lm_name, lm_file=True)\n\n        # get gen output\n        predictions_bpe_file = pre_gen+""/generate_output_bpe.txt""\n        if using_nbest:\n            print(""Using predefined n-best list from interactive.py"")\n            predictions_bpe_file = args.nbest_list\n        gen_output = rerank_utils.BitextOutputFromGen(predictions_bpe_file, bpe_symbol=args.remove_bpe,\n                                                      nbest=using_nbest, prefix_len=args.prefix_len,\n                                                      target_prefix_frac=args.target_prefix_frac)\n\n        if rerank1_is_gen:\n            bitext1 = gen_output\n        else:\n            bitext1 = rerank_utils.BitextOutput(score1_file, args.backwards1, args.right_to_left1,\n                                                args.remove_bpe, args.prefix_len, args.target_prefix_frac,\n                                                args.source_prefix_frac)\n\n        if args.score_model2 is not None or args.nbest_list is not None:\n            if rerank2_is_gen:\n                bitext2 = gen_output\n            else:\n                bitext2 = rerank_utils.BitextOutput(score2_file, args.backwards2, args.right_to_left2,\n                                                    args.remove_bpe, args.prefix_len, args.target_prefix_frac,\n                                                    args.source_prefix_frac)\n\n                assert bitext2.source_lengths == bitext1.source_lengths, \\\n                    ""source lengths for rescoring models do not match""\n                assert bitext2.target_lengths == bitext1.target_lengths, \\\n                    ""target lengths for rescoring models do not match""\n        else:\n            if args.diff_bpe:\n                assert args.score_model2 is None\n                bitext2 = gen_output\n            else:\n                bitext2 = None\n\n        if args.language_model is not None:\n            lm_res1 = rerank_utils.LMOutput(lm_score_file, args.lm_dict, args.prefix_len,\n                                            args.remove_bpe, args.target_prefix_frac)\n        else:\n            lm_res1 = None\n\n        gen_output_lst.append(gen_output)\n        bitext1_lst.append(bitext1)\n        bitext2_lst.append(bitext2)\n        lm_res1_lst.append(lm_res1)\n    return gen_output_lst, bitext1_lst, bitext2_lst, lm_res1_lst\n\n\ndef rerank(args):\n    if type(args.lenpen) is not list:\n        args.lenpen = [args.lenpen]\n    if type(args.weight1) is not list:\n        args.weight1 = [args.weight1]\n    if type(args.weight2) is not list:\n        args.weight2 = [args.weight2]\n    if type(args.weight3) is not list:\n        args.weight3 = [args.weight3]\n    if args.all_shards:\n        shard_ids = list(range(args.num_shards))\n    else:\n        shard_ids = [args.shard_id]\n\n    for shard_id in shard_ids:\n        pre_gen, left_to_right_preprocessed_dir, right_to_left_preprocessed_dir, \\\n                backwards_preprocessed_dir, lm_preprocessed_dir = \\\n                rerank_utils.get_directories(args.data_dir_name, args.num_rescore, args.gen_subset,\n                                             args.gen_model_name, shard_id, args.num_shards, args.sampling,\n                                             args.prefix_len, args.target_prefix_frac, args.source_prefix_frac)\n        rerank_generate.gen_and_reprocess_nbest(args)\n        rerank_score_bw.score_bw(args)\n        rerank_score_lm.score_lm(args)\n\n        if args.write_hypos is None:\n            write_targets = pre_gen+""/matched_targets""\n            write_hypos = pre_gen+""/matched_hypos""\n        else:\n            write_targets = args.write_hypos+""_targets"" + args.gen_subset\n            write_hypos = args.write_hypos+""_hypos"" + args.gen_subset\n\n    if args.all_shards:\n        write_targets += ""_all_shards""\n        write_hypos += ""_all_shards""\n\n    best_lenpen, best_weight1, best_weight2, best_weight3, best_score = \\\n        match_target_hypo(args, write_targets, write_hypos)\n\n    return best_lenpen, best_weight1, best_weight2, best_weight3, best_score\n\n\ndef cli_main():\n    parser = rerank_options.get_reranking_parser()\n    args = options.parse_args_and_arch(parser)\n    rerank(args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
examples/noisychannel/rerank_generate.py,0,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nGenerate n-best translations using a trained model.\n""""""\n\nfrom contextlib import redirect_stdout\nimport os\nimport subprocess\n\nfrom fairseq import options\nfrom fairseq_cli import generate, preprocess\n\nfrom . import rerank_options, rerank_utils\n\n\ndef gen_and_reprocess_nbest(args):\n    if args.score_dict_dir is None:\n        args.score_dict_dir = args.data\n    if args.prefix_len is not None:\n        assert args.right_to_left1 is False, ""prefix length not compatible with right to left models""\n        assert args.right_to_left2 is False, ""prefix length not compatible with right to left models""\n\n    if args.nbest_list is not None:\n        assert args.score_model2 is None\n\n    if args.backwards1:\n        scorer1_src = args.target_lang\n        scorer1_tgt = args.source_lang\n    else:\n        scorer1_src = args.source_lang\n        scorer1_tgt = args.target_lang\n\n    store_data = os.path.join(os.path.dirname(__file__))+""/rerank_data/""+args.data_dir_name\n    if not os.path.exists(store_data):\n        os.makedirs(store_data)\n\n    pre_gen, left_to_right_preprocessed_dir, right_to_left_preprocessed_dir, \\\n        backwards_preprocessed_dir, lm_preprocessed_dir = \\\n        rerank_utils.get_directories(args.data_dir_name, args.num_rescore, args.gen_subset,\n                                     args.gen_model_name, args.shard_id, args.num_shards,\n                                     args.sampling, args.prefix_len, args.target_prefix_frac,\n                                     args.source_prefix_frac)\n    assert not (args.right_to_left1 and args.backwards1), ""backwards right to left not supported""\n    assert not (args.right_to_left2 and args.backwards2), ""backwards right to left not supported""\n    assert not (args.prefix_len is not None and args.target_prefix_frac is not None), \\\n        ""target prefix frac and target prefix len incompatible""\n\n    # make directory to store generation results\n    if not os.path.exists(pre_gen):\n        os.makedirs(pre_gen)\n\n    rerank1_is_gen = args.gen_model == args.score_model1 and args.source_prefix_frac is None\n    rerank2_is_gen = args.gen_model == args.score_model2 and args.source_prefix_frac is None\n\n    if args.nbest_list is not None:\n        rerank2_is_gen = True\n\n    # make directories to store preprossed nbest list for reranking\n    if not os.path.exists(left_to_right_preprocessed_dir):\n        os.makedirs(left_to_right_preprocessed_dir)\n    if not os.path.exists(right_to_left_preprocessed_dir):\n        os.makedirs(right_to_left_preprocessed_dir)\n    if not os.path.exists(lm_preprocessed_dir):\n        os.makedirs(lm_preprocessed_dir)\n    if not os.path.exists(backwards_preprocessed_dir):\n        os.makedirs(backwards_preprocessed_dir)\n\n    score1_file = rerank_utils.rescore_file_name(pre_gen, args.prefix_len, args.model1_name,\n                                                 target_prefix_frac=args.target_prefix_frac,\n                                                 source_prefix_frac=args.source_prefix_frac,\n                                                 backwards=args.backwards1)\n    if args.score_model2 is not None:\n        score2_file = rerank_utils.rescore_file_name(pre_gen, args.prefix_len, args.model2_name,\n                                                     target_prefix_frac=args.target_prefix_frac,\n                                                     source_prefix_frac=args.source_prefix_frac,\n                                                     backwards=args.backwards2)\n\n    predictions_bpe_file = pre_gen+""/generate_output_bpe.txt""\n\n    using_nbest = args.nbest_list is not None\n\n    if using_nbest:\n        print(""Using predefined n-best list from interactive.py"")\n        predictions_bpe_file = args.nbest_list\n\n    else:\n        if not os.path.isfile(predictions_bpe_file):\n            print(""STEP 1: generate predictions using the p(T|S) model with bpe"")\n            print(args.data)\n            param1 = [args.data,\n                      ""--path"", args.gen_model,\n                      ""--shard-id"", str(args.shard_id),\n                      ""--num-shards"", str(args.num_shards),\n                      ""--nbest"", str(args.num_rescore),\n                      ""--batch-size"", str(args.batch_size),\n                      ""--beam"", str(args.num_rescore),\n                      ""--max-sentences"", str(args.num_rescore),\n                      ""--gen-subset"", args.gen_subset,\n                      ""--source-lang"", args.source_lang,\n                      ""--target-lang"", args.target_lang]\n            if args.sampling:\n                param1 += [""--sampling""]\n\n            gen_parser = options.get_generation_parser()\n            input_args = options.parse_args_and_arch(gen_parser, param1)\n\n            print(input_args)\n            with open(predictions_bpe_file, \'w\') as f:\n                with redirect_stdout(f):\n                    generate.main(input_args)\n\n    gen_output = rerank_utils.BitextOutputFromGen(predictions_bpe_file, bpe_symbol=args.remove_bpe,\n                                                  nbest=using_nbest, prefix_len=args.prefix_len,\n                                                  target_prefix_frac=args.target_prefix_frac)\n\n    if args.diff_bpe:\n        rerank_utils.write_reprocessed(gen_output.no_bpe_source, gen_output.no_bpe_hypo,\n                                       gen_output.no_bpe_target, pre_gen+""/source_gen_bpe.""+args.source_lang,\n                                       pre_gen+""/target_gen_bpe.""+args.target_lang,\n                                       pre_gen+""/reference_gen_bpe.""+args.target_lang)\n        bitext_bpe = args.rescore_bpe_code\n        bpe_src_param = [""-c"", bitext_bpe,\n                         ""--input"", pre_gen+""/source_gen_bpe.""+args.source_lang,\n                         ""--output"", pre_gen+""/rescore_data.""+args.source_lang]\n        bpe_tgt_param = [""-c"", bitext_bpe,\n                         ""--input"", pre_gen+""/target_gen_bpe.""+args.target_lang,\n                         ""--output"", pre_gen+""/rescore_data.""+args.target_lang]\n\n        subprocess.call([""python"",\n                         os.path.join(os.path.dirname(__file__),\n                                      ""subword-nmt/subword_nmt/apply_bpe.py"")] + bpe_src_param,\n                        shell=False)\n\n        subprocess.call([""python"",\n                         os.path.join(os.path.dirname(__file__),\n                                      ""subword-nmt/subword_nmt/apply_bpe.py"")] + bpe_tgt_param,\n                        shell=False)\n\n    if (not os.path.isfile(score1_file) and not rerank1_is_gen) or \\\n            (args.score_model2 is not None and not os.path.isfile(score2_file) and not rerank2_is_gen):\n        print(""STEP 2: process the output of generate.py so we have clean text files with the translations"")\n\n        rescore_file = ""/rescore_data""\n        if args.prefix_len is not None:\n            prefix_len_rescore_file = rescore_file + ""prefix""+str(args.prefix_len)\n        if args.target_prefix_frac is not None:\n            target_prefix_frac_rescore_file = rescore_file + ""target_prefix_frac""+str(args.target_prefix_frac)\n        if args.source_prefix_frac is not None:\n            source_prefix_frac_rescore_file = rescore_file + ""source_prefix_frac""+str(args.source_prefix_frac)\n\n        if not args.right_to_left1 or not args.right_to_left2:\n            if not args.diff_bpe:\n                rerank_utils.write_reprocessed(gen_output.source, gen_output.hypo, gen_output.target,\n                                               pre_gen+rescore_file+"".""+args.source_lang,\n                                               pre_gen+rescore_file+"".""+args.target_lang,\n                                               pre_gen+""/reference_file"", bpe_symbol=args.remove_bpe)\n                if args.prefix_len is not None:\n                    bw_rescore_file = prefix_len_rescore_file\n                    rerank_utils.write_reprocessed(gen_output.source, gen_output.hypo, gen_output.target,\n                                                   pre_gen+prefix_len_rescore_file+"".""+args.source_lang,\n                                                   pre_gen+prefix_len_rescore_file+"".""+args.target_lang,\n                                                   pre_gen+""/reference_file"", prefix_len=args.prefix_len,\n                                                   bpe_symbol=args.remove_bpe)\n                elif args.target_prefix_frac is not None:\n                    bw_rescore_file = target_prefix_frac_rescore_file\n                    rerank_utils.write_reprocessed(gen_output.source, gen_output.hypo, gen_output.target,\n                                                   pre_gen+target_prefix_frac_rescore_file+"".""+args.source_lang,\n                                                   pre_gen+target_prefix_frac_rescore_file+"".""+args.target_lang,\n                                                   pre_gen+""/reference_file"", bpe_symbol=args.remove_bpe,\n                                                   target_prefix_frac=args.target_prefix_frac)\n                else:\n                    bw_rescore_file = rescore_file\n\n                if args.source_prefix_frac is not None:\n                    fw_rescore_file = source_prefix_frac_rescore_file\n                    rerank_utils.write_reprocessed(gen_output.source, gen_output.hypo, gen_output.target,\n                                                   pre_gen+source_prefix_frac_rescore_file+"".""+args.source_lang,\n                                                   pre_gen+source_prefix_frac_rescore_file+"".""+args.target_lang,\n                                                   pre_gen+""/reference_file"", bpe_symbol=args.remove_bpe,\n                                                   source_prefix_frac=args.source_prefix_frac)\n                else:\n                    fw_rescore_file = rescore_file\n\n        if args.right_to_left1 or args.right_to_left2:\n            rerank_utils.write_reprocessed(gen_output.source, gen_output.hypo, gen_output.target,\n                                           pre_gen+""/right_to_left_rescore_data.""+args.source_lang,\n                                           pre_gen+""/right_to_left_rescore_data.""+args.target_lang,\n                                           pre_gen+""/right_to_left_reference_file"",\n                                           right_to_left=True, bpe_symbol=args.remove_bpe)\n\n        print(""STEP 3: binarize the translations"")\n        if not args.right_to_left1 or args.score_model2 is not None and not args.right_to_left2 or not rerank1_is_gen:\n\n            if args.backwards1 or args.backwards2:\n                if args.backwards_score_dict_dir is not None:\n                    bw_dict = args.backwards_score_dict_dir\n                else:\n                    bw_dict = args.score_dict_dir\n                bw_preprocess_param = [""--source-lang"", scorer1_src,\n                                       ""--target-lang"", scorer1_tgt,\n                                       ""--trainpref"", pre_gen+bw_rescore_file,\n                                       ""--srcdict"", bw_dict + ""/dict."" + scorer1_src + "".txt"",\n                                       ""--tgtdict"", bw_dict + ""/dict."" + scorer1_tgt + "".txt"",\n                                       ""--destdir"", backwards_preprocessed_dir]\n                preprocess_parser = options.get_preprocessing_parser()\n                input_args = preprocess_parser.parse_args(bw_preprocess_param)\n                preprocess.main(input_args)\n\n            preprocess_param = [""--source-lang"", scorer1_src,\n                                ""--target-lang"", scorer1_tgt,\n                                ""--trainpref"", pre_gen+fw_rescore_file,\n                                ""--srcdict"", args.score_dict_dir+""/dict.""+scorer1_src+"".txt"",\n                                ""--tgtdict"", args.score_dict_dir+""/dict.""+scorer1_tgt+"".txt"",\n                                ""--destdir"", left_to_right_preprocessed_dir]\n            preprocess_parser = options.get_preprocessing_parser()\n            input_args = preprocess_parser.parse_args(preprocess_param)\n            preprocess.main(input_args)\n\n        if args.right_to_left1 or args.right_to_left2:\n            preprocess_param = [""--source-lang"", scorer1_src,\n                                ""--target-lang"", scorer1_tgt,\n                                ""--trainpref"", pre_gen+""/right_to_left_rescore_data"",\n                                ""--srcdict"", args.score_dict_dir+""/dict.""+scorer1_src+"".txt"",\n                                ""--tgtdict"", args.score_dict_dir+""/dict.""+scorer1_tgt+"".txt"",\n                                ""--destdir"", right_to_left_preprocessed_dir]\n            preprocess_parser = options.get_preprocessing_parser()\n            input_args = preprocess_parser.parse_args(preprocess_param)\n            preprocess.main(input_args)\n\n    return gen_output\n\n\ndef cli_main():\n    parser = rerank_options.get_reranking_parser()\n    args = options.parse_args_and_arch(parser)\n    gen_and_reprocess_nbest(args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
examples/noisychannel/rerank_options.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import options\n\n\ndef get_reranking_parser(default_task=\'translation\'):\n    parser = options.get_parser(\'Generation and reranking\', default_task)\n    add_reranking_args(parser)\n    return parser\n\n\ndef get_tuning_parser(default_task=\'translation\'):\n    parser = options.get_parser(\'Reranking tuning\', default_task)\n    add_reranking_args(parser)\n    add_tuning_args(parser)\n    return parser\n\n\ndef add_reranking_args(parser):\n    group = parser.add_argument_group(""Reranking"")\n    # fmt: off\n    group.add_argument(\'--score-model1\', \'-s1\', type=str, metavar=\'FILE\', required=True,\n                       help=\'path to first model or ensemble of models for rescoring\')\n    group.add_argument(\'--score-model2\', \'-s2\', type=str, metavar=\'FILE\', required=False,\n                       help=\'path to second model or ensemble of models for rescoring\')\n    group.add_argument(\'--num-rescore\', \'-n\', type=int, metavar=\'N\', default=10,\n                       help=\'the number of candidate hypothesis to rescore\')\n    group.add_argument(\'-bz\', \'--batch-size\', type=int, metavar=\'N\', default=128,\n                       help=\'batch size for generating the nbest list\')\n    group.add_argument(\'--gen-subset\', default=\'test\', metavar=\'SET\', choices=[\'test\', \'train\', \'valid\'],\n                       help=\'data subset to generate (train, valid, test)\')\n    group.add_argument(\'--gen-model\', default=None, metavar=\'FILE\',\n                       help=\'the model to generate translations\')\n    group.add_argument(\'-b1\', \'--backwards1\', action=\'store_true\',\n                       help=\'whether or not the first model group is backwards\')\n    group.add_argument(\'-b2\', \'--backwards2\', action=\'store_true\',\n                       help=\'whether or not the second model group is backwards\')\n    group.add_argument(\'-a\', \'--weight1\', default=1, nargs=\'+\', type=float,\n                       help=\'the weight(s) of the first model\')\n    group.add_argument(\'-b\', \'--weight2\', default=1, nargs=\'+\', type=float,\n                       help=\'the weight(s) of the second model, or the gen model if using nbest from interactive.py\')\n    group.add_argument(\'-c\', \'--weight3\', default=1, nargs=\'+\', type=float,\n                       help=\'the weight(s) of the third model\')\n\n    # lm arguments\n    group.add_argument(\'-lm\', \'--language-model\', default=None, metavar=\'FILE\',\n                       help=\'language model for target language to rescore translations\')\n    group.add_argument(\'--lm-dict\', default=None, metavar=\'FILE\',\n                       help=\'the dict of the language model for the target language\')\n    group.add_argument(\'--lm-name\', default=None,\n                       help=\'the name of the language model for the target language\')\n    group.add_argument(\'--lm-bpe-code\', default=None, metavar=\'FILE\',\n                       help=\'the bpe code for the language model for the target language\')\n    group.add_argument(\'--data-dir-name\', default=None,\n                       help=\'name of data directory\')\n    group.add_argument(\'--lenpen\', default=1, nargs=\'+\', type=float,\n                       help=\'length penalty: <1.0 favors shorter, >1.0 favors longer sentences\')\n    group.add_argument(\'--score-dict-dir\', default=None,\n                       help=\'the directory with dictionaries for the scoring models\')\n    group.add_argument(\'--right-to-left1\', action=\'store_true\',\n                       help=\'whether the first model group is a right to left model\')\n    group.add_argument(\'--right-to-left2\', action=\'store_true\',\n                       help=\'whether the second model group is a right to left model\')\n    group.add_argument(\'--remove-bpe\', default=\'@@ \',\n                       help=\'the bpe symbol, used for the bitext and LM\')\n    group.add_argument(\'--prefix-len\', default=None, type=int,\n                       help=\'the length of the target prefix to use in rescoring (in terms of words wo bpe)\')\n    group.add_argument(\'--sampling\', action=\'store_true\',\n                       help=\'use sampling instead of beam search for generating n best list\')\n    group.add_argument(\'--diff-bpe\', action=\'store_true\',\n                       help=\'bpe for rescoring and nbest list not the same\')\n    group.add_argument(\'--rescore-bpe-code\', default=None,\n                       help=\'bpe code for rescoring models\')\n    group.add_argument(\'--nbest-list\', default=None,\n                       help=\'use predefined nbest list in interactive.py format\')\n    group.add_argument(\'--write-hypos\', default=None,\n                       help=\'filename prefix to write hypos to\')\n    group.add_argument(\'--ref-translation\', default=None,\n                       help=\'reference translation to use with nbest list from interactive.py\')\n    group.add_argument(\'--backwards-score-dict-dir\', default=None,\n                       help=\'the directory with dictionaries for the backwards model,\'\n                            \'if None then it is assumed the fw and backwards models share dictionaries\')\n\n    # extra scaling args\n    group.add_argument(\'--gen-model-name\', default=None,\n                       help=\'the name of the models that generated the nbest list\')\n    group.add_argument(\'--model1-name\', default=None,\n                       help=\'the name of the set for model1 group \')\n    group.add_argument(\'--model2-name\', default=None,\n                       help=\'the name of the set for model2 group\')\n    group.add_argument(\'--shard-id\', default=0, type=int,\n                       help=\'the id of the shard to generate\')\n    group.add_argument(\'--num-shards\', default=1, type=int,\n                       help=\'the number of shards to generate across\')\n    group.add_argument(\'--all-shards\', action=\'store_true\',\n                       help=\'use all shards\')\n    group.add_argument(\'--target-prefix-frac\', default=None, type=float,\n                       help=\'the fraction of the target prefix to use in rescoring (in terms of words wo bpe)\')\n    group.add_argument(\'--source-prefix-frac\', default=None, type=float,\n                       help=\'the fraction of the source prefix to use in rescoring (in terms of words wo bpe)\')\n    group.add_argument(\'--normalize\', action=\'store_true\',\n                       help=\'whether to normalize by src and target len\')\n\n    return group\n\n\ndef add_tuning_args(parser):\n    group = parser.add_argument_group(""Tuning"")\n\n    group.add_argument(\'--lower-bound\', default=[-0.7], nargs=\'+\', type=float,\n                       help=\'lower bound of search space\')\n    group.add_argument(\'--upper-bound\', default=[3], nargs=\'+\', type=float,\n                       help=\'upper bound of search space\')\n    group.add_argument(\'--tune-param\', default=[\'lenpen\'], nargs=\'+\',\n                       choices=[\'lenpen\', \'weight1\', \'weight2\', \'weight3\'],\n                       help=\'the parameter(s) to tune\')\n    group.add_argument(\'--tune-subset\', default=\'valid\', choices=[\'valid\', \'test\', \'train\'],\n                       help=\'the subset to tune on \')\n    group.add_argument(\'--num-trials\', default=1000, type=int,\n                       help=\'number of trials to do for random search\')\n    group.add_argument(\'--share-weights\', action=\'store_true\',\n                       help=\'share weight2 and weight 3\')\n    return group\n'"
examples/noisychannel/rerank_score_bw.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom contextlib import redirect_stdout\nimport os\n\nfrom fairseq import options\nfrom fairseq_cli import generate\n\nfrom . import rerank_options, rerank_utils\n\n\ndef score_bw(args):\n        if args.backwards1:\n            scorer1_src = args.target_lang\n            scorer1_tgt = args.source_lang\n        else:\n            scorer1_src = args.source_lang\n            scorer1_tgt = args.target_lang\n\n        if args.score_model2 is not None:\n            if args.backwards2:\n                scorer2_src = args.target_lang\n                scorer2_tgt = args.source_lang\n            else:\n                scorer2_src = args.source_lang\n                scorer2_tgt = args.target_lang\n\n        rerank1_is_gen = args.gen_model == args.score_model1 and args.source_prefix_frac is None\n        rerank2_is_gen = args.gen_model == args.score_model2 and args.source_prefix_frac is None\n\n        pre_gen, left_to_right_preprocessed_dir, right_to_left_preprocessed_dir, \\\n            backwards_preprocessed_dir, lm_preprocessed_dir = \\\n            rerank_utils.get_directories(args.data_dir_name, args.num_rescore, args.gen_subset,\n                                         args.gen_model_name, args.shard_id, args.num_shards,\n                                         args.sampling, args.prefix_len, args.target_prefix_frac,\n                                         args.source_prefix_frac)\n\n        score1_file = rerank_utils.rescore_file_name(pre_gen, args.prefix_len, args.model1_name,\n                                                     target_prefix_frac=args.target_prefix_frac,\n                                                     source_prefix_frac=args.source_prefix_frac,\n                                                     backwards=args.backwards1)\n\n        if args.score_model2 is not None:\n            score2_file = rerank_utils.rescore_file_name(pre_gen, args.prefix_len, args.model2_name,\n                                                         target_prefix_frac=args.target_prefix_frac,\n                                                         source_prefix_frac=args.source_prefix_frac,\n                                                         backwards=args.backwards2)\n\n        if args.right_to_left1:\n            rerank_data1 = right_to_left_preprocessed_dir\n        elif args.backwards1:\n            rerank_data1 = backwards_preprocessed_dir\n        else:\n            rerank_data1 = left_to_right_preprocessed_dir\n\n        gen_param = [""--batch-size"", str(128), ""--score-reference"", ""--gen-subset"", ""train""]\n        if not rerank1_is_gen and not os.path.isfile(score1_file):\n            print(""STEP 4: score the translations for model 1"")\n\n            model_param1 = [""--path"", args.score_model1, ""--source-lang"", scorer1_src, ""--target-lang"", scorer1_tgt]\n            gen_model1_param = [rerank_data1] + gen_param + model_param1\n\n            gen_parser = options.get_generation_parser()\n            input_args = options.parse_args_and_arch(gen_parser, gen_model1_param)\n\n            with open(score1_file, \'w\') as f:\n                with redirect_stdout(f):\n                    generate.main(input_args)\n\n        if args.score_model2 is not None and not os.path.isfile(score2_file) and not rerank2_is_gen:\n            print(""STEP 4: score the translations for model 2"")\n\n            if args.right_to_left2:\n                rerank_data2 = right_to_left_preprocessed_dir\n            elif args.backwards2:\n                rerank_data2 = backwards_preprocessed_dir\n            else:\n                rerank_data2 = left_to_right_preprocessed_dir\n\n            model_param2 = [""--path"", args.score_model2, ""--source-lang"", scorer2_src, ""--target-lang"", scorer2_tgt]\n            gen_model2_param = [rerank_data2] + gen_param + model_param2\n\n            gen_parser = options.get_generation_parser()\n            input_args = options.parse_args_and_arch(gen_parser, gen_model2_param)\n\n            with open(score2_file, \'w\') as f:\n                with redirect_stdout(f):\n                    generate.main(input_args)\n\n\ndef cli_main():\n    parser = rerank_options.get_reranking_parser()\n    args = options.parse_args_and_arch(parser)\n    score_bw(args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
examples/noisychannel/rerank_score_lm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\n\nfrom fairseq import options\n\nfrom . import rerank_options, rerank_utils\n\n\ndef score_lm(args):\n    using_nbest = args.nbest_list is not None\n    pre_gen, left_to_right_preprocessed_dir, right_to_left_preprocessed_dir, \\\n        backwards_preprocessed_dir, lm_preprocessed_dir = \\\n        rerank_utils.get_directories(args.data_dir_name, args.num_rescore, args.gen_subset,\n                                     args.gen_model_name, args.shard_id, args.num_shards,\n                                     args.sampling, args.prefix_len, args.target_prefix_frac,\n                                     args.source_prefix_frac)\n\n    predictions_bpe_file = pre_gen+""/generate_output_bpe.txt""\n    if using_nbest:\n        print(""Using predefined n-best list from interactive.py"")\n        predictions_bpe_file = args.nbest_list\n\n    gen_output = rerank_utils.BitextOutputFromGen(predictions_bpe_file, bpe_symbol=args.remove_bpe, nbest=using_nbest)\n\n    if args.language_model is not None:\n        lm_score_file = rerank_utils.rescore_file_name(pre_gen, args.prefix_len, args.lm_name, lm_file=True)\n\n    if args.language_model is not None and not os.path.isfile(lm_score_file):\n        print(""STEP 4.5: language modeling for P(T)"")\n        if args.lm_bpe_code is None:\n            bpe_status = ""no bpe""\n        elif args.lm_bpe_code == ""shared"":\n            bpe_status = ""shared""\n        else:\n            bpe_status = ""different""\n\n        rerank_utils.lm_scoring(lm_preprocessed_dir, bpe_status, gen_output, pre_gen,\n                                args.lm_dict, args.lm_name, args.language_model,\n                                args.lm_bpe_code, 128, lm_score_file, args.target_lang,\n                                args.source_lang, prefix_len=args.prefix_len)\n\n\ndef cli_main():\n    parser = rerank_options.get_reranking_parser()\n    args = options.parse_args_and_arch(parser)\n    score_lm(args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
examples/noisychannel/rerank_tune.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport random\nimport numpy as np\n\nfrom fairseq import options\n\nfrom . import rerank, rerank_options\n\n\ndef random_search(args):\n    param_values = []\n    tuneable_parameters = [\'lenpen\', \'weight1\', \'weight2\', \'weight3\']\n    initial_params = [args.lenpen, args.weight1, args.weight2, args.weight3]\n    for i, elem in enumerate(initial_params):\n        if type(elem) is not list:\n            initial_params[i] = [elem]\n        else:\n            initial_params[i] = elem\n\n    tune_parameters = args.tune_param.copy()\n    for i in range(len(args.tune_param)):\n        assert args.upper_bound[i] >= args.lower_bound[i]\n        index = tuneable_parameters.index(args.tune_param[i])\n        del tuneable_parameters[index]\n        del initial_params[index]\n\n    tune_parameters += tuneable_parameters\n    param_values += initial_params\n    random.seed(args.seed)\n\n    random_params = np.array([\n        [random.uniform(args.lower_bound[i], args.upper_bound[i]) for i in range(len(args.tune_param))]\n        for k in range(args.num_trials)\n    ])\n    set_params = np.array([\n        [initial_params[i][0] for i in range(len(tuneable_parameters))]\n        for k in range(args.num_trials)\n    ])\n    random_params = np.concatenate((random_params, set_params), 1)\n\n    rerank_args = vars(args).copy()\n    if args.nbest_list:\n        rerank_args[\'gen_subset\'] = \'test\'\n    else:\n        rerank_args[\'gen_subset\'] = args.tune_subset\n\n    for k in range(len(tune_parameters)):\n        rerank_args[tune_parameters[k]] = list(random_params[:, k])\n\n    if args.share_weights:\n        k = tune_parameters.index(\'weight2\')\n        rerank_args[\'weight3\'] = list(random_params[:, k])\n\n    rerank_args = argparse.Namespace(**rerank_args)\n    best_lenpen, best_weight1, best_weight2, best_weight3, best_score = rerank.rerank(rerank_args)\n    rerank_args = vars(args).copy()\n    rerank_args[\'lenpen\'] = [best_lenpen]\n    rerank_args[\'weight1\'] = [best_weight1]\n    rerank_args[\'weight2\'] = [best_weight2]\n    rerank_args[\'weight3\'] = [best_weight3]\n\n    # write the hypothesis from the valid set from the best trial\n\n    if args.gen_subset != ""valid"":\n        rerank_args[\'gen_subset\'] = ""valid""\n        rerank_args = argparse.Namespace(**rerank_args)\n        rerank.rerank(rerank_args)\n\n    # test with the best hyperparameters on gen subset\n    rerank_args = vars(args).copy()\n    rerank_args[\'gen_subset\'] = args.gen_subset\n    rerank_args[\'lenpen\'] = [best_lenpen]\n    rerank_args[\'weight1\'] = [best_weight1]\n    rerank_args[\'weight2\'] = [best_weight2]\n    rerank_args[\'weight3\'] = [best_weight3]\n    rerank_args = argparse.Namespace(**rerank_args)\n    rerank.rerank(rerank_args)\n\n\ndef cli_main():\n    parser = rerank_options.get_tuning_parser()\n    args = options.parse_args_and_arch(parser)\n\n    random_search(args)\n\n\nif __name__ == \'__main__\':\n    cli_main()\n'"
examples/noisychannel/rerank_utils.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom contextlib import redirect_stdout\nimport math\nimport os\nimport re\nimport subprocess\n\nfrom fairseq import options\nfrom fairseq_cli import eval_lm, preprocess\n\n\ndef reprocess(fle):\n    # takes in a file of generate.py translation generate_output\n    # returns a source dict and hypothesis dict, where keys are the ID num (as a string)\n    # and values and the corresponding source and translation. There may be several translations\n    # per source, so the values for hypothesis_dict are lists.\n    # parses output of generate.py\n\n    with open(fle, \'r\') as f:\n        txt = f.read()\n\n    """"""reprocess generate.py output""""""\n    p = re.compile(r""[STHP][-]\\d+\\s*"")\n    hp = re.compile(r""(\\s*[-]?\\d+[.]?\\d+\\s*)|(\\s*(-inf)\\s*)"")\n    source_dict = {}\n    hypothesis_dict = {}\n    score_dict = {}\n    target_dict = {}\n    pos_score_dict = {}\n    lines = txt.split(""\\n"")\n\n    for line in lines:\n        line += ""\\n""\n        prefix = re.search(p, line)\n        if prefix is not None:\n            assert len(prefix.group()) > 2, ""prefix id not found""\n            _, j = prefix.span()\n            id_num = prefix.group()[2:]\n            id_num = int(id_num)\n            line_type = prefix.group()[0]\n            if line_type == ""H"":\n                h_txt = line[j:]\n                hypo = re.search(hp, h_txt)\n                assert hypo is not None, (""regular expression failed to find the hypothesis scoring"")\n                _, i = hypo.span()\n                score = hypo.group()\n                if id_num in hypothesis_dict:\n                    hypothesis_dict[id_num].append(h_txt[i:])\n                    score_dict[id_num].append(float(score))\n                else:\n                    hypothesis_dict[id_num] = [h_txt[i:]]\n                    score_dict[id_num] = [float(score)]\n\n            elif line_type == ""S"":\n                source_dict[id_num] = (line[j:])\n            elif line_type == ""T"":\n                target_dict[id_num] = (line[j:])\n            elif line_type == ""P"":\n                pos_scores = (line[j:]).split()\n                pos_scores = [float(x) for x in pos_scores]\n                if id_num in pos_score_dict:\n                    pos_score_dict[id_num].append(pos_scores)\n                else:\n                    pos_score_dict[id_num] = [pos_scores]\n\n    return source_dict, hypothesis_dict, score_dict, target_dict, pos_score_dict\n\n\ndef reprocess_nbest(fle):\n    """"""reprocess interactive.py output""""""\n    with open(fle, \'r\') as f:\n        txt = f.read()\n\n    source_dict = {}\n    hypothesis_dict = {}\n    score_dict = {}\n    target_dict = {}\n    pos_score_dict = {}\n    lines = txt.split(""\\n"")\n\n    hp = re.compile(r\'[-]?\\d+[.]?\\d+\')\n    j = -1\n\n    for _i, line in enumerate(lines):\n        line += ""\\n""\n        line_type = line[0]\n\n        if line_type == ""H"":\n            hypo = re.search(hp, line)\n            _, start_index = hypo.span()\n            score = hypo.group()\n            if j in score_dict:\n                score_dict[j].append(float(score))\n                hypothesis_dict[j].append(line[start_index:].strip(""\\t""))\n            else:\n                score_dict[j] = [float(score)]\n                hypothesis_dict[j] = [line[start_index:].strip(""\\t"")]\n        elif line_type == ""O"":\n            j += 1\n            source_dict[j] = line[2:]\n            # we don\'t have the targets for interactive.py\n            target_dict[j] = ""filler""\n\n        elif line_type == ""P"":\n            pos_scores = [float(pos_score) for pos_score in line.split()[1:]]\n            if j in pos_score_dict:\n                pos_score_dict[j].append(pos_scores)\n            else:\n                pos_score_dict[j] = [pos_scores]\n\n    assert source_dict.keys() == hypothesis_dict.keys()\n    assert source_dict.keys() == pos_score_dict.keys()\n    assert source_dict.keys() == score_dict.keys()\n\n    return source_dict, hypothesis_dict, score_dict, target_dict, pos_score_dict\n\n\ndef write_reprocessed(sources, hypos, targets, source_outfile,\n                      hypo_outfile, target_outfile, right_to_left=False,\n                      prefix_len=None, bpe_symbol=None,\n                      target_prefix_frac=None, source_prefix_frac=None):\n\n    """"""writes nbest hypothesis for rescoring""""""\n    assert not (prefix_len is not None and target_prefix_frac is not None), \\\n        ""in writing reprocessed, only one type of prefix may be used""\n    assert not (prefix_len is not None and source_prefix_frac is not None), \\\n        ""in writing reprocessed, only one type of prefix may be used""\n    assert not (target_prefix_frac is not None and source_prefix_frac is not None), \\\n        ""in writing reprocessed, only one type of prefix may be used""\n\n    with open(source_outfile, \'w\') as source_file, \\\n            open(hypo_outfile, \'w\') as hypo_file, \\\n            open(target_outfile, \'w\') as target_file:\n\n        assert len(sources) == len(hypos), ""sources and hypos list length mismatch""\n        if right_to_left:\n            for i in range(len(sources)):\n                    for j in range(len(hypos[i])):\n                        if prefix_len is None:\n                            hypo_file.write(make_right_to_left(hypos[i][j])+""\\n"")\n                        else:\n                            raise NotImplementedError()\n                        source_file.write(make_right_to_left(sources[i])+""\\n"")\n                        target_file.write(make_right_to_left(targets[i])+""\\n"")\n        else:\n            for i in sorted(sources.keys()):\n                    for j in range(len(hypos[i])):\n                        if prefix_len is not None:\n                            shortened = get_prefix_no_bpe(hypos[i][j], bpe_symbol, prefix_len)+""\\n""\n                            hypo_file.write(shortened)\n                            source_file.write(sources[i])\n                            target_file.write(targets[i])\n                        elif target_prefix_frac is not None:\n                            num_words, shortened, num_bpe_tokens = \\\n                                    calc_length_from_frac(hypos[i][j], target_prefix_frac, bpe_symbol)\n                            shortened += ""\\n""\n                            hypo_file.write(shortened)\n                            source_file.write(sources[i])\n                            target_file.write(targets[i])\n                        elif source_prefix_frac is not None:\n                            num_words, shortened, num_bpe_tokensn = \\\n                                    calc_length_from_frac(sources[i], source_prefix_frac, bpe_symbol)\n                            shortened += ""\\n""\n                            hypo_file.write(hypos[i][j])\n                            source_file.write(shortened)\n                            target_file.write(targets[i])\n                        else:\n                            hypo_file.write(hypos[i][j])\n                            source_file.write(sources[i])\n                            target_file.write(targets[i])\n\n\ndef calc_length_from_frac(bpe_sentence, prefix_frac, bpe_symbol):\n    # return number of words, (not bpe tokens) that we want\n    no_bpe_sen = remove_bpe(bpe_sentence, bpe_symbol)\n    len_sen = len(no_bpe_sen.split())\n\n    num_words = math.ceil(len_sen * prefix_frac)\n    prefix = get_prefix_no_bpe(bpe_sentence, bpe_symbol, num_words)\n    num_bpe_tokens = len(prefix.split())\n    return num_words, prefix, num_bpe_tokens\n\n\ndef get_prefix(sentence, prefix_len):\n    """"""assuming no bpe, gets the prefix of the sentence with prefix_len words""""""\n    tokens = sentence.strip(""\\n"").split()\n    if prefix_len >= len(tokens):\n        return sentence.strip(""\\n"")\n    else:\n        return "" "".join(tokens[:prefix_len])\n\n\ndef get_prefix_no_bpe(sentence, bpe_symbol, prefix_len):\n    if bpe_symbol is None:\n        return get_prefix(sentence, prefix_len)\n    else:\n        return "" "".join(get_prefix_from_len(sentence.split(), bpe_symbol, prefix_len))\n\n\ndef get_prefix_from_len(sentence, bpe_symbol, prefix_len):\n    """"""get the prefix of sentence with bpe, with prefix len in terms of words, not bpe tokens""""""\n    bpe_count = sum([bpe_symbol.strip("" "") in t for t in sentence[:prefix_len]])\n    if bpe_count == 0:\n        return sentence[:prefix_len]\n    else:\n        return sentence[:prefix_len]+get_prefix_from_len(sentence[prefix_len:], bpe_symbol, bpe_count)\n\n\ndef get_num_bpe_tokens_from_len(sentence, bpe_symbol, prefix_len):\n    """"""given a prefix length in terms of words, return the number of bpe tokens""""""\n    prefix = get_prefix_no_bpe(sentence, bpe_symbol, prefix_len)\n    assert len(remove_bpe(prefix, bpe_symbol).split()) <= prefix_len\n    return len(prefix.split("" ""))\n\n\ndef make_right_to_left(line):\n    tokens = line.split()\n    tokens.reverse()\n    new_line = "" "".join(tokens)\n    return new_line\n\n\ndef remove_bpe(line, bpe_symbol):\n    line = line.replace(""\\n"", \'\')\n    line = (line + \' \').replace(bpe_symbol, \'\').rstrip()\n    return line+(""\\n"")\n\n\ndef remove_bpe_dict(pred_dict, bpe_symbol):\n    new_dict = {}\n    for i in pred_dict:\n        if type(pred_dict[i]) == list:\n            new_list = [remove_bpe(elem, bpe_symbol) for elem in pred_dict[i]]\n            new_dict[i] = new_list\n        else:\n            new_dict[i] = remove_bpe(pred_dict[i], bpe_symbol)\n    return new_dict\n\n\ndef parse_bleu_scoring(line):\n    p = re.compile(r\'(BLEU4 = )\\d+[.]\\d+\')\n    res = re.search(p, line)\n    assert res is not None, line\n    return float(res.group()[8:])\n\n\ndef get_full_from_prefix(hypo_prefix, hypos):\n    """"""given a hypo prefix, recover the first hypo from the list of complete hypos beginning with that prefix""""""\n    for hypo in hypos:\n        hypo_prefix = hypo_prefix.strip(""\\n"")\n        len_prefix = len(hypo_prefix)\n        if hypo[:len_prefix] == hypo_prefix:\n            return hypo\n    # no match found\n    raise Exception()\n\n\ndef get_score(a, b, c, target_len, bitext_score1, bitext_score2=None, lm_score=None,\n              lenpen=None, src_len=None, tgt_len=None, bitext1_backwards=False,\n              bitext2_backwards=False, normalize=False):\n    if bitext1_backwards:\n        bitext1_norm = src_len\n    else:\n        bitext1_norm = tgt_len\n    if bitext_score2 is not None:\n        if bitext2_backwards:\n            bitext2_norm = src_len\n        else:\n            bitext2_norm = tgt_len\n    else:\n        bitext2_norm = 1\n        bitext_score2 = 0\n    if normalize:\n        score = a*bitext_score1/bitext1_norm + b*bitext_score2/bitext2_norm+c*lm_score/src_len\n    else:\n        score = a*bitext_score1 + b*bitext_score2+c*lm_score\n\n    if lenpen is not None:\n        score /= (target_len) ** float(lenpen)\n\n    return score\n\n\nclass BitextOutput(object):\n    def __init__(self, output_file, backwards, right_to_left, bpe_symbol,\n                 prefix_len=None, target_prefix_frac=None, source_prefix_frac=None):\n        """"""process output from rescoring""""""\n        source, hypo, score, target, pos_score = reprocess(output_file)\n        if backwards:\n            self.hypo_fracs = source_prefix_frac\n        else:\n            self.hypo_fracs = target_prefix_frac\n\n        # remove length penalty so we can use raw scores\n        score, num_bpe_tokens = get_score_from_pos(pos_score, prefix_len, hypo, bpe_symbol, self.hypo_fracs, backwards)\n        source_lengths = {}\n        target_lengths = {}\n\n        assert hypo.keys() == source.keys(), ""key mismatch""\n        if backwards:\n            tmp = hypo\n            hypo = source\n            source = tmp\n        for i in source:\n            # since we are reranking, there should only be one hypo per source sentence\n            if backwards:\n                len_src = len(source[i][0].split())\n                # record length without <eos>\n                if len_src == num_bpe_tokens[i][0] - 1:\n                    source_lengths[i] = num_bpe_tokens[i][0] - 1\n                else:\n                    source_lengths[i] = num_bpe_tokens[i][0]\n\n                target_lengths[i] = len(hypo[i].split())\n\n                source[i] = remove_bpe(source[i][0], bpe_symbol)\n                target[i] = remove_bpe(target[i], bpe_symbol)\n                hypo[i] = remove_bpe(hypo[i], bpe_symbol)\n\n                score[i] = float(score[i][0])\n                pos_score[i] = pos_score[i][0]\n\n            else:\n                len_tgt = len(hypo[i][0].split())\n                # record length without <eos>\n                if len_tgt == num_bpe_tokens[i][0] - 1:\n                    target_lengths[i] = num_bpe_tokens[i][0] - 1\n                else:\n                    target_lengths[i] = num_bpe_tokens[i][0]\n\n                source_lengths[i] = len(source[i].split())\n\n                if right_to_left:\n                    source[i] = remove_bpe(make_right_to_left(source[i]), bpe_symbol)\n                    target[i] = remove_bpe(make_right_to_left(target[i]), bpe_symbol)\n                    hypo[i] = remove_bpe(make_right_to_left(hypo[i][0]), bpe_symbol)\n                    score[i] = float(score[i][0])\n                    pos_score[i] = pos_score[i][0]\n                else:\n                    assert len(hypo[i]) == 1, ""expected only one hypothesis per source sentence""\n                    source[i] = remove_bpe(source[i], bpe_symbol)\n                    target[i] = remove_bpe(target[i], bpe_symbol)\n                    hypo[i] = remove_bpe(hypo[i][0], bpe_symbol)\n                    score[i] = float(score[i][0])\n                    pos_score[i] = pos_score[i][0]\n\n        self.rescore_source = source\n        self.rescore_hypo = hypo\n        self.rescore_score = score\n        self.rescore_target = target\n        self.rescore_pos_score = pos_score\n        self.backwards = backwards\n        self.right_to_left = right_to_left\n        self.target_lengths = target_lengths\n        self.source_lengths = source_lengths\n\n\nclass BitextOutputFromGen(object):\n    def __init__(self, predictions_bpe_file, bpe_symbol=None, nbest=False, prefix_len=None, target_prefix_frac=None):\n        if nbest:\n            pred_source, pred_hypo, pred_score, pred_target, pred_pos_score = reprocess_nbest(predictions_bpe_file)\n        else:\n            pred_source, pred_hypo, pred_score, pred_target, pred_pos_score = reprocess(predictions_bpe_file)\n\n        assert len(pred_source) == len(pred_hypo)\n        assert len(pred_source) == len(pred_score)\n        assert len(pred_source) == len(pred_target)\n        assert len(pred_source) == len(pred_pos_score)\n\n        # remove length penalty so we can use raw scores\n        pred_score, num_bpe_tokens = get_score_from_pos(pred_pos_score, prefix_len, pred_hypo,\n                                                        bpe_symbol, target_prefix_frac, False)\n\n        self.source = pred_source\n        self.target = pred_target\n        self.score = pred_score\n        self.pos_score = pred_pos_score\n        self.hypo = pred_hypo\n        self.target_lengths = {}\n        self.source_lengths = {}\n\n        self.no_bpe_source = remove_bpe_dict(pred_source.copy(), bpe_symbol)\n        self.no_bpe_hypo = remove_bpe_dict(pred_hypo.copy(), bpe_symbol)\n        self.no_bpe_target = remove_bpe_dict(pred_target.copy(), bpe_symbol)\n\n        # indexes to match those from the rescoring models\n        self.rescore_source = {}\n        self.rescore_target = {}\n        self.rescore_pos_score = {}\n        self.rescore_hypo = {}\n        self.rescore_score = {}\n        self.num_hypos = {}\n        self.backwards = False\n        self.right_to_left = False\n\n        index = 0\n\n        for i in sorted(pred_source.keys()):\n            for j in range(len(pred_hypo[i])):\n\n                self.target_lengths[index] = len(self.hypo[i][j].split())\n                self.source_lengths[index] = len(self.source[i].split())\n\n                self.rescore_source[index] = self.no_bpe_source[i]\n                self.rescore_target[index] = self.no_bpe_target[i]\n                self.rescore_hypo[index] = self.no_bpe_hypo[i][j]\n                self.rescore_score[index] = float(pred_score[i][j])\n                self.rescore_pos_score[index] = pred_pos_score[i][j]\n                self.num_hypos[index] = len(pred_hypo[i])\n                index += 1\n\n\ndef get_score_from_pos(pos_score_dict, prefix_len, hypo_dict, bpe_symbol, hypo_frac, backwards):\n    score_dict = {}\n    num_bpe_tokens_dict = {}\n    assert prefix_len is None or hypo_frac is None\n    for key in pos_score_dict:\n        score_dict[key] = []\n        num_bpe_tokens_dict[key] = []\n        for i in range(len(pos_score_dict[key])):\n            if prefix_len is not None and not backwards:\n                num_bpe_tokens = get_num_bpe_tokens_from_len(hypo_dict[key][i], bpe_symbol, prefix_len)\n                score_dict[key].append(sum(pos_score_dict[key][i][:num_bpe_tokens]))\n                num_bpe_tokens_dict[key].append(num_bpe_tokens)\n            elif hypo_frac is not None:\n                num_words, shortened, hypo_prefix_len = calc_length_from_frac(hypo_dict[key][i], hypo_frac, bpe_symbol)\n                score_dict[key].append(sum(pos_score_dict[key][i][:hypo_prefix_len]))\n                num_bpe_tokens_dict[key].append(hypo_prefix_len)\n            else:\n                score_dict[key].append(sum(pos_score_dict[key][i]))\n                num_bpe_tokens_dict[key].append(len(pos_score_dict[key][i]))\n    return score_dict, num_bpe_tokens_dict\n\n\nclass LMOutput(object):\n    def __init__(self, lm_score_file, lm_dict=None, prefix_len=None, bpe_symbol=None, target_prefix_frac=None):\n        lm_sentences, lm_sen_scores, lm_sen_pos_scores, lm_no_bpe_sentences, lm_bpe_tokens = \\\n                parse_lm(lm_score_file, prefix_len=prefix_len,\n                         bpe_symbol=bpe_symbol, target_prefix_frac=target_prefix_frac)\n\n        self.sentences = lm_sentences\n        self.score = lm_sen_scores\n        self.pos_score = lm_sen_pos_scores\n        self.lm_dict = lm_dict\n        self.no_bpe_sentences = lm_no_bpe_sentences\n        self.bpe_tokens = lm_bpe_tokens\n\n\ndef parse_lm(input_file, prefix_len=None, bpe_symbol=None, target_prefix_frac=None):\n    """"""parse output of eval_lm""""""\n    with open(input_file, \'r\') as f:\n        text = f.readlines()\n        text = text[7:]\n        cleaned_text = text[:-2]\n\n        sentences = {}\n        sen_scores = {}\n        sen_pos_scores = {}\n        no_bpe_sentences = {}\n        num_bpe_tokens_dict = {}\n        for _i, line in enumerate(cleaned_text):\n            tokens = line.split()\n            if tokens[0].isdigit():\n                line_id = int(tokens[0])\n                scores = [float(x[1:-1]) for x in tokens[2::2]]\n                sentences[line_id] = "" "".join(tokens[1::2][:-1])+""\\n""\n                if bpe_symbol is not None:\n                    # exclude <eos> symbol to match output from generate.py\n                    bpe_sen = "" "".join(tokens[1::2][:-1])+""\\n""\n                    no_bpe_sen = remove_bpe(bpe_sen, bpe_symbol)\n                    no_bpe_sentences[line_id] = no_bpe_sen\n\n                if prefix_len is not None:\n                    num_bpe_tokens = get_num_bpe_tokens_from_len(bpe_sen, bpe_symbol, prefix_len)\n                    sen_scores[line_id] = sum(scores[:num_bpe_tokens])\n                    num_bpe_tokens_dict[line_id] = num_bpe_tokens\n                elif target_prefix_frac is not None:\n                    num_words, shortened, target_prefix_len = calc_length_from_frac(bpe_sen, target_prefix_frac,\n                                                                                    bpe_symbol)\n                    sen_scores[line_id] = sum(scores[:target_prefix_len])\n                    num_bpe_tokens_dict[line_id] = target_prefix_len\n                else:\n                    sen_scores[line_id] = sum(scores)\n                    num_bpe_tokens_dict[line_id] = len(scores)\n\n                sen_pos_scores[line_id] = scores\n\n    return sentences, sen_scores, sen_pos_scores, no_bpe_sentences, num_bpe_tokens_dict\n\n\ndef get_directories(data_dir_name, num_rescore, gen_subset,\n                    fw_name, shard_id, num_shards,\n                    sampling=False, prefix_len=None,\n                    target_prefix_frac=None, source_prefix_frac=None):\n    nbest_file_id = ""nbest_"" + str(num_rescore) + \\\n                    ""_subset_"" + gen_subset + \\\n                    ""_fw_name_"" + fw_name + \\\n                    ""_shard_"" + str(shard_id) + \\\n                    ""_of_"" + str(num_shards)\n\n    if sampling:\n        nbest_file_id += ""_sampling""\n\n    # the directory containing all information for this nbest list\n    pre_gen = os.path.join(os.path.dirname(__file__))+""/rerank_data/""+data_dir_name+""/""+nbest_file_id\n    # the directory to store the preprocessed nbest list, for left to right rescoring\n    left_to_right_preprocessed_dir = pre_gen+""/left_to_right_preprocessed""\n    if source_prefix_frac is not None:\n        left_to_right_preprocessed_dir = left_to_right_preprocessed_dir + ""/prefix_frac"" + str(source_prefix_frac)\n    # the directory to store the preprocessed nbest list, for right to left rescoring\n    right_to_left_preprocessed_dir = pre_gen+""/right_to_left_preprocessed""\n    # the directory to store the preprocessed nbest list, for backwards rescoring\n    backwards_preprocessed_dir = pre_gen+""/backwards""\n    if target_prefix_frac is not None:\n        backwards_preprocessed_dir = backwards_preprocessed_dir+""/prefix_frac""+str(target_prefix_frac)\n    elif prefix_len is not None:\n        backwards_preprocessed_dir = backwards_preprocessed_dir+""/prefix_""+str(prefix_len)\n\n    # the directory to store the preprocessed nbest list, for rescoring with P(T)\n    lm_preprocessed_dir = pre_gen+""/lm_preprocessed""\n\n    return pre_gen, left_to_right_preprocessed_dir, right_to_left_preprocessed_dir, \\\n        backwards_preprocessed_dir, lm_preprocessed_dir\n\n\ndef lm_scoring(preprocess_directory, bpe_status, gen_output, pre_gen,\n               cur_lm_dict, cur_lm_name, cur_language_model, cur_lm_bpe_code,\n               batch_size, lm_score_file, target_lang, source_lang, prefix_len=None):\n    if prefix_len is not None:\n        assert bpe_status == ""different"", ""bpe status must be different to use prefix len""\n    if bpe_status == ""no bpe"":\n        # run lm on output without bpe\n        write_reprocessed(gen_output.no_bpe_source, gen_output.no_bpe_hypo,\n                          gen_output.no_bpe_target, pre_gen+""/rescore_data_no_bpe.de"",\n                          pre_gen+""/rescore_data_no_bpe.en"", pre_gen+""/reference_file_no_bpe"")\n\n        preprocess_lm_param = [""--only-source"",\n                               ""--trainpref"", pre_gen+""/rescore_data_no_bpe.""+target_lang,\n                               ""--srcdict"", cur_lm_dict,\n                               ""--destdir"", preprocess_directory]\n        preprocess_parser = options.get_preprocessing_parser()\n        input_args = preprocess_parser.parse_args(preprocess_lm_param)\n        preprocess.main(input_args)\n\n        eval_lm_param = [preprocess_directory,\n                         ""--path"", cur_language_model,\n                         ""--output-word-probs"",\n                         ""--batch-size"", str(batch_size),\n                         ""--max-tokens"", ""1024"",\n                         ""--sample-break-mode"", ""eos"",\n                         ""--gen-subset"", ""train""]\n\n        eval_lm_parser = options.get_eval_lm_parser()\n        input_args = options.parse_args_and_arch(eval_lm_parser, eval_lm_param)\n\n        with open(lm_score_file, \'w\') as f:\n            with redirect_stdout(f):\n                eval_lm.main(input_args)\n\n    elif bpe_status == ""shared"":\n            preprocess_lm_param = [""--only-source"",\n                                   ""--trainpref"", pre_gen+""/rescore_data.""+target_lang,\n                                   ""--srcdict"", cur_lm_dict,\n                                   ""--destdir"", preprocess_directory]\n            preprocess_parser = options.get_preprocessing_parser()\n            input_args = preprocess_parser.parse_args(preprocess_lm_param)\n            preprocess.main(input_args)\n\n            eval_lm_param = [preprocess_directory,\n                             ""--path"", cur_language_model,\n                             ""--output-word-probs"",\n                             ""--batch-size"", str(batch_size),\n                             ""--sample-break-mode"", ""eos"",\n                             ""--gen-subset"", ""train""]\n\n            eval_lm_parser = options.get_eval_lm_parser()\n            input_args = options.parse_args_and_arch(eval_lm_parser, eval_lm_param)\n\n            with open(lm_score_file, \'w\') as f:\n                with redirect_stdout(f):\n                    eval_lm.main(input_args)\n\n    elif bpe_status == ""different"":\n        rescore_file = pre_gen+""/rescore_data_no_bpe""\n        rescore_bpe = pre_gen+""/rescore_data_new_bpe""\n\n        rescore_file += "".""\n        rescore_bpe += "".""\n\n        write_reprocessed(gen_output.no_bpe_source, gen_output.no_bpe_hypo,\n                          gen_output.no_bpe_target, rescore_file+source_lang,\n                          rescore_file+target_lang, pre_gen+""/reference_file_no_bpe"",\n                          bpe_symbol=None)\n\n        # apply LM bpe to nbest list\n        bpe_src_param = [""-c"", cur_lm_bpe_code,\n                         ""--input"", rescore_file+target_lang,\n                         ""--output"", rescore_bpe+target_lang]\n        subprocess.call([""python"",\n                         os.path.join(os.path.dirname(__file__),\n                                      ""subword-nmt/subword_nmt/apply_bpe.py"")] + bpe_src_param,\n                        shell=False)\n        # uncomment to use fastbpe instead of subword-nmt bpe\n        # bpe_src_param = [rescore_bpe+target_lang, rescore_file+target_lang, cur_lm_bpe_code]\n        # subprocess.call([""/private/home/edunov/fastBPE/fast"", ""applybpe""] + bpe_src_param, shell=False)\n\n        preprocess_dir = preprocess_directory\n\n        preprocess_lm_param = [""--only-source"",\n                               ""--trainpref"", rescore_bpe+target_lang,\n                               ""--srcdict"", cur_lm_dict,\n                               ""--destdir"", preprocess_dir]\n        preprocess_parser = options.get_preprocessing_parser()\n        input_args = preprocess_parser.parse_args(preprocess_lm_param)\n        preprocess.main(input_args)\n\n        eval_lm_param = [preprocess_dir,\n                         ""--path"", cur_language_model,\n                         ""--output-word-probs"",\n                         ""--batch-size"", str(batch_size),\n                         ""--max-tokens"", ""1024"",\n                         ""--sample-break-mode"", ""eos"",\n                         ""--gen-subset"", ""train""]\n\n        eval_lm_parser = options.get_eval_lm_parser()\n        input_args = options.parse_args_and_arch(eval_lm_parser, eval_lm_param)\n\n        with open(lm_score_file, \'w\') as f:\n            with redirect_stdout(f):\n                eval_lm.main(input_args)\n\n\ndef rescore_file_name(nbest_dir, prefix_len, scorer_name, lm_file=False,\n                      target_prefix_frac=None, source_prefix_frac=None, backwards=None):\n    if lm_file:\n        score_file = nbest_dir+""/lm_score_translations_model_""+scorer_name+"".txt""\n    else:\n        score_file = nbest_dir+""/""+scorer_name+""_score_translations.txt""\n    if backwards:\n        if prefix_len is not None:\n            score_file += ""prefix_len""+str(prefix_len)\n        elif target_prefix_frac is not None:\n            score_file += ""target_prefix_frac""+str(target_prefix_frac)\n    else:\n        if source_prefix_frac is not None:\n            score_file += ""source_prefix_frac""+str(source_prefix_frac)\n    return score_file\n'"
examples/paraphraser/paraphrase.py,0,"b'#!/usr/bin/env python3 -u\n\nimport argparse\nimport fileinput\nimport logging\nimport os\nimport sys\n\nfrom fairseq.models.transformer import TransformerModel\n\n\nlogging.getLogger().setLevel(logging.INFO)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'--en2fr\', required=True,\n                        help=\'path to en2fr model\')\n    parser.add_argument(\'--fr2en\', required=True,\n                        help=\'path to fr2en mixture of experts model\')\n    parser.add_argument(\'--user-dir\',\n                        help=\'path to fairseq examples/translation_moe/src directory\')\n    parser.add_argument(\'--num-experts\', type=int, default=10,\n                        help=\'(keep at 10 unless using a different model)\')\n    parser.add_argument(\'files\', nargs=\'*\', default=[\'-\'],\n                        help=\'input files to paraphrase; ""-"" for stdin\')\n    args = parser.parse_args()\n\n    if args.user_dir is None:\n        args.user_dir = os.path.join(\n            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),  # examples/\n            \'translation_moe\',\n            \'src\',\n        )\n        if os.path.exists(args.user_dir):\n            logging.info(\'found user_dir:\' + args.user_dir)\n        else:\n            raise RuntimeError(\n                \'cannot find fairseq examples/translation_moe/src \'\n                \'(tried looking here: {})\'.format(args.user_dir)\n            )\n\n    logging.info(\'loading en2fr model from:\' + args.en2fr)\n    en2fr = TransformerModel.from_pretrained(\n        model_name_or_path=args.en2fr,\n        tokenizer=\'moses\',\n        bpe=\'sentencepiece\',\n    ).eval()\n\n    logging.info(\'loading fr2en model from:\' + args.fr2en)\n    fr2en = TransformerModel.from_pretrained(\n        model_name_or_path=args.fr2en,\n        tokenizer=\'moses\',\n        bpe=\'sentencepiece\',\n        user_dir=args.user_dir,\n        task=\'translation_moe\',\n    ).eval()\n\n    def gen_paraphrases(en):\n        fr = en2fr.translate(en)\n        return [\n            fr2en.translate(fr, inference_step_args={\'expert\': i})\n            for i in range(args.num_experts)\n        ]\n\n    logging.info(\'Type the input sentence and press return:\')\n    for line in fileinput.input(args.files):\n        line = line.strip()\n        if len(line) == 0:\n            continue\n        for paraphrase in gen_paraphrases(line):\n            print(paraphrase)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/roberta/multiprocessing_bpe_encoder.py,0,"b'#!/usr/bin/env python\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport contextlib\nimport sys\n\nfrom collections import Counter\nfrom multiprocessing import Pool\n\nfrom fairseq.data.encoders.gpt2_bpe import get_encoder\n\n\ndef main():\n    """"""\n    Helper script to encode raw text with the GPT-2 BPE using multiple processes.\n\n    The encoder.json and vocab.bpe files can be obtained here:\n    - https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n    - https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--encoder-json"",\n        help=\'path to encoder.json\',\n    )\n    parser.add_argument(\n        ""--vocab-bpe"",\n        type=str,\n        help=\'path to vocab.bpe\',\n    )\n    parser.add_argument(\n        ""--inputs"",\n        nargs=""+"",\n        default=[\'-\'],\n        help=""input files to filter/encode"",\n    )\n    parser.add_argument(\n        ""--outputs"",\n        nargs=""+"",\n        default=[\'-\'],\n        help=""path to save encoded outputs"",\n    )\n    parser.add_argument(\n        ""--keep-empty"",\n        action=""store_true"",\n        help=""keep empty lines"",\n    )\n    parser.add_argument(""--workers"", type=int, default=20)\n    args = parser.parse_args()\n\n    assert len(args.inputs) == len(args.outputs), \\\n        ""number of input and output paths should match""\n\n    with contextlib.ExitStack() as stack:\n        inputs = [\n            stack.enter_context(open(input, ""r"", encoding=""utf-8""))\n            if input != ""-"" else sys.stdin\n            for input in args.inputs\n        ]\n        outputs = [\n            stack.enter_context(open(output, ""w"", encoding=""utf-8""))\n            if output != ""-"" else sys.stdout\n            for output in args.outputs\n        ]\n\n        encoder = MultiprocessingEncoder(args)\n        pool = Pool(args.workers, initializer=encoder.initializer)\n        encoded_lines = pool.imap(encoder.encode_lines, zip(*inputs), 100)\n\n        stats = Counter()\n        for i, (filt, enc_lines) in enumerate(encoded_lines, start=1):\n            if filt == ""PASS"":\n                for enc_line, output_h in zip(enc_lines, outputs):\n                    print(enc_line, file=output_h)\n            else:\n                stats[""num_filtered_"" + filt] += 1\n            if i % 10000 == 0:\n                print(""processed {} lines"".format(i), file=sys.stderr)\n\n        for k, v in stats.most_common():\n            print(""[{}] filtered {} lines"".format(k, v), file=sys.stderr)\n\n\nclass MultiprocessingEncoder(object):\n\n    def __init__(self, args):\n        self.args = args\n\n    def initializer(self):\n        global bpe\n        bpe = get_encoder(self.args.encoder_json, self.args.vocab_bpe)\n\n    def encode(self, line):\n        global bpe\n        ids = bpe.encode(line)\n        return list(map(str, ids))\n\n    def decode(self, tokens):\n        global bpe\n        return bpe.decode(tokens)\n\n    def encode_lines(self, lines):\n        """"""\n        Encode a set of lines. All lines will be encoded together.\n        """"""\n        enc_lines = []\n        for line in lines:\n            line = line.strip()\n            if len(line) == 0 and not self.args.keep_empty:\n                return [""EMPTY"", None]\n            tokens = self.encode(line)\n            enc_lines.append("" "".join(tokens))\n        return [""PASS"", enc_lines]\n\n    def decode_lines(self, lines):\n        dec_lines = []\n        for line in lines:\n            tokens = map(int, line.strip().split())\n            dec_lines.append(self.decode(tokens))\n        return [""PASS"", dec_lines]\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/roberta/preprocess_RACE.py,0,"b'#!/usr/bin/env python\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport json\nimport os\nimport re\n\n\nclass InputExample:\n    def __init__(self, paragraph, qa_list, label):\n        self.paragraph = paragraph\n        self.qa_list = qa_list\n        self.label = label\n\n\ndef get_examples(data_dir, set_type):\n    """"""\n    Extract paragraph and question-answer list from each json file\n    """"""\n    examples = []\n\n    levels = [""middle"", ""high""]\n    set_type_c = set_type.split(\'-\')\n    if len(set_type_c) == 2:\n        levels = [set_type_c[1]]\n        set_type = set_type_c[0]\n    for level in levels:\n        cur_dir = os.path.join(data_dir, set_type, level)\n        for filename in os.listdir(cur_dir):\n            cur_path = os.path.join(cur_dir, filename)\n            with open(cur_path, \'r\') as f:\n                cur_data = json.load(f)\n                answers = cur_data[""answers""]\n                options = cur_data[""options""]\n                questions = cur_data[""questions""]\n                context = cur_data[""article""].replace(""\\n"", "" "")\n                context = re.sub(r\'\\s+\', \' \', context)\n                for i in range(len(answers)):\n                    label = ord(answers[i]) - ord(""A"")\n                    qa_list = []\n                    question = questions[i]\n                    for j in range(4):\n                        option = options[i][j]\n                        if ""_"" in question:\n                            qa_cat = question.replace(""_"", option)\n                        else:\n                            qa_cat = "" "".join([question, option])\n                        qa_cat = re.sub(r\'\\s+\', \' \', qa_cat)\n                        qa_list.append(qa_cat)\n                    examples.append(InputExample(context, qa_list, label))\n\n    return examples\n\n\ndef main():\n    """"""\n    Helper script to extract paragraphs questions and answers from RACE datasets.\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--input-dir"",\n        help=\'input directory for downloaded RACE dataset\',\n    )\n    parser.add_argument(\n        ""--output-dir"",\n        help=\'output directory for extracted data\',\n    )\n    args = parser.parse_args()\n\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir, exist_ok=True)\n\n    for set_type in [""train"", ""dev"", ""test-middle"", ""test-high""]:\n        examples = get_examples(args.input_dir, set_type)\n        qa_file_paths = [os.path.join(args.output_dir, set_type + "".input"" + str(i + 1)) for i in range(4)]\n        qa_files = [open(qa_file_path, \'w\') for qa_file_path in qa_file_paths]\n        outf_context_path = os.path.join(args.output_dir, set_type + "".input0"")\n        outf_label_path = os.path.join(args.output_dir, set_type + "".label"")\n        outf_context = open(outf_context_path, \'w\')\n        outf_label = open(outf_label_path, \'w\')\n        for example in examples:\n            outf_context.write(example.paragraph + \'\\n\')\n            for i in range(4):\n                qa_files[i].write(example.qa_list[i] + \'\\n\')\n            outf_label.write(str(example.label) + \'\\n\')\n\n        for f in qa_files:\n            f.close()\n        outf_label.close()\n        outf_context.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/simultaneous_translation/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import criterions, models, eval  # noqa\n'"
examples/speech_recognition/__init__.py,0,"b'from . import tasks, criterions, models  # noqa\n'"
examples/speech_recognition/infer.py,2,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nRun inference for pre-processed data with a trained model.\n""""""\n\nimport logging\nimport math\nimport os\n\nimport sentencepiece as spm\nimport torch\nfrom fairseq import checkpoint_utils, options, utils, tasks\nfrom fairseq.logging import meters, progress_bar\nfrom fairseq.utils import import_user_module\n\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n\ndef add_asr_eval_argument(parser):\n    parser.add_argument(""--kspmodel"", default=None, help=""sentence piece model"")\n    parser.add_argument(\n        ""--wfstlm"", default=None, help=""wfstlm on dictonary output units""\n    )\n    parser.add_argument(\n        ""--rnnt_decoding_type"",\n        default=""greedy"",\n        help=""wfstlm on dictonary\\\noutput units"",\n    )\n    parser.add_argument(\n        ""--lm-weight"",\n        ""--lm_weight"",\n        type=float,\n        default=0.2,\n        help=""weight for lm while interpolating with neural score"",\n    )\n    parser.add_argument(\n        ""--rnnt_len_penalty"", default=-0.5, help=""rnnt length penalty on word level""\n    )\n    parser.add_argument(\n        ""--w2l-decoder"", choices=[""viterbi"", ""kenlm""], help=""use a w2l decoder""\n    )\n    parser.add_argument(""--lexicon"", help=""lexicon for w2l decoder"")\n    parser.add_argument(""--kenlm-model"", help=""kenlm model for w2l decoder"")\n    parser.add_argument(""--beam-threshold"", type=float, default=25.0)\n    parser.add_argument(""--word-score"", type=float, default=1.0)\n    parser.add_argument(""--unk-weight"", type=float, default=-math.inf)\n    parser.add_argument(""--sil-weight"", type=float, default=0.0)\n    return parser\n\n\ndef check_args(args):\n    assert args.path is not None, ""--path required for generation!""\n    assert args.results_path is not None, ""--results_path required for generation!""\n    assert (\n        not args.sampling or args.nbest == args.beam\n    ), ""--sampling requires --nbest to be equal to --beam""\n    assert (\n        args.replace_unk is None or args.dataset_impl == ""raw""\n    ), ""--replace-unk requires a raw text dataset (--dataset-impl=raw)""\n\n\ndef get_dataset_itr(args, task):\n    return task.get_batch_iterator(\n        dataset=task.dataset(args.gen_subset),\n        max_tokens=args.max_tokens,\n        max_sentences=args.max_sentences,\n        max_positions=(1000000.0, 1000000.0),\n        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=args.required_batch_size_multiple,\n        num_shards=args.num_shards,\n        shard_id=args.shard_id,\n        num_workers=args.num_workers,\n    ).next_epoch_itr(shuffle=False)\n\n\ndef process_predictions(\n    args, hypos, sp, tgt_dict, target_tokens, res_files, speaker, id\n):\n    for hypo in hypos[: min(len(hypos), args.nbest)]:\n        hyp_pieces = tgt_dict.string(hypo[""tokens""].int().cpu())\n        hyp_words = sp.DecodePieces(hyp_pieces.split())\n        print(\n            ""{} ({}-{})"".format(hyp_pieces, speaker, id), file=res_files[""hypo.units""]\n        )\n        print(""{} ({}-{})"".format(hyp_words, speaker, id), file=res_files[""hypo.words""])\n\n        tgt_pieces = tgt_dict.string(target_tokens)\n        tgt_words = sp.DecodePieces(tgt_pieces.split())\n        print(""{} ({}-{})"".format(tgt_pieces, speaker, id), file=res_files[""ref.units""])\n        print(""{} ({}-{})"".format(tgt_words, speaker, id), file=res_files[""ref.words""])\n        # only score top hypothesis\n        if not args.quiet:\n            logger.debug(""HYPO:"" + hyp_words)\n            logger.debug(""TARGET:"" + tgt_words)\n            logger.debug(""___________________"")\n\n\ndef prepare_result_files(args):\n    def get_res_file(file_prefix):\n        path = os.path.join(\n            args.results_path,\n            ""{}-{}-{}.txt"".format(\n                file_prefix, os.path.basename(args.path), args.gen_subset\n            ),\n        )\n        return open(path, ""w"", buffering=1)\n\n    return {\n        ""hypo.words"": get_res_file(""hypo.word""),\n        ""hypo.units"": get_res_file(""hypo.units""),\n        ""ref.words"": get_res_file(""ref.word""),\n        ""ref.units"": get_res_file(""ref.units""),\n    }\n\n\ndef load_models_and_criterions(filenames, arg_overrides=None, task=None):\n    models = []\n    criterions = []\n    for filename in filenames:\n        if not os.path.exists(filename):\n            raise IOError(""Model file not found: {}"".format(filename))\n        state = checkpoint_utils.load_checkpoint_to_cpu(filename, arg_overrides)\n\n        args = state[""args""]\n        if task is None:\n            task = tasks.setup_task(args)\n\n        # build model for ensemble\n        model = task.build_model(args)\n        model.load_state_dict(state[""model""], strict=True)\n        models.append(model)\n\n        criterion = task.build_criterion(args)\n        if ""criterion"" in state:\n            criterion.load_state_dict(state[""criterion""], strict=True)\n        criterions.append(criterion)\n    return models, criterions, args\n\n\ndef optimize_models(args, use_cuda, models):\n    """"""Optimize ensemble for generation\n    """"""\n    for model in models:\n        model.make_generation_fast_(\n            beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,\n            need_attn=args.print_alignment,\n        )\n        if args.fp16:\n            model.half()\n        if use_cuda:\n            model.cuda()\n\n\ndef main(args):\n    check_args(args)\n    import_user_module(args)\n\n    if args.max_tokens is None and args.max_sentences is None:\n        args.max_tokens = 30000\n    logger.info(args)\n\n    use_cuda = torch.cuda.is_available() and not args.cpu\n\n    # Load dataset splits\n    task = tasks.setup_task(args)\n    task.load_dataset(args.gen_subset)\n    logger.info(\n        ""| {} {} {} examples"".format(\n            args.data, args.gen_subset, len(task.dataset(args.gen_subset))\n        )\n    )\n\n    # Set dictionary\n    tgt_dict = task.target_dictionary\n\n    logger.info(""| decoding with criterion {}"".format(args.criterion))\n\n    # Load ensemble\n    logger.info(""| loading model(s) from {}"".format(args.path))\n    models, criterions, _model_args = load_models_and_criterions(\n        args.path.split(os.pathsep),\n        arg_overrides=eval(args.model_overrides),  # noqa\n        task=task,\n    )\n    optimize_models(args, use_cuda, models)\n\n    # hack to pass transitions to W2lDecoder\n    if args.criterion == ""asg_loss"":\n        trans = criterions[0].asg.trans.data\n        args.asg_transitions = torch.flatten(trans).tolist()\n\n    # Load dataset (possibly sharded)\n    itr = get_dataset_itr(args, task)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=args.log_format,\n        log_interval=args.log_interval,\n        default_log_format=(\'tqdm\' if not args.no_progress_bar else \'none\'),\n    )\n\n    # Initialize generator\n    gen_timer = meters.StopwatchMeter()\n    generator = task.build_generator(models, args)\n\n    num_sentences = 0\n\n    if not os.path.exists(args.results_path):\n        os.makedirs(args.results_path)\n\n    sp = spm.SentencePieceProcessor()\n    sp.Load(os.path.join(args.data, ""spm.model""))\n\n    res_files = prepare_result_files(args)\n    wps_meter = meters.TimeMeter()\n    for sample in progress:\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n        if ""net_input"" not in sample:\n            continue\n\n        prefix_tokens = None\n        if args.prefix_size > 0:\n            prefix_tokens = sample[""target""][:, : args.prefix_size]\n\n        gen_timer.start()\n        hypos = task.inference_step(generator, models, sample, prefix_tokens)\n        num_generated_tokens = sum(len(h[0][""tokens""]) for h in hypos)\n        gen_timer.stop(num_generated_tokens)\n\n        for i, sample_id in enumerate(sample[""id""].tolist()):\n            speaker = task.dataset(args.gen_subset).speakers[int(sample_id)]\n            id = task.dataset(args.gen_subset).ids[int(sample_id)]\n            target_tokens = (\n                utils.strip_pad(sample[""target""][i, :], tgt_dict.pad()).int().cpu()\n            )\n            # Process top predictions\n            process_predictions(\n                args, hypos[i], sp, tgt_dict, target_tokens, res_files, speaker, id\n            )\n\n        wps_meter.update(num_generated_tokens)\n        progress.log({""wps"": round(wps_meter.avg)})\n        num_sentences += sample[""nsentences""]\n\n    logger.info(\n        ""| Processed {} sentences ({} tokens) in {:.1f}s ({:.2f}""\n        ""sentences/s, {:.2f} tokens/s)"".format(\n            num_sentences,\n            gen_timer.n,\n            gen_timer.sum,\n            num_sentences / gen_timer.sum,\n            1.0 / gen_timer.avg,\n        )\n    )\n    logger.info(""| Generate {} with beam={}"".format(args.gen_subset, args.beam))\n\n\ndef cli_main():\n    parser = options.get_generation_parser()\n    parser = add_asr_eval_argument(parser)\n    args = options.parse_args_and_arch(parser)\n    main(args)\n\n\nif __name__ == ""__main__"":\n    cli_main()\n'"
examples/speech_recognition/w2l_decoder.py,5,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nWav2letter decoders.\n""""""\nimport math\nimport itertools as it\nimport torch\nfrom fairseq import utils\nfrom examples.speech_recognition.data.replabels import unpack_replabels\n\ntry:\n    from wav2letter.common import create_word_dict, load_words\n    from wav2letter.criterion import CpuViterbiPath, get_data_ptr_as_bytes\n    from wav2letter.decoder import (\n        CriterionType,\n        DecoderOptions,\n        KenLM,\n        SmearingMode,\n        Trie,\n        WordLMDecoder,\n    )\nexcept ImportError:\n    # wav2letter is a required dependency for the speech_recognition\n    # example, but don\'t break on import\n    pass\n\n\nclass W2lDecoder(object):\n    def __init__(self, args, tgt_dict):\n        self.tgt_dict = tgt_dict\n        self.vocab_size = len(tgt_dict)\n        self.nbest = args.nbest\n\n        # criterion-specific init\n        if args.criterion == ""ctc_loss"":\n            self.criterion_type = CriterionType.CTC\n            self.blank = tgt_dict.index(""<ctc_blank>"")\n            self.asg_transitions = None\n        elif args.criterion == ""asg_loss"":\n            self.criterion_type = CriterionType.ASG\n            self.blank = -1\n            self.asg_transitions = args.asg_transitions\n            self.max_replabel = args.max_replabel\n            assert len(self.asg_transitions) == self.vocab_size ** 2\n        else:\n            raise RuntimeError(f""unknown criterion: {args.criterion}"")\n\n    def generate(self, models, sample, prefix_tokens=None):\n        """"""Generate a batch of inferences.""""""\n        # model.forward normally channels prev_output_tokens into the decoder\n        # separately, but SequenceGenerator directly calls model.encoder\n        encoder_input = {\n            k: v for k, v in sample[""net_input""].items() if k != ""prev_output_tokens""\n        }\n        emissions = self.get_emissions(models, encoder_input)\n        return self.decode(emissions)\n\n    def get_emissions(self, models, encoder_input):\n        """"""Run encoder and normalize emissions""""""\n        encoder_out = models[0].encoder(**encoder_input)\n        if self.criterion_type == CriterionType.CTC:\n            emissions = models[0].get_normalized_probs(encoder_out, log_probs=True)\n        elif self.criterion_type == CriterionType.ASG:\n            emissions = encoder_out[""encoder_out""]\n        return emissions.transpose(0, 1).float().cpu().contiguous()\n\n    def get_tokens(self, idxs):\n        """"""Normalize tokens by handling CTC blank, ASG replabels, etc.""""""\n        idxs = (g[0] for g in it.groupby(idxs))\n        idxs = filter(lambda x: x >= 0, idxs)\n        if self.criterion_type == CriterionType.CTC:\n            idxs = filter(lambda x: x != self.blank, idxs)\n        elif self.criterion_type == CriterionType.ASG:\n            idxs = unpack_replabels(list(idxs), self.tgt_dict, self.max_replabel)\n        return torch.LongTensor(list(idxs))\n\n\nclass W2lViterbiDecoder(W2lDecoder):\n    def __init__(self, args, tgt_dict):\n        super().__init__(args, tgt_dict)\n\n    def decode(self, emissions):\n        B, T, N = emissions.size()\n        hypos = []\n        if self.asg_transitions is None:\n            transitions = torch.FloatTensor(N, N).zero_()\n        else:\n            transitions = torch.FloatTensor(self.asg_transitions).view(N, N)\n        viterbi_path = torch.IntTensor(B, T)\n        workspace = torch.ByteTensor(CpuViterbiPath.get_workspace_size(B, T, N))\n        CpuViterbiPath.compute(\n            B,\n            T,\n            N,\n            get_data_ptr_as_bytes(emissions),\n            get_data_ptr_as_bytes(transitions),\n            get_data_ptr_as_bytes(viterbi_path),\n            get_data_ptr_as_bytes(workspace),\n        )\n        return [\n            [{""tokens"": self.get_tokens(viterbi_path[b].tolist()), ""score"": 0}]\n            for b in range(B)\n        ]\n\n\nclass W2lKenLMDecoder(W2lDecoder):\n    def __init__(self, args, tgt_dict):\n        super().__init__(args, tgt_dict)\n\n        self.silence = tgt_dict.index(args.silence_token)\n\n        self.lexicon = load_words(args.lexicon)\n        self.word_dict = create_word_dict(self.lexicon)\n        self.unk_word = self.word_dict.get_index(""<unk>"")\n\n        self.lm = KenLM(args.kenlm_model, self.word_dict)\n        self.trie = Trie(self.vocab_size, self.silence)\n\n        start_state = self.lm.start(False)\n        for word, spellings in self.lexicon.items():\n            word_idx = self.word_dict.get_index(word)\n            _, score = self.lm.score(start_state, word_idx)\n            for spelling in spellings:\n                spelling_idxs = [tgt_dict.index(token) for token in spelling]\n                self.trie.insert(spelling_idxs, word_idx, score)\n        self.trie.smear(SmearingMode.MAX)\n\n        self.decoder_opts = DecoderOptions(\n            args.beam,\n            args.beam_threshold,\n            args.lm_weight,\n            args.word_score,\n            args.unk_weight,\n            False,\n            args.sil_weight,\n            self.criterion_type,\n        )\n\n        self.decoder = WordLMDecoder(\n            self.decoder_opts,\n            self.trie,\n            self.lm,\n            self.silence,\n            self.blank,\n            self.unk_word,\n            self.asg_transitions,\n        )\n\n    def decode(self, emissions):\n        B, T, N = emissions.size()\n        hypos = []\n        for b in range(B):\n            emissions_ptr = emissions.data_ptr() + 4 * b * emissions.stride(0)\n            nbest_results = self.decoder.decode(emissions_ptr, T, N)[: self.nbest]\n            hypos.append(\n                [\n                    {""tokens"": self.get_tokens(result.tokens), ""score"": result.score}\n                    for result in nbest_results\n                ]\n            )\n        return hypos\n'"
examples/translation_moe/score.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nScoring script for computing pairwise BLEU and multi-ref BLEU over a set of\ncandidate hypotheses.\n\nSee `""Mixture Models for Diverse Machine Translation: Tricks of the Trade""\n(Shen et al., 2019) <https://arxiv.org/abs/1902.07816>`_.\n""""""\n\nimport argparse\nfrom itertools import chain\nimport sys\nimport random\n\nimport numpy as np\nfrom sacrebleu import compute_bleu, corpus_bleu as _corpus_bleu\n\n\ndef main():\n    parser = argparse.ArgumentParser(sys.argv[0])\n    parser.add_argument(\'--sys\', nargs=\'*\', default=\'\', metavar=\'FILE\',\n                        help=\'path to system output\')\n    parser.add_argument(\'--ref\', default=\'\', metavar=\'FILE\',\n                        help=\'path to references\')\n    parser.add_argument(\'--output\', default=\'\', metavar=\'FILE\',\n                        help=\'print outputs into a pretty format\')\n    args = parser.parse_args()\n\n    if args.sys:\n        src, tgt, hypos, log_probs = load_sys(args.sys)\n        print(\'pairwise BLEU: %.2f\' % pairwise(hypos))\n        if args.output:\n            merge(src, tgt, hypos, log_probs, args.output)\n\n    if args.ref:\n        _, _, refs = load_ref(args.ref)\n        if args.sys:\n            multi_ref(refs, hypos)\n        else:\n            intra_ref(refs)\n\n\ndef dictolist(d):\n    a = sorted(d.items(), key=lambda i: i[0])\n    return [i[1] for i in a]\n\n\ndef load_sys(paths):\n    src, tgt, hypos, log_probs = {}, {}, {}, {}\n    for path in paths:\n        with open(path) as f:\n            for line in f:\n                line = line.rstrip()\n                if line.startswith((\'S-\', \'T-\', \'H-\')):\n                    i = int(line[line.find(\'-\')+1:line.find(\'\\t\')])\n                    if line.startswith(\'S-\'):\n                        src[i] = line.split(\'\\t\')[1]\n                    if line.startswith(\'T-\'):\n                        tgt[i] = line.split(\'\\t\')[1]\n                    if line.startswith(\'H-\'):\n                        if i not in hypos:\n                            hypos[i] = []\n                            log_probs[i] = []\n                        hypos[i].append(line.split(\'\\t\')[2])\n                        log_probs[i].append(float(line.split(\'\\t\')[1]))\n    return dictolist(src), dictolist(tgt), dictolist(hypos), dictolist(log_probs)\n\n\ndef load_ref(path):\n    with open(path) as f:\n        lines = f.readlines()\n    src, tgt, refs = [], [], []\n    i = 0\n    while i < len(lines):\n        if lines[i].startswith(\'S-\'):\n            src.append(lines[i].split(\'\\t\')[1].rstrip())\n            i += 1\n        elif lines[i].startswith(\'T-\'):\n            tgt.append(lines[i].split(\'\\t\')[1].rstrip())\n            i += 1\n        else:\n            a = []\n            while i < len(lines) and lines[i].startswith(\'R\'):\n                a.append(lines[i].split(\'\\t\')[1].rstrip())\n                i += 1\n            refs.append(a)\n    return src, tgt, refs\n\n\ndef merge(src, tgt, hypos, log_probs, path):\n    with open(path, \'w\') as f:\n        for s, t, hs, lps in zip(src, tgt, hypos, log_probs):\n            f.write(s + \'\\n\')\n            f.write(t + \'\\n\')\n            f.write(\'\\n\')\n            for h, lp in zip(hs, lps):\n                f.write(\'\\t%f\\t%s\\n\' % (lp, h.strip()))\n            f.write(\'------------------------------------------------------\\n\')\n\n\ndef corpus_bleu(sys_stream, ref_streams):\n    bleu = _corpus_bleu(sys_stream, ref_streams, tokenize=\'none\')\n    return bleu.score\n\n\ndef sentence_bleu(hypothesis, reference):\n    bleu = _corpus_bleu(hypothesis, reference)\n    for i in range(1, 4):\n        bleu.counts[i] += 1\n        bleu.totals[i] += 1\n    bleu = compute_bleu(\n        bleu.counts, bleu.totals,\n        bleu.sys_len, bleu.ref_len,\n        smooth=\'exp\', smooth_floor=0.0,\n    )\n    return bleu.score\n\n\ndef pairwise(sents):\n    _ref, _hypo = [], []\n    for s in sents:\n        for i in range(len(s)):\n            for j in range(len(s)):\n                if i != j:\n                    _ref.append(s[i])\n                    _hypo.append(s[j])\n    return corpus_bleu(_hypo, [_ref])\n\n\ndef multi_ref(refs, hypos):\n    _ref, _hypo = [], []\n    ref_cnt = 0\n    assert len(refs) == len(hypos)\n\n    # count number of refs covered\n    for rs, hs in zip(refs, hypos):\n        a = set()\n        for h in hs:\n            s = [sentence_bleu(h, r) for r in rs]\n            j = np.argmax(s)\n            _ref.append(rs[j])\n            _hypo.append(h)\n            best = [k for k in range(len(rs)) if s[k] == s[j]]\n            a.add(random.choice(best))\n        ref_cnt += len(a)\n    print(\'#refs covered: %.2f\' % (ref_cnt / len(refs)))\n\n    # transpose refs and hypos\n    refs = list(zip(*refs))\n    hypos = list(zip(*hypos))\n\n    # compute multi-ref corpus BLEU (leave-one-out to be comparable to intra_ref)\n    k = len(hypos)\n    m = len(refs)\n    flat_hypos = [hypos[j][i] for i in range(len(hypos[0])) for j in range(k)]\n    duplicated_refs = [\n        [ref for ref in refs_i for _ in range(k)]\n        for refs_i in refs\n    ]\n    loo_bleus = []\n    for held_out_ref in range(m):\n        remaining_refs = duplicated_refs[:held_out_ref] + duplicated_refs[held_out_ref+1:]\n        assert len(remaining_refs) == m - 1\n        loo_bleus.append(corpus_bleu(flat_hypos, remaining_refs))\n    print(\'average multi-reference BLEU (leave-one-out): %.2f\' % np.mean(loo_bleus))\n\n\ndef intra_ref(refs):\n    print(\'ref pairwise BLEU: %.2f\' % pairwise(refs))\n    refs = list(zip(*refs))\n    m = len(refs)\n    concat_h = []\n    concat_rest = [[] for j in range(m - 1)]\n    for i, h in enumerate(refs):\n        rest = refs[:i] + refs[i+1:]\n        concat_h.append(h)\n        for j in range(m - 1):\n            concat_rest[j].extend(rest[j])\n    concat_h = list(chain.from_iterable(concat_h))\n    bleu = corpus_bleu(concat_h, concat_rest)\n    print(\'multi-reference BLEU (leave-one-out): %.2f\' % bleu)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/wav2vec/vq-wav2vec_featurize.py,8,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nHelper script to pre-compute embeddings for a wav2letter++ dataset\n""""""\n\nimport pprint\nimport glob, os, argparse\n\nimport torch\nfrom torch import nn\n\ntry:\n    import tqdm\nexcept:\n    print(""Install tqdm to use --log-format=tqdm"")\n\nfrom fairseq.models.wav2vec import Wav2VecModel\n\nimport tqdm\nimport soundfile as sf\nfrom torch.utils.data import DataLoader\nimport os.path as osp\n\n\nclass FilesDataset:\n    def __init__(self, files, labels):\n        self.files = files\n        if labels and osp.exists(labels):\n            with open(labels, \'r\') as lbl_f:\n                self.labels = [line.rstrip() for line in lbl_f]\n        else:\n            self.labels = labels\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        fname = self.files[index]\n\n        wav, sr = sf.read(fname)\n        assert sr == 16000\n\n        wav = torch.from_numpy(wav).float()\n        lbls = None\n        if self.labels:\n            if isinstance(self.labels, str):\n                lbl_file = osp.splitext(fname)[0] + ""."" + self.labels\n                with open(lbl_file, \'r\') as lblf:\n                    lbls = lblf.readline()\n                    assert lbls is not None\n            else:\n                lbls = self.labels[index]\n        return wav, lbls\n\n    def collate(self, batch):\n        return batch\n\n\nclass ArgTypes:\n    @staticmethod\n    def existing_path(arg):\n        arg = str(arg)\n        assert osp.exists(arg), f""File {arg} does not exist""\n        return arg\n\n    @staticmethod\n    def mkdir(arg):\n        arg = str(arg)\n        os.makedirs(arg, exist_ok=True)\n        return arg\n\n\nclass DatasetWriter:\n    def __init__(self):\n\n        self.args = self.load_config()\n        pprint.pprint(self.args.__dict__)\n\n        self.model = self.load_model()\n\n    def __getattr__(self, attr):\n        return getattr(self.args, attr)\n\n    def read_manifest(self, fname):\n\n        with open(fname, ""r"") as fp:\n            lines = fp.read().split(""\\n"")\n            root = lines.pop(0).strip()\n            fnames = [\n                osp.join(root, line.split(""\\t"")[0]) for line in lines if len(line) > 0\n            ]\n\n        return fnames\n\n    def process_splits(self):\n\n        if self.args.shard is not None or self.args.num_shards is not None:\n            assert self.args.shard is not None and self.args.num_shards is not None\n\n        for split in self.splits:\n            print(split)\n\n            if self.extension == ""tsv"":\n                datadir = osp.join(self.data_dir, f""{split}.{self.extension}"")\n                print(""Reading manifest file: "", datadir)\n                files = self.read_manifest(datadir)\n            else:\n                datadir = osp.join(self.data_dir, split, f""**/*.{self.extension}"")\n                files = glob.glob(datadir, recursive=True)\n\n            assert len(files) > 0\n\n            if self.args.shard is not None:\n                files = files[self.args.shard::self.args.num_shards]\n\n            lbls = []\n            with open(self.data_file(split), \'w\') as srcf:\n                for line, lbl in self.iterate(files):\n                    print(line, file=srcf)\n                    if self.args.labels:\n                        lbls.append(lbl + \'\\n\')\n\n            if self.args.labels:\n                assert all(a is not None for a in lbls)\n                with open(self.lbl_file(split), \'w\') as lblf:\n                    lblf.writelines(lbls)\n\n    def iterate(self, files):\n\n        data = self.load_data(files)\n        for samples in tqdm.tqdm(data, total=len(files)//32):\n\n            for wav, lbl in samples:\n                x = wav.unsqueeze(0).float().cuda()\n\n                div = 1\n                while x.size(-1) // div > self.args.max_size:\n                    div += 1\n\n                xs = x.chunk(div, dim=-1)\n\n                result = []\n                for x in xs:\n                    torch.cuda.empty_cache()\n                    x = self.model.feature_extractor(x)\n                    if self.quantize_location == ""encoder"":\n                        with torch.no_grad():\n                            _, idx = self.model.vector_quantizer.forward_idx(x)\n                            idx = idx.squeeze(0).cpu()\n                    else:\n                        with torch.no_grad():\n                            z = self.model.feature_aggregator(x)\n                            _, idx = self.model.vector_quantizer.forward_idx(z)\n                            idx = idx.squeeze(0).cpu()\n                    result.append(idx)\n\n                idx = torch.cat(result, dim=0)\n                yield "" "".join(""-"".join(map(str, a.tolist())) for a in idx), lbl\n\n\n    def lbl_file(self, name):\n        shard_part = """" if self.args.shard is None else f"".{self.args.shard}""\n        return osp.join(self.output_dir, f""{name}.lbl{shard_part}"")\n\n    def data_file(self, name):\n        shard_part = """" if self.args.shard is None else f"".{self.args.shard}""\n        return osp.join(self.output_dir, f""{name}.src{shard_part}"")\n\n    def var_file(self):\n        return osp.join(self.output_dir, f""vars.pt"")\n\n    def load_config(self):\n\n        parser = argparse.ArgumentParser(""Vector Quantized wav2vec features"")\n\n        # Model Arguments\n        parser.add_argument(""--checkpoint"", type=ArgTypes.existing_path, required=True)\n        parser.add_argument(""--data-parallel"", action=""store_true"")\n\n        # Output Arguments\n        parser.add_argument(""--output-dir"", type=ArgTypes.mkdir, required=True)\n\n        # Data Arguments\n        parser.add_argument(""--data-dir"", type=ArgTypes.existing_path, required=True)\n        parser.add_argument(""--splits"", type=str, nargs=""+"", required=True)\n        parser.add_argument(""--extension"", type=str, required=True)\n        parser.add_argument(""--labels"", type=str, required=False)\n\n        parser.add_argument(""--shard"", type=int, default=None)\n        parser.add_argument(""--num-shards"", type=int, default=None)\n        parser.add_argument(""--max-size"", type=int, default=1300000)\n\n        # Logger Arguments\n        parser.add_argument(\n            ""--log-format"", type=str, choices=[""none"", ""simple"", ""tqdm""]\n        )\n\n        return parser.parse_args()\n\n    def load_data(self, fnames):\n\n        dataset = FilesDataset(fnames, self.args.labels)\n        loader = DataLoader(\n            dataset, batch_size=32, collate_fn=dataset.collate, num_workers=8\n        )\n        return loader\n\n    def load_model(self):\n        cp = torch.load(self.checkpoint, map_location=lambda x, _: x)\n\n        model = Wav2VecModel.build_model(cp[""args""], None)\n\n        self.quantize_location = getattr(cp[""args""], ""vq"", ""encoder"")\n\n        model.load_state_dict(cp[""model""])\n        model.eval().float()\n        model.cuda()\n\n        if self.data_parallel:\n            model = nn.DataParallel(model)\n\n        return model\n\n    def __call__(self):\n\n        self.process_splits()\n\n        if hasattr(self.model.feature_extractor, ""vars"") and (self.args.shard is None or self.args.shard == 0):\n            vars = (\n                self.model.feature_extractor.vars.view(\n                    self.model.feature_extractor.banks,\n                    self.model.feature_extractor.num_vars,\n                    -1,\n                )\n                .cpu()\n                .detach()\n            )\n            print(""writing learned latent variable embeddings: "", vars.shape)\n            torch.save(vars, self.var_file())\n\n\nif __name__ == ""__main__"":\n    write_data = DatasetWriter()\n\n    write_data()\n    print(""Done."")'"
examples/wav2vec/wav2vec_featurize.py,4,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nHelper script to pre-compute embeddings for a wav2letter++ dataset\n""""""\n\nimport argparse\nimport glob\nimport os\nfrom shutil import copy\n\nimport h5py\nimport soundfile as sf\nimport numpy as np\nimport torch\nfrom torch import nn\nimport tqdm\n\nfrom fairseq.models.wav2vec import Wav2VecModel\n\n\ndef read_audio(fname):\n    """""" Load an audio file and return PCM along with the sample rate """"""\n\n    wav, sr = sf.read(fname)\n    assert sr == 16e3\n\n    return wav, 16e3\n\n\nclass PretrainedWav2VecModel(nn.Module):\n\n    def __init__(self, fname):\n        super().__init__()\n\n        checkpoint = torch.load(fname)\n        self.args = checkpoint[""args""]\n        model = Wav2VecModel.build_model(self.args, None)\n        model.load_state_dict(checkpoint[""model""])\n        model.eval()\n\n        self.model = model\n\n    def forward(self, x):\n        with torch.no_grad():\n            z = self.model.feature_extractor(x)\n            if isinstance(z, tuple):\n                z = z[0]\n            c = self.model.feature_aggregator(z)\n        return z, c\n\n\nclass EmbeddingWriterConfig(argparse.ArgumentParser):\n\n    def __init__(self):\n        super().__init__(""Pre-compute embeddings for wav2letter++ datasets"")\n\n        kwargs = {""action"": ""store"", ""type"": str, ""required"": True}\n\n        self.add_argument(""--input"", ""-i"",\n                          help=""Input Directory"", **kwargs)\n        self.add_argument(""--output"", ""-o"",\n                          help=""Output Directory"", **kwargs)\n        self.add_argument(""--model"",\n                          help=""Path to model checkpoint"", **kwargs)\n        self.add_argument(""--split"",\n                          help=""Dataset Splits"", nargs=\'+\', **kwargs)\n        self.add_argument(""--ext"", default=""wav"", required=False,\n                          help=""Audio file extension"")\n\n        self.add_argument(""--no-copy-labels"", action=""store_true"",\n                          help=""Do not copy label files. Useful for large datasets, use --targetdir in wav2letter then."")\n        self.add_argument(""--use-feat"", action=""store_true"",\n                          help=""Use the feature vector (\'z\') instead of context vector (\'c\') for features"")\n        self.add_argument(""--gpu"",\n                          help=""GPU to use"", default=0, type=int)\n\n\nclass Prediction():\n    """""" Lightweight wrapper around a fairspeech embedding model """"""\n\n    def __init__(self, fname, gpu=0):\n        self.gpu = gpu\n        self.model = PretrainedWav2VecModel(fname).cuda(gpu)\n\n    def __call__(self, x):\n        x = torch.from_numpy(x).float().cuda(self.gpu)\n        with torch.no_grad():\n            z, c = self.model(x.unsqueeze(0))\n\n        return z.squeeze(0).cpu().numpy(), c.squeeze(0).cpu().numpy()\n\n\nclass H5Writer():\n    """""" Write features as hdf5 file in wav2letter++ compatible format """"""\n\n    def __init__(self, fname):\n        self.fname = fname\n        os.makedirs(os.path.dirname(self.fname), exist_ok=True)\n\n    def write(self, data):\n        channel, T = data.shape\n\n        with h5py.File(self.fname, ""w"") as out_ds:\n            data = data.T.flatten()\n            out_ds[""features""] = data\n            out_ds[""info""] = np.array([16e3 // 160, T, channel])\n\n\nclass EmbeddingDatasetWriter(object):\n    """""" Given a model and a wav2letter++ dataset, pre-compute and store embeddings\n\n    Args:\n        input_root, str :\n            Path to the wav2letter++ dataset\n        output_root, str :\n            Desired output directory. Will be created if non-existent\n        split, str :\n            Dataset split\n    """"""\n\n    def __init__(self, input_root, output_root, split,\n                 model_fname,\n                 extension=""wav"",\n                 gpu=0,\n                 verbose=False,\n                 use_feat=False,\n                 ):\n\n        assert os.path.exists(model_fname)\n\n        self.model_fname = model_fname\n        self.model = Prediction(self.model_fname, gpu)\n\n        self.input_root = input_root\n        self.output_root = output_root\n        self.split = split\n        self.verbose = verbose\n        self.extension = extension\n        self.use_feat = use_feat\n\n        assert os.path.exists(self.input_path), \\\n            ""Input path \'{}\' does not exist"".format(self.input_path)\n\n    def _progress(self, iterable, **kwargs):\n        if self.verbose:\n            return tqdm.tqdm(iterable, **kwargs)\n        return iterable\n\n    def require_output_path(self, fname=None):\n        path = self.get_output_path(fname)\n        os.makedirs(path, exist_ok=True)\n\n    @property\n    def input_path(self):\n        return self.get_input_path()\n\n    @property\n    def output_path(self):\n        return self.get_output_path()\n\n    def get_input_path(self, fname=None):\n        if fname is None:\n            return os.path.join(self.input_root, self.split)\n        return os.path.join(self.get_input_path(), fname)\n\n    def get_output_path(self, fname=None):\n        if fname is None:\n            return os.path.join(self.output_root, self.split)\n        return os.path.join(self.get_output_path(), fname)\n\n    def copy_labels(self):\n        self.require_output_path()\n\n        labels = list(filter(lambda x: self.extension not in x, glob.glob(self.get_input_path(""*""))))\n        for fname in tqdm.tqdm(labels):\n            copy(fname, self.output_path)\n\n    @property\n    def input_fnames(self):\n        return sorted(glob.glob(self.get_input_path(""*.{}"".format(self.extension))))\n\n    def __len__(self):\n        return len(self.input_fnames)\n\n    def write_features(self):\n\n        paths = self.input_fnames\n\n        fnames_context = map(lambda x: os.path.join(self.output_path, x.replace(""."" + self.extension, "".h5context"")), \\\n                             map(os.path.basename, paths))\n\n        for name, target_fname in self._progress(zip(paths, fnames_context), total=len(self)):\n            wav, sr = read_audio(name)\n            z, c = self.model(wav)\n            feat = z if self.use_feat else c\n            writer = H5Writer(target_fname)\n            writer.write(feat)\n\n    def __repr__(self):\n\n        return ""EmbeddingDatasetWriter ({n_files} files)\\n\\tinput:\\t{input_root}\\n\\toutput:\\t{output_root}\\n\\tsplit:\\t{split})"".format(\n            n_files=len(self), **self.__dict__)\n\n\nif __name__ == ""__main__"":\n\n    args = EmbeddingWriterConfig().parse_args()\n\n    for split in args.split:\n\n        writer = EmbeddingDatasetWriter(\n            input_root=args.input,\n            output_root=args.output,\n            split=split,\n            model_fname=args.model,\n            gpu=args.gpu,\n            extension=args.ext,\n            use_feat=args.use_feat,\n        )\n\n        print(writer)\n        writer.require_output_path()\n\n        print(""Writing Features..."")\n        writer.write_features()\n        print(""Done."")\n\n        if not args.no_copy_labels:\n            print(""Copying label data..."")\n            writer.copy_labels()\n            print(""Done."")\n'"
examples/wav2vec/wav2vec_manifest.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nData pre-processing: build vocabularies and binarize training data.\n""""""\n\nimport argparse\nimport glob\nimport os\nimport soundfile\nimport random\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'root\', metavar=\'DIR\', help=\'root directory containing flac files to index\')\n    parser.add_argument(\'--valid-percent\', default=0.01, type=float, metavar=\'D\',\n                        help=\'percentage of data to use as validation set (between 0 and 1)\')\n    parser.add_argument(\'--dest\', default=\'.\', type=str, metavar=\'DIR\', help=\'output directory\')\n    parser.add_argument(\'--ext\', default=\'flac\', type=str, metavar=\'EXT\', help=\'extension to look for\')\n    parser.add_argument(\'--seed\', default=42, type=int, metavar=\'N\', help=\'random seed\')\n    parser.add_argument(\'--path-must-contain\', default=None, type=str, metavar=\'FRAG\',\n                        help=\'if set, path must contain this substring for a file to be included in the manifest\')\n    return parser\n\n\ndef main(args):\n    assert args.valid_percent >= 0 and args.valid_percent <= 1.\n\n    dir_path = os.path.realpath(args.root)\n    search_path = os.path.join(dir_path, \'**/*.\' + args.ext)\n    rand = random.Random(args.seed)\n\n    with open(os.path.join(args.dest, \'train.tsv\'), \'w\') as train_f, open(\n            os.path.join(args.dest, \'valid.tsv\'), \'w\') as valid_f:\n        print(dir_path, file=train_f)\n        print(dir_path, file=valid_f)\n\n        for fname in glob.iglob(search_path, recursive=True):\n            file_path = os.path.realpath(fname)\n\n            if args.path_must_contain and args.path_must_contain not in file_path:\n                continue\n\n            frames = soundfile.info(fname).frames\n            dest = train_f if rand.random() > args.valid_percent else valid_f\n            print(\'{}\\t{}\'.format(os.path.relpath(file_path, dir_path), frames), file=dest)\n\n\nif __name__ == \'__main__\':\n    parser = get_parser()\n    args = parser.parse_args()\n    main(args)\n'"
fairseq/benchmark/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# import models/tasks to register them\nfrom . import (  # noqa\n    dummy_lm,\n    dummy_masked_lm,\n    dummy_model,\n)\n'"
fairseq/benchmark/dummy_lm.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport numpy as np\nimport torch\n\nfrom fairseq.data import Dictionary, FairseqDataset\nfrom fairseq.tasks import FairseqTask, register_task\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'dummy_lm\')\nclass DummyLMTask(FairseqTask):\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'--dict-size\', default=49996, type=int)\n        parser.add_argument(\'--dataset-size\', default=100000, type=int)\n        parser.add_argument(\'--tokens-per-sample\', default=512, type=int,\n                            help=\'max number of total tokens over all segments \'\n                                 \'per sample for BERT dataset\')\n\n    def __init__(self, args, dictionary):\n        super().__init__(args)\n        self.dictionary = dictionary\n        self.seed = args.seed\n\n        dictionary.pad_to_multiple_(8)  # often faster if divisible by 8\n\n        seq = torch.arange(args.tokens_per_sample + 1) + dictionary.pad() + 1\n\n        self.dummy_src = seq[:-1]\n        self.dummy_tgt = seq[1:]\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task. """"""\n        dictionary = Dictionary()\n        for i in range(args.dict_size):\n            dictionary.add_symbol(\'word{}\'.format(i))\n        logger.info(\'dictionary: {} types\'.format(len(dictionary)))\n        return cls(args, dictionary)\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        if self.args.max_sentences is not None:\n            bsz = self.args.max_sentences\n        else:\n            bsz = max(1, self.args.max_tokens // self.args.tokens_per_sample)\n        self.datasets[split] = DummyDataset(\n            {\n                \'id\': 1,\n                \'net_input\': {\n                    \'src_tokens\': torch.stack([self.dummy_src for _ in range(bsz)]),\n                    \'src_lengths\': torch.full(\n                        (bsz, ), self.args.tokens_per_sample, dtype=torch.long\n                    ),\n                },\n                \'target\': torch.stack([self.dummy_tgt for _ in range(bsz)]),\n                \'nsentences\': bsz,\n                \'ntokens\': bsz * self.args.tokens_per_sample,\n            },\n            num_items=self.args.dataset_size,\n            item_size=self.args.tokens_per_sample,\n        )\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\nclass DummyDataset(FairseqDataset):\n\n    def __init__(self, batch, num_items, item_size):\n        super().__init__()\n        self.batch = batch\n        self.num_items = num_items\n        self.item_size = item_size\n\n    def __getitem__(self, index):\n        return index\n\n    def __len__(self):\n        return self.num_items\n\n    def collater(self, samples):\n        return self.batch\n\n    @property\n    def sizes(self):\n        return np.array([self.item_size] * self.num_items)\n\n    def num_tokens(self, index):\n        return self.item_size\n\n    def size(self, index):\n        return self.item_size\n\n    def ordered_indices(self):\n        return np.arange(self.num_items)\n\n    @property\n    def supports_prefetch(self):\n        return False\n'"
fairseq/benchmark/dummy_masked_lm.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport numpy as np\nimport torch\n\nfrom fairseq.data import Dictionary, FairseqDataset\nfrom fairseq.tasks import FairseqTask, register_task\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'dummy_masked_lm\')\nclass DummyMaskedLMTask(FairseqTask):\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'--dict-size\', default=49995, type=int)\n        parser.add_argument(\'--dataset-size\', default=100000, type=int)\n        parser.add_argument(\'--tokens-per-sample\', default=512, type=int,\n                            help=\'max number of total tokens over all segments \'\n                                 \'per sample for BERT dataset\')\n\n    def __init__(self, args, dictionary):\n        super().__init__(args)\n        self.dictionary = dictionary\n        self.seed = args.seed\n\n        # add mask token\n        self.mask_idx = dictionary.add_symbol(\'<mask>\')\n        dictionary.pad_to_multiple_(8)  # often faster if divisible by 8\n\n        mask_idx = 0\n        pad_idx = 1\n        seq = torch.arange(args.tokens_per_sample) + pad_idx + 1\n        mask = torch.arange(2, args.tokens_per_sample, 7)  # ~15%\n        src = seq.clone()\n        src[mask] = mask_idx\n        tgt = torch.full_like(seq, pad_idx)\n        tgt[mask] = seq[mask]\n\n        self.dummy_src = src\n        self.dummy_tgt = tgt\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task. """"""\n        dictionary = Dictionary()\n        for i in range(args.dict_size):\n            dictionary.add_symbol(\'word{}\'.format(i))\n        logger.info(\'dictionary: {} types\'.format(len(dictionary)))\n        return cls(args, dictionary)\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        if self.args.max_sentences is not None:\n            bsz = self.args.max_sentences\n        else:\n            bsz = max(1, self.args.max_tokens // self.args.tokens_per_sample)\n        self.datasets[split] = DummyDataset(\n            {\n                \'id\': 1,\n                \'net_input\': {\n                    \'src_tokens\': torch.stack([self.dummy_src for _ in range(bsz)]),\n                    \'src_lengths\': torch.full(\n                        (bsz, ), self.args.tokens_per_sample, dtype=torch.long\n                    ),\n                },\n                \'target\': torch.stack([self.dummy_tgt for _ in range(bsz)]),\n                \'nsentences\': bsz,\n                \'ntokens\': bsz * self.args.tokens_per_sample,\n            },\n            num_items=self.args.dataset_size,\n            item_size=self.args.tokens_per_sample,\n        )\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\nclass DummyDataset(FairseqDataset):\n\n    def __init__(self, batch, num_items, item_size):\n        super().__init__()\n        self.batch = batch\n        self.num_items = num_items\n        self.item_size = item_size\n\n    def __getitem__(self, index):\n        return index\n\n    def __len__(self):\n        return self.num_items\n\n    def collater(self, samples):\n        return self.batch\n\n    @property\n    def sizes(self):\n        return np.array([self.item_size] * self.num_items)\n\n    def num_tokens(self, index):\n        return self.item_size\n\n    def size(self, index):\n        return self.item_size\n\n    def ordered_indices(self):\n        return np.arange(self.num_items)\n\n    @property\n    def supports_prefetch(self):\n        return False\n'"
fairseq/benchmark/dummy_model.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq.data import Dictionary\nfrom fairseq.models import (\n    FairseqDecoder,\n    FairseqLanguageModel,\n    register_model,\n    register_model_architecture,\n)\n\n\n@register_model('dummy_model')\nclass DummyModel(FairseqLanguageModel):\n\n    def __init__(self, args, encoder):\n        super().__init__(encoder)\n        self.args = args\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument('--num-layers', type=int, default=24)\n        parser.add_argument('--embed-dim', type=int, default=1024)\n\n    @classmethod\n    def build_model(cls, args, task):\n        encoder = DummyEncoder(\n            num_embed=len(task.target_dictionary),\n            embed_dim=args.embed_dim,\n            num_layers=args.num_layers,\n        )\n        return cls(args, encoder)\n\n    def forward(self, src_tokens, masked_tokens=None, **kwargs):\n        return self.decoder(src_tokens, masked_tokens=masked_tokens)\n\n\nclass DummyEncoder(FairseqDecoder):\n\n    def __init__(self, num_embed=50000, embed_dim=1024, num_layers=24):\n        super().__init__(Dictionary())\n        self.embed = nn.Embedding(\n            num_embeddings=num_embed, embedding_dim=embed_dim, padding_idx=0\n        )\n        self.layers_a = nn.ModuleList([\n            nn.Sequential(\n                nn.LayerNorm(embed_dim),\n                nn.Linear(embed_dim, 3*embed_dim),  # q, k, v input projection\n                nn.Linear(3*embed_dim, embed_dim),  # skip self-attention\n                nn.Linear(embed_dim, embed_dim),    # output projection\n                nn.Dropout(),\n            )\n            for i in range(num_layers)\n        ])\n        self.layers_b = nn.ModuleList([\n            nn.Sequential(\n                nn.LayerNorm(embed_dim),\n                nn.Linear(embed_dim, 4*embed_dim),  # FFN\n                nn.ReLU(),\n                nn.Linear(4*embed_dim, embed_dim),  # FFN\n                nn.Dropout(0.1),\n            )\n            for i in range(num_layers)\n        ])\n        self.out_proj = nn.Linear(embed_dim, num_embed)\n\n    def forward(self, tokens, masked_tokens=None):\n        x = self.embed(tokens)\n        for layer_a, layer_b in zip(self.layers_a, self.layers_b):\n            x = x + layer_a(x)\n            x = x + layer_b(x)\n        x = self.out_proj(x)\n        if masked_tokens is not None:\n            x = x[masked_tokens]\n        return (x,)\n\n    def max_positions(self):\n        return 1024\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n        logits = net_output[0].float()\n        if log_probs:\n            return F.log_softmax(logits, dim=-1)\n        else:\n            return F.softmax(logits, dim=-1)\n\n\n@register_model_architecture('dummy_model', 'dummy_model')\ndef base_architecture(args):\n    pass\n"""
fairseq/criterions/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\nfrom fairseq import registry\nfrom fairseq.criterions.fairseq_criterion import FairseqCriterion, LegacyFairseqCriterion\n\n\nbuild_criterion, register_criterion, CRITERION_REGISTRY = registry.setup_registry(\n    '--criterion',\n    base_class=FairseqCriterion,\n    default='cross_entropy',\n)\n\n\n# automatically import any Python files in the criterions/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        module = file[:file.find('.py')]\n        importlib.import_module('fairseq.criterions.' + module)\n"""
fairseq/criterions/adaptive_loss.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch.nn.functional as F\n\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(\'adaptive_loss\')\nclass AdaptiveLoss(FairseqCriterion):\n    """"""This is an implementation of the loss function accompanying the adaptive softmax approximation for\n    graphical processing units (GPU), described in the paper ""Efficient softmax approximation for GPUs""\n    (http://arxiv.org/abs/1609.04309).""""""\n\n    def __init__(self, task, sentence_avg):\n        super().__init__(task)\n        self.sentence_avg = sentence_avg\n\n    @classmethod\n    def build_criterion(cls, args, task):\n        if getattr(args, \'ddp_backend\', None) == \'c10d\':\n            raise Exception(\n                \'AdaptiveLoss is not compatible with the c10d \'\n                \'version of DistributedDataParallel. Please use \'\n                \'`--ddp-backend=no_c10d` instead.\'\n            )\n        return cls(task, args.sentence_avg)\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n\n        assert hasattr(model.decoder, \'adaptive_softmax\') and model.decoder.adaptive_softmax is not None\n        adaptive_softmax = model.decoder.adaptive_softmax\n\n        net_output = model(**sample[\'net_input\'])\n        orig_target = model.get_targets(sample, net_output)\n\n        nsentences = orig_target.size(0)\n        orig_target = orig_target.view(-1)\n\n        bsz = orig_target.size(0)\n\n        logits, target = adaptive_softmax(net_output[0], orig_target)\n        assert len(target) == len(logits)\n\n        loss = net_output[0].new(1 if reduce else bsz).zero_()\n\n        for i in range(len(target)):\n            if target[i] is not None:\n                assert (target[i].min() >= 0 and target[i].max() <= logits[i].size(1))\n                loss += F.cross_entropy(\n                    logits[i],\n                    target[i],\n                    ignore_index=self.padding_idx,\n                    reduction=\'sum\' if reduce else \'none\',\n                )\n\n        orig = utils.strip_pad(orig_target, self.padding_idx)\n        ntokens = orig.numel()\n        sample_size = sample[\'target\'].size(0) if self.sentence_avg else ntokens\n        logging_output = {\n            \'loss\': loss.data,\n            \'ntokens\': ntokens,\n            \'nsentences\': nsentences,\n            \'sample_size\': sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = utils.item(sum(log.get(\'loss\', 0) for log in logging_outputs))\n        ntokens = utils.item(sum(log.get(\'ntokens\', 0) for log in logging_outputs))\n        sample_size = utils.item(sum(log.get(\'sample_size\', 0) for log in logging_outputs))\n\n        metrics.log_scalar(\'loss\', loss_sum / sample_size / math.log(2), sample_size, round=3)\n        if sample_size != ntokens:\n            metrics.log_scalar(\'nll_loss\', loss_sum / ntokens / math.log(2), ntokens, round=3)\n            metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'nll_loss\'].avg))\n        else:\n            metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'loss\'].avg))\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/criterions/binary_cross_entropy.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(\'binary_cross_entropy\')\nclass BinaryCrossEntropyCriterion(FairseqCriterion):\n\n    def __init__(self, task, infonce=False, loss_weights=None, log_keys=None):\n        super().__init__(task)\n        self.infonce = infonce\n        self.loss_weights = None if loss_weights is None else eval(loss_weights)\n        self.log_keys = [] if log_keys is None else eval(log_keys)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--infonce\', action=\'store_true\',\n                            help=\'if set, uses cross entropy instead of binary cross entropy (i.e. InfoNCE loss)\')\n        parser.add_argument(\'--loss-weights\', type=str, default=None,\n                            help=\'weights for additional loss terms (not first one)\')\n        parser.add_argument(\'--log-keys\', type=str, default=None,\n                            help=\'output keys to log\')\n\n    def forward(self, model, sample, reduce=True, log_pred=False):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        net_output = model(**sample[\'net_input\'])\n        logits = model.get_logits(net_output).float()\n        target = model.get_targets(sample, net_output)\n\n        weights = None\n        if hasattr(model, \'get_target_weights\') and not self.infonce:\n            weights = model.get_target_weights(target, net_output)\n            if torch.is_tensor(weights):\n                weights = weights.float()\n\n        losses = []\n\n        if self.infonce:\n            loss = F.cross_entropy(logits, target, reduction=""sum"" if reduce else ""none"",)\n        else:\n            loss = F.binary_cross_entropy_with_logits(logits, target.float(), weights, reduction=""sum"" if reduce else ""none"",)\n\n        sample_size = target.numel() if self.infonce else target.long().sum().item()\n        losses.append(loss)\n\n        if self.loss_weights is not None and hasattr(model, ""get_extra_losses""):\n            extra_losses = model.get_extra_losses(net_output)\n            if torch.is_tensor(extra_losses):\n                extra_losses = [extra_losses]\n            if len(self.loss_weights) == 1 and len(extra_losses) != 1:\n                self.loss_weights = [self.loss_weights[0]] * len(extra_losses)\n            assert len(extra_losses) == len(self.loss_weights), f\'{len(extra_losses)}, {len(self.loss_weights)}\'\n            for p, coef in zip(extra_losses, self.loss_weights):\n                if coef != 0 and p is not None:\n                    p = coef * p.float() * sample_size\n                    loss += p\n                    losses.append(p)\n\n        logging_output = {\n            \'loss\': loss.item() if reduce else loss,\n            \'ntokens\': sample_size,\n            \'nsentences\': logits.size(0),\n            \'sample_size\': sample_size,\n        }\n\n        for lk in self.log_keys:\n            if lk in net_output:\n                logging_output[lk] = float((net_output[lk]))\n\n        if len(losses) > 1:\n            for i, l in enumerate(losses):\n                logging_output[f\'loss_{i}\'] = l.item()\n\n        if self.infonce:\n            with torch.no_grad():\n                if logits.numel() == 0:\n                    corr = 0\n                    count = 0\n                else:\n                    assert logits.dim() > 1, logits.shape\n                    max = logits.argmax(-1) == 0\n                    min = logits.argmin(-1) == 0\n                    both = max & min\n                    corr = max.long().sum().item() - both.long().sum().item()\n                    count = max.numel()\n\n                logging_output[""correct""] = corr\n                logging_output[""count""] = count\n\n        if log_pred:\n            logging_output[\'logits\'] = logits.cpu().numpy()\n            logging_output[\'target\'] = target.cpu().numpy()\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = utils.item(sum(log.get(\'loss\', 0) for log in logging_outputs))\n        ntokens = utils.item(sum(log.get(\'ntokens\', 0) for log in logging_outputs))\n        nsentences = utils.item(sum(log.get(\'nsentences\', 0) for log in logging_outputs))\n        sample_size = utils.item(sum(log.get(\'sample_size\', 0) for log in logging_outputs))\n        agg_output = {\n            \'loss\': loss_sum / sample_size / math.log(2),\n            \'ntokens\': ntokens,\n            \'nsentences\': nsentences,\n            \'sample_size\': sample_size,\n        }\n        if sample_size != ntokens:\n            agg_output[\'nll_loss\'] = loss_sum / ntokens / math.log(2)\n\n        correct = sum(log.get(""correct"", 0) for log in logging_outputs)\n        total = sum(log.get(""count"", 0) for log in logging_outputs)\n        if total > 0:\n            agg_output[\'accuracy\'] = correct / total\n\n        builtin_keys = {\'loss\', \'ntokens\', \'nsentences\', \'sample_size\', \'correct\', \'count\'}\n\n        for k in logging_outputs[0]:\n            if k not in builtin_keys:\n                val = sum(log.get(k, 0) for log in logging_outputs) / len(logging_outputs)\n                if k.startswith(\'loss\'):\n                    val = val / ntokens if ntokens > 0 else float(\'nan\')\n                agg_output[k] = val\n\n        return agg_output\n'"
fairseq/criterions/composite_loss.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom torch import nn\n\nfrom fairseq import utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(\'composite_loss\')\nclass CompositeLoss(FairseqCriterion):\n    """"""This is a composite loss that, given a list of model outputs and a list of targets,\n    computes an average of losses for each output-target pair""""""\n\n    def __init__(self, task, underlying_criterion):\n        super().__init__(task)\n        self.underlying_criterion = underlying_criterion\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--underlying-criterion\', type=str, metavar=\'VAL\', required=True,\n                            help=\'underlying criterion to use for the composite loss\')\n        # fmt: on\n\n    @staticmethod\n    def build_underlying_criterion(args, task):\n        saved_criterion = args.criterion\n        args.criterion = args.underlying_criterion\n        assert saved_criterion != args.underlying_criterion\n        underlying_criterion = task.build_criterion(args)\n        args.criterion = saved_criterion\n        return underlying_criterion\n\n    @classmethod\n    def build_criterion(cls, args, task):\n        underlying_criterion = CompositeLoss.build_underlying_criterion(args, task)\n\n        class FakeModel(nn.Module):\n\n            def __init__(self, model, net_out, target):\n                super().__init__()\n                self.model = model\n                self.net_out = net_out\n                self.target = target\n\n            def forward(self, **unused):\n                return self.net_out\n\n            def get_normalized_probs(self, net_output, log_probs, sample=None):\n                return self.model.get_normalized_probs(net_output, log_probs, sample=sample)\n\n            def get_targets(self, *unused):\n                return self.target\n\n            @property\n            def decoder(self):\n                return self.model.decoder\n\n        class _CompositeLoss(FairseqCriterion):\n\n            def __init__(self, task, underlying_criterion):\n                super().__init__(task)\n                self.underlying_criterion = underlying_criterion\n\n            def forward(self, model, sample, reduce=True):\n                net_outputs = model(**sample[\'net_input\'])\n                targets = sample[\'target\']\n\n                bsz = targets[0].size(0)\n                loss = net_outputs[0][0].new(1 if reduce else bsz).float().zero_()\n\n                sample_size = 0\n                logging_output = {}\n                for o, t in zip(net_outputs[0], targets):\n                    m = FakeModel(model, (o, net_outputs[1]), t)\n                    sample[\'target\'] = t\n                    l, ss, logging_output = self.underlying_criterion(m, sample, reduce)\n                    loss += l\n                    sample_size += ss\n\n                loss.div_(len(targets))\n                sample_size /= len(targets)\n\n                logging_output[\'loss\'] = utils.item(loss.data) if reduce else loss.data\n                return loss, sample_size, logging_output\n\n            @staticmethod\n            def aggregate_logging_outputs(logging_outputs):\n                return underlying_criterion.__class__.aggregate_logging_outputs(logging_outputs)\n\n            @staticmethod\n            def reduce_metrics(logging_outputs) -> None:\n                underlying_criterion.__class__.reduce_metrics(logging_outputs)\n\n        return _CompositeLoss(task, underlying_criterion)\n'"
fairseq/criterions/cross_entropy.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch.nn.functional as F\n\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(\'cross_entropy\')\nclass CrossEntropyCriterion(FairseqCriterion):\n\n    def __init__(self, task, sentence_avg):\n        super().__init__(task)\n        self.sentence_avg = sentence_avg\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        net_output = model(**sample[\'net_input\'])\n        loss, _ = self.compute_loss(model, net_output, sample, reduce=reduce)\n        sample_size = sample[\'target\'].size(0) if self.sentence_avg else sample[\'ntokens\']\n        logging_output = {\n            \'loss\': loss.data,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample[\'target\'].size(0),\n            \'sample_size\': sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    def compute_loss(self, model, net_output, sample, reduce=True):\n        lprobs = model.get_normalized_probs(net_output, log_probs=True)\n        lprobs = lprobs.view(-1, lprobs.size(-1))\n        target = model.get_targets(sample, net_output).view(-1)\n        loss = F.nll_loss(\n            lprobs,\n            target,\n            ignore_index=self.padding_idx,\n            reduction=\'sum\' if reduce else \'none\',\n        )\n        return loss, loss\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(\'loss\', 0) for log in logging_outputs)\n        ntokens = sum(log.get(\'ntokens\', 0) for log in logging_outputs)\n        sample_size = sum(log.get(\'sample_size\', 0) for log in logging_outputs)\n\n        metrics.log_scalar(\'loss\', loss_sum / sample_size / math.log(2), sample_size, round=3)\n        if sample_size != ntokens:\n            metrics.log_scalar(\'nll_loss\', loss_sum / ntokens / math.log(2), ntokens, round=3)\n            metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'nll_loss\'].avg))\n        else:\n            metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'loss\'].avg))\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/criterions/fairseq_criterion.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport inspect\nfrom typing import Any, Dict, List\n\nfrom torch.nn.modules.loss import _Loss\n\nfrom fairseq import metrics, utils\n\n\nclass FairseqCriterion(_Loss):\n\n    def __init__(self, task):\n        super().__init__()\n        self.task = task\n        if hasattr(task, \'target_dictionary\'):\n            tgt_dict = task.target_dictionary\n            self.padding_idx = tgt_dict.pad() if tgt_dict is not None else -100\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        pass\n\n    @classmethod\n    def build_criterion(cls, args, task):\n        """"""Construct a criterion from command-line args.""""""\n        # Criterions can override this, but for convenience we also try\n        # to automatically map argparse.Namespace keys to corresponding\n        # arguments in the __init__.\n        init_args = {}\n        for p in inspect.signature(cls).parameters.values():\n            if (\n                p.kind == p.POSITIONAL_ONLY\n                or p.kind == p.VAR_POSITIONAL\n                or p.kind == p.VAR_KEYWORD\n            ):\n                # we haven\'t implemented inference for these argument types,\n                # but PRs welcome :)\n                raise NotImplementedError(\'{} not supported\'.format(p.kind))\n\n            assert p.kind in {p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY}\n\n            if p.name == \'task\':\n                init_args[\'task\'] = task\n            elif hasattr(args, p.name):\n                init_args[p.name] = getattr(args, p.name)\n            elif p.default != p.empty:\n                pass  # we\'ll use the default value\n            else:\n                raise NotImplementedError(\n                    \'Unable to infer Criterion arguments, please implement \'\n                    \'{}.build_criterion\'.format(cls.__name__)\n                )\n        return cls(**init_args)\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def aggregate_logging_outputs(\n        logging_outputs: List[Dict[str, Any]],\n    ) -> Dict[str, Any]:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        utils.deprecation_warning(\n            \'The aggregate_logging_outputs API is deprecated. \'\n            \'Please use the reduce_metrics API instead.\'\n        )\n        raise NotImplementedError\n\n    @classmethod\n    def reduce_metrics(cls, logging_outputs: List[Dict[str, Any]]) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        utils.deprecation_warning(\n            \'Criterions should implement the reduce_metrics API. \'\n            \'Falling back to deprecated aggregate_logging_outputs API.\'\n        )\n        agg_logging_outputs = cls.aggregate_logging_outputs(logging_outputs)\n        for k, v in agg_logging_outputs.items():\n            if k in {\'nsentences\', \'ntokens\', \'sample_size\'}:\n                continue\n            metrics.log_scalar(k, v)\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return False\n\n\nclass LegacyFairseqCriterion(FairseqCriterion):\n\n    def __init__(self, args, task):\n        super().__init__(task=task)\n        self.args = args\n\n        utils.deprecation_warning(\n            \'Criterions should take explicit arguments instead of an \'\n            \'argparse.Namespace object, please update your criterion by \'\n            \'extending FairseqCriterion instead of LegacyFairseqCriterion.\'\n        )\n\n    @classmethod\n    def build_criterion(cls, args, task):\n        """"""Construct a criterion from command-line args.""""""\n        return cls(args, task)\n'"
fairseq/criterions/label_smoothed_cross_entropy.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\ndef label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=None, reduce=True):\n    if target.dim() == lprobs.dim() - 1:\n        target = target.unsqueeze(-1)\n    nll_loss = -lprobs.gather(dim=-1, index=target)\n    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n    if ignore_index is not None:\n        pad_mask = target.eq(ignore_index)\n        nll_loss.masked_fill_(pad_mask, 0.)\n        smooth_loss.masked_fill_(pad_mask, 0.)\n    else:\n        nll_loss = nll_loss.squeeze(-1)\n        smooth_loss = smooth_loss.squeeze(-1)\n    if reduce:\n        nll_loss = nll_loss.sum()\n        smooth_loss = smooth_loss.sum()\n    eps_i = epsilon / lprobs.size(-1)\n    loss = (1. - epsilon) * nll_loss + eps_i * smooth_loss\n    return loss, nll_loss\n\n\n@register_criterion(\'label_smoothed_cross_entropy\')\nclass LabelSmoothedCrossEntropyCriterion(FairseqCriterion):\n\n    def __init__(self, task, sentence_avg, label_smoothing):\n        super().__init__(task)\n        self.sentence_avg = sentence_avg\n        self.eps = label_smoothing\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--label-smoothing\', default=0., type=float, metavar=\'D\',\n                            help=\'epsilon for label smoothing, 0 means no label smoothing\')\n        # fmt: on\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        net_output = model(**sample[\'net_input\'])\n        loss, nll_loss = self.compute_loss(model, net_output, sample, reduce=reduce)\n        sample_size = sample[\'target\'].size(0) if self.sentence_avg else sample[\'ntokens\']\n        logging_output = {\n            \'loss\': loss.data,\n            \'nll_loss\': nll_loss.data,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample[\'target\'].size(0),\n            \'sample_size\': sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    def compute_loss(self, model, net_output, sample, reduce=True):\n        lprobs = model.get_normalized_probs(net_output, log_probs=True)\n        lprobs = lprobs.view(-1, lprobs.size(-1))\n        target = model.get_targets(sample, net_output).view(-1, 1)\n        loss, nll_loss = label_smoothed_nll_loss(\n            lprobs, target, self.eps, ignore_index=self.padding_idx, reduce=reduce,\n        )\n        return loss, nll_loss\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(\'loss\', 0) for log in logging_outputs)\n        nll_loss_sum = sum(log.get(\'nll_loss\', 0) for log in logging_outputs)\n        ntokens = sum(log.get(\'ntokens\', 0) for log in logging_outputs)\n        sample_size = sum(log.get(\'sample_size\', 0) for log in logging_outputs)\n\n        metrics.log_scalar(\'loss\', loss_sum / sample_size / math.log(2), sample_size, round=3)\n        metrics.log_scalar(\'nll_loss\', nll_loss_sum / ntokens / math.log(2), ntokens, round=3)\n        metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'nll_loss\'].avg))\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import register_criterion\n\nfrom .label_smoothed_cross_entropy import LabelSmoothedCrossEntropyCriterion\n\n\n@register_criterion(\'label_smoothed_cross_entropy_with_alignment\')\nclass LabelSmoothedCrossEntropyCriterionWithAlignment(LabelSmoothedCrossEntropyCriterion):\n\n    def __init__(self, task, sentence_avg, label_smoothing, alignment_lambda):\n        super().__init__(task, sentence_avg, label_smoothing)\n        self.alignment_lambda = alignment_lambda\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        LabelSmoothedCrossEntropyCriterion.add_args(parser)\n        parser.add_argument(\'--alignment-lambda\', default=0.05, type=float, metavar=\'D\',\n                            help=\'weight for the alignment loss\')\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        net_output = model(**sample[\'net_input\'])\n        loss, nll_loss = self.compute_loss(model, net_output, sample, reduce=reduce)\n        sample_size = sample[\'target\'].size(0) if self.sentence_avg else sample[\'ntokens\']\n        logging_output = {\n            \'loss\': utils.item(loss.data) if reduce else loss.data,\n            \'nll_loss\': utils.item(nll_loss.data) if reduce else nll_loss.data,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample[\'target\'].size(0),\n            \'sample_size\': sample_size,\n        }\n\n        alignment_loss = None\n\n        # Compute alignment loss only for training set and non dummy batches.\n        if \'alignments\' in sample and sample[\'alignments\'] is not None:\n            alignment_loss = self.compute_alignment_loss(sample, net_output)\n\n        if alignment_loss is not None:\n            logging_output[\'alignment_loss\'] = utils.item(alignment_loss.data)\n            loss += self.alignment_lambda * alignment_loss\n\n        return loss, sample_size, logging_output\n\n    def compute_alignment_loss(self, sample, net_output):\n        attn_prob = net_output[1][\'attn\'][0]\n        bsz, tgt_sz, src_sz = attn_prob.shape\n        attn = attn_prob.view(bsz * tgt_sz, src_sz)\n\n        align = sample[\'alignments\']\n        align_weights = sample[\'align_weights\'].float()\n\n        if len(align) > 0:\n            # Alignment loss computation. align (shape [:, 2]) contains the src-tgt index pairs corresponding to\n            # the alignments. align_weights (shape [:]) contains the 1 / frequency of a tgt index for normalizing.\n            loss = -((attn[align[:, 1][:, None], align[:, 0][:, None]]).log() * align_weights[:, None]).sum()\n        else:\n            return None\n\n        return loss\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = utils.item(sum(log.get(\'loss\', 0) for log in logging_outputs))\n        nll_loss_sum = utils.item(sum(log.get(\'nll_loss\', 0) for log in logging_outputs))\n        alignment_loss_sum = utils.item(sum(log.get(\'alignment_loss\', 0) for log in logging_outputs))\n        ntokens = utils.item(sum(log.get(\'ntokens\', 0) for log in logging_outputs))\n        sample_size = utils.item(sum(log.get(\'sample_size\', 0) for log in logging_outputs))\n\n        metrics.log_scalar(\'loss\', loss_sum / sample_size / math.log(2), sample_size, round=3)\n        metrics.log_scalar(\'nll_loss\', nll_loss_sum / ntokens / math.log(2), ntokens, round=3)\n        metrics.log_scalar(\'alignment_loss\', alignment_loss_sum / sample_size / math.log(2), sample_size, round=3)\n        metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'nll_loss\'].avg))\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/criterions/legacy_masked_lm.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\ndef compute_cross_entropy_loss(logits, targets, ignore_index=-100):\n    """"""\n    Function to compute the cross entropy loss. The default value of\n    ignore_index is the same as the default value for F.cross_entropy in\n    pytorch.\n    """"""\n    assert logits.size(0) == targets.size(-1), \\\n        ""Logits and Targets tensor shapes don\'t match up""\n\n    loss = F.nll_loss(\n        F.log_softmax(logits, -1, dtype=torch.float32),\n        targets,\n        reduction=""sum"",\n        ignore_index=ignore_index,\n    )\n    return loss\n\n\n@register_criterion(\'legacy_masked_lm_loss\')\nclass LegacyMaskedLmLoss(FairseqCriterion):\n    """"""\n    Implementation for the loss used in masked language model (MLM) training.\n    This optionally also computes the next sentence prediction (NSP) loss and\n    adds it to the overall loss based on the specified args. There are three\n    cases to consider:\n        1) Generic MLM training without NSP loss. In this case sentence_targets\n           and sentence_logits are both None.\n        2) BERT training without NSP loss. In this case sentence_targets is\n           not None but sentence_logits is None and we should not be computing\n           a sentence level loss.\n        3) BERT training with NSP loss. In this case both sentence_targets and\n           sentence_logits are not None and we should be computing a sentence\n           level loss. The weight of the sentence level loss is specified as\n           an argument.\n    """"""\n\n    def __init__(self, task, masked_lm_only, nsp_loss_weight):\n        super().__init__(task)\n        self.masked_lm_only = masked_lm_only\n        self.nsp_loss_weight = nsp_loss_weight\n\n    @staticmethod\n    def add_args(parser):\n        """"""Args for MaskedLM Loss""""""\n        # Default for masked_lm_only is False so as to not break BERT training\n        parser.add_argument(\'--masked-lm-only\', default=False,\n                            action=\'store_true\', help=\'compute MLM loss only\')\n        parser.add_argument(\'--nsp-loss-weight\', default=1.0, type=float,\n                            help=\'weight for next sentence prediction\'\n                                 \' loss (default 1)\')\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        lm_logits, output_metadata = model(**sample[""net_input""])\n\n        # reshape lm_logits from (N,T,C) to (N*T,C)\n        lm_logits = lm_logits.view(-1, lm_logits.size(-1))\n        lm_targets = sample[\'lm_target\'].view(-1)\n        lm_loss = compute_cross_entropy_loss(\n            lm_logits, lm_targets, self.padding_idx)\n\n        # compute the number of tokens for which loss is computed. This is used\n        # to normalize the loss\n        ntokens = utils.strip_pad(lm_targets, self.padding_idx).numel()\n        loss = lm_loss / ntokens\n        nsentences = sample[\'nsentences\']\n        # nsentences = 0\n\n        # Compute sentence loss if masked_lm_only is False\n        sentence_loss = None\n        if not self.masked_lm_only:\n            sentence_logits = output_metadata[\'sentence_logits\']\n            sentence_targets = sample[\'sentence_target\'].view(-1)\n            # This needs to be recomputed due to some differences between\n            # TokenBlock and BlockPair dataset. This can be resolved with a\n            # refactor of BERTModel which we will do in the future.\n            # TODO: Remove this after refactor of BERTModel\n            nsentences = sentence_targets.size(0)\n\n            # Check for logits being none which can happen when remove_heads\n            # is set to true in the BERT model. Ideally we should set\n            # masked_lm_only to true in this case, but that requires some\n            # refactor in the BERT model.\n            if sentence_logits is not None:\n                sentence_loss = compute_cross_entropy_loss(\n                    sentence_logits, sentence_targets)\n\n                loss += self.nsp_loss_weight * (sentence_loss / nsentences)\n\n        # NOTE: as we are summing up per token mlm loss and per sentence nsp loss\n        # we don\'t need to use sample_size as denominator for the gradient\n        # here sample_size is just used for logging\n        sample_size = 1\n        logging_output = {\n            \'loss\': utils.item(loss.data) if reduce else loss.data,\n            \'lm_loss\': utils.item(lm_loss.data) if reduce else lm_loss.data,\n            # sentence loss is not always computed\n            \'sentence_loss\': (\n                (\n                    utils.item(sentence_loss.data) if reduce\n                    else sentence_loss.data\n                ) if sentence_loss is not None else 0.0\n            ),\n            \'ntokens\': ntokens,\n            \'nsentences\': nsentences,\n            \'sample_size\': sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        lm_loss_sum = sum(log.get(\'lm_loss\', 0) for log in logging_outputs)\n        sentence_loss_sum = sum(\n            log.get(\'sentence_loss\', 0) for log in logging_outputs)\n        ntokens = sum(log.get(\'ntokens\', 0) for log in logging_outputs)\n        nsentences = sum(log.get(\'nsentences\', 0) for log in logging_outputs)\n        sample_size = sum(log.get(\'sample_size\', 0) for log in logging_outputs)\n        agg_loss = sum(log.get(\'loss\', 0) for log in logging_outputs)\n\n        agg_output = {\n            \'loss\': agg_loss / sample_size / math.log(2) if sample_size > 0 else 0.,\n            \'lm_loss\': lm_loss_sum / ntokens / math.log(2) if ntokens > 0 else 0.,\n            \'sentence_loss\': sentence_loss_sum / nsentences / math.log(2) if nsentences > 0 else 0.,\n            \'nll_loss\': lm_loss_sum / ntokens / math.log(2) if ntokens > 0 else 0.,\n            \'ntokens\': ntokens,\n            \'nsentences\': nsentences,\n            \'sample_size\': sample_size,\n        }\n        return agg_output\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/criterions/masked_lm.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import metrics, modules, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(\'masked_lm\')\nclass MaskedLmLoss(FairseqCriterion):\n    """"""\n    Implementation for the loss used in masked language model (MLM) training.\n    """"""\n\n    def __init__(self, task, tpu):\n        super().__init__(task)\n        self.tpu = tpu\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        masked_tokens = sample[\'target\'].ne(self.padding_idx)\n        sample_size = masked_tokens.int().sum()\n\n        # Rare: when all tokens are masked, project all tokens.\n        # We use torch.where to avoid device-to-host transfers,\n        # except on CPU where torch.where is not well supported\n        # (see github.com/pytorch/pytorch/issues/26247).\n        if self.tpu:\n            masked_tokens = None  # always project all tokens on TPU\n        elif masked_tokens.device == torch.device(\'cpu\'):\n            if not masked_tokens.any():\n                masked_tokens = None\n        else:\n            masked_tokens = torch.where(\n                masked_tokens.any(),\n                masked_tokens,\n                masked_tokens.new([True]),\n            )\n\n        logits = model(**sample[\'net_input\'], masked_tokens=masked_tokens)[0]\n        targets = model.get_targets(sample, [logits])\n        if masked_tokens is not None:\n            targets = targets[masked_tokens]\n\n        loss = modules.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            targets.view(-1),\n            reduction=\'sum\',\n            ignore_index=self.padding_idx,\n        )\n\n        logging_output = {\n            \'loss\': loss,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample[\'nsentences\'],\n            \'sample_size\': sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(\'loss\', 0) for log in logging_outputs)\n        sample_size = sum(log.get(\'sample_size\', 0) for log in logging_outputs)\n\n        metrics.log_scalar(\'loss\', loss_sum / sample_size / math.log(2), sample_size, round=3)\n        metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'loss\'].avg))\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/criterions/nat_loss.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch.nn.functional as F\nimport torch\nfrom torch import Tensor\n\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(""nat_loss"")\nclass LabelSmoothedDualImitationCriterion(FairseqCriterion):\n\n    def __init__(self, task, label_smoothing):\n        super().__init__(task)\n        self.label_smoothing = label_smoothing\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        parser.add_argument(\n            \'--label-smoothing\',\n            default=0.,\n            type=float,\n            metavar=\'D\',\n            help=\'epsilon for label smoothing, 0 means no label smoothing\',\n        )\n\n    def _compute_loss(\n        self, outputs, targets, masks=None, label_smoothing=0.0, name=""loss"", factor=1.0\n    ):\n        """"""\n            outputs: batch x len x d_model\n            targets: batch x len\n            masks:   batch x len\n\n            policy_logprob: if there is some policy\n                depends on the likelihood score as rewards.\n        """"""\n\n        def mean_ds(x: Tensor, dim=None) -> Tensor:\n            return (\n                x.float().mean().type_as(x)\n                if dim is None\n                else x.float().mean(dim).type_as(x)\n            )\n        if masks is not None:\n            outputs, targets = outputs[masks], targets[masks]\n\n        if masks is not None and not masks.any():\n            nll_loss = torch.tensor(0)\n            loss = nll_loss\n        else:\n            logits = F.log_softmax(outputs, dim=-1)\n            if targets.dim() == 1:\n                losses = F.nll_loss(logits, targets.to(logits.device), reduction=\'none\')\n\n            else:  # soft-labels\n                losses = F.kl_div(logits, targets.to(logits.device), reduction=\'none\')\n                losses = losses.sum(-1)\n\n            nll_loss = mean_ds(losses)\n            if label_smoothing > 0:\n                loss = nll_loss * (\n                    1 - label_smoothing) - mean_ds(logits) * label_smoothing\n            else:\n                loss = nll_loss\n\n        loss = loss * factor\n        return {""name"": name, ""loss"": loss, ""nll_loss"": nll_loss, ""factor"": factor}\n\n    def _custom_loss(self, loss, name=""loss"", factor=1.0):\n        return {""name"": name, ""loss"": loss, ""factor"": factor}\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        nsentences, ntokens = sample[""nsentences""], sample[""ntokens""]\n\n        # B x T\n        src_tokens, src_lengths = (\n            sample[""net_input""][""src_tokens""],\n            sample[""net_input""][""src_lengths""],\n        )\n        tgt_tokens, prev_output_tokens = sample[""target""], sample[""prev_target""]\n\n        outputs = model(src_tokens, src_lengths, prev_output_tokens, tgt_tokens)\n        losses, nll_loss = [], []\n\n        for obj in outputs:\n            if outputs[obj].get(""loss"", None) is None:\n                _losses = self._compute_loss(\n                    outputs[obj].get(""out""),\n                    outputs[obj].get(""tgt""),\n                    outputs[obj].get(""mask"", None),\n                    outputs[obj].get(""ls"", 0.0),\n                    name=obj + \'-loss\',\n                    factor=outputs[obj].get(""factor"", 1.0)\n                )\n            else:\n                _losses = self._custom_loss(\n                    outputs[obj].get(""loss""),\n                    name=obj + \'-loss\',\n                    factor=outputs[obj].get(""factor"", 1.0)\n                )\n\n            losses += [_losses]\n            if outputs[obj].get(""nll_loss"", False):\n                nll_loss += [_losses.get(""nll_loss"", 0.0)]\n\n        loss = sum(l[""loss""] for l in losses)\n        nll_loss = sum(l for l in nll_loss) if len(nll_loss) > 0 \\\n            else loss.new_tensor(0)\n\n        # NOTE:\n        # we don\'t need to use sample_size as denominator for the gradient\n        # here sample_size is just used for logging\n        sample_size = 1\n        logging_output = {\n            ""loss"": loss.data,\n            ""nll_loss"": nll_loss.data,\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""sample_size"": sample_size,\n        }\n\n        for l in losses:\n            logging_output[l[""name""]] = (\n                utils.item(l[""loss""].data / l[""factor""])\n                if reduce\n                else l[[""loss""]].data / l[""factor""]\n            )\n\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        sample_size = utils.item(sum(log.get(""sample_size"", 0) for log in logging_outputs))\n        loss = utils.item(sum(log.get(""loss"", 0) for log in logging_outputs))\n        nll_loss = utils.item(sum(log.get(""nll_loss"", 0) for log in logging_outputs))\n\n        metrics.log_scalar(\'loss\', loss / sample_size / math.log(2), sample_size, round=3)\n        metrics.log_scalar(\'nll_loss\', nll_loss / sample_size / math.log(2), sample_size, round=3)\n        metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'loss\'].avg))\n\n        for key in logging_outputs[0]:\n            if key[-5:] == ""-loss"":\n                val = sum(log.get(key, 0) for log in logging_outputs)\n                metrics.log_scalar(\n                    key[:-5],\n                    val / sample_size / math.log(2) if sample_size > 0 else 0.0,\n                    sample_size,\n                    round=3,\n                )\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/criterions/sentence_prediction.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(\'sentence_prediction\')\nclass SentencePredictionCriterion(FairseqCriterion):\n\n    def __init__(self, task, classification_head_name, regression_target):\n        super().__init__(task)\n        self.classification_head_name = classification_head_name\n        self.regression_target = regression_target\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument(\'--classification-head-name\',\n                            default=\'sentence_classification_head\',\n                            help=\'name of the classification head to use\')\n        # fmt: on\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        assert (\n            hasattr(model, \'classification_heads\')\n            and self.classification_head_name in model.classification_heads\n        ), \'model must provide sentence classification head for --criterion=sentence_prediction\'\n\n        logits, _ = model(\n            **sample[\'net_input\'],\n            features_only=True,\n            classification_head_name=self.classification_head_name,\n        )\n        targets = model.get_targets(sample, [logits]).view(-1)\n        sample_size = targets.numel()\n\n        if not self.regression_target:\n            lprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n            loss = F.nll_loss(lprobs, targets, reduction=\'sum\')\n        else:\n            logits = logits.view(-1).float()\n            targets = targets.float()\n            loss = F.mse_loss(logits, targets, reduction=\'sum\')\n\n        logging_output = {\n            \'loss\': loss.data,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample_size,\n            \'sample_size\': sample_size,\n        }\n        if not self.regression_target:\n            preds = logits.argmax(dim=1)\n            logging_output[\'ncorrect\'] = (preds == targets).sum()\n\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(\'loss\', 0) for log in logging_outputs)\n        ntokens = sum(log.get(\'ntokens\', 0) for log in logging_outputs)\n        nsentences = sum(log.get(\'nsentences\', 0) for log in logging_outputs)\n        sample_size = sum(log.get(\'sample_size\', 0) for log in logging_outputs)\n\n        metrics.log_scalar(\'loss\', loss_sum / sample_size / math.log(2), sample_size, round=3)\n        if sample_size != ntokens:\n            metrics.log_scalar(\'nll_loss\', loss_sum / ntokens / math.log(2), ntokens, round=3)\n\n        if len(logging_outputs) > 0 and \'ncorrect\' in logging_outputs[0]:\n            ncorrect = sum(log.get(\'ncorrect\', 0) for log in logging_outputs)\n            metrics.log_scalar(\'accuracy\', 100.0 * ncorrect / nsentences, nsentences, round=1)\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/criterions/sentence_ranking.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(\'sentence_ranking\')\nclass SentenceRankingCriterion(FairseqCriterion):\n\n    def __init__(self, task, ranking_head_name, save_predictions, num_classes):\n        super().__init__(task)\n        self.ranking_head_name = ranking_head_name\n        if save_predictions is not None:\n            self.prediction_h = open(save_predictions, \'w\')\n        else:\n            self.prediction_h = None\n        self.num_classes = num_classes\n\n    def __del__(self):\n        if self.prediction_h is not None:\n            self.prediction_h.close()\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument(\'--save-predictions\', metavar=\'FILE\',\n                            help=\'file to save predictions to\')\n        parser.add_argument(\'--ranking-head-name\',\n                            default=\'sentence_classification_head\',\n                            help=\'name of the ranking head to use\')\n        # fmt: on\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute ranking loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        assert (\n            hasattr(model, \'classification_heads\')\n            and self.ranking_head_name in model.classification_heads\n        ), \'model must provide sentence ranking head for --criterion=sentence_ranking\'\n\n        scores = []\n        for idx in range(self.num_classes):\n            score, _ = model(\n                **sample[\'net_input{idx}\'.format(idx=idx+1)],\n                classification_head_name=self.ranking_head_name,\n            )\n            scores.append(score)\n\n        logits = torch.cat(scores, dim=1)\n        sample_size = logits.size(0)\n\n        if \'target\' in sample:\n            targets = model.get_targets(sample, [logits]).view(-1)\n            lprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n            loss = F.nll_loss(lprobs, targets, reduction=\'sum\')\n        else:\n            targets = None\n            loss = torch.tensor(0.0, requires_grad=True)\n\n        if self.prediction_h is not None:\n            preds = logits.argmax(dim=1)\n            for i, (id, pred) in enumerate(zip(sample[\'id\'].tolist(), preds.tolist())):\n                if targets is not None:\n                    label = targets[i].item()\n                    print(\'{}\\t{}\\t{}\'.format(id, pred, label), file=self.prediction_h)\n                else:\n                    print(\'{}\\t{}\'.format(id, pred), file=self.prediction_h)\n\n        logging_output = {\n            \'loss\': loss.data,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample_size,\n            \'sample_size\': sample_size,\n        }\n        if targets is not None:\n            logging_output[\'ncorrect\'] = (logits.argmax(dim=1) == targets).sum()\n\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(\'loss\', 0) for log in logging_outputs)\n        ntokens = sum(log.get(\'ntokens\', 0) for log in logging_outputs)\n        nsentences = sum(log.get(\'nsentences\', 0) for log in logging_outputs)\n        sample_size = sum(log.get(\'sample_size\', 0) for log in logging_outputs)\n\n        metrics.log_scalar(\'loss\', loss_sum / sample_size / math.log(2), sample_size, round=3)\n        if sample_size != ntokens:\n            metrics.log_scalar(\'nll_loss\', loss_sum / ntokens / math.log(2), ntokens, round=3)\n\n        if len(logging_outputs) > 0 and \'ncorrect\' in logging_outputs[0]:\n            ncorrect = sum(log.get(\'ncorrect\', 0) for log in logging_outputs)\n            metrics.log_scalar(\'accuracy\', 100.0 * ncorrect / nsentences, nsentences, round=1)\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/data/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .dictionary import Dictionary, TruncatedDictionary\n\nfrom .fairseq_dataset import FairseqDataset, FairseqIterableDataset\n\nfrom .base_wrapper_dataset import BaseWrapperDataset\n\nfrom .append_token_dataset import AppendTokenDataset\nfrom .audio.raw_audio_dataset import FileAudioDataset\nfrom .backtranslation_dataset import BacktranslationDataset\nfrom .colorize_dataset import ColorizeDataset\nfrom .concat_dataset import ConcatDataset\nfrom .concat_sentences_dataset import ConcatSentencesDataset\nfrom .denoising_dataset import DenoisingDataset\nfrom .id_dataset import IdDataset\nfrom .indexed_dataset import IndexedCachedDataset, IndexedDataset, IndexedRawTextDataset, MMapIndexedDataset\nfrom .language_pair_dataset import LanguagePairDataset\nfrom .list_dataset import ListDataset\nfrom .lm_context_window_dataset import LMContextWindowDataset\nfrom .lru_cache_dataset import LRUCacheDataset\nfrom .mask_tokens_dataset import MaskTokensDataset\nfrom .monolingual_dataset import MonolingualDataset\nfrom .multi_corpus_sampled_dataset import MultiCorpusSampledDataset\nfrom .nested_dictionary_dataset import NestedDictionaryDataset\nfrom .noising import NoisingDataset\nfrom .numel_dataset import NumelDataset\nfrom .num_samples_dataset import NumSamplesDataset\nfrom .offset_tokens_dataset import OffsetTokensDataset\nfrom .pad_dataset import LeftPadDataset, PadDataset, RightPadDataset\nfrom .prepend_dataset import PrependDataset\nfrom .prepend_token_dataset import PrependTokenDataset\nfrom .raw_label_dataset import RawLabelDataset\nfrom .replace_dataset import ReplaceDataset\nfrom .resampling_dataset import ResamplingDataset\nfrom .roll_dataset import RollDataset\nfrom .round_robin_zip_datasets import RoundRobinZipDatasets\nfrom .sort_dataset import SortDataset\nfrom .strip_token_dataset import StripTokenDataset\nfrom .subsample_dataset import SubsampleDataset\nfrom .token_block_dataset import TokenBlockDataset\nfrom .transform_eos_dataset import TransformEosDataset\nfrom .transform_eos_lang_pair_dataset import TransformEosLangPairDataset\nfrom .shorten_dataset import TruncateDataset, RandomCropDataset\n\nfrom .iterators import (\n    CountingIterator,\n    EpochBatchIterator,\n    GroupedIterator,\n    ShardedIterator,\n)\n\n__all__ = [\n    'AppendTokenDataset',\n    'BacktranslationDataset',\n    'BaseWrapperDataset',\n    'ColorizeDataset',\n    'ConcatDataset',\n    'ConcatSentencesDataset',\n    'CountingIterator',\n    'DenoisingDataset',\n    'Dictionary',\n    'EpochBatchIterator',\n    'FairseqDataset',\n    'FairseqIterableDataset',\n    'GroupedIterator',\n    'IdDataset',\n    'IndexedCachedDataset',\n    'IndexedDataset',\n    'IndexedRawTextDataset',\n    'LanguagePairDataset',\n    'LeftPadDataset',\n    'ListDataset',\n    'LMContextWindowDataset',\n    'LRUCacheDataset',\n    'MaskTokensDataset',\n    'MMapIndexedDataset',\n    'MonolingualDataset',\n    'MultiCorpusSampledDataset',\n    'NestedDictionaryDataset',\n    'NoisingDataset',\n    'NumelDataset',\n    'NumSamplesDataset',\n    'OffsetTokensDataset',\n    'PadDataset',\n    'PrependDataset',\n    'PrependTokenDataset',\n    'ReplaceDataset',\n    'RollDataset',\n    'FileAudioDataset',\n    'RawLabelDataset',\n    'ResamplingDataset',\n    'RightPadDataset',\n    'RoundRobinZipDatasets',\n    'ShardedIterator',\n    'SortDataset',\n    'StripTokenDataset',\n    'SubsampleDataset',\n    'TokenBlockDataset',\n    'TransformEosDataset',\n    'TransformEosLangPairDataset',\n    'TruncateDataset',\n    'TruncatedDictionary',\n]\n"""
fairseq/data/append_token_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass AppendTokenDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, token=None):\n        super().__init__(dataset)\n        self.token = token\n        if token is not None:\n            self._sizes = np.array(dataset.sizes) + 1\n        else:\n            self._sizes = dataset.sizes\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        if self.token is not None:\n            item = torch.cat([item, item.new([self.token])])\n        return item\n\n    @property\n    def sizes(self):\n        return self._sizes\n\n    def num_tokens(self, index):\n        n = self.dataset.num_tokens(index)\n        if self.token is not None:\n            n += 1\n        return n\n\n    def size(self, index):\n        n = self.dataset.size(index)\n        if self.token is not None:\n            n += 1\n        return n\n'"
fairseq/data/backtranslation_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom fairseq import utils\n\nfrom . import FairseqDataset\n\n\ndef backtranslate_samples(samples, collate_fn, generate_fn, cuda=True):\n    """"""Backtranslate a list of samples.\n\n    Given an input (*samples*) of the form:\n\n        [{\'id\': 1, \'source\': \'hallo welt\'}]\n\n    this will return:\n\n        [{\'id\': 1, \'source\': \'hello world\', \'target\': \'hallo welt\'}]\n\n    Args:\n        samples (List[dict]): samples to backtranslate. Individual samples are\n            expected to have a \'source\' key, which will become the \'target\'\n            after backtranslation.\n        collate_fn (callable): function to collate samples into a mini-batch\n        generate_fn (callable): function to generate backtranslations\n        cuda (bool): use GPU for generation (default: ``True``)\n\n    Returns:\n        List[dict]: an updated list of samples with a backtranslated source\n    """"""\n    collated_samples = collate_fn(samples)\n    s = utils.move_to_cuda(collated_samples) if cuda else collated_samples\n    generated_sources = generate_fn(s)\n\n    id_to_src = {\n        sample[\'id\']: sample[\'source\'] for sample in samples\n    }\n\n    # Go through each tgt sentence in batch and its corresponding best\n    # generated hypothesis and create a backtranslation data pair\n    # {id: id, source: generated backtranslation, target: original tgt}\n    return [\n        {\'id\': id.item(), \'target\': id_to_src[id.item()], \'source\': hypos[0][\'tokens\'].cpu()}\n        for id, hypos in zip(collated_samples[\'id\'], generated_sources)\n    ]\n\n\nclass BacktranslationDataset(FairseqDataset):\n    """"""\n    Sets up a backtranslation dataset which takes a tgt batch, generates\n    a src using a tgt-src backtranslation function (*backtranslation_fn*),\n    and returns the corresponding `{generated src, input tgt}` batch.\n\n    Args:\n        tgt_dataset (~fairseq.data.FairseqDataset): the dataset to be\n            backtranslated. Only the source side of this dataset will be used.\n            After backtranslation, the source sentences in this dataset will be\n            returned as the targets.\n        src_dict (~fairseq.data.Dictionary): the dictionary of backtranslated\n            sentences.\n        tgt_dict (~fairseq.data.Dictionary, optional): the dictionary of\n            sentences to be backtranslated.\n        backtranslation_fn (callable, optional): function to call to generate\n            backtranslations. This is typically the `generate` method of a\n            :class:`~fairseq.sequence_generator.SequenceGenerator` object.\n            Pass in None when it is not available at initialization time, and\n            use set_backtranslation_fn function to set it when available.\n        output_collater (callable, optional): function to call on the\n            backtranslated samples to create the final batch\n            (default: ``tgt_dataset.collater``).\n        cuda: use GPU for generation\n    """"""\n\n    def __init__(\n        self,\n        tgt_dataset,\n        src_dict,\n        tgt_dict=None,\n        backtranslation_fn=None,\n        output_collater=None,\n        cuda=True,\n        **kwargs\n    ):\n        self.tgt_dataset = tgt_dataset\n        self.backtranslation_fn = backtranslation_fn\n        self.output_collater = output_collater if output_collater is not None \\\n            else tgt_dataset.collater\n        self.cuda = cuda if torch.cuda.is_available() else False\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n\n    def __getitem__(self, index):\n        """"""\n        Returns a single sample from *tgt_dataset*. Note that backtranslation is\n        not applied in this step; use :func:`collater` instead to backtranslate\n        a batch of samples.\n        """"""\n        return self.tgt_dataset[index]\n\n    def __len__(self):\n        return len(self.tgt_dataset)\n\n    def set_backtranslation_fn(self, backtranslation_fn):\n        self.backtranslation_fn = backtranslation_fn\n\n    def collater(self, samples):\n        """"""Merge and backtranslate a list of samples to form a mini-batch.\n\n        Using the samples from *tgt_dataset*, load a collated target sample to\n        feed to the backtranslation model. Then take the backtranslation with\n        the best score as the source and the original input as the target.\n\n        Note: we expect *tgt_dataset* to provide a function `collater()` that\n        will collate samples into the format expected by *backtranslation_fn*.\n        After backtranslation, we will feed the new list of samples (i.e., the\n        `(backtranslated source, original source)` pairs) to *output_collater*\n        and return the result.\n\n        Args:\n            samples (List[dict]): samples to backtranslate and collate\n\n        Returns:\n            dict: a mini-batch with keys coming from *output_collater*\n        """"""\n        if samples[0].get(\'is_dummy\', False):\n            return samples\n        samples = backtranslate_samples(\n            samples=samples,\n            collate_fn=self.tgt_dataset.collater,\n            generate_fn=(\n                lambda net_input: self.backtranslation_fn(net_input)\n            ),\n            cuda=self.cuda,\n        )\n        return self.output_collater(samples)\n\n    def num_tokens(self, index):\n        """"""Just use the tgt dataset num_tokens""""""\n        return self.tgt_dataset.num_tokens(index)\n\n    def ordered_indices(self):\n        """"""Just use the tgt dataset ordered_indices""""""\n        return self.tgt_dataset.ordered_indices()\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used\n        when filtering a dataset with ``--max-positions``.\n\n        Note: we use *tgt_dataset* to approximate the length of the source\n        sentence, since we do not know the actual length until after\n        backtranslation.\n        """"""\n        tgt_size = self.tgt_dataset.size(index)[0]\n        return (tgt_size, tgt_size)\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.tgt_dataset, \'supports_prefetch\', False)\n\n    def prefetch(self, indices):\n        return self.tgt_dataset.prefetch(indices)\n'"
fairseq/data/base_wrapper_dataset.py,1,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom torch.utils.data.dataloader import default_collate\n\nfrom . import FairseqDataset\n\n\nclass BaseWrapperDataset(FairseqDataset):\n\n    def __init__(self, dataset):\n        super().__init__()\n        self.dataset = dataset\n\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n        if hasattr(self.dataset, 'collater'):\n            return self.dataset.collater(samples)\n        else:\n            return default_collate(samples)\n\n    @property\n    def sizes(self):\n        return self.dataset.sizes\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(index)\n\n    def size(self, index):\n        return self.dataset.size(index)\n\n    def ordered_indices(self):\n        return self.dataset.ordered_indices()\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, 'supports_prefetch', False)\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(indices)\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n        if hasattr(self.dataset, 'set_epoch'):\n            self.dataset.set_epoch(epoch)\n"""
fairseq/data/colorize_dataset.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass ColorizeDataset(BaseWrapperDataset):\n    """""" Adds \'colors\' property to net input that is obtained from the provided color getter for use by models """"""\n    def __init__(self, dataset, color_getter):\n        super().__init__(dataset)\n        self.color_getter = color_getter\n\n    def collater(self, samples):\n        base_collate = super().collater(samples)\n        if len(base_collate) > 0:\n            base_collate[""net_input""][""colors""] = torch.tensor(\n                list(self.color_getter(self.dataset, s[""id""]) for s in samples),\n                dtype=torch.long,\n            )\n        return base_collate\n'"
fairseq/data/concat_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport bisect\n\nimport numpy as np\nfrom torch.utils.data.dataloader import default_collate\n\nfrom . import FairseqDataset\n\n\nclass ConcatDataset(FairseqDataset):\n    @staticmethod\n    def cumsum(sequence, sample_ratios):\n        r, s = [], 0\n        for e, ratio in zip(sequence, sample_ratios):\n            curr_len = int(ratio * len(e))\n            r.append(curr_len + s)\n            s += curr_len\n        return r\n\n    def __init__(self, datasets, sample_ratios=1):\n        super(ConcatDataset, self).__init__()\n        assert len(datasets) > 0, ""datasets should not be an empty iterable""\n        self.datasets = list(datasets)\n        if isinstance(sample_ratios, int):\n            sample_ratios = [sample_ratios] * len(self.datasets)\n        self.sample_ratios = sample_ratios\n        self.cumulative_sizes = self.cumsum(self.datasets, sample_ratios)\n        self.real_sizes = [len(d) for d in self.datasets]\n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n\n    def __getitem__(self, idx):\n        dataset_idx, sample_idx = self._get_dataset_and_sample_index(idx)\n        return self.datasets[dataset_idx][sample_idx]\n\n    def _get_dataset_and_sample_index(self, idx: int):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        sample_idx = sample_idx % self.real_sizes[dataset_idx]\n        return dataset_idx, sample_idx\n\n    def collater(self, samples):\n        # For now only supports datasets with same underlying collater implementations\n        if hasattr(self.datasets[0], \'collater\'):\n            return self.datasets[0].collater(samples)\n        else:\n            return default_collate(samples)\n\n    def size(self, idx: int):\n        """"""\n        Return an example\'s size as a float or tuple.\n        """"""\n        dataset_idx, sample_idx = self._get_dataset_and_sample_index(idx)\n        return self.datasets[dataset_idx].size(sample_idx)\n\n    def num_tokens(self, index: int):\n        return np.max(self.size(index))\n\n    def attr(self, attr: str, index: int):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, index)\n        return getattr(self.datasets[dataset_idx], attr, None)\n\n    @property\n    def sizes(self):\n        _dataset_sizes = []\n        for ds, sr in zip(self.datasets, self.sample_ratios):\n            if isinstance(ds.sizes, np.ndarray):\n                _dataset_sizes.append(np.tile(ds.sizes, sr))\n            else:\n                # Only support underlying dataset with single size array.\n                assert isinstance(ds.sizes, list)\n                _dataset_sizes.append(np.tile(ds.sizes[0], sr))\n        return np.concatenate(_dataset_sizes)\n\n    @property\n    def supports_prefetch(self):\n        return all(d.supports_prefetch for d in self.datasets)\n\n    def ordered_indices(self):\n        """"""\n        Returns indices sorted by length. So less padding is needed.\n        """"""\n        return np.argsort(self.sizes)\n\n    def prefetch(self, indices):\n        frm = 0\n        for to, ds in zip(self.cumulative_sizes, self.datasets):\n            real_size = len(ds)\n            if getattr(ds, \'supports_prefetch\', False):\n                ds.prefetch([(i - frm) % real_size for i in indices if frm <= i < to])\n            frm = to\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n        for ds in self.datasets:\n            if hasattr(ds, \'set_epoch\'):\n                ds.set_epoch(epoch)\n'"
fairseq/data/concat_sentences_dataset.py,1,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom . import FairseqDataset\n\n\nclass ConcatSentencesDataset(FairseqDataset):\n\n    def __init__(self, *datasets):\n        super().__init__()\n        self.datasets = datasets\n        assert all(len(ds) == len(datasets[0]) for ds in datasets), \\\n            'datasets must have the same length'\n\n    def __getitem__(self, index):\n        return torch.cat([ds[index] for ds in self.datasets])\n\n    def __len__(self):\n        return len(self.datasets[0])\n\n    def collater(self, samples):\n        return self.datasets[0].collater(samples)\n\n    @property\n    def sizes(self):\n        return sum(ds.sizes for ds in self.datasets)\n\n    def num_tokens(self, index):\n        return sum(ds.num_tokens(index) for ds in self.datasets)\n\n    def size(self, index):\n        return sum(ds.size(index) for ds in self.datasets)\n\n    def ordered_indices(self):\n        return self.datasets[0].ordered_indices()\n\n    @property\n    def supports_prefetch(self):\n        return any(\n            getattr(ds, 'supports_prefetch', False) for ds in self.datasets\n        )\n\n    def prefetch(self, indices):\n        for ds in self.datasets:\n            if getattr(ds, 'supports_prefetch', False):\n                ds.prefetch(indices)\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n        for ds in self.datasets:\n            if hasattr(ds, 'set_epoch'):\n                ds.set_epoch(epoch)\n"""
fairseq/data/data_utils.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\ntry:\n    from collections.abc import Iterable\nexcept ImportError:\n    from collections import Iterable\nimport contextlib\nimport itertools\nimport logging\nimport os\nimport sys\nimport types\n\nimport numpy as np\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef infer_language_pair(path):\n    """"""Infer language pair from filename: <split>.<lang1>-<lang2>.(...).idx""""""\n    src, dst = None, None\n    for filename in os.listdir(path):\n        parts = filename.split(\'.\')\n        if len(parts) >= 3 and len(parts[1].split(\'-\')) == 2:\n            return parts[1].split(\'-\')\n    return src, dst\n\n\ndef collate_tokens(values, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False):\n    """"""Convert a list of 1d tensors into a padded 2d tensor.""""""\n    size = max(v.size(0) for v in values)\n    res = values[0].new(len(values), size).fill_(pad_idx)\n\n    def copy_tensor(src, dst):\n        assert dst.numel() == src.numel()\n        if move_eos_to_beginning:\n            dst[0] = eos_idx\n            dst[1:] = src[:-1]\n        else:\n            dst.copy_(src)\n\n    for i, v in enumerate(values):\n        copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n    return res\n\n\ndef load_indexed_dataset(path, dictionary, dataset_impl=None, combine=False, default=\'cached\'):\n    """"""A helper function for loading indexed datasets.\n\n    Args:\n        path (str): path to indexed dataset (e.g., \'data-bin/train\')\n        dictionary (~fairseq.data.Dictionary): data dictionary\n        dataset_impl (str, optional): which dataset implementation to use. If\n            not provided, it will be inferred automatically. For legacy indexed\n            data we use the \'cached\' implementation by default.\n        combine (bool, optional): automatically load and combine multiple\n            datasets. For example, if *path* is \'data-bin/train\', then we will\n            combine \'data-bin/train\', \'data-bin/train1\', ... and return a\n            single ConcatDataset instance.\n    """"""\n    from fairseq.data.concat_dataset import ConcatDataset\n    import fairseq.data.indexed_dataset as indexed_dataset\n\n    datasets = []\n    for k in itertools.count():\n        path_k = path + (str(k) if k > 0 else \'\')\n\n        dataset_impl_k = dataset_impl\n        if dataset_impl_k is None:\n            dataset_impl_k = indexed_dataset.infer_dataset_impl(path_k)\n\n        dataset = indexed_dataset.make_dataset(\n            path_k,\n            impl=dataset_impl_k or default,\n            fix_lua_indexing=True,\n            dictionary=dictionary,\n        )\n        if dataset is None:\n            break\n        logger.info(\'loaded {} examples from: {}\'.format(len(dataset), path_k))\n        datasets.append(dataset)\n        if not combine:\n            break\n    if len(datasets) == 0:\n        return None\n    elif len(datasets) == 1:\n        return datasets[0]\n    else:\n        return ConcatDataset(datasets)\n\n\n@contextlib.contextmanager\ndef numpy_seed(seed, *addl_seeds):\n    """"""Context manager which seeds the NumPy PRNG with the specified seed and\n    restores the state afterward""""""\n    if seed is None:\n        yield\n        return\n    if len(addl_seeds) > 0:\n        seed = int(hash((seed, *addl_seeds)) % 1e6)\n    state = np.random.get_state()\n    np.random.seed(seed)\n    try:\n        yield\n    finally:\n        np.random.set_state(state)\n\n\ndef collect_filtered(function, iterable, filtered):\n    """"""\n    Similar to :func:`filter` but collects filtered elements in ``filtered``.\n\n    Args:\n        function (callable): function that returns ``False`` for elements that\n            should be filtered\n        iterable (iterable): iterable to filter\n        filtered (list): list to store filtered elements\n    """"""\n    for el in iterable:\n        if function(el):\n            yield el\n        else:\n            filtered.append(el)\n\n\ndef _filter_by_size_dynamic(indices, size_fn, max_positions, raise_exception=False):\n    def check_size(idx):\n        if isinstance(max_positions, float) or isinstance(max_positions, int):\n            return size_fn(idx) <= max_positions\n        elif isinstance(max_positions, dict):\n            idx_size = size_fn(idx)\n            assert isinstance(idx_size, dict)\n            intersect_keys = set(max_positions.keys()) & set(idx_size.keys())\n            return all(\n                all(a is None or b is None or a <= b\n                    for a, b in zip(idx_size[key], max_positions[key]))\n                for key in intersect_keys\n            )\n        else:\n            # Hacky as heck, for the specific case of multilingual training with RoundRobin.\n            if isinstance(size_fn(idx), dict) and isinstance(max_positions, tuple):\n                return all(\n                    a is None or b is None or a <= b\n                    for a, b in zip(size_fn(idx).values(), max_positions)\n                )\n            # For MultiCorpusSampledDataset, will generalize it later\n            if not isinstance(size_fn(idx), Iterable):\n                return all(size_fn(idx) <= b for b in max_positions)\n            return all(\n                a is None or b is None or a <= b\n                for a, b in zip(size_fn(idx), max_positions)\n            )\n    ignored = []\n    itr = collect_filtered(check_size, indices, ignored)\n    indices = np.fromiter(itr, dtype=np.int64, count=-1)\n    return indices, ignored\n\n\ndef filter_by_size(indices, dataset, max_positions, raise_exception=False):\n    """"""\n    Filter indices based on their size.\n\n    Args:\n        indices (List[int]): ordered list of dataset indices\n        dataset (FairseqDataset): fairseq dataset instance\n        max_positions (tuple): filter elements larger than this size.\n            Comparisons are done component-wise.\n        raise_exception (bool, optional): if ``True``, raise an exception if\n            any elements are filtered (default: False).\n    """"""\n    if isinstance(max_positions, float) or isinstance(max_positions, int):\n        if hasattr(dataset, \'sizes\') and isinstance(dataset.sizes, np.ndarray):\n            ignored = indices[dataset.sizes[indices] > max_positions].tolist()\n            indices = indices[dataset.sizes[indices] <= max_positions]\n        elif hasattr(dataset, \'sizes\') and isinstance(dataset.sizes, list) and len(dataset.sizes) == 1:\n            ignored = indices[dataset.sizes[0][indices] > max_positions].tolist()\n            indices = indices[dataset.sizes[0][indices] <= max_positions]\n        else:\n            indices, ignored = _filter_by_size_dynamic(indices, dataset.size, max_positions)\n    else:\n        indices, ignored = _filter_by_size_dynamic(indices, dataset.size, max_positions)\n\n    if len(ignored) > 0 and raise_exception:\n        raise Exception((\n            \'Size of sample #{} is invalid (={}) since max_positions={}, \'\n            \'skip this example with --skip-invalid-size-inputs-valid-test\'\n        ).format(ignored[0], dataset.size(ignored[0]), max_positions))\n    if len(ignored) > 0:\n        logger.warning((\n            \'{} samples have invalid sizes and will be skipped, \'\n            \'max_positions={}, first few sample ids={}\'\n        ).format(len(ignored), max_positions, ignored[:10]))\n    return indices\n\n\ndef batch_by_size(\n    indices, num_tokens_fn, max_tokens=None, max_sentences=None,\n    required_batch_size_multiple=1,\n):\n    """"""\n    Yield mini-batches of indices bucketed by size. Batches may contain\n    sequences of different lengths.\n\n    Args:\n        indices (List[int]): ordered list of dataset indices\n        num_tokens_fn (callable): function that returns the number of tokens at\n            a given index\n        max_tokens (int, optional): max number of tokens in each batch\n            (default: None).\n        max_sentences (int, optional): max number of sentences in each\n            batch (default: None).\n        required_batch_size_multiple (int, optional): require batch size to\n            be a multiple of N (default: 1).\n    """"""\n    try:\n        from fairseq.data.data_utils_fast import batch_by_size_fast\n    except ImportError:\n        raise ImportError(\n            \'Please build Cython components with: `pip install --editable .` \'\n            \'or `python setup.py build_ext --inplace`\'\n        )\n\n    max_tokens = max_tokens if max_tokens is not None else -1\n    max_sentences = max_sentences if max_sentences is not None else -1\n    bsz_mult = required_batch_size_multiple\n\n    if isinstance(indices, types.GeneratorType):\n        indices = np.fromiter(indices, dtype=np.int64, count=-1)\n\n    return batch_by_size_fast(indices, num_tokens_fn, max_tokens, max_sentences, bsz_mult)\n\n\ndef process_bpe_symbol(sentence: str, bpe_symbol: str):\n    if bpe_symbol == \'sentencepiece\':\n        sentence = sentence.replace(\' \', \'\').replace(\'\\u2581\', \' \').strip()\n    elif bpe_symbol == \'_EOW\':\n        sentence = sentence.replace(\' \', \'\').replace(\'_EOW\', \' \').strip()\n    elif bpe_symbol is not None:\n        sentence = (sentence + \' \').replace(bpe_symbol, \'\').rstrip()\n    return sentence\n'"
fairseq/data/denoising_dataset.py,25,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\nimport math\n\nfrom . import data_utils, FairseqDataset\n\n\ndef collate(\n    samples,\n    pad_idx,\n    eos_idx,\n    vocab,\n    left_pad_source=False,\n    left_pad_target=False,\n    input_feeding=True,\n):\n    assert input_feeding\n    if len(samples) == 0:\n        return {}\n\n    def merge(key, left_pad, move_eos_to_beginning=False):\n        return data_utils.collate_tokens(\n            [s[key] for s in samples],\n            pad_idx, eos_idx, left_pad, move_eos_to_beginning,\n        )\n\n    id = torch.LongTensor([s[\'id\'] for s in samples])\n    src_tokens = merge(\'source\', left_pad=left_pad_source)\n    # sort by descending source length\n    src_lengths = torch.LongTensor([s[\'source\'].numel() for s in samples])\n    src_lengths, sort_order = src_lengths.sort(descending=True)\n    id = id.index_select(0, sort_order)\n    src_tokens = src_tokens.index_select(0, sort_order)\n\n    prev_output_tokens = None\n    target = None\n    if samples[0].get(\'target\', None) is not None:\n        target = merge(\'target\', left_pad=left_pad_target)\n        target = target.index_select(0, sort_order)\n        ntokens = sum(len(s[\'target\']) for s in samples)\n\n        if input_feeding:\n            # we create a shifted version of targets for feeding the\n            # previous output token(s) into the next decoder step\n            prev_output_tokens = merge(\n                \'target\',\n                left_pad=left_pad_target,\n                move_eos_to_beginning=True,\n            )\n            prev_output_tokens = prev_output_tokens.index_select(0, sort_order)\n    else:\n        ntokens = sum(len(s[\'source\']) for s in samples)\n\n    batch = {\n        \'id\': id,\n        \'ntokens\': ntokens,\n        \'net_input\': {\n            \'src_tokens\': src_tokens,\n            \'src_lengths\': src_lengths,\n        },\n        \'target\': target,\n        \'nsentences\': samples[0][\'source\'].size(0),\n    }\n    if prev_output_tokens is not None:\n        batch[\'net_input\'][\'prev_output_tokens\'] = prev_output_tokens\n\n    return batch\n\n\nclass DenoisingDataset(FairseqDataset):\n    """"""\n    A wrapper around TokenBlockDataset for BART dataset.\n\n    Args:\n        dataset (TokenBlockDataset): dataset to wrap\n        sizes (List[int]): sentence lengths\n        vocab (~fairseq.data.Dictionary): vocabulary\n        mask_idx (int): dictionary index used for masked token\n        mask_whole_words: only mask whole words. This should be a byte mask\n            over vocab indices, indicating whether it is the beginning of a\n            word. We will extend any mask to encompass the whole word.\n        shuffle (bool, optional): shuffle the elements before batching.\n          Default: ``True``\n        seed: Seed for random number generator for reproducibility.\n        args: argparse arguments.\n    """"""\n\n    def __init__(\n        self,\n        dataset,\n        sizes,\n        vocab,\n        mask_idx,\n        mask_whole_words,\n        shuffle,\n        seed,\n        args,\n        eos=None\n    ):\n        self.dataset = dataset\n\n        self.sizes = sizes\n\n        self.vocab = vocab\n        self.shuffle = shuffle\n        self.seed = seed\n        self.mask_idx = mask_idx\n        self.mask_whole_word = mask_whole_words\n        self.mask_ratio = args.mask\n        self.random_ratio = args.mask_random\n        self.insert_ratio = args.insert\n        self.rotate_ratio = args.rotate\n        self.permute_sentence_ratio = args.permute_sentences\n        self.eos = (eos if eos is not None else vocab.eos())\n\n        if args.bpe != \'gpt2\':\n            self.full_stop_index = self.vocab.eos()\n        else:\n            assert args.bpe == \'gpt2\'\n            self.full_stop_index = self.vocab.index(\'13\')\n\n        self.replace_length = args.replace_length\n        if not self.replace_length in [-1, 0, 1]:\n            raise (f\'invalid arg: replace_length={self.replace_length}\')\n        if not args.mask_length in [\'subword\', \'word\', \'span-poisson\']:\n            raise (f\'invalid arg: mask-length={args.mask_length}\')\n        if args.mask_length == \'subword\' and not args.replace_length in [0, 1]:\n            raise (f\'if using subwords, use replace-length=1 or 0\')\n\n        self.mask_span_distribution = None\n        if args.mask_length == \'span-poisson\':\n            _lambda = args.poisson_lambda\n\n            lambda_to_the_k = 1\n            e_to_the_minus_lambda = math.exp(-_lambda)\n            k_factorial = 1\n            ps = []\n            for k in range(0, 128):\n                ps.append(e_to_the_minus_lambda * lambda_to_the_k / k_factorial)\n                lambda_to_the_k *= _lambda\n                k_factorial *= (k + 1)\n                if ps[-1] < 0.0000001:\n                    break\n            ps = torch.FloatTensor(ps)\n            self.mask_span_distribution = torch.distributions.Categorical(ps)\n\n        self.epoch = 0\n\n    def set_epoch(self, epoch, **unused):\n        self.epoch = epoch\n\n    def __getitem__(self, index):\n        with data_utils.numpy_seed(self.seed, self.epoch, index):\n            tokens = self.dataset[index]\n            assert tokens[-1] == self.eos\n            source, target = tokens, tokens.clone()\n\n            if self.permute_sentence_ratio > 0.0:\n                source = self.permute_sentences(source, self.permute_sentence_ratio)\n\n            if self.mask_ratio > 0:\n                source = self.add_whole_word_mask(source, self.mask_ratio)\n\n            if self.insert_ratio > 0:\n                source = self.add_insertion_noise(source, self.insert_ratio)\n\n            if self.rotate_ratio > 0.0 and np.random.random() < self.rotate_ratio:\n                source = self.add_rolling_noise(source)\n\n        assert (source >= 0).all()\n        assert (source[1:-1] >= 1).all()\n        assert (source <= len(self.vocab)).all()\n        assert source[0] == self.vocab.bos()\n        assert source[-1] == self.eos\n        return {\n            \'id\': index,\n            \'source\': source,\n            \'target\': target,\n        }\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def permute_sentences(self, source, p=1.0):\n        full_stops = (source == self.full_stop_index)\n        # Pretend it ends with a full stop so last span is a sentence\n        full_stops[-2] = 1\n\n        # Tokens that are full stops, where the previous token is not\n        sentence_ends = (full_stops[1:] * ~full_stops[:-1]).nonzero() + 2\n        result = source.clone()\n\n        num_sentences = sentence_ends.size(0)\n        num_to_permute = math.ceil((num_sentences * 2 * p) / 2.0)\n        substitutions = torch.randperm(num_sentences)[:num_to_permute]\n        ordering = torch.arange(0, num_sentences)\n        ordering[substitutions] = substitutions[torch.randperm(num_to_permute)]\n\n        # Ignore <bos> at start\n        index = 1\n        for i in ordering:\n            sentence = source[(sentence_ends[i - 1] if i > 0 else 1):sentence_ends[i]]\n            result[index:index + sentence.size(0)] = sentence\n            index += sentence.size(0)\n        return result\n\n    def word_starts(self, source):\n        if self.mask_whole_word is not None:\n            is_word_start = self.mask_whole_word.gather(0, source)\n        else:\n            is_word_start = torch.ones(source.size())\n        is_word_start[0] = 0\n        is_word_start[-1] = 0\n        return is_word_start\n\n    def add_whole_word_mask(self, source, p):\n        is_word_start = self.word_starts(source)\n        num_to_mask = int(math.ceil(is_word_start.float().sum() * p))\n        num_inserts = 0\n        if num_to_mask == 0:\n            return source\n\n        if self.mask_span_distribution is not None:\n            lengths = self.mask_span_distribution.sample(sample_shape=(num_to_mask,))\n\n            # Make sure we have enough to mask\n            cum_length = torch.cumsum(lengths, 0)\n            while cum_length[-1] < num_to_mask:\n                lengths = torch.cat([lengths, self.mask_span_distribution.sample(sample_shape=(num_to_mask,))], dim=0)\n                cum_length = torch.cumsum(lengths, 0)\n\n            # Trim to masking budget\n            i = 0\n            while cum_length[i] < num_to_mask:\n                i += 1\n            lengths[i] = num_to_mask - (0 if i == 0 else cum_length[i - 1])\n            num_to_mask = i + 1\n            lengths = lengths[:num_to_mask]\n\n            # Handle 0-length mask (inserts) separately\n            lengths = lengths[lengths > 0]\n            num_inserts = num_to_mask - lengths.size(0)\n            num_to_mask -= num_inserts\n            if num_to_mask == 0:\n                return self.add_insertion_noise(source, num_inserts / source.size(0))\n\n            assert (lengths > 0).all()\n        else:\n            lengths = torch.ones((num_to_mask,)).long()\n        assert is_word_start[-1] == 0\n        word_starts = is_word_start.nonzero()\n        indices = word_starts[torch.randperm(word_starts.size(0))[:num_to_mask]].squeeze(1)\n        mask_random = torch.FloatTensor(num_to_mask).uniform_() < self.random_ratio\n\n        source_length = source.size(0)\n        assert source_length - 1 not in indices\n        to_keep = torch.ones(source_length, dtype=torch.bool)\n        is_word_start[-1] = 255 # acts as a long length, so spans don\'t go over the end of doc\n        if self.replace_length == 0:\n            to_keep[indices] = 0\n        else:\n            # keep index, but replace it with [MASK]\n            source[indices] = self.mask_idx\n            source[indices[mask_random]] = torch.randint(1, len(self.vocab), size=(mask_random.sum(),))\n\n        if self.mask_span_distribution is not None:\n            assert len(lengths.size()) == 1\n            assert lengths.size() == indices.size()\n            lengths -= 1\n            while indices.size(0) > 0:\n                assert lengths.size() == indices.size()\n                lengths -= is_word_start[indices + 1].long()\n                uncompleted = lengths >= 0\n                indices = indices[uncompleted] + 1\n                mask_random = mask_random[uncompleted]\n                lengths = lengths[uncompleted]\n                if self.replace_length != -1:\n                    # delete token\n                    to_keep[indices] = 0\n                else:\n                    # keep index, but replace it with [MASK]\n                    source[indices] = self.mask_idx\n                    source[indices[mask_random]] = torch.randint(1, len(self.vocab), size=(mask_random.sum(),))\n        else:\n            # A bit faster when all lengths are 1\n            while indices.size(0) > 0:\n                uncompleted = is_word_start[indices + 1] == 0\n                indices = indices[uncompleted] + 1\n                mask_random = mask_random[uncompleted]\n                if self.replace_length != -1:\n                    # delete token\n                    to_keep[indices] = 0\n                else:\n                    # keep index, but replace it with [MASK]\n                    source[indices] = self.mask_idx\n                    source[indices[mask_random]] = torch.randint(1, len(self.vocab), size=(mask_random.sum(),))\n\n                assert source_length - 1 not in indices\n\n        source = source[to_keep]\n\n        if num_inserts > 0:\n            source = self.add_insertion_noise(source, num_inserts / source.size(0))\n\n        return source\n\n    def add_permuted_noise(self, tokens, p):\n        num_words = len(tokens)\n        num_to_permute = math.ceil(((num_words * 2) * p) / 2.0)\n        substitutions = torch.randperm(num_words - 2)[:num_to_permute] + 1\n        tokens[substitutions] = tokens[substitutions[torch.randperm(num_to_permute)]]\n        return tokens\n\n    def add_rolling_noise(self, tokens):\n        offset = np.random.randint(1, max(1, tokens.size(-1) - 1) + 1)\n        tokens = torch.cat(\n            (tokens[0:1], tokens[offset:-1], tokens[1:offset], tokens[-1:]),\n            dim=0,\n        )\n        return tokens\n\n    def add_insertion_noise(self, tokens, p):\n        if p == 0.0:\n            return tokens\n\n        num_tokens = len(tokens)\n        n = int(math.ceil(num_tokens * p))\n\n        noise_indices = torch.randperm(num_tokens + n - 2)[:n] + 1\n        noise_mask = torch.zeros(size=(num_tokens + n,), dtype=torch.bool)\n        noise_mask[noise_indices] = 1\n        result = torch.LongTensor(n + len(tokens)).fill_(-1)\n\n        num_random = int(math.ceil(n * self.random_ratio))\n        result[noise_indices[num_random:]] = self.mask_idx\n        result[noise_indices[:num_random]] = torch.randint(low=1, high=len(self.vocab), size=(num_random,))\n\n        result[~noise_mask] = tokens\n\n        assert (result >= 0).all()\n        return result\n\n    def collater(self, samples):\n        """"""Merge a list of samples to form a mini-batch.\n        Args:\n            samples (List[dict]): samples to collate\n        Returns:\n            dict: a mini-batch of data\n        """"""\n        return collate(samples, self.vocab.pad(), self.vocab.eos(), self.vocab)\n\n    def num_tokens(self, index):\n        """"""Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.""""""\n        return self.sizes[index]\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.""""""\n        return self.sizes[index]\n\n    def ordered_indices(self):\n        """"""Return an ordered list of indices. Batches will be constructed based\n        on this order.""""""\n        if self.shuffle:\n            indices = np.random.permutation(len(self))\n        else:\n            indices = np.arange(len(self))\n        return indices[np.argsort(self.sizes[indices], kind=\'mergesort\')]\n\n    def prefetch(self, indices):\n        self.src.prefetch(indices)\n        self.tgt.prefetch(indices)\n\n    @property\n    def supports_prefetch(self):\n        return (\n            hasattr(self.src, \'supports_prefetch\')\n            and self.src.supports_prefetch\n            and hasattr(self.tgt, \'supports_prefetch\')\n            and self.tgt.supports_prefetch\n        )\n'"
fairseq/data/dictionary.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nfrom collections import Counter\nfrom multiprocessing import Pool\n\nimport torch\nfrom fairseq import utils\nfrom fairseq.binarizer import safe_readline\nfrom fairseq.data import data_utils\nfrom fairseq.file_io import PathManager\nfrom fairseq.tokenizer import tokenize_line\n\n\nclass Dictionary(object):\n    """"""A mapping from symbols to consecutive integers""""""\n\n    def __init__(\n        self,\n        *,  # begin keyword-only arguments\n        pad=""<pad>"",\n        eos=""</s>"",\n        unk=""<unk>"",\n        bos=""<s>"",\n        extra_special_symbols=None,\n    ):\n        self.unk_word, self.pad_word, self.eos_word = unk, pad, eos\n        self.symbols = []\n        self.count = []\n        self.indices = {}\n        self.bos_index = self.add_symbol(bos)\n        self.pad_index = self.add_symbol(pad)\n        self.eos_index = self.add_symbol(eos)\n        self.unk_index = self.add_symbol(unk)\n        if extra_special_symbols:\n            for s in extra_special_symbols:\n                self.add_symbol(s)\n        self.nspecial = len(self.symbols)\n\n    def __eq__(self, other):\n        return self.indices == other.indices\n\n    def __getitem__(self, idx):\n        if idx < len(self.symbols):\n            return self.symbols[idx]\n        return self.unk_word\n\n    def __len__(self):\n        """"""Returns the number of symbols in the dictionary""""""\n        return len(self.symbols)\n\n    def __contains__(self, sym):\n        return sym in self.indices\n\n    def index(self, sym):\n        """"""Returns the index of the specified symbol""""""\n        assert isinstance(sym, str)\n        if sym in self.indices:\n            return self.indices[sym]\n        return self.unk_index\n\n    def string(\n        self,\n        tensor,\n        bpe_symbol=None,\n        escape_unk=False,\n        extra_symbols_to_ignore=None,\n        unk_string=None,\n    ):\n        """"""Helper for converting a tensor of token indices to a string.\n\n        Can optionally remove BPE symbols or escape <unk> words.\n        """"""\n        if torch.is_tensor(tensor) and tensor.dim() == 2:\n            return ""\\n"".join(\n                self.string(t, bpe_symbol, escape_unk, extra_symbols_to_ignore)\n                for t in tensor\n            )\n\n        extra_symbols_to_ignore = set(extra_symbols_to_ignore or [])\n        extra_symbols_to_ignore.add(self.eos())\n\n        def token_string(i):\n            if i == self.unk():\n                if unk_string is not None:\n                    return unk_string\n                else:\n                    return self.unk_string(escape_unk)\n            else:\n                return self[i]\n\n        if hasattr(self, ""bos_index""):\n            extra_symbols_to_ignore.add(self.bos())\n\n        sent = "" "".join(\n            token_string(i)\n            for i in tensor\n            if utils.item(i) not in extra_symbols_to_ignore\n        )\n\n        return data_utils.process_bpe_symbol(sent, bpe_symbol)\n\n    def unk_string(self, escape=False):\n        """"""Return unknown string, optionally escaped as: <<unk>>""""""\n        if escape:\n            return ""<{}>"".format(self.unk_word)\n        else:\n            return self.unk_word\n\n    def add_symbol(self, word, n=1, overwrite=False):\n        """"""Adds a word to the dictionary""""""\n        if word in self.indices and not overwrite:\n            idx = self.indices[word]\n            self.count[idx] = self.count[idx] + n\n            return idx\n        else:\n            idx = len(self.symbols)\n            self.indices[word] = idx\n            self.symbols.append(word)\n            self.count.append(n)\n            return idx\n\n    def update(self, new_dict):\n        """"""Updates counts from new dictionary.""""""\n        for word in new_dict.symbols:\n            idx2 = new_dict.indices[word]\n            if word in self.indices:\n                idx = self.indices[word]\n                self.count[idx] = self.count[idx] + new_dict.count[idx2]\n            else:\n                idx = len(self.symbols)\n                self.indices[word] = idx\n                self.symbols.append(word)\n                self.count.append(new_dict.count[idx2])\n\n    def finalize(self, threshold=-1, nwords=-1, padding_factor=8):\n        """"""Sort symbols by frequency in descending order, ignoring special ones.\n\n        Args:\n            - threshold defines the minimum word count\n            - nwords defines the total number of words in the final dictionary,\n                including special symbols\n            - padding_factor can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        """"""\n        if nwords <= 0:\n            nwords = len(self)\n\n        new_indices = dict(zip(self.symbols[: self.nspecial], range(self.nspecial)))\n        new_symbols = self.symbols[: self.nspecial]\n        new_count = self.count[: self.nspecial]\n\n        c = Counter(\n            dict(\n                sorted(zip(self.symbols[self.nspecial :], self.count[self.nspecial :]))\n            )\n        )\n        for symbol, count in c.most_common(nwords - self.nspecial):\n            if count >= threshold:\n                new_indices[symbol] = len(new_symbols)\n                new_symbols.append(symbol)\n                new_count.append(count)\n            else:\n                break\n\n        assert len(new_symbols) == len(new_indices)\n\n        self.count = list(new_count)\n        self.symbols = list(new_symbols)\n        self.indices = new_indices\n\n        self.pad_to_multiple_(padding_factor)\n\n    def pad_to_multiple_(self, padding_factor):\n        """"""Pad Dictionary size to be a multiple of *padding_factor*.""""""\n        if padding_factor > 1:\n            i = 0\n            while len(self) % padding_factor != 0:\n                symbol = ""madeupword{:04d}"".format(i)\n                self.add_symbol(symbol, n=0)\n                i += 1\n\n    def bos(self):\n        """"""Helper to get index of beginning-of-sentence symbol""""""\n        return self.bos_index\n\n    def pad(self):\n        """"""Helper to get index of pad symbol""""""\n        return self.pad_index\n\n    def eos(self):\n        """"""Helper to get index of end-of-sentence symbol""""""\n        return self.eos_index\n\n    def unk(self):\n        """"""Helper to get index of unk symbol""""""\n        return self.unk_index\n\n    @classmethod\n    def load(cls, f):\n        """"""Loads the dictionary from a text file with the format:\n\n        ```\n        <symbol0> <count0>\n        <symbol1> <count1>\n        ...\n        ```\n        """"""\n        d = cls()\n        d.add_from_file(f)\n        return d\n\n    def add_from_file(self, f):\n        """"""\n        Loads a pre-existing dictionary from a text file and adds its symbols\n        to this instance.\n        """"""\n        if isinstance(f, str):\n            try:\n                with PathManager.open(f, ""r"", encoding=""utf-8"") as fd:\n                    self.add_from_file(fd)\n            except FileNotFoundError as fnfe:\n                raise fnfe\n            except UnicodeError:\n                raise Exception(\n                    ""Incorrect encoding detected in {}, please ""\n                    ""rebuild the dataset"".format(f)\n                )\n            return\n\n        lines = f.readlines()\n        indices_start_line = self._load_meta(lines)\n\n        for line in lines[indices_start_line:]:\n            try:\n                line, field = line.rstrip().rsplit("" "", 1)\n                if field == ""#fairseq:overwrite"":\n                    overwrite = True\n                    line, field = line.rsplit("" "", 1)\n                else:\n                    overwrite = False\n                count = int(field)\n                word = line\n                if word in self and not overwrite:\n                    raise RuntimeError(\n                        ""Duplicate word found when loading Dictionary: \'{}\'. ""\n                        ""Duplicate words can overwrite earlier ones by adding the ""\n                        ""#fairseq:overwrite flag at the end of the corresponding row ""\n                        ""in the dictionary file. If using the Camembert model, please ""\n                        ""download an updated copy of the model file.""\n                        .format(word)\n                    )\n                self.add_symbol(word, n=count, overwrite=overwrite)\n            except ValueError:\n                raise ValueError(\n                    ""Incorrect dictionary format, expected \'<token> <cnt> [flags]\'""\n                )\n\n    def _save(self, f, kv_iterator):\n        if isinstance(f, str):\n            PathManager.mkdirs(os.path.dirname(f))\n            with PathManager.open(f, ""w"", encoding=""utf-8"") as fd:\n                return self.save(fd)\n        for k, v in kv_iterator:\n            print(""{} {}"".format(k, v), file=f)\n\n    def _get_meta(self):\n        return [], []\n\n    def _load_meta(self, lines):\n        return 0\n\n    def save(self, f):\n        """"""Stores dictionary into a text file""""""\n        ex_keys, ex_vals = self._get_meta()\n        self._save(\n            f,\n            zip(\n                ex_keys + self.symbols[self.nspecial :],\n                ex_vals + self.count[self.nspecial :],\n            ),\n        )\n\n    def dummy_sentence(self, length):\n        t = torch.Tensor(length).uniform_(self.nspecial + 1, len(self)).long()\n        t[-1] = self.eos()\n        return t\n\n    def encode_line(\n        self,\n        line,\n        line_tokenizer=tokenize_line,\n        add_if_not_exist=True,\n        consumer=None,\n        append_eos=True,\n        reverse_order=False,\n    ):\n        words = line_tokenizer(line)\n        if reverse_order:\n            words = list(reversed(words))\n        nwords = len(words)\n        ids = torch.IntTensor(nwords + 1 if append_eos else nwords)\n\n        for i, word in enumerate(words):\n            if add_if_not_exist:\n                idx = self.add_symbol(word)\n            else:\n                idx = self.index(word)\n            if consumer is not None:\n                consumer(word, idx)\n            ids[i] = idx\n        if append_eos:\n            ids[nwords] = self.eos_index\n        return ids\n\n    @staticmethod\n    def _add_file_to_dictionary_single_worker(\n        filename, tokenize, eos_word, worker_id=0, num_workers=1\n    ):\n        counter = Counter()\n        with open(PathManager.get_local_path(filename), ""r"", encoding=""utf-8"") as f:\n            size = os.fstat(f.fileno()).st_size\n            chunk_size = size // num_workers\n            offset = worker_id * chunk_size\n            end = offset + chunk_size\n            f.seek(offset)\n            if offset > 0:\n                safe_readline(f)  # drop first incomplete line\n            line = f.readline()\n            while line:\n                for word in tokenize(line):\n                    counter.update([word])\n                counter.update([eos_word])\n                if f.tell() > end:\n                    break\n                line = f.readline()\n        return counter\n\n    @staticmethod\n    def add_file_to_dictionary(filename, dict, tokenize, num_workers):\n        def merge_result(counter):\n            for w, c in sorted(counter.items()):\n                dict.add_symbol(w, c)\n\n        if num_workers > 1:\n            pool = Pool(processes=num_workers)\n            results = []\n            for worker_id in range(num_workers):\n                results.append(\n                    pool.apply_async(\n                        Dictionary._add_file_to_dictionary_single_worker,\n                        (filename, tokenize, dict.eos_word, worker_id, num_workers),\n                    )\n                )\n            pool.close()\n            pool.join()\n            for r in results:\n                merge_result(r.get())\n        else:\n            merge_result(\n                Dictionary._add_file_to_dictionary_single_worker(\n                    filename, tokenize, dict.eos_word\n                )\n            )\n\n\nclass TruncatedDictionary(object):\n    def __init__(self, wrapped_dict, length):\n        self.__class__ = type(\n            wrapped_dict.__class__.__name__,\n            (self.__class__, wrapped_dict.__class__),\n            {},\n        )\n        self.__dict__ = wrapped_dict.__dict__\n        self.wrapped_dict = wrapped_dict\n        self.length = min(len(self.wrapped_dict), length)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        if i < self.length:\n            return self.wrapped_dict[i]\n        return self.wrapped_dict.unk()\n'"
fairseq/data/fairseq_dataset.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch.utils.data\n\n\nclass EpochListening:\n    """"""Mixin for receiving updates whenever the epoch increments.""""""\n    def set_epoch(self, epoch):\n        """"""Will receive the updated epoch number at the beginning of the epoch.\n        """"""\n        pass\n\n\nclass FairseqDataset(torch.utils.data.Dataset, EpochListening):\n    """"""A dataset that provides helpers for batching.""""""\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def collater(self, samples):\n        """"""Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        """"""\n        raise NotImplementedError\n\n    def num_tokens(self, index):\n        """"""Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.""""""\n        raise NotImplementedError\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.""""""\n        raise NotImplementedError\n\n    def ordered_indices(self):\n        """"""Return an ordered list of indices. Batches will be constructed based\n        on this order.""""""\n        return np.arange(len(self))\n\n    @property\n    def supports_prefetch(self):\n        """"""Whether this dataset supports prefetching.""""""\n        return False\n\n    def attr(self, attr: str, index: int):\n        return getattr(self, attr, None)\n\n    def prefetch(self, indices):\n        """"""Prefetch the data required for this epoch.""""""\n        raise NotImplementedError\n\n\nclass FairseqIterableDataset(torch.utils.data.IterableDataset, EpochListening):\n    """"""For datasets that need to be read sequentially, usually because the data\n    is being streamed or otherwise can\'t be manipulated on a single machine.\n    """"""\n\n    def __iter__(self):\n        raise NotImplementedError\n'"
fairseq/data/id_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom . import FairseqDataset\n\n\nclass IdDataset(FairseqDataset):\n\n    def __getitem__(self, index):\n        return index\n\n    def __len__(self):\n        return 0\n\n    def collater(self, samples):\n        return torch.tensor(samples)\n'"
fairseq/data/indexed_dataset.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom functools import lru_cache\nimport os\nimport shutil\nimport struct\n\nimport numpy as np\nimport torch\n\nfrom . import FairseqDataset\n\n\ndef __best_fitting_dtype(vocab_size=None):\n    if vocab_size is not None and vocab_size < 65500:\n        return np.uint16\n    else:\n        return np.int32\n\n\ndef get_available_dataset_impl():\n    return [\'raw\', \'lazy\', \'cached\', \'mmap\']\n\n\ndef infer_dataset_impl(path):\n    if IndexedRawTextDataset.exists(path):\n        return \'raw\'\n    elif IndexedDataset.exists(path):\n        with open(index_file_path(path), \'rb\') as f:\n            magic = f.read(8)\n            if magic == IndexedDataset._HDR_MAGIC:\n                return \'cached\'\n            elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:\n                return \'mmap\'\n            else:\n                return None\n    else:\n        return None\n\n\ndef make_builder(out_file, impl, vocab_size=None):\n    if impl == \'mmap\':\n        return MMapIndexedDatasetBuilder(out_file, dtype=__best_fitting_dtype(vocab_size))\n    else:\n        return IndexedDatasetBuilder(out_file)\n\n\ndef make_dataset(path, impl, fix_lua_indexing=False, dictionary=None):\n    if impl == \'raw\' and IndexedRawTextDataset.exists(path):\n        assert dictionary is not None\n        return IndexedRawTextDataset(path, dictionary)\n    elif impl == \'lazy\' and IndexedDataset.exists(path):\n        return IndexedDataset(path, fix_lua_indexing=fix_lua_indexing)\n    elif impl == \'cached\' and IndexedDataset.exists(path):\n        return IndexedCachedDataset(path, fix_lua_indexing=fix_lua_indexing)\n    elif impl == \'mmap\' and MMapIndexedDataset.exists(path):\n        return MMapIndexedDataset(path)\n    return None\n\n\ndef dataset_exists(path, impl):\n    if impl == \'raw\':\n        return IndexedRawTextDataset.exists(path)\n    elif impl == \'mmap\':\n        return MMapIndexedDataset.exists(path)\n    else:\n        return IndexedDataset.exists(path)\n\n\ndef read_longs(f, n):\n    a = np.empty(n, dtype=np.int64)\n    f.readinto(a)\n    return a\n\n\ndef write_longs(f, a):\n    f.write(np.array(a, dtype=np.int64))\n\n\ndtypes = {\n    1: np.uint8,\n    2: np.int8,\n    3: np.int16,\n    4: np.int32,\n    5: np.int64,\n    6: np.float,\n    7: np.double,\n    8: np.uint16\n}\n\n\ndef code(dtype):\n    for k in dtypes.keys():\n        if dtypes[k] == dtype:\n            return k\n    raise ValueError(dtype)\n\n\ndef index_file_path(prefix_path):\n    return prefix_path + \'.idx\'\n\n\ndef data_file_path(prefix_path):\n    return prefix_path + \'.bin\'\n\n\nclass IndexedDataset(FairseqDataset):\n    """"""Loader for TorchNet IndexedDataset""""""\n    _HDR_MAGIC = b\'TNTIDX\\x00\\x00\'\n\n    def __init__(self, path, fix_lua_indexing=False):\n        super().__init__()\n        self.path = path\n        self.fix_lua_indexing = fix_lua_indexing\n        self.data_file = None\n        self.read_index(path)\n\n    def read_index(self, path):\n        with open(index_file_path(path), \'rb\') as f:\n            magic = f.read(8)\n            assert magic == self._HDR_MAGIC, (\n                \'Index file doesn\\\'t match expected format. \'\n                \'Make sure that --dataset-impl is configured properly.\'\n            )\n            version = f.read(8)\n            assert struct.unpack(\'<Q\', version) == (1,)\n            code, self.element_size = struct.unpack(\'<QQ\', f.read(16))\n            self.dtype = dtypes[code]\n            self._len, self.s = struct.unpack(\'<QQ\', f.read(16))\n            self.dim_offsets = read_longs(f, self._len + 1)\n            self.data_offsets = read_longs(f, self._len + 1)\n            self.sizes = read_longs(f, self.s)\n\n    def read_data(self, path):\n        self.data_file = open(data_file_path(path), \'rb\', buffering=0)\n\n    def check_index(self, i):\n        if i < 0 or i >= self._len:\n            raise IndexError(\'index out of range\')\n\n    def __del__(self):\n        if self.data_file:\n            self.data_file.close()\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, i):\n        if not self.data_file:\n            self.read_data(self.path)\n        self.check_index(i)\n        tensor_size = self.sizes[self.dim_offsets[i]:self.dim_offsets[i + 1]]\n        a = np.empty(tensor_size, dtype=self.dtype)\n        self.data_file.seek(self.data_offsets[i] * self.element_size)\n        self.data_file.readinto(a)\n        item = torch.from_numpy(a).long()\n        if self.fix_lua_indexing:\n            item -= 1  # subtract 1 for 0-based indexing\n        return item\n\n    def __len__(self):\n        return self._len\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n\n    @staticmethod\n    def exists(path):\n        return (\n            os.path.exists(index_file_path(path)) and os.path.exists(data_file_path(path))\n        )\n\n    @property\n    def supports_prefetch(self):\n        return False  # avoid prefetching to save memory\n\n\nclass IndexedCachedDataset(IndexedDataset):\n\n    def __init__(self, path, fix_lua_indexing=False):\n        super().__init__(path, fix_lua_indexing=fix_lua_indexing)\n        self.cache = None\n        self.cache_index = {}\n\n    @property\n    def supports_prefetch(self):\n        return True\n\n    def prefetch(self, indices):\n        if all(i in self.cache_index for i in indices):\n            return\n        if not self.data_file:\n            self.read_data(self.path)\n        indices = sorted(set(indices))\n        total_size = 0\n        for i in indices:\n            total_size += self.data_offsets[i + 1] - self.data_offsets[i]\n        self.cache = np.empty(total_size, dtype=self.dtype)\n        ptx = 0\n        self.cache_index.clear()\n        for i in indices:\n            self.cache_index[i] = ptx\n            size = self.data_offsets[i + 1] - self.data_offsets[i]\n            a = self.cache[ptx: ptx + size]\n            self.data_file.seek(self.data_offsets[i] * self.element_size)\n            self.data_file.readinto(a)\n            ptx += size\n        if self.data_file:\n            # close and delete data file after prefetch so we can pickle\n            self.data_file.close()\n            self.data_file = None\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, i):\n        self.check_index(i)\n        tensor_size = self.sizes[self.dim_offsets[i]:self.dim_offsets[i + 1]]\n        a = np.empty(tensor_size, dtype=self.dtype)\n        ptx = self.cache_index[i]\n        np.copyto(a, self.cache[ptx: ptx + a.size])\n        item = torch.from_numpy(a).long()\n        if self.fix_lua_indexing:\n            item -= 1  # subtract 1 for 0-based indexing\n        return item\n\n\nclass IndexedRawTextDataset(FairseqDataset):\n    """"""Takes a text file as input and binarizes it in memory at instantiation.\n    Original lines are also kept in memory""""""\n\n    def __init__(self, path, dictionary, append_eos=True, reverse_order=False):\n        self.tokens_list = []\n        self.lines = []\n        self.sizes = []\n        self.append_eos = append_eos\n        self.reverse_order = reverse_order\n        self.read_data(path, dictionary)\n        self.size = len(self.tokens_list)\n\n    def read_data(self, path, dictionary):\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for line in f:\n                self.lines.append(line.strip(\'\\n\'))\n                tokens = dictionary.encode_line(\n                    line, add_if_not_exist=False,\n                    append_eos=self.append_eos, reverse_order=self.reverse_order,\n                ).long()\n                self.tokens_list.append(tokens)\n                self.sizes.append(len(tokens))\n        self.sizes = np.array(self.sizes)\n\n    def check_index(self, i):\n        if i < 0 or i >= self.size:\n            raise IndexError(\'index out of range\')\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, i):\n        self.check_index(i)\n        return self.tokens_list[i]\n\n    def get_original_text(self, i):\n        self.check_index(i)\n        return self.lines[i]\n\n    def __del__(self):\n        pass\n\n    def __len__(self):\n        return self.size\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n\n    @staticmethod\n    def exists(path):\n        return os.path.exists(path)\n\n\nclass IndexedDatasetBuilder(object):\n    element_sizes = {\n        np.uint8: 1,\n        np.int8: 1,\n        np.int16: 2,\n        np.int32: 4,\n        np.int64: 8,\n        np.float: 4,\n        np.double: 8\n    }\n\n    def __init__(self, out_file, dtype=np.int32):\n        self.out_file = open(out_file, \'wb\')\n        self.dtype = dtype\n        self.data_offsets = [0]\n        self.dim_offsets = [0]\n        self.sizes = []\n        self.element_size = self.element_sizes[self.dtype]\n\n    def add_item(self, tensor):\n        # +1 for Lua compatibility\n        bytes = self.out_file.write(np.array(tensor.numpy() + 1, dtype=self.dtype))\n        self.data_offsets.append(self.data_offsets[-1] + bytes / self.element_size)\n        for s in tensor.size():\n            self.sizes.append(s)\n        self.dim_offsets.append(self.dim_offsets[-1] + len(tensor.size()))\n\n    def merge_file_(self, another_file):\n        index = IndexedDataset(another_file)\n        assert index.dtype == self.dtype\n\n        begin = self.data_offsets[-1]\n        for offset in index.data_offsets[1:]:\n            self.data_offsets.append(begin + offset)\n        self.sizes.extend(index.sizes)\n        begin = self.dim_offsets[-1]\n        for dim_offset in index.dim_offsets[1:]:\n            self.dim_offsets.append(begin + dim_offset)\n\n        with open(data_file_path(another_file), \'rb\') as f:\n            while True:\n                data = f.read(1024)\n                if data:\n                    self.out_file.write(data)\n                else:\n                    break\n\n    def finalize(self, index_file):\n        self.out_file.close()\n        index = open(index_file, \'wb\')\n        index.write(b\'TNTIDX\\x00\\x00\')\n        index.write(struct.pack(\'<Q\', 1))\n        index.write(struct.pack(\'<QQ\', code(self.dtype), self.element_size))\n        index.write(struct.pack(\'<QQ\', len(self.data_offsets) - 1, len(self.sizes)))\n        write_longs(index, self.dim_offsets)\n        write_longs(index, self.data_offsets)\n        write_longs(index, self.sizes)\n        index.close()\n\n\ndef _warmup_mmap_file(path):\n    with open(path, \'rb\') as stream:\n        while stream.read(100 * 1024 * 1024):\n            pass\n\n\nclass MMapIndexedDataset(torch.utils.data.Dataset):\n    class Index(object):\n        _HDR_MAGIC = b\'MMIDIDX\\x00\\x00\'\n\n        @classmethod\n        def writer(cls, path, dtype):\n            class _Writer(object):\n                def __enter__(self):\n                    self._file = open(path, \'wb\')\n\n                    self._file.write(cls._HDR_MAGIC)\n                    self._file.write(struct.pack(\'<Q\', 1))\n                    self._file.write(struct.pack(\'<B\', code(dtype)))\n\n                    return self\n\n                @staticmethod\n                def _get_pointers(sizes):\n                    dtype_size = dtype().itemsize\n                    address = 0\n                    pointers = []\n\n                    for size in sizes:\n                        pointers.append(address)\n                        address += size * dtype_size\n\n                    return pointers\n\n                def write(self, sizes):\n                    pointers = self._get_pointers(sizes)\n\n                    self._file.write(struct.pack(\'<Q\', len(sizes)))\n\n                    sizes = np.array(sizes, dtype=np.int32)\n                    self._file.write(sizes.tobytes(order=\'C\'))\n                    del sizes\n\n                    pointers = np.array(pointers, dtype=np.int64)\n                    self._file.write(pointers.tobytes(order=\'C\'))\n                    del pointers\n\n                def __exit__(self, exc_type, exc_val, exc_tb):\n                    self._file.close()\n\n            return _Writer()\n\n        def __init__(self, path):\n            with open(path, \'rb\') as stream:\n                magic_test = stream.read(9)\n                assert self._HDR_MAGIC == magic_test, (\n                    \'Index file doesn\\\'t match expected format. \'\n                    \'Make sure that --dataset-impl is configured properly.\'\n                )\n                version = struct.unpack(\'<Q\', stream.read(8))\n                assert (1,) == version\n\n                dtype_code, = struct.unpack(\'<B\', stream.read(1))\n                self._dtype = dtypes[dtype_code]\n                self._dtype_size = self._dtype().itemsize\n\n                self._len = struct.unpack(\'<Q\', stream.read(8))[0]\n                offset = stream.tell()\n\n            _warmup_mmap_file(path)\n\n            self._bin_buffer_mmap = np.memmap(path, mode=\'r\', order=\'C\')\n            self._bin_buffer = memoryview(self._bin_buffer_mmap)\n            self._sizes = np.frombuffer(self._bin_buffer, dtype=np.int32, count=self._len, offset=offset)\n            self._pointers = np.frombuffer(self._bin_buffer, dtype=np.int64, count=self._len,\n                                           offset=offset + self._sizes.nbytes)\n\n        def __del__(self):\n            self._bin_buffer_mmap._mmap.close()\n            del self._bin_buffer_mmap\n\n        @property\n        def dtype(self):\n            return self._dtype\n\n        @property\n        def sizes(self):\n            return self._sizes\n\n        @lru_cache(maxsize=8)\n        def __getitem__(self, i):\n            return self._pointers[i], self._sizes[i]\n\n        def __len__(self):\n            return self._len\n\n    def __init__(self, path):\n        super().__init__()\n\n        self._path = None\n        self._index = None\n        self._bin_buffer = None\n\n        self._do_init(path)\n\n    def __getstate__(self):\n        return self._path\n\n    def __setstate__(self, state):\n        self._do_init(state)\n\n    def _do_init(self, path):\n        self._path = path\n        self._index = self.Index(index_file_path(self._path))\n\n        _warmup_mmap_file(data_file_path(self._path))\n        self._bin_buffer_mmap = np.memmap(data_file_path(self._path), mode=\'r\', order=\'C\')\n        self._bin_buffer = memoryview(self._bin_buffer_mmap)\n\n    def __del__(self):\n        self._bin_buffer_mmap._mmap.close()\n        del self._bin_buffer_mmap\n        del self._index\n\n    def __len__(self):\n        return len(self._index)\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, i):\n        ptr, size = self._index[i]\n        np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype, count=size, offset=ptr)\n        if self._index.dtype != np.int64:\n            np_array = np_array.astype(np.int64)\n\n        return torch.from_numpy(np_array)\n\n    @property\n    def sizes(self):\n        return self._index.sizes\n\n    @property\n    def supports_prefetch(self):\n        return False\n\n    @staticmethod\n    def exists(path):\n        return (\n            os.path.exists(index_file_path(path)) and os.path.exists(data_file_path(path))\n        )\n\n\nclass MMapIndexedDatasetBuilder(object):\n    def __init__(self, out_file, dtype=np.int64):\n        self._data_file = open(out_file, \'wb\')\n        self._dtype = dtype\n        self._sizes = []\n\n    def add_item(self, tensor):\n        np_array = np.array(tensor.numpy(), dtype=self._dtype)\n        self._data_file.write(np_array.tobytes(order=\'C\'))\n        self._sizes.append(np_array.size)\n\n    def merge_file_(self, another_file):\n        # Concatenate index\n        index = MMapIndexedDataset.Index(index_file_path(another_file))\n        assert index.dtype == self._dtype\n\n        for size in index.sizes:\n            self._sizes.append(size)\n\n        # Concatenate data\n        with open(data_file_path(another_file), \'rb\') as f:\n            shutil.copyfileobj(f, self._data_file)\n\n    def finalize(self, index_file):\n        self._data_file.close()\n\n        with MMapIndexedDataset.Index.writer(index_file, self._dtype) as index:\n            index.write(self._sizes)\n'"
fairseq/data/iterators.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport itertools\nimport math\nimport operator\nimport os\nimport time\nimport numpy as np\nimport torch\nimport queue\nimport logging\nfrom threading import Thread\nfrom . import data_utils\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n# Object used by _background_consumer to signal the source is exhausted\n# to the main thread.\n_sentinel = object()\n\n\nclass CountingIterator(object):\n    """"""Wrapper around an iterable that maintains the iteration count.\n\n    Args:\n        iterable (iterable): iterable to wrap\n        start (int): starting iteration count. Note that this doesn\'t\n            actually advance the iterator.\n        total (int): override the iterator length returned by\n            ``__len__``. This can be used to truncate *iterator*.\n\n    Attributes:\n        n (int): number of elements consumed from this iterator\n    """"""\n\n    def __init__(self, iterable, start=None, total=None):\n        self.iterable = iterable\n        self.itr = iter(self)\n\n        if start is None:\n            self.n = getattr(iterable, \'n\', 0)\n        else:\n            self.n = start\n\n        if total is None:\n            self.total = self.n + len(iterable)\n        else:\n            self.total = total\n\n    def __len__(self):\n        return self.total\n\n    def __iter__(self):\n        for x in self.iterable:\n            if self.n >= self.total:\n                return\n            self.n += 1\n            yield x\n\n    def __next__(self):\n        return next(self.itr)\n\n    def has_next(self):\n        """"""Whether the iterator has been exhausted.""""""\n        return self.n < len(self)\n\n    def skip(self, num_to_skip):\n        """"""Fast-forward the iterator by skipping *num_to_skip* elements.""""""\n        next(itertools.islice(self.itr, num_to_skip, num_to_skip), None)\n        return self\n\n    def take(self, n):\n        """"""\n        Truncates the iterator to n elements at most.\n        """"""\n        self.total = min(self.total, n)\n\n\nclass EpochBatchIterating(object):\n    def __len__(self) -> int:\n        raise NotImplementedError\n\n    @property\n    def next_epoch_idx(self):\n        raise NotImplementedError\n\n    def next_epoch_itr(self, shuffle=True, fix_batches_to_gpus=False):\n        """"""Return a new iterator over the dataset.\n\n        Args:\n            shuffle (bool, optional): shuffle batches before returning the\n                iterator (default: True).\n            fix_batches_to_gpus: ensure that batches are always\n                allocated to the same shards across epochs. Requires\n                that :attr:`dataset` supports prefetching (default: False).\n        """"""\n        raise NotImplementedError\n\n    def end_of_epoch(self) -> bool:\n        """"""Returns whether the most recent epoch iterator has been exhausted""""""\n        raise NotImplementedError\n\n    @property\n    def iterations_in_epoch(self) -> int:\n        """"""The number of consumed batches in the current epoch.""""""\n        raise NotImplementedError\n\n    def state_dict(self):\n        """"""Returns a dictionary containing a whole state of the iterator.""""""\n        raise NotImplementedError\n\n    def load_state_dict(self, state_dict):\n        """"""Copies the state of the iterator from the given *state_dict*.""""""\n        raise NotImplementedError\n\n\nclass StreamingEpochBatchIterator(EpochBatchIterating):\n    def __init__(\n        self, dataset, epoch=1, num_shards=1, shard_id=0,\n    ):\n        assert isinstance(dataset, torch.utils.data.IterableDataset)\n        self.dataset = dataset\n        self.epoch = max(epoch, 1)  # we use 1-based indexing for epochs\n        self._current_epoch_iterator = None\n        self.num_shards = num_shards\n        self.shard_id = shard_id\n\n    @property\n    def next_epoch_idx(self):\n        """"""Return the epoch index after *next_epoch_itr* is called.""""""\n        if self._current_epoch_iterator is not None and self.end_of_epoch():\n            return self.epoch + 1\n        else:\n            return self.epoch\n\n    def next_epoch_itr(self, shuffle=True, fix_batches_to_gpus=False):\n        self.epoch = self.next_epoch_idx\n        self.dataset.set_epoch(self.epoch)\n        self._current_epoch_iterator = CountingIterator(\n            iterable=ShardedIterator(\n                iterable=self.dataset,\n                num_shards=self.num_shards,\n                shard_id=self.shard_id,\n            ),\n        )\n        return self._current_epoch_iterator\n\n    def end_of_epoch(self) -> bool:\n        return not self._current_epoch_iterator.has_next()\n\n    @property\n    def iterations_in_epoch(self) -> int:\n        if self._current_epoch_iterator is not None:\n            return self._current_epoch_iterator.n\n        return 0\n\n    def state_dict(self):\n        return {\n            \'epoch\': self.epoch,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.epoch = state_dict[\'epoch\']\n\n\nclass EpochBatchIterator(EpochBatchIterating):\n    """"""A multi-epoch iterator over a :class:`torch.utils.data.Dataset`.\n\n    Compared to :class:`torch.utils.data.DataLoader`, this iterator:\n\n    - can be reused across multiple epochs with the :func:`next_epoch_itr`\n      method (optionally shuffled between epochs)\n    - can be serialized/deserialized with the :func:`state_dict` and\n      :func:`load_state_dict` methods\n    - supports sharding with the *num_shards* and *shard_id* arguments\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset from which to load the data\n        collate_fn (callable): merges a list of samples to form a mini-batch\n        batch_sampler (~torch.utils.data.Sampler): an iterator over batches of\n            indices\n        seed (int, optional): seed for random number generator for\n            reproducibility (default: 1).\n        num_shards (int, optional): shard the data iterator into N\n            shards (default: 1).\n        shard_id (int, optional): which shard of the data iterator to\n            return (default: 0).\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means the data will be loaded in the main process\n            (default: 0).\n        epoch (int, optional): the epoch to start the iterator from\n            (default: 1).\n        buffer_size (int, optional): the number of batches to keep ready in the\n            queue. Helps speeding up dataloading. When buffer_size is zero, the\n            default torch.utils.data.DataLoader preloading is used.\n    """"""\n\n    def __init__(\n        self, dataset, collate_fn, batch_sampler, seed=1, num_shards=1, shard_id=0,\n        num_workers=0, epoch=1, buffer_size=0\n    ):\n        assert isinstance(dataset, torch.utils.data.Dataset)\n        self.dataset = dataset\n        self.collate_fn = collate_fn\n        self.frozen_batches = tuple(batch_sampler)\n        self.seed = seed\n        self.num_shards = num_shards\n        self.shard_id = shard_id\n        self.num_workers = num_workers\n        # This upper limit here is to prevent people from abusing this feature\n        # in a shared computing environment.\n        self.buffer_size = min(buffer_size, 5)\n\n        self.epoch = max(epoch, 1)  # we use 1-based indexing for epochs\n        self.shuffle = True\n        self._cur_epoch_itr = None\n        self._next_epoch_itr = None\n        self._supports_prefetch = getattr(dataset, \'supports_prefetch\', False)\n\n    def __len__(self):\n        return int(math.ceil(len(self.frozen_batches) / float(self.num_shards)))\n\n    @property\n    def n(self):\n        return self.iterations_in_epoch\n\n    @property\n    def next_epoch_idx(self):\n        """"""Return the epoch index after *next_epoch_itr* is called.""""""\n        if self._next_epoch_itr is not None:\n            return self.epoch\n        elif self._cur_epoch_itr is not None and self.end_of_epoch():\n            return self.epoch + 1\n        else:\n            return self.epoch\n\n    def next_epoch_itr(self, shuffle=True, fix_batches_to_gpus=False):\n        """"""Return a new iterator over the dataset.\n\n        Args:\n            shuffle (bool, optional): shuffle batches before returning the\n                iterator (default: True).\n            fix_batches_to_gpus: ensure that batches are always\n                allocated to the same shards across epochs. Requires\n                that :attr:`dataset` supports prefetching (default: False).\n        """"""\n        self.epoch = self.next_epoch_idx\n        if self._next_epoch_itr is not None:\n            self._cur_epoch_itr = self._next_epoch_itr\n            self._next_epoch_itr = None\n        else:\n            self._cur_epoch_itr = self._get_iterator_for_epoch(\n                self.epoch, shuffle, fix_batches_to_gpus=fix_batches_to_gpus,\n            )\n        self.dataset.set_epoch(self.epoch)\n        self.shuffle = shuffle\n        return self._cur_epoch_itr\n\n    def end_of_epoch(self) -> bool:\n        """"""Returns whether the most recent epoch iterator has been exhausted""""""\n        return not self._cur_epoch_itr.has_next()\n\n    @property\n    def iterations_in_epoch(self):\n        """"""The number of consumed batches in the current epoch.""""""\n        if self._cur_epoch_itr is not None:\n            return self._cur_epoch_itr.n\n        elif self._next_epoch_itr is not None:\n            return self._next_epoch_itr.n\n        return 0\n\n    def state_dict(self):\n        """"""Returns a dictionary containing a whole state of the iterator.""""""\n        return {\n            \'epoch\': self.epoch,\n            \'iterations_in_epoch\': self.iterations_in_epoch,\n            \'shuffle\': self.shuffle,\n        }\n\n    def load_state_dict(self, state_dict):\n        """"""Copies the state of the iterator from the given *state_dict*.""""""\n        self.epoch = state_dict[\'epoch\']\n        itr_pos = state_dict.get(\'iterations_in_epoch\', 0)\n        if itr_pos > 0:\n            # fast-forward epoch iterator\n            self._next_epoch_itr = self._get_iterator_for_epoch(\n                self.epoch,\n                shuffle=state_dict.get(\'shuffle\', True),\n                offset=itr_pos,\n            )\n            if self._next_epoch_itr is None:\n                # we finished the epoch, increment epoch counter\n                self.epoch += 1\n        else:\n            self._next_epoch_itr = None\n\n    def _get_iterator_for_epoch(self, epoch, shuffle, fix_batches_to_gpus=False, offset=0):\n\n        def shuffle_batches(batches, seed):\n            with data_utils.numpy_seed(seed):\n                np.random.shuffle(batches)\n            return batches\n\n        if self._supports_prefetch:\n            batches = self.frozen_batches\n\n            if shuffle and not fix_batches_to_gpus:\n                batches = shuffle_batches(list(batches), self.seed + epoch)\n\n            batches = list(ShardedIterator(\n                batches, self.num_shards, self.shard_id, fill_value=[]\n            ))\n            self.dataset.prefetch([i for s in batches for i in s])\n\n            if shuffle and fix_batches_to_gpus:\n                batches = shuffle_batches(batches, self.seed + epoch + self.shard_id)\n        else:\n            if shuffle:\n                batches = shuffle_batches(list(self.frozen_batches), self.seed + epoch)\n            else:\n                batches = self.frozen_batches\n            batches = list(ShardedIterator(\n                batches, self.num_shards, self.shard_id, fill_value=[]\n            ))\n\n        if offset > 0 and offset >= len(batches):\n            return None\n\n        if self.num_workers > 0:\n            os.environ[\'PYTHONWARNINGS\'] = \'ignore:semaphore_tracker:UserWarning\'\n\n        # Create data loader\n        itr = torch.utils.data.DataLoader(\n            self.dataset,\n            collate_fn=self.collate_fn,\n            batch_sampler=batches[offset:],\n            num_workers=self.num_workers,\n        )\n\n        # Wrap with a BufferedIterator if needed\n        if self.buffer_size > 0:\n            itr = BufferedIterator(self.buffer_size, itr)\n\n        # Wrap with CoutingIterator\n        itr = CountingIterator(itr, start=offset)\n        return itr\n\n\nclass GroupedIterator(CountingIterator):\n    """"""Wrapper around an iterable that returns groups (chunks) of items.\n\n    Args:\n        iterable (iterable): iterable to wrap\n        chunk_size (int): size of each chunk\n\n    Attributes:\n        n (int): number of elements consumed from this iterator\n    """"""\n\n    def __init__(self, iterable, chunk_size):\n        itr = _chunk_iterator(iterable, chunk_size)\n        super().__init__(\n            itr,\n            start=int(math.ceil(getattr(iterable, \'n\', 0) / float(chunk_size))),\n            total=int(math.ceil(len(iterable) / float(chunk_size))),\n        )\n        self.chunk_size = chunk_size\n\n\ndef _chunk_iterator(itr, chunk_size):\n    chunk = []\n    for x in itr:\n        chunk.append(x)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk\n\n\nclass ShardedIterator(CountingIterator):\n    """"""A sharded wrapper around an iterable, padded to length.\n\n    Args:\n        iterable (iterable): iterable to wrap\n        num_shards (int): number of shards to split the iterable into\n        shard_id (int): which shard to iterator over\n        fill_value (Any, optional): padding value when the iterable doesn\'t\n            evenly divide *num_shards* (default: None).\n\n    Attributes:\n        n (int): number of elements consumed from this iterator\n    """"""\n\n    def __init__(self, iterable, num_shards, shard_id, fill_value=None):\n        if shard_id < 0 or shard_id >= num_shards:\n            raise ValueError(\'shard_id must be between 0 and num_shards\')\n        sharded_len = int(math.ceil(len(iterable) / float(num_shards)))\n        itr = map(\n            operator.itemgetter(1),\n            itertools.zip_longest(\n                range(sharded_len),\n                itertools.islice(iterable, shard_id, len(iterable), num_shards),\n                fillvalue=fill_value,\n            ),\n        )\n        super().__init__(\n            itr,\n            start=int(math.ceil(getattr(iterable, \'n\', 0) / float(num_shards))),\n            total=sharded_len,\n        )\n\n\nclass BackgroundConsumer(Thread):\n    def __init__(self, queue, source):\n        Thread.__init__(self)\n\n        self._queue = queue\n        self._source = source\n\n    def run(self):\n        try:\n            for item in self._source:\n                self._queue.put(item)\n\n            # Signal the consumer we are done.\n            self._queue.put(_sentinel)\n        except Exception as e:\n            self._queue.put(e)\n\n\nclass BufferedIterator(object):\n    def __init__(self, size, iterable):\n        self._queue = queue.Queue(size)\n        self._iterable = iterable\n\n        self._consumer = BackgroundConsumer(self._queue, iterable)\n        self._consumer.daemon = True\n        self._consumer.start()\n\n        self.start_time = time.time()\n        self.warning_time = None\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        return len(self._iterable)\n\n    def __next__(self):\n        # Notify the user if there is a data loading bottleneck\n        if self._queue.qsize() < 2:\n            if time.time() - self.start_time > 5 * 60:\n                if self.warning_time is None or time.time() - self.warning_time > 15 * 60:\n                    logger.info(\n                        ""Data loading buffer is empty or nearly empty. This may ""\n                        ""indicate a data loading bottleneck, and increasing the ""\n                        ""number of workers (--num-workers) may help.""\n                    )\n                    self.warning_time = time.time()\n\n        # Get next example\n        item = self._queue.get(True)\n        if isinstance(item, Exception):\n            raise item\n        if item is _sentinel:\n            raise StopIteration()\n        return item\n'"
fairseq/data/language_pair_dataset.py,14,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport numpy as np\nimport torch\n\nfrom . import data_utils, FairseqDataset\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef collate(\n    samples, pad_idx, eos_idx, left_pad_source=True, left_pad_target=False,\n    input_feeding=True,\n):\n    if len(samples) == 0:\n        return {}\n\n    def merge(key, left_pad, move_eos_to_beginning=False):\n        return data_utils.collate_tokens(\n            [s[key] for s in samples],\n            pad_idx, eos_idx, left_pad, move_eos_to_beginning,\n        )\n\n    def check_alignment(alignment, src_len, tgt_len):\n        if alignment is None or len(alignment) == 0:\n            return False\n        if alignment[:, 0].max().item() >= src_len - 1 or alignment[:, 1].max().item() >= tgt_len - 1:\n            logger.warning(""alignment size mismatch found, skipping alignment!"")\n            return False\n        return True\n\n    def compute_alignment_weights(alignments):\n        """"""\n        Given a tensor of shape [:, 2] containing the source-target indices\n        corresponding to the alignments, a weight vector containing the\n        inverse frequency of each target index is computed.\n        For e.g. if alignments = [[5, 7], [2, 3], [1, 3], [4, 2]], then\n        a tensor containing [1., 0.5, 0.5, 1] should be returned (since target\n        index 3 is repeated twice)\n        """"""\n        align_tgt = alignments[:, 1]\n        _, align_tgt_i, align_tgt_c = torch.unique(align_tgt, return_inverse=True, return_counts=True)\n        align_weights = align_tgt_c[align_tgt_i[np.arange(len(align_tgt))]]\n        return 1. / align_weights.float()\n\n    id = torch.LongTensor([s[\'id\'] for s in samples])\n    src_tokens = merge(\'source\', left_pad=left_pad_source)\n    # sort by descending source length\n    src_lengths = torch.LongTensor([s[\'source\'].numel() for s in samples])\n    src_lengths, sort_order = src_lengths.sort(descending=True)\n    id = id.index_select(0, sort_order)\n    src_tokens = src_tokens.index_select(0, sort_order)\n\n    prev_output_tokens = None\n    target = None\n    if samples[0].get(\'target\', None) is not None:\n        target = merge(\'target\', left_pad=left_pad_target)\n        target = target.index_select(0, sort_order)\n        tgt_lengths = torch.LongTensor([s[\'target\'].numel() for s in samples]).index_select(0, sort_order)\n        ntokens = sum(len(s[\'target\']) for s in samples)\n\n        if input_feeding:\n            # we create a shifted version of targets for feeding the\n            # previous output token(s) into the next decoder step\n            prev_output_tokens = merge(\n                \'target\',\n                left_pad=left_pad_target,\n                move_eos_to_beginning=True,\n            )\n            prev_output_tokens = prev_output_tokens.index_select(0, sort_order)\n    else:\n        ntokens = sum(len(s[\'source\']) for s in samples)\n\n    batch = {\n        \'id\': id,\n        \'nsentences\': len(samples),\n        \'ntokens\': ntokens,\n        \'net_input\': {\n            \'src_tokens\': src_tokens,\n            \'src_lengths\': src_lengths,\n        },\n        \'target\': target,\n    }\n    if prev_output_tokens is not None:\n        batch[\'net_input\'][\'prev_output_tokens\'] = prev_output_tokens\n\n    if samples[0].get(\'alignment\', None) is not None:\n        bsz, tgt_sz = batch[\'target\'].shape\n        src_sz = batch[\'net_input\'][\'src_tokens\'].shape[1]\n\n        offsets = torch.zeros((len(sort_order), 2), dtype=torch.long)\n        offsets[:, 1] += (torch.arange(len(sort_order), dtype=torch.long) * tgt_sz)\n        if left_pad_source:\n            offsets[:, 0] += (src_sz - src_lengths)\n        if left_pad_target:\n            offsets[:, 1] += (tgt_sz - tgt_lengths)\n\n        alignments = [\n            alignment + offset\n            for align_idx, offset, src_len, tgt_len in zip(sort_order, offsets, src_lengths, tgt_lengths)\n            for alignment in [samples[align_idx][\'alignment\'].view(-1, 2)]\n            if check_alignment(alignment, src_len, tgt_len)\n        ]\n\n        if len(alignments) > 0:\n            alignments = torch.cat(alignments, dim=0)\n            align_weights = compute_alignment_weights(alignments)\n\n            batch[\'alignments\'] = alignments\n            batch[\'align_weights\'] = align_weights\n\n    return batch\n\n\nclass LanguagePairDataset(FairseqDataset):\n    """"""\n    A pair of torch.utils.data.Datasets.\n\n    Args:\n        src (torch.utils.data.Dataset): source dataset to wrap\n        src_sizes (List[int]): source sentence lengths\n        src_dict (~fairseq.data.Dictionary): source vocabulary\n        tgt (torch.utils.data.Dataset, optional): target dataset to wrap\n        tgt_sizes (List[int], optional): target sentence lengths\n        tgt_dict (~fairseq.data.Dictionary, optional): target vocabulary\n        left_pad_source (bool, optional): pad source tensors on the left side\n            (default: True).\n        left_pad_target (bool, optional): pad target tensors on the left side\n            (default: False).\n        max_source_positions (int, optional): max number of tokens in the\n            source sentence (default: 1024).\n        max_target_positions (int, optional): max number of tokens in the\n            target sentence (default: 1024).\n        shuffle (bool, optional): shuffle dataset elements before batching\n            (default: True).\n        input_feeding (bool, optional): create a shifted version of the targets\n            to be passed into the model for teacher forcing (default: True).\n        remove_eos_from_source (bool, optional): if set, removes eos from end\n            of source if it\'s present (default: False).\n        append_eos_to_target (bool, optional): if set, appends eos to end of\n            target if it\'s absent (default: False).\n        align_dataset (torch.utils.data.Dataset, optional): dataset\n            containing alignments.\n        append_bos (bool, optional): if set, appends bos to the beginning of\n            source/target sentence.\n    """"""\n\n    def __init__(\n        self, src, src_sizes, src_dict,\n        tgt=None, tgt_sizes=None, tgt_dict=None,\n        left_pad_source=True, left_pad_target=False,\n        max_source_positions=1024, max_target_positions=1024,\n        shuffle=True, input_feeding=True,\n        remove_eos_from_source=False, append_eos_to_target=False,\n        align_dataset=None,\n        append_bos=False, eos=None\n    ):\n        if tgt_dict is not None:\n            assert src_dict.pad() == tgt_dict.pad()\n            assert src_dict.eos() == tgt_dict.eos()\n            assert src_dict.unk() == tgt_dict.unk()\n        if tgt is not None:\n            assert len(src) == len(tgt), ""Source and target must contain the same number of examples""\n        self.src = src\n        self.tgt = tgt\n        self.src_sizes = np.array(src_sizes)\n        self.tgt_sizes = np.array(tgt_sizes) if tgt_sizes is not None else None\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n        self.left_pad_source = left_pad_source\n        self.left_pad_target = left_pad_target\n        self.max_source_positions = max_source_positions\n        self.max_target_positions = max_target_positions\n        self.shuffle = shuffle\n        self.input_feeding = input_feeding\n        self.remove_eos_from_source = remove_eos_from_source\n        self.append_eos_to_target = append_eos_to_target\n        self.align_dataset = align_dataset\n        if self.align_dataset is not None:\n            assert self.tgt_sizes is not None, ""Both source and target needed when alignments are provided""\n        self.append_bos = append_bos\n        self.eos = (eos if eos is not None else src_dict.eos())\n\n    def __getitem__(self, index):\n        tgt_item = self.tgt[index] if self.tgt is not None else None\n        src_item = self.src[index]\n        # Append EOS to end of tgt sentence if it does not have an EOS and remove\n        # EOS from end of src sentence if it exists. This is useful when we use\n        # use existing datasets for opposite directions i.e., when we want to\n        # use tgt_dataset as src_dataset and vice versa\n        if self.append_eos_to_target:\n            eos = self.tgt_dict.eos() if self.tgt_dict else self.src_dict.eos()\n            if self.tgt and self.tgt[index][-1] != eos:\n                tgt_item = torch.cat([self.tgt[index], torch.LongTensor([eos])])\n\n        if self.append_bos:\n            bos = self.tgt_dict.bos() if self.tgt_dict else self.src_dict.bos()\n            if self.tgt and self.tgt[index][0] != bos:\n                tgt_item = torch.cat([torch.LongTensor([bos]), self.tgt[index]])\n\n            bos = self.src_dict.bos()\n            if self.src[index][-1] != bos:\n                src_item = torch.cat([torch.LongTensor([bos]), self.src[index]])\n\n        if self.remove_eos_from_source:\n            eos = self.src_dict.eos()\n            if self.src[index][-1] == eos:\n                src_item = self.src[index][:-1]\n\n        example = {\n            \'id\': index,\n            \'source\': src_item,\n            \'target\': tgt_item,\n        }\n        if self.align_dataset is not None:\n            example[\'alignment\'] = self.align_dataset[index]\n        return example\n\n    def __len__(self):\n        return len(self.src)\n\n    def collater(self, samples):\n        """"""Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch with the following keys:\n\n                - `id` (LongTensor): example IDs in the original input order\n                - `ntokens` (int): total number of tokens in the batch\n                - `net_input` (dict): the input to the Model, containing keys:\n\n                  - `src_tokens` (LongTensor): a padded 2D Tensor of tokens in\n                    the source sentence of shape `(bsz, src_len)`. Padding will\n                    appear on the left if *left_pad_source* is ``True``.\n                  - `src_lengths` (LongTensor): 1D Tensor of the unpadded\n                    lengths of each source sentence of shape `(bsz)`\n                  - `prev_output_tokens` (LongTensor): a padded 2D Tensor of\n                    tokens in the target sentence, shifted right by one\n                    position for teacher forcing, of shape `(bsz, tgt_len)`.\n                    This key will not be present if *input_feeding* is\n                    ``False``.  Padding will appear on the left if\n                    *left_pad_target* is ``True``.\n\n                - `target` (LongTensor): a padded 2D Tensor of tokens in the\n                  target sentence of shape `(bsz, tgt_len)`. Padding will appear\n                  on the left if *left_pad_target* is ``True``.\n        """"""\n        return collate(\n            samples, pad_idx=self.src_dict.pad(), eos_idx=self.eos,\n            left_pad_source=self.left_pad_source, left_pad_target=self.left_pad_target,\n            input_feeding=self.input_feeding,\n        )\n\n    def num_tokens(self, index):\n        """"""Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.""""""\n        return max(self.src_sizes[index], self.tgt_sizes[index] if self.tgt_sizes is not None else 0)\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.""""""\n        return (self.src_sizes[index], self.tgt_sizes[index] if self.tgt_sizes is not None else 0)\n\n    def ordered_indices(self):\n        """"""Return an ordered list of indices. Batches will be constructed based\n        on this order.""""""\n        if self.shuffle:\n            indices = np.random.permutation(len(self))\n        else:\n            indices = np.arange(len(self))\n        if self.tgt_sizes is not None:\n            indices = indices[np.argsort(self.tgt_sizes[indices], kind=\'mergesort\')]\n        return indices[np.argsort(self.src_sizes[indices], kind=\'mergesort\')]\n\n    @property\n    def supports_prefetch(self):\n        return (\n            getattr(self.src, \'supports_prefetch\', False)\n            and (getattr(self.tgt, \'supports_prefetch\', False) or self.tgt is None)\n        )\n\n    def prefetch(self, indices):\n        self.src.prefetch(indices)\n        if self.tgt is not None:\n            self.tgt.prefetch(indices)\n        if self.align_dataset is not None:\n            self.align_dataset.prefetch(indices)\n'"
fairseq/data/list_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import BaseWrapperDataset\n\n\nclass ListDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, sizes=None):\n        super().__init__(dataset)\n        self._sizes = sizes\n\n    def __iter__(self):\n        for x in self.dataset:\n            yield x\n\n    def collater(self, samples):\n        return samples\n\n    @property\n    def sizes(self):\n        return self._sizes\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n\n    def set_epoch(self, epoch):\n        pass\n'"
fairseq/data/lm_context_window_dataset.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom fairseq.data.monolingual_dataset import MonolingualDataset\n\nfrom . import FairseqDataset\n\n\nclass LMContextWindowDataset(FairseqDataset):\n    """"""Wraps a MonolingualDataset and provides more context for evaluation.""""""\n\n    def __init__(self, dataset, tokens_per_sample, context_window, pad_idx):\n        assert isinstance(dataset, MonolingualDataset)\n        assert context_window > 0\n        self.dataset = dataset\n        self.tokens_per_sample = tokens_per_sample\n        self.context_window = context_window\n        self.pad_idx = pad_idx\n        self.prev_tokens = np.empty([0])\n\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n        sample = self.dataset.collater(samples)\n\n        pad = self.pad_idx\n        max_sample_len = self.tokens_per_sample + self.context_window\n\n        bsz, tsz = sample[\'net_input\'][\'src_tokens\'].shape\n        start_idxs = [0] * bsz\n        toks = sample[\'net_input\'][\'src_tokens\']\n        lengths = sample[\'net_input\'][\'src_lengths\']\n        tgt = sample[\'target\']\n        new_toks = np.empty([bsz, tsz + self.context_window], dtype=np.int64)\n        new_tgt = np.full([bsz, tsz + self.context_window], pad, dtype=np.int64)\n        sample_lens = toks.ne(pad).long().sum(dim=1).cpu()\n        for i in range(bsz):\n            sample_len = sample_lens[i]\n            extra = len(self.prev_tokens) + sample_len - max_sample_len\n            if extra > 0:\n                self.prev_tokens = self.prev_tokens[extra:]\n            pads = np.full(self.context_window - len(self.prev_tokens), pad)\n            new_toks[i] = np.concatenate([self.prev_tokens, toks[i].numpy(), pads])\n            new_tgt[i, len(self.prev_tokens):len(self.prev_tokens) + len(tgt[i])] = tgt[i]\n            start_idxs[i] = len(self.prev_tokens)\n            lengths[i] += len(self.prev_tokens)\n            self.prev_tokens = new_toks[i][new_toks[i] != pad][-self.context_window:]\n        sample[\'net_input\'][\'src_tokens\'] = torch.from_numpy(new_toks)\n        sample[\'target\'] = torch.from_numpy(new_tgt)\n        sample[\'start_indices\'] = start_idxs\n\n        return sample\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(index)\n\n    def size(self, index):\n        return self.dataset.size(index)\n\n    def ordered_indices(self):\n        # NOTE we don\'t shuffle the data to retain access to the previous dataset elements\n        return np.arange(len(self.dataset))\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, \'supports_prefetch\', False)\n\n    def prefetch(self, indices):\n        return self.dataset.prefetch(indices)\n'"
fairseq/data/lru_cache_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom functools import lru_cache\n\nfrom . import BaseWrapperDataset\n\n\nclass LRUCacheDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, token=None):\n        super().__init__(dataset)\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    @lru_cache(maxsize=8)\n    def collater(self, samples):\n        return self.dataset.collater(samples)\n'"
fairseq/data/mask_tokens_dataset.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom functools import lru_cache\n\nimport numpy as np\nimport torch\n\nfrom fairseq.data import data_utils, Dictionary\n\nfrom . import BaseWrapperDataset, LRUCacheDataset\n\n\nclass MaskTokensDataset(BaseWrapperDataset):\n    """"""\n    A wrapper Dataset for masked language modeling.\n\n    Input items are masked according to the specified masking probability.\n\n    Args:\n        dataset: Dataset to wrap.\n        sizes: Sentence lengths\n        vocab: Dictionary with the vocabulary and special tokens.\n        pad_idx: Id of pad token in vocab\n        mask_idx: Id of mask token in vocab\n        return_masked_tokens: controls whether to return the non-masked tokens\n            (the default) or to return a tensor with the original masked token\n            IDs (and *pad_idx* elsewhere). The latter is useful as targets for\n            masked LM training.\n        seed: Seed for random number generator for reproducibility.\n        mask_prob: probability of replacing a token with *mask_idx*.\n        leave_unmasked_prob: probability that a masked token is unmasked.\n        random_token_prob: probability of replacing a masked token with a\n            random token from the vocabulary.\n        freq_weighted_replacement: sample random replacement words based on\n            word frequencies in the vocab.\n        mask_whole_words: only mask whole words. This should be a byte mask\n            over vocab indices, indicating whether it is the beginning of a\n            word. We will extend any mask to encompass the whole word.\n        bpe: BPE to use for whole-word masking.\n    """"""\n\n    @classmethod\n    def apply_mask(cls, dataset: torch.utils.data.Dataset, *args, **kwargs):\n        """"""Return the source and target datasets for masked LM training.""""""\n        dataset = LRUCacheDataset(dataset)\n        return (\n            LRUCacheDataset(cls(dataset, *args, **kwargs, return_masked_tokens=False)),\n            LRUCacheDataset(cls(dataset, *args, **kwargs, return_masked_tokens=True)),\n        )\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.Dataset,\n        vocab: Dictionary,\n        pad_idx: int,\n        mask_idx: int,\n        return_masked_tokens: bool = False,\n        seed: int = 1,\n        mask_prob: float = 0.15,\n        leave_unmasked_prob: float = 0.1,\n        random_token_prob: float = 0.1,\n        freq_weighted_replacement: bool = False,\n        mask_whole_words: torch.Tensor = None,\n    ):\n        assert 0.0 < mask_prob < 1.0\n        assert 0.0 <= random_token_prob <= 1.0\n        assert 0.0 <= leave_unmasked_prob <= 1.0\n        assert random_token_prob + leave_unmasked_prob <= 1.0\n\n        self.dataset = dataset\n        self.vocab = vocab\n        self.pad_idx = pad_idx\n        self.mask_idx = mask_idx\n        self.return_masked_tokens = return_masked_tokens\n        self.seed = seed\n        self.mask_prob = mask_prob\n        self.leave_unmasked_prob = leave_unmasked_prob\n        self.random_token_prob = random_token_prob\n        self.mask_whole_words = mask_whole_words\n\n        if random_token_prob > 0.0:\n            if freq_weighted_replacement:\n                weights = np.array(self.vocab.count)\n            else:\n                weights = np.ones(len(self.vocab))\n            weights[:self.vocab.nspecial] = 0\n            self.weights = weights / weights.sum()\n\n        self.epoch = 0\n\n    def set_epoch(self, epoch, **unused):\n        super().set_epoch(epoch)\n        self.epoch = epoch\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, index: int):\n        with data_utils.numpy_seed(self.seed, self.epoch, index):\n            item = self.dataset[index]\n            sz = len(item)\n\n            assert self.mask_idx not in item, \\\n                \'Dataset contains mask_idx (={}), this is not expected!\'.format(\n                    self.mask_idx,\n                )\n\n            if self.mask_whole_words is not None:\n                word_begins_mask = self.mask_whole_words.gather(0, item)\n                word_begins_idx = word_begins_mask.nonzero().view(-1)\n                sz = len(word_begins_idx)\n                words = np.split(word_begins_mask, word_begins_idx)[1:]\n                assert len(words) == sz\n                word_lens = list(map(len, words))\n\n            # decide elements to mask\n            mask = np.full(sz, False)\n            num_mask = int(\n                # add a random number for probabilistic rounding\n                self.mask_prob * sz + np.random.rand()\n            )\n            mask[np.random.choice(sz, num_mask, replace=False)] = True\n\n            if self.return_masked_tokens:\n                # exit early if we\'re just returning the masked tokens\n                # (i.e., the targets for masked LM training)\n                if self.mask_whole_words is not None:\n                    mask = np.repeat(mask, word_lens)\n                new_item = np.full(len(mask), self.pad_idx)\n                new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8)) == 1]\n                return torch.from_numpy(new_item)\n\n            # decide unmasking and random replacement\n            rand_or_unmask_prob = self.random_token_prob + self.leave_unmasked_prob\n            if rand_or_unmask_prob > 0.0:\n                rand_or_unmask = mask & (np.random.rand(sz) < rand_or_unmask_prob)\n                if self.random_token_prob == 0.0:\n                    unmask = rand_or_unmask\n                    rand_mask = None\n                elif self.leave_unmasked_prob == 0.0:\n                    unmask = None\n                    rand_mask = rand_or_unmask\n                else:\n                    unmask_prob = self.leave_unmasked_prob / rand_or_unmask_prob\n                    decision = np.random.rand(sz) < unmask_prob\n                    unmask = rand_or_unmask & decision\n                    rand_mask = rand_or_unmask & (~decision)\n            else:\n                unmask = rand_mask = None\n\n            if unmask is not None:\n                mask = mask ^ unmask\n\n            if self.mask_whole_words is not None:\n                mask = np.repeat(mask, word_lens)\n\n            new_item = np.copy(item)\n            new_item[mask] = self.mask_idx\n            if rand_mask is not None:\n                num_rand = rand_mask.sum()\n                if num_rand > 0:\n                    if self.mask_whole_words is not None:\n                        rand_mask = np.repeat(rand_mask, word_lens)\n                        num_rand = rand_mask.sum()\n\n                    new_item[rand_mask] = np.random.choice(\n                        len(self.vocab),\n                        num_rand,\n                        p=self.weights,\n                    )\n\n            return torch.from_numpy(new_item)\n'"
fairseq/data/monolingual_dataset.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import data_utils, FairseqDataset\n\n\ndef collate(samples, pad_idx, eos_idx):\n    if len(samples) == 0:\n        return {}\n\n    def merge(key, is_list=False):\n        if is_list:\n            res = []\n            for i in range(len(samples[0][key])):\n                res.append(data_utils.collate_tokens(\n                    [s[key][i] for s in samples], pad_idx, eos_idx, left_pad=False,\n                ))\n            return res\n        else:\n            return data_utils.collate_tokens(\n                [s[key] for s in samples], pad_idx, eos_idx, left_pad=False,\n            )\n\n    src_tokens = merge(\'source\')\n    if samples[0][\'target\'] is not None:\n        is_target_list = isinstance(samples[0][\'target\'], list)\n        target = merge(\'target\', is_target_list)\n    else:\n        target = src_tokens\n\n    return {\n        \'id\': torch.LongTensor([s[\'id\'] for s in samples]),\n        \'nsentences\': len(samples),\n        \'ntokens\': sum(len(s[\'source\']) for s in samples),\n        \'net_input\': {\n            \'src_tokens\': src_tokens,\n            \'src_lengths\': torch.LongTensor([\n                s[\'source\'].numel() for s in samples\n            ]),\n        },\n        \'target\': target,\n    }\n\n\nclass MonolingualDataset(FairseqDataset):\n    """"""\n    A wrapper around torch.utils.data.Dataset for monolingual data.\n\n    Args:\n        dataset (torch.utils.data.Dataset): dataset to wrap\n        sizes (List[int]): sentence lengths\n        vocab (~fairseq.data.Dictionary): vocabulary\n        shuffle (bool, optional): shuffle the elements before batching\n            (default: True).\n    """"""\n\n    def __init__(self, dataset, sizes, src_vocab, tgt_vocab, add_eos_for_other_targets, shuffle,\n                 targets=None, add_bos_token=False):\n        self.dataset = dataset\n        self.sizes = np.array(sizes)\n        self.vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        self.add_eos_for_other_targets = add_eos_for_other_targets\n        self.shuffle = shuffle\n        self.add_bos_token = add_bos_token\n\n        assert targets is None or all(t in {\'self\', \'future\', \'past\'} for t in targets), \\\n            ""targets must be none or one of \'self\', \'future\', \'past\'""\n        if targets is not None and len(targets) == 0:\n            targets = None\n        self.targets = targets\n\n    def __getitem__(self, index):\n        if self.targets is not None:\n            # *future_target* is the original sentence\n            # *source* is shifted right by 1 (maybe left-padded with eos)\n            # *past_target* is shifted right by 2 (left-padded as needed)\n            #\n            # Left-to-right language models should condition on *source* and\n            # predict *future_target*.\n            # Right-to-left language models should condition on *source* and\n            # predict *past_target*.\n            source, future_target, past_target = self.dataset[index]\n            source, target = self._make_source_target(source, future_target, past_target)\n        else:\n            source = self.dataset[index]\n            target = None\n        source, target = self._maybe_add_bos(source, target)\n        return {\'id\': index, \'source\': source, \'target\': target}\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def _make_source_target(self, source, future_target, past_target):\n        if self.targets is not None:\n            target = []\n\n            if self.add_eos_for_other_targets and ((\'self\' in self.targets) or (\'past\' in self.targets)) \\\n                    and source[-1] != self.vocab.eos():\n                # append eos at the end of source\n                source = torch.cat([source, source.new([self.vocab.eos()])])\n\n                if \'future\' in self.targets:\n                    future_target = torch.cat([future_target, future_target.new([self.vocab.pad()])])\n                if \'past\' in self.targets:\n                    # first token is before the start of sentence which is only used in ""none"" break mode when\n                    # add_eos_for_other_targets is False\n                    past_target = torch.cat([past_target.new([self.vocab.pad()]), past_target[1:], source[-2, None]])\n\n            for t in self.targets:\n                if t == \'self\':\n                    target.append(source)\n                elif t == \'future\':\n                    target.append(future_target)\n                elif t == \'past\':\n                    target.append(past_target)\n                else:\n                    raise Exception(\'invalid target \' + t)\n\n            if len(target) == 1:\n                target = target[0]\n        else:\n            target = future_target\n\n        return source, self._filter_vocab(target)\n\n    def _maybe_add_bos(self, source, target):\n        if self.add_bos_token:\n            source = torch.cat([source.new([self.vocab.bos()]), source])\n            if target is not None:\n                target = torch.cat([target.new([self.tgt_vocab.bos()]), target])\n        return source, target\n\n    def _filter_vocab(self, target):\n        if len(self.tgt_vocab) != len(self.vocab):\n            def _filter(target):\n                mask = target.ge(len(self.tgt_vocab))\n                if mask.any():\n                    target[mask] = self.tgt_vocab.unk()\n                return target\n\n            if isinstance(target, list):\n                return [_filter(t) for t in target]\n            return _filter(target)\n        return target\n\n    def collater(self, samples):\n        """"""Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch with the following keys:\n\n                - `id` (LongTensor): example IDs in the original input order\n                - `ntokens` (int): total number of tokens in the batch\n                - `net_input` (dict): the input to the Model, containing keys:\n\n                  - `src_tokens` (LongTensor): a padded 2D Tensor of tokens in\n                    the source sentence of shape `(bsz, src_len)`. Padding will\n                    appear on the right.\n\n                - `target` (LongTensor): a padded 2D Tensor of tokens in the\n                  target sentence of shape `(bsz, tgt_len)`. Padding will appear\n                  on the right.\n        """"""\n        return collate(samples, self.vocab.pad(), self.vocab.eos())\n\n    def num_tokens(self, index):\n        """"""Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.""""""\n        return self.sizes[index]\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.""""""\n        return self.sizes[index]\n\n    def ordered_indices(self):\n        """"""Return an ordered list of indices. Batches will be constructed based\n        on this order.""""""\n        if self.shuffle:\n            order = [np.random.permutation(len(self))]\n        else:\n            order = [np.arange(len(self))]\n        order.append(self.sizes)\n        return np.lexsort(order)\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, \'supports_prefetch\', False)\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(indices)\n'"
fairseq/data/multi_corpus_sampled_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\nfrom typing import Callable, Dict, List\n\nimport numpy as np\n\nfrom . import FairseqDataset\n\n\ndef uniform_sampler(x):\n    # Sample from uniform distribution\n    return np.random.choice(x, 1).item()\n\n\nclass MultiCorpusSampledDataset(FairseqDataset):\n    """"""\n    Stores multiple instances of FairseqDataset together and in every iteration\n    creates a batch by first sampling a dataset according to a specified\n    probability distribution and then getting instances from that dataset.\n\n    Args:\n        datasets: an OrderedDict of FairseqDataset instances.\n        sampling_func: A function for sampling over list of dataset keys.\n            The default strategy is to sample uniformly.\n    """"""\n\n    def __init__(\n        self,\n        datasets: Dict[str, FairseqDataset],\n        sampling_func: Callable[[List], int] = None,\n    ):\n        super().__init__()\n        assert isinstance(datasets, OrderedDict)\n        self.datasets = datasets\n        if sampling_func is None:\n            sampling_func = uniform_sampler\n        self.sampling_func = sampling_func\n\n        self.total_num_instances = 0\n        for _, dataset in datasets.items():\n            assert isinstance(dataset, FairseqDataset)\n            self.total_num_instances += len(dataset)\n\n        self._ordered_indices = None\n\n    def __len__(self):\n        """"""\n        Length of this dataset is the sum of individual datasets\n        """"""\n        return self.total_num_instances\n\n    def ordered_indices(self):\n        """"""\n        Ordered indices for batching. Here we call the underlying\n        dataset\'s ordered_indices() so that we get the same random ordering\n        as we would have from using the underlying dataset directly.\n        """"""\n        if self._ordered_indices is None:\n            self._ordered_indices = OrderedDict(\n                [\n                    (key, dataset.ordered_indices())\n                    for key, dataset in self.datasets.items()\n                ]\n            )\n        return np.arange(len(self))\n\n    def _map_index_to_dataset(self, key: int, index: int):\n        """"""\n        Different underlying datasets have different lengths. In order to ensure\n        we are not accessing an index outside the range of the current dataset\n        size, we wrap around. This function should be called after we have\n        created an ordering for this and all underlying datasets.\n        """"""\n        assert (\n            self._ordered_indices is not None\n        ), ""Must call MultiCorpusSampledDataset.ordered_indices() first""\n        mapped_index = index % len(self.datasets[key])\n        return self._ordered_indices[key][mapped_index]\n\n    def __getitem__(self, index: int):\n        """"""\n        Get the item associated with index from each underlying dataset.\n        Since index is in the range of [0, TotalNumInstances], we need to\n        map the index to the dataset before retrieving the item.\n        """"""\n        return OrderedDict(\n            [\n                (key, dataset[self._map_index_to_dataset(key, index)])\n                for key, dataset in self.datasets.items()\n            ]\n        )\n\n    def collater(self, samples: List[Dict]):\n        """"""\n        Generate a mini-batch for this dataset.\n        To convert this into a regular mini-batch we use the following\n        logic:\n            1. Select a dataset using the specified probability distribution.\n            2. Call the collater function of the selected dataset.\n        """"""\n        if len(samples) == 0:\n            return None\n\n        selected_key = self.sampling_func(list(self.datasets.keys()))\n        selected_samples = [sample[selected_key] for sample in samples]\n        return self.datasets[selected_key].collater(selected_samples)\n\n    def num_tokens(self, index: int):\n        """"""\n        Return an example\'s length (number of tokens), used for batching. Here\n        we return the max across all examples at index across all underlying\n        datasets.\n        """"""\n        return max(\n            dataset.num_tokens(self._map_index_to_dataset(key, index))\n            for key, dataset in self.datasets.items()\n        )\n\n    def size(self, index: int):\n        """"""\n        Return an example\'s size as a float or tuple. Here we return the max\n        across all underlying datasets. This value is used when filtering a\n        dataset with max-positions.\n        """"""\n        return max(\n            dataset.size(self._map_index_to_dataset(key, index))\n            for key, dataset in self.datasets.items()\n        )\n\n    @property\n    def supports_prefetch(self):\n        return all(\n            getattr(dataset, ""supports_prefetch"", False)\n            for dataset in self.datasets.values()\n        )\n\n    def prefetch(self, indices):\n        for key, dataset in self.datasets.items():\n            dataset.prefetch(\n                [self._map_index_to_dataset(key, index) for index in indices]\n            )\n'"
fairseq/data/nested_dictionary_dataset.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\n\nimport torch\nfrom torch.utils.data.dataloader import default_collate\n\nfrom . import FairseqDataset\n\n\ndef _flatten(dico, prefix=None):\n    """"""Flatten a nested dictionary.""""""\n    new_dico = OrderedDict()\n    if isinstance(dico, dict):\n        prefix = prefix + \'.\' if prefix is not None else \'\'\n        for k, v in dico.items():\n            if v is None:\n                continue\n            new_dico.update(_flatten(v, prefix + k))\n    elif isinstance(dico, list):\n        for i, v in enumerate(dico):\n            new_dico.update(_flatten(v, prefix + \'.[\' + str(i) + \']\'))\n    else:\n        new_dico = OrderedDict({prefix: dico})\n    return new_dico\n\n\ndef _unflatten(dico):\n    """"""Unflatten a flattened dictionary into a nested dictionary.""""""\n    new_dico = OrderedDict()\n    for full_k, v in dico.items():\n        full_k = full_k.split(\'.\')\n        node = new_dico\n        for k in full_k[:-1]:\n            if k.startswith(\'[\') and k.endswith(\']\'):\n                k = int(k[1:-1])\n            if k not in node:\n                node[k] = OrderedDict()\n            node = node[k]\n        node[full_k[-1]] = v\n    return new_dico\n\n\nclass NestedDictionaryDataset(FairseqDataset):\n\n    def __init__(self, defn, sizes=None):\n        super().__init__()\n        self.defn = _flatten(defn)\n        self.sizes = [sizes] if not isinstance(sizes, (list, tuple)) else sizes\n\n        first = None\n        for v in self.defn.values():\n            if not isinstance(v, (FairseqDataset, torch.utils.data.Dataset, )):\n                raise ValueError(\'Expected Dataset but found: {}\'.format(v.__class__))\n            first = first or v\n            if len(v) > 0:\n                assert len(v) == len(first), \'dataset lengths must match\'\n\n        self._len = len(first)\n\n    def __getitem__(self, index):\n        return OrderedDict((k, ds[index]) for k, ds in self.defn.items())\n\n    def __len__(self):\n        return self._len\n\n    def collater(self, samples):\n        """"""Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        """"""\n        if len(samples) == 0:\n            return {}\n        sample = OrderedDict()\n        for k, ds in self.defn.items():\n            try:\n                sample[k] = ds.collater([s[k] for s in samples])\n            except NotImplementedError:\n                sample[k] = default_collate([s[k] for s in samples])\n        return _unflatten(sample)\n\n    def num_tokens(self, index):\n        """"""Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.""""""\n        return max(s[index] for s in self.sizes)\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.""""""\n        if len(self.sizes) == 1:\n            return self.sizes[0][index]\n        else:\n            return (s[index] for s in self.sizes)\n\n    @property\n    def supports_prefetch(self):\n        """"""Whether this dataset supports prefetching.""""""\n        return any(ds.supports_prefetch for ds in self.defn.values())\n\n    def prefetch(self, indices):\n        """"""Prefetch the data required for this epoch.""""""\n        for ds in self.defn.values():\n            if getattr(ds, \'supports_prefetch\', False):\n                ds.prefetch(indices)\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n        for ds in self.defn.values():\n            ds.set_epoch(epoch)\n'"
fairseq/data/noising.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport numpy as np\n\nfrom fairseq.data import data_utils\n\n\nclass WordNoising(object):\n    """"""Generate a noisy version of a sentence, without changing words themselves.""""""\n    def __init__(self, dictionary, bpe_cont_marker=""@@"", bpe_end_marker=None):\n        self.dictionary = dictionary\n        self.bpe_end = None\n        if bpe_cont_marker:\n            self.bpe_end = np.array([\n                not self.dictionary[i].endswith(bpe_cont_marker)\n                for i in range(len(self.dictionary))\n            ])\n        elif bpe_end_marker:\n            self.bpe_end = np.array([\n                self.dictionary[i].endswith(bpe_end_marker)\n                for i in range(len(self.dictionary))\n            ])\n\n        self.get_word_idx = (\n            self._get_bpe_word_idx\n            if self.bpe_end is not None\n            else self._get_token_idx\n        )\n\n    def noising(self, x, lengths, noising_prob=0.0):\n        raise NotImplementedError()\n\n    def _get_bpe_word_idx(self, x):\n        """"""\n        Given a list of BPE tokens, for every index in the tokens list,\n        return the index of the word grouping that it belongs to.\n        For example, for input x corresponding to [""how"", ""are"", ""y@@"", ""ou""],\n        return [[0], [1], [2], [2]].\n        """"""\n        # x: (T x B)\n        bpe_end = self.bpe_end[x]\n\n        if (x.size(0) == 1 and x.size(1) == 1):\n            # Special case when we only have one word in x. If x = [[N]],\n            # bpe_end is a scalar (bool) instead of a 2-dim array of bools,\n            # which makes the sum operation below fail.\n            return np.array([[0]])\n\n        # do a reduce front sum to generate word ids\n        word_idx = bpe_end[::-1].cumsum(0)[::-1]\n        word_idx = word_idx.max(0)[None, :] - word_idx\n        return word_idx\n\n    def _get_token_idx(self, x):\n        """"""\n        This is to extend noising functions to be able to apply to non-bpe\n        tokens, e.g. word or characters.\n        """"""\n        x = torch.t(x)\n        word_idx = np.array([range(len(x_i)) for x_i in x])\n        return np.transpose(word_idx)\n\n\nclass WordDropout(WordNoising):\n    """"""Randomly drop input words. If not passing blank_idx (default is None),\n    then dropped words will be removed. Otherwise, it will be replaced by the\n    blank_idx.""""""\n\n    def __init__(self, dictionary, default_dropout_prob=0.1, bpe_cont_marker=""@@"", bpe_end_marker=None):\n        super().__init__(dictionary, bpe_cont_marker, bpe_end_marker)\n        self.default_dropout_prob = default_dropout_prob\n\n    def noising(self, x, lengths, dropout_prob=None, blank_idx=None):\n        if dropout_prob is None:\n            dropout_prob = self.default_dropout_prob\n        # x: (T x B), lengths: B\n        if dropout_prob == 0:\n            return x, lengths\n\n        assert 0 < dropout_prob < 1\n\n        # be sure to drop entire words\n        word_idx = self.get_word_idx(x)\n        sentences = []\n        modified_lengths = []\n        for i in range(lengths.size(0)):\n            # Since dropout probabilities need to apply over non-pad tokens,\n            # it is not trivial to generate the keep mask without consider\n            # input lengths; otherwise, this could be done outside the loop\n\n            # We want to drop whole words based on word_idx grouping\n            num_words = max(word_idx[:, i]) + 1\n\n            # ith example: [x0, x1, ..., eos, pad, ..., pad]\n            # We should only generate keep probs for non-EOS tokens. Thus if the\n            # input sentence ends in EOS, the last word idx is not included in\n            # the dropout mask generation and we append True to always keep EOS.\n            # Otherwise, just generate the dropout mask for all word idx\n            # positions.\n            has_eos = x[lengths[i] - 1, i] == self.dictionary.eos()\n            if has_eos:  # has eos?\n                keep = np.random.rand(num_words - 1) >= dropout_prob\n                keep = np.append(keep, [True])  # keep EOS symbol\n            else:\n                keep = np.random.rand(num_words) >= dropout_prob\n\n            words = x[:lengths[i], i].tolist()\n\n            # TODO: speed up the following loop\n            # drop words from the input according to keep\n            new_s = [\n                w if keep[word_idx[j, i]] else blank_idx\n                for j, w in enumerate(words)\n            ]\n            new_s = [w for w in new_s if w is not None]\n            # we need to have at least one word in the sentence (more than the\n            # start / end sentence symbols)\n            if len(new_s) <= 1:\n                # insert at beginning in case the only token left is EOS\n                # EOS should be at end of list.\n                new_s.insert(0, words[np.random.randint(0, len(words))])\n            assert len(new_s) >= 1 and (\n                not has_eos  # Either don\'t have EOS at end or last token is EOS\n                or (len(new_s) >= 2 and new_s[-1] == self.dictionary.eos())\n            ), ""New sentence is invalid.""\n            sentences.append(new_s)\n            modified_lengths.append(len(new_s))\n        # re-construct input\n        modified_lengths = torch.LongTensor(modified_lengths)\n        modified_x = torch.LongTensor(\n            modified_lengths.max(),\n            modified_lengths.size(0)\n        ).fill_(self.dictionary.pad())\n        for i in range(modified_lengths.size(0)):\n            modified_x[:modified_lengths[i], i].copy_(torch.LongTensor(sentences[i]))\n\n        return modified_x, modified_lengths\n\n\nclass WordShuffle(WordNoising):\n    """"""Shuffle words by no more than k positions.""""""\n\n    def __init__(self, dictionary, default_max_shuffle_distance=3, bpe_cont_marker=""@@"", bpe_end_marker=None):\n        super().__init__(dictionary, bpe_cont_marker, bpe_end_marker)\n        self.default_max_shuffle_distance = 3\n\n    def noising(self, x, lengths, max_shuffle_distance=None):\n        if max_shuffle_distance is None:\n            max_shuffle_distance = self.default_max_shuffle_distance\n        # x: (T x B), lengths: B\n        if max_shuffle_distance == 0:\n            return x, lengths\n\n        # max_shuffle_distance < 1 will return the same sequence\n        assert max_shuffle_distance > 1\n\n        # define noise word scores\n        noise = np.random.uniform(\n            0,\n            max_shuffle_distance,\n            size=(x.size(0), x.size(1)),\n        )\n        noise[0] = -1  # do not move start sentence symbol\n        # be sure to shuffle entire words\n        word_idx = self.get_word_idx(x)\n        x2 = x.clone()\n        for i in range(lengths.size(0)):\n            length_no_eos = lengths[i]\n            if x[lengths[i] - 1, i] == self.dictionary.eos():\n                length_no_eos = lengths[i] - 1\n            # generate a random permutation\n            scores = word_idx[:length_no_eos, i] + noise[word_idx[:length_no_eos, i], i]\n            # ensure no reordering inside a word\n            scores += 1e-6 * np.arange(length_no_eos.item())\n            permutation = scores.argsort()\n            # shuffle words\n            x2[:length_no_eos, i].copy_(\n                x2[:length_no_eos, i][torch.from_numpy(permutation)]\n            )\n        return x2, lengths\n\n\nclass UnsupervisedMTNoising(WordNoising):\n    """"""\n    Implements the default configuration for noising in UnsupervisedMT\n    (github.com/facebookresearch/UnsupervisedMT)\n    """"""\n    def __init__(\n        self,\n        dictionary,\n        max_word_shuffle_distance,\n        word_dropout_prob,\n        word_blanking_prob,\n        bpe_cont_marker=""@@"",\n        bpe_end_marker=None,\n    ):\n        super().__init__(dictionary)\n        self.max_word_shuffle_distance = max_word_shuffle_distance\n        self.word_dropout_prob = word_dropout_prob\n        self.word_blanking_prob = word_blanking_prob\n\n        self.word_dropout = WordDropout(\n            dictionary=dictionary,\n            bpe_cont_marker=bpe_cont_marker,\n            bpe_end_marker=bpe_end_marker,\n        )\n        self.word_shuffle = WordShuffle(\n            dictionary=dictionary,\n            bpe_cont_marker=bpe_cont_marker,\n            bpe_end_marker=bpe_end_marker,\n        )\n\n    def noising(self, x, lengths):\n        # 1. Word Shuffle\n        noisy_src_tokens, noisy_src_lengths = self.word_shuffle.noising(\n            x=x,\n            lengths=lengths,\n            max_shuffle_distance=self.max_word_shuffle_distance,\n        )\n        # 2. Word Dropout\n        noisy_src_tokens, noisy_src_lengths = self.word_dropout.noising(\n            x=noisy_src_tokens,\n            lengths=noisy_src_lengths,\n            dropout_prob=self.word_dropout_prob,\n        )\n        # 3. Word Blanking\n        noisy_src_tokens, noisy_src_lengths = self.word_dropout.noising(\n            x=noisy_src_tokens,\n            lengths=noisy_src_lengths,\n            dropout_prob=self.word_blanking_prob,\n            blank_idx=self.dictionary.unk(),\n        )\n\n        return noisy_src_tokens\n\n\nclass NoisingDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        src_dataset,\n        src_dict,\n        seed,\n        noiser=None,\n        noising_class=UnsupervisedMTNoising,\n        **kwargs\n    ):\n        """"""\n        Wrap a :class:`~torch.utils.data.Dataset` and apply noise to the\n        samples based on the supplied noising configuration.\n\n        Args:\n            src_dataset (~torch.utils.data.Dataset): dataset to wrap.\n                to build self.src_dataset --\n                a LanguagePairDataset with src dataset as the source dataset and\n                None as the target dataset. Should NOT have padding so that\n                src_lengths are accurately calculated by language_pair_dataset\n                collate function.\n                We use language_pair_dataset here to encapsulate the tgt_dataset\n                so we can re-use the LanguagePairDataset collater to format the\n                batches in the structure that SequenceGenerator expects.\n            src_dict (~fairseq.data.Dictionary): source dictionary\n            seed (int): seed to use when generating random noise\n            noiser (WordNoising): a pre-initialized :class:`WordNoising`\n                instance. If this is None, a new instance will be created using\n                *noising_class* and *kwargs*.\n            noising_class (class, optional): class to use to initialize a\n                default :class:`WordNoising` instance.\n            kwargs (dict, optional): arguments to initialize the default\n                :class:`WordNoising` instance given by *noiser*.\n        """"""\n        self.src_dataset = src_dataset\n        self.src_dict = src_dict\n        self.seed = seed\n        self.noiser = noiser if noiser is not None else noising_class(\n            dictionary=src_dict, **kwargs,\n        )\n\n    def __getitem__(self, index):\n        """"""\n        Returns a single noisy sample. Multiple samples are fed to the collater\n        create a noising dataset batch.\n        """"""\n        src_tokens = self.src_dataset[index]\n        src_lengths = torch.LongTensor([len(src_tokens)])\n        src_tokens = src_tokens.unsqueeze(0)\n\n        # Transpose src tokens to fit expected shape of x in noising function\n        # (batch size, sequence length) -> (sequence length, batch size)\n        src_tokens_t = torch.t(src_tokens)\n\n        with data_utils.numpy_seed(self.seed + index):\n            noisy_src_tokens = self.noiser.noising(src_tokens_t, src_lengths)\n\n        # Transpose back to expected src_tokens format\n        # (sequence length, 1) -> (1, sequence length)\n        noisy_src_tokens = torch.t(noisy_src_tokens)\n        return noisy_src_tokens[0]\n\n    def __len__(self):\n        """"""\n        The length of the noising dataset is the length of src.\n        """"""\n        return len(self.src_dataset)\n\n    @property\n    def supports_prefetch(self):\n        return self.src_dataset.supports_prefetch\n\n    def prefetch(self, indices):\n        if self.src_dataset.supports_prefetch:\n            self.src_dataset.prefetch(indices)\n'"
fairseq/data/num_samples_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import FairseqDataset\n\n\nclass NumSamplesDataset(FairseqDataset):\n\n    def __getitem__(self, index):\n        return 1\n\n    def __len__(self):\n        return 0\n\n    def collater(self, samples):\n        return sum(samples)\n'"
fairseq/data/numel_dataset.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass NumelDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, reduce=False):\n        super().__init__(dataset)\n        self.reduce = reduce\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        if torch.is_tensor(item):\n            return torch.numel(item)\n        else:\n            return np.size(item)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n        if self.reduce:\n            return sum(samples)\n        else:\n            return torch.tensor(samples)\n'"
fairseq/data/offset_tokens_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import BaseWrapperDataset\n\n\nclass OffsetTokensDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, offset):\n        super().__init__(dataset)\n        self.offset = offset\n\n    def __getitem__(self, idx):\n        return self.dataset[idx] + self.offset\n'"
fairseq/data/pad_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.data import data_utils\n\nfrom . import BaseWrapperDataset\n\n\nclass PadDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, pad_idx, left_pad):\n        super().__init__(dataset)\n        self.pad_idx = pad_idx\n        self.left_pad = left_pad\n\n    def collater(self, samples):\n        return data_utils.collate_tokens(samples, self.pad_idx, left_pad=self.left_pad)\n\n\nclass LeftPadDataset(PadDataset):\n\n    def __init__(self, dataset, pad_idx):\n        super().__init__(dataset, pad_idx, left_pad=True)\n\n\nclass RightPadDataset(PadDataset):\n\n    def __init__(self, dataset, pad_idx):\n        super().__init__(dataset, pad_idx, left_pad=False)\n'"
fairseq/data/plasma_utils.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport subprocess\nimport tempfile\n\n\nclass PlasmaArray(object):\n    """"""\n    Wrapper around numpy arrays that automatically moves the data to shared\n    memory upon serialization. This is particularly helpful when passing numpy\n    arrays through multiprocessing, so that data is not unnecessarily\n    duplicated or pickled.\n    """"""\n\n    def __init__(self, array):\n        super().__init__()\n        self.array = array\n        self.disable = array.nbytes < 134217728  # disable for arrays <128MB\n        self.object_id = None\n        self.path = None\n\n        # variables with underscores shouldn\'t be pickled\n        self._client = None\n        self._server = None\n        self._server_tmp = None\n        self._plasma = None\n\n    @property\n    def plasma(self):\n        if self._plasma is None and not self.disable:\n            try:\n                import pyarrow.plasma as plasma\n                self._plasma = plasma\n            except ImportError:\n                self._plasma = None\n        return self._plasma\n\n    def start_server(self):\n        if self.plasma is None or self._server is not None:\n            return\n        assert self.object_id is None\n        assert self.path is None\n        self._server_tmp = tempfile.NamedTemporaryFile()\n        self.path = self._server_tmp.name\n        self._server = subprocess.Popen([\n            \'plasma_store\',\n            \'-m\', str(int(1.05 * self.array.nbytes)),\n            \'-s\', self.path,\n        ])\n\n    @property\n    def client(self):\n        if self._client is None:\n            assert self.path is not None\n            self._client = self.plasma.connect(self.path)\n        return self._client\n\n    def __getstate__(self):\n        if self.plasma is None:\n            return self.__dict__\n        if self.object_id is None:\n            self.start_server()\n            self.object_id = self.client.put(self.array)\n        state = self.__dict__.copy()\n        del state[\'array\']\n        state[\'_client\'] = None\n        state[\'_server\'] = None\n        state[\'_server_tmp\'] = None\n        state[\'_plasma\'] = None\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        if self.plasma is None:\n            return\n        self.array = self.client.get(self.object_id)\n\n    def __del__(self):\n        if self._server is not None:\n            self._server.kill()\n            self._server = None\n            self._server_tmp.close()\n            self._server_tmp = None\n'"
fairseq/data/prepend_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass PrependDataset(BaseWrapperDataset):\n    def __init__(self, dataset, prepend_getter, ensure_first_token_is=None):\n        super().__init__(dataset)\n        self.prepend_getter = prepend_getter\n        self.ensure_first_token = ensure_first_token_is\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        is_tuple = isinstance(item, tuple)\n        src = item[0] if is_tuple else item\n\n        assert self.ensure_first_token is None or src[0] == self.ensure_first_token\n        prepend_idx = self.prepend_getter(self.dataset, idx)\n        assert isinstance(prepend_idx, int)\n        src[0] = prepend_idx\n        item = tuple((src,) + item[1:]) if is_tuple else src\n        return item\n'"
fairseq/data/prepend_token_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass PrependTokenDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, token=None):\n        super().__init__(dataset)\n        self.token = token\n        if token is not None:\n            self._sizes = np.array(dataset.sizes) + 1\n        else:\n            self._sizes = dataset.sizes\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        if self.token is not None:\n            item = torch.cat([item.new([self.token]), item])\n        return item\n\n    @property\n    def sizes(self):\n        return self._sizes\n\n    def num_tokens(self, index):\n        n = self.dataset.num_tokens(index)\n        if self.token is not None:\n            n += 1\n        return n\n\n    def size(self, index):\n        n = self.dataset.size(index)\n        if self.token is not None:\n            n += 1\n        return n\n'"
fairseq/data/raw_label_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom . import FairseqDataset\n\n\nclass RawLabelDataset(FairseqDataset):\n\n    def __init__(self, labels):\n        super().__init__()\n        self.labels = labels\n\n    def __getitem__(self, index):\n        return self.labels[index]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def collater(self, samples):\n        return torch.tensor(samples)\n'"
fairseq/data/replace_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import BaseWrapperDataset\n\n\nclass ReplaceDataset(BaseWrapperDataset):\n    """"""Replaces tokens found in the dataset by a specified replacement token\n\n        Args:\n            dataset (~torch.utils.data.Dataset): dataset to replace tokens in\n            replace_map(Dictionary[int,int]): map of token to replace -> replacement token\n            offsets (List[int]): do not replace tokens before (from left if pos, right if neg) this offset. should be\n            as many as the number of objects returned by the underlying dataset __getitem__ method.\n        """"""\n\n    def __init__(self, dataset, replace_map, offsets):\n        super().__init__(dataset)\n        assert len(replace_map) > 0\n        self.replace_map = replace_map\n        self.offsets = offsets\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        is_tuple = isinstance(item, tuple)\n        srcs = item if is_tuple else [item]\n\n        for offset, src in zip(self.offsets, srcs):\n            for k, v in self.replace_map.items():\n                src_off = src[offset:] if offset >= 0 else src[:offset]\n                src_off.masked_fill_(src_off == k, v)\n\n        item = srcs if is_tuple else srcs[0]\n        return item\n'"
fairseq/data/resampling_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\n\nfrom . import BaseWrapperDataset, plasma_utils\n\n\nclass ResamplingDataset(BaseWrapperDataset):\n    """"""Randomly samples from a given dataset at each epoch.\n\n    Sampling is done with or without replacement, depending on the ""replace""\n    parameter.\n\n    Optionally, the epoch size can be rescaled. This is potentially desirable\n    to increase per-epoch coverage of the base dataset (since sampling with\n    replacement means that many items in the dataset will be left out). In the\n    case of sampling without replacement, size_ratio should be strictly less\n    than 1.\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset on which to sample.\n        weights (List[float]): list of probability weights\n            (default: None, which corresponds to uniform sampling).\n        replace (bool): sampling mode; True for ""with replacement"", or False\n            for ""without replacement"" (default: True)\n        size_ratio (float): the ratio to subsample to; must be positive\n            (default: 1.0).\n        batch_by_size (bool): whether or not to batch by sequence length\n            (default: True).\n        seed (int): RNG seed to use (default: 0).\n        epoch (int): starting epoch number (default: 1).\n    """"""\n\n    def __init__(\n        self,\n        dataset,\n        weights=None,\n        replace=True,\n        size_ratio=1.0,\n        batch_by_size=True,\n        seed=0,\n        epoch=1,\n    ):\n        super().__init__(dataset)\n\n        if weights is None:\n            self.weights = None\n\n        else:\n            assert len(weights) == len(dataset)\n            weights_arr = np.array(weights, dtype=np.float64)\n            weights_arr /= weights_arr.sum()\n            self.weights = plasma_utils.PlasmaArray(weights_arr)\n\n        self.replace = replace\n\n        assert size_ratio > 0.0\n        if not self.replace:\n            assert size_ratio < 1.0\n        self.size_ratio = float(size_ratio)\n        self.actual_size = np.ceil(len(dataset) * self.size_ratio).astype(int)\n\n        self.batch_by_size = batch_by_size\n        self.seed = seed\n\n        self._cur_epoch = None\n        self._cur_indices = None\n\n        self.set_epoch(epoch)\n\n    def __getitem__(self, index):\n        return self.dataset[self._cur_indices.array[index]]\n\n    def __len__(self):\n        return self.actual_size\n\n    @property\n    def sizes(self):\n        if isinstance(self.dataset.sizes, list):\n            return [s[self._cur_indices.array] for s in self.dataset.sizes]\n        return self.dataset.sizes[self._cur_indices.array]\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(self._cur_indices.array[index])\n\n    def size(self, index):\n        return self.dataset.size(self._cur_indices.array[index])\n\n    def ordered_indices(self):\n        if self.batch_by_size:\n            order = [\n                np.arange(len(self)),\n                self.sizes,\n            ]  # No need to handle `self.shuffle == True`\n            return np.lexsort(order)\n        else:\n            return np.arange(len(self))\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(self._cur_indices.array[indices])\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n\n        if epoch == self._cur_epoch:\n            return\n\n        self._cur_epoch = epoch\n\n        # Generate a weighted sample of indices as a function of the\n        # random seed and the current epoch.\n\n        rng = np.random.RandomState(\n            [\n                42,  # magic number\n                self.seed % (2 ** 32),  # global seed\n                self._cur_epoch,  # epoch index\n            ]\n        )\n        self._cur_indices = plasma_utils.PlasmaArray(\n            rng.choice(\n                len(self.dataset),\n                self.actual_size,\n                replace=self.replace,\n                p=(None if self.weights is None else self.weights.array),\n            )\n        )\n'"
fairseq/data/roll_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass RollDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, shifts):\n        super().__init__(dataset)\n        self.shifts = shifts\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        return torch.roll(item, self.shifts)\n'"
fairseq/data/round_robin_zip_datasets.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\n\nimport numpy as np\n\nfrom . import FairseqDataset\n\n\nclass RoundRobinZipDatasets(FairseqDataset):\n    """"""Zip multiple :class:`~fairseq.data.FairseqDataset` instances together.\n\n    Shorter datasets are repeated in a round-robin fashion to match the length\n    of the longest one.\n\n    Args:\n        datasets (Dict[~fairseq.data.FairseqDataset]): a dictionary of\n            :class:`~fairseq.data.FairseqDataset` instances.\n        eval_key (str, optional): a key used at evaluation time that causes\n            this instance to pass-through batches from *datasets[eval_key]*.\n    """"""\n\n    def __init__(self, datasets, eval_key=None):\n        super().__init__()\n        assert isinstance(datasets, OrderedDict)\n        self.datasets = datasets\n        self.eval_key = eval_key\n\n        self.longest_dataset = None\n        self.longest_dataset_key = None\n        for key, dataset in datasets.items():\n            assert isinstance(dataset, FairseqDataset)\n            if self.longest_dataset is None or len(dataset) > len(self.longest_dataset):\n                self.longest_dataset = dataset\n                self.longest_dataset_key = key\n\n        self._ordered_indices = None\n\n    def _map_index(self, key, index):\n        assert self._ordered_indices is not None, \\\n            \'Must call RoundRobinZipDatasets.ordered_indices() first\'\n        return self._ordered_indices[key][index % len(self.datasets[key])]\n\n    def __getitem__(self, index):\n        if self.eval_key is None:\n            return OrderedDict([\n                (key, dataset[self._map_index(key, index)])\n                for key, dataset in self.datasets.items()\n            ])\n        else:\n            # at evaluation time it\'s useful to pass-through batches from a single key\n            return self.datasets[self.eval_key][self._map_index(self.eval_key, index)]\n\n    def __len__(self):\n        return len(self.longest_dataset)\n\n    def collater(self, samples):\n        """"""Merge a list of samples to form a mini-batch.""""""\n        if len(samples) == 0:\n            return None\n        if self.eval_key is None:\n            return OrderedDict([\n                (key, dataset.collater([sample[key] for sample in samples]))\n                for key, dataset in self.datasets.items()\n            ])\n        else:\n            # at evaluation time it\'s useful to pass-through batches from a single key\n            return self.datasets[self.eval_key].collater(samples)\n\n    def num_tokens(self, index):\n        """"""Return an example\'s length (number of tokens), used for batching.""""""\n        # TODO make it configurable whether to use max() or sum() here\n        return max(\n            dataset.num_tokens(self._map_index(key, index))\n            for key, dataset in self.datasets.items()\n        )\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.""""""\n        return {\n            key: dataset.size(self._map_index(key, index))\n            for key, dataset in self.datasets.items()\n        }\n\n    def ordered_indices(self):\n        """"""Ordered indices for batching.""""""\n        if self._ordered_indices is None:\n            # Call the underlying dataset\'s ordered_indices() here, so that we\n            # get the same random ordering as we would have from using the\n            # underlying dataset directly.\n            self._ordered_indices = OrderedDict([\n                (key, dataset.ordered_indices())\n                for key, dataset in self.datasets.items()\n            ])\n        return np.arange(len(self))\n\n    @property\n    def supports_prefetch(self):\n        return all(\n            getattr(dataset, \'supports_prefetch\', False)\n            for dataset in self.datasets.values()\n        )\n\n    def prefetch(self, indices):\n        for key, dataset in self.datasets.items():\n            dataset.prefetch([self._map_index(key, index) for index in indices])\n'"
fairseq/data/shorten_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nfrom fairseq.data import data_utils\n\nfrom . import BaseWrapperDataset\n\n\nclass TruncateDataset(BaseWrapperDataset):\n    """"""Truncate a sequence by returning the first truncation_length tokens\n    """"""\n\n    def __init__(self, dataset, truncation_length):\n        super().__init__(dataset)\n        assert truncation_length is not None\n        self.truncation_length = truncation_length\n        self.dataset = dataset\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        item_len = item.size(0)\n        if item_len > self.truncation_length:\n            item = item[:self.truncation_length]\n        return item\n\n    @property\n    def sizes(self):\n        return np.minimum(self.dataset.sizes, self.truncation_length)\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nclass RandomCropDataset(TruncateDataset):\n    """"""Truncate a sequence by returning a random crop of truncation_length tokens\n    """"""\n\n    def __init__(self, dataset, truncation_length, seed=1):\n        super().__init__(dataset, truncation_length)\n        self.seed = seed\n        self.epoch = 0\n\n    def set_epoch(self, epoch, **unused):\n        super().set_epoch(epoch)\n        self.epoch = epoch\n\n    def __getitem__(self, index):\n        with data_utils.numpy_seed(self.seed, self.epoch, index):\n            item = self.dataset[index]\n            item_len = item.size(0)\n            excess = item_len - self.truncation_length\n            if excess > 0:\n                start_idx = np.random.randint(0, excess)\n                item = item[start_idx:start_idx+self.truncation_length]\n            return item\n\ndef maybe_shorten_dataset(dataset, split, shorten_data_split_whitelist, shorten_method, tokens_per_sample, seed):\n    truncate_split = split in shorten_data_split_whitelist.split(\',\') \\\n        or len(shorten_data_split_whitelist) == 0\n    if shorten_method == \'truncate\' and truncate_split:\n        dataset = TruncateDataset(dataset, tokens_per_sample)\n    elif shorten_method == \'random_crop\' and truncate_split:\n        dataset = RandomCropDataset(dataset, tokens_per_sample, seed)\n    return dataset\n'"
fairseq/data/sort_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\n\nfrom . import BaseWrapperDataset\n\n\nclass SortDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, sort_order):\n        super().__init__(dataset)\n        if not isinstance(sort_order, (list, tuple)):\n            sort_order = [sort_order]\n        self.sort_order = sort_order\n\n        assert all(len(so) == len(dataset) for so in sort_order)\n\n    def ordered_indices(self):\n        return np.lexsort(self.sort_order)\n'"
fairseq/data/strip_token_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import BaseWrapperDataset\n\n\nclass StripTokenDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, id_to_strip):\n        super().__init__(dataset)\n        self.id_to_strip = id_to_strip\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        while len(item) > 0 and item[-1] == self.id_to_strip:\n            item = item[:-1]\n        while len(item) > 0 and item[0] == self.id_to_strip:\n            item = item[1:]\n        return item\n'"
fairseq/data/subsample_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport numpy as np\n\nfrom . import BaseWrapperDataset\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass SubsampleDataset(BaseWrapperDataset):\n    """"""Subsamples a given dataset by a specified ratio. Subsampling is done on the number of examples\n\n            Args:\n                dataset (~torch.utils.data.Dataset): dataset to subsample\n                size_ratio(float): the ratio to subsample to. must be between 0 and 1 (exclusive)\n            """"""\n\n    def __init__(self, dataset, size_ratio):\n        super().__init__(dataset)\n        assert size_ratio < 1\n        self.actual_size = np.ceil(len(dataset) * size_ratio).astype(int)\n        self.indices = np.random.choice(\n            list(range(len(self.dataset))), self.actual_size, replace=False\n        )\n        logger.info(\n            ""subsampled dataset from {} to {} (ratio={})"".format(\n                len(self.dataset), self.actual_size, size_ratio\n            )\n        )\n\n    def __getitem__(self, index):\n        return self.dataset[self.indices[index]]\n\n    def __len__(self):\n        return self.actual_size\n\n    def collater(self, samples):\n        return self.dataset.collater(samples)\n\n    @property\n    def sizes(self):\n        return self.dataset.sizes[self.indices]\n\n    @property\n    def name(self):\n        return self.dataset.name\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(self.indices[index])\n\n    def size(self, index):\n        return self.dataset.size(self.indices[index])\n\n    def ordered_indices(self):\n        """"""Return an ordered list of indices. Batches will be constructed based\n        on this order.""""""\n        if self.shuffle:\n            order = [np.random.permutation(len(self))]\n        else:\n            order = [np.arange(len(self))]\n        order.append(self.sizes)\n        return np.lexsort(order)\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(self.indices[indices])\n'"
fairseq/data/token_block_dataset.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom fairseq.data import FairseqDataset, plasma_utils\n\n\nclass TokenBlockDataset(FairseqDataset):\n    """"""Break a Dataset of tokens into blocks.\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset to break into blocks\n        sizes (List[int]): sentence lengths (required for \'complete\' and \'eos\')\n        block_size (int): maximum block size (ignored in \'eos\' break mode)\n        break_mode (str, optional): Mode used for breaking tokens. Values can\n            be one of:\n            - \'none\': break tokens into equally sized blocks (up to block_size)\n            - \'complete\': break tokens into blocks (up to block_size) such that\n                blocks contains complete sentences, although block_size may be\n                exceeded if some sentences exceed block_size\n            - \'complete_doc\': similar to \'complete\' mode, but do not\n                cross document boundaries\n            - \'eos\': each block contains one sentence (block_size is ignored)\n        include_targets (bool, optional): return next tokens as targets\n            (default: False).\n        document_sep_len (int, optional): document separator size (required for\n            \'complete_doc\' break mode). Typically 1 if the sentences have eos\n            and 0 otherwise.\n    """"""\n    def __init__(\n        self,\n        dataset,\n        sizes,\n        block_size,\n        pad,\n        eos,\n        break_mode=None,\n        include_targets=False,\n        document_sep_len=1,\n    ):\n        try:\n            from fairseq.data.token_block_utils_fast import (\n                _get_slice_indices_fast,\n                _get_block_to_dataset_index_fast,\n            )\n        except ImportError:\n            raise ImportError(\n                \'Please build Cython components with: `pip install --editable .` \'\n                \'or `python setup.py build_ext --inplace`\'\n            )\n\n        super().__init__()\n        self.dataset = dataset\n        self.pad = pad\n        self.eos = eos\n        self.include_targets = include_targets\n\n        assert len(dataset) == len(sizes)\n        assert len(dataset) > 0\n\n        if isinstance(sizes, list):\n            sizes = np.array(sizes, dtype=np.int64)\n        else:\n            if torch.is_tensor(sizes):\n                sizes = sizes.numpy()\n            sizes = sizes.astype(np.int64)\n\n        break_mode = break_mode if break_mode is not None else \'none\'\n\n        # For ""eos"" break-mode, block_size is not required parameters.\n        if break_mode == ""eos"" and block_size is None:\n            block_size = 0\n\n        slice_indices = _get_slice_indices_fast(sizes, break_mode, block_size, document_sep_len)\n        self._sizes = slice_indices[:, 1] - slice_indices[:, 0]\n\n        # build index mapping block indices to the underlying dataset indices\n        if break_mode == ""eos"":\n            # much faster version for eos break mode\n            block_to_dataset_index = np.stack(\n                [\n                    np.arange(len(sizes)),  # starting index in dataset\n                    np.zeros(\n                        len(sizes), dtype=np.long\n                    ),  # starting offset within starting index\n                    np.arange(len(sizes)),  # ending index in dataset\n                ],\n                1,\n            )\n        else:\n            block_to_dataset_index = _get_block_to_dataset_index_fast(\n                sizes,\n                slice_indices,\n            )\n        self._slice_indices = plasma_utils.PlasmaArray(slice_indices)\n        self._sizes = plasma_utils.PlasmaArray(self._sizes)\n        self._block_to_dataset_index = plasma_utils.PlasmaArray(block_to_dataset_index)\n\n    @property\n    def slice_indices(self):\n        return self._slice_indices.array\n\n    @property\n    def sizes(self):\n        return self._sizes.array\n\n    @property\n    def block_to_dataset_index(self):\n        return self._block_to_dataset_index.array\n\n    def attr(self, attr: str, index: int):\n        start_ds_idx, _, _ = self.block_to_dataset_index[index]\n        return self.dataset.attr(attr, start_ds_idx)\n\n    def __getitem__(self, index):\n        start_ds_idx, start_offset, end_ds_idx = self.block_to_dataset_index[index]\n\n        buffer = torch.cat(\n            [self.dataset[idx] for idx in range(start_ds_idx, end_ds_idx + 1)]\n        )\n\n        slice_s, slice_e = self.slice_indices[index]\n        length = slice_e - slice_s\n        s, e = start_offset, start_offset + length\n        item = buffer[s:e]\n\n        if self.include_targets:\n            # *target* is the original sentence (=item)\n            # *source* is shifted right by 1 (maybe left-padded with eos)\n            # *past_target* is shifted right by 2 (left-padded as needed)\n            if s == 0:\n                source = torch.cat([item.new([self.eos]), buffer[0 : e - 1]])\n                past_target = torch.cat(\n                    [item.new([self.pad, self.eos]), buffer[0 : e - 2]]\n                )\n            else:\n                source = buffer[s - 1 : e - 1]\n                if s == 1:\n                    past_target = torch.cat([item.new([self.eos]), buffer[0 : e - 2]])\n                else:\n                    past_target = buffer[s - 2 : e - 2]\n\n            return source, item, past_target\n\n        return item\n\n    def __len__(self):\n        return len(self.slice_indices)\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, ""supports_prefetch"", False)\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(\n            {\n                ds_idx\n                for index in indices\n                for start_ds_idx, _, end_ds_idx in [self.block_to_dataset_index[index]]\n                for ds_idx in range(start_ds_idx, end_ds_idx + 1)\n            }\n        )\n'"
fairseq/data/transform_eos_dataset.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom . import FairseqDataset\n\n\nclass TransformEosDataset(FairseqDataset):\n    """"""A :class:`~fairseq.data.FairseqDataset` wrapper that appends/prepends/strips EOS.\n\n    Note that the transformation is applied in :func:`collater`.\n\n    Args:\n        dataset (~fairseq.data.FairseqDataset): dataset to wrap\n        eos (int): index of the end-of-sentence symbol\n        append_eos_to_src (bool, optional): append EOS to the end of src\n        remove_eos_from_src (bool, optional): remove EOS from the end of src\n        append_eos_to_tgt (bool, optional): append EOS to the end of tgt\n        remove_eos_from_tgt (bool, optional): remove EOS from the end of tgt\n    """"""\n\n    def __init__(\n        self,\n        dataset,\n        eos,\n        append_eos_to_src=False,\n        remove_eos_from_src=False,\n        append_eos_to_tgt=False,\n        remove_eos_from_tgt=False,\n        has_target=True,\n    ):\n        if not isinstance(dataset, FairseqDataset):\n            raise ValueError(\'dataset must be an instance of FairseqDataset\')\n        if append_eos_to_src and remove_eos_from_src:\n            raise ValueError(\'cannot combine append_eos_to_src and remove_eos_from_src\')\n        if append_eos_to_tgt and remove_eos_from_tgt:\n            raise ValueError(\'cannot combine append_eos_to_tgt and remove_eos_from_tgt\')\n\n        self.dataset = dataset\n        self.eos = torch.LongTensor([eos])\n        self.append_eos_to_src = append_eos_to_src\n        self.remove_eos_from_src = remove_eos_from_src\n        self.append_eos_to_tgt = append_eos_to_tgt\n        self.remove_eos_from_tgt = remove_eos_from_tgt\n        self.has_target = has_target\n\n        # precompute how we should adjust the reported sizes\n        self._src_delta = 0\n        self._src_delta += 1 if append_eos_to_src else 0\n        self._src_delta -= 1 if remove_eos_from_src else 0\n        self._tgt_delta = 0\n        self._tgt_delta += 1 if append_eos_to_tgt else 0\n        self._tgt_delta -= 1 if remove_eos_from_tgt else 0\n\n        self._checked_src = False\n        self._checked_tgt = False\n\n    def _check_src(self, src, expect_eos):\n        if not self._checked_src:\n            assert (src[-1] == self.eos[0]) == expect_eos\n            self._checked_src = True\n\n    def _check_tgt(self, tgt, expect_eos):\n        if self.has_target and not self._checked_tgt:\n            assert (tgt[-1] == self.eos[0]) == expect_eos\n            self._checked_tgt = True\n\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n\n        def transform(item):\n            if self.append_eos_to_src:\n                self.eos = self.eos.to(device=item[\'source\'].device)\n                self._check_src(item[\'source\'], expect_eos=False)\n                item[\'source\'] = torch.cat([item[\'source\'], self.eos])\n            if self.remove_eos_from_src:\n                self.eos = self.eos.to(device=item[\'source\'].device)\n                self._check_src(item[\'source\'], expect_eos=True)\n                item[\'source\'] = item[\'source\'][:-1]\n            if self.append_eos_to_tgt:\n                self.eos = self.eos.to(device=item[\'target\'].device)\n                self._check_tgt(item[\'target\'], expect_eos=False)\n                item[\'target\'] = torch.cat([item[\'target\'], self.eos])\n            if self.remove_eos_from_tgt:\n                self.eos = self.eos.to(device=item[\'target\'].device)\n                self._check_tgt(item[\'target\'], expect_eos=True)\n                item[\'target\'] = item[\'target\'][:-1]\n            return item\n\n        samples = list(map(transform, samples))\n        return self.dataset.collater(samples)\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(index)\n\n    def size(self, index):\n        if self.has_target:\n            src_len, tgt_len = self.dataset.size(index)\n            return (src_len + self._src_delta, tgt_len + self._tgt_delta)\n        else:\n            return self.dataset.size(index)\n\n    def ordered_indices(self):\n        # NOTE: we assume that the ordering does not change based on the\n        # addition or removal of eos\n        return self.dataset.ordered_indices()\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, \'supports_prefetch\', False)\n\n    def prefetch(self, indices):\n        return self.dataset.prefetch(indices)\n'"
fairseq/data/transform_eos_lang_pair_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom . import FairseqDataset\nimport torch\nfrom typing import Optional\n\n\nclass TransformEosLangPairDataset(FairseqDataset):\n    """"""A :class:`~fairseq.data.FairseqDataset` wrapper that transform bos on\n    collated samples of language pair dataset.\n\n    Note that the transformation is applied in :func:`collater`.\n\n    Args:\n        dataset (~fairseq.data.FairseqDataset): dataset that collates sample into\n            LanguagePairDataset schema\n        src_eos (int): original source end-of-sentence symbol index to be replaced\n        new_src_eos (int, optional): new end-of-sentence symbol index to replace source eos symbol\n        tgt_bos (int, optional): original target beginning-of-sentence symbol index to be replaced\n        new_tgt_bos (int, optional): new beginning-of-sentence symbol index to replace at the\n            beginning of \'prev_output_tokens\'\n    """"""\n\n    def __init__(\n        self,\n        dataset: FairseqDataset,\n        src_eos: int,\n        new_src_eos: Optional[int] = None,\n        tgt_bos: Optional[int] = None,\n        new_tgt_bos: Optional[int] = None,\n    ):\n        self.dataset = dataset\n        self.src_eos = src_eos\n        self.new_src_eos = new_src_eos\n        self.tgt_bos = tgt_bos\n        self.new_tgt_bos = new_tgt_bos\n\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n        samples = self.dataset.collater(samples)\n\n        if self.new_src_eos is not None:\n            if self.dataset.left_pad_source:\n                assert(samples[\'net_input\'][\'src_tokens\'][:, -1] != self.src_eos).sum() == 0\n                samples[\'net_input\'][\'src_tokens\'][:, -1] = self.new_src_eos\n            else:\n                eos_idx = samples[\'net_input\'][\'src_lengths\'] - 1\n                assert(\n                    samples[\'net_input\'][\'src_tokens\'][torch.arange(eos_idx.size(0)), eos_idx] != self.src_eos\n                ).sum() == 0\n                eos_idx = eos_idx.resize_(len(samples[\'net_input\'][\'src_lengths\']), 1)\n                samples[\'net_input\'][\'src_tokens\'].scatter_(1, eos_idx, self.new_src_eos)\n\n        if self.new_tgt_bos is not None and \'prev_output_tokens\' in samples[\'net_input\']:\n            if self.dataset.left_pad_target:\n                # TODO: support different padding direction on target side\n                raise NotImplementedError(\n                    \'TransformEosLangPairDataset does not implement --left-pad-target True option\'\n                )\n            else:\n                assert (samples[\'net_input\'][\'prev_output_tokens\'][:, 0] != self.tgt_bos).sum() == 0\n                samples[\'net_input\'][\'prev_output_tokens\'][:, 0] = self.new_tgt_bos\n\n        return samples\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(index)\n\n    def size(self, index):\n        return self.dataset.size(index)\n\n    def ordered_indices(self):\n        return self.dataset.ordered_indices()\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, \'supports_prefetch\', False)\n\n    def prefetch(self, indices):\n        return self.dataset.prefetch(indices)\n'"
fairseq/logging/__init__.py,0,b''
fairseq/logging/meters.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport bisect\nfrom collections import OrderedDict\nimport time\nfrom typing import Dict, Optional\n\ntry:\n    import torch\n\n    def type_as(a, b):\n        if torch.is_tensor(a) and torch.is_tensor(b):\n            return a.to(b)\n        else:\n            return a\nexcept ImportError:\n    torch = None\n\n    def type_as(a, b):\n        return a\n\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None\n\n\nclass Meter(object):\n    """"""Base class for Meters.""""""\n\n    def __init__(self):\n        pass\n\n    def state_dict(self):\n        return {}\n\n    def load_state_dict(self, state_dict):\n        pass\n\n    def reset(self):\n        raise NotImplementedError\n\n    @property\n    def smoothed_value(self) -> float:\n        """"""Smoothed value used for logging.""""""\n        raise NotImplementedError\n\n\ndef safe_round(number, ndigits):\n    if hasattr(number, \'__round__\'):\n        return round(number, ndigits)\n    elif torch is not None and torch.is_tensor(number) and number.numel() == 1:\n        return safe_round(number.item(), ndigits)\n    elif np is not None and np.ndim(number) == 0 and hasattr(number, \'item\'):\n        return safe_round(number.item(), ndigits)\n    else:\n        return number\n\n\nclass AverageMeter(Meter):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self, round: Optional[int] = None):\n        self.round = round\n        self.reset()\n\n    def reset(self):\n        self.val = None  # most recent update\n        self.sum = 0  # sum from all updates\n        self.count = 0  # total n from all updates\n\n    def update(self, val, n=1):\n        if val is not None:\n            self.val = val\n            if n > 0:\n                self.sum = type_as(self.sum, val) + (val * n)\n                self.count = type_as(self.count, n) + n\n\n    def state_dict(self):\n        return {\n            \'val\': self.val,\n            \'sum\': self.sum,\n            \'count\': self.count,\n            \'round\': self.round,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.val = state_dict[\'val\']\n        self.sum = state_dict[\'sum\']\n        self.count = state_dict[\'count\']\n        self.round = state_dict.get(\'round\', None)\n\n    @property\n    def avg(self):\n        return self.sum / self.count if self.count > 0 else self.val\n\n    @property\n    def smoothed_value(self) -> float:\n        val = self.avg\n        if self.round is not None and val is not None:\n            val = safe_round(val, self.round)\n        return val\n\n\nclass TimeMeter(Meter):\n    """"""Computes the average occurrence of some event per second""""""\n\n    def __init__(\n        self,\n        init: int = 0,\n        n: int = 0,\n        round: Optional[int] = None,\n    ):\n        self.round = round\n        self.reset(init, n)\n\n    def reset(self, init=0, n=0):\n        self.init = init\n        self.start = time.perf_counter()\n        self.n = n\n        self.i = 0\n\n    def update(self, val=1):\n        self.n = type_as(self.n, val) + val\n        self.i += 1\n\n    def state_dict(self):\n        return {\n            \'init\': self.elapsed_time,\n            \'n\': self.n,\n            \'round\': self.round,\n        }\n\n    def load_state_dict(self, state_dict):\n        if \'start\' in state_dict:\n            # backwards compatibility for old state_dicts\n            self.reset(init=state_dict[\'init\'])\n        else:\n            self.reset(init=state_dict[\'init\'], n=state_dict[\'n\'])\n            self.round = state_dict.get(\'round\', None)\n\n    @property\n    def avg(self):\n        return self.n / self.elapsed_time\n\n    @property\n    def elapsed_time(self):\n        return self.init + (time.perf_counter() - self.start)\n\n    @property\n    def smoothed_value(self) -> float:\n        val = self.avg\n        if self.round is not None and val is not None:\n            val = safe_round(val, self.round)\n        return val\n\n\nclass StopwatchMeter(Meter):\n    """"""Computes the sum/avg duration of some event in seconds""""""\n\n    def __init__(self, round: Optional[int] = None):\n        self.round = round\n        self.sum = 0\n        self.n = 0\n        self.start_time = None\n\n    def start(self):\n        self.start_time = time.perf_counter()\n\n    def stop(self, n=1, prehook=None):\n        if self.start_time is not None:\n            if prehook is not None:\n                prehook()\n            delta = time.perf_counter() - self.start_time\n            self.sum = self.sum + delta\n            self.n = type_as(self.n, n) + n\n\n    def reset(self):\n        self.sum = 0  # cumulative time during which stopwatch was active\n        self.n = 0  # total n across all start/stop\n        self.start()\n\n    def state_dict(self):\n        return {\n            \'sum\': self.sum,\n            \'n\': self.n,\n            \'round\': self.round,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.sum = state_dict[\'sum\']\n        self.n = state_dict[\'n\']\n        self.start_time = None\n        self.round = state_dict.get(\'round\', None)\n\n    @property\n    def avg(self):\n        return self.sum / self.n if self.n > 0 else self.sum\n\n    @property\n    def elapsed_time(self):\n        if self.start_time is None:\n            return 0.\n        return time.perf_counter() - self.start_time\n\n    @property\n    def smoothed_value(self) -> float:\n        val = self.avg if self.sum > 0 else self.elapsed_time\n        if self.round is not None and val is not None:\n            val = safe_round(val, self.round)\n        return val\n\n\nclass MetersDict(OrderedDict):\n    """"""A sorted dictionary of :class:`Meters`.\n\n    Meters are sorted according to a priority that is given when the\n    meter is first added to the dictionary.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.priorities = []\n\n    def __setitem__(self, key, value):\n        assert key not in self, ""MetersDict doesn\'t support reassignment""\n        priority, value = value\n        bisect.insort(self.priorities, (priority, len(self.priorities), key))\n        super().__setitem__(key, value)\n        for _, _, key in self.priorities:  # reorder dict to match priorities\n            self.move_to_end(key)\n\n    def add_meter(self, key, meter, priority):\n        self.__setitem__(key, (priority, meter))\n\n    def state_dict(self):\n        return [\n            (pri, key, self[key].__class__.__name__, self[key].state_dict())\n            for pri, _, key in self.priorities\n            # can\'t serialize DerivedMeter instances\n            if not isinstance(self[key], MetersDict._DerivedMeter)\n        ]\n\n    def load_state_dict(self, state_dict):\n        self.clear()\n        self.priorities.clear()\n        for pri, key, meter_cls, meter_state in state_dict:\n            meter = globals()[meter_cls]()\n            meter.load_state_dict(meter_state)\n            self.add_meter(key, meter, pri)\n\n    def get_smoothed_value(self, key: str) -> float:\n        """"""Get a single smoothed value.""""""\n        meter = self[key]\n        if isinstance(meter, MetersDict._DerivedMeter):\n            return meter.fn(self)\n        else:\n            return meter.smoothed_value\n\n    def get_smoothed_values(self) -> Dict[str, float]:\n        """"""Get all smoothed values.""""""\n        return OrderedDict([\n            (key, self.get_smoothed_value(key))\n            for key in self.keys()\n            if not key.startswith(""_"")\n        ])\n\n    def reset(self):\n        """"""Reset Meter instances.""""""\n        for meter in self.values():\n            if isinstance(meter, MetersDict._DerivedMeter):\n                continue\n            meter.reset()\n\n    class _DerivedMeter(Meter):\n        """"""A Meter whose values are derived from other Meters.""""""\n\n        def __init__(self, fn):\n            self.fn = fn\n\n        def reset(self):\n            pass\n'"
fairseq/logging/metrics.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nA standalone module for aggregating metrics.\n\nMetrics can be logged from anywhere using the `log_*` functions defined\nin this module. The logged values will be aggregated dynamically based\non the aggregation context in which the logging occurs. See the\n:func:`aggregate` context manager for more details.\n""""""\n\nfrom collections import defaultdict, OrderedDict\nimport contextlib\nimport time\nfrom typing import Callable, Dict, List, Optional\nimport uuid\n\nfrom .meters import *\n\n\n# Aggregation contexts are considered ""active"" when inside the scope\n# created by the :func:`aggregate` context manager.\n_aggregators = OrderedDict()\n_active_aggregators = OrderedDict()\n_active_aggregators_cnt = defaultdict(lambda: 0)\n\n\ndef reset() -> None:\n    """"""Reset all metrics aggregators.""""""\n    _aggregators.clear()\n    _active_aggregators.clear()\n    _active_aggregators_cnt.clear()\n\n    # The ""default"" aggregator observes all logged values.\n    _aggregators[""default""] = MetersDict()\n    _active_aggregators[""default""] = _aggregators[""default""]\n    _active_aggregators_cnt[""default""] = 1\n\n\nreset()\n\n\n@contextlib.contextmanager\ndef aggregate(name: Optional[str] = None, new_root: bool = False):\n    """"""Context manager to aggregate metrics under a given name.\n\n    Aggregations can be nested. If *new_root* is ``False``, then logged\n    metrics will be recorded along the entire stack of nested\n    aggregators, including a global ""default"" aggregator. If *new_root*\n    is ``True``, then this aggregator will be the root of a new\n    aggregation stack, thus bypassing any parent aggregators.\n\n    Note that aggregation contexts are uniquely identified by their\n    *name* (e.g., train, valid). Creating a context with an existing\n    name will reuse the corresponding :class:`MetersDict` instance.\n    If no name is given, then a temporary aggregator will be created.\n\n    Usage::\n\n        with metrics.aggregate(""train""):\n            for step, batch in enumerate(epoch):\n                with metrics.aggregate(""train_inner"") as agg:\n                    metrics.log_scalar(""loss"", get_loss(batch))\n                    if step % log_interval == 0:\n                        print(agg.get_smoothed_value(""loss""))\n                        agg.reset()\n        print(metrics.get_smoothed_values(""train"")[""loss""])\n\n    Args:\n        name (str): name of the aggregation. Defaults to a\n            random/temporary name if not given explicitly.\n        new_root (bool): make this aggregation the root of a new\n            aggregation stack.\n    """"""\n    if name is None:\n        # generate a temporary name\n        name = str(uuid.uuid4())\n        assert name not in _aggregators\n        agg = MetersDict()\n    else:\n        assert name != ""default""\n        agg = _aggregators.setdefault(name, MetersDict())\n\n    if new_root:\n        backup_aggregators = _active_aggregators.copy()\n        _active_aggregators.clear()\n        backup_aggregators_cnt = _active_aggregators_cnt.copy()\n        _active_aggregators_cnt.clear()\n\n    _active_aggregators[name] = agg\n    _active_aggregators_cnt[name] += 1\n\n    yield agg\n\n    _active_aggregators_cnt[name] -= 1\n    if _active_aggregators_cnt[name] == 0 and name in _active_aggregators:\n        del _active_aggregators[name]\n\n    if new_root:\n        _active_aggregators.clear()\n        _active_aggregators.update(backup_aggregators)\n        _active_aggregators_cnt.clear()\n        _active_aggregators_cnt.update(backup_aggregators_cnt)\n\n\ndef get_active_aggregators() -> List[MetersDict]:\n    return list(_active_aggregators.values())\n\n\ndef log_scalar(\n    key: str,\n    value: float,\n    weight: float = 1,\n    priority: int = 10,\n    round: Optional[int] = None,\n):\n    """"""Log a scalar value.\n\n    Args:\n        key (str): name of the field to log\n        value (float): value to log\n        weight (float): weight that this value contributes to the average.\n            A weight of 0 will always log the latest value.\n        priority (int): smaller values are logged earlier in the output\n        round (Optional[int]): number of digits to round to when displaying\n    """"""\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, AverageMeter(round=round), priority)\n        agg[key].update(value, weight)\n\n\ndef log_derived(key: str, fn: Callable[[MetersDict], float], priority: int = 20):\n    """"""Log a scalar value derived from other meters.\n\n    Args:\n        key (str): name of the field to log\n        fn (Callable[[MetersDict], float]): function that takes a single\n            argument *meters* and returns the derived value\n        priority (int): smaller values are logged earlier in the output\n    """"""\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, MetersDict._DerivedMeter(fn), priority)\n\n\ndef log_speed(\n    key: str,\n    value: float,\n    priority: int = 30,\n    round: Optional[int] = None,\n):\n    """"""Log the rate of some quantity per second.\n\n    Args:\n        key (str): name of the field to log\n        value (float): value to log\n        priority (int): smaller values are logged earlier in the output\n        round (Optional[int]): number of digits to round to when displaying\n    """"""\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, TimeMeter(round=round), priority)\n            agg[key].reset()  # reset meter on the first call\n        else:\n            agg[key].update(value)\n\n\ndef log_start_time(key: str, priority: int = 40, round: Optional[int] = None):\n    """"""Log the duration of some event in seconds.\n\n    The duration will be computed once :func:`log_stop_time` is called.\n\n    Args:\n        key (str): name of the field to log\n        priority (int): smaller values are logged earlier in the output\n        round (Optional[int]): number of digits to round to when displaying\n    """"""\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, StopwatchMeter(round=round), priority)\n        agg[key].start()\n\n\ndef log_stop_time(key: str, weight: float = 0., prehook=None):\n    """"""Log the duration of some event in seconds.\n\n    The duration will be computed since :func:`log_start_time` was called.\n    Set weight > 0 to report the average time instead of the sum.\n\n    Args:\n        key (str): name of the field to log\n        weight (float): weight that this time contributes to the average\n        prehook (function, no arguments): will be called before the timer\n        is stopped. For example, use prehook=torch.cuda.synchronize to\n        make sure all gpu operations are done before timer is stopped.\n    """"""\n    for agg in get_active_aggregators():\n        if key in agg:\n            agg[key].stop(weight, prehook)\n\n\ndef log_custom(\n    new_meter_fn: Callable[[], Meter],\n    key: str,\n    *args,\n    priority: int = 50,\n    **kwargs,\n):\n    """"""Log using a custom Meter.\n\n    Any extra *args* or *kwargs* will be passed through to the Meter\'s\n    *update* method.\n\n    Args:\n        new_meter_fn (Callable[[], Meter]): function that returns a new\n            Meter instance\n        key (str): name of the field to log\n        priority (int): smaller values are logged earlier in the output\n    """"""\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, new_meter_fn(), priority)\n        agg[key].update(*args, **kwargs)\n\n\ndef reset_meter(name: str, key: str) -> None:\n    """"""Reset Meter instance aggregated under a given *name* and *key*.""""""\n    meter = get_meter(name, key)\n    if meter is not None:\n        meter.reset()\n\n\ndef reset_meters(name: str) -> None:\n    """"""Reset Meter instances aggregated under a given *name*.""""""\n    meters = get_meters(name)\n    if meters is not None:\n        meters.reset()\n\n\ndef get_meter(name: str, key: str) -> Meter:\n    """"""Get a single Meter instance aggregated under *name* and *key*.\n\n    Returns:\n        Meter or None if no metrics have been logged under *name* and *key*.\n    """"""\n    if name not in _aggregators:\n        return None\n    return _aggregators[name].get(key, None)\n\n\ndef get_meters(name: str) -> MetersDict:\n    """"""Get Meter instances aggregated under a given *name*.\n\n    Returns:\n        MetersDict or None if no metrics have been logged under *name*.\n    """"""\n    return _aggregators.get(name, None)\n\n\ndef get_smoothed_value(name: str, key: str) -> float:\n    """"""Get a single smoothed value.\n\n    Raises:\n        KeyError: if no metrics have been logged under *name* and *key*.\n    """"""\n    return _aggregators[name].get_smoothed_value(key)\n\n\ndef get_smoothed_values(name: str) -> Dict[str, float]:\n    """"""Get smoothed values aggregated under a given *name*.\n\n    Raises:\n        KeyError: if no metrics have been logged under *name*.\n    """"""\n    return _aggregators[name].get_smoothed_values()\n\n\ndef state_dict():\n    return OrderedDict([\n        (name, agg.state_dict())\n        for name, agg in _aggregators.items()\n    ])\n\n\ndef load_state_dict(state_dict):\n    for name, agg_state in state_dict.items():\n        _aggregators[name] = MetersDict()\n        _aggregators[name].load_state_dict(agg_state)\n'"
fairseq/logging/progress_bar.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nWrapper around various loggers and progress bars (e.g., tqdm).\n""""""\n\nimport atexit\nimport json\nimport logging\nimport os\nimport sys\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\nfrom numbers import Number\nfrom typing import Optional\n\nimport torch\n\nfrom .meters import AverageMeter, StopwatchMeter, TimeMeter\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef progress_bar(\n    iterator,\n    log_format: Optional[str] = None,\n    log_interval: int = 100,\n    epoch: Optional[int] = None,\n    prefix: Optional[str] = None,\n    tensorboard_logdir: Optional[str] = None,\n    default_log_format: str = \'tqdm\',\n):\n    if log_format is None:\n        log_format = default_log_format\n    if log_format == \'tqdm\' and not sys.stderr.isatty():\n        log_format = \'simple\'\n\n    if log_format == \'json\':\n        bar = JsonProgressBar(iterator, epoch, prefix, log_interval)\n    elif log_format == \'none\':\n        bar = NoopProgressBar(iterator, epoch, prefix)\n    elif log_format == \'simple\':\n        bar = SimpleProgressBar(iterator, epoch, prefix, log_interval)\n    elif log_format == \'tqdm\':\n        bar = TqdmProgressBar(iterator, epoch, prefix)\n    else:\n        raise ValueError(\'Unknown log format: {}\'.format(log_format))\n\n    if tensorboard_logdir:\n        try:\n            # [FB only] custom wrapper for TensorBoard\n            import palaas  # noqa\n            from .fb_tbmf_wrapper import FbTbmfWrapper\n            bar = FbTbmfWrapper(bar, log_interval)\n        except ImportError:\n            bar = TensorboardProgressBarWrapper(bar, tensorboard_logdir)\n\n    return bar\n\n\ndef build_progress_bar(\n    args,\n    iterator,\n    epoch: Optional[int] = None,\n    prefix: Optional[str] = None,\n    default: str = \'tqdm\',\n    no_progress_bar: str = \'none\',\n):\n    """"""Legacy wrapper that takes an argparse.Namespace.""""""\n    if getattr(args, \'no_progress_bar\', False):\n        default = no_progress_bar\n    if getattr(args, \'distributed_rank\', 0) == 0:\n        tensorboard_logdir = getattr(args, \'tensorboard_logdir\', None)\n    else:\n        tensorboard_logdir = None\n    return progress_bar(\n        iterator,\n        log_format=args.log_format,\n        log_interval=args.log_interval,\n        epoch=epoch,\n        prefix=prefix,\n        tensorboard_logdir=tensorboard_logdir,\n        default_log_format=default,\n    )\n\n\ndef format_stat(stat):\n    if isinstance(stat, Number):\n        stat = \'{:g}\'.format(stat)\n    elif isinstance(stat, AverageMeter):\n        stat = \'{:.3f}\'.format(stat.avg)\n    elif isinstance(stat, TimeMeter):\n        stat = \'{:g}\'.format(round(stat.avg))\n    elif isinstance(stat, StopwatchMeter):\n        stat = \'{:g}\'.format(round(stat.sum))\n    elif torch.is_tensor(stat):\n        stat = stat.tolist()\n    return stat\n\n\nclass BaseProgressBar(object):\n    """"""Abstract class for progress bars.""""""\n    def __init__(self, iterable, epoch=None, prefix=None):\n        self.iterable = iterable\n        self.n = getattr(iterable, \'n\', 0)\n        self.epoch = epoch\n        self.prefix = \'\'\n        if epoch is not None:\n            self.prefix += \'epoch {:03d}\'.format(epoch)\n        if prefix is not None:\n            self.prefix += \' | {}\'.format(prefix)\n\n    def __len__(self):\n        return len(self.iterable)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc):\n        return False\n\n    def __iter__(self):\n        raise NotImplementedError\n\n    def log(self, stats, tag=None, step=None):\n        """"""Log intermediate stats according to log_interval.""""""\n        raise NotImplementedError\n\n    def print(self, stats, tag=None, step=None):\n        """"""Print end-of-epoch stats.""""""\n        raise NotImplementedError\n\n    def _str_commas(self, stats):\n        return \', \'.join(key + \'=\' + stats[key].strip()\n                         for key in stats.keys())\n\n    def _str_pipes(self, stats):\n        return \' | \'.join(key + \' \' + stats[key].strip()\n                          for key in stats.keys())\n\n    def _format_stats(self, stats):\n        postfix = OrderedDict(stats)\n        # Preprocess stats according to datatype\n        for key in postfix.keys():\n            postfix[key] = str(format_stat(postfix[key]))\n        return postfix\n\n\n@contextmanager\ndef rename_logger(logger, new_name):\n    old_name = logger.name\n    if new_name is not None:\n        logger.name = new_name\n    yield logger\n    logger.name = old_name\n\n\nclass JsonProgressBar(BaseProgressBar):\n    """"""Log output in JSON format.""""""\n\n    def __init__(self, iterable, epoch=None, prefix=None, log_interval=1000):\n        super().__init__(iterable, epoch, prefix)\n        self.log_interval = log_interval\n        self.i = None\n        self.size = None\n\n    def __iter__(self):\n        self.size = len(self.iterable)\n        for i, obj in enumerate(self.iterable, start=self.n):\n            self.i = i\n            yield obj\n\n    def log(self, stats, tag=None, step=None):\n        """"""Log intermediate stats according to log_interval.""""""\n        step = step or self.i or 0\n        if (\n            step > 0\n            and self.log_interval is not None\n            and step % self.log_interval == 0\n        ):\n            update = (\n                self.epoch - 1 + (self.i + 1) / float(self.size)\n                if self.epoch is not None\n                else None\n            )\n            stats = self._format_stats(stats, epoch=self.epoch, update=update)\n            with rename_logger(logger, tag):\n                logger.info(json.dumps(stats))\n\n    def print(self, stats, tag=None, step=None):\n        """"""Print end-of-epoch stats.""""""\n        self.stats = stats\n        if tag is not None:\n            self.stats = OrderedDict([(tag + \'_\' + k, v) for k, v in self.stats.items()])\n        stats = self._format_stats(self.stats, epoch=self.epoch)\n        with rename_logger(logger, tag):\n            logger.info(json.dumps(stats))\n\n    def _format_stats(self, stats, epoch=None, update=None):\n        postfix = OrderedDict()\n        if epoch is not None:\n            postfix[\'epoch\'] = epoch\n        if update is not None:\n            postfix[\'update\'] = round(update, 3)\n        # Preprocess stats according to datatype\n        for key in stats.keys():\n            postfix[key] = format_stat(stats[key])\n        return postfix\n\n\nclass NoopProgressBar(BaseProgressBar):\n    """"""No logging.""""""\n\n    def __init__(self, iterable, epoch=None, prefix=None):\n        super().__init__(iterable, epoch, prefix)\n\n    def __iter__(self):\n        for obj in self.iterable:\n            yield obj\n\n    def log(self, stats, tag=None, step=None):\n        """"""Log intermediate stats according to log_interval.""""""\n        pass\n\n    def print(self, stats, tag=None, step=None):\n        """"""Print end-of-epoch stats.""""""\n        pass\n\n\nclass SimpleProgressBar(BaseProgressBar):\n    """"""A minimal logger for non-TTY environments.""""""\n\n    def __init__(self, iterable, epoch=None, prefix=None, log_interval=1000):\n        super().__init__(iterable, epoch, prefix)\n        self.log_interval = log_interval\n        self.i = None\n        self.size = None\n\n    def __iter__(self):\n        self.size = len(self.iterable)\n        for i, obj in enumerate(self.iterable, start=self.n):\n            self.i = i\n            yield obj\n\n    def log(self, stats, tag=None, step=None):\n        """"""Log intermediate stats according to log_interval.""""""\n        step = step or self.i or 0\n        if (\n            step > 0\n            and self.log_interval is not None\n            and step % self.log_interval == 0\n        ):\n            stats = self._format_stats(stats)\n            postfix = self._str_commas(stats)\n            with rename_logger(logger, tag):\n                logger.info(\n                    \'{}:  {:5d} / {:d} {}\'\n                    .format(self.prefix, self.i + 1, self.size, postfix)\n                )\n\n    def print(self, stats, tag=None, step=None):\n        """"""Print end-of-epoch stats.""""""\n        postfix = self._str_pipes(self._format_stats(stats))\n        with rename_logger(logger, tag):\n            logger.info(\'{} | {}\'.format(self.prefix, postfix))\n\n\nclass TqdmProgressBar(BaseProgressBar):\n    """"""Log to tqdm.""""""\n\n    def __init__(self, iterable, epoch=None, prefix=None):\n        super().__init__(iterable, epoch, prefix)\n        from tqdm import tqdm\n        self.tqdm = tqdm(iterable, self.prefix, leave=False)\n\n    def __iter__(self):\n        return iter(self.tqdm)\n\n    def log(self, stats, tag=None, step=None):\n        """"""Log intermediate stats according to log_interval.""""""\n        self.tqdm.set_postfix(self._format_stats(stats), refresh=False)\n\n    def print(self, stats, tag=None, step=None):\n        """"""Print end-of-epoch stats.""""""\n        postfix = self._str_pipes(self._format_stats(stats))\n        self.tqdm.write(\'{} | {}\'.format(self.tqdm.desc, postfix))\n\n\ntry:\n    _tensorboard_writers = {}\n    from tensorboardX import SummaryWriter\nexcept ImportError:\n    SummaryWriter = None\n\n\ndef _close_writers():\n    for w in _tensorboard_writers.values():\n        w.close()\n\n\natexit.register(_close_writers)\n\n\nclass TensorboardProgressBarWrapper(BaseProgressBar):\n    """"""Log to tensorboard.""""""\n\n    def __init__(self, wrapped_bar, tensorboard_logdir):\n        self.wrapped_bar = wrapped_bar\n        self.tensorboard_logdir = tensorboard_logdir\n\n        if SummaryWriter is None:\n            logger.warning(\n                ""tensorboard not found, please install with: pip install tensorboardX""\n            )\n\n    def _writer(self, key):\n        if SummaryWriter is None:\n            return None\n        _writers = _tensorboard_writers\n        if key not in _writers:\n            _writers[key] = SummaryWriter(os.path.join(self.tensorboard_logdir, key))\n            _writers[key].add_text(\'sys.argv\', "" "".join(sys.argv))\n        return _writers[key]\n\n    def __iter__(self):\n        return iter(self.wrapped_bar)\n\n    def log(self, stats, tag=None, step=None):\n        """"""Log intermediate stats to tensorboard.""""""\n        self._log_to_tensorboard(stats, tag, step)\n        self.wrapped_bar.log(stats, tag=tag, step=step)\n\n    def print(self, stats, tag=None, step=None):\n        """"""Print end-of-epoch stats.""""""\n        self._log_to_tensorboard(stats, tag, step)\n        self.wrapped_bar.print(stats, tag=tag, step=step)\n\n    def _log_to_tensorboard(self, stats, tag=None, step=None):\n        writer = self._writer(tag or \'\')\n        if writer is None:\n            return\n        if step is None:\n            step = stats[\'num_updates\']\n        for key in stats.keys() - {\'num_updates\'}:\n            if isinstance(stats[key], AverageMeter):\n                writer.add_scalar(key, stats[key].val, step)\n            elif isinstance(stats[key], Number):\n                writer.add_scalar(key, stats[key], step)\n        writer.flush()\n'"
fairseq/model_parallel/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import criterions, modules, models  # noqa\n'"
fairseq/model_parallel/megatron_trainer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nTrain a network across multiple GPUs.\n""""""\n\nfrom fairseq import distributed_utils\nfrom fairseq.trainer import Trainer\n\ntry:\n    from fairseq.model_parallel.megatron.mpu import (\n        get_data_parallel_group,\n        get_data_parallel_rank,\n        get_data_parallel_world_size,\n        get_model_parallel_group,\n        get_model_parallel_src_rank,\n    )\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\n\nclass MegatronTrainer(Trainer):\n    """"""Main class for model parallel with data parallel training.\n    """"""\n    def __init__(self, args, task, model, criterion):\n        if not has_megatron_submodule:\n            raise ImportError(\n                \'\\n\\nPlease install the megatron submodule:\'\n                \'\\n\\n  git submodule update --init \'\n                \'fairseq/model_parallel/megatron\'\n            )\n        super().__init__(args, task, model, criterion)\n\n    @property\n    def data_parallel_world_size(self):\n        return get_data_parallel_world_size()\n\n    @property\n    def data_parallel_process_group(self):\n        return get_data_parallel_group()\n\n    @property\n    def data_parallel_rank(self):\n        return get_data_parallel_rank()\n\n    @property\n    def is_data_parallel_master(self):\n        return get_model_parallel_src_rank() == 0\n\n    def clip_grad_norm(self, clip_norm):\n        def _aggregate_model_parallel_grad_norm(total_norm):\n            total_norm = total_norm ** 2\n            distributed_utils.all_reduce(total_norm, group=get_model_parallel_group())\n            total_norm = total_norm ** 0.5\n            return total_norm\n        return self.optimizer.clip_grad_norm(\n            clip_norm,\n            aggregate_norm_fn=_aggregate_model_parallel_grad_norm,\n        )\n'"
fairseq/models/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport importlib\nimport os\n\nfrom .fairseq_decoder import FairseqDecoder\nfrom .fairseq_encoder import FairseqEncoder\nfrom .fairseq_incremental_decoder import FairseqIncrementalDecoder\nfrom .fairseq_model import (\n    BaseFairseqModel,\n    FairseqEncoderModel,\n    FairseqEncoderDecoderModel,\n    FairseqLanguageModel,\n    FairseqModel,\n    FairseqMultiModel,\n)\n\nfrom .composite_encoder import CompositeEncoder\nfrom .distributed_fairseq_model import DistributedFairseqModel\n\n\nMODEL_REGISTRY = {}\nARCH_MODEL_REGISTRY = {}\nARCH_MODEL_INV_REGISTRY = {}\nARCH_CONFIG_REGISTRY = {}\n\n\n__all__ = [\n    \'BaseFairseqModel\',\n    \'CompositeEncoder\',\n    \'DistributedFairseqModel\',\n    \'FairseqDecoder\',\n    \'FairseqEncoder\',\n    \'FairseqEncoderDecoderModel\',\n    \'FairseqEncoderModel\',\n    \'FairseqIncrementalDecoder\',\n    \'FairseqLanguageModel\',\n    \'FairseqModel\',\n    \'FairseqMultiModel\',\n]\n\n\ndef build_model(args, task):\n    return ARCH_MODEL_REGISTRY[args.arch].build_model(args, task)\n\n\ndef register_model(name):\n    """"""\n    New model types can be added to fairseq with the :func:`register_model`\n    function decorator.\n\n    For example::\n\n        @register_model(\'lstm\')\n        class LSTM(FairseqEncoderDecoderModel):\n            (...)\n\n    .. note:: All models must implement the :class:`BaseFairseqModel` interface.\n        Typically you will extend :class:`FairseqEncoderDecoderModel` for\n        sequence-to-sequence tasks or :class:`FairseqLanguageModel` for\n        language modeling tasks.\n\n    Args:\n        name (str): the name of the model\n    """"""\n\n    def register_model_cls(cls):\n        if name in MODEL_REGISTRY:\n            raise ValueError(\'Cannot register duplicate model ({})\'.format(name))\n        if not issubclass(cls, BaseFairseqModel):\n            raise ValueError(\'Model ({}: {}) must extend BaseFairseqModel\'.format(name, cls.__name__))\n        MODEL_REGISTRY[name] = cls\n        return cls\n\n    return register_model_cls\n\n\ndef register_model_architecture(model_name, arch_name):\n    """"""\n    New model architectures can be added to fairseq with the\n    :func:`register_model_architecture` function decorator. After registration,\n    model architectures can be selected with the ``--arch`` command-line\n    argument.\n\n    For example::\n\n        @register_model_architecture(\'lstm\', \'lstm_luong_wmt_en_de\')\n        def lstm_luong_wmt_en_de(args):\n            args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1000)\n            (...)\n\n    The decorated function should take a single argument *args*, which is a\n    :class:`argparse.Namespace` of arguments parsed from the command-line. The\n    decorated function should modify these arguments in-place to match the\n    desired architecture.\n\n    Args:\n        model_name (str): the name of the Model (Model must already be\n            registered)\n        arch_name (str): the name of the model architecture (``--arch``)\n    """"""\n\n    def register_model_arch_fn(fn):\n        if model_name not in MODEL_REGISTRY:\n            raise ValueError(\'Cannot register model architecture for unknown model type ({})\'.format(model_name))\n        if arch_name in ARCH_MODEL_REGISTRY:\n            raise ValueError(\'Cannot register duplicate model architecture ({})\'.format(arch_name))\n        if not callable(fn):\n            raise ValueError(\'Model architecture must be callable ({})\'.format(arch_name))\n        ARCH_MODEL_REGISTRY[arch_name] = MODEL_REGISTRY[model_name]\n        ARCH_MODEL_INV_REGISTRY.setdefault(model_name, []).append(arch_name)\n        ARCH_CONFIG_REGISTRY[arch_name] = fn\n        return fn\n\n    return register_model_arch_fn\n\n\n# automatically import any Python files in the models/ directory\nmodels_dir = os.path.dirname(__file__)\nfor file in os.listdir(models_dir):\n    path = os.path.join(models_dir, file)\n    if (\n        not file.startswith(\'_\')\n        and not file.startswith(\'.\')\n        and (file.endswith(\'.py\') or os.path.isdir(path))\n    ):\n        model_name = file[:file.find(\'.py\')] if file.endswith(\'.py\') else file\n        module = importlib.import_module(\'fairseq.models.\' + model_name)\n\n        # extra `model_parser` for sphinx\n        if model_name in MODEL_REGISTRY:\n            parser = argparse.ArgumentParser(add_help=False)\n            group_archs = parser.add_argument_group(\'Named architectures\')\n            group_archs.add_argument(\'--arch\', choices=ARCH_MODEL_INV_REGISTRY[model_name])\n            group_args = parser.add_argument_group(\'Additional command-line arguments\')\n            MODEL_REGISTRY[model_name].add_args(group_args)\n            globals()[model_name + \'_parser\'] = parser\n'"
fairseq/models/composite_encoder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.models import FairseqEncoder\n\n\nclass CompositeEncoder(FairseqEncoder):\n    """"""\n    A wrapper around a dictionary of :class:`FairseqEncoder` objects.\n\n    We run forward on each encoder and return a dictionary of outputs. The first\n    encoder\'s dictionary is used for initialization.\n\n    Args:\n        encoders (dict): a dictionary of :class:`FairseqEncoder` objects.\n    """"""\n\n    def __init__(self, encoders):\n        super().__init__(next(iter(encoders.values())).dictionary)\n        self.encoders = encoders\n        for key in self.encoders:\n            self.add_module(key, self.encoders[key])\n\n    def forward(self, src_tokens, src_lengths):\n        """"""\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n\n        Returns:\n            dict:\n                the outputs from each Encoder\n        """"""\n        encoder_out = {}\n        for key in self.encoders:\n            encoder_out[key] = self.encoders[key](src_tokens, src_lengths)\n        return encoder_out\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""Reorder encoder output according to new_order.""""""\n        for key in self.encoders:\n            encoder_out[key] = self.encoders[key].reorder_encoder_out(encoder_out[key], new_order)\n        return encoder_out\n\n    def max_positions(self):\n        return min(self.encoders[key].max_positions() for key in self.encoders)\n\n    def upgrade_state_dict(self, state_dict):\n        for key in self.encoders:\n            self.encoders[key].upgrade_state_dict(state_dict)\n        return state_dict\n'"
fairseq/models/distributed_fairseq_model.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport inspect\n\nimport torch.nn as nn\n\nfrom fairseq.legacy_distributed_data_parallel import LegacyDistributedDataParallel\nfrom fairseq.models import BaseFairseqModel\n\n\n_GOSSIP_DISABLED = False\ntry:\n    import gossip\nexcept ImportError:\n    _GOSSIP_DISABLED = True\n\n\ndef DistributedFairseqModel(args, model, process_group=None):\n    """"""\n    Wrap a *model* to support distributed data parallel training.\n\n    This is similar to the built-in DistributedDataParallel, but allows\n    additional configuration of the DistributedDataParallel class to\n    use, and also provides easier access to the wrapped model by\n    forwarding requests for missing attributes to the wrapped model.\n\n    Args:\n        args (argparse.Namespace): fairseq args\n        model (BaseFairseqModel): model to wrap\n    """"""\n    # determine which DDP class to extend\n    assert isinstance(model, nn.Module)\n    if args.distributed_wrapper == \'DDP\' and args.ddp_backend == \'c10d\':\n        ddp_class = nn.parallel.DistributedDataParallel\n        init_kwargs = dict(\n            module=model,\n            device_ids=[args.device_id],\n            output_device=args.device_id,\n            broadcast_buffers=args.broadcast_buffers,\n            bucket_cap_mb=args.bucket_cap_mb,\n            process_group=process_group,\n        )\n        # Maintain backward compatibility\n        if \'check_reduction\' in inspect.getargspec(ddp_class)[0]:\n            init_kwargs[\'check_reduction\'] = True\n        if \'find_unused_parameters\' in inspect.getargspec(ddp_class)[0]:\n            init_kwargs[\'find_unused_parameters\'] = args.find_unused_parameters\n    elif args.distributed_wrapper == \'DDP\' and args.ddp_backend == \'no_c10d\':\n        ddp_class = LegacyDistributedDataParallel\n        init_kwargs = dict(\n            module=model,\n            world_size=args.distributed_world_size,\n            buffer_size=2**28,\n            process_group=process_group,\n        )\n    elif args.distributed_wrapper == \'SlowMo\':\n        if _GOSSIP_DISABLED:\n            raise ImportError(\n                \'Cannot find gossip library. Please install from: \'\n                \'github.com/facebookresearch/stochastic_gradient_push\'\n            )\n        ddp_class = gossip.GossipDataParallel\n\n        # The values of slowmo_momentum below were obtained by tuning on the\n        # En-De 16 dataset by training the transformer_wmt_en_de_large model\n        if args.slowmo_momentum is None:\n            if args.distributed_world_size <= 16:\n                args.slowmo_momentum = 0.0\n            elif args.distributed_world_size <= 32:\n                args.slowmo_momentum = 0.2\n            elif args.distributed_world_size <= 64:\n                args.slowmo_momentum = 0.5\n            else:\n                args.slowmo_momentum = 0.6\n\n        init_kwargs = dict(\n            module=model,\n            device_ids=[args.device_id],\n            output_device=args.device_id,\n            broadcast_buffers=args.broadcast_buffers,\n            nprocs_per_node=args.nprocs_per_node,\n            slowmo_momentum=args.slowmo_momentum,\n            localsgd=(args.slowmo_algorithm == \'LocalSGD\'),\n            localsgd_frequency=args.localsgd_frequency\n        )\n    else:\n        raise ValueError(\'Unknown --ddp-backend: \' + args.ddp_backend)\n\n    class _DistributedFairseqModel(ddp_class):\n        """"""Extend DistributedDataParallel to check for missing\n        attributes in the wrapped module.""""""\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n\n        def __getattr__(self, name):\n            wrapped_module = super().__getattr__(\'module\')\n            if hasattr(wrapped_module, name):\n                return getattr(wrapped_module, name)\n            return super().__getattr__(name)\n\n    return _DistributedFairseqModel(**init_kwargs)\n'"
fairseq/models/fairseq_decoder.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch.nn as nn\nfrom fairseq import utils\nfrom torch import Tensor\n\n\nclass FairseqDecoder(nn.Module):\n    """"""Base class for decoders.""""""\n\n    def __init__(self, dictionary):\n        super().__init__()\n        self.dictionary = dictionary\n        self.onnx_trace = False\n\n    def forward(self, prev_output_tokens, encoder_out=None, **kwargs):\n        """"""\n        Args:\n            prev_output_tokens (LongTensor): shifted output tokens of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (dict, optional): output from the encoder, used for\n                encoder-side attention\n\n        Returns:\n            tuple:\n                - the decoder\'s output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        """"""\n        x, extra = self.extract_features(\n            prev_output_tokens, encoder_out=encoder_out, **kwargs\n        )\n        x = self.output_layer(x)\n        return x, extra\n\n    def extract_features(self, prev_output_tokens, encoder_out=None, **kwargs):\n        """"""\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        """"""\n        raise NotImplementedError\n\n    def output_layer(self, features, **kwargs):\n        """"""\n        Project features to the default output size, e.g., vocabulary size.\n\n        Args:\n            features (Tensor): features returned by *extract_features*.\n        """"""\n        raise NotImplementedError\n\n    def get_normalized_probs(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n        """"""Get normalized probabilities (or log probs) from a net\'s output.""""""\n\n        if hasattr(self, ""adaptive_softmax"") and self.adaptive_softmax is not None:\n            if sample is not None:\n                assert ""target"" in sample\n                target = sample[""target""]\n            else:\n                target = None\n            out = self.adaptive_softmax.get_log_prob(net_output[0], target=target)\n            return out.exp_() if not log_probs else out\n\n        logits = net_output[0]\n        if log_probs:\n            return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)\n        else:\n            return utils.softmax(logits, dim=-1, onnx_trace=self.onnx_trace)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the decoder.""""""\n        return 1e6  # an arbitrary large number\n\n    def upgrade_state_dict(self, state_dict):\n        """"""Upgrade a (possibly old) state dict for new versions of fairseq.""""""\n        return state_dict\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n'"
fairseq/models/fairseq_encoder.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, List, NamedTuple, Optional\nfrom torch import Tensor\n\nEncoderOut = NamedTuple(\n    ""EncoderOut"",\n    [\n        (""encoder_out"", Tensor),  # T x B x C\n        (""encoder_padding_mask"", Tensor),  # B x T\n        (""encoder_embedding"", Tensor),  # B x T x C\n        (""encoder_states"", Optional[List[Tensor]]),  # List[T x B x C]\n        (""src_tokens"", Optional[Tensor]),  # B x T\n        (""src_lengths"", Optional[Tensor]),  # B x 1\n    ],\n)\n\n\nclass FairseqEncoder(nn.Module):\n    """"""Base class for encoders.""""""\n\n    def __init__(self, dictionary):\n        super().__init__()\n        self.dictionary = dictionary\n\n    def forward(self, src_tokens, src_lengths=None, **kwargs):\n        """"""\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n        """"""\n        raise NotImplementedError\n\n    def forward_torchscript(self, net_input: Dict[str, Tensor]):\n        """"""A TorchScript-compatible version of forward.\n\n        Encoders which use additional arguments may want to override\n        this method for TorchScript compatibility.\n        """"""\n        if torch.jit.is_scripting():\n            return self.forward(\n                src_tokens=net_input[""src_tokens""],\n                src_lengths=net_input[""src_lengths""],\n            )\n        else:\n            return self.forward_non_torchscript(net_input)\n\n    @torch.jit.unused\n    def forward_non_torchscript(self, net_input: Dict[str, Tensor]):\n        encoder_input = {\n            k: v\n            for k, v in net_input.items()\n            if k != ""prev_output_tokens""\n        }\n        return self.forward(**encoder_input)\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""\n        Reorder encoder output according to `new_order`.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            `encoder_out` rearranged according to `new_order`\n        """"""\n        raise NotImplementedError\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return 1e6  # an arbitrary large number\n\n    def upgrade_state_dict(self, state_dict):\n        """"""Upgrade a (possibly old) state dict for new versions of fairseq.""""""\n        return state_dict\n'"
fairseq/models/fairseq_incremental_decoder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nfrom typing import Dict, Optional\n\nfrom fairseq.models import FairseqDecoder\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom torch import Tensor\n\n\n@with_incremental_state\nclass FairseqIncrementalDecoder(FairseqDecoder):\n    """"""Base class for incremental decoders.\n\n    Incremental decoding is a special mode at inference time where the Model\n    only receives a single timestep of input corresponding to the previous\n    output token (for teacher forcing) and must produce the next output\n    *incrementally*. Thus the model must cache any long-term state that is\n    needed about the sequence, e.g., hidden states, convolutional states, etc.\n\n    Compared to the standard :class:`FairseqDecoder` interface, the incremental\n    decoder interface allows :func:`forward` functions to take an extra keyword\n    argument (*incremental_state*) that can be used to cache state across\n    time-steps.\n\n    The :class:`FairseqIncrementalDecoder` interface also defines the\n    :func:`reorder_incremental_state` method, which is used during beam search\n    to select and reorder the incremental state based on the selection of beams.\n\n    To learn more about how incremental decoding works, refer to `this blog\n    <http://www.telesens.co/2019/04/21/understanding-incremental-decoding-in-fairseq/>`_.\n    """"""\n\n    def __init__(self, dictionary):\n        super().__init__(dictionary)\n\n    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):\n        """"""\n        Args:\n            prev_output_tokens (LongTensor): shifted output tokens of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (dict, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict, optional): dictionary used for storing\n                state during :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the decoder\'s output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        """"""\n        raise NotImplementedError\n\n    def extract_features(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):\n        """"""\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        """"""\n        raise NotImplementedError\n\n    def reorder_incremental_state(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        new_order: Tensor,\n    ):\n        """"""Reorder incremental state.\n\n        This should be called when the order of the input has changed from the\n        previous time step. A typical use case is beam search, where the input\n        order changes between time steps based on the selection of beams.\n        """"""\n        seen: Dict[int, Optional[Tensor]] = {}\n        for _, module in self.named_modules():\n            if hasattr(module, \'reorder_incremental_state\'):\n                if id(module) not in seen and module is not self:\n                    seen[id(module)] = None\n                    result = module.reorder_incremental_state(incremental_state, new_order)\n                    if result is not None:\n                        incremental_state = result\n\n    def set_beam_size(self, beam_size):\n        """"""Sets the beam size in the decoder and all children.""""""\n        if getattr(self, \'_beam_size\', -1) != beam_size:\n            seen = set()\n\n            def apply_set_beam_size(module):\n                if module != self and hasattr(module, \'set_beam_size\') \\\n                        and module not in seen:\n                    seen.add(module)\n                    module.set_beam_size(beam_size)\n\n            self.apply(apply_set_beam_size)\n            self._beam_size = beam_size\n'"
fairseq/models/fairseq_model.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nBase classes for various fairseq models.\n""""""\n\nimport logging\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.checkpoint_utils import prune_state_dict\nfrom fairseq.data import Dictionary\nfrom fairseq.models import FairseqDecoder, FairseqEncoder\nfrom torch import Tensor\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseFairseqModel(nn.Module):\n    """"""Base class for fairseq models.""""""\n\n    def __init__(self):\n        super().__init__()\n        self._is_generation_fast = False\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        pass\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        raise NotImplementedError(""Model must implement the build_model method"")\n\n    def get_targets(self, sample, net_output):\n        """"""Get targets from either the sample or the net\'s output.""""""\n        return sample[""target""]\n\n    def get_normalized_probs(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n        """"""Get normalized probabilities (or log probs) from a net\'s output.""""""\n        return self.get_normalized_probs_scriptable(net_output, log_probs, sample)\n\n    # TorchScript doesn\'t support super() method so that the scriptable Subclass\n    # can\'t access the base class model in Torchscript.\n    # Current workaround is to add a helper function with different name and\n    # call the helper function from scriptable Subclass.\n    def get_normalized_probs_scriptable(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n        """"""Scriptable helper function for get_normalized_probs in ~BaseFairseqModel""""""\n        if hasattr(self, ""decoder""):\n            return self.decoder.get_normalized_probs(net_output, log_probs, sample)\n        elif torch.is_tensor(net_output):\n            logits = net_output.float()\n            if log_probs:\n                return F.log_softmax(logits, dim=-1)\n            else:\n                return F.softmax(logits, dim=-1)\n        raise NotImplementedError\n\n    def extract_features(self, *args, **kwargs):\n        """"""Similar to *forward* but only return features.""""""\n        return self(*args, **kwargs)\n\n    def max_positions(self):\n        """"""Maximum length supported by the model.""""""\n        return None\n\n    def load_state_dict(self, state_dict, strict=True, args=None):\n        """"""Copies parameters and buffers from *state_dict* into this module and\n        its descendants.\n\n        Overrides the method in :class:`nn.Module`. Compared with that method\n        this additionally ""upgrades"" *state_dicts* from old checkpoints.\n        """"""\n        self.upgrade_state_dict(state_dict)\n        new_state_dict = prune_state_dict(state_dict, args)\n        return super().load_state_dict(new_state_dict, strict)\n\n    def upgrade_state_dict(self, state_dict):\n        """"""Upgrade old state dicts to work with newer code.""""""\n        self.upgrade_state_dict_named(state_dict, """")\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        """"""Upgrade old state dicts to work with newer code.\n\n        Args:\n            state_dict (dict): state dictionary to upgrade, in place\n            name (str): the state dict key corresponding to the current module\n        """"""\n        assert state_dict is not None\n\n        def do_upgrade(m, prefix):\n            if len(prefix) > 0:\n                prefix += "".""\n\n            for n, c in m.named_children():\n                name = prefix + n\n                if hasattr(c, ""upgrade_state_dict_named""):\n                    c.upgrade_state_dict_named(state_dict, name)\n                elif hasattr(c, ""upgrade_state_dict""):\n                    c.upgrade_state_dict(state_dict)\n                do_upgrade(c, name)\n\n        do_upgrade(self, name)\n\n    def set_num_updates(self, num_updates):\n        """""" State from trainer to pass along to model at every update """"""\n\n        def _apply(m):\n            if hasattr(m, \'set_num_updates\') and m != self:\n                m.set_num_updates(num_updates)\n        self.apply(_apply)\n\n\n    def make_generation_fast_(self, **kwargs):\n        """"""Optimize model for faster generation.""""""\n        if self._is_generation_fast:\n            return  # only apply once\n        self._is_generation_fast = True\n\n        # remove weight norm from all modules in the network\n        def apply_remove_weight_norm(module):\n            try:\n                nn.utils.remove_weight_norm(module)\n            except ValueError:  # this module didn\'t have weight norm\n                return\n\n        self.apply(apply_remove_weight_norm)\n\n        seen = set()\n\n        def apply_make_generation_fast_(module):\n            if (\n                module != self\n                and hasattr(module, ""make_generation_fast_"")\n                and module not in seen\n            ):\n                seen.add(module)\n                module.make_generation_fast_(**kwargs)\n\n        self.apply(apply_make_generation_fast_)\n\n        def train(mode=True):\n            if mode:\n                raise RuntimeError(""cannot train after make_generation_fast"")\n\n        # this model should no longer be used for training\n        self.eval()\n        self.train = train\n\n    def prepare_for_onnx_export_(self, **kwargs):\n        """"""Make model exportable via ONNX trace.""""""\n        seen = set()\n\n        def apply_prepare_for_onnx_export_(module):\n            if (\n                module != self\n                and hasattr(module, ""prepare_for_onnx_export_"")\n                and module not in seen\n            ):\n                seen.add(module)\n                module.prepare_for_onnx_export_(**kwargs)\n\n        self.apply(apply_prepare_for_onnx_export_)\n\n    def prepare_for_tpu_(self, **kwargs):\n        """"""Optionally modify model for use on TPUs.""""""\n        seen = set()\n\n        def apply_prepare_for_tpu_(module):\n            if (\n                module != self\n                and hasattr(module, ""prepare_for_tpu_"")\n                and module not in seen\n            ):\n                seen.add(module)\n                module.prepare_for_tpu_(**kwargs)\n\n        self.apply(apply_prepare_for_tpu_)\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_name_or_path,\n        checkpoint_file=""model.pt"",\n        data_name_or_path=""."",\n        **kwargs,\n    ):\n        """"""\n        Load a :class:`~fairseq.models.FairseqModel` from a pre-trained model\n        file. Downloads and caches the pre-trained model file if needed.\n\n        The base implementation returns a\n        :class:`~fairseq.hub_utils.GeneratorHubInterface`, which can be used to\n        generate translations or sample from language models. The underlying\n        :class:`~fairseq.models.FairseqModel` can be accessed via the\n        *generator.models* attribute.\n\n        Other models may override this to implement custom hub interfaces.\n\n        Args:\n            model_name_or_path (str): either the name of a pre-trained model to\n                load or a path/URL to a pre-trained model state dict\n            checkpoint_file (str, optional): colon-separated list of checkpoint\n                files in the model archive to ensemble (default: \'model.pt\')\n            data_name_or_path (str, optional): point args.data to the archive\n                at the given path/URL. Can start with \'.\' or \'./\' to reuse the\n                model archive path.\n        """"""\n        from fairseq import hub_utils\n\n        x = hub_utils.from_pretrained(\n            model_name_or_path,\n            checkpoint_file,\n            data_name_or_path,\n            archive_map=cls.hub_models(),\n            **kwargs,\n        )\n        logger.info(x[""args""])\n        return hub_utils.GeneratorHubInterface(x[""args""], x[""task""], x[""models""])\n\n    @classmethod\n    def hub_models(cls):\n        return {}\n\n\nclass FairseqEncoderDecoderModel(BaseFairseqModel):\n    """"""Base class for encoder-decoder models.\n\n    Args:\n        encoder (FairseqEncoder): the encoder\n        decoder (FairseqDecoder): the decoder\n    """"""\n\n    def __init__(self, encoder, decoder):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        assert isinstance(self.encoder, FairseqEncoder)\n        assert isinstance(self.decoder, FairseqDecoder)\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n        """"""\n        Run the forward pass for an encoder-decoder model.\n\n        First feed a batch of source tokens through the encoder. Then, feed the\n        encoder output and previous decoder outputs (i.e., teacher forcing) to\n        the decoder to produce the next outputs::\n\n            encoder_out = self.encoder(src_tokens, src_lengths)\n            return self.decoder(prev_output_tokens, encoder_out)\n\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n\n        Returns:\n            tuple:\n                - the decoder\'s output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        """"""\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n        decoder_out = self.decoder(\n            prev_output_tokens, encoder_out=encoder_out, **kwargs\n        )\n        return decoder_out\n\n    def forward_decoder(self, prev_output_tokens, **kwargs):\n        return self.decoder(prev_output_tokens, **kwargs)\n\n    def extract_features(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n        """"""\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        """"""\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n        features = self.decoder.extract_features(\n            prev_output_tokens, encoder_out=encoder_out, **kwargs\n        )\n        return features\n\n    def output_layer(self, features, **kwargs):\n        """"""Project features to the default output size (typically vocabulary size).""""""\n        return self.decoder.output_layer(features, **kwargs)\n\n    def max_positions(self):\n        """"""Maximum length supported by the model.""""""\n        return (self.encoder.max_positions(), self.decoder.max_positions())\n\n    def max_decoder_positions(self):\n        """"""Maximum length supported by the decoder.""""""\n        return self.decoder.max_positions()\n\n\nclass FairseqModel(FairseqEncoderDecoderModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        utils.deprecation_warning(\n            ""FairseqModel is deprecated, please use FairseqEncoderDecoderModel ""\n            ""or BaseFairseqModel instead"",\n            stacklevel=4,\n        )\n\n\nclass FairseqMultiModel(BaseFairseqModel):\n    """"""Base class for combining multiple encoder-decoder models.""""""\n\n    def __init__(self, encoders, decoders):\n        super().__init__()\n        assert encoders.keys() == decoders.keys()\n        self.keys = list(encoders.keys())\n        for key in self.keys:\n            assert isinstance(encoders[key], FairseqEncoder)\n            assert isinstance(decoders[key], FairseqDecoder)\n\n        self.models = nn.ModuleDict(\n            {\n                key: FairseqEncoderDecoderModel(encoders[key], decoders[key])\n                for key in self.keys\n            }\n        )\n\n    @staticmethod\n    def build_shared_embeddings(\n        dicts: Dict[str, Dictionary],\n        langs: List[str],\n        embed_dim: int,\n        build_embedding: callable,\n        pretrained_embed_path: Optional[str] = None,\n    ):\n        """"""\n        Helper function to build shared embeddings for a set of languages after\n        checking that all dicts corresponding to those languages are equivalent.\n\n        Args:\n            dicts: Dict of lang_id to its corresponding Dictionary\n            langs: languages that we want to share embeddings for\n            embed_dim: embedding dimension\n            build_embedding: callable function to actually build the embedding\n            pretrained_embed_path: Optional path to load pretrained embeddings\n        """"""\n        shared_dict = dicts[langs[0]]\n        if any(dicts[lang] != shared_dict for lang in langs):\n            raise ValueError(\n                ""--share-*-embeddings requires a joined dictionary: ""\n                ""--share-encoder-embeddings requires a joined source ""\n                ""dictionary, --share-decoder-embeddings requires a joined ""\n                ""target dictionary, and --share-all-embeddings requires a ""\n                ""joint source + target dictionary.""\n            )\n        return build_embedding(shared_dict, embed_dim, pretrained_embed_path)\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n        raise NotImplementedError\n\n    def max_positions(self):\n        """"""Maximum length supported by the model.""""""\n        return {\n            key: (\n                self.models[key].encoder.max_positions(),\n                self.models[key].decoder.max_positions(),\n            )\n            for key in self.keys\n        }\n\n    def max_decoder_positions(self):\n        """"""Maximum length supported by the decoder.""""""\n        return min(model.decoder.max_positions() for model in self.models.values())\n\n    @property\n    def encoder(self):\n        return self.models[self.keys[0]].encoder\n\n    @property\n    def decoder(self):\n        return self.models[self.keys[0]].decoder\n\n    def forward_decoder(self, prev_output_tokens, **kwargs):\n        return self.decoder(prev_output_tokens, **kwargs)\n\n    def load_state_dict(self, state_dict, strict=True, args=None):\n        """"""Copies parameters and buffers from *state_dict* into this module and\n        its descendants.\n\n        Overrides the method in :class:`nn.Module`. Compared with that method\n        this additionally ""upgrades"" *state_dicts* from old checkpoints.\n        """"""\n        self.upgrade_state_dict(state_dict)\n        new_state_dict = prune_state_dict(state_dict, args)\n        return super().load_state_dict(new_state_dict, strict)\n\n\nclass FairseqLanguageModel(BaseFairseqModel):\n    """"""Base class for decoder-only models.\n\n    Args:\n        decoder (FairseqDecoder): the decoder\n    """"""\n\n    def __init__(self, decoder):\n        super().__init__()\n        self.decoder = decoder\n        assert isinstance(self.decoder, FairseqDecoder)\n\n    def forward(self, src_tokens, **kwargs):\n        """"""\n        Run the forward pass for a decoder-only model.\n\n        Feeds a batch of tokens through the decoder to predict the next tokens.\n\n        Args:\n            src_tokens (LongTensor): tokens on which to condition the decoder,\n                of shape `(batch, tgt_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n\n        Returns:\n            tuple:\n                - the decoder\'s output of shape `(batch, seq_len, vocab)`\n                - a dictionary with any model-specific outputs\n        """"""\n        return self.decoder(src_tokens, **kwargs)\n\n    def forward_decoder(self, prev_output_tokens, **kwargs):\n        return self.decoder(prev_output_tokens, **kwargs)\n\n    def extract_features(self, src_tokens, **kwargs):\n        """"""\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, seq_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        """"""\n        return self.decoder.extract_features(src_tokens, **kwargs)\n\n    def output_layer(self, features, **kwargs):\n        """"""Project features to the default output size (typically vocabulary size).""""""\n        return self.decoder.output_layer(features, **kwargs)\n\n    def max_positions(self):\n        """"""Maximum length supported by the model.""""""\n        return self.decoder.max_positions()\n\n    def max_decoder_positions(self):\n        """"""Maximum length supported by the decoder.""""""\n        return self.decoder.max_positions()\n\n    @property\n    def supported_targets(self):\n        return {""future""}\n\n\nclass FairseqEncoderModel(BaseFairseqModel):\n    """"""Base class for encoder-only models.\n\n    Args:\n        encoder (FairseqEncoder): the encoder\n    """"""\n\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n        assert isinstance(self.encoder, FairseqEncoder)\n\n    def forward(self, src_tokens, src_lengths, **kwargs):\n        """"""\n        Run the forward pass for a encoder-only model.\n\n        Feeds a batch of tokens through the encoder to generate features.\n\n        Args:\n            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n\n        Returns:\n            the encoder\'s output, typically of shape `(batch, src_len, features)`\n        """"""\n        return self.encoder(src_tokens, src_lengths, **kwargs)\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n        """"""Get normalized probabilities (or log probs) from a net\'s output.""""""\n        encoder_out = net_output[""encoder_out""]\n        if torch.is_tensor(encoder_out):\n            logits = encoder_out.float()\n            if log_probs:\n                return F.log_softmax(logits, dim=-1)\n            else:\n                return F.softmax(logits, dim=-1)\n        raise NotImplementedError\n\n    def max_positions(self):\n        """"""Maximum length supported by the model.""""""\n        return self.encoder.max_positions()\n'"
fairseq/models/fconv.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqIncrementalDecoder,\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import (\n    AdaptiveSoftmax, BeamableMM, GradMultiply, LearnedPositionalEmbedding,\n    LinearizedConvolution,\n)\n\n\n@register_model(\'fconv\')\nclass FConvModel(FairseqEncoderDecoderModel):\n    """"""\n    A fully convolutional model, i.e. a convolutional encoder and a\n    convolutional decoder, as described in `""Convolutional Sequence to Sequence\n    Learning"" (Gehring et al., 2017) <https://arxiv.org/abs/1705.03122>`_.\n\n    Args:\n        encoder (FConvEncoder): the encoder\n        decoder (FConvDecoder): the decoder\n\n    The Convolutional model provides the following named architectures and\n    command-line arguments:\n\n    .. argparse::\n        :ref: fairseq.models.fconv_parser\n        :prog:\n    """"""\n\n    @classmethod\n    def hub_models(cls):\n\n        def moses_subword(path):\n            return {\n                \'path\': path,\n                \'tokenizer\': \'moses\',\n                \'bpe\': \'subword_nmt\',\n            }\n\n        return {\n            \'conv.wmt14.en-fr\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2\'),\n            \'conv.wmt14.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-de.fconv-py.tar.bz2\'),\n            \'conv.wmt17.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt17.v2.en-de.fconv-py.tar.bz2\'),\n        }\n\n    def __init__(self, encoder, decoder):\n        super().__init__(encoder, decoder)\n        self.encoder.num_attention_layers = sum(layer is not None for layer in decoder.attention)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--encoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--encoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained encoder embedding\')\n        parser.add_argument(\'--encoder-layers\', type=str, metavar=\'EXPR\',\n                            help=\'encoder layers [(dim, kernel_size), ...]\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained decoder embedding\')\n        parser.add_argument(\'--decoder-layers\', type=str, metavar=\'EXPR\',\n                            help=\'decoder layers [(dim, kernel_size), ...]\')\n        parser.add_argument(\'--decoder-out-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder output embedding dimension\')\n        parser.add_argument(\'--decoder-attention\', type=str, metavar=\'EXPR\',\n                            help=\'decoder attention [True, ...]\')\n        parser.add_argument(\'--share-input-output-embed\', action=\'store_true\',\n                            help=\'share input and output embeddings (requires\'\n                                 \' --decoder-out-embed-dim and --decoder-embed-dim\'\n                                 \' to be equal)\')\n        # fmt: on\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure that all args are properly defaulted (in case there are any new ones)\n        base_architecture(args)\n\n        encoder_embed_dict = None\n        if args.encoder_embed_path:\n            encoder_embed_dict = utils.parse_embedding(args.encoder_embed_path)\n            utils.print_embed_overlap(encoder_embed_dict, task.source_dictionary)\n\n        decoder_embed_dict = None\n        if args.decoder_embed_path:\n            decoder_embed_dict = utils.parse_embedding(args.decoder_embed_path)\n            utils.print_embed_overlap(decoder_embed_dict, task.target_dictionary)\n\n        encoder = FConvEncoder(\n            dictionary=task.source_dictionary,\n            embed_dim=args.encoder_embed_dim,\n            embed_dict=encoder_embed_dict,\n            convolutions=eval(args.encoder_layers),\n            dropout=args.dropout,\n            max_positions=args.max_source_positions,\n        )\n        decoder = FConvDecoder(\n            dictionary=task.target_dictionary,\n            embed_dim=args.decoder_embed_dim,\n            embed_dict=decoder_embed_dict,\n            convolutions=eval(args.decoder_layers),\n            out_embed_dim=args.decoder_out_embed_dim,\n            attention=eval(args.decoder_attention),\n            dropout=args.dropout,\n            max_positions=args.max_target_positions,\n            share_embed=args.share_input_output_embed,\n        )\n        return FConvModel(encoder, decoder)\n\n\nclass FConvEncoder(FairseqEncoder):\n    """"""\n    Convolutional encoder consisting of `len(convolutions)` layers.\n\n    Args:\n        dictionary (~fairseq.data.Dictionary): encoding dictionary\n        embed_dim (int, optional): embedding dimension\n        embed_dict (str, optional): filename from which to load pre-trained\n            embeddings\n        max_positions (int, optional): maximum supported input sequence length\n        convolutions (list, optional): the convolutional layer structure. Each\n            list item `i` corresponds to convolutional layer `i`. Layers are\n            given as ``(out_channels, kernel_width, [residual])``. Residual\n            connections are added between layers when ``residual=1`` (which is\n            the default behavior).\n        dropout (float, optional): dropout to be applied before each conv layer\n    """"""\n\n    def __init__(\n        self, dictionary, embed_dim=512, embed_dict=None, max_positions=1024,\n        convolutions=((512, 3),) * 20, dropout=0.1,\n    ):\n        super().__init__(dictionary)\n        self.dropout = dropout\n        self.num_attention_layers = None\n\n        num_embeddings = len(dictionary)\n        self.padding_idx = dictionary.pad()\n        self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx)\n        if embed_dict:\n            self.embed_tokens = utils.load_embedding(embed_dict, self.dictionary, self.embed_tokens)\n\n        self.embed_positions = PositionalEmbedding(\n            max_positions,\n            embed_dim,\n            self.padding_idx,\n        )\n\n        convolutions = extend_conv_spec(convolutions)\n        in_channels = convolutions[0][0]\n        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n        self.projections = nn.ModuleList()\n        self.convolutions = nn.ModuleList()\n        self.residuals = []\n\n        layer_in_channels = [in_channels]\n        for _, (out_channels, kernel_size, residual) in enumerate(convolutions):\n            if residual == 0:\n                residual_dim = out_channels\n            else:\n                residual_dim = layer_in_channels[-residual]\n            self.projections.append(Linear(residual_dim, out_channels)\n                                    if residual_dim != out_channels else None)\n            if kernel_size % 2 == 1:\n                padding = kernel_size // 2\n            else:\n                padding = 0\n            self.convolutions.append(\n                ConvTBC(in_channels, out_channels * 2, kernel_size,\n                        dropout=dropout, padding=padding)\n            )\n            self.residuals.append(residual)\n            in_channels = out_channels\n            layer_in_channels.append(out_channels)\n        self.fc2 = Linear(in_channels, embed_dim)\n\n    def forward(self, src_tokens, src_lengths):\n        """"""\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of shape\n                `(batch)`\n\n        Returns:\n            dict:\n                - **encoder_out** (tuple): a tuple with two elements, where the\n                  first element is the last encoder layer\'s output and the\n                  second element is the same quantity summed with the input\n                  embedding (used for attention). The shape of both tensors is\n                  `(batch, src_len, embed_dim)`.\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        """"""\n        # embed tokens and positions\n        x = self.embed_tokens(src_tokens) + self.embed_positions(src_tokens)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        input_embedding = x\n\n        # project to size of convolution\n        x = self.fc1(x)\n\n        # used to mask padding in input\n        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()  # -> T x B\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        residuals = [x]\n        # temporal convolutions\n        for proj, conv, res_layer in zip(self.projections, self.convolutions, self.residuals):\n            if res_layer > 0:\n                residual = residuals[-res_layer]\n                residual = residual if proj is None else proj(residual)\n            else:\n                residual = None\n\n            if encoder_padding_mask is not None:\n                x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            if conv.kernel_size[0] % 2 == 1:\n                # padding is implicit in the conv\n                x = conv(x)\n            else:\n                padding_l = (conv.kernel_size[0] - 1) // 2\n                padding_r = conv.kernel_size[0] // 2\n                x = F.pad(x, (0, 0, 0, 0, padding_l, padding_r))\n                x = conv(x)\n            x = F.glu(x, dim=2)\n\n            if residual is not None:\n                x = (x + residual) * math.sqrt(0.5)\n            residuals.append(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(1, 0)\n\n        # project back to size of embedding\n        x = self.fc2(x)\n\n        if encoder_padding_mask is not None:\n            encoder_padding_mask = encoder_padding_mask.t()  # -> B x T\n            x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n\n        # scale gradients (this only affects backward, not forward)\n        x = GradMultiply.apply(x, 1.0 / (2.0 * self.num_attention_layers))\n\n        # add output to input embedding for attention\n        y = (x + input_embedding) * math.sqrt(0.5)\n\n        return {\n            \'encoder_out\': (x, y),\n            \'encoder_padding_mask\': encoder_padding_mask,  # B x T\n        }\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        if encoder_out[\'encoder_out\'] is not None:\n            encoder_out[\'encoder_out\'] = (\n                encoder_out[\'encoder_out\'][0].index_select(0, new_order),\n                encoder_out[\'encoder_out\'][1].index_select(0, new_order),\n            )\n        if encoder_out[\'encoder_padding_mask\'] is not None:\n            encoder_out[\'encoder_padding_mask\'] = \\\n                encoder_out[\'encoder_padding_mask\'].index_select(0, new_order)\n        return encoder_out\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return self.embed_positions.max_positions\n\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, conv_channels, embed_dim, bmm=None):\n        super().__init__()\n        # projects from output of convolution to embedding dimension\n        self.in_projection = Linear(conv_channels, embed_dim)\n        # projects from embedding dimension to convolution size\n        self.out_projection = Linear(embed_dim, conv_channels)\n\n        self.bmm = bmm if bmm is not None else torch.bmm\n\n    def forward(self, x, target_embedding, encoder_out, encoder_padding_mask):\n        residual = x\n\n        # attention\n        x = (self.in_projection(x) + target_embedding) * math.sqrt(0.5)\n        x = self.bmm(x, encoder_out[0])\n\n        # don\'t attend over padding\n        if encoder_padding_mask is not None:\n            x = x.float().masked_fill(\n                encoder_padding_mask.unsqueeze(1),\n                float(\'-inf\')\n            ).type_as(x)  # FP16 support: cast to float and back\n\n        # softmax over last dim\n        sz = x.size()\n        x = F.softmax(x.view(sz[0] * sz[1], sz[2]), dim=1)\n        x = x.view(sz)\n        attn_scores = x\n\n        x = self.bmm(x, encoder_out[1])\n\n        # scale attention output (respecting potentially different lengths)\n        s = encoder_out[1].size(1)\n        if encoder_padding_mask is None:\n            x = x * (s * math.sqrt(1.0 / s))\n        else:\n            s = s - encoder_padding_mask.type_as(x).sum(dim=1, keepdim=True)  # exclude padding\n            s = s.unsqueeze(-1)\n            x = x * (s * s.rsqrt())\n\n        # project back\n        x = (self.out_projection(x) + residual) * math.sqrt(0.5)\n        return x, attn_scores\n\n    def make_generation_fast_(self, beamable_mm_beam_size=None, **kwargs):\n        """"""Replace torch.bmm with BeamableMM.""""""\n        if beamable_mm_beam_size is not None:\n            del self.bmm\n            self.add_module(\'bmm\', BeamableMM(beamable_mm_beam_size))\n\n\nclass FConvDecoder(FairseqIncrementalDecoder):\n    """"""Convolutional decoder""""""\n\n    def __init__(\n        self, dictionary, embed_dim=512, embed_dict=None, out_embed_dim=256,\n        max_positions=1024, convolutions=((512, 3),) * 20, attention=True,\n        dropout=0.1, share_embed=False, positional_embeddings=True,\n        adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0,\n    ):\n        super().__init__(dictionary)\n        self.register_buffer(\'version\', torch.Tensor([2]))\n        self.dropout = dropout\n        self.need_attn = True\n\n        convolutions = extend_conv_spec(convolutions)\n        in_channels = convolutions[0][0]\n        if isinstance(attention, bool):\n            # expand True into [True, True, ...] and do the same with False\n            attention = [attention] * len(convolutions)\n        if not isinstance(attention, list) or len(attention) != len(convolutions):\n            raise ValueError(\'Attention is expected to be a list of booleans of \'\n                             \'length equal to the number of layers.\')\n\n        num_embeddings = len(dictionary)\n        padding_idx = dictionary.pad()\n        self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n        if embed_dict:\n            self.embed_tokens = utils.load_embedding(embed_dict, self.dictionary, self.embed_tokens)\n\n        self.embed_positions = PositionalEmbedding(\n            max_positions,\n            embed_dim,\n            padding_idx,\n        ) if positional_embeddings else None\n\n        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n        self.projections = nn.ModuleList()\n        self.convolutions = nn.ModuleList()\n        self.attention = nn.ModuleList()\n        self.residuals = []\n\n        layer_in_channels = [in_channels]\n        for i, (out_channels, kernel_size, residual) in enumerate(convolutions):\n            if residual == 0:\n                residual_dim = out_channels\n            else:\n                residual_dim = layer_in_channels[-residual]\n            self.projections.append(Linear(residual_dim, out_channels)\n                                    if residual_dim != out_channels else None)\n            self.convolutions.append(\n                LinearizedConv1d(in_channels, out_channels * 2, kernel_size,\n                                 padding=(kernel_size - 1), dropout=dropout)\n            )\n            self.attention.append(AttentionLayer(out_channels, embed_dim)\n                                  if attention[i] else None)\n            self.residuals.append(residual)\n            in_channels = out_channels\n            layer_in_channels.append(out_channels)\n\n        self.adaptive_softmax = None\n        self.fc2 = self.fc3 = None\n\n        if adaptive_softmax_cutoff is not None:\n            assert not share_embed\n            self.adaptive_softmax = AdaptiveSoftmax(num_embeddings, in_channels, adaptive_softmax_cutoff,\n                                                    dropout=adaptive_softmax_dropout)\n        else:\n            self.fc2 = Linear(in_channels, out_embed_dim)\n            if share_embed:\n                assert out_embed_dim == embed_dim, \\\n                    ""Shared embed weights implies same dimensions "" \\\n                    "" out_embed_dim={} vs embed_dim={}"".format(out_embed_dim, embed_dim)\n                self.fc3 = nn.Linear(out_embed_dim, num_embeddings)\n                self.fc3.weight = self.embed_tokens.weight\n            else:\n                self.fc3 = Linear(out_embed_dim, num_embeddings, dropout=dropout)\n\n    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **unused):\n        if encoder_out is not None:\n            encoder_padding_mask = encoder_out[\'encoder_padding_mask\']\n            encoder_out = encoder_out[\'encoder_out\']\n\n            # split and transpose encoder outputs\n            encoder_a, encoder_b = self._split_encoder_out(encoder_out, incremental_state)\n\n        if self.embed_positions is not None:\n            pos_embed = self.embed_positions(prev_output_tokens, incremental_state)\n        else:\n            pos_embed = 0\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n        x = self._embed_tokens(prev_output_tokens, incremental_state)\n\n        # embed tokens and combine with positional embeddings\n        x += pos_embed\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        target_embedding = x\n\n        # project to size of convolution\n        x = self.fc1(x)\n\n        # B x T x C -> T x B x C\n        x = self._transpose_if_training(x, incremental_state)\n\n        # temporal convolutions\n        avg_attn_scores = None\n        num_attn_layers = len(self.attention)\n        residuals = [x]\n        for proj, conv, attention, res_layer in zip(self.projections, self.convolutions, self.attention,\n                                                    self.residuals):\n            if res_layer > 0:\n                residual = residuals[-res_layer]\n                residual = residual if proj is None else proj(residual)\n            else:\n                residual = None\n\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = conv(x, incremental_state)\n            x = F.glu(x, dim=2)\n\n            # attention\n            if attention is not None:\n                x = self._transpose_if_training(x, incremental_state)\n\n                x, attn_scores = attention(x, target_embedding, (encoder_a, encoder_b), encoder_padding_mask)\n\n                if not self.training and self.need_attn:\n                    attn_scores = attn_scores / num_attn_layers\n                    if avg_attn_scores is None:\n                        avg_attn_scores = attn_scores\n                    else:\n                        avg_attn_scores.add_(attn_scores)\n\n                x = self._transpose_if_training(x, incremental_state)\n\n            # residual\n            if residual is not None:\n                x = (x + residual) * math.sqrt(0.5)\n            residuals.append(x)\n\n        # T x B x C -> B x T x C\n        x = self._transpose_if_training(x, incremental_state)\n\n        # project back to size of vocabulary if not using adaptive softmax\n        if self.fc2 is not None and self.fc3 is not None:\n            x = self.fc2(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = self.fc3(x)\n\n        return x, avg_attn_scores\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        super().reorder_incremental_state(incremental_state, new_order)\n        encoder_out = utils.get_incremental_state(self, incremental_state, \'encoder_out\')\n        if encoder_out is not None:\n            encoder_out = tuple(eo.index_select(0, new_order) for eo in encoder_out)\n            utils.set_incremental_state(self, incremental_state, \'encoder_out\', encoder_out)\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return self.embed_positions.max_positions if self.embed_positions is not None else float(\'inf\')\n\n    def upgrade_state_dict(self, state_dict):\n        if utils.item(state_dict.get(\'decoder.version\', torch.Tensor([1]))[0]) < 2:\n            # old models use incorrect weight norm dimension\n            for i, conv in enumerate(self.convolutions):\n                # reconfigure weight norm\n                nn.utils.remove_weight_norm(conv)\n                self.convolutions[i] = nn.utils.weight_norm(conv, dim=0)\n            state_dict[\'decoder.version\'] = torch.Tensor([1])\n        return state_dict\n\n    def make_generation_fast_(self, need_attn=False, **kwargs):\n        self.need_attn = need_attn\n\n    def _embed_tokens(self, tokens, incremental_state):\n        if incremental_state is not None:\n            # keep only the last token for incremental forward pass\n            tokens = tokens[:, -1:]\n        return self.embed_tokens(tokens)\n\n    def _split_encoder_out(self, encoder_out, incremental_state):\n        """"""Split and transpose encoder outputs.\n\n        This is cached when doing incremental inference.\n        """"""\n        cached_result = utils.get_incremental_state(self, incremental_state, \'encoder_out\')\n        if cached_result is not None:\n            return cached_result\n\n        # transpose only once to speed up attention layers\n        encoder_a, encoder_b = encoder_out\n        encoder_a = encoder_a.transpose(1, 2).contiguous()\n        result = (encoder_a, encoder_b)\n\n        if incremental_state is not None:\n            utils.set_incremental_state(self, incremental_state, \'encoder_out\', result)\n        return result\n\n    def _transpose_if_training(self, x, incremental_state):\n        if incremental_state is None:\n            x = x.transpose(0, 1)\n        return x\n\n\ndef extend_conv_spec(convolutions):\n    """"""\n    Extends convolutional spec that is a list of tuples of 2 or 3 parameters\n    (kernel size, dim size and optionally how many layers behind to look for residual)\n    to default the residual propagation param if it is not specified\n    """"""\n    extended = []\n    for spec in convolutions:\n        if len(spec) == 3:\n            extended.append(spec)\n        elif len(spec) == 2:\n            extended.append(spec + (1,))\n        else:\n            raise Exception(\'invalid number of parameters in convolution spec \' + str(spec) + \'. expected 2 or 3\')\n    return tuple(extended)\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, 0, 0.1)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n\ndef PositionalEmbedding(num_embeddings, embedding_dim, padding_idx):\n    m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n    nn.init.normal_(m.weight, 0, 0.1)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n\ndef Linear(in_features, out_features, dropout=0):\n    """"""Weight-normalized Linear layer (input: N x T x C)""""""\n    m = nn.Linear(in_features, out_features)\n    nn.init.normal_(m.weight, mean=0, std=math.sqrt((1 - dropout) / in_features))\n    nn.init.constant_(m.bias, 0)\n    return nn.utils.weight_norm(m)\n\n\ndef LinearizedConv1d(in_channels, out_channels, kernel_size, dropout=0, **kwargs):\n    """"""Weight-normalized Conv1d layer optimized for decoding""""""\n    m = LinearizedConvolution(in_channels, out_channels, kernel_size, **kwargs)\n    std = math.sqrt((4 * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n    nn.init.normal_(m.weight, mean=0, std=std)\n    nn.init.constant_(m.bias, 0)\n    return nn.utils.weight_norm(m, dim=2)\n\n\ndef ConvTBC(in_channels, out_channels, kernel_size, dropout=0, **kwargs):\n    """"""Weight-normalized Conv1d layer""""""\n    from fairseq.modules import ConvTBC\n    m = ConvTBC(in_channels, out_channels, kernel_size, **kwargs)\n    std = math.sqrt((4 * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n    nn.init.normal_(m.weight, mean=0, std=std)\n    nn.init.constant_(m.bias, 0)\n    return nn.utils.weight_norm(m, dim=2)\n\n\n@register_model_architecture(\'fconv\', \'fconv\')\ndef base_architecture(args):\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 512)\n    args.encoder_embed_path = getattr(args, \'encoder_embed_path\', None)\n    args.encoder_layers = getattr(args, \'encoder_layers\', \'[(512, 3)] * 20\')\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.decoder_embed_path = getattr(args, \'decoder_embed_path\', None)\n    args.decoder_layers = getattr(args, \'decoder_layers\', \'[(512, 3)] * 20\')\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 256)\n    args.decoder_attention = getattr(args, \'decoder_attention\', \'True\')\n    args.share_input_output_embed = getattr(args, \'share_input_output_embed\', False)\n\n\n@register_model_architecture(\'fconv\', \'fconv_iwslt_de_en\')\ndef fconv_iwslt_de_en(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 256)\n    args.encoder_layers = getattr(args, \'encoder_layers\', \'[(256, 3)] * 4\')\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 256)\n    args.decoder_layers = getattr(args, \'decoder_layers\', \'[(256, 3)] * 3\')\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 256)\n    base_architecture(args)\n\n\n@register_model_architecture(\'fconv\', \'fconv_wmt_en_ro\')\ndef fconv_wmt_en_ro(args):\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 512)\n    base_architecture(args)\n\n\n@register_model_architecture(\'fconv\', \'fconv_wmt_en_de\')\ndef fconv_wmt_en_de(args):\n    convs = \'[(512, 3)] * 9\'  # first 9 layers have 512 units\n    convs += \' + [(1024, 3)] * 4\'  # next 4 layers have 1024 units\n    convs += \' + [(2048, 1)] * 2\'  # final 2 layers use 1x1 convolutions\n\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 768)\n    args.encoder_layers = getattr(args, \'encoder_layers\', convs)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 768)\n    args.decoder_layers = getattr(args, \'decoder_layers\', convs)\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 512)\n    base_architecture(args)\n\n\n@register_model_architecture(\'fconv\', \'fconv_wmt_en_fr\')\ndef fconv_wmt_en_fr(args):\n    convs = \'[(512, 3)] * 6\'  # first 6 layers have 512 units\n    convs += \' + [(768, 3)] * 4\'  # next 4 layers have 768 units\n    convs += \' + [(1024, 3)] * 3\'  # next 3 layers have 1024 units\n    convs += \' + [(2048, 1)] * 1\'  # next 1 layer uses 1x1 convolutions\n    convs += \' + [(4096, 1)] * 1\'  # final 1 layer uses 1x1 convolutions\n\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 768)\n    args.encoder_layers = getattr(args, \'encoder_layers\', convs)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 768)\n    args.decoder_layers = getattr(args, \'decoder_layers\', convs)\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 512)\n    base_architecture(args)\n'"
fairseq/models/fconv_lm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import options\nfrom fairseq.models import (\n    FairseqLanguageModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.fconv import FConvDecoder\n\n\n@register_model(\'fconv_lm\')\nclass FConvLanguageModel(FairseqLanguageModel):\n    def __init__(self, decoder):\n        super().__init__(decoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-layers\', type=str, metavar=\'EXPR\',\n                            help=\'decoder layers [(dim, kernel_size), ...]\')\n        parser.add_argument(\'--decoder-out-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder output embedding dimension\')\n        parser.add_argument(\'--adaptive-softmax-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive softmax cutoff points. \'\n                                 \'Must be used with adaptive_loss criterion\')\n        parser.add_argument(\'--adaptive-softmax-dropout\', type=float, metavar=\'D\',\n                            help=\'sets adaptive softmax dropout for the tail projections\')\n        parser.add_argument(\'--decoder-attention\', type=str, metavar=\'EXPR\',\n                            help=\'decoder attention [True, ...]\')\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure all arguments are present in older models\n        base_lm_architecture(args)\n\n        if hasattr(args, \'max_target_positions\') and not hasattr(args, \'tokens_per_sample\'):\n            args.tokens_per_sample = args.max_target_positions\n\n        decoder = FConvDecoder(\n            dictionary=task.target_dictionary,\n            embed_dim=args.decoder_embed_dim,\n            convolutions=eval(args.decoder_layers),\n            out_embed_dim=args.decoder_embed_dim,\n            attention=eval(args.decoder_attention),\n            dropout=args.dropout,\n            max_positions=args.tokens_per_sample,\n            share_embed=False,\n            positional_embeddings=False,\n            adaptive_softmax_cutoff=(\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int)\n                if args.criterion == \'adaptive_loss\' else None\n            ),\n            adaptive_softmax_dropout=args.adaptive_softmax_dropout,\n        )\n        return FConvLanguageModel(decoder)\n\n\n@register_model_architecture(\'fconv_lm\', \'fconv_lm\')\ndef base_lm_architecture(args):\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 128)\n    args.decoder_layers = getattr(args, \'decoder_layers\', \'[(1268, 4)] * 13\')\n    args.decoder_attention = getattr(args, \'decoder_attention\', \'False\')\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', None)\n    args.adaptive_softmax_dropout = getattr(args, \'adaptive_softmax_dropout\', 0)\n\n\n@register_model_architecture(\'fconv_lm\', \'fconv_lm_dauphin_wikitext103\')\ndef fconv_lm_dauphin_wikitext103(args):\n    layers = \'[(850, 6)] * 3\'\n    layers += \' + [(850, 1)] * 1\'\n    layers += \' + [(850, 5)] * 4\'\n    layers += \' + [(850, 1)] * 1\'\n    layers += \' + [(850, 4)] * 3\'\n    layers += \' + [(1024, 4)] * 1\'\n    layers += \' + [(2048, 4)] * 1\'\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 280)\n    args.decoder_layers = getattr(args, \'decoder_layers\', layers)\n    args.decoder_attention = getattr(args, \'decoder_attention\', \'False\')\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', \'10000,20000,200000\')\n    base_lm_architecture(args)\n\n\n@register_model_architecture(\'fconv_lm\', \'fconv_lm_dauphin_gbw\')\ndef fconv_lm_dauphin_gbw(args):\n    layers = \'[(512, 5)]\'\n    layers += \' + [(128, 1, 0), (128, 5, 0), (512, 1, 3)] * 3\'\n    layers += \' + [(512, 1, 0), (512, 5, 0), (1024, 1, 3)] * 3\'\n    layers += \' + [(1024, 1, 0), (1024, 5, 0), (2048, 1, 3)] * 6\'\n    layers += \' + [(1024, 1, 0), (1024, 5, 0), (4096, 1, 3)]\'\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 128)\n    args.decoder_layers = getattr(args, \'decoder_layers\', layers)\n    args.decoder_attention = getattr(args, \'decoder_attention\', \'False\')\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', \'10000,50000,200000\')\n    base_lm_architecture(args)\n'"
fairseq/models/fconv_self_att.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport math\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import checkpoint_utils\nfrom fairseq.models import (\n    CompositeEncoder,\n    FairseqDecoder,\n    FairseqEncoder,\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import (\n    DownsampledMultiHeadAttention,\n    GradMultiply,\n    LayerNorm,\n    LearnedPositionalEmbedding,\n    LinearizedConvolution,\n)\nfrom fairseq.incremental_decoding_utils import with_incremental_state\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(\'fconv_self_att\')\nclass FConvModelSelfAtt(FairseqEncoderDecoderModel):\n\n    @classmethod\n    def hub_models(cls):\n        return {\n            \'conv.stories.pretrained\': {\n                \'path\': \'https://dl.fbaipublicfiles.com/fairseq/models/stories_checkpoint.tar.gz\',\n                \'checkpoint_file\': \'pretrained_checkpoint.pt\',\n                \'tokenizer\': \'nltk\',\n            },\n            \'conv.stories\': {\n                \'path\': \'https://dl.fbaipublicfiles.com/fairseq/models/stories_checkpoint.tar.gz\',\n                \'checkpoint_file\': \'fusion_checkpoint.pt\',\n                \'tokenizer\': \'nltk\',\n                \'pretrained\': \'True\',\n                \'pretrained_checkpoint\': \'./pretrained_checkpoint.pt\',\n            },\n            # Test set containing dictionaries\n            \'data.stories\': \'https://dl.fbaipublicfiles.com/fairseq/data/stories_test.tar.bz2\',\n        }\n\n    def __init__(self, encoder, decoder, pretrained_encoder=None):\n        super().__init__(encoder, decoder)\n        self.encoder.num_attention_layers = sum(layer is not None for layer in decoder.attention)\n        self.pretrained_encoder = pretrained_encoder\n        if self.pretrained_encoder is None:\n            encoders = {\'encoder\': encoder}\n        else:\n            encoders = {\'encoder\': encoder, \'pretrained\': self.pretrained_encoder}\n        # for fusion model, CompositeEncoder contains both pretrained and training encoders\n        # these are forwarded and then combined in the decoder\n        self.encoder = CompositeEncoder(encoders)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--encoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--encoder-layers\', type=str, metavar=\'EXPR\',\n                            help=\'encoder layers [(dim, kernel_size), ...]\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-layers\', type=str, metavar=\'EXPR\',\n                            help=\'decoder layers [(dim, kernel_size), ...]\')\n        parser.add_argument(\'--decoder-out-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder output embedding dimension\')\n        parser.add_argument(\'--decoder-attention\', type=str, metavar=\'EXPR\',\n                            help=\'decoder attention [True, ...]\')\n        parser.add_argument(\'--self-attention\', type=str, metavar=\'EXPR\',\n                            help=\'decoder self-attention layers, ex: [True] + [False]*5\')\n        parser.add_argument(\'--multihead-attention-nheads\', type=int,\n                            help=\'Number of heads to use in attention\')\n        parser.add_argument(\'--multihead-self-attention-nheads\', type=int,\n                            help=\'Number of heads to use in self-attention\')\n        parser.add_argument(\'--encoder-attention\', type=str, metavar=\'EXPR\',\n                            help=\'encoder attention [True, ...]\')\n        parser.add_argument(\'--encoder-attention-nheads\', type=int,\n                            help=\'Number of heads to use in encoder attention\')\n        parser.add_argument(\'--project-input\', type=str, metavar=\'EXPR\',\n                            help=\'Use projections in self-attention [True, ...]\')\n        parser.add_argument(\'--gated-attention\', type=str, metavar=\'EXPR\',\n                            help=\'Use GLU layers in self-attention projections [True, ...]\')\n        parser.add_argument(\'--downsample\', type=str, metavar=\'EXPR\',\n                            help=\'Use downsampling in self-attention [True, ...]\')\n        parser.add_argument(\'--pretrained-checkpoint\', metavar=\'DIR\',\n                            help=\'path to load checkpoint from pretrained model\')\n        parser.add_argument(\'--pretrained\', type=str, metavar=\'EXPR\',\n                            help=\'use pretrained model when training [True, ...]\')\n        # fmt: on\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        trained_encoder, trained_decoder = None, None\n        pretrained = eval(args.pretrained)\n        if pretrained:\n            logger.info(\'loading pretrained model\')\n            if not os.path.exists(args.pretrained_checkpoint):\n                new_pretrained_checkpoint = os.path.join(args.data, args.pretrained_checkpoint)\n                if os.path.exists(new_pretrained_checkpoint):\n                    args.pretrained_checkpoint = new_pretrained_checkpoint\n            trained_model = checkpoint_utils.load_model_ensemble(\n                filenames=[args.pretrained_checkpoint],\n                task=task,\n            )[0][0]\n            trained_decoder = list(trained_model.children())[1]\n            trained_encoder = list(trained_model.children())[0]\n\n            # freeze pretrained model\n            for param in trained_decoder.parameters():\n                param.requires_grad = False\n            for param in trained_encoder.parameters():\n                param.requires_grad = False\n\n        encoder = FConvEncoder(\n            task.source_dictionary,\n            embed_dim=args.encoder_embed_dim,\n            convolutions=eval(args.encoder_layers),\n            dropout=args.dropout,\n            max_positions=args.max_source_positions,\n            attention=eval(args.encoder_attention),\n            attention_nheads=args.encoder_attention_nheads\n        )\n\n        decoder = FConvDecoder(\n            task.target_dictionary,\n            embed_dim=args.decoder_embed_dim,\n            convolutions=eval(args.decoder_layers),\n            out_embed_dim=args.decoder_out_embed_dim,\n            attention=eval(args.decoder_attention),\n            dropout=args.dropout,\n            max_positions=args.max_target_positions,\n            selfattention=eval(args.self_attention),\n            attention_nheads=args.multihead_attention_nheads,\n            selfattention_nheads=args.multihead_self_attention_nheads,\n            project_input=eval(args.project_input),\n            gated_attention=eval(args.gated_attention),\n            downsample=eval(args.downsample),\n            pretrained=pretrained,\n            trained_decoder=trained_decoder\n        )\n        model = FConvModelSelfAtt(encoder, decoder, trained_encoder)\n\n        return model\n\n    @property\n    def pretrained(self):\n        return self.pretrained_encoder is not None\n\n\nclass FConvEncoder(FairseqEncoder):\n    """"""Convolutional encoder""""""\n    def __init__(\n        self, dictionary, embed_dim=512, max_positions=1024,\n        convolutions=((512, 3),) * 20, dropout=0.1, attention=False,\n        attention_nheads=1,\n    ):\n        super().__init__(dictionary)\n        self.dropout = dropout\n        self.num_attention_layers = None\n\n        num_embeddings = len(dictionary)\n        self.padding_idx = dictionary.pad()\n        self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx)\n        self.embed_positions = PositionalEmbedding(\n            max_positions,\n            embed_dim,\n            self.padding_idx,\n        )\n\n        def expand_bool_array(val):\n            if isinstance(val, bool):\n                # expand True into [True, True, ...] and do the same with False\n                return [val] * len(convolutions)\n            return val\n\n        attention = expand_bool_array(attention)\n\n        in_channels = convolutions[0][0]\n        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n        self.projections = nn.ModuleList()\n        self.convolutions = nn.ModuleList()\n        self.attention = nn.ModuleList()\n        self.attproj = nn.ModuleList()\n        for i, (out_channels, kernel_size) in enumerate(convolutions):\n            self.projections.append(\n                Linear(in_channels, out_channels) if in_channels != out_channels else None\n            )\n            self.convolutions.append(\n                ConvTBC(in_channels, out_channels * 2, kernel_size, dropout=dropout)\n            )\n\n            self.attention.append(\n                SelfAttention(out_channels, embed_dim, attention_nheads) if attention[i] else None\n            )\n            in_channels = out_channels\n\n        self.fc2 = Linear(in_channels, embed_dim)\n\n    def forward(self, src_tokens, src_lengths):\n        # embed tokens and positions\n        x = self.embed_tokens(src_tokens) + self.embed_positions(src_tokens)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        input_embedding = x.transpose(0, 1)\n\n        # project to size of convolution\n        x = self.fc1(x)\n\n        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()  # -> T x B\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # temporal convolutions\n        for proj, conv, attention in zip(self.projections, self.convolutions, self.attention):\n            residual = x if proj is None else proj(x)\n\n            if encoder_padding_mask is not None:\n                x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            padding_l = (conv.kernel_size[0] - 1) // 2\n            padding_r = conv.kernel_size[0] // 2\n            x = F.pad(x, (0, 0, 0, 0, padding_l, padding_r))\n            x = conv(x)\n            x = F.glu(x, dim=2)\n            if attention is not None:\n                x = attention(x)\n            x = (x + residual) * math.sqrt(0.5)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(1, 0)\n\n        # project back to size of embedding\n        x = self.fc2(x)\n\n        if encoder_padding_mask is not None:\n            encoder_padding_mask = encoder_padding_mask.t()  # -> B x T\n            x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n\n        # scale gradients (this only affects backward, not forward)\n        x = GradMultiply.apply(x, 1.0 / (2.0 * self.num_attention_layers))\n\n        # add output to input embedding for attention\n        y = (x + input_embedding.transpose(0, 1)) * math.sqrt(0.5)\n\n        return {\n            \'encoder_out\': (x, y),\n            \'encoder_padding_mask\': encoder_padding_mask,  # B x T\n        }\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        encoder_out[\'encoder_out\'] = tuple(\n            eo.index_select(0, new_order) for eo in encoder_out[\'encoder_out\']\n        )\n\n        if encoder_out[\'encoder_padding_mask\'] is not None:\n            encoder_out[\'encoder_padding_mask\'] = \\\n                encoder_out[\'encoder_padding_mask\'].index_select(0, new_order)\n\n        if \'pretrained\' in encoder_out:\n            encoder_out[\'pretrained\'][\'encoder_out\'] = tuple(\n                eo.index_select(0, new_order)\n                for eo in encoder_out[\'pretrained\'][\'encoder_out\']\n            )\n\n        return encoder_out\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return self.embed_positions.max_positions\n\n\n@with_incremental_state\nclass FConvDecoder(FairseqDecoder):\n    """"""Convolutional decoder""""""\n    def __init__(\n        self, dictionary, embed_dim=512, out_embed_dim=256, max_positions=1024,\n        convolutions=((512, 3),) * 8, attention=True, dropout=0.1,\n        selfattention=False, attention_nheads=1, selfattention_nheads=1,\n        project_input=False, gated_attention=False, downsample=False,\n        pretrained=False, trained_decoder=None,\n    ):\n        super().__init__(dictionary)\n        self.register_buffer(\'version\', torch.Tensor([2]))\n        self.pretrained = pretrained\n        self.pretrained_decoder = trained_decoder\n        self.dropout = dropout\n        self.need_attn = True\n        in_channels = convolutions[0][0]\n\n        def expand_bool_array(val):\n            if isinstance(val, bool):\n                # expand True into [True, True, ...] and do the same with False\n                return [val] * len(convolutions)\n            return val\n\n        attention = expand_bool_array(attention)\n        selfattention = expand_bool_array(selfattention)\n\n        if not isinstance(attention, list) or len(attention) != len(convolutions):\n            raise ValueError(\'Attention is expected to be a list of booleans of \'\n                             \'length equal to the number of layers.\')\n\n        num_embeddings = len(dictionary)\n        padding_idx = dictionary.pad()\n        self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n\n        self.embed_positions = PositionalEmbedding(\n            max_positions,\n            embed_dim,\n            padding_idx,\n        )\n\n        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)\n        self.projections = nn.ModuleList()\n        self.convolutions = nn.ModuleList()\n        self.attention = nn.ModuleList()\n        self.selfattention = nn.ModuleList()\n        self.attproj = nn.ModuleList()\n        for i, (out_channels, kernel_size) in enumerate(convolutions):\n            self.projections.append(\n                Linear(in_channels, out_channels) if in_channels != out_channels else None\n            )\n            self.convolutions.append(\n                LinearizedConv1d(\n                    in_channels, out_channels * 2, kernel_size,\n                    padding=(kernel_size - 1), dropout=dropout,\n                )\n            )\n\n            self.attention.append(\n                DownsampledMultiHeadAttention(\n                    out_channels, embed_dim, attention_nheads,\n                    project_input=project_input, gated=False, downsample=False,\n                ) if attention[i] else None\n            )\n\n            self.attproj.append(\n                Linear(out_channels, embed_dim, dropout=dropout) if attention[i] else None\n            )\n            self.selfattention.append(\n                SelfAttention(\n                    out_channels, embed_dim, selfattention_nheads,\n                    project_input=project_input, gated=gated_attention,\n                    downsample=downsample,\n                ) if selfattention[i] else None\n            )\n            in_channels = out_channels\n\n        self.fc2 = Linear(in_channels, out_embed_dim)\n        self.fc3 = Linear(out_embed_dim, num_embeddings, dropout=dropout)\n\n        # model fusion\n        if self.pretrained:\n            # independent gates are learned from the concatenated input\n            self.gate1 = nn.Sequential(Linear(out_embed_dim*2, out_embed_dim), nn.Sigmoid())\n            self.gate2 = nn.Sequential(Linear(out_embed_dim*2, out_embed_dim), nn.Sigmoid())\n            # pretrained and trained models are joined\n            self.joining = nn.Sequential(\n                Linear(out_embed_dim*2, out_embed_dim*2),\n                LayerNorm(out_embed_dim*2),\n                nn.GLU(),\n                Linear(out_embed_dim, out_embed_dim*2),\n                LayerNorm(out_embed_dim*2),\n                nn.GLU(),\n                Linear(out_embed_dim, out_embed_dim),\n                LayerNorm(out_embed_dim)\n            )\n            # pretrained model contains an output layer that is nhid -> vocab size\n            # but the models are combined in their hidden state\n            # the hook stores the output of the pretrained model forward\n            self.pretrained_outputs = {}\n\n            def save_output():\n                def hook(a, b, output):\n                    self.pretrained_outputs[""out""] = output\n                return hook\n\n            self.pretrained_decoder.fc2.register_forward_hook(save_output())\n\n    def forward(self, prev_output_tokens, encoder_out):\n        trained_encoder_out = encoder_out[\'pretrained\'] if self.pretrained else None\n        encoder_out = encoder_out[\'encoder\'][\'encoder_out\']\n\n        encoder_a, encoder_b = self._split_encoder_out(encoder_out)\n\n        # embed positions\n        positions = self.embed_positions(prev_output_tokens)\n\n        # embed tokens and positions\n        x = self.embed_tokens(prev_output_tokens) + positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        target_embedding = x.transpose(0, 1)\n\n        # project to size of convolution\n        x = self.fc1(x)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # temporal convolutions\n        avg_attn_scores = None\n        for proj, conv, attention, selfattention, attproj in zip(\n            self.projections, self.convolutions, self.attention, self.selfattention, self.attproj\n        ):\n            residual = x if proj is None else proj(x)\n\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = conv(x)\n            x = F.glu(x, dim=2)\n\n            # attention\n            if attention is not None:\n                r = x\n                x, attn_scores = attention(attproj(x) + target_embedding, encoder_a, encoder_b)\n                x = x + r\n                if not self.training and self.need_attn:\n                    if avg_attn_scores is None:\n                        avg_attn_scores = attn_scores\n                    else:\n                        avg_attn_scores.add_(attn_scores)\n\n            if selfattention is not None:\n                x = selfattention(x)\n\n            x = (x + residual) * math.sqrt(0.5)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        # project back to size of vocabulary\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        if not self.pretrained:\n            x = self.fc3(x)\n\n        # fusion gating\n        if self.pretrained:\n            trained_x, _ = self.pretrained_decoder.forward(prev_output_tokens, trained_encoder_out)\n            y = torch.cat([x, self.pretrained_outputs[""out""]], dim=-1)\n            gate1 = self.gate1(y)\n            gate2 = self.gate2(y)\n            gated_x1 = gate1 * x\n            gated_x2 = gate2 * self.pretrained_outputs[""out""]\n            fusion = torch.cat([gated_x1, gated_x2], dim=-1)\n            fusion = self.joining(fusion)\n            fusion_output = self.fc3(fusion)\n            return fusion_output, avg_attn_scores\n        else:\n            return x, avg_attn_scores\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return self.embed_positions.max_positions\n\n    def make_generation_fast_(self, need_attn=False, **kwargs):\n        self.need_attn = need_attn\n\n    def _split_encoder_out(self, encoder_out):\n        """"""Split and transpose encoder outputs.""""""\n        # transpose only once to speed up attention layers\n        encoder_a, encoder_b = encoder_out\n        encoder_a = encoder_a.transpose(0, 1).contiguous()\n        encoder_b = encoder_b.transpose(0, 1).contiguous()\n        result = (encoder_a, encoder_b)\n        return result\n\n\nclass SelfAttention(nn.Module):\n\n    def __init__(self, out_channels, embed_dim, num_heads, project_input=False, gated=False, downsample=False):\n        super().__init__()\n        self.attention = DownsampledMultiHeadAttention(\n            out_channels, embed_dim, num_heads, dropout=0, bias=True,\n            project_input=project_input, gated=gated, downsample=downsample,\n        )\n        self.in_proj_q = Linear(out_channels, embed_dim)\n        self.in_proj_k = Linear(out_channels, embed_dim)\n        self.in_proj_v = Linear(out_channels, embed_dim)\n        self.ln = LayerNorm(out_channels)\n\n    def forward(self, x):\n        residual = x\n        query = self.in_proj_q(x)\n        key = self.in_proj_k(x)\n        value = self.in_proj_v(x)\n        x, _ = self.attention(query, key, value, mask_future_timesteps=True, use_scalar_bias=True)\n        return self.ln(x + residual)\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    m.weight.data.normal_(0, 0.1)\n    return m\n\n\ndef PositionalEmbedding(num_embeddings, embedding_dim, padding_idx):\n    m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n    m.weight.data.normal_(0, 0.1)\n    return m\n\n\ndef Linear(in_features, out_features, dropout=0.):\n    """"""Weight-normalized Linear layer (input: N x T x C)""""""\n    m = nn.Linear(in_features, out_features)\n    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n    m.bias.data.zero_()\n    return m\n\n\ndef LinearizedConv1d(in_channels, out_channels, kernel_size, dropout=0., **kwargs):\n    """"""Weight-normalized Conv1d layer optimized for decoding""""""\n    m = LinearizedConvolution(in_channels, out_channels, kernel_size, **kwargs)\n    std = math.sqrt((4 * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n    m.weight.data.normal_(mean=0, std=std)\n    m.bias.data.zero_()\n    return m\n\n\ndef ConvTBC(in_channels, out_channels, kernel_size, dropout=0, **kwargs):\n    """"""Weight-normalized Conv1d layer""""""\n    from fairseq.modules import ConvTBC\n    m = ConvTBC(in_channels, out_channels, kernel_size, **kwargs)\n    std = math.sqrt((4 * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n    m.weight.data.normal_(mean=0, std=std)\n    m.bias.data.zero_()\n    return m\n\n\n@register_model_architecture(\'fconv_self_att\', \'fconv_self_att\')\ndef base_architecture(args):\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 512)\n    args.encoder_layers = getattr(args, \'encoder_layers\', \'[(512, 3)] * 3\')\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.decoder_layers = getattr(args, \'decoder_layers\', \'[(512, 3)] * 8\')\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 256)\n    args.decoder_attention = getattr(args, \'decoder_attention\', \'True\')\n    args.self_attention = getattr(args, \'self_attention\', \'False\')\n    args.encoder_attention = getattr(args, \'encoder_attention\', \'False\')\n    args.multihead_attention_nheads = getattr(args, \'multihead_attention_nheads\', 1)\n    args.multihead_self_attention_nheads = getattr(args, \'multihead_self_attention_nheads\', 1)\n    args.encoder_attention_nheads = getattr(args, \'encoder_attention_nheads\', 1)\n    args.project_input = getattr(args, \'project_input\', \'False\')\n    args.gated_attention = getattr(args, \'gated_attention\', \'False\')\n    args.downsample = getattr(args, \'downsample\', \'False\')\n    args.pretrained_checkpoint = getattr(args, \'pretrained_checkpoint\', \'\')\n    args.pretrained = getattr(args, \'pretrained\', \'False\')\n\n\n@register_model_architecture(\'fconv_self_att\', \'fconv_self_att_wp\')\ndef fconv_self_att_wp(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 256)\n    args.encoder_layers = getattr(args, \'encoder_layers\', \'[(128, 3)] * 2 + [(512,3)] * 1\')\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 256)\n    args.decoder_layers = getattr(args, \'decoder_layers\', \'[(512, 4)] * 4 + [(768, 4)] * 2 + [(1024, 4)] * 1\')\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 256)\n    args.self_attention = getattr(args, \'self_attention\', \'True\')\n    args.multihead_self_attention_nheads = getattr(args, \'multihead_self_attention_nheads\', 4)\n    args.project_input = getattr(args, \'project_input\', \'True\')\n    args.gated_attention = getattr(args, \'gated_attention\', \'True\')\n    args.downsample = getattr(args, \'downsample\', \'True\')\n    base_architecture(args)\n'"
fairseq/models/lightconv.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqIncrementalDecoder,\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import (\n    AdaptiveSoftmax,\n    DynamicConv,\n    LayerNorm,\n    PositionalEmbedding,\n    LightweightConv,\n    MultiheadAttention,\n)\n\n\n@register_model(\'lightconv\')\nclass LightConvModel(FairseqEncoderDecoderModel):\n    """"""\n    LightConv and DynamicConv model from `""Pay Less Attention with Lightweight and Dynamic Convolutions"" (Wu, et al, 2019)\n    <https://openreview.net/pdf?id=SkVhlh09tX>`_.\n    To use LightConv please set ``--encoder-conv-type lightweight --decoder-conv-type lightweight``\n    To use DynamicConv please set ``--encoder-conv-type dynamic --decoder-conv-type dynamic``\n\n    Args:\n        encoder (LightConvEncoder): the encoder\n        decoder (LightConvDecoder): the decoder\n\n    The LightConv model provides the following named architectures and\n    command-line arguments:\n\n    .. argparse::\n        :ref: fairseq.models.lightconv_parser\n        :prog:\n    """"""\n\n    @classmethod\n    def hub_models(cls):\n        # fmt: off\n\n        def moses_subword(path):\n            return {\n                \'path\': path,\n                \'tokenizer\': \'moses\',\n                \'bpe\': \'subword_nmt\',\n            }\n\n        return {\n            \'lightconv.no_glu.iwslt14.de-en\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/iwslt14.de-en.lightconv.tar.gz\'),\n            \'dynamicconv.no_glu.iwslt14.de-en\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/iwslt14.de-en.dynamicconv.tar.gz\'),\n            \'lightconv.no_glu.wmt16.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt16.en-de.joined-dict.lightconv.tar.gz\'),\n            \'dynamicconv.no_glu.wmt16.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt16.en-de.joined-dict.dynamicconv.tar.gz\'),\n            \'lightconv.glu.wmt16.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt16.en-de.joined-dict.lightconv-glu.tar.gz\'),\n            \'dynamicconv.glu.wmt16.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt16.en-de.joined-dict.dynamicconv-glu.tar.gz\'),\n            \'lightconv.glu.wmt17.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt16.en-de.joined-dict.lightconv-glu.tar.gz\'),\n            \'dynamicconv.glu.wmt17.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt16.en-de.joined-dict.dynamicconv-glu.tar.gz\'),\n            \'lightconv.glu.wmt14.en-fr\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt14.en-fr.joined-dict.lightconv-glu.tar.gz\'),\n            \'dynamicconv.glu.wmt14.en-fr\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt14.en-fr.joined-dict.dynamicconv-glu.tar.gz\'),\n            \'lightconv.glu.wmt17.zh-en\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt17.zh-en.lightconv-glu.tar.gz\'),\n            \'dynamicconv.glu.wmt17.zh-en\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/dynamicconv/wmt17.zh-en.dynamicconv-glu.tar.gz\'),\n        }\n        # fmt: on\n\n    def __init__(self, encoder, decoder):\n        super().__init__(encoder, decoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--attention-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for attention weights\')\n        parser.add_argument(\'--relu-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability after ReLU in FFN\')\n        parser.add_argument(\'--input-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability of the inputs\')\n        parser.add_argument(\'--encoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained encoder embedding\')\n        parser.add_argument(\'--encoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--encoder-conv-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--encoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension for FFN\')\n        parser.add_argument(\'--encoder-layers\', type=int, metavar=\'N\',\n                            help=\'num encoder layers\')\n        parser.add_argument(\'--encoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num encoder attention heads or LightConv/DynamicConv heads\')\n        parser.add_argument(\'--encoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each encoder block\')\n        parser.add_argument(\'--encoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the encoder\')\n        parser.add_argument(\'--decoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained decoder embedding\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-conv-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension for FFN\')\n        parser.add_argument(\'--decoder-layers\', type=int, metavar=\'N\',\n                            help=\'num decoder layers\')\n        parser.add_argument(\'--decoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num decoder attention heads or LightConv/DynamicConv heads\')\n        parser.add_argument(\'--decoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the decoder\')\n        parser.add_argument(\'--decoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each decoder block\')\n        parser.add_argument(\'--share-decoder-input-output-embed\', action=\'store_true\',\n                            help=\'share decoder input and output embeddings\')\n        parser.add_argument(\'--share-all-embeddings\', action=\'store_true\',\n                            help=\'share encoder, decoder and output embeddings\'\n                                 \' (requires shared dictionary and embed dim)\')\n        parser.add_argument(\'--adaptive-softmax-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive softmax cutoff points. \'\n                                 \'Must be used with adaptive_loss criterion\'),\n        parser.add_argument(\'--adaptive-softmax-dropout\', type=float, metavar=\'D\',\n                            help=\'sets adaptive softmax dropout for the tail projections\')\n\n        """"""LightConv and DynamicConv arguments""""""\n        parser.add_argument(\'--encoder-kernel-size-list\', type=lambda x: options.eval_str_list(x, int),\n                            help=\'list of kernel size (default: ""[3,7,15,31,31,31,31]"")\')\n        parser.add_argument(\'--decoder-kernel-size-list\', type=lambda x: options.eval_str_list(x, int),\n                            help=\'list of kernel size (default: ""[3,7,15,31,31,31]"")\')\n        parser.add_argument(\'--encoder-glu\', type=options.eval_bool,\n                            help=\'glu after in proj\')\n        parser.add_argument(\'--decoder-glu\', type=options.eval_bool,\n                            help=\'glu after in proj\')\n        parser.add_argument(\'--encoder-conv-type\', default=\'dynamic\', type=str,\n                            choices=[\'dynamic\', \'lightweight\'],\n                            help=\'type of convolution\')\n        parser.add_argument(\'--decoder-conv-type\', default=\'dynamic\', type=str,\n                            choices=[\'dynamic\', \'lightweight\'],\n                            help=\'type of convolution\')\n        parser.add_argument(\'--weight-softmax\', default=True, type=options.eval_bool)\n        parser.add_argument(\'--weight-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for conv weights\')\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present in older models\n        base_architecture(args)\n\n        if not hasattr(args, \'max_source_positions\'):\n            args.max_source_positions = 1024\n        if not hasattr(args, \'max_target_positions\'):\n            args.max_target_positions = 1024\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        def build_embedding(dictionary, embed_dim, path=None):\n            num_embeddings = len(dictionary)\n            padding_idx = dictionary.pad()\n            emb = Embedding(num_embeddings, embed_dim, padding_idx)\n            # if provided, load from preloaded dictionaries\n            if path:\n                embed_dict = utils.parse_embedding(path)\n                utils.load_embedding(embed_dict, dictionary, emb)\n            return emb\n\n        if args.share_all_embeddings:\n            if src_dict != tgt_dict:\n                raise RuntimeError(\'--share-all-embeddings requires a joined dictionary\')\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise RuntimeError(\n                    \'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim\')\n            if args.decoder_embed_path and (\n                    args.decoder_embed_path != args.encoder_embed_path):\n                raise RuntimeError(\'--share-all-embeddings not compatible with --decoder-embed-path\')\n            encoder_embed_tokens = build_embedding(\n                src_dict, args.encoder_embed_dim, args.encoder_embed_path\n            )\n            decoder_embed_tokens = encoder_embed_tokens\n            args.share_decoder_input_output_embed = True\n        else:\n            encoder_embed_tokens = build_embedding(\n                src_dict, args.encoder_embed_dim, args.encoder_embed_path\n            )\n            decoder_embed_tokens = build_embedding(\n                tgt_dict, args.decoder_embed_dim, args.decoder_embed_path\n            )\n\n        encoder = LightConvEncoder(args, src_dict, encoder_embed_tokens)\n        decoder = LightConvDecoder(args, tgt_dict, decoder_embed_tokens)\n        return LightConvModel(encoder, decoder)\n\n\nclass LightConvEncoder(FairseqEncoder):\n    """"""\n    LightConv encoder consisting of *args.encoder_layers* layers. Each layer\n    is a :class:`LightConvEncoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): encoding dictionary\n        embed_tokens (torch.nn.Embedding): input embedding\n    """"""\n\n    def __init__(self, args, dictionary, embed_tokens):\n        super().__init__(dictionary)\n        self.dropout = args.dropout\n\n        embed_dim = embed_tokens.embedding_dim\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = args.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_scale = math.sqrt(embed_dim)\n        self.embed_positions = PositionalEmbedding(\n            args.max_source_positions, embed_dim, self.padding_idx,\n            learned=args.encoder_learned_pos,\n        ) if not args.no_token_positional_embeddings else None\n\n        self.layers = nn.ModuleList([])\n        self.layers.extend([\n            LightConvEncoderLayer(args, kernel_size=args.encoder_kernel_size_list[i])\n            for i in range(args.encoder_layers)\n        ])\n        self.register_buffer(\'version\', torch.Tensor([2]))\n        self.normalize = args.encoder_normalize_before\n        if self.normalize:\n            self.layer_norm = LayerNorm(embed_dim)\n\n    def forward(self, src_tokens, **unused):\n        """"""\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n\n        Returns:\n            dict:\n                - **encoder_out** (Tensor): the last encoder layer\'s output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n        """"""\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(src_tokens)\n        if self.embed_positions is not None:\n            x += self.embed_positions(src_tokens)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # compute padding mask\n        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        # encoder layers\n        for layer in self.layers:\n            x = layer(x, encoder_padding_mask)\n\n        if self.normalize:\n            x = self.layer_norm(x)\n\n        return {\n            \'encoder_out\': x,  # T x B x C\n            \'encoder_padding_mask\': encoder_padding_mask,  # B x T\n        }\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        """"""\n        if encoder_out[\'encoder_out\'] is not None:\n            encoder_out[\'encoder_out\'] = \\\n                encoder_out[\'encoder_out\'].index_select(1, new_order)\n        if encoder_out[\'encoder_padding_mask\'] is not None:\n            encoder_out[\'encoder_padding_mask\'] = \\\n                encoder_out[\'encoder_padding_mask\'].index_select(0, new_order)\n        return encoder_out\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        if self.embed_positions is None:\n            return self.max_source_positions\n        return min(self.max_source_positions, self.embed_positions.max_positions)\n\n\nclass LightConvDecoder(FairseqIncrementalDecoder):\n    """"""\n    LightConv decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`LightConvDecoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): decoding dictionary\n        embed_tokens (torch.nn.Embedding): output embedding\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs.\n            Default: ``False``\n    """"""\n\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False, final_norm=True):\n        super().__init__(dictionary)\n        self.dropout = args.dropout\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n\n        input_embed_dim = embed_tokens.embedding_dim\n        embed_dim = args.decoder_embed_dim\n        output_embed_dim = args.decoder_output_dim\n\n        padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = args.max_target_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_scale = math.sqrt(embed_dim)  # todo: try with input_embed_dim\n\n        self.project_in_dim = Linear(input_embed_dim, embed_dim, bias=False) if embed_dim != input_embed_dim else None\n\n        self.embed_positions = PositionalEmbedding(\n            args.max_target_positions, embed_dim, padding_idx,\n            learned=args.decoder_learned_pos,\n        ) if not args.no_token_positional_embeddings else None\n\n        self.layers = nn.ModuleList([])\n        self.layers.extend([\n            LightConvDecoderLayer(args, no_encoder_attn, kernel_size=args.decoder_kernel_size_list[i])\n            for i in range(args.decoder_layers)\n        ])\n\n        self.adaptive_softmax = None\n\n        self.project_out_dim = Linear(embed_dim, output_embed_dim, bias=False) \\\n            if embed_dim != output_embed_dim and not args.tie_adaptive_weights else None\n\n        if args.adaptive_softmax_cutoff is not None:\n            self.adaptive_softmax = AdaptiveSoftmax(\n                len(dictionary),\n                output_embed_dim,\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.adaptive_softmax_dropout,\n                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,\n                factor=args.adaptive_softmax_factor,\n                tie_proj=args.tie_adaptive_proj,\n            )\n        elif not self.share_input_output_embed:\n            self.embed_out = nn.Parameter(torch.Tensor(len(dictionary), output_embed_dim))\n            nn.init.normal_(self.embed_out, mean=0, std=output_embed_dim ** -0.5)\n        self.register_buffer(\'version\', torch.Tensor([2]))\n        self.normalize = args.decoder_normalize_before and final_norm\n        if self.normalize:\n            self.layer_norm = LayerNorm(embed_dim)\n\n    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):\n        """"""\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the last decoder layer\'s output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer\'s attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        """"""\n        # embed positions\n        positions = self.embed_positions(\n            prev_output_tokens,\n            incremental_state=incremental_state,\n        ) if self.embed_positions is not None else None\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            if positions is not None:\n                positions = positions[:, -1:]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        if positions is not None:\n            x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        attn = None\n\n        inner_states = [x]\n\n        # decoder layers\n        for layer in self.layers:\n            x, attn = layer(\n                x,\n                encoder_out[\'encoder_out\'] if encoder_out is not None else None,\n                encoder_out[\'encoder_padding_mask\'] if encoder_out is not None else None,\n                incremental_state,\n            )\n            inner_states.append(x)\n\n        if self.normalize:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        if self.adaptive_softmax is None:\n            # project back to size of vocabulary\n            if self.share_input_output_embed:\n                x = F.linear(x, self.embed_tokens.weight)\n            else:\n                x = F.linear(x, self.embed_out)\n\n        return x, {\'attn\': attn, \'inner_states\': inner_states}\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return min(self.max_target_positions, self.embed_positions.max_positions)\n\n    def buffered_future_mask(self, tensor):\n        dim = tensor.size(0)\n        if not hasattr(self, \'_future_mask\') or self._future_mask is None or self._future_mask.device != tensor.device:\n            self._future_mask = torch.triu(utils.fill_with_neg_inf(tensor.new(dim, dim)), 1)\n        if self._future_mask.size(0) < dim:\n            self._future_mask = torch.triu(utils.fill_with_neg_inf(self._future_mask.resize_(dim, dim)), 1)\n        return self._future_mask[:dim, :dim]\n\n\nclass LightConvEncoderLayer(nn.Module):\n    """"""Encoder layer block.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        kernel_size: kernel size of the convolution\n    """"""\n\n    def __init__(self, args, kernel_size=0):\n        super().__init__()\n        self.embed_dim = args.encoder_embed_dim\n        self.conv_dim = args.encoder_conv_dim\n        padding_l = kernel_size // 2 if kernel_size % 2 == 1 else ((kernel_size - 1) // 2, kernel_size // 2)\n\n        if args.encoder_glu:\n            self.linear1 = Linear(self.embed_dim, 2*self.conv_dim)\n            self.act = nn.GLU()\n        else:\n            self.linear1 = Linear(self.embed_dim, self.conv_dim)\n            self.act = None\n        if args.encoder_conv_type == \'lightweight\':\n            self.conv = LightweightConv(self.conv_dim, kernel_size, padding_l=padding_l,\n                                        weight_softmax=args.weight_softmax,\n                                        num_heads=args.encoder_attention_heads,\n                                        weight_dropout=args.weight_dropout)\n        elif args.encoder_conv_type == \'dynamic\':\n            self.conv = DynamicConv(self.conv_dim, kernel_size, padding_l=padding_l,\n                                    weight_softmax=args.weight_softmax,\n                                    num_heads=args.encoder_attention_heads,\n                                    weight_dropout=args.weight_dropout)\n        else:\n            raise NotImplementedError\n        self.linear2 = Linear(self.conv_dim, self.embed_dim)\n\n        self.dropout = args.dropout\n        self.relu_dropout = args.relu_dropout\n        self.input_dropout = args.input_dropout\n        self.normalize_before = args.encoder_normalize_before\n        self.fc1 = Linear(self.embed_dim, args.encoder_ffn_embed_dim)\n        self.fc2 = Linear(args.encoder_ffn_embed_dim, self.embed_dim)\n        self.layer_norms = nn.ModuleList([LayerNorm(self.embed_dim) for _ in range(2)])\n\n    def forward(self, x, encoder_padding_mask):\n        """"""\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        """"""\n        residual = x\n        x = self.maybe_layer_norm(0, x, before=True)\n        x = F.dropout(x, p=self.input_dropout, training=self.training)\n        x = self.linear1(x)\n        if self.act is not None:\n            x = self.act(x)\n        if encoder_padding_mask is not None:\n            x = x.masked_fill(encoder_padding_mask.transpose(0, 1).unsqueeze(2), 0)\n        x = self.conv(x)\n        x = self.linear2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(0, x, after=True)\n\n        residual = x\n        x = self.maybe_layer_norm(1, x, before=True)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, p=self.relu_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(1, x, after=True)\n        return x\n\n    def maybe_layer_norm(self, i, x, before=False, after=False):\n        assert before ^ after\n        if after ^ self.normalize_before:\n            return self.layer_norms[i](x)\n        else:\n            return x\n\n    def extra_repr(self):\n        return \'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}\'.format(\n            self.dropout, self.relu_dropout, self.input_dropout, self.normalize_before)\n\n\nclass LightConvDecoderLayer(nn.Module):\n    """"""Decoder layer block.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs.\n            Default: ``False``\n        kernel_size: kernel size of the convolution\n    """"""\n\n    def __init__(self, args, no_encoder_attn=False, kernel_size=0):\n        super().__init__()\n        self.embed_dim = args.decoder_embed_dim\n        self.conv_dim = args.decoder_conv_dim\n        if args.decoder_glu:\n            self.linear1 = Linear(self.embed_dim, 2*self.conv_dim)\n            self.act = nn.GLU()\n        else:\n            self.linear1 = Linear(self.embed_dim, self.conv_dim)\n            self.act = None\n        if args.decoder_conv_type == \'lightweight\':\n            self.conv = LightweightConv(self.conv_dim, kernel_size, padding_l=kernel_size-1,\n                                        weight_softmax=args.weight_softmax,\n                                        num_heads=args.decoder_attention_heads,\n                                        weight_dropout=args.weight_dropout)\n        elif args.decoder_conv_type == \'dynamic\':\n            self.conv = DynamicConv(self.conv_dim, kernel_size, padding_l=kernel_size-1,\n                                    weight_softmax=args.weight_softmax,\n                                    num_heads=args.decoder_attention_heads,\n                                    weight_dropout=args.weight_dropout)\n        else:\n            raise NotImplementedError\n        self.linear2 = Linear(self.conv_dim, self.embed_dim)\n\n        self.dropout = args.dropout\n        self.relu_dropout = args.relu_dropout\n        self.input_dropout = args.input_dropout\n        self.normalize_before = args.decoder_normalize_before\n\n        self.conv_layer_norm = LayerNorm(self.embed_dim)\n\n        if no_encoder_attn:\n            self.encoder_attn = None\n            self.encoder_attn_layer_norm = None\n        else:\n            self.encoder_attn = MultiheadAttention(\n                self.embed_dim, args.decoder_attention_heads,\n                dropout=args.attention_dropout, encoder_decoder_attention=True\n            )\n            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n\n        self.fc1 = Linear(self.embed_dim, args.decoder_ffn_embed_dim)\n        self.fc2 = Linear(args.decoder_ffn_embed_dim, self.embed_dim)\n\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n        self.need_attn = True\n\n    def forward(self, x, encoder_out, encoder_padding_mask, incremental_state,\n                prev_conv_state=None, prev_attn_state=None, conv_mask=None,\n                conv_padding_mask=None):\n        """"""\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        """"""\n        residual = x\n        x = self.maybe_layer_norm(self.conv_layer_norm, x, before=True)\n        if prev_conv_state is not None:\n            if incremental_state is None:\n                incremental_state = {}\n            self.conv._set_input_buffer(incremental_state, prev_conv_state)\n        x = F.dropout(x, p=self.input_dropout, training=self.training)\n        x = self.linear1(x)\n        if self.act is not None:\n            x = self.act(x)\n        x = self.conv(x, incremental_state=incremental_state)\n        x = self.linear2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(self.conv_layer_norm, x, after=True)\n\n        attn = None\n        if self.encoder_attn is not None:\n            residual = x\n            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, before=True)\n            if prev_attn_state is not None:\n                if incremental_state is None:\n                    incremental_state = {}\n                prev_key, prev_value = prev_attn_state\n                saved_state = {""prev_key"": prev_key, ""prev_value"": prev_value}\n                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n            x, attn = self.encoder_attn(\n                query=x,\n                key=encoder_out,\n                value=encoder_out,\n                key_padding_mask=encoder_padding_mask,\n                incremental_state=incremental_state,\n                static_kv=True,\n                need_weights=(not self.training and self.need_attn),\n            )\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = residual + x\n            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, after=True)\n\n        residual = x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, p=self.relu_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n        return x, attn\n\n    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n        assert before ^ after\n        if after ^ self.normalize_before:\n            return layer_norm(x)\n        else:\n            return x\n\n    def make_generation_fast_(self, need_attn=False, **kwargs):\n        self.need_attn = need_attn\n\n    def extra_repr(self):\n        return \'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}\'.format(\n            self.dropout, self.relu_dropout, self.input_dropout, self.normalize_before)\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.)\n    return m\n\n\n@register_model_architecture(\'lightconv\', \'lightconv\')\ndef base_architecture(args):\n    args.encoder_embed_path = getattr(args, \'encoder_embed_path\', None)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 512)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 2048)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 7)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 8)\n    args.encoder_normalize_before = getattr(args, \'encoder_normalize_before\', False)\n    args.encoder_learned_pos = getattr(args, \'encoder_learned_pos\', False)\n    args.decoder_embed_path = getattr(args, \'decoder_embed_path\', None)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', args.encoder_ffn_embed_dim)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 6)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 8)\n    args.decoder_normalize_before = getattr(args, \'decoder_normalize_before\', False)\n    args.decoder_learned_pos = getattr(args, \'decoder_learned_pos\', False)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.)\n    args.relu_dropout = getattr(args, \'relu_dropout\', 0.)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', None)\n    args.adaptive_softmax_dropout = getattr(args, \'adaptive_softmax_dropout\', 0)\n    args.share_decoder_input_output_embed = getattr(args, \'share_decoder_input_output_embed\', False)\n    args.share_all_embeddings = getattr(args, \'share_all_embeddings\', False)\n    args.no_token_positional_embeddings = getattr(args, \'no_token_positional_embeddings\', False)\n\n    args.decoder_output_dim = getattr(args, \'decoder_output_dim\', args.decoder_embed_dim)\n    args.decoder_input_dim = getattr(args, \'decoder_input_dim\', args.decoder_embed_dim)\n\n    args.encoder_conv_dim = getattr(args, \'encoder_conv_dim\', args.encoder_embed_dim)\n    args.decoder_conv_dim = getattr(args, \'decoder_conv_dim\', args.decoder_embed_dim)\n\n    args.encoder_kernel_size_list = getattr(args, \'encoder_kernel_size_list\', [3, 7, 15, 31, 31, 31, 31])\n    args.decoder_kernel_size_list = getattr(args, \'decoder_kernel_size_list\', [3, 7, 15, 31, 31, 31])\n    if len(args.encoder_kernel_size_list) == 1:\n        args.encoder_kernel_size_list = args.encoder_kernel_size_list * args.encoder_layers\n    if len(args.decoder_kernel_size_list) == 1:\n        args.decoder_kernel_size_list = args.decoder_kernel_size_list * args.decoder_layers\n    assert len(args.encoder_kernel_size_list) == args.encoder_layers, ""encoder_kernel_size_list doesn\'t match encoder_layers""\n    assert len(args.decoder_kernel_size_list) == args.decoder_layers, ""decoder_kernel_size_list doesn\'t match decoder_layers""\n    args.encoder_glu = getattr(args, \'encoder_glu\', True)\n    args.decoder_glu = getattr(args, \'decoder_glu\', True)\n    args.input_dropout = getattr(args, \'input_dropout\', 0.1)\n    args.weight_dropout = getattr(args, \'weight_dropout\', args.attention_dropout)\n\n\n@register_model_architecture(\'lightconv\', \'lightconv_iwslt_de_en\')\ndef lightconv_iwslt_de_en(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 512)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 1024)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 4)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 7)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 1024)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 4)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 6)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.weight_dropout = getattr(args, \'weight_dropout\', 0.1)\n    args.encoder_glu = getattr(args, \'encoder_glu\', False)\n    args.decoder_glu = getattr(args, \'decoder_glu\', False)\n    args.input_dropout = getattr(args, \'input_dropout\', 0.0)\n    base_architecture(args)\n\n\n@register_model_architecture(\'lightconv\', \'lightconv_wmt_en_de\')\ndef lightconv_wmt_en_de(args):\n    base_architecture(args)\n\n\n@register_model_architecture(\'lightconv\', \'lightconv_wmt_en_de_big\')\ndef lightconv_wmt_en_de_big(args):\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1024)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 4096)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 16)\n    args.encoder_normalize_before = getattr(args, \'encoder_normalize_before\', False)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 1024)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 4096)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 16)\n    args.dropout = getattr(args, \'dropout\', 0.3)\n    base_architecture(args)\n\n\n@register_model_architecture(\'lightconv\', \'lightconv_wmt_en_fr_big\')\ndef lightconv_wmt_en_fr_big(args):\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    lightconv_wmt_en_de_big(args)\n\n\n@register_model_architecture(\'lightconv\', \'lightconv_wmt_zh_en_big\')\ndef lightconv_wmt_zh_en_big(args):\n    args.dropout = getattr(args, \'dropout\', 0.2)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.2)\n    args.weight_dropout = getattr(args, \'weight_dropout\', 0.2)\n    lightconv_wmt_en_de_big(args)\n'"
fairseq/models/lightconv_lm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import options\nfrom fairseq.models import (\n    FairseqLanguageModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.lightconv import (\n    Embedding,\n    LightConvDecoder,\n)\nfrom fairseq.modules import (\n    AdaptiveInput,\n    CharacterTokenEmbedder,\n)\n\n\n@register_model(\'lightconv_lm\')\nclass LightConvLanguageModel(FairseqLanguageModel):\n    def __init__(self, decoder):\n        super().__init__(decoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\'--dropout\', default=0.1, type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--attention-dropout\', default=0., type=float, metavar=\'D\',\n                            help=\'dropout probability for attention weights\')\n        parser.add_argument(\'--relu-dropout\', default=0., type=float, metavar=\'D\',\n                            help=\'dropout probability after ReLU in FFN\')\n        parser.add_argument(\'--input-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability of the inputs\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-output-dim\', type=int, metavar=\'N\',\n                            help=\'decoder output dimension\')\n        parser.add_argument(\'--decoder-input-dim\', type=int, metavar=\'N\',\n                            help=\'decoder input dimension\')\n        parser.add_argument(\'--decoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension for FFN\')\n        parser.add_argument(\'--decoder-layers\', type=int, metavar=\'N\',\n                            help=\'num decoder layers\')\n        parser.add_argument(\'--decoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num decoder attention heads or LightConv/DynamicConv heads\')\n        parser.add_argument(\'--decoder-normalize-before\', default=False, action=\'store_true\',\n                            help=\'apply layernorm before each decoder block\')\n        parser.add_argument(\'--adaptive-softmax-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive softmax cutoff points. \'\n                                 \'Must be used with adaptive_loss criterion\')\n        parser.add_argument(\'--adaptive-softmax-dropout\', type=float, metavar=\'D\',\n                            help=\'sets adaptive softmax dropout for the tail projections\')\n        parser.add_argument(\'--adaptive-softmax-factor\', type=float, metavar=\'N\',\n                            help=\'adaptive input factor\')\n        parser.add_argument(\'--no-token-positional-embeddings\', default=False, action=\'store_true\',\n                            help=\'if set, disables positional embeddings (outside self attention)\')\n        parser.add_argument(\'--share-decoder-input-output-embed\', default=False, action=\'store_true\',\n                            help=\'share decoder input and output embeddings\')\n        parser.add_argument(\'--character-embeddings\', default=False, action=\'store_true\',\n                            help=\'if set, uses character embedding convolutions to produce token embeddings\')\n        parser.add_argument(\'--character-filters\', type=str, metavar=\'LIST\',\n                            default=\'[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]\',\n                            help=\'size of character embeddings\')\n        parser.add_argument(\'--character-embedding-dim\', type=int, metavar=\'N\', default=4,\n                            help=\'size of character embeddings\')\n        parser.add_argument(\'--char-embedder-highway-layers\', type=int, metavar=\'N\', default=2,\n                            help=\'number of highway layers for character token embeddder\')\n        parser.add_argument(\'--adaptive-input\', default=False, action=\'store_true\',\n                            help=\'if set, uses adaptive input\')\n        parser.add_argument(\'--adaptive-input-factor\', type=float, metavar=\'N\',\n                            help=\'adaptive input factor\')\n        parser.add_argument(\'--adaptive-input-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive input cutoff points.\')\n        parser.add_argument(\'--tie-adaptive-weights\', action=\'store_true\',\n                            help=\'if set, ties the weights of adaptive softmax and adaptive input\')\n        parser.add_argument(\'--tie-adaptive-proj\', action=\'store_true\',\n                            help=\'if set, ties the projection weights of adaptive softmax and adaptive input\')\n        parser.add_argument(\'--decoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the decoder\')\n\n        """"""LightConv and DynamicConv arguments""""""\n        parser.add_argument(\'--decoder-kernel-size-list\', type=lambda x: options.eval_str_list(x, int),\n                            help=\'list of kernel size (default: ""[3,7,15,31,31,31]"")\')\n        parser.add_argument(\'--decoder-glu\', type=options.eval_bool,\n                            help=\'glu after in proj\')\n        parser.add_argument(\'--decoder-conv-type\', default=\'dynamic\', type=str,\n                            choices=[\'dynamic\', \'lightweight\'],\n                            help=\'type of convolution\')\n        parser.add_argument(\'--weight-softmax\', default=True, type=options.eval_bool)\n        parser.add_argument(\'--weight-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for conv weights\')\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present in older models\n        base_lm_architecture(args)\n\n        if getattr(args, \'max_source_positions\', None) is None:\n            args.max_source_positions = args.tokens_per_sample\n        if getattr(args, \'max_target_positions\', None) is None:\n            args.max_target_positions = args.tokens_per_sample\n\n        if args.character_embeddings:\n            embed_tokens = CharacterTokenEmbedder(task.dictionary, eval(args.character_filters),\n                                                  args.character_embedding_dim,\n                                                  args.decoder_embed_dim,\n                                                  args.char_embedder_highway_layers,\n                                                  )\n        elif args.adaptive_input:\n            embed_tokens = AdaptiveInput(len(task.dictionary), task.dictionary.pad(), args.decoder_input_dim,\n                                         args.adaptive_input_factor, args.decoder_embed_dim,\n                                         options.eval_str_list(args.adaptive_input_cutoff, type=int))\n        else:\n            embed_tokens = Embedding(len(task.dictionary), args.decoder_input_dim, task.dictionary.pad())\n\n        if args.tie_adaptive_weights:\n            assert args.adaptive_input\n            assert args.adaptive_input_factor == args.adaptive_softmax_factor\n            assert args.adaptive_softmax_cutoff == args.adaptive_input_cutoff, \'{} != {}\'.format(\n                args.adaptive_softmax_cutoff, args.adaptive_input_cutoff)\n            assert args.decoder_input_dim == args.decoder_output_dim\n\n        decoder = LightConvDecoder(args, task.output_dictionary, embed_tokens, no_encoder_attn=True, final_norm=False)\n        return LightConvLanguageModel(decoder)\n\n\n@register_model_architecture(\'lightconv_lm\', \'lightconv_lm\')\ndef base_lm_architecture(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 2048)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 6)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 8)\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', None)\n    args.adaptive_softmax_dropout = getattr(args, \'adaptive_softmax_dropout\', 0)\n    args.adaptive_softmax_factor = getattr(args, \'adaptive_softmax_factor\', 4)\n    args.decoder_learned_pos = getattr(args, \'decoder_learned_pos\', False)\n\n    args.character_embeddings = getattr(args, \'character_embeddings\', False)\n\n    args.decoder_output_dim = getattr(args, \'decoder_output_dim\', args.decoder_embed_dim)\n    args.decoder_input_dim = getattr(args, \'decoder_input_dim\', args.decoder_embed_dim)\n    args.decoder_conv_dim = getattr(args, \'decoder_conv_dim\', args.decoder_embed_dim)\n\n    # The model training is not stable without this\n    args.decoder_normalize_before = True\n\n    args.adaptive_input = getattr(args, \'adaptive_input\', False)\n    args.adaptive_input_factor = getattr(args, \'adaptive_input_factor\', 4)\n    args.adaptive_input_cutoff = getattr(args, \'adaptive_input_cutoff\', None)\n\n    args.tie_adaptive_weights = getattr(args, \'tie_adaptive_weights\', False)\n    args.tie_adaptive_proj = getattr(args, \'tie_adaptive_proj\', False)\n\n    args.decoder_kernel_size_list = getattr(args, \'decoder_kernel_size_list\', [3, 7, 15, 31, 31, 31])\n    if len(args.decoder_kernel_size_list) == 1:\n        args.decoder_kernel_size_list = args.decoder_kernel_size_list * args.decoder_layers\n    assert len(args.decoder_kernel_size_list) == args.decoder_layers, ""decoder_kernel_size_list doesn\'t match decoder_layers""\n    args.decoder_glu = getattr(args, \'decoder_glu\', True)\n    args.input_dropout = getattr(args, \'input_dropout\', 0.1)\n    args.weight_dropout = getattr(args, \'weight_dropout\', args.attention_dropout)\n\n\n@register_model_architecture(\'lightconv_lm\', \'lightconv_lm_gbw\')\ndef lightconv_lm_gbw(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 4096)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 16)\n    base_lm_architecture(args)\n'"
fairseq/models/lstm.py,16,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqIncrementalDecoder,\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import AdaptiveSoftmax\nfrom torch import Tensor\nfrom typing import Dict, List, Optional, Tuple\n\n\nDEFAULT_MAX_SOURCE_POSITIONS = 1e5\nDEFAULT_MAX_TARGET_POSITIONS = 1e5\n\n\n@register_model(\'lstm\')\nclass LSTMModel(FairseqEncoderDecoderModel):\n    def __init__(self, encoder, decoder):\n        super().__init__(encoder, decoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--encoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--encoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained encoder embedding\')\n        parser.add_argument(\'--encoder-freeze-embed\', action=\'store_true\',\n                            help=\'freeze encoder embeddings\')\n        parser.add_argument(\'--encoder-hidden-size\', type=int, metavar=\'N\',\n                            help=\'encoder hidden size\')\n        parser.add_argument(\'--encoder-layers\', type=int, metavar=\'N\',\n                            help=\'number of encoder layers\')\n        parser.add_argument(\'--encoder-bidirectional\', action=\'store_true\',\n                            help=\'make all layers of encoder bidirectional\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained decoder embedding\')\n        parser.add_argument(\'--decoder-freeze-embed\', action=\'store_true\',\n                            help=\'freeze decoder embeddings\')\n        parser.add_argument(\'--decoder-hidden-size\', type=int, metavar=\'N\',\n                            help=\'decoder hidden size\')\n        parser.add_argument(\'--decoder-layers\', type=int, metavar=\'N\',\n                            help=\'number of decoder layers\')\n        parser.add_argument(\'--decoder-out-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder output embedding dimension\')\n        parser.add_argument(\'--decoder-attention\', type=str, metavar=\'BOOL\',\n                            help=\'decoder attention\')\n        parser.add_argument(\'--adaptive-softmax-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive softmax cutoff points. \'\n                                 \'Must be used with adaptive_loss criterion\')\n        parser.add_argument(\'--share-decoder-input-output-embed\', default=False,\n                            action=\'store_true\',\n                            help=\'share decoder input and output embeddings\')\n        parser.add_argument(\'--share-all-embeddings\', default=False, action=\'store_true\',\n                            help=\'share encoder, decoder and output embeddings\'\n                                 \' (requires shared dictionary and embed dim)\')\n\n        # Granular dropout settings (if not specified these default to --dropout)\n        parser.add_argument(\'--encoder-dropout-in\', type=float, metavar=\'D\',\n                            help=\'dropout probability for encoder input embedding\')\n        parser.add_argument(\'--encoder-dropout-out\', type=float, metavar=\'D\',\n                            help=\'dropout probability for encoder output\')\n        parser.add_argument(\'--decoder-dropout-in\', type=float, metavar=\'D\',\n                            help=\'dropout probability for decoder input embedding\')\n        parser.add_argument(\'--decoder-dropout-out\', type=float, metavar=\'D\',\n                            help=\'dropout probability for decoder output\')\n        # fmt: on\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure that all args are properly defaulted (in case there are any new ones)\n        base_architecture(args)\n\n        if args.encoder_layers != args.decoder_layers:\n            raise ValueError(\'--encoder-layers must match --decoder-layers\')\n\n        max_source_positions = getattr(args, \'max_source_positions\', DEFAULT_MAX_SOURCE_POSITIONS)\n        max_target_positions = getattr(args, \'max_target_positions\', DEFAULT_MAX_TARGET_POSITIONS)\n\n        def load_pretrained_embedding_from_file(embed_path, dictionary, embed_dim):\n            num_embeddings = len(dictionary)\n            padding_idx = dictionary.pad()\n            embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n            embed_dict = utils.parse_embedding(embed_path)\n            utils.print_embed_overlap(embed_dict, dictionary)\n            return utils.load_embedding(embed_dict, dictionary, embed_tokens)\n\n        if args.encoder_embed_path:\n            pretrained_encoder_embed = load_pretrained_embedding_from_file(\n                args.encoder_embed_path, task.source_dictionary, args.encoder_embed_dim)\n        else:\n            num_embeddings = len(task.source_dictionary)\n            pretrained_encoder_embed = Embedding(\n                num_embeddings, args.encoder_embed_dim, task.source_dictionary.pad()\n            )\n\n        if args.share_all_embeddings:\n            # double check all parameters combinations are valid\n            if task.source_dictionary != task.target_dictionary:\n                raise ValueError(\'--share-all-embeddings requires a joint dictionary\')\n            if args.decoder_embed_path and (\n                    args.decoder_embed_path != args.encoder_embed_path):\n                raise ValueError(\n                    \'--share-all-embed not compatible with --decoder-embed-path\'\n                )\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise ValueError(\n                    \'--share-all-embeddings requires --encoder-embed-dim to \'\n                    \'match --decoder-embed-dim\'\n                )\n            pretrained_decoder_embed = pretrained_encoder_embed\n            args.share_decoder_input_output_embed = True\n        else:\n            # separate decoder input embeddings\n            pretrained_decoder_embed = None\n            if args.decoder_embed_path:\n                pretrained_decoder_embed = load_pretrained_embedding_from_file(\n                    args.decoder_embed_path,\n                    task.target_dictionary,\n                    args.decoder_embed_dim\n                )\n        # one last double check of parameter combinations\n        if args.share_decoder_input_output_embed and (\n                args.decoder_embed_dim != args.decoder_out_embed_dim):\n            raise ValueError(\n                \'--share-decoder-input-output-embeddings requires \'\n                \'--decoder-embed-dim to match --decoder-out-embed-dim\'\n            )\n\n        if args.encoder_freeze_embed:\n            pretrained_encoder_embed.weight.requires_grad = False\n        if args.decoder_freeze_embed:\n            pretrained_decoder_embed.weight.requires_grad = False\n\n        encoder = LSTMEncoder(\n            dictionary=task.source_dictionary,\n            embed_dim=args.encoder_embed_dim,\n            hidden_size=args.encoder_hidden_size,\n            num_layers=args.encoder_layers,\n            dropout_in=args.encoder_dropout_in,\n            dropout_out=args.encoder_dropout_out,\n            bidirectional=args.encoder_bidirectional,\n            pretrained_embed=pretrained_encoder_embed,\n            max_source_positions=max_source_positions\n        )\n        decoder = LSTMDecoder(\n            dictionary=task.target_dictionary,\n            embed_dim=args.decoder_embed_dim,\n            hidden_size=args.decoder_hidden_size,\n            out_embed_dim=args.decoder_out_embed_dim,\n            num_layers=args.decoder_layers,\n            dropout_in=args.decoder_dropout_in,\n            dropout_out=args.decoder_dropout_out,\n            attention=options.eval_bool(args.decoder_attention),\n            encoder_output_units=encoder.output_units,\n            pretrained_embed=pretrained_decoder_embed,\n            share_input_output_embed=args.share_decoder_input_output_embed,\n            adaptive_softmax_cutoff=(\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int)\n                if args.criterion == \'adaptive_loss\' else None\n            ),\n            max_target_positions=max_target_positions,\n            residuals=False\n        )\n        return cls(encoder, decoder)\n\n    def forward(\n        self,\n        src_tokens,\n        src_lengths,\n        prev_output_tokens,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n    ):\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths)\n        decoder_out = self.decoder(\n            prev_output_tokens, encoder_out=encoder_out, incremental_state=incremental_state\n        )\n        return decoder_out\n\n\nclass LSTMEncoder(FairseqEncoder):\n    """"""LSTM encoder.""""""\n    def __init__(\n        self, dictionary, embed_dim=512, hidden_size=512, num_layers=1,\n        dropout_in=0.1, dropout_out=0.1, bidirectional=False,\n        left_pad=True, pretrained_embed=None, padding_idx=None,\n        max_source_positions=DEFAULT_MAX_SOURCE_POSITIONS\n    ):\n        super().__init__(dictionary)\n        self.num_layers = num_layers\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.bidirectional = bidirectional\n        self.hidden_size = hidden_size\n        self.max_source_positions = max_source_positions\n\n        num_embeddings = len(dictionary)\n        self.padding_idx = padding_idx if padding_idx is not None else dictionary.pad()\n        if pretrained_embed is None:\n            self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx)\n        else:\n            self.embed_tokens = pretrained_embed\n\n        self.lstm = LSTM(\n            input_size=embed_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout=self.dropout_out if num_layers > 1 else 0.,\n            bidirectional=bidirectional,\n        )\n        self.left_pad = left_pad\n\n        self.output_units = hidden_size\n        if bidirectional:\n            self.output_units *= 2\n\n    def forward(\n        self,\n        src_tokens: Tensor,\n        src_lengths: Tensor,\n        enforce_sorted: bool = True,\n    ):\n        """"""\n        Args:\n            src_tokens (LongTensor): tokens in the source language of\n                shape `(batch, src_len)`\n            src_lengths (LongTensor): lengths of each source sentence of\n                shape `(batch)`\n            enforce_sorted (bool, optional): if True, `src_tokens` is\n                expected to contain sequences sorted by length in a\n                decreasing order. If False, this condition is not\n                required. Default: True.\n        """"""\n        if self.left_pad:\n            # nn.utils.rnn.pack_padded_sequence requires right-padding;\n            # convert left-padding to right-padding\n            src_tokens = utils.convert_padding_direction(\n                src_tokens,\n                torch.zeros_like(src_tokens).fill_(self.padding_idx),\n                left_to_right=True,\n            )\n\n        bsz, seqlen = src_tokens.size()\n\n        # embed tokens\n        x = self.embed_tokens(src_tokens)\n        x = F.dropout(x, p=self.dropout_in, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # pack embedded source tokens into a PackedSequence\n        packed_x = nn.utils.rnn.pack_padded_sequence(\n            x, src_lengths.data, enforce_sorted=enforce_sorted\n        )\n\n        # apply LSTM\n        if self.bidirectional:\n            state_size = 2 * self.num_layers, bsz, self.hidden_size\n        else:\n            state_size = self.num_layers, bsz, self.hidden_size\n        h0 = x.new_zeros(*state_size)\n        c0 = x.new_zeros(*state_size)\n        packed_outs, (final_hiddens, final_cells) = self.lstm(packed_x, (h0, c0))\n\n        # unpack outputs and apply dropout\n        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_idx*1.0)\n        x = F.dropout(x, p=self.dropout_out, training=self.training)\n        assert list(x.size()) == [seqlen, bsz, self.output_units]\n\n        if self.bidirectional:\n            final_hiddens = self.combine_bidir(final_hiddens, bsz)\n            final_cells = self.combine_bidir(final_cells, bsz)\n\n        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n\n        return tuple((\n            x,  # seq_len x batch x hidden\n            final_hiddens,  # num_layers x batch x num_directions*hidden\n            final_cells,  # num_layers x batch x num_directions*hidden\n            encoder_padding_mask,  # seq_len x batch\n        ))\n\n    def combine_bidir(self, outs, bsz: int):\n        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n        return out.view(self.num_layers, bsz, -1)\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        return tuple((\n            encoder_out[0].index_select(1, new_order),\n            encoder_out[1].index_select(1, new_order),\n            encoder_out[2].index_select(1, new_order),\n            encoder_out[3].index_select(1, new_order),\n        ))\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return self.max_source_positions\n\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n        super().__init__()\n\n        self.input_proj = Linear(input_embed_dim, source_embed_dim, bias=bias)\n        self.output_proj = Linear(input_embed_dim + source_embed_dim, output_embed_dim, bias=bias)\n\n    def forward(self, input, source_hids, encoder_padding_mask):\n        # input: bsz x input_embed_dim\n        # source_hids: srclen x bsz x source_embed_dim\n\n        # x: bsz x source_embed_dim\n        x = self.input_proj(input)\n\n        # compute attention\n        attn_scores = (source_hids * x.unsqueeze(0)).sum(dim=2)\n\n        # don\'t attend over padding\n        if encoder_padding_mask is not None:\n            attn_scores = attn_scores.float().masked_fill_(\n                encoder_padding_mask,\n                float(\'-inf\')\n            ).type_as(attn_scores)  # FP16 support: cast to float and back\n\n        attn_scores = F.softmax(attn_scores, dim=0)  # srclen x bsz\n\n        # sum weighted sources\n        x = (attn_scores.unsqueeze(2) * source_hids).sum(dim=0)\n\n        x = torch.tanh(self.output_proj(torch.cat((x, input), dim=1)))\n        return x, attn_scores\n\nclass LSTMDecoder(FairseqIncrementalDecoder):\n    """"""LSTM decoder.""""""\n    def __init__(\n        self, dictionary, embed_dim=512, hidden_size=512, out_embed_dim=512,\n        num_layers=1, dropout_in=0.1, dropout_out=0.1, attention=True,\n        encoder_output_units=512, pretrained_embed=None,\n        share_input_output_embed=False, adaptive_softmax_cutoff=None,\n        max_target_positions=DEFAULT_MAX_TARGET_POSITIONS,\n        residuals=False\n    ):\n        super().__init__(dictionary)\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.hidden_size = hidden_size\n        self.share_input_output_embed = share_input_output_embed\n        self.need_attn = True\n        self.max_target_positions = max_target_positions\n        self.residuals = residuals\n        self.num_layers = num_layers\n\n        self.adaptive_softmax = None\n        num_embeddings = len(dictionary)\n        padding_idx = dictionary.pad()\n        if pretrained_embed is None:\n            self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n        else:\n            self.embed_tokens = pretrained_embed\n\n        self.encoder_output_units = encoder_output_units\n        if encoder_output_units != hidden_size and encoder_output_units != 0:\n            self.encoder_hidden_proj = Linear(encoder_output_units, hidden_size)\n            self.encoder_cell_proj = Linear(encoder_output_units, hidden_size)\n        else:\n            self.encoder_hidden_proj = self.encoder_cell_proj = None\n\n        # disable input feeding if there is no encoder\n        # input feeding is described in arxiv.org/abs/1508.04025\n        input_feed_size = 0 if encoder_output_units == 0 else hidden_size\n        self.layers = nn.ModuleList([\n            LSTMCell(\n                input_size=input_feed_size + embed_dim if layer == 0 else hidden_size,\n                hidden_size=hidden_size,\n            )\n            for layer in range(num_layers)\n        ])\n\n        if attention:\n            # TODO make bias configurable\n            self.attention = AttentionLayer(hidden_size, encoder_output_units, hidden_size, bias=False)\n        else:\n            self.attention = None\n\n        if hidden_size != out_embed_dim:\n            self.additional_fc = Linear(hidden_size, out_embed_dim)\n\n        if adaptive_softmax_cutoff is not None:\n            # setting adaptive_softmax dropout to dropout_out for now but can be redefined\n            self.adaptive_softmax = AdaptiveSoftmax(\n                num_embeddings, hidden_size, adaptive_softmax_cutoff, dropout=dropout_out\n            )\n        elif not self.share_input_output_embed:\n            self.fc_out = Linear(out_embed_dim, num_embeddings, dropout=dropout_out)\n\n    def get_cached_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]):\n        cached_state = self.get_incremental_state(incremental_state, \'cached_state\')\n        assert cached_state is not None\n        prev_hiddens_ = cached_state[""prev_hiddens""]\n        assert prev_hiddens_ is not None\n        prev_cells_ = cached_state[""prev_cells""]\n        assert prev_cells_ is not None\n        prev_hiddens = [prev_hiddens_[i] for i in range(self.num_layers)]\n        prev_cells = [prev_cells_[j] for j in range(self.num_layers)]\n        input_feed = cached_state[""input_feed""]  # can be None for decoder-only language models\n        return prev_hiddens, prev_cells, input_feed\n\n    def forward(\n        self,\n        prev_output_tokens,\n        encoder_out: Optional[Tuple[Tensor, Tensor, Tensor, Tensor]] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        src_lengths: Optional[Tensor] = None,\n    ):\n        x, attn_scores = self.extract_features(\n            prev_output_tokens, encoder_out, incremental_state\n        )\n        return self.output_layer(x), attn_scores\n\n    def extract_features(\n        self,\n        prev_output_tokens,\n        encoder_out: Optional[Tuple[Tensor, Tensor, Tensor, Tensor]] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n    ):\n        """"""\n        Similar to *forward* but only return features.\n        """"""\n        # get outputs from encoder\n        if encoder_out is not None:\n            encoder_outs = encoder_out[0]\n            encoder_hiddens = encoder_out[1]\n            encoder_cells = encoder_out[2]\n            encoder_padding_mask = encoder_out[3]\n        else:\n            encoder_outs = torch.empty(0)\n            encoder_hiddens = torch.empty(0)\n            encoder_cells = torch.empty(0)\n            encoder_padding_mask = torch.empty(0)\n        srclen = encoder_outs.size(0)\n\n        if incremental_state is not None and len(incremental_state) > 0:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n\n        bsz, seqlen = prev_output_tokens.size()\n\n        # embed tokens\n        x = self.embed_tokens(prev_output_tokens)\n        x = F.dropout(x, p=self.dropout_in, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # initialize previous states (or get from cache during incremental generation)\n        if incremental_state is not None and len(incremental_state) > 0:\n            prev_hiddens, prev_cells, input_feed = self.get_cached_state(incremental_state)\n        elif encoder_out is not None:\n            # setup recurrent cells\n            prev_hiddens = [encoder_hiddens[i] for i in range(self.num_layers)]\n            prev_cells = [encoder_cells[i] for i in range(self.num_layers)]\n            if self.encoder_hidden_proj is not None:\n                prev_hiddens = [self.encoder_hidden_proj(y) for y in prev_hiddens]\n                prev_cells = [self.encoder_cell_proj(y) for y in prev_cells]\n            input_feed = x.new_zeros(bsz, self.hidden_size)\n        else:\n            # setup zero cells, since there is no encoder\n            zero_state = x.new_zeros(bsz, self.hidden_size)\n            prev_hiddens = [zero_state for i in range(self.num_layers)]\n            prev_cells = [zero_state for i in range(self.num_layers)]\n            input_feed = None\n\n        assert srclen > 0 or self.attention is None, \\\n            ""attention is not supported if there are no encoder outputs""\n        attn_scores = x.new_zeros(srclen, seqlen, bsz) if self.attention is not None else None\n        outs = []\n        for j in range(seqlen):\n            # input feeding: concatenate context vector from previous time step\n            if input_feed is not None:\n                input = torch.cat((x[j, :, :], input_feed), dim=1)\n            else:\n                input = x[j]\n\n            for i, rnn in enumerate(self.layers):\n                # recurrent cell\n                hidden, cell = rnn(input, (prev_hiddens[i], prev_cells[i]))\n\n                # hidden state becomes the input to the next layer\n                input = F.dropout(hidden, p=self.dropout_out, training=self.training)\n                if self.residuals: input = input + prev_hiddens[i]\n\n                # save state for next time step\n                prev_hiddens[i] = hidden\n                prev_cells[i] = cell\n\n            # apply attention using the last layer\'s hidden state\n            if self.attention is not None:\n                assert attn_scores is not None\n                out, attn_scores[:, j, :] = self.attention(hidden, encoder_outs, encoder_padding_mask)\n            else:\n                out = hidden\n            out = F.dropout(out, p=self.dropout_out, training=self.training)\n\n            # input feeding\n            if input_feed is not None:\n                input_feed = out\n\n            # save final output\n            outs.append(out)\n\n        # Stack all the necessary tensors together and store\n        prev_hiddens_tensor = torch.stack(prev_hiddens)\n        prev_cells_tensor = torch.stack(prev_cells)\n        cache_state = torch.jit.annotate(\n            Dict[str, Optional[Tensor]],\n            {""prev_hiddens"": prev_hiddens_tensor, ""prev_cells"": prev_cells_tensor, ""input_feed"": input_feed})\n        self.set_incremental_state(\n            incremental_state, \'cached_state\', cache_state)\n\n        # collect outputs across time steps\n        x = torch.cat(outs, dim=0).view(seqlen, bsz, self.hidden_size)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(1, 0)\n\n        if hasattr(self, \'additional_fc\') and self.adaptive_softmax is None:\n            x = self.additional_fc(x)\n            x = F.dropout(x, p=self.dropout_out, training=self.training)\n        # srclen x tgtlen x bsz -> bsz x tgtlen x srclen\n        if not self.training and self.need_attn and self.attention is not None:\n            assert attn_scores is not None\n            attn_scores = attn_scores.transpose(0, 2)\n        else:\n            attn_scores = None\n        return x, attn_scores\n\n    def output_layer(self, x):\n        """"""Project features to the vocabulary size.""""""\n        if self.adaptive_softmax is None:\n            if self.share_input_output_embed:\n                x = F.linear(x, self.embed_tokens.weight)\n            else:\n                x = self.fc_out(x)\n        return x\n\n    def reorder_state(self, state: List[Tensor], new_order):\n        return [\n            state_i.index_select(0, new_order) if state_i is not None else None\n            for state_i in state\n        ]\n\n    def reorder_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], new_order):\n        if incremental_state is None or len(incremental_state) == 0:\n            return\n        prev_hiddens, prev_cells, input_feed = self.get_cached_state(incremental_state)\n        cached_state = (prev_hiddens, prev_cells, [input_feed])\n        new_state = [self.reorder_state(state, new_order) for state in cached_state]\n        prev_hiddens_tensor = torch.stack(new_state[0])\n        prev_cells_tensor = torch.stack(new_state[1])\n        cached_state_new = torch.jit.annotate(\n            Dict[str, Optional[Tensor]],\n            {""prev_hiddens"": prev_hiddens_tensor, ""prev_cells"": prev_cells_tensor, ""input_feed"": new_state[2][0]})\n        self.set_incremental_state(incremental_state, \'cached_state\', cached_state_new),\n        return\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return self.max_target_positions\n\n    def make_generation_fast_(self, need_attn=False, **kwargs):\n        self.need_attn = need_attn\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.uniform_(m.weight, -0.1, 0.1)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n\ndef LSTM(input_size, hidden_size, **kwargs):\n    m = nn.LSTM(input_size, hidden_size, **kwargs)\n    for name, param in m.named_parameters():\n        if \'weight\' in name or \'bias\' in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m\n\n\ndef LSTMCell(input_size, hidden_size, **kwargs):\n    m = nn.LSTMCell(input_size, hidden_size, **kwargs)\n    for name, param in m.named_parameters():\n        if \'weight\' in name or \'bias\' in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m\n\n\ndef Linear(in_features, out_features, bias=True, dropout=0):\n    """"""Linear layer (input: N x T x C)""""""\n    m = nn.Linear(in_features, out_features, bias=bias)\n    m.weight.data.uniform_(-0.1, 0.1)\n    if bias:\n        m.bias.data.uniform_(-0.1, 0.1)\n    return m\n\n\n@register_model_architecture(\'lstm\', \'lstm\')\ndef base_architecture(args):\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 512)\n    args.encoder_embed_path = getattr(args, \'encoder_embed_path\', None)\n    args.encoder_freeze_embed = getattr(args, \'encoder_freeze_embed\', False)\n    args.encoder_hidden_size = getattr(args, \'encoder_hidden_size\', args.encoder_embed_dim)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 1)\n    args.encoder_bidirectional = getattr(args, \'encoder_bidirectional\', False)\n    args.encoder_dropout_in = getattr(args, \'encoder_dropout_in\', args.dropout)\n    args.encoder_dropout_out = getattr(args, \'encoder_dropout_out\', args.dropout)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.decoder_embed_path = getattr(args, \'decoder_embed_path\', None)\n    args.decoder_freeze_embed = getattr(args, \'decoder_freeze_embed\', False)\n    args.decoder_hidden_size = getattr(args, \'decoder_hidden_size\', args.decoder_embed_dim)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 1)\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 512)\n    args.decoder_attention = getattr(args, \'decoder_attention\', \'1\')\n    args.decoder_dropout_in = getattr(args, \'decoder_dropout_in\', args.dropout)\n    args.decoder_dropout_out = getattr(args, \'decoder_dropout_out\', args.dropout)\n    args.share_decoder_input_output_embed = getattr(args, \'share_decoder_input_output_embed\', False)\n    args.share_all_embeddings = getattr(args, \'share_all_embeddings\', False)\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', \'10000,50000,200000\')\n\n\n@register_model_architecture(\'lstm\', \'lstm_wiseman_iwslt_de_en\')\ndef lstm_wiseman_iwslt_de_en(args):\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 256)\n    args.encoder_dropout_in = getattr(args, \'encoder_dropout_in\', 0)\n    args.encoder_dropout_out = getattr(args, \'encoder_dropout_out\', 0)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 256)\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 256)\n    args.decoder_dropout_in = getattr(args, \'decoder_dropout_in\', 0)\n    args.decoder_dropout_out = getattr(args, \'decoder_dropout_out\', args.dropout)\n    base_architecture(args)\n\n\n@register_model_architecture(\'lstm\', \'lstm_luong_wmt_en_de\')\ndef lstm_luong_wmt_en_de(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1000)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 4)\n    args.encoder_dropout_out = getattr(args, \'encoder_dropout_out\', 0)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 1000)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 4)\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 1000)\n    args.decoder_dropout_out = getattr(args, \'decoder_dropout_out\', 0)\n    base_architecture(args)\n'"
fairseq/models/lstm_lm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqLanguageModel, register_model, register_model_architecture\n)\nfrom fairseq.models.lstm import (\n    LSTMDecoder, Embedding\n)\n\nDEFAULT_MAX_TARGET_POSITIONS = 1e5\n\n@register_model(\'lstm_lm\')\nclass LSTMLanguageModel(FairseqLanguageModel):\n    def __init__(self, decoder):\n        super().__init__(decoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained decoder embedding\')\n        parser.add_argument(\'--decoder-hidden-size\', type=int, metavar=\'N\',\n                            help=\'decoder hidden size\')\n        parser.add_argument(\'--decoder-layers\', type=int, metavar=\'N\',\n                            help=\'number of decoder layers\')\n        parser.add_argument(\'--decoder-out-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder output embedding dimension\')\n        parser.add_argument(\'--decoder-attention\', type=str, metavar=\'BOOL\',\n                            help=\'decoder attention\')\n        parser.add_argument(\'--adaptive-softmax-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive softmax cutoff points. \'\n                                 \'Must be used with adaptive_loss criterion\')\n        parser.add_argument(\'--residuals\', default=False,\n                            action=\'store_true\',\n                            help=\'applying residuals between LSTM layers\')\n\n        # Granular dropout settings (if not specified these default to --dropout)\n        parser.add_argument(\'--decoder-dropout-in\', type=float, metavar=\'D\',\n                            help=\'dropout probability for decoder input embedding\')\n        parser.add_argument(\'--decoder-dropout-out\', type=float, metavar=\'D\',\n                            help=\'dropout probability for decoder output\')\n        parser.add_argument(\'--share-decoder-input-output-embed\', default=False,\n                            action=\'store_true\',\n                            help=\'share decoder input and output embeddings\')\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present in older models\n        base_architecture(args)\n\n        if getattr(args, \'max_target_positions\', None) is not None:\n            max_target_positions = args.max_target_positions\n        else:\n            max_target_positions = getattr(args, \'tokens_per_sample\', DEFAULT_MAX_TARGET_POSITIONS)\n\n        def load_pretrained_embedding_from_file(embed_path, dictionary, embed_dim):\n            num_embeddings = len(dictionary)\n            padding_idx = dictionary.pad()\n            embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n            embed_dict = utils.parse_embedding(embed_path)\n            utils.print_embed_overlap(embed_dict, dictionary)\n            return utils.load_embedding(embed_dict, dictionary, embed_tokens)\n\n        pretrained_decoder_embed = None\n        if args.decoder_embed_path:\n            pretrained_decoder_embed = load_pretrained_embedding_from_file(\n                args.decoder_embed_path,\n                task.target_dictionary,\n                args.decoder_embed_dim\n            )\n\n        if args.share_decoder_input_output_embed:\n            # double check all parameters combinations are valid\n            if task.source_dictionary != task.target_dictionary:\n                raise ValueError(\'--share-decoder-input-output-embeddings requires a joint dictionary\')\n\n            if args.decoder_embed_dim != args.decoder_out_embed_dim:\n                raise ValueError(\n                    \'--share-decoder-input-output-embeddings requires \'\n                    \'--decoder-embed-dim to match --decoder-out-embed-dim\'\n                    )\n\n        decoder = LSTMDecoder(\n            dictionary=task.dictionary,\n            embed_dim=args.decoder_embed_dim,\n            hidden_size=args.decoder_hidden_size,\n            out_embed_dim=args.decoder_out_embed_dim,\n            num_layers=args.decoder_layers,\n            dropout_in=args.decoder_dropout_in,\n            dropout_out=args.decoder_dropout_out,\n            attention=False,  # decoder-only language model doesn\'t support attention\n            encoder_output_units=0,\n            pretrained_embed=pretrained_decoder_embed,\n            share_input_output_embed=args.share_decoder_input_output_embed,\n            adaptive_softmax_cutoff=(\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int)\n                if args.criterion == \'adaptive_loss\' else None\n            ),\n            max_target_positions=max_target_positions,\n            residuals=args.residuals\n        )\n\n        return cls(decoder)\n\n\n@register_model_architecture(\'lstm_lm\', \'lstm_lm\')\ndef base_architecture(args):\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.decoder_embed_path = getattr(args, \'decoder_embed_path\', None)\n    args.decoder_hidden_size = getattr(args, \'decoder_hidden_size\', args.decoder_embed_dim)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 1)\n    args.decoder_out_embed_dim = getattr(args, \'decoder_out_embed_dim\', 512)\n    args.decoder_attention = getattr(args, \'decoder_attention\', \'0\')\n    args.decoder_dropout_in = getattr(args, \'decoder_dropout_in\', args.dropout)\n    args.decoder_dropout_out = getattr(args, \'decoder_dropout_out\', args.dropout)\n    args.share_decoder_input_output_embed = getattr(args, \'share_decoder_input_output_embed\', False)\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', \'10000,50000,200000\')\n    args.residuals = getattr(args, \'residuals\', False)\n'"
fairseq/models/masked_lm.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.models import (\n    FairseqEncoderModel,\n    FairseqEncoder,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import (\n    LayerNorm,\n    SinusoidalPositionalEmbedding,\n    TransformerSentenceEncoder,\n)\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(\'masked_lm\')\nclass MaskedLMModel(FairseqEncoderModel):\n    """"""\n    Class for training a Masked Language Model. It also supports an\n    additional sentence level prediction if the sent-loss argument is set.\n    """"""\n    def __init__(self, args, encoder):\n        super().__init__(encoder)\n        self.args = args\n\n        # if specified then apply bert initialization on the model. We need\n        # to explictly call this to make sure that the output embeddings\n        # and projection layers are also correctly initialized\n        if getattr(args, \'apply_bert_init\', False):\n            self.apply(init_bert_params)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # Arguments related to dropout\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--attention-dropout\', type=float,\n                            metavar=\'D\', help=\'dropout probability for\'\n                            \' attention weights\')\n        parser.add_argument(\'--act-dropout\', type=float,\n                            metavar=\'D\', help=\'dropout probability after\'\n                            \' activation in FFN\')\n\n        # Arguments related to hidden states and self-attention\n        parser.add_argument(\'--encoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension for FFN\')\n        parser.add_argument(\'--encoder-layers\', type=int, metavar=\'N\',\n                            help=\'num encoder layers\')\n        parser.add_argument(\'--encoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num encoder attention heads\')\n\n        # Arguments related to input and output embeddings\n        parser.add_argument(\'--encoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--share-encoder-input-output-embed\',\n                            action=\'store_true\', help=\'share encoder input\'\n                            \' and output embeddings\')\n        parser.add_argument(\'--encoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the encoder\')\n        parser.add_argument(\'--no-token-positional-embeddings\',\n                            action=\'store_true\',\n                            help=\'if set, disables positional embeddings\'\n                            \' (outside self attention)\')\n        parser.add_argument(\'--num-segment\', type=int, metavar=\'N\',\n                            help=\'num segment in the input\')\n        parser.add_argument(\'--max-positions\', type=int,\n                            help=\'number of positional embeddings to learn\')\n\n        # Arguments related to sentence level prediction\n        parser.add_argument(\'--sentence-class-num\', type=int, metavar=\'N\',\n                            help=\'number of classes for sentence task\')\n        parser.add_argument(\'--sent-loss\', action=\'store_true\', help=\'if set,\'\n                            \' calculate sentence level predictions\')\n\n        # Arguments related to parameter initialization\n        parser.add_argument(\'--apply-bert-init\', action=\'store_true\',\n                            help=\'use custom param initialization for BERT\')\n\n        # misc params\n        parser.add_argument(\'--activation-fn\',\n                            choices=utils.get_available_activation_fns(),\n                            help=\'activation function to use\')\n        parser.add_argument(\'--pooler-activation-fn\',\n                            choices=utils.get_available_activation_fns(),\n                            help=\'Which activation function to use for pooler layer.\')\n        parser.add_argument(\'--encoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each encoder block\')\n\n    def forward(self, src_tokens, segment_labels=None, **kwargs):\n        return self.encoder(src_tokens, segment_labels=segment_labels, **kwargs)\n\n    def max_positions(self):\n        return self.encoder.max_positions\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure all arguments are present in older models\n        base_architecture(args)\n\n        if not hasattr(args, \'max_positions\'):\n            args.max_positions = args.tokens_per_sample\n\n        logger.info(args)\n\n        encoder = MaskedLMEncoder(args, task.dictionary)\n        return cls(args, encoder)\n\n\nclass MaskedLMEncoder(FairseqEncoder):\n    """"""\n    Encoder for Masked Language Modelling.\n    """"""\n\n    def __init__(self, args, dictionary):\n        super().__init__(dictionary)\n\n        self.padding_idx = dictionary.pad()\n        self.vocab_size = dictionary.__len__()\n        self.max_positions = args.max_positions\n\n        self.sentence_encoder = TransformerSentenceEncoder(\n            padding_idx=self.padding_idx,\n            vocab_size=self.vocab_size,\n            num_encoder_layers=args.encoder_layers,\n            embedding_dim=args.encoder_embed_dim,\n            ffn_embedding_dim=args.encoder_ffn_embed_dim,\n            num_attention_heads=args.encoder_attention_heads,\n            dropout=args.dropout,\n            attention_dropout=args.attention_dropout,\n            activation_dropout=args.act_dropout,\n            max_seq_len=self.max_positions,\n            num_segments=args.num_segment,\n            use_position_embeddings=not args.no_token_positional_embeddings,\n            encoder_normalize_before=args.encoder_normalize_before,\n            apply_bert_init=args.apply_bert_init,\n            activation_fn=args.activation_fn,\n            learned_pos_embedding=args.encoder_learned_pos,\n        )\n\n        self.share_input_output_embed = args.share_encoder_input_output_embed\n        self.embed_out = None\n        self.sentence_projection_layer = None\n        self.sentence_out_dim = args.sentence_class_num\n        self.lm_output_learned_bias = None\n\n        # Remove head is set to true during fine-tuning\n        self.load_softmax = not getattr(args, \'remove_head\', False)\n\n        self.masked_lm_pooler = nn.Linear(\n            args.encoder_embed_dim, args.encoder_embed_dim\n        )\n        self.pooler_activation = utils.get_activation_fn(args.pooler_activation_fn)\n\n        self.lm_head_transform_weight = nn.Linear(args.encoder_embed_dim, args.encoder_embed_dim)\n        self.activation_fn = utils.get_activation_fn(args.activation_fn)\n        self.layer_norm = LayerNorm(args.encoder_embed_dim)\n\n        self.lm_output_learned_bias = None\n        if self.load_softmax:\n            self.lm_output_learned_bias = nn.Parameter(torch.zeros(self.vocab_size))\n\n            if not self.share_input_output_embed:\n                self.embed_out = nn.Linear(\n                    args.encoder_embed_dim,\n                    self.vocab_size,\n                    bias=False\n                )\n\n            if args.sent_loss:\n                self.sentence_projection_layer = nn.Linear(\n                    args.encoder_embed_dim,\n                    self.sentence_out_dim,\n                    bias=False\n                )\n\n    def forward(self, src_tokens, segment_labels=None, masked_tokens=None, **unused):\n        """"""\n        Forward pass for Masked LM encoder. This first computes the token\n        embedding using the token embedding matrix, position embeddings (if\n        specified) and segment embeddings (if specified).\n\n        Here we assume that the sentence representation corresponds to the\n        output of the classification_token (see bert_task or cross_lingual_lm\n        task for more details).\n        Args:\n            - src_tokens: B x T matrix representing sentences\n            - segment_labels: B x T matrix representing segment label for tokens\n        Returns:\n            - a tuple of the following:\n                - logits for predictions in format B x T x C to be used in\n                  softmax afterwards\n                - a dictionary of additional data, where \'pooled_output\' contains\n                  the representation for classification_token and \'inner_states\'\n                  is a list of internal model states used to compute the\n                  predictions (similar in ELMO). \'sentence_logits\'\n                  is the prediction logit for NSP task and is only computed if\n                  this is specified in the input arguments.\n        """"""\n\n        inner_states, sentence_rep = self.sentence_encoder(\n            src_tokens,\n            segment_labels=segment_labels,\n        )\n\n        x = inner_states[-1].transpose(0, 1)\n        # project masked tokens only\n        if masked_tokens is not None:\n            x = x[masked_tokens, :]\n        x = self.layer_norm(self.activation_fn(self.lm_head_transform_weight(x)))\n\n        pooled_output = self.pooler_activation(self.masked_lm_pooler(sentence_rep))\n\n        # project back to size of vocabulary\n        if self.share_input_output_embed \\\n                and hasattr(self.sentence_encoder.embed_tokens, \'weight\'):\n            x = F.linear(x, self.sentence_encoder.embed_tokens.weight)\n        elif self.embed_out is not None:\n            x = self.embed_out(x)\n        if self.lm_output_learned_bias is not None:\n            x = x + self.lm_output_learned_bias\n        sentence_logits = None\n        if self.sentence_projection_layer:\n            sentence_logits = self.sentence_projection_layer(pooled_output)\n\n        return x, {\n            \'inner_states\': inner_states,\n            \'pooled_output\': pooled_output,\n            \'sentence_logits\': sentence_logits\n        }\n\n    def max_positions(self):\n        """"""Maximum output length supported by the encoder.""""""\n        return self.max_positions\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        if isinstance(\n                self.sentence_encoder.embed_positions,\n                SinusoidalPositionalEmbedding\n        ):\n            state_dict[\n                name + \'.sentence_encoder.embed_positions._float_tensor\'\n            ] = torch.FloatTensor(1)\n        if not self.load_softmax:\n            for k in list(state_dict.keys()):\n                if (\n                    ""embed_out.weight"" in k or\n                    ""sentence_projection_layer.weight"" in k or\n                    ""lm_output_learned_bias"" in k\n                ):\n                    del state_dict[k]\n        return state_dict\n\n\n@register_model_architecture(\'masked_lm\', \'masked_lm\')\ndef base_architecture(args):\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.act_dropout = getattr(args, \'act_dropout\', 0.0)\n\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 4096)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 6)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 8)\n\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1024)\n    args.share_encoder_input_output_embed = getattr(args, \'share_encoder_input_output_embed\', False)\n    args.encoder_learned_pos = getattr(args, \'encoder_learned_pos\', False)\n    args.no_token_positional_embeddings = getattr(args, \'no_token_positional_embeddings\', False)\n    args.num_segment = getattr(args, \'num_segment\', 2)\n\n    args.sentence_class_num = getattr(args, \'sentence_class_num\', 2)\n    args.sent_loss = getattr(args, \'sent_loss\', False)\n\n    args.apply_bert_init = getattr(args, \'apply_bert_init\', False)\n\n    args.activation_fn = getattr(args, \'activation_fn\', \'relu\')\n    args.pooler_activation_fn = getattr(args, \'pooler_activation_fn\', \'tanh\')\n    args.encoder_normalize_before = getattr(args, \'encoder_normalize_before\', False)\n\n\n@register_model_architecture(\'masked_lm\', \'bert_base\')\ndef bert_base_architecture(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 768)\n    args.share_encoder_input_output_embed = getattr(\n        args, \'share_encoder_input_output_embed\', True)\n    args.no_token_positional_embeddings = getattr(\n        args, \'no_token_positional_embeddings\', False)\n    args.encoder_learned_pos = getattr(args, \'encoder_learned_pos\', True)\n    args.num_segment = getattr(args, \'num_segment\', 2)\n\n    args.encoder_layers = getattr(args, \'encoder_layers\', 12)\n\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 12)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 3072)\n\n    args.sentence_class_num = getattr(args, \'sentence_class_num\', 2)\n    args.sent_loss = getattr(args, \'sent_loss\', True)\n\n    args.apply_bert_init = getattr(args, \'apply_bert_init\', True)\n\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    args.pooler_activation_fn = getattr(args, \'pooler_activation_fn\', \'tanh\')\n    args.encoder_normalize_before = getattr(args, \'encoder_normalize_before\', True)\n    base_architecture(args)\n\n\n@register_model_architecture(\'masked_lm\', \'bert_large\')\ndef bert_large_architecture(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1024)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 24)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 16)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 4096)\n    bert_base_architecture(args)\n\n\n@register_model_architecture(\'masked_lm\', \'xlm_base\')\ndef xlm_architecture(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1024)\n    args.share_encoder_input_output_embed = getattr(\n        args, \'share_encoder_input_output_embed\', True)\n    args.no_token_positional_embeddings = getattr(\n        args, \'no_token_positional_embeddings\', False)\n    args.encoder_learned_pos = getattr(args, \'encoder_learned_pos\', True)\n    args.num_segment = getattr(args, \'num_segment\', 1)\n\n    args.encoder_layers = getattr(args, \'encoder_layers\', 6)\n\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 8)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 4096)\n\n    args.sent_loss = getattr(args, \'sent_loss\', False)\n\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    args.encoder_normalize_before = getattr(args, \'encoder_normalize_before\', False)\n    args.pooler_activation_fn = getattr(args, \'pooler_activation_fn\', \'tanh\')\n    args.apply_bert_init = getattr(args, \'apply_bert_init\', True)\n    base_architecture(args)\n'"
fairseq/models/model_utils.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Optional\n\nimport torch\nfrom torch import Tensor\n\n\n@torch.jit.script\ndef script_skip_tensor_list(x: List[Tensor], mask):\n    res = [xi[mask] if xi.size(0) == mask.size(0) else xi[:, mask] for xi in x]\n    outputs = []\n    for i, t in enumerate(res):\n        if t.numel() != 0:\n            outputs.append(t)\n        else:\n            outputs.append(x[i])\n    return outputs\n\n\n@torch.jit.script\ndef script_skip_tensor(x: Tensor, mask):\n    # None case\n    if x.size(0) == 0:\n        return x\n    res = x[mask] if x.size(0) == mask.size(0) else x[:, mask]\n    if res.numel() == 0:\n        return x\n    else:\n        return res\n\n\n@torch.jit.script\ndef expand_2d_or_3d_tensor(x, trg_dim: int, padding_idx: int):\n    """"""\n    Expand 2D/3D tensor on dim=1\n    """"""\n    if x is None:\n        return None\n\n    assert x.dim() == 2 or x.dim() == 3\n    assert trg_dim >= x.size(1), (trg_dim, x.size())\n    if trg_dim == x.size(1):\n        return x\n\n    dims = [x.size(0), trg_dim - x.size(1)]\n    if x.dim() == 3:\n        dims.append(x.size(2))\n    x = torch.cat([x, torch.zeros(dims).to(x).fill_(padding_idx)], 1)\n\n    return x\n\n\n@torch.jit.script\ndef coalesce(x: Optional[Tensor], y: Tensor) -> Tensor:\n    return x if x is not None else y\n\n\n@torch.jit.script\ndef fill_tensors(x: Optional[Tensor], mask, y: Optional[Tensor], padding_idx: int) -> Optional[Tensor]:\n    """"""\n    Filling tensor x with y at masked positions (dim=0).\n    """"""\n    if x is None or x.size()[0] == 0 or y is None:\n        return x\n    assert x.dim() == y.dim() and mask.size(0) == x.size(0)\n    assert x.dim() == 2 or (x.dim() == 3 and x.size(2) == y.size(2))\n\n    n_selected = mask.sum()\n    if n_selected == 0:\n        return x\n    assert n_selected == y.size(0)\n    if n_selected == x.size(0):\n        return y\n\n    if x.size(1) < y.size(1):\n        x = expand_2d_or_3d_tensor(x, y.size(1), padding_idx)\n        x[mask] = y\n    elif x.size(1) > y.size(1):\n        x[mask] = torch.tensor(padding_idx).type_as(x)\n        if x.dim() == 2:\n            x[mask, :y.size(1)] = y\n        else:\n            x[mask, :y.size(1), :] = y\n    else:\n        x[mask] = y\n    return x\n'"
fairseq/models/multilingual_transformer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\n\nfrom fairseq import utils\nfrom fairseq.models import (\n    FairseqMultiModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.transformer import (\n    base_architecture,\n    Embedding,\n    TransformerModel,\n    TransformerEncoder,\n    TransformerDecoder,\n)\n\n\n@register_model(\'multilingual_transformer\')\nclass MultilingualTransformerModel(FairseqMultiModel):\n    """"""Train Transformer models for multiple language pairs simultaneously.\n\n    Requires `--task multilingual_translation`.\n\n    We inherit all arguments from TransformerModel and assume that all language\n    pairs use a single Transformer architecture. In addition, we provide several\n    options that are specific to the multilingual setting.\n\n    Args:\n        --share-encoder-embeddings: share encoder embeddings across all source languages\n        --share-decoder-embeddings: share decoder embeddings across all target languages\n        --share-encoders: share all encoder params (incl. embeddings) across all source languages\n        --share-decoders: share all decoder params (incl. embeddings) across all target languages\n    """"""\n\n    def __init__(self, encoders, decoders):\n        super().__init__(encoders, decoders)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        TransformerModel.add_args(parser)\n        parser.add_argument(\'--share-encoder-embeddings\', action=\'store_true\',\n                            help=\'share encoder embeddings across languages\')\n        parser.add_argument(\'--share-decoder-embeddings\', action=\'store_true\',\n                            help=\'share decoder embeddings across languages\')\n        parser.add_argument(\'--share-encoders\', action=\'store_true\',\n                            help=\'share encoders across languages\')\n        parser.add_argument(\'--share-decoders\', action=\'store_true\',\n                            help=\'share decoders across languages\')\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        from fairseq.tasks.multilingual_translation import MultilingualTranslationTask\n        assert isinstance(task, MultilingualTranslationTask)\n\n        # make sure all arguments are present in older models\n        base_multilingual_architecture(args)\n\n        if not hasattr(args, \'max_source_positions\'):\n            args.max_source_positions = 1024\n        if not hasattr(args, \'max_target_positions\'):\n            args.max_target_positions = 1024\n\n        src_langs = [lang_pair.split(\'-\')[0] for lang_pair in task.model_lang_pairs]\n        tgt_langs = [lang_pair.split(\'-\')[1] for lang_pair in task.model_lang_pairs]\n\n        if args.share_encoders:\n            args.share_encoder_embeddings = True\n        if args.share_decoders:\n            args.share_decoder_embeddings = True\n\n        def build_embedding(dictionary, embed_dim, path=None):\n            num_embeddings = len(dictionary)\n            padding_idx = dictionary.pad()\n            emb = Embedding(num_embeddings, embed_dim, padding_idx)\n            # if provided, load from preloaded dictionaries\n            if path:\n                embed_dict = utils.parse_embedding(path)\n                utils.load_embedding(embed_dict, dictionary, emb)\n            return emb\n\n        # build shared embeddings (if applicable)\n        shared_encoder_embed_tokens, shared_decoder_embed_tokens = None, None\n        if args.share_all_embeddings:\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise ValueError(\n                    \'--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim\')\n            if args.decoder_embed_path and (\n                    args.decoder_embed_path != args.encoder_embed_path):\n                raise ValueError(\'--share-all-embeddings not compatible with --decoder-embed-path\')\n            shared_encoder_embed_tokens = FairseqMultiModel.build_shared_embeddings(\n                dicts=task.dicts,\n                langs=task.langs,\n                embed_dim=args.encoder_embed_dim,\n                build_embedding=build_embedding,\n                pretrained_embed_path=args.encoder_embed_path,\n            )\n            shared_decoder_embed_tokens = shared_encoder_embed_tokens\n            args.share_decoder_input_output_embed = True\n        else:\n            if args.share_encoder_embeddings:\n                shared_encoder_embed_tokens = (\n                    FairseqMultiModel.build_shared_embeddings(\n                        dicts=task.dicts,\n                        langs=src_langs,\n                        embed_dim=args.encoder_embed_dim,\n                        build_embedding=build_embedding,\n                        pretrained_embed_path=args.encoder_embed_path,\n                    )\n                )\n            if args.share_decoder_embeddings:\n                shared_decoder_embed_tokens = (\n                    FairseqMultiModel.build_shared_embeddings(\n                        dicts=task.dicts,\n                        langs=tgt_langs,\n                        embed_dim=args.decoder_embed_dim,\n                        build_embedding=build_embedding,\n                        pretrained_embed_path=args.decoder_embed_path,\n                    )\n                )\n\n        # encoders/decoders for each language\n        lang_encoders, lang_decoders = {}, {}\n\n        def get_encoder(lang):\n            if lang not in lang_encoders:\n                if shared_encoder_embed_tokens is not None:\n                    encoder_embed_tokens = shared_encoder_embed_tokens\n                else:\n                    encoder_embed_tokens = build_embedding(\n                        task.dicts[lang], args.encoder_embed_dim, args.encoder_embed_path\n                    )\n                lang_encoders[lang] = TransformerEncoder(args, task.dicts[lang], encoder_embed_tokens)\n            return lang_encoders[lang]\n\n        def get_decoder(lang):\n            if lang not in lang_decoders:\n                if shared_decoder_embed_tokens is not None:\n                    decoder_embed_tokens = shared_decoder_embed_tokens\n                else:\n                    decoder_embed_tokens = build_embedding(\n                        task.dicts[lang], args.decoder_embed_dim, args.decoder_embed_path\n                    )\n                lang_decoders[lang] = TransformerDecoder(args, task.dicts[lang], decoder_embed_tokens)\n            return lang_decoders[lang]\n\n        # shared encoders/decoders (if applicable)\n        shared_encoder, shared_decoder = None, None\n        if args.share_encoders:\n            shared_encoder = get_encoder(src_langs[0])\n        if args.share_decoders:\n            shared_decoder = get_decoder(tgt_langs[0])\n\n        encoders, decoders = OrderedDict(), OrderedDict()\n        for lang_pair, src, tgt in zip(task.model_lang_pairs, src_langs, tgt_langs):\n            encoders[lang_pair] = shared_encoder if shared_encoder is not None else get_encoder(src)\n            decoders[lang_pair] = shared_decoder if shared_decoder is not None else get_decoder(tgt)\n\n        return MultilingualTransformerModel(encoders, decoders)\n\n    def load_state_dict(self, state_dict, strict=True, args=None):\n        state_dict_subset = state_dict.copy()\n        for k, _ in state_dict.items():\n            assert k.startswith(\'models.\')\n            lang_pair = k.split(\'.\')[1]\n            if lang_pair not in self.models:\n                del state_dict_subset[k]\n        super().load_state_dict(state_dict_subset, strict=strict, args=args)\n\n\n@register_model_architecture(\'multilingual_transformer\', \'multilingual_transformer\')\ndef base_multilingual_architecture(args):\n    base_architecture(args)\n    args.share_encoder_embeddings = getattr(args, \'share_encoder_embeddings\', False)\n    args.share_decoder_embeddings = getattr(args, \'share_decoder_embeddings\', False)\n    args.share_encoders = getattr(args, \'share_encoders\', False)\n    args.share_decoders = getattr(args, \'share_decoders\', False)\n\n\n@register_model_architecture(\'multilingual_transformer\', \'multilingual_transformer_iwslt_de_en\')\ndef multilingual_transformer_iwslt_de_en(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 512)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 1024)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 4)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 6)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 1024)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 4)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 6)\n    base_multilingual_architecture(args)\n'"
fairseq/models/transformer.py,19,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqEncoderDecoderModel,\n    FairseqIncrementalDecoder,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.fairseq_encoder import EncoderOut\nfrom fairseq.modules import (\n    AdaptiveSoftmax,\n    LayerDropModuleList,\n    LayerNorm,\n    PositionalEmbedding,\n    SinusoidalPositionalEmbedding,\n    TransformerDecoderLayer,\n    TransformerEncoderLayer,\n)\nfrom fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\nfrom torch import Tensor\n\n\nDEFAULT_MAX_SOURCE_POSITIONS = 1024\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\n\n@register_model(""transformer"")\nclass TransformerModel(FairseqEncoderDecoderModel):\n    """"""\n    Transformer model from `""Attention Is All You Need"" (Vaswani, et al, 2017)\n    <https://arxiv.org/abs/1706.03762>`_.\n\n    Args:\n        encoder (TransformerEncoder): the encoder\n        decoder (TransformerDecoder): the decoder\n\n    The Transformer model provides the following named architectures and\n    command-line arguments:\n\n    .. argparse::\n        :ref: fairseq.models.transformer_parser\n        :prog:\n    """"""\n\n    @classmethod\n    def hub_models(cls):\n        # fmt: off\n\n        def moses_subword(path):\n            return {\n                \'path\': path,\n                \'tokenizer\': \'moses\',\n                \'bpe\': \'subword_nmt\',\n            }\n\n        def moses_fastbpe(path):\n            return {\n                \'path\': path,\n                \'tokenizer\': \'moses\',\n                \'bpe\': \'fastbpe\',\n            }\n\n        return {\n            \'transformer.wmt14.en-fr\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-fr.joined-dict.transformer.tar.bz2\'),\n            \'transformer.wmt16.en-de\': \'https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2\',\n            \'transformer.wmt18.en-de\': moses_subword(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt18.en-de.ensemble.tar.gz\'),\n            \'transformer.wmt19.en-de\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-de.joined-dict.ensemble.tar.gz\'),\n            \'transformer.wmt19.en-ru\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-ru.ensemble.tar.gz\'),\n            \'transformer.wmt19.de-en\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.de-en.joined-dict.ensemble.tar.gz\'),\n            \'transformer.wmt19.ru-en\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.ru-en.ensemble.tar.gz\'),\n            \'transformer.wmt19.en-de.single_model\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-de.joined-dict.single_model.tar.gz\'),\n            \'transformer.wmt19.en-ru.single_model\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-ru.single_model.tar.gz\'),\n            \'transformer.wmt19.de-en.single_model\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.de-en.joined-dict.single_model.tar.gz\'),\n            \'transformer.wmt19.ru-en.single_model\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/wmt19.ru-en.single_model.tar.gz\'),\n        }\n        # fmt: on\n\n    def __init__(self, args, encoder, decoder):\n        super().__init__(encoder, decoder)\n        self.args = args\n        self.supports_align_args = True\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--activation-fn\',\n                            choices=utils.get_available_activation_fns(),\n                            help=\'activation function to use\')\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--attention-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for attention weights\')\n        parser.add_argument(\'--activation-dropout\', \'--relu-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability after activation in FFN.\')\n        parser.add_argument(\'--encoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained encoder embedding\')\n        parser.add_argument(\'--encoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--encoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension for FFN\')\n        parser.add_argument(\'--encoder-layers\', type=int, metavar=\'N\',\n                            help=\'num encoder layers\')\n        parser.add_argument(\'--encoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num encoder attention heads\')\n        parser.add_argument(\'--encoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each encoder block\')\n        parser.add_argument(\'--encoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the encoder\')\n        parser.add_argument(\'--decoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained decoder embedding\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension for FFN\')\n        parser.add_argument(\'--decoder-layers\', type=int, metavar=\'N\',\n                            help=\'num decoder layers\')\n        parser.add_argument(\'--decoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num decoder attention heads\')\n        parser.add_argument(\'--decoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the decoder\')\n        parser.add_argument(\'--decoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each decoder block\')\n        parser.add_argument(\'--decoder-output-dim\', type=int, metavar=\'N\',\n                            help=\'decoder output dimension (extra linear layer \'\n                                 \'if different from decoder embed dim\')\n        parser.add_argument(\'--share-decoder-input-output-embed\', action=\'store_true\',\n                            help=\'share decoder input and output embeddings\')\n        parser.add_argument(\'--share-all-embeddings\', action=\'store_true\',\n                            help=\'share encoder, decoder and output embeddings\'\n                                 \' (requires shared dictionary and embed dim)\')\n        parser.add_argument(\'--no-token-positional-embeddings\', default=False, action=\'store_true\',\n                            help=\'if set, disables positional embeddings (outside self attention)\')\n        parser.add_argument(\'--adaptive-softmax-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive softmax cutoff points. \'\n                                 \'Must be used with adaptive_loss criterion\'),\n        parser.add_argument(\'--adaptive-softmax-dropout\', type=float, metavar=\'D\',\n                            help=\'sets adaptive softmax dropout for the tail projections\')\n        parser.add_argument(\'--layernorm-embedding\', action=\'store_true\',\n                            help=\'add layernorm to embedding\')\n        parser.add_argument(\'--no-scale-embedding\', action=\'store_true\',\n                            help=\'if True, dont scale embeddings\')\n        # args for ""Cross+Self-Attention for Transformer Models"" (Peitz et al., 2019)\n        parser.add_argument(\'--no-cross-attention\', default=False, action=\'store_true\',\n                            help=\'do not perform cross-attention\')\n        parser.add_argument(\'--cross-self-attention\', default=False, action=\'store_true\',\n                            help=\'perform cross+self-attention\')\n        # args for ""Reducing Transformer Depth on Demand with Structured Dropout"" (Fan et al., 2019)\n        parser.add_argument(\'--encoder-layerdrop\', type=float, metavar=\'D\', default=0,\n                            help=\'LayerDrop probability for encoder\')\n        parser.add_argument(\'--decoder-layerdrop\', type=float, metavar=\'D\', default=0,\n                            help=\'LayerDrop probability for decoder\')\n        parser.add_argument(\'--encoder-layers-to-keep\', default=None,\n                            help=\'which layers to *keep* when pruning as a comma-separated list\')\n        parser.add_argument(\'--decoder-layers-to-keep\', default=None,\n                            help=\'which layers to *keep* when pruning as a comma-separated list\')\n        # args for Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)\n        parser.add_argument(\'--quant-noise-pq\', type=float, metavar=\'D\', default=0,\n                            help=\'iterative PQ quantization noise at training time\')\n        parser.add_argument(\'--quant-noise-pq-block-size\', type=int, metavar=\'D\', default=8,\n                            help=\'block size of quantization noise at training time\')\n        parser.add_argument(\'--quant-noise-scalar\', type=float, metavar=\'D\', default=0,\n                            help=\'scalar quantization noise and scalar quantization at training time\')\n        # fmt: on\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present in older models\n        base_architecture(args)\n\n        if args.encoder_layers_to_keep:\n            args.encoder_layers = len(args.encoder_layers_to_keep.split("",""))\n        if args.decoder_layers_to_keep:\n            args.decoder_layers = len(args.decoder_layers_to_keep.split("",""))\n\n        if getattr(args, ""max_source_positions"", None) is None:\n            args.max_source_positions = DEFAULT_MAX_SOURCE_POSITIONS\n        if getattr(args, ""max_target_positions"", None) is None:\n            args.max_target_positions = DEFAULT_MAX_TARGET_POSITIONS\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        if args.share_all_embeddings:\n            if src_dict != tgt_dict:\n                raise ValueError(""--share-all-embeddings requires a joined dictionary"")\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise ValueError(\n                    ""--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim""\n                )\n            if args.decoder_embed_path and (\n                args.decoder_embed_path != args.encoder_embed_path\n            ):\n                raise ValueError(\n                    ""--share-all-embeddings not compatible with --decoder-embed-path""\n                )\n            encoder_embed_tokens = cls.build_embedding(\n                args, src_dict, args.encoder_embed_dim, args.encoder_embed_path\n            )\n            decoder_embed_tokens = encoder_embed_tokens\n            args.share_decoder_input_output_embed = True\n        else:\n            encoder_embed_tokens = cls.build_embedding(\n                args, src_dict, args.encoder_embed_dim, args.encoder_embed_path\n            )\n            decoder_embed_tokens = cls.build_embedding(\n                args, tgt_dict, args.decoder_embed_dim, args.decoder_embed_path\n            )\n\n        encoder = cls.build_encoder(args, src_dict, encoder_embed_tokens)\n        decoder = cls.build_decoder(args, tgt_dict, decoder_embed_tokens)\n        return cls(args, encoder, decoder)\n\n    @classmethod\n    def build_embedding(cls, args, dictionary, embed_dim, path=None):\n        num_embeddings = len(dictionary)\n        padding_idx = dictionary.pad()\n\n        emb = Embedding(num_embeddings, embed_dim, padding_idx)\n        # if provided, load from preloaded dictionaries\n        if path:\n            embed_dict = utils.parse_embedding(path)\n            utils.load_embedding(embed_dict, dictionary, emb)\n        return emb\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return TransformerEncoder(args, src_dict, embed_tokens)\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        return TransformerDecoder(\n            args,\n            tgt_dict,\n            embed_tokens,\n            no_encoder_attn=getattr(args, ""no_cross_attention"", False),\n        )\n\n    # TorchScript doesn\'t support optional arguments with variable length (**kwargs).\n    # Current workaround is to add union of all arguments in child classes.\n    def forward(\n        self,\n        src_tokens,\n        src_lengths,\n        prev_output_tokens,\n        return_all_hiddens: bool = True,\n        features_only: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n    ):\n        """"""\n        Run the forward pass for an encoder-decoder model.\n\n        Copied from the base class, but without ``**kwargs``,\n        which are not supported by TorchScript.\n        """"""\n        encoder_out = self.encoder(\n            src_tokens,\n            src_lengths=src_lengths,\n            return_all_hiddens=return_all_hiddens,\n        )\n        decoder_out = self.decoder(\n            prev_output_tokens,\n            encoder_out=encoder_out,\n            features_only=features_only,\n            alignment_layer=alignment_layer,\n            alignment_heads=alignment_heads,\n            src_lengths=src_lengths,\n            return_all_hiddens=return_all_hiddens,\n        )\n        return decoder_out\n\n    # Since get_normalized_probs is in the Fairseq Model which is not scriptable,\n    # I rewrite the get_normalized_probs from Base Class to call the\n    # helper function in the Base Class.\n    @torch.jit.export\n    def get_normalized_probs(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n        """"""Get normalized probabilities (or log probs) from a net\'s output.""""""\n        return self.get_normalized_probs_scriptable(net_output, log_probs, sample)\n\n\nclass TransformerEncoder(FairseqEncoder):\n    """"""\n    Transformer encoder consisting of *args.encoder_layers* layers. Each layer\n    is a :class:`TransformerEncoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): encoding dictionary\n        embed_tokens (torch.nn.Embedding): input embedding\n    """"""\n\n    def __init__(self, args, dictionary, embed_tokens):\n        super().__init__(dictionary)\n        self.register_buffer(""version"", torch.Tensor([3]))\n\n        self.dropout = args.dropout\n        self.encoder_layerdrop = args.encoder_layerdrop\n\n        embed_dim = embed_tokens.embedding_dim\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = args.max_source_positions\n\n        self.embed_tokens = embed_tokens\n\n        self.embed_scale = 1.0 if args.no_scale_embedding else math.sqrt(embed_dim)\n\n        self.embed_positions = (\n            PositionalEmbedding(\n                args.max_source_positions,\n                embed_dim,\n                self.padding_idx,\n                learned=args.encoder_learned_pos,\n            )\n            if not args.no_token_positional_embeddings\n            else None\n        )\n\n        if not args.adaptive_input and args.quant_noise_pq > 0:\n            self.quant_noise = apply_quant_noise_(\n                nn.Linear(embed_dim, embed_dim, bias=False),\n                args.quant_noise_pq,\n                args.quant_noise_pq_block_size,\n            )\n        else:\n            self.quant_noise = None\n\n        if self.encoder_layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.encoder_layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n        self.layers.extend([\n            self.build_encoder_layer(args)\n            for i in range(args.encoder_layers)\n        ])\n        self.num_layers = len(self.layers)\n\n        if args.encoder_normalize_before:\n            self.layer_norm = LayerNorm(embed_dim)\n        else:\n            self.layer_norm = None\n        if getattr(args, ""layernorm_embedding"", False):\n            self.layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.layernorm_embedding = None\n\n    def build_encoder_layer(self, args):\n        return TransformerEncoderLayer(args)\n\n    def forward_embedding(self, src_tokens):\n        # embed tokens and positions\n        x = embed = self.embed_scale * self.embed_tokens(src_tokens)\n        if self.embed_positions is not None:\n            x = embed + self.embed_positions(src_tokens)\n        if self.layernorm_embedding is not None:\n            x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        if self.quant_noise is not None:\n            x = self.quant_noise(x)\n        return x, embed\n\n    def forward(\n        self,\n        src_tokens,\n        src_lengths,\n        return_all_hiddens: bool = False,\n    ):\n        """"""\n        Args:\n            src_tokens (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            src_lengths (torch.LongTensor): lengths of each source sentence of\n                shape `(batch)`\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n\n        Returns:\n            namedtuple:\n                - **encoder_out** (Tensor): the last encoder layer\'s output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_padding_mask** (ByteTensor): the positions of\n                  padding elements of shape `(batch, src_len)`\n                - **encoder_embedding** (Tensor): the (scaled) embedding lookup\n                  of shape `(batch, src_len, embed_dim)`\n                - **encoder_states** (List[Tensor]): all intermediate\n                  hidden states of shape `(src_len, batch, embed_dim)`.\n                  Only populated if *return_all_hiddens* is True.\n        """"""\n        x, encoder_embedding = self.forward_embedding(src_tokens)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # compute padding mask\n        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n\n        encoder_states = [] if return_all_hiddens else None\n\n        # encoder layers\n        for layer in self.layers:\n            x = layer(x, encoder_padding_mask)\n            if return_all_hiddens:\n                assert encoder_states is not None\n                encoder_states.append(x)\n\n        if self.layer_norm is not None:\n            x = self.layer_norm(x)\n\n        return EncoderOut(\n            encoder_out=x,  # T x B x C\n            encoder_padding_mask=encoder_padding_mask,  # B x T\n            encoder_embedding=encoder_embedding,  # B x T x C\n            encoder_states=encoder_states,  # List[T x B x C]\n            src_tokens=None,\n            src_lengths=None,\n        )\n\n    @torch.jit.export\n    def reorder_encoder_out(self, encoder_out: EncoderOut, new_order):\n        """"""\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        """"""\n        new_encoder_out: Dict[str, Tensor] = {}\n\n        new_encoder_out[""encoder_out""] = (\n            encoder_out.encoder_out\n            if encoder_out.encoder_out is None\n            else encoder_out.encoder_out.index_select(1, new_order)\n        )\n        new_encoder_out[""encoder_padding_mask""] = (\n            encoder_out.encoder_padding_mask\n            if encoder_out.encoder_padding_mask is None\n            else encoder_out.encoder_padding_mask.index_select(0, new_order)\n        )\n        new_encoder_out[""encoder_embedding""] = (\n            encoder_out.encoder_embedding\n            if encoder_out.encoder_embedding is None\n            else encoder_out.encoder_embedding.index_select(0, new_order)\n        )\n        src_tokens = encoder_out.src_tokens\n        if src_tokens is not None:\n            src_tokens = src_tokens.index_select(0, new_order)\n\n        src_lengths = encoder_out.src_lengths\n        if src_lengths is not None:\n            src_lengths = src_lengths.index_select(0, new_order)\n\n        encoder_states = encoder_out.encoder_states\n        if encoder_states is not None:\n            for idx, state in enumerate(encoder_states):\n                encoder_states[idx] = state.index_select(1, new_order)\n\n        return EncoderOut(\n            encoder_out=new_encoder_out[""encoder_out""],  # T x B x C\n            encoder_padding_mask=new_encoder_out[""encoder_padding_mask""],  # B x T\n            encoder_embedding=new_encoder_out[""encoder_embedding""],  # B x T x C\n            encoder_states=encoder_states,  # List[T x B x C]\n            src_tokens=src_tokens,  # B x T\n            src_lengths=src_lengths,  # B x 1\n        )\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        if self.embed_positions is None:\n            return self.max_source_positions\n        return min(self.max_source_positions, self.embed_positions.max_positions)\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        """"""Upgrade a (possibly old) state dict for new versions of fairseq.""""""\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            weights_key = ""{}.embed_positions.weights"".format(name)\n            if weights_key in state_dict:\n                print(""deleting {0}"".format(weights_key))\n                del state_dict[weights_key]\n            state_dict[\n                ""{}.embed_positions._float_tensor"".format(name)\n            ] = torch.FloatTensor(1)\n        for i in range(self.num_layers):\n            # update layer norms\n            self.layers[i].upgrade_state_dict_named(\n                state_dict, ""{}.layers.{}"".format(name, i)\n            )\n\n        version_key = ""{}.version"".format(name)\n        if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) < 2:\n            # earlier checkpoints did not normalize after the stack of layers\n            self.layer_norm = None\n            self.normalize = False\n            state_dict[version_key] = torch.Tensor([1])\n        return state_dict\n\n\nclass TransformerDecoder(FairseqIncrementalDecoder):\n    """"""\n    Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`TransformerDecoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): decoding dictionary\n        embed_tokens (torch.nn.Embedding): output embedding\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n            (default: False).\n    """"""\n\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n        self.args = args\n        super().__init__(dictionary)\n        self.register_buffer(""version"", torch.Tensor([3]))\n        self._future_mask = torch.empty(0)\n\n        self.dropout = args.dropout\n        self.decoder_layerdrop = args.decoder_layerdrop\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n\n        input_embed_dim = embed_tokens.embedding_dim\n        embed_dim = args.decoder_embed_dim\n        self.embed_dim = embed_dim\n        self.output_embed_dim = args.decoder_output_dim\n\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = args.max_target_positions\n\n        self.embed_tokens = embed_tokens\n\n        self.embed_scale = 1.0 if args.no_scale_embedding else math.sqrt(embed_dim)\n\n        if not args.adaptive_input and args.quant_noise_pq > 0:\n            self.quant_noise = apply_quant_noise_(\n                nn.Linear(embed_dim, embed_dim, bias=False),\n                args.quant_noise_pq,\n                args.quant_noise_pq_block_size,\n            )\n        else:\n            self.quant_noise = None\n\n        self.project_in_dim = (\n            Linear(input_embed_dim, embed_dim, bias=False)\n            if embed_dim != input_embed_dim\n            else None\n        )\n\n        self.embed_positions = (\n            PositionalEmbedding(\n                args.max_target_positions,\n                embed_dim,\n                self.padding_idx,\n                learned=args.decoder_learned_pos,\n            )\n            if not args.no_token_positional_embeddings\n            else None\n        )\n\n        if getattr(args, ""layernorm_embedding"", False):\n            self.layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.layernorm_embedding = None\n\n        self.cross_self_attention = getattr(args, ""cross_self_attention"", False)\n\n        if self.decoder_layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.decoder_layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n        self.layers.extend([\n            self.build_decoder_layer(args, no_encoder_attn)\n            for _ in range(args.decoder_layers)\n        ])\n        self.num_layers = len(self.layers)\n\n        if args.decoder_normalize_before and not getattr(args, ""no_decoder_final_norm"", False):\n            self.layer_norm = LayerNorm(embed_dim)\n        else:\n            self.layer_norm = None\n\n        self.project_out_dim = (\n            Linear(embed_dim, self.output_embed_dim, bias=False)\n            if embed_dim != self.output_embed_dim and not args.tie_adaptive_weights\n            else None\n        )\n\n        self.adaptive_softmax = None\n        self.output_projection = None\n        if args.adaptive_softmax_cutoff is not None:\n            self.adaptive_softmax = AdaptiveSoftmax(\n                len(dictionary),\n                self.output_embed_dim,\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.adaptive_softmax_dropout,\n                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,\n                factor=args.adaptive_softmax_factor,\n                tie_proj=args.tie_adaptive_proj,\n            )\n        elif self.share_input_output_embed:\n            self.output_projection = nn.Linear(\n                self.embed_tokens.weight.shape[1],\n                self.embed_tokens.weight.shape[0],\n                bias=False,\n            )\n            self.output_projection.weight = self.embed_tokens.weight\n        else:\n            self.output_projection = nn.Linear(\n                self.output_embed_dim, len(dictionary), bias=False\n            )\n            nn.init.normal_(\n                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n            )\n\n    def build_decoder_layer(self, args, no_encoder_attn=False):\n        return TransformerDecoderLayer(args, no_encoder_attn)\n\n    def forward(\n        self,\n        prev_output_tokens,\n        encoder_out: Optional[EncoderOut] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        features_only: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n        src_lengths: Optional[Any] = None,\n        return_all_hiddens: bool = False,\n    ):\n        """"""\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_out (optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n            features_only (bool, optional): only return features without\n                applying output layer (default: False).\n\n        Returns:\n            tuple:\n                - the decoder\'s output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        """"""\n        x, extra = self.extract_features(\n            prev_output_tokens,\n            encoder_out=encoder_out,\n            incremental_state=incremental_state,\n            alignment_layer=alignment_layer,\n            alignment_heads=alignment_heads,\n        )\n        if not features_only:\n            x = self.output_layer(x)\n        return x, extra\n\n    def extract_features(\n        self,\n        prev_output_tokens,\n        encoder_out: Optional[EncoderOut] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        full_context_alignment: bool = False,\n        alignment_layer: Optional[int] = None,\n        alignment_heads: Optional[int] = None,\n    ):\n        """"""\n        Similar to *forward* but only return features.\n\n        Includes several features from ""Jointly Learning to Align and\n        Translate with Transformer Models"" (Garg et al., EMNLP 2019).\n\n        Args:\n            full_context_alignment (bool, optional): don\'t apply\n                auto-regressive mask to self-attention (default: False).\n            alignment_layer (int, optional): return mean alignment over\n                heads at this layer (default: last layer).\n            alignment_heads (int, optional): only average alignment over\n                this many heads (default: all heads).\n\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        """"""\n        if alignment_layer is None:\n            alignment_layer = self.num_layers - 1\n\n        # embed positions\n        positions = (\n            self.embed_positions(\n                prev_output_tokens, incremental_state=incremental_state\n            )\n            if self.embed_positions is not None\n            else None\n        )\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            if positions is not None:\n                positions = positions[:, -1:]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n\n        if self.quant_noise is not None:\n            x = self.quant_noise(x)\n\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        if positions is not None:\n            x += positions\n\n        if self.layernorm_embedding is not None:\n            x = self.layernorm_embedding(x)\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        self_attn_padding_mask: Optional[Tensor] = None\n        if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():\n            self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)\n\n        # decoder layers\n        attn: Optional[Tensor] = None\n        inner_states: List[Optional[Tensor]] = [x]\n        for idx, layer in enumerate(self.layers):\n            if incremental_state is None and not full_context_alignment:\n                self_attn_mask = self.buffered_future_mask(x)\n            else:\n                self_attn_mask = None\n\n            x, layer_attn, _ = layer(\n                x,\n                encoder_out.encoder_out if encoder_out is not None else None,\n                encoder_out.encoder_padding_mask if encoder_out is not None else None,\n                incremental_state,\n                self_attn_mask=self_attn_mask,\n                self_attn_padding_mask=self_attn_padding_mask,\n                need_attn=bool((idx == alignment_layer)),\n                need_head_weights=bool((idx == alignment_layer)),\n            )\n            inner_states.append(x)\n            if layer_attn is not None and idx == alignment_layer:\n                attn = layer_attn.float().to(x)\n\n        if attn is not None:\n            if alignment_heads is not None:\n                attn = attn[:alignment_heads]\n\n            # average probabilities over heads\n            attn = attn.mean(dim=0)\n\n        if self.layer_norm is not None:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        return x, {""attn"": [attn], ""inner_states"": inner_states}\n\n    def output_layer(self, features):\n        """"""Project features to the vocabulary size.""""""\n        if self.adaptive_softmax is None:\n            # project back to size of vocabulary\n            return self.output_projection(features)\n        else:\n            return features\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return min(self.max_target_positions, self.embed_positions.max_positions)\n\n    def buffered_future_mask(self, tensor):\n        dim = tensor.size(0)\n        # self._future_mask.device != tensor.device is not working in TorchScript. This is a workaround.\n        if (\n            self._future_mask.size(0) == 0\n            or (not self._future_mask.device == tensor.device)\n            or self._future_mask.size(0) < dim\n        ):\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1\n            )\n        self._future_mask = self._future_mask.to(tensor)\n        return self._future_mask[:dim, :dim]\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        """"""Upgrade a (possibly old) state dict for new versions of fairseq.""""""\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            weights_key = ""{}.embed_positions.weights"".format(name)\n            if weights_key in state_dict:\n                del state_dict[weights_key]\n            state_dict[\n                ""{}.embed_positions._float_tensor"".format(name)\n            ] = torch.FloatTensor(1)\n\n        if f""{name}.output_projection.weight"" not in state_dict:\n            if self.share_input_output_embed:\n                embed_out_key = f""{name}.embed_tokens.weight""\n            else:\n                embed_out_key = f""{name}.embed_out""\n            if embed_out_key in state_dict:\n                state_dict[f""{name}.output_projection.weight""] = state_dict[embed_out_key]\n                if not self.share_input_output_embed:\n                    del state_dict[embed_out_key]\n\n        for i in range(self.num_layers):\n            # update layer norms\n            layer_norm_map = {\n                ""0"": ""self_attn_layer_norm"",\n                ""1"": ""encoder_attn_layer_norm"",\n                ""2"": ""final_layer_norm"",\n            }\n            for old, new in layer_norm_map.items():\n                for m in (""weight"", ""bias""):\n                    k = ""{}.layers.{}.layer_norms.{}.{}"".format(name, i, old, m)\n                    if k in state_dict:\n                        state_dict[\n                            ""{}.layers.{}.{}.{}"".format(name, i, new, m)\n                        ] = state_dict[k]\n                        del state_dict[k]\n\n        version_key = ""{}.version"".format(name)\n        if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) <= 2:\n            # earlier checkpoints did not normalize after the stack of layers\n            self.layer_norm = None\n            self.normalize = False\n            state_dict[version_key] = torch.Tensor([1])\n\n        return state_dict\n\n    # Overwrite the method to temporaily support JIT scripting in Transformer\n    @torch.jit.export\n    def reorder_incremental_state(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        new_order: Tensor,\n    ):\n        """"""Scriptable reorder incremental state in the transformer.""""""\n        for layer in self.layers:\n            layer.reorder_incremental_state(incremental_state, new_order)\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.0)\n    return m\n\n\n@register_model_architecture(""transformer"", ""transformer"")\ndef base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.0)\n    args.activation_fn = getattr(args, ""activation_fn"", ""relu"")\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.adaptive_input = getattr(args, ""adaptive_input"", False)\n    args.no_cross_attention = getattr(args, ""no_cross_attention"", False)\n    args.cross_self_attention = getattr(args, ""cross_self_attention"", False)\n\n    args.decoder_output_dim = getattr(\n        args, ""decoder_output_dim"", args.decoder_embed_dim\n    )\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n\n    args.no_scale_embedding = getattr(args, ""no_scale_embedding"", False)\n    args.layernorm_embedding = getattr(args, ""layernorm_embedding"", False)\n    args.tie_adaptive_weights = getattr(args, ""tie_adaptive_weights"", False)\n\n\n@register_model_architecture(""transformer"", ""transformer_iwslt_de_en"")\ndef transformer_iwslt_de_en(args):\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 1024)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 4)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", 512)\n    args.decoder_ffn_embed_dim = getattr(args, ""decoder_ffn_embed_dim"", 1024)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 4)\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    base_architecture(args)\n\n\n@register_model_architecture(""transformer"", ""transformer_wmt_en_de"")\ndef transformer_wmt_en_de(args):\n    base_architecture(args)\n\n\n# parameters used in the ""Attention Is All You Need"" paper (Vaswani et al., 2017)\n@register_model_architecture(""transformer"", ""transformer_vaswani_wmt_en_de_big"")\ndef transformer_vaswani_wmt_en_de_big(args):\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 1024)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 4096)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 16)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", 1024)\n    args.decoder_ffn_embed_dim = getattr(args, ""decoder_ffn_embed_dim"", 4096)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 16)\n    args.dropout = getattr(args, ""dropout"", 0.3)\n    base_architecture(args)\n\n\n@register_model_architecture(""transformer"", ""transformer_vaswani_wmt_en_fr_big"")\ndef transformer_vaswani_wmt_en_fr_big(args):\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    transformer_vaswani_wmt_en_de_big(args)\n\n\n@register_model_architecture(""transformer"", ""transformer_wmt_en_de_big"")\ndef transformer_wmt_en_de_big(args):\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.1)\n    transformer_vaswani_wmt_en_de_big(args)\n\n\n# default parameters used in tensor2tensor implementation\n@register_model_architecture(""transformer"", ""transformer_wmt_en_de_big_t2t"")\ndef transformer_wmt_en_de_big_t2t(args):\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", True)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", True)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.1)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.1)\n    transformer_vaswani_wmt_en_de_big(args)\n'"
fairseq/models/transformer_align.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.transformer import (\n    base_architecture,\n    transformer_wmt_en_de_big,\n    TransformerModel,\n)\n\n\n@register_model(""transformer_align"")\nclass TransformerAlignModel(TransformerModel):\n    """"""\n    See ""Jointly Learning to Align and Translate with Transformer\n    Models"" (Garg et al., EMNLP 2019).\n    """"""\n\n    def __init__(self, encoder, decoder, args):\n        super().__init__(args, encoder, decoder)\n        self.alignment_heads = args.alignment_heads\n        self.alignment_layer = args.alignment_layer\n        self.full_context_alignment = args.full_context_alignment\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        super(TransformerAlignModel, TransformerAlignModel).add_args(parser)\n        parser.add_argument(\'--alignment-heads\', type=int, metavar=\'D\',\n                            help=\'Number of cross attention heads per layer to supervised with alignments\')\n        parser.add_argument(\'--alignment-layer\', type=int, metavar=\'D\',\n                            help=\'Layer number which has to be supervised. 0 corresponding to the bottommost layer.\')\n        parser.add_argument(\'--full-context-alignment\', type=bool, metavar=\'D\',\n                            help=\'Whether or not alignment is supervised conditioned on the full target context.\')\n        # fmt: on\n\n    @classmethod\n    def build_model(cls, args, task):\n        # set any default arguments\n        transformer_align(args)\n\n        transformer_model = TransformerModel.build_model(args, task)\n        return TransformerAlignModel(\n            transformer_model.encoder, transformer_model.decoder, args\n        )\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens):\n        encoder_out = self.encoder(src_tokens, src_lengths)\n        return self.forward_decoder(prev_output_tokens, encoder_out)\n\n    def forward_decoder(\n        self,\n        prev_output_tokens,\n        encoder_out=None,\n        incremental_state=None,\n        features_only=False,\n        **extra_args,\n    ):\n        attn_args = {\n            ""alignment_layer"": self.alignment_layer,\n            ""alignment_heads"": self.alignment_heads,\n        }\n        decoder_out = self.decoder(prev_output_tokens, encoder_out, **attn_args)\n\n        if self.full_context_alignment:\n            attn_args[""full_context_alignment""] = self.full_context_alignment\n            _, alignment_out = self.decoder(\n                prev_output_tokens,\n                encoder_out,\n                features_only=True,\n                **attn_args,\n                **extra_args,\n            )\n            decoder_out[1][""attn""] = alignment_out[""attn""]\n\n        return decoder_out\n\n\n@register_model_architecture(""transformer_align"", ""transformer_align"")\ndef transformer_align(args):\n    args.alignment_heads = getattr(args, ""alignment_heads"", 1)\n    args.alignment_layer = getattr(args, ""alignment_layer"", 4)\n    args.full_context_alignment = getattr(args, ""full_context_alignment"", False)\n    base_architecture(args)\n\n\n@register_model_architecture(""transformer_align"", ""transformer_wmt_en_de_big_align"")\ndef transformer_wmt_en_de_big_align(args):\n    args.alignment_heads = getattr(args, ""alignment_heads"", 1)\n    args.alignment_layer = getattr(args, ""alignment_layer"", 4)\n    transformer_wmt_en_de_big(args)\n'"
fairseq/models/transformer_from_pretrained_xlm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nfrom typing import Any, Dict\n\nfrom fairseq import checkpoint_utils\nfrom fairseq.data.legacy.masked_lm_dictionary import MaskedLMDictionary\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.transformer import (\n    TransformerDecoder,\n    TransformerEncoder,\n    TransformerModel,\n    base_architecture as transformer_base_architecture,\n)\n\n\n@register_model(""transformer_from_pretrained_xlm"")\nclass TransformerFromPretrainedXLMModel(TransformerModel):\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        TransformerModel.add_args(parser)\n        parser.add_argument(\n            ""--pretrained-xlm-checkpoint"",\n            type=str,\n            metavar=""STR"",\n            help=""XLM model to use for initializing transformer encoder and/or decoder"",\n        )\n        parser.add_argument(\n            ""--init-encoder-only"",\n            action=""store_true"",\n            help=""if set, don\'t load the XLM weights and embeddings into decoder"",\n        )\n        parser.add_argument(\n            ""--init-decoder-only"",\n            action=""store_true"",\n            help=""if set, don\'t load the XLM weights and embeddings into encoder"",\n        )\n\n    @classmethod\n    def build_model(self, args, task, cls_dictionary=MaskedLMDictionary):\n        assert hasattr(args, ""pretrained_xlm_checkpoint""), (\n            ""You must specify a path for --pretrained-xlm-checkpoint to use ""\n            ""--arch transformer_from_pretrained_xlm""\n        )\n        assert isinstance(task.source_dictionary, cls_dictionary) and isinstance(\n            task.target_dictionary, cls_dictionary\n        ), (\n            ""You should use a MaskedLMDictionary when using --arch ""\n            ""transformer_from_pretrained_xlm because the pretrained XLM model ""\n            ""was trained using data binarized with MaskedLMDictionary. ""\n            ""For translation, you may want to use --task ""\n            ""translation_from_pretrained_xlm""\n        )\n        assert not (\n            getattr(args, ""init_encoder_only"", False)\n            and getattr(args, ""init_decoder_only"", False)\n        ), ""Only one of --init-encoder-only and --init-decoder-only can be set.""\n        return super().build_model(args, task)\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return TransformerEncoderFromPretrainedXLM(args, src_dict, embed_tokens)\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        return TransformerDecoderFromPretrainedXLM(args, tgt_dict, embed_tokens)\n\n\ndef upgrade_state_dict_with_xlm_weights(\n    state_dict: Dict[str, Any], pretrained_xlm_checkpoint: str\n) -> Dict[str, Any]:\n    """"""\n    Load XLM weights into a Transformer encoder or decoder model.\n\n    Args:\n        state_dict: state dict for either TransformerEncoder or\n            TransformerDecoder\n        pretrained_xlm_checkpoint: checkpoint to load XLM weights from\n\n    Raises:\n        AssertionError: If architecture (num layers, attention heads, etc.)\n            does not match between the current Transformer encoder or\n            decoder and the pretrained_xlm_checkpoint\n    """"""\n    if not os.path.exists(pretrained_xlm_checkpoint):\n        raise IOError(""Model file not found: {}"".format(pretrained_xlm_checkpoint))\n\n    state = checkpoint_utils.load_checkpoint_to_cpu(pretrained_xlm_checkpoint)\n    xlm_state_dict = state[""model""]\n    for key in xlm_state_dict.keys():\n\n        for search_key in [""embed_tokens"", ""embed_positions"", ""layers""]:\n            if search_key in key:\n                subkey = key[key.find(search_key):]\n                assert subkey in state_dict, (\n                    ""{} Transformer encoder / decoder ""\n                    ""state_dict does not contain {}. Cannot ""\n                    ""load {} from pretrained XLM checkpoint ""\n                    ""{} into Transformer."".format(\n                        str(state_dict.keys()),\n                        subkey, key, pretrained_xlm_checkpoint)\n                    )\n\n                state_dict[subkey] = xlm_state_dict[key]\n    return state_dict\n\n\nclass TransformerEncoderFromPretrainedXLM(TransformerEncoder):\n\n    def __init__(self, args, dictionary, embed_tokens):\n        super().__init__(args, dictionary, embed_tokens)\n        if getattr(args, \'init_decoder_only\', False):\n            # Don\'t load XLM weights for encoder if --init-decoder-only\n            return\n\n        assert hasattr(args, ""pretrained_xlm_checkpoint""), (\n            ""--pretrained-xlm-checkpoint must be specified to load Transformer ""\n            ""encoder from pretrained XLM""\n        )\n        xlm_loaded_state_dict = upgrade_state_dict_with_xlm_weights(\n            state_dict=self.state_dict(),\n            pretrained_xlm_checkpoint=args.pretrained_xlm_checkpoint,\n        )\n        self.load_state_dict(xlm_loaded_state_dict, strict=True)\n\n\nclass TransformerDecoderFromPretrainedXLM(TransformerDecoder):\n\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n        super().__init__(args, dictionary, embed_tokens, no_encoder_attn)\n        if getattr(args, \'init_encoder_only\', False):\n            # Don\'t load XLM weights for decoder if --init-encoder-only\n            return\n        assert hasattr(args, ""pretrained_xlm_checkpoint""), (\n            ""--pretrained-xlm-checkpoint must be specified to load Transformer ""\n            ""decoder from pretrained XLM""\n        )\n\n        xlm_loaded_state_dict = upgrade_state_dict_with_xlm_weights(\n            state_dict=self.state_dict(),\n            pretrained_xlm_checkpoint=args.pretrained_xlm_checkpoint,\n        )\n        self.load_state_dict(xlm_loaded_state_dict, strict=True)\n\n\n@register_model_architecture(\n    ""transformer_from_pretrained_xlm"", ""transformer_from_pretrained_xlm""\n)\ndef base_architecture(args):\n    transformer_base_architecture(args)\n'"
fairseq/models/transformer_lm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqLanguageModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.transformer import (\n    Embedding,\n    TransformerDecoder,\n)\nfrom fairseq.modules import (\n    AdaptiveInput,\n    CharacterTokenEmbedder,\n)\n\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\n\n@register_model(\'transformer_lm\')\nclass TransformerLanguageModel(FairseqLanguageModel):\n\n    @classmethod\n    def hub_models(cls):\n\n        def moses_fastbpe(path):\n            return {\n                \'path\': path,\n                \'tokenizer\': \'moses\',\n                \'bpe\': \'fastbpe\',\n            }\n\n        return {\n            \'transformer_lm.gbw.adaptive_huge\': \'https://dl.fbaipublicfiles.com/fairseq/models/lm/adaptive_lm_gbw_huge.tar.bz2\',\n            \'transformer_lm.wiki103.adaptive\': \'https://dl.fbaipublicfiles.com/fairseq/models/lm/adaptive_lm_wiki103.v2.tar.bz2\',\n            \'transformer_lm.wmt19.en\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/lm/wmt19.en.tar.bz2\'),\n            \'transformer_lm.wmt19.de\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/lm/wmt19.de.tar.bz2\'),\n            \'transformer_lm.wmt19.ru\': moses_fastbpe(\'https://dl.fbaipublicfiles.com/fairseq/models/lm/wmt19.ru.tar.bz2\'),\n        }\n\n    def __init__(self, decoder):\n        super().__init__(decoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--activation-fn\',\n                            choices=utils.get_available_activation_fns(),\n                            help=\'activation function to use\')\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--attention-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for attention weights\')\n        parser.add_argument(\'--activation-dropout\', \'--relu-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability after activation in FFN.\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-output-dim\', type=int, metavar=\'N\',\n                            help=\'decoder output dimension\')\n        parser.add_argument(\'--decoder-input-dim\', type=int, metavar=\'N\',\n                            help=\'decoder input dimension\')\n        parser.add_argument(\'--decoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension for FFN\')\n        parser.add_argument(\'--decoder-layers\', type=int, metavar=\'N\',\n                            help=\'num decoder layers\')\n        parser.add_argument(\'--decoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num decoder attention heads\')\n        parser.add_argument(\'--decoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each decoder block\')\n        parser.add_argument(\'--no-decoder-final-norm\', action=\'store_true\',\n                            help=\'don\\\'t add an extra layernorm after the last decoder block\')\n        parser.add_argument(\'--adaptive-softmax-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive softmax cutoff points. \'\n                                 \'Must be used with adaptive_loss criterion\')\n        parser.add_argument(\'--adaptive-softmax-dropout\', type=float, metavar=\'D\',\n                            help=\'sets adaptive softmax dropout for the tail projections\')\n        parser.add_argument(\'--adaptive-softmax-factor\', type=float, metavar=\'N\',\n                            help=\'adaptive input factor\')\n        parser.add_argument(\'--no-token-positional-embeddings\', action=\'store_true\',\n                            help=\'if set, disables positional embeddings (outside self attention)\')\n        parser.add_argument(\'--share-decoder-input-output-embed\', action=\'store_true\',\n                            help=\'share decoder input and output embeddings\')\n        parser.add_argument(\'--character-embeddings\', action=\'store_true\',\n                            help=\'if set, uses character embedding convolutions to produce token embeddings\')\n        parser.add_argument(\'--character-filters\', type=str, metavar=\'LIST\',\n                            default=\'[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]\',\n                            help=\'size of character embeddings\')\n        parser.add_argument(\'--character-embedding-dim\', default=4, type=int, metavar=\'N\',\n                            help=\'size of character embeddings\')\n        parser.add_argument(\'--char-embedder-highway-layers\', default=2, type=int, metavar=\'N\',\n                            help=\'number of highway layers for character token embeddder\')\n        parser.add_argument(\'--adaptive-input\', action=\'store_true\',\n                            help=\'if set, uses adaptive input\')\n        parser.add_argument(\'--adaptive-input-factor\', type=float, metavar=\'N\',\n                            help=\'adaptive input factor\')\n        parser.add_argument(\'--adaptive-input-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of adaptive input cutoff points.\')\n        parser.add_argument(\'--tie-adaptive-weights\', action=\'store_true\',\n                            help=\'if set, ties the weights of adaptive softmax and adaptive input\')\n        parser.add_argument(\'--tie-adaptive-proj\', action=\'store_true\',\n                            help=\'if set, ties the projection weights of adaptive softmax and adaptive input\')\n        parser.add_argument(\'--decoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the decoder\')\n        parser.add_argument(\'--layernorm-embedding\', action=\'store_true\',\n                            help=\'add layernorm to embedding\')\n        parser.add_argument(\'--no-scale-embedding\', action=\'store_true\',\n                            help=\'if True, dont scale embeddings\')\n        # args for ""Reducing Transformer Depth on Demand with Structured Dropout"" (Fan et al., 2019)\n        parser.add_argument(\'--decoder-layerdrop\', type=float, metavar=\'D\', default=0,\n                            help=\'LayerDrop probability for decoder\')\n        parser.add_argument(\'--decoder-layers-to-keep\', default=None,\n                            help=\'which layers to *keep* when pruning as a comma-separated list\')\n        # args for Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)\n        parser.add_argument(\'--quant-noise-pq\', type=float, metavar=\'D\', default=0,\n                            help=\'iterative PQ quantization noise at training time\')\n        parser.add_argument(\'--quant-noise-pq-block-size\', type=int, metavar=\'D\', default=8,\n                            help=\'block size of quantization noise at training time\')\n        parser.add_argument(\'--quant-noise-scalar\', type=float, metavar=\'D\', default=0,\n                            help=\'scalar quantization noise and scalar quantization at training time\')\n        # fmt: on\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present in older models\n        base_lm_architecture(args)\n\n        if args.decoder_layers_to_keep:\n            args.decoder_layers = len(args.decoder_layers_to_keep.split("",""))\n\n        if getattr(args, \'max_target_positions\', None) is None:\n            args.max_target_positions = getattr(args, \'tokens_per_sample\', DEFAULT_MAX_TARGET_POSITIONS)\n\n        if args.character_embeddings:\n            embed_tokens = CharacterTokenEmbedder(\n                task.source_dictionary, eval(args.character_filters),\n                args.character_embedding_dim, args.decoder_embed_dim,\n                args.char_embedder_highway_layers,\n            )\n        elif args.adaptive_input:\n            embed_tokens = AdaptiveInput(\n                len(task.source_dictionary), task.source_dictionary.pad(), args.decoder_input_dim,\n                args.adaptive_input_factor, args.decoder_embed_dim,\n                options.eval_str_list(args.adaptive_input_cutoff, type=int),\n                args.quant_noise_pq, args.quant_noise_pq_block_size,\n            )\n        else:\n            embed_tokens = cls.build_embedding(args, task.source_dictionary, args.decoder_input_dim)\n\n        if args.tie_adaptive_weights:\n            assert args.adaptive_input\n            assert args.adaptive_input_factor == args.adaptive_softmax_factor\n            assert args.adaptive_softmax_cutoff == args.adaptive_input_cutoff, \'{} != {}\'.format(\n                args.adaptive_softmax_cutoff, args.adaptive_input_cutoff)\n            assert args.decoder_input_dim == args.decoder_output_dim\n\n        decoder = TransformerDecoder(\n            args, task.target_dictionary, embed_tokens, no_encoder_attn=True,\n        )\n        return cls(decoder)\n\n    @classmethod\n    def build_embedding(cls, args, dictionary, embed_dim, path=None):\n        embed_tokens = Embedding(len(dictionary), embed_dim, dictionary.pad())\n        return embed_tokens\n\n@register_model_architecture(\'transformer_lm\', \'transformer_lm\')\ndef base_lm_architecture(args):\n    # backward compatibility for older model checkpoints\n    if hasattr(args, \'no_tie_adaptive_proj\'):\n        # previous models defined --no-tie-adaptive-proj, so use the existence of\n        # that option to determine if this is an ""old"" model checkpoint\n        args.no_decoder_final_norm = True  # old models always set this to True\n        if args.no_tie_adaptive_proj is False:\n            args.tie_adaptive_proj = True\n    if hasattr(args, \'decoder_final_norm\'):\n        args.no_decoder_final_norm = not args.decoder_final_norm\n\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.0)\n\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 2048)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 6)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 8)\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', None)\n    args.adaptive_softmax_dropout = getattr(args, \'adaptive_softmax_dropout\', 0)\n    args.adaptive_softmax_factor = getattr(args, \'adaptive_softmax_factor\', 4)\n    args.decoder_learned_pos = getattr(args, \'decoder_learned_pos\', False)\n    args.activation_fn = getattr(args, \'activation_fn\', \'relu\')\n\n    args.add_bos_token = getattr(args, \'add_bos_token\', False)\n    args.no_token_positional_embeddings = getattr(args, \'no_token_positional_embeddings\', False)\n    args.share_decoder_input_output_embed = getattr(args, \'share_decoder_input_output_embed\', False)\n    args.character_embeddings = getattr(args, \'character_embeddings\', False)\n\n    args.decoder_output_dim = getattr(args, \'decoder_output_dim\', args.decoder_embed_dim)\n    args.decoder_input_dim = getattr(args, \'decoder_input_dim\', args.decoder_embed_dim)\n\n    # Model training is not stable without this\n    args.decoder_normalize_before = True\n    args.no_decoder_final_norm = getattr(args, \'no_decoder_final_norm\', False)\n\n    args.adaptive_input = getattr(args, \'adaptive_input\', False)\n    args.adaptive_input_factor = getattr(args, \'adaptive_input_factor\', 4)\n    args.adaptive_input_cutoff = getattr(args, \'adaptive_input_cutoff\', None)\n\n    args.tie_adaptive_weights = getattr(args, \'tie_adaptive_weights\', False)\n    args.tie_adaptive_proj = getattr(args, \'tie_adaptive_proj\', False)\n\n    args.no_scale_embedding = getattr(args, \'no_scale_embedding\', False)\n    args.layernorm_embedding = getattr(args, \'layernorm_embedding\', False)\n\n\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_big\')\ndef transformer_lm_big(args):\n    args.decoder_layers = getattr(args, \'decoder_layers\', 12)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 1024)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 4096)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 16)\n    base_lm_architecture(args)\n\n\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_wiki103\')\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_baevski_wiki103\')\ndef transformer_lm_baevski_wiki103(args):\n    args.decoder_layers = getattr(args, \'decoder_layers\', 16)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 8)\n    args.dropout = getattr(args, \'dropout\', 0.3)\n    args.adaptive_input = getattr(args, \'adaptive_input\', True)\n    args.tie_adaptive_weights = getattr(args, \'tie_adaptive_weights\', True)\n    args.adaptive_input_cutoff = getattr(args, \'adaptive_input_cutoff\', \'20000,60000\')\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', \'20000,60000\')\n    args.adaptive_softmax_dropout = getattr(args, \'adaptive_softmax_dropout\', 0.2)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_dropout = getattr(args, \'activation_dropout\', 0.1)\n    args.no_decoder_final_norm = getattr(args, \'no_decoder_final_norm\', True)\n    args.tie_adaptive_proj = getattr(args, \'tie_adaptive_proj\', True)\n    transformer_lm_big(args)\n\n\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_gbw\')\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_baevski_gbw\')\ndef transformer_lm_baevski_gbw(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 512)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.no_decoder_final_norm = getattr(args, \'no_decoder_final_norm\', True)\n    transformer_lm_big(args)\n\n\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_gpt\')\ndef transformer_lm_gpt(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 768)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 3072)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 12)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 12)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    base_lm_architecture(args)\n\n\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_gpt2_small\')\ndef transformer_lm_gpt2_small(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 1024)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 4096)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 24)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 16)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    base_lm_architecture(args)\n\n\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_gpt2_medium\')\ndef transformer_lm_gpt2_medium(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 1280)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 5120)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 36)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 20)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    base_lm_architecture(args)\n\n\n@register_model_architecture(\'transformer_lm\', \'transformer_lm_gpt2_big\')\ndef transformer_lm_gpt2_big(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 1600)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 6400)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 48)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 25)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    base_lm_architecture(args)\n'"
fairseq/models/wav2vec.py,13,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport math\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq.models import BaseFairseqModel, register_model, register_model_architecture\nfrom fairseq.modules import (\n    Fp32GroupNorm,\n    Fp32LayerNorm,\n    GumbelVectorQuantizer,\n    KmeansVectorQuantizer,\n)\nfrom fairseq.utils import buffered_arange\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(""wav2vec"")\nclass Wav2VecModel(BaseFairseqModel):\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--prediction-steps"",\n            type=int,\n            metavar=""N"",\n            help=""number of steps ahead to predict"",\n        )\n        parser.add_argument(\n            ""--sample-distance"",\n            type=int,\n            metavar=""N"",\n            help=""sample distance from target. does not work properly with cross-sampling"",\n        )\n        parser.add_argument(\n            ""--cross-sample-negatives"",\n            type=int,\n            metavar=""N"",\n            help=""num of cross sampled negatives"",\n        )\n        parser.add_argument(\n            ""--num-negatives"", type=int, metavar=""N"", help=""number of negative examples""\n        )\n        parser.add_argument(\n            ""--conv-feature-layers"",\n            type=str,\n            metavar=""EXPR"",\n            help=""convolutional feature extraction layers [(dim, kernel_size, stride), ...]"",\n        )\n        parser.add_argument(\n            ""--conv-aggregator-layers"",\n            type=str,\n            metavar=""EXPR"",\n            help=""convolutional feature extraction layers [(dim, kernel_size, stride), ...]"",\n        )\n        parser.add_argument(\n            ""--dropout"",\n            type=float,\n            metavar=""D"",\n            help=""dropout to apply within the model"",\n        )\n        parser.add_argument(\n            ""--dropout-features"",\n            type=float,\n            metavar=""D"",\n            help=""dropout to apply to the features"",\n        )\n        parser.add_argument(\n            ""--dropout-agg"",\n            type=float,\n            metavar=""D"",\n            help=""dropout to apply after aggregation step"",\n        )\n        parser.add_argument(\n            ""--encoder"", type=str, choices=[""cnn""], help=""type of encoder to use""\n        )\n        parser.add_argument(\n            ""--aggregator"",\n            type=str,\n            choices=[""cnn"", ""gru""],\n            help=""type of aggregator to use"",\n        )\n        parser.add_argument(\n            ""--gru-dim"", type=int, metavar=""N"", help=""GRU dimensionality""\n        )\n\n        parser.add_argument(\n            ""--no-conv-bias"",\n            action=""store_true"",\n            help=""if set, does not learn bias for conv layers"",\n        )\n        parser.add_argument(\n            ""--agg-zero-pad"",\n            action=""store_true"",\n            help=""if set, zero pads in aggregator instead of repl pad"",\n        )\n\n        parser.add_argument(\n            ""--skip-connections-feat"",\n            action=""store_true"",\n            help=""if set, adds skip connections to the feature extractor"",\n        )\n        parser.add_argument(\n            ""--skip-connections-agg"",\n            action=""store_true"",\n            help=""if set, adds skip connections to the aggregator"",\n        )\n        parser.add_argument(\n            ""--residual-scale"",\n            type=float,\n            metavar=""D"",\n            help=""scales residual by sqrt(value)"",\n        )\n\n        parser.add_argument(\n            ""--log-compression"",\n            action=""store_true"",\n            help=""if set, adds a log compression to feature extractor"",\n        )\n\n        parser.add_argument(\n            ""--balanced-classes"",\n            action=""store_true"",\n            help=""if set, loss is scaled to balance for number of negatives"",\n        )\n\n        parser.add_argument(\n            ""--project-features"",\n            choices=[""none"", ""same"", ""new""],\n            help=""if not none, features are projected using the (same or new) aggregator"",\n        )\n\n        parser.add_argument(\n            ""--non-affine-group-norm"",\n            action=""store_true"",\n            help=""if set, group norm is not affine"",\n        )\n\n        parser.add_argument(\n            ""--offset"",\n            help=""if set, introduces an offset from target to predictions. ""\n            \'if set to ""auto"", it is computed automatically from the receptive field\',\n        )\n\n        parser.add_argument(\n            ""--activation"",\n            type=str,\n            choices=[""relu"", ""gelu""],\n            help=""which activation function to use"",\n        )\n\n        parser.add_argument(\n            ""--vq-type"",\n            type=str,\n            choices=[""none"", ""gumbel"", ""kmeans""],\n            help=""which type of quantizer to use"",\n        )\n        parser.add_argument(\n            ""--vq-vars"",\n            type=int,\n            metavar=""N"",\n            help=""if set, project to this many vector quantized variables per group"",\n        )\n        parser.add_argument(\n            ""--vq-groups"",\n            type=int,\n            metavar=""N"",\n            help=""number of groups of latent variables"",\n        )\n        parser.add_argument(\n            ""--vq-dim"",\n            type=int,\n            metavar=""N"",\n            help=""uses this dimensionality for quantized vectors"",\n        )\n        parser.add_argument(\n            ""--vq-depth"",\n            type=int,\n            metavar=""N"",\n            help=""number of layers for vq weight projection"",\n        )\n        parser.add_argument(\n            ""--combine-groups"",\n            action=""store_true"",\n            help=""if set, variables are shared among groups"",\n        )\n        parser.add_argument(\n            ""--vq-temp"",\n            type=str,\n            metavar=""TEMP"",\n            help=""temperature for latent variable sampling with gumbel softmax. should be a tuple of 3 values (start, end, decay)"",\n        )\n        parser.add_argument(\n            ""--vq-gamma"",\n            type=float,\n            metavar=""D"",\n            help=""gamma parameter for kmeans style vector quantization"",\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present in older models\n        base_wav2vec_architecture(args)\n\n        model = Wav2VecModel(args)\n        logger.info(model)\n        return model\n\n    def __init__(self, args):\n        super().__init__()\n\n        self.prediction_steps = args.prediction_steps\n        offset = args.offset\n\n        if args.activation == ""relu"":\n            activation = nn.ReLU()\n        elif args.activation == ""gelu"":\n            activation = nn.GELU()\n        else:\n            raise Exception(""unknown activation "" + args.activation)\n\n        if args.encoder == ""cnn"":\n            feature_enc_layers = eval(args.conv_feature_layers)\n            self.feature_extractor = ConvFeatureExtractionModel(\n                conv_layers=feature_enc_layers,\n                dropout=0.0,\n                log_compression=args.log_compression,\n                skip_connections=args.skip_connections_feat,\n                residual_scale=args.residual_scale,\n                non_affine_group_norm=args.non_affine_group_norm,\n                activation=activation,\n            )\n            embed = feature_enc_layers[-1][0]\n        else:\n            raise Exception(""unknown encoder type "" + args.encoder)\n\n        self.vector_quantizer = None\n        if args.vq_type == ""gumbel"":\n            self.vector_quantizer = GumbelVectorQuantizer(\n                dim=embed,\n                num_vars=args.vq_vars,\n                temp=eval(args.vq_temp),\n                groups=args.vq_groups,\n                combine_groups=args.combine_groups,\n                vq_dim=args.vq_dim if args.vq_dim > 0 else embed,\n                time_first=False,\n                activation=activation,\n                weight_proj_depth=args.vq_depth,\n                weight_proj_factor=2,\n            )\n        elif args.vq_type == ""kmeans"":\n            self.vector_quantizer = KmeansVectorQuantizer(\n                dim=embed,\n                num_vars=args.vq_vars,\n                groups=args.vq_groups,\n                combine_groups=args.combine_groups,\n                vq_dim=args.vq_dim if args.vq_dim > 0 else embed,\n                time_first=False,\n                gamma=args.vq_gamma,\n            )\n        else:\n            assert (\n                args.vq_type == ""none"" or args.vq_type is None\n            ), ""Unknown quantizer type""\n\n        if args.offset == ""auto"":\n            assert args.encoder == ""cnn""\n            jin = 0\n            rin = 0\n            for _, k, stride in feature_enc_layers:\n                if rin == 0:\n                    rin = k\n                rin = rin + (k - 1) * jin\n                if jin == 0:\n                    jin = stride\n                else:\n                    jin *= stride\n            offset = math.ceil(rin / jin)\n\n        offset = int(offset)\n\n        def make_aggregator():\n            if args.aggregator == ""cnn"":\n                agg_layers = eval(args.conv_aggregator_layers)\n                agg_dim = agg_layers[-1][0]\n                feature_aggregator = ConvAggegator(\n                    conv_layers=agg_layers,\n                    embed=embed,\n                    dropout=args.dropout,\n                    skip_connections=args.skip_connections_agg,\n                    residual_scale=args.residual_scale,\n                    non_affine_group_norm=args.non_affine_group_norm,\n                    conv_bias=not args.no_conv_bias,\n                    zero_pad=args.agg_zero_pad,\n                    activation=activation,\n                )\n            elif args.aggregator == ""gru"":\n                agg_dim = args.gru_dim\n                feature_aggregator = nn.Sequential(\n                    TransposeLast(),\n                    nn.GRU(\n                        input_size=embed,\n                        hidden_size=agg_dim,\n                        num_layers=1,\n                        dropout=args.dropout,\n                    ),\n                    TransposeLast(deconstruct_idx=0),\n                )\n            else:\n                raise Exception(""unknown aggregator type "" + args.aggregator)\n\n            return feature_aggregator, agg_dim\n\n        self.feature_aggregator, agg_dim = make_aggregator()\n\n        self.wav2vec_predictions = Wav2VecPredictionsModel(\n            in_dim=agg_dim,\n            out_dim=embed,\n            prediction_steps=args.prediction_steps,\n            n_negatives=args.num_negatives,\n            cross_sample_negatives=args.cross_sample_negatives,\n            sample_distance=args.sample_distance,\n            dropout=args.dropout,\n            offset=offset,\n            balanced_classes=args.balanced_classes,\n            infonce=args.infonce,\n        )\n\n        self.dropout_feats = nn.Dropout(p=args.dropout_features)\n        self.dropout_agg = nn.Dropout(p=args.dropout_agg)\n\n        if args.project_features == ""none"":\n            self.project_features = None\n        elif args.project_features == ""same"":\n            self.project_features = self.feature_aggregator\n        elif args.project_features == ""new"":\n            self.project_features, _ = make_aggregator()\n\n    def forward(self, source):\n        result = {}\n\n        features = self.feature_extractor(source)\n        if self.vector_quantizer:\n            q_res = self.vector_quantizer(features)\n            features = q_res[""x""]\n            for k in q_res.keys():\n                if k != ""x"":\n                    result[k] = q_res[k]\n\n        x = self.dropout_feats(features)\n        x = self.feature_aggregator(x)\n        x = self.dropout_agg(x)\n\n        if self.project_features is not None:\n            features = self.project_features(features)\n        x, targets = self.wav2vec_predictions(x, features)\n        result[""cpc_logits""] = x\n        result[""cpc_targets""] = targets\n\n        return result\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        super().upgrade_state_dict_named(state_dict, name)\n\n    def max_positions(self):\n        """"""Maximum length supported by the model.""""""\n        return sys.maxsize\n\n    def get_logits(self, net_output):\n        logits = net_output[""cpc_logits""]\n        return logits\n\n    def get_targets(self, sample, net_output):\n        t = net_output[""cpc_targets""]\n        if isinstance(t, tuple):\n            t = t[0]\n        return t.contiguous()\n\n    def get_target_weights(self, targets, net_output):\n        targets = net_output[""cpc_targets""]\n        if isinstance(targets, tuple) and targets[-1] is not None:\n            return targets[-1]\n        return None\n\n    def get_extra_losses(self, net_output):\n        loss = None\n        if ""prob_perplexity"" in net_output:\n            loss = net_output[""num_vars""] - net_output[""prob_perplexity""]\n        elif ""kmeans_loss"" in net_output:\n            loss = net_output[""kmeans_loss""]\n\n        return loss\n\n\nclass TransposeLast(nn.Module):\n    def __init__(self, deconstruct_idx=None):\n        super().__init__()\n        self.deconstruct_idx = deconstruct_idx\n\n    def forward(self, x):\n        if self.deconstruct_idx is not None:\n            x = x[self.deconstruct_idx]\n        return x.transpose(-2, -1)\n\ndef norm_block(is_layer_norm, dim, affine=True):\n    if is_layer_norm:\n        mod = nn.Sequential(\n            TransposeLast(),\n            Fp32LayerNorm(dim, elementwise_affine=affine),\n            TransposeLast(),\n        )\n    else:\n        mod = Fp32GroupNorm(1, dim, affine=affine)\n\n    return mod\n\n\nclass ConvFeatureExtractionModel(nn.Module):\n    def __init__(\n        self,\n        conv_layers,\n        dropout,\n        log_compression,\n        skip_connections,\n        residual_scale,\n        non_affine_group_norm,\n        activation,\n    ):\n        super().__init__()\n\n        def block(n_in, n_out, k, stride):\n            return nn.Sequential(\n                nn.Conv1d(n_in, n_out, k, stride=stride, bias=False),\n                nn.Dropout(p=dropout),\n                norm_block(\n                    is_layer_norm=False, dim=n_out, affine=not non_affine_group_norm\n                ),\n                activation,\n            )\n\n        in_d = 1\n        self.conv_layers = nn.ModuleList()\n        for dim, k, stride in conv_layers:\n            self.conv_layers.append(block(in_d, dim, k, stride))\n            in_d = dim\n\n        self.log_compression = log_compression\n        self.skip_connections = skip_connections\n        self.residual_scale = math.sqrt(residual_scale)\n\n    def forward(self, x):\n        # BxT -> BxCxT\n        x = x.unsqueeze(1)\n\n        for conv in self.conv_layers:\n            residual = x\n            x = conv(x)\n            if self.skip_connections and x.size(1) == residual.size(1):\n                tsz = x.size(2)\n                r_tsz = residual.size(2)\n                residual = residual[..., :: r_tsz // tsz][..., :tsz]\n                x = (x + residual) * self.residual_scale\n\n        if self.log_compression:\n            x = x.abs()\n            x = x + 1\n            x = x.log()\n\n        return x\n\n\nclass ZeroPad1d(nn.Module):\n    def __init__(self, pad_left, pad_right):\n        super().__init__()\n        self.pad_left = pad_left\n        self.pad_right = pad_right\n\n    def forward(self, x):\n        return F.pad(x, (self.pad_left, self.pad_right))\n\n\nclass ConvAggegator(nn.Module):\n    def __init__(\n        self,\n        conv_layers,\n        embed,\n        dropout,\n        skip_connections,\n        residual_scale,\n        non_affine_group_norm,\n        conv_bias,\n        zero_pad,\n        activation,\n    ):\n        super().__init__()\n\n        def block(n_in, n_out, k, stride):\n            # padding dims only really make sense for stride = 1\n            ka = k // 2\n            kb = ka - 1 if k % 2 == 0 else ka\n\n            pad = (\n                ZeroPad1d(ka + kb, 0) if zero_pad else nn.ReplicationPad1d((ka + kb, 0))\n            )\n\n            return nn.Sequential(\n                pad,\n                nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias),\n                nn.Dropout(p=dropout),\n                norm_block(False, n_out, affine=not non_affine_group_norm),\n                activation,\n            )\n\n        in_d = embed\n        self.conv_layers = nn.ModuleList()\n        self.residual_proj = nn.ModuleList()\n        for dim, k, stride in conv_layers:\n            if in_d != dim and skip_connections:\n                self.residual_proj.append(nn.Conv1d(in_d, dim, 1, bias=False))\n            else:\n                self.residual_proj.append(None)\n\n            self.conv_layers.append(block(in_d, dim, k, stride))\n            in_d = dim\n        self.conv_layers = nn.Sequential(*self.conv_layers)\n        self.skip_connections = skip_connections\n        self.residual_scale = math.sqrt(residual_scale)\n\n    def forward(self, x):\n        for rproj, conv in zip(self.residual_proj, self.conv_layers):\n            residual = x\n            x = conv(x)\n            if self.skip_connections:\n                if rproj is not None:\n                    residual = rproj(residual)\n                x = (x + residual) * self.residual_scale\n        return x\n\n\nclass Wav2VecPredictionsModel(nn.Module):\n    def __init__(\n        self,\n        in_dim,\n        out_dim,\n        prediction_steps,\n        n_negatives,\n        cross_sample_negatives,\n        sample_distance,\n        dropout,\n        offset,\n        balanced_classes,\n        infonce,\n    ):\n        super().__init__()\n\n        self.n_negatives = n_negatives\n        self.cross_sample_negatives = cross_sample_negatives\n        self.sample_distance = sample_distance\n        self.project_to_steps = nn.ConvTranspose2d(\n            in_dim, out_dim, (1, prediction_steps)\n        )\n        self.dropout = nn.Dropout(p=dropout)\n        self.offset = offset\n        self.balanced_classes = balanced_classes\n        self.infonce = infonce\n\n    def sample_negatives(self, y):\n        bsz, fsz, tsz = y.shape\n\n        y = y.transpose(0, 1)  # BCT -> CBT\n        y = y.contiguous().view(fsz, -1)  # CBT => C(BxT)\n\n        cross_high = tsz * bsz\n        high = tsz if self.sample_distance is None else min(tsz, self.sample_distance)\n        assert high > 1\n\n        neg_idxs = torch.randint(low=0, high=high, size=(bsz, self.n_negatives * tsz))\n\n        with torch.no_grad():\n            if self.n_negatives > 0:\n                tszs = (\n                    buffered_arange(tsz)\n                    .unsqueeze(-1)\n                    .expand(-1, self.n_negatives)\n                    .flatten()\n                )\n\n                neg_idxs = torch.randint(\n                    low=0, high=high - 1, size=(bsz, self.n_negatives * tsz)\n                )\n                neg_idxs[neg_idxs >= tszs] += 1\n\n            if self.cross_sample_negatives > 0:\n                tszs = (\n                    buffered_arange(tsz)\n                    .unsqueeze(-1)\n                    .expand(-1, self.cross_sample_negatives)\n                    .flatten()\n                )\n\n                cross_neg_idxs = torch.randint(\n                    low=0,\n                    high=cross_high - 1,\n                    size=(bsz, self.cross_sample_negatives * tsz),\n                )\n                cross_neg_idxs[cross_neg_idxs >= tszs] += 1\n\n        if self.n_negatives > 0:\n            for i in range(1, bsz):\n                neg_idxs[i] += i * high\n        else:\n            neg_idxs = cross_neg_idxs\n\n        if self.cross_sample_negatives > 0 and self.n_negatives > 0:\n            neg_idxs = torch.cat([neg_idxs, cross_neg_idxs], dim=1)\n\n        negs = y[..., neg_idxs.view(-1)]\n        negs = negs.view(\n            fsz, bsz, self.n_negatives + self.cross_sample_negatives, tsz\n        ).permute(\n            2, 1, 0, 3\n        )  # to NxBxCxT\n\n        return negs\n\n    def forward(self, x, y):\n\n        x = x.unsqueeze(-1)\n        x = self.project_to_steps(x)  # BxCxTxS\n        x = self.dropout(x)\n\n        negatives = self.sample_negatives(y)\n        y = y.unsqueeze(0)\n        targets = torch.cat([y, negatives], dim=0)  # Copies x B x C x T\n\n        copies = targets.size(0)\n        bsz, dim, tsz, steps = x.shape\n        steps = min(steps, tsz - self.offset)\n\n        predictions = x.new(\n            bsz * copies * (tsz - self.offset + 1) * steps\n            - ((steps + 1) * steps // 2) * copies * bsz\n        )\n        if self.infonce:\n            labels = predictions.new_full(\n                (predictions.shape[0] // copies,), 0, dtype=torch.long\n            )\n        else:\n            labels = torch.zeros_like(predictions)\n        weights = (\n            torch.full_like(labels, 1 / self.n_negatives)\n            if self.balanced_classes and not self.infonce\n            else None\n        )\n\n        start = end = 0\n        for i in range(steps):\n            offset = i + self.offset\n            end = start + (tsz - offset) * bsz * copies\n            if self.infonce:\n                predictions[start:end] = torch.einsum(\n                    ""bct,nbct->tbn"", x[..., :-offset, i], targets[..., offset:]\n                ).flatten()\n            else:\n                pos_num = (end - start) // copies\n                predictions[start:end] = torch.einsum(\n                    ""bct,nbct->nbt"", x[..., :-offset, i], targets[..., offset:]\n                ).flatten()\n                labels[start : start + pos_num] = 1.0\n                if weights is not None:\n                    weights[start : start + pos_num] = 1.0\n            start = end\n        assert end == predictions.numel(), ""{} != {}"".format(end, predictions.numel())\n\n        if self.infonce:\n            predictions = predictions.view(-1, copies)\n        else:\n            if weights is not None:\n                labels = (labels, weights)\n\n        return predictions, labels\n\n\n@register_model_architecture(""wav2vec"", ""wav2vec"")\ndef base_wav2vec_architecture(args):\n    conv_feature_layers = ""[(512, 10, 5)]""\n    conv_feature_layers += "" + [(512, 8, 4)]""\n    conv_feature_layers += "" + [(512, 4, 2)] * 3""\n    args.conv_feature_layers = getattr(args, ""conv_feature_layers"", conv_feature_layers)\n\n    args.conv_aggregator_layers = getattr(\n        args, ""conv_aggregator_layers"", ""[(512, 3, 1)] * 9""\n    )\n\n    args.prediction_steps = getattr(args, ""prediction_steps"", 12)\n    args.num_negatives = getattr(args, ""num_negatives"", 1)\n    args.sample_distance = getattr(args, ""sample_distance"", None)\n    args.cross_sample_negatives = getattr(args, ""cross_sample_negatives"", 0)\n\n    args.dropout = getattr(args, ""dropout"", 0.0)\n    args.dropout_features = getattr(args, ""dropout_features"", 0.0)\n    args.dropout_agg = getattr(args, ""dropout_agg"", 0.0)\n    args.encoder = getattr(args, ""encoder"", ""cnn"")\n    args.aggregator = getattr(args, ""aggregator"", ""cnn"")\n\n    args.skip_connections_feat = getattr(args, ""skip_connections_feat"", False)\n    args.skip_connections_agg = getattr(args, ""skip_connections_agg"", False)\n    args.residual_scale = getattr(args, ""residual_scale"", 0.5)\n\n    args.gru_dim = getattr(args, ""gru_dim"", 512)\n\n    args.no_conv_bias = getattr(args, ""no_conv_bias"", False)\n    args.agg_zero_pad = getattr(args, ""agg_zero_pad"", False)\n\n    args.log_compression = getattr(args, ""log_compression"", False)\n\n    args.balanced_classes = getattr(args, ""balanced_classes"", False)\n    args.infonce = getattr(args, ""infonce"", False)\n    args.project_features = getattr(args, ""project_features"", ""none"")\n\n    args.non_affine_group_norm = getattr(args, ""non_affine_group_norm"", False)\n\n    args.offset = getattr(args, ""offset"", ""auto"")\n\n    args.activation = getattr(args, ""activation"", ""relu"")\n\n    args.vq_type = getattr(args, ""vq_type"", ""none"")\n    args.vq_vars = getattr(args, ""vq_vars"", 320)\n    args.vq_groups = getattr(args, ""vq_groups"", 2)\n    args.vq_dim = getattr(args, ""vq_dim"", 0)\n    args.vq_depth = getattr(args, ""vq_depth"", 1)\n    args.combine_groups = getattr(args, ""combine_groups"", False)\n    args.vq_temp = getattr(args, ""vq_temp"", ""(2.0, 0.5, 0.999995)"")\n    args.vq_gamma = getattr(args, ""vq_gamma"", 0.25)\n'"
fairseq/modules/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .adaptive_input import AdaptiveInput\nfrom .adaptive_softmax import AdaptiveSoftmax\nfrom .beamable_mm import BeamableMM\nfrom .character_token_embedder import CharacterTokenEmbedder\nfrom .conv_tbc import ConvTBC\nfrom .cross_entropy import cross_entropy\nfrom .downsampled_multihead_attention import DownsampledMultiHeadAttention\nfrom .dynamic_convolution import DynamicConv, DynamicConv1dTBC\nfrom .dynamic_crf_layer import DynamicCRF\nfrom .fp32_group_norm import Fp32GroupNorm\nfrom .gelu import gelu, gelu_accurate\nfrom .grad_multiply import GradMultiply\nfrom .gumbel_vector_quantizer import GumbelVectorQuantizer\nfrom .kmeans_vector_quantizer import KmeansVectorQuantizer\nfrom .layer_drop import LayerDropModuleList\nfrom .layer_norm import Fp32LayerNorm, LayerNorm\nfrom .learned_positional_embedding import LearnedPositionalEmbedding\nfrom .lightweight_convolution import LightweightConv, LightweightConv1dTBC\nfrom .linearized_convolution import LinearizedConvolution\nfrom .multihead_attention import MultiheadAttention\nfrom .positional_embedding import PositionalEmbedding\nfrom .scalar_bias import ScalarBias\nfrom .sinusoidal_positional_embedding import SinusoidalPositionalEmbedding\nfrom .transformer_sentence_encoder_layer import TransformerSentenceEncoderLayer\nfrom .transformer_sentence_encoder import TransformerSentenceEncoder\nfrom .unfold import unfold1d\nfrom .transformer_layer import TransformerDecoderLayer, TransformerEncoderLayer\nfrom .vggblock import VGGBlock\n\n__all__ = [\n    'AdaptiveInput',\n    'AdaptiveSoftmax',\n    'BeamableMM',\n    'CharacterTokenEmbedder',\n    'ConvTBC',\n    'cross_entropy',\n    'DownsampledMultiHeadAttention',\n    'DynamicConv1dTBC',\n    'DynamicConv',\n    'DynamicCRF',\n    'Fp32GroupNorm',\n    'Fp32LayerNorm',\n    'gelu',\n    'gelu_accurate',\n    'GradMultiply',\n    'GumbelVectorQuantizer',\n    'KmeansVectorQuantizer',\n    'LayerDropModuleList',\n    'LayerNorm',\n    'LearnedPositionalEmbedding',\n    'LightweightConv1dTBC',\n    'LightweightConv',\n    'LinearizedConvolution',\n    'MultiheadAttention',\n    'PositionalEmbedding',\n    'ScalarBias',\n    'SinusoidalPositionalEmbedding',\n    'TransformerSentenceEncoderLayer',\n    'TransformerSentenceEncoder',\n    'TransformerDecoderLayer',\n    'TransformerEncoderLayer',\n    'VGGBlock',\n    'unfold1d',\n]\n"""
fairseq/modules/adaptive_input.py,2,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport torch\nfrom torch import nn\nfrom fairseq.modules.quant_noise import quant_noise\n\nfrom typing import List\n\n\nclass AdaptiveInput(nn.Module):\n\n    def __init__(\n        self,\n        vocab_size: int,\n        padding_idx: int,\n        initial_dim: int,\n        factor: float,\n        output_dim: int,\n        cutoff: List[int],\n        q_noise: float = 0,\n        qn_block_size: int = 8,\n    ):\n        super().__init__()\n\n        if vocab_size > cutoff[-1]:\n            cutoff = cutoff + [vocab_size]\n        else:\n            assert vocab_size == cutoff[\n                -1], 'cannot specify cutoff larger than vocab size'\n\n        self.cutoff = cutoff\n        self.embedding_dim = output_dim\n        self.padding_idx = padding_idx\n\n        self.embeddings = nn.ModuleList()\n        for i in range(len(self.cutoff)):\n            prev = self.cutoff[i - 1] if i > 0 else 0\n            size = self.cutoff[i] - prev\n            dim = int(initial_dim // (factor ** i))\n            seq = nn.Sequential(\n                nn.Embedding(size, dim, self.padding_idx),\n                quant_noise(nn.Linear(dim, output_dim, bias=False), q_noise, qn_block_size),\n            )\n\n            self.embeddings.append(seq)\n            self.padding_idx = None\n        self.padding_idx = padding_idx\n\n        def init_weights(m):\n            if isinstance(m, nn.Embedding):\n                nn.init.normal_(m.weight, mean=0, std=m.weight.shape[1] ** -0.5)\n                nn.init.constant_(m.weight[padding_idx], 0)\n            elif hasattr(m, 'weight'):\n                nn.init.xavier_uniform_(m.weight)\n\n        self.apply(init_weights)\n\n        self.register_buffer('_float_tensor', torch.FloatTensor(1))\n\n    def weights_for_band(self, band: int):\n        return self.embeddings[band][0].weight, self.embeddings[band][1].weight\n\n    def forward(self, input: torch.Tensor):\n        result = self._float_tensor.new(input.shape + (self.embedding_dim,))\n        for i in range(len(self.cutoff)):\n            mask = input.lt(self.cutoff[i])\n            if i > 0:\n                mask.mul_(input.ge(self.cutoff[i - 1]))\n                chunk_input = input[mask] - self.cutoff[i - 1]\n            else:\n                chunk_input = input[mask]\n            if mask.any():\n                result[mask] = self.embeddings[i](chunk_input)\n        return result\n"""
fairseq/modules/adaptive_softmax.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport operator\nimport functools\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq.modules.quant_noise import quant_noise\nfrom torch import nn\n\n\nclass TiedLinear(nn.Module):\n    def __init__(self, weight, transpose):\n        super().__init__()\n        self.weight = weight\n        self.transpose = transpose\n\n    def forward(self, input):\n        return F.linear(input, self.weight.t() if self.transpose else self.weight)\n\n\nclass TiedHeadModule(nn.Module):\n    def __init__(self, weights, input_dim, num_classes, q_noise, qn_block_size):\n        super().__init__()\n        tied_emb, _ = weights\n        self.num_words, emb_dim = tied_emb.size()\n\n        self.word_proj = quant_noise(TiedLinear(tied_emb, transpose=False), q_noise, qn_block_size)\n        if input_dim != emb_dim:\n            self.word_proj = nn.Sequential(\n                quant_noise(nn.Linear(input_dim, emb_dim, bias=False), q_noise, qn_block_size),\n                self.word_proj,\n            )\n\n        self.class_proj = quant_noise(nn.Linear(input_dim, num_classes, bias=False), q_noise, qn_block_size)\n        self.out_dim = self.num_words + num_classes\n\n        self.register_buffer(\'_float_tensor\', torch.FloatTensor(1))\n\n    def forward(self, input):\n        inp_sz = functools.reduce(operator.mul, input.shape[:-1], 1)\n        out = self._float_tensor.new(inp_sz, self.out_dim)\n        out[:, :self.num_words] = self.word_proj(input.view(inp_sz, -1))\n        out[:, self.num_words:] = self.class_proj(input.view(inp_sz, -1))\n        return out\n\n\nclass AdaptiveSoftmax(nn.Module):\n    """"""\n    This is an implementation of the efficient softmax approximation for\n    graphical processing units (GPU), described in the paper ""Efficient softmax\n    approximation for GPUs"" (http://arxiv.org/abs/1609.04309).\n    """"""\n\n    def __init__(self, vocab_size, input_dim, cutoff, dropout, factor=4., adaptive_inputs=None, tie_proj=False, q_noise=0, qn_block_size=8):\n        super().__init__()\n\n        if vocab_size > cutoff[-1]:\n            cutoff = cutoff + [vocab_size]\n        else:\n            assert vocab_size == cutoff[\n                -1], \'cannot specify cutoff larger than vocab size\'\n\n        output_dim = cutoff[0] + len(cutoff) - 1\n\n        self.vocab_size = vocab_size\n        self.cutoff = cutoff\n        self.dropout = dropout\n        self.input_dim = input_dim\n        self.factor = factor\n        self.q_noise = q_noise\n        self.qn_block_size = qn_block_size\n\n        self.lsm = nn.LogSoftmax(dim=1)\n\n        if adaptive_inputs is not None:\n            self.head = TiedHeadModule(adaptive_inputs.weights_for_band(0), input_dim, len(cutoff) - 1, self.q_noise, self.qn_block_size)\n        else:\n            self.head = quant_noise(nn.Linear(input_dim, output_dim, bias=False), self.q_noise, self.qn_block_size)\n\n        self._make_tail(adaptive_inputs, tie_proj)\n\n        def init_weights(m):\n            if hasattr(m, \'weight\') and not isinstance(m, TiedLinear) and not isinstance(m, TiedHeadModule):\n                nn.init.xavier_uniform_(m.weight)\n\n        self.apply(init_weights)\n\n        self.register_buffer(\'version\', torch.LongTensor([1]))\n\n    def _make_tail(self, adaptive_inputs=None, tie_proj=False):\n        self.tail = nn.ModuleList()\n        for i in range(len(self.cutoff) - 1):\n            dim = int(self.input_dim // self.factor ** (i + 1))\n\n            tied_emb, tied_proj = adaptive_inputs.weights_for_band(i + 1) \\\n                if adaptive_inputs is not None else (None, None)\n\n            if tied_proj is not None:\n                if tie_proj:\n                    proj = quant_noise(TiedLinear(tied_proj, transpose=True), self.q_noise, self.qn_block_size)\n                else:\n                    proj = quant_noise(nn.Linear(tied_proj.size(0), tied_proj.size(1), bias=False), self.q_noise, self.qn_block_size)\n            else:\n                proj = quant_noise(nn.Linear(self.input_dim, dim, bias=False), self.q_noise, self.qn_block_size)\n\n            if tied_emb is None:\n                out_proj = nn.Linear(dim, self.cutoff[i + 1] - self.cutoff[i], bias=False)\n            else:\n                out_proj = TiedLinear(tied_emb, transpose=False)\n\n            m = nn.Sequential(\n                proj,\n                nn.Dropout(self.dropout),\n                quant_noise(out_proj, self.q_noise, self.qn_block_size),\n            )\n\n            self.tail.append(m)\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        version_name = name + \'.version\'\n        if version_name not in state_dict:\n            raise Exception(\'This version of the model is no longer supported\')\n\n    def adapt_target(self, target):\n        """"""\n        In order to be efficient, the AdaptiveSoftMax does not compute the\n        scores for all the word of the vocabulary for all the examples. It is\n        thus necessary to call the method adapt_target of the AdaptiveSoftMax\n        layer inside each forward pass.\n        """"""\n\n        target = target.view(-1)\n        new_target = [target.clone()]\n        target_idxs = []\n\n        for i in range(len(self.cutoff) - 1):\n            mask = target.ge(self.cutoff[i]).mul(target.lt(self.cutoff[i + 1]))\n            new_target[0][mask] = self.cutoff[0] + i\n\n            if mask.any():\n                target_idxs.append(mask.nonzero().squeeze(1))\n                new_target.append(target[mask].add(-self.cutoff[i]))\n            else:\n                target_idxs.append(None)\n                new_target.append(None)\n\n        return new_target, target_idxs\n\n    def forward(self, input, target):\n        """"""\n        Args:\n            input: (b x t x d)\n            target: (b x t)\n        Returns:\n            2 lists: output for each cutoff section and new targets by cut off\n        """"""\n\n        input = input.contiguous().view(-1, input.size(-1))\n        input = F.dropout(input, p=self.dropout, training=self.training)\n\n        new_target, target_idxs = self.adapt_target(target)\n        output = [self.head(input)]\n\n        for i in range(len(target_idxs)):\n            if target_idxs[i] is not None:\n                output.append(self.tail[i](input.index_select(0, target_idxs[i])))\n            else:\n                output.append(None)\n\n        return output, new_target\n\n    def get_log_prob(self, input, target):\n        """"""\n        Computes the log probabilities for all the words of the vocabulary,\n        given a 2D tensor of hidden vectors.\n        """"""\n\n        bsz, length, dim = input.size()\n        input = input.contiguous().view(-1, dim)\n\n        if target is not None:\n            _, target_idxs = self.adapt_target(target)\n        else:\n            target_idxs = None\n\n        head_y = self.head(input)\n        log_probs = head_y.new_zeros(input.size(0), self.vocab_size)\n\n        head_sz = self.cutoff[0] + len(self.tail)\n        log_probs[:, :head_sz] = self.lsm(head_y)\n        tail_priors = log_probs[:, self.cutoff[0]: head_sz].clone()\n\n        for i in range(len(self.tail)):\n            start = self.cutoff[i]\n            end = self.cutoff[i + 1]\n\n            if target_idxs is None:\n                tail_out = log_probs[:, start:end]\n                tail_out.copy_(self.tail[i](input))\n                log_probs[:, start:end] = self.lsm(tail_out).add_(tail_priors[:, i, None])\n            elif target_idxs[i] is not None:\n                idxs = target_idxs[i]\n                tail_out = log_probs[idxs, start:end]\n                tail_out.copy_(self.tail[i](input[idxs]))\n                log_probs[idxs, start:end] = self.lsm(tail_out).add_(tail_priors[idxs, i, None])\n\n        log_probs = log_probs.view(bsz, length, -1)\n        return log_probs\n'"
fairseq/modules/beamable_mm.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\n\nclass BeamableMM(nn.Module):\n    """"""This module provides an optimized MM for beam decoding with attention.\n\n    It leverage the fact that the source-side of the input is replicated beam\n    times and the target-side of the input is of width one. This layer speeds up\n    inference by replacing the inputs {(bsz x 1 x nhu), (bsz x sz2 x nhu)}\n    with smaller inputs {(bsz/beam x beam x nhu), (bsz/beam x sz2 x nhu)}.\n    """"""\n    def __init__(self, beam_size=None):\n        super(BeamableMM, self).__init__()\n        self.beam_size = beam_size\n\n    def forward(self, input1, input2):\n        if (\n            not self.training and           # test mode\n            self.beam_size is not None and  # beam size is set\n            input1.dim() == 3 and           # only support batched input\n            input1.size(1) == 1             # single time step update\n        ):\n            bsz, beam = input1.size(0), self.beam_size\n\n            # bsz x 1 x nhu --> bsz/beam x beam x nhu\n            input1 = input1[:, 0, :].unfold(0, beam, beam).transpose(2, 1)\n\n            # bsz x sz2 x nhu --> bsz/beam x sz2 x nhu\n            input2 = input2.unfold(0, beam, beam)[:, :, :, 0]\n\n            # use non batched operation if bsz = beam\n            if input1.size(0) == 1:\n                output = torch.mm(input1[0, :, :], input2[0, :, :])\n            else:\n                output = input1.bmm(input2)\n            return output.view(bsz, 1, -1)\n        else:\n            return input1.bmm(input2)\n\n    def set_beam_size(self, beam_size):\n        self.beam_size = beam_size\n'"
fairseq/modules/character_token_embedder.py,16,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom typing import List, Tuple\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom fairseq.data import Dictionary\n\nCHAR_PAD_IDX = 0\nCHAR_EOS_IDX = 257\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass CharacterTokenEmbedder(torch.nn.Module):\n    def __init__(\n            self,\n            vocab: Dictionary,\n            filters: List[Tuple[int, int]],\n            char_embed_dim: int,\n            word_embed_dim: int,\n            highway_layers: int,\n            max_char_len: int = 50,\n            char_inputs: bool = False\n    ):\n        super(CharacterTokenEmbedder, self).__init__()\n\n        self.onnx_trace = False\n        self.embedding_dim = word_embed_dim\n        self.max_char_len = max_char_len\n        self.char_embeddings = nn.Embedding(257, char_embed_dim, padding_idx=0)\n        self.symbol_embeddings = nn.Parameter(torch.FloatTensor(2, word_embed_dim))\n        self.eos_idx, self.unk_idx = 0, 1\n        self.char_inputs = char_inputs\n\n        self.convolutions = nn.ModuleList()\n        for width, out_c in filters:\n            self.convolutions.append(\n                nn.Conv1d(char_embed_dim, out_c, kernel_size=width)\n            )\n\n        last_dim = sum(f[1] for f in filters)\n\n        self.highway = Highway(last_dim, highway_layers) if highway_layers > 0 else None\n\n        self.projection = nn.Linear(last_dim, word_embed_dim)\n\n        assert vocab is not None or char_inputs, ""vocab must be set if not using char inputs""\n        self.vocab = None\n        if vocab is not None:\n            self.set_vocab(vocab, max_char_len)\n\n        self.reset_parameters()\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def set_vocab(self, vocab, max_char_len):\n        word_to_char = torch.LongTensor(len(vocab), max_char_len)\n\n        truncated = 0\n        for i in range(len(vocab)):\n            if i < vocab.nspecial:\n                char_idxs = [0] * max_char_len\n            else:\n                chars = vocab[i].encode()\n                # +1 for padding\n                char_idxs = [c + 1 for c in chars] + [0] * (max_char_len - len(chars))\n            if len(char_idxs) > max_char_len:\n                truncated += 1\n                char_idxs = char_idxs[:max_char_len]\n            word_to_char[i] = torch.LongTensor(char_idxs)\n\n        if truncated > 0:\n            logger.info(\'truncated {} words longer than {} characters\'.format(truncated, max_char_len))\n\n        self.vocab = vocab\n        self.word_to_char = word_to_char\n\n    @property\n    def padding_idx(self):\n        return Dictionary().pad() if self.vocab is None else self.vocab.pad()\n\n    def reset_parameters(self):\n        nn.init.xavier_normal_(self.char_embeddings.weight)\n        nn.init.xavier_normal_(self.symbol_embeddings)\n        nn.init.xavier_uniform_(self.projection.weight)\n\n        nn.init.constant_(self.char_embeddings.weight[self.char_embeddings.padding_idx], 0.)\n        nn.init.constant_(self.projection.bias, 0.)\n\n    def forward(\n            self,\n            input: torch.Tensor,\n    ):\n        if self.char_inputs:\n            chars = input.view(-1, self.max_char_len)\n            pads = chars[:, 0].eq(CHAR_PAD_IDX)\n            eos = chars[:, 0].eq(CHAR_EOS_IDX)\n            if eos.any():\n                if self.onnx_trace:\n                    chars = torch.where(eos.unsqueeze(1), chars.new_zeros(1), chars)\n                else:\n                    chars[eos] = 0\n\n            unk = None\n        else:\n            flat_words = input.view(-1)\n            chars = self.word_to_char[flat_words.type_as(self.word_to_char)].type_as(input)\n            pads = flat_words.eq(self.vocab.pad())\n            eos = flat_words.eq(self.vocab.eos())\n            unk = flat_words.eq(self.vocab.unk())\n\n        word_embs = self._convolve(chars)\n        if self.onnx_trace:\n            if pads.any():\n                word_embs = torch.where(pads.unsqueeze(1), word_embs.new_zeros(1), word_embs)\n            if eos.any():\n                word_embs = torch.where(eos.unsqueeze(1), self.symbol_embeddings[self.eos_idx], word_embs)\n            if unk is not None and unk.any():\n                word_embs = torch.where(unk.unsqueeze(1), self.symbol_embeddings[self.unk_idx], word_embs)\n        else:\n            if pads.any():\n                word_embs[pads] = 0\n            if eos.any():\n                word_embs[eos] = self.symbol_embeddings[self.eos_idx]\n            if unk is not None and unk.any():\n                word_embs[unk] = self.symbol_embeddings[self.unk_idx]\n\n        return word_embs.view(input.size()[:2] + (-1,))\n\n    def _convolve(\n            self,\n            char_idxs: torch.Tensor,\n    ):\n        char_embs = self.char_embeddings(char_idxs)\n        char_embs = char_embs.transpose(1, 2)  # BTC -> BCT\n\n        conv_result = []\n\n        for conv in self.convolutions:\n            x = conv(char_embs)\n            x, _ = torch.max(x, -1)\n            x = F.relu(x)\n            conv_result.append(x)\n\n        x = torch.cat(conv_result, dim=-1)\n\n        if self.highway is not None:\n            x = self.highway(x)\n        x = self.projection(x)\n\n        return x\n\n\nclass Highway(torch.nn.Module):\n    """"""\n    A `Highway layer <https://arxiv.org/abs/1505.00387>`_.\n    Adopted from the AllenNLP implementation.\n    """"""\n\n    def __init__(\n            self,\n            input_dim: int,\n            num_layers: int = 1\n    ):\n        super(Highway, self).__init__()\n        self.input_dim = input_dim\n        self.layers = nn.ModuleList([nn.Linear(input_dim, input_dim * 2)\n                                     for _ in range(num_layers)])\n        self.activation = nn.ReLU()\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for layer in self.layers:\n            # As per comment in AllenNLP:\n            # We should bias the highway layer to just carry its input forward.  We do that by\n            # setting the bias on `B(x)` to be positive, because that means `g` will be biased to\n            # be high, so we will carry the input forward.  The bias on `B(x)` is the second half\n            # of the bias vector in each Linear layer.\n            nn.init.constant_(layer.bias[self.input_dim:], 1)\n\n            nn.init.constant_(layer.bias[:self.input_dim], 0)\n            nn.init.xavier_normal_(layer.weight)\n\n    def forward(\n            self,\n            x: torch.Tensor\n    ):\n        for layer in self.layers:\n            projection = layer(x)\n            proj_x, gate = projection.chunk(2, dim=-1)\n            proj_x = self.activation(proj_x)\n            gate = torch.sigmoid(gate)\n            x = gate * x + (gate.new_tensor([1]) - gate) * proj_x\n        return x\n'"
fairseq/modules/conv_tbc.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom torch.nn.modules.utils import _single\n\n\nclass ConvTBC(torch.nn.Module):\n    """"""1D convolution over an input of shape (time x batch x channel)\n\n    The implementation uses gemm to perform the convolution. This implementation\n    is faster than cuDNN for small kernel sizes.\n    """"""\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n        super(ConvTBC, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _single(kernel_size)\n        self.padding = _single(padding)\n\n        self.weight = torch.nn.Parameter(torch.Tensor(\n            self.kernel_size[0], in_channels, out_channels))\n        self.bias = torch.nn.Parameter(torch.Tensor(out_channels))\n\n    def forward(self, input):\n        return torch.conv_tbc(input.contiguous(), self.weight, self.bias, self.padding[0])\n\n    def __repr__(self):\n        s = (\'{name}({in_channels}, {out_channels}, kernel_size={kernel_size}\'\n             \', padding={padding}\')\n        if self.bias is None:\n            s += \', bias=False\'\n        s += \')\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n'"
fairseq/modules/cross_entropy.py,4,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport torch\nimport torch.nn.functional as F\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _cross_entropy_pytorch(logits, target, ignore_index=None, reduction='mean'):\n    lprobs = F.log_softmax(logits, dim=-1, dtype=torch.float32)\n    return F.nll_loss(\n        lprobs, target, ignore_index=ignore_index, reduction=reduction,\n    )\n\n\ntry:\n    import xentropy_cuda\n    from apex.contrib import xentropy\n\n    logger.info('using fused cross entropy')\n\n    def cross_entropy(logits, target, ignore_index=-100, reduction='mean'):\n        if logits.device == torch.device('cpu'):\n            return _cross_entropy_pytorch(logits, target, ignore_index, reduction)\n        else:\n            half_to_float = (logits.dtype == torch.half)\n            losses = xentropy.SoftmaxCrossEntropyLoss.apply(\n                logits, target, 0.0, ignore_index, half_to_float,\n            )\n            if reduction == 'sum':\n                return losses.sum()\n            elif reduction == 'mean':\n                if ignore_index >= 0:\n                    return losses.sum() / target.ne(ignore_index).sum()\n                else:\n                    return losses.mean()\n            elif reduction == 'none':\n                return losses\n            else:\n                raise NotImplementedError\n\nexcept ImportError:\n\n    def cross_entropy(logits, target, ignore_index=-100, reduction='mean'):\n        return _cross_entropy_pytorch(logits, target, ignore_index, reduction)\n"""
fairseq/modules/downsampled_multihead_attention.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.modules.scalar_bias import scalar_bias\n\n\nclass SingleHeadAttention(nn.Module):\n    """"""\n    Single-head attention that supports Gating and Downsampling\n    """"""\n    def __init__(\n        self, out_channels, embed_dim, head_dim, head_index, dropout=0.,\n        bias=True, project_input=True, gated=False, downsample=False,\n        num_heads=1,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n        self.head_index = head_index\n        self.head_dim = head_dim\n        self.project_input = project_input\n        self.gated = gated\n        self.downsample = downsample\n        self.num_heads = num_heads\n        self.projection = None\n\n        k_layers = []\n        v_layers = []\n        if self.downsample:\n            k_layers.append(Downsample(self.head_index))\n            v_layers.append(Downsample(self.head_index))\n            out_proj_size = self.head_dim\n        else:\n            out_proj_size = self.head_dim * self.num_heads\n        if self.gated:\n            k_layers.append(GatedLinear(self.embed_dim, out_proj_size, bias=bias))\n            self.in_proj_q = GatedLinear(self.embed_dim, out_proj_size, bias=bias)\n            v_layers.append(GatedLinear(self.embed_dim, out_proj_size, bias=bias))\n        else:\n            k_layers.append(Linear(self.embed_dim, out_proj_size, bias=bias))\n            self.in_proj_q = Linear(self.embed_dim, out_proj_size, bias=bias)\n            v_layers.append(Linear(self.embed_dim, out_proj_size, bias=bias))\n\n        self.in_proj_k = nn.Sequential(*k_layers)\n        self.in_proj_v = nn.Sequential(*v_layers)\n\n        if self.downsample:\n            self.out_proj = Linear(out_proj_size, self.head_dim, bias=bias)\n        else:\n            self.out_proj = Linear(out_proj_size, out_channels, bias=bias)\n\n        self.scaling = self.head_dim**-0.5\n\n    def forward(\n        self, query, key, value, mask_future_timesteps=False,\n        key_padding_mask=None, use_scalar_bias=False,\n    ):\n        """"""Input shape: Time x Batch x Channel\n        Self-attention can be implemented by passing in the same arguments for\n        query, key and value. Future timesteps can be masked with the\n        `mask_future_timesteps` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        """"""\n        src_len, bsz, out_channels = key.size()\n        tgt_len = query.size(0)\n        assert list(query.size()) == [tgt_len, bsz, out_channels]\n        assert key.size() == value.size()\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            assert key_padding_mask.size(1) == src_len\n\n        if self.downsample:\n            size = bsz\n        else:\n            size = bsz * self.num_heads\n\n        k = key\n        v = value\n        q = query\n        if self.project_input:\n            q = self.in_proj_q(q)\n            k = self.in_proj_k(k)\n            v = self.in_proj_v(v)\n            src_len = k.size()[0]\n        q *= self.scaling\n\n        if not self.downsample:\n            q = q.view(tgt_len, size, self.head_dim)\n            k = k.view(src_len, size, self.head_dim)\n            v = v.view(src_len, size, self.head_dim)\n\n        q = q.transpose(0, 1)\n        k = k.transpose(0, 1)\n        v = v.transpose(0, 1)\n\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        if mask_future_timesteps:\n            assert query.size() == key.size(), \\\n                \'mask_future_timesteps only applies to self-attention\'\n            attn_weights *= torch.tril(\n                attn_weights.data.new([1]).expand(tgt_len, tgt_len).clone(),\n                diagonal=-1,\n            )[:, ::self.head_index + 1 if self.downsample else 1].unsqueeze(0)\n            attn_weights += torch.triu(\n                attn_weights.data.new([-math.inf]).expand(tgt_len, tgt_len).clone(),\n                diagonal=0\n            )[:, ::self.head_index + 1 if self.downsample else 1].unsqueeze(0)\n        tgt_size = tgt_len\n        if use_scalar_bias:\n            attn_weights = scalar_bias(attn_weights, 2)\n            v = scalar_bias(v, 1)\n            tgt_size += 1\n\n        if key_padding_mask is not None:\n            # don\'t attend to padding symbols\n            if key_padding_mask.max() > 0:\n                if self.downsample:\n                    attn_weights = attn_weights.view(bsz, 1, tgt_len, src_len)\n                else:\n                    attn_weights = attn_weights.view(size, self.num_heads, tgt_len, src_len)\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2),\n                    -math.inf,\n                )\n                attn_weights = attn_weights.view(size, tgt_len, src_len)\n        attn_weights = F.softmax(attn_weights, dim=-1)\n        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n\n        attn = torch.bmm(attn_weights, v)\n        if self.downsample:\n            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, self.head_dim)\n        else:\n            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)\n\n        attn = self.out_proj(attn)\n\n        return attn, attn_weights\n\n\nclass DownsampledMultiHeadAttention(nn.ModuleList):\n    """"""\n    Multi-headed attention with Gating and Downsampling\n    """"""\n    def __init__(\n        self, out_channels, embed_dim, num_heads, dropout=0., bias=True,\n        project_input=True, gated=False, downsample=False,\n    ):\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        self.downsample = downsample\n        self.gated = gated\n        self.project_input = project_input\n        assert self.head_dim * num_heads == embed_dim\n\n        if self.downsample:\n            attention_heads = []\n            for index in range(self.num_heads):\n                attention_heads.append(\n                    SingleHeadAttention(\n                        out_channels, self.embed_dim, self.head_dim, index,\n                        self.dropout, bias, self.project_input, self.gated,\n                        self.downsample, self.num_heads,\n                    )\n                )\n            super().__init__(modules=attention_heads)\n            self.out_proj = Linear(embed_dim, out_channels, bias=bias)\n        else:\n            # either we have a list of attention heads, or just one attention head\n            # if not being downsampled, we can do the heads with one linear layer instead of separate ones\n            super().__init__()\n            self.attention_module = SingleHeadAttention(\n                out_channels, self.embed_dim, self.head_dim, 1, self.dropout,\n                bias, self.project_input, self.gated, self.downsample, self.num_heads,\n            )\n\n    def forward(\n        self, query, key, value, mask_future_timesteps=False,\n        key_padding_mask=None, use_scalar_bias=False,\n    ):\n        src_len, bsz, embed_dim = key.size()\n        tgt_len = query.size(0)\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n        assert key.size() == value.size()\n\n        tgt_size = tgt_len\n        if use_scalar_bias:\n            tgt_size += 1\n\n        attn = []\n        attn_weights = []\n        if self.downsample:\n            for attention_head_number in range(self.num_heads):\n                # call the forward of each attention head\n                _attn, _attn_weight = self[attention_head_number](\n                    query, key, value, mask_future_timesteps, key_padding_mask, use_scalar_bias,\n                )\n                attn.append(_attn)\n                attn_weights.append(_attn_weight)\n            full_attn = torch.cat(attn, dim=2)\n            full_attn = self.out_proj(full_attn)\n            return full_attn, attn_weights[0].clone()\n        else:\n            _attn, _attn_weight = self.attention_module(\n                query, key, value, mask_future_timesteps, key_padding_mask, use_scalar_bias,\n            )\n            attn.append(_attn)\n            attn_weights.append(_attn_weight)\n            full_attn = torch.cat(attn, dim=2)\n            full_attn_weights = torch.cat(attn_weights)\n            full_attn_weights = full_attn_weights.view(bsz, self.num_heads, tgt_size, src_len)\n            full_attn_weights = full_attn_weights.sum(dim=1) / self.num_heads\n            return full_attn, full_attn_weights\n\n\nclass Downsample(nn.Module):\n    """"""\n    Selects every nth element, where n is the index\n    """"""\n    def __init__(self, index):\n        super().__init__()\n        self.index = index\n\n    def forward(self, x):\n        return x[::self.index+1]\n\n\ndef Linear(in_features, out_features, dropout=0., bias=True):\n    """"""Weight-normalized Linear layer (input: B x T x C)""""""\n    m = nn.Linear(in_features, out_features, bias=bias)\n    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n    m.bias.data.zero_()\n    return nn.utils.weight_norm(m)\n\n\ndef GatedLinear(in_features, out_features, dropout=0., bias=True):\n    """"""Weight-normalized Linear layer (input: B x T x C) with interspersed GLU units""""""\n    return nn.Sequential(\n        Linear(in_features, out_features*4, dropout, bias),\n        nn.GLU(),\n        Linear(out_features*2, out_features*2, dropout, bias),\n        nn.GLU(),\n        Linear(out_features, out_features, dropout, bias)\n    )\n'"
fairseq/modules/dynamic_convolution.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom .unfold import unfold1d\nfrom fairseq.incremental_decoding_utils import with_incremental_state\n\n\ndef DynamicConv(input_size, kernel_size=1, padding_l=None, num_heads=1,\n                weight_dropout=0., weight_softmax=False,\n                renorm_padding=False, bias=False, conv_bias=False,\n                query_size=None, in_proj=False):\n    if torch.cuda.is_available():\n        try:\n            from fairseq.modules.dynamicconv_layer import DynamicconvLayer\n            return DynamicconvLayer(input_size, kernel_size=kernel_size,\n                                    padding_l=padding_l, num_heads=num_heads,\n                                    weight_dropout=weight_dropout,\n                                    weight_softmax=weight_softmax, bias=bias)\n        except ImportError as e:\n            print(e)\n    return DynamicConv1dTBC(input_size, kernel_size=kernel_size,\n                            padding_l=padding_l, num_heads=num_heads,\n                            weight_dropout=weight_dropout,\n                            weight_softmax=weight_softmax, bias=bias)\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.)\n    return m\n\n\n@with_incremental_state\nclass DynamicConv1dTBC(nn.Module):\n    \'\'\'Dynamic lightweight convolution taking T x B x C inputs\n    Args:\n        input_size: # of channels of the input\n        kernel_size: convolution channels\n        padding_l: padding to the left when using ""same"" padding\n        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)\n        weight_dropout: the drop rate of the DropConnect to drop the weight\n        weight_softmax: normalize the weight with softmax before the convolution\n        renorm_padding: re-normalize the filters to ignore the padded part (only the non-padding parts sum up to 1)\n        bias: use bias\n        conv_bias: bias of the convolution\n        query_size: specified when feeding a different input as the query\n        in_proj: project the input and generate the filter together\n\n    Shape:\n        Input: TxBxC, i.e. (timesteps, batch_size, input_size)\n        Output: TxBxC, i.e. (timesteps, batch_size, input_size)\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            `(num_heads, 1, kernel_size)`\n        bias:   the learnable bias of the module of shape `(input_size)`\n    \'\'\'\n    def __init__(self, input_size, kernel_size=1, padding_l=None, num_heads=1,\n                 weight_dropout=0., weight_softmax=False,\n                 renorm_padding=False, bias=False, conv_bias=False,\n                 query_size=None, in_proj=False):\n        super().__init__()\n        self.input_size = input_size\n        self.query_size = input_size if query_size is None else query_size\n        self.kernel_size = kernel_size\n        self.padding_l = padding_l\n        self.num_heads = num_heads\n        self.weight_dropout = weight_dropout\n        self.weight_softmax = weight_softmax\n        self.renorm_padding = renorm_padding\n\n        if in_proj:\n            self.weight_linear = Linear(self.input_size, self.input_size + num_heads * kernel_size * 1)\n        else:\n            self.weight_linear = Linear(self.query_size, num_heads * kernel_size * 1, bias=bias)\n        if conv_bias:\n            self.conv_bias = nn.Parameter(torch.Tensor(input_size))\n        else:\n            self.conv_bias = None\n        self.reset_parameters()\n\n    @property\n    def in_proj(self):\n        return self.weight_linear.out_features == self.input_size + self.num_heads * self.kernel_size\n\n    def reset_parameters(self):\n        self.weight_linear.reset_parameters()\n        if self.conv_bias is not None:\n            nn.init.constant_(self.conv_bias, 0.)\n\n    def forward(self, x, incremental_state=None, query=None, unfold=None):\n        \'\'\'Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C\n        args:\n            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)\n            incremental_state: A dict to keep the state\n            unfold: unfold the input or not. If not, we use the matrix trick instead\n            query: use the specified query to predict the conv filters\n        \'\'\'\n        unfold = x.size(0) > 512 if unfold is None else unfold  # use unfold mode as default for long sequence to save memory\n        unfold = unfold or (incremental_state is not None)\n        assert query is None or not self.in_proj\n\n        if query is None:\n            query = x\n        if unfold:\n            output = self._forward_unfolded(x, incremental_state, query)\n        else:\n            output = self._forward_expanded(x, incremental_state, query)\n\n        if self.conv_bias is not None:\n            output = output + self.conv_bias.view(1, 1, -1)\n        return output\n\n    def _forward_unfolded(self, x, incremental_state, query):\n        \'\'\'The conventional implementation of convolutions.\n        Unfolding the input by having a window shifting to the right.\'\'\'\n        T, B, C = x.size()\n        K, H = self.kernel_size, self.num_heads\n        R = C // H\n        assert R * H == C == self.input_size\n\n        if self.in_proj:\n            proj = self.weight_linear(x)\n            x = proj.narrow(2, 0, self.input_size).contiguous()\n            weight = proj.narrow(2, self.input_size, H*K).contiguous().view(T*B*H, -1)\n        else:\n            weight = self.weight_linear(query).view(T*B*H, -1)\n\n        # renorm_padding is only implemented in _forward_expanded\n        assert not self.renorm_padding or incremental_state is not None\n\n        if incremental_state is not None:\n            input_buffer = self._get_input_buffer(incremental_state)\n            if input_buffer is None:\n                input_buffer = x.new()\n            x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)\n            if self.kernel_size > 1:\n                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size+1:])\n            x_unfold = x_unfold.view(T*B*H, R, -1)\n        else:\n            padding_l = self.padding_l\n            if K > T and padding_l == K-1:\n                weight = weight.narrow(1, K-T, T)\n                K, padding_l = T, T-1\n            # unfold the input: T x B x C --> T\' x B x C x K\n            x_unfold = unfold1d(x, K, padding_l, 0)\n            x_unfold = x_unfold.view(T*B*H, R, K)\n\n        if self.weight_softmax and not self.renorm_padding:\n            weight = F.softmax(weight, dim=1)\n        weight = weight.narrow(1, 0, K)\n\n        if incremental_state is not None:\n            weight = weight[:, -x_unfold.size(2):]\n            K = weight.size(1)\n\n        if self.weight_softmax and self.renorm_padding:\n            weight = F.softmax(weight, dim=1)\n\n        weight = F.dropout(weight, self.weight_dropout, training=self.training, inplace=False)\n\n        output = torch.bmm(x_unfold, weight.unsqueeze(2))  # T*B*H x R x 1\n        output = output.view(T, B, C)\n        return output\n\n    def _forward_expanded(self, x, incremental_stat, query):\n        \'\'\'Turn the convolution filters into band matrices and do matrix multiplication.\n        This is faster when the sequence is short, but less memory efficient.\n        This is not used in the decoder during inference.\n        \'\'\'\n        T, B, C = x.size()\n        K, H = self.kernel_size, self.num_heads\n        R = C // H\n        assert R * H == C == self.input_size\n        if self.in_proj:\n            proj = self.weight_linear(x)\n            x = proj.narrow(2, 0, self.input_size).contiguous()\n            weight = proj.narrow(2, self.input_size, H*K).contiguous().view(T*B*H, -1)\n        else:\n            weight = self.weight_linear(query).view(T*B*H, -1)\n\n        if not self.renorm_padding:\n            if self.weight_softmax:\n                weight = F.softmax(weight, dim=1)\n            weight = F.dropout(weight, self.weight_dropout, training=self.training, inplace=False)\n        weight = weight.narrow(1, 0, K).contiguous()\n        weight = weight.view(T, B*H, K).transpose(0, 1)\n\n        x = x.view(T, B*H, R).transpose(0, 1)\n        if self.weight_softmax and self.renorm_padding:\n            # turn the convolution filters into band matrices\n            weight_expanded = weight.new(B*H, T, T+K-1).fill_(float(\'-inf\'))\n            weight_expanded.as_strided((B*H, T, K), (T*(T+K-1), T+K, 1)).copy_(weight)\n            weight_expanded = weight_expanded.narrow(2, self.padding_l, T)\n            # normalize the weight over valid positions like self-attention\n            weight_expanded = F.softmax(weight_expanded, dim=2)\n            weight_expanded = F.dropout(weight_expanded, self.weight_dropout, training=self.training, inplace=False)\n        else:\n            P = self.padding_l\n            # For efficieny, we cut the kernel size and reduce the padding when the kernel is larger than the length\n            if K > T and P == K-1:\n                weight = weight.narrow(2, K-T, T)\n                K, P = T, T-1\n            # turn the convolution filters into band matrices\n            weight_expanded = weight.new_zeros(B*H, T, T+K-1, requires_grad=False)\n            weight_expanded.as_strided((B*H, T, K), (T*(T+K-1), T+K, 1)).copy_(weight)\n            weight_expanded = weight_expanded.narrow(2, P, T)  # B*H x T x T\n        output = torch.bmm(weight_expanded, x)\n        output = output.transpose(0, 1).contiguous().view(T, B, C)\n        return output\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            input_buffer = input_buffer.index_select(1, new_order)\n            self._set_input_buffer(incremental_state, input_buffer)\n\n    def _get_input_buffer(self, incremental_state):\n        return utils.get_incremental_state(self, incremental_state, \'input_buffer\')\n\n    def _set_input_buffer(self, incremental_state, new_buffer):\n        return utils.set_incremental_state(self, incremental_state, \'input_buffer\', new_buffer)\n\n    def extra_repr(self):\n        s = \'{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, conv_bias={}, renorm_padding={}, in_proj={}\'.format(\n            self.input_size, self.kernel_size, self.padding_l,\n            self.num_heads, self.weight_softmax, self.conv_bias is not None, self.renorm_padding,\n            self.in_proj,\n        )\n\n        if self.query_size != self.input_size:\n            s += \', query_size={}\'.format(self.query_size)\n        if self.weight_dropout > 0.:\n            s += \', weight_dropout={}\'.format(self.weight_dropout)\n        return s\n'"
fairseq/modules/dynamic_crf_layer.py,17,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nThis file is to re-implemented the low-rank and beam approximation of CRF layer\nProposed by:\n\nSun, Zhiqing, et al.\nFast Structured Decoding for Sequence Models\nhttps://arxiv.org/abs/1910.11555\n\nThe CRF implementation is mainly borrowed from\nhttps://github.com/kmkurn/pytorch-crf/blob/master/torchcrf/__init__.py\n\n""""""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\ndef logsumexp(x, dim=1):\n    return torch.logsumexp(x.float(), dim=dim).type_as(x)\n\n\nclass DynamicCRF(nn.Module):\n    """"""Dynamic CRF layer is used to approximate the traditional\n       Conditional Random Fields (CRF)\n       $P(y | x) = 1/Z(x) exp(sum_i s(y_i, x) + sum_i t(y_{i-1}, y_i, x))$\n\n       where in this function, we assume the emition scores (s) are given,\n       and the transition score is a |V| x |V| matrix $M$\n\n       in the following two aspects:\n        (1) it used a low-rank approximation for the transition matrix:\n            $M = E_1 E_2^T$\n        (2) it used a beam to estimate the normalizing factor Z(x)\n    """"""\n\n    def __init__(self, num_embedding, low_rank=32, beam_size=64):\n        super().__init__()\n\n        self.E1 = nn.Embedding(num_embedding, low_rank)\n        self.E2 = nn.Embedding(num_embedding, low_rank)\n\n        self.vocb = num_embedding\n        self.rank = low_rank\n        self.beam = beam_size\n\n    def extra_repr(self):\n        return ""vocab_size={}, low_rank={}, beam_size={}"".format(\n            self.vocb, self.rank, self.beam)\n\n    def forward(self, emissions, targets, masks, beam=None):\n        """"""\n        Compute the conditional log-likelihood of a sequence of target tokens given emission scores\n\n        Args:\n            emissions (`~torch.Tensor`): Emission score are usually the unnormalized decoder output\n                ``(batch_size, seq_len, vocab_size)``. We assume batch-first\n            targets (`~torch.LongTensor`): Sequence of target token indices\n                ``(batch_size, seq_len)\n            masks (`~torch.ByteTensor`): Mask tensor with the same size as targets\n\n        Returns:\n            `~torch.Tensor`: approximated log-likelihood\n        """"""\n        numerator = self._compute_score(emissions, targets, masks)\n        denominator = self._compute_normalizer(emissions, targets, masks, beam)\n        return numerator - denominator\n\n    def forward_decoder(self, emissions, masks=None, beam=None):\n        """"""\n        Find the most likely output sequence using Viterbi algorithm.\n\n        Args:\n            emissions (`~torch.Tensor`): Emission score are usually the unnormalized decoder output\n                ``(batch_size, seq_len, vocab_size)``. We assume batch-first\n            masks (`~torch.ByteTensor`): Mask tensor with the same size as targets\n\n        Returns:\n            `~torch.LongTensor`: decoded sequence from the CRF model\n        """"""\n        return self._viterbi_decode(emissions, masks, beam)\n\n    def _compute_score(self, emissions, targets, masks=None):\n        batch_size, seq_len = targets.size()\n        emission_scores = emissions.gather(2, targets[:, :, None])[:, :, 0]  # B x T\n        transition_scores = (self.E1(targets[:, :-1]) * self.E2(targets[:, 1:])).sum(2)\n\n        scores = emission_scores\n        scores[:, 1:] += transition_scores\n\n        if masks is not None:\n            scores = scores * masks.type_as(scores)\n        return scores.sum(-1)\n\n    def _compute_normalizer(self, emissions, targets=None, masks=None, beam=None):\n        # HACK: we include ""target"" which is a hueristic for training\n        # HACK: we use a beam of tokens to approximate the normalizing factor (which is bad?)\n\n        beam = beam if beam is not None else self.beam\n        batch_size, seq_len = emissions.size()[:2]\n        if targets is not None:\n            _emissions = emissions.scatter(2, targets[:, :, None], np.float(\'inf\'))\n            beam_targets = _emissions.topk(beam, 2)[1]\n            beam_emission_scores = emissions.gather(2, beam_targets)\n        else:\n            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n        beam_transition_matrix = torch.bmm(\n            beam_transition_score1.view(-1, beam, self.rank),\n            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n\n        # compute the normalizer in the log-space\n        score = beam_emission_scores[:, 0]  # B x K\n        for i in range(1, seq_len):\n            next_score = score[:, :, None] + beam_transition_matrix[:, i-1]\n            next_score = logsumexp(next_score, dim=1) + beam_emission_scores[:, i]\n\n            if masks is not None:\n                score = torch.where(masks[:, i:i+1], next_score, score)\n            else:\n                score = next_score\n\n        # Sum (log-sum-exp) over all possible tags\n        return logsumexp(score, dim=1)\n\n    def _viterbi_decode(self, emissions, masks=None, beam=None):\n        # HACK: we use a beam of tokens to approximate the normalizing factor (which is bad?)\n\n        beam = beam if beam is not None else self.beam\n        batch_size, seq_len = emissions.size()[:2]\n        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n        beam_transition_matrix = torch.bmm(\n            beam_transition_score1.view(-1, beam, self.rank),\n            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n\n        traj_tokens, traj_scores = [], []\n        finalized_tokens, finalized_scores = [], []\n\n        # compute the normalizer in the log-space\n        score = beam_emission_scores[:, 0]  # B x K\n        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n\n        for i in range(1, seq_len):\n            traj_scores.append(score)\n            _score = score[:, :, None] + beam_transition_matrix[:, i-1]\n            _score, _index = _score.max(dim=1)\n            _score = _score + beam_emission_scores[:, i]\n\n            if masks is not None:\n                score = torch.where(masks[:, i: i+1], _score, score)\n                index = torch.where(masks[:, i: i+1], _index, dummy)\n            else:\n                score, index = _score, _index\n            traj_tokens.append(index)\n\n        # now running the back-tracing and find the best\n        best_score, best_index = score.max(dim=1)\n        finalized_tokens.append(best_index[:, None])\n        finalized_scores.append(best_score[:, None])\n\n        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n            previous_index = finalized_tokens[-1]\n            finalized_tokens.append(idx.gather(1, previous_index))\n            finalized_scores.append(scs.gather(1, previous_index))\n\n        finalized_tokens.reverse()\n        finalized_tokens = torch.cat(finalized_tokens, 1)\n        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n\n        finalized_scores.reverse()\n        finalized_scores = torch.cat(finalized_scores, 1)\n        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n\n        return finalized_scores, finalized_tokens\n'"
fairseq/modules/fp32_group_norm.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nLayer norm done in fp32 (for fp16 training)\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Fp32GroupNorm(nn.GroupNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = F.group_norm(\n            input.float(),\n            self.num_groups,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(input)\n'"
fairseq/modules/gelu.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nSee ""Gaussian Error Linear Units (GELUs)"" by Dan Hendrycks and Kevin Gimpel with\nthe corresponding GitHub repo: https://github.com/hendrycks/GELUs\n""""""\n\nimport math\n\nimport torch\nimport torch.nn as nn\n\n\ndef gelu_accurate(x):\n    if not hasattr(gelu_accurate, ""_a""):\n        gelu_accurate._a = math.sqrt(2 / math.pi)\n    return (\n        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n    )\n\n\ndef gelu(x: torch.Tensor) -> torch.Tensor:\n    return torch.nn.functional.gelu(x.float()).type_as(x)\n'"
fairseq/modules/grad_multiply.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\n\nclass GradMultiply(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, scale):\n        ctx.scale = scale\n        res = x.new(x)\n        return res\n\n    @staticmethod\n    def backward(ctx, grad):\n        return grad * ctx.scale, None\n'"
fairseq/modules/gumbel_vector_quantizer.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GumbelVectorQuantizer(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_vars,\n        temp,\n        groups,\n        combine_groups,\n        vq_dim,\n        time_first,\n        activation=nn.GELU(),\n        weight_proj_depth=1,\n        weight_proj_factor=1,\n    ):\n        """"""Vector quantization using gumbel softmax\n\n        Args:\n            dim: input dimension (channels)\n            num_vars: number of quantized vectors per group\n            temp: temperature for training. this should be a tuple of 3 elements: (start, stop, decay factor)\n            groups: number of groups for vector quantization\n            combine_groups: whether to use the vectors for all groups\n            vq_dim: dimensionality of the resulting quantized vector\n            time_first: if true, expect input in BxTxC format, otherwise in BxCxT\n            activation: what activation to use (should be a module). this is only used if weight_proj_depth is > 1\n            weight_proj_depth: number of layers (with activation in between) to project input before computing logits\n            weight_proj_factor: this is used only if weight_proj_depth is > 1. scales the inner dimensionality of\n                                projections by this factor\n        """"""\n        super().__init__()\n\n        self.groups = groups\n        self.combine_groups = combine_groups\n        self.input_dim = dim\n        self.num_vars = num_vars\n        self.time_first = time_first\n\n        assert (\n            vq_dim % groups == 0\n        ), f""dim {vq_dim} must be divisible by groups {groups} for concatenation""\n\n        var_dim = vq_dim // groups\n        num_groups = groups if not combine_groups else 1\n\n        self.vars = nn.Parameter(torch.FloatTensor(1, num_groups * num_vars, var_dim))\n        nn.init.xavier_normal_(self.vars)\n\n        if weight_proj_depth > 1:\n\n            def block(input_dim, output_dim):\n                return nn.Sequential(nn.Linear(input_dim, output_dim), activation)\n\n            inner_dim = self.input_dim * weight_proj_factor\n            self.weight_proj = nn.Sequential(\n                *[\n                    block(self.input_dim if i == 0 else inner_dim, inner_dim)\n                    for i in range(weight_proj_depth - 1)\n                ],\n                nn.Linear(inner_dim, groups * num_vars),\n            )\n        else:\n            self.weight_proj = nn.Linear(self.input_dim, groups * num_vars)\n\n        assert len(temp) == 3, temp\n\n        self.max_temp, self.min_temp, self.temp_decay = temp\n        self.curr_temp = self.max_temp\n        self.codebook_indices = None\n\n    def set_num_updates(self, num_updates):\n        self.curr_temp = max(\n            self.max_temp * self.temp_decay ** num_updates, self.min_temp\n        )\n\n    def codebook(self):\n        if self.codebook_indices is None:\n            from itertools import product\n\n            p = [range(self.num_vars)] * self.groups\n            inds = list(product(*p))\n            self.codebook_indices = torch.tensor(\n                inds, dtype=torch.long, device=self.vars.device\n            ).flatten()\n\n            if not self.combine_groups:\n                self.codebook_indices = self.codebook_indices.view(\n                    self.num_vars ** self.groups, -1\n                )\n                for b in range(1, self.groups):\n                    self.codebook_indices[:, b] += self.num_vars * b\n                self.codebook_indices = self.codebook_indices.flatten()\n\n        return (\n            self.vars.squeeze(0)\n            .index_select(0, self.codebook_indices)\n            .view(self.num_vars ** self.groups, -1)\n        )\n\n    def forward_idx(self, x):\n        res = self.forward(x, produce_targets=True)\n        return res[""x""], res[""targets""]\n\n    def forward(self, x, produce_targets=False):\n\n        result = {""num_vars"": self.num_vars * self.groups}\n\n        if not self.time_first:\n            x = x.transpose(1, 2)\n\n        bsz, tsz, fsz = x.shape\n        x = x.reshape(-1, fsz)\n        x = self.weight_proj(x)\n        x = x.view(bsz * tsz * self.groups, -1)\n\n        _, k = x.max(-1)\n        hard_x = (\n            x.new_zeros(*x.shape)\n            .scatter_(-1, k.view(-1, 1), 1.0)\n            .view(bsz * tsz, self.groups, -1)\n        )\n        hard_probs = torch.mean(hard_x.float(), dim=0)\n        result[""code_perplexity""] = torch.exp(\n            -torch.sum(hard_probs * torch.log(hard_probs + 1e-7), dim=-1)\n        ).sum()\n\n        avg_probs = torch.softmax(\n            x.view(bsz * tsz, self.groups, -1).float(), dim=-1\n        ).mean(dim=0)\n        result[""prob_perplexity""] = torch.exp(\n            -torch.sum(avg_probs * torch.log(avg_probs + 1e-7), dim=-1)\n        ).sum()\n\n        result[""temp""] = self.curr_temp\n\n        if self.training:\n            x = F.gumbel_softmax(x.float(), tau=self.curr_temp, hard=True).type_as(x)\n        else:\n            x = hard_x\n\n        x = x.view(bsz * tsz, -1)\n\n        vars = self.vars\n        if self.combine_groups:\n            vars = vars.repeat(1, self.groups, 1)\n\n        if produce_targets:\n            result[""targets""] = (\n                x.view(bsz * tsz * self.groups, -1)\n                .argmax(dim=-1)\n                .view(bsz, tsz, self.groups)\n                .detach()\n            )\n\n        x = x.unsqueeze(-1) * vars\n        x = x.view(bsz * tsz, self.groups, self.num_vars, -1)\n        x = x.sum(-2)\n        x = x.view(bsz, tsz, -1)\n\n        if not self.time_first:\n            x = x.transpose(1, 2)  # BTC -> BCT\n\n        result[""x""] = x\n\n        return result\n'"
fairseq/modules/kmeans_vector_quantizer.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\nfrom fairseq.modules import Fp32GroupNorm\n\n\nclass KmeansVectorQuantizer(nn.Module):\n    def __init__(\n        self, dim, num_vars, groups, combine_groups, vq_dim, time_first, gamma=0.25\n    ):\n        \'\'\'Vector quantization using straight pass-through estimator (i.e. kmeans)\n\n                Args:\n                    dim: input dimension (channels)\n                    num_vars: number of quantized vectors per group\n                    groups: number of groups for vector quantization\n                    combine_groups: whether to use the vectors for all groups\n                    vq_dim: dimensionality of the resulting quantized vector\n                    time_first: if true, expect input in BxTxC format, otherwise in BxCxT\n                    gamma: commitment loss coefficient\n                \'\'\'\n        super().__init__()\n\n        self.groups = groups\n        self.combine_groups = combine_groups\n        self.input_dim = dim\n        self.num_vars = num_vars\n        self.vq_dim = vq_dim\n        self.time_first = time_first\n\n        assert (\n            vq_dim % groups == 0\n        ), f""dim {vq_dim} must be divisible by groups {groups} for concatenation""\n\n        self.var_dim = vq_dim // groups\n        num_groups = groups if not combine_groups else 1\n\n        self.embedding = nn.Parameter(\n            0.01 * torch.randn(num_vars, num_groups, self.var_dim)\n        )\n        self.projection = nn.Sequential(\n            nn.Conv1d(dim, dim, kernel_size=1, groups=groups, bias=False),\n            Fp32GroupNorm(groups, dim),\n        )\n        self.gamma = gamma\n        self.mse_mean = nn.MSELoss(reduction=""mean"")\n\n    def _pass_grad(self, x, y):\n        """""" Manually set gradient for backward pass.\n        for y = f(x), ensure that during the backward pass,\n        dL/dy = dL/dx regardless of f(x).\n        Returns:\n            y, with the gradient forced to be dL/dy = dL/dx.\n        """"""\n\n        return y.detach() + (x - x.detach())\n\n    @property\n    def expand_embedding(self):\n        if self.combine_groups:\n            return self.embedding.expand(self.num_vars, self.groups, self.var_dim)\n        return self.embedding\n\n    def forward_idx(self, x):\n        res = self.forward(x, produce_targets=True)\n        return res[""x""], res[""targets""]\n\n    def forward(self, x, produce_targets=False):\n\n        result = {""num_vars"": self.num_vars}\n\n        if self.time_first:\n            x = x.transpose(1, 2)\n\n        bsz, fsz, tsz = x.shape\n\n        ze = self.projection(x)\n        ze_ = ze.view(bsz, self.groups, self.var_dim, tsz).permute(0, 3, 1, 2)\n        d = (\n            (ze_.unsqueeze(0) - self.expand_embedding.unsqueeze(1).unsqueeze(1))\n            .view(self.num_vars, bsz, tsz, self.groups, -1)\n            .norm(dim=-1, p=2)\n        )\n        idx = d.argmin(dim=0)\n        zq = (\n            torch.stack(\n                [\n                    self.expand_embedding[idx[..., group], group]\n                    for group in range(self.groups)\n                ],\n                dim=-2,\n            )\n            .view(bsz, tsz, self.groups * self.var_dim)\n            .permute(0, 2, 1)\n        )\n        assert ze.shape == zq.shape, (ze.shape, zq.shape)\n        x = self._pass_grad(ze, zq)\n\n        hard_x = (\n            idx.new_zeros(bsz*tsz*self.groups, self.num_vars)\n                .scatter_(-1, idx.view(-1, 1), 1.0)\n                .view(bsz * tsz, self.groups, -1)\n        )\n        hard_probs = torch.mean(hard_x.float(), dim=0)\n        result[""code_perplexity""] = torch.exp(\n            -torch.sum(hard_probs * torch.log(hard_probs + 1e-7), dim=-1)\n        ).sum()\n\n        if produce_targets:\n            result[""targets""] = idx\n\n        if self.time_first:\n            x = x.transpose(1, 2)  # BCT -> BTC\n        result[""x""] = x\n\n        ze = ze.float()\n        zq = zq.float()\n        latent_loss = self.mse_mean(zq, ze.detach())\n        commitment_loss = self.mse_mean(ze, zq.detach())\n\n        result[""kmeans_loss""] = latent_loss + self.gamma * commitment_loss\n\n        return result\n'"
fairseq/modules/layer_drop.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nLayerDrop as described in https://arxiv.org/abs/1909.11556.\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass LayerDropModuleList(nn.ModuleList):\n    """"""\n    A LayerDrop implementation based on :class:`torch.nn.ModuleList`.\n\n    We refresh the choice of which layers to drop every time we iterate\n    over the LayerDropModuleList instance. During evaluation we always\n    iterate over all layers.\n\n    Usage::\n\n        layers = LayerDropList(p=0.5, modules=[layer1, layer2, layer3])\n        for layer in layers:  # this might iterate over layers 1 and 3\n            x = layer(x)\n        for layer in layers:  # this might iterate over all layers\n            x = layer(x)\n        for layer in layers:  # this might not iterate over any layers\n            x = layer(x)\n\n    Args:\n        p (float): probability of dropping out each layer\n        modules (iterable, optional): an iterable of modules to add\n    """"""\n\n    def __init__(self, p, modules=None):\n        super().__init__(modules)\n        self.p = p\n\n    def __iter__(self):\n        dropout_probs = torch.empty(len(self)).uniform_()\n        for i, m in enumerate(super().__iter__()):\n            if not self.training or (dropout_probs[i] > self.p):\n                yield m\n'"
fairseq/modules/layer_norm.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ntry:\n    from apex.normalization import FusedLayerNorm as _FusedLayerNorm\n\n    has_fused_layernorm = True\n\n    class FusedLayerNorm(_FusedLayerNorm):\n        @torch.jit.unused\n        def forward(self, x):\n            if not x.is_cuda:\n                return super().forward(x)\n            else:\n                with torch.cuda.device(x.device):\n                    return super().forward(x)\n\nexcept ImportError:\n    has_fused_layernorm = False\n\n\ndef LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):\n    if not export and torch.cuda.is_available() and has_fused_layernorm:\n        return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n\n\nclass Fp32LayerNorm(nn.LayerNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = F.layer_norm(\n            input.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(input)\n'"
fairseq/modules/learned_positional_embedding.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom torch import Tensor\n\n\nclass LearnedPositionalEmbedding(nn.Embedding):\n    """"""\n    This module learns positional embeddings up to a fixed maximum size.\n    Padding ids are ignored by either offsetting based on padding_idx\n    or by setting padding_idx to None and ensuring that the appropriate\n    position ids are passed to the forward function.\n    """"""\n\n    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int):\n        super().__init__(num_embeddings, embedding_dim, padding_idx)\n        self.onnx_trace = False\n        if self.padding_idx is not None:\n            self.max_positions = self.num_embeddings - self.padding_idx - 1\n        else:\n            self.max_positions = self.num_embeddings\n\n    def forward(\n        self,\n        input: Tensor,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        positions: Optional[Tensor] = None,\n    ):\n        """"""Input is expected to be of size [bsz x seqlen].""""""\n        assert (positions is None) or (\n            self.padding_idx is None\n        ), ""If positions is pre-computed then padding_idx should not be set.""\n\n        if positions is None:\n            if incremental_state is not None:\n                # positions is the same for every token when decoding a single step\n                # Without the int() cast, it doesn\'t work in some cases when exporting to ONNX\n                positions = torch.zeros(\n                    (1, 1), device=input.device, dtype=input.dtype\n                ).fill_(int(self.padding_idx + input.size(1)))\n            else:\n                positions = utils.make_positions(\n                    input, self.padding_idx, onnx_trace=self.onnx_trace\n                )\n        return F.embedding(\n            positions,\n            self.weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n'"
fairseq/modules/lightweight_convolution.py,10,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.modules.unfold import unfold1d\nfrom fairseq.incremental_decoding_utils import with_incremental_state\n\n\ndef LightweightConv(input_size, kernel_size=1, padding_l=None, num_heads=1,\n                    weight_dropout=0., weight_softmax=False, bias=False):\n    if torch.cuda.is_available():\n        try:\n            from fairseq.modules.lightconv_layer import LightconvLayer\n            return LightconvLayer(input_size, kernel_size=kernel_size,\n                                  padding_l=padding_l, num_heads=num_heads,\n                                  weight_dropout=weight_dropout,\n                                  weight_softmax=weight_softmax, bias=bias)\n        except ImportError as e:\n            print(e)\n    return LightweightConv1dTBC(input_size, kernel_size=kernel_size,\n                                padding_l=padding_l, num_heads=num_heads,\n                                weight_dropout=weight_dropout,\n                                weight_softmax=weight_softmax, bias=bias)\n\n\nclass LightweightConv1d(nn.Module):\n    \'\'\'Lightweight Convolution assuming the input is BxCxT\n    This is just an example that explains LightConv clearer than the TBC version.\n    We don\'t use this module in the model.\n\n    Args:\n        input_size: # of channels of the input and output\n        kernel_size: convolution channels\n        padding: padding\n        num_heads: number of heads used. The weight is of shape\n            `(num_heads, 1, kernel_size)`\n        weight_softmax: normalize the weight with softmax before the convolution\n\n    Shape:\n        Input: BxCxT, i.e. (batch_size, input_size, timesteps)\n        Output: BxCxT, i.e. (batch_size, input_size, timesteps)\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            `(num_heads, 1, kernel_size)`\n        bias: the learnable bias of the module of shape `(input_size)`\n    \'\'\'\n\n    def __init__(self, input_size, kernel_size=1, padding=0, num_heads=1,\n                 weight_softmax=False, bias=False, weight_dropout=0.):\n        super().__init__()\n        self.input_size = input_size\n        self.kernel_size = kernel_size\n        self.num_heads = num_heads\n        self.padding = padding\n        self.weight_softmax = weight_softmax\n        self.weight = nn.Parameter(torch.Tensor(num_heads, 1, kernel_size))\n\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(input_size))\n        else:\n            self.bias = None\n        self.weight_dropout = weight_dropout\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight)\n        if self.bias is not None:\n            nn.init.constant_(self.bias, 0.)\n\n    def forward(self, input):\n        \'\'\'\n        input size: B x C x T\n        output size: B x C x T\n        \'\'\'\n        B, C, T = input.size()\n        H = self.num_heads\n\n        weight = self.weight\n        if self.weight_softmax:\n            weight = F.softmax(weight, dim=-1)\n\n        weight = F.dropout(weight, self.weight_dropout, training=self.training)\n        # Merge every C/H entries into the batch dimension (C = self.input_size)\n        # B x C x T -> (B * C/H) x H x T\n        # One can also expand the weight to C x 1 x K by a factor of C/H\n        # and do not reshape the input instead, which is slow though\n        input = input.view(-1, H, T)\n        output = F.conv1d(input, weight, padding=self.padding, groups=self.num_heads)\n        output = output.view(B, C, T)\n        if self.bias is not None:\n            output = output + self.bias.view(1, -1, 1)\n\n        return output\n\n\n@with_incremental_state\nclass LightweightConv1dTBC(nn.Module):\n    \'\'\'Lightweight Convolution assuming the input is TxBxC\n    Args:\n        input_size: # of channels of the input\n        kernel_size: convolution channels\n        padding_l: padding to the left when using ""same"" padding\n        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)\n        weight_dropout: the drop rate of the DropConnect to drop the weight\n        weight_softmax: normalize the weight with softmax before the convolution\n        bias: use bias\n\n    Shape:\n        Input: TxBxC, i.e. (timesteps, batch_size, input_size)\n        Output: TxBxC, i.e. (timesteps, batch_size, input_size)\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            `(num_heads, 1, kernel_size)`\n        bias:   the learnable bias of the module of shape `(input_size)`\n    \'\'\'\n    def __init__(self, input_size, kernel_size=1, padding_l=None, num_heads=1,\n                 weight_dropout=0., weight_softmax=False, bias=False):\n        super().__init__()\n        self.input_size = input_size\n        self.kernel_size = kernel_size\n        self.padding_l = padding_l\n        self.num_heads = num_heads\n        self.weight_dropout = weight_dropout\n        self.weight_softmax = weight_softmax\n\n        self.weight = nn.Parameter(torch.Tensor(num_heads, 1, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(input_size))\n        else:\n            self.bias = None\n\n        self.reset_parameters()\n        self.onnx_trace = False\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight)\n        if self.bias is not None:\n            nn.init.constant_(self.bias, 0.)\n\n    def forward(self, x, incremental_state=None, unfold=False):\n        \'\'\'Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C\n        args:\n            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)\n            incremental_state: A dict to keep the state\n            unfold: unfold the input or not. If not, we use the matrix trick instead\n        \'\'\'\n        unfold = unfold or (incremental_state is not None)\n\n        if unfold:\n            output = self._forward_unfolded(x, incremental_state)\n        else:\n            output = self._forward_expanded(x, incremental_state)\n\n        if self.bias is not None:\n            output = output + self.bias.view(1, 1, -1)\n        return output\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def _forward_unfolded(self, x, incremental_state):\n        \'\'\'The conventional implementation of convolutions.\n        Unfolding the input by having a window shifting to the right.\'\'\'\n        T, B, C = x.size()\n        K, H = self.kernel_size, self.num_heads\n        R = C // H\n        assert R * H == C == self.input_size\n\n        weight = self.weight.view(H, K)\n        if incremental_state is not None:\n            input_buffer = self._get_input_buffer(incremental_state)\n            if input_buffer is None:\n                input_buffer = x.new()\n            x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)\n            if self.kernel_size > 1:\n                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size+1:])\n            x_unfold = x_unfold.view(T*B*H, R, -1)\n        else:\n            # unfold the input: T x B x C --> T\' x B x C x K\n            x_unfold = unfold1d(x, self.kernel_size, self.padding_l, 0)\n            x_unfold = x_unfold.view(T*B*H, R, K)\n\n        if self.weight_softmax:\n            weight = utils.softmax(weight, dim=1, onnx_trace=self.onnx_trace).type_as(weight)\n\n        if incremental_state is not None:\n            weight = weight[:, -x_unfold.size(2):]\n            K = weight.size(1)\n\n        weight = weight.view(1, H, K).expand(T*B, H, K).contiguous().view(T*B*H, K, 1)\n\n        weight = F.dropout(weight, self.weight_dropout, training=self.training)\n        output = torch.bmm(x_unfold, weight)  # T*B*H x R x 1\n        output = output.view(T, B, C)\n        return output\n\n    def _forward_expanded(self, x, incremental_state):\n        \'\'\'Turn the convolution filters into band matrices and do matrix multiplication.\n        This is faster when the sequence is short, but less memory efficient.\n        This is not used in the decoder during inference.\n        \'\'\'\n        T, B, C = x.size()\n        K, H = self.kernel_size, self.num_heads\n        R = C // H\n        assert R * H == C == self.input_size\n\n        weight = self.weight.view(H, K)\n        if self.weight_softmax:\n            weight = utils.softmax(weight, dim=1, onnx_trace=self.onnx_trace).type_as(weight)\n        weight = weight.view(1, H, K).expand(T*B, H, K).contiguous()\n        weight = weight.view(T, B*H, K).transpose(0, 1)\n\n        x = x.view(T, B*H, R).transpose(0, 1)\n        P = self.padding_l\n        if K > T and P == K-1:\n            weight = weight.narrow(2, K-T, T)\n            K, P = T, T-1\n        # turn the convolution filters into band matrices\n        weight_expanded = weight.new_zeros(B*H, T, T+K-1, requires_grad=False)\n        weight_expanded.as_strided((B*H, T, K), (T*(T+K-1), T+K, 1)).copy_(weight)\n        weight_expanded = weight_expanded.narrow(2, P, T)\n        weight_expanded = F.dropout(weight_expanded, self.weight_dropout, training=self.training)\n\n        output = torch.bmm(weight_expanded, x)\n        output = output.transpose(0, 1).contiguous().view(T, B, C)\n        return output\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            input_buffer = input_buffer.index_select(1, new_order)\n            self._set_input_buffer(incremental_state, input_buffer)\n\n    def _get_input_buffer(self, incremental_state):\n        return utils.get_incremental_state(self, incremental_state, \'input_buffer\')\n\n    def _set_input_buffer(self, incremental_state, new_buffer):\n        return utils.set_incremental_state(self, incremental_state, \'input_buffer\', new_buffer)\n\n    def extra_repr(self):\n        s = \'{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, bias={}\'.format(\n            self.input_size, self.kernel_size, self.padding_l,\n            self.num_heads, self.weight_softmax, self.bias is not None\n        )\n        if self.weight_dropout > 0.:\n            s += \', weight_dropout={}\'.format(self.weight_dropout)\n        return s\n'"
fairseq/modules/linearized_convolution.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom .conv_tbc import ConvTBC\nfrom fairseq.incremental_decoding_utils import with_incremental_state\n\n\n@with_incremental_state\nclass LinearizedConvolution(ConvTBC):\n    """"""An optimized version of nn.Conv1d.\n\n    At training time, this module uses ConvTBC, which is an optimized version\n    of Conv1d. At inference time, it optimizes incremental generation (i.e.,\n    one time step at a time) by replacing the convolutions with linear layers.\n    Note that the input order changes from training to inference.\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n        self._linearized_weight = None\n        self.register_backward_hook(self._clear_linearized_weight)\n\n    def state_dict(self, destination=None, prefix=\'\', keep_vars=False):\n        state = ConvTBC.state_dict(self, destination, prefix, keep_vars=keep_vars)\n        # don\'t store redundant _linearized_weight in checkpoints\n        if prefix + \'_linearized_weight\' in state:\n            del state[prefix + \'_linearized_weight\']\n        return state\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        prefix = name + \'.\' if name != \'\' else \'\'\n        if prefix + \'_linearized_weight\' in state_dict:\n            del state_dict[prefix + \'_linearized_weight\']\n\n    def forward(self, input, incremental_state=None):\n        """"""\n        Args:\n            incremental_state: Used to buffer signal; if not None, then input is\n                expected to contain a single frame. If the input order changes\n                between time steps, call reorder_incremental_state.\n        Input:\n            Time x Batch x Channel during training\n            Batch x Time x Channel during inference\n        """"""\n        if incremental_state is None:\n            output = super().forward(input)\n            if self.kernel_size[0] > 1 and self.padding[0] > 0:\n                # remove future timesteps added by padding\n                output = output[:-self.padding[0], :, :]\n            return output\n\n        # reshape weight\n        weight = self._get_linearized_weight()\n        kw = self.kernel_size[0]\n\n        bsz = input.size(0)  # input: bsz x len x dim\n        if kw > 1:\n            input = input.data\n            input_buffer = self._get_input_buffer(incremental_state)\n            if input_buffer is None:\n                input_buffer = input.new(bsz, kw, input.size(2)).zero_()\n                self._set_input_buffer(incremental_state, input_buffer)\n            else:\n                # shift buffer\n                input_buffer[:, :-1, :] = input_buffer[:, 1:, :].clone()\n            # append next input\n            input_buffer[:, -1, :] = input[:, -1, :]\n            input = input_buffer\n        with torch.no_grad():\n            output = F.linear(input.view(bsz, -1), weight, self.bias)\n        return output.view(bsz, 1, -1)\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            input_buffer = input_buffer.index_select(0, new_order)\n            self._set_input_buffer(incremental_state, input_buffer)\n\n    def _get_input_buffer(self, incremental_state):\n        return utils.get_incremental_state(self, incremental_state, \'input_buffer\')\n\n    def _set_input_buffer(self, incremental_state, new_buffer):\n        return utils.set_incremental_state(self, incremental_state, \'input_buffer\', new_buffer)\n\n    def _get_linearized_weight(self):\n        if self._linearized_weight is None:\n            kw = self.kernel_size[0]\n            weight = self.weight.transpose(2, 1).transpose(1, 0).contiguous()\n            assert weight.size() == (self.out_channels, kw, self.in_channels)\n            self._linearized_weight = torch.nn.Parameter(weight.view(self.out_channels, -1))\n        return self._linearized_weight\n\n    def _clear_linearized_weight(self, *args):\n        self._linearized_weight = None\n'"
fairseq/modules/multihead_attention.py,27,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom torch import Tensor, nn\nfrom torch.nn import Parameter\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom fairseq.modules.quant_noise import quant_noise\n\n\n@with_incremental_state\nclass MultiheadAttention(nn.Module):\n    """"""Multi-headed attention.\n\n    See ""Attention Is All You Need"" for more details.\n    """"""\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,\n        dropout=0.0,\n        bias=True,\n        add_bias_kv=False,\n        add_zero_attn=False,\n        self_attention=False,\n        encoder_decoder_attention=False,\n        q_noise=0.0,\n        qn_block_size=8,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), ""embed_dim must be divisible by num_heads""\n        self.scaling = self.head_dim ** -0.5\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            ""Self-attention requires query, key and "" ""value to be of the same size""\n        )\n\n        self.k_proj = quant_noise(nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size)\n        self.v_proj = quant_noise(nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size)\n        self.q_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n\n        self.out_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self.reset_parameters()\n\n        self.onnx_trace = False\n        self.tpu = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def prepare_for_tpu_(self, **kwargs):\n        self.tpu = True\n\n    def reset_parameters(self):\n        if self.qkv_same_dim:\n            # Empirically observed the convergence to be much better with\n            # the scaled initialization\n            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.k_proj.weight)\n            nn.init.xavier_uniform_(self.v_proj.weight)\n            nn.init.xavier_uniform_(self.q_proj.weight)\n\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        if self.out_proj.bias is not None:\n            nn.init.constant_(self.out_proj.bias, 0.)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n\n    def forward(\n        self,\n        query,\n        key: Optional[Tensor],\n        value: Optional[Tensor],\n        key_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        need_weights: bool = True,\n        static_kv: bool = False,\n        attn_mask: Optional[Tensor] = None,\n        before_softmax: bool = False,\n        need_head_weights: bool = False,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        """"""Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        """"""\n        if need_head_weights:\n            need_weights = True\n\n        tgt_len, bsz, embed_dim = query.size()\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n\n        if (\n            not self.onnx_trace\n            and not self.tpu  # don\'t use PyTorch version on TPUs\n            and incremental_state is None\n            and not static_kv\n            # A workaround for quantization to work. Otherwise JIT compilation\n            # treats bias in linear module as method.\n            and not torch.jit.is_scripting()\n        ):\n            assert key is not None and value is not None\n            return F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                torch.empty([0]),\n                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                self.training,\n                key_padding_mask,\n                need_weights,\n                attn_mask,\n                use_separate_proj_weight=True,\n                q_proj_weight=self.q_proj.weight,\n                k_proj_weight=self.k_proj.weight,\n                v_proj_weight=self.v_proj.weight,\n            )\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n            if saved_state is not None and ""prev_key"" in saved_state:\n                # previous time steps are cached - no need to recompute\n                # key and value if they are static\n                if static_kv:\n                    assert self.encoder_decoder_attention and not self.self_attention\n                    key = value = None\n        else:\n            saved_state = None\n\n        if self.self_attention:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        elif self.encoder_decoder_attention:\n            # encoder-decoder attention\n            q = self.q_proj(query)\n            if key is None:\n                assert value is None\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n        q *= self.scaling\n\n        if self.bias_k is not None:\n            assert self.bias_v is not None\n            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n                    ],\n                    dim=1,\n                )\n\n        q = (\n            q.contiguous()\n            .view(tgt_len, bsz * self.num_heads, self.head_dim)\n            .transpose(0, 1)\n        )\n        if k is not None:\n            k = (\n                k.contiguous()\n                .view(-1, bsz * self.num_heads, self.head_dim)\n                .transpose(0, 1)\n            )\n        if v is not None:\n            v = (\n                v.contiguous()\n                .view(-1, bsz * self.num_heads, self.head_dim)\n                .transpose(0, 1)\n            )\n\n        if saved_state is not None:\n            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n            if ""prev_key"" in saved_state:\n                _prev_key = saved_state[""prev_key""]\n                assert _prev_key is not None\n                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    k = prev_key\n                else:\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n            if ""prev_value"" in saved_state:\n                _prev_value = saved_state[""prev_value""]\n                assert _prev_value is not None\n                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    v = prev_value\n                else:\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n            prev_key_padding_mask: Optional[Tensor] = None\n            if ""prev_key_padding_mask"" in saved_state:\n                prev_key_padding_mask = saved_state[""prev_key_padding_mask""]\n            assert k is not None and v is not None\n            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n                key_padding_mask=key_padding_mask,\n                prev_key_padding_mask=prev_key_padding_mask,\n                batch_size=bsz,\n                src_len=k.size(1),\n                static_kv=static_kv,\n            )\n\n            saved_state[""prev_key""] = k.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[""prev_value""] = v.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[""prev_key_padding_mask""] = key_padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n        assert k is not None\n        src_len = k.size(1)\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            assert key_padding_mask.size(1) == src_len\n\n        if self.add_zero_attn:\n            assert v is not None\n            src_len += 1\n            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n                            key_padding_mask\n                        ),\n                    ],\n                    dim=1,\n                )\n\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        attn_weights = MultiheadAttention.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n\n        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(0)\n            if self.onnx_trace:\n                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n            attn_weights += attn_mask\n\n        if key_padding_mask is not None:\n            # don\'t attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            if not self.tpu:\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n                    float(""-inf"")\n                )\n            else:\n                attn_weights = attn_weights.transpose(0, 2)\n                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\'-inf\'))\n                attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        if before_softmax:\n            return attn_weights, v\n\n        attn_weights_float = utils.softmax(\n            attn_weights, dim=-1, onnx_trace=self.onnx_trace\n        )\n        attn_weights = attn_weights_float.type_as(attn_weights)\n        attn_probs = F.dropout(\n            attn_weights,\n            p=self.dropout,\n            training=self.training,\n        )\n        assert v is not None\n        attn = torch.bmm(attn_probs, v)\n        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n        if self.onnx_trace and attn.size(1) == 1:\n            # when ONNX tracing a single decoder step (sequence length == 1)\n            # the transpose is a no-op copy before view, thus unnecessary\n            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n        else:\n            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n        attn = self.out_proj(attn)\n        attn_weights: Optional[Tensor] = None\n        if need_weights:\n            attn_weights = attn_weights_float.view(\n                bsz, self.num_heads, tgt_len, src_len\n            ).transpose(1, 0)\n            if not need_head_weights:\n                # average attention weights over heads\n                attn_weights = attn_weights.mean(dim=0)\n\n        return attn, attn_weights\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n        key_padding_mask: Optional[Tensor],\n        prev_key_padding_mask: Optional[Tensor],\n        batch_size: int,\n        src_len: int,\n        static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n            )\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n            filler = torch.zeros(\n                (batch_size, src_len - prev_key_padding_mask.size(1)),\n                device=prev_key_padding_mask.device,\n            )\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), filler.float()], dim=1\n            )\n        elif key_padding_mask is not None:\n            filler = torch.zeros(\n                (batch_size, src_len - key_padding_mask.size(1)),\n                device=key_padding_mask.device,\n            )\n            new_key_padding_mask = torch.cat(\n                [filler.float(), key_padding_mask.float()], dim=1\n            )\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    @torch.jit.export\n    def reorder_incremental_state(\n        self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order: Tensor\n    ):\n        """"""Reorder buffered internal state (for incremental generation).""""""\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            for k in input_buffer.keys():\n                input_buffer_k = input_buffer[k]\n                if input_buffer_k is not None:\n                    if self.encoder_decoder_attention and input_buffer_k.size(0) == new_order.size(0):\n                        break\n                    input_buffer[k] = input_buffer_k.index_select(0, new_order)\n            incremental_state = self._set_input_buffer(incremental_state, input_buffer)\n        return incremental_state\n\n    def _get_input_buffer(\n        self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n    ) -> Dict[str, Optional[Tensor]]:\n        result = self.get_incremental_state(incremental_state, ""attn_state"")\n        if result is not None:\n            return result\n        else:\n            empty_result: Dict[str, Optional[Tensor]] = {}\n            return empty_result\n\n    def _set_input_buffer(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        buffer: Dict[str, Optional[Tensor]],\n    ):\n        return self.set_incremental_state(incremental_state, ""attn_state"", buffer)\n\n    def apply_sparse_mask(attn_weights, tgt_len: int, src_len: int, bsz: int):\n        return attn_weights\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        prefix = name + ""."" if name != """" else """"\n        items_to_add = {}\n        keys_to_remove = []\n        for k in state_dict.keys():\n            if k.endswith(prefix + ""in_proj_weight""):\n                # in_proj_weight used to be q + k + v with same dimensions\n                dim = int(state_dict[k].shape[0] / 3)\n                items_to_add[prefix + ""q_proj.weight""] = state_dict[k][:dim]\n                items_to_add[prefix + ""k_proj.weight""] = state_dict[k][dim : 2 * dim]\n                items_to_add[prefix + ""v_proj.weight""] = state_dict[k][2 * dim :]\n\n                keys_to_remove.append(k)\n\n                k_bias = prefix + ""in_proj_bias""\n                if k_bias in state_dict.keys():\n                    dim = int(state_dict[k].shape[0] / 3)\n                    items_to_add[prefix + ""q_proj.bias""] = state_dict[k_bias][:dim]\n                    items_to_add[prefix + ""k_proj.bias""] = state_dict[k_bias][\n                        dim : 2 * dim\n                    ]\n                    items_to_add[prefix + ""v_proj.bias""] = state_dict[k_bias][2 * dim :]\n\n                    keys_to_remove.append(prefix + ""in_proj_bias"")\n\n        for k in keys_to_remove:\n            del state_dict[k]\n\n        for key, value in items_to_add.items():\n            state_dict[key] = value\n'"
fairseq/modules/positional_embedding.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\nfrom .learned_positional_embedding import LearnedPositionalEmbedding\nfrom .sinusoidal_positional_embedding import SinusoidalPositionalEmbedding\n\n\ndef PositionalEmbedding(\n        num_embeddings: int,\n        embedding_dim: int,\n        padding_idx: int,\n        learned: bool = False,\n):\n    if learned:\n        # if padding_idx is specified then offset the embedding ids by\n        # this index and adjust num_embeddings appropriately\n        # TODO: The right place for this offset would be inside\n        # LearnedPositionalEmbedding. Move this there for a cleaner implementation.\n        if padding_idx is not None:\n            num_embeddings = num_embeddings + padding_idx + 1\n        m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n        nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n        if padding_idx is not None:\n            nn.init.constant_(m.weight[padding_idx], 0)\n    else:\n        m = SinusoidalPositionalEmbedding(\n            embedding_dim, padding_idx, init_size=num_embeddings + padding_idx + 1,\n        )\n    return m\n'"
fairseq/modules/quant_noise.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\n\ndef quant_noise(module, p, block_size):\n    """"""\n    Wraps modules and applies quantization noise to the weights for\n    subsequent quantization with Iterative Product Quantization as\n    described in ""Training with Quantization Noise for Extreme Model Compression""\n\n    Args:\n        - module: nn.Module\n        - p: amount of Quantization Noise\n        - block_size: size of the blocks for subsequent quantization with iPQ\n\n    Remarks:\n        - Module weights must have the right sizes wrt the block size\n        - Only Linear, Embedding and Conv2d modules are supported for the moment\n        - For more detail on how to quantize by blocks with convolutional weights,\n          see ""And the Bit Goes Down: Revisiting the Quantization of Neural Networks""\n        - We implement the simplest form of noise here as stated in the paper\n          which consists in randomly dropping blocks\n    """"""\n\n    # if no quantization noise, don\'t register hook\n    if p <= 0:\n        return module\n\n    # supported modules\n    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n\n    # test whether module.weight has the right sizes wrt block_size\n    is_conv = module.weight.ndim == 4\n\n    # 2D matrix\n    if not is_conv:\n        assert module.weight.size(1) % block_size == 0, ""Input features must be a multiple of block sizes""\n\n    # 4D matrix\n    else:\n        # 1x1 convolutions\n        if module.kernel_size == (1, 1):\n            assert module.in_channels % block_size == 0, ""Input channels must be a multiple of block sizes""\n        # regular convolutions\n        else:\n            k = module.kernel_size[0] * module.kernel_size[1]\n            assert k % block_size == 0, ""Kernel size must be a multiple of block size""\n\n    def _forward_pre_hook(mod, input):\n        # no noise for evaluation\n        if mod.training:\n            if not is_conv:\n                # gather weight and sizes\n                weight = mod.weight\n                in_features = weight.size(1)\n                out_features = weight.size(0)\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                mask = torch.zeros(in_features // block_size * out_features, device=weight.device)\n                mask.bernoulli_(p)\n                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n\n            else:\n                # gather weight and sizes\n                weight = mod.weight\n                in_channels = mod.in_channels\n                out_channels = mod.out_channels\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                if mod.kernel_size == (1, 1):\n                    mask = torch.zeros(int(in_channels // block_size * out_channels), device=weight.device)\n                    mask.bernoulli_(p)\n                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n                else:\n                    mask = torch.zeros(weight.size(0), weight.size(1), device=weight.device)\n                    mask.bernoulli_(p)\n                    mask = mask.unsqueeze(2).unsqueeze(3).repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n\n            # scale weights and apply mask\n            mask = mask.to(torch.bool)  # x.bool() is not currently supported in TorchScript\n            s = 1 / (1 - p)\n            mod.weight.data = s * weight.masked_fill(mask, 0)\n\n    module.register_forward_pre_hook(_forward_pre_hook)\n    return module\n'"
fairseq/modules/scalar_bias.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport torch\n\n\nclass ScalarBias(torch.autograd.Function):\n    """"""\n    Adds a vector of scalars, used in self-attention mechanism to allow\n    the model to optionally attend to this vector instead of the past\n    """"""\n\n    @staticmethod\n    def forward(ctx, input, dim, bias_init):\n        size = list(input.size())\n        size[dim] += 1\n        output = input.new(*size).fill_(bias_init)\n        output.narrow(dim, 1, size[dim] - 1).copy_(input)\n        ctx.dim = dim\n        return output\n\n    @staticmethod\n    def backward(ctx, grad):\n        return grad.narrow(ctx.dim, 1, grad.size(ctx.dim) - 1), None, None\n\n\ndef scalar_bias(input, dim, bias_init=0):\n    return ScalarBias.apply(input, dim, bias_init)\n'"
fairseq/modules/sinusoidal_positional_embedding.py,10,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Any, Optional\n\nimport torch\nimport torch.onnx.operators\nfrom fairseq import utils\nfrom torch import Tensor, nn\n\n\nclass SinusoidalPositionalEmbedding(nn.Module):\n    """"""This module produces sinusoidal positional embeddings of any length.\n\n    Padding symbols are ignored.\n    """"""\n\n    def __init__(self, embedding_dim, padding_idx, init_size=1024):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.padding_idx = padding_idx\n        self.weights = SinusoidalPositionalEmbedding.get_embedding(\n            init_size, embedding_dim, padding_idx\n        )\n        self.onnx_trace = False\n        self.register_buffer(""_float_tensor"", torch.FloatTensor(1))\n        self.max_positions = int(1e5)\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    @staticmethod\n    def get_embedding(\n        num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None\n    ):\n        """"""Build sinusoidal embeddings.\n\n        This matches the implementation in tensor2tensor, but differs slightly\n        from the description in Section 3.5 of ""Attention Is All You Need"".\n        """"""\n        half_dim = embedding_dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(\n            1\n        ) * emb.unsqueeze(0)\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(\n            num_embeddings, -1\n        )\n        if embedding_dim % 2 == 1:\n            # zero pad\n            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n        if padding_idx is not None:\n            emb[padding_idx, :] = 0\n        return emb\n\n    def forward(\n        self,\n        input,\n        incremental_state: Optional[Any] = None,\n        timestep: Optional[Tensor] = None,\n        positions: Optional[Any] = None,\n    ):\n        """"""Input is expected to be of size [bsz x seqlen].""""""\n        bspair = torch.onnx.operators.shape_as_tensor(input)\n        bsz, seq_len = bspair[0], bspair[1]\n        max_pos = self.padding_idx + 1 + seq_len\n        if self.weights is None or max_pos > self.weights.size(0):\n            # recompute/expand embeddings if needed\n            self.weights = SinusoidalPositionalEmbedding.get_embedding(\n                max_pos, self.embedding_dim, self.padding_idx\n            )\n        self.weights = self.weights.to(self._float_tensor)\n\n        if incremental_state is not None:\n            # positions is the same for every token when decoding a single step\n            pos = timestep.view(-1)[0] + 1 if timestep is not None else seq_len\n            if self.onnx_trace:\n                return (\n                    self.weights.index_select(index=self.padding_idx + pos, dim=0)\n                    .unsqueeze(1)\n                    .repeat(bsz, 1, 1)\n                )\n            return self.weights[self.padding_idx + pos, :].expand(bsz, 1, -1)\n\n        positions = utils.make_positions(\n            input, self.padding_idx, onnx_trace=self.onnx_trace\n        )\n        if self.onnx_trace:\n            flat_embeddings = self.weights.detach().index_select(0, positions.view(-1))\n            embedding_shape = torch.cat(\n                (bsz.view(1), seq_len.view(1), torch.tensor([-1], dtype=torch.long))\n            )\n            embeddings = torch.onnx.operators.reshape_from_tensor_shape(\n                flat_embeddings, embedding_shape\n            )\n            return embeddings\n        return (\n            self.weights.index_select(0, positions.view(-1))\n            .view(bsz, seq_len, -1)\n            .detach()\n        )\n'"
fairseq/modules/sparse_multihead_attention.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nimport torch\nfrom .multihead_attention import MultiheadAttention\n\n\nclass SparseMultiheadAttention(MultiheadAttention):\n    """""" Sparse Multi-Headed Attention.\n\n    ""Generating Long Sequences with Sparse Transformers"". Implements\n    fixed factorized self attention, where l=stride and c=expressivity.\n    A(1) includes all words in the stride window and A(2) takes a summary of c\n    words from the end of each stride window.\n    If is_bidirectional=False, we do not include any words past the current word,\n    as in the paper.\n    """"""\n\n    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0., bias=True,\n                 add_bias_kv=False, add_zero_attn=False, self_attention=False,\n                 encoder_decoder_attention=False, stride=32, expressivity=8, is_bidirectional=True):\n\n        super().__init__(\n            embed_dim, num_heads, kdim, vdim, dropout, bias, add_bias_kv,\n            add_zero_attn, self_attention, encoder_decoder_attention\n        )\n\n        self.is_bidirectional = is_bidirectional\n        self.stride = stride\n        self.expressivity = expressivity\n        assert(self.stride > 0 and self.stride >= self.expressivity)\n\n    # Used for Ai(2) calculations - beginning of [l-c, l] range\n    def compute_checkpoint(self, word_index):\n        if word_index % self.stride == 0 and word_index != 0:\n            checkpoint_index = word_index - self.expressivity\n        else:\n            checkpoint_index = (\n                math.floor(word_index / self.stride) * self.stride\n                + self.stride - self.expressivity\n            )\n        return checkpoint_index\n\n    # Computes Ai(2)\n    def compute_subset_summaries(self, absolute_max):\n        checkpoint_index = self.compute_checkpoint(0)\n        subset_two = set()\n        while checkpoint_index <= absolute_max-1:\n            summary = set(range(checkpoint_index, min(\n                checkpoint_index+self.expressivity+1, absolute_max)\n            ))\n            subset_two = subset_two.union(summary)\n            checkpoint_index = self.compute_checkpoint(checkpoint_index+self.stride)\n        return subset_two\n\n    # Sparse Transformer Fixed Attention Pattern: https://arxiv.org/pdf/1904.10509.pdf\n    def compute_fixed_attention_subset(self, word_index, tgt_len):\n        # +1s account for range function; [min, max) -> [min, max]\n        if not self.is_bidirectional:\n            absolute_max = word_index + 1\n        else:\n            absolute_max = tgt_len\n\n        # Subset 1 - whole window\n        rounded_index = math.floor((word_index + self.stride) / self.stride) * self.stride\n        if word_index % self.stride == 0 and word_index != 0:\n            subset_one = set(range(word_index-self.stride, min(absolute_max, word_index+1)))\n        else:\n            subset_one = set(range(max(0, rounded_index - self.stride), min(\n                absolute_max, rounded_index+1))\n            )\n\n        # Subset 2 - summary per window\n        # If bidirectional, subset 2 is the same for every index\n        subset_two = set()\n        if not self.is_bidirectional:\n            subset_two = self.compute_subset_summaries(absolute_max)\n\n        return subset_one.union(subset_two)\n\n    # Compute sparse mask - if bidirectional, can pre-compute and store\n    def buffered_sparse_mask(self, tensor, tgt_len, src_len):\n        assert(tgt_len > self.stride)\n        sparse_mask = torch.empty((tgt_len, src_len)).float().fill_(float(\'-inf\'))\n\n        # If bidirectional, subset 2 is the same for every index\n        subset_summaries = set()\n        if self.is_bidirectional:\n            subset_summaries = self.compute_subset_summaries(tgt_len)\n\n        for i in range(tgt_len):\n            fixed_attention_subset = self.compute_fixed_attention_subset(i, tgt_len)\n            fixed_attention_subset = fixed_attention_subset.union(subset_summaries)\n            included_word_indices = torch.LongTensor(list(fixed_attention_subset))\n            sparse_mask[i].index_fill_(0, included_word_indices, 0)\n        return sparse_mask.type_as(tensor)\n\n    def apply_sparse_mask(self, attn_weights, tgt_len, src_len, bsz):\n        sparse_mask = self.buffered_sparse_mask(attn_weights, tgt_len, src_len)\n        sparse_mask = sparse_mask.unsqueeze(0).expand(bsz * self.num_heads, tgt_len, src_len)\n        attn_weights += sparse_mask\n'"
fairseq/modules/sparse_transformer_sentence_encoder.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\nfrom fairseq.modules import TransformerSentenceEncoder\nfrom fairseq.modules.sparse_transformer_sentence_encoder_layer import SparseTransformerSentenceEncoderLayer\n\n\nclass SparseTransformerSentenceEncoder(TransformerSentenceEncoder):\n    """"""\n    Sparse implementation of the TransformerSentenceEncoder\n    - see SparseMultiheadAttention\n    """"""\n\n    def __init__(\n        self,\n        padding_idx: int,\n        vocab_size: int,\n        num_encoder_layers: int = 6,\n        embedding_dim: int = 768,\n        ffn_embedding_dim: int = 3072,\n        num_attention_heads: int = 8,\n        dropout: float = 0.1,\n        attention_dropout: float = 0.1,\n        activation_dropout: float = 0.1,\n        max_seq_len: int = 256,\n        num_segments: int = 2,\n        use_position_embeddings: bool = True,\n        offset_positions_by_padding: bool = True,\n        encoder_normalize_before: bool = False,\n        apply_bert_init: bool = False,\n        activation_fn: str = ""relu"",\n        learned_pos_embedding: bool = True,\n        embed_scale: float = None,\n        freeze_embeddings: bool = False,\n        n_trans_layers_to_freeze: int = 0,\n        export: bool = False,\n        is_bidirectional: bool = True,\n        stride: int = 32,\n        expressivity: int = 8,\n    ) -> None:\n\n        super().__init__(\n            padding_idx, vocab_size, num_encoder_layers, embedding_dim,\n            ffn_embedding_dim, num_attention_heads, dropout, attention_dropout,\n            activation_dropout, max_seq_len, num_segments, use_position_embeddings,\n            offset_positions_by_padding, encoder_normalize_before, apply_bert_init,\n            activation_fn, learned_pos_embedding, embed_scale, freeze_embeddings,\n            n_trans_layers_to_freeze, export\n        )\n\n        self.layers = nn.ModuleList(\n            [\n                SparseTransformerSentenceEncoderLayer(\n                    embedding_dim=self.embedding_dim,\n                    ffn_embedding_dim=ffn_embedding_dim,\n                    num_attention_heads=num_attention_heads,\n                    dropout=self.dropout,\n                    attention_dropout=attention_dropout,\n                    activation_dropout=activation_dropout,\n                    activation_fn=activation_fn,\n                    export=export,\n                    is_bidirectional=is_bidirectional,\n                    stride=stride,\n                    expressivity=expressivity,\n                )\n                for _ in range(num_encoder_layers)\n            ]\n        )\n\n        def freeze_module_params(m):\n            if m is not None:\n                for p in m.parameters():\n                    p.requires_grad = False\n\n        for layer in range(n_trans_layers_to_freeze):\n            freeze_module_params(self.layers[layer])\n'"
fairseq/modules/sparse_transformer_sentence_encoder_layer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.modules import TransformerSentenceEncoderLayer\nfrom fairseq.modules.sparse_multihead_attention import SparseMultiheadAttention\n\n\nclass SparseTransformerSentenceEncoderLayer(TransformerSentenceEncoderLayer):\n    """"""\n    Implements a Sprase Transformer Encoder Layer (see SparseMultiheadAttention)\n    """"""\n\n    def __init__(\n        self,\n        embedding_dim: int = 768,\n        ffn_embedding_dim: int = 3072,\n        num_attention_heads: int = 8,\n        dropout: float = 0.1,\n        attention_dropout: float = 0.1,\n        activation_dropout: float = 0.1,\n        activation_fn: str = \'relu\',\n        export: bool = False,\n        is_bidirectional: bool = True,\n        stride: int = 32,\n        expressivity: int = 8,\n    ) -> None:\n\n        super().__init__(\n            embedding_dim, ffn_embedding_dim, num_attention_heads, dropout,\n            attention_dropout, activation_dropout, activation_fn, export\n        )\n\n        self.self_attn = SparseMultiheadAttention(\n            self.embedding_dim,\n            num_attention_heads,\n            dropout=attention_dropout,\n            add_bias_kv=False,\n            add_zero_attn=False,\n            self_attention=True,\n            is_bidirectional=is_bidirectional,\n            stride=stride,\n            expressivity=expressivity,\n        )\n'"
fairseq/modules/transformer_layer.py,13,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, List, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.modules import LayerNorm, MultiheadAttention\nfrom fairseq.modules.quant_noise import quant_noise\nfrom torch import Tensor\n\n\nclass TransformerEncoderLayer(nn.Module):\n    """"""Encoder layer block.\n\n    In the original paper each operation (multi-head attention or FFN) is\n    postprocessed with: `dropout -> add residual -> layernorm`. In the\n    tensor2tensor code they suggest that learning is more robust when\n    preprocessing each layer with layernorm and postprocessing with:\n    `dropout -> add residual`. We default to the approach in the paper, but the\n    tensor2tensor approach can be enabled by setting\n    *args.encoder_normalize_before* to ``True``.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n    """"""\n\n    def __init__(self, args):\n        super().__init__()\n        self.embed_dim = args.encoder_embed_dim\n        self.quant_noise = getattr(args, ""quant_noise_pq"", 0)\n        self.quant_noise_block_size = getattr(args, ""quant_noise_pq_block_size"", 8)\n        self.self_attn = self.build_self_attention(self.embed_dim, args)\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.dropout = args.dropout\n        self.activation_fn = utils.get_activation_fn(\n            activation=getattr(args, ""activation_fn"", ""relu"")\n        )\n        self.activation_dropout = getattr(args, ""activation_dropout"", 0)\n        if self.activation_dropout == 0:\n            # for backwards compatibility with models that use args.relu_dropout\n            self.activation_dropout = getattr(args, ""relu_dropout"", 0)\n        self.normalize_before = args.encoder_normalize_before\n        self.fc1 = self.build_fc1(\n            self.embed_dim, args.encoder_ffn_embed_dim, self.quant_noise, self.quant_noise_block_size\n        )\n        self.fc2 = self.build_fc2(\n            args.encoder_ffn_embed_dim, self.embed_dim, self.quant_noise, self.quant_noise_block_size\n        )\n\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), p=q_noise, block_size=qn_block_size)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), p=q_noise, block_size=qn_block_size)\n\n    def build_self_attention(self, embed_dim, args):\n        return MultiheadAttention(\n            embed_dim,\n            args.encoder_attention_heads,\n            dropout=args.attention_dropout,\n            self_attention=True,\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n        )\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        """"""\n        Rename layer norm states from `...layer_norms.0.weight` to\n        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to\n        `...final_layer_norm.weight`\n        """"""\n        layer_norm_map = {""0"": ""self_attn_layer_norm"", ""1"": ""final_layer_norm""}\n        for old, new in layer_norm_map.items():\n            for m in (""weight"", ""bias""):\n                k = ""{}.layer_norms.{}.{}"".format(name, old, m)\n                if k in state_dict:\n                    state_dict[""{}.{}.{}"".format(name, new, m)] = state_dict[k]\n                    del state_dict[k]\n\n    def forward(self, x, encoder_padding_mask, attn_mask: Optional[Tensor] = None):\n        """"""\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n            attn_mask (ByteTensor): binary tensor of shape (T_tgt, T_src), where\n            T_tgt is the length of query, while T_src is the length of key,\n            though here both query and key is x here,\n            attn_mask[t_tgt, t_src] = 1 means when calculating embedding\n            for t_tgt, t_src is excluded (or masked out), =0 means it is\n            included in attention\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        """"""\n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n        if attn_mask is not None:\n            attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)\n        # anything in original attn_mask = 1, becomes -1e8\n        # anything in original attn_mask = 0, becomes 0\n        # Note that we cannot use -inf here, because at some edge cases,\n        # the attention weight (before softmax) for some padded element in query\n        # will become -inf, which results in NaN in model parameters\n        # TODO: to formally solve this problem, we need to change fairseq\'s\n        # MultiheadAttention. We will do this later on.\n\n        x, _ = self.self_attn(\n            query=x,\n            key=x,\n            value=x,\n            key_padding_mask=encoder_padding_mask,\n            attn_mask=attn_mask,\n        )\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, p=float(self.activation_dropout), training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        return x\n\n\nclass TransformerDecoderLayer(nn.Module):\n    """"""Decoder layer block.\n\n    In the original paper each operation (multi-head attention, encoder\n    attention or FFN) is postprocessed with: `dropout -> add residual ->\n    layernorm`. In the tensor2tensor code they suggest that learning is more\n    robust when preprocessing each layer with layernorm and postprocessing with:\n    `dropout -> add residual`. We default to the approach in the paper, but the\n    tensor2tensor approach can be enabled by setting\n    *args.decoder_normalize_before* to ``True``.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n            (default: False).\n    """"""\n\n    def __init__(\n        self, args, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False\n    ):\n        super().__init__()\n        self.embed_dim = args.decoder_embed_dim\n        self.dropout = args.dropout\n        self.quant_noise = getattr(args, ""quant_noise_pq"", 0)\n        self.quant_noise_block_size = getattr(args, ""quant_noise_pq_block_size"", 8)\n\n        self.cross_self_attention = getattr(args, ""cross_self_attention"", False)\n\n        self.self_attn = self.build_self_attention(\n            self.embed_dim,\n            args,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n        )\n        self.activation_fn = utils.get_activation_fn(\n            activation=getattr(args, ""activation_fn"", ""relu"")\n        )\n        self.activation_dropout = getattr(args, ""activation_dropout"", 0)\n        if self.activation_dropout == 0:\n            # for backwards compatibility with models that use args.relu_dropout\n            self.activation_dropout = getattr(args, ""relu_dropout"", 0)\n        self.normalize_before = args.decoder_normalize_before\n\n        # use layerNorm rather than FusedLayerNorm for exporting.\n        # char_inputs can be used to determint this.\n        # TODO  remove this once we update apex with the fix\n        export = getattr(args, ""char_inputs"", False)\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        if no_encoder_attn:\n            self.encoder_attn = None\n            self.encoder_attn_layer_norm = None\n        else:\n            self.encoder_attn = self.build_encoder_attention(self.embed_dim, args)\n            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        self.fc1 = self.build_fc1(\n            self.embed_dim, args.decoder_ffn_embed_dim, self.quant_noise, self.quant_noise_block_size\n        )\n        self.fc2 = self.build_fc2(\n            args.decoder_ffn_embed_dim, self.embed_dim, self.quant_noise, self.quant_noise_block_size\n        )\n\n        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n        self.need_attn = True\n\n        self.onnx_trace = False\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_self_attention(self, embed_dim, args, add_bias_kv=False, add_zero_attn=False):\n        return MultiheadAttention(\n            embed_dim,\n            args.decoder_attention_heads,\n            dropout=args.attention_dropout,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            self_attention=not getattr(args, ""cross_self_attention"", False),\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n        )\n\n    def build_encoder_attention(self, embed_dim, args):\n        return MultiheadAttention(\n            embed_dim,\n            args.decoder_attention_heads,\n            kdim=getattr(args, ""encoder_embed_dim"", None),\n            vdim=getattr(args, ""encoder_embed_dim"", None),\n            dropout=args.attention_dropout,\n            encoder_decoder_attention=True,\n            q_noise=self.quant_noise,\n            qn_block_size=self.quant_noise_block_size,\n        )\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def forward(\n        self,\n        x,\n        encoder_out: Optional[torch.Tensor] = None,\n        encoder_padding_mask: Optional[torch.Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        prev_self_attn_state: Optional[List[torch.Tensor]] = None,\n        prev_attn_state: Optional[List[torch.Tensor]] = None,\n        self_attn_mask: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[torch.Tensor] = None,\n        need_attn: bool = False,\n        need_head_weights: bool = False,\n    ):\n        """"""\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor, optional): binary\n                ByteTensor of shape `(batch, src_len)` where padding\n                elements are indicated by ``1``.\n            need_attn (bool, optional): return attention weights\n            need_head_weights (bool, optional): return attention weights\n                for each head (default: return average over heads).\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        """"""\n        if need_head_weights:\n            need_attn = True\n\n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n        if prev_self_attn_state is not None:\n            prev_key, prev_value = prev_self_attn_state[:2]\n            saved_state: Dict[str, Optional[Tensor]] = {\n                ""prev_key"": prev_key,\n                ""prev_value"": prev_value,\n            }\n            if len(prev_self_attn_state) >= 3:\n                saved_state[""prev_key_padding_mask""] = prev_self_attn_state[2]\n            assert incremental_state is not None\n            self.self_attn._set_input_buffer(incremental_state, saved_state)\n        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)\n        if self.cross_self_attention and not (\n            incremental_state is not None\n            and _self_attn_input_buffer is not None\n            and ""prev_key"" in _self_attn_input_buffer\n        ):\n            if self_attn_mask is not None:\n                assert encoder_out is not None\n                self_attn_mask = torch.cat(\n                    (x.new_zeros(x.size(0), encoder_out.size(0)), self_attn_mask), dim=1\n                )\n            if self_attn_padding_mask is not None:\n                if encoder_padding_mask is None:\n                    assert encoder_out is not None\n                    encoder_padding_mask = self_attn_padding_mask.new_zeros(\n                        encoder_out.size(1), encoder_out.size(0)\n                    )\n                self_attn_padding_mask = torch.cat(\n                    (encoder_padding_mask, self_attn_padding_mask), dim=1\n                )\n            assert encoder_out is not None\n            y = torch.cat((encoder_out, x), dim=0)\n        else:\n            y = x\n\n        x, attn = self.self_attn(\n            query=x,\n            key=y,\n            value=y,\n            key_padding_mask=self_attn_padding_mask,\n            incremental_state=incremental_state,\n            need_weights=False,\n            attn_mask=self_attn_mask,\n        )\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        if self.encoder_attn is not None:\n            residual = x\n            if self.normalize_before:\n                x = self.encoder_attn_layer_norm(x)\n            if prev_attn_state is not None:\n                prev_key, prev_value = prev_attn_state[:2]\n                saved_state: Dict[str, Optional[Tensor]] = {\n                    ""prev_key"": prev_key,\n                    ""prev_value"": prev_value,\n                }\n                if len(prev_attn_state) >= 3:\n                    saved_state[""prev_key_padding_mask""] = prev_attn_state[2]\n                assert incremental_state is not None\n                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n\n            x, attn = self.encoder_attn(\n                query=x,\n                key=encoder_out,\n                value=encoder_out,\n                key_padding_mask=encoder_padding_mask,\n                incremental_state=incremental_state,\n                static_kv=True,\n                need_weights=need_attn or (not self.training and self.need_attn),\n                need_head_weights=need_head_weights,\n            )\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = residual + x\n            if not self.normalize_before:\n                x = self.encoder_attn_layer_norm(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, p=float(self.activation_dropout), training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        if self.onnx_trace and incremental_state is not None:\n            saved_state = self.self_attn._get_input_buffer(incremental_state)\n            assert saved_state is not None\n            if self_attn_padding_mask is not None:\n                self_attn_state = [\n                    saved_state[""prev_key""],\n                    saved_state[""prev_value""],\n                    saved_state[""prev_key_padding_mask""],\n                ]\n            else:\n                self_attn_state = [saved_state[""prev_key""], saved_state[""prev_value""]]\n            return x, attn, self_attn_state\n        return x, attn, None\n\n    def make_generation_fast_(self, need_attn: bool = False, **kwargs):\n        self.need_attn = need_attn\n\n    @torch.jit.export\n    def reorder_incremental_state(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        new_order: Tensor,\n    ):\n        """"""Scriptable reorder incremental state in transformer layers.""""""\n        self.self_attn.reorder_incremental_state(incremental_state, new_order)\n\n        if self.encoder_attn is not None:\n            self.encoder_attn.reorder_incremental_state(incremental_state, new_order)\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.0)\n    return m\n'"
fairseq/modules/transformer_sentence_encoder.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.modules import (\n    LayerDropModuleList,\n    LayerNorm,\n    MultiheadAttention,\n    PositionalEmbedding,\n    TransformerSentenceEncoderLayer,\n)\nfrom fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\nimport random\n\n\ndef init_bert_params(module):\n    """"""\n    Initialize the weights specific to the BERT Model.\n    This overrides the default initializations depending on the specified arguments.\n        1. If normal_init_linear_weights is set then weights of linear\n           layer will be initialized using the normal distribution and\n           bais will be set to the specified value.\n        2. If normal_init_embed_weights is set then weights of embedding\n           layer will be initialized using the normal distribution.\n        3. If normal_init_proj_weights is set then weights of\n           in_project_weight for MultiHeadAttention initialized using\n           the normal distribution (to be validated).\n    """"""\n\n    if isinstance(module, nn.Linear):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n    if isinstance(module, nn.Embedding):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.padding_idx is not None:\n            module.weight.data[module.padding_idx].zero_()\n    if isinstance(module, MultiheadAttention):\n        module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n        module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n        module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n\n\nclass TransformerSentenceEncoder(nn.Module):\n    """"""\n    Implementation for a Bi-directional Transformer based Sentence Encoder used\n    in BERT/XLM style pre-trained models.\n\n    This first computes the token embedding using the token embedding matrix,\n    position embeddings (if specified) and segment embeddings\n    (if specified). After applying the specified number of\n    TransformerEncoderLayers, it outputs all the internal states of the\n    encoder as well as the final representation associated with the first\n    token (usually CLS token).\n\n    Input:\n        - tokens: B x T matrix representing sentences\n        - segment_labels: B x T matrix representing segment label for tokens\n\n    Output:\n        - a tuple of the following:\n            - a list of internal model states used to compute the\n              predictions where each tensor has shape T x B x C\n            - sentence representation associated with first input token\n              in format B x C.\n    """"""\n\n    def __init__(\n        self,\n        padding_idx: int,\n        vocab_size: int,\n        num_encoder_layers: int = 6,\n        embedding_dim: int = 768,\n        ffn_embedding_dim: int = 3072,\n        num_attention_heads: int = 8,\n        dropout: float = 0.1,\n        attention_dropout: float = 0.1,\n        activation_dropout: float = 0.1,\n        layerdrop : float = 0.0,\n        max_seq_len: int = 256,\n        num_segments: int = 2,\n        use_position_embeddings: bool = True,\n        offset_positions_by_padding: bool = True,\n        encoder_normalize_before: bool = False,\n        apply_bert_init: bool = False,\n        activation_fn: str = ""relu"",\n        learned_pos_embedding: bool = True,\n        embed_scale: float = None,\n        freeze_embeddings: bool = False,\n        n_trans_layers_to_freeze: int = 0,\n        export: bool = False,\n        traceable: bool = False,\n        q_noise: float = 0.0,\n        qn_block_size: int = 8,\n    ) -> None:\n\n        super().__init__()\n        self.padding_idx = padding_idx\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        self.layerdrop = layerdrop\n        self.max_seq_len = max_seq_len\n        self.embedding_dim = embedding_dim\n        self.num_segments = num_segments\n        self.use_position_embeddings = use_position_embeddings\n        self.apply_bert_init = apply_bert_init\n        self.learned_pos_embedding = learned_pos_embedding\n        self.traceable = traceable\n        self.tpu = False  # whether we\'re on TPU\n\n        self.embed_tokens = self.build_embedding(\n            self.vocab_size, self.embedding_dim, self.padding_idx\n        )\n        self.embed_scale = embed_scale\n\n        if q_noise > 0:\n            self.quant_noise = apply_quant_noise_(\n                nn.Linear(self.embedding_dim, self.embedding_dim, bias=False),\n                q_noise,\n                qn_block_size,\n            )\n        else:\n            self.quant_noise = None\n\n        self.segment_embeddings = (\n            nn.Embedding(self.num_segments, self.embedding_dim, padding_idx=None)\n            if self.num_segments > 0\n            else None\n        )\n\n        self.embed_positions = (\n            PositionalEmbedding(\n                self.max_seq_len,\n                self.embedding_dim,\n                padding_idx=(self.padding_idx if offset_positions_by_padding else None),\n                learned=self.learned_pos_embedding,\n            )\n            if self.use_position_embeddings\n            else None\n        )\n\n        if self.layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n        self.layers.extend([\n            self.build_transformer_sentence_encoder_layer(\n                embedding_dim=self.embedding_dim,\n                ffn_embedding_dim=ffn_embedding_dim,\n                num_attention_heads=num_attention_heads,\n                dropout=self.dropout,\n                attention_dropout=attention_dropout,\n                activation_dropout=activation_dropout,\n                activation_fn=activation_fn,\n                export=export,\n                q_noise=q_noise,\n                qn_block_size=qn_block_size\n            )\n            for _ in range(num_encoder_layers)\n        ])\n\n        if encoder_normalize_before:\n            self.emb_layer_norm = LayerNorm(self.embedding_dim, export=export)\n        else:\n            self.emb_layer_norm = None\n\n        # Apply initialization of model params after building the model\n        if self.apply_bert_init:\n            self.apply(init_bert_params)\n\n        def freeze_module_params(m):\n            if m is not None:\n                for p in m.parameters():\n                    p.requires_grad = False\n\n        if freeze_embeddings:\n            freeze_module_params(self.embed_tokens)\n            freeze_module_params(self.segment_embeddings)\n            freeze_module_params(self.embed_positions)\n            freeze_module_params(self.emb_layer_norm)\n\n        for layer in range(n_trans_layers_to_freeze):\n            freeze_module_params(self.layers[layer])\n\n    def build_embedding(self, vocab_size, embedding_dim, padding_idx):\n        return nn.Embedding(vocab_size, embedding_dim, padding_idx)\n\n    def build_transformer_sentence_encoder_layer(\n        self,\n        embedding_dim,\n        ffn_embedding_dim,\n        num_attention_heads,\n        dropout,\n        attention_dropout,\n        activation_dropout,\n        activation_fn,\n        export,\n        q_noise,\n        qn_block_size,\n    ):\n        return TransformerSentenceEncoderLayer(\n            embedding_dim=embedding_dim,\n            ffn_embedding_dim=ffn_embedding_dim,\n            num_attention_heads=num_attention_heads,\n            dropout=dropout,\n            attention_dropout=attention_dropout,\n            activation_dropout=activation_dropout,\n            activation_fn=activation_fn,\n            export=export,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n    def prepare_for_tpu_(self, **kwargs):\n        self.tpu = True\n\n    def forward(\n        self,\n        tokens: torch.Tensor,\n        segment_labels: torch.Tensor = None,\n        last_state_only: bool = False,\n        positions: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n\n        # compute padding mask. This is needed for multi-head attention\n        padding_mask = tokens.eq(self.padding_idx)\n        if not self.traceable and not self.tpu and not padding_mask.any():\n            padding_mask = None\n\n        x = self.embed_tokens(tokens)\n\n        if self.embed_scale is not None:\n            x *= self.embed_scale\n\n        if self.embed_positions is not None:\n            x += self.embed_positions(tokens, positions=positions)\n\n        if self.segment_embeddings is not None and segment_labels is not None:\n            x += self.segment_embeddings(segment_labels)\n\n        if self.quant_noise is not None:\n            x = self.quant_noise(x)\n\n        if self.emb_layer_norm is not None:\n            x = self.emb_layer_norm(x)\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # account for padding while computing the representation\n        if padding_mask is not None:\n            x *= 1 - padding_mask.unsqueeze(-1).type_as(x)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        inner_states = []\n        if not last_state_only:\n            inner_states.append(x)\n\n        for layer in self.layers:\n            x, _ = layer(x, self_attn_padding_mask=padding_mask)\n            if not last_state_only:\n                inner_states.append(x)\n\n        sentence_rep = x[0, :, :]\n\n        if last_state_only:\n            inner_states = [x]\n\n        if self.traceable:\n            return torch.stack(inner_states), sentence_rep\n        else:\n            return inner_states, sentence_rep\n'"
fairseq/modules/transformer_sentence_encoder_layer.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.modules import (\n    LayerNorm,\n    MultiheadAttention,\n)\nfrom fairseq.modules.quant_noise import quant_noise\n\n\nclass TransformerSentenceEncoderLayer(nn.Module):\n    """"""\n    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n    models.\n    """"""\n\n    def __init__(\n        self,\n        embedding_dim: int = 768,\n        ffn_embedding_dim: int = 3072,\n        num_attention_heads: int = 8,\n        dropout: float = 0.1,\n        attention_dropout: float = 0.1,\n        activation_dropout: float = 0.1,\n        activation_fn: str = \'relu\',\n        export: bool = False,\n        q_noise: float = 0.0,\n        qn_block_size: int = 8,\n    ) -> None:\n\n        super().__init__()\n        # Initialize parameters\n        self.embedding_dim = embedding_dim\n        self.dropout = dropout\n        self.activation_dropout = activation_dropout\n\n        # Initialize blocks\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.self_attn = self.build_self_attention(\n            self.embedding_dim,\n            num_attention_heads,\n            dropout=attention_dropout,\n            self_attention=True,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n        # layer norm associated with the self attention layer\n        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)\n\n        self.fc1 = self.build_fc1(\n            self.embedding_dim,\n            ffn_embedding_dim,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n        self.fc2 = self.build_fc2(\n            ffn_embedding_dim,\n            self.embedding_dim,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n        # layer norm associated with the position wise feed-forward NN\n        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(\n            nn.Linear(input_dim, output_dim), q_noise, qn_block_size\n        )\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(\n            nn.Linear(input_dim, output_dim), q_noise, qn_block_size\n        )\n\n    def build_self_attention(\n        self,\n        embed_dim,\n        num_attention_heads,\n        dropout,\n        self_attention,\n        q_noise,\n        qn_block_size,\n    ):\n        return MultiheadAttention(\n            embed_dim,\n            num_attention_heads,\n            dropout=dropout,\n            self_attention=True,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        self_attn_mask: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[torch.Tensor] = None,\n    ):\n        """"""\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer implementation.\n        """"""\n        residual = x\n        x, attn = self.self_attn(\n            query=x,\n            key=x,\n            value=x,\n            key_padding_mask=self_attn_padding_mask,\n            need_weights=False,\n            attn_mask=self_attn_mask,\n        )\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.self_attn_layer_norm(x)\n\n        residual = x\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.final_layer_norm(x)\n        return x, attn\n'"
fairseq/modules/unfold.py,1,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn.functional as F\n\n\ndef unfold1d(x, kernel_size, padding_l, pad_value=0):\n    '''unfold T x B x C to T x B x C x K'''\n    if kernel_size > 1:\n        T, B, C = x.size()\n        x = F.pad(x, (0, 0, 0, 0, padding_l, kernel_size - 1 - padding_l), value=pad_value)\n        x = x.as_strided((T, B, C, kernel_size), (B*C, C, 1, B*C))\n    else:\n        x = x.unsqueeze(3)\n    return x\n"""
fairseq/modules/vggblock.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom collections.abc import Iterable\nfrom itertools import repeat\n\nimport torch\nimport torch.nn as nn\n\n\ndef _pair(v):\n    if isinstance(v, Iterable):\n        assert len(v) == 2, ""len(v) != 2""\n        return v\n    return tuple(repeat(v, 2))\n\n\ndef infer_conv_output_dim(conv_op, input_dim, sample_inchannel):\n    sample_seq_len = 200\n    sample_bsz = 10\n    x = torch.randn(sample_bsz, sample_inchannel, sample_seq_len, input_dim)\n    # N x C x H x W\n    # N: sample_bsz, C: sample_inchannel, H: sample_seq_len, W: input_dim\n    x = conv_op(x)\n    # N x C x H x W\n    x = x.transpose(1, 2)\n    # N x H x C x W\n    bsz, seq = x.size()[:2]\n    per_channel_dim = x.size()[3]\n    # bsz: N, seq: H, CxW the rest\n    return x.contiguous().view(bsz, seq, -1).size(-1), per_channel_dim\n\n\nclass VGGBlock(torch.nn.Module):\n    """"""\n    VGG motibated cnn module https://arxiv.org/pdf/1409.1556.pdf\n\n    Args:\n        in_channels: (int) number of input channels (typically 1)\n        out_channels: (int) number of output channels\n        conv_kernel_size: convolution channels\n        pooling_kernel_size: the size of the pooling window to take a max over\n        num_conv_layers: (int) number of convolution layers\n        input_dim: (int) input dimension\n        conv_stride: the stride of the convolving kernel.\n            Can be a single number or a tuple (sH, sW)  Default: 1\n        padding: implicit paddings on both sides of the input.\n            Can be a single number or a tuple (padH, padW). Default: None\n        layer_norm: (bool) if layer norm is going to be applied. Default: False\n\n    Shape:\n        Input: BxCxTxfeat, i.e. (batch_size, input_size, timesteps, features)\n        Output: BxCxTxfeat, i.e. (batch_size, input_size, timesteps, features)\n    """"""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        conv_kernel_size,\n        pooling_kernel_size,\n        num_conv_layers,\n        input_dim,\n        conv_stride=1,\n        padding=None,\n        layer_norm=False,\n    ):\n        assert (\n            input_dim is not None\n        ), ""Need input_dim for LayerNorm and infer_conv_output_dim""\n        super(VGGBlock, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.conv_kernel_size = _pair(conv_kernel_size)\n        self.pooling_kernel_size = _pair(pooling_kernel_size)\n        self.num_conv_layers = num_conv_layers\n        self.padding = (\n            tuple(e // 2 for e in self.conv_kernel_size)\n            if padding is None\n            else _pair(padding)\n        )\n        self.conv_stride = _pair(conv_stride)\n\n        self.layers = nn.ModuleList()\n        for layer in range(num_conv_layers):\n            conv_op = nn.Conv2d(\n                in_channels if layer == 0 else out_channels,\n                out_channels,\n                self.conv_kernel_size,\n                stride=self.conv_stride,\n                padding=self.padding,\n            )\n            self.layers.append(conv_op)\n            if layer_norm:\n                conv_output_dim, per_channel_dim = infer_conv_output_dim(\n                    conv_op, input_dim, in_channels if layer == 0 else out_channels\n                )\n                self.layers.append(nn.LayerNorm(per_channel_dim))\n                input_dim = per_channel_dim\n            self.layers.append(nn.ReLU())\n\n        if self.pooling_kernel_size is not None:\n            pool_op = nn.MaxPool2d(kernel_size=self.pooling_kernel_size, ceil_mode=True)\n            self.layers.append(pool_op)\n            self.total_output_dim, self.output_dim = infer_conv_output_dim(\n                pool_op, input_dim, out_channels\n            )\n\n    def forward(self, x):\n        for i, _ in enumerate(self.layers):\n            x = self.layers[i](x)\n        return x\n'"
fairseq/optim/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\nfrom fairseq import registry\nfrom fairseq.optim.fairseq_optimizer import FairseqOptimizer\nfrom fairseq.optim.fp16_optimizer import FP16Optimizer, MemoryEfficientFP16Optimizer\nfrom fairseq.optim.bmuf import FairseqBMUF  # noqa\n\n\n__all__ = [\n    'FairseqOptimizer',\n    'FP16Optimizer',\n    'MemoryEfficientFP16Optimizer',\n]\n\n\nbuild_optimizer, register_optimizer, OPTIMIZER_REGISTRY = registry.setup_registry(\n    '--optimizer',\n    base_class=FairseqOptimizer,\n    default='nag',\n)\n\n\n# automatically import any Python files in the optim/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        module = file[:file.find('.py')]\n        importlib.import_module('fairseq.optim.' + module)\n"""
fairseq/optim/adadelta.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.optim\n\nfrom . import FairseqOptimizer, register_optimizer\n\n\n@register_optimizer(\'adadelta\')\nclass Adadelta(FairseqOptimizer):\n    def __init__(self, args, params):\n        super().__init__(args)\n        self._optimizer = torch.optim.Adadelta(params, **self.optimizer_config)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--adadelta-rho\', type=float, default=0.9, metavar=\'RHO\',\n                            help=\'coefficient used for computing a running average of squared gradients\')\n        parser.add_argument(\'--adadelta-eps\', type=float, default=1e-6, metavar=\'EPS\',\n                            help=\'term added to the denominator to improve numerical stability\')\n        parser.add_argument(\'--weight-decay\', \'--wd\', default=0.0, type=float, metavar=\'WD\',\n                            help=\'weight decay\')\n        parser.add_argument(\'--anneal-eps\', action=\'store_true\', help=\'flag to anneal eps\')\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        """"""\n        return {\n            \'lr\': self.args.lr[0],\n            \'rho\': self.args.adadelta_rho,\n            \'eps\': self.args.adadelta_eps,\n            \'weight_decay\': self.args.weight_decay,\n        }\n\n    @property\n    def supports_flat_params(self):\n        return True\n'"
fairseq/optim/adafactor.py,10,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nimport torch\nimport torch.optim\n\nfrom . import FairseqOptimizer, register_optimizer\n\n\n@register_optimizer(\'adafactor\')\nclass FairseqAdafactor(FairseqOptimizer):\n    def __init__(self, args, params):\n        super().__init__(args)\n        self._optimizer = Adafactor(params, **self.optimizer_config)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--adafactor-eps\', default=\'(1e-30, 1e-3)\', metavar=""E"",\n                            help=\'epsilons for Adafactor optimizer\')\n        parser.add_argument(\'--clip-threshold\', type=float, default=1.0, metavar=""C"",\n                            help=\'threshold for clipping update root mean square\')\n        parser.add_argument(\'--decay-rate\', type=float, default=-0.8, metavar=""D"",\n                            help=\'decay rate of the second moment estimator\')\n        parser.add_argument(\'--beta1\', type=float, default=None, metavar=""B"",\n                            help=\'beta for first moment estimator. Optional\')\n        parser.add_argument(\'--weight-decay\', \'--wd\', default=0.0, type=float, metavar=\'WD\',\n                            help=\'weight decay\')\n        parser.add_argument(\'--scale-parameter\', action=\'store_true\',\n                            help=\'scale learning rate by root mean square of parameter\')\n        parser.add_argument(\'--relative-step\', action=\'store_true\',\n                            help=\'set learning rate to inverse square root of timestep,\'\n                                 \'otherwise use external learning rate\')\n        parser.add_argument(\'--warmup-init\', action=\'store_true\',\n                            help=\'use relative step for warm-up learning rate schedule\')\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        Note : Convergence issues empirically observed with fp16 on.\n               Might require search for appropriate configuration.\n        """"""\n        return {\n            \'lr\': self.args.lr[0],\n            \'eps\': eval(self.args.adafactor_eps),\n            \'clip_threshold\': self.args.clip_threshold,\n            \'decay_rate\': self.args.decay_rate,\n            \'beta1\': self.args.beta1,\n            \'weight_decay\': self.args.weight_decay,\n            \'scale_parameter\': self.args.scale_parameter,  # defaults to False\n            \'relative_step\': self.args.relative_step,  # defaults to False\n            \'warmup_init\': self.args.warmup_init,\n        }\n\n\nclass Adafactor(torch.optim.Optimizer):\n    """"""Implements Adafactor algorithm.\n\n    This implementation is based on:\n    `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`\n    (see https://arxiv.org/abs/1804.04235)\n\n    Note that this optimizer internally adjusts the learning rate\n    depending on the *scale_parameter*, *relative_step* and\n    *warmup_init* options. To use a manual (external) learning rate\n    schedule you should set `scale_parameter=False` and\n    `relative_step=False`.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): external learning rate (default: None)\n        eps (tuple[float, float]): regularization constans for square gradient\n            and parameter scale respectively (default: (1e-30, 1e-3))\n        clip_threshold (float): threshold of root mean square of\n            final gradient update (default: 1.0)\n        decay_rate (float): coefficient used to compute running averages of square\n            gradient (default: -0.8)\n        beta1 (float): coefficient used for computing running averages of gradient\n            (default: None)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        scale_parameter (bool): if True, learning rate is scaled by root mean square of\n            parameter (default: True)\n        relative_step (bool): if True, time-dependent learning rate is computed\n            instead of external learning rate (default: True)\n        warmup_init (bool): time-dependent learning rate computation depends on\n            whether warm-up initialization is being used (default: False)\n    """"""\n\n    def __init__(self, params, lr=None, eps=(1e-30, 1e-3), clip_threshold=1.0,\n                 decay_rate=-0.8, beta1=None, weight_decay=0.0, scale_parameter=True,\n                 relative_step=True, warmup_init=False):\n        if lr is not None and relative_step:\n            raise ValueError(\'Cannot combine manual lr and relative_step options\')\n        if warmup_init and not relative_step:\n            raise ValueError(\'warmup_init requires relative_step=True\')\n\n        defaults = dict(lr=lr, eps=eps, clip_threshold=clip_threshold, decay_rate=decay_rate,\n                        beta1=beta1, weight_decay=weight_decay, scale_parameter=scale_parameter,\n                        relative_step=relative_step, warmup_init=warmup_init)\n        super(Adafactor, self).__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return False\n\n    def _get_lr(self, param_group, param_state):\n        rel_step_sz = param_group[\'lr\']\n        if param_group[\'relative_step\']:\n            min_step = 1e-6 * param_state[\'step\'] if param_group[\'warmup_init\'] else 1e-2\n            rel_step_sz = min(min_step, 1.0/math.sqrt(param_state[\'step\']))\n        param_scale = 1.0\n        if param_group[\'scale_parameter\']:\n            param_scale = max(param_group[\'eps\'][1], param_state[\'RMS\'])\n        return param_scale * rel_step_sz\n\n    def _get_options(self, param_group, param_shape):\n        factored = len(param_shape) >= 2\n        use_first_moment = param_group[\'beta1\'] is not None\n        return factored, use_first_moment\n\n    def _rms(self, tensor):\n        return tensor.norm(2) / (tensor.numel() ** 0.5)\n\n    def _approx_sq_grad(self, exp_avg_sq_row, exp_avg_sq_col):\n        r_factor = (\n            exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True)\n        ).rsqrt_()\n        c_factor = exp_avg_sq_col.rsqrt()\n        return torch.mm(r_factor.unsqueeze(-1), c_factor.unsqueeze(0))\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.dtype in {torch.float16, torch.bfloat16}:\n                    grad = grad.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adafactor does not support sparse gradients.\')\n\n                state = self.state[p]\n                grad_shape = grad.shape\n\n                factored, use_first_moment = self._get_options(group, grad_shape)\n                # State Initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n\n                    if use_first_moment:\n                        # Exponential moving average of gradient values\n                        state[\'exp_avg\'] = torch.zeros_like(grad)\n                    if factored:\n                        state[\'exp_avg_sq_row\'] = torch.zeros(grad_shape[:-1]).to(grad)\n                        state[\'exp_avg_sq_col\'] = torch.zeros(grad_shape[:-2] + grad_shape[-1:]).to(grad)\n                    else:\n                        state[\'exp_avg_sq\'] = torch.zeros_like(grad)\n\n                    state[\'RMS\'] = 0\n                else:\n                    if use_first_moment:\n                        state[\'exp_avg\'] = state[\'exp_avg\'].to(grad)\n                    if factored:\n                        state[\'exp_avg_sq_row\'] = state[\'exp_avg_sq_row\'].to(grad)\n                        state[\'exp_avg_sq_col\'] = state[\'exp_avg_sq_col\'].to(grad)\n                    else:\n                        state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].to(grad)\n\n                p_data_fp32 = p.data\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p_data_fp32 = p_data_fp32.float()\n\n                state[\'step\'] += 1\n                state[\'RMS\'] = self._rms(p_data_fp32)\n                group[\'lr\'] = self._get_lr(group, state)\n\n                beta2t = 1.0 - math.pow(state[\'step\'], group[\'decay_rate\'])\n                update = (grad**2) + group[\'eps\'][0]\n                if factored:\n                    exp_avg_sq_row = state[\'exp_avg_sq_row\']\n                    exp_avg_sq_col = state[\'exp_avg_sq_col\']\n\n                    exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n                    exp_avg_sq_col.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-2))\n\n                    # Approximation of exponential moving average of square of gradient\n                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)\n                    update.mul_(grad)\n                else:\n                    exp_avg_sq = state[\'exp_avg_sq\']\n\n                    exp_avg_sq.mul_(beta2t).add_(1.0 - beta2t, update)\n                    update = exp_avg_sq.rsqrt().mul_(grad)\n\n                update.div_(\n                    (self._rms(update) / group[\'clip_threshold\']).clamp_(min=1.0)\n                )\n                update.mul_(group[\'lr\'])\n\n                if use_first_moment:\n                    exp_avg = state[\'exp_avg\']\n                    exp_avg.mul_(group[\'beta1\']).add_(1 - group[\'beta1\'], update)\n                    update = exp_avg\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                p_data_fp32.add_(-update)\n\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n'"
fairseq/optim/adagrad.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.optim\n\nfrom . import FairseqOptimizer, register_optimizer\n\n\n@register_optimizer(\'adagrad\')\nclass Adagrad(FairseqOptimizer):\n    def __init__(self, args, params):\n        super().__init__(args)\n        self._optimizer = torch.optim.Adagrad(params, **self.optimizer_config)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--weight-decay\', \'--wd\', default=0.0, type=float, metavar=\'WD\',\n                            help=\'weight decay\')\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        """"""\n        return {\n            \'lr\': self.args.lr[0],\n            \'weight_decay\': self.args.weight_decay,\n        }\n\n    @property\n    def supports_flat_params(self):\n        return True\n'"
fairseq/optim/adam.py,13,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport math\nimport types\n\nimport torch\nimport torch.optim\nimport torch.distributed as dist\n\nfrom fairseq.optim import FairseqOptimizer, register_optimizer\nfrom fairseq.optim.fused_adam import get_fused_adam_class\n\nlogger = logging.getLogger(__name__)\n\n\n@register_optimizer(\'adam\')\nclass FairseqAdam(FairseqOptimizer):\n    """"""Adam optimizer for fairseq.\n\n    Important note: this optimizer corresponds to the ""AdamW"" variant of\n    Adam in its weight decay behavior. As such, it is most closely\n    analogous to torch.optim.AdamW from PyTorch.\n    """"""\n\n    def __init__(self, args, params):\n        super().__init__(args)\n        fused_adam_cls = get_fused_adam_class()\n        use_fused_adam = (\n            not getattr(args, \'use_old_adam\', False)\n            and fused_adam_cls is not None\n            and torch.cuda.is_available()\n        )\n        if getattr(args, \'tpu\', False):\n            # on TPUs we use the Adam defined here, since it\n            # automatically casts gradients to FP32\n            self._optimizer = Adam(params, **self.optimizer_config)\n        elif use_fused_adam:\n            logger.info(\'using FusedAdam\')\n            self._optimizer = fused_adam_cls(params, **self.optimizer_config)\n        else:\n            self._optimizer = Adam(params, **self.optimizer_config)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--adam-betas\', default=\'(0.9, 0.999)\', metavar=\'B\',\n                            help=\'betas for Adam optimizer\')\n        parser.add_argument(\'--adam-eps\', type=float, default=1e-8, metavar=\'D\',\n                            help=\'epsilon for Adam optimizer\')\n        parser.add_argument(\'--weight-decay\', \'--wd\', default=0.0, type=float, metavar=\'WD\',\n                            help=\'weight decay\')\n        # Maintain backward compatibility with old checkpoints that have stored\n        # optimizer state as fairseq.optim.adam.Adam.\n        parser.add_argument(\n            ""--use-old-adam"",\n            action=\'store_true\',\n            default=False,\n            help=""Use fairseq.optim.adam.Adam"",\n        )\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        """"""\n        return {\n            \'lr\': self.args.lr[0],\n            \'betas\': eval(self.args.adam_betas),\n            \'eps\': self.args.adam_eps,\n            \'weight_decay\': self.args.weight_decay,\n        }\n\n    def average_params(self):\n        """"""Reduce Params is only used during BMUF distributed training.""""""\n        state_dict = self.optimizer.state_dict()\n        total_gpus = float(dist.get_world_size())\n\n        for _, value in state_dict[""state""].items():\n            value[""exp_avg""] /= total_gpus\n            value[""exp_avg_sq""] /= total_gpus\n            dist.all_reduce(value[""exp_avg""], op=dist.ReduceOp.SUM)\n            dist.all_reduce(value[""exp_avg_sq""], op=dist.ReduceOp.SUM)\n\n\nclass Adam(torch.optim.Optimizer):\n    """"""Implements Adam algorithm.\n\n    This implementation is modified from torch.optim.Adam based on:\n    `Fixed Weight Decay Regularization in Adam`\n    (see https://arxiv.org/abs/1711.05101)\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super(Adam, self).__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return True\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.dtype in {torch.float16, torch.bfloat16}:\n                    grad = grad.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsgrad = group[\'amsgrad\']\n\n                p_data_fp32 = p.data\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p_data_fp32 = p_data_fp32.float()\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].to(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].to(p_data_fp32)\n                    if amsgrad:\n                        state[\'max_exp_avg_sq\'] = state[\'max_exp_avg_sq\'].to(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(p_data_fp32, alpha=-group[\'weight_decay\'] * group[\'lr\'])\n\n                p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size)\n\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n'"
fairseq/optim/adamax.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.optim\n\nfrom . import FairseqOptimizer, register_optimizer\n\n\n@register_optimizer(\'adamax\')\nclass FairseqAdamax(FairseqOptimizer):\n    def __init__(self, args, params):\n        super().__init__(args)\n        self._optimizer = Adamax(params, **self.optimizer_config)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--adamax-betas\', default=\'(0.9, 0.999)\', metavar=\'B\',\n                            help=\'betas for Adam optimizer\')\n        parser.add_argument(\'--adamax-eps\', type=float, default=1e-8, metavar=\'D\',\n                            help=\'epsilon for Adam optimizer\')\n        parser.add_argument(\'--weight-decay\', \'--wd\', default=0.0, type=float, metavar=\'WD\',\n                            help=\'weight decay\')\n        parser.add_argument(\'--no-bias-correction\', default=False, action=\'store_true\',\n                            help=\'disable bias correction\')\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        """"""\n        return {\n            \'lr\': self.args.lr[0],\n            \'betas\': eval(self.args.adamax_betas),\n            \'eps\': self.args.adamax_eps,\n            \'weight_decay\': self.args.weight_decay,\n            \'bias_correction\': not self.args.no_bias_correction,\n        }\n\n\nclass Adamax(torch.optim.Optimizer):\n    """"""Implements Adamax algorithm (a variant of Adam based on infinity norm).\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`__.\n\n    Compared to the version in PyTorch, this version implements a fix for weight decay.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 2e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        bias_correction (bool, optional): enable bias correction (default: True)\n\n    __ https://arxiv.org/abs/1412.6980\n    """"""\n\n    def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, bias_correction=True):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        if not 0.0 <= weight_decay:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n                        bias_correction=bias_correction)\n        super(Adamax, self).__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return True\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adamax does not support sparse gradients\')\n\n                p_data_fp32 = p.data\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p_data_fp32 = p_data_fp32.float()\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_inf\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].to(p_data_fp32)\n                    state[\'exp_inf\'] = state[\'exp_inf\'].to(p_data_fp32)\n\n                exp_avg, exp_inf = state[\'exp_avg\'], state[\'exp_inf\']\n                beta1, beta2 = group[\'betas\']\n                eps = group[\'eps\']\n\n                state[\'step\'] += 1\n\n                # Update biased first moment estimate.\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                # Update the exponentially weighted infinity norm.\n                torch.max(\n                    exp_inf.mul_(beta2),\n                    grad.abs_(),\n                    out=exp_inf,\n                )\n\n                step_size = group[\'lr\']\n                if group[\'bias_correction\']:\n                    bias_correction = 1 - beta1 ** state[\'step\']\n                    step_size /= bias_correction\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(p_data_fp32, alpha=-group[\'weight_decay\'] * group[\'lr\'])\n\n                p_data_fp32.addcdiv_(exp_avg, exp_inf.add(eps), value=-step_size)\n\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n'"
fairseq/optim/bmuf.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.distributed as dist\n\nfrom . import FairseqOptimizer\n\n\nclass FairseqBMUF(FairseqOptimizer):\n    """"""\n    Implements incremental block distributed data parallelism similar to\n    https://ieeexplore.ieee.org/document/7472805\n\n    Paper title: Scalable training of deep learning machines by incremental\n    block training with intra-block parallel optimization and blockwise\n    model-update filtering\n    """"""\n\n    def __init__(self, args, optimizer):\n\n        super().__init__(args)\n        self._optimizer = optimizer\n        self._num_updates = 0\n        self.sync_iter = self.args.global_sync_iter\n        self.block_momentum = self.args.block_momentum\n        self.block_lr = self.args.block_lr\n        self._reset_local_data()\n        self.warmup_iteration = self.args.warmup_iterations\n        self.use_nbm = self.args.use_nbm\n        self.initial_state = self._optimizer.state_dict()\n        self.average_sync = self.args.average_sync\n        self.world_size = self.args.distributed_world_size\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--block-lr"", default=1, type=float, help=""block learning rate for bmuf""\n        )\n        parser.add_argument(\n            ""--block-momentum"",\n            default=0.875,\n            type=float,\n            help=""block momentum for bmuf"",\n        )\n        parser.add_argument(\n            ""--global-sync-iter"",\n            default=50,\n            type=int,\n            help=""Iteration for syncing global model"",\n        )\n        parser.add_argument(\n            ""--warmup-iterations"",\n            default=500,\n            type=int,\n            help=""warmup iterations for model to broadcast"",\n        )\n        parser.add_argument(\n            ""--use-nbm"",\n            default=False,\n            action=""store_true"",\n            help=""Specify whether you want to use classical BM / Nesterov BM"",\n        )\n        parser.add_argument(\n            ""--average-sync"",\n            default=False,\n            action=""store_true"",\n            help=""Specify whether you want to average the local momentum after each sync"",\n        )\n\n    @property\n    def optimizer(self):\n        return self._optimizer.optimizer\n\n    @property\n    def optimizer_config(self):\n        return self._optimizer.optimizer_config\n\n    def get_lr(self):\n        return self._optimizer.get_lr()\n\n    def set_lr(self, lr):\n        self._optimizer.set_lr(lr)\n\n    def state_dict(self):\n        return self._optimizer.state_dict()\n\n    def load_state_dict(self, state_dict, optimizer_overrides=None):\n        self._optimizer.load_state_dict(state_dict, optimizer_overrides)\n        self.initial_state = self._optimizer.state_dict()\n\n    def multiply_grads(self, c):\n        """"""Multiplies grads by a constant *c*.""""""\n        self._optimizer.multiply_grads(c)\n\n    def clip_grad_norm(self, max_norm, aggregate_norm_fn=None):\n        """"""Clips gradient norm.""""""\n        return self._optimizer.clip_grad_norm(max_norm, aggregate_norm_fn)\n\n    def average_params(self):\n        self._optimizer.average_params()\n\n    def _block_sync(self):\n        if self.world_size <= 1:\n            return\n        # Update the global model using local models from all GPUs\n        # (Step-1) Calculate grad between previously synced model and\n        # currrent local model\n        if self.block_momentum != 0:\n            self._calc_grad()\n\n        # (Step-2) Average gradient from all GPUs\n        self._avg_grad_from_all_gpus()\n\n        # (Step-3) Calculate global momentum and update the global model\n        if self.block_momentum != 0:\n            self._update_global_model()\n\n        # (Step-4) Average local optimizer params\n        if self.average_sync:\n            self.average_params()\n\n    def _is_warmup_end(self):\n        # Check whether train iterations is equal to warmup iter\n        if self.get_num_updates() == self.warmup_iteration:\n            return True\n        return False\n\n    def _is_bmuf_iter(self):\n        # Check whether train iterations is equal to bmuf sync iter\n        if (self.get_num_updates() > self.warmup_iteration) and (\n            self.get_num_updates() % self.sync_iter == 0\n        ):\n            return True\n        return False\n\n    def _warmup_sync(self, root_rank=0):\n        if self.world_size <= 1:\n            return\n        # Broadcast the local model to all gpus\n        for param in self.params:\n            dist.broadcast(param.data, src=root_rank)\n\n        # Update local optimizer state\n        if self.average_sync:\n            self._optimizer.average_params()\n        else:\n            self._optimizer.load_state_dict(self.initial_state)\n\n        self._reset_local_data()\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.""""""\n        self._optimizer.step(closure)\n        self.set_num_updates(self.get_num_updates() + 1)\n        if self._is_warmup_end():\n            self._warmup_sync()\n        elif self._is_bmuf_iter():\n            self._block_sync()\n\n    def zero_grad(self):\n        """"""Clears the gradients of all optimized parameters.""""""\n        self._optimizer.zero_grad()\n\n    def get_num_updates(self):\n        """"""Get the number of parameters updates.""""""\n        return self._num_updates\n\n    def set_num_updates(self, num_updates):\n        """"""Set the number of parameters updates.""""""\n        self._num_updates = num_updates\n\n    @torch.no_grad()\n    def _reset_local_data(self):\n        # (Step-0) Initialize global momentum parameters and store global copy on each gpu\n        self.global_params = [torch.zeros_like(p.data) for p in self.params]\n        self.smoothed_grads = [p.data.new_zeros(p.data.size()) for p in self.params]\n        self.grads = [p.data.new_zeros(p.data.size()) for p in self.params]\n\n        # saving the global model locally for calculating gradient during bmuf sync\n        for param, global_param in zip(self.params, self.global_params):\n            global_param.copy_(param.data)\n\n    @torch.no_grad()\n    def _calc_grad(self):\n        # global_params is basically the global copy from the previously finished\n        # synchronisation. param.data is local parameter after block_sync_freq\n        # for the local gpu. so grad is difference between previously synced\n        # model and currrent local model.\n        for index, (param, global_param) in enumerate(\n            zip(self.params, self.global_params)\n        ):\n            self.grads[index] = global_param - param.data\n\n    def _avg_grad_from_all_gpus(self):\n        for index, param in enumerate(self.params):\n            sync_para = param.data if self.block_momentum == 0 else self.grads[index]\n            sync_para /= float(dist.get_world_size())\n            dist.all_reduce(sync_para, op=dist.ReduceOp.SUM)\n\n    @torch.no_grad()\n    def _update_global_model(self):\n        for index, (param, global_param, smoothed_grad, grad) in enumerate(\n            zip(\n                self.params,\n                self.global_params,\n                self.smoothed_grads,\n                # all gpus would share the same value of smoothed_grad, since it is\n                # always computed on synchronized gradients.\n                self.grads,\n            )\n        ):\n            # global_param is basically last syncrhornized parameter. though\n            # smoothed_grad is local, all processes will have same value of\n            # smoothed_grad and hence param is globally synchronized copy.\n            # smoothed_grad(t) = BM * smoothed_grad(t-1) + BM_lr * grad(t)\n            smoothed_grad = self.block_momentum * smoothed_grad + self.block_lr * grad\n            param.data.copy_(global_param - smoothed_grad)\n\n            # A Nesterov momentum here is to do a partial weight update before\n            # calculating the gradient\n            if self.use_nbm:\n                param.data.copy_(param.data - self.block_momentum * smoothed_grad)\n\n            # backup for the next synchronization.\n            self.smoothed_grads[index] = smoothed_grad\n            global_param.copy_(param.data)\n'"
fairseq/optim/fairseq_optimizer.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom fairseq import utils\n\n\nclass FairseqOptimizer(object):\n\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        pass\n\n    @property\n    def optimizer(self):\n        """"""Return a torch.optim.optimizer.Optimizer instance.""""""\n        if not hasattr(self, \'_optimizer\'):\n            raise NotImplementedError\n        if not isinstance(self._optimizer, torch.optim.Optimizer):\n            raise ValueError(\'_optimizer must be an instance of torch.optim.Optimizer\')\n        return self._optimizer\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def params(self):\n        """"""Return an iterable of the parameters held by the optimizer.""""""\n        for param_group in self.optimizer.param_groups:\n            for p in param_group[\'params\']:\n                yield p\n\n    def __getstate__(self):\n        return self._optimizer.__getstate__()\n\n    def get_lr(self):\n        """"""Return the current learning rate.""""""\n        return self.optimizer.param_groups[0][\'lr\']\n\n    def set_lr(self, lr):\n        """"""Set the learning rate.""""""\n        for param_group in self.optimizer.param_groups:\n            param_group[\'lr\'] = lr\n\n    def state_dict(self):\n        """"""Return the optimizer\'s state dict.""""""\n        return self.optimizer.state_dict()\n\n    def load_state_dict(self, state_dict, optimizer_overrides=None):\n        """"""Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        """"""\n        self.optimizer.load_state_dict(state_dict)\n\n        if optimizer_overrides is not None and len(optimizer_overrides) > 0:\n            # override learning rate, momentum, etc. with latest values\n            for group in self.optimizer.param_groups:\n                group.update(optimizer_overrides)\n\n    def backward(self, loss):\n        """"""Computes the sum of gradients of the given tensor w.r.t. graph leaves.""""""\n        loss.backward()\n\n    def multiply_grads(self, c):\n        """"""Multiplies grads by a constant *c*.""""""\n        for p in self.params:\n            if p.grad is not None:\n                p.grad.data.mul_(c)\n\n    def clip_grad_norm(self, max_norm, aggregate_norm_fn=None):\n        """"""Clips gradient norm.""""""\n        return utils.clip_grad_norm_(self.params, max_norm, aggregate_norm_fn)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.""""""\n        self.optimizer.step(closure)\n\n    def zero_grad(self):\n        """"""Clears the gradients of all optimized parameters.""""""\n        for p in self.params:\n            p.grad = None\n        self.optimizer.zero_grad()\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        if hasattr(self.optimizer, \'supports_memory_efficient_fp16\'):\n            return self.optimizer.supports_memory_efficient_fp16\n        return False\n\n    @property\n    def supports_flat_params(self):\n        """"""\n        Whether the optimizer supports collapsing of the model\n        parameters/gradients into a single contiguous Tensor.\n        """"""\n        if hasattr(self.optimizer, \'supports_flat_params\'):\n            return self.optimizer.supports_flat_params\n        return False\n\n    def average_params(self):\n        pass\n'"
fairseq/optim/fp16_optimizer.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom itertools import chain\n\nimport torch\n\nfrom fairseq import optim, utils\n\n\nclass DynamicLossScaler(object):\n\n    def __init__(\n        self, init_scale=2.**15, scale_factor=2., scale_window=2000,\n        tolerance=0.05, threshold=None,\n    ):\n        self.loss_scale = init_scale\n        self.scale_factor = scale_factor\n        self.scale_window = scale_window\n        self.tolerance = tolerance\n        self.threshold = threshold\n        self._iter = 0\n        self._last_overflow_iter = -1\n        self._last_rescale_iter = -1\n        self._overflows_since_rescale = 0\n\n    def update_scale(self, overflow):\n        iter_since_rescale = self._iter - self._last_rescale_iter\n        if overflow:\n            self._last_overflow_iter = self._iter\n            self._overflows_since_rescale += 1\n            pct_overflow = self._overflows_since_rescale / float(iter_since_rescale)\n            if pct_overflow >= self.tolerance:\n                self._decrease_loss_scale()\n                self._last_rescale_iter = self._iter\n                self._overflows_since_rescale = 0\n        elif (self._iter - self._last_overflow_iter) % self.scale_window == 0:\n            self.loss_scale *= self.scale_factor\n            self._last_rescale_iter = self._iter\n        self._iter += 1\n\n    def _decrease_loss_scale(self):\n        self.loss_scale /= self.scale_factor\n        if self.threshold is not None:\n            self.loss_scale = max(self.loss_scale, self.threshold)\n\n    @staticmethod\n    def has_overflow(grad_norm):\n        # detect inf and nan\n        if grad_norm == float(\'inf\') or grad_norm != grad_norm:\n            return True\n        return False\n\n\nclass _FP16OptimizerMixin(object):\n\n    def __init__(self, *args, **kwargs):\n        # forward __init__ call to the next class in mro(method resolution order)\n        super().__init__(*args, **kwargs)\n\n    @property\n    def has_flat_params(self):\n        return torch.is_tensor(self.fp32_params)\n\n    @classmethod\n    def build_fp32_params(cls, params, flatten=True):\n        # create FP32 copy of parameters and grads\n        if flatten:\n            total_param_size = sum(p.data.numel() for p in params)\n            fp32_params = torch.zeros(total_param_size, dtype=torch.float, device=params[0].device)\n            offset = 0\n            for p in params:\n                numel = p.data.numel()\n                fp32_params[offset:offset+numel].copy_(p.data.view(-1))\n                offset += numel\n            fp32_params = torch.nn.Parameter(fp32_params)\n            fp32_params.grad = fp32_params.data.new(total_param_size)\n            return fp32_params\n        else:\n            fp32_params = []\n            for p in params:\n                p32 = torch.nn.Parameter(p.data.float())\n                p32.grad = torch.zeros_like(p32.data)\n                fp32_params.append(p32)\n            return fp32_params\n\n    def state_dict(self):\n        """"""Return the optimizer\'s state dict.""""""\n        state_dict = self.fp32_optimizer.state_dict()\n        if self.scaler is not None:\n            state_dict[\'loss_scale\'] = self.scaler.loss_scale\n        return state_dict\n\n    def load_state_dict(self, state_dict, optimizer_overrides=None):\n        """"""Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        """"""\n        if \'loss_scale\' in state_dict and self.scaler is not None:\n            self.scaler.loss_scale = state_dict[\'loss_scale\']\n        self.fp32_optimizer.load_state_dict(state_dict, optimizer_overrides)\n\n    def backward(self, loss):\n        """"""Computes the sum of gradients of the given tensor w.r.t. graph leaves.\n\n        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this\n        function additionally dynamically scales the loss to avoid gradient\n        underflow.\n        """"""\n        if self.scaler is not None:\n            loss = loss * self.scaler.loss_scale\n        loss.backward()\n        self._needs_sync = True\n\n    def _sync_fp16_grads_to_fp32(self, multiply_grads=1.):\n        if self._needs_sync:\n            if self.scaler is not None:\n                # correct for dynamic loss scaler\n                multiply_grads /= self.scaler.loss_scale\n\n            # copy FP16 grads to FP32\n            if self.has_flat_params:\n                offset = 0\n                for p in self.fp16_params:\n                    if not p.requires_grad:\n                        continue\n                    grad_data = p.grad.data if p.grad is not None else p.data.new_zeros(p.data.shape)\n                    numel = grad_data.numel()\n                    self.fp32_params.grad.data[offset:offset+numel].copy_(grad_data.view(-1))\n                    offset += numel\n                self.fp32_params.grad.data.mul_(multiply_grads)\n            else:\n                for p, p32 in zip(self.fp16_params, self.fp32_params):\n                    if not p.requires_grad:\n                        continue\n                    if p.grad is not None:\n                        p32.grad.data.copy_(p.grad.data)\n                        p32.grad.data.mul_(multiply_grads)\n                    else:\n                        p32.grad = torch.zeros_like(p.data, dtype=torch.float)\n\n            self._needs_sync = False\n\n    def multiply_grads(self, c):\n        """"""Multiplies grads by a constant ``c``.""""""\n        if self._needs_sync:\n            self._sync_fp16_grads_to_fp32(c)\n        elif self.has_flat_params:\n            self.fp32_params.grad.data.mul_(c)\n        else:\n            for p32 in self.fp32_params:\n                p32.grad.data.mul_(c)\n\n    def clip_grad_norm(self, max_norm, aggregate_norm_fn=None):\n        """"""Clips gradient norm and updates dynamic loss scaler.""""""\n        self._sync_fp16_grads_to_fp32()\n        grad_norm = utils.clip_grad_norm_(self.fp32_params, max_norm, aggregate_norm_fn)\n\n        # detect overflow and adjust loss scale\n        if self.scaler is not None:\n            overflow = DynamicLossScaler.has_overflow(grad_norm)\n            prev_scale = self.scaler.loss_scale\n            self.scaler.update_scale(overflow)\n            if overflow:\n                if self.scaler.loss_scale <= self.min_loss_scale:\n                    # Use FloatingPointError as an uncommon error that parent\n                    # functions can safely catch to stop training.\n                    self.scaler.loss_scale = prev_scale\n                    raise FloatingPointError((\n                        \'Minimum loss scale reached ({}). Your loss is probably exploding. \'\n                        \'Try lowering the learning rate, using gradient clipping or \'\n                        \'increasing the batch size.\'\n                    ).format(self.min_loss_scale))\n                raise OverflowError(\'setting loss scale to: \' + str(self.scaler.loss_scale))\n\n        return grad_norm\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.""""""\n        self._sync_fp16_grads_to_fp32()\n        self.fp32_optimizer.step(closure)\n\n        # copy FP32 params back into FP16 model\n        if self.has_flat_params:\n            offset = 0\n            for p in self.fp16_params:\n                if not p.requires_grad:\n                    continue\n                numel = p.data.numel()\n                p.data.copy_(self.fp32_params.data[offset:offset+numel].view_as(p.data))\n                offset += numel\n        else:\n            for p, p32 in zip(self.fp16_params, self.fp32_params):\n                if not p.requires_grad:\n                    continue\n                p.data.copy_(p32.data)\n\n    def zero_grad(self):\n        """"""Clears the gradients of all optimized parameters.""""""\n        for p in self.fp16_params:\n            p.grad = None\n        if self.has_flat_params:\n            self.fp32_params.grad.zero_()\n        else:\n            for p32 in self.fp32_params:\n                p32.grad.zero_()\n        self._needs_sync = False\n\n\nclass FP16Optimizer(_FP16OptimizerMixin, optim.FairseqOptimizer):\n    """"""\n    Wrap an *optimizer* to support FP16 (mixed precision) training.\n    """"""\n\n    def __init__(self, args, params, fp32_optimizer, fp32_params):\n        super().__init__(args)\n        self.fp16_params = params\n        self.fp32_optimizer = fp32_optimizer\n        self.fp32_params = fp32_params\n\n        if getattr(args, \'fp16_scale_window\', None) is None:\n            if len(args.update_freq) > 1:\n                raise ValueError(\n                    \'--fp16-scale-window must be given explicitly when using a \'\n                    \'custom --update-freq schedule\'\n                )\n            data_parallel_size = int(args.distributed_world_size / args.model_parallel_size)\n            scale_window = int(2**14 / data_parallel_size / args.update_freq[0])\n        else:\n            scale_window = args.fp16_scale_window\n\n        if not getattr(args, \'bf16\', False):\n            self.scaler = DynamicLossScaler(\n                init_scale=args.fp16_init_scale,\n                scale_window=scale_window,\n                tolerance=args.fp16_scale_tolerance,\n                threshold=args.threshold_loss_scale,\n            )\n            self.min_loss_scale = self.args.min_loss_scale\n        else:\n            # disable loss scaling for bfloat16\n            self.scaler = None\n\n    @classmethod\n    def build_optimizer(cls, args, params):\n        """"""\n        Args:\n            args (argparse.Namespace): fairseq args\n            params (iterable): iterable of parameters to optimize\n        """"""\n        flatten = not getattr(args, \'fp16_no_flatten_grads\', False)\n        if getattr(args, \'bf16\', False):\n            flatten = False  # mixed precision is faster on TPUs without flat grads\n        fp32_params = cls.build_fp32_params(params, flatten=flatten)\n        if flatten:\n            fp32_optimizer = optim.build_optimizer(args, [fp32_params])\n        else:\n            fp32_optimizer = optim.build_optimizer(args, fp32_params)\n        if flatten and not fp32_optimizer.supports_flat_params:\n            raise RuntimeError(\n                \'chosen optimizer does not support flat params, \'\n                \'please set --fp16-no-flatten-grads\'\n            )\n        return cls(args, params, fp32_optimizer, fp32_params)\n\n    @property\n    def optimizer(self):\n        return self.fp32_optimizer.optimizer\n\n    @property\n    def optimizer_config(self):\n        return self.fp32_optimizer.optimizer_config\n\n    def get_lr(self):\n        return self.fp32_optimizer.get_lr()\n\n    def set_lr(self, lr):\n        self.fp32_optimizer.set_lr(lr)\n\n\nclass _MemoryEfficientFP16OptimizerMixin(object):\n\n    def __init__(self, *args, **kwargs):\n        # forward __init__ call to the next class in mro(method resolution order)\n        super().__init__(*args, **kwargs)\n\n    @property\n    def has_flat_params(self):\n        return False\n\n    def state_dict(self):\n        """"""Return the optimizer\'s state dict.""""""\n        state_dict = self.wrapped_optimizer.state_dict()\n        if self.scaler is not None:\n            state_dict[\'loss_scale\'] = self.scaler.loss_scale\n        return state_dict\n\n    def load_state_dict(self, state_dict, optimizer_overrides=None):\n        """"""Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        """"""\n        if \'loss_scale\' in state_dict and self.scaler is not None:\n            self.scaler.loss_scale = state_dict[\'loss_scale\']\n\n        self.wrapped_optimizer.load_state_dict(state_dict, optimizer_overrides)\n\n        # Hack: PyTorch automatically casts the optimizer state to match the\n        # type of the current parameters. But with --memory-efficient-fp16 the\n        # params are FP16 while the optimizer state is FP32 and we don\'t want\n        # to cast. A workaround is to manually copy back the original state\n        # after the optimizer has been loaded.\n        groups = self.optimizer.param_groups\n        saved_groups = state_dict[\'param_groups\']\n        id_map = {\n            old_id: p\n            for old_id, p in zip(\n                chain(*(g[\'params\'] for g in saved_groups)),\n                chain(*(g[\'params\'] for g in groups))\n            )\n        }\n        for k, v in state_dict[\'state\'].items():\n            if k in id_map:\n                param = id_map[k]\n                self.optimizer.state[param] = v\n\n    def backward(self, loss):\n        """"""Computes the sum of gradients of the given tensor w.r.t. graph leaves.\n\n        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this\n        function additionally dynamically scales the loss to avoid gradient\n        underflow.\n        """"""\n        if self.scaler is not None:\n            loss = loss * self.scaler.loss_scale\n            self._grads_are_scaled = True\n        loss.backward()\n\n    def _unscale_grads(self, multiply_grads=1.):\n        if self._grads_are_scaled:\n            self._grads_are_scaled = False\n\n            # correct for dynamic loss scaler\n            self.wrapped_optimizer.multiply_grads(multiply_grads / self.scaler.loss_scale)\n        else:\n            assert multiply_grads == 1.\n\n    def multiply_grads(self, c):\n        """"""Multiplies grads by a constant *c*.""""""\n        if self._grads_are_scaled:\n            self._unscale_grads(c)\n        else:\n            self.wrapped_optimizer.multiply_grads(c)\n\n    def clip_grad_norm(self, max_norm, aggregate_norm_fn=None):\n        """"""Clips gradient norm and updates dynamic loss scaler.""""""\n        self._unscale_grads()\n        grad_norm = self.wrapped_optimizer.clip_grad_norm(max_norm, aggregate_norm_fn)\n\n        # detect overflow and adjust loss scale\n        if self.scaler is not None:\n            overflow = DynamicLossScaler.has_overflow(grad_norm)\n            prev_scale = self.scaler.loss_scale\n            self.scaler.update_scale(overflow)\n            if overflow:\n                if self.scaler.loss_scale <= self.min_loss_scale:\n                    # Use FloatingPointError as an uncommon error that parent\n                    # functions can safely catch to stop training.\n                    self.scaler.loss_scale = prev_scale\n                    raise FloatingPointError((\n                        \'Minimum loss scale reached ({}). Your loss is probably exploding. \'\n                        \'Try lowering the learning rate, using gradient clipping or \'\n                        \'increasing the batch size.\'\n                    ).format(self.min_loss_scale))\n                raise OverflowError(\'setting loss scale to: \' + str(self.scaler.loss_scale))\n\n        return grad_norm\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.""""""\n        self._unscale_grads()\n        self.wrapped_optimizer.step(closure)\n\n    def zero_grad(self):\n        """"""Clears the gradients of all optimized parameters.""""""\n        self.wrapped_optimizer.zero_grad()\n        self._grads_are_scaled = False\n\n\nclass MemoryEfficientFP16Optimizer(_MemoryEfficientFP16OptimizerMixin, optim.FairseqOptimizer):\n    """"""\n    Wrap an *optimizer* to support FP16 (mixed precision) training.\n\n    Compared to :class:`fairseq.optim.FP16Optimizer`, this version does not\n    maintain an FP32 copy of the model. We instead expect the optimizer to\n    convert the gradients to FP32 internally and sync the results back to the\n    FP16 model params. This significantly reduces memory usage but slightly\n    increases the time spent in the optimizer.\n\n    Since this wrapper depends on specific functionality in the wrapped\n    optimizer (i.e., on-the-fly conversion of grads to FP32), only certain\n    optimizers can be wrapped. This is determined by the\n    *supports_memory_efficient_fp16* property.\n    """"""\n\n    def __init__(self, args, params, optimizer):\n        if not optimizer.supports_memory_efficient_fp16:\n            raise ValueError(\n                \'Unsupported optimizer: {}\'.format(optimizer.__class__.__name__)\n            )\n\n        super().__init__(args)\n        self.wrapped_optimizer = optimizer\n\n        if getattr(args, \'fp16_scale_window\', None) is None:\n            if len(args.update_freq) > 1:\n                raise ValueError(\n                    \'--fp16-scale-window must be given explicitly when using a \'\n                    \'custom --update-freq schedule\'\n                )\n            data_parallel_size = int(args.distributed_world_size / args.model_parallel_size)\n            scale_window = 2**14 / data_parallel_size / args.update_freq[0]\n        else:\n            scale_window = args.fp16_scale_window\n\n        if not getattr(args, \'bf16\', False):\n            self.scaler = DynamicLossScaler(\n                init_scale=args.fp16_init_scale,\n                scale_window=scale_window,\n                tolerance=args.fp16_scale_tolerance,\n                threshold=args.threshold_loss_scale,\n            )\n            self.min_loss_scale = self.args.min_loss_scale\n        else:\n            # disable loss scaling for bfloat16\n            self.scaler = None\n\n    @classmethod\n    def build_optimizer(cls, args, params):\n        """"""\n        Args:\n            args (argparse.Namespace): fairseq args\n            params (iterable): iterable of parameters to optimize\n        """"""\n        fp16_optimizer = optim.build_optimizer(args, params)\n        return cls(args, params, fp16_optimizer)\n\n    @property\n    def optimizer(self):\n        return self.wrapped_optimizer.optimizer\n\n    @property\n    def optimizer_config(self):\n        return self.wrapped_optimizer.optimizer_config\n\n    def get_lr(self):\n        return self.wrapped_optimizer.get_lr()\n\n    def set_lr(self, lr):\n        self.wrapped_optimizer.set_lr(lr)\n'"
fairseq/optim/fused_adam.py,13,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport types\n\nimport torch\n\n\ndef get_fused_adam_class():\n    """"""\n    Look for the FusedAdam optimizer from apex. We first try to load the\n    ""contrib"" interface, which is a bit faster than the main interface,\n    but is technically deprecated.\n    """"""\n    try:\n        # The ""deprecated"" interface in recent versions of apex is a bit\n        # faster than the main interface, since we don\'t use the apex\n        # optimizer. This can be installed by passing the\n        # `--deprecated_fused_adam` option when building apex.\n        global fused_adam_cuda\n        import importlib\n        fused_adam_cuda = importlib.import_module(""fused_adam_cuda"")\n        return FusedAdamV1\n    except ImportError:\n        try:\n            # fallback to the newer interface\n            from apex.optimizers import FusedAdam as _FusedAdam  # noqa\n            from apex.multi_tensor_apply import multi_tensor_applier\n            if multi_tensor_applier.available:\n                return FusedAdamV2\n        except ImportError:\n            pass\n    return None\n\n\nclass FusedAdamV1(torch.optim.Optimizer):\n    """"""\n    Implements Adam algorithm. Currently GPU-only. Requires Apex to be installed via\n    ``python setup.py install --cuda_ext --cpp_ext``.\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Compared to the original version in Apex, the fairseq version casts grads\n    and params to FP32 internally to support ``--memory-efficient-fp16``.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in FusedAdam!\n        eps_inside_sqrt (boolean, optional): in the \'update parameters\' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n    .. _Adam: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(self, params,\n                 lr=1e-3, bias_correction=True,\n                 betas=(0.9, 0.999), eps=1e-8, eps_inside_sqrt=False,\n                 weight_decay=0., max_grad_norm=0., amsgrad=False):\n        global fused_adam_cuda\n        import importlib\n        fused_adam_cuda = importlib.import_module(""fused_adam_cuda"")\n\n        if amsgrad:\n            raise RuntimeError(\'FusedAdam does not support the AMSGrad variant.\')\n        defaults = {\n            \'lr\': lr,\n            \'bias_correction\': bias_correction,\n            \'betas\': betas,\n            \'eps\': eps,\n            \'weight_decay\': weight_decay,\n            \'max_grad_norm\': max_grad_norm,\n        }\n        super().__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return True\n\n    def step(self, closure=None, grads=None, scale=1., grad_norms=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients. (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if grads is None:\n            grads_group = [None] * len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0]) != list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        if grad_norms is None:\n            grad_norms = [None]*len(self.param_groups)\n\n        for group, grads_this_group, grad_norm in zip(self.param_groups, grads_group, grad_norms):\n            if grads_this_group is None:\n                grads_this_group = [None]*len(group[\'params\'])\n\n            # compute combined scale factor for this group\n            combined_scale = scale\n            if group.get(\'max_grad_norm\', 0) > 0:\n                # norm is in fact norm*scale\n                clip = ((grad_norm / scale) + 1e-6) / group[\'max_grad_norm\']\n                if clip > 1:\n                    combined_scale = clip * scale\n\n            bias_correction = 1 if group.get(\'bias_correction\', 1) else 0\n\n            for p, grad in zip(group[\'params\'], grads_this_group):\n                # note: p.grad should not ever be set for correct\n                # operation of mixed precision optimizer that sometimes\n                # sends None gradients\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'FusedAdam does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].to(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].to(p_data_fp32)\n\n                exp_avg = state[\'exp_avg\']\n                exp_avg_sq = state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                out_p = p.data\n                with torch.cuda.device(p.device):\n                    fused_adam_cuda.adam(p_data_fp32,\n                                         out_p,\n                                         exp_avg,\n                                         exp_avg_sq,\n                                         grad,\n                                         group[\'lr\'],\n                                         beta1,\n                                         beta2,\n                                         group[\'eps\'],\n                                         combined_scale,\n                                         state[\'step\'],\n                                         self.eps_mode,\n                                         bias_correction,\n                                         group[\'weight_decay\'])\n\n        return loss\n\n\ntry:\n    from apex.optimizers import FusedAdam\n    from apex.multi_tensor_apply import multi_tensor_applier\n\n    class FusedAdamV2(FusedAdam):\n        """"""\n        Compared to the original version in Apex, the fairseq version casts grads\n        and params to FP32 internally to support ``--memory-efficient-fp16``.\n        """"""\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            if not hasattr(self, \'multi_tensor_adam\'):\n                raise Exception(\'Apex installation is outdated. Please install an updated version of apex.\')\n\n        @property\n        def supports_memory_efficient_fp16(self):\n            return True\n\n        @property\n        def supports_flat_params(self):\n            return True\n\n        def step(self, closure=None, grads=None, output_params=None, scale=None, grad_norms=None):\n            """"""Performs a single optimization step.""""""\n            loss = None\n            if closure is not None:\n                loss = closure()\n\n            for group in self.param_groups:\n                bias_correction = 1 if group[\'bias_correction\'] else 0\n                beta1, beta2 = group[\'betas\']\n\n                # assume same step across group now to simplify things\n                # per parameter step can be easily support by making it tensor, or pass list into kernel\n                if \'step\' in group:\n                    group[\'step\'] += 1\n                else:\n                    group[\'step\'] = 1\n\n                # create lists for multi-tensor apply\n                g_16, p_16, orig_p_16, m_16, v_16 = [], [], [], [], []\n                g_32, p_32, m_32, v_32 = [], [], [], []\n\n                for p in group[\'params\']:\n                    if p.grad is None:\n                        continue\n                    if p.grad.data.is_sparse:\n                        raise RuntimeError(\n                            \'FusedAdam does not support sparse gradients, \'\n                            \'please consider SparseAdam instead\'\n                        )\n\n                    state = self.state[p]\n                    # State initialization\n                    if len(state) == 0:\n                        # Exponential moving average of gradient values\n                        state[\'exp_avg\'] = torch.zeros_like(p.data, dtype=torch.float)\n                        # Exponential moving average of squared gradient values\n                        state[\'exp_avg_sq\'] = torch.zeros_like(p.data, dtype=torch.float)\n                    else:\n                        state[\'exp_avg\'] = state[\'exp_avg\'].to(device=p.data.device, dtype=torch.float)\n                        state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].to(device=p.data.device, dtype=torch.float)\n\n                    if p.dtype == torch.float16:\n                        g_16.append(p.grad.data.float())\n                        p_16.append(p.data.float())\n                        orig_p_16.append(p.data)\n                        m_16.append(state[\'exp_avg\'])\n                        v_16.append(state[\'exp_avg_sq\'])\n                    elif p.dtype == torch.float32:\n                        g_32.append(p.grad.data)\n                        p_32.append(p.data)\n                        m_32.append(state[\'exp_avg\'])\n                        v_32.append(state[\'exp_avg_sq\'])\n                    else:\n                        raise RuntimeError(\'FusedAdam only support fp16 and fp32.\')\n\n                with torch.cuda.device(p.device):\n                    if(len(g_16) > 0):\n                        multi_tensor_applier(self.multi_tensor_adam,\n                                             self._dummy_overflow_buf,\n                                             [g_16, p_16, m_16, v_16],\n                                             group[\'lr\'],\n                                             beta1,\n                                             beta2,\n                                             group[\'eps\'],\n                                             group[\'step\'],\n                                             self.adam_w_mode,\n                                             bias_correction,\n                                             group[\'weight_decay\'])\n                        for orig_p, p in zip(orig_p_16, p_16):\n                            orig_p.copy_(p.data)\n                    if(len(g_32) > 0):\n                        multi_tensor_applier(self.multi_tensor_adam,\n                                             self._dummy_overflow_buf,\n                                             [g_32, p_32, m_32, v_32],\n                                             group[\'lr\'],\n                                             beta1,\n                                             beta2,\n                                             group[\'eps\'],\n                                             group[\'step\'],\n                                             self.adam_w_mode,\n                                             bias_correction,\n                                             group[\'weight_decay\'])\n\n            return loss\nexcept ImportError:\n    pass\n'"
fairseq/optim/fused_lamb.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.optim import FairseqOptimizer, register_optimizer\n\n\n@register_optimizer(\'lamb\')\nclass FairseqLAMB(FairseqOptimizer):\n    """"""LAMB optimizer.""""""\n\n    def __init__(self, args, params):\n        super().__init__(args)\n        try:\n            from apex.optimizers import FusedLAMB\n            self._optimizer = FusedLAMB(params, **self.optimizer_config)\n        except ImportError:\n            raise ImportError(\'Please install apex to use LAMB optimizer\')\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--lamb-betas\', default=\'(0.9, 0.999)\', metavar=\'B\',\n                            help=\'betas for LAMB optimizer\')\n        parser.add_argument(\'--lamb-eps\', type=float, default=1e-8, metavar=\'D\',\n                            help=\'epsilon for LAMB optimizer\')\n        parser.add_argument(\'--weight-decay\', \'--wd\', default=0.0, type=float, metavar=\'WD\',\n                            help=\'weight decay\')\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        """"""\n        return {\n            \'lr\': self.args.lr[0],\n            \'betas\': eval(self.args.lamb_betas),\n            \'eps\': self.args.lamb_eps,\n            \'weight_decay\': self.args.weight_decay,\n        }\n\n    @property\n    def supports_flat_params(self):\n        return False\n'"
fairseq/optim/nag.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nfrom . import FairseqOptimizer, register_optimizer\n\n\n@register_optimizer(\'nag\')\nclass FairseqNAG(FairseqOptimizer):\n    def __init__(self, args, params):\n        super().__init__(args)\n        self._optimizer = NAG(params, **self.optimizer_config)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--momentum\', default=0.99, type=float, metavar=\'M\',\n                            help=\'momentum factor\')\n        parser.add_argument(\'--weight-decay\', \'--wd\', default=0.0, type=float, metavar=\'WD\',\n                            help=\'weight decay\')\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        """"""\n        return {\n            \'lr\': self.args.lr[0],\n            \'momentum\': self.args.momentum,\n            \'weight_decay\': self.args.weight_decay,\n        }\n\n\nclass NAG(Optimizer):\n    def __init__(self, params, lr=required, momentum=0, weight_decay=0):\n        defaults = dict(lr=lr, lr_old=lr, momentum=momentum, weight_decay=weight_decay)\n        super(NAG, self).__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return True\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            lr = group[\'lr\']\n            lr_old = group.get(\'lr_old\', lr)\n            lr_correct = lr / lr_old\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n\n                p_data_fp32 = p.data\n                if p_data_fp32.dtype in {torch.float16, torch.bfloat16}:\n                    p_data_fp32 = p_data_fp32.float()\n\n                d_p = p.grad.data.float()\n                param_state = self.state[p]\n                if \'momentum_buffer\' not in param_state:\n                    param_state[\'momentum_buffer\'] = torch.zeros_like(d_p)\n                else:\n                    param_state[\'momentum_buffer\'] = param_state[\'momentum_buffer\'].to(d_p)\n\n                buf = param_state[\'momentum_buffer\']\n\n                if weight_decay != 0:\n                    p_data_fp32.mul_(1 - lr * weight_decay)\n                p_data_fp32.add_(buf, alpha=momentum * momentum * lr_correct)\n                p_data_fp32.add_(d_p, alpha=-(1 + momentum) * lr)\n\n                buf.mul_(momentum * lr_correct).add_(d_p, alpha=-lr)\n\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p.data.copy_(p_data_fp32)\n\n            group[\'lr_old\'] = lr\n\n        return loss\n'"
fairseq/optim/sgd.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.optim\n\nfrom . import FairseqOptimizer, register_optimizer\n\n\n@register_optimizer(\'sgd\')\nclass SGD(FairseqOptimizer):\n    def __init__(self, args, params):\n        super().__init__(args)\n        self._optimizer = torch.optim.SGD(params, **self.optimizer_config)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add optimizer-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--momentum\', default=0.0, type=float, metavar=\'M\',\n                            help=\'momentum factor\')\n        parser.add_argument(\'--weight-decay\', \'--wd\', default=0.0, type=float, metavar=\'WD\',\n                            help=\'weight decay\')\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        """"""\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        """"""\n        return {\n            \'lr\': self.args.lr[0],\n            \'momentum\': self.args.momentum,\n            \'weight_decay\': self.args.weight_decay,\n        }\n\n    @property\n    def supports_flat_params(self):\n        return True\n'"
fairseq/tasks/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport importlib\nimport os\n\nfrom .fairseq_task import FairseqTask\n\nTASK_REGISTRY = {}\nTASK_CLASS_NAMES = set()\n\n\ndef setup_task(args, **kwargs):\n    return TASK_REGISTRY[args.task].setup_task(args, **kwargs)\n\n\ndef register_task(name):\n    """"""\n    New tasks can be added to fairseq with the\n    :func:`~fairseq.tasks.register_task` function decorator.\n\n    For example::\n\n        @register_task(\'classification\')\n        class ClassificationTask(FairseqTask):\n            (...)\n\n    .. note::\n\n        All Tasks must implement the :class:`~fairseq.tasks.FairseqTask`\n        interface.\n\n    Please see the\n\n    Args:\n        name (str): the name of the task\n    """"""\n\n    def register_task_cls(cls):\n        if name in TASK_REGISTRY:\n            raise ValueError(\'Cannot register duplicate task ({})\'.format(name))\n        if not issubclass(cls, FairseqTask):\n            raise ValueError(\'Task ({}: {}) must extend FairseqTask\'.format(name, cls.__name__))\n        if cls.__name__ in TASK_CLASS_NAMES:\n            raise ValueError(\'Cannot register task with duplicate class name ({})\'.format(cls.__name__))\n        TASK_REGISTRY[name] = cls\n        TASK_CLASS_NAMES.add(cls.__name__)\n        return cls\n\n    return register_task_cls\n\n\ndef get_task(name):\n    return TASK_REGISTRY[name]\n\n\n# automatically import any Python files in the tasks/ directory\ntasks_dir = os.path.dirname(__file__)\nfor file in os.listdir(tasks_dir):\n    path = os.path.join(tasks_dir, file)\n    if (\n        not file.startswith(\'_\')\n        and not file.startswith(\'.\')\n        and (file.endswith(\'.py\') or os.path.isdir(path))\n    ):\n        task_name = file[:file.find(\'.py\')] if file.endswith(\'.py\') else file\n        importlib.import_module(\'fairseq.tasks.\' + task_name)\n\n        # expose `task_parser` for sphinx\n        if task_name in TASK_REGISTRY:\n            parser = argparse.ArgumentParser(add_help=False)\n            group_task = parser.add_argument_group(\'Task name\')\n            # fmt: off\n            group_task.add_argument(\'--task\', metavar=task_name,\n                                    help=\'Enable this task with: ``--task=\' + task_name + \'``\')\n            # fmt: on\n            group_args = parser.add_argument_group(\'Additional command-line arguments\')\n            TASK_REGISTRY[task_name].add_args(group_args)\n            globals()[task_name + \'_parser\'] = parser\n'"
fairseq/tasks/audio_pretraining.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\n\nfrom fairseq.data import FileAudioDataset\nfrom . import FairseqTask, register_task\n\n\n@register_task(\'audio_pretraining\')\nclass AudioPretrainingTask(FairseqTask):\n    """"""\n\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', help=\'path to data directory\')\n        parser.add_argument(\'--sample-rate\', default=16000, type=int,\n                            help=\'target sample rate. audio files will be up/down sampled to this rate\')\n        parser.add_argument(\'--max-sample-size\', default=None, type=int,\n                            help=\'max sample size to crop to for batching. default = min sample length\')\n        parser.add_argument(\'--min-sample-size\', default=None, type=int,\n                            help=\'min sample size to crop to for batching. default = same as --max-sample-size\')\n\n    def __init__(self, args):\n        super().__init__(args)\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        """"""\n        return cls(args)\n\n    def load_dataset(self, split, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n\n        manifest = os.path.join(self.args.data, \'{}.tsv\'.format(split))\n        self.datasets[split] = FileAudioDataset(manifest,\n                                                 sample_rate=self.args.sample_rate,\n                                                 max_sample_size=self.args.max_sample_size,\n                                                 min_sample_size=self.args.min_sample_size)\n\n    @property\n    def target_dictionary(self):\n        """"""Return the :class:`~fairseq.data.Dictionary` for the language\n        model.""""""\n        return None\n'"
fairseq/tasks/cross_lingual_lm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\nimport itertools\nimport logging\nimport os\n\nimport numpy as np\n\nfrom fairseq import tokenizer\nfrom fairseq.data.legacy.masked_lm_dictionary import MaskedLMDictionary\n\nfrom fairseq.data import (\n    Dictionary,\n    ConcatDataset,\n    data_utils,\n    TokenBlockDataset,\n)\nfrom fairseq.data.legacy.masked_lm_dataset import MaskedLMDataset\nfrom fairseq.data.multi_corpus_sampled_dataset import MultiCorpusSampledDataset\nfrom fairseq.tasks import FairseqTask, register_task\nfrom fairseq import utils\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'cross_lingual_lm\')\nclass CrossLingualLMTask(FairseqTask):\n    """"""\n    Task for training cross-lingual language models.\n\n    For more details look at: https://arxiv.org/pdf/1901.07291.pdf\n\n    Args:\n        dictionary (Dictionary): the dictionary for the input of the task\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', help=\'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner\')\n        parser.add_argument(\'--tokens-per-sample\', default=512, type=int,\n                            help=\'max number of total tokens over all segments\'\n                                 \' per sample\')\n        parser.add_argument(\'--monolingual-langs\', default=\'en\', type=str,\n                            help=\'comma separated list of languages for which we\'\n                                 \' want to train XLM on\')\n        parser.add_argument(\'--shuffle\', action=\'store_true\',\n                            help=\'shuffle each monolingual dataset while\'\n                            \' training\')\n\n    def __init__(self, args, dictionary):\n        super().__init__(args)\n        self.dictionary = dictionary\n        self.seed = args.seed\n        self.distributed_world_size = args.distributed_world_size\n        self.langs2id = self._lang_to_id(args.monolingual_langs)\n\n    def _lang_to_id(\n            self,\n            languages: str\n    ):\n        """"""\n        Build a map from languages to ids. These ids are used as segment labels\n        for cross-lingual LM training.\n        """"""\n        lang2id = {}\n        langs = [l.strip() for l in languages.split(\',\')]\n        for id, lang in enumerate(langs):\n            lang2id[lang] = id\n        return lang2id\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        return MaskedLMDictionary.load(filename)\n\n    @classmethod\n    def build_dictionary(cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8):\n        d = MaskedLMDictionary()\n        for filename in filenames:\n            Dictionary.add_file_to_dictionary(filename, d, tokenizer.tokenize_line, workers)\n        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n        return d\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task.""""""\n        dictionary = MaskedLMDictionary.load(os.path.join(args.data, \'dict.txt\'))\n        logger.info(\'dictionary: {} types\'.format(len(dictionary)))\n        return cls(args, dictionary)\n\n    def _load_single_lang_dataset(self, split, epoch):\n        loaded_datasets = []\n\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n\n        for k in itertools.count():\n            split_k = split + (str(k) if k > 0 else \'\')\n            path = os.path.join(data_path, split_k)\n\n            ds = data_utils.load_indexed_dataset(path, self.dictionary, self.args.dataset_impl)\n            if ds is None:\n                if k > 0:\n                    break\n                else:\n                    raise FileNotFoundError(\'Dataset not found: {} ({})\'.format(split, data_path))\n\n            # Since we append each block with the classification_token,\n            # we need to effectively create blocks of length\n            # tokens_per_sample-1\n            loaded_datasets.append(\n                TokenBlockDataset(\n                    ds, ds.sizes, self.args.tokens_per_sample - 1,\n                    pad=self.dictionary.pad(), eos=self.dictionary.eos(),\n                )\n            )\n\n            logger.info(\'{} {} {} examples\'.format(data_path, split_k, len(loaded_datasets[-1])))\n\n        if len(loaded_datasets) == 1:\n            dataset = loaded_datasets[0]\n            sizes = dataset.sizes\n        else:\n            dataset = ConcatDataset(loaded_datasets)\n            sizes = np.concatenate([ds.sizes for ds in loaded_datasets])\n\n        return dataset, sizes\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        dataset_map = OrderedDict()\n\n        for lang in self.langs2id.keys():\n            # Datasets are expected to be in ""split.lang"" format (Eg: train.en)\n            language_split = \'{}.{}\'.format(split, lang)\n\n            block_dataset, sizes = self._load_single_lang_dataset(split=language_split, epoch=epoch)\n\n            dataset_map[lang] = MaskedLMDataset(\n                dataset=block_dataset,\n                sizes=sizes,\n                vocab=self.dictionary,\n                pad_idx=self.dictionary.pad(),\n                mask_idx=self.dictionary.mask(),\n                classif_token_idx=self.dictionary.eos(),\n                sep_token_idx=self.dictionary.eos(),\n                shuffle=getattr(self.args, \'shuffle\', False),\n                has_pairs=False,\n                segment_id=self.langs2id[lang],\n                seed=self.seed,\n            )\n\n        self.datasets[split] = MultiCorpusSampledDataset(dataset_map)\n        logger.info(\'{} {} {} examples\'.format(\n            utils.split_paths(self.args.data)[epoch - 1], split, len(self.datasets[split]))\n        )\n'"
fairseq/tasks/denoising.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\n\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,\n    AppendTokenDataset,\n    DenoisingDataset,\n    PrependTokenDataset,\n    StripTokenDataset,\n    TokenBlockDataset,\n)\nfrom fairseq.data.encoders.utils import get_whole_word_mask\nfrom fairseq.tasks import FairseqTask, register_task\nfrom fairseq import utils\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'denoising\')\nclass DenoisingTask(FairseqTask):\n    """"""\n    Denoising task for applying sequence to sequence denoising. (ie. BART)\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', help=\'path to data directory\')\n        parser.add_argument(\'--tokens-per-sample\', default=512, type=int,\n                            help=\'max number of total tokens over all segments\'\n                                 \' per sample for dataset\')\n        parser.add_argument(\n            \'--sample-break-mode\', default=""complete_doc"", type=str,\n            help=\'mode for breaking sentence\',\n        )\n        parser.add_argument(\n            \'--mask\', default=0.0, type=float,\n            help=\'fraction of words/subwords that will be masked\',\n        )\n        parser.add_argument(\n            \'--mask-random\', default=0.0, type=float,\n            help=\'instead of using [MASK], use random token this often\'\n        )\n        parser.add_argument(\n            \'--insert\', default=0.0, type=float,\n            help=\'insert this percentage of additional random tokens\',\n        )\n        parser.add_argument(\n            \'--permute\', default=0.0, type=float,\n            help=\'take this proportion of subwords and permute them\',\n        )\n        parser.add_argument(\n            \'--rotate\', default=0.5, type=float,\n            help=\'rotate this proportion of inputs\',\n        )\n        parser.add_argument(\n            \'--poisson-lambda\', default=3.0, type=float,\n            help=\'randomly shuffle sentences for this proportion of inputs\'\n        )\n        parser.add_argument(\n            \'--permute-sentences\', default=0.0, type=float,\n            help=\'shuffle this proportion of sentences in all inputs\'\n        )\n        parser.add_argument(\n            \'--mask-length\', default=""subword"", type=str,\n            choices=[\'subword\', \'word\', \'span-poisson\'],\n            help=\'mask length to choose\'\n        )\n        parser.add_argument(\n            \'--replace-length\', default=-1, type=int,\n            help=\'when masking N tokens, replace with 0, 1, or N tokens (use -1 for N)\'\n        )\n        parser.add_argument(\n            \'--max-source-positions\', default=1024, type=int, metavar=\'N\',\n            help=\'max number of tokens in the source sequence\'\n        )\n        parser.add_argument(\n            \'--max-target-positions\', default=1024, type=int, metavar=\'N\',\n            help=\'max number of tokens in the target sequence\'\n        )\n\n    def __init__(self, args, dictionary):\n        super().__init__(args)\n        self.dictionary = dictionary\n        self.seed = args.seed\n\n        # add mask token\n        self.mask_idx = self.dictionary.add_symbol(\'<mask>\')\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task.\n        """"""\n        dictionary = Dictionary.load(os.path.join(args.data, \'dict.txt\'))\n        logger.info(\'dictionary: {} types\'.format(len(dictionary)))\n        if not hasattr(args, \'shuffle_instance\'):\n            args.shuffle_instance = False\n        return cls(args, dictionary)\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n        split_path = os.path.join(data_path, split)\n\n        dataset = data_utils.load_indexed_dataset(\n            split_path,\n            self.dictionary,\n            self.args.dataset_impl,\n            combine=combine,\n        )\n        if dataset is None:\n            raise FileNotFoundError(\'Dataset not found: {} ({})\'.format(split, split_path))\n\n        dataset = StripTokenDataset(dataset, self.dictionary.eos())\n\n        # create continuous blocks of tokens\n        dataset = TokenBlockDataset(\n                dataset,\n                dataset.sizes,\n                self.args.tokens_per_sample - 2,  # one less for <s> and one for </s>\n                pad=self.dictionary.pad(),\n                eos=self.dictionary.eos(),\n                break_mode=self.args.sample_break_mode,\n                document_sep_len=0\n        )\n\n        # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n        dataset = PrependTokenDataset(dataset, self.source_dictionary.bos())\n        dataset = AppendTokenDataset(dataset, self.source_dictionary.eos())\n\n        mask_whole_words = get_whole_word_mask(self.args, self.source_dictionary) \\\n            if self.args.mask_length != \'subword\' else None\n\n        self.datasets[split] = DenoisingDataset(\n            dataset, dataset.sizes, self.dictionary, self.mask_idx,\n            mask_whole_words, shuffle=self.args.shuffle_instance,\n            seed=self.seed, args=self.args\n        )\n        logger.info(\n            ""Split: {0}, Loaded {1} samples of denoising_dataset"".format(\n                split,\n                len(self.datasets[split]),\n            )\n        )\n\n    def max_positions(self):\n        """"""Return the max sentence length allowed by the task.""""""\n        return (self.args.max_source_positions, self.args.max_target_positions)\n\n    @property\n    def source_dictionary(self):\n        """"""Return the source :class:`~fairseq.data.Dictionary`.""""""\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        """"""Return the target :class:`~fairseq.data.Dictionary`.""""""\n        return self.dictionary\n'"
fairseq/tasks/fairseq_task.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport warnings\n\nimport torch\n\nfrom fairseq import metrics, search, tokenizer, utils\nfrom fairseq.data import data_utils, FairseqDataset, iterators, Dictionary\n\n\nclass FairseqTask(object):\n    """"""\n    Tasks store dictionaries and provide helpers for loading/iterating over\n    Datasets, initializing the Model/Criterion and calculating the loss.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        pass\n\n    @staticmethod\n    def logging_outputs_can_be_summed(criterion) -> bool:\n        """"""\n        Whether the logging outputs returned by `train_step` and `valid_step` can\n        be summed across workers prior to calling `aggregate_logging_outputs`.\n        Setting this to True will improves distributed training speed.\n        """"""\n        return criterion.logging_outputs_can_be_summed()\n\n    def __init__(self, args):\n        self.args = args\n        self.datasets = {}\n        self.dataset_to_epoch_iter = {}\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        """"""Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        """"""\n        return Dictionary.load(filename)\n\n    @classmethod\n    def build_dictionary(\n        cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8\n    ):\n        """"""Build the dictionary\n\n        Args:\n            filenames (list): list of filenames\n            workers (int): number of concurrent workers\n            threshold (int): defines the minimum word count\n            nwords (int): defines the total number of words in the final dictionary,\n                including special symbols\n            padding_factor (int): can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        """"""\n        d = Dictionary()\n        for filename in filenames:\n            Dictionary.add_file_to_dictionary(\n                filename, d, tokenizer.tokenize_line, workers\n            )\n        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n        return d\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        """"""\n        return cls(args, **kwargs)\n\n    def load_dataset(self, split, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        raise NotImplementedError\n\n    def dataset(self, split):\n        """"""\n        Return a loaded dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n\n        Returns:\n            a :class:`~fairseq.data.FairseqDataset` corresponding to *split*\n        """"""\n        from fairseq.data import FairseqDataset\n\n        if split not in self.datasets:\n            raise KeyError(""Dataset not loaded: "" + split)\n        if not isinstance(self.datasets[split], FairseqDataset):\n            raise TypeError(""Datasets are expected to be of type FairseqDataset"")\n        return self.datasets[split]\n\n    def get_batch_iterator(\n        self,\n        dataset,\n        max_tokens=None,\n        max_sentences=None,\n        max_positions=None,\n        ignore_invalid_inputs=False,\n        required_batch_size_multiple=1,\n        seed=1,\n        num_shards=1,\n        shard_id=0,\n        num_workers=0,\n        epoch=1\n    ):\n        """"""\n        Get an iterator that yields batches of data from the given dataset.\n\n        Args:\n            dataset (~fairseq.data.FairseqDataset): dataset to batch\n            max_tokens (int, optional): max number of tokens in each batch\n                (default: None).\n            max_sentences (int, optional): max number of sentences in each\n                batch (default: None).\n            max_positions (optional): max sentence length supported by the\n                model (default: None).\n            ignore_invalid_inputs (bool, optional): don\'t raise Exception for\n                sentences that are too long (default: False).\n            required_batch_size_multiple (int, optional): require batch size to\n                be a multiple of N (default: 1).\n            seed (int, optional): seed for random number generator for\n                reproducibility (default: 1).\n            num_shards (int, optional): shard the data iterator into N\n                shards (default: 1).\n            shard_id (int, optional): which shard of the data iterator to\n                return (default: 0).\n            num_workers (int, optional): how many subprocesses to use for data\n                loading. 0 means the data will be loaded in the main process\n                (default: 0).\n            epoch (int, optional): the epoch to start the iterator from\n                (default: 1).\n        Returns:\n            ~fairseq.iterators.EpochBatchIterator: a batched iterator over the\n                given dataset split\n        """"""\n        # For default fairseq task, return same iterator across epochs\n        # as datasets are not dynamic, can be overridden in task specific\n        # setting.\n        if dataset in self.dataset_to_epoch_iter:\n            return self.dataset_to_epoch_iter[dataset]\n\n        assert isinstance(dataset, FairseqDataset)\n\n        # initialize the dataset with the correct starting epoch\n        dataset.set_epoch(epoch)\n\n        # get indices ordered by example size\n        with data_utils.numpy_seed(seed):\n            indices = dataset.ordered_indices()\n\n        # filter examples that are too large\n        if max_positions is not None:\n            indices = data_utils.filter_by_size(\n                indices,\n                dataset,\n                max_positions,\n                raise_exception=(not ignore_invalid_inputs),\n            )\n\n        # create mini-batches with given size constraints\n        batch_sampler = data_utils.batch_by_size(\n            indices,\n            dataset.num_tokens,\n            max_tokens=max_tokens,\n            max_sentences=max_sentences,\n            required_batch_size_multiple=required_batch_size_multiple,\n        )\n\n        # return a reusable, sharded iterator\n        epoch_iter = iterators.EpochBatchIterator(\n            dataset=dataset,\n            collate_fn=dataset.collater,\n            batch_sampler=batch_sampler,\n            seed=seed,\n            num_shards=num_shards,\n            shard_id=shard_id,\n            num_workers=num_workers,\n            epoch=epoch,\n            buffer_size=getattr(self.args, \'data_buffer_size\', 0)\n        )\n        self.dataset_to_epoch_iter[dataset] = epoch_iter\n        return epoch_iter\n\n    def build_model(self, args):\n        """"""\n        Build the :class:`~fairseq.models.BaseFairseqModel` instance for this\n        task.\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n\n        Returns:\n            a :class:`~fairseq.models.BaseFairseqModel` instance\n        """"""\n        from fairseq import models, quantization_utils\n        model = models.build_model(args, self)\n        if getattr(args, \'tpu\', False):\n            model.prepare_for_tpu_()\n        model = quantization_utils.quantize_model_scalar(model, args)\n        return model\n\n    def build_criterion(self, args):\n        """"""\n        Build the :class:`~fairseq.criterions.FairseqCriterion` instance for\n        this task.\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n\n        Returns:\n            a :class:`~fairseq.criterions.FairseqCriterion` instance\n        """"""\n        from fairseq import criterions\n\n        return criterions.build_criterion(args, self)\n\n    def build_generator(self, models, args):\n        if getattr(args, ""score_reference"", False):\n            from fairseq.sequence_scorer import SequenceScorer\n\n            return SequenceScorer(\n                self.target_dictionary,\n                compute_alignment=getattr(args, ""print_alignment"", False),\n            )\n\n        from fairseq.sequence_generator import (\n            SequenceGenerator,\n            SequenceGeneratorWithAlignment,\n        )\n\n        # Choose search strategy. Defaults to Beam Search.\n        sampling = getattr(args, ""sampling"", False)\n        sampling_topk = getattr(args, ""sampling_topk"", -1)\n        sampling_topp = getattr(args, ""sampling_topp"", -1.0)\n        diverse_beam_groups = getattr(args, ""diverse_beam_groups"", -1)\n        diverse_beam_strength = getattr(args, ""diverse_beam_strength"", 0.5)\n        match_source_len = getattr(args, ""match_source_len"", False)\n        diversity_rate = getattr(args, ""diversity_rate"", -1)\n        if (\n            sum(\n                int(cond)\n                for cond in [\n                    sampling,\n                    diverse_beam_groups > 0,\n                    match_source_len,\n                    diversity_rate > 0,\n                ]\n            )\n            > 1\n        ):\n            raise ValueError(""Provided Search parameters are mutually exclusive."")\n        assert sampling_topk < 0 or sampling, ""--sampling-topk requires --sampling""\n        assert sampling_topp < 0 or sampling, ""--sampling-topp requires --sampling""\n\n        if sampling:\n            search_strategy = search.Sampling(\n                self.target_dictionary, sampling_topk, sampling_topp\n            )\n        elif diverse_beam_groups > 0:\n            search_strategy = search.DiverseBeamSearch(\n                self.target_dictionary, diverse_beam_groups, diverse_beam_strength\n            )\n        elif match_source_len:\n            # this is useful for tagging applications where the output\n            # length should match the input length, so we hardcode the\n            # length constraints for simplicity\n            search_strategy = search.LengthConstrainedBeamSearch(\n                self.target_dictionary,\n                min_len_a=1,\n                min_len_b=0,\n                max_len_a=1,\n                max_len_b=0,\n            )\n        elif diversity_rate > -1:\n            search_strategy = search.DiverseSiblingsSearch(\n                self.target_dictionary, diversity_rate\n            )\n        else:\n            search_strategy = search.BeamSearch(self.target_dictionary)\n\n        if getattr(args, ""print_alignment"", False):\n            seq_gen_cls = SequenceGeneratorWithAlignment\n        else:\n            seq_gen_cls = SequenceGenerator\n\n        return seq_gen_cls(\n            models,\n            self.target_dictionary,\n            beam_size=getattr(args, ""beam"", 5),\n            max_len_a=getattr(args, ""max_len_a"", 0),\n            max_len_b=getattr(args, ""max_len_b"", 200),\n            min_len=getattr(args, ""min_len"", 1),\n            normalize_scores=(not getattr(args, ""unnormalized"", False)),\n            len_penalty=getattr(args, ""lenpen"", 1),\n            unk_penalty=getattr(args, ""unkpen"", 0),\n            temperature=getattr(args, ""temperature"", 1.0),\n            match_source_len=getattr(args, ""match_source_len"", False),\n            no_repeat_ngram_size=getattr(args, ""no_repeat_ngram_size"", 0),\n            search_strategy=search_strategy,\n        )\n\n    def train_step(\n        self, sample, model, criterion, optimizer, update_num, ignore_grad=False\n    ):\n        """"""\n        Do forward and backward, and return the loss as computed by *criterion*\n        for the given *model* and *sample*.\n\n        Args:\n            sample (dict): the mini-batch. The format is defined by the\n                :class:`~fairseq.data.FairseqDataset`.\n            model (~fairseq.models.BaseFairseqModel): the model\n            criterion (~fairseq.criterions.FairseqCriterion): the criterion\n            optimizer (~fairseq.optim.FairseqOptimizer): the optimizer\n            update_num (int): the current update\n            ignore_grad (bool): multiply loss by 0 if this is set to True\n\n        Returns:\n            tuple:\n                - the loss\n                - the sample size, which is used as the denominator for the\n                  gradient\n                - logging outputs to display while training\n        """"""\n        model.train()\n        model.set_num_updates(update_num)\n        loss, sample_size, logging_output = criterion(model, sample)\n        if ignore_grad:\n            loss *= 0\n        optimizer.backward(loss)\n        return loss, sample_size, logging_output\n\n    def valid_step(self, sample, model, criterion):\n        model.eval()\n        with torch.no_grad():\n            loss, sample_size, logging_output = criterion(model, sample)\n        return loss, sample_size, logging_output\n\n    def inference_step(self, generator, models, sample, prefix_tokens=None):\n        with torch.no_grad():\n            return generator.generate(models, sample, prefix_tokens=prefix_tokens)\n\n    def begin_epoch(self, epoch, model):\n        """"""Hook function called before the start of each epoch.""""""\n        pass\n\n    def aggregate_logging_outputs(self, logging_outputs, criterion):\n        """"""[deprecated] Aggregate logging outputs from data parallel training.""""""\n        utils.deprecation_warning(\n            ""The aggregate_logging_outputs API is deprecated. ""\n            ""Please use the reduce_metrics API instead.""\n        )\n        with metrics.aggregate() as agg:\n            self.reduce_metrics(logging_outputs, criterion)\n            return agg.get_smoothed_values()\n\n    def reduce_metrics(self, logging_outputs, criterion):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        # backward compatibility for tasks that override aggregate_logging_outputs\n        base_func = FairseqTask.aggregate_logging_outputs\n        self_func = getattr(self, ""aggregate_logging_outputs"").__func__\n        if self_func is not base_func:\n            utils.deprecation_warning(\n                ""Tasks should implement the reduce_metrics API. ""\n                ""Falling back to deprecated aggregate_logging_outputs API.""\n            )\n            agg_logging_outputs = self.aggregate_logging_outputs(\n                logging_outputs, criterion\n            )\n            for k, v in agg_logging_outputs.items():\n                metrics.log_scalar(k, v)\n            return\n\n        if not any(""ntokens"" in log for log in logging_outputs):\n            warnings.warn(\n                ""ntokens not found in Criterion logging outputs, cannot log wpb or wps""\n            )\n        else:\n            ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n            metrics.log_scalar(""wpb"", ntokens, priority=180, round=1)\n            metrics.log_speed(""wps"", ntokens, priority=90, round=1)\n\n        if not any(""nsentences"" in log for log in logging_outputs):\n            warnings.warn(\n                ""nsentences not found in Criterion logging outputs, cannot log bsz""\n            )\n        else:\n            nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n            metrics.log_scalar(""bsz"", nsentences, priority=190, round=1)\n\n        criterion.__class__.reduce_metrics(logging_outputs)\n\n    def max_positions(self):\n        """"""Return the max input length allowed by the task.""""""\n        return None\n\n    @property\n    def source_dictionary(self):\n        """"""Return the source :class:`~fairseq.data.Dictionary` (if applicable\n        for this task).""""""\n        raise NotImplementedError\n\n    @property\n    def target_dictionary(self):\n        """"""Return the target :class:`~fairseq.data.Dictionary` (if applicable\n        for this task).""""""\n        raise NotImplementedError\n'"
fairseq/tasks/language_modeling.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\n\nimport numpy as np\nimport torch\n\nfrom fairseq import utils\nfrom fairseq.data import (\n    AppendTokenDataset,\n    data_utils,\n    Dictionary,\n    IdDataset,\n    MonolingualDataset,\n    NestedDictionaryDataset,\n    NumelDataset,\n    PadDataset,\n    PrependTokenDataset,\n    StripTokenDataset,\n    TokenBlockDataset,\n    TransformEosDataset,\n    TruncatedDictionary,\n)\nfrom fairseq.data.shorten_dataset import maybe_shorten_dataset\nfrom fairseq.tasks import FairseqTask, register_task\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(""language_modeling"")\nclass LanguageModelingTask(FairseqTask):\n    """"""\n    Train a language model.\n\n    Args:\n        dictionary (~fairseq.data.Dictionary): the dictionary for the input of\n            the language model\n        output_dictionary (~fairseq.data.Dictionary): the dictionary for the\n            output of the language model. In most cases it will be the same as\n            *dictionary*, but could possibly be a more limited version of the\n            dictionary (if ``--output-dictionary-size`` is used).\n        targets (List[str]): list of the target types that the language model\n            should predict.  Can be one of ""self"", ""future"", and ""past"".\n            Defaults to ""future"".\n\n    .. note::\n\n        The language modeling task is compatible with :mod:`fairseq-train`,\n        :mod:`fairseq-generate`, :mod:`fairseq-interactive` and\n        :mod:`fairseq-eval-lm`.\n\n    The language modeling task provides the following additional command-line\n    arguments:\n\n    .. argparse::\n        :ref: fairseq.tasks.language_modeling_parser\n        :prog:\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'data\', help=\'path to data directory\')\n        parser.add_argument(\'--sample-break-mode\', default=\'none\',\n                            choices=[\'none\', \'complete\', \'complete_doc\', \'eos\'],\n                            help=\'If omitted or ""none"", fills each sample with tokens-per-sample \'\n                                 \'tokens. If set to ""complete"", splits samples only at the end \'\n                                 \'of sentence, but may include multiple sentences per sample. \'\n                                 \'""complete_doc"" is similar but respects doc boundaries. \'\n                                 \'If set to ""eos"", includes only one sentence per sample.\')\n        parser.add_argument(\'--tokens-per-sample\', default=1024, type=int,\n                            help=\'max number of tokens per sample for LM dataset\')\n        parser.add_argument(\'--output-dictionary-size\', default=-1, type=int,\n                            help=\'limit the size of output dictionary\')\n        parser.add_argument(\'--self-target\', action=\'store_true\',\n                            help=\'include self target\')\n        parser.add_argument(\'--future-target\', action=\'store_true\',\n                            help=\'include future target\')\n        parser.add_argument(\'--past-target\', action=\'store_true\',\n                            help=\'include past target\')\n        parser.add_argument(\'--add-bos-token\', action=\'store_true\',\n                            help=\'prepend beginning of sentence token (<s>)\')\n        parser.add_argument(\'--max-target-positions\', type=int, metavar=\'N\',\n                            help=\'max number of tokens in the target sequence\')\n        parser.add_argument(\'--shorten-method\', default=\'none\',\n                            choices=[\'none\', \'truncate\', \'random_crop\'],\n                            help=\'if not none, shorten sequences that exceed --tokens-per-sample\')\n        parser.add_argument(\'--shorten-data-split-whitelist\', default=\'\',\n                            help=\'comma-separated list of dataset splits to apply shortening to, \'\n                                 \'e.g., ""train,valid"" (default: all dataset splits)\')\n        # fmt: on\n\n    def __init__(self, args, dictionary, output_dictionary=None, targets=None):\n        super().__init__(args)\n        self.dictionary = dictionary\n        self.output_dictionary = output_dictionary or dictionary\n\n        if targets is None:\n            targets = [""future""]\n        self.targets = targets\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        """"""\n        dictionary = None\n        output_dictionary = None\n        if args.data:\n            paths = utils.split_paths(args.data)\n            assert len(paths) > 0\n            dictionary = Dictionary.load(os.path.join(paths[0], ""dict.txt""))\n            logger.info(""dictionary: {} types"".format(len(dictionary)))\n            output_dictionary = dictionary\n            if args.output_dictionary_size >= 0:\n                output_dictionary = TruncatedDictionary(\n                    dictionary, args.output_dictionary_size\n                )\n\n        # upgrade old checkpoints\n        if hasattr(args, ""exclude_self_target""):\n            args.self_target = not args.exclude_self_target\n\n        targets = []\n        if getattr(args, ""self_target"", False):\n            targets.append(""self"")\n        if getattr(args, ""future_target"", False):\n            targets.append(""future"")\n        if getattr(args, ""past_target"", False):\n            targets.append(""past"")\n        if len(targets) == 0:\n            # standard language modeling\n            targets = [""future""]\n\n        return cls(args, dictionary, output_dictionary, targets=targets)\n\n    def build_model(self, args):\n        model = super().build_model(args)\n\n        for target in self.targets:\n            if target not in model.supported_targets:\n                raise ValueError(\n                    ""Unsupported language modeling target: {}"".format(target)\n                )\n\n        return model\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n\n        data_path = paths[(epoch - 1) % len(paths)]\n        split_path = os.path.join(data_path, split)\n\n        dataset = data_utils.load_indexed_dataset(\n            split_path, self.dictionary, self.args.dataset_impl, combine=combine\n        )\n        if dataset is None:\n            raise FileNotFoundError(\n                ""Dataset not found: {} ({})"".format(split, split_path)\n            )\n\n        dataset = maybe_shorten_dataset(\n            dataset,\n            split,\n            self.args.shorten_data_split_whitelist,\n            self.args.shorten_method,\n            self.args.tokens_per_sample,\n            self.args.seed,\n        )\n\n        dataset = TokenBlockDataset(\n            dataset,\n            dataset.sizes,\n            self.args.tokens_per_sample,\n            pad=self.dictionary.pad(),\n            eos=self.dictionary.eos(),\n            break_mode=self.args.sample_break_mode,\n            include_targets=True,\n        )\n\n        add_eos_for_other_targets = (\n            self.args.sample_break_mode is not None\n            and self.args.sample_break_mode != ""none""\n        )\n\n        self.datasets[split] = MonolingualDataset(\n            dataset,\n            dataset.sizes,\n            self.dictionary,\n            self.output_dictionary,\n            add_eos_for_other_targets=add_eos_for_other_targets,\n            shuffle=True,\n            targets=self.targets,\n            add_bos_token=self.args.add_bos_token,\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths, **kwargs):\n        """"""\n        Generate batches for inference. We prepend an eos token to src_tokens\n        (or bos if `--add-bos-token` is set) and we append a <pad> to target.\n        This is convenient both for generation with a prefix and LM scoring.\n        """"""\n        dataset = StripTokenDataset(\n            TokenBlockDataset(\n                src_tokens,\n                src_lengths,\n                block_size=None,  # ignored for ""eos"" break mode\n                pad=self.source_dictionary.pad(),\n                eos=self.source_dictionary.eos(),\n                break_mode=""eos"",\n            ),\n            # remove eos from (end of) target sequence\n            self.source_dictionary.eos(),\n        )\n        src_dataset = PrependTokenDataset(\n            dataset,\n            token=(\n                self.source_dictionary.bos()\n                if getattr(self.args, ""add_bos_token"", False)\n                else self.source_dictionary.eos()\n            ),\n        )\n        tgt_dataset = AppendTokenDataset(\n            dataset,\n            token=self.source_dictionary.pad()\n        )\n        return NestedDictionaryDataset(\n            {\n                ""id"": IdDataset(),\n                ""net_input"": {\n                    ""src_tokens"": PadDataset(src_dataset, pad_idx=self.source_dictionary.pad(), left_pad=False),\n                    ""src_lengths"": NumelDataset(src_dataset, reduce=False),\n                },\n                ""target"": PadDataset(tgt_dataset, pad_idx=self.source_dictionary.pad(), left_pad=False),\n            },\n            sizes=[np.array(src_lengths)],\n        )\n\n    def inference_step(self, generator, models, sample, prefix_tokens=None):\n        with torch.no_grad():\n            # Generation will always be conditioned on bos_token\n            if getattr(self.args, ""add_bos_token"", False):\n                bos_token = self.source_dictionary.bos()\n            else:\n                bos_token = self.source_dictionary.eos()\n\n            # SequenceGenerator doesn\'t use src_tokens directly, we need to\n            # pass the `prefix_tokens` argument instead\n            if prefix_tokens is None and sample[""net_input""][""src_tokens""].nelement():\n                prefix_tokens = sample[""net_input""][""src_tokens""]\n                if prefix_tokens[:, 0].eq(bos_token).all():\n                    prefix_tokens = prefix_tokens[:, 1:]\n\n            return generator.generate(\n                models, sample, prefix_tokens=prefix_tokens, bos_token=bos_token,\n            )\n\n    @property\n    def source_dictionary(self):\n        """"""Return the :class:`~fairseq.data.Dictionary` for the language\n        model.""""""\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        """"""Return the :class:`~fairseq.data.Dictionary` for the language\n        model.""""""\n        return self.output_dictionary\n'"
fairseq/tasks/legacy_masked_lm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport itertools\nimport logging\nimport os\n\nimport numpy as np\n\nfrom fairseq import tokenizer\nfrom fairseq.data import (\n    ConcatDataset,\n    indexed_dataset,\n    data_utils,\n)\n\nfrom fairseq.data import Dictionary\nfrom fairseq.data.legacy.block_pair_dataset import BlockPairDataset\nfrom fairseq.data.legacy.masked_lm_dataset import MaskedLMDataset\nfrom fairseq.data.legacy.masked_lm_dictionary import BertDictionary\nfrom fairseq.tasks import FairseqTask, register_task\nfrom fairseq import utils\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'legacy_masked_lm\')\nclass LegacyMaskedLMTask(FairseqTask):\n    """"""\n    Task for training Masked LM (BERT) model.\n    Args:\n        dictionary (Dictionary): the dictionary for the input of the task\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', help=\'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner\')\n        parser.add_argument(\'--tokens-per-sample\', default=512, type=int,\n                            help=\'max number of total tokens over all segments\'\n                                 \' per sample for BERT dataset\')\n        parser.add_argument(\'--break-mode\', default=""doc"", type=str, help=\'mode for breaking sentence\')\n        parser.add_argument(\'--shuffle-dataset\', action=\'store_true\', default=False)\n\n    def __init__(self, args, dictionary):\n        super().__init__(args)\n        self.dictionary = dictionary\n        self.seed = args.seed\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        return BertDictionary.load(filename)\n\n    @classmethod\n    def build_dictionary(cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8):\n        d = BertDictionary()\n        for filename in filenames:\n            Dictionary.add_file_to_dictionary(filename, d, tokenizer.tokenize_line, workers)\n        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n        return d\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task.\n        """"""\n        paths = utils.split_paths(args.data)\n        assert len(paths) > 0\n        dictionary = BertDictionary.load(os.path.join(paths[0], \'dict.txt\'))\n        logger.info(\'dictionary: {} types\'.format(len(dictionary)))\n\n        return cls(args, dictionary)\n\n    def load_dataset(self, split, epoch=1, combine=False):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        loaded_datasets = []\n\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n        logger.info(""data_path"", data_path)\n\n        for k in itertools.count():\n            split_k = split + (str(k) if k > 0 else \'\')\n            path = os.path.join(data_path, split_k)\n            ds = indexed_dataset.make_dataset(\n                path,\n                impl=self.args.dataset_impl,\n                fix_lua_indexing=True,\n                dictionary=self.dictionary,\n            )\n\n            if ds is None:\n                if k > 0:\n                    break\n                else:\n                    raise FileNotFoundError(\'Dataset not found: {} ({})\'.format(split, data_path))\n\n            with data_utils.numpy_seed(self.seed + k):\n                loaded_datasets.append(\n                    BlockPairDataset(\n                        ds,\n                        self.dictionary,\n                        ds.sizes,\n                        self.args.tokens_per_sample,\n                        break_mode=self.args.break_mode,\n                        doc_break_size=1,\n                    )\n                )\n\n            logger.info(\'{} {} {} examples\'.format(data_path, split_k, len(loaded_datasets[-1])))\n\n            if not combine:\n                break\n\n        if len(loaded_datasets) == 1:\n            dataset = loaded_datasets[0]\n            sizes = dataset.sizes\n        else:\n            dataset = ConcatDataset(loaded_datasets)\n            sizes = np.concatenate([ds.sizes for ds in loaded_datasets])\n\n        self.datasets[split] = MaskedLMDataset(\n            dataset=dataset,\n            sizes=sizes,\n            vocab=self.dictionary,\n            pad_idx=self.dictionary.pad(),\n            mask_idx=self.dictionary.mask(),\n            classif_token_idx=self.dictionary.cls(),\n            sep_token_idx=self.dictionary.sep(),\n            shuffle=self.args.shuffle_dataset,\n            seed=self.seed,\n        )\n'"
fairseq/tasks/masked_lm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\n\nimport numpy as np\n\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,\n    IdDataset,\n    MaskTokensDataset,\n    NestedDictionaryDataset,\n    NumelDataset,\n    NumSamplesDataset,\n    PadDataset,\n    PrependTokenDataset,\n    SortDataset,\n    TokenBlockDataset,\n)\nfrom fairseq.data.shorten_dataset import maybe_shorten_dataset\nfrom fairseq.tasks import FairseqTask, register_task\nfrom fairseq.data.encoders.utils import get_whole_word_mask\nfrom fairseq import utils\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'masked_lm\')\nclass MaskedLMTask(FairseqTask):\n    """"""Task for training masked language models (e.g., BERT, RoBERTa).""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', help=\'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner\')\n        parser.add_argument(\'--sample-break-mode\', default=\'complete\',\n                            choices=[\'none\', \'complete\', \'complete_doc\', \'eos\'],\n                            help=\'If omitted or ""none"", fills each sample with tokens-per-sample \'\n                                 \'tokens. If set to ""complete"", splits samples only at the end \'\n                                 \'of sentence, but may include multiple sentences per sample. \'\n                                 \'""complete_doc"" is similar but respects doc boundaries. \'\n                                 \'If set to ""eos"", includes only one sentence per sample.\')\n        parser.add_argument(\'--tokens-per-sample\', default=512, type=int,\n                            help=\'max number of total tokens over all segments \'\n                                 \'per sample for BERT dataset\')\n        parser.add_argument(\'--mask-prob\', default=0.15, type=float,\n                            help=\'probability of replacing a token with mask\')\n        parser.add_argument(\'--leave-unmasked-prob\', default=0.1, type=float,\n                            help=\'probability that a masked token is unmasked\')\n        parser.add_argument(\'--random-token-prob\', default=0.1, type=float,\n                            help=\'probability of replacing a token with a random token\')\n        parser.add_argument(\'--freq-weighted-replacement\', default=False, action=\'store_true\',\n                            help=\'sample random replacement words based on word frequencies\')\n        parser.add_argument(\'--mask-whole-words\', default=False, action=\'store_true\',\n                            help=\'mask whole words; you may also want to set --bpe\')\n        parser.add_argument(\'--shorten-method\', default=\'none\',\n                            choices=[\'none\', \'truncate\', \'random_crop\'],\n                            help=\'if not none, shorten sequences that exceed --tokens-per-sample\')\n        parser.add_argument(\'--shorten-data-split-whitelist\', default=\'\',\n                            help=\'comma-separated list of dataset splits to apply shortening to, \'\n                                 \'e.g., ""train,valid"" (default: all dataset splits)\')\n\n    def __init__(self, args, dictionary):\n        super().__init__(args)\n        self.dictionary = dictionary\n        self.seed = args.seed\n\n        # add mask token\n        self.mask_idx = dictionary.add_symbol(\'<mask>\')\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        paths = utils.split_paths(args.data)\n        assert len(paths) > 0\n        dictionary = Dictionary.load(os.path.join(paths[0], \'dict.txt\'))\n        logger.info(\'dictionary: {} types\'.format(len(dictionary)))\n        return cls(args, dictionary)\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n        split_path = os.path.join(data_path, split)\n\n        dataset = data_utils.load_indexed_dataset(\n            split_path,\n            self.source_dictionary,\n            self.args.dataset_impl,\n            combine=combine,\n        )\n        if dataset is None:\n            raise FileNotFoundError(\'Dataset not found: {} ({})\'.format(split, split_path))\n\n        dataset = maybe_shorten_dataset(\n            dataset,\n            split,\n            self.args.shorten_data_split_whitelist,\n            self.args.shorten_method,\n            self.args.tokens_per_sample,\n            self.args.seed,\n        )\n\n        # create continuous blocks of tokens\n        dataset = TokenBlockDataset(\n            dataset,\n            dataset.sizes,\n            self.args.tokens_per_sample - 1,  # one less for <s>\n            pad=self.source_dictionary.pad(),\n            eos=self.source_dictionary.eos(),\n            break_mode=self.args.sample_break_mode,\n        )\n        logger.info(\'loaded {} blocks from: {}\'.format(len(dataset), split_path))\n\n        # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n        dataset = PrependTokenDataset(dataset, self.source_dictionary.bos())\n\n        # create masked input and targets\n        mask_whole_words = get_whole_word_mask(self.args, self.source_dictionary) \\\n            if self.args.mask_whole_words else None\n\n        src_dataset, tgt_dataset = MaskTokensDataset.apply_mask(\n            dataset,\n            self.source_dictionary,\n            pad_idx=self.source_dictionary.pad(),\n            mask_idx=self.mask_idx,\n            seed=self.args.seed,\n            mask_prob=self.args.mask_prob,\n            leave_unmasked_prob=self.args.leave_unmasked_prob,\n            random_token_prob=self.args.random_token_prob,\n            freq_weighted_replacement=self.args.freq_weighted_replacement,\n            mask_whole_words=mask_whole_words,\n        )\n\n        with data_utils.numpy_seed(self.args.seed + epoch):\n            shuffle = np.random.permutation(len(src_dataset))\n\n        self.datasets[split] = SortDataset(\n            NestedDictionaryDataset(\n                {\n                    \'id\': IdDataset(),\n                    \'net_input\': {\n                        \'src_tokens\': PadDataset(\n                            src_dataset,\n                            pad_idx=self.source_dictionary.pad(),\n                            left_pad=False,\n                        ),\n                        \'src_lengths\': NumelDataset(src_dataset, reduce=False),\n                    },\n                    \'target\': PadDataset(\n                        tgt_dataset,\n                        pad_idx=self.source_dictionary.pad(),\n                        left_pad=False,\n                    ),\n                    \'nsentences\': NumSamplesDataset(),\n                    \'ntokens\': NumelDataset(src_dataset, reduce=True),\n                },\n                sizes=[src_dataset.sizes],\n            ),\n            sort_order=[\n                shuffle,\n                src_dataset.sizes,\n            ],\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths, sort=True):\n        src_dataset = PadDataset(\n            TokenBlockDataset(\n                src_tokens,\n                src_lengths,\n                self.args.tokens_per_sample - 1,  # one less for <s>\n                pad=self.source_dictionary.pad(),\n                eos=self.source_dictionary.eos(),\n                break_mode=\'eos\',\n            ),\n            pad_idx=self.source_dictionary.pad(),\n            left_pad=False,\n        )\n        src_dataset = PrependTokenDataset(src_dataset, self.source_dictionary.bos())\n        src_dataset = NestedDictionaryDataset(\n            {\n                \'id\': IdDataset(),\n                \'net_input\': {\n                    \'src_tokens\': src_dataset,\n                    \'src_lengths\': NumelDataset(src_dataset, reduce=False),\n                },\n            },\n            sizes=src_lengths,\n        )\n        if sort:\n            src_dataset = SortDataset(src_dataset, sort_order=[src_lengths])\n        return src_dataset\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n'"
fairseq/tasks/multilingual_denoising.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\n\nimport numpy as np\n\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,\n    AppendTokenDataset,\n    ConcatDataset,\n    DenoisingDataset,\n    PrependTokenDataset,\n    ResamplingDataset,\n    SortDataset,\n    TokenBlockDataset,\n)\nfrom .denoising import DenoisingTask\nfrom fairseq.data.encoders.utils import get_whole_word_mask\nfrom fairseq.tasks import register_task\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'multilingual_denoising\')\nclass MultilingualDenoisingTask(DenoisingTask):\n\n    @staticmethod\n    def add_args(parser):\n        DenoisingTask.add_args(parser)\n        parser.add_argument(\'--multilang-sampling-alpha\', type=float, default=1.0,\n                            help=\'smoothing alpha for sample rations across multiple datasets\')\n        parser.add_argument(\'--add-lang-token\', default=False, action=\'store_true\')\n        parser.add_argument(\'--langs\', type=str, help=""language ids we are considering"", default=None)\n        parser.add_argument(\'--no-whole-word-mask-langs\', type=str, default=\'\', metavar=\'N\',\n                            help=\'languages without spacing between words dont support whole word masking\')\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task.\n        """"""\n        paths = args.data.split(\':\')\n        assert len(paths) > 0\n        dictionary = Dictionary.load(os.path.join(paths[0], \'dict.txt\'))\n\n        data_path = paths[0]\n        if args.langs is None:\n            languages = sorted([\n                name for name in os.listdir(data_path)\n                if os.path.isdir(os.path.join(data_path, name))\n            ])\n        else:\n            languages = args.langs.split(\',\')\n\n        if args.add_lang_token:\n            for lang in languages:\n                dictionary.add_symbol(\'[{}]\'.format(lang))\n\n        logger.info(""| dictionary: {} types"".format(len(dictionary)))\n        if not hasattr(args, \'shuffle_instance\'):\n            args.shuffle_instance = False\n        return cls(args, dictionary)\n\n    def __init__(self, args, dictionary):\n        super().__init__(args, dictionary)\n        self.dictionary = dictionary\n        self.seed = args.seed\n\n        # add mask token\n        self.mask_idx = self.dictionary.add_symbol(\'<mask>\')\n        self.langs = args.langs\n        self.args = args\n\n    def _get_sample_prob(self, dataset_lens):\n        """"""\n        Get smoothed sampling porbability by languages. This helps low resource\n        languages by upsampling them.\n        """"""\n        prob = dataset_lens / dataset_lens.sum()\n        smoothed_prob = prob ** self.args.multilang_sampling_alpha\n        smoothed_prob = smoothed_prob / smoothed_prob.sum()\n        return smoothed_prob\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        paths = self.args.data.split(\':\')\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n        split_path = os.path.join(data_path, split)\n\n        if self.langs is None:\n            languages = sorted([\n                name for name in os.listdir(data_path)\n                if os.path.isdir(os.path.join(data_path, name))\n            ])\n        else:\n            languages = self.langs.split(\',\')\n            for name in languages:\n                assert os.path.exists(os.path.join(data_path, name)), ""all the languages must exist""\n\n        logger.info(""| Training on {0} languages: {1}"".format(len(languages), languages))\n        logger.info(""| Language to id mapping: "", {\n                lang: id for id, lang in enumerate(languages)\n            }\n        )\n\n        mask_whole_words = get_whole_word_mask(self.args, self.dictionary)\n        language_without_segmentations = self.args.no_whole_word_mask_langs.split(\',\')\n        lang_datasets = []\n        for language in languages:\n            split_path = os.path.join(data_path, language, split)\n\n            dataset = data_utils.load_indexed_dataset(\n                split_path,\n                self.source_dictionary,\n                self.args.dataset_impl,\n                combine=combine,\n            )\n            if dataset is None:\n                raise FileNotFoundError(\'Dataset not found: {} ({})\'.format(split, split_path))\n\n            end_token = self.source_dictionary.index(\'[{}]\'.format(language)) \\\n                if self.args.add_lang_token else self.source_dictionary.eos()\n\n            # create continuous blocks of tokens\n            dataset = TokenBlockDataset(\n                dataset,\n                dataset.sizes,\n                self.args.tokens_per_sample - 2,  # one less for <s>\n                pad=self.source_dictionary.pad(),\n                eos=end_token,\n                break_mode=self.args.sample_break_mode,\n            )\n            logger.info(\'| loaded {} blocks from: {}\'.format(len(dataset), split_path))\n\n            # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n            dataset = PrependTokenDataset(dataset, self.source_dictionary.bos())\n            dataset = AppendTokenDataset(dataset, end_token)\n\n            lang_mask_whole_words = mask_whole_words if language not in language_without_segmentations else None\n            lang_dataset = DenoisingDataset(\n                dataset,\n                dataset.sizes,\n                self.dictionary,\n                self.mask_idx,\n                lang_mask_whole_words,\n                shuffle=self.args.shuffle_instance,\n                seed=self.seed,\n                args=self.args,\n                eos=None if not self.args.add_lang_token else self.source_dictionary.index(\'[{}]\'.format(language)),\n            )\n            lang_datasets.append(lang_dataset)\n\n        dataset_lengths = np.array(\n            [len(d) for d in lang_datasets],\n            dtype=float,\n        )\n        logger.info(\n            \'| loaded total {} blocks for all languages\'.format(\n                dataset_lengths.sum(),\n            )\n        )\n        if split == self.args.train_subset:\n            # For train subset, additionally up or down sample languages.\n            sample_probs = self._get_sample_prob(dataset_lengths)\n            logger.info(""| Sample probability by language: "", {\n                    lang: ""{0:.4f}"".format(sample_probs[id])\n                    for id, lang in enumerate(languages)\n                }\n            )\n            size_ratio = (sample_probs * dataset_lengths.sum()) / dataset_lengths\n            logger.info(""| Up/Down Sampling ratio by language: "", {\n                    lang: ""{0:.2f}"".format(size_ratio[id])\n                    for id, lang in enumerate(languages)\n                }\n            )\n\n            resampled_lang_datasets = [\n                ResamplingDataset(\n                    lang_datasets[i],\n                    size_ratio=size_ratio[i],\n                    seed=self.args.seed,\n                    epoch=epoch,\n                    replace=size_ratio[i] >= 1.0,\n                )\n                for i, d in enumerate(lang_datasets)\n            ]\n            dataset = ConcatDataset(\n                resampled_lang_datasets,\n            )\n        else:\n            dataset = ConcatDataset(lang_datasets)\n            lang_splits = [split]\n            for lang_id, lang_dataset in enumerate(lang_datasets):\n                split_name = split + \'_\' + languages[lang_id]\n                lang_splits.append(split_name)\n                self.datasets[split_name] = lang_dataset\n\n            if split in self.args.valid_subset:\n                self.args.valid_subset = self.args.valid_subset.replace(\n                    split, \',\'.join(lang_splits)\n                )\n\n        with data_utils.numpy_seed(self.args.seed + epoch):\n            shuffle = np.random.permutation(len(dataset))\n\n        self.datasets[split] = SortDataset(\n            dataset,\n            sort_order=[\n                shuffle,\n                dataset.sizes,\n            ],\n        )\n'"
fairseq/tasks/multilingual_masked_lm.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\n\nimport numpy as np\nimport torch\n\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,\n    encoders,\n    ConcatDataset,\n    IdDataset,\n    MaskTokensDataset,\n    NestedDictionaryDataset,\n    NumelDataset,\n    NumSamplesDataset,\n    PadDataset,\n    PrependTokenDataset,\n    RawLabelDataset,\n    ResamplingDataset,\n    SortDataset,\n    TokenBlockDataset,\n)\nfrom fairseq.tasks import FairseqTask, register_task\nfrom fairseq import utils\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'multilingual_masked_lm\')\nclass MultiLingualMaskedLMTask(FairseqTask):\n    """"""Task for training masked language models (e.g., BERT, RoBERTa).""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', help=\'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner\')\n        parser.add_argument(\'--sample-break-mode\', default=\'complete\',\n                            choices=[\'none\', \'complete\', \'complete_doc\', \'eos\'],\n                            help=\'If omitted or ""none"", fills each sample with tokens-per-sample \'\n                                 \'tokens. If set to ""complete"", splits samples only at the end \'\n                                 \'of sentence, but may include multiple sentences per sample. \'\n                                 \'""complete_doc"" is similar but respects doc boundaries. \'\n                                 \'If set to ""eos"", includes only one sentence per sample.\')\n        parser.add_argument(\'--tokens-per-sample\', default=512, type=int,\n                            help=\'max number of total tokens over all segments \'\n                                 \'per sample for BERT dataset\')\n        parser.add_argument(\'--mask-prob\', default=0.15, type=float,\n                            help=\'probability of replacing a token with mask\')\n        parser.add_argument(\'--leave-unmasked-prob\', default=0.1, type=float,\n                            help=\'probability that a masked token is unmasked\')\n        parser.add_argument(\'--random-token-prob\', default=0.1, type=float,\n                            help=\'probability of replacing a token with a random token\')\n        parser.add_argument(\'--freq-weighted-replacement\', action=\'store_true\',\n                            help=\'sample random replacement words based on word frequencies\')\n        parser.add_argument(\'--mask-whole-words\', default=False, action=\'store_true\',\n                            help=\'mask whole words; you may also want to set --bpe\')\n        parser.add_argument(\'--multilang-sampling-alpha\', type=float, default=1.0,\n                            help=\'smoothing alpha for sample rations across multiple datasets\')\n\n    def __init__(self, args, dictionary):\n        super().__init__(args)\n        self.dictionary = dictionary\n        self.seed = args.seed\n\n        # add mask token\n        self.mask_idx = dictionary.add_symbol(\'<mask>\')\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        paths = utils.split_paths(args.data)\n        assert len(paths) > 0\n        dictionary = Dictionary.load(os.path.join(paths[0], \'dict.txt\'))\n        logger.info(\'dictionary: {} types\'.format(len(dictionary)))\n        return cls(args, dictionary)\n\n    def _get_whole_word_mask(self):\n        # create masked input and targets\n        if self.args.mask_whole_words:\n            bpe = encoders.build_bpe(self.args)\n            if bpe is not None:\n\n                def is_beginning_of_word(i):\n                    if i < self.source_dictionary.nspecial:\n                        # special elements are always considered beginnings\n                        return True\n                    tok = self.source_dictionary[i]\n                    if tok.startswith(\'madeupword\'):\n                        return True\n                    try:\n                        return bpe.is_beginning_of_word(tok)\n                    except ValueError:\n                        return True\n\n                mask_whole_words = torch.ByteTensor(list(\n                    map(is_beginning_of_word, range(len(self.source_dictionary)))\n                ))\n        else:\n            mask_whole_words = None\n        return mask_whole_words\n\n    def _get_sample_prob(self, dataset_lens):\n        """"""\n        Get smoothed sampling porbability by languages. This helps low resource\n        languages by upsampling them.\n        """"""\n        prob = dataset_lens / dataset_lens.sum()\n        smoothed_prob = prob ** self.args.multilang_sampling_alpha\n        smoothed_prob = smoothed_prob / smoothed_prob.sum()\n        return smoothed_prob\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n\n        languages = sorted(\n            name for name in os.listdir(data_path)\n            if os.path.isdir(os.path.join(data_path, name))\n        )\n\n        logger.info(""Training on {0} languages: {1}"".format(len(languages), languages))\n        logger.info(""Language to id mapping: "", {\n                lang: id for id, lang in enumerate(languages)\n            }\n        )\n\n        mask_whole_words = self._get_whole_word_mask()\n        lang_datasets = []\n        for lang_id, language in enumerate(languages):\n            split_path = os.path.join(data_path, language, split)\n\n            dataset = data_utils.load_indexed_dataset(\n                split_path,\n                self.source_dictionary,\n                self.args.dataset_impl,\n                combine=combine,\n            )\n            if dataset is None:\n                raise FileNotFoundError(\'Dataset not found: {} ({})\'.format(split, split_path))\n\n            # create continuous blocks of tokens\n            dataset = TokenBlockDataset(\n                dataset,\n                dataset.sizes,\n                self.args.tokens_per_sample - 1,  # one less for <s>\n                pad=self.source_dictionary.pad(),\n                eos=self.source_dictionary.eos(),\n                break_mode=self.args.sample_break_mode,\n            )\n            logger.info(\'loaded {} blocks from: {}\'.format(len(dataset), split_path))\n\n            # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n            dataset = PrependTokenDataset(dataset, self.source_dictionary.bos())\n\n            src_dataset, tgt_dataset = MaskTokensDataset.apply_mask(\n                dataset,\n                self.source_dictionary,\n                pad_idx=self.source_dictionary.pad(),\n                mask_idx=self.mask_idx,\n                seed=self.args.seed,\n                mask_prob=self.args.mask_prob,\n                leave_unmasked_prob=self.args.leave_unmasked_prob,\n                random_token_prob=self.args.random_token_prob,\n                freq_weighted_replacement=self.args.freq_weighted_replacement,\n                mask_whole_words=mask_whole_words,\n            )\n\n            lang_dataset = NestedDictionaryDataset(\n                {\n                    \'net_input\': {\n                        \'src_tokens\': PadDataset(\n                            src_dataset,\n                            pad_idx=self.source_dictionary.pad(),\n                            left_pad=False,\n                        ),\n                        \'src_lengths\': NumelDataset(src_dataset, reduce=False),\n                    },\n                    \'target\': PadDataset(\n                        tgt_dataset,\n                        pad_idx=self.source_dictionary.pad(),\n                        left_pad=False,\n                    ),\n                    \'nsentences\': NumSamplesDataset(),\n                    \'ntokens\': NumelDataset(src_dataset, reduce=True),\n                    \'lang_id\': RawLabelDataset([lang_id] * src_dataset.sizes.shape[0]),\n                },\n                sizes=[src_dataset.sizes],\n            )\n            lang_datasets.append(lang_dataset)\n\n\n        dataset_lengths = np.array(\n            [len(d) for d in lang_datasets],\n            dtype=float,\n        )\n        logger.info(\n            \'loaded total {} blocks for all languages\'.format(\n                dataset_lengths.sum(),\n            )\n        )\n        if split == self.args.train_subset:\n            # For train subset, additionally up or down sample languages.\n            sample_probs = self._get_sample_prob(dataset_lengths)\n            logger.info(""Sample probability by language: "", {\n                    lang: ""{0:.4f}"".format(sample_probs[id])\n                    for id, lang in enumerate(languages)\n                }\n            )\n            size_ratio = (sample_probs * dataset_lengths.sum()) / dataset_lengths\n            logger.info(""Up/Down Sampling ratio by language: "", {\n                    lang: ""{0:.2f}"".format(size_ratio[id])\n                    for id, lang in enumerate(languages)\n                }\n            )\n\n            resampled_lang_datasets = [\n                ResamplingDataset(\n                    lang_datasets[i],\n                    size_ratio=size_ratio[i],\n                    seed=self.args.seed,\n                    epoch=epoch,\n                    replace=size_ratio[i] >= 1.0,\n                )\n                for i, d in enumerate(lang_datasets)\n            ]\n            dataset = ConcatDataset(resampled_lang_datasets)\n        else:\n            dataset = ConcatDataset(lang_datasets)\n            lang_splits = [split]\n            for lang_id, lang_dataset in enumerate(lang_datasets):\n                split_name = split + \'_\' + languages[lang_id]\n                lang_splits.append(split_name)\n                self.datasets[split_name] = lang_dataset\n\n            # [TODO]: This is hacky for now to print validation ppl for each\n            # language individually. Maybe need task API changes to allow it\n            # in more generic ways.\n            if split in self.args.valid_subset:\n                self.args.valid_subset = self.args.valid_subset.replace(\n                    split, \',\'.join(lang_splits)\n                )\n\n        with data_utils.numpy_seed(self.args.seed + epoch):\n            shuffle = np.random.permutation(len(dataset))\n\n        self.datasets[split] = SortDataset(\n            dataset,\n            sort_order=[\n                shuffle,\n                dataset.sizes,\n            ],\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths, sort=True):\n        src_dataset = PadDataset(\n            TokenBlockDataset(\n                src_tokens,\n                src_lengths,\n                self.args.tokens_per_sample - 1,  # one less for <s>\n                pad=self.source_dictionary.pad(),\n                eos=self.source_dictionary.eos(),\n                break_mode=\'eos\',\n            ),\n            pad_idx=self.source_dictionary.pad(),\n            left_pad=False,\n        )\n        src_dataset = PrependTokenDataset(src_dataset, self.source_dictionary.bos())\n        src_dataset = NestedDictionaryDataset(\n            {\n                \'id\': IdDataset(),\n                \'net_input\': {\n                    \'src_tokens\': src_dataset,\n                    \'src_lengths\': NumelDataset(src_dataset, reduce=False),\n                },\n            },\n            sizes=src_lengths,\n        )\n        if sort:\n            src_dataset = SortDataset(src_dataset, sort_order=[src_lengths])\n        return src_dataset\n\n    def get_batch_iterator(\n        self, dataset, max_tokens=None, max_sentences=None, max_positions=None,\n        ignore_invalid_inputs=False, required_batch_size_multiple=1,\n        seed=1, num_shards=1, shard_id=0, num_workers=0, epoch=1,\n    ):\n        # Recreate epoch iterator every epoch cause the underlying\n        # datasets are dynamic due to sampling.\n        self.dataset_to_epoch_iter = {}\n        epoch_iter = super().get_batch_iterator(\n            dataset, max_tokens, max_sentences, max_positions,\n            ignore_invalid_inputs, required_batch_size_multiple,\n            seed, num_shards, shard_id, num_workers, epoch,\n        )\n        self.dataset_to_epoch_iter = {}\n        return epoch_iter\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n'"
fairseq/tasks/multilingual_translation.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\nimport logging\nimport os\n\nimport torch\n\nfrom fairseq import metrics, options\nfrom fairseq.data import (\n    Dictionary,\n    LanguagePairDataset,\n    RoundRobinZipDatasets,\n    TransformEosLangPairDataset,\n)\nfrom fairseq.models import FairseqMultiModel\nfrom fairseq.tasks.translation import load_langpair_dataset\n\nfrom . import FairseqTask, register_task\nfrom fairseq import utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef _lang_token(lang: str):\n    return \'__{}__\'.format(lang)\n\n\ndef _lang_token_index(dic: Dictionary, lang: str):\n    """"""Return language token index.""""""\n    idx = dic.index(_lang_token(lang))\n    assert idx != dic.unk_index, \\\n        \'cannot find language token for lang {}\'.format(lang)\n    return idx\n\n\n@register_task(\'multilingual_translation\')\nclass MultilingualTranslationTask(FairseqTask):\n    """"""A task for training multiple translation models simultaneously.\n\n    We iterate round-robin over batches from multiple language pairs, ordered\n    according to the `--lang-pairs` argument.\n\n    The training loop is roughly:\n\n        for i in range(len(epoch)):\n            for lang_pair in args.lang_pairs:\n                batch = next_batch_for_lang_pair(lang_pair)\n                loss = criterion(model_for_lang_pair(lang_pair), batch)\n                loss.backward()\n            optimizer.step()\n\n    In practice, `next_batch_for_lang_pair` is abstracted in a FairseqDataset\n    (e.g., `RoundRobinZipDatasets`) and `model_for_lang_pair` is a model that\n    implements the `FairseqMultiModel` interface.\n\n    During inference it is required to specify a single `--source-lang` and\n    `--target-lang`, which indicates the inference langauge direction.\n    `--lang-pairs`, `--encoder-langtok`, `--decoder-langtok` have to be set to\n    the same value as training.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'data\', metavar=\'DIR\', help=\'path to data directory\')\n        parser.add_argument(\'--lang-pairs\', default=None, metavar=\'PAIRS\',\n                            help=\'comma-separated list of language pairs (in training order): en-de,en-fr,de-fr\')\n        parser.add_argument(\'-s\', \'--source-lang\', default=None, metavar=\'SRC\',\n                            help=\'source language (only needed for inference)\')\n        parser.add_argument(\'-t\', \'--target-lang\', default=None, metavar=\'TARGET\',\n                            help=\'target language (only needed for inference)\')\n        parser.add_argument(\'--left-pad-source\', default=\'True\', type=str, metavar=\'BOOL\',\n                            help=\'pad the source on the left (default: True)\')\n        parser.add_argument(\'--left-pad-target\', default=\'False\', type=str, metavar=\'BOOL\',\n                            help=\'pad the target on the left (default: False)\')\n        parser.add_argument(\'--max-source-positions\', default=1024, type=int, metavar=\'N\',\n                            help=\'max number of tokens in the source sequence\')\n        parser.add_argument(\'--max-target-positions\', default=1024, type=int, metavar=\'N\',\n                            help=\'max number of tokens in the target sequence\')\n        parser.add_argument(\'--upsample-primary\', default=1, type=int,\n                            help=\'amount to upsample primary dataset\')\n        parser.add_argument(\'--encoder-langtok\', default=None, type=str, choices=[\'src\', \'tgt\'],\n                            metavar=\'SRCTGT\',\n                            help=\'replace beginning-of-sentence in source sentence with source or target \'\n                                 \'language token. (src/tgt)\')\n        parser.add_argument(\'--decoder-langtok\', action=\'store_true\',\n                            help=\'replace beginning-of-sentence in target sentence with target language token\')\n        # fmt: on\n\n    def __init__(self, args, dicts, training):\n        super().__init__(args)\n        self.dicts = dicts\n        self.training = training\n        if training:\n            self.lang_pairs = args.lang_pairs\n        else:\n            self.lang_pairs = [\'{}-{}\'.format(args.source_lang, args.target_lang)]\n        # eval_lang_pairs for multilingual translation is usually all of the\n        # lang_pairs. However for other multitask settings or when we want to\n        # optimize for certain languages we want to use a different subset. Thus\n        # the eval_lang_pairs class variable is provided for classes that extend\n        # this class.\n        self.eval_lang_pairs = self.lang_pairs\n        # model_lang_pairs will be used to build encoder-decoder model pairs in\n        # models.build_model(). This allows multitask type of sub-class can\n        # build models other than the input lang_pairs\n        self.model_lang_pairs = self.lang_pairs\n        self.langs = list(dicts.keys())\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        dicts, training = cls.prepare(args, **kwargs)\n        return cls(args, dicts, training)\n\n    @classmethod\n    def prepare(cls, args, **kargs):\n        args.left_pad_source = options.eval_bool(args.left_pad_source)\n        args.left_pad_target = options.eval_bool(args.left_pad_target)\n\n        if args.lang_pairs is None:\n            raise ValueError(\'--lang-pairs is required. List all the language pairs in the training objective.\')\n        if isinstance(args.lang_pairs, str):\n            args.lang_pairs = args.lang_pairs.split(\',\')\n        sorted_langs = sorted(list({x for lang_pair in args.lang_pairs for x in lang_pair.split(\'-\')}))\n        if args.source_lang is not None or args.target_lang is not None:\n            training = False\n        else:\n            training = True\n\n        # load dictionaries\n        dicts = OrderedDict()\n        for lang in sorted_langs:\n            paths = utils.split_paths(args.data)\n            assert len(paths) > 0\n            dicts[lang] = Dictionary.load(os.path.join(paths[0], \'dict.{}.txt\'.format(lang)))\n            if len(dicts) > 0:\n                assert dicts[lang].pad() == dicts[sorted_langs[0]].pad()\n                assert dicts[lang].eos() == dicts[sorted_langs[0]].eos()\n                assert dicts[lang].unk() == dicts[sorted_langs[0]].unk()\n            if args.encoder_langtok is not None or args.decoder_langtok:\n                for lang_to_add in sorted_langs:\n                    dicts[lang].add_symbol(_lang_token(lang_to_add))\n            logger.info(\'[{}] dictionary: {} types\'.format(lang, len(dicts[lang])))\n        return dicts, training\n\n    def get_encoder_langtok(self, src_lang, tgt_lang):\n        if self.args.encoder_langtok is None:\n            return self.dicts[src_lang].eos()\n        if self.args.encoder_langtok == \'src\':\n            return _lang_token_index(self.dicts[src_lang], src_lang)\n        else:\n            return _lang_token_index(self.dicts[src_lang], tgt_lang)\n\n    def get_decoder_langtok(self, tgt_lang):\n        if not self.args.decoder_langtok:\n            return self.dicts[tgt_lang].eos()\n        return _lang_token_index(self.dicts[tgt_lang], tgt_lang)\n\n    def alter_dataset_langtok(self, lang_pair_dataset,\n                              src_eos=None, src_lang=None, tgt_eos=None, tgt_lang=None):\n        if self.args.encoder_langtok is None and not self.args.decoder_langtok:\n            return lang_pair_dataset\n\n        new_src_eos = None\n        if self.args.encoder_langtok is not None and src_eos is not None \\\n           and src_lang is not None and tgt_lang is not None:\n            new_src_eos = self.get_encoder_langtok(src_lang, tgt_lang)\n        else:\n            src_eos = None\n\n        new_tgt_bos = None\n        if self.args.decoder_langtok and tgt_eos is not None and tgt_lang is not None:\n            new_tgt_bos = self.get_decoder_langtok(tgt_lang)\n        else:\n            tgt_eos = None\n\n        return TransformEosLangPairDataset(\n            lang_pair_dataset,\n            src_eos=src_eos,\n            new_src_eos=new_src_eos,\n            tgt_bos=tgt_eos,\n            new_tgt_bos=new_tgt_bos,\n        )\n\n    def load_dataset(self, split, epoch=1, **kwargs):\n        """"""Load a dataset split.""""""\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n\n        def language_pair_dataset(lang_pair):\n            src, tgt = lang_pair.split(\'-\')\n            langpair_dataset = load_langpair_dataset(\n                data_path, split, src, self.dicts[src], tgt, self.dicts[tgt],\n                combine=True, dataset_impl=self.args.dataset_impl,\n                upsample_primary=self.args.upsample_primary,\n                left_pad_source=self.args.left_pad_source,\n                left_pad_target=self.args.left_pad_target,\n                max_source_positions=self.args.max_source_positions,\n                max_target_positions=self.args.max_target_positions,\n            )\n            return self.alter_dataset_langtok(\n                langpair_dataset,\n                src_eos=self.dicts[src].eos(),\n                src_lang=src,\n                tgt_eos=self.dicts[tgt].eos(),\n                tgt_lang=tgt,\n            )\n\n        self.datasets[split] = RoundRobinZipDatasets(\n            OrderedDict([\n                (lang_pair, language_pair_dataset(lang_pair))\n                for lang_pair in self.lang_pairs\n            ]),\n            eval_key=None if self.training else ""%s-%s"" % (self.args.source_lang, self.args.target_lang),\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths):\n        lang_pair = ""%s-%s"" % (self.args.source_lang, self.args.target_lang)\n        return RoundRobinZipDatasets(\n            OrderedDict([(\n                lang_pair,\n                self.alter_dataset_langtok(\n                    LanguagePairDataset(\n                        src_tokens, src_lengths,\n                        self.source_dictionary\n                    ),\n                    src_eos=self.source_dictionary.eos(),\n                    src_lang=self.args.source_lang,\n                    tgt_eos=self.target_dictionary.eos(),\n                    tgt_lang=self.args.target_lang,\n                ),\n            )]),\n            eval_key=lang_pair,\n        )\n\n    def build_model(self, args):\n        def check_args():\n            messages = []\n            if len(set(self.args.lang_pairs).symmetric_difference(args.lang_pairs)) != 0:\n                messages.append(\'--lang-pairs should include all the language pairs {}.\'.format(args.lang_pairs))\n            if self.args.encoder_langtok != args.encoder_langtok:\n                messages.append(\'--encoder-langtok should be {}.\'.format(args.encoder_langtok))\n            if self.args.decoder_langtok != args.decoder_langtok:\n                messages.append(\'--decoder-langtok should {} be set.\'.format("""" if args.decoder_langtok else ""not""))\n\n            if len(messages) > 0:\n                raise ValueError(\' \'.join(messages))\n\n        # Check if task args are consistant with model args\n        check_args()\n\n        from fairseq import models\n        model = models.build_model(args, self)\n        if not isinstance(model, FairseqMultiModel):\n            raise ValueError(\'MultilingualTranslationTask requires a FairseqMultiModel architecture\')\n        return model\n\n    def train_step(self, sample, model, criterion, optimizer, update_num, ignore_grad=False):\n        model.train()\n        from collections import defaultdict\n        agg_loss, agg_sample_size, agg_logging_output = 0., 0., defaultdict(float)\n        for lang_pair in self.model_lang_pairs:\n            if sample[lang_pair] is None or len(sample[lang_pair]) == 0:\n                continue\n            loss, sample_size, logging_output = criterion(model.models[lang_pair], sample[lang_pair])\n            if ignore_grad:\n                loss *= 0\n            optimizer.backward(loss)\n            agg_loss += loss.detach().item()\n            # TODO make summing of the sample sizes configurable\n            agg_sample_size += sample_size\n            for k in logging_output:\n                agg_logging_output[k] += logging_output[k]\n                agg_logging_output[f""{lang_pair}:{k}""] += logging_output[k]\n        return agg_loss, agg_sample_size, agg_logging_output\n\n    def valid_step(self, sample, model, criterion):\n        model.eval()\n        with torch.no_grad():\n            from collections import defaultdict\n            agg_loss, agg_sample_size, agg_logging_output = 0., 0., defaultdict(float)\n            for lang_pair in self.eval_lang_pairs:\n                if lang_pair not in sample or sample[lang_pair] is None or len(sample[lang_pair]) == 0:\n                    continue\n                loss, sample_size, logging_output = criterion(model.models[lang_pair], sample[lang_pair])\n                agg_loss += loss.data.item()\n                # TODO make summing of the sample sizes configurable\n                agg_sample_size += sample_size\n                for k in logging_output:\n                    agg_logging_output[k] += logging_output[k]\n                    agg_logging_output[f""{lang_pair}:{k}""] += logging_output[k]\n        return agg_loss, agg_sample_size, agg_logging_output\n\n    def inference_step(self, generator, models, sample, prefix_tokens=None):\n        with torch.no_grad():\n            return generator.generate(\n                    models,\n                    sample,\n                    prefix_tokens=prefix_tokens,\n                    bos_token=_lang_token_index(self.target_dictionary, self.args.target_lang)\n                    if self.args.decoder_langtok else self.target_dictionary.eos(),\n            )\n\n    def reduce_metrics(self, logging_outputs, criterion):\n        with metrics.aggregate():\n            # pass \'sample_size\', \'nsentences\', \'ntokens\' stats to fairseq_task\n            super().reduce_metrics(logging_outputs, criterion)\n            for k in [\'sample_size\', \'nsentences\', \'ntokens\']:\n                metrics.log_scalar(k, sum(l[k] for l in logging_outputs))\n\n    @property\n    def source_dictionary(self):\n        if self.training:\n            return next(iter(self.dicts.values()))\n        else:\n            return self.dicts[self.args.source_lang]\n\n    @property\n    def target_dictionary(self):\n        if self.training:\n            return next(iter(self.dicts.values()))\n        else:\n            return self.dicts[self.args.target_lang]\n\n    def max_positions(self):\n        """"""Return the max sentence length allowed by the task.""""""\n        if len(self.datasets.values()) == 0:\n            return {\'%s-%s\' % (self.args.source_lang, self.args.target_lang):\n                    (self.args.max_source_positions, self.args.max_target_positions)}\n        return OrderedDict([\n            (key, (self.args.max_source_positions, self.args.max_target_positions))\n            for split in self.datasets.keys()\n            for key in self.datasets[split].datasets.keys()\n        ])\n'"
fairseq/tasks/semisupervised_translation.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\nimport logging\nimport os\n\nfrom fairseq.data import (\n    BacktranslationDataset,\n    data_utils,\n    indexed_dataset,\n    IndexedCachedDataset,\n    IndexedDataset,\n    IndexedRawTextDataset,\n    LanguagePairDataset,\n    NoisingDataset,\n    RoundRobinZipDatasets,\n)\nfrom fairseq.models import FairseqMultiModel\nfrom fairseq.sequence_generator import SequenceGenerator\n\nfrom .multilingual_translation import MultilingualTranslationTask\nfrom . import register_task\nfrom fairseq import utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_bt_dataset_key(lang_pair):\n    return ""bt:"" + lang_pair\n\n\ndef _get_denoising_dataset_key(lang_pair):\n    return ""denoising:"" + lang_pair\n\n\n# ported from UnsupervisedMT\ndef parse_lambda_config(x):\n    """"""\n    Parse the configuration of lambda coefficient (for scheduling).\n    x = ""3""                  # lambda will be a constant equal to x\n    x = ""0:1,1000:0""         # lambda will start from 1 and linearly decrease\n                             # to 0 during the first 1000 iterations\n    x = ""0:0,1000:0,2000:1""  # lambda will be equal to 0 for the first 1000\n                             # iterations, then will linearly increase to 1 until iteration 2000\n    """"""\n    split = x.split(\',\')\n    if len(split) == 1:\n        return float(x), None\n    else:\n        split = [s.split(os.pathsep) for s in split]\n        assert all(len(s) == 2 for s in split)\n        assert all(k.isdigit() for k, _ in split)\n        assert all(int(split[i][0]) < int(split[i + 1][0]) for i in range(len(split) - 1))\n        return float(split[0][1]), [(int(k), float(v)) for k, v in split]\n\n\n@register_task(\'semisupervised_translation\')\nclass SemisupervisedTranslationTask(MultilingualTranslationTask):\n    """"""A task for training multiple translation models simultaneously.\n\n    We iterate round-robin over batches from multiple language pairs, ordered\n    according to the `--lang-pairs` argument.\n\n    The training loop is roughly:\n\n        for i in range(len(epoch)):\n            for lang_pair in args.lang_pairs:\n                batch = next_batch_for_lang_pair(lang_pair)\n                loss = criterion(model_for_lang_pair(lang_pair), batch)\n                loss.backward()\n            optimizer.step()\n\n    In practice, `next_batch_for_lang_pair` is abstracted in a FairseqDataset\n    (e.g., `RoundRobinZipDatasets`) and `model_for_lang_pair` is a model that\n    implements the `FairseqMultiModel` interface.\n\n    During inference it is required to specify a single `--source-lang` and\n    `--target-lang`, instead of `--lang-pairs`.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        MultilingualTranslationTask.add_args(parser)\n        parser.add_argument(\'--lambda-parallel-config\', default=""1.0"", type=str, metavar=\'CONFIG\',\n                            help=\'cross-entropy reconstruction coefficient (parallel data). \'\n                                 \'use fixed weight during training if set to floating point number. \'\n                                 \'use piecewise linear function over number of updates to schedule the \'\n                                 \'weight with the format: w0:step0,w1:step1,...\')\n        parser.add_argument(\'--lambda-denoising-config\', default=""0.0"", type=str, metavar=\'CONFIG\',\n                            help=\'Cross-entropy reconstruction coefficient (denoising autoencoding)\'\n                                 \'use fixed weight during training if set to floating point number. \'\n                                 \'use piecewise linear function over number of updates to schedule the \'\n                                 \'weight with the format: w0:step0,w1:step1,...\')\n        parser.add_argument(\'--lambda-otf-bt-config\', default=""0.0"", type=str, metavar=\'CONFIG\',\n                            help=\'cross-entropy reconstruction coefficient (on-the-fly back-translation parallel data)\'\n                                 \'use fixed weight during training if set to floating point number. \'\n                                 \'use piecewise linear function over number of updates to schedule the \'\n                                 \'weight with the format: w0:step0,w1:step1,...\')\n        parser.add_argument(\'--bt-max-len-a\', default=1.1, type=float, metavar=\'N\',\n                            help=\'generate back-translated sequences of maximum length ax + b, where x is the \'\n                                 \'source length\')\n        parser.add_argument(\'--bt-max-len-b\', default=10.0, type=float, metavar=\'N\',\n                            help=\'generate back-translated sequences of maximum length ax + b, where x is the \'\n                                 \'source length\')\n        parser.add_argument(\'--bt-beam-size\', default=1, type=int, metavar=\'N\',\n                            help=\'beam size used in beam search of online back-translation\')\n        parser.add_argument(\'--max-word-shuffle-distance\', default=3.0, type=float, metavar=\'N\',\n                            help=\'maximum word shuffle distance for denoising autoencoding data generation\')\n        parser.add_argument(\'--word-dropout-prob\', default=0.1, type=float, metavar=\'N\',\n                            help=\'word dropout probability for denoising autoencoding data generation\')\n        parser.add_argument(\'--word-blanking-prob\', default=0.2, type=float, metavar=\'N\',\n                            help=\'word blanking probability for denoising autoencoding data generation\')\n        # fmt: on\n\n    def __init__(self, args, dicts, training):\n        super().__init__(args, dicts, training)\n        self.lambda_parallel, self.lambda_parallel_steps = parse_lambda_config(args.lambda_parallel_config)\n        self.lambda_otf_bt, self.lambda_otf_bt_steps = parse_lambda_config(args.lambda_otf_bt_config)\n        self.lambda_denoising, self.lambda_denoising_steps = parse_lambda_config(args.lambda_denoising_config)\n        if (self.lambda_denoising > 0.0 or self.lambda_denoising_steps is not None):\n            denoising_lang_pairs = [\n                ""%s-%s"" % (tgt, tgt)\n                for tgt in {lang_pair.split(\'-\')[1] for lang_pair in args.lang_pairs}\n            ]\n            self.model_lang_pairs = self.model_lang_pairs + denoising_lang_pairs\n        self.backtranslate_datasets = {}\n        self.backtranslators = {}\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        dicts, training = MultilingualTranslationTask.prepare(args, **kwargs)\n        return cls(args, dicts, training)\n\n    def load_dataset(self, split, epoch=1, **kwargs):\n        """"""Load a dataset split.""""""\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n\n        def split_exists(split, src, tgt, lang):\n            if src is not None:\n                filename = os.path.join(data_path, \'{}.{}-{}.{}\'.format(split, src, tgt, lang))\n            else:\n                filename = os.path.join(data_path, \'{}.{}-None.{}\'.format(split, src, tgt))\n            return indexed_dataset.dataset_exists(filename, impl=self.args.dataset_impl)\n\n        def load_indexed_dataset(path, dictionary):\n            return data_utils.load_indexed_dataset(path, dictionary, self.args.dataset_impl)\n\n        # load parallel datasets\n        src_datasets, tgt_datasets = {}, {}\n        if (self.lambda_parallel > 0.0 or self.lambda_parallel_steps is not None or not split.startswith(""train"")):\n            for lang_pair in self.lang_pairs:\n                src, tgt = lang_pair.split(\'-\')\n                if split_exists(split, src, tgt, src):\n                    prefix = os.path.join(data_path, \'{}.{}-{}.\'.format(split, src, tgt))\n                elif split_exists(split, tgt, src, src):\n                    prefix = os.path.join(data_path, \'{}.{}-{}.\'.format(split, tgt, src))\n                else:\n                    continue\n                src_datasets[lang_pair] = load_indexed_dataset(prefix + src, self.dicts[src])\n                tgt_datasets[lang_pair] = load_indexed_dataset(prefix + tgt, self.dicts[tgt])\n                logger.info(\'parallel-{} {} {} examples\'.format(data_path, split, len(src_datasets[lang_pair])))\n            if len(src_datasets) == 0:\n                raise FileNotFoundError(\'Dataset not found: {} ({})\'.format(split, data_path))\n\n        # back translation datasets\n        backtranslate_datasets = {}\n        if (self.lambda_otf_bt > 0.0 or self.lambda_otf_bt_steps is not None) and split.startswith(""train""):\n            for lang_pair in self.lang_pairs:\n                src, tgt = lang_pair.split(\'-\')\n                if not split_exists(split, tgt, None, tgt):\n                    raise FileNotFoundError(\'Dataset not found: backtranslation {} ({})\'.format(split, data_path))\n                filename = os.path.join(data_path, \'{}.{}-None.{}\'.format(split, tgt, tgt))\n                dataset = load_indexed_dataset(filename, self.dicts[tgt])\n                lang_pair_dataset_tgt = LanguagePairDataset(\n                    dataset,\n                    dataset.sizes,\n                    self.dicts[tgt],\n                    left_pad_source=self.args.left_pad_source,\n                    left_pad_target=self.args.left_pad_target,\n                )\n                lang_pair_dataset = LanguagePairDataset(\n                    dataset,\n                    dataset.sizes,\n                    src_dict=self.dicts[src],\n                    tgt=dataset,\n                    tgt_sizes=dataset.sizes,\n                    tgt_dict=self.dicts[tgt],\n                    left_pad_source=self.args.left_pad_source,\n                    left_pad_target=self.args.left_pad_target,\n                )\n                backtranslate_datasets[lang_pair] = BacktranslationDataset(\n                    tgt_dataset=self.alter_dataset_langtok(\n                        lang_pair_dataset_tgt,\n                        src_eos=self.dicts[tgt].eos(),\n                        src_lang=tgt,\n                        tgt_lang=src,\n                    ),\n                    backtranslation_fn=self.backtranslators[lang_pair],\n                    src_dict=self.dicts[src], tgt_dict=self.dicts[tgt],\n                    output_collater=self.alter_dataset_langtok(\n                        lang_pair_dataset=lang_pair_dataset,\n                        src_eos=self.dicts[src].eos(),\n                        src_lang=src,\n                        tgt_eos=self.dicts[tgt].eos(),\n                        tgt_lang=tgt,\n                    ).collater,\n                )\n                logger.info(\'backtranslate-{}: {} {} {} examples\'.format(\n                    tgt, data_path, split, len(backtranslate_datasets[lang_pair]),\n                ))\n                self.backtranslate_datasets[lang_pair] = backtranslate_datasets[lang_pair]\n\n        # denoising autoencoder\n        noising_datasets = {}\n        if (self.lambda_denoising > 0.0 or self.lambda_denoising_steps is not None) and split.startswith(""train""):\n            for lang_pair in self.lang_pairs:\n                _, tgt = lang_pair.split(\'-\')\n                if not split_exists(split, tgt, None, tgt):\n                    continue\n                filename = os.path.join(data_path, \'{}.{}-None.{}\'.format(split, tgt, tgt))\n                tgt_dataset1 = load_indexed_dataset(filename, self.dicts[tgt])\n                tgt_dataset2 = load_indexed_dataset(filename, self.dicts[tgt])\n                noising_dataset = NoisingDataset(\n                    tgt_dataset1,\n                    self.dicts[tgt],\n                    seed=1,\n                    max_word_shuffle_distance=self.args.max_word_shuffle_distance,\n                    word_dropout_prob=self.args.word_dropout_prob,\n                    word_blanking_prob=self.args.word_blanking_prob,\n                )\n                noising_datasets[lang_pair] = self.alter_dataset_langtok(\n                    LanguagePairDataset(\n                        noising_dataset,\n                        tgt_dataset1.sizes,\n                        self.dicts[tgt],\n                        tgt_dataset2,\n                        tgt_dataset2.sizes,\n                        self.dicts[tgt],\n                        left_pad_source=self.args.left_pad_source,\n                        left_pad_target=self.args.left_pad_target,\n                    ),\n                    src_eos=self.dicts[tgt].eos(),\n                    src_lang=tgt,\n                    tgt_eos=self.dicts[tgt].eos(),\n                    tgt_lang=tgt,\n                )\n                logger.info(\'denoising-{}: {} {} {} examples\'.format(\n                    tgt, data_path, split, len(noising_datasets[lang_pair]),\n                ))\n\n        def language_pair_dataset(lang_pair):\n            src, tgt = lang_pair.split(\'-\')\n            src_dataset, tgt_dataset = src_datasets[lang_pair], tgt_datasets[lang_pair]\n            return self.alter_dataset_langtok(\n                LanguagePairDataset(\n                    src_dataset, src_dataset.sizes, self.dicts[src],\n                    tgt_dataset, tgt_dataset.sizes, self.dicts[tgt],\n                    left_pad_source=self.args.left_pad_source,\n                    left_pad_target=self.args.left_pad_target,\n                    max_source_positions=self.args.max_source_positions,\n                    max_target_positions=self.args.max_target_positions,\n                ),\n                self.dicts[src].eos(),\n                src,\n                self.dicts[tgt].eos(),\n                tgt,\n            )\n\n        self.datasets[split] = RoundRobinZipDatasets(\n            OrderedDict([\n                (lang_pair, language_pair_dataset(lang_pair))\n                for lang_pair in src_datasets.keys()\n            ] + [\n                (_get_bt_dataset_key(lang_pair), dataset)\n                for lang_pair, dataset in backtranslate_datasets.items()\n            ] + [\n                (_get_denoising_dataset_key(lang_pair), dataset)\n                for lang_pair, dataset in noising_datasets.items()\n            ]),\n            eval_key=None if self.training else ""%s-%s"" % (self.args.source_lang, self.args.target_lang),\n        )\n\n    def build_model(self, args):\n        from fairseq import models\n        model = models.build_model(args, self)\n        if not isinstance(model, FairseqMultiModel):\n            raise ValueError(\'SemisupervisedTranslationTask requires a FairseqMultiModel architecture\')\n\n        # create SequenceGenerator for each model that has backtranslation dependency on it\n        self.sequence_generators = {}\n        if (self.lambda_otf_bt > 0.0 or self.lambda_otf_bt_steps is not None) and self.training:\n            for lang_pair in self.lang_pairs:\n                src, tgt = lang_pair.split(\'-\')\n                key = \'{}-{}\'.format(tgt, src)\n                self.sequence_generators[key] = SequenceGenerator(\n                    tgt_dict=self.dicts[src],\n                    beam_size=args.bt_beam_size,\n                    max_len_a=args.bt_max_len_a,\n                    max_len_b=args.bt_max_len_b,\n                )\n                decoder_lang_tok_idx = self.get_decoder_langtok(src)\n\n                def backtranslate_fn(\n                    sample, model=model.models[key],\n                    bos_token=decoder_lang_tok_idx,\n                    sequence_generator=self.sequence_generators[key],\n                ):\n                    return sequence_generator.generate(\n                        [model],\n                        sample,\n                        bos_token=bos_token,\n                    )\n                self.backtranslators[lang_pair] = backtranslate_fn\n\n        return model\n\n    def train_step(self, sample, model, criterion, optimizer, update_num, ignore_grad=False):\n        model.train()\n\n        if update_num > 0:\n            self.update_step(update_num)\n\n        agg_loss, agg_sample_size, agg_logging_output = 0., 0., {}\n\n        def forward_backward(model, samples, logging_output_key, weight):\n            nonlocal agg_loss, agg_sample_size, agg_logging_output\n            if samples is None or len(samples) == 0:\n                return\n            loss, sample_size, logging_output = criterion(model, samples)\n            if ignore_grad:\n                loss *= 0\n            else:\n                loss *= weight\n            optimizer.backward(loss)\n            agg_loss += loss.detach().item()\n            # TODO make summing of the sample sizes configurable\n            agg_sample_size += sample_size\n            agg_logging_output[logging_output_key] = logging_output\n\n        if self.lambda_parallel > 0.0:\n            for lang_pair in self.lang_pairs:\n                forward_backward(model.models[lang_pair], sample[lang_pair], lang_pair, self.lambda_parallel)\n\n        if self.lambda_otf_bt > 0.0:\n            for lang_pair in self.lang_pairs:\n                sample_key = _get_bt_dataset_key(lang_pair)\n                forward_backward(model.models[lang_pair], sample[sample_key], sample_key, self.lambda_otf_bt)\n\n        if self.lambda_denoising > 0.0:\n            for lang_pair in self.lang_pairs:\n                _, tgt = lang_pair.split(\'-\')\n                sample_key = _get_denoising_dataset_key(lang_pair)\n                forward_backward(model.models[\'{0}-{0}\'.format(tgt)], sample[sample_key], sample_key, self.lambda_denoising)\n\n        return agg_loss, agg_sample_size, agg_logging_output\n\n    def update_step(self, num_updates):\n        def lambda_step_func(config, n_iter):\n            """"""\n            Update a lambda value according to its schedule configuration.\n            """"""\n            ranges = [i for i in range(len(config) - 1) if config[i][0] <= n_iter < config[i + 1][0]]\n            if len(ranges) == 0:\n                assert n_iter >= config[-1][0]\n                return config[-1][1]\n            assert len(ranges) == 1\n            i = ranges[0]\n            x_a, y_a = config[i]\n            x_b, y_b = config[i + 1]\n            return y_a + (n_iter - x_a) * float(y_b - y_a) / float(x_b - x_a)\n\n        if self.lambda_parallel_steps is not None:\n            self.lambda_parallel = lambda_step_func(self.lambda_parallel_steps, num_updates)\n        if self.lambda_denoising_steps is not None:\n            self.lambda_denoising = lambda_step_func(self.lambda_denoising_steps, num_updates)\n        if self.lambda_otf_bt_steps is not None:\n            self.lambda_otf_bt = lambda_step_func(self.lambda_otf_bt_steps, num_updates)\n\n    def aggregate_logging_outputs(self, logging_outputs, criterion):\n        # aggregate logging outputs for each language pair\n        logging_output_keys = {\n            key\n            for logging_output in logging_outputs\n            for key in logging_output\n        }\n        lang_pair_keys = set(self.lang_pairs + [\n            _get_bt_dataset_key(lang_pair)\n            for lang_pair in self.lang_pairs\n        ] + [\n            _get_denoising_dataset_key(lang_pair)\n            for lang_pair in self.lang_pairs\n        ])\n        logging_output_keys = logging_output_keys.intersection(lang_pair_keys)\n        return super().aggregate_logging_outputs(logging_outputs, criterion, logging_output_keys)\n'"
fairseq/tasks/sentence_prediction.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\n\nimport numpy as np\n\nfrom fairseq import utils\nfrom fairseq.data import (\n    ConcatSentencesDataset,\n    data_utils,\n    Dictionary,\n    IdDataset,\n    NestedDictionaryDataset,\n    NumSamplesDataset,\n    NumelDataset,\n    OffsetTokensDataset,\n    PrependTokenDataset,\n    RawLabelDataset,\n    RightPadDataset,\n    RollDataset,\n    SortDataset,\n    StripTokenDataset,\n)\nfrom fairseq.data.shorten_dataset import maybe_shorten_dataset\nfrom fairseq.tasks import FairseqTask, register_task\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'sentence_prediction\')\nclass SentencePredictionTask(FairseqTask):\n    """"""\n    Sentence (or sentence pair) prediction (classification or regression) task.\n\n    Args:\n        dictionary (Dictionary): the dictionary for the input of the task\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', metavar=\'FILE\',\n                            help=\'file prefix for data\')\n        parser.add_argument(\'--num-classes\', type=int, default=-1,\n                            help=\'number of classes or regression targets\')\n        parser.add_argument(\'--init-token\', type=int, default=None,\n                            help=\'add token at the beginning of each batch item\')\n        parser.add_argument(\'--separator-token\', type=int, default=None,\n                            help=\'add separator token between inputs\')\n        parser.add_argument(\'--regression-target\', action=\'store_true\', default=False)\n        parser.add_argument(\'--no-shuffle\', action=\'store_true\', default=False)\n        parser.add_argument(\'--shorten-method\', default=\'none\',\n                            choices=[\'none\', \'truncate\', \'random_crop\'],\n                            help=\'if not none, shorten sequences that exceed --tokens-per-sample\')\n        parser.add_argument(\'--shorten-data-split-whitelist\', default=\'\',\n                            help=\'comma-separated list of dataset splits to apply shortening to, \'\n                                 \'e.g., ""train,valid"" (default: all dataset splits)\')\n        parser.add_argument(\'--add-prev-output-tokens\', action=\'store_true\', default=False,\n                            help=\'add prev_output_tokens to sample, used for encoder-decoder arch\')\n\n    def __init__(self, args, data_dictionary, label_dictionary):\n        super().__init__(args)\n        self.dictionary = data_dictionary\n        self._label_dictionary = label_dictionary\n        if not hasattr(args, \'max_positions\'):\n            self._max_positions = (\n                args.max_source_positions,\n                args.max_target_positions,\n            )\n        else:\n            self._max_positions = args.max_positions\n        args.tokens_per_sample = self._max_positions\n\n    @classmethod\n    def load_dictionary(cls, args, filename, source=True):\n        """"""Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        """"""\n        dictionary = Dictionary.load(filename)\n        dictionary.add_symbol(\'<mask>\')\n        return dictionary\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        assert args.num_classes > 0, \'Must set --num-classes\'\n\n        # load data dictionary\n        data_dict = cls.load_dictionary(\n            args,\n            os.path.join(args.data, \'input0\', \'dict.txt\'),\n            source=True,\n        )\n        logger.info(\'[input] dictionary: {} types\'.format(len(data_dict)))\n\n        label_dict = None\n        if not args.regression_target:\n            # load label dictionary\n            label_dict = cls.load_dictionary(\n                args,\n                os.path.join(args.data, \'label\', \'dict.txt\'),\n                source=False,\n            )\n            logger.info(\'[label] dictionary: {} types\'.format(len(label_dict)))\n        else:\n            label_dict = data_dict\n        return SentencePredictionTask(args, data_dict, label_dict)\n\n    def load_dataset(self, split, combine=False, **kwargs):\n        """"""Load a given dataset split (e.g., train, valid, test).""""""\n        def get_path(type, split):\n            return os.path.join(self.args.data, type, split)\n\n        def make_dataset(type, dictionary):\n            split_path = get_path(type, split)\n\n            dataset = data_utils.load_indexed_dataset(\n                split_path,\n                dictionary,\n                self.args.dataset_impl,\n                combine=combine,\n            )\n            return dataset\n\n        input0 = make_dataset(\'input0\', self.source_dictionary)\n        assert input0 is not None, \'could not find dataset: {}\'.format(get_path(type, split))\n        input1 = make_dataset(\'input1\', self.source_dictionary)\n\n        if self.args.init_token is not None:\n            input0 = PrependTokenDataset(input0, self.args.init_token)\n\n        if input1 is None:\n            src_tokens = input0\n        else:\n            if self.args.separator_token is not None:\n                input1 = PrependTokenDataset(input1, self.args.separator_token)\n\n            src_tokens = ConcatSentencesDataset(input0, input1)\n\n        with data_utils.numpy_seed(self.args.seed):\n            shuffle = np.random.permutation(len(src_tokens))\n\n        src_tokens = maybe_shorten_dataset(\n            src_tokens,\n            split,\n            self.args.shorten_data_split_whitelist,\n            self.args.shorten_method,\n            self.args.max_positions,\n            self.args.seed,\n        )\n\n        dataset = {\n            \'id\': IdDataset(),\n            \'net_input\': {\n                \'src_tokens\': RightPadDataset(\n                    src_tokens,\n                    pad_idx=self.source_dictionary.pad(),\n                ),\n                \'src_lengths\': NumelDataset(src_tokens, reduce=False),\n            },\n            \'nsentences\': NumSamplesDataset(),\n            \'ntokens\': NumelDataset(src_tokens, reduce=True),\n        }\n\n        if self.args.add_prev_output_tokens:\n            prev_tokens_dataset = RightPadDataset(\n                RollDataset(src_tokens, 1),\n                pad_idx=self.dictionary.pad(),\n            )\n            dataset[\'net_input\'].update(\n                prev_output_tokens=prev_tokens_dataset,\n            )\n\n        if not self.args.regression_target:\n            label_dataset = make_dataset(\'label\', self.label_dictionary)\n            if label_dataset is not None:\n                dataset.update(\n                    target=OffsetTokensDataset(\n                        StripTokenDataset(\n                            label_dataset,\n                            id_to_strip=self.label_dictionary.eos(),\n                        ),\n                        offset=-self.label_dictionary.nspecial,\n                    )\n                )\n        else:\n            label_path = ""{0}.label"".format(get_path(\'label\', split))\n            if os.path.exists(label_path):\n                def parse_regression_target(i, line):\n                    values = line.split()\n                    assert len(values) == self.args.num_classes, \\\n                        f\'expected num_classes={self.args.num_classes} regression target values on line {i}, found: ""{line}""\'\n                    return [float(x) for x in values]\n                dataset.update(\n                    target=RawLabelDataset([\n                        parse_regression_target(i, line.strip()) for i, line in enumerate(open(label_path).readlines())\n                    ])\n                )\n\n        nested_dataset = NestedDictionaryDataset(\n            dataset,\n            sizes=[src_tokens.sizes],\n        )\n\n        if self.args.no_shuffle:\n            dataset = nested_dataset\n        else:\n            dataset = SortDataset(\n                nested_dataset,\n                # shuffle\n                sort_order=[shuffle],\n            )\n\n        logger.info(""Loaded {0} with #samples: {1}"".format(split, len(dataset)))\n\n        self.datasets[split] = dataset\n        return self.datasets[split]\n\n    def build_model(self, args):\n        from fairseq import models\n        model = models.build_model(args, self)\n\n        model.register_classification_head(\n            getattr(args, \'classification_head_name\', \'sentence_classification_head\'),\n            num_classes=self.args.num_classes,\n        )\n\n        return model\n\n    def max_positions(self):\n        return self._max_positions\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n    @property\n    def label_dictionary(self):\n        return self._label_dictionary\n'"
fairseq/tasks/sentence_ranking.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\n\nimport numpy as np\n\nfrom fairseq import utils\nfrom fairseq.data import (\n    ConcatSentencesDataset,\n    data_utils,\n    Dictionary,\n    IdDataset,\n    NestedDictionaryDataset,\n    NumSamplesDataset,\n    NumelDataset,\n    PrependTokenDataset,\n    RawLabelDataset,\n    RightPadDataset,\n    SortDataset,\n)\nfrom fairseq.data.shorten_dataset import maybe_shorten_dataset\nfrom fairseq.tasks import FairseqTask, register_task\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\'sentence_ranking\')\nclass SentenceRankingTask(FairseqTask):\n    """"""\n    Ranking task on multiple sentences.\n\n    Args:\n        dictionary (Dictionary): the dictionary for the input of the task\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', metavar=\'FILE\',\n                            help=\'file prefix for data\')\n        parser.add_argument(\'--num-classes\', type=int,\n                            help=\'number of sentences to be ranked\')\n        parser.add_argument(\'--init-token\', type=int,\n                            help=\'add token at the beginning of each batch item\')\n        parser.add_argument(\'--separator-token\', type=int,\n                            help=\'add separator token between inputs\')\n        parser.add_argument(\'--no-shuffle\', action=\'store_true\')\n        parser.add_argument(\'--shorten-method\', default=\'none\',\n                            choices=[\'none\', \'truncate\', \'random_crop\'],\n                            help=\'if not none, shorten sequences that exceed --tokens-per-sample\')\n        parser.add_argument(\'--shorten-data-split-whitelist\', default=\'\',\n                            help=\'comma-separated list of dataset splits to apply shortening to, \'\n                                 \'e.g., ""train,valid"" (default: all dataset splits)\')\n        parser.add_argument(\'--max-option-length\', type=int,\n                            help=\'max length for each option\')\n\n    def __init__(self, args, dictionary):\n        super().__init__(args)\n        self.dictionary = dictionary\n\n    @classmethod\n    def load_dictionary(cls, args, filename, source=True):\n        """"""Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        """"""\n        dictionary = Dictionary.load(filename)\n        dictionary.add_symbol(\'<mask>\')\n        return dictionary\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        assert args.criterion == \'sentence_ranking\', \\\n            \'Must set --criterion=sentence_ranking\'\n\n        # load data dictionary\n        data_dict = cls.load_dictionary(\n            args,\n            os.path.join(args.data, \'input0\', \'dict.txt\'),\n            source=True,\n        )\n        logger.info(\'[input] dictionary: {} types\'.format(len(data_dict)))\n        return SentenceRankingTask(args, data_dict)\n\n    def load_dataset(self, split, combine=False, **kwargs):\n        """"""Load a given dataset split (e.g., train, valid, test).""""""\n\n        def get_path(type, split):\n            return os.path.join(self.args.data, type, split)\n\n        def make_dataset(type, dictionary):\n            split_path = get_path(type, split)\n\n            dataset = data_utils.load_indexed_dataset(\n                split_path,\n                self.source_dictionary,\n                self.args.dataset_impl,\n                combine=combine,\n            )\n            return dataset\n\n        input0 = make_dataset(\'input0\', self.source_dictionary)\n        input_options = [\n            make_dataset(\n                \'input{idx}\'.format(idx=idx + 1),\n                self.source_dictionary\n            )\n            for idx in range(self.args.num_classes)\n        ]\n\n        if self.args.separator_token is not None:\n            input0 = PrependTokenDataset(input0, self.args.separator_token)\n\n        src_tokens = []\n        for input_option in input_options:\n            if self.args.init_token is not None:\n                input_option = PrependTokenDataset(input_option, self.args.init_token)\n            if self.args.max_option_length is not None:\n                input_option = TruncateDataset(input_option, self.args.max_option_length)\n            src_token = ConcatSentencesDataset(input_option, input0)\n            if self.args.truncate_sequence:\n                src_token = maybe_shorten_dataset(\n                    src_token,\n                    split,\n                    self.args.shorten_data_split_whitelist,\n                    self.args.shorten_method,\n                    self.args.max_positions,\n                    self.args.seed,\n                )\n            src_tokens.append(src_token)\n\n        with data_utils.numpy_seed(self.args.seed):\n            shuffle = np.random.permutation(len(src_tokens[0]))\n\n        dataset = {\n            \'id\': IdDataset(),\n            \'nsentences\': NumSamplesDataset(),\n            \'ntokens\': NumelDataset(src_tokens[0], reduce=True),\n        }\n\n        for src_token_idx in range(len(src_tokens)):\n            dataset.update(\n                {\n                    \'net_input{idx}\'.format(idx=src_token_idx+1): {\n                        \'src_tokens\': RightPadDataset(\n                            src_tokens[src_token_idx],\n                            pad_idx=self.source_dictionary.pad(),\n                        ),\n                        \'src_lengths\': NumelDataset(src_tokens[src_token_idx], reduce=False),\n                    }\n                }\n            )\n\n        label_path = \'{}.label\'.format(get_path(\'label\', split))\n        if os.path.exists(label_path):\n            with open(label_path) as h:\n                dataset.update(\n                    target=RawLabelDataset([\n                        int(x.strip()) for x in h.readlines()\n                    ])\n                )\n\n        nested_dataset = NestedDictionaryDataset(\n            dataset,\n            sizes=[np.maximum.reduce([src_token.sizes for src_token in src_tokens])],\n        )\n\n        if self.args.no_shuffle:\n            dataset = nested_dataset\n        else:\n            dataset = SortDataset(\n                nested_dataset,\n                # shuffle\n                sort_order=[shuffle],\n            )\n\n        logger.info(""Loaded {0} with #samples: {1}"".format(split, len(dataset)))\n\n        self.datasets[split] = dataset\n        return self.datasets[split]\n\n    def build_model(self, args):\n        from fairseq import models\n        model = models.build_model(args, self)\n\n        model.register_classification_head(\n            getattr(args, \'ranking_head_name\', \'sentence_classification_head\'),\n            num_classes=1,\n        )\n\n        return model\n\n    def max_positions(self):\n        return self.args.max_positions\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n'"
fairseq/tasks/translation.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom argparse import Namespace\nimport json\nimport itertools\nimport logging\nimport os\n\nimport numpy as np\n\nfrom fairseq import metrics, options, utils\nfrom fairseq.data import (\n    AppendTokenDataset,\n    ConcatDataset,\n    data_utils,\n    encoders,\n    indexed_dataset,\n    LanguagePairDataset,\n    PrependTokenDataset,\n    StripTokenDataset,\n    TruncateDataset,\n)\n\nfrom fairseq.tasks import FairseqTask, register_task\n\nEVAL_BLEU_ORDER = 4\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_langpair_dataset(\n    data_path, split,\n    src, src_dict,\n    tgt, tgt_dict,\n    combine, dataset_impl, upsample_primary,\n    left_pad_source, left_pad_target, max_source_positions,\n    max_target_positions, prepend_bos=False, load_alignments=False,\n    truncate_source=False, append_source_id=False\n):\n\n    def split_exists(split, src, tgt, lang, data_path):\n        filename = os.path.join(data_path, \'{}.{}-{}.{}\'.format(split, src, tgt, lang))\n        return indexed_dataset.dataset_exists(filename, impl=dataset_impl)\n\n    src_datasets = []\n    tgt_datasets = []\n\n    for k in itertools.count():\n        split_k = split + (str(k) if k > 0 else \'\')\n\n        # infer langcode\n        if split_exists(split_k, src, tgt, src, data_path):\n            prefix = os.path.join(data_path, \'{}.{}-{}.\'.format(split_k, src, tgt))\n        elif split_exists(split_k, tgt, src, src, data_path):\n            prefix = os.path.join(data_path, \'{}.{}-{}.\'.format(split_k, tgt, src))\n        else:\n            if k > 0:\n                break\n            else:\n                raise FileNotFoundError(\'Dataset not found: {} ({})\'.format(split, data_path))\n\n        src_dataset = data_utils.load_indexed_dataset(prefix + src, src_dict, dataset_impl)\n        if truncate_source:\n            src_dataset = AppendTokenDataset(\n                TruncateDataset(\n                    StripTokenDataset(src_dataset, src_dict.eos()),\n                    max_source_positions - 1,\n                ),\n                src_dict.eos(),\n            )\n        src_datasets.append(src_dataset)\n\n        tgt_dataset = data_utils.load_indexed_dataset(prefix + tgt, tgt_dict, dataset_impl)\n        if tgt_dataset is not None:\n            tgt_datasets.append(tgt_dataset)\n\n        logger.info(\'{} {} {}-{} {} examples\'.format(\n            data_path, split_k, src, tgt, len(src_datasets[-1])\n        ))\n\n        if not combine:\n            break\n\n    assert len(src_datasets) == len(tgt_datasets) or len(tgt_datasets) == 0\n\n    if len(src_datasets) == 1:\n        src_dataset = src_datasets[0]\n        tgt_dataset = tgt_datasets[0] if len(tgt_datasets) > 0 else None\n    else:\n        sample_ratios = [1] * len(src_datasets)\n        sample_ratios[0] = upsample_primary\n        src_dataset = ConcatDataset(src_datasets, sample_ratios)\n        if len(tgt_datasets) > 0:\n            tgt_dataset = ConcatDataset(tgt_datasets, sample_ratios)\n        else:\n            tgt_dataset = None\n\n    if prepend_bos:\n        assert hasattr(src_dict, ""bos_index"") and hasattr(tgt_dict, ""bos_index"")\n        src_dataset = PrependTokenDataset(src_dataset, src_dict.bos())\n        if tgt_dataset is not None:\n            tgt_dataset = PrependTokenDataset(tgt_dataset, tgt_dict.bos())\n\n    eos = None\n    if append_source_id:\n        src_dataset = AppendTokenDataset(src_dataset, src_dict.index(\'[{}]\'.format(src)))\n        if tgt_dataset is not None:\n            tgt_dataset = AppendTokenDataset(tgt_dataset, tgt_dict.index(\'[{}]\'.format(tgt)))\n        eos = tgt_dict.index(\'[{}]\'.format(tgt))\n\n    align_dataset = None\n    if load_alignments:\n        align_path = os.path.join(data_path, \'{}.align.{}-{}\'.format(split, src, tgt))\n        if indexed_dataset.dataset_exists(align_path, impl=dataset_impl):\n            align_dataset = data_utils.load_indexed_dataset(align_path, None, dataset_impl)\n\n    tgt_dataset_sizes = tgt_dataset.sizes if tgt_dataset is not None else None\n    return LanguagePairDataset(\n        src_dataset, src_dataset.sizes, src_dict,\n        tgt_dataset, tgt_dataset_sizes, tgt_dict,\n        left_pad_source=left_pad_source,\n        left_pad_target=left_pad_target,\n        max_source_positions=max_source_positions,\n        max_target_positions=max_target_positions,\n        align_dataset=align_dataset, eos=eos\n    )\n\n\n@register_task(\'translation\')\nclass TranslationTask(FairseqTask):\n    """"""\n    Translate from one (source) language to another (target) language.\n\n    Args:\n        src_dict (~fairseq.data.Dictionary): dictionary for the source language\n        tgt_dict (~fairseq.data.Dictionary): dictionary for the target language\n\n    .. note::\n\n        The translation task is compatible with :mod:`fairseq-train`,\n        :mod:`fairseq-generate` and :mod:`fairseq-interactive`.\n\n    The translation task provides the following additional command-line\n    arguments:\n\n    .. argparse::\n        :ref: fairseq.tasks.translation_parser\n        :prog:\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'data\', help=\'colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner\')\n        parser.add_argument(\'-s\', \'--source-lang\', default=None, metavar=\'SRC\',\n                            help=\'source language\')\n        parser.add_argument(\'-t\', \'--target-lang\', default=None, metavar=\'TARGET\',\n                            help=\'target language\')\n        parser.add_argument(\'--load-alignments\', action=\'store_true\',\n                            help=\'load the binarized alignments\')\n        parser.add_argument(\'--left-pad-source\', default=\'True\', type=str, metavar=\'BOOL\',\n                            help=\'pad the source on the left\')\n        parser.add_argument(\'--left-pad-target\', default=\'False\', type=str, metavar=\'BOOL\',\n                            help=\'pad the target on the left\')\n        parser.add_argument(\'--max-source-positions\', default=1024, type=int, metavar=\'N\',\n                            help=\'max number of tokens in the source sequence\')\n        parser.add_argument(\'--max-target-positions\', default=1024, type=int, metavar=\'N\',\n                            help=\'max number of tokens in the target sequence\')\n        parser.add_argument(\'--upsample-primary\', default=1, type=int,\n                            help=\'amount to upsample primary dataset\')\n        parser.add_argument(\'--truncate-source\', action=\'store_true\', default=False,\n                            help=\'truncate source to max-source-positions\')\n\n        # options for reporting BLEU during validation\n        parser.add_argument(\'--eval-bleu\', action=\'store_true\',\n                            help=\'evaluation with BLEU scores\')\n        parser.add_argument(\'--eval-bleu-detok\', type=str, default=""space"",\n                            help=\'detokenize before computing BLEU (e.g., ""moses""); \'\n                                 \'required if using --eval-bleu; use ""space"" to \'\n                                 \'disable detokenization; see fairseq.data.encoders \'\n                                 \'for other options\')\n        parser.add_argument(\'--eval-bleu-detok-args\', type=str, metavar=\'JSON\',\n                            help=\'args for building the tokenizer, if needed\')\n        parser.add_argument(\'--eval-tokenized-bleu\', action=\'store_true\', default=False,\n                            help=\'compute tokenized BLEU instead of sacrebleu\')\n        parser.add_argument(\'--eval-bleu-remove-bpe\', nargs=\'?\', const=\'@@ \', default=None,\n                            help=\'remove BPE before computing BLEU\')\n        parser.add_argument(\'--eval-bleu-args\', type=str, metavar=\'JSON\',\n                            help=\'generation args for BLUE scoring, \'\n                                 \'e.g., \\\'{""beam"": 4, ""lenpen"": 0.6}\\\'\')\n        parser.add_argument(\'--eval-bleu-print-samples\', action=\'store_true\',\n                            help=\'print sample generations during validation\')\n        # fmt: on\n\n    def __init__(self, args, src_dict, tgt_dict):\n        super().__init__(args)\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        """"""\n        args.left_pad_source = options.eval_bool(args.left_pad_source)\n        args.left_pad_target = options.eval_bool(args.left_pad_target)\n\n        paths = utils.split_paths(args.data)\n        assert len(paths) > 0\n        # find language pair automatically\n        if args.source_lang is None or args.target_lang is None:\n            args.source_lang, args.target_lang = data_utils.infer_language_pair(paths[0])\n        if args.source_lang is None or args.target_lang is None:\n            raise Exception(\'Could not infer language pair, please provide it explicitly\')\n\n        # load dictionaries\n        src_dict = cls.load_dictionary(os.path.join(paths[0], \'dict.{}.txt\'.format(args.source_lang)))\n        tgt_dict = cls.load_dictionary(os.path.join(paths[0], \'dict.{}.txt\'.format(args.target_lang)))\n        assert src_dict.pad() == tgt_dict.pad()\n        assert src_dict.eos() == tgt_dict.eos()\n        assert src_dict.unk() == tgt_dict.unk()\n        logger.info(\'[{}] dictionary: {} types\'.format(args.source_lang, len(src_dict)))\n        logger.info(\'[{}] dictionary: {} types\'.format(args.target_lang, len(tgt_dict)))\n\n        return cls(args, src_dict, tgt_dict)\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n\n        # infer langcode\n        src, tgt = self.args.source_lang, self.args.target_lang\n\n        self.datasets[split] = load_langpair_dataset(\n            data_path, split, src, self.src_dict, tgt, self.tgt_dict,\n            combine=combine, dataset_impl=self.args.dataset_impl,\n            upsample_primary=self.args.upsample_primary,\n            left_pad_source=self.args.left_pad_source,\n            left_pad_target=self.args.left_pad_target,\n            max_source_positions=self.args.max_source_positions,\n            max_target_positions=self.args.max_target_positions,\n            load_alignments=self.args.load_alignments,\n            truncate_source=self.args.truncate_source,\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths):\n        return LanguagePairDataset(src_tokens, src_lengths, self.source_dictionary)\n\n    def build_model(self, args):\n        model = super().build_model(args)\n        if getattr(args, \'eval_bleu\', False):\n            assert getattr(args, \'eval_bleu_detok\', None) is not None, (\n                \'--eval-bleu-detok is required if using --eval-bleu; \'\n                \'try --eval-bleu-detok=moses (or --eval-bleu-detok=space \'\n                \'to disable detokenization, e.g., when using sentencepiece)\'\n            )\n            detok_args = json.loads(getattr(args, \'eval_bleu_detok_args\', \'{}\') or \'{}\')\n            self.tokenizer = encoders.build_tokenizer(Namespace(\n                tokenizer=getattr(args, \'eval_bleu_detok\', None),\n                **detok_args\n            ))\n\n            gen_args = json.loads(getattr(args, \'eval_bleu_args\', \'{}\') or \'{}\')\n            self.sequence_generator = self.build_generator([model], Namespace(**gen_args))\n        return model\n\n    def valid_step(self, sample, model, criterion):\n        loss, sample_size, logging_output = super().valid_step(sample, model, criterion)\n        if self.args.eval_bleu:\n            bleu = self._inference_with_bleu(self.sequence_generator, sample, model)\n            logging_output[\'_bleu_sys_len\'] = bleu.sys_len\n            logging_output[\'_bleu_ref_len\'] = bleu.ref_len\n            # we split counts into separate entries so that they can be\n            # summed efficiently across workers using fast-stat-sync\n            assert len(bleu.counts) == EVAL_BLEU_ORDER\n            for i in range(EVAL_BLEU_ORDER):\n                logging_output[\'_bleu_counts_\' + str(i)] = bleu.counts[i]\n                logging_output[\'_bleu_totals_\' + str(i)] = bleu.totals[i]\n        return loss, sample_size, logging_output\n\n    def reduce_metrics(self, logging_outputs, criterion):\n        super().reduce_metrics(logging_outputs, criterion)\n        if self.args.eval_bleu:\n\n            def sum_logs(key):\n                return sum(log.get(key, 0) for log in logging_outputs)\n\n            counts, totals = [], []\n            for i in range(EVAL_BLEU_ORDER):\n                counts.append(sum_logs(\'_bleu_counts_\' + str(i)))\n                totals.append(sum_logs(\'_bleu_totals_\' + str(i)))\n\n            if max(totals) > 0:\n                # log counts as numpy arrays -- log_scalar will sum them correctly\n                metrics.log_scalar(\'_bleu_counts\', np.array(counts))\n                metrics.log_scalar(\'_bleu_totals\', np.array(totals))\n                metrics.log_scalar(\'_bleu_sys_len\', sum_logs(\'_bleu_sys_len\'))\n                metrics.log_scalar(\'_bleu_ref_len\', sum_logs(\'_bleu_ref_len\'))\n\n                def compute_bleu(meters):\n                    import inspect\n                    import sacrebleu\n                    fn_sig = inspect.getfullargspec(sacrebleu.compute_bleu)[0]\n                    if \'smooth_method\' in fn_sig:\n                        smooth = {\'smooth_method\': \'exp\'}\n                    else:\n                        smooth = {\'smooth\': \'exp\'}\n                    bleu = sacrebleu.compute_bleu(\n                        correct=meters[\'_bleu_counts\'].sum,\n                        total=meters[\'_bleu_totals\'].sum,\n                        sys_len=meters[\'_bleu_sys_len\'].sum,\n                        ref_len=meters[\'_bleu_ref_len\'].sum,\n                        **smooth\n                    )\n                    return round(bleu.score, 2)\n\n                metrics.log_derived(\'bleu\', compute_bleu)\n\n    def max_positions(self):\n        """"""Return the max sentence length allowed by the task.""""""\n        return (self.args.max_source_positions, self.args.max_target_positions)\n\n    @property\n    def source_dictionary(self):\n        """"""Return the source :class:`~fairseq.data.Dictionary`.""""""\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        """"""Return the target :class:`~fairseq.data.Dictionary`.""""""\n        return self.tgt_dict\n\n    def _inference_with_bleu(self, generator, sample, model):\n        import sacrebleu\n\n        def decode(toks, escape_unk=False):\n            s = self.tgt_dict.string(\n                toks.int().cpu(),\n                self.args.eval_bleu_remove_bpe,\n                # The default unknown string in fairseq is `<unk>`, but\n                # this is tokenized by sacrebleu as `< unk >`, inflating\n                # BLEU scores. Instead, we use a somewhat more verbose\n                # alternative that is unlikely to appear in the real\n                # reference, but doesn\'t get split into multiple tokens.\n                unk_string=(\n                    ""UNKNOWNTOKENINREF"" if escape_unk else ""UNKNOWNTOKENINHYP""\n                ),\n            )\n            if self.tokenizer:\n                s = self.tokenizer.decode(s)\n            return s\n\n        gen_out = self.inference_step(generator, [model], sample, None)\n        hyps, refs = [], []\n        for i in range(len(gen_out)):\n            hyps.append(decode(gen_out[i][0][\'tokens\']))\n            refs.append(decode(\n                utils.strip_pad(sample[\'target\'][i], self.tgt_dict.pad()),\n                escape_unk=True,  # don\'t count <unk> as matches to the hypo\n            ))\n        if self.args.eval_bleu_print_samples:\n            logger.info(\'example hypothesis: \' + hyps[0])\n            logger.info(\'example reference: \' + refs[0])\n        if self.args.eval_tokenized_bleu:\n            return sacrebleu.corpus_bleu(hyps, [refs], tokenize=\'none\')\n        else:\n            return sacrebleu.corpus_bleu(hyps, [refs])\n'"
fairseq/tasks/translation_from_pretrained_bart.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom fairseq.data import LanguagePairDataset\n\nfrom .translation import load_langpair_dataset, TranslationTask\nfrom . import register_task\n\n\n@register_task(\'translation_from_pretrained_bart\')\nclass TranslationFromPretrainedBARTTask(TranslationTask):\n    """"""\n    Translate from source language to target language with a model initialized with a multilingual pretrain.\n\n    Args:\n        src_dict (~fairseq.data.Dictionary): dictionary for the source language\n        tgt_dict (~fairseq.data.Dictionary): dictionary for the target language\n\n    .. note::\n\n        The translation task is compatible with :mod:`fairseq-train`,\n        :mod:`fairseq-generate` and :mod:`fairseq-interactive`.\n\n    The translation task provides the following additional command-line\n    arguments:\n\n    .. argparse::\n        :ref: fairseq.tasks.translation_parser\n        :prog:\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        TranslationTask.add_args(parser)\n        parser.add_argument(\'--langs\', required=True, metavar=\'LANG\',\n                            help=\'comma-separated list of monolingual language, for example, ""en,de,fr""\'\n                                 \'be careful these langs are what you used for pretraining (the same order),\'\n                                 \'not for finetuning.\'\n                                 \'you should always add all pretraining language idx during finetuning.\')\n        # fmt: on\n\n    def __init__(self, args, src_dict, tgt_dict):\n        super().__init__(args, src_dict, tgt_dict)\n        self.langs = args.langs.split(\',\')\n        for d in [src_dict, tgt_dict]:\n            for l in self.langs:\n                d.add_symbol(\'[{}]\'.format(l))\n            d.add_symbol(\'<mask>\')\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        paths = self.args.data.split(\':\')\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n\n        # infer langcode\n        src, tgt = self.args.source_lang, self.args.target_lang\n\n        self.datasets[split] = load_langpair_dataset(\n            data_path, split, src, self.src_dict, tgt, self.tgt_dict,\n            combine=combine, dataset_impl=self.args.dataset_impl,\n            upsample_primary=self.args.upsample_primary,\n            left_pad_source=self.args.left_pad_source,\n            left_pad_target=self.args.left_pad_target,\n            max_source_positions=getattr(self.args, \'max_source_positions\', 1024),\n            max_target_positions=getattr(self.args, \'max_target_positions\', 1024),\n            load_alignments=self.args.load_alignments,\n            prepend_bos=getattr(self.args, \'preprend_bos\', False),\n            append_source_id=True\n            )\n\n    def build_generator(self, models, args):\n        if getattr(args, \'score_reference\', False):\n            from fairseq.sequence_scorer import SequenceScorer\n            return SequenceScorer(\n                self.target_dictionary,\n                eos=self.tgt_dict.index(\'[{}]\'.format(self.args.target_lang))\n            )\n        else:\n            from fairseq.sequence_generator import SequenceGenerator\n            return SequenceGenerator(\n                models,\n                self.target_dictionary,\n                beam_size=getattr(args, \'beam\', 5),\n                max_len_a=getattr(args, \'max_len_a\', 0),\n                max_len_b=getattr(args, \'max_len_b\', 200),\n                min_len=getattr(args, \'min_len\', 1),\n                normalize_scores=(not getattr(args, \'unnormalized\', False)),\n                len_penalty=getattr(args, \'lenpen\', 1),\n                unk_penalty=getattr(args, \'unkpen\', 0),\n                temperature=getattr(args, \'temperature\', 1.),\n                match_source_len=getattr(args, \'match_source_len\', False),\n                no_repeat_ngram_size=getattr(args, \'no_repeat_ngram_size\', 0),\n                eos=self.tgt_dict.index(\'[{}]\'.format(self.args.target_lang))\n            )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths):\n        src_lang_id = self.source_dictionary.index(\'[{}]\'.format(self.args.source_lang))\n        source_tokens = []\n        for s_t in src_tokens:\n            s_t = torch.cat([s_t, s_t.new(1).fill_(src_lang_id)])\n            source_tokens.append(s_t)\n        dataset = LanguagePairDataset(source_tokens, src_lengths, self.source_dictionary)\n        return dataset\n'"
fairseq/tasks/translation_from_pretrained_xlm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.data.legacy.masked_lm_dictionary import MaskedLMDictionary\nfrom fairseq.tasks.translation import TranslationTask\n\nfrom . import register_task\n\n\n@register_task(""translation_from_pretrained_xlm"")\nclass TranslationFromPretrainedXLMTask(TranslationTask):\n    """"""\n    Same as TranslationTask except use the MaskedLMDictionary class so that\n    we can load data that was binarized with the MaskedLMDictionary class.\n\n    This task should be used for the entire training pipeline when we want to\n    train an NMT model from a pretrained XLM checkpoint: binarizing NMT data,\n    training NMT with the pretrained XLM checkpoint, and subsequent evaluation\n    of that trained model.\n    """"""\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        """"""Load the masked LM dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        """"""\n        return MaskedLMDictionary.load(filename)\n'"
fairseq/tasks/translation_lev.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\n\nimport torch\n\nfrom fairseq.utils import new_arange\nfrom fairseq.tasks import register_task\nfrom fairseq.tasks.translation import TranslationTask, load_langpair_dataset\nfrom fairseq import utils\n\n@register_task(\'translation_lev\')\nclass TranslationLevenshteinTask(TranslationTask):\n    """"""\n    Translation (Sequence Generation) task for Levenshtein Transformer\n    See `""Levenshtein Transformer"" <https://arxiv.org/abs/1905.11006>`_.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        TranslationTask.add_args(parser)\n        parser.add_argument(\n            \'--noise\',\n            default=\'random_delete\',\n            choices=[\'random_delete\', \'random_mask\', \'no_noise\', \'full_mask\'])\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n        data_path = paths[(epoch - 1) % len(paths)]\n\n        # infer langcode\n        src, tgt = self.args.source_lang, self.args.target_lang\n\n        self.datasets[split] = load_langpair_dataset(\n            data_path, split, src, self.src_dict, tgt, self.tgt_dict,\n            combine=combine, dataset_impl=self.args.dataset_impl,\n            upsample_primary=self.args.upsample_primary,\n            left_pad_source=self.args.left_pad_source,\n            left_pad_target=self.args.left_pad_target,\n            max_source_positions=self.args.max_source_positions,\n            max_target_positions=self.args.max_target_positions,\n            prepend_bos=True,\n        )\n\n    def inject_noise(self, target_tokens):\n        def _random_delete(target_tokens):\n            pad = self.tgt_dict.pad()\n            bos = self.tgt_dict.bos()\n            eos = self.tgt_dict.eos()\n\n            max_len = target_tokens.size(1)\n            target_mask = target_tokens.eq(pad)\n            target_score = target_tokens.clone().float().uniform_()\n            target_score.masked_fill_(\n                target_tokens.eq(bos) | target_tokens.eq(eos), 0.0)\n            target_score.masked_fill_(target_mask, 1)\n            target_score, target_rank = target_score.sort(1)\n            target_length = target_mask.size(1) - target_mask.float().sum(\n                1, keepdim=True)\n\n            # do not delete <bos> and <eos> (we assign 0 score for them)\n            target_cutoff = 2 + ((target_length - 2) * target_score.new_zeros(\n                target_score.size(0), 1).uniform_()).long()\n            target_cutoff = target_score.sort(1)[1] >= target_cutoff\n\n            prev_target_tokens = target_tokens.gather(\n                1, target_rank).masked_fill_(target_cutoff, pad).gather(\n                    1,\n                    target_rank.masked_fill_(target_cutoff,\n                                             max_len).sort(1)[1])\n            prev_target_tokens = prev_target_tokens[:, :prev_target_tokens.\n                                                    ne(pad).sum(1).max()]\n\n            return prev_target_tokens\n\n        def _random_mask(target_tokens):\n            pad = self.tgt_dict.pad()\n            bos = self.tgt_dict.bos()\n            eos = self.tgt_dict.eos()\n            unk = self.tgt_dict.unk()\n\n            target_masks = target_tokens.ne(pad) & \\\n                           target_tokens.ne(bos) & \\\n                           target_tokens.ne(eos)\n            target_score = target_tokens.clone().float().uniform_()\n            target_score.masked_fill_(~target_masks, 2.0)\n            target_length = target_masks.sum(1).float()\n            target_length = target_length * target_length.clone().uniform_()\n            target_length = target_length + 1  # make sure to mask at least one token.\n\n            _, target_rank = target_score.sort(1)\n            target_cutoff = new_arange(target_rank) < target_length[:, None].long()\n            prev_target_tokens = target_tokens.masked_fill(\n                target_cutoff.scatter(1, target_rank, target_cutoff), unk)\n            return prev_target_tokens\n\n        def _full_mask(target_tokens):\n            pad = self.tgt_dict.pad()\n            bos = self.tgt_dict.bos()\n            eos = self.tgt_dict.eos()\n            unk = self.tgt_dict.unk()\n\n            target_mask = target_tokens.eq(bos) | target_tokens.eq(\n                eos) | target_tokens.eq(pad)\n            return target_tokens.masked_fill(~target_mask, unk)\n\n        if self.args.noise == \'random_delete\':\n            return _random_delete(target_tokens)\n        elif self.args.noise == \'random_mask\':\n            return _random_mask(target_tokens)\n        elif self.args.noise == \'full_mask\':\n            return _full_mask(target_tokens)\n        elif self.args.noise == \'no_noise\':\n            return target_tokens\n        else:\n            raise NotImplementedError\n\n    def build_generator(self, models, args):\n        # add models input to match the API for SequenceGenerator\n        from fairseq.iterative_refinement_generator import IterativeRefinementGenerator\n        return IterativeRefinementGenerator(\n            self.target_dictionary,\n            eos_penalty=getattr(args, \'iter_decode_eos_penalty\', 0.0),\n            max_iter=getattr(args, \'iter_decode_max_iter\', 10),\n            beam_size=getattr(args, \'iter_decode_with_beam\', 1),\n            reranking=getattr(args, \'iter_decode_with_external_reranker\', False),\n            decoding_format=getattr(args, \'decoding_format\', None),\n            adaptive=not getattr(args, \'iter_decode_force_max_iter\', False),\n            retain_history=getattr(args, \'retain_iter_history\', False))\n\n    def train_step(self,\n                   sample,\n                   model,\n                   criterion,\n                   optimizer,\n                   update_num,\n                   ignore_grad=False):\n        model.train()\n        sample[\'prev_target\'] = self.inject_noise(sample[\'target\'])\n        loss, sample_size, logging_output = criterion(model, sample)\n        if ignore_grad:\n            loss *= 0\n        optimizer.backward(loss)\n        return loss, sample_size, logging_output\n\n    def valid_step(self, sample, model, criterion):\n        model.eval()\n        with torch.no_grad():\n            sample[\'prev_target\'] = self.inject_noise(sample[\'target\'])\n            loss, sample_size, logging_output = criterion(model, sample)\n        return loss, sample_size, logging_output\n'"
tests/gpu/__init__.py,0,b''
tests/gpu/test_binaries_gpu.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nimport logging\nimport os\nimport tempfile\nimport unittest\nfrom io import StringIO\n\nimport torch\nfrom fairseq import options\nfrom fairseq_cli import train\nfrom tests.utils import (\n    create_dummy_data,\n    generate_main,\n    preprocess_lm_data,\n    preprocess_translation_data,\n    train_translation_model,\n)\n\n\nclass TestTranslationGPU(unittest.TestCase):\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    @unittest.skipIf(not torch.cuda.is_available(), ""test requires a GPU"")\n    def test_fp16(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_fp16"") as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(data_dir, ""fconv_iwslt_de_en"", [""--fp16""])\n                generate_main(data_dir)\n\n    @unittest.skipIf(not torch.cuda.is_available(), ""test requires a GPU"")\n    def test_memory_efficient_fp16(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_memory_efficient_fp16"") as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(\n                    data_dir, ""fconv_iwslt_de_en"", [""--memory-efficient-fp16""]\n                )\n                generate_main(data_dir)\n\n    @unittest.skipIf(not torch.cuda.is_available(), ""test requires a GPU"")\n    def test_transformer_fp16(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_transformer"") as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    ""transformer_iwslt_de_en"",\n                    [\n                        ""--encoder-layers"",\n                        ""2"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--fp16"",\n                    ],\n                    run_validation=True,\n                )\n                generate_main(data_dir)\n\n    @unittest.skipIf(not torch.cuda.is_available(), ""test requires a GPU"")\n    def test_levenshtein_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(\n                ""test_levenshtein_transformer""\n            ) as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_translation_data(data_dir, [""--joined-dictionary""])\n                train_translation_model(\n                    data_dir,\n                    ""levenshtein_transformer"",\n                    [\n                        ""--apply-bert-init"",\n                        ""--early-exit"",\n                        ""6,6,6"",\n                        ""--criterion"",\n                        ""nat_loss"",\n                    ],\n                    task=""translation_lev"",\n                )\n                generate_main(\n                    data_dir,\n                    [\n                        ""--task"",\n                        ""translation_lev"",\n                        ""--iter-decode-max-iter"",\n                        ""9"",\n                        ""--iter-decode-eos-penalty"",\n                        ""0"",\n                        ""--print-step"",\n                    ],\n                )\n\n\ndef _quantize_language_model(data_dir, arch, extra_flags=None, run_validation=False):\n    train_parser = options.get_training_parser()\n    train_args = options.parse_args_and_arch(\n        train_parser,\n        [\n            ""--task"",\n            ""language_modeling"",\n            data_dir,\n            ""--arch"",\n            arch,\n            ""--optimizer"",\n            ""adam"",\n            ""--lr"",\n            ""0.0001"",\n            ""--criterion"",\n            ""adaptive_loss"",\n            ""--adaptive-softmax-cutoff"",\n            ""5,10,15"",\n            ""--max-tokens"",\n            ""500"",\n            ""--tokens-per-sample"",\n            ""500"",\n            ""--save-dir"",\n            data_dir,\n            ""--max-epoch"",\n            ""1"",\n            ""--no-progress-bar"",\n            ""--distributed-world-size"",\n            ""1"",\n            ""--ddp-backend"",\n            ""no_c10d"",\n            ""--num-workers"",\n            0,\n        ]\n        + (extra_flags or []),\n    )\n    train.main(train_args)\n\n    # try scalar quantization\n    scalar_quant_train_parser = options.get_training_parser()\n    scalar_quant_train_args = options.parse_args_and_arch(\n        scalar_quant_train_parser,\n        [\n            ""--task"",\n            ""language_modeling"",\n            data_dir,\n            ""--arch"",\n            arch,\n            ""--optimizer"",\n            ""adam"",\n            ""--lr"",\n            ""0.0001"",\n            ""--criterion"",\n            ""adaptive_loss"",\n            ""--adaptive-softmax-cutoff"",\n            ""5,10,15"",\n            ""--max-tokens"",\n            ""500"",\n            ""--tokens-per-sample"",\n            ""500"",\n            ""--save-dir"",\n            data_dir,\n            ""--max-update"",\n            ""3"",\n            ""--no-progress-bar"",\n            ""--distributed-world-size"",\n            ""1"",\n            ""--ddp-backend"",\n            ""no_c10d"",\n            ""--num-workers"",\n            0,\n            ""--quant-noise-scalar"",\n            ""0.5"",\n        ]\n        + (extra_flags or []),\n    )\n    train.main(scalar_quant_train_args)\n\n    # try iterative PQ quantization\n    quantize_parser = options.get_training_parser()\n    quantize_args = options.parse_args_and_arch(\n        quantize_parser,\n        [\n            ""--task"",\n            ""language_modeling"",\n            data_dir,\n            ""--arch"",\n            arch,\n            ""--optimizer"",\n            ""adam"",\n            ""--lr"",\n            ""0.0001"",\n            ""--criterion"",\n            ""adaptive_loss"",\n            ""--adaptive-softmax-cutoff"",\n            ""5,10,15"",\n            ""--max-tokens"",\n            ""50"",\n            ""--tokens-per-sample"",\n            ""50"",\n            ""--max-update"",\n            ""6"",\n            ""--no-progress-bar"",\n            ""--distributed-world-size"",\n            ""1"",\n            ""--ddp-backend"",\n            ""no_c10d"",\n            ""--num-workers"",\n            0,\n            ""--restore-file"",\n            os.path.join(data_dir, ""checkpoint_last.pt""),\n            ""--reset-optimizer"",\n            ""--quantization-config-path"",\n            os.path.join(\n                os.path.dirname(__file__), ""transformer_quantization_config.yaml""\n            ),\n        ]\n        + (extra_flags or []),\n    )\n    train.main(quantize_args)\n\n\nclass TestQuantization(unittest.TestCase):\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    @unittest.skipIf(not torch.cuda.is_available(), ""test requires a GPU"")\n    def test_quantization(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_quantization"") as data_dir:\n                create_dummy_data(data_dir)\n                preprocess_lm_data(data_dir)\n                # tests both scalar and iterative PQ quantization\n                _quantize_language_model(data_dir, ""transformer_lm"")\n\n\nclass TestOptimizersGPU(unittest.TestCase):\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    @unittest.skipIf(not torch.cuda.is_available(), ""test requires a GPU"")\n    def test_flat_grads(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_flat_grads"") as data_dir:\n                # Use just a bit of data and tiny model to keep this test runtime reasonable\n                create_dummy_data(data_dir, num_examples=10, maxlen=5)\n                preprocess_translation_data(data_dir)\n                with self.assertRaises(RuntimeError):\n                    # adafactor isn\'t compatible with flat grads, which\n                    # are used by default with --fp16\n                    train_translation_model(\n                        data_dir,\n                        ""lstm"",\n                        [\n                            ""--required-batch-size-multiple"",\n                            ""1"",\n                            ""--encoder-layers"",\n                            ""1"",\n                            ""--encoder-hidden-size"",\n                            ""32"",\n                            ""--decoder-layers"",\n                            ""1"",\n                            ""--optimizer"",\n                            ""adafactor"",\n                            ""--fp16"",\n                        ],\n                    )\n                # but it should pass once we set --fp16-no-flatten-grads\n                train_translation_model(\n                    data_dir,\n                    ""lstm"",\n                    [\n                        ""--required-batch-size-multiple"",\n                        ""1"",\n                        ""--encoder-layers"",\n                        ""1"",\n                        ""--encoder-hidden-size"",\n                        ""32"",\n                        ""--decoder-layers"",\n                        ""1"",\n                        ""--optimizer"",\n                        ""adafactor"",\n                        ""--fp16"",\n                        ""--fp16-no-flatten-grads"",\n                    ],\n                )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/speech_recognition/__init__.py,0,b''
tests/speech_recognition/asr_test_base.py,27,"b'#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport unittest\nfrom inspect import currentframe, getframeinfo\n\nimport numpy as np\nimport torch\nfrom fairseq.data import data_utils as fairseq_data_utils\nfrom fairseq.data.dictionary import Dictionary\nfrom fairseq.models import (\n    BaseFairseqModel,\n    FairseqDecoder,\n    FairseqEncoder,\n    FairseqEncoderDecoderModel,\n    FairseqEncoderModel,\n    FairseqModel,\n)\nfrom fairseq.tasks.fairseq_task import FairseqTask\nfrom examples.speech_recognition.data.data_utils import lengths_to_encoder_padding_mask\n\n\nDEFAULT_TEST_VOCAB_SIZE = 100\n\n\n# ///////////////////////////////////////////////////////////////////////////\n# utility function to setup dummy dict/task/input\n# ///////////////////////////////////////////////////////////////////////////\n\n\ndef get_dummy_dictionary(vocab_size=DEFAULT_TEST_VOCAB_SIZE):\n    dummy_dict = Dictionary()\n    # add dummy symbol to satisfy vocab size\n    for id, _ in enumerate(range(vocab_size)):\n        dummy_dict.add_symbol(""{}"".format(id), 1000)\n    return dummy_dict\n\n\nclass DummyTask(FairseqTask):\n    def __init__(self, args):\n        super().__init__(args)\n        self.dictionary = get_dummy_dictionary()\n        if getattr(self.args, ""ctc"", False):\n            self.dictionary.add_symbol(""<ctc_blank>"")\n        self.tgt_dict = self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\ndef get_dummy_task_and_parser():\n    """"""\n    to build a fariseq model, we need some dummy parse and task. This function\n    is used to create dummy task and parser to faciliate model/criterion test\n\n    Note: we use FbSpeechRecognitionTask as the dummy task. You may want\n    to use other task by providing another function\n    """"""\n    parser = argparse.ArgumentParser(\n        description=""test_dummy_s2s_task"", argument_default=argparse.SUPPRESS\n    )\n    DummyTask.add_args(parser)\n    args = parser.parse_args([])\n    task = DummyTask.setup_task(args)\n    return task, parser\n\n\ndef get_dummy_input(T=100, D=80, B=5, K=100):\n    forward_input = {}\n    # T max sequence length\n    # D feature vector dimension\n    # B batch size\n    # K target dimension size\n    feature = torch.randn(B, T, D)\n    # this (B, T, D) layout is just a convention, you can override it by\n    # write your own _prepare_forward_input function\n    src_lengths = torch.from_numpy(\n        np.random.randint(low=1, high=T, size=B, dtype=np.int64)\n    )\n    src_lengths[0] = T  # make sure the maximum length matches\n    prev_output_tokens = []\n    for b in range(B):\n        token_length = np.random.randint(low=1, high=src_lengths[b].item() + 1)\n        tokens = np.random.randint(low=0, high=K, size=token_length, dtype=np.int64)\n        prev_output_tokens.append(torch.from_numpy(tokens))\n\n    prev_output_tokens = fairseq_data_utils.collate_tokens(\n        prev_output_tokens,\n        pad_idx=1,\n        eos_idx=2,\n        left_pad=False,\n        move_eos_to_beginning=False,\n    )\n    src_lengths, sorted_order = src_lengths.sort(descending=True)\n    forward_input[""src_tokens""] = feature.index_select(0, sorted_order)\n    forward_input[""src_lengths""] = src_lengths\n    forward_input[""prev_output_tokens""] = prev_output_tokens\n\n    return forward_input\n\n\ndef get_dummy_encoder_output(encoder_out_shape=(100, 80, 5)):\n    """"""\n    This only provides an example to generate dummy encoder output\n    """"""\n    (T, B, D) = encoder_out_shape\n    encoder_out = {}\n\n    encoder_out[""encoder_out""] = torch.from_numpy(\n        np.random.randn(*encoder_out_shape).astype(np.float32)\n    )\n    seq_lengths = torch.from_numpy(np.random.randint(low=1, high=T, size=B))\n    # some dummy mask\n    encoder_out[""encoder_padding_mask""] = torch.arange(T).view(1, T).expand(\n        B, -1\n    ) >= seq_lengths.view(B, 1).expand(-1, T)\n    encoder_out[""encoder_padding_mask""].t_()\n\n    # encoer_padding_mask is (T, B) tensor, with (t, b)-th element indicate\n    # whether encoder_out[t, b] is valid (=0) or not (=1)\n    return encoder_out\n\n\ndef _current_postion_info():\n    cf = currentframe()\n    frameinfo = "" (at {}:{})"".format(\n        os.path.basename(getframeinfo(cf).filename), cf.f_back.f_lineno\n    )\n    return frameinfo\n\n\ndef check_encoder_output(encoder_output, batch_size=None):\n    """"""we expect encoder_output to be a dict with the following\n    key/value pairs:\n    - encoder_out: a Torch.Tensor\n    - encoder_padding_mask: a binary Torch.Tensor\n    """"""\n    if not isinstance(encoder_output, dict):\n        msg = (\n            ""FairseqEncoderModel.forward(...) must be a dict"" + _current_postion_info()\n        )\n        return False, msg\n\n    if ""encoder_out"" not in encoder_output:\n        msg = (\n            ""FairseqEncoderModel.forward(...) must contain encoder_out""\n            + _current_postion_info()\n        )\n        return False, msg\n\n    if ""encoder_padding_mask"" not in encoder_output:\n        msg = (\n            ""FairseqEncoderModel.forward(...) must contain encoder_padding_mask""\n            + _current_postion_info()\n        )\n        return False, msg\n\n    if not isinstance(encoder_output[""encoder_out""], torch.Tensor):\n        msg = ""encoder_out must be a torch.Tensor"" + _current_postion_info()\n        return False, msg\n\n    if encoder_output[""encoder_out""].dtype != torch.float32:\n        msg = ""encoder_out must have float32 dtype"" + _current_postion_info()\n        return False, msg\n\n    mask = encoder_output[""encoder_padding_mask""]\n    if mask is not None:\n        if not isinstance(mask, torch.Tensor):\n            msg = (\n                ""encoder_padding_mask must be a torch.Tensor"" + _current_postion_info()\n            )\n            return False, msg\n        if (\n            mask.dtype != torch.uint8\n            and (not hasattr(torch, \'bool\') or mask.dtype != torch.bool)\n        ):\n            msg = (\n                ""encoder_padding_mask must have dtype of uint8""\n                + _current_postion_info()\n            )\n            return False, msg\n\n        if mask.dim() != 2:\n            msg = (\n                ""we expect encoder_padding_mask to be a 2-d tensor, in shape (T, B)""\n                + _current_postion_info()\n            )\n            return False, msg\n\n        if batch_size is not None and mask.size(1) != batch_size:\n            msg = (\n                ""we expect encoder_padding_mask to be a 2-d tensor, with size(1)""\n                + "" being the batch size""\n                + _current_postion_info()\n            )\n            return False, msg\n    return True, None\n\n\ndef check_decoder_output(decoder_output):\n    """"""we expect output from a decoder is a tuple with the following constraint:\n    - the first element is a torch.Tensor\n    - the second element can be anything (reserved for future use)\n    """"""\n    if not isinstance(decoder_output, tuple):\n        msg = ""FariseqDecoder output must be a tuple"" + _current_postion_info()\n        return False, msg\n\n    if len(decoder_output) != 2:\n        msg = ""FairseqDecoder output must be 2-elem tuple"" + _current_postion_info()\n        return False, msg\n\n    if not isinstance(decoder_output[0], torch.Tensor):\n        msg = (\n            ""FariseqDecoder output[0] must be a torch.Tensor"" + _current_postion_info()\n        )\n        return False, msg\n\n    return True, None\n\n\n# ///////////////////////////////////////////////////////////////////////////\n# Base Test class\n# ///////////////////////////////////////////////////////////////////////////\n\n\nclass TestBaseFairseqModelBase(unittest.TestCase):\n    """"""\n    This class is used to facilitate writing unittest for any class derived from\n    `BaseFairseqModel`.\n    """"""\n\n    @classmethod\n    def setUpClass(cls):\n        if cls is TestBaseFairseqModelBase:\n            raise unittest.SkipTest(""Skipping test case in base"")\n        super().setUpClass()\n\n    def setUpModel(self, model):\n        self.assertTrue(isinstance(model, BaseFairseqModel))\n        self.model = model\n\n    def setupInput(self):\n        pass\n\n    def setUp(self):\n        self.model = None\n        self.forward_input = None\n        pass\n\n\nclass TestFairseqEncoderDecoderModelBase(TestBaseFairseqModelBase):\n    """"""\n    base code to test FairseqEncoderDecoderModel (formally known as\n    `FairseqModel`) must be derived from this base class\n    """"""\n\n    @classmethod\n    def setUpClass(cls):\n        if cls is TestFairseqEncoderDecoderModelBase:\n            raise unittest.SkipTest(""Skipping test case in base"")\n        super().setUpClass()\n\n    def setUpModel(self, model_cls, extra_args_setters=None):\n        self.assertTrue(\n            issubclass(model_cls, (FairseqEncoderDecoderModel, FairseqModel)),\n            msg=""This class only tests for FairseqModel subclasses"",\n        )\n\n        task, parser = get_dummy_task_and_parser()\n        model_cls.add_args(parser)\n\n        args = parser.parse_args([])\n        if extra_args_setters is not None:\n            for args_setter in extra_args_setters:\n                args_setter(args)\n        model = model_cls.build_model(args, task)\n        self.model = model\n\n    def setUpInput(self, input=None):\n        self.forward_input = get_dummy_input() if input is None else input\n\n    def setUp(self):\n        super().setUp()\n\n    def test_forward(self):\n        if self.model and self.forward_input:\n            forward_output = self.model.forward(**self.forward_input)\n            # for FairseqEncoderDecoderModel, forward returns a tuple of two\n            # elements, the first one is a Torch.Tensor\n            succ, msg = check_decoder_output(forward_output)\n            if not succ:\n                self.assertTrue(succ, msg=msg)\n            self.forward_output = forward_output\n\n    def test_get_normalized_probs(self):\n        if self.model and self.forward_input:\n            forward_output = self.model.forward(**self.forward_input)\n            logprob = self.model.get_normalized_probs(forward_output, log_probs=True)\n            prob = self.model.get_normalized_probs(forward_output, log_probs=False)\n\n            # in order for different models/criterion to play with each other\n            # we need to know whether the logprob or prob output is batch_first\n            # or not. We assume an additional attribute will be attached to logprob\n            # or prob. If you find your code failed here, simply override\n            # FairseqModel.get_normalized_probs, see example at\n            # https://fburl.com/batch_first_example\n            self.assertTrue(hasattr(logprob, ""batch_first""))\n            self.assertTrue(hasattr(prob, ""batch_first""))\n\n            self.assertTrue(torch.is_tensor(logprob))\n            self.assertTrue(torch.is_tensor(prob))\n\n\nclass TestFairseqEncoderModelBase(TestBaseFairseqModelBase):\n    """"""\n    base class to test FairseqEncoderModel\n    """"""\n\n    @classmethod\n    def setUpClass(cls):\n        if cls is TestFairseqEncoderModelBase:\n            raise unittest.SkipTest(""Skipping test case in base"")\n        super().setUpClass()\n\n    def setUpModel(self, model_cls, extra_args_setters=None):\n        self.assertTrue(\n            issubclass(model_cls, FairseqEncoderModel),\n            msg=""This class is only used for testing FairseqEncoderModel"",\n        )\n        task, parser = get_dummy_task_and_parser()\n        model_cls.add_args(parser)\n        args = parser.parse_args([])\n        if extra_args_setters is not None:\n            for args_setter in extra_args_setters:\n                args_setter(args)\n\n        model = model_cls.build_model(args, task)\n        self.model = model\n\n    def setUpInput(self, input=None):\n        self.forward_input = get_dummy_input() if input is None else input\n        # get_dummy_input() is originally for s2s, here we delete extra dict\n        # items, so it can be used for EncoderModel / Encoder as well\n        self.forward_input.pop(""prev_output_tokens"", None)\n\n    def setUp(self):\n        super().setUp()\n\n    def test_forward(self):\n        if self.forward_input and self.model:\n            bsz = self.forward_input[""src_tokens""].size(0)\n            forward_output = self.model.forward(**self.forward_input)\n\n            # we expect forward_output to be a dict with the following\n            # key/value pairs:\n            # - encoder_out: a Torch.Tensor\n            # - encoder_padding_mask: a binary Torch.Tensor\n            succ, msg = check_encoder_output(forward_output, batch_size=bsz)\n            if not succ:\n                self.assertTrue(succ, msg=msg)\n            self.forward_output = forward_output\n\n    def test_get_normalized_probs(self):\n        if self.model and self.forward_input:\n            forward_output = self.model.forward(**self.forward_input)\n            logprob = self.model.get_normalized_probs(forward_output, log_probs=True)\n            prob = self.model.get_normalized_probs(forward_output, log_probs=False)\n\n            # in order for different models/criterion to play with each other\n            # we need to know whether the logprob or prob output is batch_first\n            # or not. We assume an additional attribute will be attached to logprob\n            # or prob. If you find your code failed here, simply override\n            # FairseqModel.get_normalized_probs, see example at\n            # https://fburl.com/batch_first_example\n            self.assertTrue(hasattr(logprob, ""batch_first""))\n            self.assertTrue(hasattr(prob, ""batch_first""))\n\n            self.assertTrue(torch.is_tensor(logprob))\n            self.assertTrue(torch.is_tensor(prob))\n\n\nclass TestFairseqEncoderBase(unittest.TestCase):\n    """"""\n    base class to test FairseqEncoder\n    """"""\n\n    @classmethod\n    def setUpClass(cls):\n        if cls is TestFairseqEncoderBase:\n            raise unittest.SkipTest(""Skipping test case in base"")\n        super().setUpClass()\n\n    def setUpEncoder(self, encoder):\n        self.assertTrue(\n            isinstance(encoder, FairseqEncoder),\n            msg=""This class is only used for test FairseqEncoder"",\n        )\n        self.encoder = encoder\n\n    def setUpInput(self, input=None):\n        self.forward_input = get_dummy_input() if input is None else input\n        # get_dummy_input() is originally for s2s, here we delete extra dict\n        # items, so it can be used for EncoderModel / Encoder as well\n        self.forward_input.pop(""prev_output_tokens"", None)\n\n    def setUp(self):\n        self.encoder = None\n        self.forward_input = None\n\n    def test_forward(self):\n        if self.encoder and self.forward_input:\n            bsz = self.forward_input[""src_tokens""].size(0)\n\n            forward_output = self.encoder.forward(**self.forward_input)\n            succ, msg = check_encoder_output(forward_output, batch_size=bsz)\n            if not succ:\n                self.assertTrue(succ, msg=msg)\n            self.forward_output = forward_output\n\n\nclass TestFairseqDecoderBase(unittest.TestCase):\n    """"""\n    base class to test FairseqDecoder\n    """"""\n\n    @classmethod\n    def setUpClass(cls):\n        if cls is TestFairseqDecoderBase:\n            raise unittest.SkipTest(""Skipping test case in base"")\n        super().setUpClass()\n\n    def setUpDecoder(self, decoder):\n        self.assertTrue(\n            isinstance(decoder, FairseqDecoder),\n            msg=""This class is only used for test FairseqDecoder"",\n        )\n        self.decoder = decoder\n\n    def setUpInput(self, input=None):\n        self.forward_input = get_dummy_encoder_output() if input is None else input\n\n    def setUpPrevOutputTokens(self, tokens=None):\n        if tokens is None:\n            self.encoder_input = get_dummy_input()\n            self.prev_output_tokens = self.encoder_input[""prev_output_tokens""]\n        else:\n            self.prev_output_tokens = tokens\n\n    def setUp(self):\n        self.decoder = None\n        self.forward_input = None\n        self.prev_output_tokens = None\n\n    def test_forward(self):\n        if (\n            self.decoder is not None\n            and self.forward_input is not None\n            and self.prev_output_tokens is not None\n        ):\n            forward_output = self.decoder.forward(\n                prev_output_tokens=self.prev_output_tokens,\n                encoder_out=self.forward_input,\n            )\n            succ, msg = check_decoder_output(forward_output)\n            if not succ:\n                self.assertTrue(succ, msg=msg)\n            self.forward_input = forward_output\n\n\nclass DummyEncoderModel(FairseqEncoderModel):\n    def __init__(self, encoder):\n        super().__init__(encoder)\n\n    @classmethod\n    def build_model(cls, args, task):\n        return cls(DummyEncoder())\n\n    def get_logits(self, net_output):\n        # Inverse of sigmoid to use with BinaryCrossEntropyWithLogitsCriterion as\n        # F.binary_cross_entropy_with_logits combines sigmoid and CE\n        return torch.log(\n            torch.div(net_output[""encoder_out""], 1 - net_output[""encoder_out""])\n        )\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n        lprobs = super().get_normalized_probs(net_output, log_probs, sample=sample)\n        lprobs.batch_first = True\n        return lprobs\n\n\nclass DummyEncoder(FairseqEncoder):\n    def __init__(self):\n        super().__init__(None)\n\n    def forward(self, src_tokens, src_lengths):\n        mask, max_len = lengths_to_encoder_padding_mask(src_lengths)\n        return {""encoder_out"": src_tokens, ""encoder_padding_mask"": mask}\n\n\nclass CrossEntropyCriterionTestBase(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        if cls is CrossEntropyCriterionTestBase:\n            raise unittest.SkipTest(""Skipping base class test case"")\n        super().setUpClass()\n\n    def setUpArgs(self):\n        args = argparse.Namespace()\n        args.sentence_avg = False\n        args.threshold = 0.1  # to use with BinaryCrossEntropyWithLogitsCriterion\n        return args\n\n    def setUp(self):\n        args = self.setUpArgs()\n        self.model = DummyEncoderModel(encoder=DummyEncoder())\n        self.criterion = self.criterion_cls.build_criterion(args=args, task=DummyTask(args))\n\n    def get_src_tokens(self, correct_prediction, aggregate):\n        """"""\n            correct_prediction: True if the net_output (src_tokens) should\n            predict the correct target\n            aggregate: True if the criterion expects net_output (src_tokens)\n            aggregated across time axis\n        """"""\n        predicted_idx = 0 if correct_prediction else 1\n        if aggregate:\n            src_tokens = torch.zeros((2, 2), dtype=torch.float)\n            for b in range(2):\n                src_tokens[b][predicted_idx] = 1.0\n        else:\n            src_tokens = torch.zeros((2, 10, 2), dtype=torch.float)\n            for b in range(2):\n                for t in range(10):\n                    src_tokens[b][t][predicted_idx] = 1.0\n        return src_tokens\n\n    def get_target(self, soft_target):\n        if soft_target:\n            target = torch.zeros((2, 2), dtype=torch.float)\n            for b in range(2):\n                target[b][0] = 1.0\n        else:\n            target = torch.zeros((2, 10), dtype=torch.long)\n        return target\n\n    def get_test_sample(self, correct, soft_target, aggregate):\n        src_tokens = self.get_src_tokens(correct, aggregate)\n        target = self.get_target(soft_target)\n        L = src_tokens.size(1)\n        return {\n            ""net_input"": {""src_tokens"": src_tokens, ""src_lengths"": torch.tensor([L])},\n            ""target"": target,\n            ""ntokens"": src_tokens.size(0) * src_tokens.size(1),\n        }\n'"
tests/speech_recognition/test_collaters.py,5,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom examples.speech_recognition.data.collaters import Seq2SeqCollater\n\n\nclass TestSeq2SeqCollator(unittest.TestCase):\n    def test_collate(self):\n\n        eos_idx = 1\n        pad_idx = 0\n        collater = Seq2SeqCollater(\n            feature_index=0, label_index=1, pad_index=pad_idx, eos_index=eos_idx\n        )\n\n        # 2 frames in the first sample and 3 frames in the second one\n        frames1 = np.array([[7, 8], [9, 10]])\n        frames2 = np.array([[1, 2], [3, 4], [5, 6]])\n        target1 = np.array([4, 2, 3, eos_idx])\n        target2 = np.array([3, 2, eos_idx])\n        sample1 = {""id"": 0, ""data"": [frames1, target1]}\n        sample2 = {""id"": 1, ""data"": [frames2, target2]}\n        batch = collater.collate([sample1, sample2])\n\n        # collate sort inputs by frame\'s length before creating the batch\n        self.assertTensorEqual(batch[""id""], torch.tensor([1, 0]))\n        self.assertEqual(batch[""ntokens""], 7)\n        self.assertTensorEqual(\n            batch[""net_input""][""src_tokens""],\n            torch.tensor(\n                [[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [pad_idx, pad_idx]]]\n            ),\n        )\n        self.assertTensorEqual(\n            batch[""net_input""][""prev_output_tokens""],\n            torch.tensor([[eos_idx, 3, 2, pad_idx], [eos_idx, 4, 2, 3]]),\n        )\n        self.assertTensorEqual(batch[""net_input""][""src_lengths""], torch.tensor([3, 2]))\n        self.assertTensorEqual(\n            batch[""target""],\n            torch.tensor([[3, 2, eos_idx, pad_idx], [4, 2, 3, eos_idx]]),\n        )\n        self.assertEqual(batch[""nsentences""], 2)\n\n    def assertTensorEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), ""size mismatch"")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/speech_recognition/test_cross_entropy.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom examples.speech_recognition.criterions.cross_entropy_acc import CrossEntropyWithAccCriterion\nfrom .asr_test_base import CrossEntropyCriterionTestBase\n\n\nclass CrossEntropyWithAccCriterionTest(CrossEntropyCriterionTestBase):\n    def setUp(self):\n        self.criterion_cls = CrossEntropyWithAccCriterion\n        super().setUp()\n\n    def test_cross_entropy_all_correct(self):\n        sample = self.get_test_sample(correct=True, soft_target=False, aggregate=False)\n        loss, sample_size, logging_output = self.criterion(\n            self.model, sample, ""sum"", log_probs=True\n        )\n        assert logging_output[""correct""] == 20\n        assert logging_output[""total""] == 20\n        assert logging_output[""sample_size""] == 20\n        assert logging_output[""ntokens""] == 20\n\n    def test_cross_entropy_all_wrong(self):\n        sample = self.get_test_sample(correct=False, soft_target=False, aggregate=False)\n        loss, sample_size, logging_output = self.criterion(\n            self.model, sample, ""sum"", log_probs=True\n        )\n        assert logging_output[""correct""] == 0\n        assert logging_output[""total""] == 20\n        assert logging_output[""sample_size""] == 20\n        assert logging_output[""ntokens""] == 20\n'"
tests/speech_recognition/test_data_utils.py,2,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport unittest\n\nimport torch\n\nfrom examples.speech_recognition.data import data_utils\n\n\nclass DataUtilsTest(unittest.TestCase):\n\n    def test_normalization(self):\n        sample_len1 = torch.tensor([[-0.7661, -1.3889, -2.0972, -0.9134, -0.7071, -0.9765, -0.8700, -0.8283,\n                                    0.7512,  1.3211,  2.1532,  2.1174,  1.2800,  1.2633,  1.6147,  1.6322,\n                                    2.0723,  3.1522,  3.2852,  2.2309,  2.5569,  2.2183,  2.2862,  1.5886,\n                                    0.8773,  0.8725,  1.2662,  0.9899,  1.1069,  1.3926,  1.2795,  1.1199,\n                                    1.1477,  1.2687,  1.3843,  1.1903,  0.8355,  1.1367,  1.2639,  1.4707]])\n        out = data_utils.apply_mv_norm(sample_len1)\n        assert not torch.isnan(out).any()\n        assert (out == sample_len1).all()\n'"
tests/speech_recognition/test_vggtransformer.py,0,"b'#!/usr/bin/env python3\n\n# import models/encoder/decoder to be tested\nfrom examples.speech_recognition.models.vggtransformer import (\n    TransformerDecoder,\n    VGGTransformerEncoder,\n    VGGTransformerModel,\n    vggtransformer_1,\n    vggtransformer_2,\n    vggtransformer_base,\n)\n\n# import base test class\nfrom .asr_test_base import (\n    DEFAULT_TEST_VOCAB_SIZE,\n    TestFairseqDecoderBase,\n    TestFairseqEncoderBase,\n    TestFairseqEncoderDecoderModelBase,\n    get_dummy_dictionary,\n    get_dummy_encoder_output,\n    get_dummy_input,\n)\n\n\nclass VGGTransformerModelTest_mid(TestFairseqEncoderDecoderModelBase):\n    def setUp(self):\n        def override_config(args):\n            """"""\n            vggtrasformer_1 use 14 layers of transformer,\n            for testing purpose, it is too expensive. For fast turn-around\n            test, reduce the number of layers to 3.\n            """"""\n            args.transformer_enc_config = (\n                ""((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 3""\n            )\n\n        super().setUp()\n        extra_args_setter = [vggtransformer_1, override_config]\n\n        self.setUpModel(VGGTransformerModel, extra_args_setter)\n        self.setUpInput(get_dummy_input(T=50, D=80, B=5, K=DEFAULT_TEST_VOCAB_SIZE))\n\n\nclass VGGTransformerModelTest_big(TestFairseqEncoderDecoderModelBase):\n    def setUp(self):\n        def override_config(args):\n            """"""\n            vggtrasformer_2 use 16 layers of transformer,\n            for testing purpose, it is too expensive. For fast turn-around\n            test, reduce the number of layers to 3.\n            """"""\n            args.transformer_enc_config = (\n                ""((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 3""\n            )\n\n        super().setUp()\n        extra_args_setter = [vggtransformer_2, override_config]\n\n        self.setUpModel(VGGTransformerModel, extra_args_setter)\n        self.setUpInput(get_dummy_input(T=50, D=80, B=5, K=DEFAULT_TEST_VOCAB_SIZE))\n\n\nclass VGGTransformerModelTest_base(TestFairseqEncoderDecoderModelBase):\n    def setUp(self):\n        def override_config(args):\n            """"""\n            vggtrasformer_base use 12 layers of transformer,\n            for testing purpose, it is too expensive. For fast turn-around\n            test, reduce the number of layers to 3.\n            """"""\n            args.transformer_enc_config = (\n                ""((512, 8, 2048, True, 0.15, 0.15, 0.15),) * 3""\n            )\n\n        super().setUp()\n        extra_args_setter = [vggtransformer_base, override_config]\n\n        self.setUpModel(VGGTransformerModel, extra_args_setter)\n        self.setUpInput(get_dummy_input(T=50, D=80, B=5, K=DEFAULT_TEST_VOCAB_SIZE))\n\n\nclass VGGTransformerEncoderTest(TestFairseqEncoderBase):\n    def setUp(self):\n        super().setUp()\n\n        self.setUpInput(get_dummy_input(T=50, D=80, B=5))\n\n    def test_forward(self):\n        print(""1. test standard vggtransformer"")\n        self.setUpEncoder(VGGTransformerEncoder(input_feat_per_channel=80))\n        super().test_forward()\n        print(""2. test vggtransformer with limited right context"")\n        self.setUpEncoder(\n            VGGTransformerEncoder(\n                input_feat_per_channel=80, transformer_context=(-1, 5)\n            )\n        )\n        super().test_forward()\n        print(""3. test vggtransformer with limited left context"")\n        self.setUpEncoder(\n            VGGTransformerEncoder(\n                input_feat_per_channel=80, transformer_context=(5, -1)\n            )\n        )\n        super().test_forward()\n        print(""4. test vggtransformer with limited right context and sampling"")\n        self.setUpEncoder(\n            VGGTransformerEncoder(\n                input_feat_per_channel=80,\n                transformer_context=(-1, 12),\n                transformer_sampling=(2, 2),\n            )\n        )\n        super().test_forward()\n        print(""5. test vggtransformer with windowed context and sampling"")\n        self.setUpEncoder(\n            VGGTransformerEncoder(\n                input_feat_per_channel=80,\n                transformer_context=(12, 12),\n                transformer_sampling=(2, 2),\n            )\n        )\n\n\nclass TransformerDecoderTest(TestFairseqDecoderBase):\n    def setUp(self):\n        super().setUp()\n\n        dict = get_dummy_dictionary(vocab_size=DEFAULT_TEST_VOCAB_SIZE)\n        decoder = TransformerDecoder(dict)\n        dummy_encoder_output = get_dummy_encoder_output(encoder_out_shape=(50, 5, 256))\n\n        self.setUpDecoder(decoder)\n        self.setUpInput(dummy_encoder_output)\n        self.setUpPrevOutputTokens()\n'"
examples/roberta/commonsense_qa/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import commonsense_qa_task  # noqa\n'"
examples/roberta/commonsense_qa/commonsense_qa_task.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport json\nimport os\n\nimport numpy as np\nimport torch\n\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,\n    encoders,\n    IdDataset,\n    ListDataset,\n    NestedDictionaryDataset,\n    NumSamplesDataset,\n    NumelDataset,\n    RawLabelDataset,\n    RightPadDataset,\n    SortDataset,\n)\nfrom fairseq.tasks import FairseqTask, register_task\n\n\n@register_task(\'commonsense_qa\')\nclass CommonsenseQATask(FairseqTask):\n    """"""Task to finetune RoBERTa for Commonsense QA.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', metavar=\'DIR\',\n                            help=\'path to data directory; we load <split>.jsonl\')\n        parser.add_argument(\'--init-token\', type=int, default=None,\n                            help=\'add token at the beginning of each batch item\')\n        parser.add_argument(\'--num-classes\', type=int, default=5)\n\n    def __init__(self, args, vocab):\n        super().__init__(args)\n        self.vocab = vocab\n        self.mask = vocab.add_symbol(\'<mask>\')\n\n        self.bpe = encoders.build_bpe(args)\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        """"""Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        """"""\n        dictionary = Dictionary.load(filename)\n        dictionary.add_symbol(\'<mask>\')\n        return dictionary\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        assert args.criterion == \'sentence_ranking\', \'Must set --criterion=sentence_ranking\'\n\n        # load data and label dictionaries\n        vocab = cls.load_dictionary(os.path.join(args.data, \'dict.txt\'))\n        print(\'| dictionary: {} types\'.format(len(vocab)))\n\n        return cls(args, vocab)\n\n    def load_dataset(self, split, epoch=1, combine=False, data_path=None, return_only=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n\n        def binarize(s, append_bos=False):\n            if self.bpe is not None:\n                s = self.bpe.encode(s)\n            tokens = self.vocab.encode_line(\n                s, append_eos=True, add_if_not_exist=False,\n            ).long()\n            if append_bos and self.args.init_token is not None:\n                tokens = torch.cat([tokens.new([self.args.init_token]), tokens])\n            return tokens\n\n        if data_path is None:\n            data_path = os.path.join(self.args.data, split + \'.jsonl\')\n        if not os.path.exists(data_path):\n            raise FileNotFoundError(\'Cannot find data: {}\'.format(data_path))\n\n        src_tokens = [[] for i in range(self.args.num_classes)]\n        src_lengths = [[] for i in range(self.args.num_classes)]\n        labels = []\n\n        with open(data_path) as h:\n            for line in h:\n                example = json.loads(line.strip())\n                if \'answerKey\' in example:\n                    label = ord(example[\'answerKey\']) - ord(\'A\')\n                    labels.append(label)\n                question = example[\'question\'][\'stem\']\n                assert len(example[\'question\'][\'choices\']) == self.args.num_classes\n                # format: `<s> Q: Where would I not want a fox? </s> A: hen house </s>`\n                question = \'Q: \' + question\n                question_toks = binarize(question, append_bos=True)\n                for i, choice in enumerate(example[\'question\'][\'choices\']):\n                    src = \'A: \' + choice[\'text\']\n                    src_bin = torch.cat([question_toks, binarize(src)])\n                    src_tokens[i].append(src_bin)\n                    src_lengths[i].append(len(src_bin))\n        assert all(len(src_tokens[0]) == len(src_tokens[i]) for i in range(self.args.num_classes))\n        assert len(src_tokens[0]) == len(src_lengths[0])\n        assert len(labels) == 0 or len(labels) == len(src_tokens[0])\n\n        for i in range(self.args.num_classes):\n            src_lengths[i] = np.array(src_lengths[i])\n            src_tokens[i] = ListDataset(src_tokens[i], src_lengths[i])\n            src_lengths[i] = ListDataset(src_lengths[i])\n\n        dataset = {\n            \'id\': IdDataset(),\n            \'nsentences\': NumSamplesDataset(),\n            \'ntokens\': NumelDataset(src_tokens[0], reduce=True),\n        }\n\n        for i in range(self.args.num_classes):\n            dataset.update({\n                \'net_input{}\'.format(i + 1): {\n                    \'src_tokens\': RightPadDataset(\n                        src_tokens[i],\n                        pad_idx=self.source_dictionary.pad(),\n                    ),\n                    \'src_lengths\': src_lengths[i],\n                }\n            })\n\n        if len(labels) > 0:\n            dataset.update({\'target\': RawLabelDataset(labels)})\n\n        dataset = NestedDictionaryDataset(\n            dataset,\n            sizes=[np.maximum.reduce([src_token.sizes for src_token in src_tokens])],\n        )\n\n        with data_utils.numpy_seed(self.args.seed):\n            dataset = SortDataset(\n                dataset,\n                # shuffle\n                sort_order=[np.random.permutation(len(dataset))],\n            )\n\n        print(\'| Loaded {} with {} samples\'.format(split, len(dataset)))\n\n        self.datasets[split] = dataset\n        return self.datasets[split]\n\n    def build_model(self, args):\n        from fairseq import models\n        model = models.build_model(args, self)\n\n        model.register_classification_head(\n            \'sentence_classification_head\',\n            num_classes=1,\n        )\n\n        return model\n\n    @property\n    def source_dictionary(self):\n        return self.vocab\n\n    @property\n    def target_dictionary(self):\n        return self.vocab\n'"
examples/roberta/wsc/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import wsc_criterion  # noqa\nfrom . import wsc_task  # noqa\n'"
examples/roberta/wsc/wsc_criterion.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.data import encoders\nfrom fairseq.criterions import LegacyFairseqCriterion, register_criterion\n\n\n@register_criterion(\'wsc\')\nclass WSCCriterion(LegacyFairseqCriterion):\n\n    def __init__(self, args, task):\n        super().__init__(args, task)\n        if self.args.save_predictions is not None:\n            self.prediction_h = open(self.args.save_predictions, \'w\')\n        else:\n            self.prediction_h = None\n        self.bpe = encoders.build_bpe(args)\n        self.tokenizer = encoders.build_tokenizer(args)\n\n    def __del__(self):\n        if self.prediction_h is not None:\n            self.prediction_h.close()\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        parser.add_argument(\'--wsc-margin-alpha\', type=float, metavar=\'A\', default=1.0)\n        parser.add_argument(\'--wsc-margin-beta\', type=float, metavar=\'B\', default=0.0)\n        parser.add_argument(\'--wsc-cross-entropy\', action=\'store_true\',\n                            help=\'use cross entropy formulation instead of margin loss\')\n        parser.add_argument(\'--save-predictions\', metavar=\'FILE\',\n                            help=\'file to save predictions to\')\n\n    def get_masked_input(self, tokens, mask):\n        masked_tokens = tokens.clone()\n        masked_tokens[mask] = self.task.mask\n        return masked_tokens\n\n    def get_lprobs(self, model, tokens, mask):\n        logits, _ = model(src_tokens=self.get_masked_input(tokens, mask))\n        lprobs = F.log_softmax(logits, dim=-1, dtype=torch.float)\n        scores = lprobs.gather(2, tokens.unsqueeze(-1)).squeeze(-1)\n        mask = mask.type_as(scores)\n        scores = (scores * mask).sum(dim=-1) / mask.sum(dim=-1)\n        return scores\n\n    def get_loss(self, query_lprobs, cand_lprobs):\n        if self.args.wsc_cross_entropy:\n            return F.cross_entropy(\n                torch.cat([query_lprobs, cand_lprobs]).unsqueeze(0),\n                query_lprobs.new([0]).long(),\n            )\n        else:\n            return (\n                - query_lprobs\n                + self.args.wsc_margin_alpha * (\n                    cand_lprobs - query_lprobs + self.args.wsc_margin_beta\n                ).clamp(min=0)\n            ).sum()\n\n    def forward(self, model, sample, reduce=True):\n        # compute loss and accuracy\n        loss, nloss = 0., 0\n        ncorrect, nqueries = 0, 0\n\n        for i, label in enumerate(sample[\'labels\']):\n            query_lprobs = self.get_lprobs(\n                model,\n                sample[\'query_tokens\'][i].unsqueeze(0),\n                sample[\'query_masks\'][i].unsqueeze(0),\n            )\n            cand_lprobs = self.get_lprobs(\n                model,\n                sample[\'candidate_tokens\'][i],\n                sample[\'candidate_masks\'][i],\n            )\n\n            pred = (query_lprobs >= cand_lprobs).all().item()\n\n            if label is not None:\n                label = 1 if label else 0\n                ncorrect += 1 if pred == label else 0\n                nqueries += 1\n\n            if label:\n                # only compute a loss for positive instances\n                nloss += 1\n                loss += self.get_loss(query_lprobs, cand_lprobs)\n\n            id = sample[\'id\'][i].item()\n            if self.prediction_h is not None:\n                print(\'{}\\t{}\\t{}\'.format(id, pred, label), file=self.prediction_h)\n\n        if nloss == 0:\n            loss = torch.tensor(0.0, requires_grad=True)\n\n        sample_size = nqueries if nqueries > 0 else 1\n        logging_output = {\n            \'loss\': utils.item(loss.data) if reduce else loss.data,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample[\'nsentences\'],\n            \'sample_size\': sample_size,\n            \'ncorrect\': ncorrect,\n            \'nqueries\': nqueries,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(\'loss\', 0) for log in logging_outputs)\n        ntokens = sum(log.get(\'ntokens\', 0) for log in logging_outputs)\n        nsentences = sum(log.get(\'nsentences\', 0) for log in logging_outputs)\n        sample_size = sum(log.get(\'sample_size\', 0) for log in logging_outputs)\n\n        agg_output = {\n            \'loss\': loss_sum / sample_size / math.log(2),\n            \'ntokens\': ntokens,\n            \'nsentences\': nsentences,\n            \'sample_size\': sample_size,\n        }\n\n        ncorrect = sum(log.get(\'ncorrect\', 0) for log in logging_outputs)\n        nqueries = sum(log.get(\'nqueries\', 0) for log in logging_outputs)\n        if nqueries > 0:\n            agg_output[\'accuracy\'] = ncorrect / float(nqueries)\n\n        return agg_output\n\n\n@register_criterion(\'winogrande\')\nclass WinograndeCriterion(WSCCriterion):\n    def forward(self, model, sample, reduce=True):\n        # compute loss and accuracy\n        query_lprobs = self.get_lprobs(\n            model,\n            sample[\'query_tokens\'],\n            sample[\'query_masks\'],\n        )\n        cand_lprobs = self.get_lprobs(\n            model,\n            sample[\'candidate_tokens\'],\n            sample[\'candidate_masks\'],\n        )\n        pred = query_lprobs >= cand_lprobs\n        loss = self.get_loss(query_lprobs, cand_lprobs)\n\n        sample_size = sample[\'query_tokens\'].size(0)\n        ncorrect = pred.sum().item()\n        logging_output = {\n            \'loss\': utils.item(loss.data) if reduce else loss.data,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample[\'nsentences\'],\n            \'sample_size\': sample_size,\n            \'ncorrect\': ncorrect,\n            \'nqueries\': sample_size,\n        }\n        return loss, sample_size, logging_output\n'"
examples/roberta/wsc/wsc_task.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport json\nimport os\nimport tempfile\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,\n    encoders,\n    IdDataset,\n    ListDataset,\n    NestedDictionaryDataset,\n    NumSamplesDataset,\n    NumelDataset,\n    PadDataset,\n    SortDataset,\n)\nfrom fairseq.tasks import FairseqTask, register_task\n\nfrom . import wsc_utils\n\n\n@register_task(\'wsc\')\nclass WSCTask(FairseqTask):\n    """"""Task to finetune RoBERTa for Winograd Schemas.""""""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\'data\', metavar=\'DIR\',\n                            help=\'path to data directory; we load <split>.jsonl\')\n        parser.add_argument(\'--init-token\', type=int, default=None,\n                            help=\'add token at the beginning of each batch item\')\n\n    def __init__(self, args, vocab):\n        super().__init__(args)\n        self.vocab = vocab\n        self.mask = vocab.add_symbol(\'<mask>\')\n\n        self.bpe = encoders.build_bpe(args)\n        self.tokenizer = encoders.build_tokenizer(args)\n\n        # hack to handle GPT-2 BPE, which includes leading spaces\n        if args.bpe == \'gpt2\':\n            self.leading_space = True\n            self.trailing_space = False\n        else:\n            self.leading_space = False\n            self.trailing_space = True\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        """"""Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        """"""\n        dictionary = Dictionary.load(filename)\n        dictionary.add_symbol(\'<mask>\')\n        return dictionary\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        assert args.criterion == \'wsc\', \'Must set --criterion=wsc\'\n\n        # load data and label dictionaries\n        vocab = cls.load_dictionary(os.path.join(args.data, \'dict.txt\'))\n        print(\'| dictionary: {} types\'.format(len(vocab)))\n\n        return cls(args, vocab)\n\n    def binarize(self, s: str, append_eos: bool = False):\n        if self.tokenizer is not None:\n            s = self.tokenizer.encode(s)\n        if self.bpe is not None:\n            s = self.bpe.encode(s)\n        tokens = self.vocab.encode_line(\n            s, append_eos=append_eos, add_if_not_exist=False,\n        ).long()\n        if self.args.init_token is not None:\n            tokens = torch.cat([tokens.new([self.args.init_token]), tokens])\n        return tokens\n\n    def binarize_with_mask(self, txt, prefix, suffix, leading_space, trailing_space):\n        toks = self.binarize(\n            prefix + leading_space + txt + trailing_space + suffix,\n            append_eos=True,\n        )\n        mask = torch.zeros_like(toks, dtype=torch.bool)\n        mask_start = len(self.binarize(prefix))\n        mask_size = len(self.binarize(leading_space + txt))\n        mask[mask_start:mask_start + mask_size] = 1\n        return toks, mask\n\n    def load_dataset(self, split, epoch=1, combine=False, data_path=None, return_only=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        if data_path is None:\n            data_path = os.path.join(self.args.data, split + \'.jsonl\')\n        if not os.path.exists(data_path):\n            raise FileNotFoundError(\'Cannot find data: {}\'.format(data_path))\n\n        query_tokens = []\n        query_masks = []\n        query_lengths = []\n        candidate_tokens = []\n        candidate_masks = []\n        candidate_lengths = []\n        labels = []\n\n        for sentence, pronoun_span, query, label in wsc_utils.jsonl_iterator(data_path):\n            prefix = sentence[:pronoun_span.start].text\n            suffix = sentence[pronoun_span.end:].text_with_ws\n\n            # spaCy spans include trailing spaces, but we need to know about\n            # leading spaces for the GPT-2 BPE\n            leading_space = \' \' if sentence[:pronoun_span.start].text_with_ws.endswith(\' \') else \'\'\n            trailing_space = \' \' if pronoun_span.text_with_ws.endswith(\' \') else \'\'\n\n            # get noun phrases, excluding pronouns and anything overlapping with the query\n            cand_spans = wsc_utils.filter_noun_chunks(\n                wsc_utils.extended_noun_chunks(sentence),\n                exclude_pronouns=True,\n                exclude_query=query,\n                exact_match=False,\n            )\n\n            if query is not None:\n                query_toks, query_mask = self.binarize_with_mask(\n                    query, prefix, suffix, leading_space, trailing_space\n                )\n                query_len = len(query_toks)\n            else:\n                query_toks, query_mask, query_len = None, None, 0\n\n            query_tokens.append(query_toks)\n            query_masks.append(query_mask)\n            query_lengths.append(query_len)\n\n            cand_toks, cand_masks = [], []\n            for cand_span in cand_spans:\n                toks, mask = self.binarize_with_mask(\n                    cand_span.text, prefix, suffix, leading_space, trailing_space,\n                )\n                cand_toks.append(toks)\n                cand_masks.append(mask)\n\n            # collate candidates\n            cand_toks = data_utils.collate_tokens(cand_toks, pad_idx=self.vocab.pad())\n            cand_masks = data_utils.collate_tokens(cand_masks, pad_idx=0)\n            assert cand_toks.size() == cand_masks.size()\n\n            candidate_tokens.append(cand_toks)\n            candidate_masks.append(cand_masks)\n            candidate_lengths.append(cand_toks.size(1))\n\n            labels.append(label)\n\n        query_lengths = np.array(query_lengths)\n        query_tokens = ListDataset(query_tokens, query_lengths)\n        query_masks = ListDataset(query_masks, query_lengths)\n\n        candidate_lengths = np.array(candidate_lengths)\n        candidate_tokens = ListDataset(candidate_tokens, candidate_lengths)\n        candidate_masks = ListDataset(candidate_masks, candidate_lengths)\n\n        labels = ListDataset(labels, [1]*len(labels))\n\n        dataset = {\n            \'id\': IdDataset(),\n            \'query_tokens\': query_tokens,\n            \'query_masks\': query_masks,\n            \'candidate_tokens\': candidate_tokens,\n            \'candidate_masks\': candidate_masks,\n            \'labels\': labels,\n            \'nsentences\': NumSamplesDataset(),\n            \'ntokens\': NumelDataset(query_tokens, reduce=True),\n        }\n\n        nested_dataset = NestedDictionaryDataset(\n            dataset,\n            sizes=[query_lengths],\n        )\n\n        with data_utils.numpy_seed(self.args.seed):\n            shuffle = np.random.permutation(len(query_tokens))\n        dataset = SortDataset(\n            nested_dataset,\n            # shuffle\n            sort_order=[shuffle],\n        )\n\n        if return_only:\n            return dataset\n\n        self.datasets[split] = dataset\n        return self.datasets[split]\n\n    def build_dataset_for_inference(self, sample_json):\n        with tempfile.NamedTemporaryFile(buffering=0) as h:\n            h.write((json.dumps(sample_json) + \'\\n\').encode(\'utf-8\'))\n            dataset = self.load_dataset(\n                \'disambiguate_pronoun\',\n                data_path=h.name,\n                return_only=True,\n            )\n        return dataset\n\n    def disambiguate_pronoun(self, model, sentence, use_cuda=False):\n        sample_json = wsc_utils.convert_sentence_to_json(sentence)\n        dataset = self.build_dataset_for_inference(sample_json)\n        sample = dataset.collater([dataset[0]])\n        if use_cuda:\n            sample = utils.move_to_cuda(sample)\n\n        def get_masked_input(tokens, mask):\n            masked_tokens = tokens.clone()\n            masked_tokens[mask.bool()] = self.mask\n            return masked_tokens\n\n        def get_lprobs(tokens, mask):\n            logits, _ = model(src_tokens=get_masked_input(tokens, mask))\n            lprobs = F.log_softmax(logits, dim=-1, dtype=torch.float)\n            scores = lprobs.gather(2, tokens.unsqueeze(-1)).squeeze(-1)\n            mask = mask.type_as(scores)\n            scores = (scores * mask).sum(dim=-1) / mask.sum(dim=-1)\n            return scores\n\n        cand_lprobs = get_lprobs(\n            sample[\'candidate_tokens\'][0],\n            sample[\'candidate_masks\'][0],\n        )\n        if sample[\'query_tokens\'][0] is not None:\n            query_lprobs = get_lprobs(\n                sample[\'query_tokens\'][0].unsqueeze(0),\n                sample[\'query_masks\'][0].unsqueeze(0),\n            )\n            return (query_lprobs >= cand_lprobs).all().item() == 1\n        else:\n            best_idx = cand_lprobs.argmax().item()\n            full_cand = sample[\'candidate_tokens\'][0][best_idx]\n            mask = sample[\'candidate_masks\'][0][best_idx]\n            toks = full_cand[mask.bool()]\n            return self.bpe.decode(self.source_dictionary.string(toks)).strip()\n\n    @property\n    def source_dictionary(self):\n        return self.vocab\n\n    @property\n    def target_dictionary(self):\n        return self.vocab\n\n\n@register_task(\'winogrande\')\nclass WinograndeTask(WSCTask):\n    """"""\n    Task for WinoGrande dataset. Efficient implementation for Winograd schema\n    tasks with exactly two candidates, one of which is correct.\n    """"""\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        assert args.criterion == \'winogrande\', \'Must set --criterion=winogrande\'\n\n        # load data and label dictionaries\n        vocab = cls.load_dictionary(os.path.join(args.data, \'dict.txt\'))\n        print(\'| dictionary: {} types\'.format(len(vocab)))\n\n        return cls(args, vocab)\n\n    def load_dataset(self, split, epoch=1, combine=False, data_path=None, return_only=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        if data_path is None:\n            data_path = os.path.join(self.args.data, split + \'.jsonl\')\n        if not os.path.exists(data_path):\n            raise FileNotFoundError(\'Cannot find data: {}\'.format(data_path))\n\n        query_tokens = []\n        query_masks = []\n        query_lengths = []\n        candidate_tokens = []\n        candidate_masks = []\n        candidate_lengths = []\n\n        itr = wsc_utils.winogrande_jsonl_iterator(data_path, eval=(split == \'test\'))\n\n        for sample in itr:\n            sentence, pronoun_span, query, cand_text = sample\n            prefix = sentence[:pronoun_span[0]].rstrip()\n            suffix = sentence[pronoun_span[1]:]\n\n            leading_space = \' \' if sentence[:pronoun_span[0]].endswith(\' \') else \'\'\n            trailing_space = \'\'\n\n            if query is not None:\n                query_toks, query_mask = self.binarize_with_mask(\n                    query, prefix, suffix, leading_space, trailing_space,\n                )\n                query_len = len(query_toks)\n            else:\n                query_toks, query_mask, query_len = None, None, 0\n\n            query_tokens.append(query_toks)\n            query_masks.append(query_mask)\n            query_lengths.append(query_len)\n\n            cand_toks, cand_mask = self.binarize_with_mask(\n                cand_text, prefix, suffix, leading_space, trailing_space,\n            )\n\n            candidate_tokens.append(cand_toks)\n            candidate_masks.append(cand_mask)\n            candidate_lengths.append(cand_toks.size(0))\n\n        query_lengths = np.array(query_lengths)\n\n        def get_pad_dataset_fn(tokens, length, pad_idx):\n            return PadDataset(\n                ListDataset(tokens, length),\n                pad_idx=pad_idx,\n                left_pad=False,\n            )\n\n        query_tokens = get_pad_dataset_fn(query_tokens, query_lengths, self.vocab.pad())\n        query_masks = get_pad_dataset_fn(query_masks, query_lengths, 0)\n\n        candidate_lengths = np.array(candidate_lengths)\n        candidate_tokens = get_pad_dataset_fn(candidate_tokens, candidate_lengths, self.vocab.pad())\n        candidate_masks = get_pad_dataset_fn(candidate_masks, candidate_lengths, 0)\n\n        dataset = {\n            \'id\': IdDataset(),\n            \'query_tokens\': query_tokens,\n            \'query_masks\': query_masks,\n            \'candidate_tokens\': candidate_tokens,\n            \'candidate_masks\': candidate_masks,\n            \'nsentences\': NumSamplesDataset(),\n            \'ntokens\': NumelDataset(query_tokens, reduce=True),\n        }\n\n        nested_dataset = NestedDictionaryDataset(\n            dataset,\n            sizes=[query_lengths],\n        )\n\n        with data_utils.numpy_seed(self.args.seed):\n            shuffle = np.random.permutation(len(query_tokens))\n        dataset = SortDataset(\n            nested_dataset,\n            # shuffle\n            sort_order=[shuffle],\n        )\n\n        if return_only:\n            return dataset\n\n        self.datasets[split] = dataset\n        return self.datasets[split]\n'"
examples/roberta/wsc/wsc_utils.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom functools import lru_cache\nimport json\n\n\ndef convert_sentence_to_json(sentence):\n    if \'_\' in sentence:\n        prefix, rest = sentence.split(\'_\', 1)\n        query, rest = rest.split(\'_\', 1)\n        query_index = len(prefix.rstrip().split(\' \'))\n    else:\n        query, query_index = None, None\n\n    prefix, rest = sentence.split(\'[\', 1)\n    pronoun, rest = rest.split(\']\', 1)\n    pronoun_index = len(prefix.rstrip().split(\' \'))\n\n    sentence = sentence.replace(\'_\', \'\').replace(\'[\', \'\').replace(\']\', \'\')\n\n    return {\n        \'idx\': 0,\n        \'text\': sentence,\n        \'target\': {\n            \'span1_index\': query_index,\n            \'span1_text\': query,\n            \'span2_index\': pronoun_index,\n            \'span2_text\': pronoun,\n        },\n    }\n\n\ndef extended_noun_chunks(sentence):\n    noun_chunks = {(np.start, np.end) for np in sentence.noun_chunks}\n    np_start, cur_np = 0, \'NONE\'\n    for i, token in enumerate(sentence):\n        np_type = token.pos_ if token.pos_ in {\'NOUN\', \'PROPN\'} else \'NONE\'\n        if np_type != cur_np:\n            if cur_np != \'NONE\':\n                noun_chunks.add((np_start, i))\n            if np_type != \'NONE\':\n                np_start = i\n            cur_np = np_type\n    if cur_np != \'NONE\':\n        noun_chunks.add((np_start, len(sentence)))\n    return [sentence[s:e] for (s, e) in sorted(noun_chunks)]\n\n\ndef find_token(sentence, start_pos):\n    found_tok = None\n    for tok in sentence:\n        if tok.idx == start_pos:\n            found_tok = tok\n            break\n    return found_tok\n\n\ndef find_span(sentence, search_text, start=0):\n    search_text = search_text.lower()\n    for tok in sentence[start:]:\n        remainder = sentence[tok.i:].text.lower()\n        if remainder.startswith(search_text):\n            len_to_consume = len(search_text)\n            start_idx = tok.idx\n            for next_tok in sentence[tok.i:]:\n                end_idx = next_tok.idx + len(next_tok.text)\n                if end_idx - start_idx == len_to_consume:\n                    span = sentence[tok.i:next_tok.i + 1]\n                    return span\n    return None\n\n\n@lru_cache(maxsize=1)\ndef get_detokenizer():\n    from sacremoses import MosesDetokenizer\n    detok = MosesDetokenizer(lang=\'en\')\n    return detok\n\n\n@lru_cache(maxsize=1)\ndef get_spacy_nlp():\n    import en_core_web_lg\n    nlp = en_core_web_lg.load()\n    return nlp\n\n\ndef jsonl_iterator(input_fname, positive_only=False, ngram_order=3, eval=False):\n    detok = get_detokenizer()\n    nlp = get_spacy_nlp()\n\n    with open(input_fname) as fin:\n        for line in fin:\n            sample = json.loads(line.strip())\n\n            if positive_only and \'label\' in sample and not sample[\'label\']:\n                # only consider examples where the query is correct\n                continue\n\n            target = sample[\'target\']\n\n            # clean up the query\n            query = target[\'span1_text\']\n            if query is not None:\n                if \'\\n\' in query:\n                    continue\n                if query.endswith(\'.\') or query.endswith(\',\'):\n                    query = query[:-1]\n\n            # split tokens\n            tokens = sample[\'text\'].split(\' \')\n\n            def strip_pronoun(x):\n                return x.rstrip(\'.,""\')\n\n            # find the pronoun\n            pronoun_idx = target[\'span2_index\']\n            pronoun = strip_pronoun(target[\'span2_text\'])\n            if strip_pronoun(tokens[pronoun_idx]) != pronoun:\n                # hack: sometimes the index is misaligned\n                if strip_pronoun(tokens[pronoun_idx + 1]) == pronoun:\n                    pronoun_idx += 1\n                else:\n                    raise Exception(\'Misaligned pronoun!\')\n            assert strip_pronoun(tokens[pronoun_idx]) == pronoun\n\n            # split tokens before and after the pronoun\n            before = tokens[:pronoun_idx]\n            after = tokens[pronoun_idx + 1:]\n\n            # the GPT BPE attaches leading spaces to tokens, so we keep track\n            # of whether we need spaces before or after the pronoun\n            leading_space = \' \' if pronoun_idx > 0 else \'\'\n            trailing_space = \' \' if len(after) > 0 else \'\'\n\n            # detokenize\n            before = detok.detokenize(before, return_str=True)\n            pronoun = detok.detokenize([pronoun], return_str=True)\n            after = detok.detokenize(after, return_str=True)\n\n            # hack: when the pronoun ends in a period (or comma), move the\n            # punctuation to the ""after"" part\n            if pronoun.endswith(\'.\') or pronoun.endswith(\',\'):\n                after = pronoun[-1] + trailing_space + after\n                pronoun = pronoun[:-1]\n\n            # hack: when the ""after"" part begins with a comma or period, remove\n            # the trailing space\n            if after.startswith(\'.\') or after.startswith(\',\'):\n                trailing_space = \'\'\n\n            # parse sentence with spacy\n            sentence = nlp(before + leading_space + pronoun + trailing_space + after)\n\n            # find pronoun span\n            start = len(before + leading_space)\n            first_pronoun_tok = find_token(sentence, start_pos=start)\n            pronoun_span = find_span(sentence, pronoun, start=first_pronoun_tok.i)\n            assert pronoun_span.text == pronoun\n\n            if eval:\n                # convert to format where pronoun is surrounded by ""[]"" and\n                # query is surrounded by ""_""\n                query_span = find_span(sentence, query)\n                query_with_ws = \'_{}_{}\'.format(\n                    query_span.text,\n                    (\' \' if query_span.text_with_ws.endswith(\' \') else \'\')\n                )\n                pronoun_with_ws = \'[{}]{}\'.format(\n                    pronoun_span.text,\n                    (\' \' if pronoun_span.text_with_ws.endswith(\' \') else \'\')\n                )\n                if query_span.start < pronoun_span.start:\n                    first = (query_span, query_with_ws)\n                    second = (pronoun_span, pronoun_with_ws)\n                else:\n                    first = (pronoun_span, pronoun_with_ws)\n                    second = (query_span, query_with_ws)\n                sentence = (\n                    sentence[:first[0].start].text_with_ws\n                    + first[1]\n                    + sentence[first[0].end:second[0].start].text_with_ws\n                    + second[1]\n                    + sentence[second[0].end:].text\n                )\n                yield sentence, sample.get(\'label\', None)\n            else:\n                yield sentence, pronoun_span, query, sample.get(\'label\', None)\n\n\ndef winogrande_jsonl_iterator(input_fname, eval=False):\n    with open(input_fname) as fin:\n        for line in fin:\n            sample = json.loads(line.strip())\n            sentence, option1, option2 = sample[\'sentence\'], sample[\'option1\'],\\\n                sample[\'option2\']\n\n            pronoun_span = (sentence.index(\'_\'), sentence.index(\'_\') + 1)\n\n            if eval:\n                query, cand = option1, option2\n            else:\n                query = option1 if sample[\'answer\'] == \'1\' else option2\n                cand = option2 if sample[\'answer\'] == \'1\' else option1\n            yield sentence, pronoun_span, query, cand\n\n\ndef filter_noun_chunks(chunks, exclude_pronouns=False, exclude_query=None, exact_match=False):\n    if exclude_pronouns:\n        chunks = [\n            np for np in chunks if (\n                np.lemma_ != \'-PRON-\'\n                and not all(tok.pos_ == \'PRON\' for tok in np)\n            )\n        ]\n\n    if exclude_query is not None:\n        excl_txt = [exclude_query.lower()]\n        filtered_chunks = []\n        for chunk in chunks:\n            lower_chunk = chunk.text.lower()\n            found = False\n            for excl in excl_txt:\n                if (\n                    (not exact_match and (lower_chunk in excl or excl in lower_chunk))\n                    or lower_chunk == excl\n                ):\n                    found = True\n                    break\n            if not found:\n                filtered_chunks.append(chunk)\n        chunks = filtered_chunks\n\n    return chunks\n'"
examples/simultaneous_translation/criterions/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith("".py"") and not file.startswith(""_""):\n        criterion_name = file[: file.find("".py"")]\n        importlib.import_module(\n            ""examples.simultaneous_translation.criterions."" + criterion_name\n        )\n'"
examples/simultaneous_translation/criterions/label_smoothed_cross_entropy_latency_augmented.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.criterions import register_criterion\nfrom fairseq.criterions.label_smoothed_cross_entropy import (\n    LabelSmoothedCrossEntropyCriterion\n)\n\nfrom examples.simultaneous_translation.utils.latency import (\n    LatencyTraining\n)\n\n\n@register_criterion(\'latency_augmented_label_smoothed_cross_entropy\')\nclass LatencyAugmentedLabelSmoothedCrossEntropyCriterion(\n    LabelSmoothedCrossEntropyCriterion\n):\n\n    def __init__(self, args, task):\n        super().__init__(args, task)\n        self.eps = args.label_smoothing\n        self.latency_weight_avg = args.latency_weight_avg\n        self.latency_weight_avg_type = args.latency_weight_avg_type\n        self.latency_weight_var = args.latency_weight_var\n        self.latency_weight_var_type = args.latency_weight_var_type\n        self.mass_preservation = args.mass_preservation\n        self.average_method = args.average_method\n        self.latency_train = LatencyTraining(\n            self.latency_weight_avg,\n            self.latency_weight_var,\n            self.latency_weight_avg_type,\n            self.latency_weight_var_type,\n            self.mass_preservation,\n            self.average_method,\n        )\n\n    @staticmethod\n    def add_args(parser):\n        super(\n            LatencyAugmentedLabelSmoothedCrossEntropyCriterion,\n            LatencyAugmentedLabelSmoothedCrossEntropyCriterion\n        ).add_args(parser)\n        """"""Add criterion-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(""--latency-weight-avg"", default=0., type=float, metavar=\'D\',\n                            help=""Average loss weight"")\n        parser.add_argument(""--latency-weight-var"", default=0., type=float, metavar=\'D\',\n                            help=""Variance loss weight"")\n        parser.add_argument(""--latency-weight-avg-type"", default=""differentiable_average_lagging"",\n                            help=""Statistics for Average loss type"")\n        parser.add_argument(""--latency-weight-var-type"", default=""variance_delay"",\n                            help=""Statistics for variance loss type"")\n        parser.add_argument(""--average-method"", default=""weighted_average"",\n                            help=""Average loss type"")\n        # fmt: on\n\n    def compute_loss(self, model, net_output, sample, reduce=True):\n        # Compute cross entropy loss first\n        loss, nll_loss = super().compute_loss(model, net_output, sample, reduce)\n\n        # Obtain the expected alignment\n        attn_list = [item[""alpha""] for item in net_output[-1][""attn_list""]]\n\n        target_padding_mask = model.get_targets(sample, net_output).eq(self.padding_idx)\n\n        source_padding_mask = net_output[-1].get(""encoder_padding_mask"", None)\n\n        # Get latency loss\n        latency_loss = self.latency_train.loss(\n            attn_list, source_padding_mask, target_padding_mask)\n\n        loss += latency_loss\n\n        return loss, nll_loss\n'"
examples/simultaneous_translation/eval/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n'"
examples/simultaneous_translation/eval/client.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport requests\nfrom typing import Optional\nfrom scorers import build_scorer\n\n\nclass SimulSTEvaluationService(object):\n    DEFAULT_HOSTNAME = \'localhost\'\n    DEFAULT_PORT = 12321\n\n    def __init__(self, hostname=DEFAULT_HOSTNAME, port=DEFAULT_PORT):\n        self.hostname = hostname\n        self.port = port\n        self.base_url = f\'http://{self.hostname}:{self.port}\'\n\n    def __enter__(self):\n        self.new_session()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def new_session(self):\n        # start eval session\n        url = f\'{self.base_url}\'\n\n        try:\n            _ = requests.post(url)\n        except Exception as e:\n            print(f\'Failed to start an evaluation session: {e}\')\n\n        print(\'Evaluation session started.\')\n        return self\n\n    def get_scores(self):\n        # end eval session\n        url = f\'{self.base_url}/result\'\n        try:\n            r = requests.get(url)\n            print(\'Scores: {}\'.format(r.json()))\n            print(\'Evaluation session finished.\')\n        except Exception as e:\n            print(f\'Failed to end an evaluation session: {e}\')\n\n    def get_src(self, sent_id: int, extra_params: Optional[dict] = None) -> str:\n        url = f\'{self.base_url}/src\'\n        params = {""sent_id"": sent_id}\n        if extra_params is not None:\n            for key in extra_params.keys():\n                params[key] = extra_params[key]\n        try:\n            r = requests.get(\n                url,\n                params=params\n            )\n        except Exception as e:\n            print(f\'Failed to request a source segment: {e}\')\n        return r.json()\n\n    def send_hypo(self, sent_id: int, hypo: str) -> None:\n        url = f\'{self.base_url}/hypo\'\n        params = {""sent_id"": sent_id}\n\n        try:\n            requests.put(url, params=params, data=hypo.encode(""utf-8""))\n        except Exception as e:\n            print(f\'Failed to send a translated segment: {e}\')\n\n    def corpus_info(self):\n        url = f\'{self.base_url}\'\n        try:\n            r = requests.get(url)\n        except Exception as e:\n            print(f\'Failed to request corpus information: {e}\')\n\n        return r.json()\n\n\nclass SimulSTLocalEvaluationService(object):\n    def __init__(self, args):\n        self.scorer = build_scorer(args)\n\n    def get_scores(self):\n        return self.scorer.score()\n\n    def get_src(self, sent_id: int, extra_params: Optional[dict] = None) -> str:\n        if extra_params is not None:\n            segment_size = extra_params.get(""segment_size"", None)\n        else:\n            segment_size = None\n\n        return self.scorer.send_src(int(sent_id), segment_size)\n\n    def send_hypo(self, sent_id: int, hypo: str) -> None:\n        list_of_tokens = hypo.strip().split()\n        self.scorer.recv_hyp(sent_id, list_of_tokens)\n\n    def corpus_info(self):\n        return self.scorer.get_info()\n'"
examples/simultaneous_translation/eval/eval_latency.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom examples.simultaneous_translation.utils.latency import LatencyInference\nimport argparse\nimport torch\nimport json\n\n\nLATENCY_METRICS = [\n    \'differentiable_average_lagging\',\n    \'average_lagging\',\n    \'average_proportion\',\n]\n\n\nclass LatencyScorer():\n    def __init__(self, start_from_zero=True):\n        self.recorder = []\n        self.scores = {}\n        self.scorer = LatencyInference()\n        self.start_from_zero = start_from_zero\n\n    def update_reorder(self, list_of_dict):\n        self.recorder = []\n        for info in list_of_dict:\n            delays = [\n                int(x) - int(not self.start_from_zero)\n                for x in info[""delays""]\n            ]\n            delays = torch.LongTensor(delays).unsqueeze(0)\n            src_len = torch.LongTensor([info[""src_len""]]).unsqueeze(0)\n\n            self.recorder.append(self.scorer(delays, src_len))\n\n    def cal_latency(self):\n        self.scores = {}\n        for metric in LATENCY_METRICS:\n            self.scores[metric] = sum(\n                [x[metric][0, 0].item() for x in self.recorder]\n            ) / len(self.recorder)\n        return self.scores\n\n    @classmethod\n    def score(cls, list_of_dict, start_from_zero=True):\n        scorer_to_return = cls(start_from_zero)\n        scorer_to_return.update_reorder(list_of_dict)\n        scorer_to_return.cal_latency()\n        return scorer_to_return.scores\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--input"", required=True)\n    parser.add_argument(""--start-from-zero"", action=""store_true"")\n    args = parser.parse_args()\n\n    scorer = LatencyInference()\n    recorder = []\n    with open(args.input, \'r\') as f:\n        for line in f:\n            info = json.loads(line)\n\n            delays = [int(x) - int(not args.start_from_zero) for x in info[""delays""]]\n\n            delays = torch.LongTensor(delays).unsqueeze(0)\n\n            src_len = torch.LongTensor([info[""src_len""]]).unsqueeze(0)\n\n            recorder.append(scorer(delays, src_len))\n\n    average_results = {}\n\n    for metric in LATENCY_METRICS:\n        average_results[metric] = sum(\n            [x[metric][0, 0].item() for x in recorder]\n        ) / len(recorder)\n        print(f""{metric}: {average_results[metric]}"")\n'"
examples/simultaneous_translation/eval/evaluate.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\n\nfrom client import SimulSTEvaluationService, SimulSTLocalEvaluationService\nfrom fairseq.registry import REGISTRIES\nfrom agents import build_agent\n\nDEFAULT_HOSTNAME = \'localhost\'\nDEFAULT_PORT = 12321\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--hostname\', type=str, default=DEFAULT_HOSTNAME,\n                        help=\'server hostname\')\n    parser.add_argument(\'--port\', type=int, default=DEFAULT_PORT,\n                        help=\'server port number\')\n    parser.add_argument(\'--agent-type\', default=\'simul_trans_text\',\n                        help=\'Agent type\')\n    parser.add_argument(\'--scorer-type\', default=\'text\',\n                        help=\'Scorer type\')\n    parser.add_argument(\'--start-idx\', type=int, default=0,\n                        help=\'Start index of the sentence to evaluate\')\n    parser.add_argument(\'--end-idx\', type=int, default=float(\'inf\'),\n                        help=\'End index of the sentence to evaluate\')\n    parser.add_argument(\'--scores\', action=""store_true"",\n                        help=\'Request scores from server\')\n    parser.add_argument(\'--reset-server\', action=""store_true"",\n                        help=\'Reset the server\')\n    parser.add_argument(\'--num-threads\', type=int, default=10,\n                        help=\'Number of threads used by agent\')\n    parser.add_argument(\'--local\', action=""store_true"", default=False,\n                        help=\'Local evaluation\')\n\n    args, _ = parser.parse_known_args()\n\n    for registry_name, REGISTRY in REGISTRIES.items():\n        choice = getattr(args, registry_name, None)\n        if choice is not None:\n            cls = REGISTRY[""registry""][choice]\n            if hasattr(cls, ""add_args""):\n                cls.add_args(parser)\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == ""__main__"":\n    args = get_args()\n\n    if args.local:\n        session = SimulSTLocalEvaluationService(args)\n    else:\n        session = SimulSTEvaluationService(args.hostname, args.port)\n\n    if args.reset_server:\n        session.new_session()\n\n    if args.agent_type is not None:\n        agent = build_agent(args)\n        agent.decode(session, args.start_idx, args.end_idx, args.num_threads)\n\n    if args.scores:\n        session.get_scores()\n    print(session.get_scores())\n'"
examples/simultaneous_translation/eval/server.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport argparse\nimport sys\nimport json\nfrom tornado import web, ioloop\nfrom scorers import build_scorer\n\nDEFAULT_HOSTNAME = \'localhost\'\nDEFAULT_PORT = 12321\n\n\nclass ScorerHandler(web.RequestHandler):\n    def initialize(self, scorer):\n        self.scorer = scorer\n\n\nclass EvalSessionHandler(ScorerHandler):\n    def post(self):\n        self.scorer.reset()\n\n    def get(self):\n        r = json.dumps(self.scorer.get_info())\n        self.write(r)\n\n\nclass ResultHandler(ScorerHandler):\n    def get(self):\n        r = json.dumps(self.scorer.score())\n        self.write(r)\n\n\nclass SourceHandler(ScorerHandler):\n    def get(self):\n        sent_id = int(self.get_argument(\'sent_id\'))\n        segment_size = None\n        if ""segment_size"" in self.request.arguments:\n            string = self.get_argument(\'segment_size\')\n            if len(string) > 0:\n                segment_size = int(string)\n\n        r = json.dumps(self.scorer.send_src(int(sent_id), segment_size))\n\n        self.write(r)\n\n\nclass HypothesisHandler(ScorerHandler):\n    def put(self):\n        sent_id = int(self.get_argument(\'sent_id\'))\n        list_of_tokens = self.request.body.decode(\'utf-8\').strip().split()\n        self.scorer.recv_hyp(sent_id, list_of_tokens)\n\n\ndef add_args():\n    parser = argparse.ArgumentParser()\n    # fmt: off\n    parser.add_argument(\'--hostname\', type=str, default=DEFAULT_HOSTNAME,\n                        help=\'Server hostname\')\n    parser.add_argument(\'--port\', type=int, default=DEFAULT_PORT,\n                        help=\'Server port number\')\n\n    args, _ = parser.parse_known_args()\n    # fmt: on\n    return args\n\n\ndef start_server(scorer, hostname=DEFAULT_HOSTNAME, port=DEFAULT_PORT, debug=False):\n    app = web.Application([\n        (r\'/result\', ResultHandler, dict(scorer=scorer)),\n        (r\'/src\', SourceHandler, dict(scorer=scorer)),\n        (r\'/hypo\', HypothesisHandler, dict(scorer=scorer)),\n        (r\'/\', EvalSessionHandler, dict(scorer=scorer)),\n    ], debug=debug)\n    app.listen(port, max_buffer_size=1024 ** 3)\n    sys.stdout.write(f""Evaluation Server Started. Listening to port {port}\\n"")\n    ioloop.IOLoop.current().start()\n\n\nif __name__ == \'__main__\':\n    args = add_args()\n    scorer = build_scorer(args)\n    start_server(scorer, args.hostname, args.port, args.debug)\n'"
examples/simultaneous_translation/models/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        model_name = file[:file.find('.py')]\n        importlib.import_module('examples.simultaneous_translation.models.' + model_name)\n"""
examples/simultaneous_translation/models/transformer_monotonic_attention.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq.models import (\n    register_model,\n    register_model_architecture,\n)\n\n\nfrom fairseq.models.transformer import (\n    TransformerModel,\n    TransformerEncoder,\n    TransformerDecoder,\n    base_architecture,\n    transformer_iwslt_de_en,\n    transformer_vaswani_wmt_en_de_big,\n)\n\n\nfrom examples.simultaneous_translation.modules.monotonic_transformer_layer import (\n    TransformerMonotonicDecoderLayer,\n    TransformerMonotonicEncoderLayer\n)\n\nDEFAULT_MAX_SOURCE_POSITIONS = 1024\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\n\n@register_model(\'transformer_unidirectional\')\nclass TransformerUnidirectionalModel(TransformerModel):\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return TransformerMonotonicEncoder(args, src_dict, embed_tokens)\n\n\n@register_model(\'transformer_monotonic\')\nclass TransformerMonotonicModel(TransformerModel):\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return TransformerMonotonicEncoder(args, src_dict, embed_tokens)\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        return TransformerMonotonicDecoder(args, tgt_dict, embed_tokens)\n\n    def _indices_from_states(self, states):\n        if type(states[""indices""][""src""]) == list:\n            if next(self.parameters()).is_cuda:\n                tensor = torch.cuda.LongTensor\n            else:\n                tensor = torch.LongTensor\n\n            src_indices = tensor(\n                [states[""indices""][""src""][: 1 + states[""steps""][""src""]]]\n            )\n\n            tgt_indices = tensor(\n                [\n                    [self.decoder.dictionary.eos()]\n                    + states[""indices""][""tgt""]\n                ]\n            )\n        else:\n            src_indices = states[""indices""][""src""][: 1 +\n                                                   states[""steps""][""src""]]\n            tgt_indices = states[""indices""][""tgt""]\n\n        return src_indices, None, tgt_indices\n\n    def predict_from_states(self, states):\n        decoder_states = self.decoder.output_layer(\n            states[""decoder_features""]\n        )\n        lprobs = self.get_normalized_probs(\n            [decoder_states[:, -1:]],\n            log_probs=True\n        )\n\n        index = lprobs.argmax(dim=-1)\n\n        token = self.decoder.dictionary.string(index)\n\n        return token, index[0, 0].item()\n\n    def decision_from_states(self, states):\n        \'\'\'\n        This funcion take states dictionary as input, and gives the agent\n        a decision of whether read a token from server. Moreover, the decoder\n        states are also calculated here so we can directly generate a target\n        token without recompute every thing\n        \'\'\'\n\n        self.eval()\n\n        if len(states[""tokens""][""src""]) == 0:\n            return 0\n\n        src_indices, src_lengths, tgt_indices = self._indices_from_states(\n            states)\n\n        # Update encoder states if needed\n        if (\n            ""encoder_states"" not in states or\n            states[""encoder_states""][0].size(1) <= states[""steps""][""src""]\n        ):\n            encoder_out_dict = self.encoder(src_indices, src_lengths)\n            states[""encoder_states""] = encoder_out_dict\n        else:\n            encoder_out_dict = states[""encoder_states""]\n\n        # online means we still need tokens to feed the model\n        states[""model_states""][""online""] = not (\n            states[""finish_read""]\n            and len(states[""tokens""][""src""]) == states[""steps""][""src""]\n        )\n\n        states[""model_states""][""steps""] = states[""steps""]\n\n        x, outputs = self.decoder.forward(\n            prev_output_tokens=tgt_indices,\n            encoder_out=encoder_out_dict,\n            incremental_state=states[""model_states""],\n            features_only=True,\n        )\n\n        states[""decoder_features""] = x\n\n        return outputs[""action""]\n\n\nclass TransformerMonotonicEncoder(TransformerEncoder):\n\n    def __init__(self, args, dictionary, embed_tokens):\n        super().__init__(args, dictionary, embed_tokens)\n\n        self.dictionary = dictionary\n        self.layers = nn.ModuleList([])\n        self.layers.extend([\n            TransformerMonotonicEncoderLayer(args)\n            for i in range(args.encoder_layers)\n        ])\n\n\nclass TransformerMonotonicDecoder(TransformerDecoder):\n    """"""\n    Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`TransformerDecoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): decoding dictionary\n        embed_tokens (torch.nn.Embedding): output embedding\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n            (default: False).\n    """"""\n\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n        super().__init__(args, dictionary, embed_tokens, no_encoder_attn=False)\n\n        self.dictionary = dictionary\n        self.layers = nn.ModuleList([])\n        self.layers.extend([\n            TransformerMonotonicDecoderLayer(args, no_encoder_attn)\n            for _ in range(args.decoder_layers)\n        ])\n\n    def pre_attention(\n        self, prev_output_tokens, encoder_out_dict,\n        incremental_state=None\n    ):\n        positions = self.embed_positions(\n            prev_output_tokens,\n            incremental_state=incremental_state,\n        ) if self.embed_positions is not None else None\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            if positions is not None:\n                positions = positions[:, -1:]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        if positions is not None:\n            x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_out = encoder_out_dict.encoder_out\n        encoder_padding_mask = encoder_out_dict.encoder_padding_mask\n\n        return x, encoder_out, encoder_padding_mask\n\n    def post_attention(self, x):\n        if self.layer_norm:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        return x\n\n    def extract_features(\n        self, prev_output_tokens, encoder_out,\n        incremental_state=None, **unused\n    ):\n        """"""\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        """"""\n        # incremental_state = None\n        (\n            x,\n            encoder_outs,\n            encoder_padding_mask\n        ) = self.pre_attention(\n            prev_output_tokens,\n            encoder_out,\n            incremental_state\n        )\n        attn = None\n        inner_states = [x]\n        attn_list = []\n        step_list = []\n\n        for i, layer in enumerate(self.layers):\n\n            x, attn, _ = layer(\n                x=x,\n                encoder_out=encoder_outs,\n                encoder_padding_mask=encoder_padding_mask,\n                incremental_state=incremental_state,\n                self_attn_mask=self.buffered_future_mask(x)\n                if incremental_state is None else None,\n            )\n\n            inner_states.append(x)\n            attn_list.append(attn)\n\n            if incremental_state is not None:\n                curr_steps = layer.get_steps(incremental_state)\n                step_list.append(curr_steps)\n\n                if incremental_state.get(""online"", False):\n                    p_choose = attn[""p_choose""].squeeze(0).squeeze(1).gather(1, curr_steps.t())\n\n                    new_steps = (\n                        curr_steps\n                        + (p_choose < 0.5).t().type_as(curr_steps)\n                    )\n\n                    if (new_steps >= incremental_state[""steps""][""src""]).any():\n                        # We need to prune the last self_attn saved_state\n                        # if model decide not to read\n                        # otherwise there will be duplicated saved_state\n                        for j in range(i + 1):\n                            self.layers[j].prune_incremental_state(\n                                incremental_state)\n\n                        return x, {""action"": 0}\n\n        if (\n            incremental_state is not None\n            and not incremental_state.get(""online"", False)\n        ):\n            # Here is for fast evaluation\n            fastest_step = torch.max(\n                torch.cat(step_list, dim=1),\n                dim=1,\n                keepdim=True\n            )[0] + 1\n\n            if ""fastest_step"" in incremental_state:\n                incremental_state[""fastest_step""] = torch.cat(\n                    [incremental_state[""fastest_step""], fastest_step],\n                    dim=1\n                )\n            else:\n                incremental_state[""fastest_step""] = fastest_step\n\n        x = self.post_attention(x)\n\n        return x, {\n            ""action"": 1,\n            ""attn_list"": attn_list,\n            ""step_list"": step_list,\n            ""encoder_out"": encoder_out,\n            ""encoder_padding_mask"": encoder_padding_mask,\n        }\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        super().reorder_incremental_state(incremental_state, new_order)\n        if ""fastest_step"" in incremental_state:\n            incremental_state[""fastest_step""] = (\n                incremental_state[""fastest_step""]\n                .index_select(0, new_order)\n            )\n\n\n@register_model_architecture(\n    \'transformer_monotonic\',\n    \'transformer_monotonic\'\n)\ndef base_monotonic_rchitecture(args):\n    base_architecture(args)\n    args.encoder_unidirectional = getattr(\n        args, \'encoder_unidirectional\', False)\n\n\n@register_model_architecture(\n    \'transformer_monotonic\',\n    \'transformer_monotonic_iwslt_de_en\'\n)\ndef transformer_monotonic_iwslt_de_en(args):\n    transformer_iwslt_de_en(args)\n    base_monotonic_rchitecture(args)\n\n\n# parameters used in the ""Attention Is All You Need"" paper (Vaswani et al., 2017)\n@register_model_architecture(\n    \'transformer_monotonic\',\n    \'transformer_monotonic_vaswani_wmt_en_de_big\'\n)\ndef transformer_monotonic_vaswani_wmt_en_de_big(args):\n    transformer_vaswani_wmt_en_de_big(args)\n\n\n@register_model_architecture(\n    \'transformer_monotonic\',\n    \'transformer_monotonic_vaswani_wmt_en_fr_big\'\n)\ndef transformer_monotonic_vaswani_wmt_en_fr_big(args):\n    transformer_monotonic_vaswani_wmt_en_fr_big(args)\n\n\n@register_model_architecture(\n    \'transformer_unidirectional\',\n    \'transformer_unidirectional_iwslt_de_en\'\n)\ndef transformer_unidirectional_iwslt_de_en(args):\n    transformer_iwslt_de_en(args)\n'"
examples/simultaneous_translation/modules/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\nfrom fairseq import registry\n(\n    build_monotonic_attention,\n    register_monotonic_attention,\n    MONOTONIC_ATTENTION_REGISTRY\n) = registry.setup_registry('--simul-type')\n\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        model_name = file[:file.find('.py')]\n        importlib.import_module('examples.simultaneous_translation.modules.' + model_name)\n"""
examples/simultaneous_translation/modules/monotonic_multihead_attention.py,19,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nfrom fairseq import utils\n\nfrom fairseq.modules import MultiheadAttention\n\nfrom examples.simultaneous_translation.utils.functions import (\n    exclusive_cumprod,\n    lengths_to_mask\n)\n\n\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom fairseq.utils import convert_padding_direction\nfrom . import register_monotonic_attention\n\n\n@with_incremental_state\nclass MonotonicAttention(nn.Module):\n    """"""\n    Abstract class of monotonic attentions\n    """"""\n    def __init__(self, args):\n        self.eps = args.attention_eps\n        self.mass_preservation = args.mass_preservation\n\n        self.noise_mean = args.noise_mean\n        self.noise_var = args.noise_var\n\n        self.energy_bias_init = args.energy_bias_init\n        self.energy_bias = (\n            nn.Parameter(self.energy_bias_init * torch.ones([1]))\n            if args.energy_bias is True else 0\n        )\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument(\'--no-mass-preservation\', action=""store_false"", dest=""mass_preservation"",\n                            help=\'Do not stay on the last token when decoding\')\n        parser.add_argument(\'--mass-preservation\', action=""store_true"", dest=""mass_preservation"",\n                            help=\'Stay on the last token when decoding\')\n        parser.set_defaults(mass_preservation=True)\n\n        parser.add_argument(\'--noise-var\', type=float, default=1.0,\n                            help=\'Variance of discretness noise\')\n        parser.add_argument(\'--noise-mean\', type=float, default=0.0,\n                            help=\'Mean of discretness noise\')\n        parser.add_argument(\'--energy-bias\', action=""store_true"", default=False,\n                            help=\'Bias for energy\')\n        parser.add_argument(\'--energy-bias-init\', type=float, default=-2.0,\n                            help=\'Initial value of the bias for energy\')\n        parser.add_argument(\'--attention-eps\', type=float, default=1e-6,\n                            help=\'Epsilon when calculating expected attention\')\n        # fmt: on\n\n    def p_choose(self, *args):\n        raise NotImplementedError\n\n    def input_projections(self, *args):\n        raise NotImplementedError\n\n    def attn_energy(self, q_proj, k_proj, key_padding_mask=None):\n        """"""\n        Calculating monotonic energies\n\n        ============================================================\n        Expected input size\n        q_proj: bsz * num_heads, tgt_len, self.head_dim\n        k_proj: bsz * num_heads, src_len, self.head_dim\n        key_padding_mask: bsz, src_len\n        attn_mask: tgt_len, src_len\n        """"""\n        bsz, tgt_len, embed_dim = q_proj.size()\n        bsz = bsz // self.num_heads\n        src_len = k_proj.size(1)\n\n        attn_energy = torch.bmm(q_proj, k_proj.transpose(1, 2)) + self.energy_bias\n\n        attn_energy = attn_energy.view(bsz, self.num_heads, tgt_len, src_len)\n\n        if key_padding_mask is not None:\n            attn_energy = attn_energy.masked_fill(\n                key_padding_mask.unsqueeze(1).unsqueeze(2).bool(),\n                float(\'-inf\'),\n            )\n\n        return attn_energy\n\n    def expected_alignment_train(self, p_choose, key_padding_mask):\n        """"""\n        Calculating expected alignment for MMA\n        Mask is not need because p_choose will be 0 if masked\n\n        q_ij = (1 \xe2\x88\x92 p_{ij\xe2\x88\x921})q_{ij\xe2\x88\x921} + a+{i\xe2\x88\x921j}\n        a_ij = p_ij q_ij\n\n        parellel solution:\n        ai = p_i * cumprod(1 \xe2\x88\x92 pi) * cumsum(a_i / cumprod(1 \xe2\x88\x92 pi))\n\n        ============================================================\n        Expected input size\n        p_choose: bsz * num_heads, tgt_len, src_len\n        """"""\n\n        # p_choose: bsz * num_heads, tgt_len, src_len\n        bsz_num_heads, tgt_len, src_len = p_choose.size()\n\n        # cumprod_1mp : bsz * num_heads, tgt_len, src_len\n        cumprod_1mp = exclusive_cumprod(1 - p_choose, dim=2, eps=self.eps)\n        cumprod_1mp_clamp = torch.clamp(cumprod_1mp, self.eps, 1.0)\n\n        init_attention = p_choose.new_zeros([bsz_num_heads, 1, src_len])\n        init_attention[:, :, 0] = 1.0\n\n        previous_attn = [init_attention]\n\n        for i in range(tgt_len):\n            # p_choose: bsz * num_heads, tgt_len, src_len\n            # cumprod_1mp_clamp : bsz * num_heads, tgt_len, src_len\n            # previous_attn[i]: bsz * num_heads, 1, src_len\n            # alpha_i: bsz * num_heads, src_len\n            alpha_i = (\n                p_choose[:, i]\n                * cumprod_1mp[:, i]\n                * torch.cumsum(\n                    previous_attn[i][:, 0] / cumprod_1mp_clamp[:, i],\n                    dim=1\n                )\n            ).clamp(0, 1.0)\n            previous_attn.append(alpha_i.unsqueeze(1))\n\n        # alpha: bsz * num_heads, tgt_len, src_len\n        alpha = torch.cat(previous_attn[1:], dim=1)\n\n        if self.mass_preservation:\n            # Last token has the residual probabilities\n            alpha[:, :, -1] = 1 - alpha[:, :, :-1].sum(dim=-1).clamp(0.0, 1.0)\n\n        assert not torch.isnan(alpha).any(), ""NaN detected in alpha.""\n\n        return alpha\n\n    def expected_alignment_infer(self, p_choose, key_padding_mask, incremental_state):\n        """"""\n        Calculating mo alignment for MMA during inference time\n\n        ============================================================\n        Expected input size\n        p_choose: bsz * num_heads, tgt_len, src_len\n        key_padding_mask: bsz * src_len\n        incremental_state: dict\n        """"""\n        # p_choose: bsz * self.num_heads, src_len\n        bsz_num_heads, tgt_len, src_len = p_choose.size()\n        # One token at a time\n        assert tgt_len == 1\n        p_choose = p_choose[:, 0, :]\n\n        monotonic_cache = self._get_monotonic_buffer(incremental_state)\n\n        # prev_monotonic_step: bsz, num_heads\n        bsz = bsz_num_heads // self.num_heads\n        prev_monotonic_step = monotonic_cache.get(\n            ""step"",\n            p_choose.new_zeros([bsz, self.num_heads]).long()\n        )\n        bsz, num_heads = prev_monotonic_step.size()\n        assert num_heads == self.num_heads\n        assert bsz * num_heads == bsz_num_heads\n\n        # p_choose: bsz, num_heads, src_len\n        p_choose = p_choose.view(bsz, num_heads, src_len)\n\n        if key_padding_mask is not None:\n            src_lengths = src_len - \\\n                key_padding_mask.sum(dim=1, keepdim=True).long()\n        else:\n            src_lengths = prev_monotonic_step.new_ones(bsz, 1) * src_len\n\n        # src_lengths: bsz, num_heads\n        src_lengths = src_lengths.expand_as(prev_monotonic_step)\n        # new_monotonic_step: bsz, num_heads\n        new_monotonic_step = prev_monotonic_step\n\n        step_offset = 0\n        if key_padding_mask is not None:\n            if key_padding_mask[:, 0].any():\n                # left_pad_source = True:\n                step_offset = key_padding_mask.sum(dim=-1, keepdim=True)\n\n        max_steps = (\n            src_lengths - 1 if self.mass_preservation\n            else src_lengths\n        )\n\n        # finish_read: bsz, num_heads\n        finish_read = new_monotonic_step.eq(max_steps)\n\n        while finish_read.sum().item() < bsz * self.num_heads:\n            # p_choose: bsz * self.num_heads, src_len\n            # only choose the p at monotonic steps\n            # p_choose_i: bsz , self.num_heads\n            p_choose_i = (\n                p_choose\n                .gather(\n                    2,\n                    (step_offset + new_monotonic_step).unsqueeze(2)\n                    .clamp(0, src_len - 1)\n                )\n            ).squeeze(2)\n\n            action = (\n                (p_choose_i < 0.5)\n                .type_as(prev_monotonic_step)\n                .masked_fill(finish_read, 0)\n            )\n            # 1 x bsz\n            # sample actions on unfinished seq\n            # 1 means stay, finish reading\n            # 0 means leave, continue reading\n            # dist = torch.distributions.bernoulli.Bernoulli(p_choose)\n            # action = dist.sample().type_as(finish_read) * (1 - finish_read)\n\n            new_monotonic_step += action\n\n            finish_read = new_monotonic_step.eq(max_steps) | (action == 0)\n            # finish_read = (~ (finish_read.sum(dim=1, keepdim=True) < self.num_heads / 2)) | finish_read\n\n        monotonic_cache[""step""] = new_monotonic_step\n\n        # alpha: bsz * num_heads, 1, src_len\n        # new_monotonic_step: bsz, num_heads\n        alpha = (\n            p_choose\n            .new_zeros([bsz * self.num_heads, src_len])\n            .scatter(\n                1,\n                (step_offset + new_monotonic_step).view(bsz *\n                                                        self.num_heads, 1).clamp(0, src_len - 1),\n                1\n            )\n        )\n\n        if not self.mass_preservation:\n            alpha = alpha.masked_fill(\n                (new_monotonic_step == max_steps).view(bsz * self.num_heads, 1),\n                0\n            )\n\n        alpha = alpha.unsqueeze(1)\n\n        self._set_monotonic_buffer(incremental_state, monotonic_cache)\n\n        return alpha\n\n    def v_proj_output(self, value):\n        raise NotImplementedError\n\n    def forward(\n        self, query, key, value,\n        key_padding_mask=None, incremental_state=None, *args, **kwargs,\n    ):\n\n        tgt_len, bsz, embed_dim = query.size()\n        src_len = value.size(0)\n\n        # stepwise prob\n        # p_choose: bsz * self.num_heads, tgt_len, src_len\n        p_choose = self.p_choose(query, key, key_padding_mask)\n\n        # expected alignment alpha\n        # bsz * self.num_heads, tgt_len, src_len\n        if incremental_state is not None:\n            alpha = self.expected_alignment_infer(p_choose, key_padding_mask, incremental_state)\n        else:\n            alpha = self.expected_alignment_train(p_choose, key_padding_mask)\n\n        # expected attention beta\n        # bsz * self.num_heads, tgt_len, src_len\n        beta = self.expected_attention(alpha, query, key, value, key_padding_mask, incremental_state)\n\n        attn_weights = beta\n\n        v_proj = self.v_proj_output(value)\n        attn = torch.bmm(attn_weights.type_as(v_proj), v_proj)\n\n        attn = (\n            attn\n            .transpose(0, 1)\n            .contiguous()\n            .view(tgt_len, bsz, embed_dim)\n        )\n\n        attn = self.out_proj(attn)\n\n        beta = beta.view(bsz, self.num_heads, tgt_len, src_len)\n        alpha = alpha.view(bsz, self.num_heads, tgt_len, src_len)\n        p_choose = p_choose.view(bsz, self.num_heads, tgt_len, src_len)\n\n        return attn, {""alpha"": alpha, ""beta"": beta, ""p_choose"": p_choose}\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        """"""Reorder buffered internal state (for incremental generation).""""""\n        super().reorder_incremental_state(incremental_state, new_order)\n        input_buffer = self._get_monotonic_buffer(incremental_state)\n        if input_buffer is not None:\n            for k in input_buffer.keys():\n                input_buffer[k] = input_buffer[k].index_select(0, new_order)\n            self._set_monotonic_buffer(incremental_state, input_buffer)\n\n    def _get_monotonic_buffer(self, incremental_state):\n        return utils.get_incremental_state(\n            self,\n            incremental_state,\n            \'monotonic\',\n        ) or {}\n\n    def _set_monotonic_buffer(self, incremental_state, buffer):\n        utils.set_incremental_state(\n            self,\n            incremental_state,\n            \'monotonic\',\n            buffer,\n        )\n\n    def get_pointer(self, incremental_state):\n        return utils.get_incremental_state(\n            self,\n            incremental_state,\n            \'monotonic\',\n        ) or {}\n\n    def get_fastest_pointer(self, incremental_state):\n        return self.get_pointer(incremental_state)[""step""].max(0)[0]\n\n    def set_pointer(self, incremental_state, p_choose):\n        curr_pointer = self.get_pointer(incremental_state)\n        if len(curr_pointer) == 0:\n            buffer = torch.zeros_like(p_choose)\n        else:\n            buffer = self.get_pointer(incremental_state)[""step""]\n\n        buffer += (p_choose < 0.5).type_as(buffer)\n\n        utils.set_incremental_state(\n            self,\n            incremental_state,\n            \'monotonic\',\n            {""step"": buffer},\n        )\n\n\n@register_monotonic_attention(""hard_aligned"")\nclass MonotonicMultiheadAttentionHard(MonotonicAttention, MultiheadAttention):\n\n    def __init__(self, args):\n        MultiheadAttention.__init__(\n            self,\n            embed_dim=args.decoder_embed_dim,\n            num_heads=args.decoder_attention_heads,\n            kdim=getattr(args, \'encoder_embed_dim\', None),\n            vdim=getattr(args, \'encoder_embed_dim\', None),\n            dropout=args.attention_dropout,\n            encoder_decoder_attention=True\n        )\n\n        MonotonicAttention.__init__(self, args)\n\n        self.k_in_proj = {""monotonic"": self.k_proj}\n        self.q_in_proj = {""monotonic"": self.q_proj}\n        self.v_in_proj = {""output"": self.v_proj}\n\n    def input_projections(self, query, key, value, name):\n        """"""\n        Prepare inputs for multihead attention\n\n        ============================================================\n        Expected input size\n        query: tgt_len, bsz, embed_dim\n        key: src_len, bsz, embed_dim\n        value: src_len, bsz, embed_dim\n        name: monotonic or soft\n        """"""\n\n        if query is not None:\n            bsz = query.size(1)\n            q = self.q_in_proj[name](query)\n            q *= self.scaling\n            q = q.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n        else:\n            q = None\n\n        if key is not None:\n            bsz = key.size(1)\n            k = self.k_in_proj[name](key)\n            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n        else:\n            k = None\n\n        if value is not None:\n            bsz = value.size(1)\n            v = self.v_in_proj[name](value)\n            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n        else:\n            v = None\n\n        return q, k, v\n\n    def p_choose(self, query, key, key_padding_mask=None):\n        """"""\n        Calculating step wise prob for reading and writing\n        1 to read, 0 to write\n\n        ============================================================\n        Expected input size\n        query: bsz, tgt_len, embed_dim\n        key: bsz, src_len, embed_dim\n        value: bsz, src_len, embed_dim\n        key_padding_mask: bsz, src_len\n        attn_mask: bsz, src_len\n        query: bsz, tgt_len, embed_dim\n        """"""\n\n        # prepare inputs\n        q_proj, k_proj, _ = self.input_projections(query, key, None, ""monotonic"")\n\n        # attention energy\n        attn_energy = self.attn_energy(q_proj, k_proj, key_padding_mask)\n\n        noise = 0\n\n        if self.training:\n            # add noise here to encourage discretness\n            noise = (\n                torch\n                .normal(self.noise_mean, self.noise_var, attn_energy.size())\n                .type_as(attn_energy)\n                .to(attn_energy.device)\n            )\n\n        p_choose = torch.sigmoid(attn_energy + noise)\n        _, _, tgt_len, src_len = p_choose.size()\n\n        # p_choose: bsz * self.num_heads, tgt_len, src_len\n        return p_choose.view(-1, tgt_len, src_len)\n\n    def expected_attention(self, alpha, *args):\n        \'\'\'\n        For MMA-H, beta = alpha\n        \'\'\'\n        return alpha\n\n    def v_proj_output(self, value):\n        _, _, v_proj = self.input_projections(None, None, value, ""output"")\n        return v_proj\n\n\n@register_monotonic_attention(""infinite_lookback"")\nclass MonotonicMultiheadAttentionInfiniteLookback(MonotonicMultiheadAttentionHard):\n    def __init__(self, args):\n        super().__init__(args)\n        self.init_soft_attention()\n\n    def init_soft_attention(self):\n        self.k_proj_soft = nn.Linear(self.kdim, self.embed_dim, bias=True)\n        self.q_proj_soft = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        self.k_in_proj[""soft""] = self.k_proj_soft\n        self.q_in_proj[""soft""] = self.q_proj_soft\n\n        if self.qkv_same_dim:\n            # Empirically observed the convergence to be much better with\n            # the scaled initialization\n            nn.init.xavier_uniform_(self.k_in_proj[""soft""].weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.q_in_proj[""soft""].weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.k_in_proj[""soft""].weight)\n            nn.init.xavier_uniform_(self.q_in_proj[""soft""].weight)\n\n    def expected_attention(self, alpha, query, key, value, key_padding_mask, incremental_state):\n        # monotonic attention, we will calculate milk here\n        bsz_x_num_heads, tgt_len, src_len = alpha.size()\n        bsz = int(bsz_x_num_heads / self.num_heads)\n\n        q, k, _ = self.input_projections(query, key, None, ""soft"")\n        soft_energy = self.attn_energy(q, k, key_padding_mask)\n\n        assert list(soft_energy.size()) == [bsz, self.num_heads, tgt_len, src_len]\n\n        soft_energy = soft_energy.view(bsz * self.num_heads, tgt_len, src_len)\n\n        if incremental_state is not None:\n            monotonic_cache = self._get_monotonic_buffer(incremental_state)\n            monotonic_step = monotonic_cache[""step""] + 1\n            step_offset = 0\n            if key_padding_mask is not None:\n                if key_padding_mask[:, 0].any():\n                    # left_pad_source = True:\n                    step_offset = key_padding_mask.sum(dim=-1, keepdim=True)\n            monotonic_step += step_offset\n            mask = lengths_to_mask(\n                monotonic_step.view(-1), soft_energy.size(2), 1).unsqueeze(1)\n\n            soft_energy = soft_energy.masked_fill(~ mask.bool(), float(\'-inf\'))\n            soft_energy = soft_energy - soft_energy.max(dim=2, keepdim=True)[0]\n            exp_soft_energy = torch.exp(soft_energy)\n            exp_soft_energy_sum = exp_soft_energy.sum(dim=2)\n            beta = exp_soft_energy / exp_soft_energy_sum.unsqueeze(2)\n\n        else:\n            # bsz * num_heads, tgt_len, src_len\n            soft_energy = soft_energy - soft_energy.max(dim=2, keepdim=True)[0]\n            exp_soft_energy = torch.exp(soft_energy)\n            exp_soft_energy_cumsum = torch.cumsum(exp_soft_energy, dim=2)\n\n            if key_padding_mask is not None:\n                if key_padding_mask.any():\n                    exp_soft_energy_cumsum = (\n                        exp_soft_energy_cumsum.view(-1, self.num_heads, tgt_len, src_len)\n                        .masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(1), self.eps)\n                        .view(-1, tgt_len, src_len)\n                    )\n\n            inner_items = alpha / exp_soft_energy_cumsum\n\n            beta = exp_soft_energy * torch.cumsum(inner_items.flip(dims=[2]), dim=2).flip(dims=[2])\n\n            beta = F.dropout(beta, p=self.dropout, training=self.training)\n\n        assert not torch.isnan(beta).any(), ""NaN detected in beta.""\n\n        return beta\n\n\n@register_monotonic_attention(""waitk"")\nclass MonotonicMultiheadAttentionWaitk(MonotonicMultiheadAttentionInfiniteLookback):\n    def __init__(self, args):\n        super().__init__(args)\n        self.q_in_proj[""soft""] = self.q_in_proj[""monotonic""]\n        self.k_in_proj[""soft""] = self.k_in_proj[""monotonic""]\n        self.waitk_lagging = args.waitk_lagging\n        assert self.waitk_lagging > 0, f""Lagging has to been larger than 0, get {self.waitk_lagging}.""\n\n    @staticmethod\n    def add_args(parser):\n        super(\n            MonotonicMultiheadAttentionWaitk,\n            MonotonicMultiheadAttentionWaitk,\n        ).add_args(parser)\n\n        parser.add_argument(\'--waitk-lagging\', type=int, required=True,\n                            help=\'Wait k lagging\')\n\n    def p_choose(self, query, key, key_padding_mask=None, attn_mask=None, incremental_state=None):\n        """"""\n        query: bsz, tgt_len\n        key: bsz, src_len\n        key_padding_mask: bsz, src_len\n        """"""\n        src_len, bsz, _ = key.size()\n        tgt_len, bsz, _ = query.size()\n        p_choose = query.new_ones(bsz, tgt_len, src_len)\n        p_choose = torch.tril(p_choose, diagonal=self.waitk_lagging - 1)\n        p_choose = torch.triu(p_choose, diagonal=self.waitk_lagging - 1)\n\n        if key_padding_mask is not None and key_padding_mask[:, 0].eq(1).any():\n            # Left pad source\n            # add -1 to the end\n            p_choose = p_choose.masked_fill(key_padding_mask.float().flip(1).unsqueeze(1).bool(), -1)\n            p_choose = convert_padding_direction(p_choose.view(-1, src_len).long(), padding_idx=-1, right_to_left=True)\n            p_choose = p_choose.view(bsz, tgt_len, src_len).type_as(query)\n            # remove -1\n            p_choose[p_choose.eq(-1)] = 0\n\n        # Extend to each head\n        p_choose = (\n            p_choose.contiguous().unsqueeze(1)\n            .expand(-1, self.num_heads, -1, -1).contiguous()\n            .view(-1, tgt_len, src_len)\n        )\n\n        return p_choose\n'"
examples/simultaneous_translation/modules/monotonic_transformer_layer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.modules import (\n    LayerNorm,\n    TransformerEncoderLayer,\n    TransformerDecoderLayer\n)\n\nfrom . import build_monotonic_attention\n\n\nclass TransformerMonotonicEncoderLayer(TransformerEncoderLayer):\n\n    def forward(self, x, encoder_padding_mask):\n        seq_len, _, _ = x.size()\n        attn_mask = x.new_ones([seq_len, seq_len]).triu(1)\n        attn_mask = attn_mask.masked_fill(attn_mask.bool(), float(\'-inf\'))\n        return super().forward(x, encoder_padding_mask, attn_mask)\n\n\nclass TransformerMonotonicDecoderLayer(TransformerDecoderLayer):\n\n    def __init__(self, args, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False):\n        super().__init__(\n            args,\n            no_encoder_attn=True,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn\n        )\n        self.encoder_attn = build_monotonic_attention(args)\n        self.encoder_attn_layer_norm = LayerNorm(\n            self.embed_dim,\n            export=getattr(args, \'char_inputs\', False)\n        )\n\n    def prune_incremental_state(self, incremental_state):\n        def prune(module):\n            input_buffer = module._get_input_buffer(incremental_state)\n            for key in [""prev_key"", ""prev_value""]:\n                if input_buffer[key].size(2) > 1:\n                    input_buffer[key] = input_buffer[key][:, :, :-1, :]\n                else:\n                    input_buffer = {}\n                    break\n            module._set_input_buffer(incremental_state, input_buffer)\n        prune(self.self_attn)\n\n    def get_steps(self, incremental_state):\n        return (\n            self.encoder_attn\n            ._get_monotonic_buffer(\n                incremental_state\n            ).get(""step"", 0)\n        )\n'"
examples/simultaneous_translation/utils/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\n\n# automatically import any Python files in the criterions/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        module = file[:file.find('.py')]\n        importlib.import_module('examples.simultaneous_translation.utils.' + module)\n"""
examples/simultaneous_translation/utils/functions.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\n\ndef exclusive_cumprod(tensor, dim: int, eps: float = 1e-10):\n    """"""\n    Implementing exclusive cumprod.\n    There is cumprod in pytorch, however there is no exclusive mode.\n    cumprod(x) = [x1, x1x2, x2x3x4, ..., prod_{i=1}^n x_i]\n    exclusive means cumprod(x) = [1, x1, x1x2, x1x2x3, ..., prod_{i=1}^{n-1} x_i]\n    """"""\n    tensor_size = list(tensor.size())\n    tensor_size[dim] = 1\n    return_tensor = safe_cumprod(\n        torch.cat([torch.ones(tensor_size).type_as(tensor), tensor], dim=dim), dim=dim, eps=eps\n    )\n\n    if dim == 0:\n        return return_tensor[:-1]\n    elif dim == 1:\n        return return_tensor[:, :-1]\n    elif dim == 2:\n        return return_tensor[:, :, :-1]\n    else:\n        raise RuntimeError(""Cumprod on dimension 3 and more is not implemented"")\n\n\ndef safe_cumprod(tensor, dim: int, eps: float = 1e-10):\n    """"""\n    An implementation of cumprod to prevent precision issue.\n    cumprod(x)\n    = [x1, x1x2, x1x2x3, ....]\n    = [exp(log(x1)), exp(log(x1) + log(x2)), exp(log(x1) + log(x2) + log(x3)), ...]\n    = exp(cumsum(log(x)))\n    """"""\n\n    if (tensor + eps < 0).any().item():\n        raise RuntimeError(\n            ""Safe cumprod can only take non-negative tensors as input.""\n            ""Consider use torch.cumprod if you want to calculate negative values.""\n        )\n\n    log_tensor = torch.log(tensor + eps)\n    cumsum_log_tensor = torch.cumsum(log_tensor, dim)\n    exp_cumsum_log_tensor = torch.exp(cumsum_log_tensor)\n    return exp_cumsum_log_tensor\n\n\ndef lengths_to_mask(lengths, max_len: int, dim: int = 0, negative_mask: bool = False):\n    """"""\n    Convert a tensor of lengths to mask\n    For example, lengths = [[2, 3, 4]], max_len = 5\n    mask =\n       [[1, 1, 1],\n        [1, 1, 1],\n        [0, 1, 1],\n        [0, 0, 1],\n        [0, 0, 0]]\n    """"""\n    assert len(lengths.size()) <= 2\n    if len(lengths) == 2:\n        if dim == 1:\n            lengths = lengths.t()\n        lengths = lengths\n    else:\n        lengths = lengths.unsqueeze(1)\n\n    # lengths : batch_size, 1\n    lengths = lengths.view(-1, 1)\n\n    batch_size = lengths.size(0)\n    # batch_size, max_len\n    mask = torch.arange(max_len).expand(batch_size, max_len).type_as(lengths) < lengths\n\n    if negative_mask:\n        mask = ~mask\n\n    if dim == 0:\n        # max_len, batch_size\n        mask = mask.t()\n\n    return mask\n\n\ndef moving_sum(x, start_idx: int, end_idx: int):\n    """"""\n    From MONOTONIC CHUNKWISE ATTENTION\n    https://arxiv.org/pdf/1712.05382.pdf\n    Equation (18)\n\n    x = [x_1, x_2, ..., x_N]\n    MovingSum(x, start_idx, end_idx)_n = Sigma_{m=n\xe2\x88\x92(start_idx\xe2\x88\x921)}^{n+end_idx-1} x_m\n    for n in {1, 2, 3, ..., N}\n\n    x : src_len, batch_size\n    start_idx : start idx\n    end_idx : end idx\n\n    Example\n    src_len = 5\n    batch_size = 3\n    x =\n       [[ 0, 5, 10],\n        [ 1, 6, 11],\n        [ 2, 7, 12],\n        [ 3, 8, 13],\n        [ 4, 9, 14]]\n\n    MovingSum(x, 3, 1) =\n       [[ 0,  5, 10],\n        [ 1, 11, 21],\n        [ 3, 18, 33],\n        [ 6, 21, 36],\n        [ 9, 24, 39]]\n\n    MovingSum(x, 1, 3) =\n       [[ 3, 18, 33],\n        [ 6, 21, 36],\n        [ 9, 24, 39],\n        [ 7, 17, 27],\n        [ 4,  9, 14]]\n    """"""\n    assert start_idx > 0 and end_idx > 0\n    assert len(x.size()) == 2\n    src_len, batch_size = x.size()\n    # batch_size, 1, src_len\n    x = x.t().unsqueeze(1)\n    # batch_size, 1, src_len\n    moving_sum_weight = x.new_ones([1, 1, end_idx + start_idx - 1])\n\n    moving_sum = torch.nn.functional.conv1d(\n        x,\n        moving_sum_weight,\n        padding=start_idx + end_idx - 1\n    ).squeeze(1).t()\n    moving_sum = moving_sum[end_idx: -start_idx]\n\n    assert src_len == moving_sum.size(0)\n    assert batch_size == moving_sum.size(1)\n\n    return moving_sum\n'"
examples/simultaneous_translation/utils/latency.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\n\nclass LatencyMetric(object):\n    @staticmethod\n    def length_from_padding_mask(padding_mask, batch_first: bool = False):\n        dim = 1 if batch_first else 0\n        return padding_mask.size(dim) - padding_mask.sum(dim=dim, keepdim=True)\n\n    def prepare_latency_metric(\n        self,\n        delays,\n        src_lens,\n        target_padding_mask=None,\n        batch_first: bool = False,\n        start_from_zero: bool = True\n    ):\n        assert len(delays.size()) == 2\n        assert len(src_lens.size()) == 2\n\n        if start_from_zero:\n            delays = delays + 1\n\n        if batch_first:\n            # convert to batch_last\n            delays = delays.t()\n            src_lens = src_lens.t()\n            tgt_len, bsz = delays.size()\n            _, bsz_1 = src_lens.size()\n\n            if target_padding_mask is not None:\n                target_padding_mask = target_padding_mask.t()\n                tgt_len_1, bsz_2 = target_padding_mask.size()\n                assert tgt_len == tgt_len_1\n                assert bsz == bsz_2\n\n        assert bsz == bsz_1\n\n        if target_padding_mask is None:\n            tgt_lens = tgt_len * delays.new_ones([1, bsz]).float()\n        else:\n            # 1, batch_size\n            tgt_lens = self.length_from_padding_mask(target_padding_mask, False).float()\n            delays = delays.masked_fill(target_padding_mask, 0)\n\n        return delays, src_lens, tgt_lens, target_padding_mask\n\n    def __call__(\n        self,\n        delays,\n        src_lens,\n        target_padding_mask=None,\n        batch_first: bool = False,\n        start_from_zero: bool = True,\n    ):\n        delays, src_lens, tgt_lens, target_padding_mask = self.prepare_latency_metric(\n            delays,\n            src_lens,\n            target_padding_mask,\n            batch_first,\n            start_from_zero\n        )\n        return self.cal_metric(delays, src_lens, tgt_lens, target_padding_mask)\n\n    @staticmethod\n    def cal_metric(delays, src_lens, tgt_lens, target_padding_mask):\n        """"""\n        Expected sizes:\n        delays: tgt_len, batch_size\n        src_lens: 1, batch_size\n        target_padding_mask: tgt_len, batch_size\n        """"""\n        raise NotImplementedError\n\n\nclass AverageProportion(LatencyMetric):\n    """"""\n    Function to calculate Average Proportion from\n    Can neural machine translation do simultaneous translation?\n    (https://arxiv.org/abs/1606.02012)\n\n    Delays are monotonic steps, range from 1 to src_len.\n    Give src x tgt y, AP is calculated as:\n\n    AP = 1 / (|x||y]) sum_i^|Y| deleys_i\n    """"""\n    @staticmethod\n    def cal_metric(delays, src_lens, tgt_lens, target_padding_mask):\n        if target_padding_mask is not None:\n            AP = torch.sum(delays.masked_fill(target_padding_mask, 0), dim=0, keepdim=True)\n        else:\n            AP = torch.sum(delays, dim=0, keepdim=True)\n\n        AP = AP / (src_lens * tgt_lens)\n        return AP\n\n\nclass AverageLagging(LatencyMetric):\n    """"""\n    Function to calculate Average Lagging from\n    STACL: Simultaneous Translation with Implicit Anticipation\n    and Controllable Latency using Prefix-to-Prefix Framework\n    (https://arxiv.org/abs/1810.08398)\n\n    Delays are monotonic steps, range from 1 to src_len.\n    Give src x tgt y, AP is calculated as:\n\n    AL = 1 / tau sum_i^tau delays_i - (i - 1) / gamma\n\n    Where\n    gamma = |y| / |x|\n    tau = argmin_i(delays_i = |x|)\n    """"""\n    @staticmethod\n    def cal_metric(delays, src_lens, tgt_lens, target_padding_mask):\n        # tau = argmin_i(delays_i = |x|)\n        tgt_len, bsz = delays.size()\n        lagging_padding_mask = delays >= src_lens\n        lagging_padding_mask = torch.nn.functional.pad(lagging_padding_mask.t(), (1, 0)).t()[:-1, :]\n        gamma = tgt_lens / src_lens\n        lagging = delays - torch.arange(delays.size(0)).unsqueeze(1).type_as(delays).expand_as(delays) / gamma\n        lagging.masked_fill_(lagging_padding_mask, 0)\n        tau = (1 - lagging_padding_mask.type_as(lagging)).sum(dim=0, keepdim=True)\n        AL = lagging.sum(dim=0, keepdim=True) / tau\n\n        return AL\n\n\nclass DifferentiableAverageLagging(LatencyMetric):\n    """"""\n    Function to calculate Differentiable Average Lagging from\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\n    (https://arxiv.org/abs/1906.05218)\n\n    Delays are monotonic steps, range from 0 to src_len-1.\n    (In the original paper thery are from 1 to src_len)\n    Give src x tgt y, AP is calculated as:\n\n    DAL = 1 / |Y| sum_i^|Y| delays\'_i - (i - 1) / gamma\n\n    Where\n    delays\'_i =\n        1. delays_i if i == 1\n        2. max(delays_i, delays\'_{i-1} + 1 / gamma)\n\n    """"""\n    @staticmethod\n    def cal_metric(delays, src_lens, tgt_lens, target_padding_mask):\n        tgt_len, bsz = delays.size()\n\n        gamma = tgt_lens / src_lens\n        new_delays = torch.zeros_like(delays)\n\n        for i in range(delays.size(0)):\n            if i == 0:\n                new_delays[i] = delays[i]\n            else:\n                new_delays[i] = torch.cat(\n                    [\n                        new_delays[i - 1].unsqueeze(0) + 1 / gamma,\n                        delays[i].unsqueeze(0)\n                    ],\n                    dim=0\n                ).max(dim=0)[0]\n\n        DAL = (\n            new_delays - torch.arange(delays.size(0)).unsqueeze(1).type_as(delays).expand_as(delays) / gamma\n        )\n        if target_padding_mask is not None:\n            DAL = DAL.masked_fill(target_padding_mask, 0)\n\n        DAL = DAL.sum(dim=0, keepdim=True) / tgt_lens\n\n        return DAL\n\n\nclass LatencyMetricVariance(LatencyMetric):\n    def prepare_latency_metric(\n        self,\n        delays,\n        src_lens,\n        target_padding_mask=None,\n        batch_first: bool = True,\n        start_from_zero: bool = True\n    ):\n        assert batch_first\n        assert len(delays.size()) == 3\n        assert len(src_lens.size()) == 2\n\n        if start_from_zero:\n            delays = delays + 1\n\n        # convert to batch_last\n        bsz, num_heads_x_layers, tgt_len = delays.size()\n        bsz_1, _ = src_lens.size()\n        assert bsz == bsz_1\n\n        if target_padding_mask is not None:\n            bsz_2, tgt_len_1 = target_padding_mask.size()\n            assert tgt_len == tgt_len_1\n            assert bsz == bsz_2\n\n        if target_padding_mask is None:\n            tgt_lens = tgt_len * delays.new_ones([bsz, tgt_len]).float()\n        else:\n            # batch_size, 1\n            tgt_lens = self.length_from_padding_mask(target_padding_mask, True).float()\n            delays = delays.masked_fill(target_padding_mask.unsqueeze(1), 0)\n\n        return delays, src_lens, tgt_lens, target_padding_mask\n\n\nclass VarianceDelay(LatencyMetricVariance):\n    @staticmethod\n    def cal_metric(delays, src_lens, tgt_lens, target_padding_mask):\n        """"""\n        delays : bsz, num_heads_x_layers, tgt_len\n        src_lens : bsz, 1\n        target_lens : bsz, 1\n        target_padding_mask: bsz, tgt_len or None\n        """"""\n        if delays.size(1) == 1:\n            return delays.new_zeros([1])\n\n        variance_delays = delays.var(dim=1)\n\n        if target_padding_mask is not None:\n            variance_delays.masked_fill_(target_padding_mask, 0)\n\n        return variance_delays.sum(dim=1, keepdim=True) / tgt_lens\n\n\nclass LatencyInference(object):\n    def __init__(self, start_from_zero=True):\n        self.metric_calculator = {\n            ""differentiable_average_lagging"": DifferentiableAverageLagging(),\n            ""average_lagging"": AverageLagging(),\n            ""average_proportion"": AverageProportion(),\n        }\n\n        self.start_from_zero = start_from_zero\n\n    def __call__(self, monotonic_step, src_lens):\n        """"""\n        monotonic_step range from 0 to src_len. src_len means eos\n        delays: bsz, tgt_len\n        src_lens: bsz, 1\n        """"""\n        if not self.start_from_zero:\n            monotonic_step -= 1\n\n        src_lens = src_lens\n\n        delays = (\n            monotonic_step\n            .view(monotonic_step.size(0), -1, monotonic_step.size(-1))\n            .max(dim=1)[0]\n        )\n\n        delays = (\n            delays.masked_fill(delays >= src_lens, 0)\n            + (src_lens - 1)\n            .expand_as(delays)\n            .masked_fill(delays < src_lens, 0)\n        )\n        return_dict = {}\n        for key, func in self.metric_calculator.items():\n            return_dict[key] = func(\n                delays.float(), src_lens.float(),\n                target_padding_mask=None,\n                batch_first=True,\n                start_from_zero=True\n            ).t()\n\n        return return_dict\n\n\nclass LatencyTraining(object):\n    def __init__(\n        self, avg_weight, var_weight, avg_type, var_type,\n        stay_on_last_token, average_method,\n    ):\n        self.avg_weight = avg_weight\n        self.var_weight = var_weight\n        self.avg_type = avg_type\n        self.var_type = var_type\n        self.stay_on_last_token = stay_on_last_token\n        self.average_method = average_method\n\n        self.metric_calculator = {\n            ""differentiable_average_lagging"": DifferentiableAverageLagging(),\n            ""average_lagging"": AverageLagging(),\n            ""average_proportion"": AverageProportion(),\n        }\n\n        self.variance_calculator = {\n            ""variance_delay"": VarianceDelay(),\n        }\n\n    def expected_delays_from_attention(\n        self, attention, source_padding_mask=None, target_padding_mask=None\n    ):\n        if type(attention) == list:\n            # bsz, num_heads, tgt_len, src_len\n            bsz, num_heads, tgt_len, src_len = attention[0].size()\n            attention = torch.cat(attention, dim=1)\n            bsz, num_heads_x_layers, tgt_len, src_len = attention.size()\n            # bsz * num_heads * num_layers, tgt_len, src_len\n            attention = attention.view(-1, tgt_len, src_len)\n        else:\n            # bsz * num_heads * num_layers, tgt_len, src_len\n            bsz, tgt_len, src_len = attention.size()\n            num_heads_x_layers = 1\n            attention = attention.view(-1, tgt_len, src_len)\n\n        if not self.stay_on_last_token:\n            residual_attention = \\\n                1 - attention[:, :, :-1].sum(dim=2, keepdim=True)\n            attention = torch.cat(\n                [attention[:, :, :-1], residual_attention],\n                dim=2\n            )\n\n        # bsz * num_heads_x_num_layers, tgt_len, src_len for MMA\n        steps = (\n            torch\n            .arange(1, 1 + src_len)\n            .unsqueeze(0)\n            .unsqueeze(1)\n            .expand_as(attention)\n            .type_as(attention)\n        )\n\n        if source_padding_mask is not None:\n            src_offset = (\n                source_padding_mask.type_as(attention)\n                .sum(dim=1, keepdim=True)\n                .expand(bsz, num_heads_x_layers)\n                .contiguous()\n                .view(-1, 1)\n            )\n            src_lens = src_len - src_offset\n            if source_padding_mask[:, 0].any():\n                # Pad left\n                src_offset = src_offset.view(-1, 1, 1)\n                steps = steps - src_offset\n                steps = steps.masked_fill(steps <= 0, 0)\n        else:\n            src_lens = attention.new_ones([bsz, num_heads_x_layers]) * src_len\n            src_lens = src_lens.view(-1, 1)\n\n        # bsz * num_heads_num_layers, tgt_len, src_len\n        expected_delays = (steps * attention).sum(dim=2).view(\n            bsz, num_heads_x_layers, tgt_len\n        )\n\n        if target_padding_mask is not None:\n            expected_delays.masked_fill_(\n                target_padding_mask.unsqueeze(1),\n                0\n            )\n\n        return expected_delays, src_lens\n\n    def avg_loss(self, expected_delays, src_lens, target_padding_mask):\n\n        bsz, num_heads_x_layers, tgt_len = expected_delays.size()\n        target_padding_mask = (\n            target_padding_mask\n            .unsqueeze(1)\n            .expand_as(expected_delays)\n            .contiguous()\n            .view(-1, tgt_len)\n        )\n\n        if self.average_method == ""average"":\n            # bsz * tgt_len\n            expected_delays = expected_delays.mean(dim=1)\n        elif self.average_method == ""weighted_average"":\n            weights = torch.nn.functional.softmax(expected_delays, dim=1)\n            expected_delays = torch.sum(expected_delays * weights, dim=1)\n        elif self.average_method == ""max"":\n            # bsz * num_heads_x_num_layers, tgt_len\n            expected_delays = expected_delays.max(dim=1)[0]\n        else:\n            raise RuntimeError(f""{self.average_method} is not supported"")\n\n        src_lens = src_lens.view(bsz, -1)[:, :1]\n        target_padding_mask = target_padding_mask.view(bsz, -1, tgt_len)[:, 0]\n\n        if self.avg_weight > 0.0:\n            if self.avg_type in self.metric_calculator:\n                average_delays = self.metric_calculator[self.avg_type](\n                    expected_delays, src_lens, target_padding_mask,\n                    batch_first=True, start_from_zero=False\n                )\n            else:\n                raise RuntimeError(f""{self.avg_type} is not supported."")\n\n            # bsz * num_heads_x_num_layers, 1\n            return self.avg_weight * average_delays.sum()\n        else:\n            return 0.0\n\n    def var_loss(self, expected_delays, src_lens, target_padding_mask):\n        src_lens = src_lens.view(expected_delays.size(0), expected_delays.size(1))[:, :1]\n        if self.var_weight > 0.0:\n            if self.var_type in self.variance_calculator:\n                variance_delays = self.variance_calculator[self.var_type](\n                    expected_delays, src_lens, target_padding_mask,\n                    batch_first=True, start_from_zero=False\n                )\n            else:\n                raise RuntimeError(f""{self.var_type} is not supported."")\n\n            return self.var_weight * variance_delays.sum()\n        else:\n            return 0.0\n\n    def loss(self, attention, source_padding_mask=None, target_padding_mask=None):\n        expected_delays, src_lens = self.expected_delays_from_attention(\n            attention, source_padding_mask, target_padding_mask\n        )\n\n        latency_loss = 0\n\n        latency_loss += self.avg_loss(expected_delays, src_lens, target_padding_mask)\n\n        latency_loss += self.var_loss(expected_delays, src_lens, target_padding_mask)\n\n        return latency_loss\n'"
examples/speech_recognition/criterions/ASG_loss.py,8,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom fairseq import utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\nfrom examples.speech_recognition.data.replabels import pack_replabels\n\n\n@register_criterion(""asg_loss"")\nclass ASGCriterion(FairseqCriterion):\n    @staticmethod\n    def add_args(parser):\n        group = parser.add_argument_group(""ASG Loss"")\n        group.add_argument(\n            ""--asg-transitions-init"",\n            help=""initial diagonal value of transition matrix"",\n            type=float,\n            default=0.0,\n        )\n        group.add_argument(\n            ""--max-replabel"", help=""maximum # of replabels"", type=int, default=2\n        )\n        group.add_argument(\n            ""--linseg-updates"",\n            help=""# of training updates to use LinSeg initialization"",\n            type=int,\n            default=0,\n        )\n        group.add_argument(\n            ""--hide-linseg-messages"",\n            help=""hide messages about LinSeg initialization"",\n            action=""store_true"",\n        )\n\n    def __init__(\n        self,\n        task,\n        silence_token,\n        asg_transitions_init,\n        max_replabel,\n        linseg_updates,\n        hide_linseg_messages,\n    ):\n        from wav2letter.criterion import ASGLoss, CriterionScaleMode\n\n        super().__init__(task)\n        self.tgt_dict = task.target_dictionary\n        self.eos = self.tgt_dict.eos()\n        self.silence = (\n            self.tgt_dict.index(silence_token)\n            if silence_token in self.tgt_dict\n            else None\n        )\n        self.max_replabel = max_replabel\n\n        num_labels = len(self.tgt_dict)\n        self.asg = ASGLoss(num_labels, scale_mode=CriterionScaleMode.TARGET_SZ_SQRT)\n        self.asg.trans = torch.nn.Parameter(\n            asg_transitions_init * torch.eye(num_labels), requires_grad=True\n        )\n\n        self.linseg_progress = torch.nn.Parameter(\n            torch.tensor([0], dtype=torch.int), requires_grad=False\n        )\n        self.linseg_maximum = linseg_updates\n        self.linseg_message_state = ""none"" if hide_linseg_messages else ""start""\n\n    @classmethod\n    def build_criterion(cls, args, task):\n        return cls(\n            task,\n            args.silence_token,\n            args.asg_transitions_init,\n            args.max_replabel,\n            args.linseg_updates,\n            args.hide_linseg_messages,\n        )\n\n    def linseg_step(self):\n        if not self.training:\n            return False\n        if self.linseg_progress.item() < self.linseg_maximum:\n            if self.linseg_message_state == ""start"":\n                print(""| using LinSeg to initialize ASG"")\n                self.linseg_message_state = ""finish""\n            self.linseg_progress.add_(1)\n            return True\n        elif self.linseg_message_state == ""finish"":\n            print(""| finished LinSeg initialization"")\n            self.linseg_message_state = ""none""\n        return False\n\n    def replace_eos_with_silence(self, tgt):\n        if tgt[-1] != self.eos:\n            return tgt\n        elif self.silence is None or (len(tgt) > 1 and tgt[-2] == self.silence):\n            return tgt[:-1]\n        else:\n            return tgt[:-1] + [self.silence]\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n\n        net_output = model(**sample[""net_input""])\n        emissions = net_output[""encoder_out""].transpose(0, 1).contiguous()\n        B = emissions.size(0)\n        T = emissions.size(1)\n        device = emissions.device\n\n        target = torch.IntTensor(B, T)\n        target_size = torch.IntTensor(B)\n        using_linseg = self.linseg_step()\n\n        for b in range(B):\n            initial_target_size = sample[""target_lengths""][b].item()\n            if initial_target_size == 0:\n                raise ValueError(""target size cannot be zero"")\n\n            tgt = sample[""target""][b, :initial_target_size].tolist()\n            tgt = self.replace_eos_with_silence(tgt)\n            tgt = pack_replabels(tgt, self.tgt_dict, self.max_replabel)\n            tgt = tgt[:T]\n\n            if using_linseg:\n                tgt = [tgt[t * len(tgt) // T] for t in range(T)]\n\n            target[b][: len(tgt)] = torch.IntTensor(tgt)\n            target_size[b] = len(tgt)\n\n        loss = self.asg.forward(emissions, target.to(device), target_size.to(device))\n\n        if reduce:\n            loss = torch.sum(loss)\n\n        sample_size = (\n            sample[""target""].size(0) if self.args.sentence_avg else sample[""ntokens""]\n        )\n        logging_output = {\n            ""loss"": utils.item(loss.data) if reduce else loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(""loss"", 0) for log in logging_outputs)\n        ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n        nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n        sample_size = sum(log.get(""sample_size"", 0) for log in logging_outputs)\n        agg_output = {\n            ""loss"": loss_sum / nsentences,\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""sample_size"": sample_size,\n        }\n        return agg_output\n'"
examples/speech_recognition/criterions/CTC_loss.py,3,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport math\nfrom itertools import groupby\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\nfrom examples.speech_recognition.data.data_utils import encoder_padding_mask_to_lengths\nfrom examples.speech_recognition.utils.wer_utils import Code, EditDistance, Token\n\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n\ndef arr_to_toks(arr):\n    toks = []\n    for a in arr:\n        toks.append(Token(str(a), 0.0, 0.0))\n    return toks\n\n\ndef compute_ctc_uer(logprobs, targets, input_lengths, target_lengths, blank_idx):\n    """"""\n        Computes utterance error rate for CTC outputs\n\n        Args:\n            logprobs: (Torch.tensor)  N, T1, D tensor of log probabilities out\n                of the encoder\n            targets: (Torch.tensor) N, T2 tensor of targets\n            input_lengths: (Torch.tensor) lengths of inputs for each sample\n            target_lengths: (Torch.tensor) lengths of targets for each sample\n            blank_idx: (integer) id of blank symbol in target dictionary\n\n        Returns:\n            batch_errors: (float) errors in the batch\n            batch_total: (float)  total number of valid samples in batch\n    """"""\n    batch_errors = 0.0\n    batch_total = 0.0\n    for b in range(logprobs.shape[0]):\n        predicted = logprobs[b][: input_lengths[b]].argmax(1).tolist()\n        target = targets[b][: target_lengths[b]].tolist()\n        # dedup predictions\n        predicted = [p[0] for p in groupby(predicted)]\n        # remove blanks\n        nonblanks = []\n        for p in predicted:\n            if p != blank_idx:\n                nonblanks.append(p)\n        predicted = nonblanks\n\n        # compute the alignment based on EditDistance\n        alignment = EditDistance(False).align(\n            arr_to_toks(predicted), arr_to_toks(target)\n        )\n\n        # compute the number of errors\n        # note that alignment.codes can also be used for computing\n        # deletion, insersion and substitution error breakdowns in future\n        for a in alignment.codes:\n            if a != Code.match:\n                batch_errors += 1\n        batch_total += len(target)\n\n    return batch_errors, batch_total\n\n\n@register_criterion(""ctc_loss"")\nclass CTCCriterion(FairseqCriterion):\n    def __init__(self, task):\n        assert hasattr(task, ""target_dictionary"")\n        super().__init__(task)\n        self.blank_idx = task.target_dictionary.index(""<ctc_blank>"")\n\n    @classmethod\n    def build_criterion(cls, args, task):\n        return cls(task)\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(\n            ""--use-source-side-sample-size"",\n            action=""store_true"",\n            default=False,\n            help=(\n                ""when compute average loss, using number of source tokens ""\n                + ""as denominator. ""\n                + ""This argument will be no-op if sentence-avg is used.""\n            ),\n        )\n\n    def forward(self, model, sample, reduce=True, log_probs=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        net_output = model(**sample[""net_input""])\n        lprobs = model.get_normalized_probs(net_output, log_probs=log_probs)\n        if not hasattr(lprobs, ""batch_first""):\n            logging.warning(\n                ""ERROR: we need to know whether ""\n                ""batch first for the encoder output; ""\n                ""you need to set batch_first attribute for the return value of ""\n                ""model.get_normalized_probs. Now, we assume this is true, but ""\n                ""in the future, we will raise exception instead. ""\n            )\n\n        batch_first = getattr(lprobs, ""batch_first"", True)\n\n        if not batch_first:\n            max_seq_len = lprobs.size(0)\n            bsz = lprobs.size(1)\n        else:\n            max_seq_len = lprobs.size(1)\n            bsz = lprobs.size(0)\n        device = net_output[""encoder_out""].device\n\n        input_lengths = encoder_padding_mask_to_lengths(\n            net_output[""encoder_padding_mask""], max_seq_len, bsz, device\n        )\n        target_lengths = sample[""target_lengths""]\n        targets = sample[""target""]\n\n        if batch_first:\n            # N T D -> T N D (F.ctc_loss expects this)\n            lprobs = lprobs.transpose(0, 1)\n\n        pad_mask = sample[""target""] != self.padding_idx\n        targets_flat = targets.masked_select(pad_mask)\n\n        loss = F.ctc_loss(\n            lprobs,\n            targets_flat,\n            input_lengths,\n            target_lengths,\n            blank=self.blank_idx,\n            reduction=""sum"",\n            zero_infinity=True,\n        )\n\n        lprobs = lprobs.transpose(0, 1)  # T N D -> N T D\n        errors, total = compute_ctc_uer(\n            lprobs, targets, input_lengths, target_lengths, self.blank_idx\n        )\n\n        if self.args.sentence_avg:\n            sample_size = sample[""target""].size(0)\n        else:\n            if self.args.use_source_side_sample_size:\n                sample_size = torch.sum(input_lengths).item()\n            else:\n                sample_size = sample[""ntokens""]\n\n        logging_output = {\n            ""loss"": utils.item(loss.data) if reduce else loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n            ""errors"": errors,\n            ""total"": total,\n            ""nframes"": torch.sum(sample[""net_input""][""src_lengths""]).item(),\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(""loss"", 0) for log in logging_outputs)\n        ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n        nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n        sample_size = sum(log.get(""sample_size"", 0) for log in logging_outputs)\n        errors = sum(log.get(""errors"", 0) for log in logging_outputs)\n        total = sum(log.get(""total"", 0) for log in logging_outputs)\n        nframes = sum(log.get(""nframes"", 0) for log in logging_outputs)\n        agg_output = {\n            ""loss"": loss_sum / sample_size / math.log(2),\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""nframes"": nframes,\n            ""sample_size"": sample_size,\n            ""acc"": 100.0 - min(errors * 100.0 / total, 100.0),\n        }\n        if sample_size != ntokens:\n            agg_output[""nll_loss""] = loss_sum / ntokens / math.log(2)\n        return agg_output\n'"
examples/speech_recognition/criterions/__init__.py,0,"b'import importlib\nimport os\n\n\n# ASG loss requires wav2letter\nblacklist = set()\ntry:\n    import wav2letter\nexcept ImportError:\n    blacklist.add(""ASG_loss.py"")\n\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith("".py"") and not file.startswith(""_"") and file not in blacklist:\n        criterion_name = file[: file.find("".py"")]\n        importlib.import_module(\n            ""examples.speech_recognition.criterions."" + criterion_name\n        )\n'"
examples/speech_recognition/criterions/cross_entropy_acc.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(""cross_entropy_acc"")\nclass CrossEntropyWithAccCriterion(FairseqCriterion):\n    def __init__(self, task, sentence_avg):\n        super().__init__(task)\n        self.sentence_avg = sentence_avg\n\n    def compute_loss(self, model, net_output, target, reduction, log_probs):\n        # N, T -> N * T\n        target = target.view(-1)\n        lprobs = model.get_normalized_probs(net_output, log_probs=log_probs)\n        if not hasattr(lprobs, ""batch_first""):\n            logging.warning(\n                ""ERROR: we need to know whether ""\n                ""batch first for the net output; ""\n                ""you need to set batch_first attribute for the return value of ""\n                ""model.get_normalized_probs. Now, we assume this is true, but ""\n                ""in the future, we will raise exception instead. ""\n            )\n        batch_first = getattr(lprobs, ""batch_first"", True)\n        if not batch_first:\n            lprobs = lprobs.transpose(0, 1)\n\n        # N, T, D -> N * T, D\n        lprobs = lprobs.view(-1, lprobs.size(-1))\n        loss = F.nll_loss(\n            lprobs, target, ignore_index=self.padding_idx, reduction=reduction\n        )\n        return lprobs, loss\n\n    def get_logging_output(self, sample, target, lprobs, loss):\n        target = target.view(-1)\n        mask = target != self.padding_idx\n        correct = torch.sum(\n            lprobs.argmax(1).masked_select(mask) == target.masked_select(mask)\n        )\n        total = torch.sum(mask)\n        sample_size = (\n            sample[""target""].size(0) if self.sentence_avg else sample[""ntokens""]\n        )\n\n        logging_output = {\n            ""loss"": utils.item(loss.data),  # * sample[\'ntokens\'],\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n            ""correct"": utils.item(correct.data),\n            ""total"": utils.item(total.data),\n            ""nframes"": torch.sum(sample[""net_input""][""src_lengths""]).item(),\n        }\n\n        return sample_size, logging_output\n\n    def forward(self, model, sample, reduction=""sum"", log_probs=True):\n        """"""Computes the cross entropy with accuracy metric for the given sample.\n\n        This is similar to CrossEntropyCriterion in fairseq, but also\n        computes accuracy metrics as part of logging\n\n        Args:\n            logprobs (Torch.tensor) of shape N, T, D i.e.\n                batchsize, timesteps, dimensions\n            targets (Torch.tensor) of shape N, T  i.e batchsize, timesteps\n\n        Returns:\n        tuple: With three elements:\n            1) the loss\n            2) the sample size, which is used as the denominator for the gradient\n            3) logging outputs to display while training\n\n        TODO:\n            * Currently this Criterion will only work with LSTMEncoderModels or\n            FairseqModels which have decoder, or Models which return TorchTensor\n            as net_output.\n            We need to make a change to support all FairseqEncoder models.\n        """"""\n        net_output = model(**sample[""net_input""])\n        target = model.get_targets(sample, net_output)\n        lprobs, loss = self.compute_loss(\n            model, net_output, target, reduction, log_probs\n        )\n        sample_size, logging_output = self.get_logging_output(\n            sample, target, lprobs, loss\n        )\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        correct_sum = sum(log.get(""correct"", 0) for log in logging_outputs)\n        total_sum = sum(log.get(""total"", 0) for log in logging_outputs)\n        loss_sum = sum(log.get(""loss"", 0) for log in logging_outputs)\n        ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n        nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n        sample_size = sum(log.get(""sample_size"", 0) for log in logging_outputs)\n        nframes = sum(log.get(""nframes"", 0) for log in logging_outputs)\n        agg_output = {\n            ""loss"": loss_sum / sample_size / math.log(2) if sample_size > 0 else 0.0,\n            # if args.sentence_avg, then sample_size is nsentences, then loss\n            # is per-sentence loss; else sample_size is ntokens, the loss\n            # becomes per-output token loss\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""nframes"": nframes,\n            ""sample_size"": sample_size,\n            ""acc"": correct_sum * 100.0 / total_sum if total_sum > 0 else 0.0,\n            ""correct"": correct_sum,\n            ""total"": total_sum,\n            # total is the number of validate tokens\n        }\n        if sample_size != ntokens:\n            agg_output[""nll_loss""] = loss_sum / ntokens / math.log(2)\n        # loss: per output token loss\n        # nll_loss: per sentence loss\n        return agg_output\n'"
examples/speech_recognition/data/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .asr_dataset import AsrDataset\n\n__all__ = [\n    'AsrDataset',\n]\n"""
examples/speech_recognition/data/asr_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport numpy as np\nfrom fairseq.data import FairseqDataset\n\nfrom . import data_utils\nfrom .collaters import Seq2SeqCollater\n\n\nclass AsrDataset(FairseqDataset):\n    """"""\n    A dataset representing speech and corresponding transcription.\n\n    Args:\n        aud_paths: (List[str]): A list of str with paths to audio files.\n        aud_durations_ms (List[int]): A list of int containing the durations of\n            audio files.\n        tgt (List[torch.LongTensor]): A list of LongTensors containing the indices\n            of target transcriptions.\n        tgt_dict (~fairseq.data.Dictionary): target vocabulary.\n        ids (List[str]): A list of utterance IDs.\n        speakers (List[str]): A list of speakers corresponding to utterances.\n        num_mel_bins (int): Number of triangular mel-frequency bins (default: 80)\n        frame_length (float): Frame length in milliseconds (default: 25.0)\n        frame_shift (float): Frame shift in milliseconds (default: 10.0)\n    """"""\n\n    def __init__(\n        self, aud_paths, aud_durations_ms, tgt,\n        tgt_dict, ids, speakers,\n        num_mel_bins=80, frame_length=25.0, frame_shift=10.0\n    ):\n        assert frame_length > 0\n        assert frame_shift > 0\n        assert all(x > frame_length for x in aud_durations_ms)\n        self.frame_sizes = [\n            int(1 + (d - frame_length) / frame_shift)\n            for d in aud_durations_ms\n        ]\n\n        assert len(aud_paths) > 0\n        assert len(aud_paths) == len(aud_durations_ms)\n        assert len(aud_paths) == len(tgt)\n        assert len(aud_paths) == len(ids)\n        assert len(aud_paths) == len(speakers)\n        self.aud_paths = aud_paths\n        self.tgt_dict = tgt_dict\n        self.tgt = tgt\n        self.ids = ids\n        self.speakers = speakers\n        self.num_mel_bins = num_mel_bins\n        self.frame_length = frame_length\n        self.frame_shift = frame_shift\n\n        self.s2s_collater = Seq2SeqCollater(\n            0, 1, pad_index=self.tgt_dict.pad(),\n            eos_index=self.tgt_dict.eos(), move_eos_to_beginning=True\n        )\n\n    def __getitem__(self, index):\n        import torchaudio\n        import torchaudio.compliance.kaldi as kaldi\n        tgt_item = self.tgt[index] if self.tgt is not None else None\n\n        path = self.aud_paths[index]\n        if not os.path.exists(path):\n            raise FileNotFoundError(""Audio file not found: {}"".format(path))\n        sound, sample_rate = torchaudio.load_wav(path)\n        output = kaldi.fbank(\n            sound,\n            num_mel_bins=self.num_mel_bins,\n            frame_length=self.frame_length,\n            frame_shift=self.frame_shift\n        )\n        output_cmvn = data_utils.apply_mv_norm(output)\n\n        return {""id"": index, ""data"": [output_cmvn.detach(), tgt_item]}\n\n    def __len__(self):\n        return len(self.aud_paths)\n\n    def collater(self, samples):\n        """"""Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[int]): sample indices to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        """"""\n        return self.s2s_collater.collate(samples)\n\n    def num_tokens(self, index):\n        return self.frame_sizes[index]\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.""""""\n        return (\n            self.frame_sizes[index],\n            len(self.tgt[index]) if self.tgt is not None else 0,\n        )\n\n    def ordered_indices(self):\n        """"""Return an ordered list of indices. Batches will be constructed based\n        on this order.""""""\n        return np.arange(len(self))\n'"
examples/speech_recognition/data/collaters.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\n    This module contains collection of classes which implement\n    collate functionalities for various tasks.\n\n    Collaters should know what data to expect for each sample\n    and they should pack / collate them into batches\n""""""\n\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport numpy as np\n\nimport torch\nfrom fairseq.data import data_utils as fairseq_data_utils\n\n\nclass Seq2SeqCollater(object):\n    """"""\n        Implements collate function mainly for seq2seq tasks\n        This expects each sample to contain feature (src_tokens) and\n        targets.\n        This collator is also used for aligned training task.\n    """"""\n\n    def __init__(\n        self,\n        feature_index=0,\n        label_index=1,\n        pad_index=1,\n        eos_index=2,\n        move_eos_to_beginning=True,\n    ):\n        self.feature_index = feature_index\n        self.label_index = label_index\n        self.pad_index = pad_index\n        self.eos_index = eos_index\n        self.move_eos_to_beginning = move_eos_to_beginning\n\n    def _collate_frames(self, frames):\n        """"""Convert a list of 2d frames into a padded 3d tensor\n        Args:\n            frames (list): list of 2d frames of size L[i]*f_dim. Where L[i] is\n                length of i-th frame and f_dim is static dimension of features\n        Returns:\n            3d tensor of size len(frames)*len_max*f_dim where len_max is max of L[i]\n        """"""\n        len_max = max(frame.size(0) for frame in frames)\n        f_dim = frames[0].size(1)\n        res = frames[0].new(len(frames), len_max, f_dim).fill_(0.0)\n\n        for i, v in enumerate(frames):\n            res[i, : v.size(0)] = v\n\n        return res\n\n    def collate(self, samples):\n        """"""\n        utility function to collate samples into batch for speech recognition.\n        """"""\n        if len(samples) == 0:\n            return {}\n\n        # parse samples into torch tensors\n        parsed_samples = []\n        for s in samples:\n            # skip invalid samples\n            if s[""data""][self.feature_index] is None:\n                continue\n            source = s[""data""][self.feature_index]\n            if isinstance(source, (np.ndarray, np.generic)):\n                source = torch.from_numpy(source)\n            target = s[""data""][self.label_index]\n            if isinstance(target, (np.ndarray, np.generic)):\n                target = torch.from_numpy(target).long()\n            elif isinstance(target, list):\n                target = torch.LongTensor(target)\n\n            parsed_sample = {""id"": s[""id""], ""source"": source, ""target"": target}\n            parsed_samples.append(parsed_sample)\n        samples = parsed_samples\n\n        id = torch.LongTensor([s[""id""] for s in samples])\n        frames = self._collate_frames([s[""source""] for s in samples])\n        # sort samples by descending number of frames\n        frames_lengths = torch.LongTensor([s[""source""].size(0) for s in samples])\n        frames_lengths, sort_order = frames_lengths.sort(descending=True)\n        id = id.index_select(0, sort_order)\n        frames = frames.index_select(0, sort_order)\n\n        target = None\n        target_lengths = None\n        prev_output_tokens = None\n        if samples[0].get(""target"", None) is not None:\n            ntokens = sum(len(s[""target""]) for s in samples)\n            target = fairseq_data_utils.collate_tokens(\n                [s[""target""] for s in samples],\n                self.pad_index,\n                self.eos_index,\n                left_pad=False,\n                move_eos_to_beginning=False,\n            )\n            target = target.index_select(0, sort_order)\n            target_lengths = torch.LongTensor(\n                [s[""target""].size(0) for s in samples]\n            ).index_select(0, sort_order)\n            prev_output_tokens = fairseq_data_utils.collate_tokens(\n                [s[""target""] for s in samples],\n                self.pad_index,\n                self.eos_index,\n                left_pad=False,\n                move_eos_to_beginning=self.move_eos_to_beginning,\n            )\n            prev_output_tokens = prev_output_tokens.index_select(0, sort_order)\n        else:\n            ntokens = sum(len(s[""source""]) for s in samples)\n\n        batch = {\n            ""id"": id,\n            ""ntokens"": ntokens,\n            ""net_input"": {""src_tokens"": frames, ""src_lengths"": frames_lengths},\n            ""target"": target,\n            ""target_lengths"": target_lengths,\n            ""nsentences"": len(samples),\n        }\n        if prev_output_tokens is not None:\n            batch[""net_input""][""prev_output_tokens""] = prev_output_tokens\n        return batch\n'"
examples/speech_recognition/data/data_utils.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\n\ndef calc_mean_invstddev(feature):\n    if len(feature.size()) != 2:\n        raise ValueError(""We expect the input feature to be 2-D tensor"")\n    mean = feature.mean(0)\n    var = feature.var(0)\n    # avoid division by ~zero\n    eps = 1e-8\n    if (var < eps).any():\n        return mean, 1.0 / (torch.sqrt(var) + eps)\n    return mean, 1.0 / torch.sqrt(var)\n\n\ndef apply_mv_norm(features):\n    # If there is less than 2 spectrograms, the variance cannot be computed (is NaN)\n    # and normalization is not possible, so return the item as it is\n    if features.size(0) < 2:\n        return features\n    mean, invstddev = calc_mean_invstddev(features)\n    res = (features - mean) * invstddev\n    return res\n\n\ndef lengths_to_encoder_padding_mask(lengths, batch_first=False):\n    """"""\n    convert lengths (a 1-D Long/Int tensor) to 2-D binary tensor\n\n    Args:\n        lengths: a (B, )-shaped tensor\n\n    Return:\n        max_length: maximum length of B sequences\n        encoder_padding_mask: a (max_length, B) binary mask, where\n        [t, b] = 0 for t < lengths[b] and 1 otherwise\n\n    TODO:\n        kernelize this function if benchmarking shows this function is slow\n    """"""\n    max_lengths = torch.max(lengths).item()\n    bsz = lengths.size(0)\n    encoder_padding_mask = torch.arange(\n        max_lengths\n    ).to(  # a (T, ) tensor with [0, ..., T-1]\n        lengths.device\n    ).view(  # move to the right device\n        1, max_lengths\n    ).expand(  # reshape to (1, T)-shaped tensor\n        bsz, -1\n    ) >= lengths.view(  # expand to (B, T)-shaped tensor\n        bsz, 1\n    ).expand(\n        -1, max_lengths\n    )\n    if not batch_first:\n        return encoder_padding_mask.t(), max_lengths\n    else:\n        return encoder_padding_mask, max_lengths\n\n\ndef encoder_padding_mask_to_lengths(\n    encoder_padding_mask, max_lengths, batch_size, device\n):\n    """"""\n    convert encoder_padding_mask (2-D binary tensor) to a 1-D tensor\n\n    Conventionally, encoder output contains a encoder_padding_mask, which is\n    a 2-D mask in a shape (T, B), whose (t, b) element indicate whether\n    encoder_out[t, b] is a valid output (=0) or not (=1). Occasionally, we\n    need to convert this mask tensor to a 1-D tensor in shape (B, ), where\n    [b] denotes the valid length of b-th sequence\n\n    Args:\n        encoder_padding_mask: a (T, B)-shaped binary tensor or None; if None,\n        indicating all are valid\n    Return:\n        seq_lengths: a (B,)-shaped tensor, where its (b, )-th element is the\n        number of valid elements of b-th sequence\n\n        max_lengths: maximum length of all sequence, if encoder_padding_mask is\n        not None, max_lengths must equal to encoder_padding_mask.size(0)\n\n        batch_size: batch size; if encoder_padding_mask is\n        not None, max_lengths must equal to encoder_padding_mask.size(1)\n\n        device: which device to put the result on\n    """"""\n    if encoder_padding_mask is None:\n        return torch.Tensor([max_lengths] * batch_size).to(torch.int32).to(device)\n\n    assert encoder_padding_mask.size(0) == max_lengths, ""max_lengths does not match""\n    assert encoder_padding_mask.size(1) == batch_size, ""batch_size does not match""\n\n    return max_lengths - torch.sum(encoder_padding_mask, dim=0)\n'"
examples/speech_recognition/data/replabels.py,0,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nReplabel transforms for use with wav2letter\'s ASG criterion.\n""""""\n\n\ndef replabel_symbol(i):\n    """"""\n    Replabel symbols used in wav2letter, currently just ""1"", ""2"", ...\n    This prevents training with numeral tokens, so this might change in the future\n    """"""\n    return str(i)\n\n\ndef pack_replabels(tokens, dictionary, max_reps):\n    """"""\n    Pack a token sequence so that repeated symbols are replaced by replabels\n    """"""\n    if len(tokens) == 0 or max_reps <= 0:\n        return tokens\n\n    replabel_value_to_idx = [0] * (max_reps + 1)\n    for i in range(1, max_reps + 1):\n        replabel_value_to_idx[i] = dictionary.index(replabel_symbol(i))\n\n    result = []\n    prev_token = -1\n    num_reps = 0\n    for token in tokens:\n        if token == prev_token and num_reps < max_reps:\n            num_reps += 1\n        else:\n            if num_reps > 0:\n                result.append(replabel_value_to_idx[num_reps])\n                num_reps = 0\n            result.append(token)\n            prev_token = token\n    if num_reps > 0:\n        result.append(replabel_value_to_idx[num_reps])\n    return result\n\n\ndef unpack_replabels(tokens, dictionary, max_reps):\n    """"""\n    Unpack a token sequence so that replabels are replaced by repeated symbols\n    """"""\n    if len(tokens) == 0 or max_reps <= 0:\n        return tokens\n\n    replabel_idx_to_value = {}\n    for i in range(1, max_reps + 1):\n        replabel_idx_to_value[dictionary.index(replabel_symbol(i))] = i\n\n    result = []\n    prev_token = -1\n    for token in tokens:\n        try:\n            for _ in range(replabel_idx_to_value[token]):\n                result.append(prev_token)\n            prev_token = -1\n        except KeyError:\n            result.append(token)\n            prev_token = token\n    return result\n'"
examples/speech_recognition/datasets/asr_prep_json.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom collections import namedtuple\nimport concurrent.futures\nfrom itertools import chain\nimport argparse\nimport os\nimport json\nimport sentencepiece as spm\nimport multiprocessing\n\nfrom fairseq.data import Dictionary\n\nMILLISECONDS_TO_SECONDS = 0.001\n\n\ndef process_sample(aud_path, lable, utt_id, sp, tgt_dict):\n    import torchaudio\n    input = {}\n    output = {}\n    si, ei = torchaudio.info(aud_path)\n    input[""length_ms""] = int(si.length / si.channels / si.rate / MILLISECONDS_TO_SECONDS)\n    input[""path""] = aud_path\n\n    token = "" "".join(sp.EncodeAsPieces(lable))\n    ids = tgt_dict.encode_line(token, append_eos=False)\n    output[""text""] = lable\n    output[""token""] = token\n    output[""tokenid""] = \', \'.join(map(str, [t.tolist() for t in ids]))\n    return {utt_id: {""input"": input, ""output"": output}}\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--audio-dirs"", nargs=""+"", default=[\'-\'], required=True,\n                        help=""input directories with audio files"")\n    parser.add_argument(""--labels"", required=True,\n                        help=""aggregated input labels with format <ID LABEL> per line"",\n                        type=argparse.FileType(\'r\', encoding=\'UTF-8\'))\n    parser.add_argument(""--spm-model"", required=True,\n                        help=""sentencepiece model to use for encoding"",\n                        type=argparse.FileType(\'r\', encoding=\'UTF-8\'))\n    parser.add_argument(""--dictionary"", required=True,\n                        help=""file to load fairseq dictionary from"",\n                        type=argparse.FileType(\'r\', encoding=\'UTF-8\'))\n    parser.add_argument(""--audio-format"", choices=[""flac"", ""wav""], default=""wav"")\n    parser.add_argument(""--output"", required=True, type=argparse.FileType(\'w\'),\n                        help=""path to save json output"")\n    args = parser.parse_args()\n\n    sp = spm.SentencePieceProcessor()\n    sp.Load(args.spm_model.name)\n\n    tgt_dict = Dictionary.load(args.dictionary)\n\n    labels = {}\n    for line in args.labels:\n        (utt_id, label) = line.split("" "", 1)\n        labels[utt_id] = label\n    if len(labels) == 0:\n        raise Exception(\'No labels found in \', args.labels_path)\n\n    Sample = namedtuple(\'Sample\', \'aud_path utt_id\')\n    samples = []\n    for path, _, files in chain.from_iterable(os.walk(path) for path in args.audio_dirs):\n        for f in files:\n            if f.endswith(args.audio_format):\n                if len(os.path.splitext(f)) != 2:\n                    raise Exception(\'Expect <utt_id.extension> file name. Got: \', f)\n                utt_id = os.path.splitext(f)[0]\n                if utt_id not in labels:\n                    continue\n                samples.append(Sample(os.path.join(path, f), utt_id))\n\n    utts = {}\n    num_cpu = multiprocessing.cpu_count()\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_cpu) as executor:\n        future_to_sample = {executor.submit(process_sample, s.aud_path, labels[s.utt_id], s.utt_id, sp, tgt_dict): s for s in samples}\n        for future in concurrent.futures.as_completed(future_to_sample):\n            try:\n                data = future.result()\n            except Exception as exc:\n                print(\'generated an exception: \', exc)\n            else:\n                utts.update(data)\n    json.dump({""utts"": utts}, args.output, indent=4)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/speech_recognition/models/__init__.py,0,"b""import importlib\nimport os\n\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        model_name = file[:file.find('.py')]\n        importlib.import_module('examples.speech_recognition.models.' + model_name)\n"""
examples/speech_recognition/models/vggtransformer.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport math\nfrom collections.abc import Iterable\n\nimport torch\nimport torch.nn as nn\nfrom fairseq import utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqEncoderModel,\n    FairseqIncrementalDecoder,\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import LinearizedConvolution\nfrom examples.speech_recognition.data.data_utils import lengths_to_encoder_padding_mask\nfrom fairseq.modules import TransformerDecoderLayer, TransformerEncoderLayer, VGGBlock\n\n\n@register_model(""asr_vggtransformer"")\nclass VGGTransformerModel(FairseqEncoderDecoderModel):\n    """"""\n    Transformers with convolutional context for ASR\n    https://arxiv.org/abs/1904.11660\n    """"""\n    def __init__(self, encoder, decoder):\n        super().__init__(encoder, decoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--input-feat-per-channel"",\n            type=int,\n            metavar=""N"",\n            help=""encoder input dimension per input channel"",\n        )\n        parser.add_argument(\n            ""--vggblock-enc-config"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""\n    an array of tuples each containing the configuration of one vggblock:\n    [(out_channels,\n      conv_kernel_size,\n      pooling_kernel_size,\n      num_conv_layers,\n      use_layer_norm), ...])\n            """""",\n        )\n        parser.add_argument(\n            ""--transformer-enc-config"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""""\n    a tuple containing the configuration of the encoder transformer layers\n    configurations:\n    [(input_dim,\n      num_heads,\n      ffn_dim,\n      normalize_before,\n      dropout,\n      attention_dropout,\n      relu_dropout), ...]\')\n            """""",\n        )\n        parser.add_argument(\n            ""--enc-output-dim"",\n            type=int,\n            metavar=""N"",\n            help=""""""\n    encoder output dimension, can be None. If specified, projecting the\n    transformer output to the specified dimension"""""",\n        )\n        parser.add_argument(\n            ""--in-channels"",\n            type=int,\n            metavar=""N"",\n            help=""number of encoder input channels"",\n        )\n        parser.add_argument(\n            ""--tgt-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""embedding dimension of the decoder target tokens"",\n        )\n        parser.add_argument(\n            ""--transformer-dec-config"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""\n    a tuple containing the configuration of the decoder transformer layers\n    configurations:\n    [(input_dim,\n      num_heads,\n      ffn_dim,\n      normalize_before,\n      dropout,\n      attention_dropout,\n      relu_dropout), ...]\n            """""",\n        )\n        parser.add_argument(\n            ""--conv-dec-config"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""\n    an array of tuples for the decoder 1-D convolution config\n        [(out_channels, conv_kernel_size, use_layer_norm), ...]"""""",\n        )\n\n    @classmethod\n    def build_encoder(cls, args, task):\n        return VGGTransformerEncoder(\n            input_feat_per_channel=args.input_feat_per_channel,\n            vggblock_config=eval(args.vggblock_enc_config),\n            transformer_config=eval(args.transformer_enc_config),\n            encoder_output_dim=args.enc_output_dim,\n            in_channels=args.in_channels,\n        )\n\n    @classmethod\n    def build_decoder(cls, args, task):\n        return TransformerDecoder(\n            dictionary=task.target_dictionary,\n            embed_dim=args.tgt_embed_dim,\n            transformer_config=eval(args.transformer_dec_config),\n            conv_config=eval(args.conv_dec_config),\n            encoder_output_dim=args.enc_output_dim,\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure that all args are properly defaulted\n        # (in case there are any new ones)\n        base_architecture(args)\n\n        encoder = cls.build_encoder(args, task)\n        decoder = cls.build_decoder(args, task)\n        return cls(encoder, decoder)\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n        # net_output[\'encoder_out\'] is a (B, T, D) tensor\n        lprobs = super().get_normalized_probs(net_output, log_probs, sample)\n        lprobs.batch_first = True\n        return lprobs\n\n\nDEFAULT_ENC_VGGBLOCK_CONFIG = ((32, 3, 2, 2, False),) * 2\nDEFAULT_ENC_TRANSFORMER_CONFIG = ((256, 4, 1024, True, 0.2, 0.2, 0.2),) * 2\n# 256: embedding dimension\n# 4: number of heads\n# 1024: FFN\n# True: apply layerNorm before (dropout + resiaul) instead of after\n# 0.2 (dropout): dropout after MultiheadAttention and second FC\n# 0.2 (attention_dropout): dropout in MultiheadAttention\n# 0.2 (relu_dropout): dropout after ReLu\nDEFAULT_DEC_TRANSFORMER_CONFIG = ((256, 2, 1024, True, 0.2, 0.2, 0.2),) * 2\nDEFAULT_DEC_CONV_CONFIG = ((256, 3, True),) * 2\n\n\n# TODO: repace transformer encoder config from one liner\n# to explicit args to get rid of this transformation\ndef prepare_transformer_encoder_params(\n    input_dim,\n    num_heads,\n    ffn_dim,\n    normalize_before,\n    dropout,\n    attention_dropout,\n    relu_dropout,\n):\n    args = argparse.Namespace()\n    args.encoder_embed_dim = input_dim\n    args.encoder_attention_heads = num_heads\n    args.attention_dropout = attention_dropout\n    args.dropout = dropout\n    args.activation_dropout = relu_dropout\n    args.encoder_normalize_before = normalize_before\n    args.encoder_ffn_embed_dim = ffn_dim\n    return args\n\n\ndef prepare_transformer_decoder_params(\n    input_dim,\n    num_heads,\n    ffn_dim,\n    normalize_before,\n    dropout,\n    attention_dropout,\n    relu_dropout,\n):\n    args = argparse.Namespace()\n    args.decoder_embed_dim = input_dim\n    args.decoder_attention_heads = num_heads\n    args.attention_dropout = attention_dropout\n    args.dropout = dropout\n    args.activation_dropout = relu_dropout\n    args.decoder_normalize_before = normalize_before\n    args.decoder_ffn_embed_dim = ffn_dim\n    return args\n\n\nclass VGGTransformerEncoder(FairseqEncoder):\n    """"""VGG + Transformer encoder""""""\n\n    def __init__(\n        self,\n        input_feat_per_channel,\n        vggblock_config=DEFAULT_ENC_VGGBLOCK_CONFIG,\n        transformer_config=DEFAULT_ENC_TRANSFORMER_CONFIG,\n        encoder_output_dim=512,\n        in_channels=1,\n        transformer_context=None,\n        transformer_sampling=None,\n    ):\n        """"""constructor for VGGTransformerEncoder\n\n        Args:\n            - input_feat_per_channel: feature dim (not including stacked,\n              just base feature)\n            - in_channel: # input channels (e.g., if stack 8 feature vector\n                together, this is 8)\n            - vggblock_config: configuration of vggblock, see comments on\n                DEFAULT_ENC_VGGBLOCK_CONFIG\n            - transformer_config: configuration of transformer layer, see comments\n                on DEFAULT_ENC_TRANSFORMER_CONFIG\n            - encoder_output_dim: final transformer output embedding dimension\n            - transformer_context: (left, right) if set, self-attention will be focused\n              on (t-left, t+right)\n            - transformer_sampling: an iterable of int, must match with\n              len(transformer_config), transformer_sampling[i] indicates sampling\n              factor for i-th transformer layer, after multihead att and feedfoward\n              part\n        """"""\n        super().__init__(None)\n\n        self.num_vggblocks = 0\n        if vggblock_config is not None:\n            if not isinstance(vggblock_config, Iterable):\n                raise ValueError(""vggblock_config is not iterable"")\n            self.num_vggblocks = len(vggblock_config)\n\n        self.conv_layers = nn.ModuleList()\n        self.in_channels = in_channels\n        self.input_dim = input_feat_per_channel\n\n        if vggblock_config is not None:\n            for _, config in enumerate(vggblock_config):\n                (\n                    out_channels,\n                    conv_kernel_size,\n                    pooling_kernel_size,\n                    num_conv_layers,\n                    layer_norm,\n                ) = config\n                self.conv_layers.append(\n                    VGGBlock(\n                        in_channels,\n                        out_channels,\n                        conv_kernel_size,\n                        pooling_kernel_size,\n                        num_conv_layers,\n                        input_dim=input_feat_per_channel,\n                        layer_norm=layer_norm,\n                    )\n                )\n                in_channels = out_channels\n                input_feat_per_channel = self.conv_layers[-1].output_dim\n\n        transformer_input_dim = self.infer_conv_output_dim(\n            self.in_channels, self.input_dim\n        )\n        # transformer_input_dim is the output dimension of VGG part\n\n        self.validate_transformer_config(transformer_config)\n        self.transformer_context = self.parse_transformer_context(transformer_context)\n        self.transformer_sampling = self.parse_transformer_sampling(\n            transformer_sampling, len(transformer_config)\n        )\n\n        self.transformer_layers = nn.ModuleList()\n\n        if transformer_input_dim != transformer_config[0][0]:\n            self.transformer_layers.append(\n                Linear(transformer_input_dim, transformer_config[0][0])\n            )\n        self.transformer_layers.append(\n            TransformerEncoderLayer(\n                prepare_transformer_encoder_params(*transformer_config[0])\n            )\n        )\n\n        for i in range(1, len(transformer_config)):\n            if transformer_config[i - 1][0] != transformer_config[i][0]:\n                self.transformer_layers.append(\n                    Linear(transformer_config[i - 1][0], transformer_config[i][0])\n                )\n            self.transformer_layers.append(\n                TransformerEncoderLayer(\n                    prepare_transformer_encoder_params(*transformer_config[i])\n                )\n            )\n\n        self.encoder_output_dim = encoder_output_dim\n        self.transformer_layers.extend(\n            [\n                Linear(transformer_config[-1][0], encoder_output_dim),\n                LayerNorm(encoder_output_dim),\n            ]\n        )\n\n    def forward(self, src_tokens, src_lengths, **kwargs):\n        """"""\n        src_tokens: padded tensor (B, T, C * feat)\n        src_lengths: tensor of original lengths of input utterances (B,)\n        """"""\n        bsz, max_seq_len, _ = src_tokens.size()\n        x = src_tokens.view(bsz, max_seq_len, self.in_channels, self.input_dim)\n        x = x.transpose(1, 2).contiguous()\n        # (B, C, T, feat)\n\n        for layer_idx in range(len(self.conv_layers)):\n            x = self.conv_layers[layer_idx](x)\n\n        bsz, _, output_seq_len, _ = x.size()\n\n        # (B, C, T, feat) -> (B, T, C, feat) -> (T, B, C, feat) -> (T, B, C * feat)\n        x = x.transpose(1, 2).transpose(0, 1)\n        x = x.contiguous().view(output_seq_len, bsz, -1)\n\n        subsampling_factor = int(max_seq_len * 1.0 / output_seq_len + 0.5)\n        # TODO: shouldn\'t subsampling_factor determined in advance ?\n        input_lengths = (src_lengths.float() / subsampling_factor).ceil().long()\n\n        encoder_padding_mask, _ = lengths_to_encoder_padding_mask(\n            input_lengths, batch_first=True\n        )\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        attn_mask = self.lengths_to_attn_mask(input_lengths, subsampling_factor)\n\n        transformer_layer_idx = 0\n\n        for layer_idx in range(len(self.transformer_layers)):\n\n            if isinstance(self.transformer_layers[layer_idx], TransformerEncoderLayer):\n                x = self.transformer_layers[layer_idx](\n                    x, encoder_padding_mask, attn_mask\n                )\n\n                if self.transformer_sampling[transformer_layer_idx] != 1:\n                    sampling_factor = self.transformer_sampling[transformer_layer_idx]\n                    x, encoder_padding_mask, attn_mask = self.slice(\n                        x, encoder_padding_mask, attn_mask, sampling_factor\n                    )\n\n                transformer_layer_idx += 1\n\n            else:\n                x = self.transformer_layers[layer_idx](x)\n\n        # encoder_padding_maks is a (T x B) tensor, its [t, b] elements indicate\n        # whether encoder_output[t, b] is valid or not (valid=0, invalid=1)\n\n        return {\n            ""encoder_out"": x,  # (T, B, C)\n            ""encoder_padding_mask"": encoder_padding_mask.t()\n            if encoder_padding_mask is not None\n            else None,\n            # (B, T) --> (T, B)\n        }\n\n    def infer_conv_output_dim(self, in_channels, input_dim):\n        sample_seq_len = 200\n        sample_bsz = 10\n        x = torch.randn(sample_bsz, in_channels, sample_seq_len, input_dim)\n        for i, _ in enumerate(self.conv_layers):\n            x = self.conv_layers[i](x)\n        x = x.transpose(1, 2)\n        mb, seq = x.size()[:2]\n        return x.contiguous().view(mb, seq, -1).size(-1)\n\n    def validate_transformer_config(self, transformer_config):\n        for config in transformer_config:\n            input_dim, num_heads = config[:2]\n            if input_dim % num_heads != 0:\n                msg = (\n                    ""ERROR in transformer config {}:"".format(config)\n                    + ""input dimension {} "".format(input_dim)\n                    + ""not dividable by number of heads"".format(num_heads)\n                )\n                raise ValueError(msg)\n\n    def parse_transformer_context(self, transformer_context):\n        """"""\n        transformer_context can be the following:\n        -   None; indicates no context is used, i.e.,\n            transformer can access full context\n        -   a tuple/list of two int; indicates left and right context,\n            any number <0 indicates infinite context\n                * e.g., (5, 6) indicates that for query at x_t, transformer can\n                access [t-5, t+6] (inclusive)\n                * e.g., (-1, 6) indicates that for query at x_t, transformer can\n                access [0, t+6] (inclusive)\n        """"""\n        if transformer_context is None:\n            return None\n\n        if not isinstance(transformer_context, Iterable):\n            raise ValueError(""transformer context must be Iterable if it is not None"")\n\n        if len(transformer_context) != 2:\n            raise ValueError(""transformer context must have length 2"")\n\n        left_context = transformer_context[0]\n        if left_context < 0:\n            left_context = None\n\n        right_context = transformer_context[1]\n        if right_context < 0:\n            right_context = None\n\n        if left_context is None and right_context is None:\n            return None\n\n        return (left_context, right_context)\n\n    def parse_transformer_sampling(self, transformer_sampling, num_layers):\n        """"""\n        parsing transformer sampling configuration\n\n        Args:\n            - transformer_sampling, accepted input:\n                * None, indicating no sampling\n                * an Iterable with int (>0) as element\n            - num_layers, expected number of transformer layers, must match with\n              the length of transformer_sampling if it is not None\n\n        Returns:\n            - A tuple with length num_layers\n        """"""\n        if transformer_sampling is None:\n            return (1,) * num_layers\n\n        if not isinstance(transformer_sampling, Iterable):\n            raise ValueError(\n                ""transformer_sampling must be an iterable if it is not None""\n            )\n\n        if len(transformer_sampling) != num_layers:\n            raise ValueError(\n                ""transformer_sampling {} does not match with the number ""\n                + ""of layers {}"".format(transformer_sampling, num_layers)\n            )\n\n        for layer, value in enumerate(transformer_sampling):\n            if not isinstance(value, int):\n                raise ValueError(""Invalid value in transformer_sampling: "")\n            if value < 1:\n                raise ValueError(\n                    ""{} layer\'s subsampling is {}."".format(layer, value)\n                    + "" This is not allowed! ""\n                )\n        return transformer_sampling\n\n    def slice(self, embedding, padding_mask, attn_mask, sampling_factor):\n        """"""\n        embedding is a (T, B, D) tensor\n        padding_mask is a (B, T) tensor or None\n        attn_mask is a (T, T) tensor or None\n        """"""\n        embedding = embedding[::sampling_factor, :, :]\n        if padding_mask is not None:\n            padding_mask = padding_mask[:, ::sampling_factor]\n        if attn_mask is not None:\n            attn_mask = attn_mask[::sampling_factor, ::sampling_factor]\n\n        return embedding, padding_mask, attn_mask\n\n    def lengths_to_attn_mask(self, input_lengths, subsampling_factor=1):\n        """"""\n        create attention mask according to sequence lengths and transformer\n        context\n\n        Args:\n            - input_lengths: (B, )-shape Int/Long tensor; input_lengths[b] is\n              the length of b-th sequence\n            - subsampling_factor: int\n                * Note that the left_context and right_context is specified in\n                  the input frame-level while input to transformer may already\n                  go through subsampling (e.g., the use of striding in vggblock)\n                  we use subsampling_factor to scale the left/right context\n\n        Return:\n            - a (T, T) binary tensor or None, where T is max(input_lengths)\n                * if self.transformer_context is None, None\n                * if left_context is None,\n                    * attn_mask[t, t + right_context + 1:] = 1\n                    * others = 0\n                * if right_context is None,\n                    * attn_mask[t, 0:t - left_context] = 1\n                    * others = 0\n                * elsif\n                    * attn_mask[t, t - left_context: t + right_context + 1] = 0\n                    * others = 1\n        """"""\n        if self.transformer_context is None:\n            return None\n\n        maxT = torch.max(input_lengths).item()\n        attn_mask = torch.zeros(maxT, maxT)\n\n        left_context = self.transformer_context[0]\n        right_context = self.transformer_context[1]\n        if left_context is not None:\n            left_context = math.ceil(self.transformer_context[0] / subsampling_factor)\n        if right_context is not None:\n            right_context = math.ceil(self.transformer_context[1] / subsampling_factor)\n\n        for t in range(maxT):\n            if left_context is not None:\n                st = 0\n                en = max(st, t - left_context)\n                attn_mask[t, st:en] = 1\n            if right_context is not None:\n                st = t + right_context + 1\n                st = min(st, maxT - 1)\n                attn_mask[t, st:] = 1\n\n        return attn_mask.to(input_lengths.device)\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        encoder_out[""encoder_out""] = encoder_out[""encoder_out""].index_select(\n            1, new_order\n        )\n        if encoder_out[""encoder_padding_mask""] is not None:\n            encoder_out[""encoder_padding_mask""] = encoder_out[\n                ""encoder_padding_mask""\n            ].index_select(1, new_order)\n        return encoder_out\n\n\nclass TransformerDecoder(FairseqIncrementalDecoder):\n    """"""\n    Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`TransformerDecoderLayer`.\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): decoding dictionary\n        embed_tokens (torch.nn.Embedding): output embedding\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs.\n            Default: ``False``\n        left_pad (bool, optional): whether the input is left-padded. Default:\n            ``False``\n    """"""\n\n    def __init__(\n        self,\n        dictionary,\n        embed_dim=512,\n        transformer_config=DEFAULT_ENC_TRANSFORMER_CONFIG,\n        conv_config=DEFAULT_DEC_CONV_CONFIG,\n        encoder_output_dim=512,\n    ):\n\n        super().__init__(dictionary)\n        vocab_size = len(dictionary)\n        self.padding_idx = dictionary.pad()\n        self.embed_tokens = Embedding(vocab_size, embed_dim, self.padding_idx)\n\n        self.conv_layers = nn.ModuleList()\n        for i in range(len(conv_config)):\n            out_channels, kernel_size, layer_norm = conv_config[i]\n            if i == 0:\n                conv_layer = LinearizedConv1d(\n                    embed_dim, out_channels, kernel_size, padding=kernel_size - 1\n                )\n            else:\n                conv_layer = LinearizedConv1d(\n                    conv_config[i - 1][0],\n                    out_channels,\n                    kernel_size,\n                    padding=kernel_size - 1,\n                )\n            self.conv_layers.append(conv_layer)\n            if layer_norm:\n                self.conv_layers.append(nn.LayerNorm(out_channels))\n            self.conv_layers.append(nn.ReLU())\n\n        self.layers = nn.ModuleList()\n        if conv_config[-1][0] != transformer_config[0][0]:\n            self.layers.append(Linear(conv_config[-1][0], transformer_config[0][0]))\n        self.layers.append(TransformerDecoderLayer(\n            prepare_transformer_decoder_params(*transformer_config[0])\n        ))\n\n        for i in range(1, len(transformer_config)):\n            if transformer_config[i - 1][0] != transformer_config[i][0]:\n                self.layers.append(\n                    Linear(transformer_config[i - 1][0], transformer_config[i][0])\n                )\n            self.layers.append(TransformerDecoderLayer(\n                prepare_transformer_decoder_params(*transformer_config[i])\n            ))\n        self.fc_out = Linear(transformer_config[-1][0], vocab_size)\n\n    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None):\n        """"""\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n        Returns:\n            tuple:\n                - the last decoder layer\'s output of shape `(batch, tgt_len,\n                  vocab)`\n                - the last decoder layer\'s attention weights of shape `(batch,\n                  tgt_len, src_len)`\n        """"""\n        target_padding_mask = (\n            (prev_output_tokens == self.padding_idx).to(prev_output_tokens.device)\n            if incremental_state is None\n            else None\n        )\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n\n        # embed tokens\n        x = self.embed_tokens(prev_output_tokens)\n\n        # B x T x C -> T x B x C\n        x = self._transpose_if_training(x, incremental_state)\n\n        for layer in self.conv_layers:\n            if isinstance(layer, LinearizedConvolution):\n                x = layer(x, incremental_state)\n            else:\n                x = layer(x)\n\n        # B x T x C -> T x B x C\n        x = self._transpose_if_inference(x, incremental_state)\n\n        # decoder layers\n        for layer in self.layers:\n            if isinstance(layer, TransformerDecoderLayer):\n                x, *_ = layer(\n                    x,\n                    (encoder_out[""encoder_out""] if encoder_out is not None else None),\n                    (\n                        encoder_out[""encoder_padding_mask""].t()\n                        if encoder_out[""encoder_padding_mask""] is not None\n                        else None\n                    ),\n                    incremental_state,\n                    self_attn_mask=(\n                        self.buffered_future_mask(x)\n                        if incremental_state is None\n                        else None\n                    ),\n                    self_attn_padding_mask=(\n                        target_padding_mask if incremental_state is None else None\n                    ),\n                )\n            else:\n                x = layer(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        x = self.fc_out(x)\n\n        return x, None\n\n    def buffered_future_mask(self, tensor):\n        dim = tensor.size(0)\n        if (\n            not hasattr(self, ""_future_mask"")\n            or self._future_mask is None\n            or self._future_mask.device != tensor.device\n        ):\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(tensor.new(dim, dim)), 1\n            )\n        if self._future_mask.size(0) < dim:\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(self._future_mask.resize_(dim, dim)), 1\n            )\n        return self._future_mask[:dim, :dim]\n\n    def _transpose_if_training(self, x, incremental_state):\n        if incremental_state is None:\n            x = x.transpose(0, 1)\n        return x\n\n    def _transpose_if_inference(self, x, incremental_state):\n        if incremental_state:\n            x = x.transpose(0, 1)\n        return x\n\n@register_model(""asr_vggtransformer_encoder"")\nclass VGGTransformerEncoderModel(FairseqEncoderModel):\n    def __init__(self, encoder):\n        super().__init__(encoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--input-feat-per-channel"",\n            type=int,\n            metavar=""N"",\n            help=""encoder input dimension per input channel"",\n        )\n        parser.add_argument(\n            ""--vggblock-enc-config"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""\n    an array of tuples each containing the configuration of one vggblock\n    [(out_channels, conv_kernel_size, pooling_kernel_size,num_conv_layers), ...]\n    """""",\n        )\n        parser.add_argument(\n            ""--transformer-enc-config"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""\n    a tuple containing the configuration of the Transformer layers\n    configurations:\n    [(input_dim,\n      num_heads,\n      ffn_dim,\n      normalize_before,\n      dropout,\n      attention_dropout,\n      relu_dropout), ]"""""",\n        )\n        parser.add_argument(\n            ""--enc-output-dim"",\n            type=int,\n            metavar=""N"",\n            help=""encoder output dimension, projecting the LSTM output"",\n        )\n        parser.add_argument(\n            ""--in-channels"",\n            type=int,\n            metavar=""N"",\n            help=""number of encoder input channels"",\n        )\n        parser.add_argument(\n            ""--transformer-context"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""\n    either None or a tuple of two ints, indicating left/right context a\n    transformer can have access to"""""",\n        )\n        parser.add_argument(\n            ""--transformer-sampling"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""\n    either None or a tuple of ints, indicating sampling factor in each layer"""""",\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        base_architecture_enconly(args)\n        encoder = VGGTransformerEncoderOnly(\n            vocab_size=len(task.target_dictionary),\n            input_feat_per_channel=args.input_feat_per_channel,\n            vggblock_config=eval(args.vggblock_enc_config),\n            transformer_config=eval(args.transformer_enc_config),\n            encoder_output_dim=args.enc_output_dim,\n            in_channels=args.in_channels,\n            transformer_context=eval(args.transformer_context),\n            transformer_sampling=eval(args.transformer_sampling),\n        )\n        return cls(encoder)\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n        # net_output[\'encoder_out\'] is a (T, B, D) tensor\n        lprobs = super().get_normalized_probs(net_output, log_probs, sample)\n        # lprobs is a (T, B, D) tensor\n        # we need to transoose to get (B, T, D) tensor\n        lprobs = lprobs.transpose(0, 1).contiguous()\n        lprobs.batch_first = True\n        return lprobs\n\n\nclass VGGTransformerEncoderOnly(VGGTransformerEncoder):\n    def __init__(\n        self,\n        vocab_size,\n        input_feat_per_channel,\n        vggblock_config=DEFAULT_ENC_VGGBLOCK_CONFIG,\n        transformer_config=DEFAULT_ENC_TRANSFORMER_CONFIG,\n        encoder_output_dim=512,\n        in_channels=1,\n        transformer_context=None,\n        transformer_sampling=None,\n    ):\n        super().__init__(\n            input_feat_per_channel=input_feat_per_channel,\n            vggblock_config=vggblock_config,\n            transformer_config=transformer_config,\n            encoder_output_dim=encoder_output_dim,\n            in_channels=in_channels,\n            transformer_context=transformer_context,\n            transformer_sampling=transformer_sampling,\n        )\n        self.fc_out = Linear(self.encoder_output_dim, vocab_size)\n\n    def forward(self, src_tokens, src_lengths, **kwargs):\n        """"""\n        src_tokens: padded tensor (B, T, C * feat)\n        src_lengths: tensor of original lengths of input utterances (B,)\n        """"""\n\n        enc_out = super().forward(src_tokens, src_lengths)\n        x = self.fc_out(enc_out[""encoder_out""])\n        # x = F.log_softmax(x, dim=-1)\n        # Note: no need this line, because model.get_normalized_prob will call\n        # log_softmax\n        return {\n            ""encoder_out"": x,  # (T, B, C)\n            ""encoder_padding_mask"": enc_out[""encoder_padding_mask""],  # (T, B)\n        }\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return (1e6, 1e6)  # an arbitrary large number\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    # nn.init.uniform_(m.weight, -0.1, 0.1)\n    # nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n\ndef Linear(in_features, out_features, bias=True, dropout=0):\n    """"""Linear layer (input: N x T x C)""""""\n    m = nn.Linear(in_features, out_features, bias=bias)\n    # m.weight.data.uniform_(-0.1, 0.1)\n    # if bias:\n    #     m.bias.data.uniform_(-0.1, 0.1)\n    return m\n\n\ndef LinearizedConv1d(in_channels, out_channels, kernel_size, dropout=0, **kwargs):\n    """"""Weight-normalized Conv1d layer optimized for decoding""""""\n    m = LinearizedConvolution(in_channels, out_channels, kernel_size, **kwargs)\n    std = math.sqrt((4 * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n    nn.init.normal_(m.weight, mean=0, std=std)\n    nn.init.constant_(m.bias, 0)\n    return nn.utils.weight_norm(m, dim=2)\n\n\ndef LayerNorm(embedding_dim):\n    m = nn.LayerNorm(embedding_dim)\n    return m\n\n\n# seq2seq models\ndef base_architecture(args):\n    args.input_feat_per_channel = getattr(args, ""input_feat_per_channel"", 40)\n    args.vggblock_enc_config = getattr(\n        args, ""vggblock_enc_config"", DEFAULT_ENC_VGGBLOCK_CONFIG\n    )\n    args.transformer_enc_config = getattr(\n        args, ""transformer_enc_config"", DEFAULT_ENC_TRANSFORMER_CONFIG\n    )\n    args.enc_output_dim = getattr(args, ""enc_output_dim"", 512)\n    args.in_channels = getattr(args, ""in_channels"", 1)\n    args.tgt_embed_dim = getattr(args, ""tgt_embed_dim"", 128)\n    args.transformer_dec_config = getattr(\n        args, ""transformer_dec_config"", DEFAULT_ENC_TRANSFORMER_CONFIG\n    )\n    args.conv_dec_config = getattr(args, ""conv_dec_config"", DEFAULT_DEC_CONV_CONFIG)\n    args.transformer_context = getattr(args, ""transformer_context"", ""None"")\n\n\n@register_model_architecture(""asr_vggtransformer"", ""vggtransformer_1"")\ndef vggtransformer_1(args):\n    args.input_feat_per_channel = getattr(args, ""input_feat_per_channel"", 80)\n    args.vggblock_enc_config = getattr(\n        args, ""vggblock_enc_config"", ""[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]""\n    )\n    args.transformer_enc_config = getattr(\n        args,\n        ""transformer_enc_config"",\n        ""((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 14"",\n    )\n    args.enc_output_dim = getattr(args, ""enc_output_dim"", 1024)\n    args.tgt_embed_dim = getattr(args, ""tgt_embed_dim"", 128)\n    args.conv_dec_config = getattr(args, ""conv_dec_config"", ""((256, 3, True),) * 4"")\n    args.transformer_dec_config = getattr(\n        args,\n        ""transformer_dec_config"",\n        ""((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 4"",\n    )\n\n\n@register_model_architecture(""asr_vggtransformer"", ""vggtransformer_2"")\ndef vggtransformer_2(args):\n    args.input_feat_per_channel = getattr(args, ""input_feat_per_channel"", 80)\n    args.vggblock_enc_config = getattr(\n        args, ""vggblock_enc_config"", ""[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]""\n    )\n    args.transformer_enc_config = getattr(\n        args,\n        ""transformer_enc_config"",\n        ""((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 16"",\n    )\n    args.enc_output_dim = getattr(args, ""enc_output_dim"", 1024)\n    args.tgt_embed_dim = getattr(args, ""tgt_embed_dim"", 512)\n    args.conv_dec_config = getattr(args, ""conv_dec_config"", ""((256, 3, True),) * 4"")\n    args.transformer_dec_config = getattr(\n        args,\n        ""transformer_dec_config"",\n        ""((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 6"",\n    )\n\n\n@register_model_architecture(""asr_vggtransformer"", ""vggtransformer_base"")\ndef vggtransformer_base(args):\n    args.input_feat_per_channel = getattr(args, ""input_feat_per_channel"", 80)\n    args.vggblock_enc_config = getattr(\n        args, ""vggblock_enc_config"", ""[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]""\n    )\n    args.transformer_enc_config = getattr(\n        args, ""transformer_enc_config"", ""((512, 8, 2048, True, 0.15, 0.15, 0.15),) * 12""\n    )\n\n    args.enc_output_dim = getattr(args, ""enc_output_dim"", 512)\n    args.tgt_embed_dim = getattr(args, ""tgt_embed_dim"", 512)\n    args.conv_dec_config = getattr(args, ""conv_dec_config"", ""((256, 3, True),) * 4"")\n    args.transformer_dec_config = getattr(\n        args, ""transformer_dec_config"", ""((512, 8, 2048, True, 0.15, 0.15, 0.15),) * 6""\n    )\n    # Size estimations:\n    # Encoder:\n    #   - vggblock param: 64*1*3*3 + 64*64*3*3 + 128*64*3*3  + 128*128*3 = 258K\n    #   Transformer:\n    #   - input dimension adapter: 2560 x 512 -> 1.31M\n    #   - transformer_layers (x12) --> 37.74M\n    #       * MultiheadAttention: 512*512*3 (in_proj) + 512*512 (out_proj) = 1.048M\n    #       * FFN weight: 512*2048*2 = 2.097M\n    #   - output dimension adapter: 512 x 512 -> 0.26 M\n    # Decoder:\n    #   - LinearizedConv1d: 512 * 256 * 3 + 256 * 256 * 3 * 3\n    #   - transformer_layer: (x6) --> 25.16M\n    #        * MultiheadAttention (self-attention): 512*512*3 + 512*512 = 1.048M\n    #        * MultiheadAttention (encoder-attention): 512*512*3 + 512*512 = 1.048M\n    #        * FFN: 512*2048*2 = 2.097M\n    # Final FC:\n    #   - FC: 512*5000 = 256K (assuming vocab size 5K)\n    # In total:\n    #       ~65 M\n\n\n# CTC models\ndef base_architecture_enconly(args):\n    args.input_feat_per_channel = getattr(args, ""input_feat_per_channel"", 40)\n    args.vggblock_enc_config = getattr(\n        args, ""vggblock_enc_config"", ""[(32, 3, 2, 2, True)] * 2""\n    )\n    args.transformer_enc_config = getattr(\n        args, ""transformer_enc_config"", ""((256, 4, 1024, True, 0.2, 0.2, 0.2),) * 2""\n    )\n    args.enc_output_dim = getattr(args, ""enc_output_dim"", 512)\n    args.in_channels = getattr(args, ""in_channels"", 1)\n    args.transformer_context = getattr(args, ""transformer_context"", ""None"")\n    args.transformer_sampling = getattr(args, ""transformer_sampling"", ""None"")\n\n\n@register_model_architecture(""asr_vggtransformer_encoder"", ""vggtransformer_enc_1"")\ndef vggtransformer_enc_1(args):\n    # vggtransformer_1 is the same as vggtransformer_enc_big, except the number\n    # of layers is increased to 16\n    # keep it here for backward compatiablity purpose\n    args.input_feat_per_channel = getattr(args, ""input_feat_per_channel"", 80)\n    args.vggblock_enc_config = getattr(\n        args, ""vggblock_enc_config"", ""[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]""\n    )\n    args.transformer_enc_config = getattr(\n        args,\n        ""transformer_enc_config"",\n        ""((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 16"",\n    )\n    args.enc_output_dim = getattr(args, ""enc_output_dim"", 1024)\n'"
examples/speech_recognition/models/w2l_conv_glu_enc.py,3,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqEncoderModel,\n    register_model,\n    register_model_architecture,\n)\n\n\ndefault_conv_enc_config = """"""[ \n    (400, 13, 170, 0.2),\n    (440, 14, 0, 0.214),\n    (484, 15, 0, 0.22898),\n    (532, 16, 0, 0.2450086),\n    (584, 17, 0, 0.262159202),\n    (642, 18, 0, 0.28051034614),\n    (706, 19, 0, 0.30014607037),\n    (776, 20, 0, 0.321156295296),\n    (852, 21, 0, 0.343637235966),\n    (936, 22, 0, 0.367691842484),\n    (1028, 23, 0, 0.393430271458),\n    (1130, 24, 0, 0.42097039046),\n    (1242, 25, 0, 0.450438317792),\n    (1366, 26, 0, 0.481969000038),\n    (1502, 27, 0, 0.51570683004),\n    (1652, 28, 0, 0.551806308143),\n    (1816, 29, 0, 0.590432749713),\n]""""""\n\n\n@register_model(""asr_w2l_conv_glu_encoder"")\nclass W2lConvGluEncoderModel(FairseqEncoderModel):\n    def __init__(self, encoder):\n        super().__init__(encoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--input-feat-per-channel"",\n            type=int,\n            metavar=""N"",\n            help=""encoder input dimension per input channel"",\n        )\n        parser.add_argument(\n            ""--in-channels"",\n            type=int,\n            metavar=""N"",\n            help=""number of encoder input channels"",\n        )\n        parser.add_argument(\n            ""--conv-enc-config"",\n            type=str,\n            metavar=""EXPR"",\n            help=""""""\n    an array of tuples each containing the configuration of one conv layer\n    [(out_channels, kernel_size, padding, dropout), ...]\n            """""",\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        conv_enc_config = getattr(args, ""conv_enc_config"", default_conv_enc_config)\n        encoder = W2lConvGluEncoder(\n            vocab_size=len(task.target_dictionary),\n            input_feat_per_channel=args.input_feat_per_channel,\n            in_channels=args.in_channels,\n            conv_enc_config=eval(conv_enc_config),\n        )\n        return cls(encoder)\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n        lprobs = super().get_normalized_probs(net_output, log_probs, sample)\n        lprobs.batch_first = False\n        return lprobs\n\n\nclass W2lConvGluEncoder(FairseqEncoder):\n    def __init__(\n        self, vocab_size, input_feat_per_channel, in_channels, conv_enc_config\n    ):\n        super().__init__(None)\n\n        self.input_dim = input_feat_per_channel\n        if in_channels != 1:\n            raise ValueError(""only 1 input channel is currently supported"")\n\n        self.conv_layers = nn.ModuleList()\n        self.linear_layers = nn.ModuleList()\n        self.dropouts = []\n        cur_channels = input_feat_per_channel\n\n        for out_channels, kernel_size, padding, dropout in conv_enc_config:\n            layer = nn.Conv1d(cur_channels, out_channels, kernel_size, padding=padding)\n            layer.weight.data.mul_(math.sqrt(3))  # match wav2letter init\n            self.conv_layers.append(nn.utils.weight_norm(layer))\n            self.dropouts.append(dropout)\n            if out_channels % 2 != 0:\n                raise ValueError(""odd # of out_channels is incompatible with GLU"")\n            cur_channels = out_channels // 2  # halved by GLU\n\n        for out_channels in [2 * cur_channels, vocab_size]:\n            layer = nn.Linear(cur_channels, out_channels)\n            layer.weight.data.mul_(math.sqrt(3))\n            self.linear_layers.append(nn.utils.weight_norm(layer))\n            cur_channels = out_channels // 2\n\n    def forward(self, src_tokens, src_lengths, **kwargs):\n\n        """"""\n        src_tokens: padded tensor (B, T, C * feat)\n        src_lengths: tensor of original lengths of input utterances (B,)\n        """"""\n        B, T, _ = src_tokens.size()\n        x = src_tokens.transpose(1, 2).contiguous()  # (B, feat, T) assuming C == 1\n\n        for layer_idx in range(len(self.conv_layers)):\n            x = self.conv_layers[layer_idx](x)\n            x = F.glu(x, dim=1)\n            x = F.dropout(x, p=self.dropouts[layer_idx], training=self.training)\n\n        x = x.transpose(1, 2).contiguous()  # (B, T, 908)\n        x = self.linear_layers[0](x)\n        x = F.glu(x, dim=2)\n        x = F.dropout(x, p=self.dropouts[-1])\n        x = self.linear_layers[1](x)\n\n        assert x.size(0) == B\n        assert x.size(1) == T\n\n        encoder_out = x.transpose(0, 1)  # (T, B, vocab_size)\n\n        # need to debug this -- find a simpler/elegant way in pytorch APIs\n        encoder_padding_mask = (\n            torch.arange(T).view(1, T).expand(B, -1).to(x.device)\n            >= src_lengths.view(B, 1).expand(-1, T)\n        ).t()  # (B x T) -> (T x B)\n\n        return {\n            ""encoder_out"": encoder_out,  # (T, B, vocab_size)\n            ""encoder_padding_mask"": encoder_padding_mask,  # (T, B)\n        }\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        encoder_out[""encoder_out""] = encoder_out[""encoder_out""].index_select(\n            1, new_order\n        )\n        encoder_out[""encoder_padding_mask""] = encoder_out[\n            ""encoder_padding_mask""\n        ].index_select(1, new_order)\n        return encoder_out\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return (1e6, 1e6)  # an arbitrary large number\n\n\n@register_model_architecture(""asr_w2l_conv_glu_encoder"", ""w2l_conv_glu_enc"")\ndef w2l_conv_glu_enc(args):\n    args.input_feat_per_channel = getattr(args, ""input_feat_per_channel"", 80)\n    args.in_channels = getattr(args, ""in_channels"", 1)\n    args.conv_enc_config = getattr(args, ""conv_enc_config"", default_conv_enc_config)\n'"
examples/speech_recognition/tasks/__init__.py,0,"b""import importlib\nimport os\n\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        task_name = file[:file.find('.py')]\n        importlib.import_module('examples.speech_recognition.tasks.' + task_name)\n"""
examples/speech_recognition/tasks/speech_recognition.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport json\nimport os\nimport re\n\nimport torch\nfrom fairseq.data import Dictionary\nfrom fairseq.tasks import FairseqTask, register_task\nfrom examples.speech_recognition.data import AsrDataset\nfrom examples.speech_recognition.data.replabels import replabel_symbol\n\n\ndef get_asr_dataset_from_json(data_json_path, tgt_dict):\n    """"""\n    Parse data json and create dataset.\n    See scripts/asr_prep_json.py which pack json from raw files\n\n    Json example:\n    {\n    ""utts"": {\n        ""4771-29403-0025"": {\n            ""input"": {\n                ""length_ms"": 170,\n                ""path"": ""/tmp/file1.flac""\n            },\n            ""output"": {\n                ""text"": ""HELLO \\n"",\n                ""token"": ""HE LLO"",\n                ""tokenid"": ""4815, 861""\n            }\n        },\n        ""1564-142299-0096"": {\n            ...\n        }\n    }\n    """"""\n    if not os.path.isfile(data_json_path):\n        raise FileNotFoundError(""Dataset not found: {}"".format(data_json_path))\n    with open(data_json_path, ""rb"") as f:\n        data_samples = json.load(f)[""utts""]\n        assert len(data_samples) != 0\n        sorted_samples = sorted(\n            data_samples.items(),\n            key=lambda sample: int(sample[1][""input""][""length_ms""]),\n            reverse=True,\n        )\n        aud_paths = [s[1][""input""][""path""] for s in sorted_samples]\n        ids = [s[0] for s in sorted_samples]\n        speakers = []\n        for s in sorted_samples:\n            m = re.search(""(.+?)-(.+?)-(.+?)"", s[0])\n            speakers.append(m.group(1) + ""_"" + m.group(2))\n        frame_sizes = [s[1][""input""][""length_ms""] for s in sorted_samples]\n        tgt = [\n            [int(i) for i in s[1][""output""][""tokenid""].split("", "")]\n            for s in sorted_samples\n        ]\n        # append eos\n        tgt = [[*t, tgt_dict.eos()] for t in tgt]\n        return AsrDataset(aud_paths, frame_sizes, tgt, tgt_dict, ids, speakers)\n\n\n@register_task(""speech_recognition"")\nclass SpeechRecognitionTask(FairseqTask):\n    """"""\n    Task for training speech recognition model.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(""data"", help=""path to data directory"")\n        parser.add_argument(\n            ""--silence-token"", default=""\\u2581"", help=""token for silence (used by w2l)""\n        )\n\n    def __init__(self, args, tgt_dict):\n        super().__init__(args)\n        self.tgt_dict = tgt_dict\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task (e.g., load dictionaries).""""""\n        dict_path = os.path.join(args.data, ""dict.txt"")\n        if not os.path.isfile(dict_path):\n            raise FileNotFoundError(""Dict not found: {}"".format(dict_path))\n        tgt_dict = Dictionary.load(dict_path)\n\n        if args.criterion == ""ctc_loss"":\n            tgt_dict.add_symbol(""<ctc_blank>"")\n        elif args.criterion == ""asg_loss"":\n            for i in range(1, args.max_replabel + 1):\n                tgt_dict.add_symbol(replabel_symbol(i))\n\n        print(""| dictionary: {} types"".format(len(tgt_dict)))\n        return cls(args, tgt_dict)\n\n    def load_dataset(self, split, combine=False, **kwargs):\n        """"""Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        """"""\n        data_json_path = os.path.join(self.args.data, ""{}.json"".format(split))\n        self.datasets[split] = get_asr_dataset_from_json(data_json_path, self.tgt_dict)\n\n    def build_generator(self, models, args):\n        w2l_decoder = getattr(args, ""w2l_decoder"", None)\n        if w2l_decoder == ""viterbi"":\n            from examples.speech_recognition.w2l_decoder import W2lViterbiDecoder\n\n            return W2lViterbiDecoder(args, self.target_dictionary)\n        elif w2l_decoder == ""kenlm"":\n            from examples.speech_recognition.w2l_decoder import W2lKenLMDecoder\n\n            return W2lKenLMDecoder(args, self.target_dictionary)\n        else:\n            return super().build_generator(models, args)\n\n    @property\n    def target_dictionary(self):\n        """"""Return the :class:`~fairseq.data.Dictionary` for the language\n        model.""""""\n        return self.tgt_dict\n\n    @property\n    def source_dictionary(self):\n        """"""Return the source :class:`~fairseq.data.Dictionary` (if applicable\n        for this task).""""""\n        return None\n'"
examples/speech_recognition/utils/wer_utils.py,0,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport re\nfrom collections import deque\nfrom enum import Enum\n\nimport numpy as np\n\n\n""""""\n    Utility modules for computation of Word Error Rate,\n    Alignments, as well as more granular metrics like\n    deletion, insersion and substitutions.\n""""""\n\n\nclass Code(Enum):\n    match = 1\n    substitution = 2\n    insertion = 3\n    deletion = 4\n\n\nclass Token(object):\n    def __init__(self, lbl="""", st=np.nan, en=np.nan):\n        if np.isnan(st):\n            self.label, self.start, self.end = """", 0.0, 0.0\n        else:\n            self.label, self.start, self.end = lbl, st, en\n\n\nclass AlignmentResult(object):\n    def __init__(self, refs, hyps, codes, score):\n        self.refs = refs  # std::deque<int>\n        self.hyps = hyps  # std::deque<int>\n        self.codes = codes  # std::deque<Code>\n        self.score = score  # float\n\n\ndef coordinate_to_offset(row, col, ncols):\n    return int(row * ncols + col)\n\n\ndef offset_to_row(offset, ncols):\n    return int(offset / ncols)\n\n\ndef offset_to_col(offset, ncols):\n    return int(offset % ncols)\n\n\ndef trimWhitespace(str):\n    return re.sub("" +"", "" "", re.sub("" *$"", """", re.sub(""^ *"", """", str)))\n\n\ndef str2toks(str):\n    pieces = trimWhitespace(str).split("" "")\n    toks = []\n    for p in pieces:\n        toks.append(Token(p, 0.0, 0.0))\n    return toks\n\n\nclass EditDistance(object):\n    def __init__(self, time_mediated):\n        self.time_mediated_ = time_mediated\n        self.scores_ = np.nan  # Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic>\n        self.backtraces_ = (\n            np.nan\n        )  # Eigen::Matrix<size_t, Eigen::Dynamic, Eigen::Dynamic> backtraces_;\n        self.confusion_pairs_ = {}\n\n    def cost(self, ref, hyp, code):\n        if self.time_mediated_:\n            if code == Code.match:\n                return abs(ref.start - hyp.start) + abs(ref.end - hyp.end)\n            elif code == Code.insertion:\n                return hyp.end - hyp.start\n            elif code == Code.deletion:\n                return ref.end - ref.start\n            else:  # substitution\n                return abs(ref.start - hyp.start) + abs(ref.end - hyp.end) + 0.1\n        else:\n            if code == Code.match:\n                return 0\n            elif code == Code.insertion or code == Code.deletion:\n                return 3\n            else:  # substitution\n                return 4\n\n    def get_result(self, refs, hyps):\n        res = AlignmentResult(refs=deque(), hyps=deque(), codes=deque(), score=np.nan)\n\n        num_rows, num_cols = self.scores_.shape\n        res.score = self.scores_[num_rows - 1, num_cols - 1]\n\n        curr_offset = coordinate_to_offset(num_rows - 1, num_cols - 1, num_cols)\n\n        while curr_offset != 0:\n            curr_row = offset_to_row(curr_offset, num_cols)\n            curr_col = offset_to_col(curr_offset, num_cols)\n\n            prev_offset = self.backtraces_[curr_row, curr_col]\n\n            prev_row = offset_to_row(prev_offset, num_cols)\n            prev_col = offset_to_col(prev_offset, num_cols)\n\n            res.refs.appendleft(curr_row - 1)  # Note: this was .push_front() in C++\n            res.hyps.appendleft(curr_col - 1)\n            if curr_row - 1 == prev_row and curr_col == prev_col:\n                res.codes.appendleft(Code.deletion)\n            elif curr_row == prev_row and curr_col - 1 == prev_col:\n                res.codes.appendleft(Code.insertion)\n            else:\n                # assert(curr_row - 1 == prev_row and curr_col - 1 == prev_col)\n                ref_str = refs[res.refs[0]].label\n                hyp_str = hyps[res.hyps[0]].label\n\n                if ref_str == hyp_str:\n                    res.codes.appendleft(Code.match)\n                else:\n                    res.codes.appendleft(Code.substitution)\n\n                    confusion_pair = ""%s -> %s"" % (ref_str, hyp_str)\n                    if confusion_pair not in self.confusion_pairs_:\n                        self.confusion_pairs_[confusion_pair] = 1\n                    else:\n                        self.confusion_pairs_[confusion_pair] += 1\n\n            curr_offset = prev_offset\n\n        return res\n\n    def align(self, refs, hyps):\n        if len(refs) == 0 and len(hyps) == 0:\n            return np.nan\n\n        # NOTE: we\'re not resetting the values in these matrices because every value\n        # will be overridden in the loop below. If this assumption doesn\'t hold,\n        # be sure to set all entries in self.scores_ and self.backtraces_ to 0.\n        self.scores_ = np.zeros((len(refs) + 1, len(hyps) + 1))\n        self.backtraces_ = np.zeros((len(refs) + 1, len(hyps) + 1))\n\n        num_rows, num_cols = self.scores_.shape\n\n        for i in range(num_rows):\n            for j in range(num_cols):\n                if i == 0 and j == 0:\n                    self.scores_[i, j] = 0.0\n                    self.backtraces_[i, j] = 0\n                    continue\n\n                if i == 0:\n                    self.scores_[i, j] = self.scores_[i, j - 1] + self.cost(\n                        None, hyps[j - 1], Code.insertion\n                    )\n                    self.backtraces_[i, j] = coordinate_to_offset(i, j - 1, num_cols)\n                    continue\n\n                if j == 0:\n                    self.scores_[i, j] = self.scores_[i - 1, j] + self.cost(\n                        refs[i - 1], None, Code.deletion\n                    )\n                    self.backtraces_[i, j] = coordinate_to_offset(i - 1, j, num_cols)\n                    continue\n\n                # Below here both i and j are greater than 0\n                ref = refs[i - 1]\n                hyp = hyps[j - 1]\n                best_score = self.scores_[i - 1, j - 1] + (\n                    self.cost(ref, hyp, Code.match)\n                    if (ref.label == hyp.label)\n                    else self.cost(ref, hyp, Code.substitution)\n                )\n\n                prev_row = i - 1\n                prev_col = j - 1\n                ins = self.scores_[i, j - 1] + self.cost(None, hyp, Code.insertion)\n                if ins < best_score:\n                    best_score = ins\n                    prev_row = i\n                    prev_col = j - 1\n\n                delt = self.scores_[i - 1, j] + self.cost(ref, None, Code.deletion)\n                if delt < best_score:\n                    best_score = delt\n                    prev_row = i - 1\n                    prev_col = j\n\n                self.scores_[i, j] = best_score\n                self.backtraces_[i, j] = coordinate_to_offset(\n                    prev_row, prev_col, num_cols\n                )\n\n        return self.get_result(refs, hyps)\n\n\nclass WERTransformer(object):\n    def __init__(self, hyp_str, ref_str, verbose=True):\n        self.ed_ = EditDistance(False)\n        self.id2oracle_errs_ = {}\n        self.utts_ = 0\n        self.words_ = 0\n        self.insertions_ = 0\n        self.deletions_ = 0\n        self.substitutions_ = 0\n\n        self.process([""dummy_str"", hyp_str, ref_str])\n\n        if verbose:\n            print(""\'%s\' vs \'%s\'"" % (hyp_str, ref_str))\n            self.report_result()\n\n    def process(self, input):  # std::vector<std::string>&& input\n        if len(input) < 3:\n            print(\n                ""Input must be of the form <id> ... <hypo> <ref> , got "",\n                len(input),\n                "" inputs:"",\n            )\n            return None\n\n        # Align\n        # std::vector<Token> hyps;\n        # std::vector<Token> refs;\n\n        hyps = str2toks(input[-2])\n        refs = str2toks(input[-1])\n\n        alignment = self.ed_.align(refs, hyps)\n        if alignment is None:\n            print(""Alignment is null"")\n            return np.nan\n\n        # Tally errors\n        ins = 0\n        dels = 0\n        subs = 0\n        for code in alignment.codes:\n            if code == Code.substitution:\n                subs += 1\n            elif code == Code.insertion:\n                ins += 1\n            elif code == Code.deletion:\n                dels += 1\n\n        # Output\n        row = input\n        row.append(str(len(refs)))\n        row.append(str(ins))\n        row.append(str(dels))\n        row.append(str(subs))\n        # print(row)\n\n        # Accumulate\n        kIdIndex = 0\n        kNBestSep = ""/""\n\n        pieces = input[kIdIndex].split(kNBestSep)\n\n        if len(pieces) == 0:\n            print(\n                ""Error splitting "",\n                input[kIdIndex],\n                "" on \'"",\n                kNBestSep,\n                ""\', got empty list"",\n            )\n            return np.nan\n\n        id = pieces[0]\n        if id not in self.id2oracle_errs_:\n            self.utts_ += 1\n            self.words_ += len(refs)\n            self.insertions_ += ins\n            self.deletions_ += dels\n            self.substitutions_ += subs\n            self.id2oracle_errs_[id] = [ins, dels, subs]\n        else:\n            curr_err = ins + dels + subs\n            prev_err = np.sum(self.id2oracle_errs_[id])\n            if curr_err < prev_err:\n                self.id2oracle_errs_[id] = [ins, dels, subs]\n\n        return 0\n\n    def report_result(self):\n        # print(""----------  Summary ---------------"")\n        if self.words_ == 0:\n            print(""No words counted"")\n            return\n\n        # 1-best\n        best_wer = (\n            100.0\n            * (self.insertions_ + self.deletions_ + self.substitutions_)\n            / self.words_\n        )\n\n        print(\n            ""\\tWER = %0.2f%% (%i utts, %i words, %0.2f%% ins, ""\n            ""%0.2f%% dels, %0.2f%% subs)""\n            % (\n                best_wer,\n                self.utts_,\n                self.words_,\n                100.0 * self.insertions_ / self.words_,\n                100.0 * self.deletions_ / self.words_,\n                100.0 * self.substitutions_ / self.words_,\n            )\n        )\n\n    def wer(self):\n        if self.words_ == 0:\n            wer = np.nan\n        else:\n            wer = (\n                100.0\n                * (self.insertions_ + self.deletions_ + self.substitutions_)\n                / self.words_\n            )\n        return wer\n\n    def stats(self):\n        if self.words_ == 0:\n            stats = {}\n        else:\n            wer = (\n                100.0\n                * (self.insertions_ + self.deletions_ + self.substitutions_)\n                / self.words_\n            )\n            stats = dict(\n                {\n                    ""wer"": wer,\n                    ""utts"": self.utts_,\n                    ""numwords"": self.words_,\n                    ""ins"": self.insertions_,\n                    ""dels"": self.deletions_,\n                    ""subs"": self.substitutions_,\n                    ""confusion_pairs"": self.ed_.confusion_pairs_,\n                }\n            )\n        return stats\n\n\ndef calc_wer(hyp_str, ref_str):\n    t = WERTransformer(hyp_str, ref_str, verbose=0)\n    return t.wer()\n\n\ndef calc_wer_stats(hyp_str, ref_str):\n    t = WERTransformer(hyp_str, ref_str, verbose=0)\n    return t.stats()\n\n\ndef get_wer_alignment_codes(hyp_str, ref_str):\n    """"""\n    INPUT: hypothesis string, reference string\n    OUTPUT: List of alignment codes (intermediate results from WER computation)\n    """"""\n    t = WERTransformer(hyp_str, ref_str, verbose=0)\n    return t.ed_.align(str2toks(ref_str), str2toks(hyp_str)).codes\n\n\ndef merge_counts(x, y):\n    # Merge two hashes which have \'counts\' as their values\n    # This can be used for example to merge confusion pair counts\n    #   conf_pairs = merge_counts(conf_pairs, stats[\'confusion_pairs\'])\n    for k, v in y.items():\n        if k not in x:\n            x[k] = 0\n        x[k] += v\n    return x\n'"
examples/translation_moe/src/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import translation_moe  # noqa\n'"
examples/translation_moe/src/logsumexp_moe.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\n\nclass LogSumExpMoE(torch.autograd.Function):\n    """"""Standard LogSumExp forward pass, but use *posterior* for the backward.\n\n    See `""Mixture Models for Diverse Machine Translation: Tricks of the Trade""\n    (Shen et al., 2019) <https://arxiv.org/abs/1902.07816>`_.\n    """"""\n\n    @staticmethod\n    def forward(ctx, logp, posterior, dim=-1):\n        ctx.save_for_backward(posterior)\n        ctx.dim = dim\n        return torch.logsumexp(logp, dim=dim)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        posterior, = ctx.saved_tensors\n        grad_logp = grad_output.unsqueeze(ctx.dim) * posterior\n        return grad_logp, None, None\n'"
examples/translation_moe/src/mean_pool_gating_network.py,10,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn.functional as F\n\n\nclass MeanPoolGatingNetwork(torch.nn.Module):\n    """"""A simple mean-pooling gating network for selecting experts.\n\n    This module applies mean pooling over an encoder\'s output and returns\n    reponsibilities for each expert. The encoder format is expected to match\n    :class:`fairseq.models.transformer.TransformerEncoder`.\n    """"""\n\n    def __init__(self, embed_dim, num_experts, dropout=None):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_experts = num_experts\n\n        self.fc1 = torch.nn.Linear(embed_dim, embed_dim)\n        self.dropout = torch.nn.Dropout(dropout) if dropout is not None else None\n        self.fc2 = torch.nn.Linear(embed_dim, num_experts)\n\n    def forward(self, encoder_out):\n        if not (\n            hasattr(encoder_out, \'encoder_out\')\n            and hasattr(encoder_out, \'encoder_padding_mask\')\n            and encoder_out.encoder_out.size(2) == self.embed_dim\n        ):\n            raise ValueError(\'Unexpected format for encoder_out\')\n\n        # mean pooling over time\n        encoder_padding_mask = encoder_out.encoder_padding_mask  # B x T\n        encoder_out = encoder_out.encoder_out.transpose(0, 1)    # B x T x C\n        if encoder_padding_mask is not None:\n            encoder_out = encoder_out.clone()  # required because of transpose above\n            encoder_out[encoder_padding_mask] = 0\n            ntokens = torch.sum(~encoder_padding_mask, dim=1, keepdim=True)\n            x = torch.sum(encoder_out, dim=1) / ntokens.type_as(encoder_out)\n        else:\n            x = torch.mean(encoder_out, dim=1)\n\n        x = torch.tanh(self.fc1(x))\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=-1, dtype=torch.float32).type_as(x)\n'"
examples/translation_moe/src/translation_moe.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom fairseq import metrics, utils\nfrom fairseq.tasks import register_task\nfrom fairseq.tasks.translation import TranslationTask\n\nfrom .logsumexp_moe import LogSumExpMoE\nfrom .mean_pool_gating_network import MeanPoolGatingNetwork\n\n\n@register_task(\'translation_moe\')\nclass TranslationMoETask(TranslationTask):\n    """"""\n    Translation task for Mixture of Experts (MoE) models.\n\n    See `""Mixture Models for Diverse Machine Translation: Tricks of the Trade""\n    (Shen et al., 2019) <https://arxiv.org/abs/1902.07816>`_.\n\n    Args:\n        src_dict (~fairseq.data.Dictionary): dictionary for the source language\n        tgt_dict (~fairseq.data.Dictionary): dictionary for the target language\n\n    .. note::\n\n        The translation task is compatible with :mod:`fairseq-train`,\n        :mod:`fairseq-generate` and :mod:`fairseq-interactive`.\n\n    The translation task provides the following additional command-line\n    arguments:\n\n    .. argparse::\n        :ref: fairseq.tasks.translation_parser\n        :prog:\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        # fmt: off\n        TranslationTask.add_args(parser)\n        parser.add_argument(\'--method\', default=\'hMoEup\',\n                            choices=[\'sMoElp\', \'sMoEup\', \'hMoElp\', \'hMoEup\'])\n        parser.add_argument(\'--num-experts\', default=3, type=int, metavar=\'N\',\n                            help=\'number of experts\')\n        parser.add_argument(\'--mean-pool-gating-network\', action=\'store_true\',\n                            help=\'use a simple mean-pooling gating network\')\n        parser.add_argument(\'--mean-pool-gating-network-dropout\', type=float,\n                            help=\'dropout for mean-pooling gating network\')\n        parser.add_argument(\'--mean-pool-gating-network-encoder-dim\', type=float,\n                            help=\'encoder output dim for mean-pooling gating network\')\n        parser.add_argument(\'--gen-expert\', type=int, default=0,\n                            help=\'which expert to use for generation\')\n        # fmt: on\n\n    def __init__(self, args, src_dict, tgt_dict):\n        if args.method == \'sMoElp\':\n            # soft MoE with learned prior\n            self.uniform_prior = False\n            self.hard_selection = False\n        elif args.method == \'sMoEup\':\n            # soft MoE with uniform prior\n            self.uniform_prior = True\n            self.hard_selection = False\n        elif args.method == \'hMoElp\':\n            # hard MoE with learned prior\n            self.uniform_prior = False\n            self.hard_selection = True\n        elif args.method == \'hMoEup\':\n            # hard MoE with uniform prior\n            self.uniform_prior = True\n            self.hard_selection = True\n\n        # add indicator tokens for each expert\n        for i in range(args.num_experts):\n            # add to both dictionaries in case we\'re sharing embeddings\n            src_dict.add_symbol(\'<expert_{}>\'.format(i))\n            tgt_dict.add_symbol(\'<expert_{}>\'.format(i))\n\n        super().__init__(args, src_dict, tgt_dict)\n\n    def build_model(self, args):\n        from fairseq import models\n        model = models.build_model(args, self)\n        if not self.uniform_prior and not hasattr(model, \'gating_network\'):\n            if self.args.mean_pool_gating_network:\n                if getattr(args, \'mean_pool_gating_network_encoder_dim\', None):\n                    encoder_dim = args.mean_pool_gating_network_encoder_dim\n                elif getattr(args, \'encoder_embed_dim\', None):\n                    # assume that encoder_embed_dim is the encoder\'s output dimension\n                    encoder_dim = args.encoder_embed_dim\n                else:\n                    raise ValueError(\'Must specify --mean-pool-gating-network-encoder-dim\')\n\n                if getattr(args, \'mean_pool_gating_network_dropout\', None):\n                    dropout = args.mean_pool_gating_network_dropout\n                elif getattr(args, \'dropout\', None):\n                    dropout = args.dropout\n                else:\n                    raise ValueError(\'Must specify --mean-pool-gating-network-dropout\')\n\n                model.gating_network = MeanPoolGatingNetwork(\n                    encoder_dim, args.num_experts, dropout,\n                )\n            else:\n                raise ValueError(\n                    \'translation_moe task with learned prior requires the model to \'\n                    \'have a gating network; try using --mean-pool-gating-network\'\n                )\n        return model\n\n    def expert_index(self, i):\n        return i + self.tgt_dict.index(\'<expert_0>\')\n\n    def _get_loss(self, sample, model, criterion):\n        assert hasattr(criterion, \'compute_loss\'), \\\n            \'translation_moe task requires the criterion to implement the compute_loss() method\'\n\n        k = self.args.num_experts\n        bsz = sample[\'target\'].size(0)\n\n        def get_lprob_y(encoder_out, prev_output_tokens_k):\n            net_output = model.decoder(\n                prev_output_tokens=prev_output_tokens_k,\n                encoder_out=encoder_out,\n            )\n            loss, _ = criterion.compute_loss(model, net_output, sample, reduce=False)\n            loss = loss.view(bsz, -1)\n            return -loss.sum(dim=1, keepdim=True)  # -> B x 1\n\n        def get_lprob_yz(winners=None):\n            encoder_out = model.encoder(\n                src_tokens=sample[\'net_input\'][\'src_tokens\'],\n                src_lengths=sample[\'net_input\'][\'src_lengths\'],\n            )\n\n            if winners is None:\n                lprob_y = []\n                for i in range(k):\n                    prev_output_tokens_k = sample[\'net_input\'][\'prev_output_tokens\'].clone()\n                    assert not prev_output_tokens_k.requires_grad\n                    prev_output_tokens_k[:, 0] = self.expert_index(i)\n                    lprob_y.append(get_lprob_y(encoder_out, prev_output_tokens_k))\n                lprob_y = torch.cat(lprob_y, dim=1)  # -> B x K\n            else:\n                prev_output_tokens_k = sample[\'net_input\'][\'prev_output_tokens\'].clone()\n                prev_output_tokens_k[:, 0] = self.expert_index(winners)\n                lprob_y = get_lprob_y(encoder_out, prev_output_tokens_k)  # -> B\n\n            if self.uniform_prior:\n                lprob_yz = lprob_y\n            else:\n                lprob_z = model.gating_network(encoder_out)  # B x K\n                if winners is not None:\n                    lprob_z = lprob_z.gather(dim=1, index=winners.unsqueeze(-1))\n                lprob_yz = lprob_y + lprob_z.type_as(lprob_y)  # B x K\n\n            return lprob_yz\n\n        # compute responsibilities without dropout\n        with utils.eval(model):  # disable dropout\n            with torch.no_grad():  # disable autograd\n                lprob_yz = get_lprob_yz()  # B x K\n                prob_z_xy = torch.nn.functional.softmax(lprob_yz, dim=1)\n        assert not prob_z_xy.requires_grad\n\n        # compute loss with dropout\n        if self.hard_selection:\n            winners = prob_z_xy.max(dim=1)[1]\n            loss = -get_lprob_yz(winners)\n        else:\n            lprob_yz = get_lprob_yz()  # B x K\n            loss = -LogSumExpMoE.apply(lprob_yz, prob_z_xy, 1)\n\n        loss = loss.sum()\n        sample_size = sample[\'target\'].size(0) if self.args.sentence_avg else sample[\'ntokens\']\n        logging_output = {\n            \'loss\': utils.item(loss.data),\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': bsz,\n            \'sample_size\': sample_size,\n            \'posterior\': prob_z_xy.float().sum(dim=0).cpu(),\n        }\n        return loss, sample_size, logging_output\n\n    def train_step(self, sample, model, criterion, optimizer, update_num, ignore_grad=False):\n        model.train()\n        loss, sample_size, logging_output = self._get_loss(sample, model, criterion)\n        if ignore_grad:\n            loss *= 0\n        optimizer.backward(loss)\n        return loss, sample_size, logging_output\n\n    def valid_step(self, sample, model, criterion):\n        model.eval()\n        with torch.no_grad():\n            loss, sample_size, logging_output = self._get_loss(sample, model, criterion)\n        return loss, sample_size, logging_output\n\n    def inference_step(self, generator, models, sample, prefix_tokens=None, expert=None):\n        expert = expert or self.args.gen_expert\n        with torch.no_grad():\n            return generator.generate(\n                models,\n                sample,\n                prefix_tokens=prefix_tokens,\n                bos_token=self.expert_index(expert),\n            )\n\n    def reduce_metrics(self, logging_outputs, criterion):\n        super().reduce_metrics(logging_outputs, criterion)\n        metrics.log_scalar(\n            \'posterior\',\n            sum(log[\'posterior\'] for log in logging_outputs if \'posterior\' in log)\n        )\n'"
fairseq/data/audio/__init__.py,0,b''
fairseq/data/audio/raw_audio_dataset.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport os\nimport numpy as np\nimport sys\n\nimport torch\nimport torch.nn.functional as F\n\nfrom .. import FairseqDataset\n\n\nclass RawAudioDataset(FairseqDataset):\n    def __init__(\n        self,\n        sample_rate,\n        max_sample_size=None,\n        min_sample_size=None,\n        shuffle=True,\n        min_length=0,\n    ):\n        super().__init__()\n\n        self.sample_rate = sample_rate\n        self.sizes = []\n        self.max_sample_size = (\n            max_sample_size if max_sample_size is not None else sys.maxsize\n        )\n        self.min_sample_size = (\n            min_sample_size if min_sample_size is not None else self.max_sample_size\n        )\n        self.min_length = min_length\n        self.shuffle = shuffle\n\n    def __getitem__(self, index):\n        raise NotImplementedError()\n\n    def __len__(self):\n        return len(self.sizes)\n\n    def postprocess(self, feats, curr_sample_rate):\n        def resample(x, factor):\n            return F.interpolate(x.view(1, 1, -1), scale_factor=factor).squeeze()\n\n        if feats.dim() == 2:\n            feats = feats.mean(-1)\n\n        if curr_sample_rate != self.sample_rate:\n            factor = self.sample_rate / curr_sample_rate\n            feats = resample(feats, factor)\n\n        assert feats.dim() == 1, feats.dim()\n        return feats\n\n    def crop_to_max_size(self, wav, target_size):\n        size = len(wav)\n        diff = size - target_size\n        if diff <= 0:\n            return wav\n\n        start = np.random.randint(0, diff + 1)\n        end = size - diff + start\n        return wav[start:end]\n\n    def collater(self, samples):\n        samples = [\n            s for s in samples if s[""source""] is not None and len(s[""source""]) > 0\n        ]\n        if len(samples) == 0:\n            return {}\n\n        sources = [s[""source""] for s in samples]\n        sizes = [len(s) for s in sources]\n        target_size = min(min(sizes), self.max_sample_size)\n\n        if target_size < self.min_length:\n            return {}\n\n        if self.min_sample_size < target_size:\n            target_size = np.random.randint(self.min_sample_size, target_size + 1)\n\n        collated_sources = sources[0].new(len(sources), target_size)\n        for i, (source, size) in enumerate(zip(sources, sizes)):\n            diff = size - target_size\n            assert diff >= 0\n            if diff == 0:\n                collated_sources[i] = source\n            else:\n                collated_sources[i] = self.crop_to_max_size(source, target_size)\n\n        return {\n            ""id"": torch.LongTensor([s[""id""] for s in samples]),\n            ""net_input"": {""source"": collated_sources},\n        }\n\n    def num_tokens(self, index):\n        return self.size(index)\n\n    def size(self, index):\n        """"""Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.""""""\n        return min(self.sizes[index], self.max_sample_size)\n\n    def ordered_indices(self):\n        """"""Return an ordered list of indices. Batches will be constructed based\n        on this order.""""""\n\n        if self.shuffle:\n            order = [np.random.permutation(len(self))]\n        else:\n            order = [np.arange(len(self))]\n\n        order.append(self.sizes)\n        return np.lexsort(order)\n\n\nclass FileAudioDataset(RawAudioDataset):\n    def __init__(\n        self,\n        manifest_path,\n        sample_rate,\n        max_sample_size=None,\n        min_sample_size=None,\n        shuffle=True,\n        min_length=0,\n    ):\n        super().__init__(\n            sample_rate=sample_rate,\n            max_sample_size=max_sample_size,\n            min_sample_size=min_sample_size,\n            shuffle=shuffle,\n            min_length=min_length,\n        )\n\n        self.fnames = []\n\n        with open(manifest_path, ""r"") as f:\n            self.root_dir = f.readline().strip()\n            for line in f:\n                items = line.strip().split(""\\t"")\n                assert len(items) == 2, line\n                self.fnames.append(items[0])\n                self.sizes.append(int(items[1]))\n\n    def __getitem__(self, index):\n        import soundfile as sf\n\n        fname = os.path.join(self.root_dir, self.fnames[index])\n        wav, curr_sample_rate = sf.read(fname)\n        feats = torch.from_numpy(wav).float()\n        feats = self.postprocess(feats, curr_sample_rate)\n        return {""id"": index, ""source"": feats}\n'"
fairseq/data/encoders/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport importlib\nimport os\n\nfrom fairseq import registry\n\n\nbuild_tokenizer, register_tokenizer, TOKENIZER_REGISTRY = registry.setup_registry(\n    '--tokenizer',\n    default=None,\n)\n\n\nbuild_bpe, register_bpe, BPE_REGISTRY = registry.setup_registry(\n    '--bpe',\n    default=None,\n)\n\n\n# automatically import any Python files in the encoders/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        module = file[:file.find('.py')]\n        importlib.import_module('fairseq.data.encoders.' + module)\n"""
fairseq/data/encoders/byte_bpe.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom fairseq import file_utils\nfrom fairseq.data.encoders import register_bpe\nfrom fairseq.data.encoders.byte_utils import (byte_encode, smart_byte_decode,\n                                              SPACE, SPACE_ESCAPE)\n\n\n@register_bpe('byte_bpe')\nclass ByteBPE(object):\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument('--sentencepiece-model-path', type=str,\n                            help='path to sentencepiece model')\n        # fmt: on\n\n    def __init__(self, args):\n        vocab = file_utils.cached_path(args.sentencepiece_model_path)\n        try:\n            import sentencepiece as spm\n            self.sp = spm.SentencePieceProcessor()\n            self.sp.Load(vocab)\n        except ImportError:\n            raise ImportError('Please install sentencepiece with: pip install sentencepiece')\n\n    def encode(self, x: str) -> str:\n        byte_encoded = byte_encode(x)\n        return SPACE.join(self.sp.EncodeAsPieces(byte_encoded))\n\n    @staticmethod\n    def decode(x: str) -> str:\n        unescaped = x.replace(SPACE, '').replace(SPACE_ESCAPE, SPACE)\n        return smart_byte_decode(unescaped)\n"""
fairseq/data/encoders/byte_utils.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport re\n\nWHITESPACE_NORMALIZER = re.compile(r'\\s+')\nSPACE = chr(32)\nSPACE_ESCAPE = chr(9601)\n# excluding non-breaking space (160) here\nPRINTABLE_LATIN = set(\n    list(range(32, 126 + 1)) + list(range(161, 172 + 1)) +\n    list(range(174, 255 + 1))\n)\nBYTE_TO_BCHAR = {\n    b: chr(b) if b in PRINTABLE_LATIN else chr(256 + b) for b in range(256)\n}\nBCHAR_TO_BYTE = {bc: b for b, bc in BYTE_TO_BCHAR.items()}\n\n\ndef byte_encode(x: str) -> str:\n    normalized = WHITESPACE_NORMALIZER.sub(SPACE, x)\n    return ''.join([BYTE_TO_BCHAR[b] for b in normalized.encode('utf-8')])\n\n\ndef byte_decode(x: str) -> str:\n    try:\n        return bytes([BCHAR_TO_BYTE[bc] for bc in x]).decode('utf-8')\n    except ValueError:\n        return ''\n\n\ndef smart_byte_decode(x: str) -> str:\n    output = byte_decode(x)\n    if output == '':\n        # DP the best recovery (max valid chars) if it's broken\n        n_bytes = len(x)\n        f = [0 for _ in range(n_bytes + 1)]\n        pt = [0 for _ in range(n_bytes + 1)]\n        for i in range(1, n_bytes + 1):\n            f[i], pt[i] = f[i - 1], i - 1\n            for j in range(1, min(4, i) + 1):\n                if f[i - j] + 1 > f[i] and len(byte_decode(x[i - j: i])) > 0:\n                    f[i], pt[i] = f[i - j] + 1, i - j\n        cur_pt = n_bytes\n        while cur_pt > 0:\n            if f[cur_pt] == f[pt[cur_pt]] + 1:\n                output = byte_decode(x[pt[cur_pt]: cur_pt]) + output\n            cur_pt = pt[cur_pt]\n    return output\n"""
fairseq/data/encoders/bytes.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom fairseq.data.encoders import register_bpe\nfrom fairseq.data.encoders.byte_utils import (byte_encode, smart_byte_decode,\n                                              SPACE, SPACE_ESCAPE)\n\n\n@register_bpe('bytes')\nclass Bytes(object):\n    def __init__(self, args):\n        pass\n\n    @staticmethod\n    def add_args(parser):\n        pass\n\n    @staticmethod\n    def encode(x: str) -> str:\n        encoded = byte_encode(x)\n        escaped = encoded.replace(SPACE, SPACE_ESCAPE)\n        return SPACE.join(list(escaped))\n\n    @staticmethod\n    def decode(x: str) -> str:\n        unescaped = x.replace(SPACE, '').replace(SPACE_ESCAPE, SPACE)\n        return smart_byte_decode(unescaped)\n"""
fairseq/data/encoders/characters.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom fairseq.data.encoders import register_bpe\n\nSPACE = chr(32)\nSPACE_ESCAPE = chr(9601)\n\n\n@register_bpe('characters')\nclass Characters(object):\n    def __init__(self, args):\n        pass\n\n    @staticmethod\n    def add_args(parser):\n        pass\n\n    @staticmethod\n    def encode(x: str) -> str:\n        escaped = x.replace(SPACE, SPACE_ESCAPE)\n        return SPACE.join(list(escaped))\n\n    @staticmethod\n    def decode(x: str) -> str:\n        return x.replace(SPACE, '').replace(SPACE_ESCAPE, SPACE)\n"""
fairseq/data/encoders/fastbpe.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import file_utils\nfrom fairseq.data.encoders import register_bpe\n\n\n@register_bpe(\'fastbpe\')\nclass fastBPE(object):\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument(\'--bpe-codes\', type=str,\n                            help=\'path to fastBPE BPE\')\n        # fmt: on\n\n    def __init__(self, args):\n        if args.bpe_codes is None:\n            raise ValueError(\'--bpe-codes is required for --bpe=fastbpe\')\n        codes = file_utils.cached_path(args.bpe_codes)\n        try:\n            import fastBPE\n            self.bpe = fastBPE.fastBPE(codes)\n            self.bpe_symbol = ""@@ ""\n        except ImportError:\n            raise ImportError(\'Please install fastBPE with: pip install fastBPE\')\n\n    def encode(self, x: str) -> str:\n        return self.bpe.apply([x])[0]\n\n    def decode(self, x: str) -> str:\n        return (x + \' \').replace(self.bpe_symbol, \'\').rstrip()\n'"
fairseq/data/encoders/gpt2_bpe.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import file_utils\nfrom fairseq.data.encoders import register_bpe\n\nfrom .gpt2_bpe_utils import get_encoder\n\n\nDEFAULT_ENCODER_JSON = 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json'\nDEFAULT_VOCAB_BPE = 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'\n\n\n@register_bpe('gpt2')\nclass GPT2BPE(object):\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument('--gpt2-encoder-json', type=str,\n                            default=DEFAULT_ENCODER_JSON,\n                            help='path to encoder.json')\n        parser.add_argument('--gpt2-vocab-bpe', type=str,\n                            default=DEFAULT_VOCAB_BPE,\n                            help='path to vocab.bpe')\n        # fmt: on\n\n    def __init__(self, args):\n        encoder_json = file_utils.cached_path(\n            getattr(args, 'gpt2_encoder_json', DEFAULT_ENCODER_JSON)\n        )\n        vocab_bpe = file_utils.cached_path(\n            getattr(args, 'gpt2_vocab_bpe', DEFAULT_VOCAB_BPE)\n        )\n        self.bpe = get_encoder(encoder_json, vocab_bpe)\n\n    def encode(self, x: str) -> str:\n        return ' '.join(map(str, self.bpe.encode(x)))\n\n    def decode(self, x: str) -> str:\n        return self.bpe.decode([\n            int(tok) if tok not in {'<unk>', '<mask>'} else tok\n            for tok in x.split()\n        ])\n\n    def is_beginning_of_word(self, x: str) -> bool:\n        return self.decode(x).startswith(' ')\n"""
fairseq/data/encoders/gpt2_bpe_utils.py,0,"b'""""""\nByte pair encoding utilities from GPT-2.\n\nOriginal source: https://github.com/openai/gpt-2/blob/master/src/encoder.py\nOriginal license: MIT\n""""""\n\nfrom functools import lru_cache\nimport json\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    """"""\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n\n    def __init__(self, encoder, bpe_merges, errors=\'replace\'):\n        self.encoder = encoder\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        try:\n            import regex as re\n            self.re = re\n        except ImportError:\n            raise ImportError(\'Please install regex with: pip install regex\')\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = self.re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in self.re.findall(self.pat, text):\n            token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \'\'.join([self.decoder.get(token, token) for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\ndef get_encoder(encoder_json_path, vocab_bpe_path):\n    with open(encoder_json_path, \'r\') as f:\n        encoder = json.load(f)\n    with open(vocab_bpe_path, \'r\', encoding=""utf-8"") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\'\\n\')[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n'"
fairseq/data/encoders/hf_bert_bpe.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.data.encoders import register_bpe\n\n\n@register_bpe('bert')\nclass BertBPE(object):\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument('--bpe-cased', action='store_true',\n                            help='set for cased BPE',\n                            default=False)\n        parser.add_argument('--bpe-vocab-file', type=str,\n                            help='bpe vocab file.')\n        # fmt: on\n\n    def __init__(self, args):\n        try:\n            from pytorch_transformers import BertTokenizer\n            from pytorch_transformers.tokenization_utils import clean_up_tokenization\n        except ImportError:\n            raise ImportError(\n                'Please install 1.0.0 version of pytorch_transformers'\n                'with: pip install pytorch-transformers'\n            )\n\n        if 'bpe_vocab_file' in args:\n            self.bert_tokenizer = BertTokenizer(\n                args.bpe_vocab_file,\n                do_lower_case=not args.bpe_cased\n            )\n        else:\n            vocab_file_name = 'bert-base-cased' if args.bpe_cased else 'bert-base-uncased'\n            self.bert_tokenizer = BertTokenizer.from_pretrained(vocab_file_name)\n            self.clean_up_tokenization = clean_up_tokenization\n\n    def encode(self, x: str) -> str:\n        return ' '.join(self.bert_tokenizer.tokenize(x))\n\n    def decode(self, x: str) -> str:\n        return self.clean_up_tokenization(\n            self.bert_tokenizer.convert_tokens_to_string(x.split(' '))\n        )\n\n    def is_beginning_of_word(self, x: str) -> bool:\n        return not x.startswith('##')\n"""
fairseq/data/encoders/hf_byte_bpe.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.data.encoders import register_bpe\n\n\n@register_bpe('hf_byte_bpe')\nclass HuggingFaceByteLevelBPE(object):\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument('--bpe-merges', help='path to merges.txt')\n        parser.add_argument('--bpe-vocab', help='path to vocab.json')\n        parser.add_argument('--bpe-add-prefix-space', action='store_true',\n                            help='add prefix space before encoding')\n        # fmt: on\n\n    def __init__(self, args):\n        try:\n            from tokenizers import ByteLevelBPETokenizer\n        except ImportError:\n            raise ImportError(\n                'Please install huggingface/tokenizers with: '\n                'pip install tokenizers'\n            )\n\n        self.bpe = ByteLevelBPETokenizer(\n            args.bpe_vocab,\n            args.bpe_merges,\n            add_prefix_space=getattr(args, 'bpe_add_prefix_space', False),\n        )\n\n    def encode(self, x: str) -> str:\n        return ' '.join(map(str, self.bpe.encode(x).ids))\n\n    def decode(self, x: str) -> str:\n        return self.bpe.decode([\n            int(tok) if tok not in {'<unk>', '<mask>'} else tok\n            for tok in x.split()\n        ])\n\n    def is_beginning_of_word(self, x: str) -> bool:\n        return self.decode(x).startswith(' ')\n"""
fairseq/data/encoders/moses_tokenizer.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.data.encoders import register_tokenizer\n\n\n@register_tokenizer('moses')\nclass MosesTokenizer(object):\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument('--moses-source-lang', metavar='SRC',\n                            help='source language')\n        parser.add_argument('--moses-target-lang', metavar='TARGET',\n                            help='target language')\n        parser.add_argument('--moses-no-dash-splits', action='store_true', default=False,\n                            help='don\\'t apply dash split rules')\n        parser.add_argument('--moses-no-escape', action='store_true', default=False,\n                            help='don\\'t perform HTML escaping on apostrophy, quotes, etc.')\n        # fmt: on\n\n    def __init__(self, args):\n        self.args = args\n\n        if getattr(args, 'moses_source_lang', None) is None:\n            args.moses_source_lang = getattr(args, 'source_lang', 'en')\n        if getattr(args, 'moses_target_lang', None) is None:\n            args.moses_target_lang = getattr(args, 'target_lang', 'en')\n\n        try:\n            from sacremoses import MosesTokenizer, MosesDetokenizer\n            self.tok = MosesTokenizer(args.moses_source_lang)\n            self.detok = MosesDetokenizer(args.moses_target_lang)\n        except ImportError:\n            raise ImportError('Please install Moses tokenizer with: pip install sacremoses')\n\n    def encode(self, x: str) -> str:\n        return self.tok.tokenize(\n            x,\n            aggressive_dash_splits=(not self.args.moses_no_dash_splits),\n            return_str=True,\n            escape=(not self.args.moses_no_escape),\n        )\n\n    def decode(self, x: str) -> str:\n        return self.detok.detokenize(x.split())\n"""
fairseq/data/encoders/nltk_tokenizer.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.data.encoders import register_tokenizer\n\n\n@register_tokenizer('nltk')\nclass NLTKTokenizer(object):\n\n    def __init__(self, source_lang=None, target_lang=None):\n        try:\n            from nltk.tokenize import word_tokenize\n            self.word_tokenize = word_tokenize\n        except ImportError:\n            raise ImportError('Please install nltk with: pip install nltk')\n\n    def encode(self, x: str) -> str:\n        return ' '.join(self.word_tokenize(x))\n\n    def decode(self, x: str) -> str:\n        return x\n"""
fairseq/data/encoders/sentencepiece_bpe.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import file_utils\nfrom fairseq.data.encoders import register_bpe\n\n\n@register_bpe('sentencepiece')\nclass SentencepieceBPE(object):\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument('--sentencepiece-vocab', type=str,\n                            help='path to sentencepiece vocab')\n        # fmt: on\n\n    def __init__(self, args):\n        vocab = file_utils.cached_path(args.sentencepiece_vocab)\n        try:\n            import sentencepiece as spm\n            self.sp = spm.SentencePieceProcessor()\n            self.sp.Load(vocab)\n        except ImportError:\n            raise ImportError('Please install sentencepiece with: pip install sentencepiece')\n\n    def encode(self, x: str) -> str:\n        return ' '.join(self.sp.EncodeAsPieces(x))\n\n    def decode(self, x: str) -> str:\n        return x.replace(' ', '').replace('\\u2581', ' ').strip()\n\n    def is_beginning_of_word(self, x: str) -> bool:\n        if x in ['<unk>', '<s>', '</s>', '<pad>']:\n            # special elements are always considered beginnings\n            # HACK: this logic is already present in fairseq/tasks/masked_lm.py\n            # but these special tokens are also contained in the sentencepiece\n            # vocabulary which causes duplicate special tokens. This hack makes\n            # sure that they are all taken into account.\n            return True\n        return x.startswith('\\u2581')\n"""
fairseq/data/encoders/space_tokenizer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport re\n\nfrom fairseq.data.encoders import register_tokenizer\n\n\n@register_tokenizer(\'space\')\nclass SpaceTokenizer(object):\n\n    def __init__(self, source_lang=None, target_lang=None):\n        self.space_tok = re.compile(r""\\s+"")\n\n    def encode(self, x: str) -> str:\n        return self.space_tok.sub(\' \', x)\n\n    def decode(self, x: str) -> str:\n        return x\n'"
fairseq/data/encoders/subword_nmt_bpe.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq import file_utils\nfrom fairseq.data.encoders import register_bpe\n\n\n@register_bpe('subword_nmt')\nclass SubwordNMTBPE(object):\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument('--bpe-codes', type=str,\n                            help='path to subword NMT BPE')\n        parser.add_argument('--bpe-separator', default='@@',\n                            help='BPE separator')\n        # fmt: on\n\n    def __init__(self, args):\n        if args.bpe_codes is None:\n            raise ValueError('--bpe-codes is required for --bpe=subword_nmt')\n        codes = file_utils.cached_path(args.bpe_codes)\n        try:\n            from subword_nmt import apply_bpe\n            bpe_parser = apply_bpe.create_parser()\n            bpe_args = bpe_parser.parse_args([\n                '--codes', codes,\n                '--separator', args.bpe_separator,\n            ])\n            self.bpe = apply_bpe.BPE(\n                bpe_args.codes,\n                bpe_args.merges,\n                bpe_args.separator,\n                None,\n                bpe_args.glossaries,\n            )\n            self.bpe_symbol = bpe_args.separator + ' '\n        except ImportError:\n            raise ImportError('Please install subword_nmt with: pip install subword-nmt')\n\n    def encode(self, x: str) -> str:\n        return self.bpe.process_line(x)\n\n    def decode(self, x: str) -> str:\n        return (x + ' ').replace(self.bpe_symbol, '').rstrip()\n"""
fairseq/data/encoders/utils.py,1,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom fairseq.data import encoders\n\n\ndef get_whole_word_mask(args, dictionary):\n    bpe = encoders.build_bpe(args)\n    if bpe is not None:\n        def is_beginning_of_word(i):\n            if i < dictionary.nspecial:\n                # special elements are always considered beginnings\n                return True\n            tok = dictionary[i]\n            if tok.startswith('madeupword'):\n                return True\n            try:\n                return bpe.is_beginning_of_word(tok)\n            except ValueError:\n                return True\n        mask_whole_words = torch.ByteTensor(list(\n            map(is_beginning_of_word, range(len(dictionary)))\n        ))\n        return mask_whole_words\n    return None\n"""
fairseq/data/legacy/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .masked_lm_dictionary import BertDictionary, MaskedLMDictionary\nfrom .block_pair_dataset import BlockPairDataset\nfrom .masked_lm_dataset import MaskedLMDataset\n\n__all__ = [\n    'BertDictionary',\n    'BlockPairDataset',\n    'MaskedLMDataset',\n    'MaskedLMDictionary',\n]\n"""
fairseq/data/legacy/block_pair_dataset.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport numpy as np\nimport torch\n\nfrom fairseq.data import FairseqDataset\n\n\nclass BlockPairDataset(FairseqDataset):\n    """"""Break a Dataset of tokens into sentence pair blocks for next sentence\n       prediction as well as masked language model.\n\n       High-level logics are:\n       1. break input tensor to tensor blocks\n       2. pair the blocks with 50% next sentence and 50% random sentence\n       3. return paired blocks as well as related segment labels\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset to break into blocks\n        sizes: array of sentence lengths\n        dictionary: dictionary for the task\n        block_size: maximum block size\n        break_mode: mode for breaking copurs into block pairs. currently we support\n            2 modes\n            doc: respect document boundaries and each part of the pair should belong to on document\n            none: don\'t respect any boundary and cut tokens evenly\n        short_seq_prob: probability for generating shorter block pairs\n        doc_break_size: Size for empty line separating documents. Typically 1 if\n                        the sentences have eos, 0 otherwise.\n    """"""\n\n    def __init__(\n        self,\n        dataset,\n        dictionary,\n        sizes,\n        block_size,\n        break_mode=""doc"",\n        short_seq_prob=0.1,\n        doc_break_size=1,\n    ):\n        super().__init__()\n        self.dataset = dataset\n        self.pad = dictionary.pad()\n        self.eos = dictionary.eos()\n        self.cls = dictionary.cls()\n        self.mask = dictionary.mask()\n        self.sep = dictionary.sep()\n        self.break_mode = break_mode\n        self.dictionary = dictionary\n        self.short_seq_prob = short_seq_prob\n        self.block_indices = []\n\n        assert len(dataset) == len(sizes)\n\n        if break_mode == ""doc"":\n            cur_doc = []\n            for sent_id, sz in enumerate(sizes):\n                assert doc_break_size == 0 or sz != 0, (\n                    ""when doc_break_size is non-zero, we expect documents to be""\n                    ""separated by a blank line with a single eos.""\n                )\n                # empty line as document separator\n                if sz == doc_break_size:\n                    if len(cur_doc) == 0:\n                        continue\n                    self.block_indices.append(cur_doc)\n                    cur_doc = []\n                else:\n                    cur_doc.append(sent_id)\n            max_num_tokens = block_size - 3  # Account for [CLS], [SEP], [SEP]\n            self.sent_pairs = []\n            self.sizes = []\n            for doc_id, doc in enumerate(self.block_indices):\n                self._generate_sentence_pair(doc, doc_id, max_num_tokens, sizes)\n        elif break_mode is None or break_mode == ""none"":\n            # each block should have half of the block size since we are constructing block pair\n            sent_length = (block_size - 3) // 2\n            total_len = sum(dataset.sizes)\n            length = math.ceil(total_len / sent_length)\n\n            def block_at(i):\n                start = i * sent_length\n                end = min(start + sent_length, total_len)\n                return (start, end)\n\n            sent_indices = np.array([block_at(i) for i in range(length)])\n            sent_sizes = np.array([e - s for s, e in sent_indices])\n            dataset_index = self._sent_to_dataset_index(sent_sizes)\n\n            # pair sentences\n            self._pair_sentences(dataset_index)\n        else:\n            raise ValueError(""Invalid break_mode: "" + break_mode)\n\n    def _pair_sentences(self, dataset_index):\n        """"""\n        Give a list of evenly cut blocks/sentences, pair these sentences with 50%\n        consecutive sentences and 50% random sentences.\n        This is used for none break mode\n        """"""\n        # pair sentences\n        for sent_id, sent in enumerate(dataset_index):\n            next_sent_label = (\n                1 if np.random.rand() > 0.5 and sent_id != len(dataset_index) - 1 else 0\n            )\n            if next_sent_label:\n                next_sent = dataset_index[sent_id + 1]\n            else:\n                next_sent = dataset_index[\n                    self._skip_sampling(len(dataset_index), [sent_id, sent_id + 1])\n                ]\n            self.sent_pairs.append((sent, next_sent, next_sent_label))\n\n            # The current blocks don\'t include the special tokens but the\n            # sizes already account for this\n            self.sizes.append(3 + sent[3] + next_sent[3])\n\n    def _sent_to_dataset_index(self, sent_sizes):\n        """"""\n        Build index mapping block indices to the underlying dataset indices\n        """"""\n        dataset_index = []\n        ds_idx, ds_remaining = -1, 0\n        for to_consume in sent_sizes:\n            sent_size = to_consume\n            if ds_remaining == 0:\n                ds_idx += 1\n                ds_remaining = sent_sizes[ds_idx]\n            start_ds_idx = ds_idx\n            start_offset = sent_sizes[ds_idx] - ds_remaining\n            while to_consume > ds_remaining:\n                to_consume -= ds_remaining\n                ds_idx += 1\n                ds_remaining = sent_sizes[ds_idx]\n            ds_remaining -= to_consume\n            dataset_index.append(\n                (\n                    start_ds_idx,  # starting index in dataset\n                    start_offset,  # starting offset within starting index\n                    ds_idx,  # ending index in dataset\n                    sent_size,  # sentence length\n                )\n            )\n        assert ds_remaining == 0\n        assert ds_idx == len(self.dataset) - 1\n        return dataset_index\n\n    def _generate_sentence_pair(self, doc, doc_id, max_num_tokens, sizes):\n        """"""\n        Go through a single document and genrate sentence paris from it\n        """"""\n        current_chunk = []\n        current_length = 0\n        curr = 0\n        # To provide more randomness, we decrease target seq length for parts of\n        # samples (10% by default). Note that max_num_tokens is the hard threshold\n        # for batching and will never be changed.\n        target_seq_length = max_num_tokens\n        if np.random.random() < self.short_seq_prob:\n            target_seq_length = np.random.randint(2, max_num_tokens)\n        # loop through all sentences in document\n        while curr < len(doc):\n            sent_id = doc[curr]\n            current_chunk.append(sent_id)\n            current_length = sum(sizes[current_chunk])\n            # split chunk and generate pair when exceed target_seq_length or\n            # finish the loop\n            if curr == len(doc) - 1 or current_length >= target_seq_length:\n                # split the chunk into 2 parts\n                a_end = 1\n                if len(current_chunk) > 2:\n                    a_end = np.random.randint(1, len(current_chunk) - 1)\n                sent_a = current_chunk[:a_end]\n                len_a = sum(sizes[sent_a])\n                # generate next sentence label, note that if there is only 1 sentence\n                # in current chunk, label is always 0\n                next_sent_label = (\n                    1 if np.random.rand() > 0.5 and len(current_chunk) != 1 else 0\n                )\n                if not next_sent_label:\n                    # if next sentence label is 0, sample sent_b from a random doc\n                    target_b_length = target_seq_length - len_a\n                    rand_doc_id = self._skip_sampling(len(self.block_indices), [doc_id])\n                    random_doc = self.block_indices[rand_doc_id]\n                    random_start = np.random.randint(0, len(random_doc))\n                    sent_b = []\n                    len_b = 0\n                    for j in range(random_start, len(random_doc)):\n                        sent_b.append(random_doc[j])\n                        len_b = sum(sizes[sent_b])\n                        if len_b >= target_b_length:\n                            break\n                    # return the second part of the chunk since it\'s not used\n                    num_unused_segments = len(current_chunk) - a_end\n                    curr -= num_unused_segments\n                else:\n                    # if next sentence label is 1, use the second part of chunk as sent_B\n                    sent_b = current_chunk[a_end:]\n                    len_b = sum(sizes[sent_b])\n                # currently sent_a and sent_B may be longer than max_num_tokens,\n                # truncate them and return block idx and offsets for them\n                sent_a, sent_b = self._truncate_sentences(\n                    sent_a, sent_b, max_num_tokens\n                )\n                self.sent_pairs.append((sent_a, sent_b, next_sent_label))\n                self.sizes.append(3 + sent_a[3] + sent_b[3])\n                current_chunk = []\n            curr += 1\n\n    def _skip_sampling(self, total, skip_ids):\n        """"""\n        Generate a random integer which is not in skip_ids. Sample range is [0, total)\n        TODO: ids in skip_ids should be consecutive, we can extend it to more generic version later\n        """"""\n        rand_id = np.random.randint(total - len(skip_ids))\n        return rand_id if rand_id < min(skip_ids) else rand_id + len(skip_ids)\n\n    def _truncate_sentences(self, sent_a, sent_b, max_num_tokens):\n        """"""\n        Trancate a pair of sentence to limit total length under max_num_tokens\n        Logics:\n            1. Truncate longer sentence\n            2. Tokens to be truncated could be at the beginning or the end of the sentnce\n        Returns:\n            Truncated sentences represented by dataset idx\n        """"""\n        len_a, len_b = sum(self.dataset.sizes[sent_a]), sum(self.dataset.sizes[sent_b])\n        front_cut_a = front_cut_b = end_cut_a = end_cut_b = 0\n\n        while True:\n            total_length = (\n                len_a + len_b - front_cut_a - front_cut_b - end_cut_a - end_cut_b\n            )\n            if total_length <= max_num_tokens:\n                break\n\n            if len_a - front_cut_a - end_cut_a > len_b - front_cut_b - end_cut_b:\n                if np.random.rand() < 0.5:\n                    front_cut_a += 1\n                else:\n                    end_cut_a += 1\n            else:\n                if np.random.rand() < 0.5:\n                    front_cut_b += 1\n                else:\n                    end_cut_b += 1\n\n        # calculate ds indices as well as offsets and return\n        truncated_sent_a = self._cut_sentence(sent_a, front_cut_a, end_cut_a)\n        truncated_sent_b = self._cut_sentence(sent_b, front_cut_b, end_cut_b)\n        return truncated_sent_a, truncated_sent_b\n\n    def _cut_sentence(self, sent, front_cut, end_cut):\n        """"""\n        Cut a sentence based on the numbers of tokens to be cut from beginning and end\n        Represent the sentence as dataset idx and return\n        """"""\n        start_ds_idx, end_ds_idx, offset = sent[0], sent[-1], 0\n        target_len = sum(self.dataset.sizes[sent]) - front_cut - end_cut\n        while front_cut > 0:\n            if self.dataset.sizes[start_ds_idx] > front_cut:\n                offset += front_cut\n                break\n            else:\n                front_cut -= self.dataset.sizes[start_ds_idx]\n                start_ds_idx += 1\n        while end_cut > 0:\n            if self.dataset.sizes[end_ds_idx] > end_cut:\n                break\n            else:\n                end_cut -= self.dataset.sizes[end_ds_idx]\n                end_ds_idx -= 1\n        return start_ds_idx, offset, end_ds_idx, target_len\n\n    def _fetch_block(self, start_ds_idx, offset, end_ds_idx, length):\n        """"""\n        Fetch a block of tokens based on its dataset idx\n        """"""\n        buffer = torch.cat(\n            [self.dataset[idx] for idx in range(start_ds_idx, end_ds_idx + 1)]\n        )\n        s, e = offset, offset + length\n        return buffer[s:e]\n\n    def __getitem__(self, index):\n        block1, block2, next_sent_label = self.sent_pairs[index]\n        block1 = self._fetch_block(*block1)\n        block2 = self._fetch_block(*block2)\n        return block1, block2, next_sent_label\n\n    def __len__(self):\n        return len(self.sizes)\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, ""supports_prefetch"", False)\n\n    def prefetch(self, indices):\n        prefetch_idx = set()\n        for index in indices:\n            for block1, block2, _ in [self.sent_pairs[index]]:\n                for ds_idx in range(block1[0], block1[2] + 1):\n                    prefetch_idx.add(ds_idx)\n                for ds_idx in range(block2[0], block2[2] + 1):\n                    prefetch_idx.add(ds_idx)\n        self.dataset.prefetch(prefetch_idx)\n'"
fairseq/data/legacy/masked_lm_dataset.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport numpy as np\nimport torch\n\nfrom typing import Dict, List, Tuple\n\nfrom fairseq.data import FairseqDataset, data_utils\n\nfrom fairseq.data import Dictionary\nfrom fairseq.data.legacy.block_pair_dataset import BlockPairDataset\nfrom fairseq.data.token_block_dataset import TokenBlockDataset\nfrom fairseq.data.concat_dataset import ConcatDataset\n\n\nclass MaskedLMDataset(FairseqDataset):\n    """"""\n    A wrapper Dataset for masked language modelling. The dataset\n    wraps around TokenBlockDataset or BlockedPairDataset and creates a batch\n    where the input blocks are masked according to the specified masking\n    probability. Additionally the batch can also contain sentence level targets\n    if this is specified.\n\n    Args:\n        dataset: Dataset which generates blocks of data. Only BlockPairDataset\n            and TokenBlockDataset are supported.\n        sizes: Sentence lengths\n        vocab: Dictionary with the vocabulary and special tokens.\n        pad_idx: Id of padding token in dictionary\n        mask_idx: Id of mask token in dictionary\n        classif_token_idx: Id of classification token in dictionary. This is the\n            token associated with the sentence embedding (Eg: CLS for BERT)\n        sep_token_idx: Id of separator token in dictionary\n            (Eg: SEP in BERT)\n        seed: Seed for random number generator for reproducibility.\n        shuffle: Shuffle the elements before batching.\n        has_pairs: Specifies whether the underlying dataset\n            generates a pair of blocks along with a sentence_target or not.\n            Setting it to True assumes that the underlying dataset generates a\n            label for the pair of sentences which is surfaced as\n            sentence_target. The default value assumes a single block with no\n            sentence target.\n        segment_id: An optional segment id for filling in the segment labels\n            when we are in the single block setting (Eg: XLM). Default is 0.\n        masking_ratio: specifies what percentage of the blocks should be masked.\n        masking_prob: specifies the probability of a given token being\n            replaced with the ""MASK"" token.\n        random_token_prob: specifies the probability of a given token being\n            replaced by a random token from the vocabulary.\n    """"""\n\n    def __init__(\n            self,\n            dataset: FairseqDataset,\n            sizes: np.ndarray,\n            vocab: Dictionary,\n            pad_idx: int,\n            mask_idx: int,\n            classif_token_idx: int,\n            sep_token_idx: int,\n            seed: int = 1,\n            shuffle: bool = True,\n            has_pairs: bool = True,\n            segment_id: int = 0,\n            masking_ratio: float = 0.15,\n            masking_prob: float = 0.8,\n            random_token_prob: float = 0.1\n    ):\n        # Make sure the input datasets are the ones supported\n        assert (\n            isinstance(dataset, TokenBlockDataset) or\n            isinstance(dataset, BlockPairDataset) or\n            isinstance(dataset, ConcatDataset)\n        ), ""MaskedLMDataset only wraps TokenBlockDataset or BlockPairDataset or "" \\\n           ""ConcatDataset""\n\n        self.dataset = dataset\n        self.sizes = np.array(sizes)\n        self.vocab = vocab\n        self.pad_idx = pad_idx\n        self.mask_idx = mask_idx\n        self.classif_token_idx = classif_token_idx\n        self.sep_token_idx = sep_token_idx\n        self.shuffle = shuffle\n        self.seed = seed\n        self.has_pairs = has_pairs\n        self.segment_id = segment_id\n        self.masking_ratio = masking_ratio\n        self.masking_prob = masking_prob\n        self.random_token_prob = random_token_prob\n\n        # If we have only one block then sizes needs to be updated to include\n        # the classification token\n        if not has_pairs:\n            self.sizes = self.sizes + 1\n\n    def __getitem__(\n            self,\n            index: int\n    ):\n        # if has_pairs, then expect 2 blocks and a sentence target\n        if self.has_pairs:\n            (block_one, block_two, sentence_target) = self.dataset[index]\n        else:\n            block_one = self.dataset[index]\n\n        return {\n            ""id"": index,\n            ""block_one"": block_one,\n            ""block_two"": block_two if self.has_pairs else None,\n            ""sentence_target"": sentence_target if self.has_pairs else None,\n        }\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def _mask_block(\n            self,\n            sentence: np.ndarray,\n            mask_idx: int,\n            pad_idx: int,\n            dictionary_token_range: Tuple,\n    ):\n        """"""\n        Mask tokens for Masked Language Model training\n        Samples mask_ratio tokens that will be predicted by LM.\n\n        Note:This function may not be efficient enough since we had multiple\n        conversions between np and torch, we can replace them with torch\n        operators later.\n\n        Args:\n            sentence: 1d tensor to be masked\n            mask_idx: index to use for masking the sentence\n            pad_idx: index to use for masking the target for tokens we aren\'t\n                predicting\n            dictionary_token_range: range of indices in dictionary which can\n                be used for random word replacement\n                (e.g. without special characters)\n        Return:\n            masked_sent: masked sentence\n            target: target with words which we are not predicting replaced\n                by pad_idx\n        """"""\n        masked_sent = np.copy(sentence)\n        sent_length = len(sentence)\n        mask_num = math.ceil(sent_length * self.masking_ratio)\n        mask = np.random.choice(sent_length, mask_num, replace=False)\n        target = np.copy(sentence)\n\n        for i in range(sent_length):\n            if i in mask:\n                rand = np.random.random()\n\n                # replace with mask if probability is less than masking_prob\n                # (Eg: 0.8)\n                if rand < self.masking_prob:\n                    masked_sent[i] = mask_idx\n\n                # replace with random token if probability is less than\n                # masking_prob + random_token_prob (Eg: 0.9)\n                elif rand < (self.masking_prob + self.random_token_prob):\n                    # sample random token from dictionary\n                    masked_sent[i] = (\n                        np.random.randint(\n                            dictionary_token_range[0], dictionary_token_range[1]\n                        )\n                    )\n            else:\n                target[i] = pad_idx\n\n        return masked_sent, target\n\n    def _collate(\n            self,\n            samples: List[Dict],\n            pad_idx: int,\n            eos_idx: int\n    ):\n        """"""\n        Does the heavy lifting for creating a batch from the input list of\n        examples. The logic is as follows:\n            1. Mask the input blocks. In case has_pair is True then we have 2\n               blocks to mask.\n            2. Prepend the first masked block tensor with the special token\n               used as sentence embedding. Eg: CLS in BERT. This happens\n               irrespective of the value of has_pair.\n            3. If has_pair is True, then append the first masked block with the\n               special separator token (eg: SEP for BERT) and compute segment\n               label accordingly. In this case, also append the second masked\n               block with this special separator token and compute its segment\n               label.\n            4. For the targets tensor, prepend and append with padding index\n               accordingly.\n            5. Concatenate all tensors.\n        """"""\n        if len(samples) == 0:\n            return {}\n        # To ensure determinism, we reset the state of the PRNG after every\n        # batch based on the seed and the first id of the batch. This ensures\n        # that across epochs we get the same mask for the same example. This\n        # is needed for reproducibility and is how BERT does masking\n        # TODO: Can we add deteminism without this constraint?\n        with data_utils.numpy_seed(self.seed + samples[0][""id""]):\n            for s in samples:\n\n                # token range is needed for replacing with random token during\n                # masking\n                token_range = (self.vocab.nspecial, len(self.vocab))\n\n                # mask according to specified probabilities.\n                masked_blk_one, masked_tgt_one = self._mask_block(\n                    s[""block_one""], self.mask_idx, self.pad_idx, token_range,\n                )\n\n                tokens = np.concatenate([\n                    [self.classif_token_idx], masked_blk_one\n                ])\n                targets = np.concatenate([[self.pad_idx], masked_tgt_one])\n                segments = np.ones(len(tokens)) * self.segment_id\n\n                # if has_pairs is True then we need to add the SEP token to both\n                # the blocks after masking and re-compute segments based on the new\n                # lengths.\n                if self.has_pairs:\n                    tokens_one = np.concatenate([tokens, [self.sep_token_idx]])\n                    targets_one = np.concatenate([targets, [self.pad_idx]])\n\n                    masked_blk_two, masked_tgt_two = self._mask_block(\n                        s[""block_two""], self.mask_idx, self.pad_idx, token_range)\n                    tokens_two = np.concatenate(\n                        [masked_blk_two, [self.sep_token_idx]])\n                    targets_two = np.concatenate([masked_tgt_two, [self.pad_idx]])\n\n                    # block + 1 sep + 1 special (CLS)\n                    segments_one = np.zeros(len(tokens_one))\n                    # block + 1 sep\n                    segments_two = np.ones(len(tokens_two))\n\n                    tokens = np.concatenate([tokens_one, tokens_two])\n                    targets = np.concatenate([targets_one, targets_two])\n                    segments = np.concatenate([segments_one, segments_two])\n\n                s[""source""] = torch.LongTensor(tokens)\n                s[""segment_labels""] = torch.LongTensor(segments)\n                s[""lm_target""] = torch.LongTensor(targets)\n\n        def merge(key):\n            return data_utils.collate_tokens(\n                [s[key] for s in samples], pad_idx, eos_idx, left_pad=False\n            )\n        return {\n            ""id"": torch.LongTensor([s[""id""] for s in samples]),\n            ""ntokens"": sum(len(s[""source""]) for s in samples),\n            ""net_input"": {\n                ""src_tokens"": merge(""source""),\n                ""segment_labels"": merge(""segment_labels""),\n            },\n            ""lm_target"": merge(""lm_target""),\n            ""sentence_target"": torch.LongTensor(\n                [s[""sentence_target""] for s in samples]\n            ) if self.has_pairs else None,\n            ""nsentences"": len(samples),\n        }\n\n    def collater(\n            self,\n            samples: List[Dict]\n    ):\n        """"""Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch of data\n        """"""\n        return self._collate(samples, self.vocab.pad(), self.vocab.eos())\n\n    def num_tokens(\n            self,\n            index: int\n    ):\n        """"""\n        Return the number of tokens in a sample. This value is used to\n        enforce max-tokens during batching.\n        """"""\n        return self.sizes[index]\n\n    def size(\n            self,\n            index: int\n    ):\n        """"""\n        Return an example\'s size as a float or tuple. This value is used when\n        filtering a dataset with max-positions.\n        """"""\n        return self.sizes[index]\n\n    def ordered_indices(self):\n        """"""\n        Return an ordered list of indices. Batches will be constructed based\n        on this order.\n        """"""\n        if self.shuffle:\n            return np.random.permutation(len(self))\n        else:\n            order = [np.arange(len(self))]\n            order.append(self.sizes)\n            return np.lexsort(order)\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, ""supports_prefetch"", False)\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(indices)\n'"
fairseq/data/legacy/masked_lm_dictionary.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.data import Dictionary\n\n\nclass MaskedLMDictionary(Dictionary):\n    """"""\n    Dictionary for Masked Language Modelling tasks. This extends Dictionary by\n    adding the mask symbol.\n    """"""\n    def __init__(\n        self,\n        pad=\'<pad>\',\n        eos=\'</s>\',\n        unk=\'<unk>\',\n        mask=\'<mask>\',\n    ):\n        super().__init__(pad=pad, eos=eos, unk=unk)\n        self.mask_word = mask\n        self.mask_index = self.add_symbol(mask)\n        self.nspecial = len(self.symbols)\n\n    def mask(self):\n        """"""Helper to get index of mask symbol""""""\n        return self.mask_index\n\n\nclass BertDictionary(MaskedLMDictionary):\n    """"""\n    Dictionary for BERT task. This extends MaskedLMDictionary by adding support\n    for cls and sep symbols.\n    """"""\n    def __init__(\n        self,\n        pad=\'<pad>\',\n        eos=\'</s>\',\n        unk=\'<unk>\',\n        mask=\'<mask>\',\n        cls=\'<cls>\',\n        sep=\'<sep>\'\n    ):\n        super().__init__(pad=pad, eos=eos, unk=unk)\n        self.cls_word = cls\n        self.sep_word = sep\n        self.cls_index = self.add_symbol(cls)\n        self.sep_index = self.add_symbol(sep)\n        self.nspecial = len(self.symbols)\n\n    def cls(self):\n        """"""Helper to get index of cls symbol""""""\n        return self.cls_index\n\n    def sep(self):\n        """"""Helper to get index of sep symbol""""""\n        return self.sep_index\n'"
fairseq/model_parallel/criterions/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\n\n# automatically import any Python files in the criterions/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        module = file[:file.find('.py')]\n        importlib.import_module('fairseq.model_parallel.criterions.' + module)\n"""
fairseq/model_parallel/criterions/vocab_parallel_cross_entropy.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\ntry:\n    from fairseq.model_parallel.megatron.mpu.cross_entropy import vocab_parallel_cross_entropy\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\n\n@register_criterion(\'vocab_parallel_cross_entropy\')\nclass VocabParallelCrossEntropyCriterion(FairseqCriterion):\n\n    def __init__(self, task, sentence_avg):\n        super().__init__(task)\n        self.sentence_avg = sentence_avg\n        if not has_megatron_submodule:\n            raise ImportError(\n                \'\\n\\nPlease install the megatron submodule:\'\n                \'\\n\\n  git submodule update --init \'\n                \'fairseq/model_parallel/megatron\'\n            )\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        net_output = model(**sample[\'net_input\'])\n        target = sample[\'target\']\n\n        loss = vocab_parallel_cross_entropy(net_output[0].float(), target)\n        loss = (loss * (target != self.padding_idx)).sum()\n        sample_size = sample[\'target\'].size(0) if self.sentence_avg else sample[\'ntokens\']\n        logging_output = {\n            \'loss\': utils.item(loss.data) if reduce else loss.data,\n            \'ntokens\': sample[\'ntokens\'],\n            \'nsentences\': sample[\'target\'].size(0),\n            \'sample_size\': sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(\'loss\', 0) for log in logging_outputs)\n        ntokens = sum(log.get(\'ntokens\', 0) for log in logging_outputs)\n        sample_size = sum(log.get(\'sample_size\', 0) for log in logging_outputs)\n\n        metrics.log_scalar(\'loss\', loss_sum / sample_size / math.log(2), sample_size, round=3)\n        if sample_size != ntokens:\n            metrics.log_scalar(\'nll_loss\', loss_sum / ntokens / math.log(2), ntokens, round=3)\n            metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'nll_loss\'].avg))\n        else:\n            metrics.log_derived(\'ppl\', lambda meters: utils.get_perplexity(meters[\'loss\'].avg))\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        """"""\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        """"""\n        return True\n'"
fairseq/model_parallel/models/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\n\n# automatically import any Python files in the models/ directory\nmodels_dir = os.path.dirname(__file__)\nfor file in os.listdir(models_dir):\n    path = os.path.join(models_dir, file)\n    if not file.startswith('_') and not file.startswith('.') and (file.endswith('.py') or os.path.isdir(path)):\n        model_name = file[:file.find('.py')] if file.endswith('.py') else file\n        module = importlib.import_module('fairseq.model_parallel.models.' + model_name)\n"""
fairseq/model_parallel/models/transformer.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq.models import (\n    register_model,\n)\n\nfrom fairseq.models.transformer import (\n    TransformerDecoder,\n    TransformerEncoder,\n    TransformerModel,\n)\n\nfrom fairseq.model_parallel.modules import (\n    ModelParallelTransformerDecoderLayer,\n    ModelParallelTransformerEncoderLayer,\n)\n\ntry:\n    from fairseq.model_parallel.megatron.mpu import (\n        copy_to_model_parallel_region,\n        gather_from_model_parallel_region,\n        VocabParallelEmbedding,\n    )\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(\'model_parallel_transformer\')\nclass ModelParallelTransformerModel(TransformerModel):\n    """"""\n    Model parallel Transformer model.\n    """"""\n    @classmethod\n    def build_embedding(cls, args, dictionary, embed_dim, path=None):\n        if not has_megatron_submodule:\n            raise ImportError(\n                \'\\n\\nPlease install the megatron submodule:\'\n                \'\\n\\n  git submodule update --init \'\n                \'fairseq/model_parallel/megatron\'\n            )\n        num_embeddings = len(dictionary)\n        padding_idx = dictionary.pad()\n\n        def _vocab_init(tensor, **kwargs):\n            nn.init.normal_(tensor, mean=0, std=num_embeddings ** -0.5)\n            nn.init.constant_(tensor[1], 0)\n        emb = VocabParallelEmbedding(num_embeddings, embed_dim, padding_idx, init_method=_vocab_init)\n        # if provided, load from preloaded dictionaries\n        if path:\n            raise NotImplementedError(""Loading of embedding from path is not supported for model parallel"")\n        return emb\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return ModelParallelTransformerEncoder(args, src_dict, embed_tokens)\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        return ModelParallelTransformerDecoder(\n            args,\n            tgt_dict,\n            embed_tokens,\n            no_encoder_attn=getattr(args, \'no_cross_attention\', False),\n        )\n\n\nclass ModelParallelTransformerEncoder(TransformerEncoder):\n    """"""\n    Model parallel Transformer encoder consisting of *args.encoder_layers* layers. Each layer\n    is a :class:`ModelParallelTransformerEncoderLayer`.\n    """"""\n\n    def build_encoder_layer(self, args):\n        return ModelParallelTransformerEncoderLayer(args)\n\n\nclass ModelParallelTransformerDecoder(TransformerDecoder):\n    """"""\n    Model Parallel Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`ModelParallelTransformerDecoderLayer`.\n    """"""\n\n    def build_decoder_layer(self, args, no_encoder_attn=False):\n        return ModelParallelTransformerDecoderLayer(args, no_encoder_attn)\n\n    def output_layer(self, features, **kwargs):\n        """"""Project features to the vocabulary size.""""""\n        features = copy_to_model_parallel_region(features)\n\n        # project back to size of vocabulary\n        if self.share_input_output_embed:\n            x = F.linear(features, self.embed_tokens.weight)\n        else:\n            x = F.linear(features, self.embed_out)\n\n        if getattr(self.args, \'criterion\') != \'vocab_parallel_cross_entropy\':\n            x = gather_from_model_parallel_region(x).contiguous()\n        return x\n'"
fairseq/model_parallel/models/transformer_lm.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\n\nfrom fairseq.models import (\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.transformer_lm import (\n    base_lm_architecture,\n    TransformerLanguageModel,\n)\nfrom fairseq.model_parallel.models.transformer import (\n    ModelParallelTransformerDecoder,\n)\ntry:\n    from fairseq.model_parallel.megatron.mpu import VocabParallelEmbedding\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\n\n@register_model(\'model_parallel_transformer_lm\')\nclass ModelParallelTransformerLanguageModel(TransformerLanguageModel):\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        if not has_megatron_submodule:\n            raise ImportError(\n                \'\\n\\nPlease install the megatron submodule:\'\n                \'\\n\\n  git submodule update --init \'\n                \'fairseq/model_parallel/megatron\'\n            )\n\n        # make sure all arguments are present in older models\n        base_lm_architecture(args)\n\n        if args.decoder_layers_to_keep:\n            args.decoder_layers = len(args.decoder_layers_to_keep.split("",""))\n\n        if getattr(args, \'max_target_positions\', None) is None:\n            args.max_target_positions = getattr(args, \'tokens_per_sample\', DEFAULT_MAX_TARGET_POSITIONS)\n\n        if args.character_embeddings:\n            raise NotImplementedError(""Character embeddings is not supported for model parallel"")\n        elif args.adaptive_input:\n            raise NotImplementedError(""Adaptive input is not supported for model parallel"")\n        else:\n            embed_tokens = cls.build_embedding(args, task.source_dictionary, args.decoder_input_dim)\n\n        decoder = ModelParallelTransformerDecoder(\n            args, task.target_dictionary, embed_tokens, no_encoder_attn=True,\n        )\n        return cls(decoder)\n\n    @classmethod\n    def build_embedding(cls, args, dictionary, embed_dim, path=None):\n        def _vocab_init(tensor, **kwargs):\n            nn.init.normal_(tensor, mean=0, std=embed_dim ** -0.5)\n            nn.init.constant_(tensor[1], 0)\n        embed_tokens = VocabParallelEmbedding(len(dictionary), embed_dim, dictionary.pad(), init_method=_vocab_init)\n        return embed_tokens\n\n\n@register_model_architecture(\'model_parallel_transformer_lm\', \'transformer_lm_megatron\')\ndef transformer_lm_megatron(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 3072)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 3072 * 4)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 72)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 32)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    base_lm_architecture(args)\n\n\n@register_model_architecture(\'model_parallel_transformer_lm\', \'transformer_lm_megatron_11b\')\ndef transformer_lm_megatron_11b(args):\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', 3072)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', 3072 * 6)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 72)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 32)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    base_lm_architecture(args)\n'"
fairseq/model_parallel/modules/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .multihead_attention import ModelParallelMultiheadAttention\nfrom .transformer_layer import ModelParallelTransformerEncoderLayer, ModelParallelTransformerDecoderLayer\nfrom .transformer_sentence_encoder_layer import ModelParallelTransformerSentenceEncoderLayer\nfrom .transformer_sentence_encoder import ModelParallelTransformerSentenceEncoder\n\n__all__ = [\n    'ModelParallelMultiheadAttention',\n    'ModelParallelTransformerEncoderLayer',\n    'ModelParallelTransformerDecoderLayer',\n    'ModelParallelTransformerSentenceEncoder',\n    'ModelParallelTransformerSentenceEncoderLayer',\n]\n"""
fairseq/model_parallel/modules/multihead_attention.py,11,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom torch import Tensor, nn\nfrom fairseq.incremental_decoding_utils import with_incremental_state\n\ntry:\n    from fairseq.model_parallel.megatron.mpu import (\n        get_cuda_rng_tracker,\n        get_model_parallel_world_size,\n        ColumnParallelLinear,\n        RowParallelLinear,\n    )\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\n\n@with_incremental_state\nclass ModelParallelMultiheadAttention(nn.Module):\n    """"""Model parallel Multi-headed attention.\n    This performs the Multi-headed attention over multiple gpus.\n\n    See ""Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf"" for more details.\n    """"""\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,\n        dropout=0.0,\n        bias=True,\n        self_attention=False,\n        encoder_decoder_attention=False,\n    ):\n        super().__init__()\n        if not has_megatron_submodule:\n            raise ImportError(\n                \'\\n\\nPlease install the megatron submodule:\'\n                \'\\n\\n  git submodule update --init \'\n                \'fairseq/model_parallel/megatron\'\n            )\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.model_parallel_size = get_model_parallel_world_size()\n\n        self.num_heads_partition = num_heads // self.model_parallel_size\n        assert (\n            self.num_heads_partition * self.model_parallel_size == num_heads\n        ), ""Number of heads must be divisble by model parallel size""\n\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), ""embed_dim must be divisible by num_heads""\n        self.scaling = self.head_dim ** -0.5\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            ""Self-attention requires query, key and value to be of the same size""\n        )\n\n        self.k_proj = ColumnParallelLinear(self.kdim, embed_dim, bias=bias, gather_output=False)\n        self.v_proj = ColumnParallelLinear(self.vdim, embed_dim, bias=bias, gather_output=False)\n        self.q_proj = ColumnParallelLinear(embed_dim, embed_dim, bias=bias, gather_output=False)\n        self.out_proj = RowParallelLinear(embed_dim, embed_dim, bias=bias, input_is_parallel=True)\n\n    def forward(\n        self,\n        query,\n        key: Optional[Tensor],\n        value: Optional[Tensor],\n        key_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        static_kv: bool = False,\n        attn_mask: Optional[Tensor] = None,\n        **unused_kwargs,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        """"""Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n        """"""\n        tgt_len, bsz, embed_dim = query.size()\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n            if saved_state is not None and ""prev_key"" in saved_state:\n                # previous time steps are cached - no need to recompute\n                # key and value if they are static\n                if static_kv:\n                    assert self.encoder_decoder_attention and not self.self_attention\n                    key = value = None\n        else:\n            saved_state = None\n\n        if self.self_attention:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        elif self.encoder_decoder_attention:\n            # encoder-decoder attention\n            q = self.q_proj(query)\n            if key is None:\n                assert value is None\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n        q *= self.scaling\n\n\n        q = (\n            q.contiguous()\n            .view(tgt_len, bsz * self.num_heads_partition, self.head_dim)\n            .transpose(0, 1)\n        )\n        if k is not None:\n            k = (\n                k.contiguous()\n                .view(-1, bsz * self.num_heads_partition, self.head_dim)\n                .transpose(0, 1)\n            )\n        if v is not None:\n            v = (\n                v.contiguous()\n                .view(-1, bsz * self.num_heads_partition, self.head_dim)\n                .transpose(0, 1)\n            )\n\n        if saved_state is not None:\n            # saved states are stored with shape (bsz, num_heads_partition, seq_len, head_dim)\n            if ""prev_key"" in saved_state:\n                _prev_key = saved_state[""prev_key""]\n                assert _prev_key is not None\n                prev_key = _prev_key.view(bsz * self.num_heads_partition, -1, self.head_dim)\n                if static_kv:\n                    k = prev_key\n                else:\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n            if ""prev_value"" in saved_state:\n                _prev_value = saved_state[""prev_value""]\n                assert _prev_value is not None\n                prev_value = _prev_value.view(bsz * self.num_heads_partition, -1, self.head_dim)\n                if static_kv:\n                    v = prev_value\n                else:\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n            prev_key_padding_mask: Optional[Tensor] = None\n            if ""prev_key_padding_mask"" in saved_state:\n                prev_key_padding_mask = saved_state[""prev_key_padding_mask""]\n            assert k is not None and v is not None\n            key_padding_mask = ModelParallelMultiheadAttention._append_prev_key_padding_mask(\n                key_padding_mask=key_padding_mask,\n                prev_key_padding_mask=prev_key_padding_mask,\n                batch_size=bsz,\n                src_len=k.size(1),\n                static_kv=static_kv,\n            )\n\n            saved_state[""prev_key""] = k.view(bsz, self.num_heads_partition, -1, self.head_dim)\n            saved_state[""prev_value""] = v.view(bsz, self.num_heads_partition, -1, self.head_dim)\n            saved_state[""prev_key_padding_mask""] = key_padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n        assert k is not None\n        src_len = k.size(1)\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            assert key_padding_mask.size(1) == src_len\n\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n        assert list(attn_weights.size()) == [bsz * self.num_heads_partition, tgt_len, src_len]\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(0)\n            attn_weights += attn_mask\n\n        if key_padding_mask is not None:\n            # don\'t attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads_partition, tgt_len, src_len)\n            attn_weights = attn_weights.masked_fill(\n                key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(""-inf"")\n            )\n            attn_weights = attn_weights.view(bsz * self.num_heads_partition, tgt_len, src_len)\n\n        attn_weights_float = utils.softmax(\n            attn_weights, dim=-1\n        )\n        attn_weights = attn_weights_float.type_as(attn_weights)\n\n        with get_cuda_rng_tracker().fork():\n            attn_probs = F.dropout(\n                attn_weights_float.type_as(attn_weights),\n                p=self.dropout,\n                training=self.training,\n            )\n\n        assert v is not None\n        attn = torch.bmm(attn_probs, v)\n        assert list(attn.size()) == [bsz * self.num_heads_partition, tgt_len, self.head_dim]\n        embed_dim_partition = embed_dim // self.model_parallel_size\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim_partition)\n        attn = self.out_proj(attn)\n        # return attn_weights None to keep the return type same as single gpu multihead attention\n        # This will be deprecated.\n        attn_weights: Optional[Tensor] = None\n\n        return attn, attn_weights\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n        key_padding_mask: Optional[Tensor],\n        prev_key_padding_mask: Optional[Tensor],\n        batch_size: int,\n        src_len: int,\n        static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n            )\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n\n            filler = torch.zeros(batch_size, src_len - prev_key_padding_mask.size(1))\n            if prev_key_padding_mask.is_cuda:\n                filler = filler.cuda()\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), filler.float()], dim=1\n            )\n        elif key_padding_mask is not None:\n            filler = torch.zeros(batch_size, src_len - key_padding_mask.size(1))\n            if key_padding_mask.is_cuda:\n                filler = filler.cuda()\n            new_key_padding_mask = torch.cat(\n                [filler.float(), key_padding_mask.float()], dim=1\n            )\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    def reorder_incremental_state(\n        self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order\n    ):\n        """"""Reorder buffered internal state (for incremental generation).""""""\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            for k in input_buffer.keys():\n                if input_buffer[k] is not None:\n                    input_buffer[k] = input_buffer[k].index_select(0, new_order)\n            incremental_state = self._set_input_buffer(incremental_state, input_buffer)\n        return incremental_state\n\n    def _get_input_buffer(\n        self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n    ) -> Dict[str, Optional[Tensor]]:\n        result = self.get_incremental_state(incremental_state, ""attn_state"")\n        if result is not None:\n            return result\n        else:\n            empty_result: Dict[str, Optional[Tensor]] = {}\n            return empty_result\n\n    def _set_input_buffer(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        buffer: Dict[str, Optional[Tensor]],\n    ):\n        return self.set_incremental_state(incremental_state, ""attn_state"", buffer)\n'"
fairseq/model_parallel/modules/transformer_layer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom fairseq.modules import (\n    TransformerEncoderLayer,\n    TransformerDecoderLayer,\n)\n\nfrom fairseq.model_parallel.modules import ModelParallelMultiheadAttention\n\ntry:\n    from fairseq.model_parallel.megatron.mpu import (\n        ColumnParallelLinear,\n        RowParallelLinear,\n    )\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\n\nclass ModelParallelTransformerEncoderLayer(TransformerEncoderLayer):\n    """"""Encoder layer block over multiple gpus.\n\n        See ""Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf"" for more details.\n    """"""\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        if q_noise > 0:\n            raise NotImplementedError\n        return ColumnParallelLinear(input_dim, output_dim, gather_output=False)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        if q_noise > 0:\n            raise NotImplementedError\n        return RowParallelLinear(input_dim, output_dim, input_is_parallel=True)\n\n    def build_self_attention(self, embed_dim, args, **unused_kwargs):\n        return ModelParallelMultiheadAttention(\n            embed_dim,\n            args.encoder_attention_heads,\n            dropout=args.attention_dropout,\n            self_attention=True,\n        )\n\n\nclass ModelParallelTransformerDecoderLayer(TransformerDecoderLayer):\n    """"""Decoder layer block.\n\n        See ""Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf"" for more details.\n    """"""\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        if q_noise > 0:\n            raise NotImplementedError\n        return ColumnParallelLinear(input_dim, output_dim, gather_output=False)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        if q_noise > 0:\n            raise NotImplementedError\n        return RowParallelLinear(input_dim, output_dim, input_is_parallel=True)\n\n    def build_self_attention(self, embed_dim, args, **unused_kwargs):\n        return ModelParallelMultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=args.decoder_attention_heads,\n            dropout=args.attention_dropout,\n            self_attention=not getattr(args, ""cross_self_attention"", False),\n        )\n\n    def build_encoder_attention(self, embed_dim, args, **unused_kwargs):\n        return ModelParallelMultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=args.decoder_attention_heads,\n            kdim=getattr(args, ""encoder_embed_dim"", None),\n            vdim=getattr(args, ""encoder_embed_dim"", None),\n            dropout=args.attention_dropout,\n            encoder_decoder_attention=True,\n        )\n'"
fairseq/model_parallel/modules/transformer_sentence_encoder.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.modules import (\n    LayerNorm,\n    MultiheadAttention,\n    PositionalEmbedding,\n    TransformerSentenceEncoder,\n)\n\nfrom fairseq.model_parallel.modules import (\n    ModelParallelTransformerSentenceEncoderLayer,\n)\n\ntry:\n    from fairseq.model_parallel.megatron.mpu import (\n        copy_to_model_parallel_region,\n        gather_from_model_parallel_region,\n        VocabParallelEmbedding,\n    )\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\nimport random\n\n\nclass ModelParallelTransformerSentenceEncoder(TransformerSentenceEncoder):\n    """"""\n    Implementation for a Model Parallel Bi-directional Transformer based\n    Sentence Encoder used in BERT/XLM style pre-trained models.\n    """"""\n    def build_embedding(self, vocab_size, embedding_dim, padding_idx):\n        return VocabParallelEmbedding(vocab_size, embedding_dim, padding_idx)\n\n    def build_transformer_sentence_encoder_layer(\n        self,\n        embedding_dim,\n        ffn_embedding_dim,\n        num_attention_heads,\n        dropout,\n        attention_dropout,\n        activation_dropout,\n        activation_fn,\n        export,\n        **unused,\n    ):\n        return ModelParallelTransformerSentenceEncoderLayer(\n            embedding_dim=embedding_dim,\n            ffn_embedding_dim=ffn_embedding_dim,\n            num_attention_heads=num_attention_heads,\n            dropout=dropout,\n            attention_dropout=attention_dropout,\n            activation_dropout=activation_dropout,\n            activation_fn=activation_fn,\n            export=export,\n        )\n'"
fairseq/model_parallel/modules/transformer_sentence_encoder_layer.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.modules import (\n    TransformerSentenceEncoderLayer\n)\nfrom fairseq.model_parallel.modules import ModelParallelMultiheadAttention\ntry:\n    from fairseq.model_parallel.megatron.mpu import (\n        ColumnParallelLinear,\n        RowParallelLinear,\n    )\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\n\nclass ModelParallelTransformerSentenceEncoderLayer(TransformerSentenceEncoderLayer):\n    """"""\n    Implements a Model Parallel Transformer Encoder Layer used in\n    BERT/XLM style pre-trained models.\n    """"""\n    def build_fc1(self, input_dim, output_dim, **unused):\n        return ColumnParallelLinear(input_dim, output_dim, gather_output=False)\n\n    def build_fc2(self, input_dim, output_dim, **unused):\n        return RowParallelLinear(input_dim, output_dim, input_is_parallel=True)\n\n    def build_self_attention(\n        self,\n        embed_dim,\n        num_attention_heads,\n        dropout,\n        **kwargs,\n    ):\n        return ModelParallelMultiheadAttention(\n            embed_dim,\n            num_attention_heads,\n            dropout=dropout,\n            self_attention=True\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        self_attn_mask: torch.Tensor = None,\n        self_attn_padding_mask: torch.Tensor = None,\n    ):\n        """"""\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer imlementation.\n        """"""\n        residual = x\n        x = self.self_attn_layer_norm(x)\n        x, attn = self.self_attn(\n            query=x,\n            key=x,\n            value=x,\n            key_padding_mask=self_attn_padding_mask,\n            need_weights=False,\n            attn_mask=self_attn_mask,\n        )\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n\n        residual = x\n        x = self.final_layer_norm(x)\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        return x, None\n'"
fairseq/models/bart/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .hub_interface import *  # noqa\nfrom .model import *  # noqa\n'"
fairseq/models/bart/hub_interface.py,10,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nimport logging\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import List\n\nfrom fairseq import utils\nfrom fairseq.data import encoders\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BARTHubInterface(nn.Module):\n    """"""A simple PyTorch Hub interface to BART.\n\n    Usage: https://github.com/pytorch/fairseq/tree/master/examples/BART\n    """"""\n\n    def __init__(self, args, task, model):\n        super().__init__()\n        self.args = args\n        self.task = task\n        self.model = model\n\n        self.bpe = encoders.build_bpe(args)\n\n        self.max_positions = min(utils.resolve_max_positions(\n            self.task.max_positions(),\n            self.model.max_positions(),\n        ))\n\n        # this is useful for determining the device\n        self.register_buffer(\'_float_tensor\', torch.tensor([0], dtype=torch.float))\n\n    @property\n    def device(self):\n        return self._float_tensor.device\n\n    def encode(self, sentence: str, *addl_sentences, no_separator=True) -> torch.LongTensor:\n        """"""\n        BPE-encode a sentence (or multiple sentences).\n\n        Every sequence begins with a beginning-of-sentence (`<s>`) symbol.\n        Every sentence ends with an end-of-sentence (`</s>`).\n\n        Example (single sentence): `<s> a b c </s>`\n        Example (sentence pair): `<s> d e f </s> 1 2 3 </s>`\n\n        The BPE encoding follows GPT-2. One subtle detail is that the GPT-2 BPE\n        requires leading spaces. For example::\n\n            >>> bart.encode(\'Hello world\').tolist()\n            [0, 31414, 232, 2]\n            >>> bart.encode(\' world\').tolist()\n            [0, 232, 2]\n            >>> bart.encode(\'world\').tolist()\n            [0, 8331, 2]\n        """"""\n        tokens = self.bpe.encode(sentence)\n        if len(tokens.split(\' \')) > self.max_positions - 2:\n            tokens = \' \'.join(tokens.split(\' \')[:self.max_positions - 2])\n        bpe_sentence = \'<s> \' + tokens + \' </s>\'\n        for s in addl_sentences:\n            bpe_sentence += (\' </s>\' if not no_separator else \'\')\n            bpe_sentence += \' \' + self.bpe.encode(s) + \' </s>\'\n        tokens = self.task.source_dictionary.encode_line(bpe_sentence, append_eos=False)\n        return tokens.long()\n\n    def decode(self, tokens: torch.LongTensor):\n        assert tokens.dim() == 1\n        tokens = tokens.cpu().numpy()\n        if tokens[0] == self.task.source_dictionary.bos():\n            tokens = tokens[1:]  # remove <s>\n        eos_mask = (tokens == self.task.source_dictionary.eos())\n        doc_mask = eos_mask[1:] & eos_mask[:-1]\n        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)\n        sentences = [self.bpe.decode(self.task.source_dictionary.string(s)) for s in sentences]\n        if len(sentences) == 1:\n            return sentences[0]\n        return sentences\n\n    def _build_sample(self, src_tokens: List[torch.LongTensor]):\n        # assert torch.is_tensor(src_tokens)\n        dataset = self.task.build_dataset_for_inference(\n            src_tokens,\n            [x.numel() for x in src_tokens],\n        )\n        sample = dataset.collater(dataset)\n        sample = utils.apply_to_sample(\n            lambda tensor: tensor.to(self.device),\n            sample\n        )\n        return sample\n\n    def sample(self, sentences: List[str], beam: int = 1, verbose: bool = False, **kwargs) -> str:\n        input = [self.encode(sentence) for sentence in sentences]\n        hypos = self.generate(input, beam, verbose, **kwargs)\n        return [self.decode(x[\'tokens\']) for x in hypos]\n\n    def generate(self, tokens: List[torch.LongTensor], beam: int = 5, verbose: bool = False, **kwargs) -> torch.LongTensor:\n        sample = self._build_sample(tokens)\n\n        # build generator using current args as well as any kwargs\n        gen_args = copy.copy(self.args)\n        gen_args.beam = beam\n        for k, v in kwargs.items():\n            setattr(gen_args, k, v)\n        generator = self.task.build_generator([self.model], gen_args)\n        translations = self.task.inference_step(\n            generator,\n            [self.model],\n            sample,\n            prefix_tokens=sample[\'net_input\'][\'src_tokens\'].new_zeros((len(tokens), 1)).fill_(self.task.source_dictionary.bos()),\n        )\n\n        if verbose:\n            src_str_with_unk = self.string(tokens)\n            logger.info(\'S\\t{}\'.format(src_str_with_unk))\n\n        def getarg(name, default):\n            return getattr(gen_args, name, getattr(self.args, name, default))\n\n        # Process top predictions\n        hypos = [x[0] for x in translations]\n        hypos = [v for _, v in sorted(zip(sample[\'id\'].tolist(), hypos))]\n        return hypos\n\n    def extract_features(self, tokens: torch.LongTensor, return_all_hiddens: bool = False) -> torch.Tensor:\n        if tokens.dim() == 1:\n            tokens = tokens.unsqueeze(0)\n        if tokens.size(-1) > min(self.model.max_positions()):\n            raise ValueError(\'tokens exceeds maximum length: {} > {}\'.format(\n                tokens.size(-1), self.model.max_positions()\n            ))\n        tokens.to(device=self.device),\n        prev_output_tokens = tokens.clone()\n\n        prev_output_tokens[:, 0] = tokens.gather(\n            1,\n            (tokens.ne(self.task.source_dictionary.pad()).sum(dim=1)- 1).unsqueeze(-1),\n        ).squeeze()\n\n        prev_output_tokens[:, 1:] = tokens[:, :-1]\n        features, extra = self.model(\n            src_tokens=tokens,\n            src_lengths=None,\n            prev_output_tokens=prev_output_tokens,\n            features_only=True,\n            return_all_hiddens=return_all_hiddens,\n        )\n        if return_all_hiddens:\n            # convert from T x B x C -> B x T x C\n            inner_states = extra[\'inner_states\']\n            return [inner_state.transpose(0, 1) for inner_state in inner_states]\n        else:\n            return features  # just the last layer\'s features\n\n    def register_classification_head(\n        self, name: str, num_classes: int = None, embedding_size: int = None, **kwargs\n    ):\n        self.model.register_classification_head(\n            name, num_classes=num_classes, embedding_size=embedding_size, **kwargs\n        )\n\n    def predict(self, head: str, tokens: torch.LongTensor, return_logits: bool = False):\n        if tokens.dim() == 1:\n            tokens = tokens.unsqueeze(0)\n        features = self.extract_features(tokens.to(device=self.device))\n        sentence_representation = features[\n            tokens.eq(self.task.source_dictionary.eos()), :\n        ].view(features.size(0), -1, features.size(-1))[:, -1, :]\n\n        logits = self.model.classification_heads[head](sentence_representation)\n        if return_logits:\n            return logits\n        return F.log_softmax(logits, dim=-1)\n'"
fairseq/models/bart/model.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nBART: Denoising Sequence-to-Sequence Pre-training for\nNatural Language Generation, Translation, and Comprehension\n""""""\n\nimport logging\n\nimport torch\nimport torch.nn as nn\n\nfrom fairseq import utils\nfrom fairseq.models import (\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.transformer import TransformerModel\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\n\nfrom .hub_interface import BARTHubInterface\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(\'bart\')\nclass BARTModel(TransformerModel):\n\n    @classmethod\n    def hub_models(cls):\n        return {\n            \'bart.base\': \'http://dl.fbaipublicfiles.com/fairseq/models/bart.base.tar.gz\',\n            \'bart.large\': \'http://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz\',\n            \'bart.large.mnli\': \'http://dl.fbaipublicfiles.com/fairseq/models/bart.large.mnli.tar.gz\',\n            \'bart.large.cnn\': \'http://dl.fbaipublicfiles.com/fairseq/models/bart.large.cnn.tar.gz\',\n            \'bart.large.xsum\': \'http://dl.fbaipublicfiles.com/fairseq/models/bart.large.xsum.tar.gz\',\n        }\n\n    def __init__(self, args, encoder, decoder):\n        super().__init__(args, encoder, decoder)\n\n        # We follow BERT\'s random weight initialization\n        self.apply(init_bert_params)\n\n        self.classification_heads = nn.ModuleDict()\n\n    @staticmethod\n    def add_args(parser):\n        super(BARTModel, BARTModel).add_args(parser)\n        parser.add_argument(\n            \'--pooler-dropout\', type=float, metavar=\'D\',\n            help=\'dropout probability in the masked_lm pooler layers\'\n        )\n        parser.add_argument(\n            \'--pooler-activation-fn\',\n            choices=utils.get_available_activation_fns(),\n            help=\'activation function to use for pooler layer\'\n        )\n\n    @property\n    def supported_targets(self):\n        return {\'self\'}\n\n    def forward(\n        self, src_tokens, src_lengths, prev_output_tokens,\n        features_only=False, classification_head_name=None, **kwargs\n    ):\n        if classification_head_name is not None:\n            features_only = True\n\n        encoder_out = self.encoder(\n            src_tokens,\n            src_lengths=src_lengths,\n            **kwargs,\n        )\n        x, extra = self.decoder(\n            prev_output_tokens,\n            encoder_out=encoder_out,\n            features_only=features_only,\n            **kwargs,\n        )\n\n        if classification_head_name is not None:\n            sentence_representation = x[\n                src_tokens.eq(self.encoder.dictionary.eos()), :\n            ].view(x.size(0), -1, x.size(-1))[:, -1, :]\n            x = self.classification_heads[classification_head_name](\n                sentence_representation\n            )\n        return x, extra\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_name_or_path,\n        checkpoint_file=\'model.pt\',\n        data_name_or_path=\'.\',\n        bpe=\'gpt2\',\n        **kwargs,\n    ):\n        from fairseq import hub_utils\n        x = hub_utils.from_pretrained(\n            model_name_or_path,\n            checkpoint_file,\n            data_name_or_path,\n            archive_map=cls.hub_models(),\n            bpe=bpe,\n            load_checkpoint_heads=True,\n            **kwargs,\n        )\n        return BARTHubInterface(x[\'args\'], x[\'task\'], x[\'models\'][0])\n\n    def register_classification_head(self, name, num_classes=None, inner_dim=None, **kwargs):\n        """"""Register a classification head.""""""\n        logger.info(""Registering classification head: {0}"".format(name))\n        if name in self.classification_heads:\n            prev_num_classes = self.classification_heads[name].out_proj.out_features\n            prev_inner_dim = self.classification_heads[name].dense.out_features\n            if num_classes != prev_num_classes or inner_dim != prev_inner_dim:\n                logger.warning(\n                    \'re-registering head ""{}"" with num_classes {} (prev: {}) \'\n                    \'and inner_dim {} (prev: {})\'.format(\n                        name, num_classes, prev_num_classes, inner_dim, prev_inner_dim\n                    )\n                )\n        self.classification_heads[name] = BARTClassificationHead(\n            self.args.encoder_embed_dim,\n            inner_dim or self.args.encoder_embed_dim,\n            num_classes,\n            self.args.pooler_activation_fn,\n            self.args.pooler_dropout,\n        )\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        super().upgrade_state_dict_named(state_dict, name)\n\n        prefix = name + \'.\' if name != \'\' else \'\'\n        current_head_names = [] if not hasattr(self, \'classification_heads\') else \\\n            self.classification_heads.keys()\n\n        # Handle new classification heads present in the state dict.\n        keys_to_delete = []\n        for k in state_dict.keys():\n            if not k.startswith(prefix + \'classification_heads.\'):\n                continue\n\n            head_name = k[len(prefix + \'classification_heads.\'):].split(\'.\')[0]\n            num_classes = state_dict[prefix + \'classification_heads.\' + head_name + \'.out_proj.weight\'].size(0)\n            inner_dim = state_dict[prefix + \'classification_heads.\' + head_name + \'.dense.weight\'].size(0)\n\n            if getattr(self.args, \'load_checkpoint_heads\', False):\n                if head_name not in current_head_names:\n                    self.register_classification_head(head_name, num_classes, inner_dim)\n            else:\n                if head_name not in current_head_names:\n                    logger.warning(\n                        \'deleting classification head ({}) from checkpoint \'\n                        \'not present in current model: {}\'.format(head_name, k)\n                    )\n                    keys_to_delete.append(k)\n                elif (\n                    num_classes != self.classification_heads[head_name].out_proj.out_features\n                    or inner_dim != self.classification_heads[head_name].dense.out_features\n                ):\n                    logger.warning(\n                        \'deleting classification head ({}) from checkpoint \'\n                        \'with different dimensions than current model: {}\'.format(head_name, k)\n                    )\n                    keys_to_delete.append(k)\n        for k in keys_to_delete:\n            del state_dict[k]\n\n        def truncate_emb(key):\n            if key in state_dict:\n                state_dict[key] = state_dict[key][:-1, :]\n\n        # When finetuning on translation task, remove last row of\n        # embedding matrix that corresponds to mask_idx token.\n        loaded_dict_size = state_dict[\'encoder.embed_tokens.weight\'].size(0)\n        if loaded_dict_size == len(self.encoder.dictionary) + 1 and \'<mask>\' not in self.encoder.dictionary:\n            truncate_emb(\'encoder.embed_tokens.weight\')\n            truncate_emb(\'decoder.embed_tokens.weight\')\n            truncate_emb(\'encoder.output_projection.weight\')\n            truncate_emb(\'decoder.output_projection.weight\')\n\n        # When continued pretraining on new set of languages for mbart,\n        # add extra lang embeddings at the end of embed_tokens.\n        # Note: newly added languages are assumed to have been added at the end.\n        if self.args.task == \'multilingual_denoising\' and loaded_dict_size < len(self.encoder.dictionary):\n            logger.info(\n                ""Adding extra language embeddings not found in pretrained model for ""\\\n                ""continued pretraining of MBART on new set of languages.""\n            )\n            loaded_mask_token_embedding = state_dict[\'encoder.embed_tokens.weight\'][-1, :]\n\n            num_langids_to_add = len(self.encoder.dictionary) - loaded_dict_size\n            embed_dim = state_dict[\'encoder.embed_tokens.weight\'].size(1)\n\n            new_lang_embed_to_add = torch.zeros(num_langids_to_add, embed_dim)\n            nn.init.normal_(\n                new_lang_embed_to_add,\n                mean=0,\n                std=embed_dim ** -0.5\n            )\n            new_lang_embed_to_add = new_lang_embed_to_add.to(\n                dtype=state_dict[\'encoder.embed_tokens.weight\'].dtype,\n            )\n\n            state_dict[\'encoder.embed_tokens.weight\'] = torch.cat([\n                state_dict[\'encoder.embed_tokens.weight\'][:loaded_dict_size-1, :],\n                new_lang_embed_to_add,\n                loaded_mask_token_embedding.unsqueeze(0)]\n            )\n            state_dict[\'decoder.embed_tokens.weight\'] = torch.cat([\n                state_dict[\'decoder.embed_tokens.weight\'][:loaded_dict_size-1, :],\n                new_lang_embed_to_add,\n                loaded_mask_token_embedding.unsqueeze(0)]\n            )\n\n        # Copy any newly-added classification heads into the state dict\n        # with their current weights.\n        if hasattr(self, \'classification_heads\'):\n            cur_state = self.classification_heads.state_dict()\n            for k, v in cur_state.items():\n                if prefix + \'classification_heads.\' + k not in state_dict:\n                    logger.info(\'Overwriting\', prefix + \'classification_heads.\' + k)\n                    state_dict[prefix + \'classification_heads.\' + k] = v\n\n\nclass BARTClassificationHead(nn.Module):\n    """"""Head for sentence-level classification tasks.""""""\n\n    def __init__(\n        self,\n        input_dim,\n        inner_dim,\n        num_classes,\n        activation_fn,\n        pooler_dropout,\n    ):\n        super().__init__()\n        self.dense = nn.Linear(input_dim, inner_dim)\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.dropout = nn.Dropout(p=pooler_dropout)\n        self.out_proj = nn.Linear(inner_dim, num_classes)\n\n    def forward(self, features, **kwargs):\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = self.activation_fn(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n\n@register_model_architecture(\'bart\', \'bart_large\')\ndef bart_large_architecture(args):\n    args.encoder_embed_path = getattr(args, \'encoder_embed_path\', None)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1024)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 4*1024)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 12)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 16)\n    args.encoder_normalize_before = getattr(args, \'encoder_normalize_before\', False)\n    args.encoder_learned_pos = getattr(args, \'encoder_learned_pos\', True)\n    args.decoder_embed_path = getattr(args, \'decoder_embed_path\', None)\n    args.decoder_embed_dim = getattr(args, \'decoder_embed_dim\', args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(args, \'decoder_ffn_embed_dim\', args.encoder_ffn_embed_dim)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 12)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 16)\n    args.decoder_normalize_before = getattr(args, \'decoder_normalize_before\', False)\n    args.decoder_learned_pos = getattr(args, \'decoder_learned_pos\', True)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.)\n    args.relu_dropout = getattr(args, \'relu_dropout\', 0.)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.max_target_positions = getattr(args, \'max_target_positions\', 1024)\n    args.max_source_positions = getattr(args, \'max_source_positions\', 1024)\n    args.adaptive_softmax_cutoff = getattr(args, \'adaptive_softmax_cutoff\', None)\n    args.adaptive_softmax_dropout = getattr(args, \'adaptive_softmax_dropout\', 0)\n    args.share_decoder_input_output_embed = getattr(args, \'share_decoder_input_output_embed\', True)\n    args.share_all_embeddings = getattr(args, \'share_all_embeddings\', True)\n\n    args.decoder_output_dim = getattr(args, \'decoder_output_dim\', args.decoder_embed_dim)\n    args.decoder_input_dim = getattr(args, \'decoder_input_dim\', args.decoder_embed_dim)\n\n    args.no_scale_embedding = getattr(args, \'no_scale_embedding\', True)\n    args.layernorm_embedding = getattr(args, \'layernorm_embedding\', True)\n\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    args.pooler_activation_fn = getattr(args, \'pooler_activation_fn\', \'tanh\')\n    args.pooler_dropout = getattr(args, \'pooler_dropout\', 0.0)\n\n\n@register_model_architecture(\'bart\', \'bart_base\')\ndef bart_base_architecture(args):\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 768)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 4*768)\n    args.encoder_layers = getattr(args, \'encoder_layers\', 6)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 12)\n    args.decoder_layers = getattr(args, \'decoder_layers\', 6)\n    args.decoder_attention_heads = getattr(args, \'decoder_attention_heads\', 12)\n    bart_large_architecture(args)\n\n\n@register_model_architecture(\'bart\', \'mbart_large\')\ndef mbart_large_architecture(args):\n    args.no_scale_embedding = getattr(args, \'no_scale_embedding\', False)\n    bart_large_architecture(args)\n\n\n@register_model_architecture(\'bart\', \'mbart_base\')\ndef mbart_base_architecture(args):\n    args.no_scale_embedding = getattr(args, \'no_scale_embedding\', False)\n    bart_base_architecture(args)\n\n\n@register_model_architecture(\'bart\', \'mbart_base_wmt20\')\ndef mbart_base_wmt20_architecture(args):\n    args.layernorm_embedding = getattr(args, \'layernorm_embedding\', False)\n    mbart_base_architecture(args)\n'"
fairseq/models/huggingface/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\n\n# automatically import any Python files in the models/huggingface/ directory\nmodels_dir = os.path.dirname(__file__)\nfor file in os.listdir(models_dir):\n    path = os.path.join(models_dir, file)\n    if (\n        not file.startswith('_')\n        and not file.startswith('.')\n        and (file.endswith('.py') or os.path.isdir(path))\n    ):\n        model_name = file[:file.find('.py')] if file.endswith('.py') else file\n        module = importlib.import_module('fairseq.models.huggingface.' + model_name)\n"""
fairseq/models/huggingface/hf_gpt2.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\nimport sys\nfrom typing import Dict, List, Optional\n\nimport torch\nfrom fairseq.models import (\n    FairseqIncrementalDecoder,\n    FairseqLanguageModel,\n    register_model,\n    register_model_architecture,\n)\n\ntry:\n    # Prepend the transformers submodule to the path, so that\n    # it\'s prioritized over other installations. This allows\n    # making local changes in the submodule.\n    sys.path.insert(\n        0, os.path.join(os.path.dirname(__file__), \'transformers\', \'src\')\n    )\n    from transformers import AutoModel, GPT2Config, GPT2LMHeadModel\n    has_hf = True\nexcept ImportError:\n    has_hf = False\n\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\n\n@register_model(\'hf_gpt2\')\nclass HuggingFaceGPT2LanguageModel(FairseqLanguageModel):\n\n    def __init__(self, decoder):\n        super().__init__(decoder)\n        if not has_hf:\n            raise ImportError(\n                \'\\n\\nPlease install huggingface/transformers with:\'\n                \'\\n\\n  pip install transformers\'\n                \'\\n\\nOr to make local edits, install the submodule:\'\n                \'\\n\\n  git submodule update --init \'\n                \'fairseq/models/huggingface/transformers\'\n            )\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--embed-dim\', type=int, metavar=\'N\',\n                            help=\'embedding dimension\')\n        parser.add_argument(\'--num-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num attention heads\')\n        parser.add_argument(\'--num-layers\', type=int, metavar=\'N\',\n                            help=\'num layers\')\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for all fully connected layers \'\n                                 \'in the embeddings, encoder, and pooler\')\n        parser.add_argument(\'--attention-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for attention weights\')\n        # fmt: on\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        default_architecture(args)\n        return cls(HuggingFaceGPT2Decoder(args, task))\n\n\nclass HuggingFaceGPT2Decoder(FairseqIncrementalDecoder):\n\n    def __init__(self, args, task):\n        super().__init__(task.target_dictionary)\n\n        try:\n            # Prepend the transformers submodule to the path, so that\n            # it\'s prioritized over other installations. This allows\n            # making local changes in the submodule.\n            sys.path.insert(\n                0, os.path.join(os.path.dirname(__file__), \'transformers\', \'src\')\n            )\n            from transformers import GPT2Config, GPT2LMHeadModel\n        except ImportError:\n            raise ImportError(\n                \'\\n\\nPlease install huggingface/transformers with:\'\n                \'\\n\\n  pip install transformers\'\n                \'\\n\\nOr to make local edits, install the submodule:\'\n                \'\\n\\n  git submodule update --init \'\n                \'fairseq/models/huggingface/transformers\'\n            )\n\n        config = GPT2Config(\n            vocab_size=len(task.target_dictionary),\n            n_positions=args.max_target_positions + 1,\n            n_ctx=args.max_target_positions,\n            n_embd=args.embed_dim,\n            n_layer=args.num_layers,\n            n_head=args.num_attention_heads,\n            resid_pdrop=args.dropout,\n            embd_pdrop=args.dropout,\n            attn_pdrop=args.attention_dropout,\n            layer_norm_epsilon=1e-6,\n        )\n        self.model = GPT2LMHeadModel(config)\n\n        # set zero embedding for padding symbol\n        self.pad_idx = task.target_dictionary.pad()\n        self.model.transformer.wte.weight.data[self.pad_idx].zero_()\n        self.model.transformer.wpe.weight.data[0].zero_()\n\n    def forward(\n        self,\n        prev_output_tokens,\n        src_lengths=None,\n        incremental_state: Optional[Dict[str, List[torch.Tensor]]] = None,\n        encoder_out=None,\n    ):\n        features = self.extract_features(prev_output_tokens, incremental_state)\n        lm_logits = self.model.lm_head(features)\n        return (lm_logits, )\n\n    def extract_features(\n        self,\n        prev_output_tokens,\n        incremental_state: Optional[Dict[str, List[torch.Tensor]]] = None,\n    ):\n        if incremental_state:\n            past = self.get_incremental_state(""past"")\n        else:\n            past = None\n\n        # don\'t attend to padding symbols\n        attention_mask = prev_output_tokens.ne(self.pad_idx).int()\n\n        # set position ids to exclude padding symbols\n        position_ids = attention_mask * (\n            torch.arange(1, 1 + prev_output_tokens.size(1))\n            .to(prev_output_tokens)\n            .repeat(prev_output_tokens.size(0), 1)\n        )\n\n        outputs = self.model.transformer(\n            input_ids=prev_output_tokens,\n            past=past,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n        )\n        last_hidden_states = outputs[0]\n\n        if incremental_state:\n            self.set_incremental_state(incremental_state, ""past"", outputs[1])\n\n        return last_hidden_states\n\n    def max_positions(self):\n        return self.model.config.n_positions - 1\n\n\n@register_model_architecture(\'hf_gpt2\', \'hf_gpt2\')\ndef default_architecture(args):\n    if getattr(args, \'max_target_positions\', None) is None:\n        args.max_target_positions = getattr(\n            args, \'tokens_per_sample\', DEFAULT_MAX_TARGET_POSITIONS\n        )\n    args.embed_dim = getattr(args, \'embed_dim\', 768)\n    args.num_attention_heads = getattr(args, \'num_attention_heads\', 12)\n    args.num_layers = getattr(args, \'num_layers\', 12)\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n\n\n@register_model_architecture(\'hf_gpt2\', \'hf_gpt2_medium\')\ndef hf_gpt2_medium(args):\n    args.embed_dim = getattr(args, \'embed_dim\', 1024)\n    args.num_attention_heads = getattr(args, \'num_attention_heads\', 16)\n    args.num_layers = getattr(args, \'num_layers\', 24)\n    default_architecture(args)\n\n\n@register_model_architecture(\'hf_gpt2\', \'hf_gpt2_large\')\ndef hf_gpt2_large(args):\n    args.embed_dim = getattr(args, \'embed_dim\', 1280)\n    args.num_attention_heads = getattr(args, \'num_attention_heads\', 20)\n    args.num_layers = getattr(args, \'num_layers\', 36)\n    default_architecture(args)\n\n\n@register_model_architecture(\'hf_gpt2\', \'hf_gpt2_xl\')\ndef hf_gpt2_xl(args):\n    args.embed_dim = getattr(args, \'embed_dim\', 1600)\n    args.num_attention_heads = getattr(args, \'num_attention_heads\', 25)\n    args.num_layers = getattr(args, \'num_layers\', 48)\n    default_architecture(args)\n'"
fairseq/models/nat/__init__.py,0,b'from .fairseq_nat_model import *\nfrom .nonautoregressive_transformer import *\nfrom .nat_crf_transformer import *\nfrom .iterative_nonautoregressive_transformer import *\nfrom .cmlm_transformer import *\nfrom .levenshtein_transformer import *\nfrom .insertion_transformer import *\n'
fairseq/models/nat/cmlm_transformer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n""""""\nThis file implements:\nGhazvininejad, Marjan, et al.\n""Constant-time machine translation with conditional masked language models.""\narXiv preprint arXiv:1904.09324 (2019).\n""""""\n\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.nat import NATransformerModel\nfrom fairseq.utils import new_arange\n\n\ndef _skeptical_unmasking(output_scores, output_masks, p):\n    sorted_index = output_scores.sort(-1)[1]\n    boundary_len = (\n        (output_masks.sum(1, keepdim=True).type_as(output_scores) - 2) * p\n    ).long()\n    skeptical_mask = new_arange(output_masks) < boundary_len\n    return skeptical_mask.scatter(1, sorted_index, skeptical_mask)\n\n\n@register_model(""cmlm_transformer"")\nclass CMLMNATransformerModel(NATransformerModel):\n    @staticmethod\n    def add_args(parser):\n        NATransformerModel.add_args(parser)\n\n    def forward(\n        self, src_tokens, src_lengths, prev_output_tokens, tgt_tokens, **kwargs\n    ):\n        assert not self.decoder.src_embedding_copy, ""do not support embedding copy.""\n\n        # encoding\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n        # length prediction\n        length_out = self.decoder.forward_length(normalize=False, encoder_out=encoder_out)\n        length_tgt = self.decoder.forward_length_prediction(length_out, encoder_out, tgt_tokens)\n\n        # decoding\n        word_ins_out = self.decoder(\n            normalize=False,\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out)\n        word_ins_mask = prev_output_tokens.eq(self.unk)\n\n        return {\n            ""word_ins"": {\n                ""out"": word_ins_out, ""tgt"": tgt_tokens,\n                ""mask"": word_ins_mask, ""ls"": self.args.label_smoothing,\n                ""nll_loss"": True\n            },\n            ""length"": {\n                ""out"": length_out, ""tgt"": length_tgt,\n                ""factor"": self.decoder.length_loss_factor\n            }\n        }\n\n    def forward_decoder(self, decoder_out, encoder_out, decoding_format=None, **kwargs):\n\n        step = decoder_out.step\n        max_step = decoder_out.max_step\n\n        output_tokens = decoder_out.output_tokens\n        output_scores = decoder_out.output_scores\n        history = decoder_out.history\n\n        # execute the decoder\n        output_masks = output_tokens.eq(self.unk)\n        _scores, _tokens = self.decoder(\n            normalize=True,\n            prev_output_tokens=output_tokens,\n            encoder_out=encoder_out,\n        ).max(-1)\n        output_tokens.masked_scatter_(output_masks, _tokens[output_masks])\n        output_scores.masked_scatter_(output_masks, _scores[output_masks])\n\n        if history is not None:\n            history.append(output_tokens.clone())\n\n        # skeptical decoding (depend on the maximum decoding steps.)\n        if (step + 1) < max_step:\n            skeptical_mask = _skeptical_unmasking(\n                output_scores, output_tokens.ne(self.pad), 1 - (step + 1) / max_step\n            )\n\n            output_tokens.masked_fill_(skeptical_mask, self.unk)\n            output_scores.masked_fill_(skeptical_mask, 0.0)\n\n            if history is not None:\n                history.append(output_tokens.clone())\n\n        return decoder_out._replace(\n            output_tokens=output_tokens,\n            output_scores=output_scores,\n            attn=None,\n            history=history\n        )\n\n\n@register_model_architecture(""cmlm_transformer"", ""cmlm_transformer"")\ndef cmlm_base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.0)\n    args.activation_fn = getattr(args, ""activation_fn"", ""relu"")\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", True)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.adaptive_input = getattr(args, ""adaptive_input"", False)\n    args.apply_bert_init = getattr(args, ""apply_bert_init"", False)\n\n    args.decoder_output_dim = getattr(\n        args, ""decoder_output_dim"", args.decoder_embed_dim\n    )\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n\n    # --- special arguments ---\n    args.sg_length_pred = getattr(args, ""sg_length_pred"", False)\n    args.pred_length_offset = getattr(args, ""pred_length_offset"", False)\n    args.length_loss_factor = getattr(args, ""length_loss_factor"", 0.1)\n    args.ngram_predictor = getattr(args, ""ngram_predictor"", 1)\n    args.src_embedding_copy = getattr(args, ""src_embedding_copy"", False)\n\n\n@register_model_architecture(""cmlm_transformer"", ""cmlm_transformer_wmt_en_de"")\ndef cmlm_wmt_en_de(args):\n    cmlm_base_architecture(args)\n'"
fairseq/models/nat/fairseq_nat_model.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nimport torch\n\nfrom fairseq.models.transformer import TransformerModel, TransformerEncoder, TransformerDecoder\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\n\n\ndef ensemble_encoder(func):\n    def wrapper(self, *args, **kwargs):\n        if self.ensemble_models is None or len(self.ensemble_models) == 1:\n            return func(self, *args, **kwargs)\n        encoder_outs = [func(model, *args, **kwargs) for model in self.ensemble_models]\n        _encoder_out = encoder_outs[0]\n\n        def stack(key):\n            outs = [getattr(e, key) for e in encoder_outs]\n            return torch.stack(outs, -1) if outs[0] is not None else None\n\n        return _encoder_out._replace(\n            encoder_out=stack(\'encoder_out\'),\n            encoder_embedding=stack(\'encoder_embedding\'),\n            encoder_states=stack(\'encoder_states\')\n        )\n    return wrapper\n\n\ndef ensemble_decoder(func):\n    def wrapper(self, normalize=False, encoder_out=None, *args, **kwargs):\n        if self.ensemble_models is None or len(self.ensemble_models) == 1:\n            return func(self, normalize=normalize, encoder_out=encoder_out, *args, **kwargs)\n\n        action_outs = [\n            func(model, normalize=normalize, encoder_out=encoder_out._replace(\n                encoder_out=encoder_out.encoder_out[:, :, :, i]\n            ), *args, **kwargs)\n            for i, model in enumerate(self.ensemble_models)\n        ]\n\n        if not isinstance(action_outs[0], tuple):  # return multiple values\n            action_outs = [[a] for a in action_outs]\n        else:\n            action_outs = [list(a) for a in action_outs]\n\n        ensembled_outs = []\n        for i in range(len(action_outs[0])):\n            if i == 0 and normalize:\n                ensembled_outs += [\n                    torch.logsumexp(\n                        torch.stack([a[i] for a in action_outs], -1),\n                        dim=-1) - math.log(len(self.ensemble_models))\n                ]\n            elif action_outs[0][i] is not None:\n                ensembled_outs += [\n                    torch.stack([a[i] for a in action_outs], -1)\n                ]\n            else:\n                ensembled_outs += [None]\n\n        if len(ensembled_outs) == 1:\n            return ensembled_outs[0]\n        return tuple(ensembled_outs)\n    return wrapper\n\n\nclass FairseqNATModel(TransformerModel):\n    """"""\n    Abstract class for all nonautoregressive-based models\n    """"""\n    def __init__(self, args, encoder, decoder):\n        super().__init__(args, encoder, decoder)\n        self.tgt_dict = decoder.dictionary\n        self.bos = decoder.dictionary.bos()\n        self.eos = decoder.dictionary.eos()\n        self.pad = decoder.dictionary.pad()\n        self.unk = decoder.dictionary.unk()\n\n        self.ensemble_models = None\n\n    @property\n    def allow_length_beam(self):\n        return False\n\n    @property\n    def allow_ensemble(self):\n        return True\n\n    def enable_ensemble(self, models):\n        self.encoder.ensemble_models = [m.encoder for m in models]\n        self.decoder.ensemble_models = [m.decoder for m in models]\n\n    @staticmethod\n    def add_args(parser):\n        TransformerModel.add_args(parser)\n        parser.add_argument(\n            ""--apply-bert-init"",\n            action=""store_true"",\n            help=""use custom param initialization for BERT"",\n        )\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        decoder = FairseqNATDecoder(args, tgt_dict, embed_tokens)\n        if getattr(args, ""apply_bert_init"", False):\n            decoder.apply(init_bert_params)\n        return decoder\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        encoder = FairseqNATEncoder(args, src_dict, embed_tokens)\n        if getattr(args, ""apply_bert_init"", False):\n            encoder.apply(init_bert_params)\n        return encoder\n\n    def forward_encoder(self, encoder_inputs):\n        return self.encoder(*encoder_inputs)\n\n    def forward_decoder(self, *args, **kwargs):\n        return NotImplementedError\n\n    def initialize_output_tokens(self, *args, **kwargs):\n        return NotImplementedError\n\n    def forward(self, *args, **kwargs):\n        return NotImplementedError\n\n\nclass FairseqNATEncoder(TransformerEncoder):\n    def __init__(self, args, dictionary, embed_tokens):\n        super().__init__(args, dictionary, embed_tokens)\n        self.ensemble_models = None\n\n    @ensemble_encoder\n    def forward(self, *args, **kwargs):\n        return super().forward(*args, **kwargs)\n\n\nclass FairseqNATDecoder(TransformerDecoder):\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n        super().__init__(args, dictionary, embed_tokens, no_encoder_attn)\n        self.ensemble_models = None\n'"
fairseq/models/nat/insertion_transformer.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.nat import (\n    LevenshteinTransformerDecoder,\n    LevenshteinTransformerModel,\n    FairseqNATModel,\n    ensemble_decoder\n)\nfrom fairseq.models.transformer import Linear\nfrom fairseq.utils import new_arange\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\n\n\nclass NegativeDistanceScore(object):\n    def __init__(self):\n\n        # pre-compute some values\n        self.scores = {}\n\n        self.scores[0.5] = self.compute_score_full(50, 0.5)\n        self.scores[1.0] = self.compute_score_full(50, 1.0)\n        self.scores[2.0] = self.compute_score_full(50, 2.0)\n\n    def __call__(self, i, L, tau):\n        if (tau is None) or (tau > 1000):\n            return 1 / L\n\n        if tau in self.scores:\n            if L < self.scores[tau].shape[0]:\n                return self.scores[tau][L - 1, i]\n        return self.compute_score(L, tau)[i]\n\n    def compute_score(self, L, tau):\n        s = np.array([-abs(L / 2 - i) / tau for i in range(L)])\n        s = np.exp(s - s.max())\n        return s / s.sum()\n\n    def compute_score_full(self, L, tau):\n        s = -abs(np.arange(0, L - 1)[:, None] / 2 - np.arange(L)[None, :]) / tau\n        s = np.tril(s, 0) + np.triu(s - float(""inf""), 1)\n        s = np.exp(s - s.max(1, keepdims=True))\n        return s / s.sum(1, keepdims=True)\n\n\nneg_scorer = NegativeDistanceScore()\n\n\ndef _get_ins_targets(in_tokens, out_tokens, padding_idx, unk_idx, vocab_size, tau=None):\n    try:\n        from fairseq import libnat\n    except ImportError as e:\n        import sys\n        sys.stderr.write(\'ERROR: missing libnat. run `pip install --editable .`\\n\')\n        raise e\n\n    B = in_tokens.size(0)\n    T = in_tokens.size(1)\n    V = vocab_size\n\n    with torch.cuda.device_of(in_tokens):\n        in_tokens_list = [\n            [t for t in s if t != padding_idx] for i, s in enumerate(in_tokens.tolist())\n        ]\n        out_tokens_list = [\n            [t for t in s if t != padding_idx]\n            for i, s in enumerate(out_tokens.tolist())\n        ]\n\n    full_labels = libnat.suggested_ed2_path(\n        in_tokens_list, out_tokens_list, padding_idx\n    )\n    insert_labels = [a[:-1] for a in full_labels]\n\n    # numericalize1\n    insert_label_tensors = in_tokens.new_zeros(B * (T - 1) * V).float()\n    insert_index, insert_labels = zip(\n        *[\n            (w + (j + i * (T - 1)) * V, neg_scorer(k, len(label), tau))\n            for i, labels in enumerate(insert_labels)\n            for j, label in enumerate(labels[1:-1])\n            for k, w in enumerate(label)\n        ]\n    )  # HACK 1:-1\n    insert_index, insert_labels = [\n        torch.tensor(list(a), device=in_tokens.device)\n        for a in [insert_index, insert_labels]\n    ]\n    insert_label_tensors.scatter_(0, insert_index.long(), insert_labels)\n    insert_label_tensors = insert_label_tensors.view(B, T - 1, V)\n\n    return insert_label_tensors\n\n\ndef _apply_ins_words(in_tokens, in_scores, word_ins_pred, word_ins_scores, padding_idx):\n\n    padding_masks = in_tokens[:, 1:].eq(padding_idx)\n    word_ins_scores.masked_fill_(padding_masks, 0.0)\n    word_ins_pred.masked_fill_(padding_masks, padding_idx)\n\n    in_coords = new_arange(in_tokens).type_as(in_scores)\n\n    # shift all padding predictions to infinite\n    out_coords = (in_coords[:, 1:] - 0.5).masked_fill(\n        word_ins_pred.eq(padding_idx), float(""inf"")\n    )\n    out_coords = torch.cat([in_coords, out_coords], 1).sort(-1)[1]\n    out_tokens = torch.cat([in_tokens, word_ins_pred], 1).gather(1, out_coords)\n    out_scores = torch.cat([in_scores, word_ins_scores], 1).gather(1, out_coords)\n    return out_tokens, out_scores\n\n\n@register_model(""insertion_transformer"")\nclass InsertionTransformerModel(LevenshteinTransformerModel):\n    def __init__(self, args, encoder, decoder):\n        super().__init__(args, encoder, decoder)\n\n    @staticmethod\n    def add_args(parser):\n        FairseqNATModel.add_args(parser)\n        parser.add_argument(""--label-tau"", default=None, type=float)\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        decoder = InsertionTransformerDecoder(args, tgt_dict, embed_tokens)\n        if getattr(args, ""apply_bert_init"", False):\n            decoder.apply(init_bert_params)\n        return decoder\n\n    def forward(\n        self, src_tokens, src_lengths, prev_output_tokens, tgt_tokens, **kwargs\n    ):\n\n        assert tgt_tokens is not None, ""forward function only supports training.""\n\n        # encoding\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n\n        # generate training labels for insertion\n        word_ins_out = self.decoder.forward_word_ins(\n            normalize=False,\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out\n        )\n\n        word_ins_tgt = _get_ins_targets(\n            prev_output_tokens,\n            tgt_tokens,\n            self.pad,\n            self.unk,\n            len(self.tgt_dict),\n            tau=self.decoder.label_tau,\n        ).type_as(word_ins_out)\n        word_ins_masks = prev_output_tokens[:, 1:].ne(self.pad)\n\n        return {\n            ""word_ins"": {\n                ""out"": word_ins_out, ""tgt"": word_ins_tgt,\n                ""mask"": word_ins_masks, ""ls"": self.args.label_smoothing,\n                ""nll_loss"": True\n            }\n        }\n\n    def forward_decoder(\n        self, decoder_out, encoder_out, eos_penalty=0.0, max_ratio=None, **kwargs\n    ):\n\n        output_tokens = decoder_out.output_tokens\n        output_scores = decoder_out.output_scores\n        history = decoder_out.history\n\n        # TODO: decoding for InsertionTransformer\n        word_ins_score = self.decoder.forward_word_ins(\n            normalize=True,\n            prev_output_tokens=output_tokens,\n            encoder_out=encoder_out\n        )\n\n        if eos_penalty > 0.0:\n            word_ins_score[:, :, self.pad] -= eos_penalty\n        word_ins_score, word_ins_pred = word_ins_score.max(-1)\n        output_tokens, output_scores = _apply_ins_words(\n            output_tokens, output_scores, word_ins_pred, word_ins_score, self.pad\n        )\n\n        # delete some unnecessary paddings\n        cut_off = output_tokens.ne(self.pad).sum(1).max()\n        output_tokens = output_tokens[:, :cut_off]\n        output_scores = output_scores[:, :cut_off]\n\n        if history is not None:\n            history.append(output_tokens.clone())\n\n        return decoder_out._replace(\n            output_tokens=output_tokens,\n            output_scores=output_scores,\n            attn=None,\n            history=history\n        )\n\n\nclass InsertionTransformerDecoder(LevenshteinTransformerDecoder):\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n        # use the TransformerDecoder\'s __init__\n        super(LevenshteinTransformerDecoder, self).__init__(\n            args, dictionary, embed_tokens, no_encoder_attn=no_encoder_attn\n        )\n\n        self.dictionary = dictionary\n        self.bos = dictionary.bos()\n        self.unk = dictionary.unk()\n        self.eos = dictionary.eos()\n        self.pool_out = Linear(self.output_embed_dim * 2, self.output_embed_dim)\n\n        self.label_tau = getattr(args, ""label_tau"", None)\n\n    @ensemble_decoder\n    def forward_word_ins(self, normalize, encoder_out, prev_output_tokens):\n        features = self.extract_features(prev_output_tokens, encoder_out=encoder_out)[0]\n        features = self.pool_out(\n            torch.cat([features[:, :-1, :], features[:, 1:, :]], 2)\n        )\n        decoder_out = self.output_layer(features)\n        return F.log_softmax(decoder_out, -1) if normalize else decoder_out\n\n    def forward_mask_ins(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def forward_word_del(self, *args, **kwargs):\n        raise NotImplementedError\n\n\n@register_model_architecture(""insertion_transformer"", ""insertion_transformer"")\ndef insertion_base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.0)\n    args.activation_fn = getattr(args, ""activation_fn"", ""relu"")\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.adaptive_input = getattr(args, ""adaptive_input"", False)\n    args.apply_bert_init = getattr(args, ""apply_bert_init"", False)\n\n    args.decoder_output_dim = getattr(\n        args, ""decoder_output_dim"", args.decoder_embed_dim\n    )\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n\n    # special for insertion transformer\n    args.label_tau = getattr(args, ""label_tau"", None)\n'"
fairseq/models/nat/iterative_nonautoregressive_transformer.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.nat import NATransformerModel\n\n\ndef _sequential_poisoning(s, V, beta=0.33, bos=2, eos=3, pad=1):\n    # s: input batch\n    # V: vocabulary size\n    rand_words = torch.randint(low=4, high=V, size=s.size(), device=s.device)\n    choices = torch.rand(size=s.size(), device=s.device)\n    choices.masked_fill_((s == pad) | (s == bos) | (s == eos), 1)\n\n    replace = choices < beta / 3\n    repeat = (choices >= beta / 3) & (choices < beta * 2 / 3)\n    swap = (choices >= beta * 2 / 3) & (choices < beta)\n    safe = choices >= beta\n\n    for i in range(s.size(1) - 1):\n        rand_word = rand_words[:, i]\n        next_word = s[:, i + 1]\n        self_word = s[:, i]\n\n        replace_i = replace[:, i]\n        swap_i = swap[:, i] & (next_word != 3)\n        repeat_i = repeat[:, i] & (next_word != 3)\n        safe_i = safe[:, i] | ((next_word == 3) & (~replace_i))\n\n        s[:, i] = (\n            self_word * (safe_i | repeat_i).long()\n            + next_word * swap_i.long()\n            + rand_word * replace_i.long()\n        )\n        s[:, i + 1] = (\n            next_word * (safe_i | replace_i).long()\n            + self_word * (swap_i | repeat_i).long()\n        )\n    return s\n\n\ndef gumbel_noise(input, TINY=1e-8):\n    return input.new_zeros(*input.size()).uniform_().add_(\n        TINY).log_().neg_().add_(TINY).log_().neg_()\n\n\n@register_model(""iterative_nonautoregressive_transformer"")\nclass IterNATransformerModel(NATransformerModel):\n    @staticmethod\n    def add_args(parser):\n        NATransformerModel.add_args(parser)\n        parser.add_argument(""--train-step"", type=int,\n                            help=""number of refinement iterations during training"")\n        parser.add_argument(""--dae-ratio"", type=float,\n                            help=""the probability of switching to the denoising auto-encoder loss"")\n        parser.add_argument(""--stochastic-approx"", action=""store_true"",\n                            help=""sampling from the decoder as the inputs for next iteration"")\n\n    @classmethod\n    def build_model(cls, args, task):\n        model = super().build_model(args, task)\n        model.train_step = getattr(args, ""train_step"", 4)\n        model.dae_ratio = getattr(args, ""dae_ratio"", 0.5)\n        model.stochastic_approx = getattr(args, ""stochastic_approx"", False)\n        return model\n\n    def forward(\n        self, src_tokens, src_lengths, prev_output_tokens, tgt_tokens, **kwargs\n    ):\n\n        B, T = prev_output_tokens.size()\n\n        # encoding\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n\n        # length prediction\n        length_out = self.decoder.forward_length(normalize=False, encoder_out=encoder_out)\n        length_tgt = self.decoder.forward_length_prediction(length_out, encoder_out, tgt_tokens)\n\n        # decoding\n        word_ins_outs, word_ins_tgts, word_ins_masks = [], [], []\n        for t in range(self.train_step):\n            word_ins_out = self.decoder(\n                normalize=False, \n                prev_output_tokens=prev_output_tokens,\n                encoder_out=encoder_out,\n                step=t,\n            )\n            word_ins_tgt = tgt_tokens\n            word_ins_mask = word_ins_tgt.ne(self.pad)\n\n            word_ins_outs.append(word_ins_out)\n            word_ins_tgts.append(word_ins_tgt)\n            word_ins_masks.append(word_ins_mask)\n\n            if t < (self.train_step - 1):\n                # prediction for next iteration\n                if self.stochastic_approx:\n                    word_ins_prediction = (\n                        word_ins_out + gumbel_noise(word_ins_out)\n                    ).max(-1)[1]\n                else:\n                    word_ins_prediction = word_ins_out.max(-1)[1]\n\n                prev_output_tokens = prev_output_tokens.masked_scatter(\n                    word_ins_mask, word_ins_prediction[word_ins_mask]\n                )\n\n                if self.dae_ratio > 0:\n                    # we do not perform denoising for the first iteration\n                    corrputed = (\n                        torch.rand(size=(B,), device=prev_output_tokens.device)\n                        < self.dae_ratio\n                    )\n                    corrputed_tokens = _sequential_poisoning(\n                        tgt_tokens[corrputed],\n                        len(self.tgt_dict),\n                        0.33,\n                        self.bos,\n                        self.eos,\n                        self.pad,\n                    )\n                    prev_output_tokens[corrputed] = corrputed_tokens\n\n        # concat everything\n        word_ins_out = torch.cat(word_ins_outs, 0)\n        word_ins_tgt = torch.cat(word_ins_tgts, 0)\n        word_ins_mask = torch.cat(word_ins_masks, 0)\n\n        return {\n            ""word_ins"": {\n                ""out"": word_ins_out, ""tgt"": word_ins_tgt,\n                ""mask"": word_ins_mask, ""ls"": self.args.label_smoothing,\n                ""nll_loss"": True\n            },\n            ""length"": {\n                ""out"": length_out, ""tgt"": length_tgt,\n                ""factor"": self.decoder.length_loss_factor\n            }\n        }\n\n\n@register_model_architecture(\n    ""iterative_nonautoregressive_transformer"", ""iterative_nonautoregressive_transformer""\n)\ndef inat_base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.0)\n    args.activation_fn = getattr(args, ""activation_fn"", ""relu"")\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.adaptive_input = getattr(args, ""adaptive_input"", False)\n    args.apply_bert_init = getattr(args, ""apply_bert_init"", False)\n\n    args.decoder_output_dim = getattr(\n        args, ""decoder_output_dim"", args.decoder_embed_dim\n    )\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n\n    # --- special arguments ---\n    args.sg_length_pred = getattr(args, ""sg_length_pred"", False)\n    args.pred_length_offset = getattr(args, ""pred_length_offset"", False)\n    args.length_loss_factor = getattr(args, ""length_loss_factor"", 0.1)\n    args.ngram_predictor = getattr(args, ""ngram_predictor"", 1)\n    args.src_embedding_copy = getattr(args, ""src_embedding_copy"", False)\n\n    args.train_step = getattr(args, ""train_step"", 4)\n    args.dae_ratio = getattr(args, ""dae_ratio"", 0.5)\n    args.stochastic_approx = getattr(args, ""stochastic_approx"", False)\n\n\n@register_model_architecture(\n    ""iterative_nonautoregressive_transformer"",\n    ""iterative_nonautoregressive_transformer_wmt_en_de"",\n)\ndef iter_nat_wmt_en_de(args):\n    inat_base_architecture(args)\n'"
fairseq/models/nat/levenshtein_transformer.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq.iterative_refinement_generator import DecoderOut\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.transformer import (\n    Embedding,\n    TransformerDecoderLayer\n)\n\nfrom fairseq.models.nat import (\n    FairseqNATModel,\n    FairseqNATDecoder,\n    ensemble_decoder\n)\n\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\n\n\nfrom .levenshtein_utils import (\n    _skip, _skip_encoder_out, _fill,\n    _get_ins_targets, _get_del_targets,\n    _apply_ins_masks, _apply_ins_words, _apply_del_words\n)\n\n\n@register_model(""levenshtein_transformer"")\nclass LevenshteinTransformerModel(FairseqNATModel):\n\n    @property\n    def allow_length_beam(self):\n        return False\n\n    @staticmethod\n    def add_args(parser):\n        FairseqNATModel.add_args(parser)\n        parser.add_argument(\n            ""--early-exit"",\n            default=""6,6,6"",\n            type=str,\n            help=""number of decoder layers before word_del, mask_ins, word_ins"",\n        )\n        parser.add_argument(\n            ""--no-share-discriminator"",\n            action=""store_true"",\n            help=""separate parameters for discriminator"",\n        )\n        parser.add_argument(\n            ""--no-share-maskpredictor"",\n            action=""store_true"",\n            help=""separate parameters for mask-predictor"",\n        )\n        parser.add_argument(\n            ""--share-discriminator-maskpredictor"",\n            action=""store_true"",\n            help=""share the parameters for both mask-predictor and discriminator"",\n        )\n        parser.add_argument(\n            ""--sampling-for-deletion"",\n            action=\'store_true\',\n            help=\'instead of argmax, use sampling to predict the tokens\'\n        )\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        decoder = LevenshteinTransformerDecoder(args, tgt_dict, embed_tokens)\n        if getattr(args, ""apply_bert_init"", False):\n            decoder.apply(init_bert_params)\n        return decoder\n\n    def forward(\n        self, src_tokens, src_lengths, prev_output_tokens, tgt_tokens, **kwargs\n    ):\n\n        assert tgt_tokens is not None, ""forward function only supports training.""\n\n        # encoding\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n\n        # generate training labels for insertion\n        masked_tgt_masks, masked_tgt_tokens, mask_ins_targets = _get_ins_targets(\n            prev_output_tokens, tgt_tokens, self.pad, self.unk\n        )\n        mask_ins_targets = mask_ins_targets.clamp(min=0, max=255)  # for safe prediction\n        mask_ins_masks = prev_output_tokens[:, 1:].ne(self.pad)\n\n        mask_ins_out, _ = self.decoder.forward_mask_ins(\n            normalize=False,\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out\n        )\n        word_ins_out, _ = self.decoder.forward_word_ins(\n            normalize=False,\n            prev_output_tokens=masked_tgt_tokens,\n            encoder_out=encoder_out\n        )\n\n        # make online prediction\n        if self.decoder.sampling_for_deletion:\n            word_predictions = torch.multinomial(\n                F.softmax(word_ins_out, -1).view(-1, word_ins_out.size(-1)), 1).view(\n                    word_ins_out.size(0), -1)\n        else:\n            word_predictions = F.log_softmax(word_ins_out, dim=-1).max(2)[1]\n\n        word_predictions.masked_scatter_(\n            ~masked_tgt_masks, tgt_tokens[~masked_tgt_masks]\n        )\n\n        # generate training labels for deletion\n        word_del_targets = _get_del_targets(word_predictions, tgt_tokens, self.pad)\n        word_del_out, _ = self.decoder.forward_word_del(\n            normalize=False,\n            prev_output_tokens=word_predictions,\n            encoder_out=encoder_out)\n        word_del_masks = word_predictions.ne(self.pad)\n\n        return {\n            ""mask_ins"": {\n                ""out"": mask_ins_out, ""tgt"": mask_ins_targets,\n                ""mask"": mask_ins_masks, ""ls"": 0.01,\n            },\n            ""word_ins"": {\n                ""out"": word_ins_out, ""tgt"": tgt_tokens,\n                ""mask"": masked_tgt_masks, ""ls"": self.args.label_smoothing,\n                ""nll_loss"": True\n            },\n            ""word_del"": {\n                ""out"": word_del_out, ""tgt"": word_del_targets,\n                ""mask"": word_del_masks\n            }\n        }\n\n    def forward_decoder(\n        self, decoder_out, encoder_out, eos_penalty=0.0, max_ratio=None, **kwargs\n    ):\n\n        output_tokens = decoder_out.output_tokens\n        output_scores = decoder_out.output_scores\n        attn = decoder_out.attn\n        history = decoder_out.history\n\n        bsz = output_tokens.size(0)\n        if max_ratio is None:\n            max_lens = torch.zeros_like(output_tokens).fill_(255)\n        else:\n            if encoder_out.encoder_padding_mask is None:\n                max_src_len = encoder_out.encoder_out.size(0)\n                src_lens = encoder_out.encoder_out.new(bsz).fill_(max_src_len)\n            else:\n                src_lens = (~encoder_out.encoder_padding_mask).sum(1)\n            max_lens = (src_lens * max_ratio).clamp(min=10).long()\n\n        # delete words\n        # do not delete tokens if it is <s> </s>\n        can_del_word = output_tokens.ne(self.pad).sum(1) > 2\n        if can_del_word.sum() != 0:  # we cannot delete, skip\n            word_del_score, word_del_attn = self.decoder.forward_word_del(\n                normalize=True,\n                prev_output_tokens=_skip(output_tokens, can_del_word),\n                encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_del_word)\n            )\n            word_del_pred = word_del_score.max(-1)[1].bool()\n\n            _tokens, _scores, _attn = _apply_del_words(\n                output_tokens[can_del_word],\n                output_scores[can_del_word],\n                word_del_attn,\n                word_del_pred,\n                self.pad,\n                self.bos,\n                self.eos,\n            )\n            output_tokens = _fill(output_tokens, can_del_word, _tokens, self.pad)\n            output_scores = _fill(output_scores, can_del_word, _scores, 0)\n            attn = _fill(attn, can_del_word, _attn, 0.)\n\n            if history is not None:\n                history.append(output_tokens.clone())\n\n        # insert placeholders\n        can_ins_mask = output_tokens.ne(self.pad).sum(1) < max_lens\n        if can_ins_mask.sum() != 0:\n            mask_ins_score, _ = self.decoder.forward_mask_ins(\n                normalize=True,\n                prev_output_tokens=_skip(output_tokens, can_ins_mask),\n                encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_ins_mask)\n            )\n            if eos_penalty > 0.0:\n                mask_ins_score[:, :, 0] = mask_ins_score[:, :, 0] - eos_penalty\n            mask_ins_pred = mask_ins_score.max(-1)[1]\n            mask_ins_pred = torch.min(\n                mask_ins_pred, max_lens[can_ins_mask, None].expand_as(mask_ins_pred)\n            )\n\n            _tokens, _scores = _apply_ins_masks(\n                output_tokens[can_ins_mask],\n                output_scores[can_ins_mask],\n                mask_ins_pred,\n                self.pad,\n                self.unk,\n                self.eos,\n            )\n            output_tokens = _fill(output_tokens, can_ins_mask, _tokens, self.pad)\n            output_scores = _fill(output_scores, can_ins_mask, _scores, 0)\n\n            if history is not None:\n                history.append(output_tokens.clone())\n\n        # insert words\n        can_ins_word = output_tokens.eq(self.unk).sum(1) > 0\n        if can_ins_word.sum() != 0:\n            word_ins_score, word_ins_attn = self.decoder.forward_word_ins(\n                normalize=True,\n                prev_output_tokens=_skip(output_tokens, can_ins_word),\n                encoder_out=_skip_encoder_out(self.encoder, encoder_out, can_ins_word)\n            )\n            word_ins_score, word_ins_pred = word_ins_score.max(-1)\n            _tokens, _scores = _apply_ins_words(\n                output_tokens[can_ins_word],\n                output_scores[can_ins_word],\n                word_ins_pred,\n                word_ins_score,\n                self.unk,\n            )\n\n            output_tokens = _fill(output_tokens, can_ins_word, _tokens, self.pad)\n            output_scores = _fill(output_scores, can_ins_word, _scores, 0)\n            attn = _fill(attn, can_ins_word, word_ins_attn, 0.)\n\n            if history is not None:\n                history.append(output_tokens.clone())\n\n        # delete some unnecessary paddings\n        cut_off = output_tokens.ne(self.pad).sum(1).max()\n        output_tokens = output_tokens[:, :cut_off]\n        output_scores = output_scores[:, :cut_off]\n        attn = None if attn is None else attn[:, :cut_off, :]\n\n        return decoder_out._replace(\n            output_tokens=output_tokens,\n            output_scores=output_scores,\n            attn=attn,\n            history=history\n        )\n\n    def initialize_output_tokens(self, encoder_out, src_tokens):\n        initial_output_tokens = src_tokens.new_zeros(src_tokens.size(0), 2)\n        initial_output_tokens[:, 0] = self.bos\n        initial_output_tokens[:, 1] = self.eos\n\n        initial_output_scores = initial_output_tokens.new_zeros(\n            *initial_output_tokens.size()\n        ).type_as(encoder_out.encoder_out)\n\n        return DecoderOut(\n            output_tokens=initial_output_tokens,\n            output_scores=initial_output_scores,\n            attn=None,\n            step=0,\n            max_step=0,\n            history=None\n        )\n\n\nclass LevenshteinTransformerDecoder(FairseqNATDecoder):\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n        super().__init__(\n            args, dictionary, embed_tokens, no_encoder_attn=no_encoder_attn\n        )\n        self.dictionary = dictionary\n        self.bos = dictionary.bos()\n        self.unk = dictionary.unk()\n        self.eos = dictionary.eos()\n        self.sampling_for_deletion = getattr(args, ""sampling_for_deletion"", False)\n        self.embed_mask_ins = Embedding(256, self.output_embed_dim * 2, None)\n        self.embed_word_del = Embedding(2, self.output_embed_dim, None)\n\n        # del_word, ins_mask, ins_word\n        self.early_exit = [int(i) for i in args.early_exit.split(\',\')]\n        assert len(self.early_exit) == 3\n\n        # copy layers for mask-predict/deletion\n        self.layers_msk = None\n        if getattr(args, ""no_share_maskpredictor"", False):\n            self.layers_msk = nn.ModuleList([\n                                TransformerDecoderLayer(args, no_encoder_attn)\n                                for _ in range(self.early_exit[1])\n                            ])\n        self.layers_del = None\n        if getattr(args, ""no_share_discriminator"", False):\n            self.layers_del = nn.ModuleList([\n                                TransformerDecoderLayer(args, no_encoder_attn)\n                                for _ in range(self.early_exit[0])\n                            ])\n\n        if getattr(args, ""share_discriminator_maskpredictor"", False):\n            assert getattr(args, ""no_share_discriminator"", False), ""must set saperate discriminator""\n            self.layers_msk = self.layers_del\n\n    def extract_features(\n        self, prev_output_tokens, encoder_out=None, early_exit=None, layers=None, **unused\n    ):\n        """"""\n        Similar to *forward* but only return features.\n        Inputs:\n            prev_output_tokens: Tensor(B, T)\n            encoder_out: a dictionary of hidden states and masks\n\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n            the LevenshteinTransformer decoder has full-attention to all generated tokens\n        """"""\n        # embed positions\n        positions = (\n            self.embed_positions(prev_output_tokens)\n            if self.embed_positions is not None\n            else None\n        )\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        if positions is not None:\n            x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        attn = None\n        inner_states = [x]\n\n        # decoder layers\n        decoder_padding_mask = prev_output_tokens.eq(self.padding_idx)\n        layers = self.layers if layers is None else layers\n        early_exit = len(layers) if early_exit is None else early_exit\n        for _, layer in enumerate(layers[: early_exit]):\n            x, attn, _ = layer(\n                x,\n                encoder_out.encoder_out if encoder_out is not None else None,\n                encoder_out.encoder_padding_mask if encoder_out is not None else None,\n                self_attn_mask=None,\n                self_attn_padding_mask=decoder_padding_mask,\n            )\n            inner_states.append(x)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        return x, {""attn"": attn, ""inner_states"": inner_states}\n\n    @ensemble_decoder\n    def forward_mask_ins(self, normalize, encoder_out, prev_output_tokens, **unused):\n        features, extra = self.extract_features(\n            prev_output_tokens, encoder_out=encoder_out, early_exit=self.early_exit[1], layers=self.layers_msk, **unused\n        )\n        features_cat = torch.cat([features[:, :-1, :], features[:, 1:, :]], 2)\n        decoder_out = F.linear(features_cat, self.embed_mask_ins.weight)\n        if normalize:\n            return F.log_softmax(decoder_out, -1), extra[\'attn\']\n        return decoder_out, extra[\'attn\']\n\n    @ensemble_decoder\n    def forward_word_ins(self, normalize, encoder_out, prev_output_tokens, **unused):\n        features, extra = self.extract_features(\n            prev_output_tokens, encoder_out=encoder_out, early_exit=self.early_exit[2], layers=self.layers, **unused\n        )\n        decoder_out = self.output_layer(features)\n        if normalize:\n            return F.log_softmax(decoder_out, -1), extra[\'attn\']\n        return decoder_out, extra[\'attn\']\n\n    @ensemble_decoder\n    def forward_word_del(self, normalize, encoder_out, prev_output_tokens, **unused):\n        features, extra = self.extract_features(\n            prev_output_tokens, encoder_out=encoder_out, early_exit=self.early_exit[0], layers=self.layers_del, **unused\n        )\n        decoder_out = F.linear(features, self.embed_word_del.weight)\n        if normalize:\n            return F.log_softmax(decoder_out, -1), extra[\'attn\']\n        return decoder_out, extra[\'attn\']\n\n\n@register_model_architecture(""levenshtein_transformer"", ""levenshtein_transformer"")\ndef levenshtein_base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.0)\n    args.activation_fn = getattr(args, ""activation_fn"", ""relu"")\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.adaptive_input = getattr(args, ""adaptive_input"", False)\n    args.apply_bert_init = getattr(args, ""apply_bert_init"", False)\n\n    args.decoder_output_dim = getattr(\n        args, ""decoder_output_dim"", args.decoder_embed_dim\n    )\n    args.sampling_for_deletion = getattr(args, ""sampling_for_deletion"", False)\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n    args.early_exit = getattr(args, ""early_exit"", ""6,6,6"")\n    args.no_share_discriminator = getattr(args, ""no_share_discriminator"", False)\n    args.no_share_maskpredictor = getattr(args, ""no_share_maskpredictor"", False)\n    args.share_discriminator_maskpredictor = getattr(args, ""share_discriminator_maskpredictor"", False)\n    args.no_share_last_layer = getattr(args, ""no_share_last_layer"", False)\n\n\n@register_model_architecture(\n    ""levenshtein_transformer"", ""levenshtein_transformer_wmt_en_de""\n)\ndef levenshtein_transformer_wmt_en_de(args):\n    levenshtein_base_architecture(args)\n\n\n# similar parameters used in the ""Attention Is All You Need"" paper (Vaswani et al., 2017)\n@register_model_architecture(\n    ""levenshtein_transformer"", ""levenshtein_transformer_vaswani_wmt_en_de_big""\n)\ndef levenshtein_transformer_vaswani_wmt_en_de_big(args):\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 1024)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 4096)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 16)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", 1024)\n    args.decoder_ffn_embed_dim = getattr(args, ""decoder_ffn_embed_dim"", 4096)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 16)\n    args.dropout = getattr(args, ""dropout"", 0.3)\n    levenshtein_base_architecture(args)\n\n\n# default parameters used in tensor2tensor implementation\n@register_model_architecture(\n    ""levenshtein_transformer"", ""levenshtein_transformer_wmt_en_de_big""\n)\ndef levenshtein_transformer_wmt_en_de_big_t2t(args):\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", True)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", True)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.1)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.1)\n    levenshtein_transformer_vaswani_wmt_en_de_big(args)\n'"
fairseq/models/nat/levenshtein_utils.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom fairseq.utils import new_arange\n\n\n# -------------- Helper Functions --------------------------------------------------- #\n\ndef load_libnat():\n    try:\n        from fairseq import libnat_cuda\n        return libnat_cuda, True\n\n    except ImportError as e:\n        print(str(e) + \'... fall back to CPU version\')\n\n        try:\n            from fairseq import libnat\n            return libnat, False\n\n        except ImportError as e:\n            import sys\n            sys.stderr.write(""ERROR: missing libnat_cuda. run `python setup.py build_ext --inplace`\\n"")\n            raise e\n\n\ndef _get_ins_targets(in_tokens, out_tokens, padding_idx, unk_idx):\n    libnat, use_cuda = load_libnat()\n\n    def _get_ins_targets_cuda(in_tokens, out_tokens, padding_idx, unk_idx):\n        in_masks = in_tokens.ne(padding_idx)\n        out_masks = out_tokens.ne(padding_idx)\n        mask_ins_targets, masked_tgt_masks = libnat.generate_insertion_labels(\n            out_tokens.int(), libnat.levenshtein_distance(\n                in_tokens.int(), out_tokens.int(),\n                in_masks.sum(1).int(), out_masks.sum(1).int()\n            )\n        )\n        masked_tgt_masks = masked_tgt_masks.bool() & out_masks\n        mask_ins_targets = mask_ins_targets.type_as(\n            in_tokens)[:, 1:in_masks.size(1)].masked_fill_(~in_masks[:, 1:], 0)\n        masked_tgt_tokens = out_tokens.masked_fill(masked_tgt_masks, unk_idx)\n        return masked_tgt_masks, masked_tgt_tokens, mask_ins_targets\n\n    def _get_ins_targets_cpu(in_tokens, out_tokens, padding_idx, unk_idx):\n        in_seq_len, out_seq_len = in_tokens.size(1), out_tokens.size(1)\n\n        in_tokens_list = [\n            [t for t in s if t != padding_idx] for i, s in enumerate(in_tokens.tolist())\n        ]\n        out_tokens_list = [\n            [t for t in s if t != padding_idx]\n            for i, s in enumerate(out_tokens.tolist())\n        ]\n\n        full_labels = libnat.suggested_ed2_path(\n            in_tokens_list, out_tokens_list, padding_idx\n        )\n        mask_inputs = [\n            [len(c) if c[0] != padding_idx else 0 for c in a[:-1]] for a in full_labels\n        ]\n\n        # generate labels\n        masked_tgt_masks = []\n        for mask_input in mask_inputs:\n            mask_label = []\n            for beam_size in mask_input[1:-1]:  # HACK 1:-1\n                mask_label += [0] + [1 for _ in range(beam_size)]\n            masked_tgt_masks.append(\n                mask_label + [0 for _ in range(out_seq_len - len(mask_label))]\n            )\n        mask_ins_targets = [\n            mask_input[1:-1] + [0 for _ in range(in_seq_len - 1 - len(mask_input[1:-1]))]\n            for mask_input in mask_inputs\n        ]\n\n        # transform to tensor\n        masked_tgt_masks = torch.tensor(\n            masked_tgt_masks, device=out_tokens.device\n        ).bool()\n        mask_ins_targets = torch.tensor(mask_ins_targets, device=in_tokens.device)\n        masked_tgt_tokens = out_tokens.masked_fill(masked_tgt_masks, unk_idx)\n        return masked_tgt_masks, masked_tgt_tokens, mask_ins_targets\n\n    if use_cuda:\n        return _get_ins_targets_cuda(in_tokens, out_tokens, padding_idx, unk_idx)\n    return _get_ins_targets_cpu(in_tokens, out_tokens, padding_idx, unk_idx)\n\n\ndef _get_del_targets(in_tokens, out_tokens, padding_idx):\n    libnat, use_cuda = load_libnat()\n\n    def _get_del_targets_cuda(in_tokens, out_tokens, padding_idx):\n        in_masks = in_tokens.ne(padding_idx)\n        out_masks = out_tokens.ne(padding_idx)\n\n        word_del_targets = libnat.generate_deletion_labels(\n            in_tokens.int(),\n            libnat.levenshtein_distance(\n                in_tokens.int(), out_tokens.int(),\n                in_masks.sum(1).int(), out_masks.sum(1).int()\n            )\n        )\n        word_del_targets = word_del_targets.type_as(in_tokens).masked_fill_(~in_masks, 0)\n        return word_del_targets\n\n    def _get_del_targets_cpu(in_tokens, out_tokens, padding_idx):\n        out_seq_len = out_tokens.size(1)\n        with torch.cuda.device_of(in_tokens):\n            in_tokens_list = [\n                [t for t in s if t != padding_idx] for i, s in enumerate(in_tokens.tolist())\n            ]\n            out_tokens_list = [\n                [t for t in s if t != padding_idx]\n                for i, s in enumerate(out_tokens.tolist())\n            ]\n\n        full_labels = libnat.suggested_ed2_path(\n            in_tokens_list, out_tokens_list, padding_idx\n        )\n        word_del_targets = [b[-1] for b in full_labels]\n        word_del_targets = [\n            labels + [0 for _ in range(out_seq_len - len(labels))]\n            for labels in word_del_targets\n        ]\n\n        # transform to tensor\n        word_del_targets = torch.tensor(word_del_targets, device=out_tokens.device)\n        return word_del_targets\n\n    if use_cuda:\n        return _get_del_targets_cuda(in_tokens, out_tokens, padding_idx)\n    return _get_del_targets_cpu(in_tokens, out_tokens, padding_idx)\n\n\ndef _apply_ins_masks(\n    in_tokens, in_scores, mask_ins_pred, padding_idx, unk_idx, eos_idx\n):\n\n    in_masks = in_tokens.ne(padding_idx)\n    in_lengths = in_masks.sum(1)\n\n    # HACK: hacky way to shift all the paddings to eos first.\n    in_tokens.masked_fill_(~in_masks, eos_idx)\n    mask_ins_pred.masked_fill_(~in_masks[:, 1:], 0)\n\n    out_lengths = in_lengths + mask_ins_pred.sum(1)\n    out_max_len = out_lengths.max()\n    out_masks = (\n        new_arange(out_lengths, out_max_len)[None, :]\n        < out_lengths[:, None]\n    )\n\n    reordering = (mask_ins_pred + in_masks[:, 1:].long()).cumsum(1)\n    out_tokens = (\n        in_tokens.new_zeros(in_tokens.size(0), out_max_len)\n        .fill_(padding_idx)\n        .masked_fill_(out_masks, unk_idx)\n    )\n    out_tokens[:, 0] = in_tokens[:, 0]\n    out_tokens.scatter_(1, reordering, in_tokens[:, 1:])\n\n    out_scores = None\n    if in_scores is not None:\n        in_scores.masked_fill_(~in_masks, 0)\n        out_scores = in_scores.new_zeros(*out_tokens.size())\n        out_scores[:, 0] = in_scores[:, 0]\n        out_scores.scatter_(1, reordering, in_scores[:, 1:])\n\n    return out_tokens, out_scores\n\n\ndef _apply_ins_words(\n    in_tokens, in_scores, word_ins_pred, word_ins_scores, unk_idx\n):\n    word_ins_masks = in_tokens.eq(unk_idx)\n    out_tokens = in_tokens.masked_scatter(word_ins_masks, word_ins_pred[word_ins_masks])\n\n    if in_scores is not None:\n        out_scores = in_scores.masked_scatter(\n            word_ins_masks, word_ins_scores[word_ins_masks]\n        )\n    else:\n        out_scores = None\n\n    return out_tokens, out_scores\n\n\ndef _apply_del_words(\n    in_tokens, in_scores, in_attn, word_del_pred, padding_idx, bos_idx, eos_idx\n):\n    # apply deletion to a tensor\n    in_masks = in_tokens.ne(padding_idx)\n    bos_eos_masks = in_tokens.eq(bos_idx) | in_tokens.eq(eos_idx)\n\n    max_len = in_tokens.size(1)\n    word_del_pred.masked_fill_(~in_masks, 1)\n    word_del_pred.masked_fill_(bos_eos_masks, 0)\n\n    reordering = (\n        new_arange(in_tokens)\n        .masked_fill_(word_del_pred, max_len)\n        .sort(1)[1]\n    )\n\n    out_tokens = in_tokens.masked_fill(word_del_pred, padding_idx).gather(1, reordering)\n\n    out_scores = None\n    if in_scores is not None:\n        out_scores = in_scores.masked_fill(word_del_pred, 0).gather(1, reordering)\n\n    out_attn = None\n    if in_attn is not None:\n        _mask = word_del_pred[:, :, None].expand_as(in_attn)\n        _reordering = reordering[:, :, None].expand_as(in_attn)\n        out_attn = in_attn.masked_fill(_mask, 0.).gather(1, _reordering)\n\n    return out_tokens, out_scores, out_attn\n\n\ndef _skip(x, mask):\n    """"""\n    Getting sliced (dim=0) tensor by mask. Supporting tensor and list/dict of tensors.\n    """"""\n    if isinstance(x, int):\n        return x\n\n    if x is None:\n        return None\n\n    if isinstance(x, torch.Tensor):\n        if x.size(0) == mask.size(0):\n            return x[mask]\n        elif x.size(1) == mask.size(0):\n            return x[:, mask]\n\n    if isinstance(x, list):\n        return [_skip(x_i, mask) for x_i in x]\n\n    if isinstance(x, dict):\n        return {k: _skip(v, mask) for k, v in x.items()}\n\n    raise NotImplementedError\n\n\ndef _skip_encoder_out(encoder, encoder_out, mask):\n    if not mask.any():\n        return encoder_out\n    else:\n        return encoder.reorder_encoder_out(encoder_out, mask.nonzero().squeeze())\n\n\ndef _fill(x, mask, y, padding_idx):\n    """"""\n    Filling tensor x with y at masked positions (dim=0).\n    """"""\n    if x is None:\n        return y\n    assert x.dim() == y.dim() and mask.size(0) == x.size(0)\n    assert x.dim() == 2 or (x.dim() == 3 and x.size(2) == y.size(2))\n    n_selected = mask.sum()\n    assert n_selected == y.size(0)\n\n    if n_selected == x.size(0):\n        return y\n\n    if x.size(1) < y.size(1):\n        dims = [x.size(0), y.size(1) - x.size(1)]\n        if x.dim() == 3:\n            dims.append(x.size(2))\n        x = torch.cat([x, x.new_zeros(*dims).fill_(padding_idx)], 1)\n        x[mask] = y\n    elif x.size(1) > y.size(1):\n        x[mask] = padding_idx\n        if x.dim() == 2:\n            x[mask, :y.size(1)] = y\n        else:\n            x[mask, :y.size(1), :] = y\n    else:\n        x[mask] = y\n    return x\n'"
fairseq/models/nat/nat_crf_transformer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom fairseq.models.nat import NATransformerModel, base_architecture\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.modules import DynamicCRF\n\n\n@register_model(""nacrf_transformer"")\nclass NACRFTransformerModel(NATransformerModel):\n    def __init__(self, args, encoder, decoder):\n        super().__init__(args, encoder, decoder)\n        self.crf_layer = DynamicCRF(\n            num_embedding=len(self.tgt_dict),\n            low_rank=args.crf_lowrank_approx,\n            beam_size=args.crf_beam_approx\n        )\n\n    @property\n    def allow_ensemble(self):\n        return False\n\n    @staticmethod\n    def add_args(parser):\n        NATransformerModel.add_args(parser)\n        parser.add_argument(""--crf-lowrank-approx"", type=int,\n                            help=""the dimension of low-rank approximation of transition"")\n        parser.add_argument(""--crf-beam-approx"", type=int,\n                            help=""the beam size for apporixmating the normalizing factor"")\n        parser.add_argument(""--word-ins-loss-factor"", type=float,\n                            help=""weights on NAT loss used to co-training with CRF loss."")\n\n    def forward(\n        self, src_tokens, src_lengths, prev_output_tokens, tgt_tokens, **kwargs\n    ):\n        # encoding\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n\n        # length prediction\n        length_out = self.decoder.forward_length(normalize=False, encoder_out=encoder_out)\n        length_tgt = self.decoder.forward_length_prediction(length_out, encoder_out, tgt_tokens)\n\n        # decoding\n        word_ins_out = self.decoder(\n            normalize=False,\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out)\n        word_ins_tgt, word_ins_mask = tgt_tokens, tgt_tokens.ne(self.pad)\n\n        # compute the log-likelihood of CRF\n        crf_nll = -self.crf_layer(word_ins_out, word_ins_tgt, word_ins_mask)\n        crf_nll = (crf_nll / word_ins_mask.type_as(crf_nll).sum(-1)).mean()\n\n        return {\n            ""word_ins"": {\n                ""out"": word_ins_out, ""tgt"": word_ins_tgt,\n                ""mask"": word_ins_mask, ""ls"": self.args.label_smoothing,\n                ""nll_loss"": True, ""factor"": self.args.word_ins_loss_factor\n            },\n            ""word_crf"": {\n                ""loss"": crf_nll\n            },\n            ""length"": {\n                ""out"": length_out, ""tgt"": length_tgt,\n                ""factor"": self.decoder.length_loss_factor\n            }\n        }\n\n    def forward_decoder(self, decoder_out, encoder_out, decoding_format=None, **kwargs):\n        output_tokens = decoder_out.output_tokens\n        output_scores = decoder_out.output_scores\n        history = decoder_out.history\n\n        # execute the decoder and get emission scores\n        output_masks = output_tokens.ne(self.pad)\n        word_ins_out = self.decoder(\n            normalize=False,\n            prev_output_tokens=output_tokens,\n            encoder_out=encoder_out\n        )\n\n        # run viterbi decoding through CRF\n        _scores, _tokens = self.crf_layer.forward_decoder(word_ins_out, output_masks)\n        output_tokens.masked_scatter_(output_masks, _tokens[output_masks])\n        output_scores.masked_scatter_(output_masks, _scores[output_masks])\n        if history is not None:\n            history.append(output_tokens.clone())\n\n        return decoder_out._replace(\n            output_tokens=output_tokens,\n            output_scores=output_scores,\n            attn=None,\n            history=history\n        )\n\n\n@register_model_architecture(""nacrf_transformer"", ""nacrf_transformer"")\ndef nacrf_base_architecture(args):\n    args.crf_lowrank_approx = getattr(args, ""crf_lowrank_approx"", 32)\n    args.crf_beam_approx = getattr(args, ""crf_beam_approx"", 64)\n    args.word_ins_loss_factor = getattr(args, ""word_ins_loss_factor"", 0.5)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", True)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", True)\n    base_architecture(args)\n'"
fairseq/models/nat/nonautoregressive_ensembles.py,12,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq.models.nat import (\n    _fill,\n    _skip,\n    _skip_encoder_out,\n    _apply_ins_masks,\n    _apply_ins_words,\n    _apply_del_words,\n)\n\n\nclass _EnsembleModelEncoder(object):\n    def __init__(self, models):\n        self.models = models\n\n    def reorder_encoder_out(self, encoder_outs, new_order):\n        encoder_outs = [\n            model.encoder.reorder_encoder_out(encoder_out, new_order)\n            for model, encoder_out in zip(self.models, encoder_outs)\n        ]\n        return encoder_outs\n\n\nclass BasicEnsembleModel(torch.nn.Module):\n    """"""A wrapper around an ensemble of models.""""""\n\n    def __init__(self, models):\n        super().__init__()\n        self.models = torch.nn.ModuleList(models)\n        self.bos = self.models[0].decoder.dictionary.bos()\n        self.eos = self.models[0].decoder.dictionary.eos()\n        self.pad = self.models[0].decoder.dictionary.pad()\n        self.unk = self.models[0].decoder.dictionary.unk()\n        self.encoder = _EnsembleModelEncoder(self.models)\n\n    def has_encoder(self):\n        return hasattr(self.models[0], \'encoder\')\n\n    def max_decoder_positions(self):\n        return min(m.max_decoder_positions() for m in self.models)\n\n    @torch.no_grad()\n    def forward_encoder(self, encoder_input):\n        if not self.has_encoder():\n            return None\n        return [model.forward_encoder(encoder_input) for model in self.models]\n\n    @torch.no_grad()\n    def forward_decoder(self, *inputs):\n        raise NotImplementedError\n\n    def initialize_output_tokens(self, *inputs):\n        raise NotImplementedError\n\n\nclass EnsembleLevT(BasicEnsembleModel):\n    """"""A wrapper around an ensemble of models.""""""\n\n    def __init__(self, models):\n        super().__init__(models)\n\n    @torch.no_grad()\n    def forward_decoder(self, decoder_out, encoder_outs, eos_penalty=0.0, max_ratio=None, **kwargs):\n        # LevT ensembling\n        # A pipeline of three steps: deletion, placeholder, and word insertion.\n        # We need to average scores in each step in a pipeline way because of dependence.\n        # deletion\n        output_tokens = decoder_out.output_tokens\n        output_scores = decoder_out.output_scores\n        attn = decoder_out.attn\n\n        bsz = output_tokens.size(0)\n        if max_ratio is None:\n            max_lens = output_tokens.new().fill_(255)\n        else:\n            if encoder_outs[0].encoder_padding_mask is None:\n                src_lens = encoder_outs[0].encoder_out.new(bsz).fill_(encoder_outs[0].encoder_out.size(1))\n            else:\n                src_lens = (~encoder_outs[0].encoder_padding_mask).sum(1)\n            max_lens = (src_lens * max_ratio).clamp(min=10).long()\n\n        # delete words\n        # do not delete tokens if it is <s> </s>\n        can_del_word = output_tokens.ne(self.pad).sum(1) > 2\n        if can_del_word.sum() != 0:  # we cannot delete, skip\n            output_tokens, output_scores, attn = self.forward_word_del(\n                encoder_outs,\n                output_tokens,\n                output_scores,\n                attn,\n                can_del_word,\n            )\n\n        # insert placeholders\n        can_ins_mask = output_tokens.ne(self.pad).sum(1) < max_lens\n        if can_ins_mask.sum() != 0:\n            output_tokens, output_scores = self.forward_mask_ins(\n                 encoder_outs,\n                 output_tokens,\n                 output_scores,\n                 can_ins_mask,\n                 eos_penalty,\n                 max_lens,\n             )\n\n        # insert words\n        can_ins_word = output_tokens.eq(self.unk).sum(1) > 0\n        if can_ins_word.sum() != 0:\n            output_tokens, output_scores, attn = self.forward_word_ins(\n                encoder_outs,\n                output_tokens,\n                output_scores,\n                attn,\n                can_ins_word,\n            )\n\n        # delete some unnecessary paddings\n        cut_off = output_tokens.ne(self.pad).sum(1).max()\n        output_tokens = output_tokens[:, :cut_off]\n        output_scores = output_scores[:, :cut_off]\n        attn = None if attn is None else attn[:, :cut_off, :]\n        return decoder_out._replace(\n            output_tokens=output_tokens,\n            output_scores=output_scores,\n            attn=attn,\n            history=None\n        )\n\n    def forward_word_del(self, encoder_outs, output_tokens, output_scores, attn, can_del_word):\n        word_del_score_avg = []\n        word_del_attn_avg = []\n        for model, encoder_out in zip(self.models, encoder_outs):\n            word_del_out, word_del_attn = model.decoder.forward_word_del(\n                _skip(output_tokens, can_del_word),\n                _skip_encoder_out(model.encoder, encoder_out, can_del_word),\n            )\n            word_del_score = F.log_softmax(word_del_out, 2)\n            word_del_score_avg.append(word_del_score)\n            word_del_attn_avg.append(word_del_attn)\n        word_del_score_avg = torch.logsumexp(torch.stack(word_del_score_avg, dim=0), dim=0) - math.log(len(self.models))\n        word_del_pred = word_del_score_avg.max(-1)[1].bool()\n        if word_del_attn_avg[0] is not None:\n            word_del_attn_avg = torch.stack(word_del_attn_avg, dim=0)/len(self.models)\n        else:\n            word_del_attn_avg = None\n\n        _tokens, _scores, _attn = _apply_del_words(\n            output_tokens[can_del_word],\n            output_scores[can_del_word],\n            word_del_attn_avg,\n            word_del_pred,\n            self.pad,\n            self.bos,\n            self.eos,\n        )\n        output_tokens = _fill(output_tokens, can_del_word, _tokens, self.pad)\n        output_scores = _fill(output_scores, can_del_word, _scores, 0)\n        attn = _fill(attn, can_del_word, _attn, 0.)\n        return output_tokens, output_scores, attn\n\n    def forward_mask_ins(self, encoder_outs, output_tokens, output_scores, can_ins_mask, eos_penalty, max_lens):\n        mask_ins_score_avg = []\n        for model, encoder_out in zip(self.models, encoder_outs):\n            mask_ins_out, _ = model.decoder.forward_mask_ins(\n                _skip(output_tokens, can_ins_mask),\n                _skip_encoder_out(model.encoder, encoder_out, can_ins_mask),\n            )\n            mask_ins_score = F.log_softmax(mask_ins_out, 2)\n            if eos_penalty > 0.0:\n                mask_ins_score[:, :, 0] -= eos_penalty\n            mask_ins_score_avg.append(mask_ins_score)\n        mask_ins_score_avg = torch.logsumexp(torch.stack(mask_ins_score_avg, dim=0), dim=0) - math.log(len(self.models))\n        mask_ins_pred = mask_ins_score_avg.max(-1)[1]\n        mask_ins_pred = torch.min(\n            mask_ins_pred, max_lens[can_ins_mask, None].expand_as(mask_ins_pred)\n        )\n        _tokens, _scores = _apply_ins_masks(\n            output_tokens[can_ins_mask],\n            output_scores[can_ins_mask],\n            mask_ins_pred,\n            self.pad,\n            self.unk,\n            self.eos,\n        )\n        output_tokens = _fill(output_tokens, can_ins_mask, _tokens, self.pad)\n        output_scores = _fill(output_scores, can_ins_mask, _scores, 0)\n        return output_tokens, output_scores\n\n    def forward_word_ins(self, encoder_outs, output_tokens, output_scores, attn, can_ins_word):\n        word_ins_score_avg = []\n        word_ins_attn_avg = []\n        for model, encoder_out in zip(self.models, encoder_outs):\n            word_ins_out, word_ins_attn = model.decoder.forward_word_ins(\n                _skip(output_tokens, can_ins_word),\n                _skip_encoder_out(model.encoder, encoder_out, can_ins_word),\n            )\n            word_ins_score = F.log_softmax(word_ins_out, 2)\n            word_ins_score_avg.append(word_ins_score)\n            word_ins_attn_avg.append(word_ins_attn)\n        word_ins_score_avg = torch.logsumexp(torch.stack(word_ins_score_avg, dim=0), dim=0) - math.log(len(self.models))\n        if word_ins_attn_avg[0] is not None:\n            word_ins_attn_avg = torch.stack(word_ins_attn_avg, dim=0)/len(self.models)\n        else:\n            word_ins_attn_avg = None\n        word_ins_score_max, word_ins_pred = word_ins_score_avg.max(-1)\n\n        _tokens, _scores = _apply_ins_words(\n            output_tokens[can_ins_word],\n            output_scores[can_ins_word],\n            word_ins_pred,\n            word_ins_score_max,\n            self.unk,\n        )\n\n        output_tokens = _fill(output_tokens, can_ins_word, _tokens, self.pad)\n        output_scores = _fill(output_scores, can_ins_word, _scores, 0)\n        attn = _fill(attn, can_ins_word, word_ins_attn, 0.)\n        return output_tokens, output_scores, attn\n\n    def initialize_output_tokens(self, encoder_outs, src_tokens):\n        # LevT doesn\'t do length prediction.\n        return self.models[0].initialize_output_tokens(encoder_outs[0], src_tokens)\n'"
fairseq/models/nat/nonautoregressive_transformer.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.iterative_refinement_generator import DecoderOut\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.transformer import Embedding\n\nfrom fairseq.models.nat import (\n    FairseqNATModel,\n    FairseqNATDecoder,\n    ensemble_decoder\n)\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\n\n\ndef _mean_pooling(enc_feats, src_masks):\n    # enc_feats: T x B x C\n    # src_masks: B x T or None\n    if src_masks is None:\n        enc_feats = enc_feats.mean(0)\n    else:\n        src_masks = (~src_masks).transpose(0, 1).type_as(enc_feats)\n        enc_feats = (\n            (enc_feats / src_masks.sum(0)[None, :, None]) * src_masks[:, :, None]\n        ).sum(0)\n    return enc_feats\n\n\ndef _argmax(x, dim):\n    return (x == x.max(dim, keepdim=True)[0]).type_as(x)\n\n\ndef _uniform_assignment(src_lens, trg_lens):\n    max_trg_len = trg_lens.max()\n    steps = (src_lens.float() - 1) / (trg_lens.float() - 1)  # step-size\n    # max_trg_len\n    index_t = utils.new_arange(trg_lens, max_trg_len).float()\n    index_t = steps[:, None] * index_t[None, :]  # batch_size X max_trg_len\n    index_t = torch.round(index_t).long().detach()\n    return index_t\n\n\n@register_model(""nonautoregressive_transformer"")\nclass NATransformerModel(FairseqNATModel):\n\n    @property\n    def allow_length_beam(self):\n        return True\n\n    @staticmethod\n    def add_args(parser):\n        FairseqNATModel.add_args(parser)\n\n        # length prediction\n        parser.add_argument(""--src-embedding-copy"", action=""store_true"",\n                            help=""copy encoder word embeddings as the initial input of the decoder"")\n        parser.add_argument(""--pred-length-offset"", action=""store_true"",\n                            help=""predicting the length difference between the target and source sentences"")\n        parser.add_argument(""--sg-length-pred"", action=""store_true"",\n                            help=""stop the gradients back-propagated from the length predictor"")\n        parser.add_argument(""--length-loss-factor"", type=float,\n                            help=""weights on the length prediction loss"")\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        decoder = NATransformerDecoder(args, tgt_dict, embed_tokens)\n        if getattr(args, ""apply_bert_init"", False):\n            decoder.apply(init_bert_params)\n        return decoder\n\n    def forward(\n        self, src_tokens, src_lengths, prev_output_tokens, tgt_tokens, **kwargs\n    ):\n        # encoding\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n\n        # length prediction\n        length_out = self.decoder.forward_length(normalize=False, encoder_out=encoder_out)\n        length_tgt = self.decoder.forward_length_prediction(length_out, encoder_out, tgt_tokens)\n\n        # decoding\n        word_ins_out = self.decoder(\n            normalize=False,\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out)\n\n        return {\n            ""word_ins"": {\n                ""out"": word_ins_out, ""tgt"": tgt_tokens,\n                ""mask"": tgt_tokens.ne(self.pad), ""ls"": self.args.label_smoothing,\n                ""nll_loss"": True\n            },\n            ""length"": {\n                ""out"": length_out, ""tgt"": length_tgt,\n                ""factor"": self.decoder.length_loss_factor\n            }\n        }\n\n    def forward_decoder(self, decoder_out, encoder_out, decoding_format=None, **kwargs):\n        step = decoder_out.step\n        output_tokens = decoder_out.output_tokens\n        output_scores = decoder_out.output_scores\n        history = decoder_out.history\n\n        # execute the decoder\n        output_masks = output_tokens.ne(self.pad)\n        _scores, _tokens = self.decoder(\n            normalize=True,\n            prev_output_tokens=output_tokens,\n            encoder_out=encoder_out,\n            step=step,\n        ).max(-1)\n\n        output_tokens.masked_scatter_(output_masks, _tokens[output_masks])\n        output_scores.masked_scatter_(output_masks, _scores[output_masks])\n        if history is not None:\n            history.append(output_tokens.clone())\n\n        return decoder_out._replace(\n            output_tokens=output_tokens,\n            output_scores=output_scores,\n            attn=None,\n            history=history\n        )\n\n    def initialize_output_tokens(self, encoder_out, src_tokens):\n        # length prediction\n        length_tgt = self.decoder.forward_length_prediction(\n            self.decoder.forward_length(normalize=True, encoder_out=encoder_out),\n            encoder_out=encoder_out\n        )\n\n        max_length = length_tgt.clamp_(min=2).max()\n        idx_length = utils.new_arange(src_tokens, max_length)\n\n        initial_output_tokens = src_tokens.new_zeros(\n            src_tokens.size(0), max_length\n        ).fill_(self.pad)\n        initial_output_tokens.masked_fill_(\n            idx_length[None, :] < length_tgt[:, None], self.unk\n        )\n        initial_output_tokens[:, 0] = self.bos\n        initial_output_tokens.scatter_(1, length_tgt[:, None] - 1, self.eos)\n\n        initial_output_scores = initial_output_tokens.new_zeros(\n            *initial_output_tokens.size()\n        ).type_as(encoder_out.encoder_out)\n\n        return DecoderOut(\n            output_tokens=initial_output_tokens,\n            output_scores=initial_output_scores,\n            attn=None,\n            step=0,\n            max_step=0,\n            history=None\n        )\n\n    def regenerate_length_beam(self, decoder_out, beam_size):\n        output_tokens = decoder_out.output_tokens\n        length_tgt = output_tokens.ne(self.pad).sum(1)\n        length_tgt = length_tgt[:, None] + utils.new_arange(length_tgt, 1, beam_size) - beam_size // 2\n        length_tgt = length_tgt.view(-1).clamp_(min=2)\n        max_length = length_tgt.max()\n        idx_length = utils.new_arange(length_tgt, max_length)\n\n        initial_output_tokens = output_tokens.new_zeros(\n            length_tgt.size(0), max_length\n        ).fill_(self.pad)\n        initial_output_tokens.masked_fill_(\n            idx_length[None, :] < length_tgt[:, None], self.unk\n        )\n        initial_output_tokens[:, 0] = self.bos\n        initial_output_tokens.scatter_(1, length_tgt[:, None] - 1, self.eos)\n\n        initial_output_scores = initial_output_tokens.new_zeros(\n            *initial_output_tokens.size()\n        ).type_as(decoder_out.output_scores)\n\n        return decoder_out._replace(\n            output_tokens=initial_output_tokens,\n            output_scores=initial_output_scores\n        )\n\n\nclass NATransformerDecoder(FairseqNATDecoder):\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n        super().__init__(\n            args, dictionary, embed_tokens, no_encoder_attn=no_encoder_attn\n        )\n        self.dictionary = dictionary\n        self.bos = dictionary.bos()\n        self.unk = dictionary.unk()\n        self.eos = dictionary.eos()\n\n        self.encoder_embed_dim = args.encoder_embed_dim\n        self.sg_length_pred = getattr(args, ""sg_length_pred"", False)\n        self.pred_length_offset = getattr(args, ""pred_length_offset"", False)\n        self.length_loss_factor = getattr(args, ""length_loss_factor"", 0.1)\n        self.src_embedding_copy = getattr(args, ""src_embedding_copy"", False)\n        self.embed_length = Embedding(256, self.encoder_embed_dim, None)\n\n    @ensemble_decoder\n    def forward(self, normalize, encoder_out, prev_output_tokens, step=0, **unused):\n        features, _ = self.extract_features(\n            prev_output_tokens,\n            encoder_out=encoder_out,\n            embedding_copy=(step == 0) & self.src_embedding_copy,\n        )\n        decoder_out = self.output_layer(features)\n        return F.log_softmax(decoder_out, -1) if normalize else decoder_out\n\n    @ensemble_decoder\n    def forward_length(self, normalize, encoder_out):\n        enc_feats = encoder_out.encoder_out  # T x B x C\n        src_masks = encoder_out.encoder_padding_mask  # B x T or None\n        enc_feats = _mean_pooling(enc_feats, src_masks)\n        if self.sg_length_pred:\n            enc_feats = enc_feats.detach()\n        length_out = F.linear(enc_feats, self.embed_length.weight)\n        return F.log_softmax(length_out, -1) if normalize else length_out\n\n    def extract_features(\n        self,\n        prev_output_tokens,\n        encoder_out=None,\n        early_exit=None,\n        embedding_copy=False,\n        **unused\n    ):\n        """"""\n        Similar to *forward* but only return features.\n\n        Inputs:\n            prev_output_tokens: Tensor(B, T)\n            encoder_out: a dictionary of hidden states and masks\n\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n            the LevenshteinTransformer decoder has full-attention to all generated tokens\n        """"""\n        # embedding\n        if embedding_copy:\n            src_embd = encoder_out.encoder_embedding\n            src_mask = encoder_out.encoder_padding_mask\n            src_mask = (\n                ~src_mask\n                if src_mask is not None\n                else prev_output_tokens.new_ones(*src_embd.size()[:2]).bool()\n            )\n\n            x, decoder_padding_mask = self.forward_embedding(\n                prev_output_tokens,\n                self.forward_copying_source(\n                    src_embd, src_mask, prev_output_tokens.ne(self.padding_idx)\n                ),\n            )\n\n        else:\n\n            x, decoder_padding_mask = self.forward_embedding(prev_output_tokens)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        attn = None\n        inner_states = [x]\n\n        # decoder layers\n        for i, layer in enumerate(self.layers):\n\n            # early exit from the decoder.\n            if (early_exit is not None) and (i >= early_exit):\n                break\n\n            x, attn, _ = layer(\n                x,\n                encoder_out.encoder_out if encoder_out is not None else None,\n                encoder_out.encoder_padding_mask if encoder_out is not None else None,\n                self_attn_mask=None,\n                self_attn_padding_mask=decoder_padding_mask,\n            )\n            inner_states.append(x)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        return x, {""attn"": attn, ""inner_states"": inner_states}\n\n    def forward_embedding(self, prev_output_tokens, states=None):\n        # embed positions\n        positions = (\n            self.embed_positions(prev_output_tokens)\n            if self.embed_positions is not None\n            else None\n        )\n\n        # embed tokens and positions\n        if states is None:\n            x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n            if self.project_in_dim is not None:\n                x = self.project_in_dim(x)\n        else:\n            x = states\n\n        if positions is not None:\n            x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        decoder_padding_mask = prev_output_tokens.eq(self.padding_idx)\n        return x, decoder_padding_mask\n\n    def forward_copying_source(self, src_embeds, src_masks, tgt_masks):\n        length_sources = src_masks.sum(1)\n        length_targets = tgt_masks.sum(1)\n        mapped_inputs = _uniform_assignment(length_sources, length_targets).masked_fill(\n            ~tgt_masks, 0\n        )\n        copied_embedding = torch.gather(\n            src_embeds,\n            1,\n            mapped_inputs.unsqueeze(-1).expand(\n                *mapped_inputs.size(), src_embeds.size(-1)\n            ),\n        )\n        return copied_embedding\n\n    def forward_length_prediction(self, length_out, encoder_out, tgt_tokens=None):\n        enc_feats = encoder_out.encoder_out  # T x B x C\n        src_masks = encoder_out.encoder_padding_mask  # B x T or None\n        if self.pred_length_offset:\n            if src_masks is None:\n                src_lengs = enc_feats.new_ones(enc_feats.size(1)).fill_(\n                    enc_feats.size(0)\n                )\n            else:\n                src_lengs = (~src_masks).transpose(0, 1).type_as(enc_feats).sum(0)\n            src_lengs = src_lengs.long()\n\n        if tgt_tokens is not None:\n            # obtain the length target\n            tgt_lengs = tgt_tokens.ne(self.padding_idx).sum(1).long()\n            if self.pred_length_offset:\n                length_tgt = tgt_lengs - src_lengs + 128\n            else:\n                length_tgt = tgt_lengs\n            length_tgt = length_tgt.clamp(min=0, max=255)\n\n        else:\n            # predict the length target (greedy for now)\n            # TODO: implementing length-beam\n            pred_lengs = length_out.max(-1)[1]\n            if self.pred_length_offset:\n                length_tgt = pred_lengs - 128 + src_lengs\n            else:\n                length_tgt = pred_lengs\n\n        return length_tgt\n\n\n@register_model_architecture(\n    ""nonautoregressive_transformer"", ""nonautoregressive_transformer""\n)\ndef base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.0)\n    args.activation_fn = getattr(args, ""activation_fn"", ""relu"")\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.adaptive_input = getattr(args, ""adaptive_input"", False)\n    args.apply_bert_init = getattr(args, ""apply_bert_init"", False)\n\n    args.decoder_output_dim = getattr(\n        args, ""decoder_output_dim"", args.decoder_embed_dim\n    )\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n\n    # --- special arguments ---\n    args.sg_length_pred = getattr(args, ""sg_length_pred"", False)\n    args.pred_length_offset = getattr(args, ""pred_length_offset"", False)\n    args.length_loss_factor = getattr(args, ""length_loss_factor"", 0.1)\n    args.src_embedding_copy = getattr(args, ""src_embedding_copy"", False)\n\n\n@register_model_architecture(\n    ""nonautoregressive_transformer"", ""nonautoregressive_transformer_wmt_en_de""\n)\ndef nonautoregressive_transformer_wmt_en_de(args):\n    base_architecture(args)\n'"
fairseq/models/roberta/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .hub_interface import *  # noqa\nfrom .model import *  # noqa\nfrom .model_camembert import *  # noqa\nfrom .model_xlmr import *  # noqa\n'"
fairseq/models/roberta/alignment_utils.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import Counter\nfrom typing import List\n\nimport torch\n\n\ndef align_bpe_to_words(roberta, bpe_tokens: torch.LongTensor, other_tokens: List[str]):\n    """"""\n    Helper to align GPT-2 BPE to other tokenization formats (e.g., spaCy).\n\n    Args:\n        roberta (RobertaHubInterface): RoBERTa instance\n        bpe_tokens (torch.LongTensor): GPT-2 BPE tokens of shape `(T_bpe)`\n        other_tokens (List[str]): other tokens of shape `(T_words)`\n\n    Returns:\n        List[str]: mapping from *other_tokens* to corresponding *bpe_tokens*.\n    """"""\n    assert bpe_tokens.dim() == 1\n    assert bpe_tokens[0] == 0\n\n    def clean(text):\n        return text.strip()\n\n    # remove whitespaces to simplify alignment\n    bpe_tokens = [roberta.task.source_dictionary.string([x]) for x in bpe_tokens]\n    bpe_tokens = [clean(roberta.bpe.decode(x) if x not in {\'<s>\', \'\'} else x) for x in bpe_tokens]\n    other_tokens = [clean(str(o)) for o in other_tokens]\n\n    # strip leading <s>\n    bpe_tokens = bpe_tokens[1:]\n    assert \'\'.join(bpe_tokens) == \'\'.join(other_tokens)\n\n    # create alignment from every word to a list of BPE tokens\n    alignment = []\n    bpe_toks = filter(lambda item: item[1] != \'\', enumerate(bpe_tokens, start=1))\n    j, bpe_tok = next(bpe_toks)\n    for other_tok in other_tokens:\n        bpe_indices = []\n        while True:\n            if other_tok.startswith(bpe_tok):\n                bpe_indices.append(j)\n                other_tok = other_tok[len(bpe_tok):]\n                try:\n                    j, bpe_tok = next(bpe_toks)\n                except StopIteration:\n                    j, bpe_tok = None, None\n            elif bpe_tok.startswith(other_tok):\n                # other_tok spans multiple BPE tokens\n                bpe_indices.append(j)\n                bpe_tok = bpe_tok[len(other_tok):]\n                other_tok = \'\'\n            else:\n                raise Exception(\'Cannot align ""{}"" and ""{}""\'.format(other_tok, bpe_tok))\n            if other_tok == \'\':\n                break\n        assert len(bpe_indices) > 0\n        alignment.append(bpe_indices)\n    assert len(alignment) == len(other_tokens)\n\n    return alignment\n\n\ndef align_features_to_words(roberta, features, alignment):\n    """"""\n    Align given features to words.\n\n    Args:\n        roberta (RobertaHubInterface): RoBERTa instance\n        features (torch.Tensor): features to align of shape `(T_bpe x C)`\n        alignment: alignment between BPE tokens and words returned by\n            func:`align_bpe_to_words`.\n    """"""\n    assert features.dim() == 2\n\n    bpe_counts = Counter(j for bpe_indices in alignment for j in bpe_indices)\n    assert bpe_counts[0] == 0  # <s> shouldn\'t be aligned\n    denom = features.new([bpe_counts.get(j, 1) for j in range(len(features))])\n    weighted_features = features / denom.unsqueeze(-1)\n\n    output = [weighted_features[0]]\n    largest_j = -1\n    for bpe_indices in alignment:\n        output.append(weighted_features[bpe_indices].sum(dim=0))\n        largest_j = max(largest_j, *bpe_indices)\n    for j in range(largest_j + 1, len(features)):\n        output.append(weighted_features[j])\n    output = torch.stack(output)\n    assert torch.all(torch.abs(output.sum(dim=0) - features.sum(dim=0)) < 1e-4)\n    return output\n\n\ndef spacy_nlp():\n    if getattr(spacy_nlp, \'_nlp\', None) is None:\n        try:\n            from spacy.lang.en import English\n            spacy_nlp._nlp = English()\n        except ImportError:\n            raise ImportError(\'Please install spacy with: pip install spacy\')\n    return spacy_nlp._nlp\n\n\ndef spacy_tokenizer():\n    if getattr(spacy_tokenizer, \'_tokenizer\', None) is None:\n        try:\n            nlp = spacy_nlp()\n            spacy_tokenizer._tokenizer = nlp.Defaults.create_tokenizer(nlp)\n        except ImportError:\n            raise ImportError(\'Please install spacy with: pip install spacy\')\n    return spacy_tokenizer._tokenizer\n'"
fairseq/models/roberta/hub_interface.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.data import encoders\n\n\nclass RobertaHubInterface(nn.Module):\n    """"""A simple PyTorch Hub interface to RoBERTa.\n\n    Usage: https://github.com/pytorch/fairseq/tree/master/examples/roberta\n    """"""\n\n    def __init__(self, args, task, model):\n        super().__init__()\n        self.args = args\n        self.task = task\n        self.model = model\n\n        self.bpe = encoders.build_bpe(args)\n\n        # this is useful for determining the device\n        self.register_buffer(\'_float_tensor\', torch.tensor([0], dtype=torch.float))\n\n    @property\n    def device(self):\n        return self._float_tensor.device\n\n    def encode(self, sentence: str, *addl_sentences, no_separator=False) -> torch.LongTensor:\n        """"""\n        BPE-encode a sentence (or multiple sentences).\n\n        Every sequence begins with a beginning-of-sentence (`<s>`) symbol.\n        Every sentence ends with an end-of-sentence (`</s>`) and we use an\n        extra end-of-sentence (`</s>`) as a separator.\n\n        Example (single sentence): `<s> a b c </s>`\n        Example (sentence pair): `<s> d e f </s> </s> 1 2 3 </s>`\n\n        The BPE encoding follows GPT-2. One subtle detail is that the GPT-2 BPE\n        requires leading spaces. For example::\n\n            >>> roberta.encode(\'Hello world\').tolist()\n            [0, 31414, 232, 2]\n            >>> roberta.encode(\' world\').tolist()\n            [0, 232, 2]\n            >>> roberta.encode(\'world\').tolist()\n            [0, 8331, 2]\n        """"""\n        bpe_sentence = \'<s> \' + self.bpe.encode(sentence) + \' </s>\'\n        for s in addl_sentences:\n            bpe_sentence += (\' </s>\' if not no_separator else \'\')\n            bpe_sentence += \' \' + self.bpe.encode(s) + \' </s>\'\n        tokens = self.task.source_dictionary.encode_line(bpe_sentence, append_eos=False, add_if_not_exist=False)\n        return tokens.long()\n\n    def decode(self, tokens: torch.LongTensor):\n        assert tokens.dim() == 1\n        tokens = tokens.numpy()\n        if tokens[0] == self.task.source_dictionary.bos():\n            tokens = tokens[1:]  # remove <s>\n        eos_mask = (tokens == self.task.source_dictionary.eos())\n        doc_mask = eos_mask[1:] & eos_mask[:-1]\n        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)\n        sentences = [self.bpe.decode(self.task.source_dictionary.string(s)) for s in sentences]\n        if len(sentences) == 1:\n            return sentences[0]\n        return sentences\n\n    def extract_features(self, tokens: torch.LongTensor, return_all_hiddens: bool = False) -> torch.Tensor:\n        if tokens.dim() == 1:\n            tokens = tokens.unsqueeze(0)\n        if tokens.size(-1) > self.model.max_positions():\n            raise ValueError(\'tokens exceeds maximum length: {} > {}\'.format(\n                tokens.size(-1), self.model.max_positions()\n            ))\n        features, extra = self.model(\n            tokens.to(device=self.device),\n            features_only=True,\n            return_all_hiddens=return_all_hiddens,\n        )\n        if return_all_hiddens:\n            # convert from T x B x C -> B x T x C\n            inner_states = extra[\'inner_states\']\n            return [inner_state.transpose(0, 1) for inner_state in inner_states]\n        else:\n            return features  # just the last layer\'s features\n\n    def register_classification_head(\n        self, name: str, num_classes: int = None, embedding_size: int = None, **kwargs\n    ):\n        self.model.register_classification_head(\n            name, num_classes=num_classes, embedding_size=embedding_size, **kwargs\n        )\n\n    def predict(self, head: str, tokens: torch.LongTensor, return_logits: bool = False):\n        features = self.extract_features(tokens.to(device=self.device))\n        logits = self.model.classification_heads[head](features)\n        if return_logits:\n            return logits\n        return F.log_softmax(logits, dim=-1)\n\n    def extract_features_aligned_to_words(self, sentence: str, return_all_hiddens: bool = False) -> torch.Tensor:\n        """"""Extract RoBERTa features, aligned to spaCy\'s word-level tokenizer.""""""\n        from fairseq.models.roberta import alignment_utils\n        from spacy.tokens import Doc\n\n        nlp = alignment_utils.spacy_nlp()\n        tokenizer = alignment_utils.spacy_tokenizer()\n\n        # tokenize both with GPT-2 BPE and spaCy\n        bpe_toks = self.encode(sentence)\n        spacy_toks = tokenizer(sentence)\n        spacy_toks_ws = [t.text_with_ws for t in tokenizer(sentence)]\n        alignment = alignment_utils.align_bpe_to_words(self, bpe_toks, spacy_toks_ws)\n\n        # extract features and align them\n        features = self.extract_features(bpe_toks, return_all_hiddens=return_all_hiddens)\n        features = features.squeeze(0)\n        aligned_feats = alignment_utils.align_features_to_words(self, features, alignment)\n\n        # wrap in spaCy Doc\n        doc = Doc(\n            nlp.vocab,\n            words=[\'<s>\'] + [x.text for x in spacy_toks] + [\'</s>\'],\n            spaces=[True] + [x.endswith(\' \') for x in spacy_toks_ws[:-1]] + [True, False],\n        )\n        assert len(doc) == aligned_feats.size(0)\n        doc.user_token_hooks[\'vector\'] = lambda token: aligned_feats[token.i]\n        return doc\n\n    def fill_mask(self, masked_input: str, topk: int = 5):\n        masked_token = \'<mask>\'\n        assert masked_token in masked_input and masked_input.count(masked_token) == 1, \\\n            ""Please add one {0} token for the input, eg: \'He is a {0} guy\'"".format(masked_token)\n\n        text_spans = masked_input.split(masked_token)\n        text_spans_bpe = (\' {0} \'.format(masked_token)).join(\n            [self.bpe.encode(text_span.rstrip()) for text_span in text_spans]\n        ).strip()\n        tokens = self.task.source_dictionary.encode_line(\n            \'<s> \' + text_spans_bpe + \' </s>\',\n            append_eos=False,\n            add_if_not_exist=False,\n        )\n\n        masked_index = (tokens == self.task.mask_idx).nonzero()\n        if tokens.dim() == 1:\n            tokens = tokens.unsqueeze(0)\n\n        with utils.eval(self.model):\n            features, extra = self.model(\n                tokens.long().to(device=self.device),\n                features_only=False,\n                return_all_hiddens=False,\n            )\n        logits = features[0, masked_index, :].squeeze()\n        prob = logits.softmax(dim=0)\n        values, index = prob.topk(k=topk, dim=0)\n        topk_predicted_token_bpe = self.task.source_dictionary.string(index)\n\n        topk_filled_outputs = []\n        for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(\' \')):\n            predicted_token = self.bpe.decode(predicted_token_bpe)\n            # Quick hack to fix https://github.com/pytorch/fairseq/issues/1306\n            if predicted_token_bpe.startswith(\'\\u2581\'):\n                predicted_token = \' \' + predicted_token\n            if "" {0}"".format(masked_token) in masked_input:\n                topk_filled_outputs.append((\n                    masked_input.replace(\n                        \' {0}\'.format(masked_token), predicted_token\n                    ),\n                    values[index].item(),\n                    predicted_token,\n                ))\n            else:\n                topk_filled_outputs.append((\n                    masked_input.replace(masked_token, predicted_token),\n                    values[index].item(),\n                    predicted_token,\n                ))\n        return topk_filled_outputs\n\n    def disambiguate_pronoun(self, sentence: str) -> bool:\n        """"""\n        Usage::\n\n            >>> disambiguate_pronoun(\'The _trophy_ would not fit in the brown suitcase because [it] was too big.\')\n            True\n\n            >>> disambiguate_pronoun(\'The trophy would not fit in the brown suitcase because [it] was too big.\')\n            \'The trophy\'\n        """"""\n        assert hasattr(self.task, \'disambiguate_pronoun\'), \\\n            \'roberta.disambiguate_pronoun() requires a model trained with the WSC task.\'\n        with utils.eval(self.model):\n            return self.task.disambiguate_pronoun(self.model, sentence, use_cuda=self.device.type == \'cuda\')\n'"
fairseq/models/roberta/model.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nRoBERTa: A Robustly Optimized BERT Pretraining Approach.\n""""""\n\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqEncoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import (\n    LayerNorm,\n    TransformerSentenceEncoder,\n)\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\nfrom fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\n\nfrom .hub_interface import RobertaHubInterface\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(\'roberta\')\nclass RobertaModel(FairseqEncoderModel):\n\n    @classmethod\n    def hub_models(cls):\n        return {\n            \'roberta.base\': \'http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz\',\n            \'roberta.large\': \'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\',\n            \'roberta.large.mnli\': \'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.mnli.tar.gz\',\n            \'roberta.large.wsc\': \'http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.wsc.tar.gz\',\n        }\n\n    def __init__(self, args, encoder):\n        super().__init__(encoder)\n        self.args = args\n\n        # We follow BERT\'s random weight initialization\n        self.apply(init_bert_params)\n\n        self.classification_heads = nn.ModuleDict()\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\'--encoder-layers\', type=int, metavar=\'L\',\n                            help=\'num encoder layers\')\n        parser.add_argument(\'--encoder-embed-dim\', type=int, metavar=\'H\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--encoder-ffn-embed-dim\', type=int, metavar=\'F\',\n                            help=\'encoder embedding dimension for FFN\')\n        parser.add_argument(\'--encoder-attention-heads\', type=int, metavar=\'A\',\n                            help=\'num encoder attention heads\')\n        parser.add_argument(\'--activation-fn\',\n                            choices=utils.get_available_activation_fns(),\n                            help=\'activation function to use\')\n        parser.add_argument(\'--pooler-activation-fn\',\n                            choices=utils.get_available_activation_fns(),\n                            help=\'activation function to use for pooler layer\')\n        parser.add_argument(\'--encoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each encoder block\')\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--attention-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for attention weights\')\n        parser.add_argument(\'--activation-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability after activation in FFN\')\n        parser.add_argument(\'--pooler-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability in the masked_lm pooler layers\')\n        parser.add_argument(\'--max-positions\', type=int,\n                            help=\'number of positional embeddings to learn\')\n        parser.add_argument(\'--load-checkpoint-heads\', action=\'store_true\',\n                            help=\'(re-)register and load heads when loading checkpoints\')\n        # args for ""Reducing Transformer Depth on Demand with Structured Dropout"" (Fan et al., 2019)\n        parser.add_argument(\'--encoder-layerdrop\', type=float, metavar=\'D\', default=0,\n                            help=\'LayerDrop probability for encoder\')\n        parser.add_argument(\'--encoder-layers-to-keep\', default=None,\n                            help=\'which layers to *keep* when pruning as a comma-separated list\')\n        # args for Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)\n        parser.add_argument(\'--quant-noise-pq\', type=float, metavar=\'D\', default=0,\n                            help=\'iterative PQ quantization noise at training time\')\n        parser.add_argument(\'--quant-noise-pq-block-size\', type=int, metavar=\'D\', default=8,\n                            help=\'block size of quantization noise at training time\')\n        parser.add_argument(\'--quant-noise-scalar\', type=float, metavar=\'D\', default=0,\n                            help=\'scalar quantization noise and scalar quantization at training time\')\n        parser.add_argument(\'--untie-weights-roberta\', action=\'store_true\',\n                            help=\'Untie weights between embeddings and classifiers in RoBERTa\')\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present\n        base_architecture(args)\n\n        if not hasattr(args, \'max_positions\'):\n            args.max_positions = args.tokens_per_sample\n\n        encoder = RobertaEncoder(args, task.source_dictionary)\n        return cls(args, encoder)\n\n    def forward(self, src_tokens, features_only=False, return_all_hiddens=False, classification_head_name=None, **kwargs):\n        if classification_head_name is not None:\n            features_only = True\n\n        x, extra = self.encoder(src_tokens, features_only, return_all_hiddens, **kwargs)\n\n        if classification_head_name is not None:\n            x = self.classification_heads[classification_head_name](x)\n        return x, extra\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n        """"""Get normalized probabilities (or log probs) from a net\'s output.""""""\n        logits = net_output[0].float()\n        if log_probs:\n            return F.log_softmax(logits, dim=-1)\n        else:\n            return F.softmax(logits, dim=-1)\n\n    def register_classification_head(self, name, num_classes=None, inner_dim=None, **kwargs):\n        """"""Register a classification head.""""""\n        if name in self.classification_heads:\n            prev_num_classes = self.classification_heads[name].out_proj.out_features\n            prev_inner_dim = self.classification_heads[name].dense.out_features\n            if num_classes != prev_num_classes or inner_dim != prev_inner_dim:\n                logger.warning(\n                    \'re-registering head ""{}"" with num_classes {} (prev: {}) \'\n                    \'and inner_dim {} (prev: {})\'.format(\n                        name, num_classes, prev_num_classes, inner_dim, prev_inner_dim\n                    )\n                )\n        self.classification_heads[name] = RobertaClassificationHead(\n            self.args.encoder_embed_dim,\n            inner_dim or self.args.encoder_embed_dim,\n            num_classes,\n            self.args.pooler_activation_fn,\n            self.args.pooler_dropout,\n            self.args.quant_noise_pq,\n            self.args.quant_noise_pq_block_size,\n        )\n\n    @property\n    def supported_targets(self):\n        return {\'self\'}\n\n    @classmethod\n    def from_pretrained(cls, model_name_or_path, checkpoint_file=\'model.pt\', data_name_or_path=\'.\', bpe=\'gpt2\', **kwargs):\n        from fairseq import hub_utils\n        x = hub_utils.from_pretrained(\n            model_name_or_path,\n            checkpoint_file,\n            data_name_or_path,\n            archive_map=cls.hub_models(),\n            bpe=bpe,\n            load_checkpoint_heads=True,\n            **kwargs,\n        )\n        return RobertaHubInterface(x[\'args\'], x[\'task\'], x[\'models\'][0])\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        prefix = name + \'.\' if name != \'\' else \'\'\n\n        # rename decoder -> encoder before upgrading children modules\n        for k in list(state_dict.keys()):\n            if k.startswith(prefix + \'decoder\'):\n                new_k = prefix + \'encoder\' + k[len(prefix + \'decoder\'):]\n                state_dict[new_k] = state_dict[k]\n                del state_dict[k]\n\n        # upgrade children modules\n        super().upgrade_state_dict_named(state_dict, name)\n\n        # Handle new classification heads present in the state dict.\n        current_head_names = (\n            [] if not hasattr(self, \'classification_heads\')\n            else self.classification_heads.keys()\n        )\n        keys_to_delete = []\n        for k in state_dict.keys():\n            if not k.startswith(prefix + \'classification_heads.\'):\n                continue\n\n            head_name = k[len(prefix + \'classification_heads.\'):].split(\'.\')[0]\n            num_classes = state_dict[prefix + \'classification_heads.\' + head_name + \'.out_proj.weight\'].size(0)\n            inner_dim = state_dict[prefix + \'classification_heads.\' + head_name + \'.dense.weight\'].size(0)\n\n            if getattr(self.args, \'load_checkpoint_heads\', False):\n                if head_name not in current_head_names:\n                    self.register_classification_head(head_name, num_classes, inner_dim)\n            else:\n                if head_name not in current_head_names:\n                    logger.warning(\n                        \'deleting classification head ({}) from checkpoint \'\n                        \'not present in current model: {}\'.format(head_name, k)\n                    )\n                    keys_to_delete.append(k)\n                elif (\n                    num_classes != self.classification_heads[head_name].out_proj.out_features\n                    or inner_dim != self.classification_heads[head_name].dense.out_features\n                ):\n                    logger.warning(\n                        \'deleting classification head ({}) from checkpoint \'\n                        \'with different dimensions than current model: {}\'.format(head_name, k)\n                    )\n                    keys_to_delete.append(k)\n        for k in keys_to_delete:\n            del state_dict[k]\n\n        # Copy any newly-added classification heads into the state dict\n        # with their current weights.\n        if hasattr(self, \'classification_heads\'):\n            cur_state = self.classification_heads.state_dict()\n            for k, v in cur_state.items():\n                if prefix + \'classification_heads.\' + k not in state_dict:\n                    logger.info(\'Overwriting \' + prefix + \'classification_heads.\' + k)\n                    state_dict[prefix + \'classification_heads.\' + k] = v\n\n\nclass RobertaLMHead(nn.Module):\n    """"""Head for masked language modeling.""""""\n\n    def __init__(self, embed_dim, output_dim, activation_fn, weight=None):\n        super().__init__()\n        self.dense = nn.Linear(embed_dim, embed_dim)\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.layer_norm = LayerNorm(embed_dim)\n\n        if weight is None:\n            weight = nn.Linear(embed_dim, output_dim, bias=False).weight\n        self.weight = weight\n        self.bias = nn.Parameter(torch.zeros(output_dim))\n\n    def forward(self, features, masked_tokens=None, **kwargs):\n        # Only project the masked tokens while training,\n        # saves both memory and computation\n        if masked_tokens is not None:\n            features = features[masked_tokens, :]\n\n        x = self.dense(features)\n        x = self.activation_fn(x)\n        x = self.layer_norm(x)\n        # project back to size of vocabulary with bias\n        x = F.linear(x, self.weight) + self.bias\n        return x\n\n\nclass RobertaClassificationHead(nn.Module):\n    """"""Head for sentence-level classification tasks.""""""\n\n    def __init__(self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout, q_noise=0, qn_block_size=8):\n        super().__init__()\n        self.dense = nn.Linear(input_dim, inner_dim)\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.dropout = nn.Dropout(p=pooler_dropout)\n        self.out_proj = apply_quant_noise_(\n            nn.Linear(inner_dim, num_classes), q_noise, qn_block_size\n        )\n\n    def forward(self, features, **kwargs):\n        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = self.activation_fn(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n\nclass RobertaEncoder(FairseqEncoder):\n    """"""RoBERTa encoder.""""""\n\n    def __init__(self, args, dictionary):\n        super().__init__(dictionary)\n        self.args = args\n\n        if args.encoder_layers_to_keep:\n            args.encoder_layers = len(args.encoder_layers_to_keep.split("",""))\n\n        self.sentence_encoder = TransformerSentenceEncoder(\n            padding_idx=dictionary.pad(),\n            vocab_size=len(dictionary),\n            num_encoder_layers=args.encoder_layers,\n            embedding_dim=args.encoder_embed_dim,\n            ffn_embedding_dim=args.encoder_ffn_embed_dim,\n            num_attention_heads=args.encoder_attention_heads,\n            dropout=args.dropout,\n            attention_dropout=args.attention_dropout,\n            activation_dropout=args.activation_dropout,\n            layerdrop=args.encoder_layerdrop,\n            max_seq_len=args.max_positions,\n            num_segments=0,\n            encoder_normalize_before=True,\n            apply_bert_init=True,\n            activation_fn=args.activation_fn,\n            q_noise=args.quant_noise_pq,\n            qn_block_size=args.quant_noise_pq_block_size,\n        )\n        args.untie_weights_roberta = getattr(args, \'untie_weights_roberta\', False)\n\n        self.lm_head = RobertaLMHead(\n            embed_dim=args.encoder_embed_dim,\n            output_dim=len(dictionary),\n            activation_fn=args.activation_fn,\n            weight=self.sentence_encoder.embed_tokens.weight if not args.untie_weights_roberta else None,\n        )\n\n    def forward(self, src_tokens, features_only=False, return_all_hiddens=False, masked_tokens=None, **unused):\n        """"""\n        Args:\n            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n            features_only (bool, optional): skip LM head and just return\n                features. If True, the output will be of shape\n                `(batch, src_len, embed_dim)`.\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n\n        Returns:\n            tuple:\n                - the LM output of shape `(batch, src_len, vocab)`\n                - a dictionary of additional data, where \'inner_states\'\n                  is a list of hidden states. Note that the hidden\n                  states have shape `(src_len, batch, vocab)`.\n        """"""\n        x, extra = self.extract_features(src_tokens, return_all_hiddens=return_all_hiddens)\n        if not features_only:\n            x = self.output_layer(x, masked_tokens=masked_tokens)\n        return x, extra\n\n    def extract_features(self, src_tokens, return_all_hiddens=False, **unused):\n        inner_states, _ = self.sentence_encoder(\n            src_tokens,\n            last_state_only=not return_all_hiddens,\n        )\n        features = inner_states[-1].transpose(0, 1)  # T x B x C -> B x T x C\n        return features, {\'inner_states\': inner_states if return_all_hiddens else None}\n\n    def output_layer(self, features, masked_tokens=None, **unused):\n        return self.lm_head(features, masked_tokens)\n\n    def max_positions(self):\n        """"""Maximum output length supported by the encoder.""""""\n        return self.args.max_positions\n\n\n@register_model_architecture(\'roberta\', \'roberta\')\ndef base_architecture(args):\n    args.encoder_layers = getattr(args, \'encoder_layers\', 12)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 768)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 3072)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 12)\n\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    args.pooler_activation_fn = getattr(args, \'pooler_activation_fn\', \'tanh\')\n\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_dropout = getattr(args, \'activation_dropout\', 0.0)\n    args.pooler_dropout = getattr(args, \'pooler_dropout\', 0.0)\n    args.encoder_layers_to_keep = getattr(args, \'encoder_layers_to_keep\', None)\n    args.encoder_layerdrop = getattr(args, \'encoder_layerdrop\', 0.0)\n\n\n@register_model_architecture(\'roberta\', \'roberta_base\')\ndef roberta_base_architecture(args):\n    base_architecture(args)\n\n\n@register_model_architecture(\'roberta\', \'roberta_large\')\ndef roberta_large_architecture(args):\n    args.encoder_layers = getattr(args, \'encoder_layers\', 24)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1024)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 4096)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 16)\n    base_architecture(args)\n\n\n@register_model_architecture(\'roberta\', \'xlm\')\ndef xlm_architecture(args):\n    args.encoder_layers = getattr(args, \'encoder_layers\', 16)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1280)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 1280*4)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 16)\n    base_architecture(args)\n'"
fairseq/models/roberta/model_camembert.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nCamemBERT: a Tasty French Language Model\n""""""\n\nfrom fairseq.models import register_model\n\nfrom .hub_interface import RobertaHubInterface\nfrom .model import RobertaModel\n\n\n@register_model(\'camembert\')\nclass CamembertModel(RobertaModel):\n\n    @classmethod\n    def hub_models(cls):\n        return {\n            \'camembert\': \'http://dl.fbaipublicfiles.com/fairseq/models/camembert-base.tar.gz\',\n            \'camembert.v0\': \'http://dl.fbaipublicfiles.com/fairseq/models/camembert-base.tar.gz\',\n            \'camembert-base\': \'http://dl.fbaipublicfiles.com/fairseq/models/camembert-base.tar.gz\',\n            \'camembert-large\': \'http://dl.fbaipublicfiles.com/fairseq/models/camembert-large.tar.gz\',\n            \'camembert-base-ccnet\': \'http://dl.fbaipublicfiles.com/fairseq/models/camembert-base-ccnet.tar.gz\',\n            \'camembert-base-ccnet-4gb\': \'http://dl.fbaipublicfiles.com/fairseq/models/camembert-base-ccnet-4gb.tar.gz\',\n            \'camembert-base-wikipedia-4gb\': \'http://dl.fbaipublicfiles.com/fairseq/models/camembert-base-wikipedia-4gb.tar.gz\',\n            \'camembert-base-oscar-4gb\': \'http://dl.fbaipublicfiles.com/fairseq/models/camembert-base-oscar-4gb.tar.gz\',\n        }\n\n    @classmethod\n    def from_pretrained(cls, model_name_or_path, checkpoint_file=\'model.pt\', data_name_or_path=\'.\', bpe=\'sentencepiece\', **kwargs):\n        from fairseq import hub_utils\n        x = hub_utils.from_pretrained(\n            model_name_or_path,\n            checkpoint_file,\n            data_name_or_path,\n            archive_map=cls.hub_models(),\n            bpe=bpe,\n            load_checkpoint_heads=True,\n            **kwargs,\n        )\n        return RobertaHubInterface(x[\'args\'], x[\'task\'], x[\'models\'][0])\n'"
fairseq/models/roberta/model_xlmr.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nUnsupervised Cross-lingual Representation Learning at Scale\n""""""\n\nfrom fairseq.models import register_model\n\nfrom .hub_interface import RobertaHubInterface\nfrom .model import RobertaModel\n\n\n@register_model(\'xlmr\')\nclass XLMRModel(RobertaModel):\n\n    @classmethod\n    def hub_models(cls):\n        return {\n            \'xlmr.base\': \'http://dl.fbaipublicfiles.com/fairseq/models/xlmr.base.tar.gz\',\n            \'xlmr.large\': \'http://dl.fbaipublicfiles.com/fairseq/models/xlmr.large.tar.gz\',\n        }\n\n    @classmethod\n    def from_pretrained(cls, model_name_or_path, checkpoint_file=\'model.pt\', data_name_or_path=\'.\', bpe=\'sentencepiece\', **kwargs):\n        from fairseq import hub_utils\n        x = hub_utils.from_pretrained(\n            model_name_or_path,\n            checkpoint_file,\n            data_name_or_path,\n            archive_map=cls.hub_models(),\n            bpe=bpe,\n            load_checkpoint_heads=True,\n            **kwargs,\n        )\n        return RobertaHubInterface(x[\'args\'], x[\'task\'], x[\'models\'][0])\n'"
fairseq/modules/dynamicconv_layer/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .dynamicconv_layer import DynamicconvLayer  # noqa\n'"
fairseq/modules/dynamicconv_layer/cuda_function_gen.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\ndef gen_forward():\n\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    blocks = [32, 64, 128, 256]\n\n    head = """"""\n/**\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\n#include ""dynamicconv_cuda.cuh""\n\nstd::vector<at::Tensor> dynamicconv_cuda_forward(at::Tensor input, at::Tensor weight, int padding_l) {\n\n    at::DeviceGuard g(input.device());\n    const auto minibatch = input.size(0);\n    const auto numFeatures = input.size(1);\n    const auto sequenceLength = input.size(2);\n\n    const auto numHeads = weight.size(1);\n    const auto filterSize = weight.size(2);\n\n    const auto numFiltersInBlock = numFeatures / numHeads;\n    const dim3 blocks(minibatch, numFeatures);\n\n    auto output = at::zeros_like(input);\n    auto stream = at::cuda::getCurrentCUDAStream();\n""""""\n\n    switch = """"""\n    switch(filterSize) {\n""""""\n\n    case_k = """"""\n        case {k}:\n""""""\n\n    main_block = """"""\n            if (padding_l == {pad}) {{\n                AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), ""dynamicconv_forward"", ([&] {{\n                    dynamicconv_forward_kernel<{k}, {b_size}, {pad}, scalar_t>\n                    <<<blocks, {b_size}, 0, stream>>>(\n                            input.data<scalar_t>(),\n                            weight.data<scalar_t>(),\n                            minibatch,\n                            sequenceLength,\n                            numFeatures,\n                            numFiltersInBlock,\n                            numHeads,\n                            output.data<scalar_t>());\n                }}));\n            }} else\n""""""\n\n    bad_padding = """"""\n            {\n                std::cout << ""WARNING: Unsupported padding size - skipping forward pass"" << std::endl;\n            }\n            break;\\n\n""""""\n\n    end = """"""\n        default:\n            std::cout << ""WARNING: Unsupported filter length passed - skipping forward pass"" << std::endl;\n    }\n\n    return {output};\n}\n""""""\n\n    with open(""dynamicconv_cuda_forward.cu"", \'w\') as forward:\n        forward.write(head)\n        forward.write(switch)\n        for k in kernels:\n            b_size = 32\n            for b in blocks:\n                if b > k:\n                    b_size = b\n                    break\n            forward.write(case_k.format(k=k))\n            for pad in [k // 2, k - 1]:\n                forward.write(main_block.format(k=k, b_size=b_size, pad=pad))\n            forward.write(bad_padding)\n        forward.write(end)\n\n\ndef gen_backward():\n\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    thresh = [512, 512, 512, 512, 512, 380, 256, 256]\n    min_block = [64, 64, 64, 64, 64, 64, 128, 256]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n\n    head = """"""\n/**\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\n#include ""dynamicconv_cuda.cuh""\n\nstd::vector<at::Tensor> dynamicconv_cuda_backward(at::Tensor gradOutput, int padding_l, at::Tensor input, at::Tensor weight) {\n\n    at::DeviceGuard g(input.device());\n    const auto minibatch = input.size(0);\n    const auto numFeatures = input.size(1);\n    const auto sequenceLength = input.size(2);\n\n    const auto numHeads = weight.size(1);\n    const auto filterSize = weight.size(2);\n\n    const auto numFiltersInBlock = numFeatures / numHeads;\n    auto numChunks = 1;\n\n    auto gradInput = at::zeros_like(input);\n    auto gradWeight = at::zeros_like(weight);\n    auto stream = at::cuda::getCurrentCUDAStream();\n\n    dim3 blocks(minibatch, numHeads, numChunks);\n""""""\n\n    sequence_if = """"""\n    if (sequenceLength < {seq}) {{\n        switch(filterSize) {{\n""""""\n\n    case_k = """"""\n            case {k}:\n""""""\n\n    chunks_reset = """"""\n                numChunks = int(ceilf(sequenceLength/float({b_size})));\n                blocks = dim3(minibatch, numHeads, numChunks);\n""""""\n\n    main_block = """"""\n                if (padding_l == {p}) {{\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(gradOutput.scalar_type(), ""dynamicconv_backward"", ([&] {{\n                        dynamicconv_backward_kernel<{k}, {b_size}, {p}, scalar_t>\n                        <<<blocks, {b_size}, 0, stream>>>(\n                                    gradOutput.data<scalar_t>(),\n                                    input.data<scalar_t>(),\n                                    weight.data<scalar_t>(),\n                                    minibatch,\n                                    sequenceLength,\n                                    numFeatures,\n                                    numFiltersInBlock,\n                                    numHeads,\n                                    gradWeight.data<scalar_t>(),\n                                    gradInput.data<scalar_t>());\n                    }}));\n                }} else\n""""""\n\n    bad_padding = """"""\n                {\n                    std::cout << ""WARNING: Unsupported padding size - skipping backward pass"" << std::endl;\n                }\n                break;\\n\n""""""\n\n    bad_filter = """"""\n            default:\n                std::cout << ""WARNING: Unsupported filter length passed - skipping backward pass"" << std::endl;\n        }\n""""""\n\n    con_else = """"""\n    } else\n""""""\n\n    final_else = """"""\n    {\n        switch(filterSize) {\n""""""\n\n    last_return = """"""\n    }\n    return {gradInput, gradWeight};\n}\n""""""\n\n    with open(""dynamicconv_cuda_backward.cu"", \'w\') as backward:\n        backward.write(head)\n        for seq in seqs:\n            backward.write(sequence_if.format(seq=seq))\n            for k, t, m in zip(kernels, thresh, min_block):\n                backward.write(case_k.format(k=k))\n                if seq <= t:\n                    b_size = seq\n                else:\n                    b_size = m\n                    backward.write(chunks_reset.format(b_size=b_size))\n                for p in [k // 2, k - 1]:\n                    backward.write(main_block.format(k=k, b_size=b_size, p=p))\n                backward.write(bad_padding)\n            backward.write(bad_filter)\n            backward.write(con_else)\n        backward.write(final_else)\n        for k, m in zip(kernels, min_block):\n            backward.write(case_k.format(k=k))\n            backward.write(chunks_reset.format(b_size=m))\n            for p in [k // 2, k - 1]:\n                backward.write(main_block.format(k=k, b_size=m, p=p))\n            backward.write(bad_padding)\n        backward.write(bad_filter)\n        backward.write(last_return)\n\n\nif __name__ == ""__main__"":\n    gen_forward()\n    gen_backward()\n'"
fairseq/modules/dynamicconv_layer/dynamicconv_layer.py,6,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nimport torch.nn.functional as F\n\nimport dynamicconv_cuda\nfrom fairseq import utils\nfrom fairseq.modules.unfold import unfold1d\nfrom fairseq.incremental_decoding_utils import with_incremental_state\n\n\nclass dynamicconvFunction(Function):\n\n    @staticmethod\n    def forward(ctx, x, weights, padding_l):\n        ctx.padding_l = padding_l\n        outputs = dynamicconv_cuda.forward(x, weights, padding_l)\n        variables = [x, weights]\n        ctx.save_for_backward(*variables)\n        return outputs[0]\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        outputs = dynamicconv_cuda.backward(\n                grad_output.contiguous(),\n                ctx.padding_l,\n                *ctx.saved_tensors)\n        grad_input, grad_weights = outputs\n        return grad_input, grad_weights, None\n\n\n@with_incremental_state\nclass DynamicconvLayer(nn.Module):\n    def __init__(\n            self,\n            input_size,\n            kernel_size=1,\n            padding_l=None,\n            weight_softmax=False,\n            num_heads=1,\n            weight_dropout=0.,\n            bias=False,\n            renorm_padding=False,\n            conv_bias=False,\n            query_size=None):\n\n        super(DynamicconvLayer, self).__init__()\n        self.input_size = input_size\n        self.query_size = input_size if query_size is None else query_size\n        self.kernel_size = kernel_size\n        self.padding_l = padding_l\n        self.num_heads = num_heads\n        self.weight_softmax = weight_softmax\n        self.weight_dropout = weight_dropout\n        self.renorm_padding = renorm_padding\n        self.bias = bias\n\n        self.weight_linear = nn.Linear(input_size, num_heads * kernel_size, bias)\n        if conv_bias:\n            self.conv_bias = nn.Parameter(torch.Tensor(input_size))\n        else:\n            self.conv_bias = None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_linear.weight)\n        if self.conv_bias is not None:\n            nn.init.constant_(self.conv_bias, 0.)\n            nn.init.constant_(self.weight_linaer.bias, 0.)\n\n    def forward(self, x, incremental_state=None, query=None, unfold=None):\n\n        T, B, C = x.size()\n        K, H = self.kernel_size, self.num_heads\n        # R = C // H\n\n        # during inference time, incremental BMM is faster\n        if incremental_state is not None:\n            unfold = x.size(0) > 512 if unfold is None else unfold  # use unfold mode as default for long sequence to save memory\n            unfold = unfold or (incremental_state is not None)\n            assert query is None\n\n            if query is None:\n                query = x\n            if unfold:\n                output = self._forward_unfolded(x, incremental_state, query)\n            else:\n                output = self._forward_expanded(x, incremental_state, query)\n\n            if self.conv_bias is not None:\n                output = output + self.conv_bias.view(1, 1, -1)\n\n            return output\n\n        # during training time, use CUDA kernel\n        else:\n            weight = self.weight_linear(x).view(T, B, H, K)\n            if self.weight_softmax:\n                weight = F.softmax(weight, dim=-1)\n            if self.weight_dropout:\n                weight = F.dropout(weight, self.weight_dropout, training=self.training)\n\n            weight = weight.permute(1, 2, 3, 0).contiguous()\n            self.filters = weight\n            x = x.permute(1, 2, 0).contiguous()\n            output = dynamicconvFunction.apply(x, weight, self.padding_l).permute(2, 0, 1)\n            if self.conv_bias is not None:\n                output = output + self.conv_bias.view(1, 1, -1)\n            return output\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            input_buffer = input_buffer.index_select(1, new_order)\n            self._set_input_buffer(incremental_state, input_buffer)\n\n    def _get_input_buffer(self, incremental_state):\n        return utils.get_incremental_state(self, incremental_state, 'input_buffer')\n\n    def _set_input_buffer(self, incremental_state, new_buffer):\n        return utils.set_incremental_state(self, incremental_state, 'input_buffer', new_buffer)\n\n    def _forward_unfolded(self, x, incremental_state, query):\n        '''The conventional implementation of convolutions.\n        Unfolding the input by having a window shifting to the right.'''\n        T, B, C = x.size()\n        K, H = self.kernel_size, self.num_heads\n        R = C // H\n        assert R * H == C == self.input_size\n\n        weight = self.weight_linear(query).view(T*B*H, -1)\n\n        # renorm_padding is only implemented in _forward_expanded\n        assert not self.renorm_padding or incremental_state is not None\n\n        if incremental_state is not None:\n            input_buffer = self._get_input_buffer(incremental_state)\n            if input_buffer is None:\n                input_buffer = x.new()\n            x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)\n            if self.kernel_size > 1:\n                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size+1:])\n            x_unfold = x_unfold.view(T*B*H, R, -1)\n        else:\n            padding_l = self.padding_l\n            if K > T and padding_l == K-1:\n                weight = weight.narrow(1, K-T, T)\n                K, padding_l = T, T-1\n            # unfold the input: T x B x C --> T' x B x C x K\n            x_unfold = unfold1d(x, K, padding_l, 0)\n            x_unfold = x_unfold.view(T*B*H, R, K)\n\n        if self.weight_softmax and not self.renorm_padding:\n            weight = F.softmax(weight, dim=1)\n        weight = weight.narrow(1, 0, K)\n\n        if incremental_state is not None:\n            weight = weight[:, -x_unfold.size(2):]\n            K = weight.size(1)\n\n        if self.weight_softmax and self.renorm_padding:\n            weight = F.softmax(weight, dim=1)\n\n        weight = F.dropout(weight, self.weight_dropout, training=self.training, inplace=False)\n\n        output = torch.bmm(x_unfold, weight.unsqueeze(2))  # T*B*H x R x 1\n        output = output.view(T, B, C)\n        return output\n\n    def _forward_expanded(self, x, incremental_stat, query):\n        '''Turn the convolution filters into band matrices and do matrix multiplication.\n        This is faster when the sequence is short, but less memory efficient.\n        This is not used in the decoder during inference.\n        '''\n        T, B, C = x.size()\n        K, H = self.kernel_size, self.num_heads\n        R = C // H\n        assert R * H == C == self.input_size\n        weight = self.weight_linear(query).view(T*B*H, -1)\n\n        if not self.renorm_padding:\n            if self.weight_softmax:\n                weight = F.softmax(weight, dim=1)\n            weight = F.dropout(weight, self.weight_dropout, training=self.training, inplace=False)\n        weight = weight.narrow(1, 0, K).contiguous()\n        weight = weight.view(T, B*H, K).transpose(0, 1)\n\n        x = x.view(T, B*H, R).transpose(0, 1)\n        if self.weight_softmax and self.renorm_padding:\n            # turn the convolution filters into band matrices\n            weight_expanded = weight.new(B*H, T, T+K-1).fill_(float('-inf'))\n            weight_expanded.as_strided((B*H, T, K), (T*(T+K-1), T+K, 1)).copy_(weight)\n            weight_expanded = weight_expanded.narrow(2, self.padding_l, T)\n            # normalize the weight over valid positions like self-attention\n            weight_expanded = F.softmax(weight_expanded, dim=2)\n            weight_expanded = F.dropout(weight_expanded, self.weight_dropout, training=self.training, inplace=False)\n        else:\n            P = self.padding_l\n            # For efficieny, we cut the kernel size and reduce the padding when the kernel is larger than the length\n            if K > T and P == K-1:\n                weight = weight.narrow(2, K-T, T)\n                K, P = T, T-1\n            # turn the convolution filters into band matrices\n            weight_expanded = weight.new_zeros(B*H, T, T+K-1, requires_grad=False)\n            weight_expanded.as_strided((B*H, T, K), (T*(T+K-1), T+K, 1)).copy_(weight)\n            weight_expanded = weight_expanded.narrow(2, P, T)  # B*H x T x T\n        output = torch.bmm(weight_expanded, x)\n        output = output.transpose(0, 1).contiguous().view(T, B, C)\n        return output\n"""
fairseq/modules/dynamicconv_layer/setup.py,1,"b""#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CUDAExtension, BuildExtension\n\nsetup(\n    name='dynamicconv_layer',\n    ext_modules=[\n        CUDAExtension(\n            name='dynamicconv_cuda',\n            sources=[\n                'dynamicconv_cuda.cpp',\n                'dynamicconv_cuda_kernel.cu',\n            ],\n        ),\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })\n"""
fairseq/modules/lightconv_layer/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .lightconv_layer import LightconvLayer  # noqa\n'"
fairseq/modules/lightconv_layer/cuda_function_gen.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\ndef gen_forward():\n\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n\n    head = """"""\n/**\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\n#include ""lightconv_cuda.cuh""\n\nstd::vector<at::Tensor> lightconv_cuda_forward(at::Tensor input, at::Tensor filters, int padding_l) {\n\n    at::DeviceGuard g(input.device());\n    const auto minibatch = input.size(0);\n    const auto numFeatures = input.size(1);\n    const auto sequenceLength = input.size(2);\n\n    const auto numHeads = filters.size(0);\n    const auto filterSize = filters.size(1);\n\n    const auto numFiltersInBlock = numFeatures / numHeads;\n\n    const dim3 blocks(minibatch, numFeatures);\n\n    auto output = at::zeros_like(input);\n    auto stream = at::cuda::getCurrentCUDAStream();\n""""""\n\n    sequence_if = """"""\n    if (sequenceLength <= {seq}) {{\n        switch(filterSize) {{\n""""""\n\n    case_k = """"""\n            case {k}:\n""""""\n\n    main_block = """"""\n                if (padding_l == {pad}) {{\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), ""lightconv_forward"", ([&] {{\n                        lightconv_forward_kernel<{k}, {b_size}, {pad}, scalar_t>\n                        <<<blocks, {b_size}, 0, stream>>>(\n                                input.data<scalar_t>(),\n                                filters.data<scalar_t>(),\n                                minibatch,\n                                sequenceLength,\n                                numFeatures,\n                                numFiltersInBlock,\n                                output.data<scalar_t>());\n                    }}));\n                }} else\n""""""\n\n    bad_padding = """"""\n                {\n                    std::cout << ""WARNING: Unsupported padding size - skipping forward pass"" << std::endl;\n                }\n                break;\n""""""\n\n    bad_filter = """"""\n            default:\n                std::cout << ""WARNING: Unsupported filter length passed - skipping forward pass"" << std::endl;\n        }\n""""""\n\n    con_else = """"""\n    } else\n""""""\n\n    final_else = """"""\n    {\n        switch(filterSize) {\n""""""\n\n    final_return = """"""\n    }\n\n    return {output};\n}\n""""""\n\n    with open(""lightconv_cuda_forward.cu"", \'w\') as forward:\n        forward.write(head)\n        for seq in seqs:\n            forward.write(sequence_if.format(seq=seq))\n            for k in kernels:\n                forward.write(case_k.format(k=k))\n                for pad in [k // 2, k - 1]:\n                    forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n                forward.write(bad_padding)\n            forward.write(bad_filter)\n            forward.write(con_else)\n\n        forward.write(final_else)\n        for k in kernels:\n            forward.write(case_k.format(k=k))\n            for pad in [k // 2, k - 1]:\n                forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n            forward.write(bad_padding)\n        forward.write(bad_filter)\n        forward.write(final_return)\n\n\ndef gen_backward():\n\n    head = """"""\n/**\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n\n#include ""lightconv_cuda.cuh""\n\nstd::vector<at::Tensor> lightconv_cuda_backward(\n        at::Tensor gradOutput,\n        int padding_l,\n        at::Tensor input,\n        at::Tensor filters) {\n\n    // gradWrtInput\n    const int minibatch = input.size(0);\n    const int numFeatures = input.size(1);\n    const int sequenceLength = input.size(2);\n\n    const int numHeads = filters.size(0);\n    const int filterSize = filters.size(1);\n\n    const dim3 gradBlocks(minibatch, numFeatures);\n    const dim3 weightGradFirstpassShortBlocks(minibatch, numHeads);\n    const dim3 weightGradSecondpassBlocks(numHeads, filterSize);\n\n    const int numFiltersInBlock = numFeatures / numHeads;\n\n    auto gradInput = at::zeros_like(input);\n    auto gradFilters = at::zeros_like(filters);\n\n    at::DeviceGuard g(input.device());\n    auto stream = at::cuda::getCurrentCUDAStream();\n\n    switch(filterSize) {\n""""""\n\n    sequence_if = """"""\n            if (sequenceLength <= {seq}) {{\n""""""\n\n    case_k = """"""\n        case {k}:\n""""""\n\n    main_block = """"""\n                if (padding_l == {p}) {{\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), ""lightconv_backward"", ([&] {{\n                        lightconv_grad_wrt_input_kernel<{k}, {b_size}, {p}, scalar_t>\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\n                                gradOutput.data<scalar_t>(),\n                                filters.data<scalar_t>(),\n                                minibatch,\n                                sequenceLength,\n                                numFeatures,\n                                numFiltersInBlock,\n                                gradInput.data<scalar_t>());\n\n""""""\n\n    weight_grad_short = """"""\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numHeads, filterSize}}, input.options().dtype(at::kFloat));\n                        lightconv_grad_wrt_weights_firstpass_short_kernel<{k}, {b_size}, {p}, scalar_t>\n                        <<<weightGradFirstpassShortBlocks, {b_size}, 0, stream>>>(\n                                input.data<scalar_t>(),\n                                gradOutput.data<scalar_t>(),\n                                minibatch,\n                                sequenceLength,\n                                numFeatures,\n                                numFiltersInBlock,\n                                numHeads,\n                                tempSumGradFilters.data<float>()\n                        );\n\n                        lightconv_grad_wrt_weights_secondpass_short_kernel<{k}, {b_size}, scalar_t>\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\n                                tempSumGradFilters.data<float>(),\n                                minibatch,\n                                numFiltersInBlock,\n                                gradFilters.data<scalar_t>()\n                        );\n                    }}));\n                }} else\n""""""\n\n    weight_grad = """"""\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numFeatures, filterSize}}, input.options().dtype(at::kFloat));\n                        lightconv_grad_wrt_weights_firstpass_kernel<{k}, {b_size}, {p}, scalar_t>\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\n                                input.data<scalar_t>(),\n                                gradOutput.data<scalar_t>(),\n                                minibatch,\n                                sequenceLength,\n                                numFeatures,\n                                numFiltersInBlock,\n                                tempSumGradFilters.data<float>()\n                        );\n\n                        lightconv_grad_wrt_weights_secondpass_kernel<{k}, {b_size}, scalar_t>\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\n                                tempSumGradFilters.data<float>(),\n                                minibatch,\n                                numFiltersInBlock,\n                                gradFilters.data<scalar_t>()\n                        );\n                    }}));\n                }} else\n""""""\n\n    bad_padding = """"""\n                {\n                    std::cout << ""WARNING: Unsupported padding size - skipping backward pass"" << std::endl;\n                }\n""""""\n\n    breakout = """"""\n                break;\n""""""\n\n    bad_filter = """"""\n        default:\n            std::cout << ""WARNING: Unsupported filter length passed - skipping backward pass"" << std::endl;\n""""""\n\n    con_else = """"""\n            } else\n""""""\n\n    final_else = """"""\n    {\n        switch(filterSize) {\n""""""\n\n    last_return = """"""\n    }\n    return {gradInput, gradFilters};\n}\n""""""\n\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    thresh = [32, 32, 64, 128, 256, -1, -1, -1]\n    max_mem = [-1, -1, -1, -1, -1, 192, 96, 64]\n\n    with open(""lightconv_cuda_backward.cu"", \'w\') as backward:\n        backward.write(head)\n        for (k, t, mem) in zip(kernels, thresh, max_mem):\n            backward.write(case_k.format(k=k))\n            for seq in seqs:\n                if (t == -1 or seq <= t) and (mem == -1 or seq < mem):\n                    backward.write(sequence_if.format(seq=seq))\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=seq, p=p))\n                        backward.write(weight_grad_short.format(k=k, b_size=seq, p=p))\n                    backward.write(bad_padding)\n                else:\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=32, p=p))\n                        backward.write(weight_grad.format(k=k, b_size=32, p=p))\n                    backward.write(bad_padding)\n                    backward.write(breakout)\n                    break\n                backward.write(con_else)\n        backward.write(bad_filter)\n        backward.write(last_return)\n\n\nif __name__ == ""__main__"":\n    gen_forward()\n    gen_backward()\n'"
fairseq/modules/lightconv_layer/lightconv_layer.py,6,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nimport torch.nn.functional as F\n\nimport lightconv_cuda\nfrom fairseq import utils\nfrom fairseq.incremental_decoding_utils import with_incremental_state\n\n\nclass lightconvFunction(Function):\n\n    @staticmethod\n    def forward(ctx, x, weights, padding_l):\n        ctx.padding_l = padding_l\n        outputs = lightconv_cuda.forward(x, weights, padding_l)\n        variables = [x, weights]\n        ctx.save_for_backward(*variables)\n        return outputs[0]\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        outputs = lightconv_cuda.backward(\n                grad_output.contiguous(),\n                ctx.padding_l,\n                *ctx.saved_tensors)\n        grad_input, grad_weights = outputs\n        return grad_input, grad_weights, None\n\n\n@with_incremental_state\nclass LightconvLayer(nn.Module):\n    def __init__(\n            self,\n            input_size,\n            kernel_size=1,\n            padding_l=None,\n            weight_softmax=False,\n            num_heads=1,\n            weight_dropout=0.,\n            bias=False):\n        super(LightconvLayer, self).__init__()\n        self.input_size = input_size\n        self.kernel_size = kernel_size\n        self.padding_l = padding_l\n        self.num_heads = num_heads\n        self.weight_softmax = weight_softmax\n        self.weight_dropout = weight_dropout\n\n        self.weight = nn.Parameter(torch.Tensor(num_heads, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(input_size))\n        else:\n            self.bias = None\n        self.reset_parameters()\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        prefix = name + '.' if name != '' else ''\n        for k, v in state_dict.items():\n            if k.endswith(prefix + 'weight'):\n                if v.dim() == 3 and v.size(1) == 1:\n                    state_dict[k] = v.squeeze(1)\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight)\n        if self.bias is not None:\n            nn.init.constant_(self.bias, 0.)\n\n    def forward(self, x, incremental_state=None):\n\n        # during inference time, incremental BMM is faster\n        if incremental_state is not None:\n            T, B, C = x.size()\n            K, H = self.kernel_size, self.num_heads\n            R = C // H\n            input_buffer = self._get_input_buffer(incremental_state)\n            if input_buffer is None:\n                input_buffer = x.new()\n            x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)\n            if self.kernel_size > 1:\n                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size+1:])\n            x_unfold = x_unfold.view(T*B*H, R, -1)\n\n            weight = self.weight\n            if self.weight_softmax:\n                weight = F.softmax(weight.float(), dim=1).type_as(weight)\n\n            weight = weight[:, -x_unfold.size(2):]\n\n            K = weight.size(1)\n\n            weight = weight.view(1, H, K).expand(T*B, H, K).contiguous().view(T*B*H, K, 1)\n\n            weight = F.dropout(weight, self.weight_dropout, training=self.training)\n            output = torch.bmm(x_unfold, weight)  # T*B*H x R x 1\n            output = output.view(T, B, C)\n            return output\n\n        # during training time, use CUDA kernel\n        else:\n            x = x.permute(1, 2, 0).contiguous()\n            weight = self.weight\n            if self.weight_softmax:\n                weight = F.softmax(self.weight, -1)\n            if self.weight_dropout:\n                weight = F.dropout(weight, self.weight_dropout, training=self.training)\n            return lightconvFunction.apply(x, weight, self.padding_l).permute(2, 0, 1)\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            input_buffer = input_buffer.index_select(1, new_order)\n            self._set_input_buffer(incremental_state, input_buffer)\n\n    def _get_input_buffer(self, incremental_state):\n        return utils.get_incremental_state(self, incremental_state, 'input_buffer')\n\n    def _set_input_buffer(self, incremental_state, new_buffer):\n        return utils.set_incremental_state(self, incremental_state, 'input_buffer', new_buffer)\n\n    def half(self):\n        return self._apply(lambda t: t.half() if t.is_floating_point() else t)\n"""
fairseq/modules/lightconv_layer/setup.py,1,"b""#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CUDAExtension, BuildExtension\n\nsetup(\n    name='lightconv_layer',\n    ext_modules=[\n        CUDAExtension('lightconv_cuda', [\n            'lightconv_cuda.cpp',\n            'lightconv_cuda_kernel.cu',\n        ]),\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })\n"""
fairseq/modules/quantization/__init__.py,0,b''
fairseq/modules/quantization/quantization_options.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\ndef parse_config_yaml(yaml_data):\n    # Initialize to default options.\n    quantization_options = {\n        ""n_centroids"": {\n            ""Linear"": [""in_features"", {""*"": 256}],\n            ""Embedding"": [""embedding_dim"", {""*"": 256}],\n        },\n        ""block_sizes"": {\n            ""Linear"": [""fuzzy_name"", {""fc"": 8, ""attn"": 4, ""emb"": 4}],\n            ""Embedding"": [""fuzzy_name"", {""emb"": 8}],\n        },\n        ""layers_to_quantize"": [\n            ""decoder\\\\.layers\\\\.\\\\d+\\\\.fc[12]"",\n            ""decoder\\\\.embed_tokens\\\\.embeddings\\\\.[012]\\\\.[01]"",\n            ""decoder\\\\.layers\\\\.\\\\d+\\\\.self_attn\\\\.(k_proj|v_proj|q_proj|out_proj)"",\n        ],\n    }\n\n    if ""n_centroids"" in yaml_data:\n        quantization_options[""n_centroids""] = {\n            layer: convert_yaml_to_tuple(layer_data)\n            for layer, layer_data in yaml_data[""n_centroids""].items()\n        }\n    if ""block_sizes"" in yaml_data:\n        quantization_options[""block_sizes""] = {\n            layer: convert_yaml_to_tuple(layer_data)\n            for layer, layer_data in yaml_data[""block_sizes""].items()\n        }\n    if ""layers_to_quantize"" in yaml_data:\n        quantization_options[""layers_to_quantize""] = yaml_data[""layers_to_quantize""]\n\n    return quantization_options\n\n\ndef convert_yaml_to_tuple(yaml_dictionary):\n    """"""Converts a yaml dictionary with two keys: `key` and `value` into a two\n    argument tuple of those values.""""""\n    return (yaml_dictionary[""key""], yaml_dictionary[""value""])\n'"
fairseq/optim/lr_scheduler/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\nfrom fairseq import registry\nfrom fairseq.optim.lr_scheduler.fairseq_lr_scheduler import FairseqLRScheduler\n\n\nbuild_lr_scheduler, register_lr_scheduler, LR_SCHEDULER_REGISTRY = registry.setup_registry(\n    '--lr-scheduler',\n    base_class=FairseqLRScheduler,\n    default='fixed',\n)\n\n# automatically import any Python files in the optim/lr_scheduler/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        module = file[:file.find('.py')]\n        importlib.import_module('fairseq.optim.lr_scheduler.' + module)\n"""
fairseq/optim/lr_scheduler/cosine_lr_scheduler.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nfrom . import FairseqLRScheduler, register_lr_scheduler\n\n\n@register_lr_scheduler(\'cosine\')\nclass CosineSchedule(FairseqLRScheduler):\n    """"""Assign LR based on a cyclical schedule that follows the cosine function.\n\n    See https://arxiv.org/pdf/1608.03983.pdf for details.\n\n    We also support a warmup phase where we linearly increase the learning rate\n    from some initial learning rate (``--warmup-init-lr``) until the configured\n    max learning rate (``--max-lr``).\n\n    During warmup::\n\n      lrs = torch.linspace(args.warmup_init_lr, args.lr, args.warmup_updates)\n      lr = lrs[update_num]\n\n    After warmup::\n\n      lr = lr_min + 0.5*(lr_max - lr_min)*(1 + cos(t_curr / t_i))\n\n    where ``t_curr`` is current percentage of updates within the current period\n    range and ``t_i`` is the current period range, which is scaled by ``t_mul``\n    after every iteration.\n    """"""\n\n    def __init__(self, args, optimizer):\n        super().__init__(args, optimizer)\n        if len(args.lr) > 1:\n            raise ValueError(\n                \'Cannot use a fixed learning rate schedule with cosine.\'\n                \' Consider --lr-scheduler=fixed instead.\'\n            )\n\n        warmup_end_lr = args.max_lr\n        if args.warmup_init_lr < 0:\n            args.warmup_init_lr = args.lr[0]\n\n        self.min_lr = args.lr[0]\n        self.max_lr = args.max_lr\n\n        assert self.max_lr > self.min_lr, \'max_lr must be more than lr\'\n\n        self.t_mult = args.t_mult\n        self.period = args.lr_period_updates\n\n        if self.period <= 0:\n            assert args.max_update >= 0, \'Either --max_update or --lr-period-updates must be set\'\n            self.period = args.max_update - args.warmup_updates\n\n        if args.warmup_updates > 0:\n            # linearly warmup for the first args.warmup_updates\n            self.lr_step = (warmup_end_lr - args.warmup_init_lr) / args.warmup_updates\n        else:\n            self.lr_step = 1\n\n        self.warmup_updates = args.warmup_updates\n        self.lr_shrink = args.lr_shrink\n\n        # initial learning rate\n        self.lr = args.warmup_init_lr\n        self.optimizer.set_lr(self.lr)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add arguments to the parser for this LR scheduler.""""""\n        # fmt: off\n        parser.add_argument(\'--warmup-updates\', default=0, type=int, metavar=\'N\',\n                            help=\'warmup the learning rate linearly for the first N updates\')\n        parser.add_argument(\'--warmup-init-lr\', default=-1, type=float, metavar=\'LR\',\n                            help=\'initial learning rate during warmup phase; default is args.lr\')\n        parser.add_argument(\'--max-lr\', type=float, metavar=\'LR\',\n                            help=\'max learning rate, must be more than args.lr\')\n        parser.add_argument(\'--t-mult\', default=1, type=float, metavar=\'LR\',\n                            help=\'factor to grow the length of each period\')\n        parser.add_argument(\'--lr-period-updates\', default=-1, type=float, metavar=\'LR\',\n                            help=\'initial number of updates per period\')\n        parser.add_argument(\'--lr-shrink\', default=0.1, type=float, metavar=\'LS\',\n                            help=\'shrink factor for annealing\')\n        # fmt: on\n\n    def step(self, epoch, val_loss=None):\n        """"""Update the learning rate at the end of the given epoch.""""""\n        super().step(epoch, val_loss)\n        # we don\'t change the learning rate at epoch boundaries\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        """"""Update the learning rate after each update.""""""\n        if num_updates < self.args.warmup_updates:\n            self.lr = self.args.warmup_init_lr + num_updates * self.lr_step\n        else:\n            curr_updates = num_updates - self.args.warmup_updates\n            if self.t_mult != 1:\n                i = math.floor(math.log(1 - curr_updates / self.period * (1 - self.t_mult), self.t_mult))\n                t_i = self.t_mult ** i * self.period\n                t_curr = curr_updates - (1 - self.t_mult ** i) / (1 - self.t_mult) * self.period\n            else:\n                i = math.floor(curr_updates / self.period)\n                t_i = self.period\n                t_curr = curr_updates - (self.period * i)\n\n            lr_shrink = self.lr_shrink ** i\n            min_lr = self.min_lr * lr_shrink\n            max_lr = self.max_lr * lr_shrink\n\n            self.lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * t_curr / t_i))\n\n        self.optimizer.set_lr(self.lr)\n        return self.lr\n'"
fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .. import FairseqOptimizer\n\n\nclass FairseqLRScheduler(object):\n\n    def __init__(self, args, optimizer):\n        super().__init__()\n        if not isinstance(optimizer, FairseqOptimizer):\n            raise ValueError(\'optimizer must be an instance of FairseqOptimizer\')\n        self.args = args\n        self.optimizer = optimizer\n        self.best = None\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add arguments to the parser for this LR scheduler.""""""\n        pass\n\n    def state_dict(self):\n        """"""Return the LR scheduler state dict.""""""\n        return {\'best\': self.best}\n\n    def load_state_dict(self, state_dict):\n        """"""Load an LR scheduler state dict.""""""\n        self.best = state_dict[\'best\']\n\n    def step(self, epoch, val_loss=None):\n        """"""Update the learning rate at the end of the given epoch.""""""\n        if val_loss is not None:\n            if self.best is None:\n                self.best = val_loss\n            else:\n                self.best = min(self.best, val_loss)\n\n    def step_update(self, num_updates):\n        """"""Update the learning rate after each update.""""""\n        return self.optimizer.get_lr()\n'"
fairseq/optim/lr_scheduler/fixed_schedule.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import FairseqLRScheduler, register_lr_scheduler\n\n\n@register_lr_scheduler(\'fixed\')\nclass FixedSchedule(FairseqLRScheduler):\n    """"""Decay the LR on a fixed schedule.""""""\n\n    def __init__(self, args, optimizer):\n        super().__init__(args, optimizer)\n\n        # set defaults\n        args.warmup_updates = getattr(args, \'warmup_updates\', 0) or 0\n\n        self.lr = args.lr[0]\n        if args.warmup_updates > 0:\n            self.warmup_factor = 1. / args.warmup_updates\n        else:\n            self.warmup_factor = 1\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add arguments to the parser for this LR scheduler.""""""\n        # fmt: off\n        parser.add_argument(\'--force-anneal\', \'--fa\', type=int, metavar=\'N\',\n                            help=\'force annealing at specified epoch\')\n        parser.add_argument(\'--lr-shrink\', default=0.1, type=float, metavar=\'LS\',\n                            help=\'shrink factor for annealing, lr_new = (lr * lr_shrink)\')\n        parser.add_argument(\'--warmup-updates\', default=0, type=int, metavar=\'N\',\n                            help=\'warmup the learning rate linearly for the first N updates\')\n        # fmt: on\n\n    def get_next_lr(self, epoch):\n        lrs = self.args.lr\n        if self.args.force_anneal is None or epoch < self.args.force_anneal:\n            # use fixed LR schedule\n            next_lr = lrs[min(epoch, len(lrs) - 1)]\n        else:\n            # annneal based on lr_shrink\n            next_lr = lrs[-1] * self.args.lr_shrink ** (epoch + 1 - self.args.force_anneal)\n        return next_lr\n\n    def step(self, epoch, val_loss=None):\n        """"""Update the learning rate at the end of the given epoch.""""""\n        super().step(epoch, val_loss)\n        self.lr = self.get_next_lr(epoch)\n        self.optimizer.set_lr(self.warmup_factor * self.lr)\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        """"""Update the learning rate after each update.""""""\n        if self.args.warmup_updates > 0 and num_updates < self.args.warmup_updates:\n            self.warmup_factor = (num_updates + 1) / float(self.args.warmup_updates)\n            self.optimizer.set_lr(self.warmup_factor * self.lr)\n        else:\n            self.optimizer.set_lr(self.lr)\n        return self.optimizer.get_lr()\n'"
fairseq/optim/lr_scheduler/inverse_square_root_schedule.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import FairseqLRScheduler, register_lr_scheduler\n\n\n@register_lr_scheduler(\'inverse_sqrt\')\nclass InverseSquareRootSchedule(FairseqLRScheduler):\n    """"""Decay the LR based on the inverse square root of the update number.\n\n    We also support a warmup phase where we linearly increase the learning rate\n    from some initial learning rate (``--warmup-init-lr``) until the configured\n    learning rate (``--lr``). Thereafter we decay proportional to the number of\n    updates, with a decay factor set to align with the configured learning rate.\n\n    During warmup::\n\n      lrs = torch.linspace(args.warmup_init_lr, args.lr, args.warmup_updates)\n      lr = lrs[update_num]\n\n    After warmup::\n\n      decay_factor = args.lr * sqrt(args.warmup_updates)\n      lr = decay_factor / sqrt(update_num)\n    """"""\n\n    def __init__(self, args, optimizer):\n        super().__init__(args, optimizer)\n        if len(args.lr) > 1:\n            raise ValueError(\n                \'Cannot use a fixed learning rate schedule with inverse_sqrt.\'\n                \' Consider --lr-scheduler=fixed instead.\'\n            )\n        warmup_end_lr = args.lr[0]\n        if args.warmup_init_lr < 0:\n            args.warmup_init_lr = 0 if args.warmup_updates > 0 else warmup_end_lr\n\n        # linearly warmup for the first args.warmup_updates\n        self.lr_step = (warmup_end_lr - args.warmup_init_lr) / args.warmup_updates\n\n        # then, decay prop. to the inverse square root of the update number\n        self.decay_factor = warmup_end_lr * args.warmup_updates**0.5\n\n        # initial learning rate\n        self.lr = args.warmup_init_lr\n        self.optimizer.set_lr(self.lr)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add arguments to the parser for this LR scheduler.""""""\n        # fmt: off\n        parser.add_argument(\'--warmup-updates\', default=4000, type=int, metavar=\'N\',\n                            help=\'warmup the learning rate linearly for the first N updates\')\n        parser.add_argument(\'--warmup-init-lr\', default=-1, type=float, metavar=\'LR\',\n                            help=\'initial learning rate during warmup phase; default is args.lr\')\n        # fmt: on\n\n    def step(self, epoch, val_loss=None):\n        """"""Update the learning rate at the end of the given epoch.""""""\n        super().step(epoch, val_loss)\n        # we don\'t change the learning rate at epoch boundaries\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        """"""Update the learning rate after each update.""""""\n        if num_updates < self.args.warmup_updates:\n            self.lr = self.args.warmup_init_lr + num_updates*self.lr_step\n        else:\n            self.lr = self.decay_factor * num_updates**-0.5\n        self.optimizer.set_lr(self.lr)\n        return self.lr\n'"
fairseq/optim/lr_scheduler/polynomial_decay_schedule.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import FairseqLRScheduler, register_lr_scheduler\n\n\n@register_lr_scheduler(\'polynomial_decay\')\nclass PolynomialDecaySchedule(FairseqLRScheduler):\n    """"""Decay the LR on a fixed schedule.""""""\n\n    def __init__(self, args, optimizer):\n        super().__init__(args, optimizer)\n\n        # set defaults\n        args.warmup_updates = getattr(args, \'warmup_updates\', 0) or 0\n\n        self.lr = args.lr[0]\n        if args.warmup_updates > 0:\n            self.warmup_factor = 1. / args.warmup_updates\n        else:\n            self.warmup_factor = 1\n        self.end_learning_rate = args.end_learning_rate\n        self.total_num_update = args.total_num_update\n        self.power = args.power\n        self.optimizer.set_lr(self.warmup_factor * self.lr)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add arguments to the parser for this LR scheduler.""""""\n        parser.add_argument(\'--force-anneal\', \'--fa\', type=int, metavar=\'N\',\n                            help=\'force annealing at specified epoch\')\n        parser.add_argument(\'--warmup-updates\', default=0, type=int, metavar=\'N\',\n                            help=\'warmup the learning rate linearly for the first N updates\')\n        parser.add_argument(\'--end-learning-rate\', default=0.0, type=float)\n        parser.add_argument(\'--power\', default=1.0, type=float)\n        parser.add_argument(\'--total-num-update\', default=1000000, type=int)\n\n    def get_next_lr(self, epoch):\n        lrs = self.args.lr\n        if self.args.force_anneal is None or epoch < self.args.force_anneal:\n            # use fixed LR schedule\n            next_lr = lrs[min(epoch, len(lrs) - 1)]\n        else:\n            # annneal based on lr_shrink\n            next_lr = self.optimizer.get_lr()\n        return next_lr\n\n    def step(self, epoch, val_loss=None):\n        """"""Update the learning rate at the end of the given epoch.""""""\n        super().step(epoch, val_loss)\n        self.lr = self.get_next_lr(epoch)\n        self.optimizer.set_lr(self.warmup_factor * self.lr)\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        """"""Update the learning rate after each update.""""""\n        if self.args.warmup_updates > 0 and num_updates <= self.args.warmup_updates:\n            self.warmup_factor = num_updates / float(self.args.warmup_updates)\n            lr = self.warmup_factor * self.lr\n        elif num_updates >= self.total_num_update:\n            lr = self.end_learning_rate\n        else:\n            warmup = self.args.warmup_updates\n            lr_range = self.lr - self.end_learning_rate\n            pct_remaining = 1 - (num_updates - warmup) / (self.total_num_update - warmup)\n            lr = lr_range * pct_remaining ** (self.power) + self.end_learning_rate\n        self.optimizer.set_lr(lr)\n        return self.optimizer.get_lr()\n'"
fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.optim.lr_scheduler\n\nfrom . import FairseqLRScheduler, register_lr_scheduler\n\n\n@register_lr_scheduler(\'reduce_lr_on_plateau\')\nclass ReduceLROnPlateau(FairseqLRScheduler):\n    """"""\n    Decay the LR by a factor every time the validation loss plateaus.\n    Also comes with optional warmup phase, where we linearly increase\n    the learning rate from some initial learning rate\n    (``--warmup-init-lr``) until the configured learning rate\n    (``--lr``). Thereafter the lr is adjusted according to original\n    reduce_on_plateau scheme.\n\n    During warmup::\n\n      lrs = torch.linspace(\n          args.warmup_init_lr, args.lr, args.warmup_updates\n      )\n      lr = lrs[update_num]\n    """"""\n\n    def __init__(self, args, optimizer):\n        super().__init__(args, optimizer)\n        if len(args.lr) > 1:\n            raise ValueError(\n                \'Cannot use a fixed learning rate schedule with reduce_lr_on_plateau.\'\n                \' Consider --lr-scheduler=fixed instead.\'\n            )\n        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer.optimizer, patience=0, factor=args.lr_shrink,\n            threshold=args.lr_threshold)\n        warmup_end_lr = args.lr[0]\n        # if no warm up, sets initial lr to be args.lr[0]\n        if args.warmup_init_lr < 0:\n            args.warmup_init_lr = 0 if args.warmup_updates > 0 else warmup_end_lr\n\n        # linearly warmup for the first args.warmup_updates\n        if args.warmup_updates > 0:\n            self.lr_step = (warmup_end_lr - args.warmup_init_lr) / args.warmup_updates\n        # this flag is either set from arg when no warm up, or set by\n        # step_update() when warmup finishes\n        self.warmup_end = True if args.warmup_updates <= 0 else False\n        # initial learning rate\n        # this self.lr is used only during init and/or warm up period\n        self.lr = args.warmup_init_lr\n        self.optimizer.set_lr(self.lr)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add arguments to the parser for this LR scheduler.""""""\n        # fmt: off\n        parser.add_argument(\'--lr-shrink\', default=0.1, type=float, metavar=\'LS\',\n                            help=\'shrink factor for annealing, lr_new = (lr * lr_shrink)\')\n        parser.add_argument(\'--lr-threshold\', default=1e-4, type=float, metavar=\'LT\',\n                            help=\'Threshold for measuring the new optimum, \\\n                            to only focus on significant changes\')\n        parser.add_argument(\'--warmup-updates\', default=0, type=int, metavar=\'N\',\n                            help=\'warmup the learning rate linearly for the first N updates\')\n        parser.add_argument(\'--warmup-init-lr\', default=-1, type=float, metavar=\'LR\',\n                            help=\'initial learning rate during warmup phase; default is args.lr\')\n        # fmt: on\n\n    def state_dict(self):\n        """"""Return the LR scheduler state dict.""""""\n        return {\n            \'best\': self.lr_scheduler.best,\n            \'last_epoch\': self.lr_scheduler.last_epoch,\n        }\n\n    def load_state_dict(self, state_dict):\n        """"""Load an LR scheduler state dict.""""""\n        self.lr_scheduler.best = state_dict[\'best\']\n        if \'last_epoch\' in state_dict:\n            self.lr_scheduler.last_epoch = state_dict[\'last_epoch\']\n\n    def step(self, epoch, val_loss=None):\n        """"""\n        Update the learning rate at the end of the given epoch if warmup\n        finishes otherwise no update of lr on epoch boundaries\n        """"""\n        if val_loss is not None and self.warmup_end is True:\n            self.lr_scheduler.step(val_loss)\n        else:\n            self.lr_scheduler.last_epoch = epoch\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        """"""\n        Update the learning rate after each update.""""""\n        # if there is warmup\n        if self.args.warmup_updates > 0:\n            if num_updates <= self.args.warmup_updates:\n                self.lr = self.args.warmup_init_lr + num_updates*self.lr_step\n                self.optimizer.set_lr(self.lr)\n            else:\n                if self.warmup_end is False:\n                    self.warmup_end = True\n        # else do nothing\n        return self.optimizer.get_lr()\n'"
fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import FairseqLRScheduler, register_lr_scheduler\nimport math\n\n\n@register_lr_scheduler(\'tri_stage\')\nclass TriStageLRSchedule(FairseqLRScheduler):\n    """"""Tristage learning rate schedulr\n\n    Implement the learning rate scheduler in https://arxiv.org/pdf/1904.08779.pdf\n\n    Similar to inverse_squre_root scheduler, but tri_stage learning rate employs\n    three stages LR scheduling:\n\n        - warmup stage, starting from `lr` * `init_lr_scale`, linearly\n          increased to `lr` in `warmup_steps` iterations\n\n        - hold stage, after `warmup_steps`, keep the LR as `lr` for `hold_steps`\n          iterations\n\n        - decay stage, after hold stage, decay LR exponetially to\n          `lr` * `final_lr_scale` in `decay_steps`;\n          after that LR is keep as `final_lr_scale` * `lr`\n\n    During warmup::\n\n      init_lr = args.init_lr_scale * args.lr\n      lrs = torch.linspace(init_lr, args.lr, args.warmup_steps)\n      lr = lrs[update_num]\n\n    During hold::\n\n      lr = args.lr\n\n    During decay::\n\n      decay_factor = - math.log(args.final_lr_scale) / args.decay_steps\n      lr = args.lr * exp(- (update_num - warmup_steps - decay_steps) * decay_factor)\n\n    After that::\n\n      lr = args.lr * args.final_lr_scale\n    """"""\n\n    def __init__(self, args, optimizer):\n        super().__init__(args, optimizer)\n        if len(args.lr) > 1:\n            raise ValueError(\n                \'Cannot use a fixed learning rate schedule with tri-stage lr.\'\n                \' Consider --lr-scheduler=fixed instead.\'\n            )\n\n        # calculate LR at each point\n        self.peak_lr = args.lr[0]\n        self.init_lr = args.init_lr_scale * args.lr[0]\n        self.final_lr = args.final_lr_scale * args.lr[0]\n\n        # remember the steps at each stage\n        self.warmup_steps = args.warmup_steps\n        self.hold_steps = args.hold_steps\n        self.decay_steps = args.decay_steps\n\n        self.warmup_rate = (self.peak_lr - self.init_lr) / self.warmup_steps\n        self.decay_factor = -math.log(args.final_lr_scale) / args.decay_steps\n\n        # initial learning rate\n        self.lr = self.init_lr\n        self.optimizer.set_lr(self.lr)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add arguments to the parser for this LR scheduler.""""""\n        # fmt: off\n        parser.add_argument(\n            \'--warmup-steps\',\n            default=4000,\n            type=int,\n            metavar=\'N\',\n            help=\'warmup the learning rate linearly for the first N updates\'\n        )\n        parser.add_argument(\n            \'--hold-steps\',\n            default=20000,\n            type=int,\n            metavar=\'N\',\n            help=\'steps in hold stage.\'\n        )\n        parser.add_argument(\n            \'--decay-steps\',\n            default=60000,\n            type=int,\n            metavar=\'N\',\n            help=\'steps in decay stages\'\n        )\n        parser.add_argument(\n            \'--init-lr-scale\',\n            default=0.01,\n            type=float,\n            help=""""""\n    initial learning rate scale during warmup phase; default is 0.01"""""")\n        parser.add_argument(\n            \'--final-lr-scale\',\n            default=0.01,\n            type=float,\n            help=""final learning rate scale; default to 0.01""\n        )\n        # fmt: on\n\n    def _decide_stage(self, update_step):\n        """"""\n        return stage, and the corresponding steps within the current stage\n        """"""\n        if update_step < self.warmup_steps:\n            # warmup state\n            return 0, update_step\n\n        offset = self.warmup_steps\n\n        if update_step < offset + self.hold_steps:\n            # hold stage\n            return 1, update_step - offset\n\n        offset += self.hold_steps\n\n        if update_step <= offset + self.decay_steps:\n            # decay stage\n            return 2, update_step - offset\n\n        offset += self.decay_steps\n\n        # still here ? constant lr stage\n        return 3, update_step - offset\n\n    def step(self, epoch, val_loss=None):\n        """"""Update the learning rate at the end of the given epoch.""""""\n        super().step(epoch, val_loss)\n        # we don\'t change the learning rate at epoch boundaries\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        """"""Update the learning rate after each update.""""""\n        stage, steps_in_stage = self._decide_stage(num_updates)\n        if stage == 0:\n            self.lr = self.init_lr + self.warmup_rate * steps_in_stage\n        elif stage == 1:\n            self.lr = self.peak_lr\n        elif stage == 2:\n            self.lr = self.peak_lr * math.exp(-self.decay_factor * steps_in_stage)\n        elif stage == 3:\n            self.lr = self.final_lr\n        else:\n            raise ValueError(""Undefined stage"")\n\n        self.optimizer.set_lr(self.lr)\n\n        return self.lr\n'"
fairseq/optim/lr_scheduler/triangular_lr_scheduler.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nfrom . import FairseqLRScheduler, register_lr_scheduler\n\n\n@register_lr_scheduler(\'triangular\')\nclass TriangularSchedule(FairseqLRScheduler):\n    """"""Assign LR based on a triangular cyclical schedule.\n\n    See https://arxiv.org/pdf/1506.01186.pdf for details.\n    """"""\n\n    def __init__(self, args, optimizer):\n        super().__init__(args, optimizer)\n        if len(args.lr) > 1:\n            raise ValueError(\n                \'Cannot use a fixed learning rate schedule with triangular.\'\n                \' Consider --lr-scheduler=fixed instead.\'\n            )\n\n        lr = args.lr[0]\n\n        assert args.max_lr > lr, \'max_lr must be more than lr\'\n        self.min_lr = lr\n        self.max_lr = args.max_lr\n        self.stepsize = args.lr_period_updates // 2\n        self.lr_shrink = args.lr_shrink\n        self.shrink_min = args.shrink_min\n\n        # initial learning rate\n        self.lr = self.min_lr\n        self.optimizer.set_lr(self.lr)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add arguments to the parser for this LR scheduler.""""""\n        # fmt: off\n        parser.add_argument(\'--max-lr\', required=True, type=float, metavar=\'LR\',\n                            help=\'max learning rate, must be more than args.lr\')\n        parser.add_argument(\'--lr-period-updates\', default=5000, type=float, metavar=\'LR\',\n                            help=\'initial number of updates per period (cycle length)\')\n        parser.add_argument(\'--lr-shrink\', default=0.1, type=float, metavar=\'LS\',\n                            help=\'shrink factor for annealing\')\n        parser.add_argument(\'--shrink-min\', action=\'store_true\',\n                            help=\'if set, also shrinks min lr\')\n        # fmt: on\n\n    def step(self, epoch, val_loss=None):\n        """"""Update the learning rate at the end of the given epoch.""""""\n        super().step(epoch, val_loss)\n        # we don\'t change the learning rate at epoch boundaries\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        """"""Update the learning rate after each update.""""""\n        cycle = math.floor(num_updates / (2 * self.stepsize))\n\n        lr_shrink = self.lr_shrink ** cycle\n        max_lr = self.max_lr * lr_shrink\n        if self.shrink_min:\n            min_lr = self.min_lr * lr_shrink\n        else:\n            min_lr = self.min_lr\n\n        x = abs(num_updates / self.stepsize - 2 * (cycle + 1) + 1)\n        self.lr = min_lr + (max_lr - min_lr) * max(0, (1 - x))\n\n        self.optimizer.set_lr(self.lr)\n        return self.lr\n'"
examples/simultaneous_translation/eval/agents/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\nfrom fairseq import registry\n\nbuild_agent, register_agent, MONOTONIC_AGENT = registry.setup_registry('--agent-type')\n\n\nDEFAULT_EOS = '</s>'\nGET = 0\nSEND = 1\n\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        module = file[:file.find('.py')]\n        importlib.import_module('agents.' + module)\n"""
examples/simultaneous_translation/eval/agents/agent.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import GET, SEND, DEFAULT_EOS\nimport time\nfrom multiprocessing.pool import ThreadPool as Pool\nfrom functools import partial\n\n\nclass Agent(object):\n    ""an agent needs to follow this pattern""\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def init_states(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def update_states(self, states, new_state):\n        raise NotImplementedError\n\n    def finish_eval(self, states, new_state):\n        raise NotImplementedError\n\n    def policy(self, state):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def decode(self, session, low=0, high=100000, num_thread=10):\n        corpus_info = session.corpus_info()\n        high = min(corpus_info[""num_sentences""] - 1, high)\n        if low >= high:\n            return\n\n        t0 = time.time()\n        if num_thread > 1:\n            with Pool(10) as p:\n                p.map(\n                    partial(self._decode_one, session),\n                    [sent_id for sent_id in range(low, high + 1)]\n                )\n        else:\n            for sent_id in range(low, high + 1):\n                self._decode_one(session, sent_id)\n\n        print(f\'Finished {low} to {high} in {time.time() - t0}s\')\n\n    def _decode_one(self, session, sent_id):\n        action = {}\n        self.reset()\n        states = self.init_states()\n        while action.get(\'value\', None) != DEFAULT_EOS:\n            # take an action\n            action = self.policy(states)\n\n            if action[\'key\'] == GET:\n                new_states = session.get_src(sent_id, action[""value""])\n                states = self.update_states(states, new_states)\n\n            elif action[\'key\'] == SEND:\n                session.send_hypo(sent_id, action[\'value\'])\n        print("" "".join(states[""tokens""][""tgt""]))\n'"
examples/simultaneous_translation/eval/agents/simul_trans_agent.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . agent import Agent\nfrom . import DEFAULT_EOS, GET, SEND\nfrom fairseq import checkpoint_utils, utils, tasks\nimport os\nimport json\n\n\nclass SimulTransAgent(Agent):\n    def __init__(self, args):\n        # Load Model\n        self.load_model(args)\n\n        # build word spliter\n        self.build_word_splitter(args)\n\n        self.max_len = args.max_len\n\n        self.eos = DEFAULT_EOS\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument(\'--model-path\', type=str, required=True,\n                            help=\'path to your pretrained model.\')\n        parser.add_argument(""--data-bin"", type=str, required=True,\n                            help=""Path of data binary"")\n        parser.add_argument(""--user-dir"", type=str, default=""example/simultaneous_translation"",\n                            help=""User directory for simultaneous translation"")\n        parser.add_argument(""--src-splitter-type"", type=str, default=None,\n                            help=""Subword splitter type for source text"")\n        parser.add_argument(""--tgt-splitter-type"", type=str, default=None,\n                            help=""Subword splitter type for target text"")\n        parser.add_argument(""--src-splitter-path"", type=str, default=None,\n                            help=""Subword splitter model path for source text"")\n        parser.add_argument(""--tgt-splitter-path"", type=str, default=None,\n                            help=""Subword splitter model path for target text"")\n        parser.add_argument(""--max-len"", type=int, default=150,\n                            help=""Maximum length difference between source and target prediction"")\n        parser.add_argument(\'--model-overrides\', default=""{}"", type=str, metavar=\'DICT\',\n                            help=\'A dictionary used to override model args at generation \'\n                                 \'that were used during model training\')\n        # fmt: on\n        return parser\n\n    def load_dictionary(self, task):\n        raise NotImplementedError\n\n    def load_model(self, args):\n        args.user_dir = os.path.join(os.path.dirname(__file__), \'..\', \'..\')\n        utils.import_user_module(args)\n        filename = args.model_path\n        if not os.path.exists(filename):\n            raise IOError(""Model file not found: {}"".format(filename))\n\n        state = checkpoint_utils.load_checkpoint_to_cpu(filename, json.loads(args.model_overrides))\n\n        saved_args = state[""args""]\n        saved_args.data = args.data_bin\n\n        task = tasks.setup_task(saved_args)\n\n        # build model for ensemble\n        self.model = task.build_model(saved_args)\n        self.model.load_state_dict(state[""model""], strict=True)\n\n        # Set dictionary\n        self.load_dictionary(task)\n\n    def init_states(self):\n        return {\n            ""indices"": {""src"": [], ""tgt"": []},\n            ""tokens"": {""src"": [], ""tgt"": []},\n            ""segments"": {""src"": [], ""tgt"": []},\n            ""steps"": {""src"": 0, ""tgt"": 0},\n            ""finished"": False,\n            ""finish_read"": False,\n            ""model_states"": {}\n        }\n\n    def update_states(self, states, new_state):\n        raise NotImplementedError\n\n    def policy(self, states):\n        # Read and Write policy\n        action = None\n\n        while action is None:\n            if states[""finished""]:\n                # Finish the hypo by sending eos to server\n                return self.finish_action()\n\n            # Model make decision given current states\n            decision = self.model.decision_from_states(states)\n\n            if decision == 0 and not self.finish_read(states):\n                # READ\n                action = self.read_action(states)\n            else:\n                # WRITE\n                action = self.write_action(states)\n\n            # None means we make decision again but not sending server anything\n            # This happened when read a bufffered token\n            # Or predict a subword\n        return action\n\n    def finish_read(self, states):\n        raise NotImplementedError\n\n    def write_action(self, states):\n        token, index = self.model.predict_from_states(states)\n\n        if index == self.dict[""tgt""].eos() or len(states[""tokens""][""tgt""]) > self.max_len:\n            # Finish this sentence is predict EOS\n            states[""finished""] = True\n            end_idx_last_full_word = self._target_length(states)\n\n        else:\n            states[""tokens""][""tgt""] += [token]\n            end_idx_last_full_word = (\n                self.word_splitter[""tgt""]\n                .end_idx_last_full_word(states[""tokens""][""tgt""])\n            )\n            self._append_indices(states, [index], ""tgt"")\n\n        if end_idx_last_full_word > states[""steps""][""tgt""]:\n            # Only sent detokenized full words to the server\n            word = self.word_splitter[""tgt""].merge(\n                states[""tokens""][""tgt""][\n                    states[""steps""][""tgt""]: end_idx_last_full_word\n                ]\n            )\n            states[""steps""][""tgt""] = end_idx_last_full_word\n            states[""segments""][""tgt""] += [word]\n\n            return {\'key\': SEND, \'value\': word}\n        else:\n            return None\n\n    def read_action(self, states):\n        return {\'key\': GET, \'value\': None}\n\n    def finish_action(self):\n        return {\'key\': SEND, \'value\': DEFAULT_EOS}\n\n    def reset(self):\n        pass\n\n    def finish_eval(self, states, new_state):\n        if len(new_state) == 0 and len(states[""indices""][""src""]) == 0:\n            return True\n        return False\n\n    def _append_indices(self, states, new_indices, key):\n        states[""indices""][key] += new_indices\n\n    def _target_length(self, states):\n        return len(states[""tokens""][\'tgt\'])\n'"
examples/simultaneous_translation/eval/agents/simul_trans_text_agent.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . simul_trans_agent import SimulTransAgent\nfrom . import DEFAULT_EOS, GET\nfrom . import register_agent\nfrom . word_splitter import SPLITTER_DICT\n\n\n@register_agent(""simul_trans_text"")\nclass SimulTransTextAgent(SimulTransAgent):\n    def build_word_splitter(self, args):\n        self.word_splitter = {}\n\n        self.word_splitter[""src""] = SPLITTER_DICT[args.src_splitter_type](\n                getattr(args, f""src_splitter_path"")\n            )\n        self.word_splitter[""tgt""] = SPLITTER_DICT[args.tgt_splitter_type](\n                getattr(args, f""tgt_splitter_path"")\n            )\n\n    def load_dictionary(self, task):\n        self.dict = {}\n        self.dict[""tgt""] = task.target_dictionary\n        self.dict[""src""] = task.source_dictionary\n\n    def update_states(self, states, new_state):\n        if states[""finish_read""]:\n            return states\n\n        new_word = new_state[""segment""]\n\n        # Split words and index the token\n        if new_word not in [DEFAULT_EOS]:\n            tokens = self.word_splitter[""src""].split(new_word)\n            # Get indices from dictionary\n            # You can change to you own dictionary\n            indices = self.dict[""src""].encode_line(\n                tokens,\n                line_tokenizer=lambda x: x,\n                add_if_not_exist=False,\n                append_eos=False\n            ).tolist()\n        else:\n            tokens = [new_word]\n            indices = [self.dict[""src""].eos()]\n            states[""finish_read""] = True\n\n        # Update states\n        states[""segments""][""src""] += [new_word]\n        states[""tokens""][""src""] += tokens\n        self._append_indices(states, indices, ""src"")\n\n        return states\n\n    def read_action(self, states):\n        # Increase source step by one\n        states[""steps""][""src""] += 1\n\n        # At leat one word is read\n        if len(states[""tokens""][""src""]) == 0:\n            return {\'key\': GET, \'value\': None}\n\n        # Only request new word if there is no buffered tokens\n        if len(states[""tokens""][""src""]) <= states[""steps""][""src""]:\n            return {\'key\': GET, \'value\': None}\n\n        return None\n\n    def finish_read(self, states):\n        # The first means all segments (full words) has been read from server\n        # The second means all tokens (subwords) has been read locally\n        return (\n            states[""finish_read""]\n            and len(states[""tokens""][""src""]) == states[""steps""][""src""]\n        )\n'"
examples/simultaneous_translation/eval/agents/word_splitter.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nclass SubwordSplitter(object):\n    def process_line(self, string):\n        raise NotImplementedError\n\n    def split(self, string):\n        raise NotImplementedError\n\n\nclass NoneWordSplitter(object):\n    def __init__(self, model):\n        pass\n\n    def split(self, string):\n        return [string]\n\n    def process_line(self, string):\n        return [string]\n\n    def finished_word(self, string):\n        return True\n\n    def merge(self, list_of_string):\n        return """".join(list_of_string)\n\n    def last_full_word_step(self, tokens, step):\n        return len(tokens)\n\n    def end_idx_last_full_word(self, tokens):\n        return len(tokens)\n\n\nclass BPEWordSplitter(object):\n    # TODO: lock back here\n    def __init__(self, model_path):\n        super().__init__()\n        from subword_nmt.apply_bpe import BPE\n        with open(model_path) as f:\n            self.model = BPE(f)\n\n    def split(self, string):\n        return self.model.process_line(string).split()\n\n    def end_idx_last_full_word(self, tokens):\n        # Begin of word indices\n        bow_indices = [0] + [i + 1 for i, t in enumerate(tokens[1:]) if t[-2:] != \'@@\']\n\n        if len(bow_indices) < 2:\n            return 0\n        else:\n            return bow_indices[-1]\n\n    def merge(self, list_of_string):\n        return "" "".join([item.replace(""@@"", """") for item in list_of_string])\n\n\nclass SentencePieceModelWordSplitter(object):\n    def __init__(self, model_path):\n        super().__init__()\n        import sentencepiece as spm\n        self.model = spm.SentencePieceProcessor()\n        self.model.Load(model_path)\n\n    def split(self, string):\n        return self.model.EncodeAsPieces(string)\n\n    def end_idx_last_full_word(self, tokens):\n        # Begin of word indices\n        bow_indices = [i for i, t in enumerate(tokens) if t[0] == \'\\u2581\']\n\n        if len(bow_indices) < 2:\n            return 0\n        else:\n            return bow_indices[-1]\n\n    def merge(self, list_of_string):\n        return self.model.DecodePieces(list_of_string)\n\n\nSPLITTER_DICT = {\n    None: NoneWordSplitter,\n    ""BPE"": BPEWordSplitter,\n    ""SentencePieceModel"": SentencePieceModelWordSplitter,\n}\n'"
examples/simultaneous_translation/eval/scorers/__init__.py,0,"b""# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\nfrom fairseq import registry\n(\n    build_scorer,\n    register_scorer,\n    SCORER_REGISTRIES\n) = registry.setup_registry('--scorer-type')\n\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith('.py') and not file.startswith('_'):\n        module = file[:file.find('.py')]\n        importlib.import_module('scorers.' + module)\n"""
examples/simultaneous_translation/eval/scorers/scorer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom vizseq.scorers.bleu import BLEUScorer\nfrom vizseq.scorers.ter import TERScorer\nfrom vizseq.scorers.meteor import METEORScorer\nfrom examples.simultaneous_translation.eval.eval_latency import LatencyScorer\nfrom collections import defaultdict\nimport json\nimport os\n\n\nDEFAULT_EOS = \'</s>\'\n\n\nclass SimulScorer(object):\n    def __init__(self, args):\n        self.tokenizer = args.tokenizer\n        self.output_dir = args.output\n        if args.output is not None:\n            self.output_files = {\n                ""text"": os.path.join(args.output, ""text""),\n                ""delay"": os.path.join(args.output, ""delay""),\n                ""scores"": os.path.join(args.output, ""scores"")\n            }\n        else:\n            self.output_files = None\n        self.eos = DEFAULT_EOS\n        self.data = {""tgt"": []}\n        self.reset()\n\n    def get_info(self):\n        return {""num_sentences"": len(self)}\n\n    @staticmethod\n    def add_args(parser):\n        # fmt: off\n        parser.add_argument(\'--src-file\', type=str, required=True,\n                            help=\'Source input file\')\n        parser.add_argument(\'--tgt-file\', type=str, required=True,\n                            help=\'Target reference file\')\n        parser.add_argument(\'--tokenizer\', default=""13a"", choices=[""none"", ""13a""],\n                            help=\'Tokenizer used for sacrebleu\')\n        parser.add_argument(\'--output\', type=str, default=None,\n                            help=\'Path for output directory\')\n        # fmt: on\n\n    def send_src(self, sent_id, *args):\n        raise NotImplementedError\n\n    def recv_hyp(self, sent_id, list_of_tokens):\n        for token in list_of_tokens:\n            self.translations[\n                sent_id\n            ].append(\n                (\n                    token,\n                    self.steps[sent_id]\n                )\n            )\n\n    def reset(self):\n        self.steps = defaultdict(int)\n        self.translations = defaultdict(list)\n\n    def src_lengths(self):\n        raise NotImplementedError\n\n    def score(self):\n        translations = []\n        delays = []\n        for i in range(1 + max(self.translations.keys())):\n            translations += ["" "".join(t[0] for t in self.translations[i][:-1])]\n            delays += [[t[1] for t in self.translations[i]]]\n\n        bleu_score = BLEUScorer(\n            sent_level=False, corpus_level=True,\n            extra_args={\'bleu_tokenizer\': self.tokenizer}\n        ).score(translations, [self.data[""tgt""]])\n\n        ter_score = TERScorer(sent_level=False, corpus_level=True).score(\n            translations, [self.data[""tgt""]]\n        )\n        meteor_score = METEORScorer(sent_level=False, corpus_level=True).score(\n            translations, [self.data[""tgt""]]\n        )\n\n        latency_score = LatencyScorer().score(\n            [\n                {""src_len"": src_len, ""delays"": delay}\n                for src_len, delay in zip(self.src_lengths(), delays)\n            ],\n            start_from_zero=False\n        )\n\n        scores = {\n            \'BLEU\': bleu_score[0],\n            \'TER\': ter_score[0],\n            \'METEOR\': meteor_score[0],\n            \'DAL\': latency_score[\'differentiable_average_lagging\'],\n            \'AL\': latency_score[\'average_lagging\'],\n            \'AP\': latency_score[\'average_proportion\'],\n        }\n\n        if self.output_files is not None:\n            try:\n                os.makedirs(self.output_dir, exist_ok=True)\n                self.write_results_to_file(translations, delays, scores)\n            except BaseException as be:\n                print(f\'Failed to write results to {self.output_dir}.\')\n                print(be)\n                print(\'Skip writing predictions\')\n\n        return scores\n\n    def write_results_to_file(self, translations, delays, scores):\n        if self.output_files[""text""] is not None:\n            with open(self.output_files[""text""], ""w"") as f:\n                for line in translations:\n                    f.write(line + ""\\n"")\n\n        if self.output_files[""delay""] is not None:\n            with open(self.output_files[""delay""], ""w"") as f:\n                for i, delay in enumerate(delays):\n                    f.write(\n                        json.dumps(\n                            {\n                                ""src_len"": self.src_lengths()[i],\n                                ""delays"": delay\n                            }\n                        ) + ""\\n""\n                    )\n\n        with open(self.output_files[""scores""], ""w"") as f:\n            for key, value in scores.items():\n                f.write(f""{key}, {value}\\n"")\n\n    @classmethod\n    def _load_text_file(cls, file, split=False):\n        with open(file) as f:\n            if split:\n                return [r.strip().split() for r in f]\n            else:\n                return [r.strip() for r in f]\n\n    @classmethod\n    def _load_text_from_json(cls, file):\n        list_to_return = []\n        with open(file) as f:\n            content = json.load(f)\n            for item in content[""utts""].values():\n                list_to_return.append(item[""output""][""text""].strip())\n        return list_to_return\n\n    @classmethod\n    def _load_wav_info_from_json(cls, file):\n        list_to_return = []\n        with open(file) as f:\n            content = json.load(f)\n            for item in content[""utts""].values():\n                list_to_return.append(\n                    {\n                        ""path"": item[""input""][""path""].strip(),\n                        ""length"": item[""input""][""length_ms""]\n                    }\n                )\n        return list_to_return\n\n    @classmethod\n    def _load_wav_info_from_list(cls, file):\n        list_to_return = []\n        with open(file) as f:\n            for line in f:\n                list_to_return.append(\n                    {\n                        ""path"": line.strip(),\n                    }\n                )\n        return list_to_return\n\n    def __len__(self):\n        return len(self.data[""tgt""])\n'"
examples/simultaneous_translation/eval/scorers/text_scorer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . scorer import SimulScorer\nfrom . import register_scorer\n\n\n@register_scorer(""text"")\nclass SimulTextScorer(SimulScorer):\n    def __init__(self, args):\n        super().__init__(args)\n        self.data = {\n            ""src"": self._load_text_file(args.src_file, split=True),\n            ""tgt"": self._load_text_file(args.tgt_file, split=False)\n        }\n\n    def send_src(self, sent_id, *args):\n        if self.steps[sent_id] >= len(self.data[""src""][sent_id]):\n            dict_to_return = {\n                ""sent_id"": sent_id,\n                ""segment_id"": self.steps[sent_id],\n                ""segment"": self.eos\n            }\n            # Consider EOS\n            self.steps[sent_id] = len(self.data[""src""][sent_id]) + 1\n        else:\n            dict_to_return = {\n                ""sent_id"": sent_id,\n                ""segment_id"": self.steps[sent_id],\n                ""segment"": self.data[""src""][sent_id][self.steps[sent_id]]\n            }\n\n            self.steps[sent_id] += 1\n\n        return dict_to_return\n\n    def src_lengths(self):\n        # +1 for eos\n        return [len(sent) + 1 for sent in self.data[""src""]]\n'"
fairseq/model_parallel/models/roberta/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .model import *  # noqa\n'"
fairseq/model_parallel/models/roberta/model.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n""""""\nRoBERTa: A Robustly Optimized BERT Pretraining Approach.\n""""""\n\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom fairseq import utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.roberta import (\n    RobertaModel,\n    RobertaEncoder,\n    RobertaLMHead,\n    RobertaClassificationHead,\n)\nfrom fairseq.modules import (\n    LayerNorm,\n    TransformerSentenceEncoder,\n)\nfrom fairseq.model_parallel.modules import (\n    ModelParallelTransformerSentenceEncoder,\n)\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\ntry:\n    from fairseq.model_parallel.megatron.mpu import (\n        copy_to_model_parallel_region,\n        gather_from_model_parallel_region,\n        ColumnParallelLinear,\n        RowParallelLinear,\n    )\n    has_megatron_submodule = True\nexcept (ImportError, ModuleNotFoundError):\n    has_megatron_submodule = False\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(\'model_parallel_roberta\')\nclass ModelParallelRobertaModel(RobertaModel):\n\n\n    def __init__(self, args, encoder):\n        super().__init__(args, encoder)\n\n        self.classification_heads = nn.ModuleDict()\n\n    @staticmethod\n    def add_args(parser):\n        super(ModelParallelRobertaModel, ModelParallelRobertaModel).add_args(parser)\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present\n        base_architecture(args)\n\n        if not hasattr(args, \'max_positions\'):\n            args.max_positions = args.tokens_per_sample\n\n        encoder = ModelParallelRobertaEncoder(args, task.source_dictionary)\n        return cls(args, encoder)\n\n    def forward(self, src_tokens, features_only=False, return_all_hiddens=False, classification_head_name=None, **kwargs):\n        if classification_head_name is not None:\n            features_only = True\n\n        x, extra = self.encoder(src_tokens, features_only, return_all_hiddens, **kwargs)\n\n        if classification_head_name is not None:\n            x = self.classification_heads[classification_head_name](x)\n        return x, extra\n\n    def register_classification_head(self, name, num_classes=None, inner_dim=None, **kwargs):\n        """"""Register a classification head.""""""\n        if name in self.classification_heads:\n            prev_num_classes = self.classification_heads[name].out_proj.out_features\n            prev_inner_dim = self.classification_heads[name].dense.out_features\n            if num_classes != prev_num_classes or inner_dim != prev_inner_dim:\n                logger.warning(\n                    \'re-registering head ""{}"" with num_classes {} (prev: {}) \'\n                    \'and inner_dim {} (prev: {})\'.format(\n                        name, num_classes, prev_num_classes, inner_dim, prev_inner_dim\n                    )\n                )\n        self.classification_heads[name] = ModelParallelRobertaClassificationHead(\n            self.args.encoder_embed_dim,\n            inner_dim or self.args.encoder_embed_dim,\n            num_classes,\n            self.args.pooler_activation_fn,\n            self.args.pooler_dropout,\n        )\n\n\nclass ModelParallelRobertaLMHead(nn.Module):\n    """"""Head for masked language modeling.""""""\n\n    def __init__(self, embed_dim, output_dim, activation_fn, weight=None):\n        super().__init__()\n        self.dense = ColumnParallelLinear(embed_dim, embed_dim, gather_output=True)\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.layer_norm = LayerNorm(embed_dim)\n\n        if weight is None:\n            weight = nn.Linear(embed_dim, output_dim, bias=False).weight\n        self.weight = weight\n        self.bias = nn.Parameter(torch.zeros(output_dim))\n\n    def forward(self, features, masked_tokens=None, **kwargs):\n        # Only project the unmasked tokens while training,\n        # saves both memory and computation\n        if masked_tokens is not None:\n            features = features[masked_tokens, :]\n\n        x = self.dense(features)\n        x = self.activation_fn(x)\n        x = self.layer_norm(x)\n\n        features = copy_to_model_parallel_region(features)\n        # project back to size of vocabulary with bias\n        x = F.linear(x, self.weight)\n        x = gather_from_model_parallel_region(x).contiguous()\n        x = x + self.bias\n        return x\n\n\nclass ModelParallelRobertaClassificationHead(nn.Module):\n    """"""Head for sentence-level classification tasks.""""""\n\n    def __init__(self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout):\n        super().__init__()\n        self.dense = ColumnParallelLinear(input_dim, inner_dim, gather_output=True)\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.dropout = nn.Dropout(p=pooler_dropout)\n        self.out_proj = nn.Linear(inner_dim, num_classes)\n\n    def forward(self, features, **kwargs):\n        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = self.activation_fn(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n\nclass ModelParallelRobertaEncoder(FairseqEncoder):\n    """"""RoBERTa encoder.\n\n    Implements the :class:`~fairseq.models.FairseqDecoder` interface required\n    by :class:`~fairseq.models.FairseqLanguageModel`.\n    """"""\n\n    def __init__(self, args, dictionary):\n        super().__init__(dictionary)\n        self.args = args\n\n        # RoBERTa is a sentence encoder model, so users will intuitively trim\n        # encoder layers. However, the implementation uses the fairseq decoder,\n        # so we fix here.\n        if args.encoder_layers_to_keep:\n            args.encoder_layers = len(args.encoder_layers_to_keep.split("",""))\n            args.decoder_layers_to_keep = args.encoder_layers_to_keep\n            args.encoder_layers_to_keep = None\n\n        self.sentence_encoder = ModelParallelTransformerSentenceEncoder(\n            padding_idx=dictionary.pad(),\n            vocab_size=len(dictionary),\n            num_encoder_layers=args.encoder_layers,\n            embedding_dim=args.encoder_embed_dim,\n            ffn_embedding_dim=args.encoder_ffn_embed_dim,\n            num_attention_heads=args.encoder_attention_heads,\n            dropout=args.dropout,\n            attention_dropout=args.attention_dropout,\n            activation_dropout=args.activation_dropout,\n            layerdrop=args.encoder_layerdrop,\n            max_seq_len=args.max_positions,\n            num_segments=0,\n            encoder_normalize_before=False,\n            apply_bert_init=False,\n            activation_fn=args.activation_fn,\n        )\n        self.lm_head = ModelParallelRobertaLMHead(\n            embed_dim=args.encoder_embed_dim,\n            output_dim=len(dictionary),\n            activation_fn=args.activation_fn,\n            weight=self.sentence_encoder.embed_tokens.weight,\n        )\n\n    def forward(self, src_tokens, features_only=False, return_all_hiddens=False, masked_tokens=None, **unused):\n        """"""\n        Args:\n            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n            features_only (bool, optional): skip LM head and just return\n                features. If True, the output will be of shape\n                `(batch, src_len, embed_dim)`.\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n\n        Returns:\n            tuple:\n                - the LM output of shape `(batch, src_len, vocab)`\n                - a dictionary of additional data, where \'inner_states\'\n                  is a list of hidden states. Note that the hidden\n                  states have shape `(src_len, batch, vocab)`.\n        """"""\n        x, extra = self.extract_features(src_tokens, return_all_hiddens=return_all_hiddens)\n        if not features_only:\n            x = self.output_layer(x, masked_tokens=masked_tokens)\n        return x, extra\n\n    def extract_features(self, src_tokens, return_all_hiddens=False, **unused):\n        inner_states, _ = self.sentence_encoder(\n            src_tokens,\n            last_state_only=not return_all_hiddens,\n        )\n        features = inner_states[-1].transpose(0, 1)  # T x B x C -> B x T x C\n        return features, {\'inner_states\': inner_states if return_all_hiddens else None}\n\n    def output_layer(self, features, masked_tokens=None, **unused):\n        return self.lm_head(features, masked_tokens)\n\n    def max_positions(self):\n        """"""Maximum output length supported by the encoder.""""""\n        return self.args.max_positions\n\n\n@register_model_architecture(\'model_parallel_roberta\', \'model_parallel_roberta\')\ndef base_architecture(args):\n    args.encoder_layers = getattr(args, \'encoder_layers\', 12)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 768)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 3072)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 12)\n\n    args.activation_fn = getattr(args, \'activation_fn\', \'gelu\')\n    args.pooler_activation_fn = getattr(args, \'pooler_activation_fn\', \'tanh\')\n\n    args.dropout = getattr(args, \'dropout\', 0.1)\n    args.attention_dropout = getattr(args, \'attention_dropout\', 0.1)\n    args.activation_dropout = getattr(args, \'activation_dropout\', 0.0)\n    args.pooler_dropout = getattr(args, \'pooler_dropout\', 0.0)\n    args.encoder_layers_to_keep = getattr(args, \'encoder_layers_to_keep\', None)\n    args.encoder_layerdrop = getattr(args, \'encoder_layerdrop\', 0.0)\n\n\n@register_model_architecture(\'model_parallel_roberta\', \'model_parallel_roberta_base\')\ndef roberta_base_architecture(args):\n    base_architecture(args)\n\n\n@register_model_architecture(\'model_parallel_roberta\', \'model_parallel_roberta_large\')\ndef roberta_large_architecture(args):\n    args.encoder_layers = getattr(args, \'encoder_layers\', 24)\n    args.encoder_embed_dim = getattr(args, \'encoder_embed_dim\', 1024)\n    args.encoder_ffn_embed_dim = getattr(args, \'encoder_ffn_embed_dim\', 4096)\n    args.encoder_attention_heads = getattr(args, \'encoder_attention_heads\', 16)\n    base_architecture(args)\n'"
fairseq/modules/quantization/pq/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .utils import SizeTracker, quantize_model_  # NOQA\n'"
fairseq/modules/quantization/pq/em.py,14,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport random\nimport logging\nfrom collections import Counter\n\nimport torch\n\n\nclass EM:\n    """"""\n    EM algorithm used to quantize the columns of W to minimize\n\n                         ||W - W_hat||^2\n\n    Args:\n        - W: weight matrix of size (in_features x out_features)\n        - n_iter: number of k-means iterations\n        - n_centroids: number of centroids (size of codebook)\n        - eps: for cluster reassignment when an empty cluster is found\n        - max_tentatives for cluster reassignment when an empty cluster is found\n        - verbose: print error after each iteration\n\n    Remarks:\n        - If one cluster is empty, the most populated cluster is split into\n          two clusters\n        - All the relevant dimensions are specified in the code\n    """"""\n\n    def __init__(\n        self, W, n_centroids=256, n_iter=20, eps=1e-6, max_tentatives=30, verbose=True\n    ):\n        self.W = W\n        self.n_centroids = n_centroids\n        self.n_iter = n_iter\n        self.eps = eps\n        self.max_tentatives = max_tentatives\n        self.verbose = verbose\n        self.centroids = torch.Tensor()\n        self.assignments = torch.Tensor()\n        self.objective = []\n\n    def initialize_centroids(self):\n        """"""\n        Initializes the centroids by sampling random columns from W.\n        """"""\n\n        in_features, out_features = self.W.size()\n        indices = torch.randint(\n            low=0, high=out_features, size=(self.n_centroids,)\n        ).long()\n        self.centroids = self.W[:, indices].t()  # (n_centroids x in_features)\n\n    def step(self, i):\n        """"""\n        There are two standard steps for each iteration: expectation (E) and\n        minimization (M). The E-step (assignment) is performed with an exhaustive\n        search and the M-step (centroid computation) is performed with\n        the exact solution.\n\n        Args:\n            - i: step number\n\n        Remarks:\n            - The E-step heavily uses PyTorch broadcasting to speed up computations\n              and reduce the memory overhead\n        """"""\n\n        # assignments (E-step)\n        distances = self.compute_distances()  # (n_centroids x out_features)\n        self.assignments = torch.argmin(distances, dim=0)  # (out_features)\n        n_empty_clusters = self.resolve_empty_clusters()\n\n        # centroids (M-step)\n        for k in range(self.n_centroids):\n            W_k = self.W[:, self.assignments == k]  # (in_features x size_of_cluster_k)\n            self.centroids[k] = W_k.mean(dim=1)  # (in_features)\n\n        # book-keeping\n        obj = (self.centroids[self.assignments].t() - self.W).norm(p=2).item()\n        self.objective.append(obj)\n        if self.verbose:\n            logging.info(\n                f""Iteration: {i},\\t""\n                f""objective: {obj:.6f},\\t""\n                f""resolved empty clusters: {n_empty_clusters}""\n            )\n\n    def resolve_empty_clusters(self):\n        """"""\n        If one cluster is empty, the most populated cluster is split into\n        two clusters by shifting the respective centroids. This is done\n        iteratively for a fixed number of tentatives.\n        """"""\n\n        # empty clusters\n        counts = Counter(map(lambda x: x.item(), self.assignments))\n        empty_clusters = set(range(self.n_centroids)) - set(counts.keys())\n        n_empty_clusters = len(empty_clusters)\n\n        tentatives = 0\n        while len(empty_clusters) > 0:\n            # given an empty cluster, find most populated cluster and split it into two\n            k = random.choice(list(empty_clusters))\n            m = counts.most_common(1)[0][0]\n            e = torch.randn_like(self.centroids[m]) * self.eps\n            self.centroids[k] = self.centroids[m].clone()\n            self.centroids[k] += e\n            self.centroids[m] -= e\n\n            # recompute assignments\n            distances = self.compute_distances()  # (n_centroids x out_features)\n            self.assignments = torch.argmin(distances, dim=0)  # (out_features)\n\n            # check for empty clusters\n            counts = Counter(map(lambda x: x.item(), self.assignments))\n            empty_clusters = set(range(self.n_centroids)) - set(counts.keys())\n\n            # increment tentatives\n            if tentatives == self.max_tentatives:\n                logging.info(\n                    f""Could not resolve all empty clusters, {len(empty_clusters)} remaining""\n                )\n                raise EmptyClusterResolveError\n            tentatives += 1\n\n        return n_empty_clusters\n\n    def compute_distances(self):\n        """"""\n        For every centroid m, computes\n\n                          ||M - m[None, :]||_2\n\n        Remarks:\n            - We rely on PyTorch\'s broadcasting to speed up computations\n              and reduce the memory overhead\n            - Without chunking, the sizes in the broadcasting are modified as:\n              (n_centroids x n_samples x out_features) -> (n_centroids x out_features)\n            - The broadcasting computation is automatically chunked so that\n              the tensors fit into the memory of the GPU\n        """"""\n\n        nb_centroids_chunks = 1\n\n        while True:\n            try:\n                return torch.cat(\n                    [\n                        (self.W[None, :, :] - centroids_c[:, :, None]).norm(p=2, dim=1)\n                        for centroids_c in self.centroids.chunk(\n                            nb_centroids_chunks, dim=0\n                        )\n                    ],\n                    dim=0,\n                )\n            except RuntimeError:\n                nb_centroids_chunks *= 2\n\n    def assign(self):\n        """"""\n        Assigns each column of W to its closest centroid, thus essentially\n        performing the E-step in train().\n\n        Remarks:\n            - The function must be called after train() or after loading\n              centroids using self.load(), otherwise it will return empty tensors\n        """"""\n\n        distances = self.compute_distances()  # (n_centroids x out_features)\n        self.assignments = torch.argmin(distances, dim=0)  # (out_features)\n\n    def save(self, path, layer):\n        """"""\n        Saves centroids and assignments.\n\n        Args:\n            - path: folder used to save centroids and assignments\n        """"""\n\n        torch.save(self.centroids, os.path.join(path, ""{}_centroids.pth"".format(layer)))\n        torch.save(\n            self.assignments, os.path.join(path, ""{}_assignments.pth"".format(layer))\n        )\n        torch.save(self.objective, os.path.join(path, ""{}_objective.pth"".format(layer)))\n\n    def load(self, path, layer):\n        """"""\n        Loads centroids and assignments from a given path\n\n        Args:\n            - path: folder use to load centroids and assignments\n        """"""\n\n        self.centroids = torch.load(\n            os.path.join(path, ""{}_centroids.pth"".format(layer))\n        )\n        self.assignments = torch.load(\n            os.path.join(path, ""{}_assignments.pth"".format(layer))\n        )\n        self.objective = torch.load(\n            os.path.join(path, ""{}_objective.pth"".format(layer))\n        )\n\n\nclass EmptyClusterResolveError(Exception):\n    pass\n'"
fairseq/modules/quantization/pq/pq.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .em import EM, EmptyClusterResolveError\n\n\nclass PQ(EM):\n    """"""\n    Quantizes the layer weights W with the standard Product Quantization\n    technique. This learns a codebook of codewords or centroids of size\n    block_size from W. For further reference on using PQ to quantize\n    neural networks, see ""And the Bit Goes Down: Revisiting the Quantization\n    of Neural Networks"", Stock et al., ICLR 2020.\n\n    PQ is performed in two steps:\n    (1) The matrix W (weights or fully-connected or convolutional layer)\n        is reshaped to (block_size, -1).\n            - If W is fully-connected (2D), its columns are split into\n              blocks of size block_size.\n            - If W is convolutional (4D), its filters are split along the\n              spatial dimension.\n    (2) We apply the standard EM/k-means algorithm to the resulting reshaped matrix.\n\n    Args:\n        - W: weight matrix to quantize of size (in_features x out_features)\n        - block_size: size of the blocks (subvectors)\n        - n_centroids: number of centroids\n        - n_iter: number of k-means iterations\n        - eps: for cluster reassignment when an empty cluster is found\n        - max_tentatives for cluster reassignment when an empty cluster is found\n        - verbose: print information after each iteration\n\n    Remarks:\n        - block_size be compatible with the shape of W\n    """"""\n\n    def __init__(\n        self,\n        W,\n        block_size,\n        n_centroids=256,\n        n_iter=20,\n        eps=1e-6,\n        max_tentatives=30,\n        verbose=True,\n    ):\n        self.block_size = block_size\n        W_reshaped = self._reshape(W)\n        super(PQ, self).__init__(\n            W_reshaped,\n            n_centroids=n_centroids,\n            n_iter=n_iter,\n            eps=eps,\n            max_tentatives=max_tentatives,\n            verbose=verbose,\n        )\n\n    def _reshape(self, W):\n        """"""\n        Reshapes the matrix W as expained in step (1).\n        """"""\n\n        # fully connected: by convention the weight has size out_features x in_features\n        if len(W.size()) == 2:\n            self.out_features, self.in_features = W.size()\n            assert (\n                self.in_features % self.block_size == 0\n            ), ""Linear: n_blocks must be a multiple of in_features""\n            return (\n                W.reshape(self.out_features, -1, self.block_size)\n                .permute(2, 1, 0)\n                .flatten(1, 2)\n            )\n\n        # convolutional: we reshape along the spatial dimension\n        elif len(W.size()) == 4:\n            self.out_channels, self.in_channels, self.k_h, self.k_w = W.size()\n            assert (\n                self.in_channels * self.k_h * self.k_w\n            ) % self.block_size == 0, (\n                ""Conv2d: n_blocks must be a multiple of in_channels * k_h * k_w""\n            )\n            return (\n                W.reshape(self.out_channels, -1, self.block_size)\n                .permute(2, 1, 0)\n                .flatten(1, 2)\n            )\n        # not implemented\n        else:\n            raise NotImplementedError(W.size())\n\n    def encode(self):\n        """"""\n        Performs self.n_iter EM steps.\n        """"""\n\n        self.initialize_centroids()\n        for i in range(self.n_iter):\n            try:\n                self.step(i)\n            except EmptyClusterResolveError:\n                break\n\n    def decode(self):\n        """"""\n        Returns the encoded full weight matrix. Must be called after\n        the encode function.\n        """"""\n\n        # fully connected case\n        if ""k_h"" not in self.__dict__:\n            return (\n                self.centroids[self.assignments]\n                .reshape(-1, self.out_features, self.block_size)\n                .permute(1, 0, 2)\n                .flatten(1, 2)\n            )\n\n        # convolutional case\n        else:\n            return (\n                self.centroids[self.assignments]\n                .reshape(-1, self.out_channels, self.block_size)\n                .permute(1, 0, 2)\n                .reshape(self.out_channels, self.in_channels, self.k_h, self.k_w)\n            )\n'"
fairseq/modules/quantization/pq/utils.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport re\nfrom operator import attrgetter, itemgetter\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.distributed as dist\n\nfrom .modules import PQConv2d, PQLinear, PQEmbedding\nfrom .pq import PQ\n\n\ndef quantize_model_(\n    model,\n    size_tracker,\n    layers_to_quantize,\n    block_sizes_config,\n    n_centroids_config,\n    step=0,\n    n_iter=15,\n    eps=1e-6,\n    max_tentatives=100,\n    verbose=True,\n):\n    """"""\n    Quantize a model in-place by stages. All the targeted\n    layers are replaced by their quantized counterpart,\n    and the model is ready for the finetuning of the\n    centroids in a standard training loop (no modifications\n    required). Note that we do not quantize biases.\n\n    Args:\n        - model: a nn.Module\n        - size_tracker: useful for tracking quatization statistics\n        - layers_to_quantize: a list containing regexps for\n          filtering the layers to quantize at each stage according\n          to their name (as in model.named_parameters())\n        - block_sizes_config: dict like\n          {\n              \'Conv2d\': (\'kernel_size\', {\'(3, 3)\': 9, \'(1, 1)\': 4}),\n              \'Linear\': (\'in_features\', {\'*\': 8})\n          }\n          For instance, all conv2d layers with kernel size 3x3 have\n          a block size of 9 and all Linear layers are quantized with\n          a block size of 8, irrespective of their size.\n        - n_centroids_config: dict like\n          {\n              \'Conv2d\': (\'kernel_size\', {\'*\': 256}),\n              \'Linear\': (\'in_features\', {\'*\': 256})\n          }\n          For instance, all conv2d layers are quantized with 256 centroids\n        - step: the layers to quantize inplace corresponding\n          to layers_to_quantize[step]\n    """"""\n\n    quantized_layers = get_layers(model, layers_to_quantize[step])\n\n    for layer in quantized_layers:\n\n        # book-keeping\n        is_master_process = (not dist.is_initialized()) or (dist.is_initialized() and dist.get_rank() == 0)\n        verbose = verbose and is_master_process\n\n        # get block size and centroids\n        module = attrgetter(layer)(model)\n        block_size = get_param(module, layer, block_sizes_config)\n        n_centroids = get_param(module, layer, n_centroids_config)\n        if verbose:\n            logging.info(f""Quantizing layer {layer} with block size {block_size} and {n_centroids} centroids"")\n\n        # quantize layer\n        weight = module.weight.data.clone()\n        is_bias = \'bias\' in [x[0] for x in module.named_parameters()]\n        bias = module.bias.data.clone() if is_bias else None\n        quantizer = PQ(\n            weight,\n            block_size,\n            n_centroids=n_centroids,\n            n_iter=n_iter,\n            eps=eps,\n            max_tentatives=max_tentatives,\n            verbose=verbose,\n        )\n\n        # quantization performed on all GPUs with same seed\n        quantizer.encode()\n        centroids = quantizer.centroids.contiguous()\n        assignments = quantizer.assignments.contiguous()\n\n        # broadcast results to make sure weights are up-to-date\n        if dist.is_initialized():\n            dist.broadcast(centroids, 0)\n            dist.broadcast(assignments, 0)\n\n        # instantiate the quantized counterpart\n        if isinstance(module, nn.Linear):\n            out_features, in_features = map(\n                lambda k: module.__dict__[k], [""out_features"", ""in_features""]\n            )\n            quantized_module = PQLinear(\n                centroids, assignments, bias, in_features, out_features\n            )\n        elif isinstance(module, nn.Embedding):\n            num_embeddings, embedding_dim = map(\n                lambda k: module.__dict__[k], [""num_embeddings"", ""embedding_dim""]\n            )\n            quantized_module = PQEmbedding(\n                centroids, assignments, num_embeddings, embedding_dim\n            )\n        elif isinstance(module, nn.Conv2d):\n            out_channels, in_channels, kernel_size = map(\n                lambda k: module.__dict__[k],\n                [""out_channels"", ""in_channels"", ""kernel_size""],\n            )\n            stride, padding, dilation, groups, padding_mode = map(\n                lambda k: module.__dict__[k],\n                [""stride"", ""padding"", ""dilation"", ""groups"", ""padding_mode""],\n            )\n\n            quantized_module = PQConv2d(\n                centroids,\n                assignments,\n                bias,\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=padding,\n                dilation=dilation,\n                groups=groups,\n                padding_mode=padding_mode,\n            )\n        else:\n            raise ValueError(f""Module {module} not yet supported for quantization"")\n\n        # replace layer by its quantized counterpart\n        attrsetter(layer)(model, quantized_module)\n\n        # update statistics\n        size_tracker.update(weight, block_size, n_centroids)\n\n    # return name of quantized layers\n    return quantized_layers\n\n\ndef get_layers(model, filter_regexp):\n    """"""\n    Filters out the layers according to a regexp. Note that\n    we omit biases.\n\n    Args:\n        - model: a nn.Module\n        - filter_regexp: a regexp to filter the layers to keep\n          according to their name in model.named_parameters().\n          For instance, the regexp:\n\n             down_layers\\\\.[123456]\\\\.(conv[12]|identity\\\\.conv))\n\n          is keeping blocks down_layers from 1 to 6, and inside\n          each block is keeping conv1, conv2 and identity.conv.\n\n    Remarks:\n        - We add (module\\\\.)? at the beginning of the regexp to\n          account for the possible use of nn.parallel.DataParallel\n    """"""\n\n    # get all parameter names\n    all_layers = map(itemgetter(0), model.named_parameters())\n\n    # remove biases\n    all_layers = filter(lambda x: ""bias"" not in x, all_layers)\n\n    # remove .weight in all other names (or .weight_orig is spectral norm)\n    all_layers = map(lambda x: x.replace("".weight_orig"", """"), all_layers)\n    all_layers = map(lambda x: x.replace("".weight"", """"), all_layers)\n\n    # return filtered layers\n    filter_regexp = ""(module\\\\.)?"" + ""("" + filter_regexp + "")""\n    r = re.compile(filter_regexp)\n\n    return list(filter(r.match, all_layers))\n\n\ndef get_param(module, layer_name, param_config):\n    """"""\n    Given a quantization configuration, get the right parameter\n    for the module to be quantized.\n\n    Args:\n        - module: a nn.Module\n        - layer_name: the name of the layer\n        - param_config: a dict like\n          {\n              \'Conv2d\': (\'kernel_size\', {\'(3, 3)\': 9, \'(1, 1)\': 4}),\n              \'Linear\': (\'in_features\', {\'*\': 8})\n          }\n          For instance, all conv2d layers with kernel size 3x3 have\n          a block size of 9 and all Linear layers are quantized with\n          a block size of 8, irrespective of their size.\n\n    Remarks:\n        - if \'fuzzy_name\' is passed as a parameter, layers whose layer_name\n          include \'fuzzy_name\' will be assigned the given parameter.\n          In the following example, conv.expand layers will have a block\n          size of 9 while conv.reduce will have a block size of 4 and all\n          other layers will have a block size of 2.\n          {\n              \'Conv2d\': (\'fuzzy_name\', {\'expand\': 9, \'reduce\': 4, \'*\': 2}),\n              \'Linear\': (\'fuzzy_name\', {\'classifier\': 8, \'projection\': 4})\n          }\n\n    """"""\n\n    layer_type = module.__class__.__name__\n\n    if layer_type not in param_config:\n        raise KeyError(f""Layer type {layer_type} not in config for layer {module}"")\n\n    feature, params = param_config[module.__class__.__name__]\n\n    if feature != ""fuzzy_name"":\n        feature_value = str(getattr(module, feature))\n        if feature_value not in params:\n            if ""*"" in params:\n                feature_value = ""*""\n            else:\n                raise KeyError(\n                    f""{feature}={feature_value} not in config for layer {module}""\n                )\n    else:\n        feature_values = [name for name in params if name in layer_name]\n        if len(feature_values) == 0:\n            if ""*"" in params:\n                feature_value = ""*""\n            else:\n                raise KeyError(\n                    f""name={layer_name} not in config for {module}""\n                )\n        else:\n            feature_value = feature_values[0]\n\n    return params[feature_value]\n\n\nclass SizeTracker(object):\n    """"""\n    Class to keep track of the compressed network size with iPQ.\n\n    Args:\n        - model: a nn.Module\n\n    Remarks:\n        - The compressed size is the sum of three components\n          for each layer in the network:\n              (1) Storing the centroids given by iPQ in fp16\n              (2) Storing the assignments of the blocks in int8\n              (3) Storing all non-compressed elements such as biases\n        - This cost in only valid if we use 256 centroids (then\n          indexing can indeed by done with int8).\n    """"""\n\n    def __init__(self, model):\n        self.model = model\n        self.size_non_compressed_model = self.compute_size()\n        self.size_non_quantized = self.size_non_compressed_model\n        self.size_index = 0\n        self.size_centroids = 0\n        self.n_quantized_layers = 0\n\n    def compute_size(self):\n        """"""\n        Computes the size of the model (in MB).\n        """"""\n\n        res = 0\n        for _, p in self.model.named_parameters():\n            res += p.numel()\n        return res * 4 / 1024 / 1024\n\n    def update(self, W, block_size, n_centroids):\n        """"""\n        Updates the running statistics when quantizing a new layer.\n        """"""\n\n        # bits per weights\n        bits_per_weight = np.log2(n_centroids) / block_size\n        self.n_quantized_layers += 1\n\n        # size of indexing the subvectors of size block_size (in MB)\n        size_index_layer = bits_per_weight * W.numel() / 8 / 1024 / 1024\n        self.size_index += size_index_layer\n\n        # size of the centroids stored in float16 (in MB)\n        size_centroids_layer = n_centroids * block_size * 2 / 1024 / 1024\n        self.size_centroids += size_centroids_layer\n\n        # size of non-compressed layers, e.g. LayerNorms or biases (in MB)\n        size_uncompressed_layer = W.numel() * 4 / 1024 / 1024\n        self.size_non_quantized -= size_uncompressed_layer\n\n    def __repr__(self):\n        size_compressed = (\n            self.size_index + self.size_centroids + self.size_non_quantized\n        )\n        compression_ratio = self.size_non_compressed_model / size_compressed  # NOQA\n        return (\n            f""Non-compressed model size: {self.size_non_compressed_model:.2f} MB. ""\n            f""After quantizing {self.n_quantized_layers} layers, size ""\n            f""(indexing + centroids + other): {self.size_index:.2f} MB + ""\n            f""{self.size_centroids:.2f} MB + {self.size_non_quantized:.2f} MB = ""\n            f""{size_compressed:.2f} MB, compression ratio: {compression_ratio:.2f}x""\n        )\n\n\ndef attrsetter(*items):\n    def resolve_attr(obj, attr):\n        attrs = attr.split(""."")\n        head = attrs[:-1]\n        tail = attrs[-1]\n\n        for name in head:\n            obj = getattr(obj, name)\n        return obj, tail\n\n    def g(obj, val):\n        for attr in items:\n            resolved_obj, resolved_attr = resolve_attr(obj, attr)\n            setattr(resolved_obj, resolved_attr, val)\n\n    return g\n'"
fairseq/modules/quantization/scalar/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .utils import quantize_model_  # NOQA\n'"
fairseq/modules/quantization/scalar/ops.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\n\ndef emulate_int(w, bits, method, scale=None, zero_point=None):\n    q = globals()[f""emulate_int{bits}_{method}""]\n    return q(w, scale=scale, zero_point=zero_point)\n\n\ndef quantize(w, scale, zero_point):\n    return (torch.clamp(torch.round(w / scale + zero_point), 0, 255) - zero_point) * scale\n\n\ndef emulate_int8_histogram(w, scale=None, zero_point=None):\n    if scale is None:\n        obs = torch.quantization.observer.HistogramObserver()\n        _ = obs(w.float())\n        scale, zero_point = obs.calculate_qparams()\n        scale = scale.cuda().type_as(w)\n        zero_point = zero_point.cuda().type_as(w)\n    return quantize(w, scale, zero_point), scale, zero_point\n\n\ndef emulate_int8_channel(w, scale=None, zero_point=None):\n    if scale is None:\n        obs = torch.quantization.observer.PerChannelMinMaxObserver(\n            ch_axis=-1, qscheme=torch.per_channel_symmetric\n        )\n        _ = obs(w)\n        scale, zero_point, ch_axis = obs.get_qparams()\n        scale = scale.cuda().type_as(w)\n        zero_point = zero_point.cuda().type_as(w)\n    return quantize(w, scale, zero_point), scale, zero_point\n\n\ndef emulate_int8_tensor(w, scale=None, zero_point=None):\n    if scale is None:\n        obs = torch.quantization.observer.MinMaxObserver()\n        _ = obs(w)\n        scale, zero_point = obs.calculate_qparams()\n        scale = scale.cuda().type_as(w)\n        zero_point = zero_point.cuda().type_as(w)\n    return quantize(w, scale, zero_point), scale, zero_point\n'"
fairseq/modules/quantization/scalar/utils.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom operator import attrgetter\n\nimport torch.nn as nn\nimport torch.distributed as dist\n\nfrom ..pq.utils import get_layers, attrsetter\nfrom .modules import IntConv2d, IntLinear, IntEmbedding, ActivationQuantizer\n\n\nMAPPING = {nn.Linear: IntLinear, nn.Embedding: IntEmbedding, nn.Conv2d: IntConv2d}\n\n\ndef quantize_model_(model, p=0.2, bits=8, update_step=3000):\n    """"""\n    Replaces all modules with their scalar quantized counterpart and\n    registers hooks to quantize the post-ativations of those modules.\n\n    Args:\n        - model: a nn.Module\n        - p: amount of noise (0 for no noise, 1 to quantize all the weights/activations)\n        - bits: number of bits\n        - update_step: update quantization parameters every update_step steps\n    """"""\n\n    # quantize all layers\n    quantized_layers = get_layers(model, ""(.*?)"")\n\n    for layer in quantized_layers:\n\n        # book-keeping\n        is_master_process = (not dist.is_initialized()) or (dist.is_initialized() and dist.get_rank() == 0)\n\n        # recover module\n        module = attrgetter(layer)(model)\n        if is_master_process:\n            logging.info(f""Quantizing layer {layer} with bits={bits} and QuantNoise={p}"")\n\n        # quantization params\n        q_params = {""p"": p, ""update_step"": update_step, ""bits"": bits, ""method"": ""histogram"", ""counter"": 0}\n\n        # instantiate the quantized counterpart\n        if isinstance(module, tuple(MAPPING.keys())):\n            QuantizedModule = MAPPING[module.__class__]\n            quantized_module = QuantizedModule.__new__(QuantizedModule)\n            params = module.__dict__\n            params.update(q_params)\n            quantized_module.__dict__.update(params)\n\n        else:\n            if is_master_process:\n                logging.info(f""Module {module} not yet supported for quantization"")\n            continue\n\n        # activation quantization\n        a_q = ActivationQuantizer(quantized_module, p=0, bits=bits, method=""histogram"")\n\n        # replace layer by its quantized counterpart\n        attrsetter(layer)(model, quantized_module)\n\n    # return name of quantized layers\n    return quantized_layers\n'"
fairseq/modules/quantization/pq/modules/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .qconv import PQConv2d  # NOQA\nfrom .qlinear import PQLinear  # NOQA\nfrom .qemb import PQEmbedding  # NOQA\n'"
fairseq/modules/quantization/pq/modules/qconv.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _pair\n\n\nclass PQConv2d(nn.Module):\n    """"""\n    Quantized counterpart of nn.Conv2d module. Stores the centroid, the assignments\n    and the non-quantized biases. The full weight is re-instantiated at each forward\n    pass and autograd automatically computes the gradients with respect to the\n    centroids.\n\n    Args:\n        - centroids: centroids of size n_centroids x block_size\n        - assignments: assignments of the centroids to the subvectors\n          of size self.out_channels x n_blocks\n        - bias: the non-quantized bias, must be either torch.Tensor or None\n\n    Remarks:\n        - We refer the reader to the official documentation of the nn.Conv2d module\n          for the other arguments and the behavior of the module.\n        - Performance tests on GPU show that this implementation is 10% slower than\n          the non-quantized nn.Conv2d module for a standard training loop.\n        - During the backward, the gradients are averaged by cluster and not summed.\n          This explains the hook registered to the centroids.\n    """"""\n\n    def __init__(\n        self,\n        centroids,\n        assignments,\n        bias,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        padding_mode=""zeros"",\n    ):\n        super(PQConv2d, self).__init__()\n        self.block_size = centroids.size(1)\n        self.n_centroids = centroids.size(0)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.groups = groups\n        self.padding_mode = padding_mode\n        # check compatibility\n        if in_channels // groups * np.prod(self.kernel_size) % self.block_size != 0:\n            raise ValueError(""Wrong PQ sizes"")\n        if len(assignments) % out_channels != 0:\n            raise ValueError(""Wrong PQ sizes"")\n        if in_channels % groups != 0:\n            raise ValueError(""in_channels must be divisible by groups"")\n        if out_channels % groups != 0:\n            raise ValueError(""out_channels must be divisible by groups"")\n        # define parameters\n        self.centroids = nn.Parameter(centroids, requires_grad=True)\n        self.register_buffer(""assignments"", assignments)\n        self.register_buffer(""counts"", torch.bincount(assignments).type_as(centroids))\n        if bias is not None:\n            self.bias = nn.Parameter(bias)\n        else:\n            self.register_parameter(""bias"", None)\n        # register hook for averaging gradients per centroids instead of summing\n        self.centroids.register_hook(lambda x: x / self.counts[:, None])\n\n    @property\n    def weight(self):\n        return (\n            self.centroids[self.assignments]\n            .reshape(-1, self.out_channels, self.block_size)\n            .permute(1, 0, 2)\n            .reshape(\n                self.out_channels, self.in_channels // self.groups, *self.kernel_size\n            )\n        )\n\n    def forward(self, x):\n        return F.conv2d(\n            x,\n            self.weight,\n            self.bias,\n            self.stride,\n            self.padding,\n            self.dilation,\n            self.groups,\n        )\n\n    def extra_repr(self):\n        s = ""{in_channels}, {out_channels}, kernel_size={kernel_size}, stride={stride}""\n        if self.padding != (0,) * len(self.padding):\n            s += "", padding={padding}""\n        if self.dilation != (1,) * len(self.dilation):\n            s += "", dilation={dilation}""\n        if self.groups != 1:\n            s += "", groups={groups}""\n        if self.bias is None:\n            s += "", bias=False""\n        if self.padding_mode != ""zeros"":\n            s += "", padding_mode={padding_mode}""\n        s += "", n_centroids={n_centroids}, block_size={block_size}""\n        return s.format(**self.__dict__)\n'"
fairseq/modules/quantization/pq/modules/qemb.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PQEmbedding(nn.Module):\n    """"""\n    Quantized counterpart of nn.Embedding module. Stores the centroids and\n    the assignments. The full weight is re-instantiated at each forward\n    pass.\n\n    Args:\n        - centroids: centroids of size n_centroids x block_size\n        - assignments: assignments of the centroids to the subvectors\n          of size self.out_features x n_blocks\n        - bias: the non-quantized bias\n\n    Remarks:\n        - We refer the reader to the official documentation of the nn.Embedding module\n          for the other arguments and the behavior of the module\n        - Performance tests on GPU show that this implementation is 10% slower than\n          the non-quantized nn.Embedding module for a standard training loop.\n    """"""\n\n    def __init__(self, centroids, assignments, num_embeddings, embedding_dim,\n                     padding_idx=None, max_norm=None, norm_type=2.,\n                     scale_grad_by_freq=False, sparse=False, _weight=None):\n        super(PQEmbedding, self).__init__()\n        self.block_size = centroids.size(1)\n        self.n_centroids = centroids.size(0)\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        if padding_idx is not None:\n            if padding_idx > 0:\n                assert padding_idx < self.num_embeddings, \'Padding_idx must be within num_embeddings\'\n            elif padding_idx < 0:\n                assert padding_idx >= -self.num_embeddings, \'Padding_idx must be within num_embeddings\'\n                padding_idx = self.num_embeddings + padding_idx\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.sparse = sparse\n        # check compatibility\n        if self.embedding_dim % self.block_size != 0:\n            raise ValueError(""Wrong PQ sizes"")\n        if len(assignments) % self.num_embeddings != 0:\n            raise ValueError(""Wrong PQ sizes"")\n        # define parameters\n        self.centroids = nn.Parameter(centroids, requires_grad=True)\n        self.register_buffer(""assignments"", assignments)\n        self.register_buffer(""counts"", torch.bincount(assignments).type_as(centroids))\n\n    @property\n    def weight(self):\n        return (\n            self.centroids[self.assignments]\n            .reshape(-1, self.num_embeddings, self.block_size)\n            .permute(1, 0, 2)\n            .flatten(1, 2)\n        )\n\n    def forward(self, input):\n        return F.embedding(\n            input, self.weight, self.padding_idx, self.max_norm,\n            self.norm_type, self.scale_grad_by_freq, self.sparse)\n\n    def extra_repr(self):\n        s = \'{num_embeddings}, {embedding_dim}\'\n        if self.padding_idx is not None:\n            s += \', padding_idx={padding_idx}\'\n        if self.max_norm is not None:\n            s += \', max_norm={max_norm}\'\n        if self.norm_type != 2:\n            s += \', norm_type={norm_type}\'\n        if self.scale_grad_by_freq is not False:\n            s += \', scale_grad_by_freq={scale_grad_by_freq}\'\n        if self.sparse is not False:\n            s += \', sparse=True\'\n        s += \', n_centroids={n_centroids}, block_size={block_size}\'\n\n        return s.format(**self.__dict__)\n'"
fairseq/modules/quantization/pq/modules/qlinear.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PQLinear(nn.Module):\n    """"""\n    Quantized counterpart of nn.Linear module. Stores the centroid, the assignments\n    and the non-quantized biases. The full weight is re-instantiated at each forward\n    pass.\n\n    Args:\n        - centroids: centroids of size n_centroids x block_size\n        - assignments: assignments of the centroids to the subvectors\n          of size self.out_features x n_blocks\n        - bias: the non-quantized bias\n\n    Remarks:\n        - We refer the reader to the official documentation of the nn.Linear module\n          for the other arguments and the behavior of the module\n        - Performance tests on GPU show that this implementation is 15% slower than\n          the non-quantized nn.Linear module for a standard training loop.\n    """"""\n\n    def __init__(self, centroids, assignments, bias, in_features, out_features):\n        super(PQLinear, self).__init__()\n        self.block_size = centroids.size(1)\n        self.n_centroids = centroids.size(0)\n        self.in_features = in_features\n        self.out_features = out_features\n        # check compatibility\n        if self.in_features % self.block_size != 0:\n            raise ValueError(""Wrong PQ sizes"")\n        if len(assignments) % self.out_features != 0:\n            raise ValueError(""Wrong PQ sizes"")\n        # define parameters\n        self.centroids = nn.Parameter(centroids, requires_grad=True)\n        self.register_buffer(""assignments"", assignments)\n        self.register_buffer(""counts"", torch.bincount(assignments).type_as(centroids))\n        if bias is not None:\n            self.bias = nn.Parameter(bias)\n        else:\n            self.register_parameter(""bias"", None)\n\n    @property\n    def weight(self):\n        return (\n            self.centroids[self.assignments]\n            .reshape(-1, self.out_features, self.block_size)\n            .permute(1, 0, 2)\n            .flatten(1, 2)\n        )\n\n    def forward(self, x):\n        return F.linear(\n            x,\n            self.weight,\n            self.bias,\n        )\n\n    def extra_repr(self):\n        return f""in_features={self.in_features},\\\n                 out_features={self.out_features},\\\n                 n_centroids={self.n_centroids},\\\n                 block_size={self.block_size},\\\n                 bias={self.bias is not None}""\n'"
fairseq/modules/quantization/scalar/modules/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .qconv import IntConv2d  # NOQA\nfrom .qlinear import IntLinear  # NOQA\nfrom .qemb import IntEmbedding  # NOQA\nfrom .qact import ActivationQuantizer  # NOQA\n'"
fairseq/modules/quantization/scalar/modules/qact.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom ..ops import emulate_int\n\n\nclass ActivationQuantizer:\n    """"""\n    Fake scalar quantization of the activations using a forward hook.\n\n    Args:\n        - module. a nn.Module for which we quantize the *post-activations*\n        - p: proportion of activations to quantize, set by default to 1\n        - update_step: to recompute quantization parameters\n        - bits: number of bits for quantization\n        - method: choose among {""tensor"", ""histogram"", ""channel""}\n        - clamp_threshold: to prevent gradients overflow\n\n    Remarks:\n        - Parameters scale and zero_point are recomputed every update_step\n          forward pass to reduce the overhead\n        - For the list of quantization methods and number of bits, see ops.py\n        - To remove the hook from the module, simply call self.handle.remove()\n        - At test time, the activations are fully quantized\n        - We use the straight-through estimator so that the gradients\n          back-propagate nicely in the network, this is implemented with\n          the detach() trick\n        - The activations are hard-clamped in [-clamp_threshold, clamp_threshold]\n          to prevent overflow during the backward pass\n    """"""\n    def __init__(self, module, p=1, update_step=1000, bits=8,\n                 method=""histogram"", clamp_threshold=5):\n        self.module = module\n        self.p = p\n        self.update_step = update_step\n        self.counter = 0\n        self.bits = bits\n        self.method = method\n        self.clamp_threshold = clamp_threshold\n        self.handle = None\n        self.register_hook()\n\n    def register_hook(self):\n        # forward hook\n        def quantize_hook(module, x, y):\n\n            # update parameters every 1000 iterations\n            if self.counter % self.update_step == 0:\n                self.scale = None\n                self.zero_point = None\n            self.counter += 1\n\n            # train with QuantNoise and evaluate the fully quantized network\n            p = self.p if self.module.training else 1\n\n            # quantize activations\n            y_q, self.scale, self.zero_point = emulate_int(\n                y.detach(),\n                bits=self.bits,\n                method=self.method,\n                scale=self.scale,\n                zero_point=self.zero_point,\n            )\n\n            # mask to apply noise\n            mask = torch.zeros_like(y)\n            mask.bernoulli_(1 - p)\n            noise = (y_q - y).masked_fill(mask.bool(), 0)\n\n            # using straight-through estimator (STE)\n            clamp_low = - self.scale * self.zero_point\n            clamp_high = self.scale * (2 ** self.bits - 1 - self.zero_point)\n            return torch.clamp(y, clamp_low.item(), clamp_high.item()) + noise.detach()\n\n        # register hook\n        self.handle = self.module.register_forward_hook(quantize_hook)\n'"
fairseq/modules/quantization/scalar/modules/qconv.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.conv import _ConvNd\nfrom torch.nn.modules.utils import _pair\n\nfrom ..ops import emulate_int\n\n\nclass IntConv2d(_ConvNd):\n    """"""\n    Quantized counterpart of the nn.Conv2d module that applies QuantNoise during training.\n\n    Args:\n        - standard nn.Conv2d parameters\n        - p: amount of noise to inject (0 = no quantization, 1 = quantize all the weights)\n        - bits: number of bits\n        - method: choose among {""tensor"", ""histogram"", ""channel""}\n        - update_step: recompute scale and zero_point every update_steps iterations\n\n    Remarks:\n        - We use the straight-thgourh estimator so that the gradients\n          back-propagate nicely in the network, this is implemented with\n          the detach() trick\n        - Parameters scale and zero_point are recomputed every update_step\n          forward pass to reduce the overhead\n        - At test time, the weights are fully quantized\n    """"""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        padding_mode=""zeros"",\n        p=0,\n        bits=8,\n        method=""histogram"",\n        update_step=1000,\n    ):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(IntConv2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            False,\n            _pair(0),\n            groups,\n            bias,\n            padding_mode,\n        )\n\n        # quantization parameters\n        self.p = p\n        self.bits = bits\n        self.method = method\n        self.update_step = update_step\n        self.counter = 0\n\n    def _conv_forward(self, input, weight):\n        if self.padding_mode != ""zeros"":\n            return F.conv2d(\n                F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n                weight,\n                self.bias,\n                self.stride,\n                _pair(0),\n                self.dilation,\n                self.groups,\n            )\n        return F.conv2d(\n            input,\n            weight,\n            self.bias,\n            self.stride,\n            self.padding,\n            self.dilation,\n            self.groups,\n        )\n\n    def forward(self, input):\n        # train with QuantNoise and evaluate the fully quantized network\n        p = self.p if self.training else 1\n\n        # update parameters every 100 iterations\n        if self.counter % self.update_step == 0:\n            self.scale = None\n            self.zero_point = None\n        self.counter += 1\n\n        # quantize weight\n        weight_quantized, self.scale, self.zero_point = emulate_int(\n            self.weight.detach(),\n            bits=self.bits,\n            method=self.method,\n            scale=self.scale,\n            zero_point=self.zero_point,\n        )\n\n        # mask to apply noise\n        mask = torch.zeros_like(self.weight)\n        mask.bernoulli_(1 - p)\n        noise = (weight_quantized - self.weight).masked_fill(mask.bool(), 0)\n\n        # using straight-through estimator (STE)\n        clamp_low = - self.scale * self.zero_point\n        clamp_high = self.scale * (2 ** self.bits - 1 - self.zero_point)\n        weight = torch.clamp(self.weight, clamp_low.item(), clamp_high.item()) + noise.detach()\n\n        # return output\n        output = self._conv_forward(input, weight)\n        return output\n\n    def extra_repr(self):\n        return (\n            ""in_channels={}, out_channels={}, kernel_size={}, stride={}, ""\n            ""padding={}, dilation={}, groups={}, bias={}, quant_noise={}, ""\n            ""bits={}, method={}"".format(\n                self.in_channels,\n                self.out_channels,\n                self.kernel_size,\n                self.stride,\n                self.padding,\n                self.dilation,\n                self.groups,\n                self.bias is not None,\n                self.p,\n                self.bits,\n                self.method,\n            )\n        )\n'"
fairseq/modules/quantization/scalar/modules/qemb.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..ops import emulate_int\n\n\nclass IntEmbedding(nn.Module):\n    """"""\n    Quantized counterpart of the nn.Embedding module that applies QuantNoise during training.\n\n    Args:\n        - num_embeddings: number of tokens\n        - embedding_dim: embedding dimension\n        - p: amount of noise to inject (0 = no quantization, 1 = quantize all the weights)\n        - bits: number of bits\n        - method: choose among {""tensor"", ""histogram"", ""channel""}\n        - update_step: recompute scale and zero_point every update_steps iterations\n\n    Remarks:\n        - We use the straight-through estimator so that the gradients\n          back-propagate nicely in the network, this is implemented with\n          the detach() trick\n        - Parameters scale and zero_point are recomputed every update_step\n          forward pass to reduce the overhead\n        - At test time, the weights are fully quantized\n    """"""\n\n    def __init__(\n        self,\n        num_embeddings,\n        embedding_dim,\n        padding_idx=None,\n        max_norm=None,\n        norm_type=2.,\n        scale_grad_by_freq=False,\n        sparse=False,\n        _weight=None,\n        p=0,\n        update_step=1000,\n        bits=8,\n        method=""histogram"",\n    ):\n        super(IntEmbedding, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        if padding_idx is not None:\n            if padding_idx > 0:\n                assert padding_idx < self.num_embeddings, \'Padding_idx must be within num_embeddings\'\n            elif padding_idx < 0:\n                assert padding_idx >= -self.num_embeddings, \'Padding_idx must be within num_embeddings\'\n                padding_idx = self.num_embeddings + padding_idx\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        if _weight is None:\n            self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n            self.reset_parameters()\n        else:\n            assert list(_weight.shape) == [num_embeddings, embedding_dim], \\\n                \'Shape of weight does not match num_embeddings and embedding_dim\'\n            self.weight = nn.Parameter(_weight)\n        self.sparse = sparse\n\n        # quantization parameters\n        self.p = p\n        self.bits = bits\n        self.method = method\n        self.update_step = update_step\n        self.counter = 0\n\n    def reset_parameters(self):\n        nn.init.normal_(self.weight)\n        if self.padding_idx is not None:\n            with torch.no_grad():\n                self.weight[self.padding_idx].fill_(0)\n\n    def forward(self, input):\n        # train with QuantNoise and evaluate the fully quantized network\n        p = self.p if self.training else 1\n\n        # update parameters every 1000 iterations\n        if self.counter % self.update_step == 0:\n            self.scale = None\n            self.zero_point = None\n        self.counter += 1\n\n        # quantize weight\n        weight_quantized, self.scale, self.zero_point = emulate_int(\n            self.weight.detach(),\n            bits=self.bits,\n            method=self.method,\n            scale=self.scale,\n            zero_point=self.zero_point,\n        )\n\n        # mask to apply noise\n        mask = torch.zeros_like(self.weight)\n        mask.bernoulli_(1 - p)\n        noise = (weight_quantized - self.weight).masked_fill(mask.bool(), 0)\n\n        # using straight-through estimator (STE)\n        clamp_low = - self.scale * self.zero_point\n        clamp_high = self.scale * (2 ** self.bits - 1 - self.zero_point)\n        weight = torch.clamp(self.weight, clamp_low.item(), clamp_high.item()) + noise.detach()\n\n        # return output\n        output = F.embedding(\n            input, weight, self.padding_idx, self.max_norm,\n            self.norm_type, self.scale_grad_by_freq, self.sparse)\n        return output\n\n    def extra_repr(self):\n        s = \'{num_embeddings}, {embedding_dim}\'\n        if self.padding_idx is not None:\n            s += \', padding_idx={padding_idx}\'\n        if self.max_norm is not None:\n            s += \', max_norm={max_norm}\'\n        if self.norm_type != 2:\n            s += \', norm_type={norm_type}\'\n        if self.scale_grad_by_freq is not False:\n            s += \', scale_grad_by_freq={scale_grad_by_freq}\'\n        if self.sparse is not False:\n            s += \', sparse=True\'\n        s += \'quant_noise={p}, bits={bits}, method={method}\'\n        return s.format(**self.__dict__)\n'"
fairseq/modules/quantization/scalar/modules/qlinear.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..ops import emulate_int\n\n\nclass IntLinear(nn.Module):\n    """"""\n    Quantized counterpart of the nn.Linear module that applies QuantNoise during training.\n\n    Args:\n        - in_features: input features\n        - out_features: output features\n        - bias: bias or not\n        - p: amount of noise to inject (0 = no quantization, 1 = quantize all the weights)\n        - bits: number of bits\n        - method: choose among {""tensor"", ""histogram"", ""channel""}\n        - update_step: recompute scale and zero_point every update_steps iterations\n\n    Remarks:\n        - We use the straight-through estimator so that the gradients\n          back-propagate nicely in the network, this is implemented with\n          the detach() trick.\n        - Parameters scale and zero_point are recomputed every update_step\n          forward pass to reduce the overhead\n        - At test time, the weights are fully quantized\n    """"""\n\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        bias=True,\n        p=0,\n        update_step=3000,\n        bits=8,\n        method=""histogram"",\n    ):\n        super(IntLinear, self).__init__()\n        self.in_features = int(in_features)\n        self.out_features = int(out_features)\n        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.chosen_bias = bias\n        if self.chosen_bias:\n            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(""bias"", None)\n        self.reset_parameters()\n\n        # quantization parameters\n        self.p = p\n        self.bits = bits\n        self.method = method\n        self.update_step = update_step\n        self.counter = 0\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight)\n        if self.chosen_bias:\n            nn.init.constant_(self.bias, 0.0)\n        return\n\n    def forward(self, input):\n        # train with QuantNoise and evaluate the fully quantized network\n        p = self.p if self.training else 1\n\n        # update parameters every 100 iterations\n        if self.counter % self.update_step == 0:\n            self.scale = None\n            self.zero_point = None\n        self.counter += 1\n\n        # quantize weight\n        weight_quantized, self.scale, self.zero_point = emulate_int(\n            self.weight.detach(),\n            bits=self.bits,\n            method=self.method,\n            scale=self.scale,\n            zero_point=self.zero_point,\n        )\n\n        # mask to apply noise\n        mask = torch.zeros_like(self.weight)\n        mask.bernoulli_(1 - p)\n        noise = (weight_quantized - self.weight).masked_fill(mask.bool(), 0)\n\n        # using straight-through estimator (STE)\n        clamp_low = - self.scale * self.zero_point\n        clamp_high = self.scale * (2 ** self.bits - 1 - self.zero_point)\n        weight = torch.clamp(self.weight, clamp_low.item(), clamp_high.item()) + noise.detach()\n\n        # return output\n        output = F.linear(input, weight, self.bias)\n        return output\n\n    def extra_repr(self):\n        return ""in_features={}, out_features={}, bias={}, quant_noise={}, bits={}, method={}"".format(\n            self.in_features,\n            self.out_features,\n            self.bias is not None,\n            self.p,\n            self.bits,\n            self.method,\n        )\n'"
