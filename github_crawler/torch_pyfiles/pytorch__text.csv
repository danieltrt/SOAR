file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\nimport io\nimport os\nimport shutil\nimport subprocess\nfrom pathlib import Path\nimport distutils.command.clean\nfrom setuptools import setup, find_packages\n\nfrom build_tools import setup_helpers\n\nROOT_DIR = Path(__file__).parent.resolve()\n\n\ndef read(*names, **kwargs):\n    with io.open(ROOT_DIR.joinpath(*names), encoding=kwargs.get(""encoding"", ""utf8"")) as fp:\n        return fp.read()\n\n\ndef _get_version():\n    version = \'0.6.0a0\'\n    sha = None\n\n    try:\n        cmd = [\'git\', \'rev-parse\', \'HEAD\']\n        sha = subprocess.check_output(cmd, cwd=str(ROOT_DIR)).decode(\'ascii\').strip()\n    except Exception:\n        pass\n\n    if os.getenv(\'BUILD_VERSION\'):\n        version = os.getenv(\'BUILD_VERSION\')\n    elif sha is not None:\n        version += \'+\' + sha[:7]\n\n    if sha is None:\n        sha = \'Unknown\'\n    return version, sha\n\n\ndef _export_version(version, sha):\n    version_path = ROOT_DIR / \'torchtext\' / \'version.py\'\n    with open(version_path, \'w\') as fileobj:\n        fileobj.write(""__version__ = \'{}\'\\n"".format(version))\n        fileobj.write(""git_version = {}\\n"".format(repr(sha)))\n\n\nVERSION, SHA = _get_version()\n_export_version(VERSION, SHA)\n\nprint(\'-- Building version \' + VERSION)\n\n\nclass clean(distutils.command.clean.clean):\n    def run(self):\n        # Run default behavior first\n        distutils.command.clean.clean.run(self)\n\n        # Remove torchtext extension\n        for path in (ROOT_DIR / \'torchtext\').glob(\'**/*.so\'):\n            print(f\'removing \\\'{path}\\\'\')\n            path.unlink()\n        # Remove build directory\n        build_dirs = [\n            ROOT_DIR / \'build\',\n            ROOT_DIR / \'third_party\' / \'build\',\n        ]\n        for path in build_dirs:\n            if path.exists():\n                print(f\'removing \\\'{path}\\\' (and everything under it)\')\n                shutil.rmtree(str(path), ignore_errors=True)\n\n\nsetup_info = dict(\n    # Metadata\n    name=\'torchtext\',\n    version=VERSION,\n    author=\'PyTorch core devs and James Bradbury\',\n    author_email=\'jekbradbury@gmail.com\',\n    url=\'https://github.com/pytorch/text\',\n    description=\'Text utilities and datasets for PyTorch\',\n    long_description=read(\'README.rst\'),\n    license=\'BSD\',\n\n    install_requires=[\n        \'tqdm\', \'requests\', \'torch\', \'numpy\', \'sentencepiece\'\n    ],\n    python_requires=\'>=3.5\',\n    classifiers=[\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n        \'Programming Language :: Python :: 3 :: Only\',\n    ],\n    # Package info\n    packages=find_packages(exclude=(\'test*\', \'build_tools*\')),\n    zip_safe=False,\n    # Extension info\n    # If you are trying to use torchtext.so and see no registered op.\n    # See here: https://github.com/pytorch/vision/issues/2134""\n    ext_modules=setup_helpers.get_ext_modules(),\n    cmdclass={\n        \'build_ext\': setup_helpers.BuildExtension.with_options(no_python_abi_suffix=True),\n        \'clean\': clean,\n    },\n)\n\nsetup(**setup_info)\n'"
.circleci/regenerate.py,0,"b'#!/usr/bin/env python3\n\n""""""\nThis script should use a very simple, functional programming style.\nAvoid Jinja macros in favor of native Python functions.\n\nDon\'t go overboard on code generation; use Python only to generate\ncontent that can\'t be easily declared statically using CircleCI\'s YAML API.\n\nData declarations (e.g. the nested loops for defining the configuration matrix)\nshould be at the top of the file for easy updating.\n\nSee this comment for design rationale:\nhttps://github.com/pytorch/vision/pull/1321#issuecomment-531033978\n""""""\n\nimport jinja2\nimport yaml\nimport os.path\n\n\nPYTHON_VERSIONS = [""3.6"", ""3.7"", ""3.8""]\n\n\ndef build_workflows(prefix=\'\', upload=False, filter_branch=None, indentation=6):\n    w = []\n    for btype in [""wheel"", ""conda""]:\n        for os_type in [""linux"", ""macos"", ""windows""]:\n            for python_version in PYTHON_VERSIONS:\n                w += build_workflow_pair(btype, os_type, python_version, filter_branch, prefix, upload)\n    return indent(indentation, w)\n\n\ndef build_workflow_pair(btype, os_type, python_version, filter_branch, prefix=\'\', upload=False):\n    w = []\n    base_workflow_name = f""{prefix}binary_{os_type}_{btype}_py{python_version}""\n    w.append(generate_base_workflow(base_workflow_name, python_version, filter_branch, os_type, btype))\n\n    if upload:\n        w.append(generate_upload_workflow(base_workflow_name, filter_branch, btype))\n        if filter_branch == \'nightly\' and os_type == \'linux\':\n            pydistro = \'pip\' if btype == \'wheel\' else \'conda\'\n            w.append(generate_smoketest_workflow(pydistro, base_workflow_name, filter_branch, python_version))\n    return w\n\n\ndef generate_base_workflow(base_workflow_name, python_version, filter_branch, os_type, btype):\n    d = {\n        ""name"": base_workflow_name,\n        ""python_version"": python_version,\n    }\n\n    if filter_branch:\n        d[""filters""] = gen_filter_branch_tree(filter_branch)\n\n    return {f""binary_{os_type}_{btype}"": d}\n\n\ndef gen_filter_branch_tree(branch_name):\n    return {""branches"": {""only"": branch_name}}\n\n\ndef generate_upload_workflow(base_workflow_name, filter_branch, btype):\n    d = {\n        ""name"": f""{base_workflow_name}_upload"",\n        ""context"": ""org-member"",\n        ""requires"": [base_workflow_name],\n    }\n\n    if filter_branch:\n        d[""filters""] = gen_filter_branch_tree(filter_branch)\n\n    return {f""binary_{btype}_upload"": d}\n\n\ndef generate_smoketest_workflow(pydistro, base_workflow_name, filter_branch, python_version):\n\n    required_build_suffix = ""_upload""\n    required_build_name = base_workflow_name + required_build_suffix\n\n    smoke_suffix = f""smoke_test_{pydistro}""\n    d = {\n        ""name"": f""{base_workflow_name}_{smoke_suffix}"",\n        ""requires"": [required_build_name],\n        ""python_version"": python_version,\n    }\n\n    if filter_branch:\n        d[""filters""] = gen_filter_branch_tree(filter_branch)\n\n    return {f""smoke_test_linux_{pydistro}"": d}\n\n\ndef indent(indentation, data_list):\n    return (""\\n"" + "" "" * indentation).join(yaml.dump(data_list).splitlines())\n\n\ndef unittest_workflows(indentation=6):\n    w = []\n    for os_type in [""linux"", ""windows""]:\n        for i, python_version in enumerate(PYTHON_VERSIONS):\n            w.append({\n                f""unittest_{os_type}"": {\n                    ""name"": f""unittest_{os_type}_py{python_version}"",\n                    ""python_version"": python_version,\n                }\n            })\n\n            if i == 0 and os_type == ""linux"":\n                w.append({\n                    f""stylecheck"": {\n                        ""name"": f""stylecheck_py{python_version}"",\n                        ""python_version"": python_version,\n                    }\n                })\n    return indent(indentation, w)\n\n\nif __name__ == ""__main__"":\n    d = os.path.dirname(__file__)\n    env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(d),\n        lstrip_blocks=True,\n        autoescape=False,\n    )\n\n    with open(os.path.join(d, \'config.yml\'), \'w\') as f:\n        f.write(env.get_template(\'config.yml.in\').render(\n            build_workflows=build_workflows,\n            unittest_workflows=unittest_workflows,\n        ))\n'"
build_tools/__init__.py,0,b''
test/__init__.py,0,b''
test/babi.py,0,"b'from torchtext import datasets\n\n# en-valid\nTRAIN_NUM = [0] + [900] * 16 + [904, 905, 900, 904]\nVAL_NUM = [0] + [100] * 16 + [96, 95, 100, 96]\nTEST_NUM = [0] + [1000] * 20\n\n# Testcase 1 (joint training)\ntrain_iter, val_iter, test_iter = datasets.BABI20.iters(task=1, joint=True)\nassert len(train_iter.dataset) == sum(TRAIN_NUM)\nassert len(val_iter.dataset) == VAL_NUM[1]\nassert len(test_iter.dataset) == TEST_NUM[1]\n\n# Testcase 2 (only supporting)\ntrain_iter, val_iter, test_iter = datasets.BABI20.iters(task=1, only_supporting=True)\nassert len(train_iter.dataset) == TRAIN_NUM[2]\nassert len(val_iter.dataset) == VAL_NUM[2]\nassert len(test_iter.dataset) == TEST_NUM[2]\n\n# Testcase 3 (single task)\nfor i in range(1, 21):\n    train_iter, val_iter, test_iter = datasets.BABI20.iters(task=i)\n    assert len(train_iter.dataset) == TRAIN_NUM[i]\n    assert len(val_iter.dataset) == VAL_NUM[i]\n    assert len(test_iter.dataset) == TEST_NUM[i]\n\n# en-valid-10k\nTRAIN_NUM = [0] + [9000] * 17 + [8996, 9000, 9002]\nVAL_NUM = [0] + [1000] * 17 + [1004, 1000, 998]\nTEST_NUM = [0] + [1000] * 20\n\n# Testcase 1 (joint training)\ntrain_iter, val_iter, test_iter = datasets.BABI20.iters(task=1, joint=True, tenK=True)\nassert len(train_iter.dataset) == sum(TRAIN_NUM)\nassert len(val_iter.dataset) == VAL_NUM[1]\nassert len(test_iter.dataset) == TEST_NUM[1]\n\n# Testcase 2 (only supporting)\ntrain_iter, val_iter, test_iter = datasets.BABI20.iters(task=1, only_supporting=True,\n                                                        tenK=True)\nassert len(train_iter.dataset) == TRAIN_NUM[2]\nassert len(val_iter.dataset) == VAL_NUM[2]\nassert len(test_iter.dataset) == TEST_NUM[2]\n\n# Testcase 3 (single task)\nfor i in range(1, 21):\n    train_iter, val_iter, test_iter = datasets.BABI20.iters(task=i, tenK=True)\n    assert len(train_iter.dataset) == TRAIN_NUM[i]\n    assert len(val_iter.dataset) == VAL_NUM[i]\n    assert len(test_iter.dataset) == TEST_NUM[i]\n'"
test/data.py,0,"b'from torchtext import data\n\n\nTEXT = data.Field()\nLABELS = data.Field()\n\ntrain, val, test = data.TabularDataset.splits(\n    path=\'~/chainer-research/jmt-data/pos_wsj/pos_wsj\', train=\'.train\',\n    validation=\'.dev\', test=\'.test\', format=\'tsv\',\n    fields=[(\'text\', TEXT), (\'labels\', LABELS)])\n\nprint(train.fields)\nprint(len(train))\nprint(vars(train[0]))\n\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_size=3, sort_key=lambda x: len(x.text), device=""cuda:0"")\n\nLABELS.build_vocab(train.labels)\nTEXT.build_vocab(train.text)\n\nprint(TEXT.vocab.freqs.most_common(10))\nprint(LABELS.vocab.itos)\n\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.labels)\n'"
test/imdb.py,0,"b'from torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import GloVe\n\n\n# Approach 1:\n# set up fields\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\nLABEL = data.Field(sequential=False)\n\n\n# make splits for data\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)\n\n# print information about the data\nprint(\'train.fields\', train.fields)\nprint(\'len(train)\', len(train))\nprint(\'vars(train[0])\', vars(train[0]))\n\n# build the vocabulary\nTEXT.build_vocab(train, vectors=GloVe(name=\'6B\', dim=300))\nLABEL.build_vocab(train)\n\n# print vocab information\nprint(\'len(TEXT.vocab)\', len(TEXT.vocab))\nprint(\'TEXT.vocab.vectors.size()\', TEXT.vocab.vectors.size())\n\n# make iterator for splits\ntrain_iter, test_iter = data.BucketIterator.splits(\n    (train, test), batch_size=3, device=""cuda:0"")\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n\n# Approach 2:\ntrain_iter, test_iter = datasets.IMDB.iters(batch_size=4)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n'"
test/language_modeling.py,0,"b'from torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import GloVe\n\n# Approach 1:\n# set up fields\nTEXT = data.Field(lower=True, batch_first=True)\n\n# make splits for data\ntrain, valid, test = datasets.WikiText2.splits(TEXT)\n\n# print information about the data\nprint(\'train.fields\', train.fields)\nprint(\'len(train)\', len(train))\nprint(\'vars(train[0])\', vars(train[0])[\'text\'][0:10])\n\n# build the vocabulary\nTEXT.build_vocab(train, vectors=GloVe(name=\'6B\', dim=300))\n\n# print vocab information\nprint(\'len(TEXT.vocab)\', len(TEXT.vocab))\n\n# make iterator for splits\ntrain_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n    (train, valid, test), batch_size=3, bptt_len=30, device=""cuda:0"")\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.target)\n\n# Approach 2:\ntrain_iter, valid_iter, test_iter = datasets.WikiText2.iters(batch_size=4, bptt_len=30)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.target)\n'"
test/nli.py,26,"b""import torch\nfrom .common.torchtext_test_case import TorchtextTestCase\n\nfrom torchtext.datasets import SNLI, MultiNLI, XNLI\nfrom torchtext.datasets.nli import ParsedTextField, ShiftReduceField\nfrom torchtext.data import Field, LabelField, Iterator\n\nimport shutil\n\n\nclass TestNLI(TorchtextTestCase):\n\n    def test_snli(self):\n        batch_size = 4\n\n        # create fields\n        TEXT = ParsedTextField()\n        TREE = ShiftReduceField()\n        LABEL = LabelField()\n\n        # create train/val/test splits\n        train, val, test = SNLI.splits(TEXT, LABEL, TREE)\n\n        # check all are SNLI datasets\n        assert type(train) == type(val) == type(test) == SNLI\n\n        # check all have correct number of fields\n        assert len(train.fields) == len(val.fields) == len(test.fields) == 5\n\n        # check fields are the correct type\n        assert type(train.fields['premise']) == ParsedTextField\n        assert type(train.fields['premise_transitions']) == ShiftReduceField\n        assert type(train.fields['hypothesis']) == ParsedTextField\n        assert type(train.fields['hypothesis_transitions']) == ShiftReduceField\n        assert type(train.fields['label']) == LabelField\n\n        assert type(val.fields['premise']) == ParsedTextField\n        assert type(val.fields['premise_transitions']) == ShiftReduceField\n        assert type(val.fields['hypothesis']) == ParsedTextField\n        assert type(val.fields['hypothesis_transitions']) == ShiftReduceField\n        assert type(val.fields['label']) == LabelField\n\n        assert type(test.fields['premise']) == ParsedTextField\n        assert type(test.fields['premise_transitions']) == ShiftReduceField\n        assert type(test.fields['hypothesis']) == ParsedTextField\n        assert type(test.fields['hypothesis_transitions']) == ShiftReduceField\n        assert type(test.fields['label']) == LabelField\n\n        # check each is the correct length\n        assert len(train) == 549367\n        assert len(val) == 9842\n        assert len(test) == 9824\n\n        # build vocabulary\n        TEXT.build_vocab(train)\n        LABEL.build_vocab(train)\n\n        # ensure vocabulary has been created\n        assert hasattr(TEXT, 'vocab')\n        assert hasattr(TEXT.vocab, 'itos')\n        assert hasattr(TEXT.vocab, 'stoi')\n\n        # create iterators\n        train_iter, val_iter, test_iter = Iterator.splits((train, val, test),\n                                                          batch_size=batch_size)\n\n        # get a batch to test\n        batch = next(iter(train_iter))\n\n        # split premise and hypothesis from tuples to tensors\n        premise, premise_transitions = batch.premise\n        hypothesis, hypothesis_transitions = batch.hypothesis\n        label = batch.label\n\n        # check each is actually a tensor\n        assert type(premise) == torch.Tensor\n        assert type(premise_transitions) == torch.Tensor\n        assert type(hypothesis) == torch.Tensor\n        assert type(hypothesis_transitions) == torch.Tensor\n        assert type(label) == torch.Tensor\n\n        # check have the correct batch dimension\n        assert premise.shape[-1] == batch_size\n        assert premise_transitions.shape[-1] == batch_size\n        assert hypothesis.shape[-1] == batch_size\n        assert hypothesis_transitions.shape[-1] == batch_size\n        assert label.shape[-1] == batch_size\n\n        # repeat the same tests with iters instead of split\n        train_iter, val_iter, test_iter = SNLI.iters(batch_size=batch_size,\n                                                     trees=True)\n\n        # split premise and hypothesis from tuples to tensors\n        premise, premise_transitions = batch.premise\n        hypothesis, hypothesis_transitions = batch.hypothesis\n        label = batch.label\n\n        # check each is actually a tensor\n        assert type(premise) == torch.Tensor\n        assert type(premise_transitions) == torch.Tensor\n        assert type(hypothesis) == torch.Tensor\n        assert type(hypothesis_transitions) == torch.Tensor\n        assert type(label) == torch.Tensor\n\n        # check have the correct batch dimension\n        assert premise.shape[-1] == batch_size\n        assert premise_transitions.shape[-1] == batch_size\n        assert hypothesis.shape[-1] == batch_size\n        assert hypothesis_transitions.shape[-1] == batch_size\n        assert label.shape[-1] == batch_size\n\n        # remove downloaded snli directory\n        shutil.rmtree('.data/snli')\n\n    def test_multinli(self):\n        batch_size = 4\n\n        # create fields\n        TEXT = ParsedTextField()\n        TREE = ShiftReduceField()\n        GENRE = LabelField()\n        LABEL = LabelField()\n\n        # create train/val/test splits\n        train, val, test = MultiNLI.splits(TEXT, LABEL, TREE, GENRE)\n\n        # check all are MultiNLI datasets\n        assert type(train) == type(val) == type(test) == MultiNLI\n\n        # check all have correct number of fields\n        assert len(train.fields) == len(val.fields) == len(test.fields) == 6\n\n        # check fields are the correct type\n        assert type(train.fields['premise']) == ParsedTextField\n        assert type(train.fields['premise_transitions']) == ShiftReduceField\n        assert type(train.fields['hypothesis']) == ParsedTextField\n        assert type(train.fields['hypothesis_transitions']) == ShiftReduceField\n        assert type(train.fields['label']) == LabelField\n        assert type(train.fields['genre']) == LabelField\n\n        assert type(val.fields['premise']) == ParsedTextField\n        assert type(val.fields['premise_transitions']) == ShiftReduceField\n        assert type(val.fields['hypothesis']) == ParsedTextField\n        assert type(val.fields['hypothesis_transitions']) == ShiftReduceField\n        assert type(val.fields['label']) == LabelField\n        assert type(val.fields['genre']) == LabelField\n\n        assert type(test.fields['premise']) == ParsedTextField\n        assert type(test.fields['premise_transitions']) == ShiftReduceField\n        assert type(test.fields['hypothesis']) == ParsedTextField\n        assert type(test.fields['hypothesis_transitions']) == ShiftReduceField\n        assert type(test.fields['label']) == LabelField\n        assert type(test.fields['genre']) == LabelField\n\n        # check each is the correct length\n        assert len(train) == 392702\n        assert len(val) == 9815\n        assert len(test) == 9832\n\n        # build vocabulary\n        TEXT.build_vocab(train)\n        LABEL.build_vocab(train)\n        GENRE.build_vocab(train)\n\n        # ensure vocabulary has been created\n        assert hasattr(TEXT, 'vocab')\n        assert hasattr(TEXT.vocab, 'itos')\n        assert hasattr(TEXT.vocab, 'stoi')\n\n        # create iterators\n        train_iter, val_iter, test_iter = Iterator.splits((train, val, test),\n                                                          batch_size=batch_size)\n\n        # get a batch to test\n        batch = next(iter(train_iter))\n\n        # split premise and hypothesis from tuples to tensors\n        premise, premise_transitions = batch.premise\n        hypothesis, hypothesis_transitions = batch.hypothesis\n        label = batch.label\n        genre = batch.genre\n\n        # check each is actually a tensor\n        assert type(premise) == torch.Tensor\n        assert type(premise_transitions) == torch.Tensor\n        assert type(hypothesis) == torch.Tensor\n        assert type(hypothesis_transitions) == torch.Tensor\n        assert type(label) == torch.Tensor\n        assert type(genre) == torch.Tensor\n\n        # check have the correct batch dimension\n        assert premise.shape[-1] == batch_size\n        assert premise_transitions.shape[-1] == batch_size\n        assert hypothesis.shape[-1] == batch_size\n        assert hypothesis_transitions.shape[-1] == batch_size\n        assert label.shape[-1] == batch_size\n        assert genre.shape[-1] == batch_size\n\n        # repeat the same tests with iters instead of split\n        train_iter, val_iter, test_iter = MultiNLI.iters(batch_size=batch_size,\n                                                         trees=True)\n\n        # split premise and hypothesis from tuples to tensors\n        premise, premise_transitions = batch.premise\n        hypothesis, hypothesis_transitions = batch.hypothesis\n        label = batch.label\n\n        # check each is actually a tensor\n        assert type(premise) == torch.Tensor\n        assert type(premise_transitions) == torch.Tensor\n        assert type(hypothesis) == torch.Tensor\n        assert type(hypothesis_transitions) == torch.Tensor\n        assert type(label) == torch.Tensor\n\n        # check have the correct batch dimension\n        assert premise.shape[-1] == batch_size\n        assert premise_transitions.shape[-1] == batch_size\n        assert hypothesis.shape[-1] == batch_size\n        assert hypothesis_transitions.shape[-1] == batch_size\n        assert label.shape[-1] == batch_size\n\n        # remove downloaded multinli directory\n        shutil.rmtree('.data/multinli')\n\n    def test_xnli(self):\n        batch_size = 4\n\n        # create fields\n        TEXT = Field()\n        GENRE = LabelField()\n        LABEL = LabelField()\n        LANGUAGE = LabelField()\n\n        # create val/test splits, XNLI does not have a test set\n        val, test = XNLI.splits(TEXT, LABEL, GENRE, LANGUAGE)\n\n        # check both are XNLI datasets\n        assert type(val) == type(test) == XNLI\n\n        # check all have the correct number of fields\n        assert len(val.fields) == len(test.fields) == 5\n\n        # check fields are the correct type\n        assert type(val.fields['premise']) == Field\n        assert type(val.fields['hypothesis']) == Field\n        assert type(val.fields['label']) == LabelField\n        assert type(val.fields['genre']) == LabelField\n        assert type(val.fields['language']) == LabelField\n\n        assert type(test.fields['premise']) == Field\n        assert type(test.fields['hypothesis']) == Field\n        assert type(test.fields['label']) == LabelField\n        assert type(test.fields['genre']) == LabelField\n        assert type(test.fields['language']) == LabelField\n\n        # check each is the correct length\n        assert len(val) == 37350\n        assert len(test) == 75150\n\n        # build vocabulary\n        TEXT.build_vocab(val)\n        LABEL.build_vocab(val)\n        GENRE.build_vocab(val)\n        LANGUAGE.build_vocab(val)\n\n        # ensure vocabulary has been created\n        assert hasattr(TEXT, 'vocab')\n        assert hasattr(TEXT.vocab, 'itos')\n        assert hasattr(TEXT.vocab, 'stoi')\n\n        # create iterators\n        val_iter, test_iter = Iterator.splits((val, test),\n                                              batch_size=batch_size)\n\n        # get a batch to test\n        batch = next(iter(val_iter))\n\n        # split premise and hypothesis from tuples to tensors\n        premise = batch.premise\n        hypothesis = batch.hypothesis\n        label = batch.label\n        genre = batch.genre\n        language = batch.language\n\n        # check each is actually a tensor\n        assert type(premise) == torch.Tensor\n        assert type(hypothesis) == torch.Tensor\n        assert type(label) == torch.Tensor\n        assert type(genre) == torch.Tensor\n        assert type(language) == torch.Tensor\n\n        # check have the correct batch dimension\n        assert premise.shape[-1] == batch_size\n        assert hypothesis.shape[-1] == batch_size\n        assert label.shape[-1] == batch_size\n        assert genre.shape[-1] == batch_size\n        assert language.shape[-1] == batch_size\n\n        # xnli cannot use the iters method, ensure raises error\n        with self.assertRaises(NotImplementedError):\n            val_iter, test_iter = XNLI.iters(batch_size=batch_size)\n\n        # remove downloaded xnli directory\n        shutil.rmtree('.data/xnli')\n"""
test/sequence_tagging.py,0,"b'from torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import GloVe\n\n# Define the fields associated with the sequences.\nWORD = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\nUD_TAG = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\n\n# Download and the load default data.\ntrain, val, test = datasets.UDPOS.splits(\n    fields=((\'word\', WORD), (\'udtag\', UD_TAG), (None, None)))\n\nprint(train.fields)\nprint(len(train))\nprint(vars(train[0]))\n\n# We can also define more than two columns.\nWORD = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\nUD_TAG = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\nPTB_TAG = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\n\n# Load the specified data.\ntrain, val, test = datasets.UDPOS.splits(\n    fields=((\'word\', WORD), (\'udtag\', UD_TAG), (\'ptbtag\', PTB_TAG)),\n    path="".data/sequence-labeling/en-ud-v2"",\n    train=""en-ud-tag.v2.train.txt"",\n    validation=""en-ud-tag.v2.dev.txt"",\n    test=""en-ud-tag.v2.test.txt"")\n\nprint(train.fields)\nprint(len(train))\nprint(vars(train[0]))\n\nWORD.build_vocab(train.word, min_freq=3)\nUD_TAG.build_vocab(train.udtag)\nPTB_TAG.build_vocab(train.ptbtag)\n\nprint(UD_TAG.vocab.freqs)\nprint(PTB_TAG.vocab.freqs)\n\ntrain_iter, val_iter = data.BucketIterator.splits(\n    (train, val), batch_size=3, device=""cuda:0"")\n\nbatch = next(iter(train_iter))\n\nprint(""words"", batch.word)\nprint(""udtags"", batch.udtag)\nprint(""ptbtags"", batch.ptbtag)\n\n# Now lets try both word and character embeddings\nWORD = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\nPTB_TAG = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\n\n# We\'ll use NestedField to tokenize each word into list of chars\nCHAR_NESTING = data.Field(tokenize=list, init_token=""<bos>"", eos_token=""<eos>"")\nCHAR = data.NestedField(CHAR_NESTING, init_token=""<bos>"", eos_token=""<eos>"")\n\nfields = [((\'word\', \'char\'), (WORD, CHAR)), (None, None), (\'ptbtag\', PTB_TAG)]\ntrain, val, test = datasets.UDPOS.splits(fields=fields)\n\nprint(train.fields)\nprint(len(train))\nprint(vars(train[0]))\n\nWORD.build_vocab(train.word, val.word, test.word, vectors=[GloVe(name=\'6B\', dim=\'300\')])\nCHAR.build_vocab(train.char, val.char, test.char)\nPTB_TAG.build_vocab(train.ptbtag)\n\nprint(CHAR.vocab.freqs)\ntrain_iter, val_iter = data.BucketIterator.splits(\n    (train, val), batch_size=3)\n\nbatch = next(iter(train_iter))\n\nprint(""words"", batch.word)\nprint(""chars"", batch.char)\nprint(""ptbtags"", batch.ptbtag)\n\n# Using the CoNLL 2000 Chunking dataset:\nINPUTS = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\nCHUNK_TAGS = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\n\ntrain, val, test = datasets.CoNLL2000Chunking.splits(\n    fields=((\'inputs\', INPUTS), (None, None), (\'tags\', CHUNK_TAGS))\n)\nprint(len(train), len(val), len(test))\n'"
test/sst.py,0,"b""from torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n\n\n# Approach 1:\n# set up fields\nTEXT = data.Field()\nLABEL = data.Field(sequential=False)\n\n# make splits for data\ntrain, val, test = datasets.SST.splits(\n    TEXT, LABEL, fine_grained=True, train_subtrees=True,\n    filter_pred=lambda ex: ex.label != 'neutral')\n\n# print information about the data\nprint('train.fields', train.fields)\nprint('len(train)', len(train))\nprint('vars(train[0])', vars(train[0]))\n\n# build the vocabulary\nurl = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec'\nTEXT.build_vocab(train, vectors=Vectors('wiki.simple.vec', url=url))\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\n# make iterator for splits\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_size=3)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n\n# Approach 2:\nTEXT.build_vocab(train, vectors=[GloVe(name='840B', dim='300'), CharNGram(), FastText()])\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\ntrain_iter, val_iter, test_iter = datasets.SST.iters(batch_size=4)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n\n# Approach 3:\nf = FastText()\nTEXT.build_vocab(train, vectors=f)\nTEXT.vocab.extend(f)\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\ntrain_iter, val_iter, test_iter = datasets.SST.iters(batch_size=4)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n"""
test/test_build.py,20,"b'#!/usr/bin/env python3\n""""""Tests that requires external resources (Network access to fetch dataset)""""""\nimport os\nfrom collections import Counter\n\nimport numpy as np\nimport torch\nimport torchtext.data\n\nfrom .common.torchtext_test_case import TorchtextTestCase\n\n\nclass TestNestedField(TorchtextTestCase):\n    def test_build_vocab(self):\n        nesting_field = torchtext.data.Field(tokenize=list, init_token=""<w>"", eos_token=""</w>"")\n\n        field = torchtext.data.NestedField(\n            nesting_field, init_token=\'<s>\', eos_token=\'</s>\',\n            include_lengths=True,\n            pad_first=True)\n\n        sources = [\n            [[\'a\'], [\'s\', \'e\', \'n\', \'t\', \'e\', \'n\', \'c\', \'e\'], [\'o\', \'f\'], [\'d\', \'a\', \'t\', \'a\'], [\'.\']],\n            [[\'y\', \'e\', \'t\'], [\'a\', \'n\', \'o\', \'t\', \'h\', \'e\', \'r\']],\n            [[\'o\', \'n\', \'e\'], [\'l\', \'a\', \'s\', \'t\'], [\'s\', \'e\', \'n\', \'t\']]\n        ]\n\n        field.build_vocab(\n            sources, vectors=\'glove.6B.50d\',\n            unk_init=torch.nn.init.normal_, vectors_cache="".vector_cache"")\n\n\nclass TestDataset(TorchtextTestCase):\n    def test_csv_file_no_header_one_col_multiple_fields(self):\n        self.write_test_ppid_dataset(data_format=""csv"")\n\n        question_field = torchtext.data.Field(sequential=True)\n        spacy_tok_question_field = torchtext.data.Field(sequential=True, tokenize=""spacy"")\n        label_field = torchtext.data.Field(sequential=False)\n        # Field name/value as nested tuples\n        fields = [(""ids"", None),\n                  ((""q1"", ""q1_spacy""), (question_field, spacy_tok_question_field)),\n                  ((""q2"", ""q2_spacy""), (question_field, spacy_tok_question_field)),\n                  (""label"", label_field)]\n        dataset = torchtext.data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""csv"", fields=fields)\n        expected_examples = [\n            ([""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"", ""instead"", ""of"", ""\xe3\x81\x97?""],\n             [""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"", ""instead"", ""of"", ""\xe3\x81\x97"", ""?""],\n             [""When"", ""do"", ""you"", ""use"", ""\\""&\\"""",\n              ""instead"", ""of"", ""\\""and\\""?""],\n             [""When"", ""do"", ""you"", ""use"", ""\\"""", ""&"", ""\\"""",\n              ""instead"", ""of"", ""\\"""", ""and"", ""\\"""", ""?""], ""0""),\n            ([""Where"", ""was"", ""Lincoln"", ""born?""],\n             [""Where"", ""was"", ""Lincoln"", ""born"", ""?""],\n             [""Which"", ""location"", ""was"", ""Abraham"", ""Lincoln"", ""born?""],\n             [""Which"", ""location"", ""was"", ""Abraham"", ""Lincoln"", ""born"", ""?""],\n             ""1""),\n            ([""What"", ""is"", ""2+2""], [""What"", ""is"", ""2"", ""+"", ""2""],\n             [""2+2=?""], [""2"", ""+"", ""2="", ""?""], ""1"")]\n        for i, example in enumerate(dataset):\n            self.assertEqual(example.q1, expected_examples[i][0])\n            self.assertEqual(example.q1_spacy, expected_examples[i][1])\n            self.assertEqual(example.q2, expected_examples[i][2])\n            self.assertEqual(example.q2_spacy, expected_examples[i][3])\n            self.assertEqual(example.label, expected_examples[i][4])\n\n        # 6 Fields including None for ids\n        assert len(dataset.fields) == 6\n\n    def test_json_dataset_one_key_multiple_fields(self):\n        self.write_test_ppid_dataset(data_format=""json"")\n\n        question_field = torchtext.data.Field(sequential=True)\n        spacy_tok_question_field = torchtext.data.Field(sequential=True, tokenize=""spacy"")\n        label_field = torchtext.data.Field(sequential=False)\n        fields = {""question1"": [(""q1"", question_field),\n                                (""q1_spacy"", spacy_tok_question_field)],\n                  ""question2"": [(""q2"", question_field),\n                                (""q2_spacy"", spacy_tok_question_field)],\n                  ""label"": (""label"", label_field)}\n        dataset = torchtext.data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""json"", fields=fields)\n        expected_examples = [\n            ([""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"", ""instead"", ""of"", ""\xe3\x81\x97?""],\n             [""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"", ""instead"", ""of"", ""\xe3\x81\x97"", ""?""],\n             [""When"", ""do"", ""you"", ""use"", ""\\""&\\"""",\n              ""instead"", ""of"", ""\\""and\\""?""],\n             [""When"", ""do"", ""you"", ""use"", ""\\"""", ""&"", ""\\"""",\n              ""instead"", ""of"", ""\\"""", ""and"", ""\\"""", ""?""], ""0""),\n            ([""Where"", ""was"", ""Lincoln"", ""born?""],\n             [""Where"", ""was"", ""Lincoln"", ""born"", ""?""],\n             [""Which"", ""location"", ""was"", ""Abraham"", ""Lincoln"", ""born?""],\n             [""Which"", ""location"", ""was"", ""Abraham"", ""Lincoln"", ""born"", ""?""],\n             ""1""),\n            ([""What"", ""is"", ""2+2""], [""What"", ""is"", ""2"", ""+"", ""2""],\n             [""2+2=?""], [""2"", ""+"", ""2="", ""?""], ""1"")]\n        for i, example in enumerate(dataset):\n            self.assertEqual(example.q1, expected_examples[i][0])\n            self.assertEqual(example.q1_spacy, expected_examples[i][1])\n            self.assertEqual(example.q2, expected_examples[i][2])\n            self.assertEqual(example.q2_spacy, expected_examples[i][3])\n            self.assertEqual(example.label, expected_examples[i][4])\n\n\nclass TestDataUtils(TorchtextTestCase):\n    TEST_STR = ""A string, particularly one with slightly complex punctuation.""\n\n    def test_get_tokenizer_spacy(self):\n        # Test SpaCy option, and verify it properly handles punctuation.\n        assert torchtext.data.get_tokenizer(""spacy"")(str(self.TEST_STR)) == [\n            ""A"", ""string"", "","", ""particularly"", ""one"", ""with"", ""slightly"",\n            ""complex"", ""punctuation"", "".""]\n\n    def test_get_tokenizer_moses(self):\n        # Test Moses option.\n        # Note that internally, MosesTokenizer converts to unicode if applicable\n        moses_tokenizer = torchtext.data.get_tokenizer(""moses"")\n        assert moses_tokenizer(self.TEST_STR) == [\n            ""A"", ""string"", "","", ""particularly"", ""one"", ""with"", ""slightly"",\n            ""complex"", ""punctuation"", "".""]\n\n        # Nonbreaking prefixes should tokenize the final period.\n        assert moses_tokenizer(""abc def."") == [""abc"", ""def"", "".""]\n\n\nclass TestVocab(TorchtextTestCase):\n    def test_vectors_get_vecs(self):\n        vec = torchtext.vocab.GloVe(name=\'twitter.27B\', dim=\'25\')\n        self.assertEqual(vec.vectors.shape[0], len(vec))\n\n        tokens = [\'chip\', \'baby\', \'Beautiful\']\n        token_vecs = vec.get_vecs_by_tokens(tokens).numpy()\n        self.assertEqual(token_vecs.shape[0], len(tokens))\n        self.assertEqual(token_vecs.shape[1], vec.dim)\n        torch.testing.assert_allclose(vec[tokens[0]].numpy(), token_vecs[0])\n        torch.testing.assert_allclose(vec[tokens[1]].numpy(), token_vecs[1])\n        torch.testing.assert_allclose(vec[\'<unk>\'].numpy(), token_vecs[2])\n\n        token_one_vec = vec.get_vecs_by_tokens(tokens[0], lower_case_backup=True).numpy()\n        self.assertEqual(token_one_vec.shape[0], vec.dim)\n        torch.testing.assert_allclose(vec[tokens[0].lower()].numpy(), token_one_vec)\n\n    def test_download_charngram_vectors(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        # Build a vocab and get vectors twice to test caching, then once more\n        # to test string aliases.\n        for i in range(3):\n            if i == 2:\n                vectors = ""charngram.100d""\n            else:\n                vectors = torchtext.vocab.CharNGram()\n            v = torchtext.vocab.Vocab(\n                c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'], vectors=vectors)\n            expected_itos = [\'<unk>\', \'<pad>\', \'<bos>\',\n                             \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\']\n            expected_stoi = {x: index for index, x in enumerate(expected_itos)}\n            self.assertEqual(v.itos, expected_itos)\n            self.assertEqual(dict(v.stoi), expected_stoi)\n            vectors = v.vectors.numpy()\n\n            # The first 5 entries in each vector.\n            expected_charngram = {\n                \'hello\': [-0.44782442, -0.08937783, -0.34227219,\n                          -0.16233221, -0.39343098],\n                \'world\': [-0.29590717, -0.05275926, -0.37334684, 0.27117205, -0.3868292],\n            }\n\n            for word in expected_charngram:\n                torch.testing.assert_allclose(\n                    vectors[v.stoi[word], :5], expected_charngram[word])\n\n            torch.testing.assert_allclose(vectors[v.stoi[\'<unk>\']], np.zeros(100))\n            torch.testing.assert_allclose(vectors[v.stoi[\'OOV token\']], np.zeros(100))\n\n    def test_download_custom_vectors(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        # Build a vocab and get vectors twice to test caching.\n        for _ in range(2):\n            v = torchtext.vocab.Vocab(\n                c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'],\n                vectors=torchtext.vocab.Vectors(\n                    \'wiki.simple.vec\',\n                    url=torchtext.vocab.FastText.url_base.format(\'simple\')\n                )\n            )\n\n            self.assertEqual(v.itos, [\'<unk>\', \'<pad>\', \'<bos>\',\n                                      \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\'])\n            vectors = v.vectors.numpy()\n\n            # The first 5 entries in each vector.\n            expected_fasttext_simple_en = {\n                \'hello\': [0.39567, 0.21454, -0.035389, -0.24299, -0.095645],\n                \'world\': [0.10444, -0.10858, 0.27212, 0.13299, -0.33165],\n            }\n\n            for word in expected_fasttext_simple_en:\n                torch.testing.assert_allclose(\n                    vectors[v.stoi[word], :5], expected_fasttext_simple_en[word])\n\n            torch.testing.assert_allclose(vectors[v.stoi[\'<unk>\']], np.zeros(300))\n\n    def test_download_fasttext_vectors(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        # Build a vocab and get vectors twice to test caching, then once more\n        # to test string aliases.\n        for i in range(3):\n            if i == 2:\n                vectors = ""fasttext.simple.300d""\n            else:\n                vectors = torchtext.vocab.FastText(language=\'simple\')\n\n            v = torchtext.vocab.Vocab(\n                c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'], vectors=vectors)\n\n            expected_itos = [\'<unk>\', \'<pad>\', \'<bos>\',\n                             \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\']\n            expected_stoi = {x: index for index, x in enumerate(expected_itos)}\n            self.assertEqual(v.itos, expected_itos)\n            self.assertEqual(dict(v.stoi), expected_stoi)\n            vectors = v.vectors.numpy()\n\n            # The first 5 entries in each vector.\n            expected_fasttext_simple_en = {\n                \'hello\': [0.39567, 0.21454, -0.035389, -0.24299, -0.095645],\n                \'world\': [0.10444, -0.10858, 0.27212, 0.13299, -0.33165],\n            }\n\n            for word in expected_fasttext_simple_en:\n                torch.testing.assert_allclose(\n                    vectors[v.stoi[word], :5], expected_fasttext_simple_en[word])\n\n            torch.testing.assert_allclose(vectors[v.stoi[\'<unk>\']], np.zeros(300))\n            torch.testing.assert_allclose(vectors[v.stoi[\'OOV token\']], np.zeros(300))\n\n    def test_download_glove_vectors(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n\n        # Build a vocab and get vectors twice to test caching, then once more\n        # to test string aliases.\n        for i in range(3):\n            if i == 2:\n                vectors = ""glove.twitter.27B.25d""\n            else:\n                vectors = torchtext.vocab.GloVe(name=\'twitter.27B\', dim=\'25\')\n            v = torchtext.vocab.Vocab(\n                c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'], vectors=vectors)\n\n            expected_itos = [\'<unk>\', \'<pad>\', \'<bos>\',\n                             \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\']\n            expected_stoi = {x: index for index, x in enumerate(expected_itos)}\n            self.assertEqual(v.itos, expected_itos)\n            self.assertEqual(dict(v.stoi), expected_stoi)\n\n            vectors = v.vectors.numpy()\n\n            # The first 5 entries in each vector.\n            expected_twitter = {\n                \'hello\': [-0.77069, 0.12827, 0.33137, 0.0050893, -0.47605],\n                \'world\': [0.10301, 0.095666, -0.14789, -0.22383, -0.14775],\n            }\n\n            for word in expected_twitter:\n                torch.testing.assert_allclose(\n                    vectors[v.stoi[word], :5], expected_twitter[word])\n\n            torch.testing.assert_allclose(vectors[v.stoi[\'<unk>\']], np.zeros(25))\n            torch.testing.assert_allclose(vectors[v.stoi[\'OOV token\']], np.zeros(25))\n\n    def test_extend(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        # Build a vocab and get vectors twice to test caching.\n        for _ in range(2):\n            f = torchtext.vocab.FastText(language=\'simple\')\n            v = torchtext.vocab.Vocab(\n                c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'], vectors=f)\n            n_vocab = len(v)\n            v.extend(f)  # extend the vocab with the words contained in f.itos\n            self.assertGreater(len(v), n_vocab)\n\n            self.assertEqual(v.itos[:6], [\'<unk>\', \'<pad>\', \'<bos>\',\n                                          \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\'])\n            vectors = v.vectors.numpy()\n\n            # The first 5 entries in each vector.\n            expected_fasttext_simple_en = {\n                \'hello\': [0.39567, 0.21454, -0.035389, -0.24299, -0.095645],\n                \'world\': [0.10444, -0.10858, 0.27212, 0.13299, -0.33165],\n            }\n\n            for word in expected_fasttext_simple_en:\n                torch.testing.assert_allclose(\n                    vectors[v.stoi[word], :5], expected_fasttext_simple_en[word])\n\n            torch.testing.assert_allclose(vectors[v.stoi[\'<unk>\']], np.zeros(300))\n\n    def test_vectors_custom_cache(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        vector_cache = os.path.join(\'/tmp\', \'vector_cache\')\n        # Build a vocab and get vectors twice to test caching.\n        for i in range(2):\n            if i == 1:\n                self.assertTrue(os.path.exists(vector_cache))\n\n            v = torchtext.vocab.Vocab(\n                c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'],\n                vectors=torchtext.vocab.Vectors(\n                    \'wiki.simple.vec\', cache=vector_cache,\n                    url=torchtext.vocab.FastText.url_base.format(\'simple\'))\n            )\n\n            self.assertEqual(v.itos, [\'<unk>\', \'<pad>\', \'<bos>\',\n                                      \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\'])\n            vectors = v.vectors.numpy()\n\n            # The first 5 entries in each vector.\n            expected_fasttext_simple_en = {\n                \'hello\': [0.39567, 0.21454, -0.035389, -0.24299, -0.095645],\n                \'world\': [0.10444, -0.10858, 0.27212, 0.13299, -0.33165],\n            }\n\n            for word in expected_fasttext_simple_en:\n                torch.testing.assert_allclose(\n                    vectors[v.stoi[word], :5], expected_fasttext_simple_en[word])\n\n            torch.testing.assert_allclose(vectors[v.stoi[\'<unk>\']], np.zeros(300))\n'"
test/test_utils.py,0,"b""#!/usr/bin/env python3\n# Note that all the tests in this module require dataset (either network access or cached)\nimport os\nfrom torchtext import utils\nfrom .common.torchtext_test_case import TorchtextTestCase\n\n\ndef conditional_remove(f):\n    if os.path.isfile(f):\n        os.remove(f)\n\n\nclass TestUtils(TorchtextTestCase):\n\n    def test_download_extract_tar(self):\n        # create root directory for downloading data\n        root = '.data'\n        if not os.path.exists(root):\n            os.makedirs(root)\n\n        # ensure archive is not already downloaded, if it is then delete\n        url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\n        target_archive_path = os.path.join(root, 'validation.tar.gz')\n        conditional_remove(target_archive_path)\n\n        # download archive and ensure is in correct location\n        archive_path = utils.download_from_url(url)\n        assert target_archive_path == archive_path\n\n        # extract files and ensure they are correct\n        files = utils.extract_archive(archive_path)\n        assert files == [os.path.join(root, 'val.de'),\n                         os.path.join(root, 'val.en')]\n\n        # extract files with overwrite option True\n        files = utils.extract_archive(archive_path, overwrite=True)\n        assert files == [os.path.join(root, 'val.de'),\n                         os.path.join(root, 'val.en')]\n\n        # remove files and archive\n        for f in files:\n            conditional_remove(f)\n        conditional_remove(archive_path)\n\n    def test_download_extract_gz(self):\n        # create root directory for downloading data\n        root = '.data'\n        if not os.path.exists(root):\n            os.makedirs(root)\n\n        # ensure archive is not already downloaded, if it is then delete\n        url = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.5.en.gz'\n        target_archive_path = os.path.join(root, 'val.5.en.gz')\n        conditional_remove(target_archive_path)\n\n        # download archive and ensure is in correct location\n        archive_path = utils.download_from_url(url)\n        assert target_archive_path == archive_path\n\n        # extract files and ensure they are correct\n        files = utils.extract_archive(archive_path)\n        assert files == [os.path.join(root, 'val.5.en')]\n\n        # extract files with overwrite option True\n        files = utils.extract_archive(archive_path, overwrite=True)\n        assert files == [os.path.join(root, 'val.5.en')]\n\n        # remove files and archive\n        for f in files:\n            conditional_remove(f)\n        conditional_remove(archive_path)\n\n    def test_download_extract_zip(self):\n        # create root directory for downloading data\n        root = '.data'\n        if not os.path.exists(root):\n            os.makedirs(root)\n\n        # ensure archive is not already downloaded, if it is then delete\n        url = 'https://bitbucket.org/sivareddyg/public/downloads/en-ud-v2.zip'\n        target_archive_path = os.path.join(root, 'en-ud-v2.zip')\n        conditional_remove(target_archive_path)\n\n        # download archive and ensure is in correct location\n        archive_path = utils.download_from_url(url)\n        assert target_archive_path == archive_path\n\n        correct_files = ['en-ud-v2/en-ud-tag.v2.dev.txt',\n                         'en-ud-v2/en-ud-tag.v2.test.txt',\n                         'en-ud-v2/en-ud-tag.v2.train.txt',\n                         'en-ud-v2/LICENSE.txt',\n                         'en-ud-v2/README.txt']\n        # extract files and ensure they are correct\n        files = utils.extract_archive(archive_path)\n        assert files == [os.path.join(root, f) for f in correct_files]\n\n        # extract files with overwrite option True\n        files = utils.extract_archive(archive_path, overwrite=True)\n        assert files == [os.path.join(root, f) for f in correct_files]\n\n        # remove files and archive\n        for f in files:\n            conditional_remove(f)\n        os.rmdir(os.path.join(root, 'en-ud-v2'))\n        conditional_remove(archive_path)\n\n    def test_download_extract_to_path(self):\n        # create root directory for downloading data\n        root = '.data'\n        if not os.path.exists(root):\n            os.makedirs(root)\n\n        # create directory to extract archive to\n        to_path = '.new_data'\n        if not os.path.exists(root):\n            os.makedirs(root)\n\n        # ensure archive is not already downloaded, if it is then delete\n        url = 'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz'\n        target_archive_path = os.path.join(root, 'validation.tar.gz')\n        conditional_remove(target_archive_path)\n\n        # download archive and ensure is in correct location\n        archive_path = utils.download_from_url(url)\n        assert target_archive_path == archive_path\n\n        # extract files and ensure they are in the to_path directory\n        files = utils.extract_archive(archive_path, to_path)\n        assert files == [os.path.join(to_path, 'val.de'),\n                         os.path.join(to_path, 'val.en')]\n\n        # extract files with overwrite option True\n        files = utils.extract_archive(archive_path, to_path, overwrite=True)\n        assert files == [os.path.join(to_path, 'val.de'),\n                         os.path.join(to_path, 'val.en')]\n\n        # remove files and archive\n        for f in files:\n            conditional_remove(f)\n        conditional_remove(archive_path)\n\n    def test_extract_non_tar_zip(self):\n        # create root directory for downloading data\n        root = '.data'\n        if not os.path.exists(root):\n            os.makedirs(root)\n\n        # ensure file is not already downloaded, if it is then delete\n        url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec'\n        target_archive_path = os.path.join(root, 'wiki.simple.vec')\n        conditional_remove(target_archive_path)\n\n        # download file and ensure is in correct location\n        archive_path = utils.download_from_url(url)\n        assert target_archive_path == archive_path\n\n        # assert that non-valid file (not an archive) raises error\n        with self.assertRaises(NotImplementedError):\n            utils.extract_archive(archive_path)\n\n        # remove file\n        conditional_remove(archive_path)\n"""
test/test_vocab.py,1,"b'# -*- coding: utf-8 -*-\nfrom collections import Counter\nimport os\nimport pickle\n\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\nimport torch\nfrom torchtext import vocab\n\nfrom .common.torchtext_test_case import TorchtextTestCase\n\n\ndef conditional_remove(f):\n    if os.path.isfile(f):\n        os.remove(f)\n\n\nclass TestVocab(TorchtextTestCase):\n\n    def test_vocab_basic(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        v = vocab.Vocab(c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'])\n\n        expected_itos = [\'<unk>\', \'<pad>\', \'<bos>\',\n                         \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\']\n        expected_stoi = {x: index for index, x in enumerate(expected_itos)}\n        self.assertEqual(v.itos, expected_itos)\n        self.assertEqual(dict(v.stoi), expected_stoi)\n\n    def test_vocab_specials_first(self):\n        c = Counter(""a a b b c c"".split())\n\n        # add specials into vocabulary at first\n        v = vocab.Vocab(c, max_size=2, specials=[\'<pad>\', \'<eos>\'])\n        expected_itos = [\'<pad>\', \'<eos>\', \'a\', \'b\']\n        expected_stoi = {x: index for index, x in enumerate(expected_itos)}\n        self.assertEqual(v.itos, expected_itos)\n        self.assertEqual(dict(v.stoi), expected_stoi)\n\n        # add specials into vocabulary at last\n        v = vocab.Vocab(c, max_size=2, specials=[\'<pad>\', \'<eos>\'], specials_first=False)\n        expected_itos = [\'a\', \'b\', \'<pad>\', \'<eos>\']\n        expected_stoi = {x: index for index, x in enumerate(expected_itos)}\n        self.assertEqual(v.itos, expected_itos)\n        self.assertEqual(dict(v.stoi), expected_stoi)\n\n    def test_vocab_without_unk(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        oov_word = \'OOVWORD\'\n        self.assertNotIn(oov_word, c)\n\n        # tests for specials_first=True\n        v_first = vocab.Vocab(c, min_freq=3, specials=[\'<pad>\'], specials_first=True)\n        expected_itos_first = [\'<pad>\', \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\']\n        expected_stoi_first = {x: index for index, x in enumerate(expected_itos_first)}\n        self.assertEqual(v_first.itos, expected_itos_first)\n        self.assertEqual(dict(v_first.stoi), expected_stoi_first)\n        self.assertNotIn(oov_word, v_first.itos)\n        self.assertNotIn(oov_word, v_first.stoi)\n\n        # tests for specials_first=False\n        v_last = vocab.Vocab(c, min_freq=3, specials=[\'<pad>\'], specials_first=False)\n        expected_itos_last = [\'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\', \'hello\', \'world\', \'<pad>\']\n        expected_stoi_last = {x: index for index, x in enumerate(expected_itos_last)}\n        self.assertEqual(v_last.itos, expected_itos_last)\n        self.assertEqual(dict(v_last.stoi), expected_stoi_last)\n        self.assertNotIn(oov_word, v_last.itos)\n        self.assertNotIn(oov_word, v_last.stoi)\n\n        # check if pad is mapped to the first index\n        self.assertEqual(v_first.stoi[\'<pad>\'], 0)\n        # check if pad is mapped to the last index\n        self.assertEqual(v_last.stoi[\'<pad>\'], max(v_last.stoi.values()))\n\n        # check if an oovword is not in vocab and a default unk_id is not assigned to it\n        self.assertRaises(KeyError, v_first.stoi.__getitem__, oov_word)\n        self.assertRaises(KeyError, v_last.stoi.__getitem__, oov_word)\n\n    def test_vocab_set_vectors(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5,\n                     \'\xef\xbd\x94\xef\xbd\x85\xef\xbd\x93\xef\xbd\x94\': 4, \'freq_too_low\': 2})\n        v = vocab.Vocab(c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'])\n        stoi = {""hello"": 0, ""world"": 1, ""\xef\xbd\x94\xef\xbd\x85\xef\xbd\x93\xef\xbd\x94"": 2}\n        vectors = torch.FloatTensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n        dim = 2\n        v.set_vectors(stoi, vectors, dim)\n        expected_vectors = np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0],\n                                     [0.0, 0.0], [0.1, 0.2], [0.5, 0.6],\n                                     [0.3, 0.4]])\n        assert_allclose(v.vectors.numpy(), expected_vectors)\n\n    def test_errors(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        with self.assertRaises(ValueError):\n            # Test proper error raised when using unknown string alias\n            vocab.Vocab(c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'],\n                        vectors=[""fasttext.english.300d""])\n            vocab.Vocab(c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'],\n                        vectors=""fasttext.english.300d"")\n        with self.assertRaises(ValueError):\n            # Test proper error is raised when vectors argument is\n            # non-string or non-Vectors\n            vocab.Vocab(c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'],\n                        vectors={""word"": [1, 2, 3]})\n\n    def test_serialization(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        v = vocab.Vocab(c, min_freq=3, specials=[\'<unk>\', \'<pad>\', \'<bos>\'])\n        pickle_path = os.path.join(self.test_dir, ""vocab.pkl"")\n        pickle.dump(v, open(pickle_path, ""wb""))\n        v_loaded = pickle.load(open(pickle_path, ""rb""))\n        assert v == v_loaded\n\n    def test_serialization_backcompat(self):\n        # Test whether loading works on models saved in which\n        #  the state was not required to have an ""unk_index"".\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        v = vocab.Vocab(c, min_freq=3, specials=[\'<pad>\', \'<bos>\'])  # no unk special\n        # Mock old vocabulary\n        del v.__dict__[""unk_index""]\n\n        pickle_path = os.path.join(self.test_dir, ""vocab.pkl"")\n        pickle.dump(v, open(pickle_path, ""wb""))\n        v_loaded = pickle.load(open(pickle_path, ""rb""))\n        assert v == v_loaded\n\n    def test_has_unk(self):\n        c = Counter({\'hello\': 4, \'world\': 3, \'\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT\': 5, \'freq_too_low\': 2})\n        v = vocab.Vocab(c)\n        self.assertEqual(v[\'not_in_it\'], 0)\n'"
test/translation.py,0,"b""from torchtext import data\nfrom torchtext import datasets\n\nimport re\nimport spacy\n\nspacy_de = spacy.load('de')\nspacy_en = spacy.load('en')\n\nurl = re.compile('(<url>.*</url>)')\n\n\ndef tokenize_de(text):\n    return [tok.text for tok in spacy_de.tokenizer(url.sub('@URL@', text))]\n\n\ndef tokenize_en(text):\n    return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n\n\n# Testing IWSLT\nDE = data.Field(tokenize=tokenize_de)\nEN = data.Field(tokenize=tokenize_en)\n\ntrain, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN))\n\nprint(train.fields)\nprint(len(train))\nprint(vars(train[0]))\nprint(vars(train[100]))\n\nDE.build_vocab(train.src, min_freq=3)\nEN.build_vocab(train.trg, max_size=50000)\n\ntrain_iter, val_iter = data.BucketIterator.splits(\n    (train, val), batch_size=3)\n\nprint(DE.vocab.freqs.most_common(10))\nprint(len(DE.vocab))\nprint(EN.vocab.freqs.most_common(10))\nprint(len(EN.vocab))\n\nbatch = next(iter(train_iter))\nprint(batch.src)\nprint(batch.trg)\n\n\n# Testing Multi30k\nDE = data.Field(tokenize=tokenize_de)\nEN = data.Field(tokenize=tokenize_en)\n\ntrain, val, test = datasets.Multi30k.splits(exts=('.de', '.en'), fields=(DE, EN))\n\nprint(train.fields)\nprint(len(train))\nprint(vars(train[0]))\nprint(vars(train[100]))\n\nDE.build_vocab(train.src, min_freq=3)\nEN.build_vocab(train.trg, max_size=50000)\n\ntrain_iter, val_iter = data.BucketIterator.splits(\n    (train, val), batch_size=3)\n\nprint(DE.vocab.freqs.most_common(10))\nprint(len(DE.vocab))\nprint(EN.vocab.freqs.most_common(10))\nprint(len(EN.vocab))\n\nbatch = next(iter(train_iter))\nprint(batch.src)\nprint(batch.trg)\n\n\n# Testing custom paths\nDE = data.Field(tokenize=tokenize_de)\nEN = data.Field(tokenize=tokenize_en)\n\ntrain, val = datasets.TranslationDataset.splits(\n    path='.data/multi30k/', train='train',\n    validation='val', test=None, exts=('.de', '.en'),\n    fields=(DE, EN))\n\nprint(train.fields)\nprint(len(train))\nprint(vars(train[0]))\nprint(vars(train[100]))\n\nDE.build_vocab(train.src, min_freq=3)\nEN.build_vocab(train.trg, max_size=50000)\n\ntrain_iter, val_iter = data.BucketIterator.splits(\n    (train, val), batch_size=3)\n\nprint(DE.vocab.freqs.most_common(10))\nprint(len(DE.vocab))\nprint(EN.vocab.freqs.most_common(10))\nprint(len(EN.vocab))\n\nbatch = next(iter(train_iter))\nprint(batch.src)\nprint(batch.trg)\n"""
test/trec.py,0,"b""from torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import GloVe, CharNGram\n\n\n# Approach 1:\n# set up fields\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\nLABEL = data.Field(sequential=False)\n\n\n# make splits for data\ntrain, test = datasets.TREC.splits(TEXT, LABEL, fine_grained=True)\n\n# print information about the data\nprint('train.fields', train.fields)\nprint('len(train)', len(train))\nprint('vars(train[0])', vars(train[0]))\n\n# build the vocabulary\nTEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\n# make iterator for splits\ntrain_iter, test_iter = data.BucketIterator.splits(\n    (train, test), batch_size=3)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n\n# Approach 2:\nTEXT.build_vocab(train, vectors=[GloVe(name='840B', dim='300'), CharNGram()])\nLABEL.build_vocab(train)\n\ntrain_iter, test_iter = datasets.TREC.iters(batch_size=4)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n"""
torchtext/__init__.py,2,"b'from . import data\nfrom . import datasets\nfrom . import utils\nfrom . import vocab\nfrom . import experimental\n\n\ntry:\n    from .version import __version__, git_version  # noqa: F401\nexcept ImportError:\n    pass\n\n__all__ = [\'data\',\n           \'datasets\',\n           \'utils\',\n           \'vocab\',\n           \'experimental\']\n\n\ndef _init_extension():\n    import os\n    import importlib\n    import torch\n\n    # load the custom_op_library and register the custom ops\n    lib_dir = os.path.dirname(__file__)\n    loader_details = (\n        importlib.machinery.ExtensionFileLoader,\n        importlib.machinery.EXTENSION_SUFFIXES\n    )\n\n    extfinder = importlib.machinery.FileFinder(lib_dir, loader_details)\n    ext_specs = extfinder.find_spec(""_torchtext"")\n    if ext_specs is None:\n        raise ImportError(""torchtext C++ Extension is not found."")\n    torch.ops.load_library(ext_specs.origin)\n    torch.classes.load_library(ext_specs.origin)\n\n\n_init_extension()\n\n\ndel _init_extension\n'"
torchtext/utils.py,0,"b'import requests\nimport csv\nfrom tqdm import tqdm\nimport os\nimport tarfile\nimport logging\nimport re\nimport sys\nimport zipfile\nimport gzip\n\n\ndef reporthook(t):\n    """"""https://github.com/tqdm/tqdm""""""\n    last_b = [0]\n\n    def inner(b=1, bsize=1, tsize=None):\n        """"""\n        b: int, optional\n        Number of blocks just transferred [default: 1].\n        bsize: int, optional\n        Size of each block (in tqdm units) [default: 1].\n        tsize: int, optional\n        Total size (in tqdm units). If [default: None] remains unchanged.\n        """"""\n        if tsize is not None:\n            t.total = tsize\n        t.update((b - last_b[0]) * bsize)\n        last_b[0] = b\n    return inner\n\n\ndef download_from_url(url, path=None, root=\'.data\', overwrite=False):\n    """"""Download file, with logic (from tensor2tensor) for Google Drive.\n    Returns the path to the downloaded file.\n\n    Arguments:\n        url: the url of the file\n        path: explicitly set the filename, otherwise attempts to\n            detect the file name from URL header. (None)\n        root: download folder used to store the file in (.data)\n        overwrite: overwrite existing files (False)\n\n    Examples:\n        >>> url = \'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\'\n        >>> torchtext.utils.download_from_url(url)\n        >>> \'.data/validation.tar.gz\'\n    """"""\n\n    def _process_response(r, root, filename):\n        chunk_size = 16 * 1024\n        total_size = int(r.headers.get(\'Content-length\', 0))\n        if filename is None:\n            d = r.headers[\'content-disposition\']\n            filename = re.findall(""filename=\\""(.+)\\"""", d)\n            if filename is None:\n                raise RuntimeError(""Filename could not be autodetected"")\n            filename = filename[0]\n        path = os.path.join(root, filename)\n        if os.path.exists(path):\n            logging.info(\'File %s already exists.\' % path)\n            if not overwrite:\n                return path\n            logging.info(\'Overwriting file %s.\' % path)\n        logging.info(\'Downloading file {} to {}.\'.format(filename, path))\n        with open(path, ""wb"") as file:\n            with tqdm(total=total_size, unit=\'B\',\n                      unit_scale=1, desc=path.split(\'/\')[-1]) as t:\n                for chunk in r.iter_content(chunk_size):\n                    if chunk:\n                        file.write(chunk)\n                        t.update(len(chunk))\n        logging.info(\'File {} downloaded.\'.format(path))\n        return path\n\n    if path is None:\n        _, filename = os.path.split(url)\n    else:\n        root, filename = os.path.split(path)\n\n    if not os.path.exists(root):\n        try:\n            os.makedirs(root)\n        except OSError:\n            print(""Can\'t create the download directory {}."".format(root))\n            raise\n\n    if \'drive.google.com\' not in url:\n        response = requests.get(url, headers={\'User-Agent\': \'Mozilla/5.0\'}, stream=True)\n        return _process_response(response, root, filename)\n    else:\n        # google drive links get filename from google drive\n        filename = None\n\n    logging.info(\'Downloading from Google Drive; may take a few minutes\')\n    confirm_token = None\n    session = requests.Session()\n    response = session.get(url, stream=True)\n    for k, v in response.cookies.items():\n        if k.startswith(""download_warning""):\n            confirm_token = v\n\n    if confirm_token:\n        url = url + ""&confirm="" + confirm_token\n        response = session.get(url, stream=True)\n\n    return _process_response(response, root, filename)\n\n\ndef unicode_csv_reader(unicode_csv_data, **kwargs):\n    r""""""Since the standard csv library does not handle unicode in Python 2, we need a wrapper.\n    Borrowed and slightly modified from the Python docs:\n    https://docs.python.org/2/library/csv.html#csv-examples\n\n    Arguments:\n        unicode_csv_data: unicode csv data (see example below)\n\n    Examples:\n        >>> from torchtext.utils import unicode_csv_reader\n        >>> import io\n        >>> with io.open(data_path, encoding=""utf8"") as f:\n        >>>     reader = unicode_csv_reader(f)\n\n    """"""\n\n    # Fix field larger than field limit error\n    maxInt = sys.maxsize\n    while True:\n        # decrease the maxInt value by factor 10\n        # as long as the OverflowError occurs.\n        try:\n            csv.field_size_limit(maxInt)\n            break\n        except OverflowError:\n            maxInt = int(maxInt / 10)\n    csv.field_size_limit(maxInt)\n\n    for line in csv.reader(unicode_csv_data, **kwargs):\n        yield line\n\n\ndef utf_8_encoder(unicode_csv_data):\n    for line in unicode_csv_data:\n        yield line.encode(\'utf-8\')\n\n\ndef extract_archive(from_path, to_path=None, overwrite=False):\n    """"""Extract archive.\n\n    Arguments:\n        from_path: the path of the archive.\n        to_path: the root path of the extracted files (directory of from_path)\n        overwrite: overwrite existing files (False)\n\n    Returns:\n        List of paths to extracted files even if not overwritten.\n\n    Examples:\n        >>> url = \'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\'\n        >>> from_path = \'./validation.tar.gz\'\n        >>> to_path = \'./\'\n        >>> torchtext.utils.download_from_url(url, from_path)\n        >>> torchtext.utils.extract_archive(from_path, to_path)\n        >>> [\'.data/val.de\', \'.data/val.en\']\n    """"""\n\n    if to_path is None:\n        to_path = os.path.dirname(from_path)\n\n    if from_path.endswith((\'.tar.gz\', \'.tgz\')):\n        logging.info(\'Opening tar file {}.\'.format(from_path))\n        with tarfile.open(from_path, \'r\') as tar:\n            files = []\n            for file_ in tar:\n                file_path = os.path.join(to_path, file_.name)\n                if file_.isfile():\n                    files.append(file_path)\n                    if os.path.exists(file_path):\n                        logging.info(\'{} already extracted.\'.format(file_path))\n                        if not overwrite:\n                            continue\n                tar.extract(file_, to_path)\n            return files\n\n    elif from_path.endswith(\'.zip\'):\n        assert zipfile.is_zipfile(from_path), from_path\n        logging.info(\'Opening zip file {}.\'.format(from_path))\n        with zipfile.ZipFile(from_path, \'r\') as zfile:\n            files = []\n            for file_ in zfile.namelist():\n                file_path = os.path.join(to_path, file_)\n                files.append(file_path)\n                if os.path.exists(file_path):\n                    logging.info(\'{} already extracted.\'.format(file_path))\n                    if not overwrite:\n                        continue\n                zfile.extract(file_, to_path)\n        files = [f for f in files if os.path.isfile(f)]\n        return files\n\n    elif from_path.endswith(\'.gz\'):\n        default_block_size = 65536\n        filename = from_path[:-3]\n        files = [filename]\n        with gzip.open(from_path, \'rb\') as gzfile, \\\n                open(filename, \'wb\') as d_file:\n            while True:\n                block = gzfile.read(default_block_size)\n                if not block:\n                    break\n                else:\n                    d_file.write(block)\n            d_file.write(block)\n        return files\n\n    else:\n        raise NotImplementedError(\n            ""We currently only support tar.gz, .tgz, .gz and zip achives."")\n'"
torchtext/vocab.py,16,"b'from collections import defaultdict\nfrom functools import partial\nimport logging\nimport os\nimport zipfile\nimport gzip\n\nfrom urllib.request import urlretrieve\nimport torch\nfrom tqdm import tqdm\nimport tarfile\n\nfrom .utils import reporthook\n\nfrom collections import Counter\n\nlogger = logging.getLogger(__name__)\n\n\nclass Vocab(object):\n    """"""Defines a vocabulary object that will be used to numericalize a field.\n\n    Attributes:\n        freqs: A collections.Counter object holding the frequencies of tokens\n            in the data used to build the Vocab.\n        stoi: A collections.defaultdict instance mapping token strings to\n            numerical identifiers.\n        itos: A list of token strings indexed by their numerical identifiers.\n    """"""\n\n    # TODO (@mttk): Populate classs with default values of special symbols\n    UNK = \'<unk>\'\n\n    def __init__(self, counter, max_size=None, min_freq=1, specials=(\'<unk>\', \'<pad>\'),\n                 vectors=None, unk_init=None, vectors_cache=None, specials_first=True):\n        """"""Create a Vocab object from a collections.Counter.\n\n        Arguments:\n            counter: collections.Counter object holding the frequencies of\n                each value found in the data.\n            max_size: The maximum size of the vocabulary, or None for no\n                maximum. Default: None.\n            min_freq: The minimum frequency needed to include a token in the\n                vocabulary. Values less than 1 will be set to 1. Default: 1.\n            specials: The list of special tokens (e.g., padding or eos) that\n                will be prepended to the vocabulary. Default: [\'<unk\'>, \'<pad>\']\n            vectors: One of either the available pretrained vectors\n                or custom pretrained vectors (see Vocab.load_vectors);\n                or a list of aforementioned vectors\n            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n                to zero vectors; can be any function that takes in a Tensor and\n                returns a Tensor of the same size. Default: torch.Tensor.zero_\n            vectors_cache: directory for cached vectors. Default: \'.vector_cache\'\n            specials_first: Whether to add special tokens into the vocabulary at first.\n                If it is False, they are added into the vocabulary at last.\n                Default: True.\n        """"""\n        self.freqs = counter\n        counter = counter.copy()\n        min_freq = max(min_freq, 1)\n\n        self.itos = list()\n        self.unk_index = None\n        if specials_first:\n            self.itos = list(specials)\n            # only extend max size if specials are prepended\n            max_size = None if max_size is None else max_size + len(specials)\n\n        # frequencies of special tokens are not counted when building vocabulary\n        # in frequency order\n        for tok in specials:\n            del counter[tok]\n\n        # sort by frequency, then alphabetically\n        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n\n        for word, freq in words_and_frequencies:\n            if freq < min_freq or len(self.itos) == max_size:\n                break\n            self.itos.append(word)\n\n        if Vocab.UNK in specials:  # hard-coded for now\n            unk_index = specials.index(Vocab.UNK)  # position in list\n            # account for ordering of specials, set variable\n            self.unk_index = unk_index if specials_first else len(self.itos) + unk_index\n            self.stoi = defaultdict(self._default_unk_index)\n        else:\n            self.stoi = defaultdict()\n\n        if not specials_first:\n            self.itos.extend(list(specials))\n\n        # stoi is simply a reverse dict for itos\n        self.stoi.update({tok: i for i, tok in enumerate(self.itos)})\n\n        self.vectors = None\n        if vectors is not None:\n            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n        else:\n            assert unk_init is None and vectors_cache is None\n\n    def _default_unk_index(self):\n        return self.unk_index\n\n    def __getitem__(self, token):\n        return self.stoi.get(token, self.stoi.get(Vocab.UNK))\n\n    def __getstate__(self):\n        # avoid picking defaultdict\n        attrs = dict(self.__dict__)\n        # cast to regular dict\n        attrs[\'stoi\'] = dict(self.stoi)\n        return attrs\n\n    def __setstate__(self, state):\n        if state.get(""unk_index"", None) is None:\n            stoi = defaultdict()\n        else:\n            stoi = defaultdict(self._default_unk_index)\n        stoi.update(state[\'stoi\'])\n        state[\'stoi\'] = stoi\n        self.__dict__.update(state)\n\n    def __eq__(self, other):\n        if self.freqs != other.freqs:\n            return False\n        if self.stoi != other.stoi:\n            return False\n        if self.itos != other.itos:\n            return False\n        if self.vectors != other.vectors:\n            return False\n        return True\n\n    def __len__(self):\n        return len(self.itos)\n\n    def extend(self, v, sort=False):\n        words = sorted(v.itos) if sort else v.itos\n        for w in words:\n            if w not in self.stoi:\n                self.itos.append(w)\n                self.stoi[w] = len(self.itos) - 1\n\n    def load_vectors(self, vectors, **kwargs):\n        """"""\n        Arguments:\n            vectors: one of or a list containing instantiations of the\n                GloVe, CharNGram, or Vectors classes. Alternatively, one\n                of or a list of available pretrained vectors:\n                charngram.100d\n                fasttext.en.300d\n                fasttext.simple.300d\n                glove.42B.300d\n                glove.840B.300d\n                glove.twitter.27B.25d\n                glove.twitter.27B.50d\n                glove.twitter.27B.100d\n                glove.twitter.27B.200d\n                glove.6B.50d\n                glove.6B.100d\n                glove.6B.200d\n                glove.6B.300d\n            Remaining keyword arguments: Passed to the constructor of Vectors classes.\n        """"""\n        if not isinstance(vectors, list):\n            vectors = [vectors]\n        for idx, vector in enumerate(vectors):\n            if isinstance(vector, str):\n                # Convert the string pretrained vector identifier\n                # to a Vectors object\n                if vector not in pretrained_aliases:\n                    raise ValueError(\n                        ""Got string input vector {}, but allowed pretrained ""\n                        ""vectors are {}"".format(\n                            vector, list(pretrained_aliases.keys())))\n                vectors[idx] = pretrained_aliases[vector](**kwargs)\n            elif not isinstance(vector, Vectors):\n                raise ValueError(\n                    ""Got input vectors of type {}, expected str or ""\n                    ""Vectors object"".format(type(vector)))\n\n        tot_dim = sum(v.dim for v in vectors)\n        self.vectors = torch.Tensor(len(self), tot_dim)\n        for i, token in enumerate(self.itos):\n            start_dim = 0\n            for v in vectors:\n                end_dim = start_dim + v.dim\n                self.vectors[i][start_dim:end_dim] = v[token.strip()]\n                start_dim = end_dim\n            assert(start_dim == tot_dim)\n\n    def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):\n        """"""\n        Set the vectors for the Vocab instance from a collection of Tensors.\n\n        Arguments:\n            stoi: A dictionary of string to the index of the associated vector\n                in the `vectors` input argument.\n            vectors: An indexed iterable (or other structure supporting __getitem__) that\n                given an input index, returns a FloatTensor representing the vector\n                for the token associated with the index. For example,\n                vector[stoi[""string""]] should return the vector for ""string"".\n            dim: The dimensionality of the vectors.\n            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n                to zero vectors; can be any function that takes in a Tensor and\n                returns a Tensor of the same size. Default: torch.Tensor.zero_\n        """"""\n        self.vectors = torch.Tensor(len(self), dim)\n        for i, token in enumerate(self.itos):\n            wv_index = stoi.get(token, None)\n            if wv_index is not None:\n                self.vectors[i] = vectors[wv_index]\n            else:\n                self.vectors[i] = unk_init(self.vectors[i])\n\n\nclass SubwordVocab(Vocab):\n\n    def __init__(self, counter, max_size=None, specials=(\'<pad>\'),\n                 vectors=None, unk_init=torch.Tensor.zero_):\n        """"""Create a revtok subword vocabulary from a collections.Counter.\n\n        Arguments:\n            counter: collections.Counter object holding the frequencies of\n                each word found in the data.\n            max_size: The maximum size of the subword vocabulary, or None for no\n                maximum. Default: None.\n            specials: The list of special tokens (e.g., padding or eos) that\n                will be prepended to the vocabulary in addition to an <unk>\n                token.\n            vectors: One of either the available pretrained vectors\n                or custom pretrained vectors (see Vocab.load_vectors);\n                or a list of aforementioned vectors\n            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n                to zero vectors; can be any function that takes in a Tensor and\n                returns a Tensor of the same size. Default: torch.Tensor.zero_\n        """"""\n        try:\n            import revtok\n        except ImportError:\n            print(""Please install revtok."")\n            raise\n\n        # Hardcode unk_index as subword_vocab has no specials_first argument\n        self.unk_index = (specials.index(SubwordVocab.UNK)\n                          if SubwordVocab.UNK in specials else None)\n\n        if self.unk_index is None:\n            self.stoi = defaultdict()\n        else:\n            self.stoi = defaultdict(self._default_unk_index)\n\n        self.stoi.update({tok: i for i, tok in enumerate(specials)})\n        self.itos = specials.copy()\n\n        self.segment = revtok.SubwordSegmenter(counter, max_size)\n\n        max_size = None if max_size is None else max_size + len(self.itos)\n\n        # sort by frequency/entropy, then alphabetically\n        toks = sorted(self.segment.vocab.items(),\n                      key=lambda tup: (len(tup[0]) != 1, -tup[1], tup[0]))\n\n        for tok, _ in toks:\n            if len(self.itos) == max_size:\n                break\n            self.itos.append(tok)\n            self.stoi[tok] = len(self.itos) - 1\n\n        if vectors is not None:\n            self.load_vectors(vectors, unk_init=unk_init)\n\n\ndef _infer_shape(f):\n    num_lines, vector_dim = 0, None\n    for line in f:\n        if vector_dim is None:\n            row = line.rstrip().split(b"" "")\n            vector = row[1:]\n            # Assuming word, [vector] format\n            if len(vector) > 2:\n                # The header present in some (w2v) formats contains two elements.\n                vector_dim = len(vector)\n                num_lines += 1  # First element read\n        else:\n            num_lines += 1\n    f.seek(0)\n    return num_lines, vector_dim\n\n\nclass Vectors(object):\n\n    def __init__(self, name, cache=None,\n                 url=None, unk_init=None, max_vectors=None):\n        """"""\n        Arguments:\n           name: name of the file that contains the vectors\n           cache: directory for cached vectors\n           url: url for download if vectors not found in cache\n           unk_init (callback): by default, initialize out-of-vocabulary word vectors\n               to zero vectors; can be any function that takes in a Tensor and\n               returns a Tensor of the same size\n           max_vectors (int): this can be used to limit the number of\n               pre-trained vectors loaded.\n               Most pre-trained vector sets are sorted\n               in the descending order of word frequency.\n               Thus, in situations where the entire set doesn\'t fit in memory,\n               or is not needed for another reason, passing `max_vectors`\n               can limit the size of the loaded set.\n        """"""\n        cache = \'.vector_cache\' if cache is None else cache\n        self.itos = None\n        self.stoi = None\n        self.vectors = None\n        self.dim = None\n        self.unk_init = torch.Tensor.zero_ if unk_init is None else unk_init\n        self.cache(name, cache, url=url, max_vectors=max_vectors)\n\n    def __getitem__(self, token):\n        if token in self.stoi:\n            return self.vectors[self.stoi[token]]\n        else:\n            return self.unk_init(torch.Tensor(self.dim))\n\n    def cache(self, name, cache, url=None, max_vectors=None):\n        import ssl\n        ssl._create_default_https_context = ssl._create_unverified_context\n        if os.path.isfile(name):\n            path = name\n            if max_vectors:\n                file_suffix = \'_{}.pt\'.format(max_vectors)\n            else:\n                file_suffix = \'.pt\'\n            path_pt = os.path.join(cache, os.path.basename(name)) + file_suffix\n        else:\n            path = os.path.join(cache, name)\n            if max_vectors:\n                file_suffix = \'_{}.pt\'.format(max_vectors)\n            else:\n                file_suffix = \'.pt\'\n            path_pt = path + file_suffix\n\n        if not os.path.isfile(path_pt):\n            if not os.path.isfile(path) and url:\n                logger.info(\'Downloading vectors from {}\'.format(url))\n                if not os.path.exists(cache):\n                    os.makedirs(cache)\n                dest = os.path.join(cache, os.path.basename(url))\n                if not os.path.isfile(dest):\n                    with tqdm(unit=\'B\', unit_scale=True, miniters=1, desc=dest) as t:\n                        try:\n                            urlretrieve(url, dest, reporthook=reporthook(t))\n                        except KeyboardInterrupt as e:  # remove the partial zip file\n                            os.remove(dest)\n                            raise e\n                logger.info(\'Extracting vectors into {}\'.format(cache))\n                ext = os.path.splitext(dest)[1][1:]\n                if ext == \'zip\':\n                    with zipfile.ZipFile(dest, ""r"") as zf:\n                        zf.extractall(cache)\n                elif ext == \'gz\':\n                    if dest.endswith(\'.tar.gz\'):\n                        with tarfile.open(dest, \'r:gz\') as tar:\n                            tar.extractall(path=cache)\n            if not os.path.isfile(path):\n                raise RuntimeError(\'no vectors found at {}\'.format(path))\n\n            logger.info(""Loading vectors from {}"".format(path))\n            ext = os.path.splitext(path)[1][1:]\n            if ext == \'gz\':\n                open_file = gzip.open\n            else:\n                open_file = open\n\n            vectors_loaded = 0\n            with open_file(path, \'rb\') as f:\n                num_lines, dim = _infer_shape(f)\n                if not max_vectors or max_vectors > num_lines:\n                    max_vectors = num_lines\n\n                itos, vectors, dim = [], torch.zeros((max_vectors, dim)), None\n\n                for line in tqdm(f, total=max_vectors):\n                    # Explicitly splitting on "" "" is important, so we don\'t\n                    # get rid of Unicode non-breaking spaces in the vectors.\n                    entries = line.rstrip().split(b"" "")\n\n                    word, entries = entries[0], entries[1:]\n                    if dim is None and len(entries) > 1:\n                        dim = len(entries)\n                    elif len(entries) == 1:\n                        logger.warning(""Skipping token {} with 1-dimensional ""\n                                       ""vector {}; likely a header"".format(word, entries))\n                        continue\n                    elif dim != len(entries):\n                        raise RuntimeError(\n                            ""Vector for token {} has {} dimensions, but previously ""\n                            ""read vectors have {} dimensions. All vectors must have ""\n                            ""the same number of dimensions."".format(word, len(entries),\n                                                                    dim))\n\n                    try:\n                        if isinstance(word, bytes):\n                            word = word.decode(\'utf-8\')\n                    except UnicodeDecodeError:\n                        logger.info(""Skipping non-UTF8 token {}"".format(repr(word)))\n                        continue\n\n                    vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\n                    vectors_loaded += 1\n                    itos.append(word)\n\n                    if vectors_loaded == max_vectors:\n                        break\n\n            self.itos = itos\n            self.stoi = {word: i for i, word in enumerate(itos)}\n            self.vectors = torch.Tensor(vectors).view(-1, dim)\n            self.dim = dim\n            logger.info(\'Saving vectors to {}\'.format(path_pt))\n            if not os.path.exists(cache):\n                os.makedirs(cache)\n            torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n        else:\n            logger.info(\'Loading vectors from {}\'.format(path_pt))\n            self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n\n    def __len__(self):\n        return len(self.vectors)\n\n    def get_vecs_by_tokens(self, tokens, lower_case_backup=False):\n        """"""Look up embedding vectors of tokens.\n\n        Arguments:\n            tokens: a token or a list of tokens. if `tokens` is a string,\n                returns a 1-D tensor of shape `self.dim`; if `tokens` is a\n                list of strings, returns a 2-D tensor of shape=(len(tokens),\n                self.dim).\n            lower_case_backup : Whether to look up the token in the lower case.\n                If False, each token in the original case will be looked up;\n                if True, each token in the original case will be looked up first,\n                if not found in the keys of the property `stoi`, the token in the\n                lower case will be looked up. Default: False.\n\n        Examples:\n            >>> examples = [\'chip\', \'baby\', \'Beautiful\']\n            >>> vec = text.vocab.GloVe(name=\'6B\', dim=50)\n            >>> ret = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n        """"""\n        to_reduce = False\n\n        if not isinstance(tokens, list):\n            tokens = [tokens]\n            to_reduce = True\n\n        if not lower_case_backup:\n            indices = [self[token] for token in tokens]\n        else:\n            indices = [self[token] if token in self.stoi\n                       else self[token.lower()]\n                       for token in tokens]\n\n        vecs = torch.stack(indices)\n        return vecs[0] if to_reduce else vecs\n\n\nclass GloVe(Vectors):\n    url = {\n        \'42B\': \'http://nlp.stanford.edu/data/glove.42B.300d.zip\',\n        \'840B\': \'http://nlp.stanford.edu/data/glove.840B.300d.zip\',\n        \'twitter.27B\': \'http://nlp.stanford.edu/data/glove.twitter.27B.zip\',\n        \'6B\': \'http://nlp.stanford.edu/data/glove.6B.zip\',\n    }\n\n    def __init__(self, name=\'840B\', dim=300, **kwargs):\n        url = self.url[name]\n        name = \'glove.{}.{}d.txt\'.format(name, str(dim))\n        super(GloVe, self).__init__(name, url=url, **kwargs)\n\n\nclass FastText(Vectors):\n\n    url_base = \'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.vec\'\n\n    def __init__(self, language=""en"", **kwargs):\n        url = self.url_base.format(language)\n        name = os.path.basename(url)\n        super(FastText, self).__init__(name, url=url, **kwargs)\n\n\nclass CharNGram(Vectors):\n\n    name = \'charNgram.txt\'\n    url = (\'http://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/\'\n           \'jmt_pre-trained_embeddings.tar.gz\')\n\n    def __init__(self, **kwargs):\n        super(CharNGram, self).__init__(self.name, url=self.url, **kwargs)\n\n    def __getitem__(self, token):\n        vector = torch.Tensor(1, self.dim).zero_()\n        if token == ""<unk>"":\n            return self.unk_init(vector)\n        chars = [\'#BEGIN#\'] + list(token) + [\'#END#\']\n        num_vectors = 0\n        for n in [2, 3, 4]:\n            end = len(chars) - n + 1\n            grams = [chars[i:(i + n)] for i in range(end)]\n            for gram in grams:\n                gram_key = \'{}gram-{}\'.format(n, \'\'.join(gram))\n                if gram_key in self.stoi:\n                    vector += self.vectors[self.stoi[gram_key]]\n                    num_vectors += 1\n        if num_vectors > 0:\n            vector /= num_vectors\n        else:\n            vector = self.unk_init(vector)\n        return vector\n\n\npretrained_aliases = {\n    ""charngram.100d"": partial(CharNGram),\n    ""fasttext.en.300d"": partial(FastText, language=""en""),\n    ""fasttext.simple.300d"": partial(FastText, language=""simple""),\n    ""glove.42B.300d"": partial(GloVe, name=""42B"", dim=""300""),\n    ""glove.840B.300d"": partial(GloVe, name=""840B"", dim=""300""),\n    ""glove.twitter.27B.25d"": partial(GloVe, name=""twitter.27B"", dim=""25""),\n    ""glove.twitter.27B.50d"": partial(GloVe, name=""twitter.27B"", dim=""50""),\n    ""glove.twitter.27B.100d"": partial(GloVe, name=""twitter.27B"", dim=""100""),\n    ""glove.twitter.27B.200d"": partial(GloVe, name=""twitter.27B"", dim=""200""),\n    ""glove.6B.50d"": partial(GloVe, name=""6B"", dim=""50""),\n    ""glove.6B.100d"": partial(GloVe, name=""6B"", dim=""100""),\n    ""glove.6B.200d"": partial(GloVe, name=""6B"", dim=""200""),\n    ""glove.6B.300d"": partial(GloVe, name=""6B"", dim=""300"")\n}\n""""""Mapping from string name to factory function""""""\n\n\ndef build_vocab_from_iterator(iterator):\n    """"""\n    Build a Vocab from an iterator.\n\n    Arguments:\n        iterator: Iterator used to build Vocab. Must yield list or iterator of tokens.\n    """"""\n\n    counter = Counter()\n    with tqdm(unit_scale=0, unit=\'lines\') as t:\n        for tokens in iterator:\n            counter.update(tokens)\n            t.update(1)\n    word_vocab = Vocab(counter)\n    return word_vocab\n'"
.circleci/utils/test_sort_yaml.py,0,"b'#!/usr/bin/env python3\n\n""""""\nTo compare new version with previous:\n\n    ./regenerate.sh\n    meld <(git show HEAD:./config.yml | ./sort-yaml.py) <(cat config.yml | ./sort-yaml.py)\n""""""\n\n\nimport sys\nimport yaml\n\nsys.stdout.write(yaml.dump(yaml.load(sys.stdin, Loader=yaml.FullLoader), sort_keys=True))\n'"
build_tools/setup_helpers/__init__.py,0,b'from .extension import *  # noqa\n'
build_tools/setup_helpers/extension.py,1,"b'import os\nimport platform\nimport subprocess\nfrom pathlib import Path\n\nfrom torch.utils.cpp_extension import (\n    CppExtension,\n    BuildExtension as TorchBuildExtension\n)\n\n__all__ = [\n    \'get_ext_modules\',\n    \'BuildExtension\',\n]\n\n_ROOT_DIR = Path(__file__).parent.parent.parent.resolve()\n_CSRC_DIR = _ROOT_DIR / \'torchtext\' / \'csrc\'\n_TP_BASE_DIR = _ROOT_DIR / \'third_party\'\n_TP_INSTALL_DIR = _TP_BASE_DIR / \'build\'\n\n\ndef _get_eca(debug):\n    eca = []\n    if platform.system() == ""Windows"":\n        eca += [\'/MT\']\n    if debug:\n        eca += [""-O0"", ""-g""]\n    else:\n        if platform.system() == ""Windows"":\n            eca += [\'-O2\']\n        else:\n            eca += [""-O3""]\n    return eca\n\n\ndef _get_ela(debug):\n    ela = []\n    if debug:\n        if platform.system() == ""Windows"":\n            ela += [""/DEBUG:FULL""]\n        else:\n            ela += [""-O0"", ""-g""]\n    else:\n        if platform.system() != ""Windows"":\n            ela += [""-O3""]\n    return ela\n\n\ndef _get_srcs():\n    return [str(p) for p in _CSRC_DIR.glob(\'**/*.cpp\')]\n\n\ndef _get_include_dirs():\n    return [\n        str(_CSRC_DIR),\n        str(_TP_INSTALL_DIR / \'include\'),\n    ]\n\n\ndef _get_library_dirs():\n    return [\n        str(_TP_INSTALL_DIR / \'lib\'),\n    ]\n\n\ndef _get_libraries():\n    # NOTE: The order of the library listed bellow matters.\n    #\n    # For example, the symbol `sentencepiece::unigram::Model` is\n    # defined in sentencepiece but UNDEFINED in sentencepiece_train.\n    # GCC only remembers the last encountered symbol.\n    # Therefore placing \'sentencepiece_train\' after \'sentencepiece\' cause runtime error.\n    #\n    # $ nm third_party/build/lib/libsentencepiece_train.a | grep _ZTIN13sentencepiece7unigram5ModelE\n    #                  U _ZTIN13sentencepiece7unigram5ModelE\n    # $ nm third_party/build/lib/libsentencepiece.a       | grep _ZTIN13sentencepiece7unigram5ModelE\n    # 0000000000000000 V _ZTIN13sentencepiece7unigram5ModelE\n    return [\n        \'sentencepiece_train\',\n        \'sentencepiece\',\n    ]\n\n\ndef _build_sentence_piece(debug):\n    build_dir = _TP_BASE_DIR / \'sentencepiece\' / \'build\'\n    build_dir.mkdir(exist_ok=True)\n    build_env = os.environ.copy()\n    config = \'Debug\' if debug else \'Release\'\n    if platform.system() == \'Windows\':\n        extra_args = [\'-GNinja\']\n        build_env.setdefault(\'CC\', \'cl\')\n        build_env.setdefault(\'CXX\', \'cl\')\n    else:\n        extra_args = []\n    subprocess.run(\n        args=[\'cmake\', f\'-DSPM_ENABLE_SHARED=OFF\', f\'-DCMAKE_INSTALL_PREFIX={_TP_INSTALL_DIR}\',\n              f\'-DCMAKE_BUILD_TYPE={config}\'] + extra_args + [\'..\'],\n        cwd=str(build_dir),\n        check=True,\n        env=build_env,\n    )\n    subprocess.run(\n        args=[\'cmake\', \'--build\', \'.\', \'--target\', \'install\', \'--config\', config],\n        cwd=str(build_dir),\n        check=True,\n        env=build_env,\n    )\n\n\ndef _configure_third_party(debug):\n    _build_sentence_piece(debug)\n\n\n_EXT_NAME = \'torchtext._torchtext\'\n\n\ndef get_ext_modules(debug=False):\n    return [\n        CppExtension(\n            _EXT_NAME,\n            _get_srcs(),\n            libraries=_get_libraries(),\n            include_dirs=_get_include_dirs(),\n            library_dirs=_get_library_dirs(),\n            extra_compile_args=_get_eca(debug),\n            extra_link_args=_get_ela(debug),\n        ),\n    ]\n\n\nclass BuildExtension(TorchBuildExtension):\n    def build_extension(self, ext):\n        if ext.name == _EXT_NAME:\n            _configure_third_party(self.debug)\n        super().build_extension(ext)\n'"
docs/source/conf.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# torchtext documentation build configuration file, created by\n# sphinx-quickstart on Thu Nov 16 01:05:05 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\nimport torch\nimport torchtext\nimport pytorch_sphinx_theme\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n]\n\nnapoleon_use_ivar = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'torchtext\'\ncopyright = \'2017, Torch Contributors\'\nauthor = \'Torch Contributors\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = torchtext.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = torchtext.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'pytorch_sphinx_theme\'\nhtml_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'pytorch_project\': \'docs\',    \n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\nhtml_logo = \'_static/img/pytorch-logo-dark.svg\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\ndef setup(app):\n    app.add_stylesheet(\'css/pytorch_theme.css\')\n    app.add_stylesheet(\'https://fonts.googleapis.com/css/family=Lato\')\n\n\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'torch\': (\'http://pytorch.org/docs/0.3.0/\', None)\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'PyTorchdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'pytorch.tex\', \'torchtext Documentation\',\n     \'Torch Contributors\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'torchtext\', \'torchtext Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'torchtext\', \'torchtext Documentation\',\n     author, \'torchtext\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- A patch that prevents Sphinx from cross-referencing ivar tags -------\n# See http://stackoverflow.com/a/41184353/3343043\n\nfrom docutils import nodes\nfrom sphinx.util.docfields import TypedField\nfrom sphinx import addnodes\n\n\ndef patched_make_field(self, types, domain, items, **kw):\n    # `kw` catches `env=None` needed for newer sphinx while maintaining\n    #  backwards compatibility when passed along further down!\n\n    # type: (List, unicode, Tuple) -> nodes.field\n    def handle_item(fieldarg, content):\n        par = nodes.paragraph()\n        par += addnodes.literal_strong(\'\', fieldarg)  # Patch: this line added\n        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,\n        #                           addnodes.literal_strong))\n        if fieldarg in types:\n            par += nodes.Text(\' (\')\n            # NOTE: using .pop() here to prevent a single type node to be\n            # inserted twice into the doctree, which leads to\n            # inconsistencies later when references are resolved\n            fieldtype = types.pop(fieldarg)\n            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):\n                typename = u\'\'.join(n.astext() for n in fieldtype)\n                typename = typename.replace(\'int\', \'python:int\')\n                typename = typename.replace(\'long\', \'python:long\')\n                typename = typename.replace(\'float\', \'python:float\')\n                typename = typename.replace(\'type\', \'python:type\')\n                par.extend(self.make_xrefs(self.typerolename, domain, typename,\n                                           addnodes.literal_emphasis, **kw))\n            else:\n                par += fieldtype\n            par += nodes.Text(\')\')\n        par += nodes.Text(\' -- \')\n        par += content\n        return par\n\n    fieldname = nodes.field_name(\'\', self.label)\n    if len(items) == 1 and self.can_collapse:\n        fieldarg, content = items[0]\n        bodynode = handle_item(fieldarg, content)\n    else:\n        bodynode = self.list_type()\n        for fieldarg, content in items:\n            bodynode += nodes.list_item(\'\', handle_item(fieldarg, content))\n    fieldbody = nodes.field_body(\'\', bodynode)\n    return nodes.field(\'\', fieldname, fieldbody)\n\n\nTypedField.make_field = patched_make_field\n'"
examples/text_classification/create_datasets.py,2,"b'import os\nimport logging\nimport argparse\n\nimport torch\n\nfrom torchtext.datasets import text_classification\n\nr""""""\nOnce you have the datasets, you can save them as a list of tensors\nand load later on in other projects. Here is an example to load/save\ntext_classification datasets.\n""""""\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=(\n        \'Create list of Tensors for training and \'\n        \'testing based on given datasets\'))\n    parser.add_argument(\'dataset\', choices=text_classification.DATASETS,\n                        help=\'dataset name\')\n    parser.add_argument(\'--logging-level\', default=\'WARNING\',\n                        help=\'logging level (default=WARNING)\')\n    parser.add_argument(\'--ngrams\', type=int, default=2,\n                        help=\'ngrams (default=2)\')\n    parser.add_argument(\'--root\', default=\'.data\',\n                        help=\'data directory (default=.data)\')\n    args = parser.parse_args()\n\n    logging.basicConfig(level=getattr(logging, args.logging_level))\n    train_dataset, test_dataset = text_classification.DATASETS[args.dataset](\n        root=args.root, ngrams=args.ngrams)\n    train_data_path = os.path.join(\n        args.root,\n        args.dataset +\n        ""_ngrams_{}_train.data"".format(\n            args.ngrams))\n    test_data_path = os.path.join(\n        args.root,\n        args.dataset +\n        ""_ngrams_{}_test.data"".format(\n            args.ngrams))\n    print(""Saving train data to {}"".format(train_data_path))\n    torch.save(train_dataset, train_data_path)\n    print(""Saving test data to {}"".format(test_data_path))\n    torch.save(test_dataset, test_data_path)\n'"
examples/text_classification/iterable_train.py,15,"b'import logging\nimport argparse\n\nimport torch\nimport io\nimport time\n\nfrom torch.utils.data import DataLoader\n\nfrom model import TextSentiment\n\nfrom torchtext.data.utils import ngrams_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.utils import unicode_csv_reader\n\nfrom tqdm import tqdm\n\nr""""""\nThis example shows how to build an iterable dataset from the iterator. The\nget_csv_iterator() function is used to read CSV file for the data. An abstract\ndataset class setups the iterators for training the model.\n""""""\n\n\ndef generate_batch(batch):\n    """"""\n    Since the text entries have different lengths, a custom function\n    generate_batch() is used to generate data batches and offsets,\n    which are compatible with EmbeddingBag. The function is passed\n    to \'collate_fn\' in torch.utils.data.DataLoader. The input to\n    \'collate_fn\' is a list of tensors with the size of batch_size,\n    and the \'collate_fn\' function packs them into a mini-batch.\n    Pay attention here and make sure that \'collate_fn\' is declared\n    as a top level def. This ensures that the function is available\n    in each worker.\n    Output:\n        text: the text entries in the data_batch are packed into a list and\n            concatenated as a single tensor for the input of nn.EmbeddingBag.\n        offsets: the offsets is a tensor of delimiters to represent the beginning\n            index of the individual sequence in the text tensor.\n        label: a tensor saving the labels of individual text entries.\n    """"""\n\n    label = torch.tensor([entry[0] for entry in batch])\n    text = [entry[1] for entry in batch]\n    offsets = [0] + [len(entry) for entry in text]\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text = torch.cat(text)\n    return text, offsets, label\n\n\nr""""""\ntorch.utils.data.DataLoader is recommended for PyTorch users to load data.\nWe use DataLoader here to load datasets and send it to the train()\nand text() functions.\n""""""\n\n\ndef train_and_valid(lr_, num_epoch, train_data_, valid_data_):\n    r""""""\n    Here we use SGD optimizer to train the model.\n\n    Arguments:\n        lr_: learning rate\n        num_epoch: the number of epoches for training the model\n        train_data_: the data used to train the model\n        valid_data_: the data used to validation\n        trian_len: the length of training dataset.\n    """"""\n    train_data = DataLoader(\n        train_data_,\n        batch_size=batch_size,\n        collate_fn=generate_batch,\n        num_workers=args.num_workers,\n        pin_memory=True)\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr_)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=args.lr_gamma)\n\n    for epoch in range(num_epochs):\n\n        print(""Training on epoch {}"".format(epoch))\n        # Train the model\n        with tqdm(unit_scale=0, unit=\'lines\', total=train_len) as t:\n            avg_loss = 0.0\n            for i, (text, offsets, label) in enumerate(train_data):\n                t.update(len(label))\n                optimizer.zero_grad()\n                text, offsets, label = text.to(device), offsets.to(device), \\\n                    label.to(device)\n                output = model(text, offsets)\n                loss = criterion(output, label)\n                loss.backward()\n                avg_loss += loss.item()\n                optimizer.step()\n                if i % (16 * batch_size) == 0:\n                    avg_loss = avg_loss / (16 * batch_size)\n                    avg_loss = 0\n                    t.set_description(\n                        ""lr: {:9.3f} loss: {:9.3f}"".format(\n                            scheduler.get_lr()[0], loss))\n\n        # Adjust the learning rate\n        scheduler.step()\n\n        # Test the model on valid set\n        print(""Valid - Accuracy: {}"".format(test(valid_data_)))\n\n\ndef test(data_):\n    r""""""\n    Arguments:\n        data_: the data used to train the model\n    """"""\n    data = DataLoader(\n        data_,\n        batch_size=batch_size,\n        collate_fn=generate_batch,\n        num_workers=args.num_workers,\n        pin_memory=True)\n    total_accuracy = []\n    for text, offsets, label in data:\n        text, offsets, label = text.to(device), offsets.to(device), label.to(device)\n        with torch.no_grad():\n            output = model(text, offsets)\n            accuracy = (output.argmax(1) == label).float().mean().item()\n            total_accuracy.append(accuracy)\n\n    # In case that nothing in the dataset\n    if total_accuracy == []:\n        return 0.0\n\n    return sum(total_accuracy) / len(total_accuracy)\n\n\ndef get_csv_iterator(data_path, ngrams, vocab, start=0, num_lines=None):\n    r""""""\n    Generate an iterator to read CSV file.\n    The yield values are an integer for the label and a tensor for the text part.\n\n    Arguments:\n        data_path: a path for the data file.\n        ngrams: the number used for ngrams.\n        vocab: a vocab object saving the string-to-index information\n        start: the starting line to read (Default: 0). This is useful for\n            on-fly multi-processing data loading.\n        num_lines: the number of lines read by the iterator (Default: None).\n\n    """"""\n    def iterator(start, num_lines):\n        tokenizer = get_tokenizer(""basic_english"")\n        with io.open(data_path, encoding=""utf8"") as f:\n            reader = unicode_csv_reader(f)\n            for i, row in enumerate(reader):\n                if i == start:\n                    break\n            for _ in range(num_lines):\n                tokens = \' \'.join(row[1:])\n                tokens = ngrams_iterator(tokenizer(tokens), ngrams)\n                yield int(row[0]) - 1, torch.tensor([vocab[token] for token in tokens])\n                try:\n                    row = next(reader)\n                except StopIteration:\n                    f.seek(0)\n                    reader = unicode_csv_reader(f)\n                    row = next(reader)\n    return iterator\n\n\nclass Dataset(torch.utils.data.IterableDataset):\n    r""""""\n    An iterable dataset to save the data. This dataset supports multi-processing\n    to load the data.\n\n    Arguments:\n        iterator: the iterator to read data.\n        num_lines: the number of lines read by the individual iterator.\n    """"""\n    def __init__(self, iterator, num_lines):\n        super(Dataset, self).__init__()\n        self._num_lines = num_lines\n        self._iterator = iterator\n        self._setup = False\n\n    def _setup_iterator(self):\n        r""""""\n        _setup_iterator() function assign the starting line and the number\n        of lines to read for the individual worker. Then, send them to the iterator\n        to load the data.\n\n        If worker info is not avaialble, it will read all the lines across epochs.\n        """"""\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info:\n            chunk = int(self._num_lines / worker_info.num_workers)\n            start = chunk * worker_info.id\n            read = chunk\n            if worker_info.id == worker_info.num_workers - 1:\n                # The last worker needs to pick up some extra lines\n                # if the number of lines aren\'t exactly divisible\n                # by the number of workers.\n                # Each epoch we loose an \'extra\' number of lines.\n                extra = self._num_lines % worker_info.num_workers\n                read += extra\n        else:\n            start = 0\n            read = self._num_lines\n        self._iterator = self._iterator(start, read)\n\n    def __iter__(self):\n        if self._setup is False:\n            self._setup_iterator()\n            self._setup = True\n        for x in self._iterator:\n            yield x\n\n\ndef count(data_path):\n    r""""""\n    return the total numerber of text entries and labels.\n    """"""\n    with io.open(data_path, encoding=""utf8"") as f:\n        reader = unicode_csv_reader(f)\n        labels = [int(row[0]) for row in reader]\n        num_lines = len(labels)\n        num_labels = len(set(labels))\n        return num_labels, num_lines\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=\'Train a text classification model on text classification datasets.\')\n    parser.add_argument(\'train_data_path\', help=\'path for train data\')\n    parser.add_argument(\'test_data_path\', help=\'path for test data\')\n    parser.add_argument(\'vocab\', help=\'path for vocab object\')\n    parser.add_argument(\'--num-epochs\', type=int, default=5,\n                        help=\'num epochs (default=5)\')\n    parser.add_argument(\'--embed-dim\', type=int, default=32,\n                        help=\'embed dim. (default=32)\')\n    parser.add_argument(\'--batch-size\', type=int, default=16,\n                        help=\'batch size (default=16)\')\n    parser.add_argument(\'--split-ratio\', type=float, default=0.95,\n                        help=\'train/valid split ratio (default=0.95)\')\n    parser.add_argument(\'--lr\', type=float, default=4.0,\n                        help=\'learning rate (default=4.0)\')\n    parser.add_argument(\'--lr-gamma\', type=float, default=0.9,\n                        help=\'gamma value for lr (default=0.9)\')\n    parser.add_argument(\'--ngrams\', type=int, default=2,\n                        help=\'ngrams (default=2)\')\n    parser.add_argument(\'--num-workers\', type=int, default=1,\n                        help=\'num of workers (default=1)\')\n    parser.add_argument(\'--device\', default=\'cpu\',\n                        help=\'device (default=cpu)\')\n    parser.add_argument(\'--data\', default=\'.data\',\n                        help=\'data directory (default=.data)\')\n    parser.add_argument(\'--save-model-path\',\n                        help=\'path for saving model\')\n    parser.add_argument(\'--logging-level\', default=\'WARNING\',\n                        help=\'logging level (default=WARNING)\')\n    args = parser.parse_args()\n\n    num_epochs = args.num_epochs\n    embed_dim = args.embed_dim\n    batch_size = args.batch_size\n    lr = args.lr\n    device = args.device\n    data = args.data\n    ngrams = args.ngrams\n    split_ratio = args.split_ratio\n\n    train_data_path = args.train_data_path\n    test_data_path = args.test_data_path\n\n    logging.basicConfig(level=getattr(logging, args.logging_level))\n\n    start_time = time.time()\n    logging.info(""Loading vocab from: {}"".format(args.vocab))\n    vocab = torch.load(args.vocab)\n\n    logging.info(""Counting training lines and labels"")\n    num_labels, train_num_lines = count(train_data_path)\n    logging.info(""Counting testing lines and labels"")\n    num_labels, test_num_lines = count(test_data_path)\n\n    # Split training dataset into train and valid\n    train_len = int(train_num_lines * split_ratio)\n\n    logging.info(""Loading iterable datasets"")\n    train_dataset = Dataset(\n        get_csv_iterator(\n            train_data_path,\n            ngrams,\n            vocab, start=0, num_lines=train_len),\n        train_len)\n\n    valid_dataset = Dataset(\n        get_csv_iterator(\n            train_data_path,\n            ngrams,\n            vocab, start=train_len),\n        train_num_lines - train_len)\n\n    test_dataset = Dataset(\n        get_csv_iterator(\n            test_data_path,\n            ngrams,\n            vocab),\n        test_num_lines)\n\n    logging.info(""Creating models"")\n    model = TextSentiment(len(vocab),\n                          embed_dim, num_labels).to(device)\n    criterion = torch.nn.CrossEntropyLoss().to(device)\n    logging.info(""Setup took: {:3.0f}s"".format(time.time() - start_time))\n\n    logging.info(""Starting training"")\n    train_and_valid(lr, num_epochs, train_dataset, valid_dataset)\n    print(""Test - Accuracy: {}"".format(test(test_dataset)))\n\n    if args.save_model_path:\n        print(""Saving model to {}"".format(args.save_model_path))\n        torch.save(model.to(\'cpu\'), args.save_model_path)\n'"
examples/text_classification/model.py,1,"b'import torch.nn as nn\n\nr""""""\nThe model is composed of the embeddingbag layer and the linear layer.\n\nnn.EmbeddingBag computes the mean of \'bags\' of embeddings. The text\nentries here have different lengths. nn.EmbeddingBag requires no\npadding because the lengths of sentences are saved in offsets.\nTherefore, this method is much faster than the original one\nwith TorchText Iterator and Batch.\n\nAdditionally, since it accumulates the average across the embeddings on the fly,\nnn.EmbeddingBag can enhance the performance and memory efficiency\nto process a sequence of tensors.\n\n""""""\n\n\nclass TextSentiment(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n\n    def forward(self, text, offsets):\n        r""""""\n        Arguments:\n            text: 1-D tensor representing a bag of text tensors\n            offsets: a list of offsets to delimit the 1-D text tensor\n                into the individual sequences.\n\n        """"""\n        return self.fc(self.embedding(text, offsets))\n'"
examples/text_classification/predict.py,5,"b'import torch\nimport sys\nimport argparse\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.data.utils import ngrams_iterator\n\n\ndef predict(text, model, dictionary, ngrams):\n    r""""""\n    The predict() function here is used to test the model on a sample text.\n    The input text is numericalized with the vocab and then sent to\n    the model for inference.\n\n    Arguments:\n        text: a sample text string\n        model: the trained model\n        dictionary: a vocab object for the information of string-to-index\n        ngrams: the number of ngrams.\n    """"""\n    tokenizer = get_tokenizer(""basic_english"")\n    with torch.no_grad():\n        text = torch.tensor([dictionary[token]\n                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n        output = model(text, torch.tensor([0]))\n        return output.argmax(1).item() + 1\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=\'Predict text from stdin given model and dictionary\')\n    parser.add_argument(\'model\', help=\'the path for model\')\n    parser.add_argument(\'dictionary\', help=\'the path for dictionary\')\n    parser.add_argument(\'--ngrams\', type=int, default=2,\n                        help=\'ngrams (default=2)\')\n    args = parser.parse_args()\n\n    model = torch.load(args.model)\n    dictionary = torch.load(args.dictionary)\n    for line in sys.stdin:\n        print(predict(line, model, dictionary, args.ngrams))\n'"
examples/text_classification/spm_dataset.py,1,"b'import logging\nimport torch\nimport io\nfrom torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\nfrom os import path\nfrom torchtext.datasets.text_classification import URLS\nfrom torchtext.data.functional import generate_sp_model, \\\n    load_sp_model, sentencepiece_numericalizer\nfrom torchtext.datasets import text_classification\n\n\ndef _create_data_with_sp_transform(sp_generator, data_path):\n\n    data = []\n    labels = []\n    with io.open(data_path, encoding=""utf8"") as f:\n        reader = unicode_csv_reader(f)\n        for row in reader:\n            corpus = \' \'.join(row[1:])\n            token_ids = list(sp_generator([corpus]))[0]\n            label = int(row[0]) - 1\n            data.append((label, torch.tensor(token_ids)))\n            labels.append(label)\n    return data, set(labels)\n\n\ndef setup_datasets(dataset_name, root=\'.data\', vocab_size=20000, include_unk=False):\n    dataset_tar = download_from_url(URLS[dataset_name], root=root)\n    extracted_files = extract_archive(dataset_tar)\n\n    for fname in extracted_files:\n        if fname.endswith(\'train.csv\'):\n            train_csv_path = fname\n        if fname.endswith(\'test.csv\'):\n            test_csv_path = fname\n\n    # generate sentencepiece  pretrained tokenizer\n    if not path.exists(\'m_user.model\'):\n        logging.info(\'Generate SentencePiece pretrained tokenizer...\')\n        generate_sp_model(train_csv_path, vocab_size)\n\n    sp_model = load_sp_model(""m_user.model"")\n    sp_generator = sentencepiece_numericalizer(sp_model)\n    train_data, train_labels = _create_data_with_sp_transform(sp_generator,\n                                                              train_csv_path)\n    test_data, test_labels = _create_data_with_sp_transform(sp_generator,\n                                                            test_csv_path)\n\n    if len(train_labels ^ test_labels) > 0:\n        raise ValueError(""Training and test labels don\'t match"")\n    return (text_classification.TextClassificationDataset(None, train_data, train_labels),\n            text_classification.TextClassificationDataset(None, test_data, test_labels))\n'"
examples/text_classification/train.py,13,"b'import os\nimport logging\nimport argparse\n\nimport torch\nimport sys\n\nfrom torchtext.datasets import text_classification\nfrom torch.utils.data import DataLoader\n\nfrom model import TextSentiment\nfrom torch.utils.data.dataset import random_split\n\nr""""""\nThis file shows the training process of the text classification model.\n""""""\n\n\ndef generate_batch(batch):\n    r""""""\n    Since the text entries have different lengths, a custom function\n    generate_batch() is used to generate data batches and offsets,\n    which are compatible with EmbeddingBag. The function is passed\n    to \'collate_fn\' in torch.utils.data.DataLoader. The input to\n    \'collate_fn\' is a list of tensors with the size of batch_size,\n    and the \'collate_fn\' function packs them into a mini-batch.\n    Pay attention here and make sure that \'collate_fn\' is declared\n    as a top level def. This ensures that the function is available\n    in each worker.\n\n    Output:\n        text: the text entries in the data_batch are packed into a list and\n            concatenated as a single tensor for the input of nn.EmbeddingBag.\n        offsets: the offsets is a tensor of delimiters to represent the beginning\n            index of the individual sequence in the text tensor.\n        cls: a tensor saving the labels of individual text entries.\n    """"""\n    label = torch.tensor([entry[0] for entry in batch])\n    text = [entry[1] for entry in batch]\n    offsets = [0] + [len(entry) for entry in text]\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text = torch.cat(text)\n    return text, offsets, label\n\n\nr""""""\ntorch.utils.data.DataLoader is recommended for PyTorch users to load data.\nWe use DataLoader here to load datasets and send it to the train_and_valid()\nand text() functions.\n\n""""""\n\n\ndef train_and_valid(lr_, sub_train_, sub_valid_):\n    r""""""\n    We use a SGD optimizer to train the model here and the learning rate\n    decreases linearly with the progress of the training process.\n\n    Arguments:\n        lr_: learning rate\n        sub_train_: the data used to train the model\n        sub_valid_: the data used for validation\n    """"""\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr_)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=args.lr_gamma)\n    train_data = DataLoader(sub_train_, batch_size=batch_size, shuffle=True,\n                            collate_fn=generate_batch, num_workers=args.num_workers)\n    num_lines = num_epochs * len(train_data)\n\n    for epoch in range(num_epochs):\n\n        # Train the model\n        for i, (text, offsets, cls) in enumerate(train_data):\n            optimizer.zero_grad()\n            text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n            output = model(text, offsets)\n            loss = criterion(output, cls)\n            loss.backward()\n            optimizer.step()\n            processed_lines = i + len(train_data) * epoch\n            progress = processed_lines / float(num_lines)\n            if processed_lines % 128 == 0:\n                sys.stderr.write(\n                    ""\\rProgress: {:3.0f}% lr: {:3.3f} loss: {:3.3f}"".format(\n                        progress * 100, scheduler.get_lr()[0], loss))\n        # Adjust the learning rate\n        scheduler.step()\n\n        # Test the model on valid set\n        print("""")\n        print(""Valid - Accuracy: {}"".format(test(sub_valid_)))\n\n\ndef test(data_):\n    r""""""\n    Arguments:\n        data_: the data used to train the model\n    """"""\n    data = DataLoader(data_, batch_size=batch_size, collate_fn=generate_batch)\n    total_accuracy = []\n    for text, offsets, cls in data:\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        with torch.no_grad():\n            output = model(text, offsets)\n            accuracy = (output.argmax(1) == cls).float().mean().item()\n            total_accuracy.append(accuracy)\n\n    # In case that nothing in the dataset\n    if total_accuracy == []:\n        return 0.0\n\n    return sum(total_accuracy) / len(total_accuracy)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=\'Train a text classification model on text classification datasets.\')\n    parser.add_argument(\'dataset\', choices=text_classification.DATASETS)\n    parser.add_argument(\'--num-epochs\', type=int, default=5,\n                        help=\'num epochs (default=5)\')\n    parser.add_argument(\'--embed-dim\', type=int, default=32,\n                        help=\'embed dim. (default=32)\')\n    parser.add_argument(\'--batch-size\', type=int, default=16,\n                        help=\'batch size (default=16)\')\n    parser.add_argument(\'--split-ratio\', type=float, default=0.95,\n                        help=\'train/valid split ratio (default=0.95)\')\n    parser.add_argument(\'--lr\', type=float, default=4.0,\n                        help=\'learning rate (default=4.0)\')\n    parser.add_argument(\'--lr-gamma\', type=float, default=0.8,\n                        help=\'gamma value for lr (default=0.8)\')\n    parser.add_argument(\'--ngrams\', type=int, default=2,\n                        help=\'ngrams (default=2)\')\n    parser.add_argument(\'--num-workers\', type=int, default=1,\n                        help=\'num of workers (default=1)\')\n    parser.add_argument(\'--device\', default=\'cpu\',\n                        help=\'device (default=cpu)\')\n    parser.add_argument(\'--data\', default=\'.data\',\n                        help=\'data directory (default=.data)\')\n    parser.add_argument(\'--use-sp-tokenizer\', type=bool, default=False,\n                        help=\'use sentencepiece tokenizer (default=False)\')\n    parser.add_argument(\'--sp-vocab-size\', type=int, default=20000,\n                        help=\'vocab size in sentencepiece model (default=20000)\')\n    parser.add_argument(\'--dictionary\',\n                        help=\'path to save vocab\')\n    parser.add_argument(\'--save-model-path\',\n                        help=\'path for saving model\')\n    parser.add_argument(\'--logging-level\', default=\'WARNING\',\n                        help=\'logging level (default=WARNING)\')\n    args = parser.parse_args()\n\n    num_epochs = args.num_epochs\n    embed_dim = args.embed_dim\n    batch_size = args.batch_size\n    lr = args.lr\n    device = args.device\n    data = args.data\n    split_ratio = args.split_ratio\n    # two args for sentencepiece tokenizer\n    use_sp_tokenizer = args.use_sp_tokenizer\n    sp_vocab_size = args.sp_vocab_size\n\n    logging.basicConfig(level=getattr(logging, args.logging_level))\n\n    if not os.path.exists(data):\n        print(""Creating directory {}"".format(data))\n        os.mkdir(data)\n\n    if use_sp_tokenizer:\n        import spm_dataset\n        train_dataset, test_dataset = spm_dataset.setup_datasets(args.dataset,\n                                                                 root=\'.data\',\n                                                                 vocab_size=sp_vocab_size)\n        model = TextSentiment(sp_vocab_size, embed_dim,\n                              len(train_dataset.get_labels())).to(device)\n\n    else:\n        train_dataset, test_dataset = text_classification.DATASETS[args.dataset](\n            root=data, ngrams=args.ngrams)\n        model = TextSentiment(len(train_dataset.get_vocab()),\n                              embed_dim, len(train_dataset.get_labels())).to(device)\n\n    criterion = torch.nn.CrossEntropyLoss().to(device)\n\n    # split train_dataset into train and valid\n    train_len = int(len(train_dataset) * split_ratio)\n    sub_train_, sub_valid_ = \\\n        random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n    train_and_valid(lr, sub_train_, sub_valid_)\n    print(""Test - Accuracy: {}"".format(test(test_dataset)))\n\n    if args.save_model_path:\n        print(""Saving model to {}"".format(args.save_model_path))\n        torch.save(model.to(\'cpu\'), args.save_model_path)\n\n    if args.dictionary is not None:\n        print(""Save vocab to {}"".format(args.dictionary))\n        torch.save(train_dataset.get_vocab(), args.dictionary)\n'"
examples/utils/download_extract.py,0,"b""import logging\nimport argparse\n\nfrom torchtext.utils import extract_archive\nfrom torchtext.utils import download_from_url\n\nparser = argparse.ArgumentParser(\n    description='Download and extract a given dataset')\nparser.add_argument('--url',\n                    default='http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/'\n                    'validation.tar.gz')\nparser.add_argument('--data', default='./validation.tar.gz')\nparser.add_argument('--logging-level', default='WARNING')\nargs = parser.parse_args()\n\nlogging.basicConfig(level=getattr(logging, args.logging_level))\n\ntar_file = download_from_url(args.url, args.data)\nextracted_files = extract_archive(args.data, 'extracted_files')\n"""
examples/vocab/vocab.py,1,"b'import logging\nimport argparse\n\nimport torch\nimport io\n\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import ngrams_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.utils import unicode_csv_reader\n\n\ndef csv_iterator(data_path, ngrams):\n    tokenizer = get_tokenizer(""basic_english"")\n    with io.open(data_path, encoding=""utf8"") as f:\n        reader = unicode_csv_reader(f)\n        for row in reader:\n            tokens = \' \'.join(row[1:])\n            yield ngrams_iterator(tokenizer(tokens), ngrams)\n\n\nparser = argparse.ArgumentParser(\n    description=\'Train a text classification model on AG_NEWS\')\nparser.add_argument(\'data_path\')\nparser.add_argument(\'save_vocab_path\')\nparser.add_argument(\'--ngrams\', type=int, default=2)\nparser.add_argument(\'--logging-level\', default=\'WARNING\')\nargs = parser.parse_args()\n\nngrams = args.ngrams\n\nlogging.basicConfig(level=getattr(logging, args.logging_level))\n\nvocab = build_vocab_from_iterator(csv_iterator(args.data_path, ngrams))\n\nprint(""Saving vocab to {}"".format(args.save_vocab_path))\ntorch.save(vocab, args.save_vocab_path)\n'"
test/common/__init__.py,0,b''
test/common/assets.py,0,"b'from pathlib import Path\n\n_ASSET_DIR = (Path(__file__).parent.parent / ""asset"").resolve()\n\n\ndef get_asset_path(*path_components):\n    """"""Get the path to the file under `test/assets` directory.""""""\n    return str(_ASSET_DIR.joinpath(*path_components))\n'"
test/common/torchtext_test_case.py,0,"b'# -*- coding: utf-8 -*-\nfrom unittest import TestCase\nimport json\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport tempfile\n\nlogger = logging.getLogger(__name__)\n\n\nclass TorchtextTestCase(TestCase):\n    def setUp(self):\n        logging.basicConfig(format=(\'%(asctime)s - %(levelname)s - \'\n                                    \'%(name)s - %(message)s\'),\n                            level=logging.INFO)\n        # Directory where everything temporary and test-related is written\n        self.project_root = os.path.abspath(os.path.realpath(os.path.join(\n            os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir)))\n        self.test_dir = tempfile.mkdtemp()\n        self.test_ppid_dataset_path = os.path.join(self.test_dir, ""test_ppid_dataset"")\n        self.test_numerical_features_dataset_path = os.path.join(\n            self.test_dir, ""test_numerical_features_dataset"")\n        self.test_newline_dataset_path = os.path.join(self.test_dir,\n                                                      ""test_newline_dataset"")\n        self.test_has_header_dataset_path = os.path.join(self.test_dir,\n                                                         ""test_has_header_dataset"")\n        self.test_missing_field_dataset_path = os.path.join(self.test_dir,\n                                                            ""test_msg_field_dst"")\n        self.test_dataset_splitting_path = os.path.join(self.test_dir,\n                                                        ""test_dataset_split"")\n        self.test_nested_key_json_dataset_path = os.path.join(self.test_dir,\n                                                              ""test_nested_key_json"")\n\n    def tearDown(self):\n        try:\n            shutil.rmtree(self.test_dir)\n        except:\n            subprocess.call([""rm"", ""-rf"", self.test_dir])\n\n    def write_test_ppid_dataset(self, data_format=""csv""):\n        data_format = data_format.lower()\n        if data_format == ""csv"":\n            delim = "",""\n        elif data_format == ""tsv"":\n            delim = ""\\t""\n        dict_dataset = [\n            {""id"": ""0"", ""question1"": ""When do you use \xe3\x82\xb7 instead of \xe3\x81\x97?"",\n             ""question2"": ""When do you use \\""&\\"" instead of \\""and\\""?"",\n             ""label"": ""0""},\n            {""id"": ""1"", ""question1"": ""Where was Lincoln born?"",\n             ""question2"": ""Which location was Abraham Lincoln born?"",\n             ""label"": ""1""},\n            {""id"": ""2"", ""question1"": ""What is 2+2"",\n             ""question2"": ""2+2=?"",\n             ""label"": ""1""},\n        ]\n        with open(self.test_ppid_dataset_path, ""w"", encoding=""utf-8"") as test_ppid_dataset_file:\n            for example in dict_dataset:\n                if data_format == ""json"":\n                    test_ppid_dataset_file.write(json.dumps(example) + ""\\n"")\n                elif data_format == ""csv"" or data_format == ""tsv"":\n                    test_ppid_dataset_file.write(""{}\\n"".format(\n                        delim.join([example[""id""], example[""question1""],\n                                    example[""question2""], example[""label""]])))\n                else:\n                    raise ValueError(""Invalid format {}"".format(data_format))\n\n    def write_test_nested_key_json_dataset(self):\n        """"""\n        Used only to test nested key parsing of Example.fromJSON()\n        """"""\n        dict_dataset = [\n            {""foods"":\n                {""fruits"": [""Apple"", ""Banana""],\n                 ""vegetables"": [\n                    {""name"": ""Broccoli""},\n                    {""name"": ""Cabbage""}]}},\n            {""foods"":\n                {""fruits"": [""Cherry"", ""Grape"", ""Lemon""],\n                 ""vegetables"": [\n                    {""name"": ""Cucumber""},\n                    {""name"": ""Lettuce""}]}},\n            {""foods"":\n                {""fruits"": [""Orange"", ""Pear"", ""Strawberry""],\n                 ""vegetables"": [\n                    {""name"": ""Marrow""},\n                    {""name"": ""Spinach""}]}},\n        ]\n        with open(self.test_nested_key_json_dataset_path,\n                  ""w"") as test_nested_key_json_dataset_file:\n            for example in dict_dataset:\n                test_nested_key_json_dataset_file.write(json.dumps(example) + ""\\n"")\n\n    def write_test_numerical_features_dataset(self):\n        with open(self.test_numerical_features_dataset_path,\n                  ""w"") as test_numerical_features_dataset_file:\n            test_numerical_features_dataset_file.write(""0.1\\t1\\tteststring1\\n"")\n            test_numerical_features_dataset_file.write(""0.5\\t12\\tteststring2\\n"")\n            test_numerical_features_dataset_file.write(""0.2\\t0\\tteststring3\\n"")\n            test_numerical_features_dataset_file.write(""0.4\\t12\\tteststring4\\n"")\n            test_numerical_features_dataset_file.write(""0.9\\t9\\tteststring5\\n"")\n\n    def make_mock_dataset(self, num_examples=30, num_labels=3):\n        num_repetitions = int(round(num_examples / num_labels)) + 1\n\n        texts = [str(i) for i in range(num_examples)]\n        labels = list(range(num_labels)) * num_repetitions\n        labels = [str(l) for l in labels[:num_examples]]\n\n        dict_dataset = [\n            {\'text\': t, \'label\': l} for t, l in zip(texts, labels)\n        ]\n        return dict_dataset\n\n    def write_test_splitting_dataset(self, num_examples=30, num_labels=3):\n        dict_dataset = self.make_mock_dataset(num_examples, num_labels)\n        delim = "",""\n\n        with open(self.test_dataset_splitting_path,\n                  ""w"") as test_splitting_dataset_file:\n            for example in dict_dataset:\n                test_splitting_dataset_file.write(""{}\\n"".format(\n                    delim.join([example[\'text\'], example[\'label\']])))\n\n\ndef verify_numericalized_example(field, test_example_data,\n                                 test_example_numericalized,\n                                 test_example_lengths=None,\n                                 batch_first=False, train=True):\n    """"""\n    Function to verify that numericalized example is correct\n    with respect to the Field\'s Vocab.\n    """"""\n    if isinstance(test_example_numericalized, tuple):\n        test_example_numericalized, lengths = test_example_numericalized\n        assert test_example_lengths == lengths.tolist()\n    if batch_first:\n        test_example_numericalized.t_()\n    # Transpose numericalized example so we can compare over batches\n    for example_idx, numericalized_single_example in enumerate(\n            test_example_numericalized.t()):\n        assert len(test_example_data[example_idx]) == len(numericalized_single_example)\n        assert numericalized_single_example.volatile is not train\n        for token_idx, numericalized_token in enumerate(\n                numericalized_single_example):\n            # Convert from Variable to int\n            numericalized_token = numericalized_token.item()  # Pytorch v4 compatibility\n            test_example_token = test_example_data[example_idx][token_idx]\n            # Check if the numericalized example is correct, taking into\n            # account unknown tokens.\n            if field.vocab.stoi[test_example_token] != 0:\n                # token is in-vocabulary\n                assert (field.vocab.itos[numericalized_token]\n                        == test_example_token)\n            else:\n                # token is OOV and <unk> always has an index of 0\n                assert numericalized_token == 0\n'"
test/data/__init__.py,0,b''
test/data/test_batch.py,1,"b'import torch\nimport torchtext.data as data\n\nfrom ..common.torchtext_test_case import TorchtextTestCase\n\n\nclass TestDataset(TorchtextTestCase):\n    def test_batch_with_missing_field(self):\n        # smoke test to see if batches with missing attributes are shown properly\n        with open(self.test_missing_field_dataset_path, ""wt"") as f:\n            f.write(""text,label\\n1,0"")\n\n        dst = data.TabularDataset(path=self.test_missing_field_dataset_path,\n                                  format=""csv"", skip_header=True,\n                                  fields=[(""text"", data.Field(use_vocab=False,\n                                                              sequential=False)),\n                                          (""label"", None)])\n        itr = data.Iterator(dst, batch_size=64)\n        str(next(itr.__iter__()))\n\n    def test_batch_iter(self):\n        self.write_test_numerical_features_dataset()\n        FLOAT = data.Field(use_vocab=False, sequential=False,\n                           dtype=torch.float)\n        INT = data.Field(use_vocab=False, sequential=False, is_target=True)\n        TEXT = data.Field(sequential=False)\n\n        dst = data.TabularDataset(path=self.test_numerical_features_dataset_path,\n                                  format=""tsv"", skip_header=False,\n                                  fields=[(""float"", FLOAT),\n                                          (""int"", INT),\n                                          (""text"", TEXT)])\n        TEXT.build_vocab(dst)\n        itr = data.Iterator(dst, batch_size=2, device=-1, shuffle=False)\n        fld_order = [k for k, v in dst.fields.items() if\n                     v is not None and not v.is_target]\n        batch = next(iter(itr))\n        (x1, x2), y = batch\n        x = (x1, x2)[fld_order.index(""float"")]\n        self.assertEquals(y.data[0], 1)\n        self.assertEquals(y.data[1], 12)\n        self.assertAlmostEqual(x.data[0], 0.1, places=4)\n        self.assertAlmostEqual(x.data[1], 0.5, places=4)\n'"
test/data/test_builtin_datasets.py,7,"b'#!/user/bin/env python3\n# Note that all the tests in this module require dataset (either network access or cached)\nimport os\nimport glob\nimport shutil\nimport torchtext.data as data\nfrom torchtext.datasets import AG_NEWS\nimport torch\nfrom torch.testing import assert_allclose\nfrom ..common.torchtext_test_case import TorchtextTestCase\n\n\ndef conditional_remove(f):\n    for path in glob.glob(f):\n        if os.path.isfile(path):\n            os.remove(path)\n        elif os.path.isdir(path):\n            shutil.rmtree(path)\n\n\nclass TestDataset(TorchtextTestCase):\n    def test_wikitext2_legacy(self):\n        from torchtext.datasets import WikiText2\n        # smoke test to ensure wikitext2 works properly\n\n        # NOTE\n        # test_wikitext2 and test_wikitext2_legacy have some cache incompatibility.\n        # Keeping one\'s cache make the other fail. So we need to clean up the cache dir\n        cachedir = os.path.join(self.project_root, "".data"", ""wikitext-2"")\n        conditional_remove(cachedir)\n\n        ds = WikiText2\n        TEXT = data.Field(lower=True, batch_first=True)\n        train, valid, test = ds.splits(TEXT)\n        TEXT.build_vocab(train)\n        train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n            (train, valid, test), batch_size=3, bptt_len=30)\n\n        train_iter, valid_iter, test_iter = ds.iters(batch_size=4,\n                                                     bptt_len=30)\n\n        conditional_remove(cachedir)\n\n    def test_wikitext2(self):\n        from torchtext.experimental.datasets import WikiText2\n        # smoke test to ensure wikitext2 works properly\n\n        # NOTE\n        # test_wikitext2 and test_wikitext2_legacy have some cache incompatibility.\n        # Keeping one\'s cache make the other fail. So we need to clean up the cache dir\n        cachedir = os.path.join(self.project_root, "".data"", ""wikitext-2"")\n        conditional_remove(cachedir)\n        cachefile = os.path.join(self.project_root, "".data"", ""wikitext-2-v1.zip"")\n        conditional_remove(cachefile)\n\n        train_dataset, test_dataset, valid_dataset = WikiText2()\n        self.assertEqual(len(train_dataset), 2049990)\n        self.assertEqual(len(test_dataset), 241859)\n        self.assertEqual(len(valid_dataset), 214417)\n\n        vocab = train_dataset.get_vocab()\n        tokens_ids = [vocab[token] for token in \'the player characters rest\'.split()]\n        self.assertEqual(tokens_ids, [2, 286, 503, 700])\n\n        conditional_remove(cachedir)\n        conditional_remove(cachefile)\n\n    def test_penntreebank_legacy(self):\n        from torchtext.datasets import PennTreebank\n        # smoke test to ensure penn treebank works properly\n        TEXT = data.Field(lower=True, batch_first=True)\n        ds = PennTreebank\n        train, valid, test = ds.splits(TEXT)\n        TEXT.build_vocab(train)\n        train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n            (train, valid, test), batch_size=3, bptt_len=30)\n\n        train_iter, valid_iter, test_iter = ds.iters(batch_size=4,\n                                                     bptt_len=30)\n\n    def test_penntreebank(self):\n        from torchtext.experimental.datasets import PennTreebank\n        # smoke test to ensure penn treebank works properly\n        train_dataset, test_dataset, valid_dataset = PennTreebank()\n        self.assertEqual(len(train_dataset), 924412)\n        self.assertEqual(len(test_dataset), 82114)\n        self.assertEqual(len(valid_dataset), 73339)\n\n        vocab = train_dataset.get_vocab()\n        tokens_ids = [vocab[token] for token in \'the player characters rest\'.split()]\n        self.assertEqual(tokens_ids, [2, 2550, 3344, 1125])\n\n    def test_text_classification(self):\n        # smoke test to ensure ag_news dataset works properly\n\n        datadir = os.path.join(self.project_root, "".data"")\n        if not os.path.exists(datadir):\n            os.makedirs(datadir)\n        ag_news_train, ag_news_test = AG_NEWS(root=datadir, ngrams=3)\n        self.assertEqual(len(ag_news_train), 120000)\n        self.assertEqual(len(ag_news_test), 7600)\n        assert_allclose(ag_news_train[-1][1][:10],\n                        torch.tensor([3525, 319, 4053, 34, 5407, 3607, 70, 6798, 10599, 4053]).long())\n        assert_allclose(ag_news_test[-1][1][:10],\n                        torch.tensor([2351, 758, 96, 38581, 2351, 220, 5, 396, 3, 14786]).long())\n\n    def test_imdb(self):\n        from torchtext.experimental.datasets import IMDB\n        from torchtext.vocab import Vocab\n        # smoke test to ensure imdb works properly\n        train_dataset, test_dataset = IMDB()\n        self.assertEqual(len(train_dataset), 25000)\n        self.assertEqual(len(test_dataset), 25000)\n        assert_allclose(train_dataset[0][1][:10],\n                        torch.tensor([13, 1568, 13, 246, 35468, 43, 64, 398, 1135, 92]).long())\n        assert_allclose(train_dataset[-1][1][:10],\n                        torch.tensor([2, 71, 4555, 194, 3328, 15144, 42, 227, 148, 8]).long())\n        assert_allclose(test_dataset[0][1][:10],\n                        torch.tensor([13, 125, 1051, 5, 246, 1652, 8, 277, 66, 20]).long())\n        assert_allclose(test_dataset[-1][1][:10],\n                        torch.tensor([13, 1035, 14, 21, 28, 2, 1051, 1275, 1008, 3]).long())\n\n        # Test API with a vocab input object\n        old_vocab = train_dataset.get_vocab()\n        new_vocab = Vocab(counter=old_vocab.freqs, max_size=2500)\n        new_train_data, new_test_data = IMDB(vocab=new_vocab)\n\n    def test_multi30k(self):\n        from torchtext.experimental.datasets.translation import Multi30k\n        # smoke test to ensure multi30k works properly\n        train_dataset, valid_dataset, test_dataset = Multi30k()\n        self.assertEqual(len(train_dataset), 29000)\n        self.assertEqual(len(valid_dataset), 1000)\n        self.assertEqual(len(test_dataset), 1014)\n\n        de_vocab, en_vocab = train_dataset.get_vocab()\n        de_tokens_ids = [\n            de_vocab[token] for token in\n            \'Zwei M\xc3\xa4nner verpacken Donuts in Kunststofffolie\'.split()\n        ]\n        self.assertEqual(de_tokens_ids, [19, 29, 18703, 4448, 5, 6240])\n\n        en_tokens_ids = [\n            en_vocab[token] for token in\n            \'Two young White males are outside near many bushes\'.split()\n        ]\n        self.assertEqual(en_tokens_ids,\n                         [17, 23, 1167, 806, 15, 55, 82, 334, 1337])\n\n        datafile = os.path.join(self.project_root, "".data"", ""train*"")\n        conditional_remove(datafile)\n        datafile = os.path.join(self.project_root, "".data"", ""val*"")\n        conditional_remove(datafile)\n        datafile = os.path.join(self.project_root, "".data"", ""test*"")\n        conditional_remove(datafile)\n        datafile = os.path.join(self.project_root, "".data"",\n                                ""multi30k_task*.tar.gz"")\n        conditional_remove(datafile)\n'"
test/data/test_dataset.py,0,"b'# -*- coding: utf-8 -*-\nimport torchtext.data as data\nimport os\nimport sys\nimport tempfile\nimport unittest\n\nimport pytest\n\nfrom ..common.torchtext_test_case import TorchtextTestCase\n\n\nclass TestDataset(TorchtextTestCase):\n    def test_tabular_simple_data(self):\n        for data_format in [""csv"", ""tsv"", ""json""]:\n            self.write_test_ppid_dataset(data_format=data_format)\n\n            if data_format == ""json"":\n                question_field = data.Field(sequential=True)\n                label_field = data.Field(sequential=False)\n                fields = {""question1"": (""q1"", question_field),\n                          ""question2"": (""q2"", question_field),\n                          ""label"": (""label"", label_field)}\n            else:\n                question_field = data.Field(sequential=True)\n                label_field = data.Field(sequential=False)\n                fields = [(""id"", None), (""q1"", question_field),\n                          (""q2"", question_field), (""label"", label_field)]\n\n            dataset = data.TabularDataset(\n                path=self.test_ppid_dataset_path, format=data_format, fields=fields)\n\n            assert len(dataset) == 3\n\n            expected_examples = [\n                ([""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"", ""instead"", ""of"", ""\xe3\x81\x97?""],\n                 [""When"", ""do"", ""you"", ""use"", ""\\""&\\"""",\n                  ""instead"", ""of"", ""\\""and\\""?""], ""0""),\n                ([""Where"", ""was"", ""Lincoln"", ""born?""],\n                 [""Which"", ""location"", ""was"", ""Abraham"", ""Lincoln"", ""born?""], ""1""),\n                ([""What"", ""is"", ""2+2""], [""2+2=?""], ""1"")]\n\n            # Ensure examples have correct contents / test __getitem__\n            for i in range(len(dataset)):\n                self.assertEqual(dataset[i].q1, expected_examples[i][0])\n                self.assertEqual(dataset[i].q2, expected_examples[i][1])\n                self.assertEqual(dataset[i].label, expected_examples[i][2])\n\n            # Test __getattr__\n            for i, (q1, q2, label) in enumerate(zip(dataset.q1, dataset.q2,\n                                                    dataset.label)):\n                self.assertEqual(q1, expected_examples[i][0])\n                self.assertEqual(q2, expected_examples[i][1])\n                self.assertEqual(label, expected_examples[i][2])\n\n            # Test __iter__\n            for i, example in enumerate(dataset):\n                self.assertEqual(example.q1, expected_examples[i][0])\n                self.assertEqual(example.q2, expected_examples[i][1])\n                self.assertEqual(example.label, expected_examples[i][2])\n\n    def test_json_valid_and_invalid_nested_key(self):\n        self.write_test_nested_key_json_dataset()\n        valid_fields = {\'foods.vegetables.name\': (\'vegs\', data.Field()),\n                        \'foods.fruits\': (\'fruits\', data.Field())}\n        invalid_fields = {\'foods.vegetables.color\': (\'vegs\', data.Field())}\n\n        expected_examples = [\n            {""fruits"": [""Apple"", ""Banana""],\n             ""vegs"": [""Broccoli"", ""Cabbage""]},\n            {""fruits"": [""Cherry"", ""Grape"", ""Lemon""],\n             ""vegs"": [""Cucumber"", ""Lettuce""]},\n            {""fruits"": [""Orange"", ""Pear"", ""Strawberry""],\n             ""vegs"": [""Marrow"", ""Spinach""]}\n        ]\n        dataset = data.TabularDataset(\n            path=self.test_nested_key_json_dataset_path,\n            format=""json"",\n            fields=valid_fields)\n        # check results\n        for example, expect in zip(dataset.examples, expected_examples):\n            self.assertEqual(example.vegs, expect[\'vegs\'])\n            self.assertEqual(example.fruits, expect[\'fruits\'])\n\n        with self.assertRaises(ValueError):\n            data.TabularDataset(\n                path=self.test_nested_key_json_dataset_path,\n                format=""json"",\n                fields=invalid_fields)\n\n    def test_errors(self):\n        # Ensure that trying to retrieve a key not in JSON data errors\n        self.write_test_ppid_dataset(data_format=""json"")\n\n        question_field = data.Field(sequential=True)\n        label_field = data.Field(sequential=False)\n        fields = {""qeustion1"": (""q1"", question_field),\n                  ""question2"": (""q2"", question_field),\n                  ""label"": (""label"", label_field)}\n\n        with self.assertRaises(ValueError):\n            data.TabularDataset(\n                path=self.test_ppid_dataset_path, format=""json"", fields=fields)\n\n    def test_input_with_newlines_in_text(self):\n        # Smoke test for ensuring that TabularDataset works with files with newlines\n        example_with_newlines = [(""\\""hello \\n world\\"""", ""1""),\n                                 (""\\""there is a \\n newline\\"""", ""0""),\n                                 (""\\""there is no newline\\"""", ""1"")]\n        fields = [(""text"", data.Field(lower=True)),\n                  (""label"", data.Field(sequential=False))]\n\n        for delim in ["","", ""\\t""]:\n            with open(self.test_newline_dataset_path, ""wt"") as f:\n                for line in example_with_newlines:\n                    f.write(""{}\\n"".format(delim.join(line)))\n\n            format_ = ""csv"" if delim == "","" else ""tsv""\n            dataset = data.TabularDataset(\n                path=self.test_newline_dataset_path, format=format_, fields=fields)\n            # if the newline is not parsed correctly, this should raise an error\n            for example in dataset:\n                self.assert_(hasattr(example, ""text""))\n                self.assert_(hasattr(example, ""label""))\n\n    def test_csv_file_with_header(self):\n        example_with_header = [(""text"", ""label""),\n                               (""HELLO WORLD"", ""0""),\n                               (""goodbye world"", ""1"")]\n\n        TEXT = data.Field(lower=True, tokenize=lambda x: x.split())\n        fields = {\n            ""label"": (""label"", data.Field(use_vocab=False,\n                                          sequential=False)),\n            ""text"": (""text"", TEXT)\n        }\n\n        for format_, delim in zip([""csv"", ""tsv""], ["","", ""\\t""]):\n            with open(self.test_has_header_dataset_path, ""wt"") as f:\n                for line in example_with_header:\n                    f.write(""{}\\n"".format(delim.join(line)))\n\n            # check that an error is raised here if a non-existent field is specified\n            with self.assertRaises(ValueError):\n                data.TabularDataset(\n                    path=self.test_has_header_dataset_path, format=format_,\n                    fields={""non_existent"": (""label"", data.Field())})\n\n            dataset = data.TabularDataset(\n                path=self.test_has_header_dataset_path, format=format_,\n                skip_header=False, fields=fields)\n\n            TEXT.build_vocab(dataset)\n\n            for i, example in enumerate(dataset):\n                self.assertEqual(example.text,\n                                 example_with_header[i + 1][0].lower().split())\n                self.assertEqual(example.label, example_with_header[i + 1][1])\n\n            # check that the vocabulary is built correctly (#225)\n            expected_freqs = {""hello"": 1, ""world"": 2, ""goodbye"": 1, ""text"": 0}\n            for k, v in expected_freqs.items():\n                self.assertEqual(TEXT.vocab.freqs[k], v)\n\n            data_iter = data.Iterator(dataset, batch_size=1,\n                                      sort_within_batch=False, repeat=False)\n            next(data_iter.__iter__())\n\n    @unittest.skipIf(sys.platform == ""win32"", ""FIXME: tempfile could not be opened twice on Windows"")\n    def test_csv_dataset_quotechar(self):\n        # Based on issue #349\n        example_data = [(""text"", ""label""),\n                        (\'"" hello world\', ""0""),\n                        (\'goodbye "" world\', ""1""),\n                        (\'this is a pen "" \', ""0"")]\n\n        with tempfile.NamedTemporaryFile(dir=self.test_dir) as f:\n            for example in example_data:\n                f.write(""{}\\n"".format("","".join(example)).encode(""latin-1""))\n\n            TEXT = data.Field(lower=True, tokenize=lambda x: x.split())\n            fields = {\n                ""label"": (""label"", data.Field(use_vocab=False,\n                                              sequential=False)),\n                ""text"": (""text"", TEXT)\n            }\n\n            f.seek(0)\n\n            dataset = data.TabularDataset(\n                path=f.name, format=""csv"",\n                skip_header=False, fields=fields,\n                csv_reader_params={""quotechar"": None})\n\n            TEXT.build_vocab(dataset)\n\n            self.assertEqual(len(dataset), len(example_data) - 1)\n\n            for i, example in enumerate(dataset):\n                self.assertEqual(example.text,\n                                 example_data[i + 1][0].lower().split())\n                self.assertEqual(example.label, example_data[i + 1][1])\n\n    def test_dataset_split_arguments(self):\n        num_examples, num_labels = 30, 3\n        self.write_test_splitting_dataset(num_examples=num_examples,\n                                          num_labels=num_labels)\n        text_field = data.Field()\n        label_field = data.LabelField()\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n\n        dataset = data.TabularDataset(\n            path=self.test_dataset_splitting_path, format=""csv"", fields=fields)\n\n        # Test default split ratio (0.7)\n        expected_train_size = 21\n        expected_test_size = 9\n\n        train, test = dataset.split()\n        assert len(train) == expected_train_size\n        assert len(test) == expected_test_size\n\n        # Test array arguments with same ratio\n        split_ratio = [0.7, 0.3]\n        train, test = dataset.split(split_ratio=split_ratio)\n        assert len(train) == expected_train_size\n        assert len(test) == expected_test_size\n\n        # Add validation set\n        split_ratio = [0.6, 0.3, 0.1]\n        expected_train_size = 18\n        expected_valid_size = 3\n        expected_test_size = 9\n\n        train, valid, test = dataset.split(split_ratio=split_ratio)\n        assert len(train) == expected_train_size\n        assert len(valid) == expected_valid_size\n        assert len(test) == expected_test_size\n\n        # Test ratio normalization\n        split_ratio = [6, 3, 1]\n        train, valid, test = dataset.split(split_ratio=split_ratio)\n        assert len(train) == expected_train_size\n        assert len(valid) == expected_valid_size\n        assert len(test) == expected_test_size\n\n        # Test only two splits returned for too small valid split size\n        split_ratio = [0.66, 0.33, 0.01]\n        expected_length = 2\n        splits = dataset.split(split_ratio=split_ratio)\n        assert len(splits) == expected_length\n\n        # Test invalid arguments\n        split_ratio = 1.1\n        with pytest.raises(AssertionError):\n            dataset.split(split_ratio=split_ratio)\n\n        split_ratio = -1.\n        with pytest.raises(AssertionError):\n            dataset.split(split_ratio=split_ratio)\n\n        split_ratio = [0.7]\n        with pytest.raises(AssertionError):\n            dataset.split(split_ratio=split_ratio)\n\n        split_ratio = [1, 2, 3, 4]\n        with pytest.raises(AssertionError):\n            dataset.split(split_ratio=split_ratio)\n\n        split_ratio = ""string""\n        with pytest.raises(ValueError):\n            dataset.split(split_ratio=split_ratio)\n\n    def test_stratified_dataset_split(self):\n        num_examples, num_labels = 30, 3\n        self.write_test_splitting_dataset(num_examples=num_examples,\n                                          num_labels=num_labels)\n        text_field = data.Field()\n        label_field = data.LabelField()\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n\n        dataset = data.TabularDataset(\n            path=self.test_dataset_splitting_path, format=""csv"", fields=fields)\n\n        # Default split ratio\n        expected_train_size = 21\n        expected_test_size = 9\n\n        train, test = dataset.split(stratified=True)\n        assert len(train) == expected_train_size\n        assert len(test) == expected_test_size\n\n        # Test array arguments with same ratio\n        split_ratio = [0.7, 0.3]\n        train, test = dataset.split(split_ratio=split_ratio, stratified=True)\n        assert len(train) == expected_train_size\n        assert len(test) == expected_test_size\n\n        # Test strata_field argument\n        train, test = dataset.split(split_ratio=split_ratio, stratified=True,\n                                    strata_field=\'label\')\n        assert len(train) == expected_train_size\n        assert len(test) == expected_test_size\n\n        # Test invalid field name\n        strata_field = \'dummy\'\n        with pytest.raises(ValueError):\n            dataset.split(split_ratio=split_ratio, stratified=True,\n                          strata_field=strata_field)\n\n        # Test uneven stratify sizes\n        num_examples, num_labels = 28, 3\n        self.write_test_splitting_dataset(num_examples=num_examples,\n                                          num_labels=num_labels)\n        # 10 examples for class 1 and 9 examples for classes 2,3\n        dataset = data.TabularDataset(\n            path=self.test_dataset_splitting_path, format=""csv"", fields=fields)\n\n        expected_train_size = 7 + 6 + 6\n        expected_test_size = 3 + 3 + 3\n        train, test = dataset.split(split_ratio=split_ratio, stratified=True)\n        assert len(train) == expected_train_size\n        assert len(test) == expected_test_size\n\n        split_ratio = [0.7, 0.3]\n        train, test = dataset.split(split_ratio=split_ratio, stratified=True)\n        assert len(train) == expected_train_size\n        assert len(test) == expected_test_size\n\n        # Add validation set\n        split_ratio = [0.6, 0.3, 0.1]\n        expected_train_size = 6 + 5 + 5\n        expected_valid_size = 1 + 1 + 1\n        expected_test_size = 3 + 3 + 3\n        train, valid, test = dataset.split(split_ratio=split_ratio, stratified=True)\n        assert len(train) == expected_train_size\n        assert len(valid) == expected_valid_size\n        assert len(test) == expected_test_size\n\n    def test_filter(self):\n        # Create test examples\n        sentence11 = [[""who"", ""is"", ""there""]]\n        sentence12 = [[""bernardo"", ""is"", ""there""]]\n        label1 = [1]\n        sentence21 = [[""nay"", ""answer"", ""me""]]\n        sentence22 = [[""stand"", ""unfold"", ""yourself""]]\n        label2 = [0]\n        sentence31 = [[""is"", ""Horatio"", ""there""]]\n        sentence32 = [[""a"", ""piece"", ""of"", ""him""]]\n        label3 = [0]\n\n        example1_values = sentence11 + sentence12 + label1\n        example2_values = sentence21 + sentence22 + label2\n        example3_values = sentence31 + sentence32 + label3\n\n        # Test filter remove words from single field only\n        dataset, text_field = filter_init(\n            example1_values,\n            example2_values,\n            example3_values\n        )\n\n        text_field.vocab.stoi.pop(""there"")\n        text_field.vocab.stoi.pop(""bernardo"")\n\n        dataset.filter_examples([""text1""])\n\n        assert dataset[0].text1 == [""who"", ""is""]\n        assert dataset[0].text2 == [""bernardo"", ""is"", ""there""]\n        assert dataset[0].label == 1\n\n        assert dataset[1].text1 == [""nay"", ""answer"", ""me""]\n        assert dataset[1].text2 == [""stand"", ""unfold"", ""yourself""]\n        assert dataset[1].label == 0\n\n        assert dataset[2].text1 == [""is"", ""Horatio""]\n        assert dataset[2].text2 == [""a"", ""piece"", ""of"", ""him""]\n        assert dataset[2].label == 0\n\n        # Test filter remove words from multiple fields\n        dataset, text_field = filter_init(\n            example1_values,\n            example2_values,\n            example3_values\n        )\n\n        text_field.vocab.stoi.pop(""there"")\n        text_field.vocab.stoi.pop(""bernardo"")\n\n        dataset.filter_examples([""text1"", ""text2""])\n\n        assert dataset[0].text1 == [""who"", ""is""]\n        assert dataset[0].text2 == [""is""]\n        assert dataset[0].label == 1\n\n        assert dataset[1].text1 == [""nay"", ""answer"", ""me""]\n        assert dataset[1].text2 == [""stand"", ""unfold"", ""yourself""]\n        assert dataset[1].label == 0\n\n        assert dataset[2].text1 == [""is"", ""Horatio""]\n        assert dataset[2].text2 == [""a"", ""piece"", ""of"", ""him""]\n        assert dataset[2].label == 0\n\n        # Test filter remove all words in example\n        dataset, text_field = filter_init(\n            example1_values,\n            example2_values,\n            example3_values\n        )\n\n        text_field.vocab.stoi.pop(""who"")\n        text_field.vocab.stoi.pop(""is"")\n        text_field.vocab.stoi.pop(""there"")\n\n        dataset.filter_examples([""text1"", ""text2""])\n\n        assert dataset[0].text1 == []\n        assert dataset[0].text2 == [""bernardo""]\n        assert dataset[0].label == 1\n\n        assert dataset[1].text1 == [""nay"", ""answer"", ""me""]\n        assert dataset[1].text2 == [""stand"", ""unfold"", ""yourself""]\n        assert dataset[1].label == 0\n\n        assert dataset[2].text1 == [""Horatio""]\n        assert dataset[2].text2 == [""a"", ""piece"", ""of"", ""him""]\n        assert dataset[2].label == 0\n\n    def test_gz_extraction(self):\n        # tar.gz file contains train.txt and test.txt\n        tgz = (b\'\\x1f\\x8b\\x08\\x00\\x1e\\xcc\\xd5Z\\x00\\x03\\xed\\xd1;\\n\\x800\\x10E\'\n               b\'\\xd1,%+\\x90\\xc9G\\xb3\\x1e\\x0b\\x0b\\x1b\\x03q\\x04\\x97\\xef\\xa7\'\n               b\'\\xb0\\xb0P,R\\x08\\xf74o`\\x9aa\\x9e\\x96~\\x9c\\x1a]\\xd5\\xd4#\\xbb\'\n               b\'\\x94\\xd2\\x99\\xbb{\\x9e\\xb3\\x0b\\xbekC\\x8c\\x12\\x9c\\x11\\xe7b\\x10c\'\n               b\'\\xa5\\xe2M\\x97e\\xd6\\xbeXkJ\\xce\\x8f?x\\xdb\\xff\\x94\\x0e\\xb3V\\xae\'\n               b\'\\xff[\\xffQ\\x8e\\xfe}\\xf2\\xf4\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\'\n               b\'\\x00\\x00\\x00\\x00\\x00\\x00O6\\x1c\\xc6\\xbd\\x89\\x00(\\x00\\x00\')\n\n        # .gz file contains dummy.txt\n        gz = (b\'\\x1f\\x8b\\x08\\x08W\\xce\\xd5Z\\x00\\x03dummy.txt\\x00\\x0bq\\r\\x0e\\x01\'\n              b\'\\x00\\xb8\\x93\\xea\\xee\\x04\\x00\\x00\\x00\')\n\n        # Create both files\n        with open(os.path.join(self.test_dir, \'dummy.tar.gz\'), \'wb\') as fp:\n            fp.write(tgz)\n\n        with open(os.path.join(self.test_dir, \'dummy.txt.gz\'), \'wb\') as fp:\n            fp.write(gz)\n\n        # Set the urls in a dummy class\n        class DummyDataset(data.Dataset):\n            urls = [\'dummy.tar.gz\', \'dummy.txt.gz\']\n            name = \'\'\n            dirname = \'\'\n\n        # Run extraction\n        DummyDataset.download(self.test_dir, check=\'\')\n\n        # Check if files were extracted correctly\n        assert os.path.isfile(os.path.join(self.test_dir, \'dummy.txt\'))\n        assert os.path.isfile(os.path.join(self.test_dir, \'train.txt\'))\n        assert os.path.isfile(os.path.join(self.test_dir, \'test.txt\'))\n\n\ndef filter_init(ex_val1, ex_val2, ex_val3):\n    text_field = data.Field(sequential=True)\n    label_field = data.Field(sequential=False)\n    fields = [(""text1"", text_field), (""text2"", text_field),\n              (""label"", label_field)]\n\n    example1 = data.Example.fromlist(ex_val1, fields)\n    example2 = data.Example.fromlist(ex_val2, fields)\n    example3 = data.Example.fromlist(ex_val3, fields)\n    examples = [example1, example2, example3]\n\n    dataset = data.Dataset(examples, fields)\n    text_field.build_vocab(dataset)\n\n    return dataset, text_field\n'"
test/data/test_field.py,14,"b'# -*- coding: utf-8 -*-\nfrom collections import Counter\nimport os\n\nfrom numpy.testing import assert_allclose\nimport torch\nimport torchtext.data as data\nimport pytest\n\nfrom ..common.torchtext_test_case import TorchtextTestCase, verify_numericalized_example\n\n\nclass TestField(TorchtextTestCase):\n    def test_process(self):\n        raw_field = data.RawField()\n        field = data.Field(sequential=True, use_vocab=False, batch_first=True)\n\n        # Test tensor-like batch data which is accepted by both RawField and Field\n        batch = [[1, 2, 3], [2, 3, 4]]\n        batch_tensor = torch.LongTensor(batch)\n\n        raw_field_processed = raw_field.process(batch)\n        field_processed = field.process(batch)\n\n        assert raw_field_processed == batch\n        assert field_processed.data.equal(batch_tensor)\n\n        # Test non-tensor data which is only accepted by RawField\n        any_obj = [object() for _ in range(5)]\n\n        raw_field_processed = raw_field.process(any_obj)\n        assert any_obj == raw_field_processed\n\n        with pytest.raises(TypeError):\n            field.process(any_obj)\n\n    def test_preprocess(self):\n        # Default case.\n        field = data.Field()\n        assert field.preprocess(""Test string."") == [""Test"", ""string.""]\n\n        # Test that lowercase is properly applied.\n        field_lower = data.Field(lower=True)\n        assert field_lower.preprocess(""Test string."") == [""test"", ""string.""]\n\n        # Test that custom preprocessing pipelines are properly applied.\n        preprocess_pipeline = data.Pipeline(lambda x: x + ""!"")\n        field_preprocessing = data.Field(preprocessing=preprocess_pipeline,\n                                         lower=True)\n        assert field_preprocessing.preprocess(""Test string."") == [""test!"", ""string.!""]\n\n        # Test that non-sequential data is properly handled.\n        field_not_sequential = data.Field(sequential=False, lower=True,\n                                          preprocessing=preprocess_pipeline)\n        assert field_not_sequential.preprocess(""Test string."") == ""test string.!""\n\n        # Non-regression test that we do not try to decode unicode strings to unicode\n        field_not_sequential = data.Field(sequential=False, lower=True,\n                                          preprocessing=preprocess_pipeline)\n        assert field_not_sequential.preprocess(""\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT"") == ""\xe1\x91\x8c\xe1\x91\x8ei\xe1\x91\x95o\xe1\x97\xaa\xe1\x95\xae_t\xe1\x95\xae\xe1\x99\xadt!""\n\n    def test_pad(self):\n        # Default case.\n        field = data.Field()\n        minibatch = [[""a"", ""sentence"", ""of"", ""data"", "".""],\n                     [""yet"", ""another""],\n                     [""one"", ""last"", ""sent""]]\n        expected_padded_minibatch = [[""a"", ""sentence"", ""of"", ""data"", "".""],\n                                     [""yet"", ""another"", ""<pad>"", ""<pad>"", ""<pad>""],\n                                     [""one"", ""last"", ""sent"", ""<pad>"", ""<pad>""]]\n        expected_lengths = [5, 2, 3]\n        assert field.pad(minibatch) == expected_padded_minibatch\n        field = data.Field(include_lengths=True)\n        assert field.pad(minibatch) == (expected_padded_minibatch, expected_lengths)\n\n        # Test fix_length properly truncates and pads.\n        field = data.Field(fix_length=3)\n        minibatch = [[""a"", ""sentence"", ""of"", ""data"", "".""],\n                     [""yet"", ""another""],\n                     [""one"", ""last"", ""sent""]]\n        expected_padded_minibatch = [[""a"", ""sentence"", ""of""],\n                                     [""yet"", ""another"", ""<pad>""],\n                                     [""one"", ""last"", ""sent""]]\n        expected_lengths = [3, 2, 3]\n        assert field.pad(minibatch) == expected_padded_minibatch\n        field = data.Field(fix_length=3, include_lengths=True)\n        assert field.pad(minibatch) == (expected_padded_minibatch, expected_lengths)\n        field = data.Field(fix_length=3, truncate_first=True)\n        expected_padded_minibatch = [[""of"", ""data"", "".""],\n                                     [""yet"", ""another"", ""<pad>""],\n                                     [""one"", ""last"", ""sent""]]\n        assert field.pad(minibatch) == expected_padded_minibatch\n\n        # Test init_token is properly handled.\n        field = data.Field(fix_length=4, init_token=""<bos>"")\n        minibatch = [[""a"", ""sentence"", ""of"", ""data"", "".""],\n                     [""yet"", ""another""],\n                     [""one"", ""last"", ""sent""]]\n        expected_padded_minibatch = [[""<bos>"", ""a"", ""sentence"", ""of""],\n                                     [""<bos>"", ""yet"", ""another"", ""<pad>""],\n                                     [""<bos>"", ""one"", ""last"", ""sent""]]\n        expected_lengths = [4, 3, 4]\n        assert field.pad(minibatch) == expected_padded_minibatch\n        field = data.Field(fix_length=4, init_token=""<bos>"", include_lengths=True)\n        assert field.pad(minibatch) == (expected_padded_minibatch, expected_lengths)\n\n        # Test init_token and eos_token are properly handled.\n        field = data.Field(init_token=""<bos>"", eos_token=""<eos>"")\n        minibatch = [[""a"", ""sentence"", ""of"", ""data"", "".""],\n                     [""yet"", ""another""],\n                     [""one"", ""last"", ""sent""]]\n        expected_padded_minibatch = [\n            [""<bos>"", ""a"", ""sentence"", ""of"", ""data"", ""."", ""<eos>""],\n            [""<bos>"", ""yet"", ""another"", ""<eos>"", ""<pad>"", ""<pad>"", ""<pad>""],\n            [""<bos>"", ""one"", ""last"", ""sent"", ""<eos>"", ""<pad>"", ""<pad>""]]\n        expected_lengths = [7, 4, 5]\n        assert field.pad(minibatch) == expected_padded_minibatch\n        field = data.Field(init_token=""<bos>"", eos_token=""<eos>"", include_lengths=True)\n        assert field.pad(minibatch) == (expected_padded_minibatch, expected_lengths)\n\n        # Test that non-sequential data is properly handled.\n        field = data.Field(init_token=""<bos>"", eos_token=""<eos>"", sequential=False)\n        minibatch = [[""contradiction""],\n                     [""neutral""],\n                     [""entailment""]]\n        assert field.pad(minibatch) == minibatch\n        field = data.Field(init_token=""<bos>"", eos_token=""<eos>"",\n                           sequential=False, include_lengths=True)\n        assert field.pad(minibatch) == minibatch\n\n    def test_build_vocab(self):\n        # Set up fields\n        question_field = data.Field(sequential=True)\n        label_field = data.Field(sequential=False)\n\n        # Write TSV dataset and construct a Dataset\n        self.write_test_ppid_dataset(data_format=""tsv"")\n        tsv_fields = [(""id"", None), (""q1"", question_field),\n                      (""q2"", question_field), (""label"", label_field)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n\n        # Write JSON dataset and construct a Dataset\n        self.write_test_ppid_dataset(data_format=""json"")\n        json_fields = {""question1"": (""q1"", question_field),\n                       ""question2"": (""q2"", question_field),\n                       ""label"": (""label"", label_field)}\n        json_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""json"",\n            fields=json_fields)\n\n        # Test build_vocab default\n        question_field.build_vocab(tsv_dataset, json_dataset, specials=[\'<space>\'])\n        assert question_field.vocab.freqs == Counter(\n            {\'When\': 4, \'do\': 4, \'you\': 4, \'use\': 4, \'instead\': 4,\n             \'of\': 4, \'was\': 4, \'Lincoln\': 4, \'born?\': 4, \'\xe3\x82\xb7\': 2,\n             \'\xe3\x81\x97?\': 2, \'Where\': 2, \'What\': 2, \'is\': 2, \'2+2\': 2,\n             \'""&""\': 2, \'""and""?\': 2, \'Which\': 2, \'location\': 2,\n             \'Abraham\': 2, \'2+2=?\': 2})\n        expected_stoi = {\'<unk>\': 0, \'<pad>\': 1, \'<space>\': 2,\n                         \'Lincoln\': 3, \'When\': 4,\n                         \'born?\': 5, \'do\': 6, \'instead\': 7, \'of\': 8,\n                         \'use\': 9, \'was\': 10, \'you\': 11, \'""&""\': 12,\n                         \'""and""?\': 13, \'2+2\': 14, \'2+2=?\': 15, \'Abraham\': 16,\n                         \'What\': 17, \'Where\': 18, \'Which\': 19, \'is\': 20,\n                         \'location\': 21, \'\xe3\x81\x97?\': 22, \'\xe3\x82\xb7\': 23}\n        assert dict(question_field.vocab.stoi) == expected_stoi\n        # Turn the stoi dictionary into an itos list\n        expected_itos = [x[0] for x in sorted(expected_stoi.items(),\n                                              key=lambda tup: tup[1])]\n        assert question_field.vocab.itos == expected_itos\n\n        label_field.build_vocab(tsv_dataset, json_dataset)\n        assert label_field.vocab.freqs == Counter({\'1\': 4, \'0\': 2})\n        expected_stoi = {\'1\': 1, \'0\': 2, \'<unk>\': 0}\n        assert dict(label_field.vocab.stoi) == expected_stoi\n        # Turn the stoi dictionary into an itos list\n        expected_itos = [x[0] for x in sorted(expected_stoi.items(),\n                                              key=lambda tup: tup[1])]\n        assert label_field.vocab.itos == expected_itos\n\n        # Test build_vocab default\n        question_field.build_vocab(tsv_dataset, json_dataset)\n        assert question_field.vocab.freqs == Counter(\n            {\'When\': 4, \'do\': 4, \'you\': 4, \'use\': 4, \'instead\': 4,\n             \'of\': 4, \'was\': 4, \'Lincoln\': 4, \'born?\': 4, \'\xe3\x82\xb7\': 2,\n             \'\xe3\x81\x97?\': 2, \'Where\': 2, \'What\': 2, \'is\': 2, \'2+2\': 2,\n             \'""&""\': 2, \'""and""?\': 2, \'Which\': 2, \'location\': 2,\n             \'Abraham\': 2, \'2+2=?\': 2})\n        expected_stoi = {\'<unk>\': 0, \'<pad>\': 1, \'Lincoln\': 2, \'When\': 3,\n                         \'born?\': 4, \'do\': 5, \'instead\': 6, \'of\': 7,\n                         \'use\': 8, \'was\': 9, \'you\': 10, \'""&""\': 11,\n                         \'""and""?\': 12, \'2+2\': 13, \'2+2=?\': 14, \'Abraham\': 15,\n                         \'What\': 16, \'Where\': 17, \'Which\': 18, \'is\': 19,\n                         \'location\': 20, \'\xe3\x81\x97?\': 21, \'\xe3\x82\xb7\': 22}\n        assert dict(question_field.vocab.stoi) == expected_stoi\n        # Turn the stoi dictionary into an itos list\n        expected_itos = [x[0] for x in sorted(expected_stoi.items(),\n                                              key=lambda tup: tup[1])]\n        assert question_field.vocab.itos == expected_itos\n\n        label_field.build_vocab(tsv_dataset, json_dataset)\n        assert label_field.vocab.freqs == Counter({\'1\': 4, \'0\': 2})\n        expected_stoi = {\'1\': 1, \'0\': 2, \'<unk>\': 0}\n        assert dict(label_field.vocab.stoi) == expected_stoi\n        # Turn the stoi dictionary into an itos list\n        expected_itos = [x[0] for x in sorted(expected_stoi.items(),\n                                              key=lambda tup: tup[1])]\n        assert label_field.vocab.itos == expected_itos\n\n        # Test build_vocab with extra kwargs passed to Vocab\n        question_field.build_vocab(tsv_dataset, json_dataset, max_size=8,\n                                   min_freq=3)\n        assert question_field.vocab.freqs == Counter(\n            {\'When\': 4, \'do\': 4, \'you\': 4, \'use\': 4, \'instead\': 4,\n             \'of\': 4, \'was\': 4, \'Lincoln\': 4, \'born?\': 4, \'\xe3\x82\xb7\': 2,\n             \'\xe3\x81\x97?\': 2, \'Where\': 2, \'What\': 2, \'is\': 2, \'2+2\': 2,\n             \'""&""\': 2, \'""and""?\': 2, \'Which\': 2, \'location\': 2,\n             \'Abraham\': 2, \'2+2=?\': 2})\n        expected_stoi = {\'<unk>\': 0, \'<pad>\': 1, \'Lincoln\': 2, \'When\': 3,\n                         \'born?\': 4, \'do\': 5, \'instead\': 6, \'of\': 7,\n                         \'use\': 8, \'was\': 9}\n        assert dict(question_field.vocab.stoi) == expected_stoi\n        # Turn the stoi dictionary into an itos list\n        expected_itos = [x[0] for x in sorted(expected_stoi.items(),\n                                              key=lambda tup: tup[1])]\n        assert question_field.vocab.itos == expected_itos\n\n    def test_numericalize_basic(self):\n        self.write_test_ppid_dataset(data_format=""tsv"")\n        question_field = data.Field(sequential=True)\n        tsv_fields = [(""id"", None), (""q1"", question_field),\n                      (""q2"", question_field), (""label"", None)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n        question_field.build_vocab(tsv_dataset)\n\n        test_example_data = [[""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"",\n                              ""instead"", ""of"", ""\xe3\x81\x97?""],\n                             [""What"", ""is"", ""2+2"", ""<pad>"", ""<pad>"",\n                              ""<pad>"", ""<pad>"", ""<pad>""],\n                             [""Here"", ""is"", ""a"", ""sentence"", ""with"",\n                              ""some"", ""oovs"", ""<pad>""]]\n\n        # Test default\n        default_numericalized = question_field.numericalize(test_example_data)\n        verify_numericalized_example(question_field, test_example_data,\n                                     default_numericalized)\n\n    def test_numericalize_include_lengths(self):\n        self.write_test_ppid_dataset(data_format=""tsv"")\n        question_field = data.Field(sequential=True, include_lengths=True)\n        tsv_fields = [(""id"", None), (""q1"", question_field),\n                      (""q2"", question_field), (""label"", None)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n        question_field.build_vocab(tsv_dataset)\n\n        test_example_data = [[""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"",\n                              ""instead"", ""of"", ""\xe3\x81\x97?""],\n                             [""What"", ""is"", ""2+2"", ""<pad>"", ""<pad>"",\n                              ""<pad>"", ""<pad>"", ""<pad>""],\n                             [""Here"", ""is"", ""a"", ""sentence"", ""with"",\n                              ""some"", ""oovs"", ""<pad>""]]\n        test_example_lengths = [8, 3, 7]\n\n        # Test with include_lengths\n        include_lengths_numericalized = question_field.numericalize(\n            (test_example_data, test_example_lengths))\n        verify_numericalized_example(question_field,\n                                     test_example_data,\n                                     include_lengths_numericalized,\n                                     test_example_lengths)\n\n    def test_numericalize_batch_first(self):\n        self.write_test_ppid_dataset(data_format=""tsv"")\n        question_field = data.Field(sequential=True, batch_first=True)\n        tsv_fields = [(""id"", None), (""q1"", question_field),\n                      (""q2"", question_field), (""label"", None)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n        question_field.build_vocab(tsv_dataset)\n\n        test_example_data = [[""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"",\n                              ""instead"", ""of"", ""\xe3\x81\x97?""],\n                             [""What"", ""is"", ""2+2"", ""<pad>"", ""<pad>"",\n                              ""<pad>"", ""<pad>"", ""<pad>""],\n                             [""Here"", ""is"", ""a"", ""sentence"", ""with"",\n                              ""some"", ""oovs"", ""<pad>""]]\n\n        # Test with batch_first\n        include_lengths_numericalized = question_field.numericalize(\n            test_example_data)\n        verify_numericalized_example(question_field,\n                                     test_example_data,\n                                     include_lengths_numericalized,\n                                     batch_first=True)\n\n    def test_numericalize_postprocessing(self):\n        self.write_test_ppid_dataset(data_format=""tsv"")\n\n        def reverse_postprocess(arr, vocab):\n            return [list(reversed(sentence)) for sentence in arr]\n\n        question_field = data.Field(sequential=True,\n                                    postprocessing=reverse_postprocess)\n        tsv_fields = [(""id"", None), (""q1"", question_field),\n                      (""q2"", question_field), (""label"", None)]\n\n        tsv_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n        question_field.build_vocab(tsv_dataset)\n\n        test_example_data = [[""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"",\n                              ""instead"", ""of"", ""\xe3\x81\x97?""],\n                             [""What"", ""is"", ""2+2"", ""<pad>"", ""<pad>"",\n                              ""<pad>"", ""<pad>"", ""<pad>""],\n                             [""Here"", ""is"", ""a"", ""sentence"", ""with"",\n                              ""some"", ""oovs"", ""<pad>""]]\n        reversed_test_example_data = [list(reversed(sentence)) for sentence in\n                                      test_example_data]\n\n        postprocessed_numericalized = question_field.numericalize(\n            (test_example_data))\n        verify_numericalized_example(question_field,\n                                     reversed_test_example_data,\n                                     postprocessed_numericalized)\n\n    def test_numericalize_stop_words(self):\n        # Based on request from #354\n        self.write_test_ppid_dataset(data_format=""tsv"")\n        question_field = data.Field(sequential=True, batch_first=True,\n                                    stop_words=set([""do"", ""you""]))\n        tsv_fields = [(""id"", None), (""q1"", question_field),\n                      (""q2"", question_field), (""label"", None)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n        question_field.build_vocab(tsv_dataset)\n\n        test_example_data = question_field.pad(\n            [question_field.preprocess(x) for x in\n             [[""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"",\n               ""instead"", ""of"", ""\xe3\x81\x97?""],\n              [""What"", ""is"", ""2+2"", ""<pad>"", ""<pad>"",\n               ""<pad>"", ""<pad>"", ""<pad>""],\n              [""Here"", ""is"", ""a"", ""sentence"", ""with"",\n               ""some"", ""oovs"", ""<pad>""]]]\n        )\n\n        # Test with batch_first\n        stopwords_removed_numericalized = question_field.numericalize(test_example_data)\n        verify_numericalized_example(question_field,\n                                     test_example_data,\n                                     stopwords_removed_numericalized,\n                                     batch_first=True)\n\n    def test_numerical_features_no_vocab(self):\n        self.write_test_numerical_features_dataset()\n        # Test basic usage\n        int_field = data.Field(sequential=False, use_vocab=False)\n        float_field = data.Field(sequential=False, use_vocab=False,\n                                 dtype=torch.float)\n        tsv_fields = [(""int"", int_field), (""float"", float_field), (""string"", None)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_numerical_features_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n        int_field.build_vocab(tsv_dataset)\n        float_field.build_vocab(tsv_dataset)\n        test_int_data = [""1"", ""0"", ""1"", ""3"", ""19""]\n        test_float_data = [""1.1"", ""0.1"", ""3.91"", ""0.2"", ""10.2""]\n\n        numericalized_int = int_field.numericalize(test_int_data)\n        assert_allclose(numericalized_int.data.numpy(), [1, 0, 1, 3, 19])\n        numericalized_float = float_field.numericalize(test_float_data)\n        assert_allclose(numericalized_float.data.numpy(), [1.1, 0.1, 3.91, 0.2, 10.2])\n\n        # Test with postprocessing applied\n        int_field = data.Field(sequential=False, use_vocab=False,\n                               postprocessing=lambda arr, _: [x + 1 for x in arr])\n        float_field = data.Field(sequential=False, use_vocab=False,\n                                 dtype=torch.float,\n                                 postprocessing=lambda arr, _: [x * 0.5 for x in arr])\n        tsv_fields = [(""int"", int_field), (""float"", float_field), (""string"", None)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_numerical_features_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n        int_field.build_vocab(tsv_dataset)\n        float_field.build_vocab(tsv_dataset)\n        test_int_data = [""1"", ""0"", ""1"", ""3"", ""19""]\n        test_float_data = [""1.1"", ""0.1"", ""3.91"", ""0.2"", ""10.2""]\n\n        numericalized_int = int_field.numericalize(test_int_data)\n        assert_allclose(numericalized_int.data.numpy(), [2, 1, 2, 4, 20])\n        numericalized_float = float_field.numericalize(test_float_data)\n        assert_allclose(numericalized_float.data.numpy(), [0.55, 0.05, 1.955, 0.1, 5.1])\n\n    def test_errors(self):\n        # Test that passing a non-tuple (of data and length) to numericalize\n        # with Field.include_lengths = True raises an error.\n        with self.assertRaises(ValueError):\n            self.write_test_ppid_dataset(data_format=""tsv"")\n            question_field = data.Field(sequential=True, include_lengths=True)\n            tsv_fields = [(""id"", None), (""q1"", question_field),\n                          (""q2"", question_field), (""label"", None)]\n            tsv_dataset = data.TabularDataset(\n                path=self.test_ppid_dataset_path, format=""tsv"",\n                fields=tsv_fields)\n            question_field.build_vocab(tsv_dataset)\n            test_example_data = [[""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"",\n                                  ""instead"", ""of"", ""\xe3\x81\x97?""],\n                                 [""What"", ""is"", ""2+2"", ""<pad>"", ""<pad>"",\n                                  ""<pad>"", ""<pad>"", ""<pad>""],\n                                 [""Here"", ""is"", ""a"", ""sentence"", ""with"",\n                                  ""some"", ""oovs"", ""<pad>""]]\n            question_field.numericalize(\n                test_example_data)\n\n    def test_serialization_pre_build(self):\n        self.write_test_ppid_dataset(data_format=""tsv"")\n        question_field = data.Field(sequential=True)\n\n        question_pickle_filename = ""question.pl""\n        question_pickle_path = os.path.join(self.test_dir, question_pickle_filename)\n        torch.save(question_field, question_pickle_path)\n\n        loaded_question_field = torch.load(question_pickle_path)\n\n        assert loaded_question_field == question_field\n\n    def test_serialization_built_vocab(self):\n        self.write_test_ppid_dataset(data_format=""tsv"")\n        question_field = data.Field(sequential=True)\n        tsv_fields = [(""id"", None), (""q1"", question_field),\n                      (""q2"", question_field), (""label"", None)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n\n        question_field.build_vocab(tsv_dataset)\n\n        question_pickle_filename = ""question.pl""\n        question_pickle_path = os.path.join(self.test_dir, question_pickle_filename)\n        torch.save(question_field, question_pickle_path)\n\n        loaded_question_field = torch.load(question_pickle_path)\n\n        assert loaded_question_field == question_field\n\n        test_example_data = [[""When"", ""do"", ""you"", ""use"", ""\xe3\x82\xb7"",\n                              ""instead"", ""of"", ""\xe3\x81\x97?""],\n                             [""What"", ""is"", ""2+2"", ""<pad>"", ""<pad>"",\n                              ""<pad>"", ""<pad>"", ""<pad>""],\n                             [""Here"", ""is"", ""a"", ""sentence"", ""with"",\n                              ""some"", ""oovs"", ""<pad>""]]\n\n        # Test results of numericalization\n        original_numericalization = question_field.numericalize(test_example_data)\n        pickled_numericalization = loaded_question_field.numericalize(test_example_data)\n\n        assert torch.all(torch.eq(original_numericalization, pickled_numericalization))\n\n\nclass TestNestedField(TorchtextTestCase):\n    def test_init_minimal(self):\n        nesting_field = data.Field()\n        field = data.NestedField(nesting_field)\n\n        assert isinstance(field, data.Field)\n        assert field.nesting_field is nesting_field\n        assert field.sequential\n        assert field.use_vocab\n        assert field.init_token is None\n        assert field.eos_token is None\n        assert field.unk_token == nesting_field.unk_token\n        assert field.fix_length is None\n        assert field.dtype is torch.long\n        assert field.preprocessing is None\n        assert field.postprocessing is None\n        assert field.lower == nesting_field.lower\n        assert field.tokenize(""a b c"") == ""a b c"".split()\n        assert not field.include_lengths\n        assert field.batch_first\n        assert field.pad_token == nesting_field.pad_token\n        assert not field.pad_first\n\n    def test_init_when_nesting_field_is_not_sequential(self):\n        nesting_field = data.Field(sequential=False)\n        field = data.NestedField(nesting_field)\n\n        assert field.pad_token == ""<pad>""\n\n    def test_init_when_nesting_field_has_include_lengths_equal_true(self):\n        nesting_field = data.Field(include_lengths=True)\n\n        with pytest.raises(ValueError) as excinfo:\n            data.NestedField(nesting_field)\n        assert ""nesting field cannot have include_lengths=True"" in str(excinfo.value)\n\n    def test_init_with_nested_field_as_nesting_field(self):\n        nesting_field = data.NestedField(data.Field())\n\n        with pytest.raises(ValueError) as excinfo:\n            data.NestedField(nesting_field)\n        assert ""nesting field must not be another NestedField"" in str(excinfo.value)\n\n    def test_init_full(self):\n        nesting_field = data.Field()\n        field = data.NestedField(\n            nesting_field,\n            use_vocab=False,\n            init_token=""<s>"",\n            eos_token=""</s>"",\n            fix_length=10,\n            dtype=torch.float,\n            preprocessing=lambda xs: list(reversed(xs)),\n            postprocessing=lambda xs: [x.upper() for x in xs],\n            tokenize=list,\n            pad_first=True,\n        )\n\n        assert not field.use_vocab\n        assert field.init_token == ""<s>""\n        assert field.eos_token == ""</s>""\n        assert field.fix_length == 10\n        assert field.dtype is torch.float\n        assert field.preprocessing(""a b c"".split()) == ""c b a"".split()\n        assert field.postprocessing(""a b c"".split()) == ""A B C"".split()\n        assert field.tokenize(""abc"") == [""a"", ""b"", ""c""]\n        assert field.pad_first\n\n    def test_preprocess(self):\n        nesting_field = data.Field(\n            tokenize=list, preprocessing=lambda xs: [x.upper() for x in xs])\n        field = data.NestedField(nesting_field, preprocessing=lambda xs: reversed(xs))\n        preprocessed = field.preprocess(""john loves mary"")\n\n        assert preprocessed == [list(""MARY""), list(""LOVES""), list(""JOHN"")]\n\n    def test_build_vocab_from_dataset(self):\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"", eos_token=""</s>"")\n        ex1 = data.Example.fromlist([""aaa bbb c""], [(""chars"", CHARS)])\n        ex2 = data.Example.fromlist([""bbb aaa""], [(""chars"", CHARS)])\n        dataset = data.Dataset([ex1, ex2], [(""chars"", CHARS)])\n\n        CHARS.build_vocab(dataset, min_freq=2)\n\n        expected = ""a b <w> </w> <s> </s> <cunk> <cpad>"".split()\n        assert len(CHARS.vocab) == len(expected)\n        for c in expected:\n            assert c in CHARS.vocab.stoi\n\n        expected_freqs = Counter({""a"": 6, ""b"": 6, ""c"": 1})\n        assert CHARS.vocab.freqs == CHARS.nesting_field.vocab.freqs == expected_freqs\n\n    def test_build_vocab_from_iterable(self):\n        nesting_field = data.Field(unk_token=""<cunk>"", pad_token=""<cpad>"")\n        CHARS = data.NestedField(nesting_field)\n        CHARS.build_vocab(\n            [[list(""aaa""), list(""bbb""), [""c""]], [list(""bbb""), list(""aaa"")]],\n            [[list(""ccc""), list(""bbb"")], [list(""bbb"")]],\n        )\n\n        expected = ""a b c <cunk> <cpad>"".split()\n        assert len(CHARS.vocab) == len(expected)\n        for c in expected:\n            assert c in CHARS.vocab.stoi\n\n        expected_freqs = Counter({""a"": 6, ""b"": 12, ""c"": 4})\n        assert CHARS.vocab.freqs == CHARS.nesting_field.vocab.freqs == expected_freqs\n\n    def test_pad(self):\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"", eos_token=""</s>"")\n        minibatch = [\n            [list(""john""), list(""loves""), list(""mary"")],\n            [list(""mary""), list(""cries"")],\n        ]\n        expected = [\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""john"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""loves"") + [""</w>""],\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n            ],\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""cries"") + [""</w>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<cpad>""] * 7,\n            ]\n        ]\n\n        assert CHARS.pad(minibatch) == expected\n\n        # test include_length\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"",\n                                 eos_token=""</s>"", include_lengths=True)\n        arr, seq_len, words_len = CHARS.pad(minibatch)\n        assert arr == expected\n        assert seq_len == [5, 4]\n        assert words_len == [[3, 6, 7, 6, 3], [3, 6, 7, 3, 0]]\n\n    def test_pad_when_nesting_field_is_not_sequential(self):\n        nesting_field = data.Field(sequential=False, unk_token=""<cunk>"",\n                                   pad_token=""<cpad>"", init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"", eos_token=""</s>"")\n        minibatch = [\n            [""john"", ""loves"", ""mary""],\n            [""mary"", ""cries""]\n        ]\n        expected = [\n            [""<s>"", ""john"", ""loves"", ""mary"", ""</s>""],\n            [""<s>"", ""mary"", ""cries"", ""</s>"", ""<pad>""],\n        ]\n\n        assert CHARS.pad(minibatch) == expected\n\n    def test_pad_when_nesting_field_has_fix_length(self):\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"", fix_length=5)\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"", eos_token=""</s>"")\n        minibatch = [\n            [""john"", ""loves"", ""mary""],\n            [""mary"", ""cries""]\n        ]\n        expected = [\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 2,\n                [""<w>""] + list(""joh"") + [""</w>""],\n                [""<w>""] + list(""lov"") + [""</w>""],\n                [""<w>""] + list(""mar"") + [""</w>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 2,\n            ],\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 2,\n                [""<w>""] + list(""mar"") + [""</w>""],\n                [""<w>""] + list(""cri"") + [""</w>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 2,\n                [""<cpad>""] * 5,\n            ]\n        ]\n\n        assert CHARS.pad(minibatch) == expected\n\n        # test include length\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"", fix_length=5)\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"",\n                                 eos_token=""</s>"", include_lengths=True)\n        arr, seq_len, words_len = CHARS.pad(minibatch)\n        assert arr == expected\n        assert seq_len == [5, 4]\n        assert words_len == [[3, 5, 5, 5, 3], [3, 5, 5, 3, 0]]\n\n    def test_pad_when_fix_length_is_not_none(self):\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(\n            nesting_field, init_token=""<s>"", eos_token=""</s>"", fix_length=3)\n        minibatch = [\n            [""john"", ""loves"", ""mary""],\n            [""mary"", ""cries""]\n        ]\n        expected = [\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""john"") + [""</w>"", ""<cpad>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n            ],\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n            ]\n        ]\n\n        assert CHARS.pad(minibatch) == expected\n\n        # test include length\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"",\n                                 eos_token=""</s>"", include_lengths=True, fix_length=3)\n        arr, seq_len, words_len = CHARS.pad(minibatch)\n        assert arr == expected\n        assert seq_len == [3, 3]\n        assert words_len == [[3, 6, 3], [3, 6, 3]]\n\n    def test_pad_when_no_init_and_eos_tokens(self):\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(nesting_field)\n        minibatch = [\n            [""john"", ""loves"", ""mary""],\n            [""mary"", ""cries""]\n        ]\n        expected = [\n            [\n                [""<w>""] + list(""john"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""loves"") + [""</w>""],\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n            ],\n            [\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""cries"") + [""</w>""],\n                [""<cpad>""] * 7,\n            ]\n        ]\n\n        assert CHARS.pad(minibatch) == expected\n\n    def test_pad_when_pad_first_is_true(self):\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"", eos_token=""</s>"",\n                                 pad_first=True)\n        minibatch = [\n            [list(""john""), list(""loves""), list(""mary"")],\n            [list(""mary""), list(""cries"")],\n        ]\n        expected = [\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""john"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""loves"") + [""</w>""],\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n            ],\n            [\n                [""<cpad>""] * 7,\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""cries"") + [""</w>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n            ]\n        ]\n\n        assert CHARS.pad(minibatch) == expected\n\n        # test include_length\n        nesting_field = data.Field(tokenize=list, unk_token=""<cunk>"", pad_token=""<cpad>"",\n                                   init_token=""<w>"", eos_token=""</w>"")\n        CHARS = data.NestedField(nesting_field, init_token=""<s>"",\n                                 eos_token=""</s>"", include_lengths=True,\n                                 pad_first=True)\n        arr, seq_len, words_len = CHARS.pad(minibatch)\n        assert arr == expected\n        assert seq_len == [5, 4]\n        assert words_len == [[3, 6, 7, 6, 3], [0, 3, 6, 7, 3]]\n\n    def test_numericalize(self):\n        nesting_field = data.Field(batch_first=True)\n        field = data.NestedField(nesting_field)\n        ex1 = data.Example.fromlist([""john loves mary""], [(""words"", field)])\n        ex2 = data.Example.fromlist([""mary cries""], [(""words"", field)])\n        dataset = data.Dataset([ex1, ex2], [(""words"", field)])\n        field.build_vocab(dataset)\n        examples_data = [\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""john"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""loves"") + [""</w>""],\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n            ],\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""cries"") + [""</w>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<cpad>""] * 7,\n            ]\n        ]\n        numericalized = field.numericalize(examples_data)\n\n        assert numericalized.dim() == 3\n        assert numericalized.size(0) == len(examples_data)\n        for example, numericalized_example in zip(examples_data, numericalized):\n            verify_numericalized_example(\n                field, example, numericalized_example, batch_first=True)\n\n        # test include_lengths\n        nesting_field = data.Field(batch_first=True)\n        field = data.NestedField(nesting_field, include_lengths=True)\n        ex1 = data.Example.fromlist([""john loves mary""], [(""words"", field)])\n        ex2 = data.Example.fromlist([""mary cries""], [(""words"", field)])\n        dataset = data.Dataset([ex1, ex2], [(""words"", field)])\n        field.build_vocab(dataset)\n        examples_data = [\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""john"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""loves"") + [""</w>""],\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n            ],\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""cries"") + [""</w>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<cpad>""] * 7,\n            ]\n        ]\n\n        numericalized, seq_len, word_len = field.numericalize(\n            (examples_data, [5, 4], [[3, 6, 7, 6, 3], [3, 6, 7, 3, 0]]))\n\n        assert numericalized.dim() == 3\n        assert len(seq_len) == 2\n        assert len(word_len) == 2\n\n        assert numericalized.size(0) == len(examples_data)\n        for example, numericalized_example in zip(examples_data, numericalized):\n            verify_numericalized_example(\n                field, example, numericalized_example, batch_first=True)\n\n    def test_serialization(self):\n        nesting_field = data.Field(batch_first=True)\n        field = data.NestedField(nesting_field)\n        ex1 = data.Example.fromlist([""john loves mary""], [(""words"", field)])\n        ex2 = data.Example.fromlist([""mary cries""], [(""words"", field)])\n        dataset = data.Dataset([ex1, ex2], [(""words"", field)])\n        field.build_vocab(dataset)\n        examples_data = [\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""john"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""loves"") + [""</w>""],\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n            ],\n            [\n                [""<w>"", ""<s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<w>""] + list(""mary"") + [""</w>"", ""<cpad>""],\n                [""<w>""] + list(""cries"") + [""</w>""],\n                [""<w>"", ""</s>"", ""</w>""] + [""<cpad>""] * 4,\n                [""<cpad>""] * 7,\n            ]\n        ]\n\n        field_pickle_filename = ""char_field.pl""\n        field_pickle_path = os.path.join(self.test_dir, field_pickle_filename)\n        torch.save(field, field_pickle_path)\n\n        loaded_field = torch.load(field_pickle_path)\n        assert loaded_field == field\n\n        original_numericalization = field.numericalize(examples_data)\n        pickled_numericalization = loaded_field.numericalize(examples_data)\n\n        assert torch.all(torch.eq(original_numericalization, pickled_numericalization))\n\n\nclass TestLabelField(TorchtextTestCase):\n    def test_init(self):\n        # basic init\n        label_field = data.LabelField()\n        assert label_field.sequential is False\n        assert label_field.unk_token is None\n\n        # init with preset fields\n        label_field = data.LabelField(sequential=True, unk_token=""<unk>"")\n        assert label_field.sequential is False\n        assert label_field.unk_token is None\n\n    def test_vocab_size(self):\n        # Set up fields\n        question_field = data.Field(sequential=True)\n        label_field = data.LabelField()\n\n        # Copied from test_build_vocab with minor changes\n        # Write TSV dataset and construct a Dataset\n        self.write_test_ppid_dataset(data_format=""tsv"")\n        tsv_fields = [(""id"", None), (""q1"", question_field),\n                      (""q2"", question_field), (""label"", label_field)]\n        tsv_dataset = data.TabularDataset(\n            path=self.test_ppid_dataset_path, format=""tsv"",\n            fields=tsv_fields)\n\n        # Skipping json dataset as we can rely on the original build vocab test\n        label_field.build_vocab(tsv_dataset)\n        assert label_field.vocab.freqs == Counter({\'1\': 2, \'0\': 1})\n        expected_stoi = {\'1\': 0, \'0\': 1}  # No <unk>\n        assert dict(label_field.vocab.stoi) == expected_stoi\n        # Turn the stoi dictionary into an itos list\n        expected_itos = [x[0] for x in sorted(expected_stoi.items(),\n                                              key=lambda tup: tup[1])]\n        assert label_field.vocab.itos == expected_itos\n'"
test/data/test_functional.py,9,"b'import os\nimport uuid\nimport shutil\nimport unittest\nimport tempfile\n\nimport sentencepiece as spm\nimport torch\nimport torchtext.data as data\nfrom torchtext.data.functional import (\n    generate_sp_model,\n    load_sp_model,\n    sentencepiece_numericalizer,\n    sentencepiece_tokenizer,\n    custom_replace,\n    simple_space_split,\n)\nfrom torchtext.experimental.transforms import (\n    BasicEnglishNormalize,\n    RegexTokenizer\n)\n\nfrom ..common.torchtext_test_case import TorchtextTestCase\nfrom ..common.assets import get_asset_path\n\n\nclass TestFunctional(TorchtextTestCase):\n    def test_generate_sp_model(self):\n        """"""\n        Test the function to train a sentencepiece tokenizer.\n        """"""\n\n        asset_name = \'text_normalization_ag_news_test.csv\'\n        asset_path = get_asset_path(asset_name)\n        # We use temporary directory for two reasons:\n        # 1. buck (fb internal) generates test environment which contains \',\' in its path.\n        #    SentencePieceTrainer considers such path as comma-delimited file list.\n        #    So as workaround we copy the asset data to temporary directory and load it from there.\n        # 2. when fb infra performs stress tests, multiple instances of this test run.\n        #    The name of the generated models have to be unique and they need to be cleaned up.\n        with tempfile.TemporaryDirectory() as dir_name:\n            data_path = os.path.join(dir_name, asset_name)\n            shutil.copy(asset_path, data_path)\n\n            model_prefix = os.path.join(dir_name, f\'spm_user_{uuid.uuid4()}\')\n            model_file = f\'{model_prefix}.model\'\n            generate_sp_model(data_path, vocab_size=23456, model_prefix=model_prefix)\n\n            sp_user = spm.SentencePieceProcessor()\n            sp_user.Load(model_file)\n\n            self.assertEqual(len(sp_user), 23456)\n\n    def test_sentencepiece_numericalizer(self):\n        test_sample = \'SentencePiece is an unsupervised text tokenizer and detokenizer\'\n        model_path = get_asset_path(\'spm_example.model\')\n        sp_model = load_sp_model(model_path)\n        self.assertEqual(sp_model.GetPieceSize(), 20000)\n        spm_generator = sentencepiece_numericalizer(sp_model)\n\n        ref_results = [15340, 4286, 981, 1207, 1681, 17, 84, 684, 8896, 5366,\n                       144, 3689, 9, 5602, 12114, 6, 560, 649, 5602, 12114]\n\n        self.assertEqual(list(spm_generator([test_sample]))[0],\n                         ref_results)\n\n    def test_sentencepiece_tokenizer(self):\n        test_sample = \'SentencePiece is an unsupervised text tokenizer and detokenizer\'\n        model_path = get_asset_path(\'spm_example.model\')\n        sp_model = load_sp_model(model_path)\n        self.assertEqual(sp_model.GetPieceSize(), 20000)\n        spm_generator = sentencepiece_tokenizer(sp_model)\n\n        ref_results = [\'\\u2581Sent\', \'ence\', \'P\', \'ie\', \'ce\', \'\\u2581is\',\n                       \'\\u2581an\', \'\\u2581un\', \'super\', \'vis\', \'ed\', \'\\u2581text\',\n                       \'\\u2581to\', \'ken\', \'izer\', \'\\u2581and\',\n                       \'\\u2581de\', \'to\', \'ken\', \'izer\']\n\n        self.assertEqual(list(spm_generator([test_sample]))[0],\n                         ref_results)\n\n    # TODO(Nayef211): uncomment and replace the test below with this once\n    # https://github.com/pytorch/pytorch/issues/38207 is closed\n    # def test_BasicEnglishNormalize(self):\n    #     test_sample = \'\\\'"".<br />,()!?;:   Basic English Normalization for a Line of Text   \\\'"".<br />,()!?;:\'\n    #     ref_results = [""\'"", \'.\', \',\', \'(\', \')\', \'!\', \'?\', \'basic\', \'english\', \'normalization\',\n    #                    \'for\', \'a\', \'line\', \'of\', \'text\', ""\'"", \'.\', \',\', \'(\', \')\', \'!\', \'?\']\n\n    #     basic_english_normalize = BasicEnglishNormalize()\n    #     experimental_eager_tokens = basic_english_normalize(test_sample)\n\n    #     jit_basic_english_normalize = torch.jit.script(basic_english_normalize)\n    #     experimental_jit_tokens = jit_basic_english_normalize(test_sample)\n\n    #     basic_english_tokenizer = data.get_tokenizer(""basic_english"")\n    #     eager_tokens = basic_english_tokenizer(test_sample)\n\n    #     self.assertEqual(experimental_jit_tokens, ref_results)\n    #     self.assertEqual(experimental_jit_tokens, eager_tokens)\n    #     self.assertEqual(experimental_jit_tokens, experimental_eager_tokens)\n\n    def test_BasicEnglishNormalize(self):\n        test_sample = \'Basic English Normalization for a Line of Text\'\n        ref_results = [\'basic\', \'english\', \'normalization\',\n                       \'for\', \'a\', \'line\', \'of\', \'text\']\n\n        basic_english_normalize = BasicEnglishNormalize()\n        experimental_eager_tokens = basic_english_normalize(test_sample)\n\n        basic_english_tokenizer = data.get_tokenizer(""basic_english"")\n        tokens_eager = basic_english_tokenizer(test_sample)\n\n        self.assertEqual(experimental_eager_tokens, ref_results)\n        self.assertEqual(experimental_eager_tokens, tokens_eager)\n\n    # TODO(Nayef211): uncomment and replace the test below with this once\n    # https://github.com/pytorch/pytorch/issues/38207 is closed\n    # def test_RegexTokenizer(self):\n    #     test_sample = \'\\\'"".<br />,()!?;:   Basic Regex Tokenization for a Line of Text   \\\'"".<br />,()!?;:\'\n    #     ref_results = [""\'"", \'.\', \',\', \'(\', \')\', \'!\', \'?\', \'Basic\', \'Regex\', \'Tokenization\',\n    #                    \'for\', \'a\', \'Line\', \'of\', \'Text\', ""\'"", \'.\', \',\', \'(\', \')\', \'!\', \'?\']\n    #     patterns_list = [\n    #         (r\'\\\'\', \' \\\'  \'),\n    #         (r\'\\""\', \'\'),\n    #         (r\'\\.\', \' . \'),\n    #         (r\'<br \\/>\', \' \'),\n    #         (r\',\', \' , \'),\n    #         (r\'\\(\', \' ( \'),\n    #         (r\'\\)\', \' ) \'),\n    #         (r\'\\!\', \' ! \'),\n    #         (r\'\\?\', \' ? \'),\n    #         (r\'\\;\', \' \'),\n    #         (r\'\\:\', \' \'),\n    #         (r\'\\s+\', \' \')]\n\n    #     regex_tokenizer = RegexTokenizer(patterns_list)\n    #     eager_tokens = regex_tokenizer(test_sample)\n\n    #     jit_regex_tokenizer = torch.jit.script(regex_tokenizer)\n    #     jit_tokens = jit_regex_tokenizer(test_sample)\n\n    #     self.assertEqual(jit_tokens, ref_results)\n    #     self.assertEqual(jit_tokens, eager_tokens)\n\n    def test_RegexTokenizer(self):\n        test_sample = \'""Basic Regex Tokenization"". For a Line of Text\'\n        ref_results = [\'Basic\', \'Regex\', \'Tokenization\', \'.\',\n                       \'For\', \'a\', \'Line\', \'of\', \'Text\']\n        patterns_list = [\n            (r\'\\""\', \'\'),\n            (r\'\\.\', \' . \'),\n            (r\'\\s+\', \' \')]\n\n        regex_tokenizer = RegexTokenizer(patterns_list)\n        eager_tokens = regex_tokenizer(test_sample)\n\n        jit_regex_tokenizer = torch.jit.script(regex_tokenizer)\n        jit_tokens = jit_regex_tokenizer(test_sample)\n\n        self.assertEqual(jit_tokens, eager_tokens)\n        self.assertEqual(jit_tokens, ref_results)\n\n    def test_custom_replace(self):\n        custom_replace_transform = custom_replace([(r\'S\', \'s\'), (r\'\\s+\', \' \')])\n        test_sample = [\'test     cuStom   replace\', \'with   uSer   instruction\']\n        ref_results = [\'test custom replace\', \'with user instruction\']\n        self.assertEqual(list(custom_replace_transform(test_sample)),\n                         ref_results)\n\n    def test_simple_space_split(self):\n        test_sample = [\'test simple space split function\']\n        ref_results = [\'test\', \'simple\', \'space\', \'split\', \'function\']\n        self.assertEqual(list(simple_space_split(test_sample))[0],\n                         ref_results)\n\n\nclass ScriptableSP(torch.jit.ScriptModule):\n    def __init__(self, model_path):\n        super().__init__()\n        self.spm = load_sp_model(model_path)\n\n    @torch.jit.script_method\n    def encode(self, input: str):\n        return self.spm.Encode(input)\n\n    @torch.jit.script_method\n    def encode_as_ids(self, input: str):\n        return self.spm.EncodeAsIds(input)\n\n    @torch.jit.script_method\n    def encode_as_pieces(self, input: str):\n        return self.spm.EncodeAsPieces(input)\n\n\nclass TestScriptableSP(unittest.TestCase):\n    def setUp(self):\n        model_path = get_asset_path(\'spm_example.model\')\n        with tempfile.TemporaryDirectory() as dir_name:\n            jit_model_path = os.path.join(dir_name, \'spm_example.model\')\n            torch.jit.script(ScriptableSP(model_path)).save(jit_model_path)\n            self.model = torch.jit.load(jit_model_path)\n\n    def test_encode(self):\n        input = \'SentencePiece is an unsupervised text tokenizer and detokenizer\'\n        expected = [\n            \'\xe2\x96\x81Sent\', \'ence\', \'P\', \'ie\', \'ce\', \'\xe2\x96\x81is\',\n            \'\xe2\x96\x81an\', \'\xe2\x96\x81un\', \'super\', \'vis\', \'ed\', \'\xe2\x96\x81text\',\n            \'\xe2\x96\x81to\', \'ken\', \'izer\', \'\xe2\x96\x81and\',\n            \'\xe2\x96\x81de\', \'to\', \'ken\', \'izer\',\n        ]\n        output = self.model.encode(input)\n        self.assertEqual(expected, output)\n\n    def test_encode_as_ids(self):\n        input = \'SentencePiece is an unsupervised text tokenizer and detokenizer\'\n        expected = [\n            15340, 4286, 981, 1207, 1681, 17, 84, 684, 8896, 5366,\n            144, 3689, 9, 5602, 12114, 6, 560, 649, 5602, 12114]\n        output = self.model.encode_as_ids(input)\n        self.assertEqual(expected, output)\n\n    def test_encode_as_pieces(self):\n        input = \'SentencePiece is an unsupervised text tokenizer and detokenizer\'\n        expected = [\n            \'\\u2581Sent\', \'ence\', \'P\', \'ie\', \'ce\', \'\\u2581is\',\n            \'\\u2581an\', \'\\u2581un\', \'super\', \'vis\', \'ed\', \'\\u2581text\',\n            \'\\u2581to\', \'ken\', \'izer\', \'\\u2581and\',\n            \'\\u2581de\', \'to\', \'ken\', \'izer\',\n        ]\n        output = self.model.encode_as_pieces(input)\n        self.assertEqual(expected, output)\n'"
test/data/test_metrics.py,1,"b""from torchtext.data.metrics import bleu_score\nfrom torch.testing import assert_allclose\nfrom ..common.torchtext_test_case import TorchtextTestCase\n\n\nclass TestUtils(TorchtextTestCase):\n\n    def test_bleu_score(self):\n        # Full match\n        candidate = [['My', 'full', 'pytorch', 'test']]\n        refs = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']]]\n        assert bleu_score(candidate, refs) == 1\n\n        # No 4-gram\n        candidate = [['My', 'full', 'pytorch']]\n        refs = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']]]\n        assert bleu_score(candidate, refs) == 0\n\n        # Partial match\n        candidate = [['My', 'full', 'pytorch', 'test']]\n        refs = [[['My', 'full', 'pytorch', 'test', '!'], ['Different']]]\n        assert_allclose(bleu_score(candidate, refs), 0.7788007)\n\n        # Bigrams and unigrams only\n        candidate = [['My', 'pytorch', 'test']]\n        refs = [[['My', 'full', 'pytorch', 'test'], ['Different']]]\n        assert_allclose(bleu_score(candidate, refs, max_n=2,\n                                   weights=[0.5, 0.5]), 0.5066641)\n\n        # Multi-sentence corpus\n        candidate = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\n        refs = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']],\n                [['No', 'Match']]]\n        assert_allclose(bleu_score(candidate, refs), 0.8408964)\n\n        # Empty input\n        candidate = [[]]\n        refs = [[[]]]\n        assert bleu_score(candidate, refs) == 0\n\n        # Long input, compared to NLTK implementation score\n        # nltl version used: 3.4.5\n        candidate = [['Lucille', 'B', 'has', '3', 'sons'],\n                     ['She', 'loves', 'all', 'her', 'children', 'equally'],\n                     ['No', 'match', 'here', 'at', 'all']]\n\n        refs = [[['I', 'heard', 'Lucille', 'has', 'three', 'sons'],\n                ['Rumor', 'has', 'it', 'Lucille', 'has', '3', 'sons', '!']],\n                [['I', 'love', 'all', 'my', 'children', 'equally'],\n                ['She', 'loves', 'all', 'her', 'children', 'equally']],\n                [['I', 'have', 'made', 'a', 'terrible', 'mistake'], ['Big', 'mistake']]]\n\n        # The comments below give the code used to get each hardcoded bleu score\n        # nltk.translate.bleu_score.corpus_bleu(refs, candidate)\n        assert_allclose(bleu_score(candidate, refs), 0.4573199)\n        # nltk.translate.bleu_score.corpus_bleu(refs, candidate, weights=[0.33]*3)\n        assert_allclose(bleu_score(candidate, refs, 3,\n                        weights=[0.33, 0.33, 0.33]), 0.4901113)\n        # nltk.translate.bleu_score.corpus_bleu(refs, candidate, weights=[0.5]*2)\n        assert_allclose(bleu_score(candidate, refs, 2,\n                        weights=[0.5, 0.5]), 0.5119535)\n        # nltk.translate.bleu_score.corpus_bleu(refs, candidate, weights=[1])\n        assert_allclose(bleu_score(candidate, refs, 1,\n                        weights=[1]), 0.5515605)\n"""
test/data/test_pipeline.py,0,"b'# -*- coding: utf-8 -*-\nimport torchtext.data as data\n\nfrom ..common.torchtext_test_case import TorchtextTestCase\n\n\nclass TestPipeline(TorchtextTestCase):\n    @staticmethod\n    def repeat_n(x, n=3):\n        """"""\n        Given a sequence, repeat it n times.\n        """"""\n        return x * n\n\n    def test_pipeline(self):\n        id_pipeline = data.Pipeline()\n        assert id_pipeline(""Test STring"") == ""Test STring""\n        assert id_pipeline(""\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT"") == ""\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT""\n        assert id_pipeline([""1241"", ""Some String""]) == [""1241"", ""Some String""]\n\n        pipeline = data.Pipeline(str.lower)\n        assert pipeline(""Test STring"") == ""test string""\n        assert pipeline(""\xe1\x91\x8c\xe1\x91\x8eI\xe1\x91\x95O\xe1\x97\xaa\xe1\x95\xae_T\xe1\x95\xae\xe1\x99\xadT"") == ""\xe1\x91\x8c\xe1\x91\x8ei\xe1\x91\x95o\xe1\x97\xaa\xe1\x95\xae_t\xe1\x95\xae\xe1\x99\xadt""\n        assert pipeline([""1241"", ""Some String""]) == [""1241"", ""some string""]\n\n        args_pipeline = data.Pipeline(TestPipeline.repeat_n)\n        assert args_pipeline(""test"", 5) == ""testtesttesttesttest""\n        assert args_pipeline([""ele1"", ""ele2""], 2) == [""ele1ele1"", ""ele2ele2""]\n\n    def test_composition(self):\n        id_pipeline = data.Pipeline()\n        pipeline = data.Pipeline(TestPipeline.repeat_n)\n        pipeline.add_before(id_pipeline)\n        pipeline.add_after(id_pipeline)\n        pipeline.add_before(str.lower)\n        pipeline.add_after(str.capitalize)\n\n        other_pipeline = data.Pipeline(str.swapcase)\n        other_pipeline.add_before(pipeline)\n\n        # Assert pipeline gives proper results after composition\n        # (test that we aren\'t modfifying pipes member)\n        assert pipeline(""teST"") == ""Testtesttest""\n        assert pipeline([""ElE1"", ""eLe2""]) == [""Ele1ele1ele1"", ""Ele2ele2ele2""]\n\n        # Assert pipeline that we added to gives proper results\n        assert other_pipeline(""teST"") == ""tESTTESTTEST""\n        assert other_pipeline([""ElE1"", ""eLe2""]) == [""eLE1ELE1ELE1"", ""eLE2ELE2ELE2""]\n\n    def test_exceptions(self):\n        with self.assertRaises(ValueError):\n            data.Pipeline(""Not Callable"")\n'"
test/data/test_subword.py,0,"b""#!/usr/bin/env python3\n# Note that all the tests in this module require dataset (either network access or cached)\nimport unittest\n\nfrom torchtext import data\nfrom torchtext.datasets import TREC\n\n\nclass TestSubword(unittest.TestCase):\n    def test_subword_trec(self):\n        TEXT = data.SubwordField()\n        LABEL = data.Field(sequential=False)\n        RAW = data.Field(sequential=False, use_vocab=False)\n        raw, _ = TREC.splits(RAW, LABEL)\n        cooked, _ = TREC.splits(TEXT, LABEL)\n        LABEL.build_vocab(cooked)\n        TEXT.build_vocab(cooked, max_size=100)\n        TEXT.segment(cooked)\n        print(cooked[0].text)\n        batch = next(iter(data.Iterator(cooked, 1, shuffle=False)))\n        self.assertEqual(TEXT.reverse(batch.text.data)[0], raw[0].text)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
test/data/test_utils.py,0,"b'import io\n\nimport torchtext.data as data\nfrom torchtext.utils import unicode_csv_reader\nfrom torchtext.experimental.functional import ngrams_func\nfrom ..common.torchtext_test_case import TorchtextTestCase\nfrom ..common.assets import get_asset_path\n\n\nclass TestUtils(TorchtextTestCase):\n    TEST_STR = ""A string, particularly one with slightly complex punctuation.""\n\n    def test_get_tokenizer_split(self):\n        # Test the default case with str.split\n        assert data.get_tokenizer(str.split) == str.split\n        assert data.get_tokenizer(str.split)(self.TEST_STR) == str.split(self.TEST_STR)\n\n    def test_get_tokenizer_toktokt(self):\n        # Test Toktok option. Test strings taken from NLTK doctests.\n        # Note that internally, MosesTokenizer converts to unicode if applicable\n        toktok_tokenizer = data.get_tokenizer(""toktok"")\n        assert toktok_tokenizer(self.TEST_STR) == [\n            ""A"", ""string"", "","", ""particularly"", ""one"", ""with"", ""slightly"",\n            ""complex"", ""punctuation"", "".""]\n\n        # Test that errors are raised for invalid input arguments.\n        with self.assertRaises(ValueError):\n            data.get_tokenizer(1)\n        with self.assertRaises(ValueError):\n            data.get_tokenizer(""some other string"")\n\n    def test_text_nomalize_function(self):\n        # Test text_nomalize function in torchtext.datasets.text_classification\n        ref_lines = []\n        test_lines = []\n\n        tokenizer = data.get_tokenizer(""basic_english"")\n        data_path = get_asset_path(\'text_normalization_ag_news_test.csv\')\n        with io.open(data_path, encoding=""utf8"") as f:\n            reader = unicode_csv_reader(f)\n            for row in reader:\n                test_lines.append(tokenizer(\' , \'.join(row)))\n\n        data_path = get_asset_path(\'text_normalization_ag_news_ref_results.test\')\n        with io.open(data_path, encoding=""utf8"") as ref_data:\n            for line in ref_data:\n                line = line.split()\n                self.assertEqual(line[0][:9], \'__label__\')\n                line[0] = line[0][9:]  # remove \'__label__\'\n                ref_lines.append(line)\n\n        self.assertEqual(ref_lines, test_lines)\n\n    def test_ngrams_func(self):\n        func = ngrams_func(1)\n        assert func([\'A\', \'string\', \'particularly\', \'one\', \'with\', \'slightly\']) == \\\n            [\'A\', \'string\', \'particularly\', \'one\', \'with\', \'slightly\']\n        func = ngrams_func(2)\n        assert func([\'A\', \'string\', \'particularly\', \'one\', \'with\', \'slightly\']) == \\\n            [\'A\', \'string\', \'particularly\', \'one\', \'with\', \'slightly\', \'A string\', \'string particularly\',\n             \'particularly one\', \'one with\', \'with slightly\']\n        func = ngrams_func(3)\n        assert func([\'A\', \'string\', \'particularly\', \'one\', \'with\', \'slightly\']) == \\\n            [\'A\', \'string\', \'particularly\', \'one\', \'with\', \'slightly\', \'A string\', \'string particularly\',\n             \'particularly one\', \'one with\', \'with slightly\', \'A string particularly\',\n             \'string particularly one\', \'particularly one with\', \'one with slightly\']\n'"
torchtext/data/__init__.py,0,"b'from .batch import Batch\nfrom .dataset import Dataset, TabularDataset\nfrom .example import Example\nfrom .field import RawField, Field, ReversibleField, SubwordField, NestedField, LabelField\nfrom .iterator import (batch, BucketIterator, Iterator, BPTTIterator,\n                       pool)\nfrom .metrics import bleu_score\nfrom .pipeline import Pipeline\nfrom .utils import get_tokenizer, interleave_keys\nfrom .functional import generate_sp_model, \\\n    load_sp_model, \\\n    sentencepiece_numericalizer, \\\n    sentencepiece_tokenizer, custom_replace, simple_space_split, \\\n    numericalize_tokens_from_iterator\n\n__all__ = [""Batch"",\n           ""Dataset"", ""TabularDataset"",\n           ""Example"",\n           ""RawField"", ""Field"", ""ReversibleField"", ""SubwordField"", ""NestedField"",\n           ""LabelField"",\n           ""batch"", ""BucketIterator"", ""Iterator"", ""BPTTIterator"",\n           ""pool"",\n           ""bleu_score"",\n           ""Pipeline"",\n           ""get_tokenizer"", ""interleave_keys"",\n           ""generate_sp_model"", ""load_sp_model"",\n           ""sentencepiece_numericalizer"", ""sentencepiece_tokenizer"",\n           ""custom_replace"", ""simple_space_split"",\n           ""numericalize_tokens_from_iterator""]\n'"
torchtext/data/batch.py,4,"b'import torch\n\n\nclass Batch(object):\n    """"""Defines a batch of examples along with its Fields.\n\n    Attributes:\n        batch_size: Number of examples in the batch.\n        dataset: A reference to the dataset object the examples come from\n            (which itself contains the dataset\'s Field objects).\n        train: Deprecated: this attribute is left for backwards compatibility,\n            however it is UNUSED as of the merger with pytorch 0.4.\n        input_fields: The names of the fields that are used as input for the model\n        target_fields: The names of the fields that are used as targets during\n                       model training\n\n    Also stores the Variable for each column in the batch as an attribute.\n    """"""\n\n    def __init__(self, data=None, dataset=None, device=None):\n        """"""Create a Batch from a list of examples.""""""\n        if data is not None:\n            self.batch_size = len(data)\n            self.dataset = dataset\n            self.fields = dataset.fields.keys()  # copy field names\n            self.input_fields = [k for k, v in dataset.fields.items() if\n                                 v is not None and not v.is_target]\n            self.target_fields = [k for k, v in dataset.fields.items() if\n                                  v is not None and v.is_target]\n\n            for (name, field) in dataset.fields.items():\n                if field is not None:\n                    batch = [getattr(x, name) for x in data]\n                    setattr(self, name, field.process(batch, device=device))\n\n    @classmethod\n    def fromvars(cls, dataset, batch_size, train=None, **kwargs):\n        """"""Create a Batch directly from a number of Variables.""""""\n        batch = cls()\n        batch.batch_size = batch_size\n        batch.dataset = dataset\n        batch.fields = dataset.fields.keys()\n        for k, v in kwargs.items():\n            setattr(batch, k, v)\n        return batch\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        if not self.__dict__:\n            return \'Empty {} instance\'.format(torch.typename(self))\n\n        fields_to_index = filter(lambda field: field is not None, self.fields)\n        var_strs = \'\\n\'.join([\'\\t[.\' + name + \']\' + "":"" + _short_str(getattr(self, name))\n                              for name in fields_to_index if hasattr(self, name)])\n\n        data_str = (\' from {}\'.format(self.dataset.name.upper())\n                    if hasattr(self.dataset, \'name\')\n                    and isinstance(self.dataset.name, str) else \'\')\n\n        strt = \'[{} of size {}{}]\\n{}\'.format(torch.typename(self),\n                                              self.batch_size, data_str, var_strs)\n        return \'\\n\' + strt\n\n    def __len__(self):\n        return self.batch_size\n\n    def _get_field_values(self, fields):\n        if len(fields) == 0:\n            return None\n        elif len(fields) == 1:\n            return getattr(self, fields[0])\n        else:\n            return tuple(getattr(self, f) for f in fields)\n\n    def __iter__(self):\n        yield self._get_field_values(self.input_fields)\n        yield self._get_field_values(self.target_fields)\n\n\ndef _short_str(tensor):\n    # unwrap variable to tensor\n    if not torch.is_tensor(tensor):\n        # (1) unpack variable\n        if hasattr(tensor, \'data\'):\n            tensor = getattr(tensor, \'data\')\n        # (2) handle include_lengths\n        elif isinstance(tensor, tuple):\n            return str(tuple(_short_str(t) for t in tensor))\n        # (3) fallback to default str\n        else:\n            return str(tensor)\n\n    # copied from torch _tensor_str\n    size_str = \'x\'.join(str(size) for size in tensor.size())\n    device_str = \'\' if not tensor.is_cuda else \\\n        \' (GPU {})\'.format(tensor.get_device())\n    strt = \'[{} of size {}{}]\'.format(torch.typename(tensor),\n                                      size_str, device_str)\n    return strt\n'"
torchtext/data/dataset.py,2,"b'import io\nimport os\nimport zipfile\nimport tarfile\nimport gzip\nimport shutil\nfrom functools import partial\n\nimport torch.utils.data\n\nfrom .utils import RandomShuffler\nfrom .example import Example\nfrom ..utils import download_from_url, unicode_csv_reader\n\n\nclass Dataset(torch.utils.data.Dataset):\n    """"""Defines a dataset composed of Examples along with its Fields.\n\n    Attributes:\n        sort_key (callable): A key to use for sorting dataset examples for batching\n            together examples with similar lengths to minimize padding.\n        examples (list(Example)): The examples in this dataset.\n        fields (dict[str, Field]): Contains the name of each column or field, together\n            with the corresponding Field object. Two fields with the same Field object\n            will have a shared vocabulary.\n    """"""\n    sort_key = None\n\n    def __init__(self, examples, fields, filter_pred=None):\n        """"""Create a dataset from a list of Examples and Fields.\n\n        Arguments:\n            examples: List of Examples.\n            fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n                string is a field name, and the Field is the associated field.\n            filter_pred (callable or None): Use only examples for which\n                filter_pred(example) is True, or use all examples if None.\n                Default is None.\n        """"""\n        if filter_pred is not None:\n            make_list = isinstance(examples, list)\n            examples = filter(filter_pred, examples)\n            if make_list:\n                examples = list(examples)\n        self.examples = examples\n        self.fields = dict(fields)\n        # Unpack field tuples\n        for n, f in list(self.fields.items()):\n            if isinstance(n, tuple):\n                self.fields.update(zip(n, f))\n                del self.fields[n]\n\n    @classmethod\n    def splits(cls, path=None, root=\'.data\', train=None, validation=None,\n               test=None, **kwargs):\n        """"""Create Dataset objects for multiple splits of a dataset.\n\n        Arguments:\n            path (str): Common prefix of the splits\' file paths, or None to use\n                the result of cls.download(root).\n            root (str): Root dataset storage directory. Default is \'.data\'.\n            train (str): Suffix to add to path for the train set, or None for no\n                train set. Default is None.\n            validation (str): Suffix to add to path for the validation set, or None\n                for no validation set. Default is None.\n            test (str): Suffix to add to path for the test set, or None for no test\n                set. Default is None.\n            Remaining keyword arguments: Passed to the constructor of the\n                Dataset (sub)class being used.\n\n        Returns:\n            Tuple[Dataset]: Datasets for train, validation, and\n            test splits in that order, if provided.\n        """"""\n        if path is None:\n            path = cls.download(root)\n        train_data = None if train is None else cls(\n            os.path.join(path, train), **kwargs)\n        val_data = None if validation is None else cls(\n            os.path.join(path, validation), **kwargs)\n        test_data = None if test is None else cls(\n            os.path.join(path, test), **kwargs)\n        return tuple(d for d in (train_data, val_data, test_data)\n                     if d is not None)\n\n    def split(self, split_ratio=0.7, stratified=False, strata_field=\'label\',\n              random_state=None):\n        """"""Create train-test(-valid?) splits from the instance\'s examples.\n\n        Arguments:\n            split_ratio (float or List of floats): a number [0, 1] denoting the amount\n                of data to be used for the training split (rest is used for test),\n                or a list of numbers denoting the relative sizes of train, test and valid\n                splits respectively. If the relative size for valid is missing, only the\n                train-test split is returned. Default is 0.7 (for the train set).\n            stratified (bool): whether the sampling should be stratified.\n                Default is False.\n            strata_field (str): name of the examples Field stratified over.\n                Default is \'label\' for the conventional label field.\n            random_state (tuple): the random seed used for shuffling.\n                A return value of `random.getstate()`.\n\n        Returns:\n            Tuple[Dataset]: Datasets for train, validation, and\n            test splits in that order, if the splits are provided.\n        """"""\n        train_ratio, test_ratio, val_ratio = check_split_ratio(split_ratio)\n\n        # For the permutations\n        rnd = RandomShuffler(random_state)\n        if not stratified:\n            train_data, test_data, val_data = rationed_split(self.examples, train_ratio,\n                                                             test_ratio, val_ratio, rnd)\n        else:\n            if strata_field not in self.fields:\n                raise ValueError(""Invalid field name for strata_field {}""\n                                 .format(strata_field))\n            strata = stratify(self.examples, strata_field)\n            train_data, test_data, val_data = [], [], []\n            for group in strata:\n                # Stratify each group and add together the indices.\n                group_train, group_test, group_val = rationed_split(group, train_ratio,\n                                                                    test_ratio, val_ratio,\n                                                                    rnd)\n                train_data += group_train\n                test_data += group_test\n                val_data += group_val\n\n        splits = tuple(Dataset(d, self.fields)\n                       for d in (train_data, val_data, test_data) if d)\n\n        # In case the parent sort key isn\'t none\n        if self.sort_key:\n            for subset in splits:\n                subset.sort_key = self.sort_key\n        return splits\n\n    def __getitem__(self, i):\n        return self.examples[i]\n\n    def __len__(self):\n        try:\n            return len(self.examples)\n        except TypeError:\n            return 2**32\n\n    def __iter__(self):\n        for x in self.examples:\n            yield x\n\n    def __getattr__(self, attr):\n        if attr in self.fields:\n            for x in self.examples:\n                yield getattr(x, attr)\n\n    @classmethod\n    def download(cls, root, check=None):\n        """"""Download and unzip an online archive (.zip, .gz, or .tgz).\n\n        Arguments:\n            root (str): Folder to download data to.\n            check (str or None): Folder whose existence indicates\n                that the dataset has already been downloaded, or\n                None to check the existence of root/{cls.name}.\n\n        Returns:\n            str: Path to extracted dataset.\n        """"""\n        path = os.path.join(root, cls.name)\n        check = path if check is None else check\n        if not os.path.isdir(check):\n            for url in cls.urls:\n                if isinstance(url, tuple):\n                    url, filename = url\n                else:\n                    filename = os.path.basename(url)\n                zpath = os.path.join(path, filename)\n                if not os.path.isfile(zpath):\n                    if not os.path.exists(os.path.dirname(zpath)):\n                        os.makedirs(os.path.dirname(zpath))\n                    print(\'downloading {}\'.format(filename))\n                    download_from_url(url, zpath)\n                zroot, ext = os.path.splitext(zpath)\n                _, ext_inner = os.path.splitext(zroot)\n                if ext == \'.zip\':\n                    with zipfile.ZipFile(zpath, \'r\') as zfile:\n                        print(\'extracting\')\n                        zfile.extractall(path)\n                # tarfile cannot handle bare .gz files\n                elif ext == \'.tgz\' or ext == \'.gz\' and ext_inner == \'.tar\':\n                    with tarfile.open(zpath, \'r:gz\') as tar:\n                        dirs = [member for member in tar.getmembers()]\n                        tar.extractall(path=path, members=dirs)\n                elif ext == \'.gz\':\n                    with gzip.open(zpath, \'rb\') as gz:\n                        with open(zroot, \'wb\') as uncompressed:\n                            shutil.copyfileobj(gz, uncompressed)\n\n        return os.path.join(path, cls.dirname)\n\n    def filter_examples(self, field_names):\n        """"""Remove unknown words from dataset examples with respect to given field.\n\n        Arguments:\n            field_names (list(str)): Within example only the parts with field names in\n                field_names will have their unknown words deleted.\n        """"""\n        for i, example in enumerate(self.examples):\n            for field_name in field_names:\n                vocab = set(self.fields[field_name].vocab.stoi)\n                text = getattr(example, field_name)\n                example_part = [word for word in text if word in vocab]\n                setattr(example, field_name, example_part)\n            self.examples[i] = example\n\n\nclass TabularDataset(Dataset):\n    """"""Defines a Dataset of columns stored in CSV, TSV, or JSON format.""""""\n\n    def __init__(self, path, format, fields, skip_header=False,\n                 csv_reader_params={}, **kwargs):\n        """"""Create a TabularDataset given a path, file format, and field list.\n\n        Arguments:\n            path (str): Path to the data file.\n            format (str): The format of the data file. One of ""CSV"", ""TSV"", or\n                ""JSON"" (case-insensitive).\n            fields (list(tuple(str, Field)) or dict[str: tuple(str, Field)]:\n                If using a list, the format must be CSV or TSV, and the values of the list\n                should be tuples of (name, field).\n                The fields should be in the same order as the columns in the CSV or TSV\n                file, while tuples of (name, None) represent columns that will be ignored.\n\n                If using a dict, the keys should be a subset of the JSON keys or CSV/TSV\n                columns, and the values should be tuples of (name, field).\n                Keys not present in the input dictionary are ignored.\n                This allows the user to rename columns from their JSON/CSV/TSV key names\n                and also enables selecting a subset of columns to load.\n            skip_header (bool): Whether to skip the first line of the input file.\n            csv_reader_params(dict): Parameters to pass to the csv reader.\n                Only relevant when format is csv or tsv.\n                See\n                https://docs.python.org/3/library/csv.html#csv.reader\n                for more details.\n        """"""\n        format = format.lower()\n        make_example = {\n            \'json\': Example.fromJSON, \'dict\': Example.fromdict,\n            \'tsv\': Example.fromCSV, \'csv\': Example.fromCSV}[format]\n\n        with io.open(os.path.expanduser(path), encoding=""utf8"") as f:\n            if format == \'csv\':\n                reader = unicode_csv_reader(f, **csv_reader_params)\n            elif format == \'tsv\':\n                reader = unicode_csv_reader(f, delimiter=\'\\t\', **csv_reader_params)\n            else:\n                reader = f\n\n            if format in [\'csv\', \'tsv\'] and isinstance(fields, dict):\n                if skip_header:\n                    raise ValueError(\'When using a dict to specify fields with a {} file,\'\n                                     \'skip_header must be False and\'\n                                     \'the file must have a header.\'.format(format))\n                header = next(reader)\n                field_to_index = {f: header.index(f) for f in fields.keys()}\n                make_example = partial(make_example, field_to_index=field_to_index)\n\n            if skip_header:\n                next(reader)\n\n            examples = [make_example(line, fields) for line in reader]\n\n        if isinstance(fields, dict):\n            fields, field_dict = [], fields\n            for field in field_dict.values():\n                if isinstance(field, list):\n                    fields.extend(field)\n                else:\n                    fields.append(field)\n\n        super(TabularDataset, self).__init__(examples, fields, **kwargs)\n\n\ndef check_split_ratio(split_ratio):\n    """"""Check that the split ratio argument is not malformed""""""\n    valid_ratio = 0.\n    if isinstance(split_ratio, float):\n        # Only the train set relative ratio is provided\n        # Assert in bounds, validation size is zero\n        assert 0. < split_ratio < 1., (\n            ""Split ratio {} not between 0 and 1"".format(split_ratio))\n\n        test_ratio = 1. - split_ratio\n        return (split_ratio, test_ratio, valid_ratio)\n    elif isinstance(split_ratio, list):\n        # A list of relative ratios is provided\n        length = len(split_ratio)\n        assert length == 2 or length == 3, (\n            ""Length of split ratio list should be 2 or 3, got {}"".format(split_ratio))\n\n        # Normalize if necessary\n        ratio_sum = sum(split_ratio)\n        if not ratio_sum == 1.:\n            split_ratio = [float(ratio) / ratio_sum for ratio in split_ratio]\n\n        if length == 2:\n            return tuple(split_ratio + [valid_ratio])\n        return tuple(split_ratio)\n    else:\n        raise ValueError(\'Split ratio must be float or a list, got {}\'\n                         .format(type(split_ratio)))\n\n\ndef stratify(examples, strata_field):\n    # The field has to be hashable otherwise this doesn\'t work\n    # There\'s two iterations over the whole dataset here, which can be\n    # reduced to just one if a dedicated method for stratified splitting is used\n    unique_strata = set(getattr(example, strata_field) for example in examples)\n    strata_maps = {s: [] for s in unique_strata}\n    for example in examples:\n        strata_maps[getattr(example, strata_field)].append(example)\n    return list(strata_maps.values())\n\n\ndef rationed_split(examples, train_ratio, test_ratio, val_ratio, rnd):\n    """"""Create a random permutation of examples, then split them by ratios\n\n    Arguments:\n        examples: a list of data\n        train_ratio, test_ratio, val_ratio: split fractions.\n        rnd: a random shuffler\n\n    Examples:\n        >>> examples = []\n        >>> train_ratio, test_ratio, val_ratio = 0.7, 0.2, 0.1\n        >>> rnd = torchtext.data.dataset.RandomShuffler(None)\n        >>> train_examples, test_examples, valid_examples = \\\n                torchtext.data.dataset.rationed_split(examples, train_ratio,\n                                                      test_ratio, val_ratio,\n                                                      rnd)\n    """"""\n    N = len(examples)\n    randperm = rnd(range(N))\n    train_len = int(round(train_ratio * N))\n\n    # Due to possible rounding problems\n    if not val_ratio:\n        test_len = N - train_len\n    else:\n        test_len = int(round(test_ratio * N))\n\n    indices = (randperm[:train_len],  # Train\n               randperm[train_len:train_len + test_len],  # Test\n               randperm[train_len + test_len:])  # Validation\n\n    # There\'s a possibly empty list for the validation set\n    data = tuple([examples[i] for i in index] for index in indices)\n\n    return data\n'"
torchtext/data/example.py,0,"b'import json\nfrom functools import reduce\n\n\nclass Example(object):\n    """"""Defines a single training or test example.\n\n    Stores each column of the example as an attribute.\n    """"""\n    @classmethod\n    def fromJSON(cls, data, fields):\n        ex = cls()\n        obj = json.loads(data)\n\n        for key, vals in fields.items():\n            if vals is not None:\n                if not isinstance(vals, list):\n                    vals = [vals]\n\n                for val in vals:\n                    # for processing the key likes \'foo.bar\'\n                    name, field = val\n                    ks = key.split(\'.\')\n\n                    def reducer(obj, key):\n                        if isinstance(obj, list):\n                            results = []\n                            for data in obj:\n                                if key not in data:\n                                    # key error\n                                    raise ValueError(""Specified key {} was not found in ""\n                                                     ""the input data"".format(key))\n                                else:\n                                    results.append(data[key])\n                            return results\n                        else:\n                            # key error\n                            if key not in obj:\n                                raise ValueError(""Specified key {} was not found in ""\n                                                 ""the input data"".format(key))\n                            else:\n                                return obj[key]\n\n                    v = reduce(reducer, ks, obj)\n                    setattr(ex, name, field.preprocess(v))\n        return ex\n\n    @classmethod\n    def fromdict(cls, data, fields):\n        ex = cls()\n        for key, vals in fields.items():\n            if key not in data:\n                raise ValueError(""Specified key {} was not found in ""\n                                 ""the input data"".format(key))\n            if vals is not None:\n                if not isinstance(vals, list):\n                    vals = [vals]\n                for val in vals:\n                    name, field = val\n                    setattr(ex, name, field.preprocess(data[key]))\n        return ex\n\n    @classmethod\n    def fromCSV(cls, data, fields, field_to_index=None):\n        if field_to_index is None:\n            return cls.fromlist(data, fields)\n        else:\n            assert(isinstance(fields, dict))\n            data_dict = {f: data[idx] for f, idx in field_to_index.items()}\n            return cls.fromdict(data_dict, fields)\n\n    @classmethod\n    def fromlist(cls, data, fields):\n        ex = cls()\n        for (name, field), val in zip(fields, data):\n            if field is not None:\n                if isinstance(val, str):\n                    val = val.rstrip(\'\\n\')\n                # Handle field tuples\n                if isinstance(name, tuple):\n                    for n, f in zip(name, field):\n                        setattr(ex, n, f.preprocess(val))\n                else:\n                    setattr(ex, name, field.preprocess(val))\n        return ex\n\n    @classmethod\n    def fromtree(cls, data, fields, subtrees=False):\n        try:\n            from nltk.tree import Tree\n        except ImportError:\n            print(""Please install NLTK. ""\n                  ""See the docs at http://nltk.org for more information."")\n            raise\n        tree = Tree.fromstring(data)\n        if subtrees:\n            return [cls.fromlist(\n                [\' \'.join(t.leaves()), t.label()], fields) for t in tree.subtrees()]\n        return cls.fromlist([\' \'.join(tree.leaves()), tree.label()], fields)\n'"
torchtext/data/field.py,30,"b'# coding: utf8\nfrom collections import Counter, OrderedDict\nfrom itertools import chain\nimport torch\nfrom tqdm import tqdm\n\nfrom .dataset import Dataset\nfrom .pipeline import Pipeline\nfrom .utils import get_tokenizer, dtype_to_attr, is_tokenizer_serializable\nfrom ..vocab import Vocab, SubwordVocab\n\n\nclass RawField(object):\n    """""" Defines a general datatype.\n\n    Every dataset consists of one or more types of data. For instance, a text\n    classification dataset contains sentences and their classes, while a\n    machine translation dataset contains paired examples of text in two\n    languages. Each of these types of data is represented by a RawField object.\n    A RawField object does not assume any property of the data type and\n    it holds parameters relating to how a datatype should be processed.\n\n    Attributes:\n        preprocessing: The Pipeline that will be applied to examples\n            using this field before creating an example.\n            Default: None.\n        postprocessing: A Pipeline that will be applied to a list of examples\n            using this field before assigning to a batch.\n            Function signature: (batch(list)) -> object\n            Default: None.\n        is_target: Whether this field is a target variable.\n            Affects iteration over batches. Default: False\n    """"""\n\n    def __init__(self, preprocessing=None, postprocessing=None, is_target=False):\n        self.preprocessing = preprocessing\n        self.postprocessing = postprocessing\n        self.is_target = is_target\n\n    def preprocess(self, x):\n        """""" Preprocess an example if the `preprocessing` Pipeline is provided. """"""\n        if self.preprocessing is not None:\n            return self.preprocessing(x)\n        else:\n            return x\n\n    def process(self, batch, *args, **kwargs):\n        """""" Process a list of examples to create a batch.\n\n        Postprocess the batch with user-provided Pipeline.\n\n        Args:\n            batch (list(object)): A list of object from a batch of examples.\n        Returns:\n            object: Processed object given the input and custom\n            postprocessing Pipeline.\n        """"""\n        if self.postprocessing is not None:\n            batch = self.postprocessing(batch)\n        return batch\n\n\nclass Field(RawField):\n    """"""Defines a datatype together with instructions for converting to Tensor.\n\n    Field class models common text processing datatypes that can be represented\n    by tensors.  It holds a Vocab object that defines the set of possible values\n    for elements of the field and their corresponding numerical representations.\n    The Field object also holds other parameters relating to how a datatype\n    should be numericalized, such as a tokenization method and the kind of\n    Tensor that should be produced.\n\n    If a Field is shared between two columns in a dataset (e.g., question and\n    answer in a QA dataset), then they will have a shared vocabulary.\n\n    Attributes:\n        sequential: Whether the datatype represents sequential data. If False,\n            no tokenization is applied. Default: True.\n        use_vocab: Whether to use a Vocab object. If False, the data in this\n            field should already be numerical. Default: True.\n        init_token: A token that will be prepended to every example using this\n            field, or None for no initial token. Default: None.\n        eos_token: A token that will be appended to every example using this\n            field, or None for no end-of-sentence token. Default: None.\n        fix_length: A fixed length that all examples using this field will be\n            padded to, or None for flexible sequence lengths. Default: None.\n        dtype: The torch.dtype class that represents a batch of examples\n            of this kind of data. Default: torch.long.\n        preprocessing: The Pipeline that will be applied to examples\n            using this field after tokenizing but before numericalizing. Many\n            Datasets replace this attribute with a custom preprocessor.\n            Default: None.\n        postprocessing: A Pipeline that will be applied to examples using\n            this field after numericalizing but before the numbers are turned\n            into a Tensor. The pipeline function takes the batch as a list, and\n            the field\'s Vocab.\n            Default: None.\n        lower: Whether to lowercase the text in this field. Default: False.\n        tokenize: The function used to tokenize strings using this field into\n            sequential examples. If ""spacy"", the SpaCy tokenizer is\n            used. If a non-serializable function is passed as an argument,\n            the field will not be able to be serialized. Default: string.split.\n        tokenizer_language: The language of the tokenizer to be constructed.\n            Various languages currently supported only in SpaCy.\n        include_lengths: Whether to return a tuple of a padded minibatch and\n            a list containing the lengths of each examples, or just a padded\n            minibatch. Default: False.\n        batch_first: Whether to produce tensors with the batch dimension first.\n            Default: False.\n        pad_token: The string token used as padding. Default: ""<pad>"".\n        unk_token: The string token used to represent OOV words. Default: ""<unk>"".\n        pad_first: Do the padding of the sequence at the beginning. Default: False.\n        truncate_first: Do the truncating of the sequence at the beginning. Default: False\n        stop_words: Tokens to discard during the preprocessing step. Default: None\n        is_target: Whether this field is a target variable.\n            Affects iteration over batches. Default: False\n    """"""\n\n    vocab_cls = Vocab\n    # Dictionary mapping PyTorch tensor dtypes to the appropriate Python\n    # numeric type.\n    dtypes = {\n        torch.float32: float,\n        torch.float: float,\n        torch.float64: float,\n        torch.double: float,\n        torch.float16: float,\n        torch.half: float,\n\n        torch.uint8: int,\n        torch.int8: int,\n        torch.int16: int,\n        torch.short: int,\n        torch.int32: int,\n        torch.int: int,\n        torch.int64: int,\n        torch.long: int,\n    }\n\n    ignore = [\'dtype\', \'tokenize\']\n\n    def __init__(self, sequential=True, use_vocab=True, init_token=None,\n                 eos_token=None, fix_length=None, dtype=torch.long,\n                 preprocessing=None, postprocessing=None, lower=False,\n                 tokenize=None, tokenizer_language=\'en\', include_lengths=False,\n                 batch_first=False, pad_token=""<pad>"", unk_token=""<unk>"",\n                 pad_first=False, truncate_first=False, stop_words=None,\n                 is_target=False):\n        self.sequential = sequential\n        self.use_vocab = use_vocab\n        self.init_token = init_token\n        self.eos_token = eos_token\n        self.unk_token = unk_token\n        self.fix_length = fix_length\n        self.dtype = dtype\n        self.preprocessing = preprocessing\n        self.postprocessing = postprocessing\n        self.lower = lower\n        # store params to construct tokenizer for serialization\n        # in case the tokenizer isn\'t picklable (e.g. spacy)\n        self.tokenizer_args = (tokenize, tokenizer_language)\n        self.tokenize = get_tokenizer(tokenize, tokenizer_language)\n        self.include_lengths = include_lengths\n        self.batch_first = batch_first\n        self.pad_token = pad_token if self.sequential else None\n        self.pad_first = pad_first\n        self.truncate_first = truncate_first\n        try:\n            self.stop_words = set(stop_words) if stop_words is not None else None\n        except TypeError:\n            raise ValueError(""Stop words must be convertible to a set"")\n        self.is_target = is_target\n\n    def __getstate__(self):\n        str_type = dtype_to_attr(self.dtype)\n        if is_tokenizer_serializable(*self.tokenizer_args):\n            tokenize = self.tokenize\n        else:\n            # signal to restore in `__setstate__`\n            tokenize = None\n        attrs = {k: v for k, v in self.__dict__.items() if k not in self.ignore}\n        attrs[\'dtype\'] = str_type\n        attrs[\'tokenize\'] = tokenize\n\n        return attrs\n\n    def __setstate__(self, state):\n        state[\'dtype\'] = getattr(torch, state[\'dtype\'])\n        if not state[\'tokenize\']:\n            state[\'tokenize\'] = get_tokenizer(*state[\'tokenizer_args\'])\n        self.__dict__.update(state)\n\n    def __hash__(self):\n        # we don\'t expect this to be called often\n        return 42\n\n    def __eq__(self, other):\n        if not isinstance(other, RawField):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def preprocess(self, x):\n        """"""Load a single example using this field, tokenizing if necessary.\n\n        If `sequential=True`, the input will be tokenized. Then the input\n        will be optionally lowercased and passed to the user-provided\n        `preprocessing` Pipeline.""""""\n        if self.sequential and isinstance(x, str):\n            x = self.tokenize(x.rstrip(\'\\n\'))\n        if self.lower:\n            x = Pipeline(str.lower)(x)\n        if self.sequential and self.use_vocab and self.stop_words is not None:\n            x = [w for w in x if w not in self.stop_words]\n        if self.preprocessing is not None:\n            return self.preprocessing(x)\n        else:\n            return x\n\n    def process(self, batch, device=None):\n        """""" Process a list of examples to create a torch.Tensor.\n\n        Pad, numericalize, and postprocess a batch and create a tensor.\n\n        Args:\n            batch (list(object)): A list of object from a batch of examples.\n        Returns:\n            torch.autograd.Variable: Processed object given the input\n            and custom postprocessing Pipeline.\n        """"""\n        padded = self.pad(batch)\n        tensor = self.numericalize(padded, device=device)\n        return tensor\n\n    def pad(self, minibatch):\n        """"""Pad a batch of examples using this field.\n\n        Pads to self.fix_length if provided, otherwise pads to the length of\n        the longest example in the batch. Prepends self.init_token and appends\n        self.eos_token if those attributes are not None. Returns a tuple of the\n        padded list and a list containing lengths of each example if\n        `self.include_lengths` is `True` and `self.sequential` is `True`, else just\n        returns the padded list. If `self.sequential` is `False`, no padding is applied.\n        """"""\n        minibatch = list(minibatch)\n        if not self.sequential:\n            return minibatch\n        if self.fix_length is None:\n            max_len = max(len(x) for x in minibatch)\n        else:\n            max_len = self.fix_length + (\n                self.init_token, self.eos_token).count(None) - 2\n        padded, lengths = [], []\n        for x in minibatch:\n            if self.pad_first:\n                padded.append(\n                    [self.pad_token] * max(0, max_len - len(x))\n                    + ([] if self.init_token is None else [self.init_token])\n                    + list(x[-max_len:] if self.truncate_first else x[:max_len])\n                    + ([] if self.eos_token is None else [self.eos_token]))\n            else:\n                padded.append(\n                    ([] if self.init_token is None else [self.init_token])\n                    + list(x[-max_len:] if self.truncate_first else x[:max_len])\n                    + ([] if self.eos_token is None else [self.eos_token])\n                    + [self.pad_token] * max(0, max_len - len(x)))\n            lengths.append(len(padded[-1]) - max(0, max_len - len(x)))\n        if self.include_lengths:\n            return (padded, lengths)\n        return padded\n\n    def build_vocab(self, *args, **kwargs):\n        """"""Construct the Vocab object for this field from one or more datasets.\n\n        Arguments:\n            Positional arguments: Dataset objects or other iterable data\n                sources from which to construct the Vocab object that\n                represents the set of possible values for this field. If\n                a Dataset object is provided, all columns corresponding\n                to this field are used; individual columns can also be\n                provided directly.\n            Remaining keyword arguments: Passed to the constructor of Vocab.\n        """"""\n        counter = Counter()\n        sources = []\n        for arg in args:\n            if isinstance(arg, Dataset):\n                sources += [getattr(arg, name) for name, field in\n                            arg.fields.items() if field is self]\n            else:\n                sources.append(arg)\n        for data in sources:\n            for x in data:\n                if not self.sequential:\n                    x = [x]\n                try:\n                    counter.update(x)\n                except TypeError:\n                    counter.update(chain.from_iterable(x))\n        specials = list(OrderedDict.fromkeys(\n            tok for tok in [self.unk_token, self.pad_token, self.init_token,\n                            self.eos_token] + kwargs.pop(\'specials\', [])\n            if tok is not None))\n        self.vocab = self.vocab_cls(counter, specials=specials, **kwargs)\n\n    def numericalize(self, arr, device=None):\n        """"""Turn a batch of examples that use this field into a Variable.\n\n        If the field has include_lengths=True, a tensor of lengths will be\n        included in the return value.\n\n        Arguments:\n            arr (List[List[str]], or tuple of (List[List[str]], List[int])):\n                List of tokenized and padded examples, or tuple of List of\n                tokenized and padded examples and List of lengths of each\n                example if self.include_lengths is True.\n            device (str or torch.device): A string or instance of `torch.device`\n                specifying which device the Variables are going to be created on.\n                If left as default, the tensors will be created on cpu. Default: None.\n        """"""\n        if self.include_lengths and not isinstance(arr, tuple):\n            raise ValueError(""Field has include_lengths set to True, but ""\n                             ""input data is not a tuple of ""\n                             ""(data batch, batch lengths)."")\n        if isinstance(arr, tuple):\n            arr, lengths = arr\n            lengths = torch.tensor(lengths, dtype=self.dtype, device=device)\n\n        if self.use_vocab:\n            if self.sequential:\n                arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\n            else:\n                arr = [self.vocab.stoi[x] for x in arr]\n\n            if self.postprocessing is not None:\n                arr = self.postprocessing(arr, self.vocab)\n        else:\n            if self.dtype not in self.dtypes:\n                raise ValueError(\n                    ""Specified Field dtype {} can not be used with ""\n                    ""use_vocab=False because we do not know how to numericalize it. ""\n                    ""Please raise an issue at ""\n                    ""https://github.com/pytorch/text/issues"".format(self.dtype))\n            numericalization_func = self.dtypes[self.dtype]\n            # It doesn\'t make sense to explicitly coerce to a numeric type if\n            # the data is sequential, since it\'s unclear how to coerce padding tokens\n            # to a numeric type.\n            if not self.sequential:\n                arr = [numericalization_func(x) if isinstance(x, str)\n                       else x for x in arr]\n            if self.postprocessing is not None:\n                arr = self.postprocessing(arr, None)\n\n        var = torch.tensor(arr, dtype=self.dtype, device=device)\n\n        if self.sequential and not self.batch_first:\n            var.t_()\n        if self.sequential:\n            var = var.contiguous()\n\n        if self.include_lengths:\n            return var, lengths\n        return var\n\n\nclass ReversibleField(Field):\n    def __init__(self, **kwargs):\n        if kwargs.get(\'tokenize\') is list:\n            self.use_revtok = False\n        else:\n            self.use_revtok = True\n        if kwargs.get(\'tokenize\') is None:\n            kwargs[\'tokenize\'] = \'revtok\'\n        if \'unk_token\' not in kwargs:\n            kwargs[\'unk_token\'] = \' UNK \'\n        super(ReversibleField, self).__init__(**kwargs)\n\n    def reverse(self, batch):\n        if self.use_revtok:\n            try:\n                import revtok\n            except ImportError:\n                print(""Please install revtok."")\n                raise\n        if not self.batch_first:\n            batch = batch.t()\n        with torch.cuda.device_of(batch):\n            batch = batch.tolist()\n        batch = [[self.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n\n        def trim(s, t):\n            sentence = []\n            for w in s:\n                if w == t:\n                    break\n                sentence.append(w)\n            return sentence\n\n        batch = [trim(ex, self.eos_token) for ex in batch]  # trim past frst eos\n\n        def filter_special(tok):\n            return tok not in (self.init_token, self.pad_token)\n\n        batch = [filter(filter_special, ex) for ex in batch]\n        if self.use_revtok:\n            return [revtok.detokenize(ex) for ex in batch]\n        return [\'\'.join(ex) for ex in batch]\n\n\nclass SubwordField(ReversibleField):\n    vocab_cls = SubwordVocab\n\n    def __init__(self, **kwargs):\n        kwargs[\'tokenize\'] = \'subword\'\n        if \'unk_token\' not in kwargs:\n            kwargs[\'unk_token\'] = \'\xef\xbf\xbd\'\n        super(SubwordField, self).__init__(**kwargs)\n\n    def segment(self, *args):\n        """"""Segment one or more datasets with this subword field.\n\n        Arguments:\n            Positional arguments: Dataset objects or other indexable\n                mutable sequences to segment. If a Dataset object is provided,\n                all columns corresponding to this field are used; individual\n                columns can also be provided directly.\n        """"""\n        sources = []\n        for arg in args:\n            if isinstance(arg, Dataset):\n                sources += [getattr(arg, name) for name, field in\n                            arg.fields.items() if field is self]\n            else:\n                sources.append(arg)\n        for data in sources:\n            for x in tqdm(data, \'segmenting\'):\n                x[:] = self.vocab.segment(x)\n\n\nclass NestedField(Field):\n    """"""A nested field.\n\n    A nested field holds another field (called *nesting field*), accepts an untokenized\n    string or a list string tokens and groups and treats them as one field as described\n    by the nesting field. Every token will be preprocessed, padded, etc. in the manner\n    specified by the nesting field. Note that this means a nested field always has\n    ``sequential=True``. The two fields\' vocabularies will be shared. Their\n    numericalization results will be stacked into a single tensor. And NestedField will\n    share the same include_lengths with nesting_field, so one shouldn\'t specify the\n    include_lengths in the nesting_field. This field is\n    primarily used to implement character embeddings. See ``tests/data/test_field.py``\n    for examples on how to use this field.\n\n    Arguments:\n        nesting_field (Field): A field contained in this nested field.\n        use_vocab (bool): Whether to use a Vocab object. If False, the data in this\n            field should already be numerical. Default: ``True``.\n        init_token (str): A token that will be prepended to every example using this\n            field, or None for no initial token. Default: ``None``.\n        eos_token (str): A token that will be appended to every example using this\n            field, or None for no end-of-sentence token. Default: ``None``.\n        fix_length (int): A fixed length that all examples using this field will be\n            padded to, or ``None`` for flexible sequence lengths. Default: ``None``.\n        dtype: The torch.dtype class that represents a batch of examples\n            of this kind of data. Default: ``torch.long``.\n        preprocessing (Pipeline): The Pipeline that will be applied to examples\n            using this field after tokenizing but before numericalizing. Many\n            Datasets replace this attribute with a custom preprocessor.\n            Default: ``None``.\n        postprocessing (Pipeline): A Pipeline that will be applied to examples using\n            this field after numericalizing but before the numbers are turned\n            into a Tensor. The pipeline function takes the batch as a list, and\n            the field\'s Vocab. Default: ``None``.\n        include_lengths: Whether to return a tuple of a padded minibatch and\n            a list containing the lengths of each examples, or just a padded\n            minibatch. Default: False.\n        tokenize: The function used to tokenize strings using this field into\n            sequential examples. If ""spacy"", the SpaCy tokenizer is\n            used. If a non-serializable function is passed as an argument,\n            the field will not be able to be serialized. Default: string.split.\n        tokenizer_language: The language of the tokenizer to be constructed.\n            Various languages currently supported only in SpaCy.\n        pad_token (str): The string token used as padding. If ``nesting_field`` is\n            sequential, this will be set to its ``pad_token``. Default: ``""<pad>""``.\n        pad_first (bool): Do the padding of the sequence at the beginning. Default:\n            ``False``.\n    """"""\n\n    def __init__(self, nesting_field, use_vocab=True, init_token=None, eos_token=None,\n                 fix_length=None, dtype=torch.long, preprocessing=None,\n                 postprocessing=None, tokenize=None, tokenizer_language=\'en\',\n                 include_lengths=False, pad_token=\'<pad>\',\n                 pad_first=False, truncate_first=False):\n        if isinstance(nesting_field, NestedField):\n            raise ValueError(\'nesting field must not be another NestedField\')\n        if nesting_field.include_lengths:\n            raise ValueError(\'nesting field cannot have include_lengths=True\')\n\n        if nesting_field.sequential:\n            pad_token = nesting_field.pad_token\n        super(NestedField, self).__init__(\n            use_vocab=use_vocab,\n            init_token=init_token,\n            eos_token=eos_token,\n            fix_length=fix_length,\n            dtype=dtype,\n            preprocessing=preprocessing,\n            postprocessing=postprocessing,\n            lower=nesting_field.lower,\n            tokenize=tokenize,\n            tokenizer_language=tokenizer_language,\n            batch_first=True,\n            pad_token=pad_token,\n            unk_token=nesting_field.unk_token,\n            pad_first=pad_first,\n            truncate_first=truncate_first,\n            include_lengths=include_lengths\n        )\n        self.nesting_field = nesting_field\n        # in case the user forget to do that\n        self.nesting_field.batch_first = True\n\n    def preprocess(self, xs):\n        """"""Preprocess a single example.\n\n        Firstly, tokenization and the supplied preprocessing pipeline is applied. Since\n        this field is always sequential, the result is a list. Then, each element of\n        the list is preprocessed using ``self.nesting_field.preprocess`` and the resulting\n        list is returned.\n\n        Arguments:\n            xs (list or str): The input to preprocess.\n\n        Returns:\n            list: The preprocessed list.\n        """"""\n        return [self.nesting_field.preprocess(x)\n                for x in super(NestedField, self).preprocess(xs)]\n\n    def pad(self, minibatch):\n        """"""Pad a batch of examples using this field.\n\n        If ``self.nesting_field.sequential`` is ``False``, each example in the batch must\n        be a list of string tokens, and pads them as if by a ``Field`` with\n        ``sequential=True``. Otherwise, each example must be a list of list of tokens.\n        Using ``self.nesting_field``, pads the list of tokens to\n        ``self.nesting_field.fix_length`` if provided, or otherwise to the length of the\n        longest list of tokens in the batch. Next, using this field, pads the result by\n        filling short examples with ``self.nesting_field.pad_token``.\n\n        Example:\n            >>> import pprint\n            >>> pp = pprint.PrettyPrinter(indent=4)\n            >>>\n            >>> nesting_field = Field(pad_token=\'<c>\', init_token=\'<w>\', eos_token=\'</w>\')\n            >>> field = NestedField(nesting_field, init_token=\'<s>\', eos_token=\'</s>\')\n            >>> minibatch = [\n            ...     [list(\'john\'), list(\'loves\'), list(\'mary\')],\n            ...     [list(\'mary\'), list(\'cries\')],\n            ... ]\n            >>> padded = field.pad(minibatch)\n            >>> pp.pprint(padded)\n            [   [   [\'<w>\', \'<s>\', \'</w>\', \'<c>\', \'<c>\', \'<c>\', \'<c>\'],\n                    [\'<w>\', \'j\', \'o\', \'h\', \'n\', \'</w>\', \'<c>\'],\n                    [\'<w>\', \'l\', \'o\', \'v\', \'e\', \'s\', \'</w>\'],\n                    [\'<w>\', \'m\', \'a\', \'r\', \'y\', \'</w>\', \'<c>\'],\n                    [\'<w>\', \'</s>\', \'</w>\', \'<c>\', \'<c>\', \'<c>\', \'<c>\']],\n                [   [\'<w>\', \'<s>\', \'</w>\', \'<c>\', \'<c>\', \'<c>\', \'<c>\'],\n                    [\'<w>\', \'m\', \'a\', \'r\', \'y\', \'</w>\', \'<c>\'],\n                    [\'<w>\', \'c\', \'r\', \'i\', \'e\', \'s\', \'</w>\'],\n                    [\'<w>\', \'</s>\', \'</w>\', \'<c>\', \'<c>\', \'<c>\', \'<c>\'],\n                    [\'<c>\', \'<c>\', \'<c>\', \'<c>\', \'<c>\', \'<c>\', \'<c>\']]]\n\n        Arguments:\n            minibatch (list): Each element is a list of string if\n                ``self.nesting_field.sequential`` is ``False``, a list of list of string\n                otherwise.\n\n        Returns:\n            list: The padded minibatch. or (padded, sentence_lens, word_lengths)\n        """"""\n        minibatch = list(minibatch)\n        if not self.nesting_field.sequential:\n            return super(NestedField, self).pad(minibatch)\n\n        # Save values of attributes to be monkeypatched\n        old_pad_token = self.pad_token\n        old_init_token = self.init_token\n        old_eos_token = self.eos_token\n        old_fix_len = self.nesting_field.fix_length\n        # Monkeypatch the attributes\n        if self.nesting_field.fix_length is None:\n            max_len = max(len(xs) for ex in minibatch for xs in ex)\n            fix_len = max_len + 2 - (self.nesting_field.init_token,\n                                     self.nesting_field.eos_token).count(None)\n            self.nesting_field.fix_length = fix_len\n        self.pad_token = [self.pad_token] * self.nesting_field.fix_length\n        if self.init_token is not None:\n            # self.init_token = self.nesting_field.pad([[self.init_token]])[0]\n            self.init_token = [self.init_token]\n        if self.eos_token is not None:\n            # self.eos_token = self.nesting_field.pad([[self.eos_token]])[0]\n            self.eos_token = [self.eos_token]\n        # Do padding\n        old_include_lengths = self.include_lengths\n        self.include_lengths = True\n        self.nesting_field.include_lengths = True\n        padded, sentence_lengths = super(NestedField, self).pad(minibatch)\n        padded_with_lengths = [self.nesting_field.pad(ex) for ex in padded]\n        word_lengths = []\n        final_padded = []\n        max_sen_len = len(padded[0])\n        for (pad, lens), sentence_len in zip(padded_with_lengths, sentence_lengths):\n            if sentence_len == max_sen_len:\n                lens = lens\n                pad = pad\n            elif self.pad_first:\n                lens[:(max_sen_len - sentence_len)] = (\n                    [0] * (max_sen_len - sentence_len))\n                pad[:(max_sen_len - sentence_len)] = (\n                    [self.pad_token] * (max_sen_len - sentence_len))\n            else:\n                lens[-(max_sen_len - sentence_len):] = (\n                    [0] * (max_sen_len - sentence_len))\n                pad[-(max_sen_len - sentence_len):] = (\n                    [self.pad_token] * (max_sen_len - sentence_len))\n            word_lengths.append(lens)\n            final_padded.append(pad)\n        padded = final_padded\n\n        # Restore monkeypatched attributes\n        self.nesting_field.fix_length = old_fix_len\n        self.pad_token = old_pad_token\n        self.init_token = old_init_token\n        self.eos_token = old_eos_token\n        self.include_lengths = old_include_lengths\n        if self.include_lengths:\n            return padded, sentence_lengths, word_lengths\n        return padded\n\n    def build_vocab(self, *args, **kwargs):\n        """"""Construct the Vocab object for nesting field and combine it with this field\'s vocab.\n\n        Arguments:\n            Positional arguments: Dataset objects or other iterable data\n                sources from which to construct the Vocab object that\n                represents the set of possible values for the nesting field. If\n                a Dataset object is provided, all columns corresponding\n                to this field are used; individual columns can also be\n                provided directly.\n            Remaining keyword arguments: Passed to the constructor of Vocab.\n        """"""\n        sources = []\n        for arg in args:\n            if isinstance(arg, Dataset):\n                sources.extend(\n                    [getattr(arg, name) for name, field in arg.fields.items()\n                     if field is self]\n                )\n            else:\n                sources.append(arg)\n\n        flattened = []\n        for source in sources:\n            flattened.extend(source)\n        old_vectors = None\n        old_unk_init = None\n        old_vectors_cache = None\n        if ""vectors"" in kwargs.keys():\n            old_vectors = kwargs[""vectors""]\n            kwargs[""vectors""] = None\n        if ""unk_init"" in kwargs.keys():\n            old_unk_init = kwargs[""unk_init""]\n            kwargs[""unk_init""] = None\n        if ""vectors_cache"" in kwargs.keys():\n            old_vectors_cache = kwargs[""vectors_cache""]\n            kwargs[""vectors_cache""] = None\n        # just build vocab and does not load vector\n        self.nesting_field.build_vocab(*flattened, **kwargs)\n        super(NestedField, self).build_vocab()\n        self.vocab.extend(self.nesting_field.vocab)\n        self.vocab.freqs = self.nesting_field.vocab.freqs.copy()\n        if old_vectors is not None:\n            self.vocab.load_vectors(old_vectors,\n                                    unk_init=old_unk_init, cache=old_vectors_cache)\n\n        self.nesting_field.vocab = self.vocab\n\n    def numericalize(self, arrs, device=None):\n        """"""Convert a padded minibatch into a variable tensor.\n\n        Each item in the minibatch will be numericalized independently and the resulting\n        tensors will be stacked at the first dimension.\n\n        Arguments:\n            arr (List[List[str]]): List of tokenized and padded examples.\n            device (str or torch.device): A string or instance of `torch.device`\n                specifying which device the Variables are going to be created on.\n                If left as default, the tensors will be created on cpu. Default: None.\n        """"""\n        numericalized = []\n        self.nesting_field.include_lengths = False\n        if self.include_lengths:\n            arrs, sentence_lengths, word_lengths = arrs\n\n        for arr in arrs:\n            numericalized_ex = self.nesting_field.numericalize(\n                arr, device=device)\n            numericalized.append(numericalized_ex)\n        padded_batch = torch.stack(numericalized)\n\n        self.nesting_field.include_lengths = True\n        if self.include_lengths:\n            sentence_lengths = \\\n                torch.tensor(sentence_lengths, dtype=self.dtype, device=device)\n            word_lengths = torch.tensor(word_lengths, dtype=self.dtype, device=device)\n            return (padded_batch, sentence_lengths, word_lengths)\n        return padded_batch\n\n\nclass LabelField(Field):\n    """"""A Label field.\n\n    A label field is a shallow wrapper around a standard field designed to hold labels\n    for a classification task. Its only use is to set the unk_token and sequential to\n    `None` by default.\n    """"""\n\n    def __init__(self, **kwargs):\n        # whichever value is set for sequential, unk_token, and is_target\n        # will be overwritten\n        kwargs[\'sequential\'] = False\n        kwargs[\'unk_token\'] = None\n        kwargs[\'is_target\'] = True\n\n        super(LabelField, self).__init__(**kwargs)\n'"
torchtext/data/functional.py,2,"b'import re\n\nimport torch\n\n\n__all__ = [\n    ""generate_sp_model"", ""load_sp_model"",\n    ""sentencepiece_numericalizer"", ""sentencepiece_tokenizer"",\n    ""numericalize_tokens_from_iterator""\n]\n\n\n""""""\nThis file contains experimental functionality.\nAll of these are experimental, unstable, and subject to change or deletion.\n""""""\n\n\ndef generate_sp_model(filename, vocab_size=20000,\n                      model_type=""unigram"",\n                      model_prefix=\'m_user\'):\n    r""""""Train a SentencePiece tokenizer.\n\n    Arguments:\n        filename: the data file for training SentencePiece model.\n        vocab_size: the size of vocabulary (Default: 20,000).\n        model_type: the type of SentencePiece model, including unigram,\n            bpe, char, word.\n        model_prefix: the prefix of the files saving model and vocab.\n\n    Outputs:\n        The model and vocab are saved in two separate files with\n            model_prefix.\n\n    Examples:\n        >>> from torchtext.data.functional import generate_sp_model\n        >>> generate_sp_model(\'test.csv\', vocab_size=23456, model_prefix=\'spm_user\')\n    """"""\n    torch.ops.torchtext.generate_sp_model(filename, vocab_size, model_type, model_prefix)\n\n\ndef load_sp_model(spm_path):\n    r""""""Load a  sentencepiece model for file.\n\n    Arguments:\n        spm_path: the file path saving the sentencepiece model.\n\n    Outputs:\n        output: a SentencePiece model.\n\n    Examples:\n        >>> from torchtext.data.functional import load_sp_model\n        >>> sp_model = load_sp_model(""m_user.model"")\n    """"""\n    return torch.ops.torchtext.load_sp_model(spm_path)\n\n\ndef sentencepiece_numericalizer(sp_model):\n    r""""""A sentencepiece model to numericalize a text sentence into\n       a generator over the ids.\n\n    Arguments:\n        sp_model: a SentencePiece model.\n\n    Outputs:\n        output: a generator with the input of text sentence and the output of the\n            corresponding ids based on SentencePiece model.\n\n    Examples:\n        >>> from torchtext.data.functional import sentencepiece_numericalizer\n        >>> sp_id_generator = sentencepiece_numericalizer(sp_model)\n        >>> list_a = [""sentencepiece encode as pieces"", ""examples to   try!""]\n        >>> list(sp_id_generator(list_a))\n            [[9858, 9249, 1629, 1305, 1809, 53, 842],\n             [2347, 13, 9, 150, 37]]\n    """"""\n\n    def _internal_func(txt_iter):\n        for line in txt_iter:\n            yield sp_model.EncodeAsIds(line)\n    return _internal_func\n\n\ndef sentencepiece_tokenizer(sp_model):\n    r""""""A sentencepiece model to tokenize a text sentence into\n       a generator over the tokens.\n\n    Arguments:\n        sp_model: a SentencePiece model.\n\n    Outputs:\n        output: a generator with the input of text sentence and the output of the\n            corresponding tokens based on SentencePiece model.\n\n    Examples:\n        >>> from torchtext.data.functional import sentencepiece_tokenizer\n        >>> sp_tokens_generator = sentencepiece_tokenizer(sp_model)\n        >>> list_a = [""sentencepiece encode as pieces"", ""examples to   try!""]\n        >>> list(sp_tokens_generator(list_a))\n            [[\'_sentence\', \'piece\', \'_en\', \'co\', \'de\', \'_as\', \'_pieces\'],\n             [\'_example\', \'s\', \'_to\', \'_try\', \'!\']]\n    """"""\n\n    def _internal_func(txt_iter):\n        for line in txt_iter:\n            yield sp_model.EncodeAsPieces(line)\n    return _internal_func\n\n\ndef custom_replace(replace_pattern):\n    r""""""A transform to convert text string.\n\n    Examples:\n        >>> from torchtext.data.functional import custom_replace\n        >>> custom_replace_transform = custom_replace([(r\'S\', \'s\'), (r\'\\s+\', \' \')])\n        >>> list_a = [""Sentencepiece encode  aS  pieces"", ""exampleS to   try!""]\n        >>> list(custom_replace_transform(list_a))\n            [\'sentencepiece encode as pieces\', \'examples to try!\']\n    """"""\n\n    _patterns = list((re.compile(p), r)\n                     for (p, r) in replace_pattern)\n\n    def _internal_func(txt_iter):\n        for line in txt_iter:\n            for pattern_re, replaced_str in _patterns:\n                line = pattern_re.sub(replaced_str, line)\n            yield line\n    return _internal_func\n\n\ndef simple_space_split(iterator):\n    r""""""A transform to split text string by spaces.\n\n    Examples:\n        >>> from torchtext.data.functional import simple_space_split\n        >>> list_a = [""Sentencepiece encode as pieces"", ""example to try!""]\n        >>> list(simple_space_split(list_a))\n            [[\'Sentencepiece\', \'encode\', \'as\', \'pieces\'], [\'example\', \'to\', \'try!\']]\n    """"""\n\n    for line in iterator:\n        yield line.split()\n\n\ndef numericalize_tokens_from_iterator(vocab, iterator, removed_tokens=None):\n    r""""""Yield a list of ids from an token iterator with a vocab.\n\n    Arguments:\n        vocab: the vocabulary convert token into id.\n        iterator: the iterator yield a list of tokens.\n        removed_tokens: removed tokens from output dataset (Default: None)\n\n    Examples:\n        >>> from torchtext.data.functional import simple_space_split\n        >>> from torchtext.data.functional import numericalize_tokens_from_iterator\n        >>> vocab = {\'Sentencepiece\' : 0, \'encode\' : 1, \'as\' : 2, \'pieces\' : 3}\n        >>> ids_iter = numericalize_tokens_from_iterator(vocab,\n        >>>                               simple_space_split([""Sentencepiece as pieces"",\n        >>>                                                   ""as pieces""]))\n        >>> for ids in ids_iter:\n        >>>     print([num for num in ids])\n        >>> [0, 2, 3]\n        >>> [2, 3]\n    """"""\n    for tokens in iterator:\n        if removed_tokens is None:\n            yield iter(vocab[token] for token in tokens)\n        else:\n            yield iter(map(lambda x: vocab[x],\n                       filter(lambda x: x not in removed_tokens, tokens)))\n'"
torchtext/data/iterator.py,5,"b'import math\nimport random\n\nimport logging\n\nimport torch\nfrom .utils import RandomShuffler\nfrom .batch import Batch\nfrom .dataset import Dataset\n\nlogger = logging.getLogger(__name__)\n\n\nclass Iterator(object):\n    """"""Defines an iterator that loads batches of data from a Dataset.\n\n    Attributes:\n        dataset: The Dataset object to load Examples from.\n        batch_size: Batch size.\n        batch_size_fn: Function of three arguments (new example to add, current\n            count of examples in the batch, and current effective batch size)\n            that returns the new effective batch size resulting from adding\n            that example to a batch. This is useful for dynamic batching, where\n            this function would add to the current effective batch size the\n            number of tokens in the new example.\n        sort_key: A key to use for sorting examples in order to batch together\n            examples with similar lengths and minimize padding. The sort_key\n            provided to the Iterator constructor overrides the sort_key\n            attribute of the Dataset, or defers to it if None.\n        train: Whether the iterator represents a train set.\n        repeat: Whether to repeat the iterator for multiple epochs. Default: False.\n        shuffle: Whether to shuffle examples between epochs.\n        sort: Whether to sort examples according to self.sort_key.\n            Note that shuffle and sort default to train and (not train).\n        sort_within_batch: Whether to sort (in descending order according to\n            self.sort_key) within each batch. If None, defaults to self.sort.\n            If self.sort is True and this is False, the batch is left in the\n            original (ascending) sorted order.\n        device (str or `torch.device`): A string or instance of `torch.device`\n            specifying which device the Variables are going to be created on.\n            If left as default, the tensors will be created on cpu. Default: None.\n    """"""\n\n    def __init__(self, dataset, batch_size, sort_key=None, device=None,\n                 batch_size_fn=None, train=True,\n                 repeat=False, shuffle=None, sort=None,\n                 sort_within_batch=None):\n        self.batch_size, self.train, self.dataset = batch_size, train, dataset\n        self.batch_size_fn = batch_size_fn\n        self.iterations = 0\n        self.repeat = repeat\n        self.shuffle = train if shuffle is None else shuffle\n        self.sort = not train if sort is None else sort\n\n        if sort_within_batch is None:\n            self.sort_within_batch = self.sort\n        else:\n            self.sort_within_batch = sort_within_batch\n        if sort_key is None:\n            self.sort_key = dataset.sort_key\n        else:\n            self.sort_key = sort_key\n\n        if isinstance(device, int):\n            logger.warning(""The `device` argument should be set by using `torch.device`""\n                           + "" or passing a string as an argument. This behavior will be""\n                           + "" deprecated soon and currently defaults to cpu."")\n            device = None\n\n        if device is None:\n            device = torch.device(\'cpu\')\n        elif isinstance(device, str):\n            device = torch.device(device)\n\n        self.device = device\n        self.random_shuffler = RandomShuffler()\n\n        # For state loading/saving only\n        self._iterations_this_epoch = 0\n        self._random_state_this_epoch = None\n        self._restored_from_state = False\n\n    @classmethod\n    def splits(cls, datasets, batch_sizes=None, **kwargs):\n        """"""Create Iterator objects for multiple splits of a dataset.\n\n        Arguments:\n            datasets: Tuple of Dataset objects corresponding to the splits. The\n                first such object should be the train set.\n            batch_sizes: Tuple of batch sizes to use for the different splits,\n                or None to use the same batch_size for all splits.\n            Remaining keyword arguments: Passed to the constructor of the\n                iterator class being used.\n        """"""\n        if batch_sizes is None:\n            batch_sizes = [kwargs.pop(\'batch_size\')] * len(datasets)\n        ret = []\n        for i in range(len(datasets)):\n            train = i == 0\n            ret.append(cls(\n                datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))\n        return tuple(ret)\n\n    def data(self):\n        """"""Return the examples in the dataset in order, sorted, or shuffled.""""""\n        if self.sort:\n            xs = sorted(self.dataset, key=self.sort_key)\n        elif self.shuffle:\n            xs = [self.dataset[i] for i in self.random_shuffler(range(len(self.dataset)))]\n        else:\n            xs = self.dataset\n        return xs\n\n    def init_epoch(self):\n        """"""Set up the batch generator for a new epoch.""""""\n\n        if self._restored_from_state:\n            self.random_shuffler.random_state = self._random_state_this_epoch\n        else:\n            self._random_state_this_epoch = self.random_shuffler.random_state\n\n        self.create_batches()\n\n        if self._restored_from_state:\n            self._restored_from_state = False\n        else:\n            self._iterations_this_epoch = 0\n\n        if not self.repeat:\n            self.iterations = 0\n\n    def create_batches(self):\n        self.batches = batch(self.data(), self.batch_size, self.batch_size_fn)\n\n    @property\n    def epoch(self):\n        return math.floor(self.iterations / len(self))\n\n    def __len__(self):\n        if self.batch_size_fn is not None:\n            raise NotImplementedError\n        return math.ceil(len(self.dataset) / self.batch_size)\n\n    def __iter__(self):\n        while True:\n            self.init_epoch()\n            for idx, minibatch in enumerate(self.batches):\n                # fast-forward if loaded from state\n                if self._iterations_this_epoch > idx:\n                    continue\n                self.iterations += 1\n                self._iterations_this_epoch += 1\n                if self.sort_within_batch:\n                    # NOTE: `rnn.pack_padded_sequence` requires that a minibatch\n                    # be sorted by decreasing order, which requires reversing\n                    # relative to typical sort keys\n                    if self.sort:\n                        minibatch.reverse()\n                    else:\n                        minibatch.sort(key=self.sort_key, reverse=True)\n                yield Batch(minibatch, self.dataset, self.device)\n            if not self.repeat:\n                return\n\n    def state_dict(self):\n        return {\n            ""iterations"": self.iterations,\n            ""iterations_this_epoch"": self._iterations_this_epoch,\n            ""random_state_this_epoch"": self._random_state_this_epoch}\n\n    def load_state_dict(self, state_dict):\n        self.iterations = state_dict[""iterations""]\n        self._iterations_this_epoch = state_dict[""iterations_this_epoch""]\n        self._random_state_this_epoch = state_dict[""random_state_this_epoch""]\n        self._restored_from_state = True\n\n\nclass BPTTIterator(Iterator):\n    """"""Defines an iterator for language modeling tasks that use BPTT.\n\n    Provides contiguous streams of examples together with targets that are\n    one timestep further forward, for language modeling training with\n    backpropagation through time (BPTT). Expects a Dataset with a single\n    example and a single field called \'text\' and produces Batches with text and\n    target attributes.\n\n    Attributes:\n        dataset: The Dataset object to load Examples from.\n        batch_size: Batch size.\n        bptt_len: Length of sequences for backpropagation through time.\n        sort_key: A key to use for sorting examples in order to batch together\n            examples with similar lengths and minimize padding. The sort_key\n            provided to the Iterator constructor overrides the sort_key\n            attribute of the Dataset, or defers to it if None.\n        train: Whether the iterator represents a train set.\n        repeat: Whether to repeat the iterator for multiple epochs. Default: False.\n        shuffle: Whether to shuffle examples between epochs.\n        sort: Whether to sort examples according to self.sort_key.\n            Note that shuffle and sort default to train and (not train).\n        device (str or torch.device): A string or instance of `torch.device`\n            specifying which device the Variables are going to be created on.\n            If left as default, the tensors will be created on cpu. Default: None.\n    """"""\n\n    def __init__(self, dataset, batch_size, bptt_len, **kwargs):\n        self.bptt_len = bptt_len\n        super(BPTTIterator, self).__init__(dataset, batch_size, **kwargs)\n\n    def __len__(self):\n        return math.ceil((len(self.dataset[0].text) / self.batch_size - 1)\n                         / self.bptt_len)\n\n    def __iter__(self):\n        text = self.dataset[0].text\n        TEXT = self.dataset.fields[\'text\']\n        TEXT.eos_token = None\n        text = text + ([TEXT.pad_token] * int(math.ceil(len(text) / self.batch_size)\n                                              * self.batch_size - len(text)))\n        data = TEXT.numericalize(\n            [text], device=self.device)\n        data = data.view(self.batch_size, -1).t().contiguous()\n        dataset = Dataset(examples=self.dataset.examples, fields=[\n            (\'text\', TEXT), (\'target\', TEXT)])\n        while True:\n            for i in range(0, len(self) * self.bptt_len, self.bptt_len):\n                self.iterations += 1\n                seq_len = min(self.bptt_len, len(data) - i - 1)\n                batch_text = data[i:i + seq_len]\n                batch_target = data[i + 1:i + 1 + seq_len]\n                if TEXT.batch_first:\n                    batch_text = batch_text.t().contiguous()\n                    batch_target = batch_target.t().contiguous()\n                yield Batch.fromvars(\n                    dataset, self.batch_size,\n                    text=batch_text,\n                    target=batch_target)\n            if not self.repeat:\n                return\n\n\nclass BucketIterator(Iterator):\n    """"""Defines an iterator that batches examples of similar lengths together.\n\n    Minimizes amount of padding needed while producing freshly shuffled\n    batches for each new epoch. See pool for the bucketing procedure used.\n    """"""\n\n    def create_batches(self):\n        if self.sort:\n            self.batches = batch(self.data(), self.batch_size,\n                                 self.batch_size_fn)\n        else:\n            self.batches = pool(self.data(), self.batch_size,\n                                self.sort_key, self.batch_size_fn,\n                                random_shuffler=self.random_shuffler,\n                                shuffle=self.shuffle,\n                                sort_within_batch=self.sort_within_batch)\n\n\ndef batch(data, batch_size, batch_size_fn=None):\n    """"""Yield elements from data in chunks of batch_size.""""""\n    if batch_size_fn is None:\n        def batch_size_fn(new, count, sofar):\n            return count\n    minibatch, size_so_far = [], 0\n    for ex in data:\n        minibatch.append(ex)\n        size_so_far = batch_size_fn(ex, len(minibatch), size_so_far)\n        if size_so_far == batch_size:\n            yield minibatch\n            minibatch, size_so_far = [], 0\n        elif size_so_far > batch_size:\n            yield minibatch[:-1]\n            minibatch, size_so_far = minibatch[-1:], batch_size_fn(ex, 1, 0)\n    if minibatch:\n        yield minibatch\n\n\ndef pool(data, batch_size, key, batch_size_fn=lambda new, count, sofar: count,\n         random_shuffler=None, shuffle=False, sort_within_batch=False):\n    """"""Sort within buckets, then batch, then shuffle batches.\n\n    Partitions data into chunks of size 100*batch_size, sorts examples within\n    each chunk using sort_key, then batch these examples and shuffle the\n    batches.\n    """"""\n    if random_shuffler is None:\n        random_shuffler = random.shuffle\n    for p in batch(data, batch_size * 100, batch_size_fn):\n        p_batch = batch(sorted(p, key=key), batch_size, batch_size_fn) \\\n            if sort_within_batch \\\n            else batch(p, batch_size, batch_size_fn)\n        if shuffle:\n            for b in random_shuffler(list(p_batch)):\n                yield b\n        else:\n            for b in list(p_batch):\n                yield b\n'"
torchtext/data/metrics.py,5,"b'import math\nimport collections\nimport torch\nfrom torchtext.data.utils import ngrams_iterator\n\n\ndef _compute_ngram_counter(tokens, max_n):\n    """""" Create a Counter with a count of unique n-grams in the tokens list\n\n    Arguments:\n        tokens: a list of tokens (typically a string split on whitespaces)\n        max_n: the maximum order of n-gram wanted\n\n    Outputs:\n        output: a collections.Counter object with the unique n-grams and their\n            associated count\n\n    Examples:\n        >>> from torchtext.data.metrics import _compute_ngram_counter\n        >>> tokens = [\'me\', \'me\', \'you\']\n        >>> _compute_ngram_counter(tokens, 2)\n            Counter({(\'me\',): 2,\n             (\'you\',): 1,\n             (\'me\', \'me\'): 1,\n             (\'me\', \'you\'): 1,\n             (\'me\', \'me\', \'you\'): 1})\n    """"""\n    assert max_n > 0\n    ngrams_counter = collections.Counter(tuple(x.split(\' \'))\n                                         for x in ngrams_iterator(tokens, max_n))\n\n    return ngrams_counter\n\n\ndef bleu_score(candidate_corpus, references_corpus, max_n=4, weights=[0.25] * 4):\n    """"""Computes the BLEU score between a candidate translation corpus and a references\n    translation corpus. Based on https://www.aclweb.org/anthology/P02-1040.pdf\n\n    Arguments:\n        candidate_corpus: an iterable of candidate translations. Each translation is an\n            iterable of tokens\n        references_corpus: an iterable of iterables of reference translations. Each\n            translation is an iterable of tokens\n        max_n: the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams,\n            bigrams and trigrams\n        weights: a list of weights used for each n-gram category (uniform by default)\n\n    Examples:\n        >>> from torchtext.data.metrics import bleu_score\n        >>> candidate_corpus = [[\'My\', \'full\', \'pytorch\', \'test\'], [\'Another\', \'Sentence\']]\n        >>> references_corpus = [[[\'My\', \'full\', \'pytorch\', \'test\'], [\'Completely\', \'Different\']], [[\'No\', \'Match\']]]\n        >>> bleu_score(candidate_corpus, references_corpus)\n            0.8408964276313782\n    """"""\n\n    assert max_n == len(weights), \'Length of the ""weights"" list has be equal to max_n\'\n    assert len(candidate_corpus) == len(references_corpus),\\\n        \'The length of candidate and reference corpus should be the same\'\n\n    clipped_counts = torch.zeros(max_n)\n    total_counts = torch.zeros(max_n)\n    weights = torch.tensor(weights)\n\n    candidate_len = 0.0\n    refs_len = 0.0\n\n    for (candidate, refs) in zip(candidate_corpus, references_corpus):\n        candidate_len += len(candidate)\n\n        # Get the length of the reference that\'s closest in length to the candidate\n        refs_len_list = [float(len(ref)) for ref in refs]\n        refs_len += min(refs_len_list, key=lambda x: abs(len(candidate) - x))\n\n        reference_counters = _compute_ngram_counter(refs[0], max_n)\n        for ref in refs[1:]:\n            reference_counters = reference_counters | _compute_ngram_counter(ref, max_n)\n\n        candidate_counter = _compute_ngram_counter(candidate, max_n)\n\n        clipped_counter = candidate_counter & reference_counters\n\n        for ngram in clipped_counter:\n            clipped_counts[len(ngram) - 1] += clipped_counter[ngram]\n\n        for ngram in candidate_counter:  # TODO: no need to loop through the whole counter\n            total_counts[len(ngram) - 1] += candidate_counter[ngram]\n\n    if min(clipped_counts) == 0:\n        return 0.0\n    else:\n        pn = clipped_counts / total_counts\n        log_pn = weights * torch.log(pn)\n        score = torch.exp(sum(log_pn))\n\n        bp = math.exp(min(1 - refs_len / candidate_len, 0))\n\n        return bp * score.item()\n'"
torchtext/data/pipeline.py,0,"b'class Pipeline(object):\n    """"""Defines a pipeline for transforming sequence data.\n\n    The input is assumed to be utf-8 encoded `str`.\n\n    Attributes:\n        convert_token: The function to apply to input sequence data.\n        pipes: The Pipelines that will be applied to input sequence\n            data in order.\n    """"""\n\n    def __init__(self, convert_token=None):\n        """"""Create a pipeline.\n\n        Arguments:\n            convert_token: The function to apply to input sequence data.\n                If None, the identity function is used. Default: None\n        """"""\n        if convert_token is None:\n            self.convert_token = Pipeline.identity\n        elif callable(convert_token):\n            self.convert_token = convert_token\n        else:\n            raise ValueError(""Pipeline input convert_token {} is not None ""\n                             ""or callable"".format(convert_token))\n        self.pipes = [self]\n\n    def __call__(self, x, *args):\n        """"""Apply the the current Pipeline(s) to an input.\n\n        Arguments:\n            x: The input to process with the Pipeline(s).\n            Positional arguments: Forwarded to the `call` function\n                of the Pipeline(s).\n        """"""\n        for pipe in self.pipes:\n            x = pipe.call(x, *args)\n        return x\n\n    def call(self, x, *args):\n        """"""Apply _only_ the convert_token function of the current pipeline\n        to the input. If the input is a list, a list with the results of\n        applying the `convert_token` function to all input elements is\n        returned.\n\n        Arguments:\n            x: The input to apply the convert_token function to.\n            Positional arguments: Forwarded to the `convert_token` function\n                of the current Pipeline.\n        """"""\n        if isinstance(x, list):\n            return [self.convert_token(tok, *args) for tok in x]\n        return self.convert_token(x, *args)\n\n    def add_before(self, pipeline):\n        """"""Add a Pipeline to be applied before this processing pipeline.\n\n        Arguments:\n            pipeline: The Pipeline or callable to apply before this\n                Pipeline.\n        """"""\n        if not isinstance(pipeline, Pipeline):\n            pipeline = Pipeline(pipeline)\n        self.pipes = pipeline.pipes[:] + self.pipes[:]\n        return self\n\n    def add_after(self, pipeline):\n        """"""Add a Pipeline to be applied after this processing pipeline.\n\n        Arguments:\n            pipeline: The Pipeline or callable to apply after this\n                Pipeline.\n        """"""\n        if not isinstance(pipeline, Pipeline):\n            pipeline = Pipeline(pipeline)\n        self.pipes = self.pipes[:] + pipeline.pipes[:]\n        return self\n\n    @staticmethod\n    def identity(x):\n        """"""Return a copy of the input.\n\n        This is here for serialization compatibility with pickle.\n        """"""\n        return x\n'"
torchtext/data/utils.py,3,"b'import random\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nimport re\n\nfrom functools import partial\n\n\ndef _split_tokenizer(x):  # noqa: F821\n    # type: (str) -> List[str]\n    return x.split()\n\n\ndef _spacy_tokenize(x, spacy):\n    return [tok.text for tok in spacy.tokenizer(x)]\n\n\n_patterns = [r\'\\\'\',\n             r\'\\""\',\n             r\'\\.\',\n             r\'<br \\/>\',\n             r\',\',\n             r\'\\(\',\n             r\'\\)\',\n             r\'\\!\',\n             r\'\\?\',\n             r\'\\;\',\n             r\'\\:\',\n             r\'\\s+\']\n\n_replacements = [\' \\\'  \',\n                 \'\',\n                 \' . \',\n                 \' \',\n                 \' , \',\n                 \' ( \',\n                 \' ) \',\n                 \' ! \',\n                 \' ? \',\n                 \' \',\n                 \' \',\n                 \' \']\n\n_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n\n\ndef _basic_english_normalize(line):\n    r""""""\n    Basic normalization for a line of text.\n    Normalization includes\n    - lowercasing\n    - complete some basic text normalization for English words as follows:\n        add spaces before and after \'\\\'\'\n        remove \'\\""\',\n        add spaces before and after \'.\'\n        replace \'<br \\/>\'with single space\n        add spaces before and after \',\'\n        add spaces before and after \'(\'\n        add spaces before and after \')\'\n        add spaces before and after \'!\'\n        add spaces before and after \'?\'\n        replace \';\' with single space\n        replace \':\' with single space\n        replace multiple spaces with single space\n\n    Returns a list of tokens after splitting on whitespace.\n    """"""\n\n    line = line.lower()\n    for pattern_re, replaced_str in _patterns_dict:\n        line = pattern_re.sub(replaced_str, line)\n    return line.split()\n\n\ndef get_tokenizer(tokenizer, language=\'en\'):\n    r""""""\n    Generate tokenizer function for a string sentence.\n\n    Arguments:\n        tokenizer: the name of tokenizer function. If None, it returns split()\n            function, which splits the string sentence by space.\n            If basic_english, it returns _basic_english_normalize() function,\n            which normalize the string first and split by space. If a callable\n            function, it will return the function. If a tokenizer library\n            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n            corresponding library.\n        language: Default en\n\n    Examples:\n        >>> import torchtext\n        >>> from torchtext.data import get_tokenizer\n        >>> tokenizer = get_tokenizer(""basic_english"")\n        >>> tokens = tokenizer(""You can now install TorchText using pip!"")\n        >>> tokens\n        >>> [\'you\', \'can\', \'now\', \'install\', \'torchtext\', \'using\', \'pip\', \'!\']\n\n    """"""\n\n    # default tokenizer is string.split(), added as a module function for serialization\n    if tokenizer is None:\n        return _split_tokenizer\n\n    if tokenizer == ""basic_english"":\n        if language != \'en\':\n            raise ValueError(""Basic normalization is only available for Enlish(en)"")\n        return _basic_english_normalize\n\n    # simply return if a function is passed\n    if callable(tokenizer):\n        return tokenizer\n\n    if tokenizer == ""spacy"":\n        try:\n            import spacy\n            spacy = spacy.load(language)\n            return partial(_spacy_tokenize, spacy=spacy)\n        except ImportError:\n            print(""Please install SpaCy. ""\n                  ""See the docs at https://spacy.io for more information."")\n            raise\n        except AttributeError:\n            print(""Please install SpaCy and the SpaCy {} tokenizer. ""\n                  ""See the docs at https://spacy.io for more ""\n                  ""information."".format(language))\n            raise\n    elif tokenizer == ""moses"":\n        try:\n            from sacremoses import MosesTokenizer\n            moses_tokenizer = MosesTokenizer()\n            return moses_tokenizer.tokenize\n        except ImportError:\n            print(""Please install SacreMoses. ""\n                  ""See the docs at https://github.com/alvations/sacremoses ""\n                  ""for more information."")\n            raise\n    elif tokenizer == ""toktok"":\n        try:\n            from nltk.tokenize.toktok import ToktokTokenizer\n            toktok = ToktokTokenizer()\n            return toktok.tokenize\n        except ImportError:\n            print(""Please install NLTK. ""\n                  ""See the docs at https://nltk.org  for more information."")\n            raise\n    elif tokenizer == \'revtok\':\n        try:\n            import revtok\n            return revtok.tokenize\n        except ImportError:\n            print(""Please install revtok."")\n            raise\n    elif tokenizer == \'subword\':\n        try:\n            import revtok\n            return partial(revtok.tokenize, decap=True)\n        except ImportError:\n            print(""Please install revtok."")\n            raise\n    raise ValueError(""Requested tokenizer {}, valid choices are a ""\n                     ""callable that takes a single string as input, ""\n                     ""\\""revtok\\"" for the revtok reversible tokenizer, ""\n                     ""\\""subword\\"" for the revtok caps-aware tokenizer, ""\n                     ""\\""spacy\\"" for the SpaCy English tokenizer, or ""\n                     ""\\""moses\\"" for the NLTK port of the Moses tokenization ""\n                     ""script."".format(tokenizer))\n\n\ndef is_tokenizer_serializable(tokenizer, language):\n    """"""Extend with other tokenizers which are found to not be serializable\n    """"""\n    if tokenizer == \'spacy\':\n        return False\n    return True\n\n\ndef interleave_keys(a, b):\n    """"""Interleave bits from two sort keys to form a joint sort key.\n\n    Examples that are similar in both of the provided keys will have similar\n    values for the key defined by this function. Useful for tasks with two\n    text fields like machine translation or natural language inference.\n    """"""\n    def interleave(args):\n        return \'\'.join([x for t in zip(*args) for x in t])\n    return int(\'\'.join(interleave(format(x, \'016b\') for x in (a, b))), base=2)\n\n\ndef get_torch_version():\n    import torch\n    v = torch.__version__\n    version_substrings = v.split(\'.\')\n    major, minor = version_substrings[0], version_substrings[1]\n    return int(major), int(minor)\n\n\ndef dtype_to_attr(dtype):\n    # convert torch.dtype to dtype string id\n    # e.g. torch.int32 -> ""int32""\n    # used for serialization\n    _, dtype = str(dtype).split(\'.\')\n    return dtype\n\n\n# TODO: Write more tests!\ndef ngrams_iterator(token_list, ngrams):\n    """"""Return an iterator that yields the given tokens and their ngrams.\n\n    Arguments:\n        token_list: A list of tokens\n        ngrams: the number of ngrams.\n\n    Examples:\n        >>> token_list = [\'here\', \'we\', \'are\']\n        >>> list(ngrams_iterator(token_list, 2))\n        >>> [\'here\', \'here we\', \'we\', \'we are\', \'are\']\n    """"""\n\n    def _get_ngrams(n):\n        return zip(*[token_list[i:] for i in range(n)])\n\n    for x in token_list:\n        yield x\n    for n in range(2, ngrams + 1):\n        for x in _get_ngrams(n):\n            yield \' \'.join(x)\n\n\nclass RandomShuffler(object):\n    """"""Use random functions while keeping track of the random state to make it\n    reproducible and deterministic.""""""\n\n    def __init__(self, random_state=None):\n        self._random_state = random_state\n        if self._random_state is None:\n            self._random_state = random.getstate()\n\n    @contextmanager\n    def use_internal_state(self):\n        """"""Use a specific RNG state.""""""\n        old_state = random.getstate()\n        random.setstate(self._random_state)\n        yield\n        self._random_state = random.getstate()\n        random.setstate(old_state)\n\n    @property\n    def random_state(self):\n        return deepcopy(self._random_state)\n\n    @random_state.setter\n    def random_state(self, s):\n        self._random_state = s\n\n    def __call__(self, data):\n        """"""Shuffle and return a new list.""""""\n        with self.use_internal_state():\n            return random.sample(data, len(data))\n'"
torchtext/datasets/__init__.py,0,"b""from .language_modeling import LanguageModelingDataset, WikiText2, WikiText103, PennTreebank  # NOQA\nfrom .nli import SNLI, MultiNLI, XNLI\nfrom .sst import SST\nfrom .translation import TranslationDataset, Multi30k, IWSLT, WMT14  # NOQA\nfrom .sequence_tagging import SequenceTaggingDataset, UDPOS, CoNLL2000Chunking  # NOQA\nfrom .trec import TREC\nfrom .imdb import IMDB\nfrom .babi import BABI20\nfrom .text_classification import TextClassificationDataset, \\\n    AG_NEWS, SogouNews, DBpedia, YelpReviewPolarity, \\\n    YelpReviewFull, YahooAnswers, \\\n    AmazonReviewPolarity, AmazonReviewFull\nfrom .unsupervised_learning import EnWik9\n\n__all__ = ['LanguageModelingDataset',\n           'SNLI',\n           'MultiNLI',\n           'XNLI',\n           'SST',\n           'TranslationDataset',\n           'Multi30k',\n           'IWSLT',\n           'WMT14',\n           'WikiText2',\n           'WikiText103',\n           'PennTreebank',\n           'TREC',\n           'IMDB',\n           'SequenceTaggingDataset',\n           'UDPOS',\n           'CoNLL2000Chunking',\n           'BABI20',\n           'TextClassificationDataset',\n           'AG_NEWS',\n           'SogouNews',\n           'DBpedia',\n           'YelpReviewPolarity',\n           'YelpReviewFull',\n           'YahooAnswers',\n           'AmazonReviewPolarity',\n           'AmazonReviewFull',\n           'EnWik9']\n"""
torchtext/datasets/babi.py,1,"b'import os\nfrom io import open\n\nimport torch\n\nfrom ..data import Dataset, Field, Example, Iterator\n\n\nclass BABI20Field(Field):\n\n    def __init__(self, memory_size, **kwargs):\n        super(BABI20Field, self).__init__(**kwargs)\n        self.memory_size = memory_size\n        self.unk_token = None\n        self.batch_first = True\n\n    def preprocess(self, x):\n        if isinstance(x, list):\n            return [super(BABI20Field, self).preprocess(s) for s in x]\n        else:\n            return super(BABI20Field, self).preprocess(x)\n\n    def pad(self, minibatch):\n        if isinstance(minibatch[0][0], list):\n            self.fix_length = max(max(len(x) for x in ex) for ex in minibatch)\n            padded = []\n            for ex in minibatch:\n                # sentences are indexed in reverse order and truncated to memory_size\n                nex = ex[::-1][:self.memory_size]\n                padded.append(\n                    super(BABI20Field, self).pad(nex)\n                    + [[self.pad_token] * self.fix_length]\n                    * (self.memory_size - len(nex)))\n            self.fix_length = None\n            return padded\n        else:\n            return super(BABI20Field, self).pad(minibatch)\n\n    def numericalize(self, arr, device=None):\n        if isinstance(arr[0][0], list):\n            tmp = [\n                super(BABI20Field, self).numericalize(x, device=device).data\n                for x in arr\n            ]\n            arr = torch.stack(tmp)\n            if self.sequential:\n                arr = arr.contiguous()\n            return arr\n        else:\n            return super(BABI20Field, self).numericalize(arr, device=device)\n\n\nclass BABI20(Dataset):\n    urls = [\'http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\']\n    name = \'\'\n    dirname = \'\'\n\n    def __init__(self, path, text_field, only_supporting=False, **kwargs):\n        fields = [(\'story\', text_field), (\'query\', text_field), (\'answer\', text_field)]\n        self.sort_key = lambda x: len(x.query)\n\n        with open(path, \'r\', encoding=""utf-8"") as f:\n            triplets = self._parse(f, only_supporting)\n            examples = [Example.fromlist(triplet, fields) for triplet in triplets]\n\n        super(BABI20, self).__init__(examples, fields, **kwargs)\n\n    @staticmethod\n    def _parse(file, only_supporting):\n        data, story = [], []\n        for line in file:\n            tid, text = line.rstrip(\'\\n\').split(\' \', 1)\n            if tid == \'1\':\n                story = []\n            # sentence\n            if text.endswith(\'.\'):\n                story.append(text[:-1])\n            # question\n            else:\n                # remove any leading or trailing whitespace after splitting\n                query, answer, supporting = (x.strip() for x in text.split(\'\\t\'))\n                if only_supporting:\n                    substory = [story[int(i) - 1] for i in supporting.split()]\n                else:\n                    substory = [x for x in story if x]\n                data.append((substory, query[:-1], answer))    # remove \'?\'\n                story.append("""")\n        return data\n\n    @classmethod\n    def splits(cls, text_field, path=None, root=\'.data\', task=1, joint=False, tenK=False,\n               only_supporting=False, train=None, validation=None, test=None, **kwargs):\n        assert isinstance(task, int) and 1 <= task <= 20\n        if tenK:\n            cls.dirname = os.path.join(\'tasks_1-20_v1-2\', \'en-valid-10k\')\n        else:\n            cls.dirname = os.path.join(\'tasks_1-20_v1-2\', \'en-valid\')\n        if path is None:\n            path = cls.download(root)\n        if train is None:\n            if joint:    # put all tasks together for joint learning\n                train = \'all_train.txt\'\n                if not os.path.isfile(os.path.join(path, train)):\n                    with open(os.path.join(path, train), \'w\') as tf:\n                        for task in range(1, 21):\n                            with open(\n                                    os.path.join(path,\n                                                 \'qa\' + str(task) + \'_train.txt\')) as f:\n                                tf.write(f.read())\n            else:\n                train = \'qa\' + str(task) + \'_train.txt\'\n        if validation is None:\n            if joint:    # put all tasks together for joint learning\n                validation = \'all_valid.txt\'\n                if not os.path.isfile(os.path.join(path, validation)):\n                    with open(os.path.join(path, validation), \'w\') as tf:\n                        for task in range(1, 21):\n                            with open(\n                                    os.path.join(path,\n                                                 \'qa\' + str(task) + \'_valid.txt\')) as f:\n                                tf.write(f.read())\n            else:\n                validation = \'qa\' + str(task) + \'_valid.txt\'\n        if test is None:\n            test = \'qa\' + str(task) + \'_test.txt\'\n        return super(BABI20,\n                     cls).splits(path=path, root=root, text_field=text_field, train=train,\n                                 validation=validation, test=test, **kwargs)\n\n    @classmethod\n    def iters(cls, batch_size=32, root=\'.data\', memory_size=50, task=1, joint=False,\n              tenK=False, only_supporting=False, sort=False, shuffle=False, device=None,\n              **kwargs):\n        text = BABI20Field(memory_size)\n        train, val, test = BABI20.splits(text, root=root, task=task, joint=joint,\n                                         tenK=tenK, only_supporting=only_supporting,\n                                         **kwargs)\n        text.build_vocab(train)\n        return Iterator.splits((train, val, test), batch_size=batch_size, sort=sort,\n                               shuffle=shuffle, device=device)\n'"
torchtext/datasets/imdb.py,0,"b'import os\nimport glob\nimport io\n\nfrom .. import data\n\n\nclass IMDB(data.Dataset):\n\n    urls = [\'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\']\n    name = \'imdb\'\n    dirname = \'aclImdb\'\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    def __init__(self, path, text_field, label_field, **kwargs):\n        """"""Create an IMDB dataset instance given a path and fields.\n\n        Arguments:\n            path: Path to the dataset\'s highest level directory\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        """"""\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n        examples = []\n\n        for label in [\'pos\', \'neg\']:\n            for fname in glob.iglob(os.path.join(path, label, \'*.txt\')):\n                with io.open(fname, \'r\', encoding=""utf-8"") as f:\n                    text = f.readline()\n                examples.append(data.Example.fromlist([text, label], fields))\n\n        super(IMDB, self).__init__(examples, fields, **kwargs)\n\n    @classmethod\n    def splits(cls, text_field, label_field, root=\'.data\',\n               train=\'train\', test=\'test\', **kwargs):\n        """"""Create dataset objects for splits of the IMDB dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            root: Root dataset storage directory. Default is \'.data\'.\n            train: The directory that contains the training examples\n            test: The directory that contains the test examples\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        return super(IMDB, cls).splits(\n            root=root, text_field=text_field, label_field=label_field,\n            train=train, validation=None, test=test, **kwargs)\n\n    @classmethod\n    def iters(cls, batch_size=32, device=0, root=\'.data\', vectors=None, **kwargs):\n        """"""Create iterator objects for splits of the IMDB dataset.\n\n        Arguments:\n            batch_size: Batch_size\n            device: Device to create batches on. Use - 1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that contains the imdb dataset subdirectory\n            vectors: one of the available pretrained vectors or a list with each\n                element one of the available pretrained vectors (see Vocab.load_vectors)\n\n            Remaining keyword arguments: Passed to the splits method.\n        """"""\n        TEXT = data.Field()\n        LABEL = data.Field(sequential=False)\n\n        train, test = cls.splits(TEXT, LABEL, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n        LABEL.build_vocab(train)\n\n        return data.BucketIterator.splits(\n            (train, test), batch_size=batch_size, device=device)\n'"
torchtext/datasets/language_modeling.py,0,"b'from torchtext import data\nimport io\n\n\nclass LanguageModelingDataset(data.Dataset):\n    """"""Defines a dataset for language modeling.""""""\n\n    def __init__(self, path, text_field, newline_eos=True,\n                 encoding=\'utf-8\', **kwargs):\n        """"""Create a LanguageModelingDataset given a path and a field.\n\n        Arguments:\n            path: Path to the data file.\n            text_field: The field that will be used for text data.\n            newline_eos: Whether to add an <eos> token for every newline in the\n                data file. Default: True.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        """"""\n        fields = [(\'text\', text_field)]\n        text = []\n        with io.open(path, encoding=encoding) as f:\n            for line in f:\n                text += text_field.preprocess(line)\n                if newline_eos:\n                    text.append(u\'<eos>\')\n\n        examples = [data.Example.fromlist([text], fields)]\n        super(LanguageModelingDataset, self).__init__(\n            examples, fields, **kwargs)\n\n\nclass WikiText2(LanguageModelingDataset):\n\n    urls = [\'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\']\n    name = \'wikitext-2\'\n    dirname = \'wikitext-2\'\n\n    @classmethod\n    def splits(cls, text_field, root=\'.data\', train=\'wiki.train.tokens\',\n               validation=\'wiki.valid.tokens\', test=\'wiki.test.tokens\',\n               **kwargs):\n        """"""Create dataset objects for splits of the WikiText-2 dataset.\n\n        This is the most flexible way to use the dataset.\n\n        Arguments:\n            text_field: The field that will be used for text data.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose wikitext-2\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: \'wiki.train.tokens\'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: \'wiki.valid.tokens\'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: \'wiki.test.tokens\'.\n        """"""\n        return super(WikiText2, cls).splits(\n            root=root, train=train, validation=validation, test=test,\n            text_field=text_field, **kwargs)\n\n    @classmethod\n    def iters(cls, batch_size=32, bptt_len=35, device=0, root=\'.data\',\n              vectors=None, **kwargs):\n        """"""Create iterator objects for splits of the WikiText-2 dataset.\n\n        This is the simplest way to use the dataset, and assumes common\n        defaults for field, vocabulary, and iterator parameters.\n\n        Arguments:\n            batch_size: Batch size.\n            bptt_len: Length of sequences for backpropagation through time.\n            device: Device to create batches on. Use -1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose wikitext-2\n                subdirectory the data files will be stored.\n            wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n                text field. The word vectors are accessible as\n                train.dataset.fields[\'text\'].vocab.vectors.\n            Remaining keyword arguments: Passed to the splits method.\n        """"""\n        TEXT = data.Field()\n\n        train, val, test = cls.splits(TEXT, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n\n        return data.BPTTIterator.splits(\n            (train, val, test), batch_size=batch_size, bptt_len=bptt_len,\n            device=device)\n\n\nclass WikiText103(LanguageModelingDataset):\n\n    urls = [\'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\']\n    name = \'wikitext-103\'\n    dirname = \'wikitext-103\'\n\n    @classmethod\n    def splits(cls, text_field, root=\'.data\', train=\'wiki.train.tokens\',\n               validation=\'wiki.valid.tokens\', test=\'wiki.test.tokens\',\n               **kwargs):\n        """"""Create dataset objects for splits of the WikiText-103 dataset.\n\n        This is the most flexible way to use the dataset.\n\n        Arguments:\n            text_field: The field that will be used for text data.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose wikitext-103\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: \'wiki.train.tokens\'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: \'wiki.valid.tokens\'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: \'wiki.test.tokens\'.\n        """"""\n        return super(WikiText103, cls).splits(\n            root=root, train=train, validation=validation, test=test,\n            text_field=text_field, **kwargs)\n\n    @classmethod\n    def iters(cls, batch_size=32, bptt_len=35, device=0, root=\'.data\',\n              vectors=None, **kwargs):\n        """"""Create iterator objects for splits of the WikiText-103 dataset.\n\n        This is the simplest way to use the dataset, and assumes common\n        defaults for field, vocabulary, and iterator parameters.\n\n        Arguments:\n            batch_size: Batch size.\n            bptt_len: Length of sequences for backpropagation through time.\n            device: Device to create batches on. Use -1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose wikitext-2\n                subdirectory the data files will be stored.\n            wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n                text field. The word vectors are accessible as\n                train.dataset.fields[\'text\'].vocab.vectors.\n            Remaining keyword arguments: Passed to the splits method.\n        """"""\n        TEXT = data.Field()\n\n        train, val, test = cls.splits(TEXT, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n\n        return data.BPTTIterator.splits(\n            (train, val, test), batch_size=batch_size, bptt_len=bptt_len,\n            device=device)\n\n\nclass PennTreebank(LanguageModelingDataset):\n    """"""The Penn Treebank dataset.\n    A relatively small dataset originally created for POS tagging.\n\n    References\n    ----------\n    Marcus, Mitchell P., Marcinkiewicz, Mary Ann & Santorini, Beatrice (1993).\n    Building a Large Annotated Corpus of English: The Penn Treebank\n    """"""\n\n    urls = [\'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\',\n            \'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt\',\n            \'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt\']\n    name = \'penn-treebank\'\n    dirname = \'\'\n\n    @classmethod\n    def splits(cls, text_field, root=\'.data\', train=\'ptb.train.txt\',\n               validation=\'ptb.valid.txt\', test=\'ptb.test.txt\',\n               **kwargs):\n        """"""Create dataset objects for splits of the Penn Treebank dataset.\n\n        Arguments:\n            text_field: The field that will be used for text data.\n            root: The root directory where the data files will be stored.\n            train: The filename of the train data. Default: \'ptb.train.txt\'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: \'ptb.valid.txt\'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: \'ptb.test.txt\'.\n        """"""\n        return super(PennTreebank, cls).splits(\n            root=root, train=train, validation=validation, test=test,\n            text_field=text_field, **kwargs)\n\n    @classmethod\n    def iters(cls, batch_size=32, bptt_len=35, device=0, root=\'.data\',\n              vectors=None, **kwargs):\n        """"""Create iterator objects for splits of the Penn Treebank dataset.\n\n        This is the simplest way to use the dataset, and assumes common\n        defaults for field, vocabulary, and iterator parameters.\n\n        Arguments:\n            batch_size: Batch size.\n            bptt_len: Length of sequences for backpropagation through time.\n            device: Device to create batches on. Use -1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory where the data files will be stored.\n            wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n                text field. The word vectors are accessible as\n                train.dataset.fields[\'text\'].vocab.vectors.\n            Remaining keyword arguments: Passed to the splits method.\n        """"""\n        TEXT = data.Field()\n\n        train, val, test = cls.splits(TEXT, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n\n        return data.BPTTIterator.splits(\n            (train, val, test), batch_size=batch_size, bptt_len=bptt_len,\n            device=device)\n'"
torchtext/datasets/nli.py,0,"b'from .. import data\n\n\nclass ShiftReduceField(data.Field):\n\n    def __init__(self):\n\n        super(ShiftReduceField, self).__init__(preprocessing=lambda parse: [\n            \'reduce\' if t == \')\' else \'shift\' for t in parse if t != \'(\'])\n\n        self.build_vocab([[\'reduce\'], [\'shift\']])\n\n\nclass ParsedTextField(data.Field):\n    """"""\n        Field for parsed sentences data in NLI datasets.\n        Expensive tokenization could be omitted from the pipeline as\n        the parse tree annotations are already in tokenized form.\n    """"""\n\n    def __init__(self, eos_token=\'<pad>\', lower=False, reverse=False):\n        if reverse:\n            super(ParsedTextField, self).__init__(\n                eos_token=eos_token, lower=lower,\n                preprocessing=lambda parse: [t for t in parse if t not in (\'(\', \')\')],\n                postprocessing=lambda parse, _: [list(reversed(p)) for p in parse],\n                include_lengths=True)\n        else:\n            super(ParsedTextField, self).__init__(\n                eos_token=eos_token, lower=lower,\n                preprocessing=lambda parse: [t for t in parse if t not in (\'(\', \')\')],\n                include_lengths=True)\n\n\nclass NLIDataset(data.TabularDataset):\n\n    urls = []\n    dirname = \'\'\n    name = \'nli\'\n\n    @staticmethod\n    def sort_key(ex):\n        return data.interleave_keys(\n            len(ex.premise), len(ex.hypothesis))\n\n    @classmethod\n    def splits(cls, text_field, label_field, parse_field=None,\n               extra_fields={}, root=\'.data\', train=\'train.jsonl\',\n               validation=\'val.jsonl\', test=\'test.jsonl\'):\n        """"""Create dataset objects for splits of the SNLI dataset.\n\n        This is the most flexible way to use the dataset.\n\n        Arguments:\n            text_field: The field that will be used for premise and hypothesis\n                data.\n            label_field: The field that will be used for label data.\n            parse_field: The field that will be used for shift-reduce parser\n                transitions, or None to not include them.\n            extra_fields: A dict[json_key: Tuple(field_name, Field)]\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into.\n            train: The filename of the train data. Default: \'train.jsonl\'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: \'dev.jsonl\'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: \'test.jsonl\'.\n        """"""\n        path = cls.download(root)\n\n        if parse_field is None:\n            fields = {\'sentence1\': (\'premise\', text_field),\n                      \'sentence2\': (\'hypothesis\', text_field),\n                      \'gold_label\': (\'label\', label_field)}\n        else:\n            fields = {\'sentence1_binary_parse\': [(\'premise\', text_field),\n                                                 (\'premise_transitions\', parse_field)],\n                      \'sentence2_binary_parse\': [(\'hypothesis\', text_field),\n                                                 (\'hypothesis_transitions\', parse_field)],\n                      \'gold_label\': (\'label\', label_field)}\n\n        for key in extra_fields:\n            if key not in fields.keys():\n                fields[key] = extra_fields[key]\n\n        return super(NLIDataset, cls).splits(\n            path, root, train, validation, test,\n            format=\'json\', fields=fields,\n            filter_pred=lambda ex: ex.label != \'-\')\n\n    @classmethod\n    def iters(cls, batch_size=32, device=0, root=\'.data\',\n              vectors=None, trees=False, **kwargs):\n        """"""Create iterator objects for splits of the SNLI dataset.\n\n        This is the simplest way to use the dataset, and assumes common\n        defaults for field, vocabulary, and iterator parameters.\n\n        Arguments:\n            batch_size: Batch size.\n            device: Device to create batches on. Use -1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose wikitext-2\n                subdirectory the data files will be stored.\n            vectors: one of the available pretrained vectors or a list with each\n                element one of the available pretrained vectors (see Vocab.load_vectors)\n            trees: Whether to include shift-reduce parser transitions.\n                Default: False.\n            Remaining keyword arguments: Passed to the splits method.\n        """"""\n        if trees:\n            TEXT = ParsedTextField()\n            TRANSITIONS = ShiftReduceField()\n        else:\n            TEXT = data.Field(tokenize=\'spacy\')\n            TRANSITIONS = None\n        LABEL = data.Field(sequential=False)\n\n        train, val, test = cls.splits(\n            TEXT, LABEL, TRANSITIONS, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n        LABEL.build_vocab(train)\n\n        return data.BucketIterator.splits(\n            (train, val, test), batch_size=batch_size, device=device)\n\n\nclass SNLI(NLIDataset):\n    urls = [\'http://nlp.stanford.edu/projects/snli/snli_1.0.zip\']\n    dirname = \'snli_1.0\'\n    name = \'snli\'\n\n    @classmethod\n    def splits(cls, text_field, label_field, parse_field=None, root=\'.data\',\n               train=\'snli_1.0_train.jsonl\', validation=\'snli_1.0_dev.jsonl\',\n               test=\'snli_1.0_test.jsonl\'):\n        return super(SNLI, cls).splits(text_field, label_field, parse_field=parse_field,\n                                       root=root, train=train, validation=validation,\n                                       test=test)\n\n\nclass MultiNLI(NLIDataset):\n    urls = [\'http://www.nyu.edu/projects/bowman/multinli/multinli_1.0.zip\']\n    dirname = \'multinli_1.0\'\n    name = \'multinli\'\n\n    @classmethod\n    def splits(cls, text_field, label_field, parse_field=None, genre_field=None,\n               root=\'.data\',\n               train=\'multinli_1.0_train.jsonl\',\n               validation=\'multinli_1.0_dev_matched.jsonl\',\n               test=\'multinli_1.0_dev_mismatched.jsonl\'):\n        extra_fields = {}\n        if genre_field is not None:\n            extra_fields[""genre""] = (""genre"", genre_field)\n\n        return super(MultiNLI, cls).splits(text_field, label_field,\n                                           parse_field=parse_field,\n                                           extra_fields=extra_fields,\n                                           root=root, train=train,\n                                           validation=validation, test=test)\n\n\nclass XNLI(NLIDataset):\n    urls = [\'http://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip\']\n    dirname = \'XNLI-1.0\'\n    name = \'xnli\'\n\n    @classmethod\n    def splits(cls, text_field, label_field, genre_field=None, language_field=None,\n               root=\'.data\',\n               validation=\'xnli.dev.jsonl\',\n               test=\'xnli.test.jsonl\'):\n        extra_fields = {}\n        if genre_field is not None:\n            extra_fields[""genre""] = (""genre"", genre_field)\n        if language_field is not None:\n            extra_fields[""language""] = (""language"", language_field)\n\n        return super(XNLI, cls).splits(text_field, label_field,\n                                       extra_fields=extra_fields,\n                                       root=root, train=None,\n                                       validation=validation, test=test)\n\n    @classmethod\n    def iters(cls, *args, **kwargs):\n        raise NotImplementedError(\'XNLI dataset does not support iters\')\n'"
torchtext/datasets/sequence_tagging.py,0,"b'from .. import data\nimport random\n\n\nclass SequenceTaggingDataset(data.Dataset):\n    """"""Defines a dataset for sequence tagging. Examples in this dataset\n    contain paired lists -- paired list of words and tags.\n\n    For example, in the case of part-of-speech tagging, an example is of the\n    form\n    [I, love, PyTorch, .] paired with [PRON, VERB, PROPN, PUNCT]\n\n    See torchtext/test/sequence_tagging.py on how to use this class.\n    """"""\n\n    @staticmethod\n    def sort_key(example):\n        for attr in dir(example):\n            if not callable(getattr(example, attr)) and \\\n                    not attr.startswith(""__""):\n                return len(getattr(example, attr))\n        return 0\n\n    def __init__(self, path, fields, encoding=""utf-8"", separator=""\\t"", **kwargs):\n        examples = []\n        columns = []\n\n        with open(path, encoding=encoding) as input_file:\n            for line in input_file:\n                line = line.strip()\n                if line == """":\n                    if columns:\n                        examples.append(data.Example.fromlist(columns, fields))\n                    columns = []\n                else:\n                    for i, column in enumerate(line.split(separator)):\n                        if len(columns) < i + 1:\n                            columns.append([])\n                        columns[i].append(column)\n\n            if columns:\n                examples.append(data.Example.fromlist(columns, fields))\n        super(SequenceTaggingDataset, self).__init__(examples, fields,\n                                                     **kwargs)\n\n\nclass UDPOS(SequenceTaggingDataset):\n\n    # Universal Dependencies English Web Treebank.\n    # Download original at http://universaldependencies.org/\n    # License: http://creativecommons.org/licenses/by-sa/4.0/\n    urls = [\'https://bitbucket.org/sivareddyg/public/downloads/en-ud-v2.zip\']\n    dirname = \'en-ud-v2\'\n    name = \'udpos\'\n\n    @classmethod\n    def splits(cls, fields, root="".data"", train=""en-ud-tag.v2.train.txt"",\n               validation=""en-ud-tag.v2.dev.txt"",\n               test=""en-ud-tag.v2.test.txt"", **kwargs):\n        """"""Downloads and loads the Universal Dependencies Version 2 POS Tagged\n        data.\n        """"""\n\n        return super(UDPOS, cls).splits(\n            fields=fields, root=root, train=train, validation=validation,\n            test=test, **kwargs)\n\n\nclass CoNLL2000Chunking(SequenceTaggingDataset):\n    # CoNLL 2000 Chunking Dataset\n    # https://www.clips.uantwerpen.be/conll2000/chunking/\n    urls = [\'https://www.clips.uantwerpen.be/conll2000/chunking/train.txt.gz\',\n            \'https://www.clips.uantwerpen.be/conll2000/chunking/test.txt.gz\']\n    dirname = \'\'\n    name = \'conll2000\'\n\n    @classmethod\n    def splits(cls, fields, root="".data"", train=""train.txt"",\n               test=""test.txt"", validation_frac=0.1, **kwargs):\n        """"""Downloads and loads the CoNLL 2000 Chunking dataset.\n        NOTE: There is only a train and test dataset so we use\n              10% of the train set as validation\n        """"""\n\n        train, test = super(CoNLL2000Chunking, cls).splits(\n            fields=fields, root=root, train=train,\n            test=test, separator=\' \', **kwargs)\n\n        # HACK: Saving the sort key function as the split() call removes it\n        sort_key = train.sort_key\n\n        # Now split the train set\n        # Force a random seed to make the split deterministic\n        random.seed(0)\n        train, val = train.split(1 - validation_frac, random_state=random.getstate())\n        # Reset the seed\n        random.seed()\n\n        # HACK: Set the sort key\n        train.sort_key = sort_key\n        val.sort_key = sort_key\n\n        return train, val, test\n'"
torchtext/datasets/sst.py,0,"b'import os\n\nfrom .. import data\n\n\nclass SST(data.Dataset):\n\n    urls = [\'http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\']\n    dirname = \'trees\'\n    name = \'sst\'\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    def __init__(self, path, text_field, label_field, subtrees=False,\n                 fine_grained=False, **kwargs):\n        """"""Create an SST dataset instance given a path and fields.\n\n        Arguments:\n            path: Path to the data file\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            subtrees: Whether to include sentiment-tagged subphrases\n                in addition to complete examples. Default: False.\n            fine_grained: Whether to use 5-class instead of 3-class\n                labeling. Default: False.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        """"""\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n\n        def get_label_str(label):\n            pre = \'very \' if fine_grained else \'\'\n            return {\'0\': pre + \'negative\', \'1\': \'negative\', \'2\': \'neutral\',\n                    \'3\': \'positive\', \'4\': pre + \'positive\', None: None}[label]\n        label_field.preprocessing = data.Pipeline(get_label_str)\n        with open(os.path.expanduser(path)) as f:\n            if subtrees:\n                examples = [ex for line in f for ex in\n                            data.Example.fromtree(line, fields, True)]\n            else:\n                examples = [data.Example.fromtree(line, fields) for line in f]\n        super(SST, self).__init__(examples, fields, **kwargs)\n\n    @classmethod\n    def splits(cls, text_field, label_field, root=\'.data\',\n               train=\'train.txt\', validation=\'dev.txt\', test=\'test.txt\',\n               train_subtrees=False, **kwargs):\n        """"""Create dataset objects for splits of the SST dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: \'train.txt\'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: \'dev.txt\'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: \'test.txt\'.\n            train_subtrees: Whether to use all subtrees in the training set.\n                Default: False.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        path = cls.download(root)\n\n        train_data = None if train is None else cls(\n            os.path.join(path, train), text_field, label_field, subtrees=train_subtrees,\n            **kwargs)\n        val_data = None if validation is None else cls(\n            os.path.join(path, validation), text_field, label_field, **kwargs)\n        test_data = None if test is None else cls(\n            os.path.join(path, test), text_field, label_field, **kwargs)\n        return tuple(d for d in (train_data, val_data, test_data)\n                     if d is not None)\n\n    @classmethod\n    def iters(cls, batch_size=32, device=0, root=\'.data\', vectors=None, **kwargs):\n        """"""Create iterator objects for splits of the SST dataset.\n\n        Arguments:\n            batch_size: Batch_size\n            device: Device to create batches on. Use - 1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            vectors: one of the available pretrained vectors or a list with each\n                element one of the available pretrained vectors (see Vocab.load_vectors)\n            Remaining keyword arguments: Passed to the splits method.\n        """"""\n        TEXT = data.Field()\n        LABEL = data.Field(sequential=False)\n\n        train, val, test = cls.splits(TEXT, LABEL, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n        LABEL.build_vocab(train)\n\n        return data.BucketIterator.splits(\n            (train, val, test), batch_size=batch_size, device=device)\n'"
torchtext/datasets/text_classification.py,3,"b'import logging\nimport torch\nimport io\nfrom torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\nfrom torchtext.data.utils import ngrams_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.vocab import Vocab\nfrom tqdm import tqdm\n\nURLS = {\n    \'AG_NEWS\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms\',\n    \'SogouNews\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUkVqNEszd0pHaFE\',\n    \'DBpedia\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k\',\n    \'YelpReviewPolarity\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbNUpYQ2N3SGlFaDg\',\n    \'YelpReviewFull\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbZlU4dXhHTFhZQU0\',\n    \'YahooAnswers\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU\',\n    \'AmazonReviewPolarity\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbaW12WVVZS2drcnM\',\n    \'AmazonReviewFull\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbZVhsUnRWRDhETzA\'\n}\n\n\ndef _csv_iterator(data_path, ngrams, yield_cls=False):\n    tokenizer = get_tokenizer(""basic_english"")\n    with io.open(data_path, encoding=""utf8"") as f:\n        reader = unicode_csv_reader(f)\n        for row in reader:\n            tokens = \' \'.join(row[1:])\n            tokens = tokenizer(tokens)\n            if yield_cls:\n                yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)\n            else:\n                yield ngrams_iterator(tokens, ngrams)\n\n\ndef _create_data_from_iterator(vocab, iterator, include_unk):\n    data = []\n    labels = []\n    with tqdm(unit_scale=0, unit=\'lines\') as t:\n        for cls, tokens in iterator:\n            if include_unk:\n                tokens = torch.tensor([vocab[token] for token in tokens])\n            else:\n                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n                                        for token in tokens]))\n                tokens = torch.tensor(token_ids)\n            if len(tokens) == 0:\n                logging.info(\'Row contains no tokens.\')\n            data.append((cls, tokens))\n            labels.append(cls)\n            t.update(1)\n    return data, set(labels)\n\n\nclass TextClassificationDataset(torch.utils.data.Dataset):\n    """"""Defines an abstract text classification datasets.\n       Currently, we only support the following datasets:\n\n             - AG_NEWS\n             - SogouNews\n             - DBpedia\n             - YelpReviewPolarity\n             - YelpReviewFull\n             - YahooAnswers\n             - AmazonReviewPolarity\n             - AmazonReviewFull\n\n    """"""\n\n    def __init__(self, vocab, data, labels):\n        """"""Initiate text-classification dataset.\n\n        Arguments:\n            vocab: Vocabulary object used for dataset.\n            data: a list of label/tokens tuple. tokens are a tensor after\n                numericalizing the string tokens. label is an integer.\n                [(label1, tokens1), (label2, tokens2), (label2, tokens3)]\n            label: a set of the labels.\n                {label1, label2}\n\n        Examples:\n            See the examples in examples/text_classification/\n\n        """"""\n\n        super(TextClassificationDataset, self).__init__()\n        self._data = data\n        self._labels = labels\n        self._vocab = vocab\n\n    def __getitem__(self, i):\n        return self._data[i]\n\n    def __len__(self):\n        return len(self._data)\n\n    def __iter__(self):\n        for x in self._data:\n            yield x\n\n    def get_labels(self):\n        return self._labels\n\n    def get_vocab(self):\n        return self._vocab\n\n\ndef _setup_datasets(dataset_name, root=\'.data\', ngrams=1, vocab=None, include_unk=False):\n    dataset_tar = download_from_url(URLS[dataset_name], root=root)\n    extracted_files = extract_archive(dataset_tar)\n\n    for fname in extracted_files:\n        if fname.endswith(\'train.csv\'):\n            train_csv_path = fname\n        if fname.endswith(\'test.csv\'):\n            test_csv_path = fname\n\n    if vocab is None:\n        logging.info(\'Building Vocab based on {}\'.format(train_csv_path))\n        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))\n    else:\n        if not isinstance(vocab, Vocab):\n            raise TypeError(""Passed vocabulary is not of type Vocab"")\n    logging.info(\'Vocab has {} entries\'.format(len(vocab)))\n    logging.info(\'Creating training data\')\n    train_data, train_labels = _create_data_from_iterator(\n        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=True), include_unk)\n    logging.info(\'Creating testing data\')\n    test_data, test_labels = _create_data_from_iterator(\n        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=True), include_unk)\n    if len(train_labels ^ test_labels) > 0:\n        raise ValueError(""Training and test labels don\'t match"")\n    return (TextClassificationDataset(vocab, train_data, train_labels),\n            TextClassificationDataset(vocab, test_data, test_labels))\n\n\ndef AG_NEWS(*args, **kwargs):\n    """""" Defines AG_NEWS datasets.\n        The labels includes:\n            - 0 : World\n            - 1 : Sports\n            - 2 : Business\n            - 3 : Sci/Tech\n\n    Create supervised learning dataset: AG_NEWS\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        include_unk: include unknown token in the data (Default: False)\n\n    Examples:\n        >>> train_dataset, test_dataset = torchtext.datasets.AG_NEWS(ngrams=3)\n\n    """"""\n\n    return _setup_datasets(*((""AG_NEWS"",) + args), **kwargs)\n\n\ndef SogouNews(*args, **kwargs):\n    """""" Defines SogouNews datasets.\n        The labels includes:\n            - 0 : Sports\n            - 1 : Finance\n            - 2 : Entertainment\n            - 3 : Automobile\n            - 4 : Technology\n\n    Create supervised learning dataset: SogouNews\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        include_unk: include unknown token in the data (Default: False)\n\n    Examples:\n        >>> train_dataset, test_dataset = torchtext.datasets.SogouNews(ngrams=3)\n\n    """"""\n\n    return _setup_datasets(*((""SogouNews"",) + args), **kwargs)\n\n\ndef DBpedia(*args, **kwargs):\n    """""" Defines DBpedia datasets.\n        The labels includes:\n            - 0 : Company\n            - 1 : EducationalInstitution\n            - 2 : Artist\n            - 3 : Athlete\n            - 4 : OfficeHolder\n            - 5 : MeanOfTransportation\n            - 6 : Building\n            - 7 : NaturalPlace\n            - 8 : Village\n            - 9 : Animal\n            - 10 : Plant\n            - 11 : Album\n            - 12 : Film\n            - 13 : WrittenWork\n\n    Create supervised learning dataset: DBpedia\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        include_unk: include unknown token in the data (Default: False)\n\n    Examples:\n        >>> train_dataset, test_dataset = torchtext.datasets.DBpedia(ngrams=3)\n\n    """"""\n\n    return _setup_datasets(*((""DBpedia"",) + args), **kwargs)\n\n\ndef YelpReviewPolarity(*args, **kwargs):\n    """""" Defines YelpReviewPolarity datasets.\n        The labels includes:\n            - 0 : Negative polarity.\n            - 1 : Positive polarity.\n\n    Create supervised learning dataset: YelpReviewPolarity\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        include_unk: include unknown token in the data (Default: False)\n\n    Examples:\n        >>> train_dataset, test_dataset = torchtext.datasets.YelpReviewPolarity(ngrams=3)\n\n    """"""\n\n    return _setup_datasets(*((""YelpReviewPolarity"",) + args), **kwargs)\n\n\ndef YelpReviewFull(*args, **kwargs):\n    """""" Defines YelpReviewFull datasets.\n        The labels includes:\n            0 - 4 : rating classes (4 is highly recommended).\n\n    Create supervised learning dataset: YelpReviewFull\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        include_unk: include unknown token in the data (Default: False)\n\n    Examples:\n        >>> train_dataset, test_dataset = torchtext.datasets.YelpReviewFull(ngrams=3)\n\n    """"""\n\n    return _setup_datasets(*((""YelpReviewFull"",) + args), **kwargs)\n\n\ndef YahooAnswers(*args, **kwargs):\n    """""" Defines YahooAnswers datasets.\n        The labels includes:\n            - 0 : Society & Culture\n            - 1 : Science & Mathematics\n            - 2 : Health\n            - 3 : Education & Reference\n            - 4 : Computers & Internet\n            - 5 : Sports\n            - 6 : Business & Finance\n            - 7 : Entertainment & Music\n            - 8 : Family & Relationships\n            - 9 : Politics & Government\n\n    Create supervised learning dataset: YahooAnswers\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        include_unk: include unknown token in the data (Default: False)\n\n    Examples:\n        >>> train_dataset, test_dataset = torchtext.datasets.YahooAnswers(ngrams=3)\n\n    """"""\n\n    return _setup_datasets(*((""YahooAnswers"",) + args), **kwargs)\n\n\ndef AmazonReviewPolarity(*args, **kwargs):\n    """""" Defines AmazonReviewPolarity datasets.\n        The labels includes:\n            - 0 : Negative polarity\n            - 1 : Positive polarity\n\n    Create supervised learning dataset: AmazonReviewPolarity\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        include_unk: include unknown token in the data (Default: False)\n\n    Examples:\n       >>> train_dataset, test_dataset = torchtext.datasets.AmazonReviewPolarity(ngrams=3)\n\n    """"""\n\n    return _setup_datasets(*((""AmazonReviewPolarity"",) + args), **kwargs)\n\n\ndef AmazonReviewFull(*args, **kwargs):\n    """""" Defines AmazonReviewFull datasets.\n        The labels includes:\n            0 - 4 : rating classes (4 is highly recommended)\n\n    Create supervised learning dataset: AmazonReviewFull\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the dataset are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        include_unk: include unknown token in the data (Default: False)\n\n    Examples:\n        >>> train_dataset, test_dataset = torchtext.datasets.AmazonReviewFull(ngrams=3)\n\n    """"""\n\n    return _setup_datasets(*((""AmazonReviewFull"",) + args), **kwargs)\n\n\nDATASETS = {\n    \'AG_NEWS\': AG_NEWS,\n    \'SogouNews\': SogouNews,\n    \'DBpedia\': DBpedia,\n    \'YelpReviewPolarity\': YelpReviewPolarity,\n    \'YelpReviewFull\': YelpReviewFull,\n    \'YahooAnswers\': YahooAnswers,\n    \'AmazonReviewPolarity\': AmazonReviewPolarity,\n    \'AmazonReviewFull\': AmazonReviewFull\n}\n\n\nLABELS = {\n    \'AG_NEWS\': {0: \'World\',\n                1: \'Sports\',\n                2: \'Business\',\n                3: \'Sci/Tech\'},\n    \'SogouNews\': {0: \'Sports\',\n                  1: \'Finance\',\n                  2: \'Entertainment\',\n                  3: \'Automobile\',\n                  4: \'Technology\'},\n    \'DBpedia\': {0: \'Company\',\n                1: \'EducationalInstitution\',\n                2: \'Artist\',\n                3: \'Athlete\',\n                4: \'OfficeHolder\',\n                5: \'MeanOfTransportation\',\n                6: \'Building\',\n                7: \'NaturalPlace\',\n                8: \'Village\',\n                9: \'Animal\',\n                10: \'Plant\',\n                11: \'Album\',\n                12: \'Film\',\n                13: \'WrittenWork\'},\n    \'YelpReviewPolarity\': {0: \'Negative polarity\',\n                           1: \'Positive polarity\'},\n    \'YelpReviewFull\': {0: \'score 1\',\n                       1: \'score 2\',\n                       2: \'score 3\',\n                       3: \'score 4\',\n                       4: \'score 5\'},\n    \'YahooAnswers\': {0: \'Society & Culture\',\n                     1: \'Science & Mathematics\',\n                     2: \'Health\',\n                     3: \'Education & Reference\',\n                     4: \'Computers & Internet\',\n                     5: \'Sports\',\n                     6: \'Business & Finance\',\n                     7: \'Entertainment & Music\',\n                     8: \'Family & Relationships\',\n                     9: \'Politics & Government\'},\n    \'AmazonReviewPolarity\': {0: \'Negative polarity\',\n                             1: \'Positive polarity\'},\n    \'AmazonReviewFull\': {0: \'score 1\',\n                         1: \'score 2\',\n                         2: \'score 3\',\n                         3: \'score 4\',\n                         4: \'score 5\'}\n}\n'"
torchtext/datasets/translation.py,0,"b'import os\nimport xml.etree.ElementTree as ET\nimport glob\nimport io\nimport codecs\n\nfrom .. import data\n\n\nclass TranslationDataset(data.Dataset):\n    """"""Defines a dataset for machine translation.""""""\n\n    @staticmethod\n    def sort_key(ex):\n        return data.interleave_keys(len(ex.src), len(ex.trg))\n\n    def __init__(self, path, exts, fields, **kwargs):\n        """"""Create a TranslationDataset given paths and fields.\n\n        Arguments:\n            path: Common prefix of paths to the data files for both languages.\n            exts: A tuple containing the extension to path for each language.\n            fields: A tuple containing the fields that will be used for data\n                in each language.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        """"""\n        if not isinstance(fields[0], (tuple, list)):\n            fields = [(\'src\', fields[0]), (\'trg\', fields[1])]\n\n        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n\n        examples = []\n        with io.open(src_path, mode=\'r\', encoding=\'utf-8\') as src_file, \\\n                io.open(trg_path, mode=\'r\', encoding=\'utf-8\') as trg_file:\n            for src_line, trg_line in zip(src_file, trg_file):\n                src_line, trg_line = src_line.strip(), trg_line.strip()\n                if src_line != \'\' and trg_line != \'\':\n                    examples.append(data.Example.fromlist(\n                        [src_line, trg_line], fields))\n\n        super(TranslationDataset, self).__init__(examples, fields, **kwargs)\n\n    @classmethod\n    def splits(cls, exts, fields, path=None, root=\'.data\',\n               train=\'train\', validation=\'val\', test=\'test\', **kwargs):\n        """"""Create dataset objects for splits of a TranslationDataset.\n\n        Arguments:\n            exts: A tuple containing the extension to path for each language.\n            fields: A tuple containing the fields that will be used for data\n                in each language.\n            path (str): Common prefix of the splits\' file paths, or None to use\n                the result of cls.download(root).\n            root: Root dataset storage directory. Default is \'.data\'.\n            train: The prefix of the train data. Default: \'train\'.\n            validation: The prefix of the validation data. Default: \'val\'.\n            test: The prefix of the test data. Default: \'test\'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        if path is None:\n            path = cls.download(root)\n\n        train_data = None if train is None else cls(\n            os.path.join(path, train), exts, fields, **kwargs)\n        val_data = None if validation is None else cls(\n            os.path.join(path, validation), exts, fields, **kwargs)\n        test_data = None if test is None else cls(\n            os.path.join(path, test), exts, fields, **kwargs)\n        return tuple(d for d in (train_data, val_data, test_data)\n                     if d is not None)\n\n\nclass Multi30k(TranslationDataset):\n    """"""The small-dataset WMT 2016 multimodal task, also known as Flickr30k""""""\n\n    urls = [\'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz\',\n            \'http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\',\n            \'http://www.quest.dcs.shef.ac.uk/\'\n            \'wmt17_files_mmt/mmt_task1_test2016.tar.gz\']\n    name = \'multi30k\'\n    dirname = \'\'\n\n    @classmethod\n    def splits(cls, exts, fields, root=\'.data\',\n               train=\'train\', validation=\'val\', test=\'test2016\', **kwargs):\n        """"""Create dataset objects for splits of the Multi30k dataset.\n\n        Arguments:\n            exts: A tuple containing the extension to path for each language.\n            fields: A tuple containing the fields that will be used for data\n                in each language.\n            root: Root dataset storage directory. Default is \'.data\'.\n            train: The prefix of the train data. Default: \'train\'.\n            validation: The prefix of the validation data. Default: \'val\'.\n            test: The prefix of the test data. Default: \'test\'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n\n        # TODO: This is a _HORRIBLE_ patch related to #208\n        # \'path\' can be passed as a kwarg to the translation dataset constructor\n        # or has to be set (so the download wouldn\'t be duplicated). A good idea\n        # seems to rename the existence check variable from path to something else\n        if \'path\' not in kwargs:\n            expected_folder = os.path.join(root, cls.name)\n            path = expected_folder if os.path.exists(expected_folder) else None\n        else:\n            path = kwargs[\'path\']\n            del kwargs[\'path\']\n\n        return super(Multi30k, cls).splits(\n            exts, fields, path, root, train, validation, test, **kwargs)\n\n\nclass IWSLT(TranslationDataset):\n    """"""The IWSLT 2016 TED talk translation task""""""\n\n    base_url = \'https://wit3.fbk.eu/archive/2016-01//texts/{}/{}/{}.tgz\'\n    name = \'iwslt\'\n    base_dirname = \'{}-{}\'\n\n    @classmethod\n    def splits(cls, exts, fields, root=\'.data\',\n               train=\'train\', validation=\'IWSLT16.TED.tst2013\',\n               test=\'IWSLT16.TED.tst2014\', **kwargs):\n        """"""Create dataset objects for splits of the IWSLT dataset.\n\n        Arguments:\n            exts: A tuple containing the extension to path for each language.\n            fields: A tuple containing the fields that will be used for data\n                in each language.\n            root: Root dataset storage directory. Default is \'.data\'.\n            train: The prefix of the train data. Default: \'train\'.\n            validation: The prefix of the validation data. Default: \'val\'.\n            test: The prefix of the test data. Default: \'test\'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        cls.dirname = cls.base_dirname.format(exts[0][1:], exts[1][1:])\n        cls.urls = [cls.base_url.format(exts[0][1:], exts[1][1:], cls.dirname)]\n        check = os.path.join(root, cls.name, cls.dirname)\n        path = cls.download(root, check=check)\n\n        train = \'.\'.join([train, cls.dirname])\n        validation = \'.\'.join([validation, cls.dirname])\n        if test is not None:\n            test = \'.\'.join([test, cls.dirname])\n\n        if not os.path.exists(os.path.join(path, train) + exts[0]):\n            cls.clean(path)\n\n        train_data = None if train is None else cls(\n            os.path.join(path, train), exts, fields, **kwargs)\n        val_data = None if validation is None else cls(\n            os.path.join(path, validation), exts, fields, **kwargs)\n        test_data = None if test is None else cls(\n            os.path.join(path, test), exts, fields, **kwargs)\n        return tuple(d for d in (train_data, val_data, test_data)\n                     if d is not None)\n\n    @staticmethod\n    def clean(path):\n        for f_xml in glob.iglob(os.path.join(path, \'*.xml\')):\n            print(f_xml)\n            f_txt = os.path.splitext(f_xml)[0]\n            with codecs.open(f_txt, mode=\'w\', encoding=\'utf-8\') as fd_txt:\n                root = ET.parse(f_xml).getroot()[0]\n                for doc in root.findall(\'doc\'):\n                    for e in doc.findall(\'seg\'):\n                        fd_txt.write(e.text.strip() + \'\\n\')\n\n        xml_tags = [\'<url\', \'<keywords\', \'<talkid\', \'<description\',\n                    \'<reviewer\', \'<translator\', \'<title\', \'<speaker\']\n        for f_orig in glob.iglob(os.path.join(path, \'train.tags*\')):\n            print(f_orig)\n            f_txt = f_orig.replace(\'.tags\', \'\')\n            with codecs.open(f_txt, mode=\'w\', encoding=\'utf-8\') as fd_txt, \\\n                    io.open(f_orig, mode=\'r\', encoding=\'utf-8\') as fd_orig:\n                for l in fd_orig:\n                    if not any(tag in l for tag in xml_tags):\n                        fd_txt.write(l.strip() + \'\\n\')\n\n\nclass WMT14(TranslationDataset):\n    """"""The WMT 2014 English-German dataset, as preprocessed by Google Brain.\n\n    Though this download contains test sets from 2015 and 2016, the train set\n    differs slightly from WMT 2015 and 2016 and significantly from WMT 2017.""""""\n\n    urls = [(\'https://drive.google.com/uc?export=download&\'\n             \'id=0B_bZck-ksdkpM25jRUN2X2UxMm8\', \'wmt16_en_de.tar.gz\')]\n    name = \'wmt14\'\n    dirname = \'\'\n\n    @classmethod\n    def splits(cls, exts, fields, root=\'.data\',\n               train=\'train.tok.clean.bpe.32000\',\n               validation=\'newstest2013.tok.bpe.32000\',\n               test=\'newstest2014.tok.bpe.32000\', **kwargs):\n        """"""Create dataset objects for splits of the WMT 2014 dataset.\n\n        Arguments:\n            exts: A tuple containing the extensions for each language. Must be\n                either (\'.en\', \'.de\') or the reverse.\n            fields: A tuple containing the fields that will be used for data\n                in each language.\n            root: Root dataset storage directory. Default is \'.data\'.\n            train: The prefix of the train data. Default:\n                \'train.tok.clean.bpe.32000\'.\n            validation: The prefix of the validation data. Default:\n                \'newstest2013.tok.bpe.32000\'.\n            test: The prefix of the test data. Default:\n                \'newstest2014.tok.bpe.32000\'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        # TODO: This is a _HORRIBLE_ patch related to #208\n        # \'path\' can be passed as a kwarg to the translation dataset constructor\n        # or has to be set (so the download wouldn\'t be duplicated). A good idea\n        # seems to rename the existence check variable from path to something else\n        if \'path\' not in kwargs:\n            expected_folder = os.path.join(root, cls.name)\n            path = expected_folder if os.path.exists(expected_folder) else None\n        else:\n            path = kwargs[\'path\']\n            del kwargs[\'path\']\n\n        return super(WMT14, cls).splits(\n            exts, fields, path, root, train, validation, test, **kwargs)\n'"
torchtext/datasets/trec.py,0,"b'import os\n\nfrom .. import data\n\n\nclass TREC(data.Dataset):\n\n    urls = [\'http://cogcomp.org/Data/QA/QC/train_5500.label\',\n            \'http://cogcomp.org/Data/QA/QC/TREC_10.label\']\n    name = \'trec\'\n    dirname = \'\'\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    def __init__(self, path, text_field, label_field,\n                 fine_grained=False, **kwargs):\n        """"""Create an TREC dataset instance given a path and fields.\n\n        Arguments:\n            path: Path to the data file.\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            fine_grained: Whether to use the fine-grained (50-class) version of TREC\n                or the coarse grained (6-class) version.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        """"""\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n        examples = []\n\n        def get_label_str(label):\n            return label.split(\':\')[0] if not fine_grained else label\n        label_field.preprocessing = data.Pipeline(get_label_str)\n\n        for line in open(os.path.expanduser(path), \'rb\'):\n            # there is one non-ASCII byte: sisterBADBYTEcity; replaced with space\n            label, _, text = line.replace(b\'\\xf0\', b\' \').decode().partition(\' \')\n            examples.append(data.Example.fromlist([text, label], fields))\n\n        super(TREC, self).__init__(examples, fields, **kwargs)\n\n    @classmethod\n    def splits(cls, text_field, label_field, root=\'.data\',\n               train=\'train_5500.label\', test=\'TREC_10.label\', **kwargs):\n        """"""Create dataset objects for splits of the TREC dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            root: Root dataset storage directory. Default is \'.data\'.\n            train: The filename of the train data. Default: \'train_5500.label\'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: \'TREC_10.label\'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        return super(TREC, cls).splits(\n            root=root, text_field=text_field, label_field=label_field,\n            train=train, validation=None, test=test, **kwargs)\n\n    @classmethod\n    def iters(cls, batch_size=32, device=0, root=\'.data\', vectors=None, **kwargs):\n        """"""Create iterator objects for splits of the TREC dataset.\n\n        Arguments:\n            batch_size: Batch_size\n            device: Device to create batches on. Use - 1 for CPU and None for\n                the currently active GPU device.\n            root: The root directory that contains the trec dataset subdirectory\n            vectors: one of the available pretrained vectors or a list with each\n                element one of the available pretrained vectors (see Vocab.load_vectors)\n            Remaining keyword arguments: Passed to the splits method.\n        """"""\n        TEXT = data.Field()\n        LABEL = data.Field(sequential=False)\n\n        train, test = cls.splits(TEXT, LABEL, root=root, **kwargs)\n\n        TEXT.build_vocab(train, vectors=vectors)\n        LABEL.build_vocab(train)\n\n        return data.BucketIterator.splits(\n            (train, test), batch_size=batch_size, device=device)\n'"
torchtext/datasets/unsupervised_learning.py,1,"b'from torchtext.data.functional import custom_replace\nimport torch\nfrom torchtext.utils import download_from_url, extract_archive\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.functional import simple_space_split\nimport os\n\n\n_patterns = [(r\'<.*>\', \'\'),\n             (r\'&amp;\', \'&\'),\n             (r\'&lt;\', \'<\'),\n             (r\'&gt;\', \'>\'),\n             (r\'<ref[^<]*<\\/ref>\', \'\'),\n             (r\'<[^>]*>\', \'\'),\n             (r\'\\[http:[^] ]*\', \'[\'),\n             (r\'\\|thumb\', \'\'),\n             (r\'\\|left\', \'\'),\n             (r\'\\|right\', \'\'),\n             (r\'\\|\\d+px\', \'\'),\n             (r\'\\[\\[image:[^\\[\\]]*\\|\', \'\'),\n             (r\'\\[\\[category:([^|\\]]*)[^]]*\\]\\]\', \'[[$1]]\'),\n             (r\'\\[\\[[a-z\\-]*:[^\\]]*\\]\\]\', \'\'),\n             (r\'\\[\\[[^\\|\\]]*\\|\', \'[[\'),\n             (r\'\\{\\{[^\\}]*\\}\\}\', \'\'),\n             (r\'\\{[^\\}]*\\}\', \'\'),\n             (r\'\\[\', \'\'),\n             (r\'\\]\', \'\'),\n             (r\'&[^;]*;\', \' \'),\n             (r\'A\', \'a\'), (r\'B\', \'b\'), (r\'C\', \'c\'),\n             (r\'D\', \'d\'), (r\'E\', \'e\'), (r\'F\', \'f\'),\n             (r\'G\', \'g\'), (r\'H\', \'h\'), (r\'I\', \'i\'),\n             (r\'J\', \'j\'), (r\'K\', \'k\'), (r\'L\', \'l\'),\n             (r\'M\', \'m\'), (r\'N\', \'n\'), (r\'O\', \'o\'),\n             (r\'P\', \'p\'), (r\'Q\', \'q\'), (r\'R\', \'r\'),\n             (r\'S\', \'s\'), (r\'T\', \'t\'), (r\'U\', \'u\'),\n             (r\'V\', \'v\'), (r\'W\', \'w\'), (r\'X\', \'x\'),\n             (r\'Y\', \'y\'), (r\'Z\', \'z\'),\n             (r\'0\', \' zero \'), (r\'1\', \' one \'), (r\'2\', \' two \'),\n             (r\'3\', \' three \'), (r\'4\', \' four \'), (r\'5\', \' five \'),\n             (r\'6\', \' six \'), (r\'7\', \' seven \'), (r\'8\', \' eight \'),\n             (r\'9\', \' nine \'),\n             (r\'[^a-z\\n]+\', \' \'),\n             (r\'\\n \', \'\'),\n             (r\'\\s+\', \' \'),\n             (r\'\\n\\s*\\n\', r\'\\n\')\n             ]\nenwik9_norm_transform = custom_replace(_patterns)\n\n\ndef generate_offsets(filename):\n    offsets = []\n    with open(filename) as f:\n        offsets.append(f.tell())\n        while f.readline():\n            offsets.append(f.tell())\n    return offsets\n\n\ndef read_lines_from_iterator(data_path, offsets, begin_line, num_lines):\n    with open(data_path) as f:\n        f.seek(offsets[begin_line])\n        for i in range(num_lines):\n            yield f.readline()\n\n\ndef preprocess_raw_enwik9(input_filename, output_filename):\n    with open(input_filename, \'r\') as f1:\n        with open(output_filename, \'w\') as f2:\n            while True:\n                line = f1.readline()\n                if not line:\n                    break\n                line = list(enwik9_norm_transform([line]))[0]\n                if line != \' \' and line != \'\':\n                    if line[0] == \' \':\n                        line = line[1:]\n                    f2.writelines(line + \'\\n\')\n\n\nclass EnWik9(torch.utils.data.Dataset):\n    r""""""Compressed size of first 10^9 bytes of enwiki-20060303-pages-articles.xml.\n        It\'s part of Large Text Compression Benchmark project\n    """"""\n\n    def __init__(self, begin_line=0, num_lines=6348957, root=\'.data\'):\n        """"""Initiate EnWik9 dataset.\n\n        Arguments:\n            begin_line: the number of beginning line. Default: 0\n            num_lines: the number of lines to be loaded. Default: 6348957\n            root: Directory where the datasets are saved. Default: "".data""\n            data: a list of label/tokens tuple. tokens are a tensor after\n\n        Examples:\n            >>> from torchtext.datasets import EnWik9\n            >>> enwik9 = EnWik9(num_lines=20000)\n            >>> vocab = enwik9.get_vocab()\n        """"""\n\n        super(EnWik9, self).__init__()\n\n        processed_file = os.path.join(root, \'norm_enwik9\')\n        if not os.path.exists(processed_file):\n            url = \'http://mattmahoney.net/dc/enwik9.zip\'\n            dataset_zip = download_from_url(url,\n                                            path=os.path.join(root, \'enwik9.zip\'),\n                                            root=root)\n            extracted_file = extract_archive(dataset_zip)\n            raw_file = extracted_file[0]\n            preprocess_raw_enwik9(raw_file, processed_file)\n\n        # Meta information\n        offsets = generate_offsets(processed_file)\n        read_lines = read_lines_from_iterator(processed_file,\n                                              offsets, begin_line, num_lines)\n\n        self._data = []\n        for item in simple_space_split(read_lines):\n            self._data += item\n\n        self._vocab = None\n\n    def __getitem__(self, i):\n        return self._data[i]\n\n    def __len__(self):\n        return len(self._data)\n\n    def __iter__(self):\n        for x in self._data:\n            yield x\n\n    def get_vocab(self):\n        if self._vocab is None:\n            self._vocab = build_vocab_from_iterator([self._data])\n        return self._vocab\n'"
torchtext/experimental/__init__.py,0,"b""from . import datasets\n\n__all__ = ['datasets']\n"""
torchtext/experimental/functional.py,1,"b'import torch\nfrom torchtext.data.utils import ngrams_iterator\n\n\ndef vocab_func(vocab):\n    def func(tok_iter):\n        return [vocab[tok] for tok in tok_iter]\n\n    return func\n\n\ndef totensor(dtype):\n    def func(ids_list):\n        return torch.tensor(ids_list).to(dtype)\n\n    return func\n\n\ndef ngrams_func(ngrams):\n    def func(token_list):\n        return list(ngrams_iterator(token_list, ngrams))\n\n    return func\n\n\ndef sequential_transforms(*transforms):\n    def func(txt_input):\n        for transform in transforms:\n            txt_input = transform(txt_input)\n        return txt_input\n\n    return func\n'"
torchtext/experimental/transforms.py,7,"b'import torch\nimport torch.nn as nn\nfrom typing import List, Tuple\n\n\n__all__ = [\n    \'BasicEnglishNormalize\',\n    \'RegexTokenizer\'\n]\n\n\nclass BasicEnglishNormalize(nn.Module):\n    r""""""Basic normalization for a string sentence.\n\n    Normalization includes\n        - lowercasing\n        - complete some basic text normalization for English words as follows:\n            - add spaces before and after \'\\\'\'\n            - remove \'\\""\',\n            - add spaces before and after \'.\'\n            - replace \'<br \\/>\'with single space\n            - add spaces before and after \',\'\n            - add spaces before and after \'(\'\n            - add spaces before and after \')\'\n            - add spaces before and after \'!\'\n            - add spaces before and after \'?\'\n            - replace \';\' with single space\n            - replace \':\' with single space\n            - replace multiple spaces with single space\n\n    Examples:\n        >>> import torch\n        >>> from torchtext.experimental.transforms import BasicEnglishNormalize\n        >>> test_sample = \'Basic English Normalization for a Line of Text\'\n        >>> basic_english_normalize = BasicEnglishNormalize()\n        >>> jit_basic_english_normalize = torch.jit.script(basic_english_normalize)\n        >>> tokens = jit_basic_english_normalize(test_sample)\n    """"""\n\n    regex_and_replacement_string_pairs: List[Tuple[torch.classes.torchtext.Regex, str]]\n\n    def __init__(self):\n        super(BasicEnglishNormalize, self).__init__()\n        patterns_list = [\n            (r\'\\\'\', \' \\\'  \'),\n            (r\'\\""\', \'\'),\n            (r\'\\.\', \' . \'),\n            (r\'<br \\/>\', \' \'),\n            (r\',\', \' , \'),\n            (r\'\\(\', \' ( \'),\n            (r\'\\)\', \' ) \'),\n            (r\'\\!\', \' ! \'),\n            (r\'\\?\', \' ? \'),\n            (r\'\\;\', \' \'),\n            (r\'\\:\', \' \'),\n            (r\'\\s+\', \' \')]\n\n        regex_objects = map(lambda pattern_tuple: torch.classes.torchtext.Regex(pattern_tuple[0]), patterns_list)\n        replacement_strings = map(lambda pattern_tuple: pattern_tuple[1], patterns_list)\n        self.regex_and_replacement_string_pairs = list(zip(regex_objects, replacement_strings))\n\n    def forward(self, line: str) -> List[str]:\n        r""""""\n        Args:\n            line (str): a line of text to tokenize.\n        Returns:\n            List[str]: a list of tokens after normalizing and splitting on whitespace.\n        """"""\n\n        line = line.lower()\n        for regex, replacement_string in self.regex_and_replacement_string_pairs:\n            line = regex.Sub(line, replacement_string)\n        return line.split()\n\n\nclass RegexTokenizer(nn.Module):\n    r""""""Regex tokenizer for a string sentence that applies all regex replacements defined in patterns_list.\n\n    Args:\n        patterns_list (List[Tuple[str, str]]): a list of tuples (ordered pairs) which contain the regex pattern string\n        as the first element and the replacement string as the second element.\n\n    Examples:\n        >>> import torch\n        >>> from torchtext.experimental.transforms import RegexTokenizer\n        >>> test_sample = \'Basic Regex Tokenization for a Line of Text\'\n        >>> patterns_list = [\n            (r\'\\\'\', \' \\\'  \'),\n            (r\'\\""\', \'\')]\n        >>> regex_tokenizer = RegexTokenizer(patterns_list)\n        >>> jit_regex_tokenizer = torch.jit.script(regex_tokenizer)\n        >>> tokens = jit_regex_tokenizer(test_sample)\n    """"""\n\n    regex_and_replacement_string_pairs: List[Tuple[torch.classes.torchtext.Regex, str]]\n\n    def __init__(self, patterns_list: List[Tuple[str, str]]):\n        super(RegexTokenizer, self).__init__()\n\n        regex_objects = map(lambda pattern_tuple: torch.classes.torchtext.Regex(pattern_tuple[0]), patterns_list)\n        replacement_strings = map(lambda pattern_tuple: pattern_tuple[1], patterns_list)\n        self.regex_and_replacement_string_pairs = list(zip(regex_objects, replacement_strings))\n\n    def forward(self, line: str) -> List[str]:\n        r""""""\n        Args:\n            line (str): a line of text to tokenize.\n        Returns:\n            List[str]: a list of tokens after normalizing and splitting on whitespace.\n        """"""\n\n        for regex, replacement_string in self.regex_and_replacement_string_pairs:\n            line = regex.Sub(line, replacement_string)\n        return line.split()\n'"
torchtext/experimental/datasets/__init__.py,0,"b""from .language_modeling import LanguageModelingDataset, WikiText2, WikiText103, PennTreebank, WMTNewsCrawl  # NOQA\nfrom .text_classification import AG_NEWS, SogouNews, DBpedia, YelpReviewPolarity, \\\n    YelpReviewFull, YahooAnswers, \\\n    AmazonReviewPolarity, AmazonReviewFull, IMDB\n\n__all__ = ['LanguageModelingDataset',\n           'WikiText2',\n           'WikiText103',\n           'PennTreebank',\n           'WMTNewsCrawl',\n           'IMDB',\n           'AG_NEWS',\n           'SogouNews',\n           'DBpedia',\n           'YelpReviewPolarity',\n           'YelpReviewFull',\n           'YahooAnswers',\n           'AmazonReviewPolarity',\n           'AmazonReviewFull']\n"""
torchtext/experimental/datasets/language_modeling.py,4,"b'import torch\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.experimental.datasets.raw import language_modeling as raw\nfrom torchtext.experimental.functional import vocab_func, totensor, sequential_transforms\n\n\ndef build_vocab(data, transforms):\n    tok_list = []\n    for txt in data:\n        tok_list.append(transforms(txt))\n    return build_vocab_from_iterator(tok_list)\n\n\nclass LanguageModelingDataset(torch.utils.data.Dataset):\n    """"""Defines a dataset for language modeling.\n       Currently, we only support the following datasets:\n\n             - WikiText2\n             - WikiText103\n             - PennTreebank\n             - WMTNewsCrawl\n\n    """"""\n\n    def __init__(self, data, vocab, transforms, single_line):\n        """"""Initiate language modeling dataset.\n\n        Arguments:\n            data: a tensor of tokens. tokens are ids after\n                numericalizing the string tokens.\n                torch.tensor([token_id_1, token_id_2, token_id_3, token_id1]).long()\n            vocab: Vocabulary object used for dataset.\n            transforms: Text string transforms.\n\n        """"""\n\n        super(LanguageModelingDataset, self).__init__()\n        self.vocab = vocab\n        self.transforms = transforms\n        self.single_line = single_line\n        if single_line:\n            self.data = torch.cat(tuple(transforms(row) for row in data), axis=0)\n        else:\n            self.data = data\n\n    def __getitem__(self, i):\n        if self.single_line:\n            return self.data[i]\n        else:\n            return self.transforms(self.data[i])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __iter__(self):\n        for x in self.data:\n            yield x\n\n    def get_vocab(self):\n        return self.vocab\n\n\ndef _setup_datasets(dataset_name, tokenizer=None, root=\'.data\', vocab=None,\n                    data_select=(\'train\', \'test\', \'valid\'), single_line=True):\n    if tokenizer is None:\n        tokenizer = get_tokenizer(\'basic_english\')\n    text_transform = sequential_transforms(tokenizer)\n\n    if isinstance(data_select, str):\n        data_select = [data_select]\n    if not set(data_select).issubset(set((\'train\', \'valid\', \'test\'))):\n        raise TypeError(\'Given data selection {} is not supported!\'.format(data_select))\n\n    if not single_line and dataset_name != \'WikiText103\':\n        raise TypeError(\'single_line must be True except for WikiText103\')\n    if dataset_name == \'WMTNewsCrawl\':\n        train, = raw.DATASETS[dataset_name](root=root, data_select=(\'train\',))\n        if single_line:\n            raw_data = {\'train\': ["" "".join([txt for txt in train]), ]}\n        else:\n            raw_data = {\'train\': [txt for txt in train]}\n    else:\n        train, test, valid = raw.DATASETS[dataset_name](root=root, data_select=(\'train\', \'test\', \'valid\'))\n        # Cache raw text iterable dataset\n        if single_line:\n            raw_data = {\'train\': ["" "".join([txt for txt in train]), ],\n                        \'valid\': ["" "".join(txt for txt in valid), ],\n                        \'test\': ["" "".join(txt for txt in test), ]}\n        else:\n            raw_data = {\'train\': [txt for txt in train],\n                        \'valid\': [txt for txt in valid],\n                        \'test\': [txt for txt in test]}\n\n    if vocab is None:\n        if \'train\' not in data_select:\n            raise TypeError(""Must pass a vocab if train is not selected."")\n        vocab = build_vocab(raw_data[\'train\'], text_transform)\n    text_transform = sequential_transforms(text_transform, vocab_func(vocab),\n                                           totensor(dtype=torch.long))\n    return tuple(LanguageModelingDataset(raw_data[item], vocab, text_transform, single_line)\n                 for item in data_select)\n\n\ndef WikiText2(*args, **kwargs):\n    """""" Defines WikiText2 datasets.\n\n    Create language modeling dataset: WikiText2\n    Separately returns the train/test/valid set\n\n    Arguments:\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well (see example below). A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        root: Directory where the datasets are saved. Default: "".data""\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        data_select: a string or tupel for the returned datasets\n            (Default: (\'train\', \'test\',\'valid\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n        single_line: whether to return all tokens in a single line.\n            (Default: True)\n            By default, all lines in raw text file are concatenated into a single line.\n            Use `single_line = False` if one wants to get data line by line.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import WikiText2\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train_dataset, test_dataset, valid_dataset = WikiText2(tokenizer=tokenizer)\n        >>> vocab = train_dataset.get_vocab()\n        >>> valid_dataset, = WikiText2(tokenizer=tokenizer, vocab=vocab,\n                                       data_select=\'valid\')\n\n    """"""\n\n    return _setup_datasets(*((""WikiText2"",) + args), **kwargs)\n\n\ndef WikiText103(*args, **kwargs):\n    """""" Defines WikiText103 datasets.\n\n    Create language modeling dataset: WikiText103\n    Separately returns the train/test/valid set\n\n    Arguments:\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well (see example below). A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        root: Directory where the datasets are saved. Default: "".data""\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        data_select: a string or tupel for the returned datasets\n            (Default: (\'train\', \'test\',\'valid\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n        single_line: whether to return all tokens in a single line.\n            (Default: True)\n            By default, all lines in raw text file are concatenated into a single line.\n            Use `single_line = False` if one wants to get data line by line.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import WikiText103\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train_dataset, test_dataset, valid_dataset = WikiText103(tokenizer=tokenizer)\n        >>> vocab = train_dataset.get_vocab()\n        >>> valid_dataset, = WikiText103(tokenizer=tokenizer, vocab=vocab,\n                                         data_select=\'valid\')\n\n    """"""\n\n    return _setup_datasets(*((""WikiText103"",) + args), **kwargs)\n\n\ndef PennTreebank(*args, **kwargs):\n    """""" Defines PennTreebank datasets.\n\n    Create language modeling dataset: PennTreebank\n    Separately returns the train/test/valid set\n\n    Arguments:\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well (see example below). A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        root: Directory where the datasets are saved. Default: "".data""\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        data_select: a string or tupel for the returned datasets\n            (Default: (\'train\', \'test\',\'valid\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n        single_line: whether to return all tokens in a single line.\n            (Default: True)\n            By default, all lines in raw text file are concatenated into a single line.\n            Use `single_line = False` if one wants to get data line by line.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import PennTreebank\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train_dataset, test_dataset, valid_dataset = PennTreebank(tokenizer=tokenizer)\n        >>> vocab = train_dataset.get_vocab()\n        >>> valid_dataset, = PennTreebank(tokenizer=tokenizer, vocab=vocab,\n                                          data_select=\'valid\')\n\n    """"""\n\n    return _setup_datasets(*((""PennTreebank"",) + args), **kwargs)\n\n\ndef WMTNewsCrawl(*args, **kwargs):\n    """""" Defines WMTNewsCrawl datasets.\n\n    Create language modeling dataset: WMTNewsCrawl\n    returns the train set\n\n    Arguments:\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well (see example below). A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        root: Directory where the datasets are saved. Default: "".data""\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        data_select: a string or tupel for the returned datasets\n            (Default: (\'train\',))\n        single_line: whether to return all tokens in a single line.\n            (Default: True)\n            By default, all lines in raw text file are concatenated into a single line.\n            Use `single_line = False` if one wants to get data line by line.\n    Examples:\n        >>> from torchtext.experimental.datasets import WMTNewsCrawl\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train_dataset, = WMTNewsCrawl(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""WMTNewsCrawl"",) + args), **kwargs)\n\n\nDATASETS = {\n    \'WikiText2\': WikiText2,\n    \'WikiText103\': WikiText103,\n    \'PennTreebank\': PennTreebank,\n    \'WMTNewsCrawl\': WMTNewsCrawl\n}\n'"
torchtext/experimental/datasets/text_classification.py,3,"b'import torch\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.experimental.datasets.raw import text_classification as raw\nfrom torchtext.experimental.functional import (\n    vocab_func,\n    totensor,\n    ngrams_func,\n    sequential_transforms,\n)\n\n\ndef _build_vocab(data, transforms):\n    tok_list = []\n    for _, txt in data:\n        tok_list.append(transforms(txt))\n    return build_vocab_from_iterator(tok_list)\n\n\nclass TextClassificationDataset(torch.utils.data.Dataset):\n    """"""Defines an abstract text classification datasets.\n       Currently, we only support the following datasets:\n             - AG_NEWS\n             - SogouNews\n             - DBpedia\n             - YelpReviewPolarity\n             - YelpReviewFull\n             - YahooAnswers\n             - AmazonReviewPolarity\n             - AmazonReviewFull\n    """"""\n\n    def __init__(self, data, vocab, transforms):\n        """"""Initiate text-classification dataset.\n\n        Arguments:\n            data: a list of label and text tring tuple. label is an integer.\n                [(label1, text1), (label2, text2), (label2, text3)]\n            vocab: Vocabulary object used for dataset.\n            transforms: a tuple of label and text string transforms.\n        """"""\n\n        super(TextClassificationDataset, self).__init__()\n        self.data = data\n        self.vocab = vocab\n        self.transforms = transforms  # (label_transforms, tokens_transforms)\n\n    def __getitem__(self, i):\n        label = self.data[i][0]\n        txt = self.data[i][1]\n        return (self.transforms[0](label), self.transforms[1](txt))\n\n    def __len__(self):\n        return len(self.data)\n\n    def get_labels(self):\n        labels = []\n        for item in self.data:\n            label = item[0]\n            labels.apppend(self.transforms[0](label))\n        return set(labels)\n\n    def get_vocab(self):\n        return self.vocab\n\n\ndef _setup_datasets(\n    dataset_name,\n    root="".data"",\n    ngrams=1,\n    vocab=None,\n    tokenizer=None,\n    data_select=(""train"", ""test""),\n):\n    text_transform = []\n    if tokenizer is None:\n        tokenizer = get_tokenizer(""basic_english"")\n    text_transform = sequential_transforms(tokenizer, ngrams_func(ngrams))\n\n    if isinstance(data_select, str):\n        data_select = [data_select]\n    if not set(data_select).issubset(set((""train"", ""test""))):\n        raise TypeError(""Given data selection {} is not supported!"".format(data_select))\n    train, test = raw.DATASETS[dataset_name](root=root)\n    # Cache raw text iterable dataset\n    raw_data = {\n        ""train"": [(label, txt) for (label, txt) in train],\n        ""test"": [(label, txt) for (label, txt) in test],\n    }\n\n    if vocab is None:\n        if ""train"" not in data_select:\n            raise TypeError(""Must pass a vocab if train is not selected."")\n        vocab = _build_vocab(raw_data[""train""], text_transform)\n    text_transform = sequential_transforms(\n        text_transform, vocab_func(vocab), totensor(dtype=torch.long)\n    )\n    label_transform = sequential_transforms(totensor(dtype=torch.long))\n    return tuple(\n        TextClassificationDataset(\n            raw_data[item], vocab, (label_transform, text_transform)\n        )\n        for item in data_select\n    )\n\n\ndef AG_NEWS(*args, **kwargs):\n    """""" Defines AG_NEWS datasets.\n        The labels includes:\n            - 1 : World\n            - 2 : Sports\n            - 3 : Business\n            - 4 : Sci/Tech\n\n    Create text classification dataset: AG_NEWS\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import AG_NEWS\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = AG_NEWS(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = AG_NEWS(tokenizer=tokenizer)\n        >>> train, = AG_NEWS(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""AG_NEWS"",) + args), **kwargs)\n\n\ndef SogouNews(*args, **kwargs):\n    """""" Defines SogouNews datasets.\n        The labels includes:\n            - 1 : Sports\n            - 2 : Finance\n            - 3 : Entertainment\n            - 4 : Automobile\n            - 5 : Technology\n\n    Create text classification dataset: SogouNews\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import SogouNews\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = SogouNews(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = SogouNews(tokenizer=tokenizer)\n        >>> train, = SogouNews(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""SogouNews"",) + args), **kwargs)\n\n\ndef DBpedia(*args, **kwargs):\n    """""" Defines DBpedia datasets.\n        The labels includes:\n            - 1 : Company\n            - 2 : EducationalInstitution\n            - 3 : Artist\n            - 4 : Athlete\n            - 5 : OfficeHolder\n            - 6 : MeanOfTransportation\n            - 7 : Building\n            - 8 : NaturalPlace\n            - 9 : Village\n            - 10 : Animal\n            - 11 : Plant\n            - 12 : Album\n            - 13 : Film\n            - 14 : WrittenWork\n\n    Create text classification dataset: DBpedia\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import DBpedia\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = DBpedia(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = DBpedia(tokenizer=tokenizer)\n        >>> train, = DBpedia(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""DBpedia"",) + args), **kwargs)\n\n\ndef YelpReviewPolarity(*args, **kwargs):\n    """""" Defines YelpReviewPolarity datasets.\n        The labels includes:\n            - 1 : Negative polarity.\n            - 2 : Positive polarity.\n\n    Create text classification dataset: YelpReviewPolarity\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import YelpReviewPolarity\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = YelpReviewPolarity(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = YelpReviewPolarity(tokenizer=tokenizer)\n        >>> train, = YelpReviewPolarity(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""YelpReviewPolarity"",) + args), **kwargs)\n\n\ndef YelpReviewFull(*args, **kwargs):\n    """""" Defines YelpReviewFull datasets.\n        The labels includes:\n            1 - 5 : rating classes (5 is highly recommended).\n\n    Create text classification dataset: YelpReviewFull\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import YelpReviewFull\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = YelpReviewFull(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = YelpReviewFull(tokenizer=tokenizer)\n        >>> train, = YelpReviewFull(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""YelpReviewFull"",) + args), **kwargs)\n\n\ndef YahooAnswers(*args, **kwargs):\n    """""" Defines YahooAnswers datasets.\n        The labels includes:\n            - 1 : Society & Culture\n            - 2 : Science & Mathematics\n            - 3 : Health\n            - 4 : Education & Reference\n            - 5 : Computers & Internet\n            - 6 : Sports\n            - 7 : Business & Finance\n            - 8 : Entertainment & Music\n            - 9 : Family & Relationships\n            - 10 : Politics & Government\n\n    Create text classification dataset: YahooAnswers\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import YahooAnswers\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = YahooAnswers(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = YahooAnswers(tokenizer=tokenizer)\n        >>> train, = YahooAnswers(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""YahooAnswers"",) + args), **kwargs)\n\n\ndef AmazonReviewPolarity(*args, **kwargs):\n    """""" Defines AmazonReviewPolarity datasets.\n        The labels includes:\n            - 1 : Negative polarity\n            - 2 : Positive polarity\n\n    Create text classification dataset: AmazonReviewPolarity\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import AmazonReviewPolarity\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = AmazonReviewPolarity(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = AmazonReviewPolarity(tokenizer=tokenizer)\n        >>> train, = AmazonReviewPolarity(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""AmazonReviewPolarity"",) + args), **kwargs)\n\n\ndef AmazonReviewFull(*args, **kwargs):\n    """""" Defines AmazonReviewFull datasets.\n        The labels includes:\n            1 - 5 : rating classes (5 is highly recommended)\n\n    Create text classification dataset: AmazonReviewFull\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import AmazonReviewFull\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = AmazonReviewFull(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = AmazonReviewFull(tokenizer=tokenizer)\n        >>> train, = AmazonReviewFull(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""AmazonReviewFull"",) + args), **kwargs)\n\n\ndef IMDB(*args, **kwargs):\n    """""" Defines IMDB datasets.\n        The labels includes:\n            - 0 : Negative\n            - 1 : Positive\n\n    Create sentiment analysis dataset: IMDB\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        ngrams: a contiguous sequence of n items from s string text.\n            Default: 1\n        vocab: Vocabulary used for dataset. If None, it will generate a new\n            vocabulary based on the train data set.\n        removed_tokens: removed tokens from output dataset (Default: [])\n        tokenizer: the tokenizer used to preprocess raw text data.\n            The default one is basic_english tokenizer in fastText. spacy tokenizer\n            is supported as well. A custom tokenizer is callable\n            function with input of a string and output of a token list.\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets import IMDB\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> train, test = IMDB(ngrams=3)\n        >>> tokenizer = get_tokenizer(""spacy"")\n        >>> train, test = IMDB(tokenizer=tokenizer)\n        >>> train, = IMDB(tokenizer=tokenizer, data_select=\'train\')\n\n    """"""\n\n    return _setup_datasets(*((""IMDB"",) + args), **kwargs)\n\n\nDATASETS = {\n    ""AG_NEWS"": AG_NEWS,\n    ""SogouNews"": SogouNews,\n    ""DBpedia"": DBpedia,\n    ""YelpReviewPolarity"": YelpReviewPolarity,\n    ""YelpReviewFull"": YelpReviewFull,\n    ""YahooAnswers"": YahooAnswers,\n    ""AmazonReviewPolarity"": AmazonReviewPolarity,\n    ""AmazonReviewFull"": AmazonReviewFull,\n    ""IMDB"": IMDB,\n}\n\n\nLABELS = {\n    ""AG_NEWS"": {1: ""World"", 2: ""Sports"", 3: ""Business"", 4: ""Sci/Tech""},\n    ""SogouNews"": {\n        1: ""Sports"",\n        2: ""Finance"",\n        3: ""Entertainment"",\n        4: ""Automobile"",\n        5: ""Technology"",\n    },\n    ""DBpedia"": {\n        1: ""Company"",\n        2: ""EducationalInstitution"",\n        3: ""Artist"",\n        4: ""Athlete"",\n        5: ""OfficeHolder"",\n        6: ""MeanOfTransportation"",\n        7: ""Building"",\n        8: ""NaturalPlace"",\n        9: ""Village"",\n        10: ""Animal"",\n        11: ""Plant"",\n        12: ""Album"",\n        13: ""Film"",\n        14: ""WrittenWork"",\n    },\n    ""YelpReviewPolarity"": {1: ""Negative polarity"", 2: ""Positive polarity""},\n    ""YelpReviewFull"": {\n        1: ""score 1"",\n        2: ""score 2"",\n        3: ""score 3"",\n        4: ""score 4"",\n        5: ""score 5"",\n    },\n    ""YahooAnswers"": {\n        1: ""Society & Culture"",\n        2: ""Science & Mathematics"",\n        3: ""Health"",\n        4: ""Education & Reference"",\n        5: ""Computers & Internet"",\n        6: ""Sports"",\n        7: ""Business & Finance"",\n        8: ""Entertainment & Music"",\n        9: ""Family & Relationships"",\n        10: ""Politics & Government"",\n    },\n    ""AmazonReviewPolarity"": {1: ""Negative polarity"", 2: ""Positive polarity""},\n    ""AmazonReviewFull"": {\n        1: ""score 1"",\n        2: ""score 2"",\n        3: ""score 3"",\n        4: ""score 4"",\n        5: ""score 5"",\n    },\n    ""IMDB"": {0: ""Negative"", 1: ""Positive""},\n}\n'"
torchtext/experimental/datasets/translation.py,5,"b'import torch\nimport logging\n\nfrom torchtext.experimental.datasets import raw\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom ..functional import vocab_func, totensor, sequential_transforms\n\n\ndef build_vocab(data, transforms, index):\n    tok_list = []\n    for line in data:\n        tok_list.append(transforms(line[index]))\n    return build_vocab_from_iterator(tok_list)\n\n\ndef _setup_datasets(dataset_name,\n                    train_filenames,\n                    valid_filenames,\n                    test_filenames,\n                    data_select=(\'train\', \'test\', \'valid\'),\n                    root=\'.data\',\n                    vocab=(None, None),\n                    tokenizer=None,\n                    removed_tokens=[\'<unk>\']):\n    src_vocab, tgt_vocab = vocab\n    if tokenizer is None:\n        src_tokenizer = get_tokenizer(""spacy"", language=\'de_core_news_sm\')\n        tgt_tokenizer = get_tokenizer(""spacy"", language=\'en_core_web_sm\')\n    elif isinstance(tokenizer, tuple):\n        if len(tokenizer) == 2:\n            src_tokenizer, tgt_tokenizer = tokenizer\n        else:\n            raise ValueError(""tokenizer must have length of two for""\n                             ""source and target"")\n    else:\n        raise ValueError(\n            ""tokenizer must be an instance of tuple with length two""\n            ""or None"")\n    train, val, test = DATASETS[dataset_name](train_filenames=train_filenames,\n                                              valid_filenames=valid_filenames,\n                                              test_filenames=test_filenames,\n                                              root=root)\n    raw_data = {\n        ""train"": [line for line in train],\n        ""valid"": [line for line in val],\n        ""test"": [line for line in test]\n    }\n    src_text_vocab_transform = sequential_transforms(src_tokenizer)\n    tgt_text_vocab_transform = sequential_transforms(tgt_tokenizer)\n\n    if src_vocab is None:\n        if \'train\' not in data_select:\n            raise TypeError(""Must pass a vocab if train is not selected."")\n        logging.info(\'Building src Vocab based on train data\')\n        src_vocab = build_vocab(raw_data[""train""],\n                                src_text_vocab_transform,\n                                index=0)\n    else:\n        if not isinstance(src_vocab, Vocab):\n            raise TypeError(""Passed src vocabulary is not of type Vocab"")\n    logging.info(\'src Vocab has {} entries\'.format(len(src_vocab)))\n\n    if tgt_vocab is None:\n        if \'train\' not in data_select:\n            raise TypeError(""Must pass a vocab if train is not selected."")\n        logging.info(\'Building tgt Vocab based on train data\')\n        tgt_vocab = build_vocab(raw_data[""train""],\n                                tgt_text_vocab_transform,\n                                index=1)\n    else:\n        if not isinstance(tgt_vocab, Vocab):\n            raise TypeError(""Passed tgt vocabulary is not of type Vocab"")\n    logging.info(\'tgt Vocab has {} entries\'.format(len(tgt_vocab)))\n\n    logging.info(\'Building datasets for {}\'.format(data_select))\n    datasets = []\n    for key in data_select:\n        src_text_transform = sequential_transforms(src_text_vocab_transform,\n                                                   vocab_func(src_vocab),\n                                                   totensor(dtype=torch.long))\n        tgt_text_transform = sequential_transforms(tgt_text_vocab_transform,\n                                                   vocab_func(tgt_vocab),\n                                                   totensor(dtype=torch.long))\n        datasets.append(\n            TranslationDataset(raw_data[key], (src_vocab, tgt_vocab),\n                               (src_text_transform, tgt_text_transform)))\n\n    return tuple(datasets)\n\n\nclass TranslationDataset(torch.utils.data.Dataset):\n    """"""Defines a dataset for translation.\n       Currently, we only support the following datasets:\n             - Multi30k\n             - WMT14\n             - IWSLT\n    """"""\n    def __init__(self, data, vocab, transforms):\n        """"""Initiate translation dataset.\n\n        Arguments:\n            data: a tuple of source and target tensors, which include token ids\n                numericalizing the string tokens.\n                [(src_tensor0, tgt_tensor0), (src_tensor1, tgt_tensor1)]\n            vocab: source and target Vocabulary object used for dataset.\n                (src_vocab, tgt_vocab)\n            transforms: a tuple of source and target string transforms.\n\n        Examples:\n            >>> from torchtext.vocab import build_vocab_from_iterator\n            >>> src_data = torch.Tensor([token_id_s1, token_id_s2,\n                                         token_id_s3, token_id_s1]).long()\n            >>> tgt_data = torch.Tensor([token_id_t1, token_id_t2,\n                                         token_id_t3, token_id_t1]).long()\n            >>> src_vocab = build_vocab_from_iterator([[\'\xc3\x9cbersetzungsdatensatz\']])\n            >>> tgt_vocab = build_vocab_from_iterator([[\'translation\', \'dataset\']])\n            >>> dataset = TranslationDataset([(src_data, tgt_data)],\n                                              (src_vocab, tgt_vocab))\n        """"""\n\n        super(TranslationDataset, self).__init__()\n        self.data = data\n        self.vocab = vocab\n        self.transforms = transforms\n\n    def __getitem__(self, i):\n        source = self.transforms[0](self.data[i][0])\n        target = self.transforms[1](self.data[i][1])\n        return (source, target)\n\n    def __len__(self):\n        return len(self.data)\n\n    def get_vocab(self):\n        return self.vocab\n\n\ndef Multi30k(train_filenames=(""train.de"", ""train.en""),\n             valid_filenames=(""val.de"", ""val.en""),\n             test_filenames=(""test_2016_flickr.de"", ""test_2016_flickr.en""),\n             tokenizer=None,\n             root=\'.data\',\n             vocab=(None, None),\n             data_select=(\'train\', \'valid\', \'test\'),\n             removed_tokens=[\'<unk>\']):\n    """""" Define translation datasets: Multi30k\n        Separately returns train/valid/test datasets as a tuple\n\n    Arguments:\n        train_filenames: the source and target filenames for training.\n            Default: (\'train.de\', \'train.en\')\n        valid_filenames: the source and target filenames for valid.\n            Default: (\'val.de\', \'val.en\')\n        test_filenames: the source and target filenames for test.\n            Default: (\'test2016.de\', \'test2016.en\')\n        tokenizer: the tokenizer used to preprocess source and target raw text data.\n            It has to be in a form of tuple.\n            Default: (get_tokenizer(""spacy"", language=\'de_core_news_sm\'),\n                      get_tokenizer(""spacy"", language=\'en_core_web_sm\'))\n        root: Directory where the datasets are saved. Default: "".data""\n        vocab: Source and target Vocabulary objects used for dataset. If None, it\n            will generate a new vocabulary based on the train data set. It has to be\n            in a form of tuple.\n            Default: (None, None)\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'valid\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n        removed_tokens: removed tokens from output dataset (Default: \'<unk>\')\n        The available dataset include:\n            test_2016_flickr.cs\n            test_2016_flickr.de\n            test_2016_flickr.en\n            test_2016_flickr.fr\n            test_2017_flickr.de\n            test_2017_flickr.en\n            test_2017_flickr.fr\n            test_2017_mscoco.de\n            test_2017_mscoco.en\n            test_2017_mscoco.fr\n            test_2018_flickr.en\n            train.cs\n            train.de\n            train.en\n            train.fr\n            val.cs\n            val.de\n            val.en\n            val.fr\n            test_2016.1.de\n            test_2016.1.en\n            test_2016.2.de\n            test_2016.2.en\n            test_2016.3.de\n            test_2016.3.en\n            test_2016.4.de\n            test_2016.4.en\n            test_2016.5.de\n            test_2016.5.en\n            train.1.de\n            train.1.en\n            train.2.de\n            train.2.en\n            train.3.de\n            train.3.en\n            train.4.de\n            train.4.en\n            train.5.de\n            train.5.en\n            val.1.de\n            val.1.en\n            val.2.de\n            val.2.en\n            val.3.de\n            val.3.en\n            val.4.de\n            val.4.en\n            val.5.de\n            val.5.en\n\n    Examples:\n        >>> from torchtext.datasets import Multi30k\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> tokenizer = (get_tokenizer(""spacy"", language=\'de\'),\n                         get_tokenizer(""basic_english""))\n        >>> train_dataset, valid_dataset, test_dataset = Multi30k(tokenizer=tokenizer)\n        >>> src_vocab, tgt_vocab = train_dataset.get_vocab()\n        >>> src_data, tgt_data = train_dataset[10]\n    """"""\n    return _setup_datasets(""Multi30k"",\n                           train_filenames=train_filenames,\n                           valid_filenames=valid_filenames,\n                           test_filenames=test_filenames,\n                           tokenizer=tokenizer,\n                           root=root,\n                           vocab=vocab,\n                           removed_tokens=removed_tokens)\n\n\ndef IWSLT(train_filenames=(\'train.de-en.de\', \'train.de-en.en\'),\n          valid_filenames=(\'IWSLT16.TED.tst2013.de-en.de\',\n                           \'IWSLT16.TED.tst2013.de-en.en\'),\n          test_filenames=(\'IWSLT16.TED.tst2014.de-en.de\',\n                          \'IWSLT16.TED.tst2014.de-en.en\'),\n          tokenizer=None,\n          root=\'.data\',\n          vocab=(None, None),\n          data_select=(\'train\', \'valid\', \'test\'),\n          removed_tokens=[\'<unk>\']):\n    """""" Define translation datasets: IWSLT\n        Separately returns train/valid/test datasets\n        The available datasets include:\n\n    Arguments:\n        train_filenames: the source and target filenames for training.\n            Default: (\'train.de-en.de\', \'train.de-en.en\')\n        valid_filenames: the source and target filenames for valid.\n            Default: (\'IWSLT16.TED.tst2013.de-en.de\', \'IWSLT16.TED.tst2013.de-en.en\')\n        test_filenames: the source and target filenames for test.\n            Default: (\'IWSLT16.TED.tst2014.de-en.de\', \'IWSLT16.TED.tst2014.de-en.en\')\n        tokenizer: the tokenizer used to preprocess source and target raw text data.\n            It has to be in a form of tuple.\n            Default: (get_tokenizer(""spacy"", language=\'de_core_news_sm\'),\n                      get_tokenizer(""spacy"", language=\'en_core_web_sm\'))\n        root: Directory where the datasets are saved. Default: "".data""\n        vocab: Source and target Vocabulary objects used for dataset. If None, it\n            will generate a new vocabulary based on the train data set. It has to be\n            in a form of tuple.\n            Default: (None, None)\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'valid\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n        removed_tokens: removed tokens from output dataset (Default: \'<unk>\')\n        The available datasets include:\n            IWSLT16.TED.dev2010.ar-en.ar\n            IWSLT16.TED.dev2010.ar-en.en\n            IWSLT16.TED.dev2010.cs-en.cs\n            IWSLT16.TED.dev2010.cs-en.en\n            IWSLT16.TED.dev2010.de-en.de\n            IWSLT16.TED.dev2010.de-en.en\n            IWSLT16.TED.dev2010.en-ar.ar\n            IWSLT16.TED.dev2010.en-ar.en\n            IWSLT16.TED.dev2010.en-cs.cs\n            IWSLT16.TED.dev2010.en-cs.en\n            IWSLT16.TED.dev2010.en-de.de\n            IWSLT16.TED.dev2010.en-de.en\n            IWSLT16.TED.dev2010.en-fr.en\n            IWSLT16.TED.dev2010.en-fr.fr\n            IWSLT16.TED.dev2010.fr-en.en\n            IWSLT16.TED.dev2010.fr-en.fr\n            IWSLT16.TED.tst2010.ar-en.ar\n            IWSLT16.TED.tst2010.ar-en.en\n            IWSLT16.TED.tst2010.cs-en.cs\n            IWSLT16.TED.tst2010.cs-en.en\n            IWSLT16.TED.tst2010.de-en.de\n            IWSLT16.TED.tst2010.de-en.en\n            IWSLT16.TED.tst2010.en-ar.ar\n            IWSLT16.TED.tst2010.en-ar.en\n            IWSLT16.TED.tst2010.en-cs.cs\n            IWSLT16.TED.tst2010.en-cs.en\n            IWSLT16.TED.tst2010.en-de.de\n            IWSLT16.TED.tst2010.en-de.en\n            IWSLT16.TED.tst2010.en-fr.en\n            IWSLT16.TED.tst2010.en-fr.fr\n            IWSLT16.TED.tst2010.fr-en.en\n            IWSLT16.TED.tst2010.fr-en.fr\n            IWSLT16.TED.tst2011.ar-en.ar\n            IWSLT16.TED.tst2011.ar-en.en\n            IWSLT16.TED.tst2011.cs-en.cs\n            IWSLT16.TED.tst2011.cs-en.en\n            IWSLT16.TED.tst2011.de-en.de\n            IWSLT16.TED.tst2011.de-en.en\n            IWSLT16.TED.tst2011.en-ar.ar\n            IWSLT16.TED.tst2011.en-ar.en\n            IWSLT16.TED.tst2011.en-cs.cs\n            IWSLT16.TED.tst2011.en-cs.en\n            IWSLT16.TED.tst2011.en-de.de\n            IWSLT16.TED.tst2011.en-de.en\n            IWSLT16.TED.tst2011.en-fr.en\n            IWSLT16.TED.tst2011.en-fr.fr\n            IWSLT16.TED.tst2011.fr-en.en\n            IWSLT16.TED.tst2011.fr-en.fr\n            IWSLT16.TED.tst2012.ar-en.ar\n            IWSLT16.TED.tst2012.ar-en.en\n            IWSLT16.TED.tst2012.cs-en.cs\n            IWSLT16.TED.tst2012.cs-en.en\n            IWSLT16.TED.tst2012.de-en.de\n            IWSLT16.TED.tst2012.de-en.en\n            IWSLT16.TED.tst2012.en-ar.ar\n            IWSLT16.TED.tst2012.en-ar.en\n            IWSLT16.TED.tst2012.en-cs.cs\n            IWSLT16.TED.tst2012.en-cs.en\n            IWSLT16.TED.tst2012.en-de.de\n            IWSLT16.TED.tst2012.en-de.en\n            IWSLT16.TED.tst2012.en-fr.en\n            IWSLT16.TED.tst2012.en-fr.fr\n            IWSLT16.TED.tst2012.fr-en.en\n            IWSLT16.TED.tst2012.fr-en.fr\n            IWSLT16.TED.tst2013.ar-en.ar\n            IWSLT16.TED.tst2013.ar-en.en\n            IWSLT16.TED.tst2013.cs-en.cs\n            IWSLT16.TED.tst2013.cs-en.en\n            IWSLT16.TED.tst2013.de-en.de\n            IWSLT16.TED.tst2013.de-en.en\n            IWSLT16.TED.tst2013.en-ar.ar\n            IWSLT16.TED.tst2013.en-ar.en\n            IWSLT16.TED.tst2013.en-cs.cs\n            IWSLT16.TED.tst2013.en-cs.en\n            IWSLT16.TED.tst2013.en-de.de\n            IWSLT16.TED.tst2013.en-de.en\n            IWSLT16.TED.tst2013.en-fr.en\n            IWSLT16.TED.tst2013.en-fr.fr\n            IWSLT16.TED.tst2013.fr-en.en\n            IWSLT16.TED.tst2013.fr-en.fr\n            IWSLT16.TED.tst2014.ar-en.ar\n            IWSLT16.TED.tst2014.ar-en.en\n            IWSLT16.TED.tst2014.de-en.de\n            IWSLT16.TED.tst2014.de-en.en\n            IWSLT16.TED.tst2014.en-ar.ar\n            IWSLT16.TED.tst2014.en-ar.en\n            IWSLT16.TED.tst2014.en-de.de\n            IWSLT16.TED.tst2014.en-de.en\n            IWSLT16.TED.tst2014.en-fr.en\n            IWSLT16.TED.tst2014.en-fr.fr\n            IWSLT16.TED.tst2014.fr-en.en\n            IWSLT16.TED.tst2014.fr-en.fr\n            IWSLT16.TEDX.dev2012.de-en.de\n            IWSLT16.TEDX.dev2012.de-en.en\n            IWSLT16.TEDX.tst2013.de-en.de\n            IWSLT16.TEDX.tst2013.de-en.en\n            IWSLT16.TEDX.tst2014.de-en.de\n            IWSLT16.TEDX.tst2014.de-en.en\n            train.ar\n            train.ar-en.ar\n            train.ar-en.en\n            train.cs\n            train.cs-en.cs\n            train.cs-en.en\n            train.de\n            train.de-en.de\n            train.de-en.en\n            train.en\n            train.en-ar.ar\n            train.en-ar.en\n            train.en-cs.cs\n            train.en-cs.en\n            train.en-de.de\n            train.en-de.en\n            train.en-fr.en\n            train.en-fr.fr\n            train.fr\n            train.fr-en.en\n            train.fr-en.fr\n            train.tags.ar-en.ar\n            train.tags.ar-en.en\n            train.tags.cs-en.cs\n            train.tags.cs-en.en\n            train.tags.de-en.de\n            train.tags.de-en.en\n            train.tags.en-ar.ar\n            train.tags.en-ar.en\n            train.tags.en-cs.cs\n            train.tags.en-cs.en\n            train.tags.en-de.de\n            train.tags.en-de.en\n            train.tags.en-fr.en\n            train.tags.en-fr.fr\n            train.tags.fr-en.en\n            train.tags.fr-en.fr\n\n    Examples:\n        >>> from torchtext.datasets import IWSLT\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> src_tokenizer = get_tokenizer(""spacy"", language=\'de\')\n        >>> tgt_tokenizer = get_tokenizer(""basic_english"")\n        >>> train_dataset, valid_dataset, test_dataset = IWSLT(tokenizer=(src_tokenizer,\n                                                                          tgt_tokenizer))\n        >>> src_vocab, tgt_vocab = train_dataset.get_vocab()\n        >>> src_data, tgt_data = train_dataset[10]\n    """"""\n\n    return _setup_datasets(""IWSLT"",\n                           train_filenames=train_filenames,\n                           valid_filenames=valid_filenames,\n                           test_filenames=test_filenames,\n                           tokenizer=tokenizer,\n                           root=root,\n                           vocab=vocab,\n                           removed_tokens=removed_tokens)\n\n\ndef WMT14(train_filenames=(\'train.tok.clean.bpe.32000.de\',\n                           \'train.tok.clean.bpe.32000.en\'),\n          valid_filenames=(\'newstest2013.tok.bpe.32000.de\',\n                           \'newstest2013.tok.bpe.32000.en\'),\n          test_filenames=(\'newstest2014.tok.bpe.32000.de\',\n                          \'newstest2014.tok.bpe.32000.en\'),\n          tokenizer=None,\n          root=\'.data\',\n          vocab=(None, None),\n          data_select=(\'train\', \'valid\', \'test\'),\n          removed_tokens=[\'<unk>\']):\n    """""" Define translation datasets: WMT14\n        Separately returns train/valid/test datasets\n        The available datasets include:\n            newstest2016.en\n            newstest2016.de\n            newstest2015.en\n            newstest2015.de\n            newstest2014.en\n            newstest2014.de\n            newstest2013.en\n            newstest2013.de\n            newstest2012.en\n            newstest2012.de\n            newstest2011.tok.de\n            newstest2011.en\n            newstest2011.de\n            newstest2010.tok.de\n            newstest2010.en\n            newstest2010.de\n            newstest2009.tok.de\n            newstest2009.en\n            newstest2009.de\n            newstest2016.tok.de\n            newstest2015.tok.de\n            newstest2014.tok.de\n            newstest2013.tok.de\n            newstest2012.tok.de\n            newstest2010.tok.en\n            newstest2009.tok.en\n            newstest2015.tok.en\n            newstest2014.tok.en\n            newstest2013.tok.en\n            newstest2012.tok.en\n            newstest2011.tok.en\n            newstest2016.tok.en\n            newstest2009.tok.bpe.32000.en\n            newstest2011.tok.bpe.32000.en\n            newstest2010.tok.bpe.32000.en\n            newstest2013.tok.bpe.32000.en\n            newstest2012.tok.bpe.32000.en\n            newstest2015.tok.bpe.32000.en\n            newstest2014.tok.bpe.32000.en\n            newstest2016.tok.bpe.32000.en\n            train.tok.clean.bpe.32000.en\n            newstest2009.tok.bpe.32000.de\n            newstest2010.tok.bpe.32000.de\n            newstest2011.tok.bpe.32000.de\n            newstest2013.tok.bpe.32000.de\n            newstest2012.tok.bpe.32000.de\n            newstest2014.tok.bpe.32000.de\n            newstest2016.tok.bpe.32000.de\n            newstest2015.tok.bpe.32000.de\n            train.tok.clean.bpe.32000.de\n\n    Arguments:\n        train_filenames: the source and target filenames for training.\n            Default: (\'train.tok.clean.bpe.32000.de\', \'train.tok.clean.bpe.32000.en\')\n        valid_filenames: the source and target filenames for valid.\n            Default: (\'newstest2013.tok.bpe.32000.de\', \'newstest2013.tok.bpe.32000.en\')\n        test_filenames: the source and target filenames for test.\n            Default: (\'newstest2014.tok.bpe.32000.de\', \'newstest2014.tok.bpe.32000.en\')\n        tokenizer: the tokenizer used to preprocess source and target raw text data.\n            It has to be in a form of tuple.\n            Default: (get_tokenizer(""spacy"", language=\'de_core_news_sm\'),\n                      get_tokenizer(""spacy"", language=\'en_core_web_sm\'))\n        root: Directory where the datasets are saved. Default: "".data""\n        vocab: Source and target Vocabulary objects used for dataset. If None, it\n            will generate a new vocabulary based on the train data set. It has to be\n            in a form of tuple.\n            Default: (None, None)\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'valid\', \'test\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n        removed_tokens: removed tokens from output dataset (Default: \'<unk>\')\n\n    Examples:\n        >>> from torchtext.datasets import WMT14\n        >>> from torchtext.data.utils import get_tokenizer\n        >>> src_tokenizer = get_tokenizer(""spacy"", language=\'de\')\n        >>> tgt_tokenizer = get_tokenizer(""basic_english"")\n        >>> train_dataset, valid_dataset, test_dataset = WMT14(tokenizer=(src_tokenizer,\n                                                                          tgt_tokenizer))\n        >>> src_vocab, tgt_vocab = train_dataset.get_vocab()\n        >>> src_data, tgt_data = train_dataset[10]\n    """"""\n\n    return _setup_datasets(""WMT14"",\n                           train_filenames=train_filenames,\n                           valid_filenames=valid_filenames,\n                           test_filenames=test_filenames,\n                           tokenizer=tokenizer,\n                           root=root,\n                           vocab=vocab,\n                           removed_tokens=removed_tokens)\n\n\nDATASETS = {\'Multi30k\': raw.Multi30k, \'IWSLT\': raw.IWSLT, \'WMT14\': raw.WMT14}\n'"
torchtext/experimental/datasets/raw/__init__.py,0,"b""from .text_classification import AG_NEWS, SogouNews, DBpedia, YelpReviewPolarity, \\\n    YelpReviewFull, YahooAnswers, \\\n    AmazonReviewPolarity, AmazonReviewFull, IMDB\nfrom .translation import Multi30k, IWSLT, WMT14\nfrom .language_modeling import WikiText2, WikiText103, PennTreebank, WMTNewsCrawl\n\n__all__ = ['IMDB',\n           'AG_NEWS',\n           'SogouNews',\n           'DBpedia',\n           'YelpReviewPolarity',\n           'YelpReviewFull',\n           'YahooAnswers',\n           'AmazonReviewPolarity',\n           'AmazonReviewFull',\n           'Multi30k',\n           'IWSLT',\n           'WMT14',\n           'WikiText2',\n           'WikiText103',\n           'PennTreebank',\n           'WMTNewsCrawl']\n"""
torchtext/experimental/datasets/raw/language_modeling.py,1,"b'import torch\nimport logging\nimport io\nfrom torchtext.utils import download_from_url, extract_archive\n\nURLS = {\n    \'WikiText2\':\n        \'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\',\n    \'WikiText103\':\n        \'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\',\n    \'PennTreebank\':\n        [\'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\',\n         \'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt\',\n         \'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt\'],\n    \'WMTNewsCrawl\': \'http://www.statmt.org/wmt11/training-monolingual-news-2010.tgz\'\n}\n\n\nclass RawTextIterableDataset(torch.utils.data.IterableDataset):\n    """"""Defines an abstraction for raw text iterable datasets.\n    """"""\n\n    def __init__(self, iterator, start=0, num_lines=None):\n        """"""Initiate language modeling dataset.\n        """"""\n        super(RawTextIterableDataset, self).__init__()\n        self._iterator = iterator\n        self.has_setup = False\n        self.start = start\n        self.num_lines = num_lines\n\n    def setup_iter(self, start=0, num_lines=None):\n        self.start = start\n        self.num_lines = num_lines\n        self.has_setup = True\n\n    def __iter__(self):\n        if not self.has_setup:\n            self.setup_iter()\n        for i, item in enumerate(self._iterator):\n            if i >= self.start:\n                yield item\n            if (self.num_lines is not None) and (i == (self.start + self.num_lines)):\n                break\n\n    def get_iterator(self):\n        return self._iterator\n\n\ndef _setup_datasets(dataset_name, root=\'.data\', data_select=(\'train\', \'test\', \'valid\'), **kwargs):\n    if isinstance(data_select, str):\n        data_select = [data_select]\n    if not set(data_select).issubset(set((\'train\', \'test\', \'valid\'))):\n        raise TypeError(\'data_select is not supported!\')\n\n    if dataset_name == \'PennTreebank\':\n        extracted_files = []\n        select_to_index = {\'train\': 0, \'test\': 1, \'valid\': 2}\n        extracted_files = [download_from_url(URLS[\'PennTreebank\'][select_to_index[key]],\n                                             root=root) for key in data_select]\n    elif dataset_name == \'WMTNewsCrawl\':\n        if not (data_select == [\'train\'] or set(data_select).issubset(set((\'train\',)))):\n            raise ValueError(""WMTNewsCrawl only creates a training dataset. ""\n                             ""data_select should be \'train\' ""\n                             ""or (\'train\',), got {}."".format(data_select))\n        dataset_tar = download_from_url(URLS[dataset_name], root=root)\n        extracted_files = extract_archive(dataset_tar)\n        year = kwargs.get(\'year\', 2010)\n        language = kwargs.get(\'language\', \'en\')\n        file_name = \'news.{}.{}.shuffled\'.format(year, language)\n        extracted_files = [f for f in extracted_files if file_name in f]\n    else:\n        dataset_tar = download_from_url(URLS[dataset_name], root=root)\n        extracted_files = extract_archive(dataset_tar)\n\n    _path = {}\n    for item in data_select:\n        for fname in extracted_files:\n            if item in fname:\n                _path[item] = fname\n\n    data = {}\n    for item in _path.keys():\n        logging.info(\'Creating {} data\'.format(item))\n        data[item] = iter(io.open(_path[item], encoding=""utf8""))\n\n    return tuple(RawTextIterableDataset(data[item]) for item in data_select)\n\n\ndef WikiText2(*args, **kwargs):\n    """""" Defines WikiText2 datasets.\n\n    Create language modeling dataset: WikiText2\n    Separately returns the train/test/valid set\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        data_select: a string or tupel for the returned datasets\n            (Default: (\'train\', \'test\',\'valid\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.raw.datasets import WikiText2\n        >>> train_dataset, test_dataset, valid_dataset = WikiText2()\n        >>> valid_dataset, = WikiText2(data_select=\'valid\')\n\n    """"""\n\n    return _setup_datasets(*((""WikiText2"",) + args), **kwargs)\n\n\ndef WikiText103(*args, **kwargs):\n    """""" Defines WikiText103 datasets.\n\n    Create language modeling dataset: WikiText103\n    Separately returns the train/test/valid set\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        data_select: the returned datasets (Default: (\'train\', \'test\',\'valid\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\').\n            If \'train\' is not in the tuple, an vocab object should be provided which will\n            be used to process valid and/or test data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets.raw import WikiText103\n        >>> train_dataset, test_dataset, valid_dataset = WikiText103()\n        >>> valid_dataset, = WikiText103(data_select=\'valid\')\n    """"""\n\n    return _setup_datasets(*((""WikiText103"",) + args), **kwargs)\n\n\ndef PennTreebank(*args, **kwargs):\n    """""" Defines PennTreebank datasets.\n\n    Create language modeling dataset: PennTreebank\n    Separately returns the train/test/valid set\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        data_select: a string or tuple for the returned datasets\n            (Default: (\'train\', \'test\',\'valid\'))\n            By default, all the three datasets (train, test, valid) are generated. Users\n            could also choose any one or two of them, for example (\'train\', \'test\') or\n            just a string \'train\'. If \'train\' is not in the tuple or string, a vocab\n            object should be provided which will be used to process valid and/or test\n            data.\n\n    Examples:\n        >>> from torchtext.experimental.datasets.raw import PennTreebank\n        >>> train_dataset, test_dataset, valid_dataset = PennTreebank()\n        >>> valid_dataset, = PennTreebank(data_select=\'valid\')\n\n    """"""\n\n    return _setup_datasets(*((""PennTreebank"",) + args), **kwargs)\n\n\ndef WMTNewsCrawl(*args, **kwargs):\n    """""" Defines WMT News Crawl.\n\n    Create language modeling dataset: WMTNewsCrawl\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n        data_select: a string or tuple for the returned datasets.\n            (Default: \'train\')\n    """"""\n\n    return _setup_datasets(*((""WMTNewsCrawl"",) + args), **kwargs)\n\n\nDATASETS = {\n    \'WikiText2\': WikiText2,\n    \'WikiText103\': WikiText103,\n    \'PennTreebank\': PennTreebank,\n    \'WMTNewsCrawl\': WMTNewsCrawl\n}\n'"
torchtext/experimental/datasets/raw/text_classification.py,1,"b'import torch\nimport io\nfrom torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\n\nURLS = {\n    \'AG_NEWS\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms\',\n    \'SogouNews\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUkVqNEszd0pHaFE\',\n    \'DBpedia\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k\',\n    \'YelpReviewPolarity\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbNUpYQ2N3SGlFaDg\',\n    \'YelpReviewFull\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbZlU4dXhHTFhZQU0\',\n    \'YahooAnswers\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU\',\n    \'AmazonReviewPolarity\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbaW12WVVZS2drcnM\',\n    \'AmazonReviewFull\':\n        \'https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbZVhsUnRWRDhETzA\',\n    \'IMDB\':\n        \'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\'\n}\n\n\ndef _create_data_from_csv(data_path):\n    with io.open(data_path, encoding=""utf8"") as f:\n        reader = unicode_csv_reader(f)\n        for row in reader:\n            yield int(row[0]), \' \'.join(row[1:])\n\n\nclass RawTextIterableDataset(torch.utils.data.IterableDataset):\n    """"""Defines an abstraction for raw text iterable datasets.\n    """"""\n\n    def __init__(self, iterator):\n        """"""Initiate text-classification dataset.\n        """"""\n        super(RawTextIterableDataset, self).__init__()\n        self._iterator = iterator\n        self.has_setup = False\n        self.start = 0\n        self.num_lines = None\n\n    def setup_iter(self, start=0, num_lines=None):\n        self.start = start\n        self.num_lines = num_lines\n        self.has_setup = True\n\n    def __iter__(self):\n        if not self.has_setup:\n            self.setup_iter()\n\n        for i, item in enumerate(self._iterator):\n            if i >= self.start:\n                yield item\n            if self.num_lines is not None and i == (self.start + self.num_lines):\n                break\n\n    def get_iterator(self):\n        return self._iterator\n\n\ndef _setup_datasets(dataset_name, root=\'.data\'):\n    dataset_tar = download_from_url(URLS[dataset_name], root=root)\n    extracted_files = extract_archive(dataset_tar)\n\n    for fname in extracted_files:\n        if fname.endswith(\'train.csv\'):\n            train_csv_path = fname\n        if fname.endswith(\'test.csv\'):\n            test_csv_path = fname\n\n    train_iter = _create_data_from_csv(train_csv_path)\n    test_iter = _create_data_from_csv(test_csv_path)\n    return (RawTextIterableDataset(train_iter),\n            RawTextIterableDataset(test_iter))\n\n\ndef AG_NEWS(*args, **kwargs):\n    """""" Defines AG_NEWS datasets.\n\n    Create supervised learning dataset: AG_NEWS\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.AG_NEWS()\n    """"""\n\n    return _setup_datasets(*((""AG_NEWS"",) + args), **kwargs)\n\n\ndef SogouNews(*args, **kwargs):\n    """""" Defines SogouNews datasets.\n\n    Create supervised learning dataset: SogouNews\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.SogouNews()\n    """"""\n\n    return _setup_datasets(*((""SogouNews"",) + args), **kwargs)\n\n\ndef DBpedia(*args, **kwargs):\n    """""" Defines DBpedia datasets.\n\n    Create supervised learning dataset: DBpedia\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.DBpedia()\n    """"""\n\n    return _setup_datasets(*((""DBpedia"",) + args), **kwargs)\n\n\ndef YelpReviewPolarity(*args, **kwargs):\n    """""" Defines YelpReviewPolarity datasets.\n\n    Create supervised learning dataset: YelpReviewPolarity\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.YelpReviewPolarity()\n    """"""\n\n    return _setup_datasets(*((""YelpReviewPolarity"",) + args), **kwargs)\n\n\ndef YelpReviewFull(*args, **kwargs):\n    """""" Defines YelpReviewFull datasets.\n\n    Create supervised learning dataset: YelpReviewFull\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.YelpReviewFull()\n    """"""\n\n    return _setup_datasets(*((""YelpReviewFull"",) + args), **kwargs)\n\n\ndef YahooAnswers(*args, **kwargs):\n    """""" Defines YahooAnswers datasets.\n\n    Create supervised learning dataset: YahooAnswers\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.YahooAnswers()\n    """"""\n\n    return _setup_datasets(*((""YahooAnswers"",) + args), **kwargs)\n\n\ndef AmazonReviewPolarity(*args, **kwargs):\n    """""" Defines AmazonReviewPolarity datasets.\n\n    Create supervised learning dataset: AmazonReviewPolarity\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.AmazonReviewPolarity()\n    """"""\n\n    return _setup_datasets(*((""AmazonReviewPolarity"",) + args), **kwargs)\n\n\ndef AmazonReviewFull(*args, **kwargs):\n    """""" Defines AmazonReviewFull datasets.\n\n    Create supervised learning dataset: AmazonReviewFull\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.AmazonReviewFull()\n    """"""\n\n    return _setup_datasets(*((""AmazonReviewFull"",) + args), **kwargs)\n\n\ndef generate_imdb_data(key, extracted_files):\n    for fname in extracted_files:\n        if \'urls\' in fname:\n            continue\n        elif key in fname and (\'pos\' in fname or \'neg\' in fname):\n            with io.open(fname, encoding=""utf8"") as f:\n                label = 1 if \'pos\' in fname else 0\n                yield label, f.read()\n\n\ndef IMDB(root=\'.data\'):\n    """""" Defines IMDB datasets.\n\n    Create supervised learning dataset: IMDB\n\n    Separately returns the training and test dataset\n\n    Arguments:\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> train, test = torchtext.experimental.datasets.raw.IMDB()\n    """"""\n\n    dataset_tar = download_from_url(URLS[\'IMDB\'], root=root)\n    extracted_files = extract_archive(dataset_tar)\n    train_iter = generate_imdb_data(\'train\', extracted_files)\n    test_iter = generate_imdb_data(\'test\', extracted_files)\n    return (RawTextIterableDataset(train_iter),\n            RawTextIterableDataset(test_iter))\n\n\nDATASETS = {\n    \'AG_NEWS\': AG_NEWS,\n    \'SogouNews\': SogouNews,\n    \'DBpedia\': DBpedia,\n    \'YelpReviewPolarity\': YelpReviewPolarity,\n    \'YelpReviewFull\': YelpReviewFull,\n    \'YahooAnswers\': YahooAnswers,\n    \'AmazonReviewPolarity\': AmazonReviewPolarity,\n    \'AmazonReviewFull\': AmazonReviewFull,\n    \'IMDB\': IMDB\n}\n'"
torchtext/experimental/datasets/raw/translation.py,1,"b'import torch\nimport os\nimport io\nimport codecs\nimport xml.etree.ElementTree as ET\nfrom collections import defaultdict\n\nfrom torchtext.utils import (download_from_url, extract_archive,\n                             unicode_csv_reader)\n\nURLS = {\n    \'Multi30k\': [\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.cs.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.fr.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2017_flickr.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2017_flickr.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2017_flickr.fr.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2017_mscoco.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2017_mscoco.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2017_mscoco.fr.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2018_flickr.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.cs.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.fr.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.cs.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.fr.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.1.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.1.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.2.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.2.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.3.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.3.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.4.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.4.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.5.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/test_2016.5.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.1.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.1.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.2.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.2.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.3.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.3.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.4.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.4.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.5.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/train.5.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.1.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.1.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.2.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.2.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.3.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.3.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.4.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.4.en.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.5.de.gz"",\n        ""https://raw.githubusercontent.com/multi30k/dataset/master/data/task2/raw/val.5.en.gz""\n    ],\n    \'WMT14\':\n    \'https://drive.google.com/uc?export=download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8\',\n    \'IWSLT\':\n    \'https://wit3.fbk.eu/archive/2016-01//texts/{}/{}/{}.tgz\'\n}\n\n\ndef _read_text_iterator(path):\n    with io.open(path, encoding=""utf8"") as f:\n        reader = unicode_csv_reader(f)\n        for row in reader:\n            yield "" "".join(row)\n\n\ndef _clean_xml_file(f_xml):\n    f_txt = os.path.splitext(f_xml)[0]\n    with codecs.open(f_txt, mode=\'w\', encoding=\'utf-8\') as fd_txt:\n        root = ET.parse(f_xml).getroot()[0]\n        for doc in root.findall(\'doc\'):\n            for e in doc.findall(\'seg\'):\n                fd_txt.write(e.text.strip() + \'\\n\')\n\n\ndef _clean_tags_file(f_orig):\n    xml_tags = [\n        \'<url\', \'<keywords\', \'<talkid\', \'<description\', \'<reviewer\',\n        \'<translator\', \'<title\', \'<speaker\'\n    ]\n    f_txt = f_orig.replace(\'.tags\', \'\')\n    with codecs.open(f_txt, mode=\'w\', encoding=\'utf-8\') as fd_txt, \\\n            io.open(f_orig, mode=\'r\', encoding=\'utf-8\') as fd_orig:\n        for l in fd_orig:\n            if not any(tag in l for tag in xml_tags):\n                # TODO: Fix utf-8 next line mark\n                #                fd_txt.write(l.strip() + \'\\n\')\n                #                fd_txt.write(l.strip() + u""\\u0085"")\n                #                fd_txt.write(l.lstrip())\n                fd_txt.write(l.strip() + \'\\n\')\n\n\ndef _construct_filenames(filename, languages):\n    filenames = []\n    for lang in languages:\n        filenames.append(filename + ""."" + lang)\n    return filenames\n\n\ndef _construct_filepaths(paths, src_filename, tgt_filename):\n    src_path = None\n    tgt_path = None\n    for p in paths:\n        src_path = p if src_filename in p else src_path\n        tgt_path = p if tgt_filename in p else tgt_path\n    return (src_path, tgt_path)\n\n\ndef _setup_datasets(dataset_name,\n                    train_filenames,\n                    valid_filenames,\n                    test_filenames,\n                    root=\'.data\'):\n    if not isinstance(train_filenames, tuple) and not isinstance(valid_filenames, tuple) \\\n            and not isinstance(test_filenames, tuple):\n        raise ValueError(""All filenames must be tuples"")\n\n    src_train, tgt_train = train_filenames\n    src_eval, tgt_eval = valid_filenames\n    src_test, tgt_test = test_filenames\n\n    extracted_files = []\n    if isinstance(URLS[dataset_name], list):\n        for f in URLS[dataset_name]:\n            dataset_tar = download_from_url(f, root=root)\n            extracted_files.extend(extract_archive(dataset_tar))\n    elif isinstance(URLS[dataset_name], str):\n        dataset_tar = download_from_url(URLS[dataset_name], root=root)\n        extracted_files.extend(extract_archive(dataset_tar))\n    else:\n        raise ValueError(\n            ""URLS for {} has to be in a form or list or string"".format(\n                dataset_name))\n\n    # Clean the xml and tag file in the archives\n    file_archives = []\n    for fname in extracted_files:\n        if \'xml\' in fname:\n            _clean_xml_file(fname)\n            file_archives.append(os.path.splitext(fname)[0])\n        elif ""tags"" in fname:\n            _clean_tags_file(fname)\n            file_archives.append(fname.replace(\'.tags\', \'\'))\n        else:\n            file_archives.append(fname)\n\n    data_filenames = defaultdict(dict)\n    data_filenames = {\n        ""train"": _construct_filepaths(file_archives, src_train, tgt_train),\n        ""valid"": _construct_filepaths(file_archives, src_eval, tgt_eval),\n        ""test"": _construct_filepaths(file_archives, src_test, tgt_test)\n    }\n\n    for key in data_filenames.keys():\n        if len(data_filenames[key]) == 0 or data_filenames[key] is None:\n            raise FileNotFoundError(\n                ""Files are not found for data type {}"".format(key))\n\n    datasets = []\n    for key in data_filenames.keys():\n        src_data_iter = _read_text_iterator(data_filenames[key][0])\n        tgt_data_iter = _read_text_iterator(data_filenames[key][1])\n\n        datasets.append(\n            RawTranslationIterableDataset(src_data_iter, tgt_data_iter))\n\n    return tuple(datasets)\n\n\nclass RawTranslationIterableDataset(torch.utils.data.IterableDataset):\n    """"""Defines an abstraction for raw text iterable datasets.\n    """"""\n    def __init__(self, src_iterator, tgt_iterator):\n        """"""Initiate text-classification dataset.\n        """"""\n        super(RawTranslationIterableDataset, self).__init__()\n        self._src_iterator = src_iterator\n        self._tgt_iterator = tgt_iterator\n        self.has_setup = False\n        self.start = 0\n        self.num_lines = None\n\n    def setup_iter(self, start=0, num_lines=None):\n        self.start = start\n        self.num_lines = num_lines\n        self.has_setup = True\n\n    def __iter__(self):\n        if not self.has_setup:\n            self.setup_iter()\n\n        for i, item in enumerate(zip(self._src_iterator, self._tgt_iterator)):\n            if i >= self.start:\n                yield item\n            if (self.num_lines is not None) and (i == (self.start +\n                                                       self.num_lines)):\n                break\n\n    def get_iterator(self):\n        return (self._src_iterator, self._tgt_iterator)\n\n\ndef Multi30k(train_filenames=(""train.de"", ""train.en""),\n             valid_filenames=(""val.de"", ""val.en""),\n             test_filenames=(""test_2016_flickr.de"", ""test_2016_flickr.en""),\n             root=\'.data\'):\n    """""" Define translation datasets: Multi30k\n        Separately returns train/valid/test datasets as a tuple\n        The available dataset include:\n            test_2016_flickr.cs\n            test_2016_flickr.de\n            test_2016_flickr.en\n            test_2016_flickr.fr\n            test_2017_flickr.de\n            test_2017_flickr.en\n            test_2017_flickr.fr\n            test_2017_mscoco.de\n            test_2017_mscoco.en\n            test_2017_mscoco.fr\n            test_2018_flickr.en\n            train.cs\n            train.de\n            train.en\n            train.fr\n            val.cs\n            val.de\n            val.en\n            val.fr\n            test_2016.1.de\n            test_2016.1.en\n            test_2016.2.de\n            test_2016.2.en\n            test_2016.3.de\n            test_2016.3.en\n            test_2016.4.de\n            test_2016.4.en\n            test_2016.5.de\n            test_2016.5.en\n            train.1.de\n            train.1.en\n            train.2.de\n            train.2.en\n            train.3.de\n            train.3.en\n            train.4.de\n            train.4.en\n            train.5.de\n            train.5.en\n            val.1.de\n            val.1.en\n            val.2.de\n            val.2.en\n            val.3.de\n            val.3.en\n            val.4.de\n            val.4.en\n            val.5.de\n            val.5.en\n\n    Arguments:\n        train_filenames: the source and target filenames for training.\n            Default: (\'train.de\', \'train.en\')\n        valid_filenames: the source and target filenames for valid.\n            Default: (\'val.de\', \'val.en\')\n        test_filenames: the source and target filenames for test.\n            Default: (\'test2016.de\', \'test2016.en\')\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> from torchtext.datasets import Multi30k\n        >>> train_dataset, valid_dataset, test_dataset = Multi30k()\n    """"""\n    return _setup_datasets(""Multi30k"",\n                           train_filenames=train_filenames,\n                           valid_filenames=valid_filenames,\n                           test_filenames=test_filenames,\n                           root=root)\n\n\ndef IWSLT(train_filenames=(\'train.de-en.de\', \'train.de-en.en\'),\n          valid_filenames=(\'IWSLT16.TED.tst2013.de-en.de\',\n                           \'IWSLT16.TED.tst2013.de-en.en\'),\n          test_filenames=(\'IWSLT16.TED.tst2014.de-en.de\',\n                          \'IWSLT16.TED.tst2014.de-en.en\'),\n          root=\'.data\'):\n    """""" Define translation datasets: IWSLT\n        Separately returns train/valid/test datasets\n        The available datasets include:\n            IWSLT16.TED.dev2010.ar-en.ar\n            IWSLT16.TED.dev2010.ar-en.en\n            IWSLT16.TED.dev2010.cs-en.cs\n            IWSLT16.TED.dev2010.cs-en.en\n            IWSLT16.TED.dev2010.de-en.de\n            IWSLT16.TED.dev2010.de-en.en\n            IWSLT16.TED.dev2010.en-ar.ar\n            IWSLT16.TED.dev2010.en-ar.en\n            IWSLT16.TED.dev2010.en-cs.cs\n            IWSLT16.TED.dev2010.en-cs.en\n            IWSLT16.TED.dev2010.en-de.de\n            IWSLT16.TED.dev2010.en-de.en\n            IWSLT16.TED.dev2010.en-fr.en\n            IWSLT16.TED.dev2010.en-fr.fr\n            IWSLT16.TED.dev2010.fr-en.en\n            IWSLT16.TED.dev2010.fr-en.fr\n            IWSLT16.TED.tst2010.ar-en.ar\n            IWSLT16.TED.tst2010.ar-en.en\n            IWSLT16.TED.tst2010.cs-en.cs\n            IWSLT16.TED.tst2010.cs-en.en\n            IWSLT16.TED.tst2010.de-en.de\n            IWSLT16.TED.tst2010.de-en.en\n            IWSLT16.TED.tst2010.en-ar.ar\n            IWSLT16.TED.tst2010.en-ar.en\n            IWSLT16.TED.tst2010.en-cs.cs\n            IWSLT16.TED.tst2010.en-cs.en\n            IWSLT16.TED.tst2010.en-de.de\n            IWSLT16.TED.tst2010.en-de.en\n            IWSLT16.TED.tst2010.en-fr.en\n            IWSLT16.TED.tst2010.en-fr.fr\n            IWSLT16.TED.tst2010.fr-en.en\n            IWSLT16.TED.tst2010.fr-en.fr\n            IWSLT16.TED.tst2011.ar-en.ar\n            IWSLT16.TED.tst2011.ar-en.en\n            IWSLT16.TED.tst2011.cs-en.cs\n            IWSLT16.TED.tst2011.cs-en.en\n            IWSLT16.TED.tst2011.de-en.de\n            IWSLT16.TED.tst2011.de-en.en\n            IWSLT16.TED.tst2011.en-ar.ar\n            IWSLT16.TED.tst2011.en-ar.en\n            IWSLT16.TED.tst2011.en-cs.cs\n            IWSLT16.TED.tst2011.en-cs.en\n            IWSLT16.TED.tst2011.en-de.de\n            IWSLT16.TED.tst2011.en-de.en\n            IWSLT16.TED.tst2011.en-fr.en\n            IWSLT16.TED.tst2011.en-fr.fr\n            IWSLT16.TED.tst2011.fr-en.en\n            IWSLT16.TED.tst2011.fr-en.fr\n            IWSLT16.TED.tst2012.ar-en.ar\n            IWSLT16.TED.tst2012.ar-en.en\n            IWSLT16.TED.tst2012.cs-en.cs\n            IWSLT16.TED.tst2012.cs-en.en\n            IWSLT16.TED.tst2012.de-en.de\n            IWSLT16.TED.tst2012.de-en.en\n            IWSLT16.TED.tst2012.en-ar.ar\n            IWSLT16.TED.tst2012.en-ar.en\n            IWSLT16.TED.tst2012.en-cs.cs\n            IWSLT16.TED.tst2012.en-cs.en\n            IWSLT16.TED.tst2012.en-de.de\n            IWSLT16.TED.tst2012.en-de.en\n            IWSLT16.TED.tst2012.en-fr.en\n            IWSLT16.TED.tst2012.en-fr.fr\n            IWSLT16.TED.tst2012.fr-en.en\n            IWSLT16.TED.tst2012.fr-en.fr\n            IWSLT16.TED.tst2013.ar-en.ar\n            IWSLT16.TED.tst2013.ar-en.en\n            IWSLT16.TED.tst2013.cs-en.cs\n            IWSLT16.TED.tst2013.cs-en.en\n            IWSLT16.TED.tst2013.de-en.de\n            IWSLT16.TED.tst2013.de-en.en\n            IWSLT16.TED.tst2013.en-ar.ar\n            IWSLT16.TED.tst2013.en-ar.en\n            IWSLT16.TED.tst2013.en-cs.cs\n            IWSLT16.TED.tst2013.en-cs.en\n            IWSLT16.TED.tst2013.en-de.de\n            IWSLT16.TED.tst2013.en-de.en\n            IWSLT16.TED.tst2013.en-fr.en\n            IWSLT16.TED.tst2013.en-fr.fr\n            IWSLT16.TED.tst2013.fr-en.en\n            IWSLT16.TED.tst2013.fr-en.fr\n            IWSLT16.TED.tst2014.ar-en.ar\n            IWSLT16.TED.tst2014.ar-en.en\n            IWSLT16.TED.tst2014.de-en.de\n            IWSLT16.TED.tst2014.de-en.en\n            IWSLT16.TED.tst2014.en-ar.ar\n            IWSLT16.TED.tst2014.en-ar.en\n            IWSLT16.TED.tst2014.en-de.de\n            IWSLT16.TED.tst2014.en-de.en\n            IWSLT16.TED.tst2014.en-fr.en\n            IWSLT16.TED.tst2014.en-fr.fr\n            IWSLT16.TED.tst2014.fr-en.en\n            IWSLT16.TED.tst2014.fr-en.fr\n            IWSLT16.TEDX.dev2012.de-en.de\n            IWSLT16.TEDX.dev2012.de-en.en\n            IWSLT16.TEDX.tst2013.de-en.de\n            IWSLT16.TEDX.tst2013.de-en.en\n            IWSLT16.TEDX.tst2014.de-en.de\n            IWSLT16.TEDX.tst2014.de-en.en\n            train.ar\n            train.ar-en.ar\n            train.ar-en.en\n            train.cs\n            train.cs-en.cs\n            train.cs-en.en\n            train.de\n            train.de-en.de\n            train.de-en.en\n            train.en\n            train.en-ar.ar\n            train.en-ar.en\n            train.en-cs.cs\n            train.en-cs.en\n            train.en-de.de\n            train.en-de.en\n            train.en-fr.en\n            train.en-fr.fr\n            train.fr\n            train.fr-en.en\n            train.fr-en.fr\n            train.tags.ar-en.ar\n            train.tags.ar-en.en\n            train.tags.cs-en.cs\n            train.tags.cs-en.en\n            train.tags.de-en.de\n            train.tags.de-en.en\n            train.tags.en-ar.ar\n            train.tags.en-ar.en\n            train.tags.en-cs.cs\n            train.tags.en-cs.en\n            train.tags.en-de.de\n            train.tags.en-de.en\n            train.tags.en-fr.en\n            train.tags.en-fr.fr\n            train.tags.fr-en.en\n            train.tags.fr-en.fr\n\n    Arguments:\n        train_filenames: the source and target filenames for training.\n            Default: (\'train.de-en.de\', \'train.de-en.en\')\n        valid_filenames: the source and target filenames for valid.\n            Default: (\'IWSLT16.TED.tst2013.de-en.de\', \'IWSLT16.TED.tst2013.de-en.en\')\n        test_filenames: the source and target filenames for test.\n            Default: (\'IWSLT16.TED.tst2014.de-en.de\', \'IWSLT16.TED.tst2014.de-en.en\')\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> from torchtext.datasets.raw import IWSLT\n        >>> train_dataset, valid_dataset, test_dataset = IWSLT()\n    """"""\n    src_language = train_filenames[0].split(""."")[-1]\n    tgt_language = train_filenames[1].split(""."")[-1]\n    languages = ""-"".join([src_language, tgt_language])\n    URLS[""IWSLT""] = URLS[""IWSLT""].format(src_language, tgt_language, languages)\n\n    return _setup_datasets(\n        ""IWSLT"",\n        train_filenames=train_filenames,\n        valid_filenames=valid_filenames,\n        test_filenames=test_filenames,\n        root=root,\n    )\n\n\ndef WMT14(train_filenames=(\'train.tok.clean.bpe.32000.de\',\n                           \'train.tok.clean.bpe.32000.en\'),\n          valid_filenames=(\'newstest2013.tok.bpe.32000.de\',\n                           \'newstest2013.tok.bpe.32000.en\'),\n          test_filenames=(\'newstest2014.tok.bpe.32000.de\',\n                          \'newstest2014.tok.bpe.32000.en\'),\n          root=\'.data\'):\n    """""" Define translation datasets: WMT14\n        Separately returns train/valid/test datasets\n        The available datasets include:\n            newstest2016.en\n            newstest2016.de\n            newstest2015.en\n            newstest2015.de\n            newstest2014.en\n            newstest2014.de\n            newstest2013.en\n            newstest2013.de\n            newstest2012.en\n            newstest2012.de\n            newstest2011.tok.de\n            newstest2011.en\n            newstest2011.de\n            newstest2010.tok.de\n            newstest2010.en\n            newstest2010.de\n            newstest2009.tok.de\n            newstest2009.en\n            newstest2009.de\n            newstest2016.tok.de\n            newstest2015.tok.de\n            newstest2014.tok.de\n            newstest2013.tok.de\n            newstest2012.tok.de\n            newstest2010.tok.en\n            newstest2009.tok.en\n            newstest2015.tok.en\n            newstest2014.tok.en\n            newstest2013.tok.en\n            newstest2012.tok.en\n            newstest2011.tok.en\n            newstest2016.tok.en\n            newstest2009.tok.bpe.32000.en\n            newstest2011.tok.bpe.32000.en\n            newstest2010.tok.bpe.32000.en\n            newstest2013.tok.bpe.32000.en\n            newstest2012.tok.bpe.32000.en\n            newstest2015.tok.bpe.32000.en\n            newstest2014.tok.bpe.32000.en\n            newstest2016.tok.bpe.32000.en\n            train.tok.clean.bpe.32000.en\n            newstest2009.tok.bpe.32000.de\n            newstest2010.tok.bpe.32000.de\n            newstest2011.tok.bpe.32000.de\n            newstest2013.tok.bpe.32000.de\n            newstest2012.tok.bpe.32000.de\n            newstest2014.tok.bpe.32000.de\n            newstest2016.tok.bpe.32000.de\n            newstest2015.tok.bpe.32000.de\n            train.tok.clean.bpe.32000.de\n\n    Arguments:\n        train_filenames: the source and target filenames for training.\n            Default: (\'train.tok.clean.bpe.32000.de\', \'train.tok.clean.bpe.32000.en\')\n        valid_filenames: the source and target filenames for valid.\n            Default: (\'newstest2013.tok.bpe.32000.de\', \'newstest2013.tok.bpe.32000.en\')\n        test_filenames: the source and target filenames for test.\n            Default: (\'newstest2014.tok.bpe.32000.de\', \'newstest2014.tok.bpe.32000.en\')\n        root: Directory where the datasets are saved. Default: "".data""\n\n    Examples:\n        >>> from torchtext.datasets import WMT14\n        >>> train_dataset, valid_dataset, test_dataset = WMT14()\n    """"""\n\n    return _setup_datasets(""WMT14"",\n                           train_filenames=train_filenames,\n                           valid_filenames=valid_filenames,\n                           test_filenames=test_filenames,\n                           root=root)\n\n\nDATASETS = {\'Multi30k\': Multi30k, \'IWSLT\': IWSLT, \'WMT14\': WMT14}\n'"
