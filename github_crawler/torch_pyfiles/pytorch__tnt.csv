file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\nimport os\nimport shutil\nimport sys\nfrom setuptools import setup, find_packages\n\nVERSION = \'0.0.5.1\'\n\nlong_description = ""Simple tools for logging and visualizing, loading and training""\n\nsetup_info = dict(\n    # Metadata\n    name=\'torchnet\',\n    version=VERSION,\n    author=\'PyTorch\',\n    author_email=\'sergey.zagoruyko@enpc.fr\',\n    url=\'https://github.com/pytorch/tnt/\',\n    description=\'an abstraction to train neural networks\',\n    long_description=long_description,\n    license=\'BSD\',\n\n    # Package info\n    packages=find_packages(exclude=(\'test\', \'docs\')),\n\n    zip_safe=True,\n\n    install_requires=[\n        \'torch\',\n        \'six\',\n        \'visdom\'\n    ]\n)\n\nsetup(**setup_info)\n'"
docs/conf.py,1,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/stable/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../torchnet\'))\nsys.path.insert(0, os.path.abspath(\'..\'))\n\nfrom unittest.mock import MagicMock\n\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n        return MagicMock()\n\nMOCK_MODULES = [\'numpy\', \'torch\', \'torch.utils\', \'torch.utils.data\', \'visdom\']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\nimport torchnet\n# from sphinx.util import logging\n# logger = logging.getLogger(__name__)\n# logger.info(""Loading successful {}"".format(torchnet))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'TNT\'\ncopyright = \'2018, Torch Contributors\'\nauthor = \'Torch Contributors\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\nhtml_logo = \'_static/img/pytorch-logo-dark.png\'\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'TNTdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'TNT.tex\', \'TNT Documentation\',\n     \'Torch Contributors\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'tnt\', \'TNT Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'TNT\', \'TNT Documentation\',\n     author, \'TNT\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None}\n\n\ndef setup(app):\n    app.add_stylesheet(\'css/pytorch_theme.css\')\n\n# sphinx-apidoc -o source/ ../\n'"
example/mnist.py,13,"b""from tqdm import tqdm\nimport torch\nimport torch.optim\nimport torchnet as tnt\nfrom torchvision.datasets.mnist import MNIST\nfrom torchnet.engine import Engine\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.nn.init import kaiming_normal\n\n\ndef get_iterator(mode):\n    ds = MNIST(root='./', download=True, train=mode)\n    data = getattr(ds, 'train_data' if mode else 'test_data')\n    labels = getattr(ds, 'train_labels' if mode else 'test_labels')\n    tds = tnt.dataset.TensorDataset([data, labels])\n    return tds.parallel(batch_size=128, num_workers=4, shuffle=mode)\n\n\ndef conv_init(ni, no, k):\n    return kaiming_normal(torch.Tensor(no, ni, k, k))\n\n\ndef linear_init(ni, no):\n    return kaiming_normal(torch.Tensor(no, ni))\n\n\ndef f(params, inputs, mode):\n    o = inputs.view(inputs.size(0), 1, 28, 28)\n    o = F.conv2d(o, params['conv0.weight'], params['conv0.bias'], stride=2)\n    o = F.relu(o)\n    o = F.conv2d(o, params['conv1.weight'], params['conv1.bias'], stride=2)\n    o = F.relu(o)\n    o = o.view(o.size(0), -1)\n    o = F.linear(o, params['linear2.weight'], params['linear2.bias'])\n    o = F.relu(o)\n    o = F.linear(o, params['linear3.weight'], params['linear3.bias'])\n    return o\n\n\ndef main():\n    params = {\n        'conv0.weight': conv_init(1, 50, 5), 'conv0.bias': torch.zeros(50),\n        'conv1.weight': conv_init(50, 50, 5), 'conv1.bias': torch.zeros(50),\n        'linear2.weight': linear_init(800, 512), 'linear2.bias': torch.zeros(512),\n        'linear3.weight': linear_init(512, 10), 'linear3.bias': torch.zeros(10),\n    }\n    params = {k: Variable(v, requires_grad=True) for k, v in params.items()}\n\n    optimizer = torch.optim.SGD(\n        params.values(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n\n    engine = Engine()\n    meter_loss = tnt.meter.AverageValueMeter()\n    classerr = tnt.meter.ClassErrorMeter(accuracy=True)\n\n    def h(sample):\n        inputs = Variable(sample[0].float() / 255.0)\n        targets = Variable(torch.LongTensor(sample[1]))\n        o = f(params, inputs, sample[2])\n        return F.cross_entropy(o, targets), o\n\n    def reset_meters():\n        classerr.reset()\n        meter_loss.reset()\n\n    def on_sample(state):\n        state['sample'].append(state['train'])\n\n    def on_forward(state):\n        classerr.add(state['output'].data,\n                     torch.LongTensor(state['sample'][1]))\n        meter_loss.add(state['loss'].data[0])\n\n    def on_start_epoch(state):\n        reset_meters()\n        state['iterator'] = tqdm(state['iterator'])\n\n    def on_end_epoch(state):\n        print('Training loss: %.4f, accuracy: %.2f%%' % (meter_loss.value()[0], classerr.value()[0]))\n        # do validation at the end of each epoch\n        reset_meters()\n        engine.test(h, get_iterator(False))\n        print('Testing loss: %.4f, accuracy: %.2f%%' % (meter_loss.value()[0], classerr.value()[0]))\n\n    engine.hooks['on_sample'] = on_sample\n    engine.hooks['on_forward'] = on_forward\n    engine.hooks['on_start_epoch'] = on_start_epoch\n    engine.hooks['on_end_epoch'] = on_end_epoch\n    engine.train(h, get_iterator(True), maxepoch=10, optimizer=optimizer)\n\n\nif __name__ == '__main__':\n    main()\n"""
example/mnist_with_meterlogger.py,12,"b'"""""" Run MNIST example and log to visdom\n    Notes:\n        - Visdom must be installed (pip works)\n        - the Visdom server must be running at start!\n\n    Example:\n        $ python -m visdom.server -port 8097 &\n        $ python mnist_with_visdom.py\n""""""\nfrom tqdm import tqdm\nimport torch\nimport torch.optim\nimport torchnet as tnt\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.nn.init import kaiming_normal\nfrom torchnet.engine import Engine\nfrom torchnet.logger import MeterLogger\nfrom torchvision.datasets.mnist import MNIST\n\n\ndef get_iterator(mode):\n    ds = MNIST(root=\'./\', download=True, train=mode)\n    data = getattr(ds, \'train_data\' if mode else \'test_data\')\n    labels = getattr(ds, \'train_labels\' if mode else \'test_labels\')\n    tds = tnt.dataset.TensorDataset([data, labels])\n    return tds.parallel(batch_size=128, num_workers=4, shuffle=mode)\n\n\ndef conv_init(ni, no, k):\n    return kaiming_normal(torch.Tensor(no, ni, k, k))\n\n\ndef linear_init(ni, no):\n    return kaiming_normal(torch.Tensor(no, ni))\n\n\ndef f(params, inputs, mode):\n    o = inputs.view(inputs.size(0), 1, 28, 28)\n    o = F.conv2d(o, params[\'conv0.weight\'], params[\'conv0.bias\'], stride=2)\n    o = F.relu(o)\n    o = F.conv2d(o, params[\'conv1.weight\'], params[\'conv1.bias\'], stride=2)\n    o = F.relu(o)\n    o = o.view(o.size(0), -1)\n    o = F.linear(o, params[\'linear2.weight\'], params[\'linear2.bias\'])\n    o = F.relu(o)\n    o = F.linear(o, params[\'linear3.weight\'], params[\'linear3.bias\'])\n    return o\n\n\ndef main():\n    params = {\n        \'conv0.weight\': conv_init(1, 50, 5), \'conv0.bias\': torch.zeros(50),\n        \'conv1.weight\': conv_init(50, 50, 5), \'conv1.bias\': torch.zeros(50),\n        \'linear2.weight\': linear_init(800, 512), \'linear2.bias\': torch.zeros(512),\n        \'linear3.weight\': linear_init(512, 10), \'linear3.bias\': torch.zeros(10),\n    }\n    params = {k: Variable(v, requires_grad=True) for k, v in params.items()}\n\n    optimizer = torch.optim.SGD(\n        params.values(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n\n    engine = Engine()\n\n    mlog = MeterLogger(nclass=10, title=""mnist_meterlogger"")\n\n    def h(sample):\n        inputs = Variable(sample[0].float() / 255.0)\n        targets = Variable(torch.LongTensor(sample[1]))\n        o = f(params, inputs, sample[2])\n        return F.cross_entropy(o, targets), o\n\n    def on_sample(state):\n        state[\'sample\'].append(state[\'train\'])\n\n    def on_forward(state):\n        loss = state[\'loss\']\n        output = state[\'output\']\n        target = state[\'sample\'][1]\n        # online ploter\n        mlog.update_loss(loss, meter=\'loss\')\n        mlog.update_meter(output, target, meters={\'accuracy\', \'map\', \'confusion\'})\n\n    def on_start_epoch(state):\n        mlog.timer.reset()\n        state[\'iterator\'] = tqdm(state[\'iterator\'])\n\n    def on_end_epoch(state):\n        mlog.print_meter(mode=""Train"", iepoch=state[\'epoch\'])\n        mlog.reset_meter(mode=""Train"", iepoch=state[\'epoch\'])\n\n        # do validation at the end of each epoch\n        engine.test(h, get_iterator(False))\n        mlog.print_meter(mode=""Test"", iepoch=state[\'epoch\'])\n        mlog.reset_meter(mode=""Test"", iepoch=state[\'epoch\'])\n\n    engine.hooks[\'on_sample\'] = on_sample\n    engine.hooks[\'on_forward\'] = on_forward\n    engine.hooks[\'on_start_epoch\'] = on_start_epoch\n    engine.hooks[\'on_end_epoch\'] = on_end_epoch\n    engine.train(h, get_iterator(True), maxepoch=10, optimizer=optimizer)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
example/mnist_with_visdom.py,14,"b'"""""" Run MNIST example and log to visdom\n    Notes:\n        - Visdom must be installed (pip works)\n        - the Visdom server must be running at start!\n\n    Example:\n        $ python -m visdom.server -port 8097 &\n        $ python mnist_with_visdom.py\n""""""\nfrom tqdm import tqdm\nimport torch\nimport torch.optim\nimport torchnet as tnt\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.nn.init import kaiming_normal\nfrom torchnet.engine import Engine\nfrom torchnet.logger import VisdomPlotLogger, VisdomLogger\nfrom torchvision.datasets.mnist import MNIST\n\n\ndef get_iterator(mode):\n    ds = MNIST(root=\'./\', download=True, train=mode)\n    data = getattr(ds, \'train_data\' if mode else \'test_data\')\n    labels = getattr(ds, \'train_labels\' if mode else \'test_labels\')\n    tds = tnt.dataset.TensorDataset([data, labels])\n    return tds.parallel(batch_size=128, num_workers=4, shuffle=mode)\n\n\ndef conv_init(ni, no, k):\n    return kaiming_normal(torch.Tensor(no, ni, k, k))\n\n\ndef linear_init(ni, no):\n    return kaiming_normal(torch.Tensor(no, ni))\n\n\ndef f(params, inputs, mode):\n    o = inputs.view(inputs.size(0), 1, 28, 28)\n    o = F.conv2d(o, params[\'conv0.weight\'], params[\'conv0.bias\'], stride=2)\n    o = F.relu(o)\n    o = F.conv2d(o, params[\'conv1.weight\'], params[\'conv1.bias\'], stride=2)\n    o = F.relu(o)\n    o = o.view(o.size(0), -1)\n    o = F.linear(o, params[\'linear2.weight\'], params[\'linear2.bias\'])\n    o = F.relu(o)\n    o = F.linear(o, params[\'linear3.weight\'], params[\'linear3.bias\'])\n    return o\n\n\ndef main():\n    params = {\n        \'conv0.weight\': conv_init(1, 50, 5), \'conv0.bias\': torch.zeros(50),\n        \'conv1.weight\': conv_init(50, 50, 5), \'conv1.bias\': torch.zeros(50),\n        \'linear2.weight\': linear_init(800, 512), \'linear2.bias\': torch.zeros(512),\n        \'linear3.weight\': linear_init(512, 10), \'linear3.bias\': torch.zeros(10),\n    }\n    params = {k: Variable(v, requires_grad=True) for k, v in params.items()}\n\n    optimizer = torch.optim.SGD(\n        params.values(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n\n    engine = Engine()\n    meter_loss = tnt.meter.AverageValueMeter()\n    classerr = tnt.meter.ClassErrorMeter(accuracy=True)\n    confusion_meter = tnt.meter.ConfusionMeter(10, normalized=True)\n\n    port = 8097\n    train_loss_logger = VisdomPlotLogger(\n        \'line\', port=port, opts={\'title\': \'Train Loss\'})\n    train_err_logger = VisdomPlotLogger(\n        \'line\', port=port, opts={\'title\': \'Train Class Error\'})\n    test_loss_logger = VisdomPlotLogger(\n        \'line\', port=port, opts={\'title\': \'Test Loss\'})\n    test_err_logger = VisdomPlotLogger(\n        \'line\', port=port, opts={\'title\': \'Test Class Error\'})\n    confusion_logger = VisdomLogger(\'heatmap\', port=port, opts={\'title\': \'Confusion matrix\',\n                                                                \'columnnames\': list(range(10)),\n                                                                \'rownames\': list(range(10))})\n\n    def h(sample):\n        inputs = Variable(sample[0].float() / 255.0)\n        targets = Variable(torch.LongTensor(sample[1]))\n        o = f(params, inputs, sample[2])\n        return F.cross_entropy(o, targets), o\n\n    def reset_meters():\n        classerr.reset()\n        meter_loss.reset()\n        confusion_meter.reset()\n\n    def on_sample(state):\n        state[\'sample\'].append(state[\'train\'])\n\n    def on_forward(state):\n        classerr.add(state[\'output\'].data,\n                     torch.LongTensor(state[\'sample\'][1]))\n        confusion_meter.add(state[\'output\'].data,\n                            torch.LongTensor(state[\'sample\'][1]))\n        meter_loss.add(state[\'loss\'].data[0])\n\n    def on_start_epoch(state):\n        reset_meters()\n        state[\'iterator\'] = tqdm(state[\'iterator\'])\n\n    def on_end_epoch(state):\n        print(\'Training loss: %.4f, accuracy: %.2f%%\' % (meter_loss.value()[0], classerr.value()[0]))\n        train_loss_logger.log(state[\'epoch\'], meter_loss.value()[0])\n        train_err_logger.log(state[\'epoch\'], classerr.value()[0])\n\n        # do validation at the end of each epoch\n        reset_meters()\n        engine.test(h, get_iterator(False))\n        test_loss_logger.log(state[\'epoch\'], meter_loss.value()[0])\n        test_err_logger.log(state[\'epoch\'], classerr.value()[0])\n        confusion_logger.log(confusion_meter.value())\n        print(\'Testing loss: %.4f, accuracy: %.2f%%\' % (meter_loss.value()[0], classerr.value()[0]))\n\n    engine.hooks[\'on_sample\'] = on_sample\n    engine.hooks[\'on_forward\'] = on_forward\n    engine.hooks[\'on_start_epoch\'] = on_start_epoch\n    engine.hooks[\'on_end_epoch\'] = on_end_epoch\n    engine.train(h, get_iterator(True), maxepoch=10, optimizer=optimizer)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
test/test_datasets.py,5,"b'import unittest\nimport torch\nimport torchnet.dataset as dataset\nimport numpy as np\nimport os\nimport tempfile\n\n\nclass TestDatasets(unittest.TestCase):\n    def testListDataset(self):\n        h = [0, 1, 2]\n        d = dataset.ListDataset(elem_list=h, load=lambda x: x)\n        self.assertEqual(len(d), 3)\n        self.assertEqual(d[0], 0)\n\n        t = torch.LongTensor([0, 1, 2])\n        d = dataset.ListDataset(elem_list=t, load=lambda x: x)\n        self.assertEqual(len(d), 3)\n        self.assertEqual(d[0], 0)\n\n        a = np.asarray([0, 1, 2])\n        d = dataset.ListDataset(elem_list=a, load=lambda x: x)\n        self.assertEqual(len(d), 3)\n        self.assertEqual(d[0], 0)\n\n    def testListDataset_path(self):\n        tbl = [0, 1, 2]\n        d = dataset.ListDataset(tbl, \'bar/{}\'.format, \'foo\')\n        self.assertEqual(len(d), 3)\n        self.assertEqual(d[2], \'bar/foo/2\')\n\n    def testListDataset_file(self):\n        _, filename = tempfile.mkstemp()\n        with open(filename, \'w\') as f:\n            for i in range(0, 50):\n                f.write(str(i) + \'\\n\')\n\n        d = dataset.ListDataset(filename, lambda x: x, \'foo\')\n        self.assertEqual(len(d), 50)\n        self.assertEqual(d[15], \'foo/15\')\n\n        os.remove(filename)\n\n    def testTensorDataset(self):\n        # dict input\n        data = {\n            # \'input\': torch.arange(0,8),\n            \'input\': np.arange(0, 8),\n            \'target\': np.arange(0, 8),\n        }\n        d = dataset.TensorDataset(data)\n        self.assertEqual(len(d), 8)\n        self.assertEqual(d[2], {\'input\': 2, \'target\': 2})\n\n        # tensor input\n        a = torch.randn(8)\n        d = dataset.TensorDataset(a)\n        self.assertEqual(len(a), len(d))\n        self.assertEqual(a[1], d[1])\n\n        # list of tensors input\n        d = dataset.TensorDataset([a])\n        self.assertEqual(len(a), len(d))\n        self.assertEqual(a[1], d[1][0])\n\n    def testBatchDataset(self):\n        if hasattr(torch, ""arange""):\n            t = torch.arange(0, 16).long()\n        else:\n            t = torch.range(0, 15).long()\n        batchsize = 8\n        d = dataset.ListDataset(t, lambda x: {\'input\': x})\n        d = dataset.BatchDataset(d, batchsize)\n        ex = d[0][\'input\']\n        self.assertEqual(len(ex), batchsize)\n        self.assertEqual(ex[-1], batchsize - 1)\n\n    # def testTransformDataset(self):\n    #     d = dataset.TransformDataset(dataset.TensorDataset()\n\n    def testResampleDataset(self):\n        tbl = dataset.TensorDataset(np.asarray([0, 1, 2]))\n        d = dataset.ResampleDataset(tbl, lambda dataset, i: i % 2)\n        self.assertEqual(len(d), 3)\n        self.assertEqual(d[0], 0)\n        self.assertEqual(d[2], 0)\n\n    def testShuffleDataset(self):\n        tbl = dataset.TensorDataset(np.asarray([0, 1, 2, 3, 4]))\n        d = dataset.ShuffleDataset(tbl)\n        self.assertEqual(len(d), 5)\n        # TODO: every item should appear exactly once\n\n    def testSplitDataset(self):\n        h = [0, 1, 2, 3]\n        listdataset = dataset.ListDataset(elem_list=h)\n        splitdataset = dataset.SplitDataset(\n            listdataset, {\'train\': 3, \'val\': 1})\n\n        splitdataset.select(\'train\')\n        self.assertEqual(len(splitdataset), 3)\n        self.assertEqual(splitdataset[2], 2)\n\n        splitdataset.select(\'val\')\n        self.assertEqual(len(splitdataset), 1)\n        self.assertEqual(splitdataset[0], 3)\n\n        # test fluent api\n        splitdataset = listdataset.split({\'train\': 3, \'val\': 1})\n        splitdataset.select(\'train\')\n        self.assertEqual(len(splitdataset), 3)\n        self.assertEqual(splitdataset[2], 2)\n\n    def testSplitDataset_fractions(self):\n        h = [0, 1, 2, 3]\n        listdataset = dataset.ListDataset(elem_list=h)\n        splitdataset = dataset.SplitDataset(listdataset, {\'train\': 0.75,\n                                                          \'val\': 0.25})\n\n        splitdataset.select(\'train\')\n        self.assertEqual(len(splitdataset), 3)\n        self.assertEqual(splitdataset[2], 2)\n\n        splitdataset.select(\'val\')\n        self.assertEqual(len(splitdataset), 1)\n        self.assertEqual(splitdataset[0], 3)\n\n    def testConcatDataset(self):\n        l1 = dataset.ListDataset(elem_list=[0, 1, 2, 3])\n        l2 = dataset.ListDataset(elem_list=[10, 11, 13])\n        concatdataset = dataset.ConcatDataset([l1, l2])\n\n        self.assertEqual(len(concatdataset), 7)\n        self.assertEqual(concatdataset[0], 0)\n        self.assertEqual(concatdataset[3], 3)\n        self.assertEqual(concatdataset[4], 10)\n        self.assertEqual(concatdataset[6], 13)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
test/test_meters.py,56,"b'import unittest\nimport math\nimport torch\nimport torchnet.meter as meter\nimport numpy as np\n\n\nclass TestMeters(unittest.TestCase):\n\n    def testAverageValueMeter(self):\n        m = meter.AverageValueMeter()\n        for i in range(1, 10):\n            m.add(i)\n        mean, std = m.value()\n        self.assertEqual(mean, 5.0)\n        m.reset()\n        mean, std = m.value()\n\n        self.assertTrue(np.isnan(mean))\n\n    def testAverageValueMeter_np_2d(self):\n        m = meter.AverageValueMeter()\n        for i in range(1, 10):\n            m.add(np.float32([[i, i + 1]]))\n        mean, std = m.value()\n        self.assertTrue(np.allclose(mean, [[5.0, 6.0]]))\n        self.assertTrue(np.allclose(std, [[2.738613, 2.738613]]))\n        m.reset()\n        mean, std = m.value()\n\n        self.assertTrue(np.isnan(mean))\n\n    def testAverageValueMeter_torch_2d(self):\n        m = meter.AverageValueMeter()\n        for i in range(1, 10):\n            m.add(torch.Tensor([[i, i + 1]]))\n        mean, std = m.value()\n        self.assertTrue(np.allclose(mean, [[5.0, 6.0]]))\n        self.assertTrue(np.allclose(std, [[2.738613, 2.738613]]))\n        m.reset()\n        mean, std = m.value()\n\n        self.assertTrue(np.isnan(mean))\n\n    def testAverageValueMeter_n(self):\n        """"""Test the case of adding more than 1 value.\n        """"""\n        m = meter.AverageValueMeter()\n        for i in range(1, 11):\n            m.add(i, n=i)\n        mean, std = m.value()\n        self.assertEqual(mean, 7.0)\n        m.reset()\n        mean, std = m.value()\n\n        self.assertTrue(np.isnan(mean))\n\n    def testAverageValueMeter_stable(self):\n        """"""Test the case of near-zero variance.\n\n        The test compares the results to numpy, and uses\n        isclose() to allow for some small differences in\n        the results, which are due to slightly different arithmetic\n        operations and order.\n        """"""\n        def isclose(a, b, rel_tol=1e-09, abs_tol=0.0):\n            return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n\n        m = meter.AverageValueMeter()\n        samples = [0.7] * 10\n        truth = np.array([])\n        for sample in samples:\n            truth = np.append(truth, sample)\n            m.add(sample)\n            mean, std = m.value()\n            self.assertTrue(isclose(truth.mean(), mean))\n            self.assertTrue(\n                (math.isnan(std) and math.isnan(truth.std(ddof=1))) or\n                # When there is one sample in the dataset, numpy returns NaN and\n                # AverageValueMeter returns Inf.  We forgive AverageValueMeter :-)\n                (math.isinf(std) and math.isnan(truth.std(ddof=1))) or\n                isclose(std, truth.std(ddof=1), abs_tol=1e-07))\n\n    def testClassErrorMeter(self):\n        mtr = meter.ClassErrorMeter(topk=[1])\n        output = torch.eye(3)\n        if hasattr(torch, ""arange""):\n            target = torch.arange(0, 3)\n        else:\n            target = torch.range(0, 2)\n        mtr.add(output, target)\n        err = mtr.value()\n\n        self.assertEqual(err, [0], ""All should be correct"")\n\n        target[0] = 1\n        target[1] = 0\n        target[2] = 0\n        mtr.add(output, target)\n        err = mtr.value()\n        self.assertEqual(err, [50.0], ""Half should be correct"")\n\n    def testClassErrorMeteri_batch1(self):\n        mtr = meter.ClassErrorMeter(topk=[1])\n        output = torch.tensor([1, 0, 0])\n        if hasattr(torch, ""arange""):\n            target = torch.arange(0, 1)\n        else:\n            target = torch.range(0, 0)\n        mtr.add(output, target)\n        err = mtr.value()\n        self.assertEqual(err, [0], ""All should be correct"")\n\n    def testConfusionMeter(self):\n        mtr = meter.ConfusionMeter(k=3)\n\n        output = torch.Tensor([[.8, 0.1, 0.1], [10, 11, 10], [0.2, 0.2, .3]])\n        if hasattr(torch, ""arange""):\n            target = torch.arange(0, 3)\n        else:\n            target = torch.range(0, 2)\n        mtr.add(output, target)\n\n        conf_mtrx = mtr.value()\n        self.assertEqual(conf_mtrx.sum(), 3, ""All should be correct"")\n        self.assertEqual(conf_mtrx.diagonal().sum(),\n                         3, ""All should be correct"")\n\n        target = torch.Tensor([1, 0, 0])\n        mtr.add(output, target)\n\n        self.assertEqual(conf_mtrx.sum(), 6,\n                         ""Six tests should give six values"")\n        self.assertEqual(conf_mtrx.diagonal().sum(), 3,\n                         ""Shouldn\'t have changed since all new values were false"")\n        self.assertEqual(conf_mtrx[0].sum(), 3,\n                         ""All top have gotten one guess"")\n        self.assertEqual(conf_mtrx[1].sum(), 2,\n                         ""Two first at the 2nd row have a guess"")\n        self.assertEqual(conf_mtrx[1][2], 0,\n                         ""The last one should be empty"")\n        self.assertEqual(conf_mtrx[2].sum(), 1,\n                         ""Bottom row has only the first test correct"")\n        self.assertEqual(conf_mtrx[2][2], 1,\n                         ""Bottom row has only the first test correct"")\n\n        mtr = meter.ConfusionMeter(k=4, normalized=True)\n        output = torch.Tensor([\n            [.8, 0.1, 0.1, 0],\n            [10, 11, 10, 0],\n            [0.2, 0.2, .3, 0],\n            [0, 0, 0, 1],\n        ])\n\n        target = torch.Tensor([0, 1, 2, 3])\n        mtr.add(output, target)\n        conf_mtrx = mtr.value()\n\n        self.assertEqual(conf_mtrx.sum(), output.size(1),\n                         ""All should be correct"")\n        self.assertEqual(conf_mtrx.diagonal().sum(), output.size(1),\n                         ""All should be correct"")\n\n        target[0] = 1\n        target[1] = 0\n        target[2] = 0\n\n        mtr.add(output, target)\n        conf_mtrx = mtr.value()\n\n        self.assertEqual(conf_mtrx.sum(), output.size(1),\n                         ""The normalization should sum all values to 1"")\n        for i, row in enumerate(conf_mtrx):\n            self.assertEqual(row.sum(), 1,\n                             ""Row no "" + str(i) + "" fails to sum to one in normalized mode"")\n\n    def testMSEMeter(self):\n        a = torch.ones(7)\n        b = torch.zeros(7)\n\n        mtr = meter.MSEMeter()\n        mtr.add(a, b)\n        self.assertEqual(1.0, mtr.value())\n\n    def testMovingAverageValueMeter(self):\n        mtr = meter.MovingAverageValueMeter(3)\n\n        mtr.add(1)\n        avg, var = mtr.value()\n\n        self.assertEqual(avg, 1.0)\n        self.assertEqual(var, 0.0)\n        mtr.add(3)\n        avg, var = mtr.value()\n        self.assertEqual(avg, 2.0)\n        self.assertEqual(var, math.sqrt(2))\n\n        mtr.add(5)\n        avg, var = mtr.value()\n        self.assertEqual(avg, 3.0)\n        self.assertEqual(var, 2.0)\n\n        mtr.add(4)\n        avg, var = mtr.value()\n        self.assertEqual(avg, 4.0)\n        self.assertEqual(var, 1.0)\n\n        mtr.add(0)\n        avg, var = mtr.value()\n        self.assertEqual(avg, 3.0)\n        self.assertEqual(var, math.sqrt(7))\n\n    def testAUCMeter(self):\n        mtr = meter.AUCMeter()\n\n        test_size = 1000\n        mtr.add(torch.rand(test_size), torch.zeros(test_size))\n        mtr.add(torch.rand(test_size), torch.Tensor(test_size).fill_(1))\n\n        val, tpr, fpr = mtr.value()\n        self.assertTrue(math.fabs(val - 0.5) < 0.1, msg=""AUC Meter fails"")\n\n        mtr.reset()\n        mtr.add(torch.Tensor(test_size).fill_(0), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(0.1), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(0.2), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(0.3), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(0.4), torch.zeros(test_size))\n        mtr.add(torch.Tensor(test_size).fill_(1),\n                torch.Tensor(test_size).fill_(1))\n        val, tpr, fpr = mtr.value()\n\n        self.assertEqual(val, 1.0, msg=""AUC Meter fails"")\n\n    def testAPMeter(self):\n        mtr = meter.APMeter()\n\n        target = torch.Tensor([0, 1, 0, 1])\n        output = torch.Tensor([0.1, 0.2, 0.3, 4])\n        weight = torch.Tensor([0.5, 1.0, 2.0, 0.1])\n        mtr.add(output, target, weight)\n\n        ap = mtr.value()\n        val = (1 * 0.1 / 0.1 + 0 * 2.0 / 2.1 + 1.1 * 1 / 3.1 + 0 * 1 / 4) / 2.0\n        self.assertTrue(\n            math.fabs(ap[0] - val) < 0.01,\n            msg=\'ap test1 failed\'\n        )\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        val = (1 * 1.0 / 1.0 + 0 * 1.0 / 2.0 +\n               2 * 1.0 / 3.0 + 0 * 1.0 / 4.0) / 2.0\n        self.assertTrue(\n            math.fabs(ap[0] - val) < 0.01, msg=\'ap test2 failed\')\n\n        target = torch.Tensor([0, 1, 0, 1])\n        output = torch.Tensor([4, 3, 2, 1])\n        weight = torch.Tensor([1, 2, 3, 4])\n\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        val = (0 * 1.0 / 1.0 + 1.0 * 2.0 / 3.0 +\n               2.0 * 0 / 6.0 + 6.0 * 1.0 / 10.0) / 2.0\n        self.assertTrue(math.fabs(ap[0] - val) < 0.01, msg=\'ap test3 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        val = (0 * 1.0 + 1 * 1.0 / 2.0 + 0 * 1.0 / 3.0 + 2 * 1.0 / 4.0) / 2.0\n        self.assertTrue(math.fabs(ap[0] - val) < 0.01, msg=\'ap test4 failed\')\n\n        target = torch.Tensor([0, 1, 0, 1])\n        output = torch.Tensor([1, 4, 2, 3])\n        weight = torch.Tensor([1, 2, 3, 4])\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        val = (4 * 1.0 / 4.0 + 6 * 1.0 / 6.0 + 0 *\n               6.0 / 9.0 + 0 * 6.0 / 10.0) / 2.0\n        self.assertTrue(math.fabs(ap[0] - val) < 0.01, msg=\'ap test5 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        val = (1 * 1.0 + 2 * 1.0 / 2.0 + 0 * 1.0 / 3.0 + 0 * 1.0 / 4.0) / 2.0\n        self.assertTrue(math.fabs(ap[0] - val) < 0.01, msg=\'ap test6 failed\')\n\n        target = torch.Tensor([0, 0, 0, 0])\n        output = torch.Tensor([1, 4, 2, 3])\n        weight = torch.Tensor([1.0, 0.1, 0.0, 0.5])\n        mtr.reset()\n        mtr.add(output, target, weight)\n\n        ap = mtr.value()\n        self.assertEqual(ap[0], 0.)\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        self.assertEqual(ap[0], 0.)\n\n        target = torch.Tensor([1, 1, 0])\n        output = torch.Tensor([3, 1, 2])\n        weight = torch.Tensor([1, 0.1, 3])\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        val = (1 * 1.0 / 1.0 + 1 * 0.0 / 4.0 + 1.1 / 4.1) / 2.0\n        self.assertTrue(math.fabs(ap[0] - val) < 0.01, msg=\'ap test7 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        val = (1 * 1.0 + 0 * 1.0 / 2.0 + 2 * 1.0 / 3.0) / 2.0\n        self.assertTrue(math.fabs(ap[0] - val) < 0.01, msg=\'ap test8 failed\')\n\n        # Test multiple K\'s\n        target = torch.Tensor([[0, 1, 0, 1], [0, 1, 0, 1]]).transpose(0, 1)\n        output = torch.Tensor([[.1, .2, .3, 4], [4, 3, 2, 1]]).transpose(0, 1)\n        weight = torch.Tensor([[1.0, 0.5, 2.0, 3.0]]).transpose(0, 1)\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        self.assertTrue(\n            math.fabs(ap.sum() -\n                      torch.Tensor([\n                          (1 * 3.0 / 3.0 + 0 * 3.0 / 5.0 +\n                           3.5 * 1 / 5.5 + 0 * 3.5 / 6.5) / 2.0,\n                          (0 * 1.0 / 1.0 + 1 * 0.5 / 1.5 +\n                              0 * 0.5 / 3.5 + 1 * 3.5 / 6.5) / 2.0\n                      ]).sum()) < 0.01, msg=\'ap test9 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        self.assertTrue(\n            math.fabs(ap.sum() -\n                      torch.Tensor([\n                          (1 * 1.0 + 0 * 1.0 / 2.0 + 2 *\n                           1.0 / 3 + 0 * 1.0 / 4.0) / 2.0,\n                          (0 * 1.0 + 1 * 1.0 / 2.0 + 0 *\n                              1.0 / 3.0 + 2.0 * 1.0 / 4.0) / 2.0\n                      ]).sum()) < 0.01, msg=\'ap test10 failed\')\n\n        mtr.reset()\n        output = torch.Tensor(5, 4).fill_(0.25)\n        target = torch.ones(5, 4)\n        mtr.add(output, target)\n        output = torch.Tensor(1, 4).fill_(0.25)\n        target = torch.ones(1, 4)\n        mtr.add(output, target)\n        self.assertEqual(mtr.value().size(0), 4, msg=\'ap test11 failed\')\n\n    def testmAPMeter(self):\n        mtr = meter.mAPMeter()\n        target = torch.Tensor([0, 1, 0, 1])\n        output = torch.Tensor([0.1, 0.2, 0.3, 4])\n        weight = torch.Tensor([0.5, 1.0, 2.0, 0.1])\n        mtr.add(output, target)\n\n        ap = mtr.value()\n        val = (1 * 1.0 / 1.0 + 0 * 1.0 / 2.0 +\n               2.0 * 1.0 / 3.0 + 0 * 1.0 / 4.0) / 2.0\n        self.assertTrue(\n            math.fabs(ap - val) < 0.01,\n            msg=\'mAP test1 failed\'\n        )\n\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        val = (1 * 0.1 / 0.1 + 0 * 2.0 / 2.1 +\n               1.1 * 1 / 3.1 + 0 * 1.0 / 4.0) / 2.0\n        self.assertTrue(\n            math.fabs(ap - val) < 0.01, msg=\'mAP test2 failed\')\n\n        # Test multiple K\'s\n        target = torch.Tensor([[0, 1, 0, 1], [0, 1, 0, 1]]).transpose(0, 1)\n        output = torch.Tensor([[.1, .2, .3, 4], [4, 3, 2, 1]]).transpose(0, 1)\n        weight = torch.Tensor([[1.0, 0.5, 2.0, 3.0]]).transpose(0, 1)\n        mtr.reset()\n        mtr.add(output, target, weight)\n        ap = mtr.value()\n        self.assertTrue(\n            math.fabs(ap -\n                      torch.Tensor([\n                          (1 * 3.0 / 3.0 + 0 * 3.0 / 5.0 +\n                           3.5 * 1 / 5.5 + 0 * 3.5 / 6.5) / 2.0,\n                          (0 * 1.0 / 1.0 + 1 * 0.5 / 1.5 +\n                              0 * 0.5 / 3.5 + 1 * 3.5 / 6.5) / 2.0\n                      ]).mean()) < 0.01, msg=\'mAP test3 failed\')\n\n        mtr.reset()\n        mtr.add(output, target)\n        ap = mtr.value()\n        self.assertTrue(\n            math.fabs(ap -\n                      torch.Tensor([\n                          (1 * 1.0 + 0 * 1.0 / 2.0 + 2 *\n                           1.0 / 3.0 + 0 * 1.0 / 4.0) / 2.0,\n                          (0 * 1.0 + 1 * 1.0 / 2.0 + 0 *\n                              1.0 / 3.0 + 2 * 1.0 / 4.0) / 2.0\n                      ]).mean()) < 0.01, msg=\'mAP test4 failed\')\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
test/test_transforms.py,3,"b'import torchnet.transform as transform\nimport unittest\nimport torch\n\n\nclass TestTransforms(unittest.TestCase):\n    def testCompose(self):\n        self.assertEqual(transform.compose([lambda x: x + 1, lambda x: x + 2, lambda x: x / 2])(1), 2)\n\n    def testTableMergeKeys(self):\n        x = {\n            \'sample1\': {\'input\': 1, \'target\': ""a""},\n            \'sample2\': {\'input\': 2, \'target\': ""b"", \'flag\': ""hard""}\n        }\n\n        y = transform.tablemergekeys()(x)\n\n        self.assertEqual(y[\'input\'], {\'sample1\': 1, \'sample2\': 2})\n        self.assertEqual(y[\'target\'], {\'sample1\': ""a"", \'sample2\': ""b""})\n        self.assertEqual(y[\'flag\'], {\'sample2\': ""hard""})\n\n    def testTableApply(self):\n        x = {1: 1, 2: 2}\n        y = transform.tableapply(lambda x: x + 1)(x)\n        self.assertEqual(y, {1: 2, 2: 3})\n\n    def testMakeBatch(self):\n        x = [\n            {\'input\': torch.randn(4), \'target\': ""a""},\n            {\'input\': torch.randn(4), \'target\': ""b""},\n        ]\n        y = transform.makebatch()(x)\n        self.assertEqual(y[\'input\'].size(), torch.Size([2, 4]))\n        self.assertEqual(y[\'target\'], [""a"", ""b""])\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
torchnet/__init__.py,0,"b""from . import dataset, meter, engine, transform, logger\n__all__ = ['dataset', 'meter', 'engine', 'transform', 'logger']\n"""
torchnet/transform.py,0,"b""from six import iteritems\nfrom .utils.table import canmergetensor as canmerge\nfrom .utils.table import mergetensor as mergetensor\n\n\ndef compose(transforms):\n    assert isinstance(transforms, list)\n    for tr in transforms:\n        assert callable(tr), 'list of functions expected'\n\n    def composition(z):\n        for tr in transforms:\n            z = tr(z)\n        return z\n    return composition\n\n\ndef tablemergekeys():\n    def mergekeys(tbl):\n        mergetbl = {}\n        if isinstance(tbl, dict):\n            for idx, elem in tbl.items():\n                for key, value in elem.items():\n                    if key not in mergetbl:\n                        mergetbl[key] = {}\n                    mergetbl[key][idx] = value\n        elif isinstance(tbl, list):\n            for elem in tbl:\n                for key, value in elem.items():\n                    if key not in mergetbl:\n                        mergetbl[key] = []\n                    mergetbl[key].append(value)\n        return mergetbl\n    return mergekeys\n\n\ndef tableapply(f):\n    return lambda d: dict(map(lambda kv: (kv[0], f(kv[1])), iteritems(d)))\n\n\ndef makebatch(merge=None):\n    if merge:\n        makebatch = compose([tablemergekeys(), merge])\n    else:\n        makebatch = compose([\n            tablemergekeys(),\n            tableapply(lambda field: mergetensor(field)\n                       if canmerge(field) else field)\n        ])\n\n    return lambda samples: makebatch(samples)\n"""
torchnet/dataset/__init__.py,0,b'from .batchdataset import BatchDataset\nfrom .listdataset import ListDataset\nfrom .tensordataset import TensorDataset\nfrom .transformdataset import TransformDataset\nfrom .resampledataset import ResampleDataset\nfrom .shuffledataset import ShuffleDataset\nfrom .concatdataset import ConcatDataset\nfrom .splitdataset import SplitDataset\n'
torchnet/dataset/batchdataset.py,0,"b'import math\nfrom .dataset import Dataset\nfrom torchnet import transform\n\n\nclass BatchDataset(Dataset):\n    """"""\n    Dataset which batches the data from a given dataset.\n\n    Given a `dataset`, `BatchDataset` merges samples from this dataset to\n    form a new sample which can be interpreted as a batch of size `batchsize`.\n\n    The `merge` function controls how the batching is performed. By default\n    the occurrences are supposed to be tensors, and they aggregated along the\n    first dimension.\n\n    It is often important to shuffle examples while performing the batch\n    operation. `perm(idx, size)` is a function which returns the shuffled index\n    of the sample at position `idx` in the underlying dataset. For convenience,\n    the `size` of the underlying dataset is also passed to the function. By\n    default, the function is the identity.\n\n    The underlying dataset size might or might not be always divisible by\n    `batchsize`.  The optional `policy` string specify how to handle corner\n    cases.\n\n    Purpose: the concept of batch is problem dependent. In *torchnet*, it is up\n    to the user to interpret a sample as a batch or not. When one wants to\n    assemble samples from an existing dataset into a batch, then\n    `BatchDataset` is suited for the job. Sometimes it is however more\n    convenient to write a dataset from scratch providing ""batched"" samples.\n\n    Args:\n        dataset (Dataset): Dataset to be batched.\n        batchsize (int): Size of the batch.\n        perm (function, optional): Function used to shuffle the dataset before\n            batching. `perm(idx, size)` should return the shuffled index of\n            `idx` th sample. By default, the function is the identity.\n        merge (function, optional): Function to control batching behaviour.\n             `transform.makebatch(merge)` is used to make the batch. Default is\n             None.\n        policy (str, optional): Policy to handle the corner cases when the\n            underlying dataset size is not divisible by `batchsize`. One of\n            (`include-last`, `skip-last`, `divisible-only`).\n\n            - `include-last` makes sure all samples of the underlying dataset\n               will be seen, batches will be of size equal or inferior to\n               `batchsize`.\n            - `skip-last` will skip last examples of the underlying dataset if\n               its size is not properly divisible. Batches will be always of\n               size equal to `batchsize`.\n            - `divisible-only` will raise an error if the underlying dataset\n               has not a size divisible by `batchsize`.\n        filter (function, optional): Function to filter the sample before\n            batching. If `filter(sample)` is True, then sample is included for\n            batching. Otherwise, it is excluded. By default, `filter(sample)`\n            returns True for any `sample`.\n\n    """"""\n\n    def __init__(self,\n                 dataset,\n                 batchsize,\n                 perm=lambda idx, size: idx,\n                 merge=None,\n                 policy=\'include-last\',\n                 filter=lambda sample: True):\n        super(BatchDataset, self).__init__()\n        self.dataset = dataset\n        self.perm = perm\n        self.batchsize = batchsize\n        self.policy = policy\n        self.filter = filter\n        self.makebatch = transform.makebatch(merge)\n        len(self)\n\n    def __len__(self):\n        if self.policy == \'include-last\':\n            return int(math.ceil(float(len(self.dataset) / self.batchsize)))\n        elif self.policy == \'skip-last\':\n            return int(math.floor(float(len(self.dataset) / self.batchsize)))\n        elif self.policy == \'divisible-only\':\n            assert len(self.dataset) % self.batchsize == 0, \\\n                \'dataset size is not divisible by batch size\'\n            return len(self.dataset) / self.batchsize\n        else:\n            assert False, \'invalid policy (include-last | skip-last | \\\n                divisible-only expected)\'\n\n    def __getitem__(self, idx):\n        super(BatchDataset, self).__getitem__(idx)\n        maxidx = len(self.dataset)\n\n        samples = []\n        for i in range(0, self.batchsize):\n            j = idx * self.batchsize + i\n            if j >= maxidx:\n                break\n\n            j = self.perm(j, maxidx)\n            sample = self.dataset[j]\n\n            if self.filter(sample):\n                samples.append(sample)\n\n        samples = self.makebatch(samples)\n        return samples\n'"
torchnet/dataset/concatdataset.py,0,"b'from .dataset import Dataset\nimport numpy as np\n\n\nclass ConcatDataset(Dataset):\n    """"""\n    Dataset to concatenate multiple datasets.\n\n    Purpose: useful to assemble different existing datasets, possibly\n    large-scale datasets as the concatenation operation is done in an\n    on-the-fly manner.\n\n    Args:\n        datasets (iterable): List of datasets to be concatenated\n    """"""\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__()\n\n        self.datasets = list(datasets)\n        assert len(datasets) > 0, \'datasets should not be an empty iterable\'\n        self.cum_sizes = np.cumsum([len(x) for x in self.datasets])\n\n    def __len__(self):\n        return self.cum_sizes[-1]\n\n    def __getitem__(self, idx):\n        super(ConcatDataset, self).__getitem__(idx)\n        dataset_index = self.cum_sizes.searchsorted(idx, \'right\')\n\n        if dataset_index == 0:\n            dataset_idx = idx\n        else:\n            dataset_idx = idx - self.cum_sizes[dataset_index - 1]\n\n        return self.datasets[dataset_index][dataset_idx]\n'"
torchnet/dataset/dataset.py,1,"b'import torchnet\nfrom torch.utils.data import DataLoader\n\n\nclass Dataset(object):\n    def __init__(self):\n        pass\n\n    def __len__(self):\n        pass\n\n    def __getitem__(self, idx):\n        if idx >= len(self):\n            raise IndexError(""CustomRange index out of range"")\n        pass\n\n    def batch(self, *args, **kwargs):\n        return torchnet.dataset.BatchDataset(self, *args, **kwargs)\n\n    def transform(self, *args, **kwargs):\n        return torchnet.dataset.TransformDataset(self, *args, **kwargs)\n\n    def shuffle(self, *args, **kwargs):\n        return torchnet.dataset.ShuffleDataset(self, *args, **kwargs)\n\n    def parallel(self, *args, **kwargs):\n        return DataLoader(self, *args, **kwargs)\n\n    def split(self, *args, **kwargs):\n        return torchnet.dataset.SplitDataset(self, *args, **kwargs)\n'"
torchnet/dataset/listdataset.py,0,"b'from .dataset import Dataset\n\n\nclass ListDataset(Dataset):\n    """"""\n    Dataset which loads data from a list using given function.\n\n    Considering a `elem_list` (can be an iterable or a `string` ) i-th sample\n    of a dataset will be returned by `load(elem_list[i])`, where `load()`\n    is a function provided by the user.\n\n    If `path` is provided, `elem_list` is assumed to be a list of strings, and\n    each element `elem_list[i]` will prefixed by `path/` when fed to `load()`.\n\n    Purpose: many low or medium-scale datasets can be seen as a list of files\n    (for example representing input samples). For this list of file, a target\n    can be often inferred in a simple manner.\n\n    Args:\n        elem_list (iterable/str): List of arguments which will be passed to\n            `load` function. It can also be a path to file with each line\n            containing the arguments to `load`\n        load (function, optional): Function which loads the data.\n            i-th sample is returned by `load(elem_list[i])`. By default `load`\n            is identity i.e, `lambda x: x`\n        path (str, optional): Defaults to None. If a string is provided,\n            `elem_list` is assumed to be a list of strings, and each element\n            `elem_list[i]` will prefixed by this string when fed to `load()`.\n\n    """"""\n\n    def __init__(self, elem_list, load=lambda x: x, path=None):\n        super(ListDataset, self).__init__()\n\n        if isinstance(elem_list, str):\n            with open(elem_list) as f:\n                self.list = [line.replace(\'\\n\', \'\') for line in f]\n        else:\n            # just assume iterable\n            self.list = elem_list\n\n        self.path = path\n        self.load = load\n\n    def __len__(self):\n        return len(self.list)\n\n    def __getitem__(self, idx):\n        super(ListDataset, self).__getitem__(idx)\n\n        if self.path is not None:\n            return self.load(""%s/%s"" % (self.path, self.list[idx]))\n        else:\n            return self.load(self.list[idx])\n'"
torchnet/dataset/resampledataset.py,0,"b'from .dataset import Dataset\n\n\nclass ResampleDataset(Dataset):\n    """"""\n    Dataset which resamples a given dataset.\n\n    Given a `dataset`, creates a new dataset which will (re-)sample from this\n    underlying dataset using the provided `sampler(dataset, idx)` function.\n\n    If `size` is provided, then the newly created dataset will have the\n    specified `size`, which might be different than the underlying dataset\n    size. If `size` is not provided, then the new dataset will have the same\n    size as the underlying one.\n\n    Purpose: shuffling data, re-weighting samples, getting a subset of the\n    data. Note that an important sub-class `ShuffleDataset` is provided for\n    convenience.\n\n    Args:\n        dataset (Dataset): Dataset to be resampled.\n        sampler (function, optional): Function used for sampling. `idx`th\n            sample is returned by `dataset[sampler(dataset, idx)]`. By default\n            `sampler(dataset, idx)` is the identity, simply returning `idx`.\n            `sampler(dataset, idx)` must return an index in the range\n            acceptable for the underlying `dataset`.\n        size (int, optional): Desired size of the dataset after resampling. By\n            default, the new dataset will have the same size as the underlying\n            one.\n\n    """"""\n\n    def __init__(self, dataset, sampler=lambda ds, idx: idx, size=None):\n        super(ResampleDataset, self).__init__()\n        self.dataset = dataset\n        self.sampler = sampler\n        self.size = size\n\n    def __len__(self):\n        return (self.size and self.size > 0) and self.size or len(self.dataset)\n\n    def __getitem__(self, idx):\n        super(ResampleDataset, self).__getitem__(idx)\n        idx = self.sampler(self.dataset, idx)\n\n        if idx < 0 or idx >= len(self.dataset):\n            raise IndexError(\'out of range\')\n\n        return self.dataset[idx]\n'"
torchnet/dataset/shuffledataset.py,4,"b'from .resampledataset import ResampleDataset\nimport torch\n\n\nclass ShuffleDataset(ResampleDataset):\n    """"""\n    Dataset which shuffles a given dataset.\n\n    `ShuffleDataset` is a sub-class of `ResampleDataset` provided for\n    convenience. It samples uniformly from the given `dataset` with, or without\n    `replacement`. The chosen partition can be redrawn by calling `resample()`\n\n    If `replacement` is `true`, then the specified `size` may be larger than\n    the underlying `dataset`.\n    If `size` is not provided, then the new dataset size will be equal to the\n    underlying `dataset` size.\n\n    Purpose: the easiest way to shuffle a dataset!\n\n    Args:\n        dataset (Dataset): Dataset to be shuffled.\n        size (int, optional): Desired size of the shuffled dataset. If\n            `replacement` is `true`, then can be larger than the `len(dataset)`.\n            By default, the new dataset will have the same size as `dataset`.\n        replacement (bool, optional): True if uniform sampling is to be done\n            with replacement. False otherwise. Defaults to false.\n\n    Raises:\n        ValueError: If `size` is larger than the size of the underlying dataset\n            and `replacement` is False.\n    """"""\n\n    def __init__(self, dataset, size=None, replacement=False):\n        if size and not replacement and size > len(dataset):\n            raise ValueError(\'size cannot be larger than underlying dataset \\\n                    size when sampling without replacement\')\n\n        super(ShuffleDataset, self).__init__(dataset,\n                                             lambda dataset, idx: self.perm[idx],\n                                             size)\n        self.replacement = replacement\n        self.resample()\n\n    def resample(self, seed=None):\n        """"""Resample the dataset.\n\n        Args:\n            seed (int, optional): Seed for resampling. By default no seed is\n            used.\n        """"""\n        if seed is not None:\n            gen = torch.manual_seed(seed)\n        else:\n            gen = torch.default_generator\n\n        if self.replacement:\n            self.perm = torch.LongTensor(len(self)).random_(\n                len(self.dataset), generator=gen)\n        else:\n            self.perm = torch.randperm(\n                len(self.dataset), generator=gen).narrow(0, 0, len(self))\n'"
torchnet/dataset/splitdataset.py,0,"b'from .dataset import Dataset\nimport numpy as np\n\n\nclass SplitDataset(Dataset):\n    """"""\n    Dataset to partition a given dataset.\n\n    Partition a given `dataset`, according to the specified `partitions`. Use\n    the method `select()` to select the current partition in use.\n\n    The `partitions` is a dictionary where a key is a user-chosen string\n    naming the partition, and value is a number representing the weight (as a\n    number between 0 and 1) or the size (in number of samples) of the\n    corresponding partition.\n\n    Partioning is achieved linearly (no shuffling). See `ShuffleDataset` if you\n    want to shuffle the dataset before partitioning.\n\n    Args:\n        dataset (Dataset): Dataset to be split.\n        partitions (dict): Dictionary where key is a user-chosen string\n            naming the partition, and value is a number representing the weight\n            (as a number between 0 and 1) or the size (in number of samples)\n            of the corresponding partition.\n        initial_partition (str, optional): Initial parition to be selected.\n\n    """"""\n\n    def __init__(self, dataset, partitions, initial_partition=None):\n        super(SplitDataset, self).__init__()\n\n        self.dataset = dataset\n        self.partitions = partitions\n\n        # A few assertions\n        assert isinstance(partitions, dict), \'partitions must be a dict\'\n        assert len(partitions) >= 2, \\\n            \'SplitDataset should have at least two partitions\'\n        assert min(partitions.values()) >= 0, \\\n            \'partition sizes cannot be negative\'\n        assert max(partitions.values()) > 0, \'all partitions cannot be empty\'\n\n        self.partition_names = sorted(list(self.partitions.keys()))\n        self.partition_index = {partition: i for i, partition in\n                                enumerate(self.partition_names)}\n\n        self.partition_sizes = [self.partitions[parition] for parition in\n                                self.partition_names]\n        # if partition sizes are fractions, convert to sizes:\n        if sum(self.partition_sizes) <= 1:\n            self.partition_sizes = [round(x * len(dataset)) for x in\n                                    self.partition_sizes]\n        else:\n            for x in self.partition_sizes:\n                assert x == int(x), (\'partition sizes should be integer\'\n                                     \' numbers, or sum up to <= 1 \')\n\n        self.partition_cum_sizes = np.cumsum(self.partition_sizes)\n\n        if initial_partition is not None:\n            self.select(initial_partition)\n\n    def select(self, partition):\n        """"""\n        Select the parition.\n\n        Args:\n            partition (str): Partition to be selected.\n        """"""\n        self.current_partition_idx = self.partition_index[partition]\n\n    def __len__(self):\n        try:\n            return self.partition_sizes[self.current_partition_idx]\n        except AttributeError:\n            raise ValueError(""Select a partition before accessing data."")\n\n    def __getitem__(self, idx):\n        super(SplitDataset, self).__getitem__(idx)\n        try:\n            if self.current_partition_idx == 0:\n                return self.dataset[idx]\n            else:\n                offset = self.partition_cum_sizes[self.current_partition_idx - 1]\n                return self.dataset[int(offset) + idx]\n        except AttributeError:\n            raise ValueError(""Select a partition before accessing data."")\n'"
torchnet/dataset/tensordataset.py,2,"b'from .dataset import Dataset\nimport torch\nimport numpy as np\n\n\nclass TensorDataset(Dataset):\n    """"""\n    Dataset from a tensor or array or list or dict.\n\n    `TensorDataset` provides a way to create a dataset out of the data that is\n    already loaded into memory. It accepts data in the following forms:\n\n    tensor or numpy array\n        `idx`th sample is `data[idx]`\n\n    dict of tensors or numpy arrays\n        `idx`th sample is `{k: v[idx] for k, v in data.items()}`\n\n    list of tensors or numpy arrays\n        `idx`th sample is `[v[idx] for v in data]`\n\n    Purpose: Easy way to create a dataset out of standard data structures.\n\n    Args:\n        data (dict/list/tensor/ndarray): Data for the dataset.\n    """"""\n\n    def __init__(self, data):\n        super(TensorDataset, self).__init__()\n\n        if isinstance(data, dict):\n            assert len(data) > 0, ""Should have at least one element""\n            # check that all fields have the same size\n            n_elem = len(list(data.values())[0])\n            for v in data.values():\n                assert len(v) == n_elem, ""All values must have the same size""\n        elif isinstance(data, list):\n            assert len(data) > 0, ""Should have at least one element""\n            n_elem = len(data[0])\n            for v in data:\n                assert len(v) == n_elem, ""All elements must have the same size""\n\n        self.data = data\n\n    def __len__(self):\n        if isinstance(self.data, dict):\n            return len(list(self.data.values())[0])\n        elif isinstance(self.data, list):\n            return len(self.data[0])\n        elif torch.is_tensor(self.data) or isinstance(self.data, np.ndarray):\n            return len(self.data)\n\n    def __getitem__(self, idx):\n        super(TensorDataset, self).__getitem__(idx)\n        if isinstance(self.data, dict):\n            return {k: v[idx] for k, v in self.data.items()}\n        elif isinstance(self.data, list):\n            return [v[idx] for v in self.data]\n        elif torch.is_tensor(self.data) or isinstance(self.data, np.ndarray):\n            return self.data[idx]\n'"
torchnet/dataset/transformdataset.py,0,"b'from .dataset import Dataset\n\n\nclass TransformDataset(Dataset):\n    """"""\n    Dataset which transforms a given dataset with a given function.\n\n    Given a function `transform`, and a `dataset`, `TransformDataset` applies\n    the function in an on-the-fly manner when querying a sample with\n    `__getitem__(idx)` and therefore returning `transform[dataset[idx]]`.\n\n    `transform` can also be a dict with functions as values. In this case, it\n    is assumed that `dataset[idx]` is a dict which has all the keys in\n    `transform`. Then, `transform[key]` is applied to dataset[idx][key] for\n    each key in `transform`\n\n    The size of the new dataset is equal to the size of the underlying\n    `dataset`.\n\n    Purpose: when performing pre-processing operations, it is convenient to be\n    able to perform on-the-fly transformations to a dataset.\n\n    Args:\n        dataset (Dataset): Dataset which has to be transformed.\n        transforms (function/dict): Function or dict with function as values.\n            These functions will be applied to data.\n    """"""\n\n    def __init__(self, dataset, transforms):\n        super(TransformDataset, self).__init__()\n\n        assert isinstance(transforms, dict) or callable(transforms), \\\n            \'expected a dict of transforms or a function\'\n        if isinstance(transforms, dict):\n            for k, v in transforms.items():\n                assert callable(v), str(k) + \' is not a function\'\n\n        self.dataset = dataset\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        super(TransformDataset, self).__getitem__(idx)\n        z = self.dataset[idx]\n\n        if isinstance(self.transforms, dict):\n            for k, transform in self.transforms.items():\n                z[k] = transform(z[k])\n        else:\n            z = self.transforms(z)\n\n        return z\n'"
torchnet/engine/__init__.py,0,b'from .engine import Engine\n'
torchnet/engine/engine.py,3,"b'class Engine(object):\n    def __init__(self):\n        self.hooks = {}\n\n    def hook(self, name, state):\n        r""""""Registers a backward hook.\n\n        The hook will be called every time a gradient with respect to the\n        Tensor is computed. The hook should have the following signature::\n\n            hook (grad) -> Tensor or None\n\n        The hook should not modify its argument, but it can optionally return\n        a new gradient which will be used in place of :attr:`grad`.\n        This function returns a handle with a method ``handle.remove()``\n        that removes the hook from the module.\n\n        Example:\n            >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n            >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n            >>> v.backward(torch.tensor([1., 2., 3.]))\n            >>> v.grad\n             2\n             4\n             6\n            [torch.FloatTensor of size (3,)]\n            >>> h.remove()  # removes the hook\n\n        """"""\n        if name in self.hooks:\n            self.hooks[name](state)\n\n    def train(self, network, iterator, maxepoch, optimizer):\n        state = {\n            \'network\': network,\n            \'iterator\': iterator,\n            \'maxepoch\': maxepoch,\n            \'optimizer\': optimizer,\n            \'epoch\': 0,\n            \'t\': 0,\n            \'train\': True,\n        }\n\n        self.hook(\'on_start\', state)\n        while state[\'epoch\'] < state[\'maxepoch\']:\n            self.hook(\'on_start_epoch\', state)\n            for sample in state[\'iterator\']:\n                state[\'sample\'] = sample\n                self.hook(\'on_sample\', state)\n\n                def closure():\n                    loss, output = state[\'network\'](state[\'sample\'])\n                    state[\'output\'] = output\n                    state[\'loss\'] = loss\n                    loss.backward()\n                    self.hook(\'on_forward\', state)\n                    # to free memory in save_for_backward\n                    state[\'output\'] = None\n                    state[\'loss\'] = None\n                    return loss\n\n                state[\'optimizer\'].zero_grad()\n                state[\'optimizer\'].step(closure)\n                self.hook(\'on_update\', state)\n                state[\'t\'] += 1\n            state[\'epoch\'] += 1\n            self.hook(\'on_end_epoch\', state)\n        self.hook(\'on_end\', state)\n        return state\n\n    def test(self, network, iterator):\n        state = {\n            \'network\': network,\n            \'iterator\': iterator,\n            \'t\': 0,\n            \'train\': False,\n        }\n\n        self.hook(\'on_start\', state)\n        for sample in state[\'iterator\']:\n            state[\'sample\'] = sample\n            self.hook(\'on_sample\', state)\n\n            def closure():\n                loss, output = state[\'network\'](state[\'sample\'])\n                state[\'output\'] = output\n                state[\'loss\'] = loss\n                self.hook(\'on_forward\', state)\n                # to free memory in save_for_backward\n                state[\'output\'] = None\n                state[\'loss\'] = None\n\n            closure()\n            state[\'t\'] += 1\n        self.hook(\'on_end\', state)\n        return state\n'"
torchnet/logger/__init__.py,0,"b'from .visdomlogger import VisdomLogger, VisdomPlotLogger, VisdomSaver, VisdomTextLogger\nfrom .meterlogger import MeterLogger\n'"
torchnet/logger/logger.py,0,"b'"""""" Logging values to various sinks """"""\n\n\nclass Logger(object):\n    _fields = None\n\n    @property\n    def fields(self):\n        assert self._fields is not None, ""self.fields is not set!""\n        return self._fields\n\n    @fields.setter\n    def fields(self, value):\n        self._fields\n\n    def __init__(self, fields=None):\n        """""" Automatically logs the variables in \'fields\' """"""\n        self.fields = fields\n\n    def log(self, *args, **kwargs):\n        pass\n\n    def log_state(self, state_dict):\n        pass\n'"
torchnet/logger/meterlogger.py,4,"b'import torch\nimport torchnet as tnt\nfrom torchnet.logger import VisdomPlotLogger, VisdomLogger\n\n\nclass MeterLogger(object):\n    \'\'\' A class to package and visualize meters.\n\n    Args:\n        server: The uri of the Visdom server\n        env: Visdom environment to log to.\n        port: Port of the visdom server.\n        title: The title of the MeterLogger. This will be used as a prefix for all plots.\n        nclass: If logging for classification problems, the number of classes.\n        plotstylecombined: Whether to plot train/test curves in the same window.\n    \'\'\'\n    def __init__(self, server=""localhost"", env=\'main\', port=8097, title=""DNN"", nclass=21, plotstylecombined=True):\n        self.nclass = nclass\n        self.meter = {}\n        self.server = server\n        self.port = port\n        self.env = env\n        self.nclass = nclass\n        self.topk = 5 if nclass > 5 else nclass\n        self.title = title\n        self.logger = {\'Train\': {}, \'Test\': {}}\n        self.timer = tnt.meter.TimeMeter(None)\n        self.plotstylecombined = plotstylecombined\n\n    def _ver2tensor(self, target):\n        target_mat = torch.zeros(target.shape[0], self.nclass)\n        for i, j in enumerate(target):\n            target_mat[i][j] = 1\n        return target_mat\n\n    def __to_tensor(self, var):\n        if isinstance(var, torch.autograd.Variable):\n            var = var.data\n        if not torch.is_tensor(var):\n            var = torch.from_numpy(var)\n        return var\n\n    def __addlogger(self, meter, ptype):\n        if ptype == \'line\':\n            if self.plotstylecombined:\n                opts = {\'title\': self.title + \' \' + meter}\n                self.logger[\'Train\'][meter] = VisdomPlotLogger(ptype, env=self.env, server=self.server,\n                                                               port=self.port, opts=opts)\n                opts = {}\n                self.logger[\'Test\'][meter] = self.logger[\'Train\'][meter]\n            else:\n                opts = {\'title\': self.title + \'Train \' + meter}\n                self.logger[\'Train\'][meter] = VisdomPlotLogger(ptype, env=self.env, server=self.server,\n                                                               port=self.port, opts=opts)\n                opts = {\'title\': self.title + \'Test \' + meter}\n                self.logger[\'Test\'][meter] = VisdomPlotLogger(ptype, env=self.env, server=self.server,\n                                                              port=self.port, opts=opts)\n        elif ptype == \'heatmap\':\n            names = list(range(self.nclass))\n            opts = {\'title\': self.title + \' Train \' + meter, \'columnnames\': names, \'rownames\': names}\n            self.logger[\'Train\'][meter] = VisdomLogger(\'heatmap\', env=self.env, server=self.server,\n                                                       port=self.port, opts=opts)\n            opts = {\'title\': self.title + \' Test \' + meter, \'columnnames\': names, \'rownames\': names}\n            self.logger[\'Test\'][meter] = VisdomLogger(\'heatmap\', env=self.env, server=self.server,\n                                                      port=self.port, opts=opts)\n\n    def __addloss(self, meter):\n        self.meter[meter] = tnt.meter.AverageValueMeter()\n        self.__addlogger(meter, \'line\')\n\n    def __addmeter(self, meter):\n        if meter == \'accuracy\':\n            self.meter[meter] = tnt.meter.ClassErrorMeter(topk=(1, self.topk), accuracy=True)\n            self.__addlogger(meter, \'line\')\n        elif meter == \'map\':\n            self.meter[meter] = tnt.meter.mAPMeter()\n            self.__addlogger(meter, \'line\')\n        elif meter == \'auc\':\n            self.meter[meter] = tnt.meter.AUCMeter()\n            self.__addlogger(meter, \'line\')\n        elif meter == \'confusion\':\n            self.meter[meter] = tnt.meter.ConfusionMeter(self.nclass, normalized=True)\n            self.__addlogger(meter, \'heatmap\')\n\n    def update_meter(self, output, target, meters={\'accuracy\'}):\n        output = self.__to_tensor(output)\n        target = self.__to_tensor(target)\n        for meter in meters:\n            if meter not in self.meter.keys():\n                self.__addmeter(meter)\n            if meter in [\'ap\', \'map\', \'confusion\']:\n                target_th = self._ver2tensor(target)\n                self.meter[meter].add(output, target_th)\n            else:\n                self.meter[meter].add(output, target)\n\n    def update_loss(self, loss, meter=\'loss\'):\n        loss = self.__to_tensor(loss)\n        if meter not in self.meter.keys():\n            self.__addloss(meter)\n        self.meter[meter].add(loss.item())\n\n    def peek_meter(self):\n        \'\'\'Returns a dict of all meters and their values.\'\'\'\n        result = {}\n        for key in self.meter.keys():\n            val = self.meter[key].value()\n            val = val[0] if isinstance(val, (list, tuple)) else val\n            result[key] = val\n        return result\n\n    def reset_meter(self, iepoch, mode=\'Train\'):\n        self.timer.reset()\n        for key in self.meter.keys():\n            val = self.meter[key].value()\n            val = val[0] if isinstance(val, (list, tuple)) else val\n            if key in [\'confusion\', \'histogram\', \'image\']:\n                self.logger[mode][key].log(val)\n            else:\n                self.logger[mode][key].log(iepoch, val, name=mode)\n            self.meter[key].reset()\n\n    def print_meter(self, mode, iepoch, ibatch=1, totalbatch=1, meterlist=None):\n        pstr = ""%s:\\t[%d][%d/%d] \\t""\n        tval = []\n        tval.extend([mode, iepoch, ibatch, totalbatch])\n        if meterlist is None:\n            meterlist = self.meter.keys()\n            for meter in meterlist:\n                if meter in [\'confusion\', \'histogram\', \'image\']:\n                    continue\n                if meter == \'accuracy\':\n                    pstr += ""Acc@1 %.2f%% \\t Acc@"" + str(self.topk) + "" %.2f%% \\t""\n                    tval.extend([self.meter[meter].value()[0], self.meter[meter].value()[1]])\n                elif meter == \'map\':\n                    pstr += ""mAP %.3f \\t""\n                    tval.extend([self.meter[meter].value()])\n                elif meter == \'auc\':\n                    pstr += ""AUC %.3f \\t""\n                    tval.extend([self.meter[meter].value()])\n                else:\n                    pstr += meter + "" %.3f (%.3f)\\t""\n                    tval.extend([self.meter[meter].val, self.meter[meter].mean])\n                    pstr += "" %.2fs/its\\t""\n                    tval.extend([self.timer.value()])\n        print(pstr % tuple(tval))\n'"
torchnet/logger/visdomlogger.py,0,"b'"""""" Logging to Visdom server """"""\nimport numpy as np\nimport visdom\n\nfrom .logger import Logger\n\n\nclass BaseVisdomLogger(Logger):\n    \'\'\'\n        The base class for logging output to Visdom.\n\n        ***THIS CLASS IS ABSTRACT AND MUST BE SUBCLASSED***\n\n        Note that the Visdom server is designed to also handle a server architecture,\n        and therefore the Visdom server must be running at all times. The server can\n        be started with\n        $ python -m visdom.server\n        and you probably want to run it from screen or tmux.\n    \'\'\'\n\n    @property\n    def viz(self):\n        return self._viz\n\n    def __init__(self, fields=None, win=None, env=None, opts={}, port=8097, server=""localhost""):\n        super(BaseVisdomLogger, self).__init__(fields)\n        self.win = win\n        self.env = env\n        self.opts = opts\n        self._viz = visdom.Visdom(server=""http://"" + server, port=port)\n\n    def log(self, *args, **kwargs):\n        raise NotImplementedError(\n            ""log not implemented for BaseVisdomLogger, which is an abstract class."")\n\n    def _viz_prototype(self, vis_fn):\n        \'\'\' Outputs a function which will log the arguments to Visdom in an appropriate way.\n\n            Args:\n                vis_fn: A function, such as self.vis.image\n        \'\'\'\n        def _viz_logger(*args, **kwargs):\n            self.win = vis_fn(*args,\n                              win=self.win,\n                              env=self.env,\n                              opts=self.opts,\n                              **kwargs)\n        return _viz_logger\n\n    def log_state(self, state):\n        """""" Gathers the stats from self.trainer.stats and passes them into\n            self.log, as a list """"""\n        results = []\n        for field_idx, field in enumerate(self.fields):\n            parent, stat = None, state\n            for f in field:\n                parent, stat = stat, stat[f]\n            results.append(stat)\n        self.log(*results)\n\n\nclass VisdomSaver(object):\n    \'\'\' Serialize the state of the Visdom server to disk.\n        Unless you have a fancy schedule, where different are saved with different frequencies,\n        you probably only need one of these.\n    \'\'\'\n\n    def __init__(self, envs=None, port=8097, server=""localhost""):\n        super(VisdomSaver, self).__init__()\n        self.envs = envs\n        self.viz = visdom.Visdom(server=""http://"" + server, port=port)\n\n    def save(self, *args, **kwargs):\n        self.viz.save(self.envs)\n\n\nclass VisdomLogger(BaseVisdomLogger):\n    \'\'\'\n        A generic Visdom class that works with the majority of Visdom plot types.\n    \'\'\'\n\n    def __init__(self, plot_type, fields=None, win=None, env=None, opts={}, port=8097, server=""localhost""):\n        \'\'\'\n            Args:\n                fields: Currently unused\n                plot_type: The name of the plot type, in Visdom\n\n            Examples:\n                >>> # Image example\n                >>> img_to_use = skimage.data.coffee().swapaxes(0,2).swapaxes(1,2)\n                >>> image_logger = VisdomLogger(\'image\')\n                >>> image_logger.log(img_to_use)\n\n                >>> # Histogram example\n                >>> hist_data = np.random.rand(10000)\n                >>> hist_logger = VisdomLogger(\'histogram\', , opts=dict(title=\'Random!\', numbins=20))\n                >>> hist_logger.log(hist_data)\n        \'\'\'\n        super(VisdomLogger, self).__init__(fields, win, env, opts, port, server)\n        self.plot_type = plot_type\n        self.chart = getattr(self.viz, plot_type)\n        self.viz_logger = self._viz_prototype(self.chart)\n\n    def log(self, *args, **kwargs):\n        self.viz_logger(*args, **kwargs)\n\n\nclass VisdomPlotLogger(BaseVisdomLogger):\n\n    def __init__(self, plot_type, fields=None, win=None, env=None, opts={}, port=8097, server=""localhost"", name=None):\n        \'\'\'\n            Multiple lines can be added to the same plot with the ""name"" attribute (see example)\n            Args:\n                fields: Currently unused\n                plot_type: {scatter, line}\n\n            Examples:\n                >>> scatter_logger = VisdomPlotLogger(\'line\')\n                >>> scatter_logger.log(stats[\'epoch\'], loss_meter.value()[0], name=""train"")\n                >>> scatter_logger.log(stats[\'epoch\'], loss_meter.value()[0], name=""test"")\n        \'\'\'\n        super(VisdomPlotLogger, self).__init__(fields, win, env, opts, port, server)\n        valid_plot_types = {\n            ""scatter"": self.viz.scatter,\n            ""line"": self.viz.line}\n        self.plot_type = plot_type\n        # Set chart type\n        if plot_type not in valid_plot_types.keys():\n            raise ValueError(""plot_type \\\'{}\\\' not found. Must be one of {}"".format(\n                plot_type, valid_plot_types.keys()))\n        self.chart = valid_plot_types[plot_type]\n\n    def log(self, *args, **kwargs):\n        if self.win is not None and self.viz.win_exists(win=self.win, env=self.env):\n            if len(args) != 2:\n                raise ValueError(""When logging to {}, must pass in x and y values (and optionally z)."".format(\n                    type(self)))\n            x, y = args\n            self.chart(\n                X=np.array([x]),\n                Y=np.array([y]),\n                update=\'append\',\n                win=self.win,\n                env=self.env,\n                opts=self.opts,\n                **kwargs)\n        else:\n            if self.plot_type == \'scatter\':\n                chart_args = {\'X\': np.array([args])}\n            else:\n                chart_args = {\'X\': np.array([args[0]]),\n                              \'Y\': np.array([args[1]])}\n            self.win = self.chart(\n                win=self.win,\n                env=self.env,\n                opts=self.opts,\n                **chart_args)\n            # For some reason, the first point is a different trace. So for now\n            # we can just add the point again, this time on the correct curve.\n            self.log(*args, **kwargs)\n\n\nclass VisdomTextLogger(BaseVisdomLogger):\n    \'\'\'Creates a text window in visdom and logs output to it.\n\n    The output can be formatted with fancy HTML, and it new output can\n    be set to \'append\' or \'replace\' mode.\n\n    Args:\n        fields: Currently not used\n        update_type: One of {\'REPLACE\', \'APPEND\'}. Default \'REPLACE\'.\n\n    For examples, make sure that your visdom server is running.\n\n    Example:\n        >>> notes_logger = VisdomTextLogger(update_type=\'APPEND\')\n        >>> for i in range(10):\n        >>>     notes_logger.log(""Printing: {} of {}"".format(i+1, 10))\n        # results will be in Visdom environment (default: http://localhost:8097)\n\n    \'\'\'\n    valid_update_types = [\'REPLACE\', \'APPEND\']\n\n    def __init__(self, fields=None, win=None, env=None, opts={}, update_type=valid_update_types[0],\n                 port=8097, server=""localhost""):\n\n        super(VisdomTextLogger, self).__init__(fields, win, env, opts, port, server)\n        self.text = \'\'\n\n        if update_type not in self.valid_update_types:\n            raise ValueError(""update type \'{}\' not found. Must be one of {}"".format(\n                update_type, self.valid_update_types))\n        self.update_type = update_type\n\n        self.viz_logger = self._viz_prototype(self.viz.text)\n\n    def log(self, msg, *args, **kwargs):\n        text = msg\n        if self.update_type == \'APPEND\' and self.text:\n            self.text = ""<br>"".join([self.text, text])\n        else:\n            self.text = text\n        self.viz_logger([self.text])\n\n    def _log_all(self, stats, log_fields, prefix=None, suffix=None, require_dict=False):\n        results = []\n        for field_idx, field in enumerate(self.fields):\n            parent, stat = None, stats\n            for f in field:\n                parent, stat = stat, stat[f]\n            name, output = self._gather_outputs(field, log_fields,\n                                                parent, stat, require_dict)\n            if not output:\n                continue\n            self._align_output(field_idx, output)\n            results.append((name, output))\n        if not results:\n            return\n        output = self._join_results(results)\n        if prefix is not None:\n            self.log(prefix)\n        self.log(output)\n        if suffix is not None:\n            self.log(suffix)\n\n    def _align_output(self, field_idx, output):\n        for output_idx, o in enumerate(output):\n            if len(o) < self.field_widths[field_idx][output_idx]:\n                num_spaces = self.field_widths[field_idx][output_idx] - len(o)\n                output[output_idx] += \' \' * num_spaces\n            else:\n                self.field_widths[field_idx][output_idx] = len(o)\n\n    def _join_results(self, results):\n        joined_out = map(lambda i: (i[0], \' \'.join(i[1])), results)\n        joined_fields = map(lambda i: \'{}: {}\'.format(i[0], i[1]), joined_out)\n        return \'\\t\'.join(joined_fields)\n\n    def _gather_outputs(self, field, log_fields, stat_parent, stat, require_dict=False):\n        output = []\n        name = \'\'\n        if isinstance(stat, dict):\n            log_fields = stat.get(log_fields, [])\n            name = stat.get(\'log_name\', \'.\'.join(field))\n            for f in log_fields:\n                output.append(f.format(**stat))\n        elif not require_dict:\n            name = \'.\'.join(field)\n            number_format = stat_parent.get(\'log_format\', \'\')\n            unit = stat_parent.get(\'log_unit\', \'\')\n            fmt = \'{\' + number_format + \'}\' + unit\n            output.append(fmt.format(stat))\n        return name, output\n'"
torchnet/meter/__init__.py,0,b'from .averagevaluemeter import AverageValueMeter\nfrom .classerrormeter import ClassErrorMeter\nfrom .confusionmeter import ConfusionMeter\nfrom .timemeter import TimeMeter\nfrom .msemeter import MSEMeter\nfrom .movingaveragevaluemeter import MovingAverageValueMeter\nfrom .aucmeter import AUCMeter\nfrom .apmeter import APMeter\nfrom .mapmeter import mAPMeter\n'
torchnet/meter/apmeter.py,15,"b'import math\nfrom . import meter\nimport torch\n\n\nclass APMeter(meter.Meter):\n    """"""\n    The APMeter measures the average precision per class.\n\n    The APMeter is designed to operate on `NxK` Tensors `output` and\n    `target`, and optionally a `Nx1` Tensor weight where (1) the `output`\n    contains model output scores for `N` examples and `K` classes that ought to\n    be higher when the model is more convinced that the example should be\n    positively labeled, and smaller when the model believes the example should\n    be negatively labeled (for instance, the output of a sigmoid function); (2)\n    the `target` contains only values 0 (for negative examples) and 1\n    (for positive examples); and (3) the `weight` ( > 0) represents weight for\n    each sample.\n\n    """"""\n\n    def __init__(self):\n        super(APMeter, self).__init__()\n        self.reset()\n\n    def reset(self):\n        """"""Resets the meter with empty member variables""""""\n        self.scores = torch.FloatTensor(torch.FloatStorage())\n        self.targets = torch.LongTensor(torch.LongStorage())\n        self.weights = torch.FloatTensor(torch.FloatStorage())\n\n    def add(self, output, target, weight=None):\n        """"""Add a new observation\n\n        Args:\n            output (Tensor): NxK tensor that for each of the N examples\n                indicates the probability of the example belonging to each of\n                the K classes, according to the model. The probabilities should\n                sum to one over all classes\n            target (Tensor): binary NxK tensort that encodes which of the K\n                classes are associated with the N-th input\n                (eg: a row [0, 1, 0, 1] indicates that the example is\n                associated with classes 2 and 4)\n            weight (optional, Tensor): Nx1 tensor representing the weight for\n                each example (each weight > 0)\n\n        """"""\n        if not torch.is_tensor(output):\n            output = torch.from_numpy(output)\n        if not torch.is_tensor(target):\n            target = torch.from_numpy(target)\n\n        if weight is not None:\n            if not torch.is_tensor(weight):\n                weight = torch.from_numpy(weight)\n            weight = weight.squeeze()\n        if output.dim() == 1:\n            output = output.view(-1, 1)\n        else:\n            assert output.dim() == 2, \\\n                \'wrong output size (should be 1D or 2D with one column \\\n                per class)\'\n        if target.dim() == 1:\n            target = target.view(-1, 1)\n        else:\n            assert target.dim() == 2, \\\n                \'wrong target size (should be 1D or 2D with one column \\\n                per class)\'\n        if weight is not None:\n            assert weight.dim() == 1, \'Weight dimension should be 1\'\n            assert weight.numel() == target.size(0), \\\n                \'Weight dimension 1 should be the same as that of target\'\n            assert torch.min(weight) >= 0, \'Weight should be non-negative only\'\n        assert torch.equal(target**2, target), \\\n            \'targets should be binary (0 or 1)\'\n        if self.scores.numel() > 0:\n            assert target.size(1) == self.targets.size(1), \\\n                \'dimensions for output should match previously added examples.\'\n\n        # make sure storage is of sufficient size\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\n            new_weight_size = math.ceil(self.weights.storage().size() * 1.5)\n            self.scores.storage().resize_(int(new_size + output.numel()))\n            self.targets.storage().resize_(int(new_size + output.numel()))\n            if weight is not None:\n                self.weights.storage().resize_(int(new_weight_size + output.size(0)))\n\n        # store scores and targets\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\n        self.scores.resize_(offset + output.size(0), output.size(1))\n        self.targets.resize_(offset + target.size(0), target.size(1))\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)\n\n        if weight is not None:\n            self.weights.resize_(offset + weight.size(0))\n            self.weights.narrow(0, offset, weight.size(0)).copy_(weight)\n\n    def value(self):\n        """"""Returns the model\'s average precision for each class\n\n        Return:\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\n\n        """"""\n\n        if self.scores.numel() == 0:\n            return 0\n        ap = torch.zeros(self.scores.size(1))\n        if hasattr(torch, ""arange""):\n            rg = torch.arange(1, self.scores.size(0) + 1).float()\n        else:\n            rg = torch.range(1, self.scores.size(0)).float()\n        if self.weights.numel() > 0:\n            weight = self.weights.new(self.weights.size())\n            weighted_truth = self.weights.new(self.weights.size())\n\n        # compute average precision for each class\n        for k in range(self.scores.size(1)):\n            # sort scores\n            scores = self.scores[:, k]\n            targets = self.targets[:, k]\n            _, sortind = torch.sort(scores, 0, True)\n            truth = targets[sortind]\n            if self.weights.numel() > 0:\n                weight = self.weights[sortind]\n                weighted_truth = truth.float() * weight\n                rg = weight.cumsum(0)\n\n            # compute true positive sums\n            if self.weights.numel() > 0:\n                tp = weighted_truth.cumsum(0)\n            else:\n                tp = truth.float().cumsum(0)\n\n            # compute precision curve\n            precision = tp.div(rg)\n\n            # compute average precision\n            ap[k] = precision[truth.bool()].sum() / max(float(truth.sum()), 1)\n        return ap\n'"
torchnet/meter/aucmeter.py,5,"b'import numbers\nfrom . import meter\nimport numpy as np\nimport torch\n\n\nclass AUCMeter(meter.Meter):\n    """"""\n    The AUCMeter measures the area under the receiver-operating characteristic\n    (ROC) curve for binary classification problems. The area under the curve (AUC)\n    can be interpreted as the probability that, given a randomly selected positive\n    example and a randomly selected negative example, the positive example is\n    assigned a higher score by the classification model than the negative example.\n\n    The AUCMeter is designed to operate on one-dimensional Tensors `output`\n    and `target`, where (1) the `output` contains model output scores that ought to\n    be higher when the model is more convinced that the example should be positively\n    labeled, and smaller when the model believes the example should be negatively\n    labeled (for instance, the output of a signoid function); and (2) the `target`\n    contains only values 0 (for negative examples) and 1 (for positive examples).\n    """"""\n\n    def __init__(self):\n        super(AUCMeter, self).__init__()\n        self.reset()\n\n    def reset(self):\n        self.scores = torch.DoubleTensor(torch.DoubleStorage()).numpy()\n        self.targets = torch.LongTensor(torch.LongStorage()).numpy()\n\n    def add(self, output, target):\n        if torch.is_tensor(output):\n            output = output.cpu().squeeze().numpy()\n        if torch.is_tensor(target):\n            target = target.cpu().squeeze().numpy()\n        elif isinstance(target, numbers.Number):\n            target = np.asarray([target])\n        assert np.ndim(output) == 1, \\\n            \'wrong output size (1D expected)\'\n        assert np.ndim(target) == 1, \\\n            \'wrong target size (1D expected)\'\n        assert output.shape[0] == target.shape[0], \\\n            \'number of outputs and targets does not match\'\n        assert np.all(np.add(np.equal(target, 1), np.equal(target, 0))), \\\n            \'targets should be binary (0, 1)\'\n\n        self.scores = np.append(self.scores, output)\n        self.targets = np.append(self.targets, target)\n\n    def value(self):\n        # case when number of elements added are 0\n        if self.scores.shape[0] == 0:\n            return (0.5, 0.0, 0.0)\n\n        # sorting the arrays\n        scores, sortind = torch.sort(torch.from_numpy(\n            self.scores), dim=0, descending=True)\n        scores = scores.numpy()\n        sortind = sortind.numpy()\n\n        # creating the roc curve\n        tpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n        fpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n\n        for i in range(1, scores.size + 1):\n            if self.targets[sortind[i - 1]] == 1:\n                tpr[i] = tpr[i - 1] + 1\n                fpr[i] = fpr[i - 1]\n            else:\n                tpr[i] = tpr[i - 1]\n                fpr[i] = fpr[i - 1] + 1\n\n        tpr /= (self.targets.sum() * 1.0)\n        fpr /= ((self.targets - 1.0).sum() * -1.0)\n\n        # calculating area under curve using trapezoidal rule\n        n = tpr.shape[0]\n        h = fpr[1:n] - fpr[0:n - 1]\n        sum_h = np.zeros(fpr.shape)\n        sum_h[0:n - 1] = h\n        sum_h[1:n] += h\n        area = (sum_h * tpr).sum() / 2.0\n\n        return (area, tpr, fpr)\n'"
torchnet/meter/averagevaluemeter.py,0,"b'import math\nfrom . import meter\nimport numpy as np\n\n\nclass AverageValueMeter(meter.Meter):\n    def __init__(self):\n        super(AverageValueMeter, self).__init__()\n        self.reset()\n        self.val = 0\n\n    def add(self, value, n=1):\n        self.val = value\n        self.sum += value * n\n        if n <= 0:\n            raise ValueError(""Cannot use a non-positive weight for the running stat."")\n        elif self.n == 0:\n            self.mean = 0.0 + value  # This is to force a copy in torch/numpy\n            self.std = np.inf\n            self.mean_old = self.mean\n            self.m_s = 0.0\n        else:\n            self.mean = self.mean_old + n * (value - self.mean_old) / float(self.n + n)\n            self.m_s += n * (value - self.mean_old) * (value - self.mean)\n            self.mean_old = self.mean\n            self.std = np.sqrt(self.m_s / (self.n + n - 1.0))\n        self.var = self.std ** 2\n\n        self.n += n\n\n    def value(self):\n        return self.mean, self.std\n\n    def reset(self):\n        self.n = 0\n        self.sum = 0.0\n        self.var = 0.0\n        self.val = 0.0\n        self.mean = np.nan\n        self.mean_old = 0.0\n        self.m_s = 0.0\n        self.std = np.nan\n'"
torchnet/meter/classerrormeter.py,3,"b""import numpy as np\nimport torch\nimport numbers\nfrom . import meter\n\n\nclass ClassErrorMeter(meter.Meter):\n    def __init__(self, topk=[1], accuracy=False):\n        super(ClassErrorMeter, self).__init__()\n        self.topk = np.sort(topk)\n        self.accuracy = accuracy\n        self.reset()\n\n    def reset(self):\n        self.sum = {v: 0 for v in self.topk}\n        self.n = 0\n\n    def add(self, output, target):\n        if torch.is_tensor(output):\n            output = output.cpu().squeeze().numpy()\n        if torch.is_tensor(target):\n            target = np.atleast_1d(target.cpu().squeeze().numpy())\n        elif isinstance(target, numbers.Number):\n            target = np.asarray([target])\n        if np.ndim(output) == 1:\n            output = output[np.newaxis]\n        else:\n            assert np.ndim(output) == 2, \\\n                'wrong output size (1D or 2D expected)'\n            assert np.ndim(target) == 1, \\\n                'target and output do not match'\n        assert target.shape[0] == output.shape[0], \\\n            'target and output do not match'\n        topk = self.topk\n        maxk = int(topk[-1])  # seems like Python3 wants int and not np.int64\n        no = output.shape[0]\n\n        pred = torch.from_numpy(output).topk(maxk, 1, True, True)[1].numpy()\n        correct = pred == target[:, np.newaxis].repeat(pred.shape[1], 1)\n\n        for k in topk:\n            self.sum[k] += no - correct[:, 0:k].sum()\n        self.n += no\n\n    def value(self, k=-1):\n        if k != -1:\n            assert k in self.sum.keys(), \\\n                'invalid k (this k was not provided at construction time)'\n            if self.accuracy:\n                return (1. - float(self.sum[k]) / self.n) * 100.0\n            else:\n                return float(self.sum[k]) / self.n * 100.0\n        else:\n            return [self.value(k_) for k_ in self.topk]\n"""
torchnet/meter/confusionmeter.py,0,"b'from . import meter\nimport numpy as np\n\n\nclass ConfusionMeter(meter.Meter):\n    """"""Maintains a confusion matrix for a given calssification problem.\n\n    The ConfusionMeter constructs a confusion matrix for a multi-class\n    classification problems. It does not support multi-label, multi-class problems:\n    for such problems, please use MultiLabelConfusionMeter.\n\n    Args:\n        k (int): number of classes in the classification problem\n        normalized (boolean): Determines whether or not the confusion matrix\n            is normalized or not\n\n    """"""\n\n    def __init__(self, k, normalized=False):\n        super(ConfusionMeter, self).__init__()\n        self.conf = np.ndarray((k, k), dtype=np.int32)\n        self.normalized = normalized\n        self.k = k\n        self.reset()\n\n    def reset(self):\n        self.conf.fill(0)\n\n    def add(self, predicted, target):\n        """"""Computes the confusion matrix of K x K size where K is no of classes\n\n        Args:\n            predicted (tensor): Can be an N x K tensor of predicted scores obtained from\n                the model for N examples and K classes or an N-tensor of\n                integer values between 0 and K-1.\n            target (tensor): Can be a N-tensor of integer values assumed to be integer\n                values between 0 and K-1 or N x K tensor, where targets are\n                assumed to be provided as one-hot vectors\n\n        """"""\n        predicted = predicted.cpu().numpy()\n        target = target.cpu().numpy()\n\n        assert predicted.shape[0] == target.shape[0], \\\n            \'number of targets and predicted outputs do not match\'\n\n        if np.ndim(predicted) != 1:\n            assert predicted.shape[1] == self.k, \\\n                \'number of predictions does not match size of confusion matrix\'\n            predicted = np.argmax(predicted, 1)\n        else:\n            assert (predicted.max() < self.k) and (predicted.min() >= 0), \\\n                \'predicted values are not between 1 and k\'\n\n        onehot_target = np.ndim(target) != 1\n        if onehot_target:\n            assert target.shape[1] == self.k, \\\n                \'Onehot target does not match size of confusion matrix\'\n            assert (target >= 0).all() and (target <= 1).all(), \\\n                \'in one-hot encoding, target values should be 0 or 1\'\n            assert (target.sum(1) == 1).all(), \\\n                \'multi-label setting is not supported\'\n            target = np.argmax(target, 1)\n        else:\n            assert (predicted.max() < self.k) and (predicted.min() >= 0), \\\n                \'predicted values are not between 0 and k-1\'\n\n        # hack for bincounting 2 arrays together\n        x = predicted + self.k * target\n        bincount_2d = np.bincount(x.astype(np.int32),\n                                  minlength=self.k ** 2)\n        assert bincount_2d.size == self.k ** 2\n        conf = bincount_2d.reshape((self.k, self.k))\n\n        self.conf += conf\n\n    def value(self):\n        """"""\n        Returns:\n            Confustion matrix of K rows and K columns, where rows corresponds\n            to ground-truth targets and columns corresponds to predicted\n            targets.\n        """"""\n        if self.normalized:\n            conf = self.conf.astype(np.float32)\n            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n        else:\n            return self.conf\n'"
torchnet/meter/mapmeter.py,0,"b'from . import meter, APMeter\n\n\nclass mAPMeter(meter.Meter):\n    """"""\n    The mAPMeter measures the mean average precision over all classes.\n\n    The mAPMeter is designed to operate on `NxK` Tensors `output` and\n    `target`, and optionally a `Nx1` Tensor weight where (1) the `output`\n    contains model output scores for `N` examples and `K` classes that ought to\n    be higher when the model is more convinced that the example should be\n    positively labeled, and smaller when the model believes the example should\n    be negatively labeled (for instance, the output of a sigmoid function); (2)\n    the `target` contains only values 0 (for negative examples) and 1\n    (for positive examples); and (3) the `weight` ( > 0) represents weight for\n    each sample.\n    """"""\n\n    def __init__(self):\n        super(mAPMeter, self).__init__()\n        self.apmeter = APMeter()\n\n    def reset(self):\n        self.apmeter.reset()\n\n    def add(self, output, target, weight=None):\n        self.apmeter.add(output, target, weight)\n\n    def value(self):\n        return self.apmeter.value().mean()\n'"
torchnet/meter/meter.py,0,"b""\nclass Meter(object):\n    '''Meters provide a way to keep track of important statistics in an online manner.\n\n    This class is abstract, but provides a standard interface for all meters to follow.\n\n    '''\n\n    def reset(self):\n        '''Resets the meter to default settings.'''\n        pass\n\n    def add(self, value):\n        '''Log a new value to the meter\n\n        Args:\n            value: Next restult to include.\n\n        '''\n        pass\n\n    def value(self):\n        '''Get the value of the meter in the current state.'''\n        pass\n"""
torchnet/meter/movingaveragevaluemeter.py,1,"b'import math\nfrom . import meter\nimport torch\n\n\nclass MovingAverageValueMeter(meter.Meter):\n    def __init__(self, windowsize):\n        super(MovingAverageValueMeter, self).__init__()\n        self.windowsize = windowsize\n        self.valuequeue = torch.Tensor(windowsize)\n        self.reset()\n\n    def reset(self):\n        self.sum = 0.0\n        self.n = 0\n        self.var = 0.0\n        self.valuequeue.fill_(0)\n\n    def add(self, value):\n        queueid = (self.n % self.windowsize)\n        oldvalue = self.valuequeue[queueid]\n        self.sum += value - oldvalue\n        self.var += value * value - oldvalue * oldvalue\n        self.valuequeue[queueid] = value\n        self.n += 1\n\n    def value(self):\n        n = min(self.n, self.windowsize)\n        mean = self.sum / max(1, n)\n        std = math.sqrt(max((self.var - n * mean * mean) / max(1, n - 1), 0))\n        return mean, std\n'"
torchnet/meter/msemeter.py,4,"b'import math\nfrom . import meter\nimport torch\n\n\nclass MSEMeter(meter.Meter):\n    def __init__(self, root=False):\n        super(MSEMeter, self).__init__()\n        self.reset()\n        self.root = root\n\n    def reset(self):\n        self.n = 0\n        self.sesum = 0.0\n\n    def add(self, output, target):\n        if not torch.is_tensor(output) and not torch.is_tensor(target):\n            output = torch.from_numpy(output)\n            target = torch.from_numpy(target)\n        self.n += output.numel()\n        self.sesum += torch.sum((output - target) ** 2)\n\n    def value(self):\n        mse = self.sesum / max(1, self.n)\n        return math.sqrt(mse) if self.root else mse\n'"
torchnet/meter/timemeter.py,0,"b'import time\nfrom . import meter\n\n\nclass TimeMeter(meter.Meter):\n    """"""\n    <a name=""TimeMeter"">\n    #### tnt.TimeMeter(@ARGP)\n    @ARGT\n\n    The `tnt.TimeMeter` is designed to measure the time between events and can be\n    used to measure, for instance, the average processing time per batch of data.\n    It is different from most other meters in terms of the methods it provides:\n\n    The `tnt.TimeMeter` provides the following methods:\n\n       * `reset()` resets the timer, setting the timer and unit counter to zero.\n       * `value()` returns the time passed since the last `reset()`; divided by the counter value when `unit=true`.\n    """"""\n\n    def __init__(self, unit):\n        super(TimeMeter, self).__init__()\n        self.unit = unit\n        self.reset()\n\n    def add(self, n=1):\n        self.n += n\n\n    def reset(self):\n        self.n = 0\n        self.time = time.time()\n\n    def value(self):\n        if self.unit and self.n == 0:\n            raise ValueError(""Trying to divide by zero in TimeMeter"")\n        elif self.unit:\n            return (time.time() - self.time) / self.n\n        else:\n            return time.time() - self.time\n'"
torchnet/utils/__init__.py,0,b'from .resultswriter import ResultsWriter\nfrom .multitaskdataloader import MultiTaskDataLoader\n'
torchnet/utils/multitaskdataloader.py,2,"b""from itertools import islice, chain, repeat\nimport torch.utils.data\n\n\nclass MultiTaskDataLoader(object):\n    '''Loads batches simultaneously from multiple datasets.\n\n    The MultiTaskDataLoader is designed to make multi-task learning simpler. It is\n    ideal for jointly training a model for multiple tasks or multiple datasets.\n    MultiTaskDataLoader is initialzes with an iterable of :class:`Dataset` objects,\n    and provides an iterator which will return one batch that contains an equal number\n    of samples from each of the :class:`Dataset` s.\n\n    Specifically, it returns batches of  ``[(B_0, 0), (B_1, 1), ..., (B_k, k)]``\n    from datasets ``(D_0, ..., D_k)``, where each `B_i` has :attr:`batch_size` samples\n\n\n    Args:\n        datasets: A list of :class:`Dataset` objects to serve batches from\n        batch_size: Each batch from each :class:`Dataset` will have this many samples\n        use_all (bool): If True, then the iterator will return batches until all\n            datasets are exhausted. If False, then iteration stops as soon as one dataset\n            runs out\n        loading_kwargs: These are passed to the children dataloaders\n\n\n    Example:\n        >>> train_loader = MultiTaskDataLoader([dataset1, dataset2], batch_size=3)\n        >>> for ((datas1, labels1), task1), (datas2, labels2), task2) in train_loader:\n        >>>     print(task1, task2)\n        0 1\n        0 1\n        ...\n        0 1\n\n    '''\n\n    def __init__(self, datasets, batch_size=1, use_all=False, **loading_kwargs):\n        self.loaders = []\n        self.batch_size = batch_size\n        self.use_all = use_all\n        self.loading_kwargs = loading_kwargs\n        for dataset in datasets:\n            loader = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=self.batch_size,\n                **self.loading_kwargs)\n            self.loaders.append(loader)\n        self.min_loader_size = min([len(l) for l in self.loaders])\n        self.current_loader = 0\n\n    def __iter__(self):\n        '''Returns an iterator that simultaneously returns batches from each dataset.\n        Specifically, it returns batches of\n            [(B_0, 0), (B_1, 1), ..., (B_k, k)]\n        from datasets\n            (D_0, ..., D_k),\n\n        '''\n        return zip_batches(*[zip(iter(l), repeat(loader_num)) for loader_num, l in enumerate(self.loaders)],\n                           use_all=self.use_all)\n\n    def __len__(self):\n        if self.use_all:\n            return max([len(l) for loader in self.loaders])\n        else:\n            return self.min_loader_size\n\n\ndef zip_batches(*iterables, **kwargs):\n    use_all = kwargs.pop('use_all', False)\n    if use_all:\n        try:\n            from itertools import izip_longest as zip_longest\n        except ImportError:\n            from itertools import zip_longest\n        return zip_longest(fillvalue=None, *iterables)\n    else:\n        return zip(*iterables)\n"""
torchnet/utils/resultswriter.py,0,"b'import os\nimport pickle\n\n\nclass ResultsWriter(object):\n    \'\'\'Logs results to a file.\n\n    The ResultsWriter provides a convenient interface for periodically writing\n    results to a file. It is designed to capture all information for a given\n    experiment, which may have a sequence of distinct tasks. Therefore, it writes\n    results in the format::\n\n        {\n            \'tasks\': [...]\n            \'results\': [...]\n        }\n\n    The ResultsWriter class chooses to use a top-level list instead of a dictionary\n    to preserve temporal order of tasks (by default).\n\n    Args:\n        filepath (str): Path to write results to\n        overwrite (bool): whether to clobber a file if it exists\n\n    Example:\n        >>> result_writer = ResultWriter(path)\n        >>> for task in [\'CIFAR-10\', \'SVHN\']:\n        >>>    train_results = train_model()\n        >>>    test_results = test_model()\n        >>>    result_writer.update(task, {\'Train\': train_results, \'Test\': test_results})\n\n    \'\'\'\n\n    def __init__(self, filepath, overwrite=False):\n        if overwrite:\n            with open(filepath, \'wb\') as f:\n                pickle.dump({\n                    \'tasks\': [],\n                    \'results\': []\n                }, f)\n        else:\n            assert not os.path.exists(filepath), \'Cannot write results to ""{}"". Already exists!\'.format(filepath)\n        self.filepath = filepath\n        self.tasks = set()\n\n    def _add_task(self, task_name):\n        assert task_name not in self.tasks, ""Task already added! Use a different name.""\n        self.tasks.add(task_name)\n\n    def update(self, task_name, result):\n        \'\'\' Update the results file with new information.\n\n        Args:\n            task_name (str): Name of the currently running task. A previously unseen\n                ``task_name`` will create a new entry in both :attr:`tasks`\n                and :attr:`results`.\n            result: This will be appended to the list in :attr:`results` which\n                corresponds to the ``task_name`` in ``task_name``:attr:`tasks`.\n\n        \'\'\'\n        with open(self.filepath, \'rb\') as f:\n            existing_results = pickle.load(f)\n        if task_name not in self.tasks:\n            self._add_task(task_name)\n            existing_results[\'tasks\'].append(task_name)\n            existing_results[\'results\'].append([])\n        task_name_idx = existing_results[\'tasks\'].index(task_name)\n        results = existing_results[\'results\'][task_name_idx]\n        results.append(result)\n        with open(self.filepath, \'wb\') as f:\n            pickle.dump(existing_results, f)\n'"
torchnet/utils/table.py,2,"b'import torch\n\n\ndef canmergetensor(tbl):\n    if not isinstance(tbl, list):\n        return False\n\n    if torch.is_tensor(tbl[0]):\n        sz = tbl[0].numel()\n        for v in tbl:\n            if v.numel() != sz:\n                return False\n        return True\n    return False\n\n\ndef mergetensor(tbl):\n    sz = [len(tbl)] + list(tbl[0].size())\n    res = tbl[0].new(torch.Size(sz))\n    for i, v in enumerate(tbl):\n        res[i].copy_(v)\n    return res\n'"
