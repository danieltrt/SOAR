file_path,api_count,code
setup.py,0,"b""#!/usr/bin/env python3\n\nfrom setuptools import setup, find_packages\n\n\ndef readme():\n    with open('README.md') as f:\n        return f.read()\n\n\ndef requirements():\n    with open('requirements.txt') as f:\n        return f.read()\n\n\nsetup(\n    name='pytorch-translate',\n    version='0.1',\n    author='Facebook AI',\n    description=('Facebook Translation System'),\n    long_description=readme(),\n    url='https://github.com/pytorch/translate',\n    license='BSD',\n    packages=find_packages(),\n    install_requires=[\n        'fairseq>=0.5.0',\n    ],\n    dependency_links=[\n        'git+https://github.com/pytorch/fairseq.git#egg=fairseq-0.5.0',\n    ],\n    test_suite='pytorch_translate',\n)\n"""
pytorch_translate/__init__.py,0,b''
pytorch_translate/average_attention.py,9,"b'#!/usr/bin/env python3\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom torch import nn\n\n\n@with_incremental_state\nclass AttentionAbstract(nn.Module):\n    """"""Abstract class for attention modules\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.incremental_clone_ids = set("""")\n\n    def forward(\n        self,\n        query,\n        key,\n        value,\n        mask_future_timesteps=False,\n        key_padding_mask=None,\n        incremental_state=None,\n        need_weights=True,\n        static_kv=False,\n    ):\n        raise NotImplementedError()\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        """"""Reorder buffered internal state (for incremental generation).""""""\n        for incremental_clone_id in self.incremental_clone_ids:\n            input_buffer = self._get_input_buffer(\n                incremental_state, incremental_clone_id=incremental_clone_id\n            )\n            if input_buffer is not None:\n                for k in input_buffer.keys():\n                    if torch.is_tensor(input_buffer[k]) and input_buffer[k].size(1) > 1:\n                        input_buffer[k] = input_buffer[k].index_select(1, new_order)\n                self._set_input_buffer(\n                    incremental_state,\n                    input_buffer,\n                    incremental_clone_id=incremental_clone_id,\n                )\n\n    def _get_input_buffer(self, incremental_state, incremental_clone_id=""""):\n        return (\n            utils.get_incremental_state(\n                self, incremental_state, ""attn_state"" + incremental_clone_id\n            )\n            or {}\n        )\n\n    def _set_input_buffer(self, incremental_state, buffer, incremental_clone_id=""""):\n        self.incremental_clone_ids.add(incremental_clone_id)\n        utils.set_incremental_state(\n            self, incremental_state, ""attn_state"" + incremental_clone_id, buffer\n        )\n\n\nclass AverageAttention(AttentionAbstract):\n    """"""Average attention.\n    See ""Accelerating Neural Transformer via an Average Attention Network""\n    for more details.\n    """"""\n\n    def __init__(self, embed_dim, dropout=0.0, bias=True):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n\n    def forward(\n        self,\n        value,\n        mask_trick=False,\n        mask_future_timesteps=False,\n        incremental_state=None,\n    ):\n        """"""Input shape: Time x Batch x Channel\n       ` mask_trick` is to use matrix multiplication instead of cumulative sum\n        to average the inputs.\n        Future timesteps can be masked with the\n        `mask_future_timesteps` argument. Padding elements can be excluded from\n        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n        batch x src_len, where padding elements are indicated by 1s.\n        """"""\n\n        assert mask_future_timesteps or incremental_state is None\n        if incremental_state is None:\n            return self._forward(value, mask_trick, mask_future_timesteps)\n        else:\n            return self._forward_incremental(\n                value, mask_trick, mask_future_timesteps, incremental_state\n            )\n\n    def _forward(self, value, mask_trick, mask_future_timesteps):\n        length, batch_size = value.size()[:2]\n        if not mask_future_timesteps:\n            attn = value.mean(dim=0, keepdim=True).repeat(length, 1, 1)\n            attn_weights = None\n        elif mask_trick:\n            v = value.transpose(0, 1)\n            attn_weights = torch.arange(1, length + 1, out=v.new(), requires_grad=False)\n            attn_weights = (\n                attn_weights.reciprocal_().unsqueeze_(1).repeat(1, length).tril(0)\n            )\n            attn_weights = attn_weights.unsqueeze_(0).repeat(batch_size, 1, 1)\n            attn_weights = F.dropout(\n                attn_weights, p=self.dropout, training=self.training\n            )\n            attn = torch.bmm(attn_weights, v)\n            attn = attn.transpose(0, 1).contiguous()\n        else:\n            batch_size = value.size()\n            attn_weights = torch.arange(\n                1, length + 1, out=value.new(), requires_grad=False\n            ).view(length, 1, 1)\n            attn = value.cumsum(0) / attn_weights\n            attn_weights = None\n        return attn, attn_weights\n\n    def _forward_incremental(\n        self, value, mask_trick, mask_future_timesteps, incremental_state\n    ):\n        if mask_trick:\n            saved_state = self._get_input_buffer(incremental_state)\n            if ""prev_vec"" in saved_state:\n                value = torch.cat([saved_state[""prev_vec""], value], dim=0)\n            saved_state[""prev_vec""] = value\n            self._set_input_buffer(incremental_state, saved_state)\n            attn_weights = None\n            attn = value.mean(0, keepdim=True)\n        else:\n            saved_state = self._get_input_buffer(incremental_state)\n            if ""prev_sum"" in saved_state:\n                prev_sum = saved_state[""prev_sum""]\n                if len(prev_sum) == 2:\n                    # for tracing, prev_sum does not have sequence axis\n                    prev_sum = prev_sum.unsqueeze(0)\n                pos = saved_state[""prev_pos""] + 1\n                curr_sum = prev_sum + value\n                attn = curr_sum / pos\n            else:\n                curr_sum = value\n                attn = value\n                pos = 1\n            saved_state[""prev_sum""] = curr_sum\n            saved_state[""prev_pos""] = pos\n            self._set_input_buffer(incremental_state, saved_state)\n            attn_weights = None\n        return attn, attn_weights\n\n    def extra_repr(self):\n        return ""embed_dim={}, dropout={}"".format(self.embed_dim, self.dropout)\n\n\nclass AverageWindowAttention(AverageAttention):\n    """"""Average attention with window.\n    See ""Accelerating Neural Transformer via an Average Attention Network""\n    for more details.\n    """"""\n\n    def __init__(self, embed_dim, dropout=0.0, bias=True, window_size=0):\n        super().__init__(embed_dim, dropout, bias)\n        self.window_size = window_size\n\n    def _forward(self, value, mask_trick, mask_future_timesteps):\n        if self.window_size == 1:\n            return value, None\n        length, batch_size = value.size()[:2]\n        if not mask_future_timesteps:\n            raise NotImplementedError()\n        else:\n            v = value.transpose(0, 1)\n            attn_weights = value.new_ones(length, length, requires_grad=False)\n            if self.window_size > 0:\n                attn_weights.tril_(0).triu_(1 - self.window_size)\n            attn_weights.div_(attn_weights.sum(1, keepdim=True))\n            attn_weights = attn_weights.unsqueeze_(0).repeat(batch_size, 1, 1)\n            attn = torch.bmm(attn_weights, v)\n            attn = attn.transpose(0, 1).contiguous()\n\n        return attn, attn_weights\n\n    def _forward_incremental(\n        self, value, mask_trick, mask_future_timesteps, incremental_state\n    ):\n        if self.window_size == 1:\n            return value, None\n        length, batch_size = value.size()[:2]\n        if mask_trick:\n            saved_state = self._get_input_buffer(incremental_state)\n            if ""prev_vec"" in saved_state:\n                value = torch.cat([saved_state[""prev_vec""], value], dim=0)\n            saved_state[""prev_vec""] = value[-self.window_size :]\n            self._set_input_buffer(incremental_state, saved_state)\n            attn_weights = None\n            attn = value.mean(0, keepdim=True)\n        else:\n            saved_state = self._get_input_buffer(incremental_state)\n            if ""prev_sum"" in saved_state:\n                prev_sum = saved_state[""prev_sum""]\n                values = torch.cat([saved_state[""prev_vec""], value], dim=0)\n                curr_sum = prev_sum + value\n                if values.size(0) > self.window_size:\n                    curr_sum -= values[:1]\n                avg_size = min(values.size(0), self.window_size)\n                attn = curr_sum / avg_size\n            else:\n                curr_sum = value\n                values = value\n                attn = value\n            saved_state[""prev_vec""] = values[-self.window_size :]\n            saved_state[""prev_sum""] = curr_sum\n            self._set_input_buffer(incremental_state, saved_state)\n            attn_weights = None\n        return attn, attn_weights\n\n    def extra_repr(self):\n        return ""embed_dim={}, dropout={}, window_size={}"".format(\n            self.embed_dim, self.dropout, self.window_size\n        )\n'"
pytorch_translate/beam_decode.py,53,"b'#!/usr/bin/env python3\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\nfrom fairseq import search, utils\nfrom fairseq.models import FairseqIncrementalDecoder\nfrom pytorch_translate import utils as pytorch_translate_utils\nfrom torch import Tensor\n\n\nclass SequenceGenerator(object):\n    def __init__(\n        self,\n        models,\n        tgt_dict,\n        beam_size=1,\n        minlen=1,\n        maxlen=None,\n        stop_early=True,\n        normalize_scores=True,\n        len_penalty=0,\n        unk_reward=0,\n        lexicon_reward=0,\n        retain_dropout=False,\n        word_reward=0,\n        model_weights=None,\n        use_char_source=False,\n        diverse_beam_groups=-1,\n        diverse_beam_strength=0.5,\n        diversity_sibling_gamma=0.0,\n        sampling=False,\n        sampling_topk=-1,\n        temperature=1,\n    ):\n        """"""Generates translations of a given source sentence.\n        Args:\n            models: List of FairseqEncoderDecoderModel objects. Each one must\n                implement reorder_encoder_output() method to replicate encoder\n                outputs.\n            min/maxlen: The length of the generated output will be bounded by\n                minlen and maxlen (not including the end-of-sentence marker).\n            stop_early: Stop generation immediately after we finalize beam_size\n                hypotheses, even though longer hypotheses might have better\n                normalized scores.\n            normalize_scores: Normalize scores by the length of the output.\n            word_reward: add this value to score each token except EOS\n                (an alternative method to len_penalty for encouraging longer\n                output)\n            model_weights: None or list of Python floats of the same length as\n                `models` with ensemble interpolation weights.\n            use_char_source: if True, encoder inputs consist of (src_tokens,\n                src_lengths, char_inds, word_lengths)\n            diverse_beam_groups: number of groups for Diverse Beam Search\n                (-1 by default is vanilla beam search)\n            diverse_beam_strength: strength of diversity penalty for Diverse\n                Beam Search.\n            diversity_sibling_gamma: The diversity rate of sibling rank (-0.0 by default\n               to disable sibling rank penalty)\n            sampling (bool, optional): sample outputs instead of beam search\n                (default: False)\n            sampling_topk (int, optional): only sample among the top-k choices\n                at each step (default: -1)\n            temperature (float, optional): temperature, where values\n                >1.0 produce more uniform samples and values <1.0 produce\n                sharper samples (default: 1.0)\n        """"""\n        self.models = models\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.beam_size = beam_size\n        self.minlen = minlen\n        max_decoder_len = min(m.max_decoder_positions() for m in self.models)\n        self.maxlen = (\n            max_decoder_len if maxlen is None else min(maxlen, max_decoder_len)\n        )\n        self.stop_early = stop_early\n        self.normalize_scores = normalize_scores\n        self.len_penalty = len_penalty\n        self.unk_reward = unk_reward\n        self.lexicon_reward = lexicon_reward\n        self.lexicon_indices = tgt_dict.lexicon_indices_list()\n        self.retain_dropout = retain_dropout\n        self.temperature = temperature\n        self.word_reward = word_reward\n        if model_weights is not None:\n            assert len(models) == len(model_weights)\n            self.model_weights = model_weights\n        else:\n            self.model_weights = [1.0 / len(models)] * len(models)\n        self.use_char_source = use_char_source\n        assert sampling_topk < 0 or sampling, ""--sampling-topk requires --sampling""\n        assert temperature > 0, ""--temperature must be greater than 0""\n        if sampling:\n            self.search = search.Sampling(tgt_dict, sampling_topk)\n        elif diverse_beam_groups > 0:\n            self.search = search.DiverseBeamSearch(\n                tgt_dict, diverse_beam_groups, diverse_beam_strength\n            )\n        else:\n            self.search = search.BeamSearch(tgt_dict)\n        self.diversity_sibling_gamma = diversity_sibling_gamma\n\n    def cuda(self):\n        for model in self.models:\n            model.cuda()\n        return self\n\n    def generate_batched_itr(\n        self,\n        data_itr,\n        beam_size=None,\n        maxlen_a=0.0,\n        maxlen_b=None,\n        cuda=False,\n        timer=None,\n        prefix_size=0,\n    ):\n        """"""Iterate over a batched dataset and yield individual translations.\n\n        Args:\n            maxlen_a/b: generate sequences of maximum length ax + b,\n                where x is the source sentence length.\n            cuda: use GPU for generation\n            timer: StopwatchMeter for timing generations.\n        """"""\n        if maxlen_b is None:\n            maxlen_b = self.maxlen\n\n        for sample in data_itr:\n            if ""net_input"" not in sample:\n                continue\n\n            if cuda:\n                s = utils.move_to_cuda(sample)\n            else:\n                s = sample\n            input = s[""net_input""]\n            srclen = input[""src_tokens""].size(1)\n            if self.use_char_source:\n                encoder_input = {\n                    k: v\n                    for k, v in input.items()\n                    if k in [""src_tokens"", ""src_lengths"", ""char_inds"", ""word_lengths""]\n                }\n            else:\n                encoder_input = {\n                    k: v for k, v in input.items() if k in [""src_tokens"", ""src_lengths""]\n                }\n            if timer is not None:\n                timer.start()\n            with torch.no_grad():\n                hypos = self.generate(\n                    encoder_input=encoder_input,\n                    beam_size=beam_size,\n                    maxlen=int(maxlen_a * srclen + maxlen_b),\n                    prefix_tokens=s[""target""][:, :prefix_size]\n                    if prefix_size > 0\n                    else None,\n                )\n            if timer is not None:\n                timer.stop(s[""ntokens""])\n            for i, id in enumerate(s[""id""]):\n                # remove padding\n                src = utils.strip_pad(input[""src_tokens""][i, :], self.pad)\n                ref = utils.strip_pad(s[""target""][i, :], self.pad)\n                yield id, src, ref, hypos[i]\n\n    @torch.no_grad()\n    def generate(self, encoder_input, beam_size=None, maxlen=None, prefix_tokens=None):\n        encoder_inputs = self.prepare_encoder_inputs(encoder_input)\n        encoder_outs, incremental_states = self._encode(encoder_input=encoder_inputs)\n        return self._decode_target(\n            encoder_input,\n            encoder_outs,\n            incremental_states,\n            self.diversity_sibling_gamma,\n            beam_size,\n            maxlen,\n            prefix_tokens,\n        )\n\n    def prepare_encoder_inputs(self, encoder_input):\n        if self.use_char_source:\n            encoder_inputs = (\n                encoder_input[""src_tokens""],\n                encoder_input[""src_lengths""],\n                encoder_input[""char_inds""],\n                encoder_input[""word_lengths""],\n            )\n        else:\n            encoder_inputs = (encoder_input[""src_tokens""], encoder_input[""src_lengths""])\n        return encoder_inputs\n\n    def _build_constraints(self, src_tokens, beam_size):\n        """"""\n        Stub functions for adding application specific constraint checks on\n        the candidates being generated during beam search. This and the below\n        stub functions can be implemented in a child class without needing to\n        touch the actual beam search code\n        """"""\n        pass\n\n    def _apply_constraint_penalty(self, scores):\n        pass\n\n    def _update_constraints(self, constraints, next_tokens, idx):\n        pass\n\n    def _reorder_constraints(self, constraints, new_indices):\n        pass\n\n    def _apply_eos_constraints(self, constraints, eos_bbsz_idx, eos_scores):\n        pass\n\n    def _finalize_constrained_results(self, finalized, device):\n        pass\n\n    def _decode_target(\n        self,\n        encoder_input,\n        encoder_outs,\n        incremental_states,\n        diversity_sibling_gamma=0.0,\n        beam_size=None,\n        maxlen=None,\n        prefix_tokens=None,\n    ):\n        src_tokens_tensor = pytorch_translate_utils.get_source_tokens_tensor(\n            encoder_input[""src_tokens""]\n        )\n        beam_size = beam_size if beam_size is not None else self.beam_size\n        bsz = src_tokens_tensor.size(0)\n        reorder_indices = (\n            torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1).long()\n        )\n        for i, model in enumerate(self.models):\n            encoder_outs[i] = model.encoder.reorder_encoder_out(\n                encoder_out=encoder_outs[i],\n                new_order=reorder_indices.type_as(src_tokens_tensor),\n            )\n        maxlen = min(maxlen, self.maxlen) if maxlen is not None else self.maxlen\n        # initialize buffers\n        scores = src_tokens_tensor.new(bsz * beam_size, maxlen + 1).float().fill_(0)\n        scores_buf = scores.clone()\n        tokens = src_tokens_tensor.new(bsz * beam_size, maxlen + 2).fill_(self.pad)\n        tokens_buf = tokens.clone()\n        tokens[:, 0] = self.eos\n\n        # may differ from input length\n        if isinstance(encoder_outs[0], (list, tuple)):\n            src_encoding_len = encoder_outs[0][0].size(0)\n        elif isinstance(encoder_outs[0], dict):\n            if isinstance(encoder_outs[0][""encoder_out""], tuple):\n                # Fairseq compatibility\n                src_encoding_len = encoder_outs[0][""encoder_out""][0].size(1)\n            else:\n                src_encoding_len = encoder_outs[0][""encoder_out""].size(0)\n\n        attn = scores.new(bsz * beam_size, src_encoding_len, maxlen + 2)\n        attn_buf = attn.clone()\n\n        # list of completed sentences\n        finalized = [[] for i in range(bsz)]\n        finished = [False for i in range(bsz)]\n        worst_finalized = [{""idx"": None, ""score"": -math.inf} for i in range(bsz)]\n        num_remaining_sent = bsz\n\n        # number of candidate hypos per step\n        cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n\n        # offset arrays for converting between different indexing schemes\n        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n        cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n\n        # helper function for allocating buffers on the fly\n        buffers = {}\n\n        # init constraints\n        constraints = self._build_constraints(src_tokens_tensor, beam_size)\n\n        def buffer(name, type_of=tokens):  # noqa\n            if name not in buffers:\n                buffers[name] = type_of.new()\n            return buffers[name]\n\n        def is_finished(sent, step, unfinalized_scores=None):\n            """"""\n            Check whether we\'ve finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            """"""\n            assert len(finalized[sent]) <= beam_size\n            if len(finalized[sent]) == beam_size:\n                if self.stop_early or step == maxlen or unfinalized_scores is None:\n                    return True\n                # stop if the best unfinalized score is worse than the worst\n                # finalized one\n                best_unfinalized_score = unfinalized_scores[sent].max()\n                if self.normalize_scores:\n                    best_unfinalized_score /= (maxlen + 1) ** self.len_penalty\n                if worst_finalized[sent][""score""] >= best_unfinalized_score:\n                    return True\n            return False\n\n        def finalize_hypos(step, bbsz_idx, eos_scores, unfinalized_scores=None):\n            """"""\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    scores for each hypothesis\n                unfinalized_scores: A vector containing scores for all\n                    unfinalized hypotheses\n            """"""\n            assert bbsz_idx.numel() == eos_scores.numel()\n\n            # clone relevant token and attention tensors\n            tokens_clone = tokens.index_select(0, bbsz_idx)\n            tokens_clone = tokens_clone[\n                :, 1 : step + 2\n            ]  # skip the first index, which is EOS\n            tokens_clone[:, step] = self.eos\n            attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1 : step + 2]\n\n            # compute scores per token position\n            pos_scores = scores.index_select(0, bbsz_idx)[:, : step + 1]\n            pos_scores[:, step] = eos_scores\n            # convert from cumulative to per-position scores\n            pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n\n            # normalize sentence-level scores\n            if self.normalize_scores:\n                eos_scores /= (step + 1) ** self.len_penalty\n\n            sents_seen = set()\n            for i, (idx, score) in enumerate(\n                zip(bbsz_idx.tolist(), eos_scores.tolist())\n            ):\n                sent = idx // beam_size\n                sents_seen.add(sent)\n\n                def get_hypo():\n                    _, alignment = attn_clone[i].max(dim=0)\n                    return {\n                        ""tokens"": tokens_clone[i],\n                        ""score"": score,\n                        ""attention"": attn_clone[i],  # src_len x tgt_len\n                        ""alignment"": alignment,\n                        ""positional_scores"": pos_scores[i],\n                    }\n\n                if len(finalized[sent]) < beam_size:\n                    finalized[sent].append(get_hypo())\n                elif not self.stop_early and score > worst_finalized[sent][""score""]:\n                    # replace worst hypo for this sentence with new/better one\n                    worst_idx = worst_finalized[sent][""idx""]\n                    if worst_idx is not None:\n                        finalized[sent][worst_idx] = get_hypo()\n\n                    # find new worst finalized hypo for this sentence\n                    idx, s = min(\n                        enumerate(finalized[sent]), key=lambda r: r[1][""score""]\n                    )\n                    worst_finalized[sent] = {""score"": s[""score""], ""idx"": idx}\n\n            # return number of hypotheses finished this step\n            num_finished = 0\n            for sent in sents_seen:\n                # check termination conditions for this sentence\n                if not finished[sent] and is_finished(sent, step, unfinalized_scores):\n                    finished[sent] = True\n                    num_finished += 1\n            return num_finished\n\n        reorder_state = None\n        possible_translation_tokens = None\n        for step in range(maxlen + 1):  # one extra step for EOS marker\n            # reorder decoder internal states based on the prev choice of beams\n            if reorder_state is not None:\n                for model in self.models:\n                    if isinstance(model.decoder, FairseqIncrementalDecoder):\n                        model.decoder.reorder_incremental_state(\n                            incremental_states[model], reorder_state\n                        )\n            # Run decoder for one step\n            logprobs, avg_attn, possible_translation_tokens = self._decode(\n                tokens[:, : step + 1],\n                encoder_outs,\n                incremental_states,\n                possible_translation_tokens,\n            )\n\n            logprobs[:, self.pad] = -math.inf  # never select pad\n            # apply unk reward\n            if possible_translation_tokens is None:\n                # No vocab reduction, so unk is represented by self.unk at\n                # position self.unk\n                unk_index = self.unk\n                logprobs[:, unk_index] += self.unk_reward\n            else:\n                # When we use vocab reduction, the token value self.unk may not\n                # be at the position self.unk, but somewhere else in the list\n                # of possible_translation_tokens. It\'s also possible not to\n                # show up in possible_translation_tokens at all, meaning we\n                # can\'t generate an unk.\n                unk_pos = torch.nonzero(possible_translation_tokens == self.unk)\n                if unk_pos.size()[0] != 0:\n                    # only add unk_reward if unk index appears in\n                    # possible_translation_tokens\n                    unk_index = unk_pos[0][0]\n                    logprobs[:, unk_index] += self.unk_reward\n            # external lexicon reward\n            logprobs[:, self.lexicon_indices] += self.lexicon_reward\n\n            logprobs += self.word_reward\n            logprobs[:, self.eos] -= self.word_reward\n            # Record attention scores\n            if avg_attn is not None:\n                attn[:, :, step + 1].copy_(avg_attn)\n\n            cand_scores = buffer(""cand_scores"", type_of=scores)\n            cand_indices = buffer(""cand_indices"")\n            cand_beams = buffer(""cand_beams"")\n            eos_bbsz_idx = buffer(""eos_bbsz_idx"")\n            eos_scores = buffer(""eos_scores"", type_of=scores)\n            scores = scores.type_as(logprobs)\n            scores_buf = scores_buf.type_as(logprobs)\n\n            if step < maxlen:\n                self._apply_constraint_penalty(scores)  # stub call\n                if prefix_tokens is not None and step < prefix_tokens.size(1):\n                    logprobs_slice = logprobs.view(bsz, -1, logprobs.size(-1))[:, 0, :]\n                    cand_scores = torch.gather(\n                        logprobs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1)\n                    ).expand(-1, cand_size)\n                    cand_indices = (\n                        prefix_tokens[:, step].view(-1, 1).expand(bsz, cand_size)\n                    )\n                    cand_beams.resize_as_(cand_indices).fill_(0)\n                else:\n                    possible_tokens_size = self.vocab_size\n                    if possible_translation_tokens is not None:\n                        possible_tokens_size = possible_translation_tokens.size(0)\n                    if diversity_sibling_gamma > 0:\n                        logprobs = self.diversity_sibling_rank(\n                            logprobs.view(bsz, -1, possible_tokens_size),\n                            diversity_sibling_gamma,\n                        )\n                    cand_scores, cand_indices, cand_beams = self.search.step(\n                        step,\n                        logprobs.view(bsz, -1, possible_tokens_size),\n                        scores.view(bsz, beam_size, -1)[:, :, :step],\n                    )\n                    # vocabulary reduction\n                    if possible_translation_tokens is not None:\n                        possible_translation_tokens = possible_translation_tokens.view(\n                            1, possible_tokens_size\n                        ).expand(cand_indices.size(0), possible_tokens_size)\n                        cand_indices = torch.gather(\n                            possible_translation_tokens,\n                            dim=1,\n                            index=cand_indices,\n                            out=cand_indices,\n                        )\n            else:\n                # finalize all active hypotheses once we hit maxlen\n                # pick the hypothesis with the highest log prob of EOS right now\n                logprobs.add_(scores[:, step - 1].view(-1, 1))\n                torch.sort(\n                    logprobs[:, self.eos],\n                    descending=True,\n                    out=(eos_scores, eos_bbsz_idx),\n                )\n                num_remaining_sent -= finalize_hypos(step, eos_bbsz_idx, eos_scores)\n                assert num_remaining_sent == 0\n                break\n\n            # cand_bbsz_idx contains beam indices for the top candidate\n            # hypotheses, with a range of values: [0, bsz*beam_size),\n            # and dimensions: [bsz, cand_size]\n            cand_bbsz_idx = cand_beams.add_(bbsz_offsets)\n\n            # finalize hypotheses that end in eos\n            eos_mask = cand_indices.eq(self.eos)\n            if step >= self.minlen:\n                # only consider eos when it\'s among the top beam_size indices\n                torch.masked_select(\n                    cand_bbsz_idx[:, :beam_size],\n                    mask=eos_mask[:, :beam_size],\n                    out=eos_bbsz_idx,\n                )\n                if eos_bbsz_idx.numel() > 0:\n                    torch.masked_select(\n                        cand_scores[:, :beam_size],\n                        mask=eos_mask[:, :beam_size],\n                        out=eos_scores,\n                    )\n                    self._apply_eos_constraints(constraints, eos_bbsz_idx, eos_scores)\n                    num_remaining_sent -= finalize_hypos(\n                        step, eos_bbsz_idx, eos_scores, cand_scores\n                    )\n\n            assert num_remaining_sent >= 0\n            if num_remaining_sent == 0:\n                break\n            assert step < maxlen\n\n            # set active_mask so that values > cand_size indicate eos hypos\n            # and values < cand_size indicate candidate active hypos.\n            # After, the min values per row are the top candidate active hypos\n            active_mask = buffer(""active_mask"")\n            torch.add(\n                eos_mask.type_as(cand_offsets) * cand_size,\n                cand_offsets[: eos_mask.size(1)],\n                out=active_mask,\n            )\n\n            # get the top beam_size active hypotheses, which are just the hypos\n            # with the smallest values in active_mask\n            active_hypos, _ignore = buffer(""active_hypos""), buffer(""_ignore"")\n            torch.topk(\n                active_mask,\n                k=beam_size,\n                dim=1,\n                largest=False,\n                out=(_ignore, active_hypos),\n            )\n            active_bbsz_idx = buffer(""active_bbsz_idx"")\n            torch.gather(cand_bbsz_idx, dim=1, index=active_hypos, out=active_bbsz_idx)\n            active_scores = torch.gather(\n                cand_scores,\n                dim=1,\n                index=active_hypos,\n                out=scores[:, step].view(bsz, beam_size),\n            )\n            active_bbsz_idx = active_bbsz_idx.view(-1)\n            active_scores = active_scores.view(-1)\n\n            # copy tokens and scores for active hypotheses\n            torch.index_select(\n                tokens[:, : step + 1],\n                dim=0,\n                index=active_bbsz_idx,\n                out=tokens_buf[:, : step + 1],\n            )\n            torch.gather(\n                cand_indices,\n                dim=1,\n                index=active_hypos,\n                out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1],\n            )\n            # update constraints for next step\n            constraints = self._reorder_constraints(constraints, active_bbsz_idx)\n            self._update_constraints(constraints, tokens_buf[:, step + 1], step)\n            if step > 0:\n                torch.index_select(\n                    scores[:, :step],\n                    dim=0,\n                    index=active_bbsz_idx,\n                    out=scores_buf[:, :step],\n                )\n            torch.gather(\n                cand_scores,\n                dim=1,\n                index=active_hypos,\n                out=scores_buf.view(bsz, beam_size, -1)[:, :, step],\n            )\n\n            # copy attention for active hypotheses\n            torch.index_select(\n                attn[:, :, : step + 2],\n                dim=0,\n                index=active_bbsz_idx,\n                out=attn_buf[:, :, : step + 2],\n            )\n\n            # swap buffers\n            tokens, tokens_buf = tokens_buf, tokens\n            scores, scores_buf = scores_buf, scores\n            attn, attn_buf = attn_buf, attn\n\n            # reorder incremental state in decoder\n            reorder_state = active_bbsz_idx\n\n        # sort by score descending\n        for sent in range(bsz):\n            finalized[sent] = sorted(\n                finalized[sent], key=lambda r: r[""score""], reverse=True\n            )\n        self._finalize_constrained_results(finalized, scores.device)\n        return finalized\n\n    def _encode(self, encoder_input):\n        encoder_outs = []\n        incremental_states = {}\n        for model in self.models:\n            if not self.retain_dropout:\n                model.eval()\n            if isinstance(model.decoder, FairseqIncrementalDecoder):\n                incremental_states[model] = {}\n            else:\n                incremental_states[model] = None\n            encoder_out = model.encoder(*encoder_input)\n            encoder_outs.append(encoder_out)\n        return encoder_outs, incremental_states\n\n    @staticmethod\n    def gather_probs(all_translation_tokens, all_probs):\n        """"""\n        Maps probabilities for multiple models with different output softmax\n        dimensions to the same combined token space. This is a simplified\n        example, normally probs would be in log space and would be size\n        [bsz, len(possible_translation_tokens)]\n\n        Model 1:\n        possible_translation_tokens: [3, 7, 8, 9]\n        probs: [0.25, 0.25, 0.25, 0.25]\n\n        Model 2:\n        possible_translation_tokens: [0, 3, 5]\n        probs: [0.4, 0.5, 0.1]\n\n        all_translation_tokens: [[3, 7, 8, 9], [0, 3, 5]]\n        all_probs: [[0.25, 0.25, 0.25, 0.25], [0.4, 0.5, 0.1]]\n        possible_translation_tokens = [0, 3, 5, 7, 8, 9] (order varies)\n        mapped_probs for model 1: [0  , 0.25, 0  , 0.25, 0.25, 0.25]\n        mapped_probs for model 2: [0.4, 0.5 , 0.1, 0   , 0   , 0]\n\n        avg_probs = [0.4, 0.75, 0.1, 0.25, 0.25, 0.25] (order varies but\n        corresponds to possible_translation_tokens)\n\n        Inputs:\n            all_translation_tokens: List[Optional[possible_translation_tokens]]\n                where possible_translation_tokens is a flat Tensor representing\n                the possible translation tokens from model output. Note that the\n                possible_translation_tokens will be None only if vocab reduction\n                was not used.\n            all_probs: List[probs] where probs is a flat Tensor of normalized\n                probs for each model output. If vocab reduction was not used,\n                each probs list will be of length vocab size. Otherwise, each\n                probs will be the same length as that model\'s\n                possible_translation_tokens\n\n        Returns:\n            avg_probs: average probabilities of tokens from a merged list of\n                possible_translation_tokens from every model.\n            possible_translation_tokens: merged list of\n                possible_translation_tokens from every model.\n        """"""\n\n        assert len(all_translation_tokens) == len(all_probs), (\n            f""Number of possible_translation_tokens tensors in ""\n            f""all_translation_tokens list -- got length ""\n            f""{len(all_translation_tokens)} -- should match the number of ""\n            f""probs tensors in all_probs list -- got length {len(all_probs)}.\\n""\n            f""all_translation_tokens: {all_translation_tokens}\\n""\n            f""all_probs: {all_probs}""\n        )\n        possible_translation_tokens = None\n        inv_indices_per_model = [None] * len(all_translation_tokens)\n        if all_translation_tokens[0] is not None:\n            # Get unique translation tokens out of all the\n            # possible_translation_tokens for every model.\n            # inverse indices for the example above: [5, 4, 2, 1, 3, 5, 0]\n            possible_translation_tokens, inverse_indices = torch.unique(\n                torch.cat(all_translation_tokens, dim=0),\n                sorted=False,\n                return_inverse=True,\n            )\n            # softmax_sizes for the example above: [4, 3]\n            softmax_sizes = [\n                translation_tokens.size(0)\n                for translation_tokens in all_translation_tokens\n            ]\n            inv_indices_per_model = torch.split(\n                inverse_indices, split_size_or_sections=softmax_sizes\n            )\n\n        avg_probs = None\n        for inv_ind, probs in zip(inv_indices_per_model, all_probs):\n            mapped_probs = probs\n            if possible_translation_tokens is not None:\n                # The corresponding model did not use vocab reduction if\n                # possible_translation_tokens is None.\n                mapped_probs = torch.zeros(\n                    (probs.size(0), possible_translation_tokens.size(0)),\n                    device=probs.device,\n                )\n\n                mapped_probs[:, inv_ind] = probs\n            if avg_probs is None:\n                avg_probs = mapped_probs\n            else:\n                avg_probs.add_(mapped_probs)\n        return avg_probs, possible_translation_tokens\n\n    def _decode(\n        self, tokens, encoder_outs, incremental_states, possible_translation_tokens=None\n    ):\n        avg_attn = None\n        all_translation_tokens = []\n        all_log_probs = []\n        for model_weight, model, encoder_out in zip(\n            self.model_weights, self.models, encoder_outs\n        ):\n            with torch.no_grad():\n                if (\n                    possible_translation_tokens is not None\n                    and len(possible_translation_tokens.shape) > 1\n                ):\n                    # reverse beam replication\n                    possible_translation_tokens = possible_translation_tokens[0]\n\n                decoder_out = list(\n                    model.decoder(\n                        tokens,\n                        encoder_out,\n                        incremental_states[model],\n                        possible_translation_tokens=possible_translation_tokens,\n                    )\n                )\n                decoder_out[0] = decoder_out[0][:, -1, :]\n                if self.temperature != 1.0:\n                    decoder_out[0].div_(self.temperature)\n                attn = decoder_out[1]\n                if len(decoder_out) == 3:\n                    possible_translation_tokens = decoder_out[2]\n                else:\n                    possible_translation_tokens = None\n                if (\n                    hasattr(model.decoder, ""adaptive_softmax"")\n                    and model.decoder.adaptive_softmax is not None\n                ):\n                    decoder_out[0] = decoder_out[0].unsqueeze(1)\n                    # to use get_normalized_probs in adaptive softmax decoder\n                    # the sample object is needed. During inference, the target\n                    # should be set to None\n                    log_probs = model.get_normalized_probs(\n                        decoder_out, log_probs=True, sample={""target"": None}\n                    )\n                    log_probs = model_weight * log_probs[:, -1, :]\n                else:\n                    log_probs = model.get_normalized_probs(decoder_out, log_probs=True)\n                    log_probs = model_weight * log_probs\n                all_translation_tokens.append(possible_translation_tokens)\n                all_log_probs.append(log_probs)\n\n            if attn is not None:\n                attn = attn[:, -1, :].data\n                if avg_attn is None:\n                    avg_attn = attn\n                else:\n                    avg_attn.add_(attn)\n\n        avg_log_probs, possible_translation_tokens = SequenceGenerator.gather_probs(\n            all_translation_tokens=all_translation_tokens, all_probs=all_log_probs\n        )\n        if avg_attn is not None:\n            avg_attn.div_(len(self.models))\n\n        return avg_log_probs, avg_attn, possible_translation_tokens\n\n    def diversity_sibling_rank(self, logprobs, gamma):\n        """"""\n        See ""A Simple, Fast Diverse Decoding Algorithm for Neural Generation""\n        for details\n        """"""\n        _, beam_size, vocab_size = logprobs.size()\n        logprobs = logprobs.view(-1, vocab_size)\n        # Keep consistent with beamsearch class in fairseq\n        k = min(2 * beam_size, vocab_size)\n        _, indices = torch.topk(logprobs, k)\n        # Set diverse penalty as k for all words\n        diverse_penalty = torch.ones_like(logprobs) * k\n        diversity_sibling_rank = (\n            torch.arange(0, k).view(-1, 1).expand(k, logprobs.size(0)).type_as(logprobs)\n        )\n        # Set diversity penalty accordingly for top-k words\n        diverse_penalty[\n            torch.arange(0, logprobs.size(0)).long(), indices.transpose(0, 1)\n        ] = diversity_sibling_rank\n        logprobs -= gamma * diverse_penalty\n        return logprobs\n\n\nclass BeamDecode(torch.jit.ScriptModule):\n    """"""\n    Decodes the output of Beam Search to get the top hypotheses\n    """"""\n\n    def __init__(self, eos_token_id, length_penalty, nbest, beam_size, stop_at_eos):\n        super().__init__()\n        self.eos_token_id = torch.jit.Attribute(eos_token_id, int)\n        self.length_penalty = torch.jit.Attribute(length_penalty, float)\n        self.nbest = torch.jit.Attribute(nbest, int)\n        self.beam_size = torch.jit.Attribute(beam_size, int)\n        self.stop_at_eos = torch.jit.Attribute(int(stop_at_eos), int)\n\n    @torch.jit.script_method\n    @torch.no_grad()\n    def forward(\n        self,\n        beam_tokens: Tensor,\n        beam_scores: Tensor,\n        token_weights: Tensor,\n        beam_prev_indices: Tensor,\n        num_steps: int,\n    ) -> List[Tuple[Tensor, float, List[float], Tensor, Tensor]]:\n\n        self._check_dimensions(\n            beam_tokens, beam_scores, token_weights, beam_prev_indices, num_steps\n        )\n\n        end_states = self._get_all_end_states(\n            beam_tokens, beam_scores, beam_prev_indices, num_steps\n        )\n\n        # outputs is list of the following for each hypothesis:\n        # Tuple[Hypothesis, Hypothesis score, Token level scores, Attention Weights, Best indices]\n        outputs = torch.jit.annotate(\n            List[Tuple[Tensor, float, List[float], Tensor, Tensor]], []\n        )\n\n        for state_idx in range(len(end_states)):\n            state = end_states[state_idx]\n            hypothesis_score = float(state[0])\n            beam_indices = self._get_output_steps_to_beam_indices(\n                state, beam_prev_indices\n            )\n            beam_output = torch.jit.annotate(List[Tensor], [])\n            token_level_scores = torch.jit.annotate(List[float], [])\n            position = int(state[1])\n            hyp_index = int(state[2])\n\n            # best_indices represents the ending position of one hypothesis,\n            # the first index corresponds num_step, the second corresponds beam_index\n            best_indices = torch.tensor([position, hyp_index])\n            back_alignment_weights = []\n\n            assert position + 1 == len(beam_indices)\n            pos = 1\n            prev_beam_index = -1\n            while pos < len(beam_indices):\n                beam_index = beam_indices[pos]\n                beam_output.append(beam_tokens[pos][beam_index])\n                if pos == 1:\n                    # beam_scores[0][:] are all 0s\n                    token_level_scores.append(float(beam_scores[pos][beam_index]))\n                else:\n                    token_level_scores.append(\n                        float(beam_scores[pos][beam_index])\n                        - float(beam_scores[pos - 1][prev_beam_index])\n                    )\n                back_alignment_weights.append(token_weights[pos][beam_index].detach())\n                prev_beam_index = beam_index\n                pos += 1\n            outputs.append(\n                (\n                    torch.stack(beam_output),\n                    hypothesis_score,\n                    token_level_scores,\n                    torch.stack(back_alignment_weights, dim=1),\n                    best_indices,\n                )\n            )\n\n        return outputs\n\n    @torch.jit.script_method\n    def _get_output_steps_to_beam_indices(\n        self, end_state: Tensor, beam_prev_indices: Tensor\n    ) -> List[int]:\n        """"""\n        Returns a mapping from each output position and the beam index that was\n        picked from the beam search results.\n        """"""\n        present_position = int(end_state[1])\n        beam_index = int(end_state[2])\n        beam_indices = torch.jit.annotate(List[int], [])\n        while present_position >= 0:\n            beam_indices.insert(0, beam_index)\n            beam_index = int(beam_prev_indices[present_position][beam_index])\n            present_position = present_position - 1\n        return beam_indices\n\n    @torch.jit.script_method\n    def _add_to_end_states(\n        self, end_states: List[Tensor], min_score: float, state: Tensor, min_index: int\n    ) -> Tuple[List[Tensor], float, int]:\n        """"""\n        Maintains a list of atmost `nbest` highest end states\n        """"""\n        if len(end_states) < self.nbest:\n            end_states.append(state)\n            # keep min_score and min_index updated\n            if float(state[0]) <= min_score:\n                min_score = float(state[0])\n                min_index = len(end_states) - 1\n        elif bool(state[0] > min_score):\n            # replace worst hypo with the new one\n            end_states[min_index] = state\n            # find new worst hypo, keep min_score and min_index updated\n            min_index = -1\n            min_score = float(""inf"")\n            for idx in range(len(end_states)):\n                s = end_states[idx]\n                if bool(float(s[0]) <= min_score):\n                    min_index = idx\n                    min_score = float(s[0])\n        return end_states, min_score, min_index\n\n    @torch.jit.script_method\n    def _get_all_end_states(\n        self,\n        beam_tokens: Tensor,\n        beam_scores: Tensor,\n        beam_prev_indices: Tensor,\n        num_steps: int,\n    ) -> Tensor:\n        """"""\n        Return all end states and hypothesis scores for those end states.\n        """"""\n        min_score = float(""inf"")\n        min_index = -1\n        end_states = torch.jit.annotate(List[Tensor], [])\n        prev_hypo_is_finished = torch.zeros(self.beam_size).byte()\n\n        position = 1\n        while bool(position <= num_steps):\n            hypo_is_finished = torch.zeros(self.beam_size).byte()\n\n            for hyp_index in range(self.beam_size):\n                prev_pos = beam_prev_indices[position][hyp_index]\n                hypo_is_finished[hyp_index] = prev_hypo_is_finished[prev_pos]\n\n                # If hypothesis was completed in the previous index,\n                # then just continue\n                if bool(hypo_is_finished[hyp_index] == 0):\n                    # If the present token is EOS or we have reached max_length\n                    # then hypothesis is complete\n                    if bool(\n                        beam_tokens[position][hyp_index] == self.eos_token_id\n                    ) or bool(position == num_steps):\n\n                        if bool(self.stop_at_eos):\n                            hypo_is_finished[hyp_index] = 1\n\n                        hypo_score = float(beam_scores[position][hyp_index])\n                        if bool(self.length_penalty != 0):\n                            hypo_score = hypo_score / float(position) ** float(\n                                self.length_penalty\n                            )\n\n                        end_states, min_score, min_index = self._add_to_end_states(\n                            end_states,\n                            min_score,\n                            torch.tensor(\n                                [hypo_score, float(position), float(hyp_index)]\n                            ),\n                            min_index,\n                        )\n\n            prev_hypo_is_finished = hypo_is_finished\n            position = position + 1\n\n        end_states = torch.stack(end_states)\n\n        _, sorted_end_state_indices = end_states[:, 0].sort(dim=0, descending=True)\n        end_states = end_states[sorted_end_state_indices, :]\n        return end_states\n\n    @torch.jit.script_method\n    def _check_dimensions(\n        self,\n        beam_tokens: Tensor,\n        beam_scores: Tensor,\n        token_weights: Tensor,\n        beam_prev_indices: Tensor,\n        num_steps: int,\n    ) -> None:\n\n        assert (\n            beam_tokens.size(1) == self.beam_size\n        ), ""Dimension of beam_tokens : {} and beam size : {} are not consistent"".format(\n            beam_tokens.size(), self.beam_size\n        )\n        assert beam_scores.size(1) == self.beam_size, (\n            ""Dimension of beam_scores : {} and beam size : {} ""\n            ""are not consistent"".format(beam_scores.size(), self.beam_size)\n        )\n        assert token_weights.size(1) == self.beam_size, (\n            ""Dimension of token_weights : {} and beam size : {} ""\n            ""are not consistent"".format(token_weights.size(), self.beam_size)\n        )\n        assert (\n            beam_prev_indices.size(1) == self.beam_size\n        ), ""Dimension of beam_prev_indices : {} and beam size : {} ""\n        ""are not consistent"".format(beam_prev_indices.size(), self.beam_size)\n\n        assert beam_tokens.size(0) <= num_steps + 1, (\n            ""Dimension of beam_tokens : {} and num_steps : {} ""\n            ""are not consistent"".format(beam_tokens.size(), num_steps)\n        )\n        assert beam_scores.size(0) <= num_steps + 1, (\n            ""Dimension of beam_scores : {} and num_steps : {} ""\n            ""are not consistent"".format(beam_scores.size(), num_steps)\n        )\n        assert token_weights.size(0) <= num_steps + 1, (\n            ""Dimension of token_weights : {} and num_steps : {} ""\n            ""are not consistent"".format(token_weights.size(), num_steps)\n        )\n        assert beam_prev_indices.size(0) <= num_steps + 1, (\n            ""Dimension of beam_prev_indices : {} and num_steps : {} ""\n            ""are not consistent"".format(beam_prev_indices.size(), num_steps)\n        )\n'"
pytorch_translate/beam_search_and_decode_v2.py,64,"b'#!/usr/bin/env python3\n\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.jit\nimport torch.jit.quantized\nfrom pytorch_translate.beam_decode import BeamDecode\nfrom pytorch_translate.ensemble_export import (\n    DecoderBatchedStepEnsemble,\n    EncoderEnsemble,\n    FakeCharSourceEncoderEnsemble,\n    load_models_from_checkpoints,\n)\nfrom torch import Tensor\n\n\nclass DecoderBatchedStepEnsemble2BeamWithEOS(DecoderBatchedStepEnsemble):\n    """"""\n    This class inherits DecoderBatchedStepEnsemble class. While keeping the basic\n    functionality of running decoding ensemble, two new features are added:\n    expanding double beam size at each search step in case half are eos, appending\n    extra EOS tokens at the end.\n    """"""\n\n    # TODO:this class will be merged with upstream after BeamSearchAndDecodeV1 shipped.\n    def forward(\n        self,\n        input_tokens,\n        prev_scores,\n        active_hypos,\n        timestep,\n        final_step,\n        *inputs,\n        src_tuple=None,\n    ):\n        # input_tokens size: 2 * beam_size -> beam_size,\n        # since we only need half of them which are active.\n        input_tokens = input_tokens.index_select(dim=0, index=active_hypos).unsqueeze(1)\n        prev_scores = prev_scores.index_select(dim=0, index=active_hypos)\n\n        eos_token = torch.LongTensor([self.tgt_dict.eos()])\n\n        (\n            log_probs_per_model,\n            attn_weights_per_model,\n            state_outputs,\n            beam_axis_per_state,\n            possible_translation_tokens,\n        ) = self._get_decoder_outputs(\n            input_tokens, prev_scores, timestep, *inputs, src_tuple=src_tuple\n        )\n\n        average_log_probs = torch.mean(\n            torch.cat(log_probs_per_model, dim=1), dim=1, keepdim=True\n        )\n\n        if possible_translation_tokens is None:\n            word_rewards = self.word_rewards\n        else:\n            word_rewards = self.word_rewards.index_select(\n                0, possible_translation_tokens\n            )\n        word_rewards = word_rewards.unsqueeze(dim=0).unsqueeze(dim=0)\n\n        average_log_probs_with_rewards = average_log_probs + word_rewards\n\n        average_attn_weights = torch.mean(\n            torch.cat(attn_weights_per_model, dim=1), dim=1, keepdim=True\n        )\n\n        # need control-flow to see if it\'s final_step, thus written in script.\n        @torch.jit.script\n        def generate_outputs(\n            final_step: Tensor,\n            average_log_probs_with_rewards: Tensor,\n            average_attn_weights: Tensor,\n            prev_scores: Tensor,\n            eos_token: Tensor,\n            beam_size: int,\n        ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n            # expand 2 * beam_size in case half of them are eos tokens\n            double_beam_size = 2 * beam_size\n\n            if bool(final_step):\n\n                # at final step, we just select eos token and its corresponding score\n                # as best_tokens and eos_scores.\n                cand_tokens = eos_token.repeat(double_beam_size)\n                # eos_scores size: (beam_size, 1, 1)\n                eos_scores = average_log_probs_with_rewards.index_select(\n                    dim=2, index=eos_token\n                )\n                eos_scores_flat = eos_scores.view(-1)\n                cand_scores = prev_scores.view(-1) + eos_scores_flat\n                # cand_scores size: beam_size -> 2 * beam_size\n                cand_scores = cand_scores.repeat(2)\n                cand_prev_hypos = torch.arange(0, double_beam_size).type_as(cand_tokens)\n                cand_attention_weights = average_attn_weights.squeeze(1).repeat(2, 1)\n                # active_hypos size: beam_size\n                active_hypos = torch.arange(0, beam_size).type_as(cand_tokens)\n            else:\n\n                # Here we keep consistent with SequenceGenerator, take 2*beam_size best\n                # predictions per step, will select top beam_size of these which don\'t\n                # predict eos to continue with.\n                cand_scores_k_by_2k, cand_tokens_k_by_2k = torch.topk(\n                    average_log_probs_with_rewards.squeeze(1), k=double_beam_size\n                )\n\n                prev_scores_k_by_2k = prev_scores.view(-1, 1).expand(\n                    -1, double_beam_size\n                )\n                # size is (beam_size, 2 * beam_size)\n                total_scores_k_by_2k = cand_scores_k_by_2k + prev_scores_k_by_2k\n\n                total_scores_flat_2k = total_scores_k_by_2k.view(-1)\n                # size is (beam_size * 2 * beam_size)\n                cand_tokens_flat_2k = cand_tokens_k_by_2k.view(-1)\n                # size is (2 * beam_size)\n                cand_scores, cand_indices = torch.topk(\n                    total_scores_flat_2k, k=double_beam_size\n                )\n                # size is (2 * beam_size)\n                cand_tokens = cand_tokens_flat_2k.index_select(\n                    dim=0, index=cand_indices\n                ).view(-1)\n\n                # size is (2 * beam_size)\n                eos_mask = cand_tokens.eq(eos_token[0])\n                cand_prev_hypos = cand_indices // double_beam_size\n                cand_prev_hypos = cand_prev_hypos.type_as(cand_tokens)\n\n                cand_offsets = torch.arange(0, double_beam_size)\n\n                active_mask = torch.add(\n                    eos_mask.type_as(cand_offsets) * double_beam_size, cand_offsets\n                )\n                # select active hypos, size is (beam_size)\n                _, active_hypos = torch.topk(\n                    active_mask, k=beam_size, dim=0, largest=False, sorted=True\n                )\n\n                cand_attention_weights = average_attn_weights.index_select(\n                    dim=0, index=cand_prev_hypos\n                ).squeeze(1)\n\n            return (\n                cand_tokens,\n                cand_scores,\n                cand_prev_hypos,\n                cand_attention_weights,\n                active_hypos,\n            )\n\n        (\n            cand_tokens,\n            cand_scores,\n            cand_prev_hypos,\n            cand_attention_weights,\n            active_hypos,\n        ) = generate_outputs(\n            final_step,\n            average_log_probs_with_rewards,\n            average_attn_weights,\n            prev_scores,\n            eos_token=eos_token,\n            beam_size=self.beam_size,\n        )\n\n        # select active prev_hypos\n        active_prev_hypos = cand_prev_hypos.index_select(dim=0, index=active_hypos)\n        if possible_translation_tokens is not None:\n            cand_tokens = possible_translation_tokens.index_select(\n                dim=0, index=cand_tokens\n            )\n\n        self.input_names = [""prev_tokens"", ""prev_scores"", ""active_hypos"", ""timestep""]\n        for i in range(len(self.models)):\n            self.input_names.append(f""fixed_input_{i}"")\n\n        if possible_translation_tokens is not None:\n            self.input_names.append(""possible_translation_tokens"")\n\n        active_outputs = [\n            cand_tokens,\n            cand_scores,\n            cand_prev_hypos,\n            cand_attention_weights,\n            active_hypos,\n        ]\n        self.output_names = [\n            ""cand_tokens"",\n            ""cand_scores"",\n            ""cand_prev_hypos"",\n            ""cand_attention_weights"",\n            ""active_hypos"",\n        ]\n        for i in range(len(self.models)):\n            self.output_names.append(f""fixed_input_{i}"")\n            if self.tile_internal:\n                active_outputs.append(inputs[i].repeat(1, self.beam_size, 1))\n            else:\n                active_outputs.append(inputs[i])\n\n        if possible_translation_tokens is not None:\n            self.output_names.append(""possible_translation_tokens"")\n            active_outputs.append(possible_translation_tokens)\n\n        # just keep states for active_hypos\n        for i, state in enumerate(state_outputs):\n            beam_axis = beam_axis_per_state[i]\n            if beam_axis is None:\n                next_state = state\n            else:\n                next_state = state.index_select(dim=beam_axis, index=active_prev_hypos)\n            active_outputs.append(next_state)\n            self.output_names.append(f""state_output_{i}"")\n            self.input_names.append(f""state_input_{i}"")\n\n        return tuple(active_outputs)\n\n\nclass BeamDecodeWithEOS(BeamDecode):\n    """"""\n    Run beam decoding based on the beam search output from\n    DecoderBatchedStepEnsemble2BeamWithEOS. The differences compared with BeamDecode is:\n    1.there\'s no need to check prev_hypos finished or not when trying to get all end\n    states since we don\'t expand at eos token in DecoderBatchedStepEnsemble2BeamWithEOS.\n    2. add extra step for eos token at the end.\n    """"""\n\n    # TODO: (lizguo) This class will be merged with upstream later.\n    @torch.jit.script_method\n    def _get_all_end_states(\n        self,\n        beam_tokens: Tensor,\n        beam_scores: Tensor,\n        beam_prev_indices: Tensor,\n        num_steps: int,\n    ) -> Tensor:\n        min_score = float(""inf"")\n        min_index = -1\n        end_states = torch.jit.annotate(List[Tensor], [])\n\n        position = 1\n        while bool(position <= num_steps + 1):\n            for hyp_index in range(self.beam_size):\n                if bool(beam_tokens[position][hyp_index] == self.eos_token_id) or bool(\n                    position == num_steps + 1\n                ):\n                    hypo_score = float(beam_scores[position][hyp_index])\n                    if bool(self.length_penalty != 0):\n                        hypo_score = hypo_score / float(position) ** float(\n                            self.length_penalty\n                        )\n                    end_states, min_score, min_index = self._add_to_end_states(\n                        end_states,\n                        min_score,\n                        torch.tensor([hypo_score, float(position), float(hyp_index)]),\n                        min_index,\n                    )\n            position = position + 1\n\n        end_states = torch.stack(end_states)\n\n        _, sorted_end_state_indices = end_states[:, 0].sort(dim=0, descending=True)\n        end_states = end_states[sorted_end_state_indices, :]\n        return end_states\n\n    @torch.jit.script_method\n    def _check_dimensions(\n        self,\n        beam_tokens: Tensor,\n        beam_scores: Tensor,\n        token_weights: Tensor,\n        beam_prev_indices: Tensor,\n        num_steps: int,\n    ) -> None:\n\n        assert (\n            beam_tokens.size(1) == 2 * self.beam_size\n        ), ""Dimension of beam_tokens : {} and beam size : {} are not consistent"".format(\n            beam_tokens.size(), self.beam_size\n        )\n        assert beam_scores.size(1) == 2 * self.beam_size, (\n            ""Dimension of beam_scores : {} and beam size : {} ""\n            ""are not consistent"".format(beam_scores.size(), self.beam_size)\n        )\n        assert token_weights.size(1) == 2 * self.beam_size, (\n            ""Dimension of token_weights : {} and beam size : {} ""\n            ""are not consistent"".format(token_weights.size(), self.beam_size)\n        )\n        assert (\n            beam_prev_indices.size(1) == 2 * self.beam_size\n        ), ""Dimension of beam_prev_indices : {} and beam size : {} ""\n        ""are not consistent"".format(beam_prev_indices.size(), self.beam_size)\n\n        assert beam_tokens.size(0) <= num_steps + 2, (\n            ""Dimension of beam_tokens : {} and num_steps : {} ""\n            ""are not consistent"".format(beam_tokens.size(), num_steps)\n        )\n        assert beam_scores.size(0) <= num_steps + 2, (\n            ""Dimension of beam_scores : {} and num_steps : {} ""\n            ""are not consistent"".format(beam_scores.size(), num_steps)\n        )\n        assert token_weights.size(0) <= num_steps + 2, (\n            ""Dimension of token_weights : {} and num_steps : {} ""\n            ""are not consistent"".format(token_weights.size(), num_steps)\n        )\n        assert beam_prev_indices.size(0) <= num_steps + 2, (\n            ""Dimension of beam_prev_indices : {} and num_steps : {} ""\n            ""are not consistent"".format(beam_prev_indices.size(), num_steps)\n        )\n\n\nclass BeamSearchAndDecodeV2(torch.jit.ScriptModule):\n    """"""\n    The difference between BeamSearchAndDecodeV2 and BeamSearchAndDecode is: V2 calls\n    DecoderBatchedStepEnsemble2BeamWithEOS instead of DecoderBatchedStepEnsemble when\n    running beam search. Also, since extra EOS token has been added, it calls\n    BeamDecodeWithEOS when running beam decoding which supports adding extra EOS token.\n    """"""\n\n    def __init__(\n        self,\n        models,\n        tgt_dict,\n        src_tokens,\n        src_lengths,\n        eos_token_id,\n        length_penalty,\n        nbest,\n        beam_size,\n        stop_at_eos,\n        word_reward=0,\n        unk_reward=0,\n        quantize=False,\n    ):\n        super().__init__()\n\n        self.models = models\n        self.tgt_dict = tgt_dict\n        self.beam_size = torch.jit.Attribute(beam_size, int)\n        self.word_reward = torch.jit.Attribute(word_reward, float)\n        self.unk_reward = torch.jit.Attribute(unk_reward, float)\n\n        encoder_ens = EncoderEnsemble(self.models)\n        encoder_ens.enable_precompute_reduced_weights = True\n\n        if quantize:\n            encoder_ens = torch.jit.quantized.quantize_linear_modules(encoder_ens)\n            encoder_ens = torch.jit.quantized.quantize_rnn_cell_modules(encoder_ens)\n\n        # not support char source model\n        self.is_char_source = False\n        enc_inputs = (src_tokens, src_lengths)\n        example_encoder_outs = encoder_ens(*enc_inputs)\n        self.encoder_ens = torch.jit.trace(\n            encoder_ens, enc_inputs, _force_outplace=True\n        )\n        self.encoder_ens_char_source = FakeCharSourceEncoderEnsemble()\n\n        decoder_ens = DecoderBatchedStepEnsemble2BeamWithEOS(\n            self.models,\n            tgt_dict,\n            beam_size,\n            word_reward,\n            unk_reward,\n            tile_internal=False,\n        )\n        decoder_ens.enable_precompute_reduced_weights = True\n        if quantize:\n            decoder_ens = torch.jit.quantized.quantize_linear_modules(decoder_ens)\n            decoder_ens = torch.jit.quantized.quantize_rnn_cell_modules(decoder_ens)\n            decoder_ens = torch.jit.quantized.quantize_rnn_modules(decoder_ens)\n        decoder_ens_tile = DecoderBatchedStepEnsemble2BeamWithEOS(\n            self.models,\n            tgt_dict,\n            beam_size,\n            word_reward,\n            unk_reward,\n            tile_internal=True,\n        )\n        decoder_ens_tile.enable_precompute_reduced_weights = True\n        if quantize:\n            decoder_ens_tile = torch.jit.quantized.quantize_linear_modules(\n                decoder_ens_tile\n            )\n            decoder_ens_tile = torch.jit.quantized.quantize_rnn_cell_modules(\n                decoder_ens_tile\n            )\n            decoder_ens_tile = torch.jit.quantized.quantize_rnn_modules(\n                decoder_ens_tile\n            )\n        prev_token = torch.LongTensor([0])\n        prev_scores = torch.FloatTensor([0.0])\n        ts = torch.LongTensor([0])\n        final_step = torch.tensor([False], dtype=torch.bool)\n        active_hypos = torch.LongTensor([0])\n\n        _, _, _, _, _, *tiled_states = decoder_ens_tile(\n            prev_token, prev_scores, active_hypos, ts, final_step, *example_encoder_outs\n        )\n\n        self.decoder_ens_tile = torch.jit.trace(\n            decoder_ens_tile,\n            (\n                prev_token,\n                prev_scores,\n                active_hypos,\n                ts,\n                final_step,\n                *example_encoder_outs,\n            ),\n            _force_outplace=True,\n        )\n        self.decoder_ens = torch.jit.trace(\n            decoder_ens,\n            (\n                prev_token.repeat(self.beam_size),\n                prev_scores.repeat(self.beam_size),\n                active_hypos.repeat(self.beam_size),\n                ts,\n                final_step,\n                *tiled_states,\n            ),\n            _force_outplace=True,\n        )\n\n        self.beam_decode = BeamDecodeWithEOS(\n            eos_token_id, length_penalty, nbest, beam_size, stop_at_eos\n        )\n\n        self.input_names = [\n            ""src_tokens"",\n            ""src_lengths"",\n            ""prev_token"",\n            ""prev_scores"",\n            ""attn_weights"",\n            ""prev_hypos_indices"",\n            ""num_steps"",\n        ]\n        self.output_names = [\n            ""beam_output"",\n            ""hypothesis_score"",\n            ""token_level_scores"",\n            ""back_alignment_weights"",\n            ""best_indices"",\n        ]\n\n    @torch.jit.script_method\n    def forward(\n        self,\n        src_tokens: torch.Tensor,\n        src_lengths: torch.Tensor,\n        prev_token: torch.Tensor,\n        prev_scores: torch.Tensor,\n        attn_weights: torch.Tensor,\n        prev_hypos_indices: torch.Tensor,\n        active_hypos: torch.Tensor,\n        num_steps: int,\n    ) -> List[Tuple[Tensor, float, List[float], Tensor, Tensor]]:\n\n        enc_states = self.encoder_ens(src_tokens, src_lengths)\n\n        # enc_states ends up being optional because of the above branch, one\n        # side returns None. We should never take the path that returns None\n        # so we unrap the optional type here.\n        enc_states = torch.jit._unwrap_optional(enc_states)\n\n        # remove torch.cat, keep things in a list\n        all_tokens = [prev_token.repeat(repeats=[2 * self.beam_size])]\n        all_scores = [prev_scores.repeat(repeats=[2 * self.beam_size])]\n        all_weights = [\n            attn_weights.unsqueeze(dim=0).repeat(repeats=[2 * self.beam_size, 1])\n        ]\n        all_prev_indices = [prev_hypos_indices]\n\n        prev_token, prev_scores, prev_hypos_indices, attn_weights, active_hypos, *states = self.decoder_ens_tile(\n            prev_token,\n            prev_scores,\n            active_hypos,\n            torch.tensor([0]),\n            torch.tensor([False]),\n            *enc_states,  # noqa\n        )\n        all_tokens = all_tokens.append(prev_token)\n        all_scores = all_scores.append(prev_scores)\n        all_weights = all_weights.append(attn_weights)\n        all_prev_indices = all_prev_indices.append(prev_hypos_indices)\n\n        for i in range(num_steps - 1):\n            (\n                prev_token,\n                prev_scores,\n                prev_hypos_indices,\n                attn_weights,\n                active_hypos,\n                *states,\n            ) = self.decoder_ens(\n                prev_token,\n                prev_scores,\n                active_hypos,\n                torch.tensor([i + 1]),\n                torch.tensor([False]),\n                *states,  # noqa\n            )\n\n            all_tokens = all_tokens.append(prev_token)\n            all_scores = all_scores.append(prev_scores)\n            all_weights = all_weights.append(attn_weights)\n            all_prev_indices = all_prev_indices.append(prev_hypos_indices)\n\n        # add eos token as extra step\n        prev_token, prev_scores, prev_hypos_indices, attn_weights, active_hypos, *states = self.decoder_ens(\n            prev_token,\n            prev_scores,\n            active_hypos,\n            torch.tensor([num_steps]),\n            torch.tensor([True]),\n            *states,\n        )\n\n        all_tokens = all_tokens.append(prev_token)\n        all_scores = all_scores.append(prev_scores)\n        all_weights = all_weights.append(attn_weights)\n        all_prev_indices = all_prev_indices.append(prev_hypos_indices)\n\n        outputs = torch.jit.annotate(\n            List[Tuple[Tensor, float, List[float], Tensor, Tensor]], []\n        )\n        outputs = self.beam_decode(\n            torch.stack(all_tokens, dim=0),\n            torch.stack(all_scores, dim=0),\n            torch.stack(all_weights, dim=0),\n            torch.stack(all_prev_indices, dim=0),\n            num_steps,\n        )\n\n        return outputs\n\n    @classmethod\n    def build_from_checkpoints(\n        cls,\n        checkpoint_filenames,\n        src_dict_filename,\n        dst_dict_filename,\n        beam_size,\n        length_penalty,\n        nbest,\n        word_reward=0,\n        unk_reward=0,\n        lexical_dict_paths=None,\n    ):\n        length = 10\n        models, _, tgt_dict = load_models_from_checkpoints(\n            checkpoint_filenames,\n            src_dict_filename,\n            dst_dict_filename,\n            lexical_dict_paths,\n        )\n        src_tokens = torch.LongTensor(np.ones((length, 1), dtype=""int64""))\n        src_lengths = torch.IntTensor(np.array([length], dtype=""int32""))\n        eos_token_id = tgt_dict.eos()\n\n        return cls(\n            models,\n            tgt_dict,\n            src_tokens,\n            src_lengths,\n            eos_token_id,\n            length_penalty=length_penalty,\n            nbest=nbest,\n            beam_size=beam_size,\n            stop_at_eos=True,\n            word_reward=word_reward,\n            unk_reward=unk_reward,\n            quantize=True,\n        )\n\n    def save_to_pytorch(self, output_path):\n        def pack(s):\n            if hasattr(s, ""_pack""):\n                s._pack()\n\n        def unpack(s):\n            if hasattr(s, ""_unpack""):\n                s._unpack()\n\n        self.apply(pack)\n        torch.jit.save(self, output_path)\n        self.apply(unpack)\n'"
pytorch_translate/benchmark.py,0,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport tempfile\n\nfrom fairseq import options, tasks\nfrom pytorch_translate import (\n    generate as pytorch_translate_generate,\n    options as pytorch_translate_options,\n    utils as pytorch_translate_utils,\n)\nfrom pytorch_translate.constants import CHECKPOINT_PATHS_DELIMITER\n\n\nfrom pytorch_translate import rnn  # noqa; noqa\n\n\ndef get_parser_with_args():\n    parser = options.get_parser(""Generation"", default_task=""pytorch_translate"")\n    pytorch_translate_options.add_verbosity_args(parser)\n    pytorch_translate_options.add_dataset_args(parser, gen=True)\n    generation_group = options.add_generation_args(parser)\n    pytorch_translate_options.expand_generation_args(generation_group)\n\n    generation_group.add_argument(\n        ""--source-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file representing the Dictionary to use."",\n    )\n    generation_group.add_argument(\n        ""--char-source-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=(\n            ""Same as --source-vocab-file except using characters. ""\n            ""(For use with char_source and char_aware models only.)""\n        ),\n    )\n    generation_group.add_argument(\n        ""--target-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file representing the Dictionary to use."",\n    )\n    generation_group.add_argument(\n        ""--char-target-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=(\n            ""Same as --source-target-file except using characters. ""\n            ""(For use with char_aware models only.)""\n        ),\n    )\n    generation_group.add_argument(\n        ""--multiling-source-lang"",\n        action=""append"",\n        metavar=""SRC"",\n        help=(\n            ""Must be set for decoding with multilingual models. ""\n            ""Must match an entry from --multiling-encoder-lang from training.""\n        ),\n    )\n    generation_group.add_argument(\n        ""--multiling-target-lang"",\n        action=""append"",\n        metavar=""TARGET"",\n        help=(\n            ""Must be set for decoding with multilingual models. ""\n            ""Must match an entry from --multiling-decoder-lang from training.""\n        ),\n    )\n\n    # Add args related to benchmarking.\n    group = parser.add_argument_group(""Benchmarking"")\n    group.add_argument(\n        ""--runs-per-length"",\n        default=10,\n        type=int,\n        help=""Number of times to run generation on each length."",\n    )\n    group.add_argument(\n        ""--examples-per-length"",\n        default=1,\n        type=int,\n        help=""Sentences of each length to include in each eval (batched if >1)."",\n    )\n\n    return parser\n\n\ndef main():\n    parser = get_parser_with_args()\n    # args = parser.parse_args()\n    args = options.parse_args_and_arch(parser)\n    # Disable printout of all source and target sentences\n    args.quiet = True\n    benchmark(args)\n\n\ndef generate_synthetic_text(dialect, dialect_symbols, length, examples):\n    temp_file = tempfile.NamedTemporaryFile(mode=""w"", delete=False, dir=""/tmp"")\n    temp_file_name = temp_file.name\n    temp_file.close()\n    with open(temp_file_name, ""w"") as temp_file:\n        for _ in range(examples):\n            temp_file.write("" "".join(random.sample(dialect_symbols, length)) + ""\\n"")\n    return temp_file_name\n\n\ndef benchmark(args):\n    assert args.source_vocab_file and os.path.isfile(\n        args.source_vocab_file\n    ), ""Please specify a valid file for --source-vocab-file""\n    assert args.target_vocab_file and os.path.isfile(\n        args.target_vocab_file\n    ), ""Please specify a valid file for --target-vocab_file""\n    assert args.path is not None, ""--path required for generation!""\n\n    print(args)\n\n    # Benchmarking should be language-agnostic\n    args.source_lang = ""src""\n    args.target_lang = ""tgt""\n\n    models, model_args, task = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n        args.path.split(CHECKPOINT_PATHS_DELIMITER)\n    )\n\n    append_eos_to_source = model_args[0].append_eos_to_source\n    reverse_source = model_args[0].reverse_source\n    assert all(\n        a.append_eos_to_source == append_eos_to_source\n        and a.reverse_source == reverse_source\n        for a in model_args\n    )\n\n    def benchmark_length(n):\n        # Generate synthetic raw text files\n        source_text_file = generate_synthetic_text(\n            dialect=args.source_lang,\n            dialect_symbols=task.source_dictionary.symbols,\n            length=n,\n            examples=args.examples_per_length,\n        )\n        target_text_file = generate_synthetic_text(\n            dialect=args.target_lang,\n            dialect_symbols=task.target_dictionary.symbols,\n            length=n,\n            examples=args.examples_per_length,\n        )\n\n        task.load_dataset_from_text(\n            args.gen_subset,\n            source_text_file=source_text_file,\n            target_text_file=target_text_file,\n            append_eos=append_eos_to_source,\n            reverse_source=reverse_source,\n        )\n\n        # Remove temporary text files\n        os.remove(source_text_file)\n        os.remove(target_text_file)\n\n        # priming\n        scorer, num_sentences, gen_timer, _ = pytorch_translate_generate.generate_score(\n            models=models, args=args, task=task, dataset=task.dataset(args.gen_subset)\n        )\n\n        total_time = 0.0\n        for _ in range(args.runs_per_length):\n            scorer, num_sentences, gen_timer, _ = pytorch_translate_generate.generate_score(\n                models=models,\n                args=args,\n                task=task,\n                dataset=task.dataset(args.gen_subset),\n            )\n            total_time += gen_timer.sum\n            gen_timer.reset()\n\n        sentences_per_run = args.examples_per_length\n        runs = args.runs_per_length\n        total_sentences = sentences_per_run * runs\n        total_tokens = total_sentences * n\n\n        print(f""--- {n} tokens ---"")\n        print(f""Generated {total_tokens} tokens ({runs} runs of {sentences_per_run})"")\n        print(f""Total time: {total_time:.3f} seconds"")\n        time_per_sentence = total_time / total_sentences\n        print(f""Time per sentence: {time_per_sentence:.3f} seconds\\n"")\n\n    benchmark_length(6)\n    benchmark_length(10)\n    benchmark_length(20)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_translate/bleu_significance.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\nfrom typing import List, NamedTuple, Optional\n\nimport numpy as np\nimport pandas as pd\nimport sacrebleu\n\n\ndef get_sufficient_stats(\n    translations: List[str], references: List[str]\n) -> pd.DataFrame:\n    assert len(translations) == len(references), (\n        f""There are {len(translations)} translated sentences ""\n        f""but {len(references)} reference sentences""\n    )\n    assert sacrebleu.NGRAM_ORDER == 4, (\n        f""Expected SacreBLEU to be using n-gram order 4 ""\n        f""instead of {sacrebleu.NGRAM_ORDER}.""\n    )\n\n    sufficient_stats: List[List[int]] = []\n    for sentence, ref in zip(translations, references):\n        sentence_bleu = sacrebleu.corpus_bleu(\n            sys_stream=sentence,\n            ref_streams=ref,\n            lowercase=False,\n            tokenize=""none"",\n            use_effective_order=False,\n        )\n        sufficient_stats.append(\n            [\n                # Number of correct 1-grams, .., 4-grams\n                sentence_bleu.counts[0],\n                sentence_bleu.counts[1],\n                sentence_bleu.counts[2],\n                sentence_bleu.counts[3],\n                # Total number of 1-grams, .., 4-grams\n                sentence_bleu.totals[0],\n                sentence_bleu.totals[1],\n                sentence_bleu.totals[2],\n                sentence_bleu.totals[3],\n                # Length of translated sentence.\n                sentence_bleu.sys_len,\n                # Length of reference sentence.\n                sentence_bleu.ref_len,\n            ]\n        )\n    return pd.DataFrame(\n        sufficient_stats,\n        columns=[\n            ""correct_1_grams"",\n            ""correct_2_grams"",\n            ""correct_3_grams"",\n            ""correct_4_grams"",\n            ""total_1_grams"",\n            ""total_2_grams"",\n            ""total_3_grams"",\n            ""total_4_grams"",\n            ""translation_length"",\n            ""reference_length"",\n        ],\n    )\n\n\ndef calc_bleu_from_stats(sentence_stats: pd.DataFrame) -> sacrebleu.BLEU:\n    corpus_stats = sentence_stats.sum(axis=0)\n    corpus_bleu = sacrebleu.compute_bleu(\n        correct=[\n            corpus_stats.correct_1_grams,\n            corpus_stats.correct_2_grams,\n            corpus_stats.correct_3_grams,\n            corpus_stats.correct_4_grams,\n        ],\n        total=[\n            corpus_stats.total_1_grams,\n            corpus_stats.total_2_grams,\n            corpus_stats.total_3_grams,\n            corpus_stats.total_4_grams,\n        ],\n        sys_len=corpus_stats.translation_length,\n        ref_len=corpus_stats.reference_length,\n    )\n    return corpus_bleu\n\n\nclass PairedBootstrapOutput(NamedTuple):\n    baseline_bleu: sacrebleu.BLEU\n    new_bleu: sacrebleu.BLEU\n    num_samples: int\n    # Number of samples where the baseline was better than the new.\n    baseline_better: int\n    # Number of samples where the baseline and new had identical BLEU score.\n    num_equal: int\n    # Number of samples where the new was better than baseline.\n    new_better: int\n\n\ndef paired_bootstrap_resample(\n    baseline_stats: pd.DataFrame,\n    new_stats: pd.DataFrame,\n    num_samples: int = 1000,\n    sample_size: Optional[int] = None,\n) -> PairedBootstrapOutput:\n    """"""\n    From http://aclweb.org/anthology/W04-3250\n    Statistical significance tests for machine translation evaluation (Koehn, 2004)\n    """"""\n    assert len(baseline_stats) == len(new_stats), (\n        f""Length mismatch - baseline has {len(baseline_stats)} lines ""\n        f""while new has {len(new_stats)} lines.""\n    )\n    num_sentences = len(baseline_stats)\n    if not sample_size:\n        # Defaults to sampling new corpora of the same size as the original.\n        # This is not identical to the original corpus since we are sampling\n        # with replacement.\n        sample_size = num_sentences\n    indices = np.random.randint(\n        low=0, high=num_sentences, size=(num_samples, sample_size)\n    )\n\n    baseline_better: int = 0\n    new_better: int = 0\n    num_equal: int = 0\n    for index in indices:\n        baseline_bleu = calc_bleu_from_stats(baseline_stats.iloc[index]).score\n        new_bleu = calc_bleu_from_stats(new_stats.iloc[index]).score\n        if new_bleu > baseline_bleu:\n            new_better += 1\n        elif baseline_bleu > new_bleu:\n            baseline_better += 1\n        else:\n            # If the baseline corpus and new corpus are identical, this\n            # degenerate case may occur.\n            num_equal += 1\n\n    return PairedBootstrapOutput(\n        baseline_bleu=calc_bleu_from_stats(baseline_stats),\n        new_bleu=calc_bleu_from_stats(new_stats),\n        num_samples=num_samples,\n        baseline_better=baseline_better,\n        num_equal=num_equal,\n        new_better=new_better,\n    )\n\n\ndef paired_bootstrap_resample_from_files(\n    reference_file: str,\n    baseline_file: str,\n    new_file: str,\n    num_samples: int = 1000,\n    sample_size: Optional[int] = None,\n) -> PairedBootstrapOutput:\n    with open(reference_file, ""r"") as f:\n        references: List[str] = [line for line in f]\n\n    with open(baseline_file, ""r"") as f:\n        baseline_translations: List[str] = [line for line in f]\n    baseline_stats: pd.DataFrame = get_sufficient_stats(\n        translations=baseline_translations, references=references\n    )\n\n    with open(new_file, ""r"") as f:\n        new_translations: List[str] = [line for line in f]\n    new_stats: pd.DataFrame = get_sufficient_stats(\n        translations=new_translations, references=references\n    )\n\n    return paired_bootstrap_resample(\n        baseline_stats=baseline_stats,\n        new_stats=new_stats,\n        num_samples=num_samples,\n        sample_size=sample_size,\n    )\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--reference-file"",\n        type=str,\n        required=True,\n        help=""Text file containing reference tokenized (with whitespace separator) sentences."",\n    )\n    parser.add_argument(\n        ""--baseline-file"",\n        type=str,\n        required=True,\n        help=""Text file containing tokenized sentences translated by baseline system."",\n    )\n    parser.add_argument(\n        ""--new-file"",\n        type=str,\n        required=True,\n        help=""Text file containing tokenized sentences translated by new system."",\n    )\n    args = parser.parse_args()\n\n    output = paired_bootstrap_resample_from_files(\n        reference_file=args.reference_file,\n        baseline_file=args.baseline_file,\n        new_file=args.new_file,\n    )\n\n    print(f""Baseline BLEU: {output.baseline_bleu.score:.2f}"")\n    print(f""New BLEU: {output.new_bleu.score:.2f}"")\n    print(f""BLEU delta: {output.new_bleu.score - output.baseline_bleu.score:.2f} "")\n    print(\n        f""Baseline better confidence: {output.baseline_better / output.num_samples:.2%}""\n    )\n    print(f""New better confidence: {output.new_better / output.num_samples:.2%}"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_translate/char_aware_hybrid.py,5,"b'#!/usr/bin/env python3\n\nimport math\nfrom ast import literal_eval\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.models import register_model, register_model_architecture\nfrom pytorch_translate import (\n    char_encoder,\n    char_source_hybrid,\n    char_source_model,\n    hybrid_transformer_rnn,\n    transformer as pytorch_translate_transformer,\n    vocab_constants,\n)\nfrom pytorch_translate.data.dictionary import TAGS\nfrom pytorch_translate.utils import maybe_cuda\n\n\n@register_model(""char_aware_hybrid"")\nclass CharAwareHybridModel(char_source_hybrid.CharSourceHybridModel):\n    """"""\n    An architecture combining hybrid Transformer/RNN with character-based\n    inputs (token embeddings created via character-input CNN) and outputs.\n    This model is very similar to https://arxiv.org/pdf/1809.02223.pdf.\n    """"""\n\n    def __init__(self, task, encoder, decoder):\n        super().__init__(task, encoder, decoder)\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        src_dict, dst_dict = task.source_dictionary, task.target_dictionary\n        base_architecture(args)\n\n        assert hasattr(args, ""char_source_dict_size""), (\n            ""args.char_source_dict_size required. ""\n            ""should be set by load_binarized_dataset()""\n        )\n        assert hasattr(args, ""char_target_dict_size""), (\n            ""args.char_target_dict_size required. ""\n            ""should be set by load_binarized_dataset()""\n        )\n\n        assert hasattr(\n            args, ""char_cnn_params""\n        ), ""Only char CNN is supported for the char encoder hybrid model""\n\n        args.embed_bytes = getattr(args, ""embed_bytes"", False)\n\n        # In case use_pretrained_weights is true, verify the model params\n        # are correctly set\n        if args.embed_bytes and getattr(args, ""use_pretrained_weights"", False):\n            char_source_model.verify_pretrain_params(args)\n\n        encoder = char_source_hybrid.CharSourceHybridModel.build_encoder(\n            args=args, src_dict=src_dict\n        )\n        decoder = CharAwareHybridModel.build_decoder(\n            args=args, src_dict=src_dict, dst_dict=dst_dict\n        )\n\n        return cls(task, encoder, decoder)\n\n    def forward(\n        self,\n        src_tokens,\n        src_lengths,\n        char_inds,\n        word_lengths,\n        prev_output_tokens,\n        prev_output_chars,\n        prev_output_word_lengths=None,\n    ):\n        encoder_out = self.encoder(src_tokens, src_lengths, char_inds, word_lengths)\n        decoder_out = self.decoder(\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out,\n            prev_output_chars=prev_output_chars,\n        )\n        return decoder_out\n\n    @classmethod\n    def build_decoder(cls, args, src_dict, dst_dict):\n        # If we embed bytes then the number of indices is fixed and does not\n        # depend on the dictionary\n        if args.embed_bytes:\n            num_chars = vocab_constants.NUM_BYTE_INDICES + TAGS.__len__() + 1\n        else:\n            num_chars = args.char_target_dict_size\n\n        decoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=dst_dict,\n            embed_dim=args.decoder_embed_dim,\n            path=args.decoder_pretrained_embed,\n            freeze=args.decoder_freeze_embed,\n        )\n        return CharAwareHybridRNNDecoder(\n            args,\n            src_dict=src_dict,\n            dst_dict=dst_dict,\n            embed_tokens=decoder_embed_tokens,\n            num_chars=num_chars,\n            char_embed_dim=args.char_embed_dim,\n            char_cnn_params=args.char_cnn_params,\n            char_cnn_nonlinear_fn=args.char_cnn_nonlinear_fn,\n            char_cnn_num_highway_layers=args.char_cnn_num_highway_layers,\n            use_pretrained_weights=False,\n            finetune_pretrained_weights=False,\n        )\n\n\nclass CharAwareHybridRNNDecoder(hybrid_transformer_rnn.HybridRNNDecoder):\n    """"""\n        A decoder that is similar to the HybridRNNDecoder but has a character\n        CNN encoder to get the representation for each generated previous token.\n        The decoder is similar to https://arxiv.org/pdf/1809.02223.pdf.\n    """"""\n\n    def __init__(\n        self,\n        args,\n        src_dict,\n        dst_dict,\n        embed_tokens,\n        num_chars=50,\n        char_embed_dim=32,\n        char_cnn_params=""[(128, 3), (128, 5)]"",\n        char_cnn_nonlinear_fn=""tanh"",\n        char_cnn_num_highway_layers=0,\n        use_pretrained_weights=False,\n        finetune_pretrained_weights=False,\n    ):\n        super().__init__(args, src_dict, dst_dict, embed_tokens)\n        convolutions_params = literal_eval(char_cnn_params)\n        self.char_cnn_encoder = char_encoder.CharCNNModel(\n            dictionary=dst_dict,\n            num_chars=num_chars,\n            char_embed_dim=char_embed_dim,\n            convolutions_params=convolutions_params,\n            nonlinear_fn_type=char_cnn_nonlinear_fn,\n            num_highway_layers=char_cnn_num_highway_layers,\n            # char_cnn_output_dim should match the word embedding dimension.\n            char_cnn_output_dim=embed_tokens.embedding_dim,\n            use_pretrained_weights=use_pretrained_weights,\n            finetune_pretrained_weights=finetune_pretrained_weights,\n        )\n        self.char_layer_norm = nn.LayerNorm(embed_tokens.embedding_dim)\n\n        # By default (before training ends), character representations are\n        # not precomputed. After precomputation, this value should be used in place of\n        # the two embeddings.\n        self._is_precomputed = False\n        self.combined_word_char_embed = nn.Embedding(\n            embed_tokens.num_embeddings, embed_tokens.embedding_dim\n        )\n\n    def _get_char_cnn_output(self, char_inds):\n        if char_inds.dim() == 2:\n            char_inds = char_inds.unsqueeze(1)\n        bsz, seqlen, maxchars = char_inds.size()\n\n        # char_cnn_encoder takes input (max_word_length, total_words)\n        char_inds_flat = char_inds.view(-1, maxchars).t()\n        # output (total_words, encoder_dim)\n        char_cnn_output = self.char_cnn_encoder(char_inds_flat)\n\n        char_cnn_output = char_cnn_output.view(bsz, seqlen, char_cnn_output.shape[-1])\n        # (seqlen, bsz, char_cnn_output_dim)\n        char_cnn_output = char_cnn_output.transpose(0, 1)\n        char_cnn_output = self.char_layer_norm(char_cnn_output)\n        return char_cnn_output\n\n    def _embed_prev_outputs(\n        self, prev_output_tokens, incremental_state=None, prev_output_chars=None\n    ):\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            if prev_output_chars is not None:\n                prev_output_chars = prev_output_chars[:, -1:, :].squeeze(1)\n\n        combined_embed = self._combined_word_char_embed(\n            prev_output_tokens=prev_output_tokens, prev_output_chars=prev_output_chars\n        )\n        return combined_embed, prev_output_tokens\n\n    def _combined_word_char_embed(self, prev_output_tokens, prev_output_chars):\n        """"""\n        If the embeddings are precomputed for character compositions (this holds\n        in inference), use the cached embeddings, otherwise compute it.\n        """"""\n        if self._is_precomputed:\n            combined_embedding = (\n                self.combined_word_char_embed(prev_output_tokens)\n                .squeeze(1)\n                .unsqueeze(0)\n            )\n        else:\n            x = self.embed_tokens(prev_output_tokens)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            # B x T x C -> T x B x C\n            x = x.transpose(0, 1)\n            char_cnn_output = self._get_char_cnn_output(prev_output_chars)\n            combined_embedding = x + char_cnn_output\n        return combined_embedding\n\n    def forward(\n        self,\n        prev_output_tokens,\n        encoder_out,\n        incremental_state=None,\n        possible_translation_tokens=None,\n        timestep=None,\n        prev_output_chars=None,\n    ):\n        """"""\n        The assumption is that during inference, the word embedding values are\n        summed with their corresponding character representations. Thus the model\n        will look like the same as a word-based decoder.\n        """"""\n        if self.training:\n            x, prev_output_tokens = self._embed_prev_outputs(\n                prev_output_tokens=prev_output_tokens,\n                incremental_state=incremental_state,\n                prev_output_chars=prev_output_chars,\n            )\n        else:\n            x, prev_output_tokens = super()._embed_prev_outputs(\n                prev_output_tokens=prev_output_tokens,\n                incremental_state=incremental_state,\n            )\n        return self._forward_given_embeddings(\n            embed_out=x,\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out,\n            incremental_state=incremental_state,\n            possible_translation_tokens=possible_translation_tokens,\n            timestep=timestep,\n        )\n\n    def precompute_char_representations(\n        self, char_dict, embed_bytes=False, batch_size=5000\n    ):\n        """"""\n        Precomputes the embeddings from character CNNs. Then adds that to the\n        word embeddings.\n        Args:\n            batch_size: maximum number of words in one batch\n        """"""\n        character_list = self._char_list_from_dict(\n            char_dict=char_dict, embed_bytes=embed_bytes\n        )\n        all_idx = maybe_cuda(\n            torch.LongTensor([i for i in range(self.embed_tokens.num_embeddings)])\n        )\n        word_embeds = self.embed_tokens(all_idx)\n        num_minibatches = math.ceil(len(character_list) / batch_size)\n        for i in range(num_minibatches):\n            character_sublist = character_list[\n                i * batch_size : min((i + 1) * batch_size, len(character_list))\n            ]\n            max_word_len = max(len(chars) for chars in character_sublist)\n            char_inds = (\n                torch.Tensor(len(character_sublist), max_word_len)\n                .long()\n                .fill_(char_dict.pad_index)\n            )\n\n            for j, chars in enumerate(character_sublist):\n                char_inds[j, : len(chars)] = torch.LongTensor(chars)\n\n            char_cnn_output = self._get_char_cnn_output(maybe_cuda(char_inds))\n\n            # Filling in the precomputed embedding values.\n            index_offset = i * batch_size\n            for j in range(char_cnn_output.size()[1]):\n                cur_idx = j + index_offset\n                self.combined_word_char_embed.weight[cur_idx] = (\n                    char_cnn_output[0, j, :] + word_embeds[cur_idx]\n                )\n\n        self._is_precomputed = True\n        self.combined_word_char_embed.weight.detach()\n\n    def _char_list_from_dict(self, char_dict, embed_bytes=False) -> List[List[int]]:\n        """"""\n        From self.word_dict, extracts all character sequneces, and convert\n        them to their corresponding list of characters.\n        """"""\n        character_list = []\n        for word_index, word in enumerate(self.dictionary.symbols):\n            character_list.append(\n                self._char_list_for_word(\n                    word_index=word_index,\n                    word=word,\n                    char_dict=char_dict,\n                    embed_bytes=embed_bytes,\n                )\n            )\n        return character_list\n\n    def _char_list_for_word(\n        self, word_index: int, word: str, char_dict, embed_bytes=False\n    ) -> List[int]:\n        """"""\n        Extracts character\n        For special words except pad, we put eos, because we actually\n        do not need their character sequences.\n        """"""\n        if word_index == self.dictionary.pad_index:\n            char_inds = [char_dict.pad_index]\n        elif word_index < self.dictionary.nspecial:\n            char_inds = [char_dict.eos_index]\n        else:\n            if embed_bytes:\n                # The byte_id needs to be incremented by 1 to account for the\n                # padding id (0) in the embedding table\n                char_inds = (\n                    [vocab_constants.NUM_BYTE_INDICES + TAGS.index(word) + 1]\n                    if word in TAGS\n                    else [byte_id + 1 for byte_id in word.encode(""utf8"", ""ignore"")]\n                )\n            else:\n                chars = [word] if word in TAGS else list(word)\n                char_inds = [char_dict.index(c) for c in chars]\n        return char_inds\n\n\n@register_model_architecture(""char_aware_hybrid"", ""char_aware_hybrid"")\ndef base_architecture(args):\n    # default architecture\n    hybrid_transformer_rnn.base_architecture(args)\n    args.char_cnn_params = getattr(args, ""char_cnn_params"", ""[(50, 1), (100,2)]"")\n    args.char_cnn_nonlinear_fn = getattr(args, ""chr_cnn_nonlinear_fn"", ""relu"")\n    args.char_cnn_num_highway_layers = getattr(args, ""char_cnn_num_highway_layers"", ""2"")\n'"
pytorch_translate/char_encoder.py,28,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pytorch_translate import common_layers  # noqa\nfrom pytorch_translate import rnn, utils\nfrom pytorch_translate.data.dictionary import TAGS\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n\ndef add_args(parser):\n    parser.add_argument(\n        ""--char-embed-dim"",\n        type=int,\n        default=128,\n        metavar=""N"",\n        help=(""Character embedding dimension.""),\n    )\n    parser.add_argument(\n        ""--char-rnn-units"",\n        type=int,\n        default=256,\n        metavar=""N"",\n        help=(""Number of units for Character LSTM.""),\n    )\n    parser.add_argument(\n        ""--char-rnn-layers"",\n        type=int,\n        default=1,\n        metavar=""N"",\n        help=(""Number of Character LSTM layers.""),\n    )\n    parser.add_argument(\n        ""--char-cnn-params"",\n        type=str,\n        metavar=""EXPR"",\n        help=(""String experission, [(dim, kernel_size), ...].""),\n    )\n    parser.add_argument(\n        ""--char-cnn-nonlinear-fn"",\n        type=str,\n        default=""tanh"",\n        metavar=""EXPR"",\n        help=(""Nonlinearity applied to char conv outputs. Values: relu, tanh.""),\n    )\n    parser.add_argument(\n        ""--char-cnn-num-highway-layers"",\n        type=int,\n        default=0,\n        metavar=""N"",\n        help=(""Char cnn encoder highway layers.""),\n    )\n    parser.add_argument(\n        ""--char-cnn-output-dim"",\n        type=int,\n        default=-1,\n        metavar=""N"",\n        help=""Output dim of the CNN layer. If set to -1, this is computed ""\n        ""from char-cnn-params."",\n    )\n    parser.add_argument(\n        ""--use-pretrained-weights"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=""Use pretrained weights for the character model including ""\n        ""the char embeddings, CNN filters, highway networks"",\n    )\n    parser.add_argument(\n        ""--finetune-pretrained-weights"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=""Boolean flag to specify whether or not to update the ""\n        ""pretrained weights as part of training"",\n    )\n    parser.add_argument(\n        ""--pretrained-weights-file"",\n        type=str,\n        default="""",\n        help=(""Weights file for loading pretrained weights""),\n    )\n    parser.add_argument(\n        ""--unk-only-char-encoding"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=(\n            ""Boolean flag. When True, taking words embeddings""\n            ""for in-vocab tokens and char encoder\'s outputs for oov tokens""\n            ""When False, concatenating words embeddings and char encoder\'s outputs""\n            ""for all tokens.""\n        ),\n    )\n\n\nclass HighwayLayer(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        transform_activation=F.relu,\n        gate_activation=F.softmax,\n        # Srivastava et al. (2015) recommend initializing bT to a negative\n        # value, in order to militate the initial behavior towards carry.\n        # We initialized bT to a small interval around \xe2\x88\x922\n        gate_bias=-2,\n    ):\n        super().__init__()\n        self.highway_transform_activation = transform_activation\n        self.highway_gate_activation = gate_activation\n        self.highway_transform = nn.Linear(input_dim, input_dim)\n        self.highway_gate = nn.Linear(input_dim, input_dim)\n        self.highway_gate.bias.data.fill_(gate_bias)\n\n    def forward(self, x):\n        transform_output = self.highway_transform_activation(self.highway_transform(x))\n        gate_output = self.highway_gate_activation(self.highway_gate(x))\n\n        transformation_part = torch.mul(transform_output, gate_output)\n        # TODO: https://github.com/pytorch/pytorch/issues/10747 makes the\n        # torch.FloatTensor() expression necessary. Once that gets fixed we\n        # can just write 1 - gate_output\n        carry_part = torch.mul(\n            (torch.FloatTensor([1.0]).type_as(gate_output) - gate_output), x\n        )\n        return torch.add(transformation_part, carry_part)\n\n\nclass CharCNNModel(nn.Module):\n    """"""\n    A Conv network to generate word embedding from character embeddings, from\n    Character-Aware Neural Language Models, https://arxiv.org/abs/1508.06615.\n\n    Components include convolutional filters, pooling, and\n    optional highway network. We also have the ability to use pretrained ELMo\n    which corresponds to the byte embeddings, CNN weights and the highway layer.\n    """"""\n\n    def __init__(\n        self,\n        dictionary,\n        num_chars=50,\n        char_embed_dim=32,\n        convolutions_params=""((128, 3), (128, 5))"",\n        nonlinear_fn_type=""tanh"",\n        num_highway_layers=0,\n        # A value of -1 for char_cnn_output_dim implies no projection layer\n        # layer at the output of the highway network\n        char_cnn_output_dim=-1,\n        use_pretrained_weights=False,\n        finetune_pretrained_weights=False,\n        weights_file=None,\n    ):\n        super().__init__()\n        self.dictionary = dictionary\n        self.padding_idx = dictionary.pad()\n        self.use_pretrained_weights = use_pretrained_weights\n\n        self.convolutions_params = convolutions_params\n        self.num_highway_layers = num_highway_layers\n        self.char_embed_dim = char_embed_dim\n        self.num_embeddings = num_chars\n        self.char_cnn_output_dim = char_cnn_output_dim\n        self.filter_dims = sum(f[0] for f in self.convolutions_params)\n\n        # If specified, load the pretrained weights from file\n        if use_pretrained_weights:\n            self._weight_file = weights_file\n            self._finetune_pretrained_weights = finetune_pretrained_weights\n            self._load_weights()\n        else:\n            if nonlinear_fn_type == ""tanh"":\n                nonlinear_fn = nn.Tanh\n            elif nonlinear_fn_type == ""relu"":\n                nonlinear_fn = nn.ReLU\n            else:\n                raise Exception(""Invalid nonlinear type: {}"".format(nonlinear_fn_type))\n\n            self.embed_chars = rnn.Embedding(\n                num_embeddings=num_chars,\n                embedding_dim=char_embed_dim,\n                padding_idx=self.padding_idx,\n                freeze_embed=False,\n            )\n            self.convolutions = nn.ModuleList(\n                [\n                    nn.Sequential(\n                        nn.Conv1d(\n                            char_embed_dim,\n                            num_filters,\n                            kernel_size,\n                            padding=kernel_size,\n                        ),\n                        nonlinear_fn(),\n                    )\n                    for (num_filters, kernel_size) in self.convolutions_params\n                ]\n            )\n\n            highway_layers = []\n            for _ in range(self.num_highway_layers):\n                highway_layers.append(HighwayLayer(self.filter_dims))\n            self.highway_layers = nn.ModuleList(highway_layers)\n\n            if char_cnn_output_dim != -1:\n                self.projection = nn.Linear(\n                    self.filter_dims, self.char_cnn_output_dim, bias=True\n                )\n\n    def _load_weights(self):\n        """"""\n        Function to load pretrained weights including byte embeddings.\n        """"""\n        self.npz_weights = np.load(self._weight_file)\n        self._load_byte_embedding()\n        self._load_cnn_weights()\n        self._load_highway()\n        self._load_projection()\n\n    def _load_byte_embedding(self):\n        """"""\n        Function to load the pre-trained byte embeddings. We need to ensure that\n        the embeddings account for special yoda tags as well.\n        """"""\n        char_embed_weights = self.npz_weights[""char_embed""]\n\n        num_tags = TAGS.__len__()\n        weights = np.zeros(\n            (char_embed_weights.shape[0] + num_tags + 1, char_embed_weights.shape[1]),\n            dtype=""float32"",\n        )\n        weights[1:-num_tags, :] = char_embed_weights\n\n        self.embed_chars = rnn.Embedding(\n            num_embeddings=self.num_embeddings,\n            embedding_dim=self.char_embed_dim,\n            padding_idx=self.padding_idx,\n            freeze_embed=self._finetune_pretrained_weights,\n        )\n        self.embed_chars.weight.data.copy_(torch.FloatTensor(weights))\n\n    def _load_cnn_weights(self):\n        """"""\n        Function to load the weights associated with the pretrained CNN filters.\n        For this to work correctly, the cnn params specified in the input arguments\n        should match up with the pretrained architecture.\n        """"""\n        convolutions = []\n        for i, (num_filters, kernel_size) in enumerate(self.convolutions_params):\n            conv = torch.nn.Conv1d(\n                in_channels=self.char_embed_dim,\n                out_channels=num_filters,\n                kernel_size=kernel_size,\n                padding=kernel_size,\n                bias=True,\n            )\n\n            weight = self.npz_weights[""W_cnn_{}"".format(i)]\n            bias = self.npz_weights[""b_cnn_{}"".format(i)]\n\n            w_reshaped = np.transpose(weight.squeeze(axis=0), axes=(2, 1, 0))\n            if w_reshaped.shape != tuple(conv.weight.data.shape):\n                raise ValueError(""Invalid weight file"")\n\n            # Load the pretrained weights and bias. In order to ensure the\n            # norm of the weights match up with the rest of the model, we need\n            # to normalize the pretrained weights. Here we divide by kernel_size\n            conv.weight.data.copy_(\n                torch.div(torch.FloatTensor(w_reshaped), (kernel_size * 1.0))\n            )\n            conv.bias.data.copy_(\n                torch.div(torch.FloatTensor(bias), (kernel_size * 1.0))\n            )\n\n            conv.weight.requires_grad = self._finetune_pretrained_weights\n            conv.bias.requires_grad = self._finetune_pretrained_weights\n\n            convolutions.append(nn.Sequential(conv))\n        self.convolutions = nn.ModuleList(convolutions)\n\n    def _load_highway(self):\n        """"""\n        Function to load the weights associated with the pretrained highway\n        network. In order to ensure the norm of the weights match up with the\n        rest of the model, we need to normalize the pretrained weights.\n        Here we divide by a fixed constant.\n        """"""\n        # the highway layers have same dimensionality as the number of cnn filters\n        input_dim = sum(f[0] for f in self.convolutions_params)\n\n        highway_layers = []\n        for k in range(self.num_highway_layers):\n            highway_layer = HighwayLayer(input_dim)\n\n            # Update the transform params\n            w_transform = np.transpose(self.npz_weights[""W_transform_{}"".format(k)])\n            b_transform = self.npz_weights[""b_transform_{}"".format(k)]\n            highway_layer.highway_transform.weight.data.copy_(\n                torch.div(torch.FloatTensor(w_transform), 6.0)\n            )\n            highway_layer.highway_transform.bias.data.copy_(\n                torch.FloatTensor(b_transform)\n            )\n            highway_layer.highway_transform.weight.requires_grad = (\n                self._finetune_pretrained_weights\n            )\n            highway_layer.highway_transform.bias.requires_grad = (\n                self._finetune_pretrained_weights\n            )\n\n            # Update the carry weights\n            w_carry = np.transpose(self.npz_weights[""W_carry_{}"".format(k)])\n            highway_layer.highway_gate.weight.data.copy_(\n                torch.div(torch.FloatTensor(w_carry), 6.0)\n            )\n            highway_layer.highway_gate.weight.requires_grad = (\n                self._finetune_pretrained_weights\n            )\n            b_carry = self.npz_weights[""b_carry_{}"".format(k)]\n            highway_layer.highway_gate.bias.data.copy_(torch.FloatTensor(b_carry))\n            highway_layer.highway_gate.bias.requires_grad = (\n                self._finetune_pretrained_weights\n            )\n\n        highway_layers.append(highway_layer)\n        self.highway_layers = nn.ModuleList(highway_layers)\n\n    def _load_projection(self):\n        """"""\n        Function to load the weights associated with the pretrained projection\n        layer. In order to ensure the norm of the weights match up with the\n        rest of the model, we need to normalize the pretrained weights.\n        Here we divide by a fixed constant.\n        """"""\n        input_dim = self.filter_dims\n\n        self.projection = nn.Linear(input_dim, self.char_cnn_output_dim, bias=True)\n        weight = self.npz_weights[""W_proj""]\n        bias = self.npz_weights[""b_proj""]\n        self.projection.weight.data.copy_(\n            torch.div(torch.FloatTensor(np.transpose(weight)), 10.0)\n        )\n        self.projection.bias.data.copy_(\n            torch.div(torch.FloatTensor(np.transpose(bias)), 10.0)\n        )\n\n        self.projection.weight.requires_grad = self._finetune_pretrained_weights\n        self.projection.bias.requires_grad = self._finetune_pretrained_weights\n\n    def forward(self, char_inds_flat):\n        x = self.embed_chars(char_inds_flat)\n        encoder_padding_mask = char_inds_flat.eq(self.padding_idx)\n        char_lengths = torch.sum(~encoder_padding_mask, dim=0)\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        kernel_outputs = []\n        for conv in self.convolutions:\n            if encoder_padding_mask is not None:\n                x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n            # conv input: [total_words, char_emb_dim, seq_len]\n            # conv output: [total_words, in_channel_dim, seq_len]\n            conv_output = conv(x.permute(1, 2, 0))\n            kernel_outputs.append(conv_output)\n        # Pooling over the entire seq\n        pools = [self.pooling(conv, char_lengths, dim=2) for conv in kernel_outputs]\n        # [total_words, sum(output_channel_dim)]\n        encoder_output = torch.cat([p for p in pools], 1)\n\n        for highway_layer in self.highway_layers:\n            encoder_output = highway_layer(encoder_output)\n\n        if self.char_cnn_output_dim != -1:\n            encoder_output = self.projection(encoder_output)\n\n        # (total_words, output_dim)\n        return encoder_output\n\n    def pooling(self, inputs, char_lengths, dim):\n        return torch.max(inputs, dim=dim)[0]\n\n\nclass CharRNNModel(nn.Module):\n    """"""Bi-LSTM over characters to produce a word embedding from characters""""""\n\n    def __init__(\n        self, dictionary, num_chars, char_embed_dim, char_rnn_units, char_rnn_layers\n    ):\n        super().__init__()\n        self.num_chars = num_chars\n        self.padding_idx = dictionary.pad()\n        self.embed_chars = rnn.Embedding(\n            num_embeddings=num_chars,\n            embedding_dim=char_embed_dim,\n            padding_idx=self.padding_idx,\n            freeze_embed=False,\n        )\n\n        assert (\n            char_rnn_units % 2 == 0\n        ), ""char_rnn_units must be even (to be divided evenly between directions)""\n        self.char_lstm_encoder = rnn.LSTMSequenceEncoder.LSTM(\n            char_embed_dim,\n            char_rnn_units // 2,\n            num_layers=char_rnn_layers,\n            bidirectional=True,\n        )\n\n        self.onnx_export_model = False\n\n    def forward(self, src_tokens, src_lengths, char_inds, word_lengths):\n        # char_inds has shape (batch_size, max_words_per_sent, max_word_len)\n        bsz, seqlen, maxchars = char_inds.size()\n\n        if self.onnx_export_model:\n            assert bsz == 1\n            maxchars_tensor = torch.onnx.operators.shape_as_tensor(char_inds)[2]\n            char_inds_flat_shape = torch.cat(\n                (torch.LongTensor([-1]), maxchars_tensor.view(1))\n            )\n            char_inds_flat = torch.onnx.operators.reshape_from_tensor_shape(\n                char_inds, char_inds_flat_shape\n            ).t()\n            char_rnn_input = self.embed_chars(char_inds_flat)\n            packed_char_input = pack_padded_sequence(\n                char_rnn_input, word_lengths.view(-1)\n            )\n        else:\n            # shape (batch_size, max_words_per_sent)\n            nonzero_word_locations = word_lengths > 0\n\n            # (total_words,)\n            word_lengths_flat = word_lengths[nonzero_word_locations]\n\n            # (max_word_length, total_words)\n            char_inds_flat = char_inds[nonzero_word_locations].t()\n\n            # inputs to RNN must be in descending order of length\n            sorted_word_lengths, word_length_order = torch.sort(\n                word_lengths_flat, descending=True\n            )\n\n            char_rnn_input = self.embed_chars(char_inds_flat[:, word_length_order])\n\n            packed_char_input = pack_padded_sequence(\n                char_rnn_input, sorted_word_lengths\n            )\n\n        # h_last shape: (num_layers * num_directions, batch_size, hidden_dim)\n        _, (h_last, _) = self.char_lstm_encoder(packed_char_input)\n\n        # take last-layer output only (shape: (total_words, hidden_dim))\n        # concatenating forward and backward outputs at end/beginning of words\n        char_rnn_output = torch.cat((h_last[-2, :, :], h_last[-1, :, :]), dim=1)\n\n        if self.onnx_export_model:\n            # (seqlen, bsz==1, char_rnn_units)\n            x = char_rnn_output.unsqueeze(1)\n        else:\n            # ""unsort"" (total_words, char_rnn_units)\n            _, inverted_word_length_order = torch.sort(word_length_order)\n            unsorted_rnn_output = char_rnn_output[inverted_word_length_order, :]\n\n            x = char_rnn_output.new(bsz, seqlen, unsorted_rnn_output.shape[1])\n            x[nonzero_word_locations] = unsorted_rnn_output\n            x = x.transpose(0, 1)  # (seqlen, bsz, char_rnn_units)\n\n        return x\n\n    def prepare_for_onnx_export_(self, **kwargs):\n        self.onnx_export_model = True\n'"
pytorch_translate/char_source_hybrid.py,5,"b'#!/usr/bin/env python3\n\nimport logging\nimport math\nfrom ast import literal_eval\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.models import (\n    FairseqEncoder,\n    register_model,\n    register_model_architecture,\n    transformer as fairseq_transformer,\n)\nfrom fairseq.modules import SinusoidalPositionalEmbedding\nfrom pytorch_translate import (\n    char_encoder,\n    char_source_model,\n    hybrid_transformer_rnn,\n    model_constants,\n    rnn,\n    transformer as pytorch_translate_transformer,\n    utils,\n    vocab_constants,\n)\nfrom pytorch_translate.common_layers import (\n    TransformerEncoderGivenEmbeddings,\n    VariableTracker,\n)\nfrom pytorch_translate.data.dictionary import TAGS\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(""char_source_hybrid"")\nclass CharSourceHybridModel(hybrid_transformer_rnn.HybridTransformerRNNModel):\n    """"""\n    An architecture combining hybrid Transformer/RNN with character-based\n    inputs (token embeddings created via character-input CNN)\n    """"""\n\n    def __init__(self, task, encoder, decoder):\n        super().__init__(task, encoder, decoder)\n\n    @staticmethod\n    def add_args(parser):\n        hybrid_transformer_rnn.HybridTransformerRNNModel.add_args(parser)\n        parser.add_argument(\n            ""--char-embed-dim"",\n            type=int,\n            default=128,\n            metavar=""N"",\n            help=(""Character embedding dimension.""),\n        )\n        parser.add_argument(\n            ""--char-cnn-params"",\n            type=str,\n            metavar=""EXPR"",\n            help=(""String experission, [(dim, kernel_size), ...].""),\n        )\n        parser.add_argument(\n            ""--char-cnn-nonlinear-fn"",\n            type=str,\n            default=""tanh"",\n            metavar=""EXPR"",\n            help=(""Nonlinearity applied to char conv outputs. Values: relu, tanh.""),\n        )\n        parser.add_argument(\n            ""--char-cnn-num-highway-layers"",\n            type=int,\n            default=0,\n            metavar=""N"",\n            help=(""Char cnn encoder highway layers.""),\n        )\n        parser.add_argument(\n            ""--char-cnn-output-dim"",\n            type=int,\n            default=-1,\n            metavar=""N"",\n            help=""Output dim of the CNN layer. If set to -1, this is computed ""\n            ""from char-cnn-params."",\n        )\n        parser.add_argument(\n            ""--use-pretrained-weights"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=False,\n            help=""Use pretrained weights for the character model including ""\n            ""the char embeddings, CNN filters, highway networks"",\n        )\n        parser.add_argument(\n            ""--finetune-pretrained-weights"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=False,\n            help=""Boolean flag to specify whether or not to update the ""\n            ""pretrained weights as part of training"",\n        )\n        parser.add_argument(\n            ""--pretrained-weights-file"",\n            type=str,\n            default="""",\n            help=(""Weights file for loading pretrained weights""),\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        src_dict, dst_dict = task.source_dictionary, task.target_dictionary\n        base_architecture(args)\n\n        assert hasattr(args, ""char_source_dict_size""), (\n            ""args.char_source_dict_size required. ""\n            ""should be set by load_binarized_dataset()""\n        )\n\n        assert hasattr(\n            args, ""char_cnn_params""\n        ), ""Only char CNN is supported for the char encoder hybrid model""\n\n        args.embed_bytes = getattr(args, ""embed_bytes"", False)\n\n        # In case use_pretrained_weights is true, verify the model params\n        # are correctly set\n        if args.embed_bytes and getattr(args, ""use_pretrained_weights"", False):\n            char_source_model.verify_pretrain_params(args)\n\n        encoder = CharSourceHybridModel.build_encoder(args=args, src_dict=src_dict)\n        decoder = CharSourceHybridModel.build_decoder(\n            args=args, src_dict=src_dict, dst_dict=dst_dict\n        )\n\n        return cls(task, encoder, decoder)\n\n    def forward(\n        self, src_tokens, src_lengths, char_inds, word_lengths, prev_output_tokens\n    ):\n        """"""\n        Overriding FairseqEncoderDecoderModel.forward() due to different encoder\n        inputs.\n        """"""\n        encoder_out = self.encoder(src_tokens, src_lengths, char_inds, word_lengths)\n        decoder_out = self.decoder(prev_output_tokens, encoder_out)\n        return decoder_out\n\n    @classmethod\n    def build_encoder(cls, args, src_dict):\n        # If we embed bytes then the number of indices is fixed and does not\n        # depend on the dictionary\n        if args.embed_bytes:\n            num_chars = vocab_constants.NUM_BYTE_INDICES + TAGS.__len__() + 1\n        else:\n            num_chars = args.char_source_dict_size\n\n        encoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=src_dict,\n            embed_dim=args.encoder_embed_dim,\n            path=args.encoder_pretrained_embed,\n            freeze=args.encoder_freeze_embed,\n        )\n        return CharCNNEncoder(\n            args,\n            src_dict,\n            encoder_embed_tokens,\n            num_chars=num_chars,\n            embed_dim=args.char_embed_dim,\n            char_cnn_params=args.char_cnn_params,\n            char_cnn_nonlinear_fn=args.char_cnn_nonlinear_fn,\n            char_cnn_num_highway_layers=args.char_cnn_num_highway_layers,\n            char_cnn_output_dim=getattr(args, ""char_cnn_output_dim"", -1),\n            use_pretrained_weights=getattr(args, ""use_pretrained_weights"", False),\n            finetune_pretrained_weights=getattr(\n                args, ""finetune_pretrained_weights"", False\n            ),\n            weights_file=getattr(args, ""pretrained_weights_file"", """"),\n        )\n\n    @classmethod\n    def build_decoder(cls, args, src_dict, dst_dict):\n        decoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=dst_dict,\n            embed_dim=args.decoder_embed_dim,\n            path=args.decoder_pretrained_embed,\n            freeze=args.decoder_freeze_embed,\n        )\n        return hybrid_transformer_rnn.HybridRNNDecoder(\n            args, src_dict, dst_dict, decoder_embed_tokens\n        )\n\n\nclass CharCNNEncoder(FairseqEncoder):\n    """"""\n    Character-level CNN encoder to generate word representations, as input to\n    transformer encoder.\n    """"""\n\n    def __init__(\n        self,\n        args,\n        dictionary,\n        embed_tokens,\n        num_chars=50,\n        embed_dim=32,\n        char_cnn_params=""[(128, 3), (128, 5)]"",\n        char_cnn_nonlinear_fn=""tanh"",\n        char_cnn_num_highway_layers=0,\n        char_cnn_output_dim=-1,\n        use_pretrained_weights=False,\n        finetune_pretrained_weights=False,\n        weights_file=None,\n    ):\n        super().__init__(dictionary)\n\n        convolutions_params = literal_eval(char_cnn_params)\n        self.char_cnn_encoder = char_encoder.CharCNNModel(\n            dictionary,\n            num_chars,\n            embed_dim,\n            convolutions_params,\n            char_cnn_nonlinear_fn,\n            char_cnn_num_highway_layers,\n            char_cnn_output_dim,\n            use_pretrained_weights,\n            finetune_pretrained_weights,\n            weights_file,\n        )\n\n        self.embed_tokens = embed_tokens\n        token_embed_dim = embed_tokens.embedding_dim\n        self.word_layer_norm = nn.LayerNorm(token_embed_dim)\n\n        char_embed_dim = (\n            char_cnn_output_dim\n            if char_cnn_output_dim != -1\n            else sum(out_dim for (out_dim, _) in convolutions_params)\n        )\n        self.char_layer_norm = nn.LayerNorm(char_embed_dim)\n        self.word_dim = char_embed_dim + token_embed_dim\n        self.char_scale = math.sqrt(char_embed_dim / self.word_dim)\n        self.word_scale = math.sqrt(token_embed_dim / self.word_dim)\n        if self.word_dim != args.encoder_embed_dim:\n            self.word_to_transformer_embed = fairseq_transformer.Linear(\n                self.word_dim, args.encoder_embed_dim\n            )\n\n        self.dropout = args.dropout\n\n        self.padding_idx = dictionary.pad()\n        self.embed_positions = fairseq_transformer.PositionalEmbedding(\n            1024,\n            args.encoder_embed_dim,\n            self.padding_idx,\n            learned=args.encoder_learned_pos,\n        )\n\n        self.transformer_encoder_given_embeddings = TransformerEncoderGivenEmbeddings(\n            args=args, proj_to_decoder=False\n        )\n\n        # Variable tracker\n        self.tracker = VariableTracker()\n        # Initialize adversarial mode\n        self.set_gradient_tracking_mode(False)\n        self.set_embed_noising_mode(False)\n\n        # disables sorting and word-length thresholding if True\n        # (enables ONNX tracing of length-sorted input with batch_size = 1)\n        self.onnx_export_model = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_export_model = True\n\n    def set_gradient_tracking_mode(self, mode=True):\n        """""" This allows AdversarialTrainer to turn on retrain_grad when\n        running adversarial example generation model.""""""\n        self.tracker.reset()\n        self.track_gradients = mode\n\n    def set_embed_noising_mode(self, mode=True):\n        """"""This allows adversarial trainer to turn on and off embedding noising\n        layers. In regular training, this mode is off, and it is not included\n        in forward pass.\n        """"""\n        self.embed_noising_mode = mode\n\n    def forward(self, src_tokens, src_lengths, char_inds, word_lengths):\n        self.tracker.reset()\n        # char_inds has shape (batch_size, max_words_per_sent, max_word_len)\n        bsz, seqlen, maxchars = char_inds.size()\n        # char_cnn_encoder takes input (max_word_length, total_words)\n        char_inds_flat = char_inds.view(-1, maxchars).t()\n        # output (total_words, encoder_dim)\n        char_cnn_output = self.char_cnn_encoder(char_inds_flat)\n        x = char_cnn_output.view(bsz, seqlen, char_cnn_output.shape[-1])\n        x = x.transpose(0, 1)  # (seqlen, bsz, char_cnn_output_dim)\n        x = self.char_layer_norm(x)\n        x = self.char_scale * x\n\n        embedded_tokens = self.embed_tokens(src_tokens)\n        # (seqlen, bsz, token_embed_dim)\n        embedded_tokens = embedded_tokens.transpose(0, 1)\n        embedded_tokens = self.word_layer_norm(embedded_tokens)\n        embedded_tokens = self.word_scale * embedded_tokens\n        x = torch.cat([x, embedded_tokens], dim=2)\n\n        self.tracker.track(x, ""token_embeddings"", retain_grad=self.track_gradients)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.word_to_transformer_embed is not None:\n            x = self.word_to_transformer_embed(x)\n        positions = self.embed_positions(src_tokens)\n        x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        # compute padding mask (B x T)\n        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        x = self.transformer_encoder_given_embeddings(\n            x=x, positions=positions, encoder_padding_mask=encoder_padding_mask\n        )\n\n        # tracing requires a tensor value\n        if self.onnx_export_model and encoder_padding_mask is None:\n            encoder_padding_mask = torch.Tensor([]).type_as(src_tokens)\n\n        return x, src_tokens, encoder_padding_mask\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        (x, src_tokens, encoder_padding_mask) = encoder_out\n        if x is not None:\n            x = x.index_select(1, new_order)\n        if src_tokens is not None:\n            src_tokens = src_tokens.index_select(0, new_order)\n        if encoder_padding_mask is not None:\n            encoder_padding_mask = encoder_padding_mask.index_select(0, new_order)\n        return (x, src_tokens, encoder_padding_mask)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return self.embed_positions.max_positions\n\n    def upgrade_state_dict(self, state_dict):\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            if ""encoder.embed_positions.weights"" in state_dict:\n                del state_dict[""encoder.embed_positions.weights""]\n            state_dict[""encoder.embed_positions._float_tensor""] = torch.FloatTensor(1)\n        return state_dict\n\n\n@register_model_architecture(""char_source_hybrid"", ""char_source_hybrid"")\ndef base_architecture(args):\n    # default architecture\n    hybrid_transformer_rnn.base_architecture(args)\n    args.char_cnn_params = getattr(args, ""char_cnn_params"", ""[(50, 1), (100,2)]"")\n    args.char_cnn_nonlinear_fn = getattr(args, ""chr_cnn_nonlinear_fn"", ""relu"")\n    args.char_cnn_num_highway_layers = getattr(args, ""char_cnn_num_highway_layers"", ""2"")\n'"
pytorch_translate/char_source_model.py,5,"b'#!/usr/bin/env python3\n\nimport logging\nfrom ast import literal_eval\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq.models import FairseqEncoder, register_model, register_model_architecture\nfrom pytorch_translate import char_encoder, model_constants, rnn, utils, vocab_constants\nfrom pytorch_translate.common_layers import Embedding, VariableTracker\nfrom pytorch_translate.data.dictionary import TAGS\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(""char_source"")\nclass CharSourceModel(rnn.RNNModel):\n    def __init__(self, task, encoder, decoder):\n        super().__init__(task, encoder, decoder)\n\n    @staticmethod\n    def add_args(parser):\n        rnn.RNNModel.add_args(parser)\n        char_encoder.add_args(parser)\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        src_dict, dst_dict = task.source_dictionary, task.target_dictionary\n        base_architecture(args)\n\n        assert args.sequence_lstm, ""CharRNNModel only supports sequence_lstm""\n        assert args.cell_type == ""lstm"", ""CharRNNModel only supports cell_type lstm""\n\n        assert hasattr(args, ""char_source_dict_size""), (\n            ""args.char_source_dict_size required. ""\n            ""should be set by load_binarized_dataset()""\n        )\n\n        if hasattr(args, ""char_cnn_params""):\n            args.embed_bytes = getattr(args, ""embed_bytes"", False)\n\n            # If we embed bytes then the number of indices is fixed and does not\n            # depend on the dictionary\n            if args.embed_bytes:\n                num_chars = vocab_constants.NUM_BYTE_INDICES + TAGS.__len__() + 1\n            else:\n                num_chars = args.char_source_dict_size\n\n            # In case use_pretrained_weights is true, verify the model params\n            # are correctly set\n            if args.embed_bytes and getattr(args, ""use_pretrained_weights"", False):\n                verify_pretrain_params(args)\n            encoder = CharCNNEncoder(\n                src_dict,\n                num_chars=num_chars,\n                unk_only_char_encoding=args.unk_only_char_encoding,\n                embed_dim=args.char_embed_dim,\n                token_embed_dim=args.encoder_embed_dim,\n                freeze_embed=args.encoder_freeze_embed,\n                normalize_embed=args.encoder_normalize_embed,\n                char_cnn_params=args.char_cnn_params,\n                char_cnn_nonlinear_fn=args.char_cnn_nonlinear_fn,\n                char_cnn_num_highway_layers=args.char_cnn_num_highway_layers,\n                char_cnn_output_dim=getattr(args, ""char_cnn_output_dim"", -1),\n                num_layers=args.encoder_layers,\n                hidden_dim=args.encoder_hidden_dim,\n                dropout_in=args.encoder_dropout_in,\n                dropout_out=args.encoder_dropout_out,\n                residual_level=args.residual_level,\n                bidirectional=bool(args.encoder_bidirectional),\n                use_pretrained_weights=getattr(args, ""use_pretrained_weights"", False),\n                finetune_pretrained_weights=getattr(\n                    args, ""finetune_pretrained_weights"", False\n                ),\n                weights_file=getattr(args, ""pretrained_weights_file"", """"),\n            )\n        else:\n            assert (\n                args.unk_only_char_encoding is False\n            ), ""unk_only_char_encoding should be False when using CharRNNEncoder""\n\n            encoder = CharRNNEncoder(\n                src_dict,\n                num_chars=args.char_source_dict_size,\n                char_embed_dim=args.char_embed_dim,\n                token_embed_dim=args.encoder_embed_dim,\n                normalize_embed=args.encoder_normalize_embed,\n                char_rnn_units=args.char_rnn_units,\n                char_rnn_layers=args.char_rnn_layers,\n                num_layers=args.encoder_layers,\n                hidden_dim=args.encoder_hidden_dim,\n                dropout_in=args.encoder_dropout_in,\n                dropout_out=args.encoder_dropout_out,\n                residual_level=args.residual_level,\n                bidirectional=bool(args.encoder_bidirectional),\n            )\n\n        decoder_embed_tokens = Embedding(\n            num_embeddings=len(dst_dict),\n            embedding_dim=args.decoder_embed_dim,\n            padding_idx=dst_dict.pad(),\n            freeze_embed=args.decoder_freeze_embed,\n        )\n\n        utils.load_embedding(\n            embedding=decoder_embed_tokens,\n            dictionary=dst_dict,\n            pretrained_embed=args.decoder_pretrained_embed,\n        )\n        decoder = rnn.RNNDecoder(\n            src_dict=src_dict,\n            dst_dict=dst_dict,\n            embed_tokens=decoder_embed_tokens,\n            vocab_reduction_params=args.vocab_reduction_params,\n            encoder_hidden_dim=args.encoder_hidden_dim,\n            embed_dim=args.decoder_embed_dim,\n            out_embed_dim=args.decoder_out_embed_dim,\n            cell_type=args.cell_type,\n            num_layers=args.decoder_layers,\n            hidden_dim=args.decoder_hidden_dim,\n            attention_type=args.attention_type,\n            dropout_in=args.decoder_dropout_in,\n            dropout_out=args.decoder_dropout_out,\n            residual_level=args.residual_level,\n            averaging_encoder=args.averaging_encoder,\n        )\n        return cls(task, encoder, decoder)\n\n    def forward(\n        self, src_tokens, src_lengths, char_inds, word_lengths, prev_output_tokens\n    ):\n        """"""\n        Overriding FairseqEncoderDecoderModel.forward() due to different encoder\n        inputs.\n        """"""\n        encoder_out = self.encoder(src_tokens, src_lengths, char_inds, word_lengths)\n        decoder_out = self.decoder(prev_output_tokens, encoder_out)\n        return decoder_out\n\n\nclass CharRNNEncoder(FairseqEncoder):\n    """"""\n    RNN encoder encoding each word via a bidirectional LSTM over character\n    embeddings to obtain word representations, and then an LSTM (optionally\n    bidirectional in first layer) to combine word representations.\n\n    Uses nn.LSTM for cuDNN support / ONNX exportability.\n    """"""\n\n    def __init__(\n        self,\n        dictionary,\n        num_chars,\n        char_embed_dim,\n        token_embed_dim,\n        normalize_embed,\n        char_rnn_units,\n        char_rnn_layers,\n        hidden_dim,\n        num_layers,\n        dropout_in,\n        dropout_out,\n        residual_level,\n        bidirectional,\n    ):\n        super().__init__(dictionary)\n        self.dropout_in = dropout_in\n\n        self.embed_chars = char_encoder.CharRNNModel(\n            dictionary=dictionary,\n            num_chars=num_chars,\n            char_embed_dim=char_embed_dim,\n            char_rnn_units=char_rnn_units,\n            char_rnn_layers=char_rnn_layers,\n        )\n\n        self.embed_tokens = None\n        if token_embed_dim > 0:\n            self.embed_tokens = rnn.Embedding(\n                num_embeddings=len(dictionary),\n                embedding_dim=token_embed_dim,\n                padding_idx=dictionary.pad(),\n                freeze_embed=False,\n                normalize_embed=normalize_embed,\n            )\n\n        self.word_dim = char_rnn_units + token_embed_dim\n\n        self.bilstm = rnn.BiLSTM(\n            num_layers=num_layers,\n            bidirectional=bidirectional,\n            embed_dim=self.word_dim,\n            hidden_dim=hidden_dim,\n            dropout=dropout_out,\n            residual_level=residual_level,\n        )\n\n        # disables sorting and word-length thresholding if True\n        # (enables ONNX tracing of length-sorted input with batch_size = 1)\n        self.onnx_export_model = False\n\n    def forward(self, src_tokens, src_lengths, char_inds, word_lengths):\n        x = self.embed_chars(\n            src_tokens=src_tokens,\n            src_lengths=src_lengths,\n            char_inds=char_inds,\n            word_lengths=word_lengths,\n        )\n\n        if self.embed_tokens is not None:\n            embedded_tokens = self.embed_tokens(src_tokens)\n\n            # (seqlen, bsz, token_embed_dim)\n            embedded_tokens = embedded_tokens.transpose(0, 1)\n\n            # (seqlen, bsz, total_word_embed_dim)\n            x = torch.cat([x, embedded_tokens], dim=2)\n\n        if self.dropout_in != 0:\n            x = F.dropout(x, p=self.dropout_in, training=self.training)\n\n        embedded_words = x\n\n        unpacked_output, final_hiddens, final_cells = self.bilstm(\n            embeddings=x, lengths=src_lengths\n        )\n\n        return (\n            unpacked_output,\n            final_hiddens,\n            final_cells,\n            src_lengths,\n            src_tokens,\n            embedded_words,\n        )\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""Reorder all outputs according to new_order.""""""\n        return rnn.reorder_encoder_output(encoder_out, new_order)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n\nclass CharCNNEncoder(FairseqEncoder):\n    """"""\n    Character-level CNN encoder to generate word representations, as input to\n    RNN encoder.\n    """"""\n\n    def __init__(\n        self,\n        dictionary,\n        num_chars=50,\n        unk_only_char_encoding=False,\n        embed_dim=32,\n        token_embed_dim=256,\n        freeze_embed=False,\n        normalize_embed=False,\n        char_cnn_params=""[(128, 3), (128, 5)]"",\n        char_cnn_nonlinear_fn=""tanh"",\n        char_cnn_num_highway_layers=0,\n        char_cnn_output_dim=-1,\n        hidden_dim=512,\n        num_layers=1,\n        dropout_in=0.1,\n        dropout_out=0.1,\n        residual_level=None,\n        bidirectional=False,\n        use_pretrained_weights=False,\n        finetune_pretrained_weights=False,\n        weights_file=None,\n    ):\n        super().__init__(dictionary)\n        self.dropout_in = dropout_in\n\n        convolutions_params = literal_eval(char_cnn_params)\n        self.char_cnn_encoder = char_encoder.CharCNNModel(\n            dictionary,\n            num_chars,\n            embed_dim,\n            convolutions_params,\n            char_cnn_nonlinear_fn,\n            char_cnn_num_highway_layers,\n            char_cnn_output_dim,\n            use_pretrained_weights,\n            finetune_pretrained_weights,\n            weights_file,\n        )\n\n        self.embed_tokens = None\n        num_tokens = len(dictionary)\n        self.padding_idx = dictionary.pad()\n        self.unk_idx = dictionary.unk()\n        if token_embed_dim > 0:\n            self.embed_tokens = rnn.Embedding(\n                num_embeddings=num_tokens,\n                embedding_dim=token_embed_dim,\n                padding_idx=self.padding_idx,\n                freeze_embed=freeze_embed,\n                normalize_embed=normalize_embed,\n            )\n        self.word_dim = (\n            char_cnn_output_dim\n            if char_cnn_output_dim != -1\n            else sum(out_dim for (out_dim, _) in convolutions_params)\n        )\n        self.token_embed_dim = token_embed_dim\n\n        self.unk_only_char_encoding = unk_only_char_encoding\n        if self.unk_only_char_encoding:\n            assert char_cnn_output_dim == token_embed_dim, (\n                ""char_cnn_output_dim (%d) must equal to token_embed_dim (%d)""\n                % (char_cnn_output_dim, token_embed_dim)\n            )\n            self.word_dim = token_embed_dim\n        else:\n            self.word_dim = self.word_dim + token_embed_dim\n\n        self.bilstm = rnn.BiLSTM(\n            num_layers=num_layers,\n            bidirectional=bidirectional,\n            embed_dim=self.word_dim,\n            hidden_dim=hidden_dim,\n            dropout=dropout_out,\n            residual_level=residual_level,\n        )\n\n        # Variable tracker\n        self.tracker = VariableTracker()\n        # Initialize adversarial mode\n        self.set_gradient_tracking_mode(False)\n        self.set_embed_noising_mode(False)\n\n    def set_gradient_tracking_mode(self, mode=True):\n        """""" This allows AdversarialTrainer to turn on retrain_grad when\n        running adversarial example generation model.""""""\n        self.tracker.reset()\n        self.track_gradients = mode\n\n    def set_embed_noising_mode(self, mode=True):\n        """"""This allows adversarial trainer to turn on and off embedding noising\n        layers. In regular training, this mode is off, and it is not included\n        in forward pass.\n        """"""\n        self.embed_noising_mode = mode\n\n    def forward(self, src_tokens, src_lengths, char_inds, word_lengths):\n        self.tracker.reset()\n        # char_inds has shape (batch_size, max_words_per_sent, max_word_len)\n        bsz, seqlen, maxchars = char_inds.size()\n        # char_cnn_encoder takes input (max_word_length, total_words)\n        char_inds_flat = char_inds.view(-1, maxchars)  # .t()\n        # output (total_words, encoder_dim)\n        if self.unk_only_char_encoding:\n            assert (\n                self.embed_tokens is not None\n            ), ""token_embed_dim should > 0 when unk_only_char_encoding is true!""\n\n            unk_masks = (src_tokens == self.unk_idx).view(-1)\n            unk_indices = torch.nonzero(unk_masks).squeeze()\n            if unk_indices.dim() > 0 and unk_indices.size(0) > 0:\n                char_inds_flat = torch.index_select(char_inds_flat, 0, unk_indices)\n                char_inds_flat = char_inds_flat.view(-1, maxchars)\n            else:\n                char_inds_flat = None\n\n        if char_inds_flat is not None:\n            # (bsz * seqlen, encoder_dim)\n            char_cnn_output = self.char_cnn_encoder(char_inds_flat.t())\n            x = char_cnn_output\n        else:  # charCNN is not needed\n            x = None\n\n        if self.embed_tokens is not None:\n            # (bsz, seqlen, token_embed_dim)\n            embedded_tokens = self.embed_tokens(src_tokens)\n            # (bsz * seqlen, token_embed_dim)\n            embedded_tokens = embedded_tokens.view(-1, self.token_embed_dim)\n            if self.unk_only_char_encoding:  # charCNN for UNK words only\n                if x is not None:\n                    x = embedded_tokens.index_copy(0, unk_indices, x)\n                else:  # no UNK, so charCNN is not needed\n                    x = embedded_tokens\n            else:  # charCNN for all words\n                x = torch.cat([x, embedded_tokens], dim=1)\n\n        # (bsz, seqlen, x.shape[-1])\n        x = x.view(bsz, seqlen, x.shape[-1])\n        # (seqlen, bsz, x.shape[-1])\n        x = x.transpose(0, 1)\n\n        self.tracker.track(x, ""token_embeddings"", retain_grad=self.track_gradients)\n        if self.dropout_in != 0:\n            x = F.dropout(x, p=self.dropout_in, training=self.training)\n        embedded_words = x\n\n        unpacked_output, final_hiddens, final_cells = self.bilstm(\n            embeddings=x, lengths=src_lengths\n        )\n\n        return (\n            unpacked_output,\n            final_hiddens,\n            final_cells,\n            src_lengths,\n            src_tokens,\n            embedded_words,\n        )\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""Reorder all outputs according to new_order.""""""\n        return rnn.reorder_encoder_output(encoder_out, new_order)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n\ndef verify_pretrain_params(args):\n    """"""\n    Function which verifies the model params in case we are using a pretrained\n    model like ELMo. This is needed because the model params must match up\n    with the weights that are being loaded.\n    """"""\n    assert (\n        args.embed_bytes\n    ), ""To use pretrained weights, embed_bytes must be set to True.""\n\n    assert (\n        args.char_cnn_nonlinear_fn == model_constants.PRETRAINED_CHAR_CNN_NONLINEAR_FN\n    ), ""To use pretrained weights, the non linearity used should be relu.""\n\n    assert (\n        args.char_embed_dim == model_constants.PRETRAINED_CHAR_EMBED_DIM\n    ), ""To use pretrained weights char_embed_dim should be set to 16.""\n\n    assert (\n        args.char_cnn_output_dim == model_constants.PRETRAINED_CHAR_CNN_OUTPUT_DIM\n    ), ""To use pretrained weights, the output dim of the CNN layer should be 512.""\n    assert (\n        literal_eval(args.char_cnn_params) == model_constants.PRETRAINED_CHAR_CNN_PARAMS\n    ), ""CNN Params don\'t match with the ones needed for loading pretrained weights""\n\n\n@register_model_architecture(""char_source"", ""char_source"")\ndef base_architecture(args):\n    # default architecture\n    rnn.base_architecture(args)\n    args.char_rnn_units = getattr(args, ""char_embed_dim"", 128)\n    args.char_rnn_units = getattr(args, ""char_rnn_units"", 256)\n    args.char_rnn_layers = getattr(args, ""char_rnn_layers"", 1)\n'"
pytorch_translate/char_source_transformer_model.py,5,"b'#!/usr/bin/env python3\n\nimport logging\nimport math\nfrom ast import literal_eval\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.models import (\n    FairseqEncoder,\n    register_model,\n    register_model_architecture,\n    transformer as fairseq_transformer,\n)\nfrom fairseq.modules import SinusoidalPositionalEmbedding\nfrom pytorch_translate import char_encoder, transformer, utils, vocab_constants\nfrom pytorch_translate.char_source_model import verify_pretrain_params\nfrom pytorch_translate.common_layers import (\n    TransformerEncoderGivenEmbeddings,\n    VariableTracker,\n)\nfrom pytorch_translate.data.dictionary import TAGS\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(""char_source_transformer"")\nclass CharSourceTransformerModel(transformer.TransformerModel):\n    def __init__(self, task, encoder, decoder):\n        super().__init__(task, encoder, decoder)\n\n    @staticmethod\n    def add_args(parser):\n        transformer.TransformerModel.add_args(parser)\n        parser.add_argument(\n            ""--char-embed-dim"",\n            type=int,\n            default=128,\n            metavar=""N"",\n            help=(""Character embedding dimension.""),\n        )\n        parser.add_argument(\n            ""--char-cnn-params"",\n            type=str,\n            metavar=""EXPR"",\n            help=(""String experission, [(dim, kernel_size), ...].""),\n        )\n        parser.add_argument(\n            ""--char-cnn-nonlinear-fn"",\n            type=str,\n            default=""tanh"",\n            metavar=""EXPR"",\n            help=(""Nonlinearity applied to char conv outputs. Values: relu, tanh.""),\n        )\n        parser.add_argument(\n            ""--char-cnn-num-highway-layers"",\n            type=int,\n            default=0,\n            metavar=""N"",\n            help=(""Char cnn encoder highway layers.""),\n        )\n        parser.add_argument(\n            ""--char-cnn-output-dim"",\n            type=int,\n            default=-1,\n            metavar=""N"",\n            help=""Output dim of the CNN layer. If set to -1, this is computed ""\n            ""from char-cnn-params."",\n        )\n        parser.add_argument(\n            ""--use-pretrained-weights"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=False,\n            help=""Use pretrained weights for the character model including ""\n            ""the char embeddings, CNN filters, highway networks"",\n        )\n        parser.add_argument(\n            ""--finetune-pretrained-weights"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=False,\n            help=""Boolean flag to specify whether or not to update the ""\n            ""pretrained weights as part of training"",\n        )\n        parser.add_argument(\n            ""--pretrained-weights-file"",\n            type=str,\n            default="""",\n            help=(""Weights file for loading pretrained weights""),\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        src_dict, dst_dict = task.source_dictionary, task.target_dictionary\n        base_architecture(args)\n\n        assert hasattr(args, ""char_source_dict_size""), (\n            ""args.char_source_dict_size required. ""\n            ""should be set by load_binarized_dataset()""\n        )\n\n        if args.share_all_embeddings:\n            if src_dict != dst_dict:\n                raise RuntimeError(\n                    ""--share-all-embeddings requires a joined dictionary""\n                )\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise RuntimeError(\n                    ""--share-all-embeddings requires --encoder-embed-dim ""\n                    ""to match --decoder-embed-dim""\n                )\n            if args.decoder_pretrained_embed and (\n                args.decoder_pretrained_embed != args.encoder_pretrained_embed\n            ):\n                raise RuntimeError(\n                    ""--share-all-embeddings not compatible with ""\n                    ""--decoder-pretrained-embed""\n                )\n            encoder_embed_tokens = transformer.build_embedding(\n                src_dict,\n                args.encoder_embed_dim,\n                args.encoder_pretrained_embed,\n                args.encoder_freeze_embed,\n            )\n            decoder_embed_tokens = encoder_embed_tokens\n            args.share_decoder_input_output_embed = True\n        else:\n            encoder_embed_tokens = transformer.build_embedding(\n                src_dict,\n                args.encoder_embed_dim,\n                args.encoder_pretrained_embed,\n                args.encoder_freeze_embed,\n            )\n            decoder_embed_tokens = transformer.build_embedding(\n                dst_dict,\n                args.decoder_embed_dim,\n                args.decoder_pretrained_embed,\n                args.decoder_freeze_embed,\n            )\n\n        args.embed_bytes = getattr(args, ""embed_bytes"", False)\n\n        # If we embed bytes then the number of indices is fixed and does not\n        # depend on the dictionary\n        if args.embed_bytes:\n            num_chars = vocab_constants.NUM_BYTE_INDICES + TAGS.__len__() + 1\n        else:\n            num_chars = args.char_source_dict_size\n\n        # In case use_pretrained_weights is true, verify the model params\n        # are correctly set\n        if args.embed_bytes and getattr(args, ""use_pretrained_weights"", False):\n            verify_pretrain_params(args)\n\n        encoder = CharCNNEncoder(\n            args,\n            src_dict,\n            encoder_embed_tokens,\n            num_chars=num_chars,\n            embed_dim=args.char_embed_dim,\n            char_cnn_params=args.char_cnn_params,\n            char_cnn_nonlinear_fn=args.char_cnn_nonlinear_fn,\n            char_cnn_num_highway_layers=args.char_cnn_num_highway_layers,\n            char_cnn_output_dim=getattr(args, ""char_cnn_output_dim"", -1),\n            use_pretrained_weights=getattr(args, ""use_pretrained_weights"", False),\n            finetune_pretrained_weights=getattr(\n                args, ""finetune_pretrained_weights"", False\n            ),\n            weights_file=getattr(args, ""pretrained_weights_file"", """"),\n        )\n        decoder = transformer.TransformerDecoder(\n            args=args,\n            src_dict=src_dict,\n            dst_dict=dst_dict,\n            embed_tokens=decoder_embed_tokens,\n        )\n        return cls(task, encoder, decoder)\n\n    def forward(\n        self, src_tokens, src_lengths, char_inds, word_lengths, prev_output_tokens\n    ):\n        """"""\n        Overriding FairseqEncoderDecoderModel.forward() due to different encoder\n        inputs.\n        """"""\n        encoder_out = self.encoder(src_tokens, src_lengths, char_inds, word_lengths)\n        decoder_out = self.decoder(prev_output_tokens, encoder_out)\n        return decoder_out\n\n\nclass CharCNNEncoder(FairseqEncoder):\n    """"""\n    Character-level CNN encoder to generate word representations, as input to\n    transformer encoder.\n    """"""\n\n    def __init__(\n        self,\n        args,\n        dictionary,\n        embed_tokens,\n        num_chars=50,\n        embed_dim=32,\n        char_cnn_params=""[(128, 3), (128, 5)]"",\n        char_cnn_nonlinear_fn=""tanh"",\n        char_cnn_num_highway_layers=0,\n        char_cnn_output_dim=-1,\n        use_pretrained_weights=False,\n        finetune_pretrained_weights=False,\n        weights_file=None,\n    ):\n        super().__init__(dictionary)\n\n        convolutions_params = literal_eval(char_cnn_params)\n        self.char_cnn_encoder = char_encoder.CharCNNModel(\n            dictionary,\n            num_chars,\n            embed_dim,\n            convolutions_params,\n            char_cnn_nonlinear_fn,\n            char_cnn_num_highway_layers,\n            char_cnn_output_dim,\n            use_pretrained_weights,\n            finetune_pretrained_weights,\n            weights_file,\n        )\n\n        self.embed_tokens = embed_tokens\n        token_embed_dim = embed_tokens.embedding_dim\n        self.word_layer_norm = nn.LayerNorm(token_embed_dim)\n\n        char_embed_dim = (\n            char_cnn_output_dim\n            if char_cnn_output_dim != -1\n            else sum(out_dim for (out_dim, _) in convolutions_params)\n        )\n        self.char_layer_norm = nn.LayerNorm(char_embed_dim)\n        self.word_dim = char_embed_dim + token_embed_dim\n        self.char_scale = math.sqrt(char_embed_dim / self.word_dim)\n        self.word_scale = math.sqrt(token_embed_dim / self.word_dim)\n        if self.word_dim != args.encoder_embed_dim:\n            self.word_to_transformer_embed = fairseq_transformer.Linear(\n                self.word_dim, args.encoder_embed_dim\n            )\n\n        self.dropout = args.dropout\n\n        self.padding_idx = dictionary.pad()\n        self.embed_positions = fairseq_transformer.PositionalEmbedding(\n            1024,\n            args.encoder_embed_dim,\n            self.padding_idx,\n            learned=args.encoder_learned_pos,\n        )\n\n        self.transformer_encoder_given_embeddings = TransformerEncoderGivenEmbeddings(\n            args=args, proj_to_decoder=True\n        )\n\n        # Variable tracker\n        self.tracker = VariableTracker()\n        # Initialize adversarial mode\n        self.set_gradient_tracking_mode(False)\n        self.set_embed_noising_mode(False)\n\n        # disables sorting and word-length thresholding if True\n        # (enables ONNX tracing of length-sorted input with batch_size = 1)\n        self.onnx_export_model = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_export_model = True\n\n    def set_gradient_tracking_mode(self, mode=True):\n        """""" This allows AdversarialTrainer to turn on retrain_grad when\n        running adversarial example generation model.""""""\n        self.tracker.reset()\n        self.track_gradients = mode\n\n    def set_embed_noising_mode(self, mode=True):\n        """"""This allows adversarial trainer to turn on and off embedding noising\n        layers. In regular training, this mode is off, and it is not included\n        in forward pass.\n        """"""\n        self.embed_noising_mode = mode\n\n    def forward(self, src_tokens, src_lengths, char_inds, word_lengths):\n        self.tracker.reset()\n        # char_inds has shape (batch_size, max_words_per_sent, max_word_len)\n        bsz, seqlen, maxchars = char_inds.size()\n        # char_cnn_encoder takes input (max_word_length, total_words)\n        char_inds_flat = char_inds.view(-1, maxchars).t()\n        # output (total_words, encoder_dim)\n        char_cnn_output = self.char_cnn_encoder(char_inds_flat)\n        x = char_cnn_output.view(bsz, seqlen, char_cnn_output.shape[-1])\n        x = x.transpose(0, 1)  # (seqlen, bsz, char_cnn_output_dim)\n        x = self.char_layer_norm(x)\n        x = self.char_scale * x\n\n        embedded_tokens = self.embed_tokens(src_tokens)\n        # (seqlen, bsz, token_embed_dim)\n        embedded_tokens = embedded_tokens.transpose(0, 1)\n        embedded_tokens = self.word_layer_norm(embedded_tokens)\n        embedded_tokens = self.word_scale * embedded_tokens\n        x = torch.cat([x, embedded_tokens], dim=2)\n\n        self.tracker.track(x, ""token_embeddings"", retain_grad=self.track_gradients)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.word_to_transformer_embed is not None:\n            x = self.word_to_transformer_embed(x)\n        positions = self.embed_positions(src_tokens)\n        x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        # compute padding mask (B x T)\n        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        x = self.transformer_encoder_given_embeddings(\n            x=x, positions=positions, encoder_padding_mask=encoder_padding_mask\n        )\n\n        if self.onnx_export_model and encoder_padding_mask is None:\n            encoder_padding_mask = torch.Tensor([]).type_as(src_tokens)\n\n        return x, src_tokens, encoder_padding_mask\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        (x, src_tokens, encoder_padding_mask) = encoder_out\n        if x is not None:\n            x = x.index_select(1, new_order)\n        if src_tokens is not None:\n            src_tokens = src_tokens.index_select(0, new_order)\n        if encoder_padding_mask is not None:\n            encoder_padding_mask = encoder_padding_mask.index_select(0, new_order)\n        return (x, src_tokens, encoder_padding_mask)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return self.embed_positions.max_positions\n\n    def upgrade_state_dict(self, state_dict):\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            if ""encoder.embed_positions.weights"" in state_dict:\n                del state_dict[""encoder.embed_positions.weights""]\n            state_dict[""encoder.embed_positions._float_tensor""] = torch.FloatTensor(1)\n        return state_dict\n\n\n@register_model_architecture(""char_source_transformer"", ""char_source_transformer"")\ndef base_architecture(args):\n    # default architecture\n    transformer.base_architecture(args)\n    args.char_cnn_params = getattr(args, ""char_cnn_params"", ""[(50, 1), (100,2)]"")\n    args.char_cnn_nonlinear_fn = getattr(args, ""chr_cnn_nonlinear_fn"", ""relu"")\n    args.char_cnn_num_highway_layers = getattr(args, ""char_cnn_num_highway_layers"", ""2"")\n'"
pytorch_translate/checkpoint.py,15,"b'#!/usr/bin/env python3\n\nimport os\nfrom collections import OrderedDict, deque\nfrom typing import Any, Deque, Dict, List, Optional, Tuple\n\nimport torch\nfrom fairseq import checkpoint_utils\nfrom pytorch_translate import constants\n\n# TODO(T55884145): Replace with\n# from fvcore.common.file_io import PathManager\nfrom pytorch_translate.file_io import PathManager\n\n\ndef save_checkpoint_atomic(trainer, final_filename, extra_state):\n    """"""Wrapper around trainer.save_checkpoint to make save atomic.""""""\n    temp_filename = os.path.join(final_filename + "".tmp"")\n    trainer.save_checkpoint(temp_filename, extra_state)\n    # TODO(T56266125): Use mv() instead of copy() + rm() after it\'s added to\n    # PathManager.\n    assert PathManager.copy(\n        temp_filename, final_filename, overwrite=True\n    ), f""Failed to copy {temp_filename} to {final_filename}""\n    PathManager.rm(temp_filename)\n\n\ndef load_existing_checkpoint(\n    checkpoint_path, trainer, restore_state=True\n) -> Tuple[bool, Optional[Dict]]:\n    loaded = False\n    extra_state = None\n\n    if not PathManager.isfile(checkpoint_path):\n        print(\n            f""| No existing checkpoint at {checkpoint_path}. ""\n            f""Starting training from scratch.""\n        )\n        return loaded, extra_state\n\n    if restore_state:\n        extra_state = trainer.load_checkpoint(checkpoint_path)\n        if extra_state is None:\n            loaded = False\n            print(f""| Failed to load checkpoint and state from {checkpoint_path}."")\n        else:\n            loaded = True\n            print(\n                f""| Loaded checkpoint {checkpoint_path} ""\n                f""(epoch {extra_state[\'epoch\']}) with restored extra state.""\n            )\n            # batch_offset being None denotes this was a checkpoint saved at\n            # the end of an epoch (after the last batch).\n            if extra_state[""batch_offset""] is None:\n                trainer.lr_step(extra_state[""epoch""])\n                extra_state[""epoch""] += 1\n                extra_state[""batch_offset""] = 0\n\n    else:\n        dummy_state = trainer.load_checkpoint(checkpoint_path, reset_optimizer=True)\n        if dummy_state is None:\n            loaded = False\n            print(f""| Failed to load checkpoint weights from {checkpoint_path}."")\n        else:\n            loaded = True\n            print(f""| Loaded checkpoint weights from {checkpoint_path}."")\n\n    return loaded, extra_state\n\n\ndef load_to_cpu(path: str) -> Dict[str, Any]:\n    """"""\n    This is just fairseq\'s utils.load_checkpoint_to_cpu(), except we don\'t try\n    to upgrade the state dict for backward compatibility - to make cases\n    where we only care about loading the model params easier to unit test.\n    """"""\n    with PathManager.open(path, ""rb"") as f:\n        state = torch.load(\n            f,\n            map_location=(\n                lambda s, _: torch.serialization.default_restore_location(s, ""cpu"")\n            ),\n        )\n    return state\n\n\ndef load_to_gpu(path: str) -> Dict[str, Any]:\n    """"""\n    Similar to load_to_cpu, but load model to cuda\n    """"""\n    with PathManager.open(path, ""rb"") as f:\n        state = torch.load(\n            f,\n            map_location=(\n                lambda s, _: torch.serialization.default_restore_location(s, ""cuda"")\n            ),\n        )\n    return state\n\n\ndef is_integer_tensor(tensor: torch.Tensor) -> bool:\n    return (\n        isinstance(tensor, torch.ByteTensor)\n        or isinstance(tensor, torch.CharTensor)\n        or isinstance(tensor, torch.ShortTensor)\n        or isinstance(tensor, torch.IntTensor)\n        or isinstance(tensor, torch.LongTensor)\n    )\n\n\ndef sanity_check_tensor(\n    tensor_name: str, old_tensor: torch.Tensor, new_tensor: torch.Tensor\n) -> None:\n    if old_tensor.type() != new_tensor.type():\n        raise ValueError(\n            f""Type mismatch for tensor {tensor_name}. Old tensor had type of ""\n            f""{old_tensor.type()} while new tensor has type of {new_tensor.type()}.""\n        )\n    if old_tensor.size() != new_tensor.size():\n        raise ValueError(\n            f""Size mismatch for tensor {tensor_name} of type {old_tensor.type()}. ""\n            f""Old tensor had size of {old_tensor.size()} while new tensor ""\n            f""has size of {new_tensor.size()}.""\n        )\n\n    if is_integer_tensor(old_tensor):\n        # The following sanity check is only relevant for integer tensors - which\n        # we expect to be index-like, and therefore should remain constant and not\n        # be averaged over.\n        if not torch.all(old_tensor == new_tensor):\n            raise ValueError(\n                f""Integer tensor {tensor_name} of type {old_tensor.type()} ""\n                f""and size {old_tensor.size()} had ""\n                f""{torch.sum(old_tensor != new_tensor).item()} mismatched elements.""\n            )\n\n\ndef convert_tensor(tensor: torch.Tensor, clone: bool) -> torch.Tensor:\n    tensor = tensor.detach().cpu()\n    if isinstance(tensor, torch.HalfTensor):\n        # We convert any fp16 params to fp32 to make sure operations like\n        # division by a scalar value are supported.\n        tensor = tensor.float()\n    elif clone:\n        # tensor.float() would have effectively cloned the fp16 tensor already,\n        # so we don\'t need to do it again even if clone=True.\n        tensor = tensor.clone()\n    return tensor\n\n\nclass CheckpointManager:\n    """"""Class to help manage, save, clean up, and average checkpoints.\n    """"""\n\n    def __init__(\n        self,\n        num_avg_checkpoints: int,\n        auto_clear_checkpoints: bool,\n        log_verbose: bool,\n        checkpoint_files: List[str],\n    ):\n        """"""\n        Args:\n          num_avg_checkpoints: Number of checkpoints to average over.\n          auto_clear_checkpoints: If True, we automatically delete\n              checkpoints older than args.num_avg_checkpoints.\n          log_verbose:\n        """"""\n        assert num_avg_checkpoints > 0, ""Must average over at least one checkpoint.""\n        self._num_avg_checkpoints: int = num_avg_checkpoints\n        self._auto_clear_checkpoints: bool = auto_clear_checkpoints\n        self._log_verbose: bool = log_verbose\n        self._averaged_params: OrderedDict = OrderedDict()\n        self._checkpoint_files: Deque[str] = deque(maxlen=self._num_avg_checkpoints)\n        # Updates the checkpoint files deque and discards any checkpoint files\n        # older than the limit.\n        for file in checkpoint_files:\n            self._checkpoint_files.append(file)\n        # Defers actually reading the checkpoint files until later due to\n        # T39501955.\n        self._initialized = False\n\n    def __repr__(self):\n        return (\n            f""CheckpointManager(num_avg_checkpoints={self._num_avg_checkpoints}, ""\n            f""auto_clear_checkpoints={self._auto_clear_checkpoints}, ""\n            f""log_verbose={self._log_verbose}, ""\n            f""checkpoint_files={self._checkpoint_files})""\n        )\n\n    def _initialize(self):\n        # Loads and intializes the previous checkpoint params average.\n        for f in self._checkpoint_files:\n            # Loads everything to CPU memory to save space on GPU memory.\n            state: Dict[str, Any] = load_to_cpu(f)\n            model_params: OrderedDict = state[""model""]\n            for k, v in model_params.items():\n                v = convert_tensor(v, clone=False)\n                if k not in self._averaged_params:\n                    self._averaged_params[k] = (\n                        v if is_integer_tensor(v) else v / len(self._checkpoint_files)\n                    )\n                else:\n                    sanity_check_tensor(\n                        tensor_name=k, old_tensor=self._averaged_params[k], new_tensor=v\n                    )\n                    if not is_integer_tensor(v):\n                        self._averaged_params[k].add_(v / len(self._checkpoint_files))\n\n        self._initialized = True\n\n    def log_if_verbose(self, msg: str):\n        if self._log_verbose:\n            print(msg, flush=True)\n\n    def get_averaged_params(self, new_params: OrderedDict) -> OrderedDict:\n        if not self._initialized:\n            self._initialize()\n\n        self.log_if_verbose(f""| Preparing to average {len(new_params)} params."")\n        # Special case for the first time or when we\'re not doing checkpoint\n        # averaging.\n        if len(self._averaged_params) == 0 or self._checkpoint_files.maxlen == 1:\n            copied_params: OrderedDict = OrderedDict()\n            for k, v in new_params.items():\n                copied_params[k] = convert_tensor(v, clone=True)\n            self.log_if_verbose(\n                f""| Finished copying {len(new_params)} params to CPU ""\n                f""(no averaging needed).""\n            )\n            return copied_params\n\n        new_average: OrderedDict = OrderedDict()\n        new_denom: int = min(  # noqa\n            self._checkpoint_files.maxlen, len(self._checkpoint_files) + 1\n        )\n\n        if len(self._checkpoint_files) == self._checkpoint_files.maxlen:\n            # We\'ve reached the maximum number of checkpoints to average over,\n            # so the denominator won\'t change even when we add a new param - we\n            # just kick out the values from the oldest checkpoint.\n            self.log_if_verbose(\n                f""| Preparing to load old checkpoint ""\n                f""{self._checkpoint_files[0]} to calculate average.""\n            )\n            state: Dict[str, Any] = load_to_cpu(self._checkpoint_files[0])\n            self.log_if_verbose(\n                f""| Finished loading old checkpoint ""\n                f""{self._checkpoint_files[0]} to calculate average.""\n            )\n            old_params: OrderedDict = state[""model""]\n            for k, v in old_params.items():\n                v = convert_tensor(v, clone=False)\n                sanity_check_tensor(\n                    tensor_name=k, old_tensor=self._averaged_params[k], new_tensor=v\n                )\n                if is_integer_tensor(v):\n                    new_average[k] = v\n                else:\n                    new_average[k] = self._averaged_params[k] - (\n                        v / len(self._checkpoint_files)\n                    )\n\n        else:\n            # We haven\'t reached the maximum number of checkpoints to average\n            # over, so we simply adjust the denominator and the existing average\n            # to account for the larger number of checkpoints we\'re now\n            # averaging over.\n            for k, v in self._averaged_params.items():\n                if is_integer_tensor(v):\n                    new_average[k] = v.clone()\n                else:\n                    new_average[k] = v * (len(self._checkpoint_files) / new_denom)\n\n        # Actually add the new params after the existing average has been\n        # adjusted accordingly.\n        for k, v in new_params.items():\n            v = convert_tensor(v, clone=False)\n            sanity_check_tensor(tensor_name=k, old_tensor=new_average[k], new_tensor=v)\n            if not is_integer_tensor(v):\n                new_average[k].add_(v / new_denom)\n\n        self.log_if_verbose(f""| Finished averaging {len(new_params)} params."")\n        return new_average\n\n    def _update_state(\n        self, new_params_filename: str, new_averaged_params: OrderedDict\n    ) -> Optional[str]:\n        # Consider making a copy of each tensor here if we run into issues in\n        # the future with callers later modifying the params passed in.\n        self._averaged_params = new_averaged_params\n\n        checkpoint_to_remove = None\n        if (\n            self._auto_clear_checkpoints\n            and len(self._checkpoint_files) == self._checkpoint_files.maxlen\n        ):\n            # We delay actually removing this checkpoint until after the newest\n            # checkpoint has been successfully written.\n            checkpoint_to_remove = self._checkpoint_files.popleft()\n\n        # Make sure to include the checkpoint itself in its list of checkpoint\n        # files - this is to ensure we can still restore everything correctly\n        # even if the file gets copied to another name (ex: checkpoint_last.py).\n        self._checkpoint_files.append(new_params_filename)\n        return checkpoint_to_remove\n\n    def _remove_checkpoint(self, checkpoint_to_remove: Optional[str]):\n        if checkpoint_to_remove:\n            self.log_if_verbose(\n                f""| Preparing to remove old checkpoint {checkpoint_to_remove}.""\n            )\n            try:\n                PathManager.rm(checkpoint_to_remove)\n                self.log_if_verbose(\n                    f""| Finished removing old checkpoint {checkpoint_to_remove}.""\n                )\n            except OSError as e:\n                print(\n                    f""| Failed to remove old checkpoint {checkpoint_to_remove} ""\n                    f""- exception: {e}"",\n                    flush=True,\n                )\n\n    def remove_all_checkpoints(self):\n        """"""\n        Removes all checkpoints besides\n         average_checkpoint.pt and last_checkpoint.pt\n        """"""\n        for checkpoint_to_remove in self._checkpoint_files:\n            self._remove_checkpoint(checkpoint_to_remove)\n\n    def save(\n        self,\n        args,\n        trainer,\n        extra_state: Dict[str, Any],\n        new_averaged_params: OrderedDict,\n    ) -> Dict[str, Any]:\n        """"""Saves the model params contained in trainer.\n\n        Takes ownership of new_averaged_params, so the caller should not modify\n        them afterwards.\n\n        Args:\n          trainer: Trainer containing the model to be saved.\n          extra_state: Dictionary containing any extra information about the\n              model beyond the param weights.\n          new_averaged_params: If specified, takes ownership of the params and\n              sets them as current set of averaged params. If not specified,\n              we will recalculate the averaged params using the model params\n              in trainer.\n\n        Returns:\n          Updated extra_state dictionary.\n        """"""\n        epoch = extra_state[""epoch""]\n        batch_offset = extra_state[""batch_offset""]\n\n        # batch_offset being None means that we\'re at the end of an epoch.\n        if batch_offset is None:\n            filename = os.path.join(args.save_dir, f""checkpoint{epoch}_end.pt"")\n        # Otherwise, we\'re in the middle of an epoch.\n        else:\n            filename = os.path.join(\n                args.save_dir, f""checkpoint{epoch}_{batch_offset}.pt""\n            )\n\n        checkpoint_to_remove = self._update_state(\n            new_params_filename=filename, new_averaged_params=new_averaged_params\n        )\n        extra_state[""checkpoint_files""] = list(self._checkpoint_files)\n\n        self.log_if_verbose(\n            f""| Preparing to save checkpoints for epoch {epoch}, ""\n            f""offset {batch_offset}.""\n        )\n        # Saves two copies of the checkpoint - one under a specific name\n        # corresponding to its epoch/offset, and another under the generic\n        # ""checkpoint_last.py"" that we restore from in case training is\n        # interrupted.\n        save_checkpoint_atomic(\n            trainer=trainer, final_filename=filename, extra_state=extra_state\n        )\n        # We update checkpoint_last.pt only after the new averaged checkpoint\n        # and epoch/offset-named copy have been written - so that in case either\n        # write fails, we\'d still be able to resume from the previous\n        # checkpoint_last.pt\n        last_checkpoint_path = os.path.join(\n            args.save_dir, constants.LAST_CHECKPOINT_FILENAME\n        )\n        assert PathManager.copy(\n            filename, last_checkpoint_path, overwrite=True\n        ), f""Failed to copy {filename} to {last_checkpoint_path}""\n        self.log_if_verbose(\n            f""| Finished saving checkpoints for epoch {epoch}, ""\n            f""offset {batch_offset}.""\n        )\n\n        # Wait until after checkpoint_last.py has been written to remove the\n        # oldest checkpoint. This is so that in case we fail to write a new\n        # checkpoint_last.py, we\'d still have access to all the files listed in\n        # the previous checkpoint_last.py\n        self._remove_checkpoint(checkpoint_to_remove)\n        return extra_state\n\n    def save_best_averaged_checkpoint(self, args, trainer, extra_state: Dict[str, Any]):\n        """"""\n        save() should always be called before calling this function - to ensure\n        that extra_state and self._averaged_params have been updated correctly.\n        """"""\n        best_averaged_checkpoint_filename = os.path.join(\n            args.save_dir, constants.AVERAGED_CHECKPOINT_BEST_FILENAME\n        )\n        self.log_if_verbose(\n            f""| Preparing to save new best averaged checkpoint to ""\n            f""{best_averaged_checkpoint_filename}.""\n        )\n        checkpoint_utils.save_state(\n            filename=best_averaged_checkpoint_filename,\n            args=args,\n            model_state_dict=self._averaged_params,\n            criterion=trainer.criterion,\n            optimizer=trainer.optimizer,\n            lr_scheduler=trainer.lr_scheduler,\n            num_updates=trainer._num_updates,\n            optim_history=trainer._optim_history,\n            extra_state=extra_state,\n        )\n        self.log_if_verbose(\n            f""| Finished saving new best averaged checkpoint to ""\n            f""{best_averaged_checkpoint_filename}.""\n        )\n'"
pytorch_translate/common_layers.py,37,"b'#!/usr/bin/env python3\n\nimport abc\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.models import FairseqIncrementalDecoder, transformer as fairseq_transformer\nfrom pytorch_translate import rnn_cell  # noqa\nfrom pytorch_translate import utils as pytorch_translate_utils, vocab_reduction\nfrom pytorch_translate.research.lexical_choice import lexical_translation\n\n\nclass ContextEmbedding(nn.Module):\n    """"""\n    This class implements context-dependent word embeddings as described in\n    https://arxiv.org/pdf/1607.00578.pdf\n    """"""\n\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.nonlinear = NonlinearLayer(\n            embed_dim, embed_dim, bias=True, activation_fn=nn.ReLU\n        )\n        self.linear = Linear(embed_dim, embed_dim, bias=True)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, src):\n        c = torch.mean(self.nonlinear(src), 1, True)\n        return src * self.sigmoid(self.linear(c))\n\n\nclass VariableLengthRecurrent(nn.Module):\n    """"""\n    This class acts as a generator of autograd for varying seq lengths with\n    different padding behaviors, such as right padding, and order of seq lengths,\n    such as descending order.\n\n    The logic is mostly inspired from torch/nn/_functions/rnn.py, so it may be\n    merged in the future.\n    """"""\n\n    def __init__(self, rnn_cell, reverse=False):\n        super().__init__()\n        self.rnn_cell = rnn_cell\n        self.reverse = reverse\n\n    def forward(self, x, hidden, batch_size_per_step):\n        self.batch_size_per_step = batch_size_per_step\n        self.starting_batch_size = (\n            batch_size_per_step[-1] if self.reverse else batch_size_per_step[0]\n        )\n\n        output = []\n        input_offset = x.size(0) if self.reverse else 0\n\n        hiddens = []\n        flat_hidden = not isinstance(hidden, tuple)\n        if flat_hidden:\n            hidden = (hidden,)\n        initial_hidden = hidden\n\n        if self.reverse:\n            hidden = tuple(h[: self.batch_size_per_step[-1]] for h in hidden)\n\n        last_batch_size = self.starting_batch_size\n\n        # Iterate over time steps with varying batch_size\n        for i in range(len(self.batch_size_per_step)):\n            if self.reverse:\n                step_batch_size = self.batch_size_per_step[-1 - i]\n                step_input = x[(input_offset - step_batch_size) : input_offset]\n                input_offset -= step_batch_size\n            else:\n                step_batch_size = self.batch_size_per_step[i]\n                step_input = x[input_offset : (input_offset + step_batch_size)]\n                input_offset += step_batch_size\n\n            new_pads = last_batch_size - step_batch_size\n            if new_pads > 0:\n                # First slice out the pieces for pads\n                hiddens.insert(0, tuple(h[-new_pads:] for h in hidden))\n                # Only pass the non-pad part of hidden states\n                hidden = tuple(h[:-new_pads] for h in hidden)\n            if new_pads < 0:\n                hidden = tuple(\n                    torch.cat((h, ih[last_batch_size:step_batch_size]), 0)\n                    for h, ih in zip(hidden, initial_hidden)\n                )\n\n            last_batch_size = step_batch_size\n            if flat_hidden:\n                hidden = (self.rnn_cell(step_input, hidden[0]),)\n            else:\n                hidden = self.rnn_cell(step_input, hidden)\n            output.append(hidden[0])\n\n        if not self.reverse:\n            hiddens.insert(0, hidden)\n            hidden = tuple(torch.cat(h, 0) for h in zip(*hiddens))\n\n        assert output[0].size(0) == self.starting_batch_size\n\n        if flat_hidden:\n            hidden = hidden[0]\n        if self.reverse:\n            output.reverse()\n\n        output = torch.cat(output, 0)\n        return hidden, output\n\n\nclass RNNLayer(nn.Module):\n    """"""\n    A wrapper of rnn cells, with their corresponding forward function.\n    If bidirectional, halve the hidden_size for each cell.\n    """"""\n\n    def __init__(\n        self, input_size, hidden_size, cell_type=""lstm"", is_bidirectional=False\n    ):\n        super().__init__()\n        self.is_bidirectional = is_bidirectional\n        num_directions = 2 if is_bidirectional else 1\n\n        if cell_type == ""lstm"":\n            cell_class = rnn_cell.LSTMCell\n        elif cell_type == ""milstm"":\n            cell_class = rnn_cell.MILSTMCell\n        elif cell_type == ""layer_norm_lstm"":\n            cell_class = rnn_cell.LayerNormLSTMCell\n        else:\n            raise Exception(f""{cell_type} not implemented"")\n\n        self.fwd_cell = cell_class(input_size, hidden_size // num_directions)\n        if is_bidirectional:\n            self.bwd_cell = cell_class(input_size, hidden_size // num_directions)\n\n        self.fwd_func = VariableLengthRecurrent(rnn_cell=self.fwd_cell, reverse=False)\n        if is_bidirectional:\n            self.bwd_func = VariableLengthRecurrent(\n                rnn_cell=self.bwd_cell, reverse=True\n            )\n\n    def forward(self, x, hidden, batch_size_per_step):\n        fwd_hidden, fwd_output = self.fwd_func.forward(x, hidden, batch_size_per_step)\n        if self.is_bidirectional:\n            bwd_hidden, bwd_output = self.bwd_func.forward(\n                x, hidden, batch_size_per_step\n            )\n            # concat hidden and outputs\n            combined_hidden = [fwd_hidden, bwd_hidden]\n            bi_hiddens, bi_cells = zip(*combined_hidden)\n            next_hidden = (\n                torch.cat(bi_hiddens, bi_hiddens[0].dim() - 1),\n                torch.cat(bi_cells, bi_cells[0].dim() - 1),\n            )\n            output = torch.cat([fwd_output, bwd_output], x.dim() - 1)\n        else:\n            next_hidden = fwd_hidden\n            output = fwd_output\n\n        return next_hidden, output\n\n\nclass Embedding(nn.Embedding):\n    """"""\n    A wrapper around the embedding layer, which can be randomly initialized or\n    loaded from a .npy file. Also supports normalization of embeddings to have\n    zero mean and unit variance (weighted by token frequency) - this is useful\n    for example when creating adversarial perturbations of the embeddings that\n    should have norms relative to the embeddings\' norms.\n    """"""\n\n    def __init__(\n        self,\n        num_embeddings,\n        embedding_dim,\n        padding_idx,\n        freeze_embed=False,\n        normalize_embed=False,\n        normalize_decay_rate=0.99,\n    ):\n        super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n        nn.init.uniform_(self.weight, -0.1, 0.1)\n        nn.init.constant_(self.weight[padding_idx], 0.0)\n        if freeze_embed:\n            self.weight.requires_grad = False\n\n        assert 0.0 < normalize_decay_rate < 1.0\n        self.normalize = normalize_embed\n        self.normalize_decay_rate = normalize_decay_rate\n        self.mean = None\n        self.var = None\n        self.init_normalization_if_needed()\n\n    def forward(self, x):\n        x = super().forward(x)\n\n        if self.normalize:\n            if self.training:\n                self._update_normalize_params(x)\n            x = (x - self.mean) / torch.sqrt(self.var + 1e-6)\n\n        return x\n\n    def init_normalization_if_needed(self):\n        if not self.normalize:\n            return\n\n        self.mean = nn.Parameter(self.weight.mean(dim=0), requires_grad=False)\n        self.var = nn.Parameter(self.weight.var(dim=0), requires_grad=False)\n\n    def _update_normalize_params(self, x):\n        """"""\n        Updates the observed mean and variance of the token embeddings. Note\n        that these will be weighted by the empirical frequency of each token\n        (i.e. common tokens will be more heavily weighted in the params).\n        """"""\n        # Flatten x to be a tensor of embeddings.\n        assert x.size()[-1:] == self.mean.size()\n        x_flattened = x.view(-1, x.size(-1))\n\n        # Update mean.\n        x_mean = x_flattened.mean(dim=0)\n        self.mean.data = (\n            self.normalize_decay_rate * self.mean.data\n            + (1.0 - self.normalize_decay_rate) * x_mean\n        )\n\n        # Update variance.\n        x_var = ((x_flattened - self.mean) ** 2).mean(dim=0)\n        self.var.data = (\n            self.normalize_decay_rate * self.var.data\n            + (1.0 - self.normalize_decay_rate) * x_var\n        )\n\n\ndef Linear(in_features, out_features, bias=True):\n    """"""Weight-normalized Linear layer (input: N x T x C)""""""\n    m = nn.Linear(in_features, out_features, bias=bias)\n    m.weight.data.uniform_(-0.1, 0.1)\n    if bias:\n        m.bias.data.uniform_(-0.1, 0.1)\n    return m\n\n\ndef NonlinearLayer(in_features, out_features, bias=True, activation_fn=nn.ReLU):\n    """"""Weight-normalized non-linear layer (input: N x T x C)""""""\n    m = nn.Linear(in_features, out_features, bias=bias)\n    m.weight.data.uniform_(-0.1, 0.1)\n    if bias:\n        m.bias.data.uniform_(-0.1, 0.1)\n    return nn.Sequential(m, activation_fn())\n\n\nclass DecoderWithOutputProjection(FairseqIncrementalDecoder):\n    """"""Common super class for decoder networks with output projection layers.\n\n    This class couples common functionality for `FairseqDecoder`s with large\n    output projection layers such as ONNX compatibility and vocabulary reduction.\n    """"""\n\n    def __init__(\n        self,\n        src_dict,\n        dst_dict,\n        vocab_reduction_params=None,\n        out_embed_dim=512,\n        project_output=True,\n        pretrained_embed=None,\n        out_embed_norm=None,\n        att_weighted_src_embeds=False,\n        src_embed_dim=512,\n        att_weighted_activation_type=""tanh"",\n        predictor=None,\n        fp16: bool = False,\n    ):\n        super().__init__(dst_dict)\n        self.project_output = project_output\n        if project_output:\n            self.num_embeddings = len(dst_dict)\n            self.out_embed_dim = out_embed_dim\n            self.out_embed_norm = out_embed_norm\n            self.att_weighted_src_embeds = att_weighted_src_embeds\n            self.src_embed_dim = src_embed_dim\n            self.vocab_reduction_module = None\n            if vocab_reduction_params or predictor is not None:\n                self.vocab_reduction_module = vocab_reduction.VocabReduction(\n                    src_dict=src_dict,\n                    dst_dict=dst_dict,\n                    vocab_reduction_params=vocab_reduction_params,\n                    predictor=predictor,\n                    fp16=fp16,\n                )\n\n            projection_weights = torch.FloatTensor(\n                self.num_embeddings, self.out_embed_dim\n            ).uniform_(-0.1, 0.1)\n            if isinstance(pretrained_embed, nn.Embedding):\n                projection_weights.data = pretrained_embed.weights.data\n            elif pretrained_embed is not None:\n                embed_dict = utils.parse_embedding(pretrained_embed)\n                # equivalent to utils.load_embedding but for nn.Parameter\n                for idx in range(len(dst_dict)):\n                    token = dst_dict[idx]\n                    if token in embed_dict:\n                        projection_weights[idx] = embed_dict[token]\n            self.output_projection_w = nn.Parameter(projection_weights)\n            self.output_projection_b = nn.Parameter(\n                torch.FloatTensor(self.num_embeddings).zero_()\n            )\n            if att_weighted_activation_type == ""tanh"":\n                activation_fn = nn.Tanh\n                self.att_weighted_activation_fn = torch.tanh\n            elif att_weighted_activation_type == ""relu"":\n                activation_fn = nn.ReLU\n                self.att_weighted_activation_fn = torch.relu\n            else:\n                raise Exception(\n                    ""att_weighted_activation_type \'%s\' not implemented""\n                    % att_weighted_activation_type\n                )\n            if att_weighted_src_embeds:\n                print(att_weighted_activation_type)\n                self.lexical_layer = NonlinearLayer(\n                    self.src_embed_dim,\n                    self.out_embed_dim,\n                    bias=False,\n                    activation_fn=activation_fn,\n                )\n                self.output_projection_w_lex = nn.Parameter(\n                    torch.FloatTensor(self.num_embeddings, self.out_embed_dim).uniform_(\n                        -0.1, 0.1\n                    )\n                )\n                self.output_projection_b_lex = nn.Parameter(\n                    torch.FloatTensor(self.num_embeddings).zero_()\n                )\n\n    def forward(\n        self,\n        input_tokens,\n        encoder_out,\n        incremental_state=None,\n        possible_translation_tokens=None,\n        reduced_output_weights=None,\n    ):\n        (_, _, _, src_lengths, src_tokens, src_embeddings) = encoder_out\n        x, attn_scores = self.forward_unprojected(\n            input_tokens, encoder_out, incremental_state\n        )\n        if not self.project_output:\n            return x, attn_scores, None\n        decoder_input_tokens = input_tokens.contiguous()\n\n        if reduced_output_weights is not None:\n            (output_projection_w, output_projection_b) = reduced_output_weights\n        else:\n            output_projection_w = self.output_projection_w\n            output_projection_b = self.output_projection_b\n\n            if self.vocab_reduction_module and possible_translation_tokens is None:\n                possible_translation_tokens = self.vocab_reduction_module(\n                    src_tokens,\n                    encoder_output=encoder_out,\n                    decoder_input_tokens=decoder_input_tokens,\n                )\n\n            if possible_translation_tokens is not None:\n                output_projection_w = output_projection_w.index_select(\n                    dim=0, index=possible_translation_tokens\n                )\n                output_projection_b = output_projection_b.index_select(\n                    dim=0, index=possible_translation_tokens\n                )\n\n        # avoiding transpose of projection weights during ONNX tracing\n        batch_time_hidden = torch.onnx.operators.shape_as_tensor(x)\n        x_flat_shape = torch.cat((torch.LongTensor([-1]), batch_time_hidden[2].view(1)))\n        x_flat = torch.onnx.operators.reshape_from_tensor_shape(x, x_flat_shape)\n\n        if self.out_embed_norm is not None:\n            # fix the norm of both output word embeddings and context vector\n            output_projection_w = self.out_embed_norm * F.normalize(\n                output_projection_w, p=2, dim=1\n            )\n            x_flat = self.out_embed_norm * F.normalize(x_flat, p=2, dim=1)\n\n        projection_flat = torch.matmul(output_projection_w, x_flat.t()).t()\n        logits_shape = torch.cat((batch_time_hidden[:2], torch.LongTensor([-1])))\n        logits = (\n            torch.onnx.operators.reshape_from_tensor_shape(\n                projection_flat, logits_shape\n            )\n            + output_projection_b\n        )\n        if self.att_weighted_src_embeds:\n            # use the attention weights to form a weighted average of embeddings\n            lex = lexical_translation.attention_weighted_src_embedding(\n                src_embeddings, attn_scores, self.att_weighted_activation_fn\n            )\n            # avoiding transpose of projection weights during ONNX tracing\n            batch_time_hidden_lex = torch.onnx.operators.shape_as_tensor(lex)\n            lex_flat_shape = torch.cat(\n                (torch.LongTensor([-1]), batch_time_hidden_lex[2].view(1))\n            )\n            lex_flat = torch.onnx.operators.reshape_from_tensor_shape(\n                lex, lex_flat_shape\n            )\n            lex_logits_shape = torch.cat(\n                (batch_time_hidden_lex[:2], torch.LongTensor([-1]))\n            )\n            # add one-hidden-layer FFNN\n            lex_h = self.lexical_layer(lex_flat)\n            # lexical logits of output of previous FFNN\n            lex_logits = lexical_translation.lex_logits(\n                lex_h,\n                self.output_projection_w_lex,\n                self.output_projection_b_lex,\n                lex_logits_shape,\n            )\n            # combine lexical logits with the original decoder logits\n            logits.add_(lex_logits)\n\n        return logits, attn_scores, possible_translation_tokens\n\n    def _precompute_reduced_weights(self, possible_translation_tokens):\n        output_projection_w = self.output_projection_w.index_select(\n            dim=0, index=possible_translation_tokens\n        )\n        output_projection_b = self.output_projection_b.index_select(\n            dim=0, index=possible_translation_tokens\n        )\n        return (output_projection_w, output_projection_b)\n\n    @abc.abstractmethod\n    def forward_unprojected(\n        self,\n        input_tokens,\n        encoder_out,\n        incremental_state=None,\n        possible_translation_tokens=None,\n    ):\n        """"""Forward pass through the decoder without output projection.""""""\n        raise NotImplementedError()\n\n\nclass OutputProjection(nn.Module):\n    """"""Output projection layer.""""""\n\n    def __init__(self, out_embed_dim, vocab_size, vocab_reduction_module=None):\n        super().__init__()\n        self.out_embed_dim = out_embed_dim\n        self.vocab_size = vocab_size\n\n        self.output_projection_w = nn.Parameter(\n            torch.FloatTensor(self.vocab_size, self.out_embed_dim).uniform_(-0.1, 0.1)\n        )\n        self.output_projection_b = nn.Parameter(\n            torch.FloatTensor(self.vocab_size).zero_()\n        )\n        self.vocab_reduction_module = vocab_reduction_module\n\n    def forward(\n        self, x, src_tokens=None, input_tokens=None, possible_translation_tokens=None\n    ):\n        output_projection_w = self.output_projection_w\n        output_projection_b = self.output_projection_b\n        decoder_input_tokens = input_tokens if self.training else None\n\n        if self.vocab_reduction_module and possible_translation_tokens is None:\n            possible_translation_tokens = self.vocab_reduction_module(\n                src_tokens, decoder_input_tokens=decoder_input_tokens\n            )\n\n        if possible_translation_tokens is not None:\n            output_projection_w = output_projection_w.index_select(\n                dim=0, index=possible_translation_tokens\n            )\n            output_projection_b = output_projection_b.index_select(\n                dim=0, index=possible_translation_tokens\n            )\n\n        # avoiding transpose of projection weights during ONNX tracing\n        batch_time_hidden = torch.onnx.operators.shape_as_tensor(x)\n        x_flat_shape = torch.cat((torch.LongTensor([-1]), batch_time_hidden[2].view(1)))\n        x_flat = torch.onnx.operators.reshape_from_tensor_shape(x, x_flat_shape)\n\n        projection_flat = torch.matmul(output_projection_w, x_flat.t()).t()\n        logits_shape = torch.cat((batch_time_hidden[:2], torch.LongTensor([-1])))\n        logits = (\n            torch.onnx.operators.reshape_from_tensor_shape(\n                projection_flat, logits_shape\n            )\n            + output_projection_b\n        )\n        return logits, possible_translation_tokens\n\n\nclass VariableTracker(object):\n    """"""This class implements several methods to keep track of intermediate\n    variables.\n\n    This is useful for eg. visualizing or retrieving gradients wrt. inputs\n    later on""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.tracker = {}\n\n    def track(self, variable, name, retain_grad=False):\n        """"""Adds variable to the tracker\n\n        Specify `retain_grad=True` to retrieve the gradient later.""""""\n        if retain_grad:\n            variable.retain_grad()\n        self.tracker[name] = variable\n\n    def __getitem__(self, name):\n        return self.tracker[name]\n\n\ndef build_embedding(dictionary, embed_dim, path=None):\n    num_embeddings = len(dictionary)\n    padding_idx = dictionary.pad()\n    emb = Embedding(num_embeddings, embed_dim, padding_idx)\n    # if provided, load from preloaded dictionaries\n    if path:\n        embed_dict = utils.parse_embedding(path)\n        utils.load_embedding(embed_dict, dictionary, emb)\n    return emb\n\n\nclass TransformerEncoderGivenEmbeddings(nn.Module):\n    def __init__(self, args, proj_to_decoder):\n        super().__init__()\n\n        self.layers = nn.ModuleList([])\n        self.layers.extend(\n            [\n                fairseq_transformer.TransformerEncoderLayer(args)\n                for i in range(args.encoder_layers)\n            ]\n        )\n\n    def forward(self, x, positions, encoder_padding_mask):\n        # encoder layers\n        for layer in self.layers:\n            x = layer(x, encoder_padding_mask)\n\n        return x\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        for i in range(len(self.layers)):\n            # update layer norms\n            self.layers[i].upgrade_state_dict_named(state_dict, f""{name}.layers.{i}"")\n\n\ndef TransformerTokenEmbedding(\n    num_embeddings, embedding_dim, padding_idx, freeze_embed=False\n):\n    """"""\n    Different weight initialization from common_layers.Embedding\n    """"""\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    if freeze_embed:\n        m.weight.requires_grad = False\n    return m\n\n\nclass TransformerEmbedding(nn.Module):\n    def __init__(self, args, embed_tokens):\n        super().__init__()\n        self.dropout = args.dropout\n        embed_dim = embed_tokens.embedding_dim\n        self.padding_idx = embed_tokens.padding_idx\n        self.embed_tokens = embed_tokens\n        self.embed_scale = math.sqrt(embed_dim)\n        self.embed_positions = fairseq_transformer.PositionalEmbedding(\n            1024, embed_dim, self.padding_idx, learned=args.encoder_learned_pos\n        )\n\n    def forward(self, src_tokens, src_lengths):\n        # Embed tokens\n        x = self.embed_tokens(src_tokens)\n        src_tokens_tensor = pytorch_translate_utils.get_source_tokens_tensor(src_tokens)\n        # Add position embeddings and dropout\n        x = self.embed_scale * x\n        positions = self.embed_positions(src_tokens_tensor)\n        x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # compute padding mask (B x T)\n        encoder_padding_mask = src_tokens_tensor.eq(self.padding_idx)\n        if not encoder_padding_mask.any():\n            encoder_padding_mask = None\n\n        return x, encoder_padding_mask, positions\n'"
pytorch_translate/constants.py,0,"b'#!/usr/bin/env python3\n\nAVERAGED_CHECKPOINT_BEST_FILENAME = ""averaged_checkpoint_best.pt""\nLAST_CHECKPOINT_FILENAME = ""checkpoint_last.pt""\n\nMONOLINGUAL_DATA_IDENTIFIER = ""mono""\n\nSEMI_SUPERVISED_TASK = ""pytorch_translate_semi_supervised""\nKNOWLEDGE_DISTILLATION_TASK = ""pytorch_translate_knowledge_distillation""\nDENOISING_AUTOENCODER_TASK = ""pytorch_translate_denoising_autoencoder""\nMULTILINGUAL_TRANSLATION_TASK = ""pytorch_translate_multilingual_task""\nLATENT_VARIABLE_TASK = ""translation_vae""\n\nARCHS_FOR_CHAR_SOURCE = {\n    ""char_source"",\n    ""char_source_hybrid"",\n    ""char_source_transformer"",\n    ""char_aware_hybrid"",\n}\nARCHS_FOR_CHAR_TARGET = {""char_aware_hybrid""}\nCHECKPOINT_PATHS_DELIMITER = ""|""\n'"
pytorch_translate/ensemble_export.py,128,"b'#!/usr/bin/env python3\n\nimport copy\nimport logging\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.jit\nimport torch.jit.quantized\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.onnx.operators\nfrom fairseq import tasks, utils\nfrom fairseq.iterative_refinement_generator import DecoderOut\nfrom fairseq.models import ARCH_MODEL_REGISTRY\nfrom fairseq.models.model_utils import script_skip_tensor\nfrom fairseq.models.transformer import EncoderOut\nfrom pytorch_translate.beam_decode import BeamDecode\nfrom pytorch_translate.checkpoint import load_to_cpu, load_to_gpu\nfrom pytorch_translate.data import dictionary\nfrom pytorch_translate.research.knowledge_distillation import (\n    dual_decoder_kd_model,\n    hybrid_dual_decoder_kd_model,\n)\nfrom pytorch_translate.tasks.pytorch_translate_task import DictionaryHolderTask\nfrom pytorch_translate.word_prediction import word_prediction_model\nfrom torch import Tensor\n\n\ntry:\n    from pytorch_translate import latent_var_models  # noqa;\n    from fairseq.models import fb_levenshtein_transformer as levenshtein_transformer\nexcept ImportError:\n    pass\n\n\nfrom pytorch_translate import (  # noqa; noqa\n    char_aware_hybrid,\n    char_source_hybrid,\n    char_source_model,\n    char_source_transformer_model,\n    hybrid_transformer_rnn,\n    rnn,\n    semi_supervised,\n    transformer,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_models_from_checkpoints(\n    checkpoint_filenames,\n    src_dict_filename,\n    dst_dict_filename,\n    lexical_dict_paths=None,\n    use_cuda=False,\n):\n    src_dict = dictionary.Dictionary.load(src_dict_filename)\n    dst_dict = dictionary.Dictionary.load(dst_dict_filename)\n    models = []\n    for filename in checkpoint_filenames:\n        if use_cuda:\n            checkpoint_data = load_to_gpu(filename)\n        else:\n            checkpoint_data = load_to_cpu(filename)\n        if lexical_dict_paths is not None:\n            assert (\n                checkpoint_data[""args""].vocab_reduction_params is not None\n            ), ""lexical dictionaries can only be replaced in vocab-reduction models""\n            checkpoint_data[""args""].vocab_reduction_params[\n                ""lexical_dictionaries""\n            ] = lexical_dict_paths\n        task = DictionaryHolderTask(src_dict, dst_dict)\n\n        architecture = checkpoint_data[""args""].arch\n        if architecture == ""rnn"":\n            model = rnn.RNNModel.build_model(checkpoint_data[""args""], task)\n        elif architecture == ""char_source"":\n            model = char_source_model.CharSourceModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif architecture == ""char_source_transformer"":\n            model = char_source_transformer_model.CharSourceTransformerModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif architecture == ""rnn_word_pred"":\n            model = word_prediction_model.RNNWordPredictionModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif architecture == ""ptt_transformer"":\n            model = transformer.TransformerModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif architecture == ""hybrid_transformer_rnn"":\n            model = hybrid_transformer_rnn.HybridTransformerRNNModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif architecture == ""char_source_hybrid"":\n            model = char_source_hybrid.CharSourceHybridModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif architecture == ""dual_decoder_kd"":\n            model = dual_decoder_kd_model.DualDecoderKDModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif architecture == ""hybrid_dual_decoder_kd"":\n            model = hybrid_dual_decoder_kd_model.HybridDualDecoderKDModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif ""semi_supervised"" in architecture:\n            model_args = copy.deepcopy(checkpoint_data[""args""])\n            model_args.source_vocab_file = src_dict_filename\n            model_args.target_vocab_file = dst_dict_filename\n            task = tasks.setup_task(model_args)\n\n            model = ARCH_MODEL_REGISTRY[model_args.arch].build_model(model_args, task)\n        elif architecture == ""latent_var_transformer"":\n            task = tasks.setup_task(checkpoint_data[""args""])\n            model = latent_var_models.LatentVarModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        elif architecture == ""fb_levenshtein_transformer"":\n            task = tasks.setup_task(checkpoint_data[""args""])\n            model = levenshtein_transformer.LevenshteinTransformerModel.build_model(\n                checkpoint_data[""args""], task\n            )\n        else:\n            raise RuntimeError(f""Architecture not supported: {architecture}"")\n        model.load_state_dict(checkpoint_data[""model""])\n        if hasattr(model, ""get_student_model""):\n            model = model.get_student_model()\n\n        if isinstance(model, semi_supervised.SemiSupervisedModel):\n            if (\n                model_args.source_lang is not None\n                and model_args.target_lang is not None\n            ):\n                direction = model_args.source_lang + ""-"" + model_args.target_lang\n            else:\n                direction = ""src-tgt""\n            models.append(model.models[direction])\n        else:\n            models.append(model)\n\n    return models, src_dict, dst_dict\n\n\nclass EncoderEnsemble(nn.Module):\n    def __init__(self, models, src_dict=None):\n        super().__init__()\n        self.models = models\n        self.src_dict = src_dict\n        for i, model in enumerate(self.models):\n            model.prepare_for_onnx_export_()\n            if hasattr(model, ""get_student_model""):\n                model = model.get_student_model()\n                self.models[i] = model\n            self._modules[f""model_{i}""] = model\n\n        self.enable_precompute_reduced_weights = False\n\n    def forward(self, src_tokens, src_lengths):\n        # (seq_length, batch_size) for compatibility with Caffe2\n        src_tokens_seq_first = src_tokens.t()\n\n        futures = []\n        for model in self.models:\n            # evaluation mode\n            model.eval()\n\n            futures.append(\n                torch.jit._fork(model.encoder, src_tokens_seq_first, src_lengths)\n            )\n\n        return self.get_outputs(src_tokens, futures)\n\n    def get_outputs(self, src_tokens, encoder_futures):\n        outputs = []\n        output_names = []\n        states = []\n\n        possible_translation_tokens = None\n\n        # underlying assumption is each model has same vocab_reduction_module\n        if hasattr(self.models[0].decoder, ""vocab_reduction_module""):\n            vocab_reduction_module = self.models[0].decoder.vocab_reduction_module\n            if vocab_reduction_module is not None:\n                possible_translation_tokens = vocab_reduction_module(\n                    src_tokens=src_tokens, decoder_input_tokens=None\n                )\n\n        # Precompute reduced decoder weight matrices.\n        # Once we have possible_translation_tokens, we need to gather rows\n        # out of each output_projection_{w,b} tensor for the decoders to\n        # use. We do it here because these reduced matrices are used on each\n        # step of the beam search, and this turns out to be a relatively\n        # expensive operation.\n        reduced_weights = {}\n        for i, model in enumerate(self.models):\n            if (\n                self.enable_precompute_reduced_weights\n                and hasattr(model.decoder, ""_precompute_reduced_weights"")\n                and possible_translation_tokens is not None\n            ):\n                reduced_weights[i] = torch.jit._fork(\n                    model.decoder._precompute_reduced_weights,\n                    possible_translation_tokens,\n                )\n\n        # XXX: This loop is where we wait() for each encoder\'s output to be\n        # ready. If you\'re trying to add more ops, they should probably not\n        # go in this loop!\n        for i, (model, future) in enumerate(zip(self.models, encoder_futures)):\n            encoder_out = torch.jit._wait(future)\n            # ""primary"" encoder output (vector representations per source token)\n            encoder_outputs = encoder_out[0]\n            outputs.append(encoder_outputs)\n            output_names.append(f""encoder_output_{i}"")\n            if hasattr(model.decoder, ""_init_prev_states""):\n                states.extend(model.decoder._init_prev_states(encoder_out))\n            if (\n                self.enable_precompute_reduced_weights\n                and hasattr(model.decoder, ""_precompute_reduced_weights"")\n                and possible_translation_tokens is not None\n            ):\n                states.extend(torch.jit._wait(reduced_weights[i]))\n\n        if possible_translation_tokens is not None:\n            outputs.append(possible_translation_tokens)\n            output_names.append(""possible_translation_tokens"")\n\n        for i, state in enumerate(states):\n            outputs.append(state)\n            output_names.append(f""initial_state_{i}"")\n\n        self.output_names = output_names\n\n        return tuple(outputs)\n\n    @classmethod\n    def build_from_checkpoints(\n        cls,\n        checkpoint_filenames,\n        src_dict_filename,\n        dst_dict_filename,\n        lexical_dict_paths=None,\n    ):\n        models, src_dict, _ = load_models_from_checkpoints(\n            checkpoint_filenames,\n            src_dict_filename,\n            dst_dict_filename,\n            lexical_dict_paths,\n        )\n        return cls(models, src_dict=src_dict)\n\n\nclass DecoderBatchedStepEnsemble(nn.Module):\n    def __init__(\n        self,\n        models,\n        tgt_dict,\n        beam_size,\n        word_reward=0,\n        unk_reward=0,\n        tile_internal=False,\n    ):\n        super().__init__()\n        self.models = models\n        for i, model in enumerate(self.models):\n            model.prepare_for_onnx_export_()\n            if hasattr(model, ""get_student_model""):\n                model = model.get_student_model()\n                self.models[i] = model\n            self._modules[f""model_{i}""] = model\n\n        self.tgt_dict = tgt_dict\n        self.beam_size = beam_size\n        self.word_reward = word_reward\n        self.unk_reward = unk_reward\n\n        vocab_size = len(tgt_dict.indices)\n        self.word_rewards = torch.FloatTensor(vocab_size).fill_(word_reward)\n        self.word_rewards[tgt_dict.eos()] = 0\n        self.word_rewards[tgt_dict.unk()] = word_reward + unk_reward\n\n        self.tile_internal = tile_internal\n\n        self.enable_precompute_reduced_weights = False\n\n    def forward(self, input_tokens, prev_scores, timestep, *inputs, src_tuple=None):\n        """"""\n        Decoder step inputs correspond one-to-one to encoder outputs.\n        HOWEVER: after the first step, encoder outputs (i.e, the first\n        len(self.models) elements of inputs) must be tiled k (beam size)\n        times on the batch dimension (axis 1).\n        """"""\n        # from flat to (batch x 1)\n        input_tokens = input_tokens.unsqueeze(1)\n\n        (\n            log_probs_per_model,\n            attn_weights_per_model,\n            state_outputs,\n            beam_axis_per_state,\n            possible_translation_tokens,\n        ) = self._get_decoder_outputs(\n            input_tokens, prev_scores, timestep, *inputs, src_tuple=src_tuple\n        )\n\n        average_log_probs = torch.mean(\n            torch.cat(log_probs_per_model, dim=1), dim=1, keepdim=True\n        )\n\n        if possible_translation_tokens is None:\n            word_rewards = self.word_rewards\n        else:\n            word_rewards = self.word_rewards.index_select(\n                0, possible_translation_tokens\n            )\n        word_rewards = word_rewards.unsqueeze(dim=0).unsqueeze(dim=0)\n\n        average_log_probs_with_rewards = average_log_probs + word_rewards\n\n        average_attn_weights = torch.mean(\n            torch.cat(attn_weights_per_model, dim=1), dim=1, keepdim=True\n        )\n\n        best_scores_k_by_k, best_tokens_k_by_k = torch.topk(\n            average_log_probs_with_rewards.squeeze(1), k=self.beam_size\n        )\n\n        prev_scores_k_by_k = prev_scores.view(-1, 1).expand(-1, self.beam_size)\n        total_scores_k_by_k = best_scores_k_by_k + prev_scores_k_by_k\n\n        # flatten to take top k over all (beam x beam) hypos\n        total_scores_flat = total_scores_k_by_k.view(-1)\n        best_tokens_flat = best_tokens_k_by_k.view(-1)\n\n        best_scores, best_indices = torch.topk(total_scores_flat, k=self.beam_size)\n\n        best_tokens = best_tokens_flat.index_select(dim=0, index=best_indices).view(-1)\n\n        # integer division to determine which input produced each successor\n        prev_hypos = best_indices // self.beam_size\n\n        attention_weights = average_attn_weights.index_select(dim=0, index=prev_hypos)\n\n        if possible_translation_tokens is not None:\n            best_tokens = possible_translation_tokens.index_select(\n                dim=0, index=best_tokens\n            )\n\n        self.input_names = [""prev_tokens"", ""prev_scores"", ""timestep""]\n        for i in range(len(self.models)):\n            self.input_names.append(f""fixed_input_{i}"")\n\n        if possible_translation_tokens is not None:\n            self.input_names.append(""possible_translation_tokens"")\n\n        # \'attention_weights_average\' output shape: (src_length x beam_size)\n        attention_weights = attention_weights.squeeze(1)\n\n        outputs = [best_tokens, best_scores, prev_hypos, attention_weights]\n        self.output_names = [\n            ""best_tokens_indices"",\n            ""best_scores"",\n            ""prev_hypos_indices"",\n            ""attention_weights_average"",\n        ]\n        for i in range(len(self.models)):\n            self.output_names.append(f""fixed_input_{i}"")\n            if self.tile_internal:\n                outputs.append(inputs[i].repeat(1, self.beam_size, 1))\n            else:\n                outputs.append(inputs[i])\n\n        if possible_translation_tokens is not None:\n            self.output_names.append(""possible_translation_tokens"")\n            outputs.append(possible_translation_tokens)\n\n        for i, state in enumerate(state_outputs):\n            beam_axis = beam_axis_per_state[i]\n            if beam_axis is None:\n                next_state = state\n            else:\n                next_state = state.index_select(dim=beam_axis, index=prev_hypos)\n            outputs.append(next_state)\n            self.output_names.append(f""state_output_{i}"")\n            self.input_names.append(f""state_input_{i}"")\n\n        return tuple(outputs)\n\n    def _get_decoder_outputs(\n        self, input_tokens, prev_scores, timestep, *inputs, src_tuple=None\n    ):\n        log_probs_per_model = []\n        attn_weights_per_model = []\n        state_outputs = []\n        beam_axis_per_state = []\n        reduced_output_weights_per_model = []\n\n        next_state_input = len(self.models)\n\n        # size of ""batch"" dimension of input as tensor\n        batch_size = torch.onnx.operators.shape_as_tensor(input_tokens)[0]\n\n        possible_translation_tokens = None\n        # underlying assumption is each model has same vocab_reduction_module\n        if hasattr(self.models[0].decoder, ""vocab_reduction_module""):\n            vocab_reduction_module = self.models[0].decoder.vocab_reduction_module\n            if vocab_reduction_module is not None:\n                possible_translation_tokens = inputs[len(self.models)]\n                next_state_input += 1\n\n        futures = []\n\n        for i, model in enumerate(self.models):\n            if (\n                isinstance(model, rnn.RNNModel)\n                or isinstance(model, rnn.DummyPyTextRNNPointerModel)\n                or isinstance(model, char_source_model.CharSourceModel)\n                or isinstance(model, word_prediction_model.WordPredictionModel)\n            ):\n                encoder_output = inputs[i]\n                prev_hiddens = []\n                prev_cells = []\n\n                for _ in range(len(model.decoder.layers)):\n                    prev_hiddens.append(inputs[next_state_input])\n                    prev_cells.append(inputs[next_state_input + 1])\n                    next_state_input += 2\n\n                # ensure previous attention context has batch dimension\n                input_feed_shape = torch.cat(\n                    (batch_size.view(1), torch.LongTensor([-1]))\n                )\n                prev_input_feed = torch.onnx.operators.reshape_from_tensor_shape(\n                    inputs[next_state_input], input_feed_shape\n                )\n                next_state_input += 1\n\n                if (\n                    self.enable_precompute_reduced_weights\n                    and hasattr(model.decoder, ""_precompute_reduced_weights"")\n                    and possible_translation_tokens is not None\n                ):\n                    # (output_projection_w, output_projection_b)\n                    reduced_output_weights = inputs[\n                        next_state_input : next_state_input + 2\n                    ]\n                    next_state_input += 2\n                else:\n                    reduced_output_weights = None\n                reduced_output_weights_per_model.append(reduced_output_weights)\n\n                # no batching, we only care about care about ""max"" length\n                if src_tuple:\n                    src_tokens, src_length = src_tuple\n                    src_tokens = src_tokens.t()\n                else:\n                    # notional, not actually used for decoder computation\n                    src_length_int = int(encoder_output.size()[0])\n                    src_length = torch.LongTensor(np.array([src_length_int]))\n                    src_tokens = torch.LongTensor(np.array([[0] * src_length_int]))\n                src_embeddings = encoder_output.new_zeros(encoder_output.shape)\n\n                encoder_out = (\n                    encoder_output,\n                    prev_hiddens,\n                    prev_cells,\n                    src_length,\n                    src_tokens,\n                    src_embeddings,\n                )\n\n                def forked_section(\n                    input_tokens,\n                    encoder_out,\n                    possible_translation_tokens,\n                    prev_hiddens,\n                    prev_cells,\n                    prev_input_feed,\n                    reduced_output_weights,\n                ):\n                    # store cached states, use evaluation mode\n                    model.decoder._is_incremental_eval = True\n                    model.eval()\n\n                    # placeholder\n                    incremental_state = {}\n\n                    # cache previous state inputs\n                    utils.set_incremental_state(\n                        model.decoder,\n                        incremental_state,\n                        ""cached_state"",\n                        (prev_hiddens, prev_cells, prev_input_feed),\n                    )\n\n                    decoder_output = model.decoder(\n                        input_tokens,\n                        encoder_out,\n                        incremental_state=incremental_state,\n                        possible_translation_tokens=possible_translation_tokens,\n                        reduced_output_weights=reduced_output_weights,\n                    )\n                    logits, attn_scores, _ = decoder_output\n\n                    log_probs = (\n                        logits\n                        if isinstance(model, rnn.DummyPyTextRNNPointerModel)\n                        else F.log_softmax(logits, dim=2)\n                    )\n\n                    log_probs_per_model.append(log_probs)\n                    attn_weights_per_model.append(attn_scores)\n\n                    (\n                        next_hiddens,\n                        next_cells,\n                        next_input_feed,\n                    ) = utils.get_incremental_state(\n                        model.decoder, incremental_state, ""cached_state""\n                    )\n\n                    return (\n                        log_probs,\n                        attn_scores,\n                        tuple(next_hiddens),\n                        tuple(next_cells),\n                        next_input_feed,\n                    )\n\n                fut = torch.jit._fork(\n                    forked_section,\n                    input_tokens,\n                    encoder_out,\n                    possible_translation_tokens,\n                    prev_hiddens,\n                    prev_cells,\n                    prev_input_feed,\n                    reduced_output_weights,\n                )\n\n                futures.append(fut)\n            elif isinstance(model, transformer.TransformerModel) or isinstance(\n                model, char_source_transformer_model.CharSourceTransformerModel\n            ):\n                encoder_output = inputs[i]\n                # store cached states, use evaluation mode\n                model.decoder._is_incremental_eval = True\n                model.eval()\n\n                states_per_layer = 4\n                state_inputs = []\n                for i, _ in enumerate(model.decoder.layers):\n                    # (prev_key, prev_value) for self- and encoder-attention\n                    if hasattr(model.decoder, ""decoder_layers_to_keep"") and (\n                        i not in model.decoder.decoder_layers_to_keep.keys()\n                    ):\n                        continue\n\n                    state_inputs.extend(\n                        inputs[next_state_input : next_state_input + states_per_layer]\n                    )\n                    next_state_input += states_per_layer\n\n                encoder_out = (encoder_output, None, None)\n\n                # TODO(jcross)\n                reduced_output_weights = None\n                reduced_output_weights_per_model.append(reduced_output_weights)\n\n                def forked_section(\n                    input_tokens,\n                    encoder_out,\n                    state_inputs,\n                    possible_translation_tokens,\n                    timestep,\n                ):\n                    decoder_output = model.decoder(\n                        input_tokens,\n                        encoder_out,\n                        incremental_state=state_inputs,\n                        possible_translation_tokens=possible_translation_tokens,\n                        timestep=timestep,\n                    )\n                    logits, attn_scores, _, attention_states = decoder_output\n\n                    log_probs = F.log_softmax(logits, dim=2)\n\n                    return log_probs, attn_scores, tuple(attention_states)\n\n                fut = torch.jit._fork(\n                    forked_section,\n                    input_tokens,\n                    encoder_out,\n                    state_inputs,\n                    possible_translation_tokens,\n                    timestep,\n                )\n\n                futures.append(fut)\n\n            elif isinstance(model, levenshtein_transformer.LevenshteinTransformerModel):\n                encoder_output = inputs[i]\n                # store cached states, use evaluation mode\n                model.decoder._is_incremental_eval = True\n                model.eval()\n\n                states_per_layer = 4\n\n                state_inputs = []\n                for _ in model.decoder.layers:\n                    # (prev_key, prev_value) for self- and encoder-attention\n                    state_inputs.extend(\n                        inputs[next_state_input : next_state_input + states_per_layer]\n                    )\n                    next_state_input += states_per_layer\n\n                encoder_out = (encoder_output, None, None)\n\n                # TODO(jcross)\n                reduced_output_weights = None\n                reduced_output_weights_per_model.append(reduced_output_weights)\n\n                def forked_section(\n                    input_tokens,\n                    encoder_out,\n                    state_inputs,\n                    possible_translation_tokens,\n                    timestep,\n                ):\n                    decoder_output = model.decoder(\n                        input_tokens,\n                        encoder_out,\n                        incremental_state=state_inputs,\n                        possible_translation_tokens=possible_translation_tokens,\n                        timestep=timestep,\n                    )\n                    logits, attn_scores, attention_states = decoder_output\n\n                    log_probs = F.log_softmax(logits, dim=2)\n\n                    return log_probs, attn_scores, tuple(attention_states)\n\n                fut = torch.jit._fork(\n                    forked_section,\n                    input_tokens,\n                    encoder_out,\n                    state_inputs,\n                    possible_translation_tokens,\n                    timestep,\n                )\n\n                futures.append(fut)\n            elif isinstance(model, latent_var_models.LatentVarModel):\n                encoder_output = inputs[i]\n                # store cached states, use evaluation mode\n                model.decoder._is_incremental_eval = True\n                model.eval()\n                state_inputs = []\n                state_inputs.extend(inputs[next_state_input : next_state_input + 3])\n                next_state_input += 3\n                for _ in list(model.decoder.decoders.values())[0].layers:\n                    # (prev_key, prev_value) for self- and encoder-attention\n                    state_inputs.extend(inputs[next_state_input : next_state_input + 4])\n                    next_state_input += 4\n\n                encoder_out = encoder_output\n\n                # TODO(jcross)\n                reduced_output_weights = None\n                reduced_output_weights_per_model.append(reduced_output_weights)\n\n                def forked_section(\n                    input_tokens,\n                    encoder_out,\n                    state_inputs,\n                    possible_translation_tokens,\n                    timestep,\n                ):\n                    decoder_output = model.decoder(\n                        input_tokens, encoder_out, incremental_state=state_inputs\n                    )\n                    logits, attn_scores, _, _, attention_states = decoder_output\n\n                    log_probs = F.log_softmax(logits, dim=2)\n\n                    return log_probs, attn_scores, tuple(attention_states)\n\n                fut = torch.jit._fork(\n                    forked_section,\n                    input_tokens,\n                    encoder_out,\n                    state_inputs,\n                    possible_translation_tokens,\n                    timestep,\n                )\n\n                futures.append(fut)\n\n            elif isinstance(\n                model, hybrid_transformer_rnn.HybridTransformerRNNModel\n            ) or isinstance(model, char_source_hybrid.CharSourceHybridModel):\n                encoder_output = inputs[i]\n\n                # store cached states, use evaluation mode\n                model.decoder._is_incremental_eval = True\n                model.eval()\n\n                encoder_out = (encoder_output, None, None)\n\n                num_states = (1 + model.decoder.num_layers) * 2\n                state_inputs = inputs[next_state_input : next_state_input + num_states]\n                next_state_input += num_states\n\n                # TODO(jcross)\n                reduced_output_weights = None\n                reduced_output_weights_per_model.append(reduced_output_weights)\n\n                def forked_section(\n                    input_tokens,\n                    encoder_out,\n                    state_inputs,\n                    possible_translation_tokens,\n                    timestep,\n                ):\n                    incremental_state = {}\n                    utils.set_incremental_state(\n                        model.decoder, incremental_state, ""cached_state"", state_inputs\n                    )\n\n                    decoder_output = model.decoder(\n                        input_tokens,\n                        encoder_out,\n                        incremental_state=incremental_state,\n                        possible_translation_tokens=possible_translation_tokens,\n                        timestep=timestep,\n                    )\n                    logits, attn_scores, _ = decoder_output\n\n                    log_probs = F.log_softmax(logits, dim=2)\n\n                    next_states = utils.get_incremental_state(\n                        model.decoder, incremental_state, ""cached_state""\n                    )\n\n                    return log_probs, attn_scores, tuple(next_states)\n\n                fut = torch.jit._fork(\n                    forked_section,\n                    input_tokens,\n                    encoder_out,\n                    state_inputs,\n                    possible_translation_tokens,\n                    timestep,\n                )\n\n                futures.append(fut)\n            else:\n                raise RuntimeError(f""Not a supported model: {type(model)}"")\n\n        for i, (model, fut) in enumerate(zip(self.models, futures)):\n            if (\n                isinstance(model, rnn.RNNModel)\n                or isinstance(model, rnn.DummyPyTextRNNPointerModel)\n                or isinstance(model, char_source_model.CharSourceModel)\n                or isinstance(model, word_prediction_model.WordPredictionModel)\n            ):\n                (\n                    log_probs,\n                    attn_scores,\n                    next_hiddens,\n                    next_cells,\n                    next_input_feed,\n                ) = torch.jit._wait(fut)\n\n                for h, c in zip(next_hiddens, next_cells):\n                    state_outputs.extend([h, c])\n                    beam_axis_per_state.extend([0, 0])\n\n                state_outputs.append(next_input_feed)\n                beam_axis_per_state.append(0)\n\n                if reduced_output_weights_per_model[i] is not None:\n                    state_outputs.extend(reduced_output_weights_per_model[i])\n                    beam_axis_per_state.extend(\n                        [None for _ in reduced_output_weights_per_model[i]]\n                    )\n\n            elif isinstance(model, transformer.TransformerModel) or isinstance(\n                model, char_source_transformer_model.CharSourceTransformerModel\n            ):\n                log_probs, attn_scores, attention_states = torch.jit._wait(fut)\n\n                log_probs_per_model.append(log_probs)\n                attn_weights_per_model.append(attn_scores)\n\n                state_outputs.extend(attention_states)\n                beam_axis_per_state.extend([0 for _ in attention_states])\n            elif isinstance(model, levenshtein_transformer.LevenshteinTransformerModel):\n                log_probs, attn_scores, attention_states = torch.jit._wait(fut)\n\n                log_probs_per_model.append(log_probs)\n                attn_weights_per_model.append(attn_scores)\n\n                state_outputs.extend(attention_states)\n                beam_axis_per_state.extend([None for _ in attention_states])\n            elif isinstance(model, latent_var_models.LatentVarModel):\n                log_probs, attn_scores, attention_states = torch.jit._wait(fut)\n\n                log_probs_per_model.append(log_probs)\n                attn_weights_per_model.append(attn_scores)\n                state_outputs.extend(attention_states)\n                beam_axis_per_state.extend([0 for _ in attention_states])\n            elif isinstance(\n                model, hybrid_transformer_rnn.HybridTransformerRNNModel\n            ) or isinstance(model, char_source_hybrid.CharSourceHybridModel):\n                log_probs, attn_scores, next_states = torch.jit._wait(fut)\n\n                log_probs_per_model.append(log_probs)\n                attn_weights_per_model.append(attn_scores)\n\n                state_outputs.extend(next_states)\n                # sequence RNN states have beam along axis 1\n                beam_axis_per_state.extend([1 for _ in next_states[:-2]])\n                # encoder input projections have beam along axis 0\n                beam_axis_per_state.extend([0, 0])\n            else:\n                raise RuntimeError(f""Not a supported model: {type(model)}"")\n\n        return (\n            log_probs_per_model,\n            attn_weights_per_model,\n            state_outputs,\n            beam_axis_per_state,\n            possible_translation_tokens,\n        )\n\n    @classmethod\n    def build_from_checkpoints(\n        cls,\n        checkpoint_filenames,\n        src_dict_filename,\n        dst_dict_filename,\n        beam_size,\n        word_reward=0,\n        unk_reward=0,\n        lexical_dict_paths=None,\n    ):\n        models, _, tgt_dict = load_models_from_checkpoints(\n            checkpoint_filenames,\n            src_dict_filename,\n            dst_dict_filename,\n            lexical_dict_paths,\n        )\n        return cls(\n            models,\n            tgt_dict,\n            beam_size=beam_size,\n            word_reward=word_reward,\n            unk_reward=unk_reward,\n        )\n\n\nclass FakeEncoderEnsemble(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, src_tokens, src_lengths) -> None:\n        raise RuntimeError(\n            ""Called EncoderEnsemble on a BeamSearch thats not word-source""\n        )\n\n\nclass FakeCharSourceEncoderEnsemble(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, src_tokens, src_lengths, char_inds, word_lengths) -> None:\n        raise RuntimeError(\n            ""Called CharSourceEncoderEnsemble on a BeamSearch thats not char-source""\n        )\n\n\nclass BeamSearch(torch.jit.ScriptModule):\n\n    __constants__ = [""beam_size"", ""is_char_source""]\n\n    def __init__(\n        self,\n        model_list,\n        tgt_dict,\n        src_tokens,\n        src_lengths,\n        beam_size=1,\n        word_reward=0,\n        unk_reward=0,\n        quantize=False,\n        # Tracing inputs for CharSourceModel\n        char_inds=None,\n        word_lengths=None,\n    ):\n        super().__init__()\n        self.models = model_list\n        self.tgt_dict = tgt_dict\n        self.beam_size = beam_size\n        self.word_reward = word_reward\n        self.unk_reward = unk_reward\n\n        if (\n            isinstance(self.models[0], char_source_model.CharSourceModel)\n            or isinstance(\n                self.models[0], char_source_transformer_model.CharSourceTransformerModel\n            )\n            or isinstance(self.models[0], char_source_hybrid.CharSourceHybridModel)\n        ):\n            encoder_ens = CharSourceEncoderEnsemble(self.models)\n        else:\n            encoder_ens = EncoderEnsemble(self.models)\n        encoder_ens.enable_precompute_reduced_weights = True\n\n        if quantize:\n            torch.quantization.quantize_dynamic(\n                encoder_ens, {torch.nn.Linear}, dtype=torch.qint8, inplace=True\n            )\n            encoder_ens = torch.jit.quantized.quantize_rnn_cell_modules(encoder_ens)\n\n        if (\n            isinstance(self.models[0], char_source_model.CharSourceModel)\n            or isinstance(\n                self.models[0], char_source_transformer_model.CharSourceTransformerModel\n            )\n            or isinstance(self.models[0], char_source_hybrid.CharSourceHybridModel)\n        ):\n            self.is_char_source = True\n            enc_inputs = (src_tokens, src_lengths, char_inds, word_lengths)\n            example_encoder_outs = encoder_ens(*enc_inputs)\n            self.encoder_ens = FakeEncoderEnsemble()\n            self.encoder_ens_char_source = torch.jit.trace(\n                encoder_ens, enc_inputs, _force_outplace=True, check_trace=False\n            )\n        else:\n            self.is_char_source = False\n            enc_inputs = (src_tokens, src_lengths)\n            example_encoder_outs = encoder_ens(*enc_inputs)\n            self.encoder_ens = torch.jit.trace(\n                encoder_ens, enc_inputs, _force_outplace=True, check_trace=False\n            )\n            self.encoder_ens_char_source = FakeCharSourceEncoderEnsemble()\n\n        decoder_ens = DecoderBatchedStepEnsemble(\n            self.models,\n            tgt_dict,\n            beam_size,\n            word_reward,\n            unk_reward,\n            tile_internal=False,\n        )\n        decoder_ens.enable_precompute_reduced_weights = True\n        if quantize:\n            torch.quantization.quantize_dynamic(\n                decoder_ens, {torch.nn.Linear}, dtype=torch.qint8, inplace=True\n            )\n            decoder_ens = torch.jit.quantized.quantize_rnn_cell_modules(decoder_ens)\n            decoder_ens = torch.jit.quantized.quantize_rnn_modules(decoder_ens)\n        decoder_ens_tile = DecoderBatchedStepEnsemble(\n            self.models,\n            tgt_dict,\n            beam_size,\n            word_reward,\n            unk_reward,\n            tile_internal=True,\n        )\n        decoder_ens_tile.enable_precompute_reduced_weights = True\n        if quantize:\n            torch.quantization.quantize_dynamic(\n                decoder_ens_tile, {torch.nn.Linear}, dtype=torch.qint8, inplace=True\n            )\n            decoder_ens_tile = torch.jit.quantized.quantize_rnn_cell_modules(\n                decoder_ens_tile\n            )\n            decoder_ens_tile = torch.jit.quantized.quantize_rnn_modules(\n                decoder_ens_tile\n            )\n        prev_token = torch.LongTensor([0])\n        prev_scores = torch.FloatTensor([0.0])\n        ts = torch.LongTensor([0])\n        _, _, _, _, *tiled_states = decoder_ens_tile(\n            prev_token, prev_scores, ts, *example_encoder_outs\n        )\n        self.decoder_ens_tile = torch.jit.trace(\n            decoder_ens_tile,\n            (prev_token, prev_scores, ts, *example_encoder_outs),\n            _force_outplace=True,\n            check_trace=False,\n        )\n        self.decoder_ens = torch.jit.trace(\n            decoder_ens,\n            (\n                prev_token.repeat(self.beam_size),\n                prev_scores.repeat(self.beam_size),\n                ts,\n                *tiled_states,\n            ),\n            _force_outplace=True,\n            check_trace=False,\n        )\n\n        self.input_names = [\n            ""src_tokens"",\n            ""src_lengths"",\n            ""prev_token"",\n            ""prev_scores"",\n            ""attn_weights"",\n            ""prev_hypos_indices"",\n            ""num_steps"",\n        ]\n        self.output_names = [\n            ""all_tokens"",\n            ""all_scores"",\n            ""all_weights"",\n            ""all_prev_indices"",\n        ]\n\n    @torch.jit.script_method\n    def forward(\n        self,\n        src_tokens: torch.Tensor,\n        src_lengths: torch.Tensor,\n        prev_token: torch.Tensor,\n        prev_scores: torch.Tensor,\n        attn_weights: torch.Tensor,\n        prev_hypos_indices: torch.Tensor,\n        num_steps: int,\n        char_inds: Optional[torch.Tensor] = None,\n        word_lengths: Optional[torch.Tensor] = None,\n    ):\n        if self.is_char_source:\n            if char_inds is None or word_lengths is None:\n                raise RuntimeError(\n                    ""char_inds and word_lengths must be specified ""\n                    ""for char-source models""\n                )\n            char_inds = torch.jit._unwrap_optional(char_inds)\n            word_lengths = torch.jit._unwrap_optional(word_lengths)\n            enc_states = self.encoder_ens_char_source(\n                src_tokens, src_lengths, char_inds, word_lengths\n            )\n        else:\n            enc_states = self.encoder_ens(src_tokens, src_lengths)\n\n        # enc_states ends up being optional because of the above branch, one\n        # side returns None. We should never take the path that returns None\n        # so we unrap the optional type here.\n        enc_states = torch.jit._unwrap_optional(enc_states)\n\n        all_tokens = prev_token.repeat(repeats=[self.beam_size]).unsqueeze(dim=0)\n        all_scores = prev_scores.repeat(repeats=[self.beam_size]).unsqueeze(dim=0)\n        all_weights = (\n            attn_weights.unsqueeze(dim=0)\n            .repeat(repeats=[self.beam_size, 1])\n            .unsqueeze(dim=0)\n        )\n        all_prev_indices = prev_hypos_indices.unsqueeze(dim=0)\n\n        prev_token, prev_scores, prev_hypos_indices, attn_weights, *states = self.decoder_ens_tile(\n            prev_token, prev_scores, _to_tensor(0), *enc_states  # noqa\n        )\n\n        all_tokens = torch.cat((all_tokens, prev_token.unsqueeze(dim=0)), dim=0)\n        all_scores = torch.cat((all_scores, prev_scores.unsqueeze(dim=0)), dim=0)\n        all_weights = torch.cat((all_weights, attn_weights.unsqueeze(dim=0)), dim=0)\n        all_prev_indices = torch.cat(\n            (all_prev_indices, prev_hypos_indices.unsqueeze(dim=0)), dim=0\n        )\n\n        for i in range(num_steps - 1):\n            (\n                prev_token,\n                prev_scores,\n                prev_hypos_indices,\n                attn_weights,\n                *states,\n            ) = self.decoder_ens(\n                prev_token, prev_scores, _to_tensor(i + 1), *states  # noqa\n            )\n\n            all_tokens = torch.cat((all_tokens, prev_token.unsqueeze(dim=0)), dim=0)\n            all_scores = torch.cat((all_scores, prev_scores.unsqueeze(dim=0)), dim=0)\n            all_weights = torch.cat((all_weights, attn_weights.unsqueeze(dim=0)), dim=0)\n            all_prev_indices = torch.cat(\n                (all_prev_indices, prev_hypos_indices.unsqueeze(dim=0)), dim=0\n            )\n\n        return all_tokens, all_scores, all_weights, all_prev_indices\n\n    @classmethod\n    def build_from_checkpoints(\n        cls,\n        checkpoint_filenames,\n        src_dict_filename,\n        dst_dict_filename,\n        beam_size,\n        word_reward=0,\n        unk_reward=0,\n        lexical_dict_paths=None,\n    ):\n        length = 10\n        models, _, tgt_dict = load_models_from_checkpoints(\n            checkpoint_filenames,\n            src_dict_filename,\n            dst_dict_filename,\n            lexical_dict_paths,\n        )\n        src_tokens = torch.LongTensor(np.ones((length, 1), dtype=""int64""))\n        src_lengths = torch.IntTensor(np.array([length], dtype=""int32""))\n        if (\n            isinstance(models[0], char_source_model.CharSourceModel)\n            or isinstance(\n                models[0], char_source_transformer_model.CharSourceTransformerModel\n            )\n            or isinstance(models[0], char_source_hybrid.CharSourceHybridModel)\n        ):\n            word_length = 3\n            char_inds = torch.LongTensor(\n                np.ones((1, length, word_length), dtype=""int64"")\n            )\n            word_lengths = torch.IntTensor(\n                np.array([word_length] * length, dtype=""int32"")\n            ).reshape((1, length))\n        else:\n            char_inds = None\n            word_lengths = None\n        return cls(\n            models,\n            tgt_dict,\n            src_tokens,\n            src_lengths,\n            beam_size=beam_size,\n            word_reward=word_reward,\n            unk_reward=unk_reward,\n            quantize=True,\n            char_inds=char_inds,\n            word_lengths=word_lengths,\n        )\n\n    def save_to_pytorch(self, output_path):\n        def pack(s):\n            if hasattr(s, ""_pack""):\n                s._pack()\n\n        def unpack(s):\n            if hasattr(s, ""_unpack""):\n                s._unpack()\n\n        self.apply(pack)\n        torch.jit.save(self, output_path)\n        self.apply(unpack)\n\n\nclass KnownOutputDecoderStepEnsemble(nn.Module):\n    def __init__(self, models, tgt_dict, word_reward=0, unk_reward=0):\n        super().__init__()\n        self.models = models\n        self.tgt_dict = tgt_dict\n        for i, model in enumerate(self.models):\n            model.prepare_for_onnx_export_()\n            self._modules[f""model_{i}""] = model\n\n        self.word_reward = word_reward\n        self.unk_reward = unk_reward\n\n        vocab_size = len(tgt_dict.indices)\n        self.word_rewards = torch.FloatTensor(vocab_size).fill_(word_reward)\n        self.word_rewards[tgt_dict.eos()] = 0\n        self.word_rewards[tgt_dict.unk()] = word_reward + unk_reward\n        self.vocab_size = vocab_size\n        self.unk_token = tgt_dict.unk()\n\n        self.enable_precompute_reduced_weights = False\n\n    def forward(self, input_token, target_token, timestep, *inputs):\n        """"""\n        Decoder step inputs correspond one-to-one to encoder outputs.\n        """"""\n        log_probs_per_model = []\n        state_outputs = []\n\n        next_state_input = len(self.models)\n\n        # underlying assumption is each model has same vocab_reduction_module\n        vocab_reduction_module = self.models[0].decoder.vocab_reduction_module\n        if vocab_reduction_module is not None:\n            possible_translation_tokens = inputs[len(self.models)]\n            next_state_input += 1\n        else:\n            possible_translation_tokens = None\n\n        for i, model in enumerate(self.models):\n            encoder_output = inputs[i]\n            prev_hiddens = []\n            prev_cells = []\n\n            for _ in range(len(model.decoder.layers)):\n                prev_hiddens.append(inputs[next_state_input])\n                prev_cells.append(inputs[next_state_input + 1])\n                next_state_input += 2\n            prev_input_feed = inputs[next_state_input].view(1, -1)\n            next_state_input += 1\n\n            if (\n                self.enable_precompute_reduced_weights\n                and hasattr(model.decoder, ""_precompute_reduced_weights"")\n                and possible_translation_tokens is not None\n            ):\n                # (output_projection_w, output_projection_b)\n                reduced_output_weights = inputs[next_state_input : next_state_input + 2]\n                next_state_input += 2\n            else:\n                reduced_output_weights = None\n\n            # no batching, we only care about care about ""max"" length\n            src_length_int = int(encoder_output.size()[0])\n            src_length = torch.LongTensor(np.array([src_length_int]))\n\n            # notional, not actually used for decoder computation\n            src_tokens = torch.LongTensor(np.array([[0] * src_length_int]))\n            src_embeddings = encoder_output.new_zeros(encoder_output.shape)\n\n            encoder_out = (\n                encoder_output,\n                prev_hiddens,\n                prev_cells,\n                src_length,\n                src_tokens,\n                src_embeddings,\n            )\n\n            # store cached states, use evaluation mode\n            model.decoder._is_incremental_eval = True\n            model.eval()\n\n            # placeholder\n            incremental_state = {}\n\n            # cache previous state inputs\n            utils.set_incremental_state(\n                model.decoder,\n                incremental_state,\n                ""cached_state"",\n                (prev_hiddens, prev_cells, prev_input_feed),\n            )\n\n            decoder_output = model.decoder(\n                input_token.view(1, 1),\n                encoder_out,\n                incremental_state=incremental_state,\n                possible_translation_tokens=possible_translation_tokens,\n            )\n            logits, _, _ = decoder_output\n\n            log_probs = F.log_softmax(logits, dim=2)\n\n            log_probs_per_model.append(log_probs)\n\n            (next_hiddens, next_cells, next_input_feed) = utils.get_incremental_state(\n                model.decoder, incremental_state, ""cached_state""\n            )\n\n            for h, c in zip(next_hiddens, next_cells):\n                state_outputs.extend([h, c])\n            state_outputs.append(next_input_feed)\n\n            if reduced_output_weights is not None:\n                state_outputs.extend(reduced_output_weights)\n\n        average_log_probs = torch.mean(\n            torch.cat(log_probs_per_model, dim=0), dim=0, keepdim=True\n        )\n\n        if possible_translation_tokens is not None:\n            reduced_indices = torch.zeros(self.vocab_size).long().fill_(self.unk_token)\n            # ONNX-exportable arange (ATen op)\n            possible_translation_token_range = torch._dim_arange(\n                like=possible_translation_tokens, dim=0\n            )\n            reduced_indices[\n                possible_translation_tokens\n            ] = possible_translation_token_range\n            reduced_index = reduced_indices.index_select(dim=0, index=target_token)\n            score = average_log_probs.view((-1,)).index_select(\n                dim=0, index=reduced_index\n            )\n        else:\n            score = average_log_probs.view((-1,)).index_select(\n                dim=0, index=target_token\n            )\n\n        word_reward = self.word_rewards.index_select(0, target_token)\n        score += word_reward\n\n        self.input_names = [""prev_token"", ""target_token"", ""timestep""]\n        for i in range(len(self.models)):\n            self.input_names.append(f""fixed_input_{i}"")\n\n        if possible_translation_tokens is not None:\n            self.input_names.append(""possible_translation_tokens"")\n\n        outputs = [score]\n        self.output_names = [""score""]\n\n        for i in range(len(self.models)):\n            self.output_names.append(f""fixed_input_{i}"")\n            outputs.append(inputs[i])\n\n        if possible_translation_tokens is not None:\n            self.output_names.append(""possible_translation_tokens"")\n            outputs.append(possible_translation_tokens)\n\n        for i, state in enumerate(state_outputs):\n            outputs.append(state)\n            self.output_names.append(f""state_output_{i}"")\n            self.input_names.append(f""state_input_{i}"")\n\n        return tuple(outputs)\n\n\nclass CharSourceEncoderEnsemble(nn.Module):\n    def __init__(self, models, src_dict=None):\n        super().__init__()\n        self.models = models\n        self.src_dict = src_dict\n        for i, model in enumerate(self.models):\n            model.prepare_for_onnx_export_()\n            self._modules[f""model_{i}""] = model\n\n        self.enable_precompute_reduced_weights = False\n\n    def forward(self, src_tokens, src_lengths, char_inds, word_lengths):\n        outputs = []\n        output_names = []\n        states = []\n\n        # (seq_length, batch_size) for compatibility with Caffe2\n        src_tokens_seq_first = src_tokens.t()\n\n        futures = []\n        for model in self.models:\n            # evaluation mode\n            model.eval()\n            futures.append(\n                torch.jit._fork(\n                    model.encoder,\n                    src_tokens_seq_first,\n                    src_lengths,\n                    char_inds,\n                    word_lengths,\n                )\n            )\n\n        # underlying assumption is each model has same vocab_reduction_module\n        vocab_reduction_module = self.models[0].decoder.vocab_reduction_module\n        possible_translation_tokens = None\n        if vocab_reduction_module is not None:\n            possible_translation_tokens = vocab_reduction_module(\n                src_tokens=src_tokens, decoder_input_tokens=None\n            )\n\n        # Precompute reduced decoder weight matrices.\n        # Once we have possible_translation_tokens, we need to gather rows\n        # out of each output_projection_{w,b} tensor for the decoders to\n        # use. We do it here because these reduced matrices are used on each\n        # step of the beam search, and this turns out to be a relatively\n        # expensive operation.\n        reduced_weights = {}\n        for i, model in enumerate(self.models):\n            if (\n                self.enable_precompute_reduced_weights\n                and hasattr(model.decoder, ""_precompute_reduced_weights"")\n                and possible_translation_tokens is not None\n            ):\n                reduced_weights[i] = torch.jit._fork(\n                    model.decoder._precompute_reduced_weights,\n                    possible_translation_tokens,\n                )\n\n        # XXX: This loop is where we wait() for each encoder\'s output to be\n        # ready. If you\'re trying to add more ops, they should probably not\n        # go in this loop!\n        for i, (model, future) in enumerate(zip(self.models, futures)):\n            encoder_out = torch.jit._wait(future)\n\n            # ""primary"" encoder output (vector representations per source token)\n            encoder_outputs = encoder_out[0]\n            outputs.append(encoder_outputs)\n            output_names.append(f""encoder_output_{i}"")\n\n            if hasattr(model.decoder, ""_init_prev_states""):\n                states.extend(model.decoder._init_prev_states(encoder_out))\n            if (\n                self.enable_precompute_reduced_weights\n                and hasattr(model.decoder, ""_precompute_reduced_weights"")\n                and possible_translation_tokens is not None\n            ):\n                states.extend(torch.jit._wait(reduced_weights[i]))\n\n        if possible_translation_tokens is not None:\n            outputs.append(possible_translation_tokens)\n            output_names.append(""possible_translation_tokens"")\n\n        for i, state in enumerate(states):\n            outputs.append(state)\n            output_names.append(f""initial_state_{i}"")\n\n        self.output_names = output_names\n\n        return tuple(outputs)\n\n    @classmethod\n    def build_from_checkpoints(\n        cls,\n        checkpoint_filenames,\n        src_dict_filename,\n        dst_dict_filename,\n        lexical_dict_paths=None,\n    ):\n        models, src_dict, _ = load_models_from_checkpoints(\n            checkpoint_filenames,\n            src_dict_filename,\n            dst_dict_filename,\n            lexical_dict_paths,\n        )\n        return cls(models, src_dict=src_dict)\n\n\nclass BeamSearchAndDecode(torch.jit.ScriptModule):\n    """"""\n    Combines the functionality of BeamSearch and BeamDecode\n    """"""\n\n    def __init__(\n        self,\n        models,\n        tgt_dict,\n        src_tokens,\n        src_lengths,\n        eos_token_id,\n        length_penalty,\n        nbest,\n        beam_size,\n        stop_at_eos,\n        word_reward=0,\n        unk_reward=0,\n        quantize=False,\n    ):\n        super().__init__()\n\n        self.beam_search = BeamSearch(\n            models,\n            tgt_dict,\n            src_tokens,\n            src_lengths,\n            beam_size,\n            word_reward,\n            unk_reward,\n            quantize,\n        )\n\n        self.beam_decode = BeamDecode(\n            eos_token_id, length_penalty, nbest, beam_size, stop_at_eos\n        )\n\n        self.input_names = [\n            ""src_tokens"",\n            ""src_lengths"",\n            ""prev_token"",\n            ""prev_scores"",\n            ""attn_weights"",\n            ""prev_hypos_indices"",\n            ""num_steps"",\n        ]\n        self.output_names = [\n            ""beam_output"",\n            ""hypothesis_score"",\n            ""token_level_scores"",\n            ""back_alignment_weights"",\n            ""best_indices"",\n        ]\n\n    @torch.jit.script_method\n    def forward(\n        self,\n        src_tokens: torch.Tensor,\n        src_lengths: torch.Tensor,\n        prev_token: torch.Tensor,\n        prev_scores: torch.Tensor,\n        attn_weights: torch.Tensor,\n        prev_hypos_indices: torch.Tensor,\n        num_steps: int,\n    ) -> List[Tuple[Tensor, float, List[float], Tensor, Tensor]]:\n\n        beam_search_out = self.beam_search(\n            src_tokens,\n            src_lengths,\n            prev_token,\n            prev_scores,\n            attn_weights,\n            prev_hypos_indices,\n            num_steps,\n        )\n        all_tokens, all_scores, all_weights, all_prev_indices = beam_search_out\n\n        outputs = torch.jit.annotate(\n            List[Tuple[Tensor, float, List[float], Tensor, Tensor]], []\n        )\n        outputs = self.beam_decode(\n            all_tokens, all_scores, all_weights, all_prev_indices, num_steps\n        )\n\n        return outputs\n\n    @classmethod\n    def build_from_checkpoints(\n        cls,\n        checkpoint_filenames,\n        src_dict_filename,\n        dst_dict_filename,\n        beam_size,\n        length_penalty,\n        nbest,\n        word_reward=0,\n        unk_reward=0,\n        lexical_dict_paths=None,\n    ):\n        length = 10\n        models, _, tgt_dict = load_models_from_checkpoints(\n            checkpoint_filenames,\n            src_dict_filename,\n            dst_dict_filename,\n            lexical_dict_paths,\n        )\n        src_tokens = torch.LongTensor(np.ones((length, 1), dtype=""int64""))\n        src_lengths = torch.IntTensor(np.array([length], dtype=""int32""))\n        eos_token_id = tgt_dict.eos()\n\n        return cls(\n            models,\n            tgt_dict,\n            src_tokens,\n            src_lengths,\n            eos_token_id,\n            length_penalty=length_penalty,\n            nbest=nbest,\n            beam_size=beam_size,\n            stop_at_eos=True,\n            word_reward=word_reward,\n            unk_reward=unk_reward,\n            quantize=True,\n        )\n\n    def save_to_pytorch(self, output_path):\n        def pack(s):\n            if hasattr(s, ""_pack""):\n                s._pack()\n\n        def unpack(s):\n            if hasattr(s, ""_unpack""):\n                s._unpack()\n\n        self.apply(pack)\n        torch.jit.save(self, output_path)\n        self.apply(unpack)\n\n\n@torch.jit.script\ndef finalize_hypos_loop_tokens(\n    finalized_tokens_list: List[Tensor],\n    finalized_idxs,\n    pad_idx: int,\n    finalized_tokens,\n    finalized_scores,\n):\n    for i in range(finalized_idxs.size(0)):\n        cutoff = finalized_tokens[i].ne(pad_idx)\n        tokens = finalized_tokens[i][cutoff]\n        finalized_tokens_list[finalized_idxs[i]] = tokens\n    return finalized_tokens_list\n\n\n@torch.jit.script\ndef finalize_hypos_loop_scores(\n    finalized_scores_list: List[Tensor],\n    finalized_idxs,\n    pad_idx: int,\n    finalized_tokens,\n    finalized_scores,\n):\n    for i in range(finalized_idxs.size(0)):\n        cutoff = finalized_scores[i].ne(pad_idx)\n        scores = finalized_scores[i][cutoff]\n        finalized_scores_list[finalized_idxs[i]] = scores\n    return finalized_scores_list\n\n\n@torch.jit.script\ndef finalize_hypos_loop_attns(\n    finalized_attns_list: List[Tensor],\n    finalized_alignments_list: List[Tensor],\n    finalized_idxs,\n    pad_idx: int,\n    finalized_tokens,\n    finalized_scores,\n    finalized_attn,\n):\n    for i in range(finalized_idxs.size(0)):\n        cutoff = finalized_tokens[i].ne(pad_idx)\n        hypo_attn = finalized_attn[i][cutoff]\n        alignment = hypo_attn.max(dim=1)[1]\n        finalized_attns_list[finalized_idxs[i]] = hypo_attn\n        finalized_alignments_list[finalized_idxs[i]] = alignment\n\n    return finalized_attns_list, finalized_alignments_list\n\n\nclass IterativeRefinementGenerateAndDecode(torch.jit.ScriptModule):\n    def __init__(self, models, tgt_dict, max_iter=1, quantize=True, check_trace=True):\n        super().__init__()\n        src_tokens = torch.tensor([[4, 2]])\n        src_lengths = torch.tensor([2])\n        self.models = models\n\n        generator = IterativeRefinementGenerator(\n            self.models, tgt_dict, max_iter=max_iter\n        )\n        if quantize:\n            generator = torch.quantization.quantize_dynamic(\n                generator, {torch.nn.Linear}, dtype=torch.qint8, inplace=True\n            )\n        enc_inputs = (src_tokens, src_lengths)\n        self.generator = torch.jit.trace(\n            generator, enc_inputs, _force_outplace=True, check_trace=check_trace\n        )\n\n    @torch.jit.script_method\n    def forward(\n        self, src_tokens: torch.Tensor, src_lengths: torch.Tensor\n    ) -> List[Tuple[Tensor, float, Tensor]]:\n\n        return [\n            (x.long(), float(y), at)\n            for x, y, at in list(self.generator(src_tokens.t(), src_lengths))\n        ]\n\n    def save_to_pytorch(self, output_path):\n        def pack(s):\n            if hasattr(s, ""_pack""):\n                s._pack()\n\n        def unpack(s):\n            if hasattr(s, ""_unpack""):\n                s._unpack()\n\n        self.apply(pack)\n        torch.jit.save(self, output_path)\n        self.apply(unpack)\n\n    @classmethod\n    def build_from_checkpoints(\n        cls,\n        checkpoint_filenames,\n        src_dict_filename,\n        tgt_dict_filename,\n        lexical_dict_paths=None,\n        max_iter=1,\n    ):\n        models, _, tgt_dict = load_models_from_checkpoints(\n            checkpoint_filenames,\n            src_dict_filename,\n            tgt_dict_filename,\n            lexical_dict_paths,\n        )\n        return cls(models, tgt_dict=tgt_dict, max_iter=max_iter)\n\n\n@torch.jit.script\ndef is_a_loop(pad_idx: int, x, y, s, a):\n    b, l_x, l_y = x.size(0), x.size(1), y.size(1)\n    if l_x > l_y:\n        y = torch.cat([y, torch.zeros([b, l_x - l_y]).to(y).fill_(pad_idx)], 1)\n        s = torch.cat([s, torch.zeros([b, l_x - l_y]).to(s)], 1)\n        if a.size()[0] > 0:\n            a = torch.cat([a, torch.zeros([b, l_x - l_y, a.size(2)]).to(a)], 1)\n    elif l_x < l_y:\n        x = torch.cat([x, torch.zeros([b, l_y - l_x]).to(x).fill_(pad_idx)], 1)\n    return (x == y).all(1), y, s, a\n\n\n@torch.jit.script\ndef last_step(step: int, max_iter: int, terminated):\n    if step == max_iter:  # reach last iteration, terminate\n        terminated.fill_(1)\n    return terminated\n\n\nclass IterativeRefinementGenerator(nn.Module):\n    def __init__(\n        self,\n        models,\n        tgt_dict,\n        eos_penalty=0.0,\n        max_iter=2,\n        max_ratio=2,\n        decoding_format=None,\n        retain_dropout=False,\n        adaptive=True,\n    ):\n        """"""\n        Generates translations based on iterative refinement.\n\n        Args:\n            tgt_dict: target dictionary\n        """"""\n        super().__init__()\n        self.models = models\n        self.bos = tgt_dict.bos()\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.eos_penalty = eos_penalty\n        self.max_iter = max_iter\n        self.max_ratio = max_ratio\n        self.decoding_format = decoding_format\n        self.retain_dropout = retain_dropout\n        self.adaptive = adaptive\n        for i, model in enumerate(self.models):\n            model.prepare_for_onnx_export_()\n            model.eval()\n            if hasattr(model, ""get_student_model""):\n                model = model.get_student_model()\n                self.models[i] = model\n            self._modules[f""model_{i}""] = model\n\n    def forward(\n        self, src_tokens: torch.Tensor, src_lengths: torch.Tensor\n    ) -> Tuple[Tuple[Tensor, Tensor, Tensor]]:\n\n        o1, o2, o3, _ = self.generate(self.models, src_tokens, src_lengths)\n        return tuple((x, y.float().mean(), z) for x, y, z in zip(o1, o2, o3))\n\n    @torch.no_grad()\n    def generate(self, models, src_tokens, src_lengths, prefix_tokens=None):\n\n        # TODO: model ensemble\n        assert len(models) == 1, ""only support single model""\n        model = models[0]\n        bsz, src_len = src_tokens.size()\n        sent_idxs = torch.arange(bsz)\n        # encoding\n        encoder_out = model.encoder(src_tokens, src_lengths)\n\n        # initialize buffers (very model specific, with length prediction or not)\n        prev_decoder_out = model.initialize_output_tokens(encoder_out, src_tokens)\n        prev_output_tokens = prev_decoder_out.output_tokens.clone()\n\n        finalized_tokens_list = [torch.tensor(0) for _ in range(bsz)]\n        finalized_scores_list = [torch.tensor(0) for _ in range(bsz)]\n        finalized_attns_list = [torch.tensor(0) for _ in range(bsz)]\n        finalized_alignments_list = [torch.tensor(0) for _ in range(bsz)]\n\n        for step in range(self.max_iter + 1):\n            prev_decoder_out = prev_decoder_out._replace(\n                step=step, max_step=self.max_iter + 1\n            )\n            decoder_out = model.forward_decoder(\n                prev_decoder_out,\n                encoder_out,\n                eos_penalty=self.eos_penalty,\n                max_ratio=self.max_ratio,\n                decoding_format=self.decoding_format,\n            )\n            terminated, output_tokens, output_scores, output_attn = is_a_loop(\n                self.pad,\n                prev_output_tokens,\n                decoder_out.output_tokens,\n                decoder_out.output_scores,\n                decoder_out.attn,\n            )\n            decoder_out = decoder_out._replace(\n                output_tokens=output_tokens,\n                output_scores=output_scores,\n                attn=output_attn,\n            )\n\n            terminated = last_step(step, self.max_iter, terminated)\n            # collect finalized sentences\n            finalized_idxs = sent_idxs[terminated]\n            finalized_tokens = decoder_out.output_tokens[terminated]\n            finalized_scores = decoder_out.output_scores[terminated]\n            finalized_attn = (\n                None if decoder_out.attn is None else decoder_out.attn[terminated]\n            )\n            finalized_tokens_list = finalize_hypos_loop_tokens(\n                finalized_tokens_list,\n                finalized_idxs,\n                self.pad,\n                finalized_tokens,\n                finalized_scores,\n            )\n            finalized_scores_list = finalize_hypos_loop_scores(\n                finalized_scores_list,\n                finalized_idxs,\n                self.pad,\n                finalized_tokens,\n                finalized_scores,\n            )\n            finalized_attns_list, finalized_alignments_list = finalize_hypos_loop_attns(\n                finalized_attns_list,\n                finalized_alignments_list,\n                finalized_idxs,\n                self.pad,\n                finalized_tokens,\n                finalized_scores,\n                finalized_attn,\n            )\n\n            # for next step\n            not_terminated = ~terminated\n            prev_decoder_out = decoder_out._replace(\n                output_tokens=script_skip_tensor(\n                    decoder_out.output_tokens, not_terminated\n                ),\n                output_scores=script_skip_tensor(\n                    decoder_out.output_scores, not_terminated\n                ),\n                attn=decoder_out.attn,\n                step=decoder_out.step,\n                max_step=decoder_out.max_step,\n            )\n            encoder_out = EncoderOut(\n                encoder_out=script_skip_tensor(encoder_out.encoder_out, ~terminated),\n                encoder_padding_mask=None,\n                encoder_embedding=script_skip_tensor(\n                    encoder_out.encoder_embedding, ~terminated\n                ),\n                encoder_states=None,\n                src_tokens=None,\n                src_lengths=None,\n            )\n            sent_idxs = script_skip_tensor(sent_idxs, not_terminated)\n\n            prev_output_tokens = prev_decoder_out.output_tokens.clone()\n\n        return (\n            finalized_tokens_list,\n            finalized_scores_list,\n            finalized_attns_list,\n            finalized_alignments_list,\n        )\n'"
pytorch_translate/evals.py,0,"b'#!/usr/bin/env python3\n\nimport math\nimport time\nfrom collections import OrderedDict, defaultdict\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom fairseq import distributed_utils, progress_bar, utils\nfrom fairseq.meters import AverageMeter\nfrom pytorch_translate import checkpoint, generate, utils as pytorch_translate_utils\nfrom pytorch_translate.dual_learning.dual_learning_task import DualLearningTask\nfrom pytorch_translate.tasks.pytorch_translate_multi_task import (\n    PyTorchTranslateMultiTask,\n)\n\n\ndef log_mid_epoch_stats(trainer, progress, extra_meters, log_output):\n    stats = get_training_stats(trainer)\n    for k, v in log_output.items():\n        if k in [""loss"", ""nll_loss"", ""ntokens"", ""nsentences"", ""sample_size""]:\n            continue  # these are already logged above\n        if ""loss"" in k:\n            extra_meters[k].update(v, log_output[""sample_size""])\n        else:\n            extra_meters[k].update(v)\n        stats[k] = extra_meters[k].avg\n    progress.log(stats)\n    return stats\n\n\ndef log_end_epoch_stats(trainer, progress, extra_meters):\n    stats = get_training_stats(trainer)\n    for k, meter in extra_meters.items():\n        stats[k] = meter.avg\n    progress.print(stats)\n    return stats\n\n\ndef get_training_stats(trainer):\n    stats = OrderedDict()\n    if trainer.get_meter(""train_loss"") is not None:\n        avg = trainer.get_meter(""train_loss"").avg\n        if avg is not None:\n            stats[""loss""] = f""{avg:.3f}""\n    if trainer.get_meter(""train_nll_loss"").count > 0:\n        nll_loss = trainer.get_meter(""train_nll_loss"").avg\n        stats[""nll_loss""] = f""{nll_loss:.3f}""\n    else:\n        nll_loss = trainer.get_meter(""train_nll_loss"").avg\n    stats[""ppl""] = get_perplexity(nll_loss) if nll_loss is not None else -1.0\n    if trainer.get_meter(""wps"") is not None:\n        stats[""wps""] = (\n            round(utils.item(trainer.get_meter(""wps"").avg))\n            if trainer.get_meter(""wps"").avg\n            else None\n        )\n    if trainer.get_meter(""ups"") is not None:\n        stats[""ups""] = (\n            f""{trainer.get_meter(\'ups\').avg:.1f}""\n            if trainer.get_meter(""ups"").avg\n            else None\n        )\n    if trainer.get_meter(""wpb"") is not None:\n        stats[""wpb""] = (\n            round(utils.item(trainer.get_meter(""wpb"").avg))\n            if trainer.get_meter(""wpb"").avg\n            else None\n        )\n    if trainer.get_meter(""bsz"") is not None:\n        stats[""bsz""] = (\n            round(utils.item(trainer.get_meter(""bsz"").avg))\n            if trainer.get_meter(""bsz"").avg\n            else None\n        )\n    stats[""num_updates""] = trainer.get_num_updates()\n    stats[""lr""] = trainer.get_lr()\n    if trainer.get_meter(""gnorm"") is not None:\n        stats[""gnorm""] = (\n            f""{trainer.get_meter(\'gnorm\').avg:.3f}""\n            if trainer.get_meter(""gnorm"").avg\n            else None\n        )\n    if trainer.get_meter(""clip"") is not None:\n        stats[""clip""] = (\n            f""{trainer.get_meter(\'clip\').avg:.0%}""\n            if trainer.get_meter(""clip"").avg\n            else None\n        )\n    if trainer.get_meter(""oom"") is not None:\n        stats[""oom""] = (\n            trainer.get_meter(""oom"").avg if trainer.get_meter(""oom"").avg else None\n        )\n    if trainer.get_meter(""loss_scale"") is not None:\n        stats[""loss_scale""] = (\n            f""{trainer.get_meter(\'loss_scale\').avg:.3f}""\n            if trainer.get_meter(""loss_scale"").avg\n            else None\n        )\n    if trainer.get_meter(""wall"") is not None:\n        stats[""wall""] = (\n            round(utils.item(trainer.get_meter(""wall"").elapsed_time))\n            if trainer.get_meter(""wall"").elapsed_time\n            else None\n        )\n    if trainer.get_meter(""train_wall"") is not None:\n        stats[""train_wall""] = (\n            round(utils.item(trainer.get_meter(""train_wall"").sum))\n            if trainer.get_meter(""train_wall"").sum\n            else None\n        )\n    return stats\n\n\ndef get_valid_stats(trainer):\n    stats = OrderedDict()\n    stats[""valid_loss""] = trainer.get_meter(""valid_loss"").avg\n    if trainer.get_meter(""valid_nll_loss"").count > 0:\n        nll_loss = trainer.get_meter(""valid_nll_loss"").avg\n        stats[""valid_nll_loss""] = nll_loss\n    else:\n        nll_loss = trainer.get_meter(""valid_loss"").avg\n    stats[""valid_ppl""] = get_perplexity(nll_loss)\n    return stats\n\n\ndef get_perplexity(loss):\n    try:\n        return f""{math.pow(2, loss):.2f}""\n    except OverflowError:\n        return float(""inf"")\n\n\ndef eval_tune_loss(args, trainer, task, subset, extra_state):\n    """"""Evaluate the model on the validation set and return the average loss.""""""\n    # Initialize dataloader\n    itr = task.get_batch_iterator(\n        dataset=task.dataset(subset),\n        max_tokens=args.max_tokens,\n        max_sentences=args.max_sentences_valid,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), trainer.get_model().max_positions()\n        ),\n        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=8,\n        seed=args.seed,\n        num_shards=args.distributed_world_size,\n        shard_id=args.distributed_rank,\n        num_workers=args.num_workers,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.build_progress_bar(\n        args=args,\n        iterator=itr,\n        epoch=extra_state[""epoch""],\n        prefix=f""valid on \'{subset}\' subset"",\n        no_progress_bar=""simple"",\n    )\n\n    # reset validation loss meters\n    for k in [""valid_loss"", ""valid_nll_loss""]:\n        meter = trainer.get_meter(k)\n        if meter is not None:\n            meter.reset()\n\n    extra_meters = defaultdict(lambda: AverageMeter())\n    for sample in progress:\n        log_output = trainer.valid_step(sample)\n\n        # log mid-validation stats\n        stats = get_valid_stats(trainer)\n        for k, v in log_output.items():\n            if k in [""loss"", ""nll_loss"", ""ntokens"", ""nsentences"", ""sample_size""]:\n                continue\n            if ""loss"" in k:\n                extra_meters[k].update(v, log_output[""sample_size""])\n            else:\n                extra_meters[k].update(v)\n            stats[k] = extra_meters[k].avg\n        progress.log(stats)\n\n    # log validation stats\n    stats = get_valid_stats(trainer)\n    for k, meter in extra_meters.items():\n        stats[k] = meter.avg\n    progress.print(stats)\n\n    extra_state[""tune_eval""][""loss""] = stats[""valid_loss""]\n    extra_state[""tune_eval""][""perplexity""] = stats[""valid_ppl""]\n\n    if (\n        extra_state[""tune_eval""][""lowest_loss""] is None\n        or extra_state[""tune_eval""][""loss""] < extra_state[""tune_eval""][""lowest_loss""]\n    ):\n        extra_state[""tune_eval""][""lowest_loss""] = extra_state[""tune_eval""][""loss""]\n        extra_state[""tune_eval""][""num_since_best""] = 0\n    else:\n        extra_state[""tune_eval""][""num_since_best""] += 1\n\n    stop_due_to_tune_loss = False\n    if (\n        args.stop_no_best_validate_loss >= 0\n        and extra_state[""tune_eval""][""num_since_best""] > args.stop_no_best_validate_loss\n    ):\n        stop_due_to_tune_loss = True\n        print(\n            f""Stopping training due to eval tune loss stagnation - last best ""\n            f""eval tune loss of {extra_state[\'tune_eval\'][\'lowest_loss\']} ""\n            f""(current loss: {extra_state[\'tune_eval\'][\'loss\']}) ""\n            f""was {extra_state[\'tune_eval\'][\'num_since_best\']} validations ago.""\n        )\n    return extra_state, stop_due_to_tune_loss\n\n\ndef is_training_over_time_limit(extra_state: Dict[str, Any], stop_time: float) -> bool:\n    elapsed_hr = (\n        time.time() - extra_state[""start_time""] + extra_state[""previous_training_time""]\n    ) / (60 * 60)\n    if stop_time >= 0 and elapsed_hr > stop_time:\n        print(\n            f""Stopping training due to stop time limit of {stop_time} hours - ""\n            f""we\'ve trained for {elapsed_hr} hours.""\n        )\n        return True\n    return False\n\n\ndef save_and_eval(\n    args,\n    trainer,\n    task,\n    extra_state: Dict[str, Any],\n    checkpoint_manager: Optional[checkpoint.CheckpointManager],\n    end_of_epoch=False,\n) -> Tuple[Dict[str, Any], bool, Optional[List]]:\n    # Checks for time limit stopping criterion even when we\'re not doing\n    # eval/saving checkpoints.\n    max_update = args.max_update or math.inf\n    stop_due_to_max_update = trainer.get_num_updates() > max_update\n    stop_due_to_time_limit = is_training_over_time_limit(extra_state, args.stop_time_hr)\n    if not end_of_epoch and (\n        args.save_interval_updates <= 0\n        or (extra_state[""num_iterations""] % args.save_interval_updates != 0)\n    ):\n        return extra_state, stop_due_to_time_limit\n\n    # Update training time before saving the checkpoint.\n    time_now: float = time.time()\n    extra_state[""previous_training_time""] += time_now - extra_state[""start_time""]\n    extra_state[""start_time""] = time_now\n\n    # Under multiprocessing, each process will run eval over a different\n    # shard of the tune data set and then aggregate the results across all\n    # processes, so the eval stats from all processes\' trainer should\n    # remain synchronized.\n\n    # Tune loss\n    extra_state, stop_due_to_tune_loss = eval_tune_loss(\n        args=args,\n        trainer=trainer,\n        task=task,\n        subset=args.valid_subset,\n        extra_state=extra_state,\n    )\n\n    is_master: bool = distributed_utils.is_master(args)\n    if is_master:\n        assert checkpoint_manager is not None, (\n            f""Master worker (rank {args.distributed_rank}) should ""\n            f""have a checkpoint_manager defined.""\n        )\n    else:\n        assert checkpoint_manager is None, (\n            f""Non-master worker (rank {args.distributed_rank}) should not ""\n            f""have a checkpoint_manager defined.""\n        )\n\n    if is_master:\n        averaged_params: OrderedDict = checkpoint_manager.get_averaged_params(\n            new_params=trainer.get_model().state_dict()\n        )\n        new_best_averaged_checkpoint = extra_state[""tune_eval""][""num_since_best""] == 0\n        # checkpoint_manager takes ownership of averaged_params.\n        extra_state = checkpoint_manager.save(\n            args=args,\n            trainer=trainer,\n            extra_state=extra_state,\n            new_averaged_params=averaged_params,\n        )\n        if new_best_averaged_checkpoint:\n            checkpoint_manager.save_best_averaged_checkpoint(\n                args=args, trainer=trainer, extra_state=extra_state\n            )\n\n    master_stop_training = None\n    if is_master:\n        master_stop_training = (\n            stop_due_to_time_limit or stop_due_to_tune_loss or stop_due_to_max_update\n        )\n    stop_training = pytorch_translate_utils.all_gather_from_master(\n        args=args, data=[master_stop_training]\n    )[0]\n\n    # TODO: fix after masked lm work completes\n    if ""save_only"" not in args or not args.save_only:\n        # Basic sanity checks that extra_state is populated correctly.\n        assert (\n            extra_state[""tune_eval""][""loss""] is not None\n            and extra_state[""tune_eval""][""perplexity""] is not None\n        )\n    return extra_state, stop_training\n'"
pytorch_translate/file_io.py,0,"b'#!/usr/bin/env python3\n\n""""""\nTODO(T55884145): Deprecate this in favor of using\nfvcore.common.file_io.PathManager directly.\n""""""\nfrom fairseq.file_io import PathManager  # noqa\n'"
pytorch_translate/generate.py,7,"b'#!/usr/bin/env python3\n\nimport argparse\nimport collections\nimport os\nimport pickle\nfrom typing import List, NamedTuple, Optional\n\nimport numpy as np\nimport torch\nfrom fairseq import (\n    bleu,\n    data,\n    iterative_refinement_generator,\n    options,\n    progress_bar,\n    tasks,\n    utils,\n)\nfrom fairseq.meters import StopwatchMeter, TimeMeter\nfrom fairseq.models import FairseqEncoderDecoderModel, FairseqMultiModel\nfrom pytorch_translate import hybrid_transformer_rnn  # noqa\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate import transformer  # noqa\nfrom pytorch_translate import (\n    beam_decode,\n    char_aware_hybrid,\n    char_source_hybrid,\n    char_source_model,\n    char_source_transformer_model,\n    options as pytorch_translate_options,\n    utils as pytorch_translate_utils,\n)\nfrom pytorch_translate.constants import CHECKPOINT_PATHS_DELIMITER\nfrom pytorch_translate.data import data as pytorch_translate_data\nfrom pytorch_translate.dual_learning.dual_learning_models import DualLearningModel\nfrom pytorch_translate.research.beam_search import competing_completed\nfrom pytorch_translate.research.multisource import multisource_data, multisource_decode\nfrom pytorch_translate.tasks.pytorch_translate_multi_task import (\n    PyTorchTranslateMultiTask,\n)\nfrom pytorch_translate.tasks.pytorch_translate_task import PytorchTranslateTask\n\n\ntry:\n    from fairseq.models.fb_levenshtein_transformer import (\n        LevenshteinTransformerModel,\n    )  # noqa\nexcept ImportError:\n    from fairseq.models.nat import LevenshteinTransformerModel\n\n\ndef generate_score(\n    args: argparse.Namespace,\n    task: tasks.FairseqTask,\n    dataset: data.FairseqDataset,\n    models: List[FairseqEncoderDecoderModel],\n    lang_pair: Optional[str] = None,\n    modify_target_dict: bool = True,\n):\n    """"""\n    Generation for single and multi model training\n\n    Args:\n        args: Command-line arguments.\n        task: FairseqTask object.\n        dataset: Dataset set object for a specific split for a specific model\n        models: List[FairseqEncoderDecoderModel], an ensemble of models\n        lang_pair: Optional model key in a multi model object. Specify None in\n            single model set up\n    """"""\n    if lang_pair and len(models) > 0 and isinstance(models[0], FairseqMultiModel):\n        if isinstance(dataset, data.RoundRobinZipDatasets):\n            dataset = dataset.datasets[lang_pair]\n        return _generate_score(\n            models=[multi_model.models[lang_pair] for multi_model in models],\n            args=args,\n            task=task,\n            dataset=dataset,\n            modify_target_dict=modify_target_dict,\n        )\n    elif lang_pair and len(models) > 0 and isinstance(models[0], DualLearningModel):\n        # TODO: this could be refactored to use lang_pari as key too\n        return _generate_score(\n            models=[\n                multi_model.models[""primal""]\n                if lang_pair == ""primal_parallel""\n                else multi_model.models[""dual""]\n                for multi_model in models\n            ],\n            args=args,\n            task=task,\n            dataset=dataset,\n            modify_target_dict=modify_target_dict,\n        )\n\n    else:\n        return _generate_score(\n            models=models,\n            args=args,\n            task=task,\n            dataset=dataset,\n            modify_target_dict=modify_target_dict,\n        )\n\n\nclass TranslationInfo(NamedTuple):\n    sample_id: torch.Tensor\n    src_tokens: torch.Tensor\n    target_tokens: torch.Tensor\n    hypo_tokens: torch.Tensor\n    src_str: str\n    target_str: str\n    hypo_str: str\n    hypo_score: float\n    best_hypo_tokens: Optional[torch.Tensor]\n    hypos: List[dict]\n\n\ndef build_sequence_generator(args, task, models):\n    use_cuda = torch.cuda.is_available() and not args.cpu\n    # Initialize generator\n    model_weights = None\n    if args.model_weights:\n        model_weights = [float(w.strip()) for w in args.model_weights.split("","")]\n    use_char_source = (\n        isinstance(models[0], char_source_model.CharSourceModel)\n        or isinstance(\n            models[0], char_source_transformer_model.CharSourceTransformerModel\n        )\n        or isinstance(models[0], char_source_hybrid.CharSourceHybridModel)\n    )\n    # Use a different sequence generator in the multisource setting\n    if getattr(args, ""source_ensembling"", False):\n        translator_class = multisource_decode.MultiSourceSequenceGenerator\n    elif getattr(args, ""competing_completed_beam_search"", False):\n        translator_class = competing_completed.CompetingCompletedSequenceGenerator\n    elif isinstance(models[0], LevenshteinTransformerModel):\n        translator_class = iterative_refinement_generator.IterativeRefinementGenerator\n        return translator_class(tgt_dict=task.target_dictionary, models=models)\n    else:\n        translator_class = beam_decode.SequenceGenerator\n    translator = translator_class(\n        models,\n        tgt_dict=task.target_dictionary,\n        beam_size=args.beam,\n        stop_early=(not args.no_early_stop),\n        normalize_scores=(not args.unnormalized),\n        len_penalty=args.length_penalty,\n        unk_reward=args.unk_reward,\n        word_reward=args.word_reward,\n        model_weights=model_weights,\n        use_char_source=use_char_source,\n        diverse_beam_groups=args.diverse_beam_groups,\n        diverse_beam_strength=args.diverse_beam_strength,\n        diversity_sibling_gamma=args.diversity_sibling_gamma,\n        sampling=args.sampling,\n        sampling_topk=args.sampling_topk,\n        temperature=args.temperature,\n    )\n    if use_cuda:\n        translator.cuda()\n    return translator\n\n\ndef _generate_score(models, args, task, dataset, modify_target_dict):\n    use_cuda = torch.cuda.is_available() and not args.cpu\n\n    # Load ensemble\n    if not args.quiet:\n        print(\n            ""| loading model(s) from {}"".format(\n                "", "".join(args.path.split(CHECKPOINT_PATHS_DELIMITER))\n            )\n        )\n\n    # Optimize ensemble for generation\n    for model in models:\n        model.make_generation_fast_(\n            beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,\n            need_attn=True,\n        )\n\n    translator = build_sequence_generator(args, task, models)\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(args.replace_unk)\n\n    print(""seed number is"" + str(args.max_examples_to_evaluate_seed))\n    if args.max_examples_to_evaluate > 0:\n        pytorch_translate_data.subsample_pair_dataset(\n            dataset, args.max_examples_to_evaluate, args.max_examples_to_evaluate_seed\n        )\n\n    # Keep track of translations\n    # Initialize with empty translations\n    # and zero probs scores\n    translated_sentences = [""""] * len(dataset)\n    translated_scores = [0.0] * len(dataset)\n    hypos_list = []\n\n    collect_output_hypos = getattr(args, ""output_hypos_binary_path"", False)\n    if collect_output_hypos:\n        output_hypos_token_arrays = [None] * len(dataset)\n\n    # Generate and compute BLEU score\n    dst_dict = task.target_dictionary\n    if args.sacrebleu:\n        scorer = bleu.SacrebleuScorer()\n    else:\n        scorer = bleu.Scorer(dst_dict.pad(), dst_dict.eos(), dst_dict.unk())\n\n    itr = task.get_batch_iterator(\n        dataset=dataset,\n        max_tokens=args.max_tokens,\n        max_sentences=args.max_sentences,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), *[model.max_positions() for model in models]\n        ),\n        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=8,\n        num_shards=args.num_shards,\n        shard_id=args.shard_id,\n        num_workers=args.num_workers,\n    ).next_epoch_itr(shuffle=False)\n\n    oracle_scorer = None\n    if args.report_oracle_bleu:\n        oracle_scorer = bleu.Scorer(dst_dict.pad(), dst_dict.eos(), dst_dict.unk())\n\n    rescorer = None\n    num_sentences = 0\n    translation_samples = []\n    translation_info_list = []\n    with progress_bar.build_progress_bar(args, itr) as t:\n        wps_meter = TimeMeter()\n        gen_timer = StopwatchMeter()\n        translations = translator.generate_batched_itr(\n            t,\n            maxlen_a=args.max_len_a,\n            maxlen_b=args.max_len_b,\n            cuda=use_cuda,\n            timer=gen_timer,\n            prefix_size=1\n            if pytorch_translate_data.is_multilingual_many_to_one(args)\n            else 0,\n        )\n\n        for trans_info in _iter_translations(\n            args, task, dataset, translations, align_dict, rescorer, modify_target_dict\n        ):\n            if hasattr(scorer, ""add_string""):\n                scorer.add_string(trans_info.target_str, trans_info.hypo_str)\n            else:\n                scorer.add(trans_info.target_tokens, trans_info.hypo_tokens)\n            if oracle_scorer is not None:\n                oracle_scorer.add(trans_info.target_tokens, trans_info.best_hypo_tokens)\n\n            if getattr(args, ""translation_output_file"", False):\n                translated_sentences[trans_info.sample_id] = trans_info.hypo_str\n            if getattr(args, ""translation_probs_file"", False):\n                translated_scores[trans_info.sample_id] = trans_info.hypo_score\n            if getattr(args, ""hypotheses_export_path"", False):\n                hypos_list.append(trans_info.hypos)\n            if collect_output_hypos:\n                output_hypos_token_arrays[\n                    trans_info.sample_id\n                ] = trans_info.best_hypo_tokens\n            if args.translation_info_export_path is not None:\n                # Strip expensive data from hypotheses before saving\n                hypos = [\n                    {k: v for k, v in hypo.items() if k in [""tokens"", ""score""]}\n                    for hypo in trans_info.hypos\n                ]\n                # Make sure everything is on cpu before exporting\n                hypos = [\n                    {""score"": hypo[""score""], ""tokens"": hypo[""tokens""].cpu()}\n                    for hypo in hypos\n                ]\n                translation_info_list.append(\n                    {\n                        ""src_tokens"": trans_info.src_tokens.cpu(),\n                        ""target_tokens"": trans_info.target_tokens,\n                        ""hypos"": hypos,\n                    }\n                )\n            translation_samples.append(\n                collections.OrderedDict(\n                    {\n                        ""sample_id"": trans_info.sample_id.item(),\n                        ""src_str"": trans_info.src_str,\n                        ""target_str"": trans_info.target_str,\n                        ""hypo_str"": trans_info.hypo_str,\n                    }\n                )\n            )\n            wps_meter.update(trans_info.src_tokens.size(0))\n            t.log({""wps"": round(wps_meter.avg)})\n            num_sentences += 1\n\n    # If applicable, save collected hypothesis tokens to binary output file\n    if collect_output_hypos:\n        output_dataset = pytorch_translate_data.InMemoryIndexedDataset()\n        output_dataset.load_from_sequences(output_hypos_token_arrays)\n        output_dataset.save(args.output_hypos_binary_path)\n    if args.output_source_binary_path:\n        dataset.src.save(args.output_source_binary_path)\n    if args.translation_info_export_path is not None:\n        f = open(args.translation_info_export_path, ""wb"")\n        pickle.dump(translation_info_list, f)\n        f.close()\n\n    # If applicable, save the translations and scores to the output files\n    # These two ouputs are used in dual learning for weighted backtranslation\n    if getattr(args, ""translation_output_file"", False) and getattr(\n        args, ""translation_probs_file"", False\n    ):\n        with open(args.translation_output_file, ""w"") as translation_file, open(\n            args.translation_probs_file, ""w""\n        ) as score_file:\n            for hypo_str, hypo_score in zip(translated_sentences, translated_scores):\n                if len(hypo_str.strip()) > 0:\n                    print(hypo_str, file=translation_file)\n                    print(np.exp(hypo_score), file=score_file)\n\n    # For eg. external evaluation\n    if getattr(args, ""hypotheses_export_path"", False):\n        with open(args.hypotheses_export_path, ""w"") as out_file:\n            for hypos in hypos_list:\n                for hypo in hypos:\n                    print(\n                        task.tgt_dict.string(\n                            hypo[""tokens""], bpe_symbol=args.remove_bpe\n                        ),\n                        file=out_file,\n                    )\n\n    if oracle_scorer is not None:\n        print(f""| Oracle BLEU (best hypo in beam): {oracle_scorer.result_string()}"")\n\n    return scorer, num_sentences, gen_timer, translation_samples\n\n\ndef smoothed_sentence_bleu(task, target_tokens, hypo_tokens):\n    """"""\n    Implements ""Smoothing 3"" method from Chen and Cherry. ""A Systematic\n    Comparison of Smoothing Techniques for Sentence-Level BLEU"".\n    http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n    """"""\n    dst_dict = task.target_dictionary\n    scorer = bleu.Scorer(dst_dict.pad(), dst_dict.eos(), dst_dict.unk())\n    scorer.add(target_tokens, hypo_tokens)\n\n    invcnt = 1\n    ratios = []\n    for (match, count) in [\n        (scorer.stat.match1, scorer.stat.count1),\n        (scorer.stat.match2, scorer.stat.count2),\n        (scorer.stat.match3, scorer.stat.count3),\n        (scorer.stat.match4, scorer.stat.count4),\n    ]:\n        if count == 0:\n            # disregard n-grams for values of n larger than hypothesis length\n            continue\n        if match == 0:\n            invcnt *= 2\n            match = 1.0 / invcnt\n        ratios.append(match / count)\n\n    brevity_penalty = np.min(\n        [1, np.exp(1 - (scorer.stat.reflen / scorer.stat.predlen))]\n    )\n    geometric_mean = np.exp(np.log(ratios).mean())\n    smoothed_bleu = brevity_penalty * geometric_mean\n    return smoothed_bleu\n\n\ndef _iter_translations(\n    args, task, dataset, translations, align_dict, rescorer, modify_target_dict\n):\n    """"""Iterate over translations.\n\n    This is a generator function which wraps the beam-search sequence generator,\n    performing such work on the output as converting token indices to\n    strings, printing output where applicable (not args.quiet), collecting\n    oracle translations where applicable, and removing language-ID tokens\n    for multilingual translation.\n\n    Args:\n        args: Command-line arguments.\n        task: FairseqTask object.\n        dataset: Dataset set object for a specific split.\n        translations: Batched translation iterator, as returned by\n            SequenceGenerator.generate_batched_itr().\n        align_dict: Dictionary for UNK replacement.\n\n    Yields:\n        For each sentence in `translations`, yields a TranslationInfo.\n    """"""\n    is_multilingual = pytorch_translate_data.is_multilingual_many_to_one(args)\n\n    for sample_id, src_tokens, target_tokens, hypos in translations:\n        # Process input and ground truth\n        target_tokens = target_tokens.int().cpu()\n\n        if is_multilingual:\n            src_lang_id = (\n                src_tokens[-1] - pytorch_translate_data.MULTILING_DIALECT_ID_OFFSET\n            )\n            target_lang_id = (\n                target_tokens[0] - pytorch_translate_data.MULTILING_DIALECT_ID_OFFSET\n            )\n\n            # remove language ID tokens\n            src_tokens = src_tokens[:-1]\n            target_tokens = target_tokens[1:]\n\n            # Select dictionaries\n            src_dict = task.source_dictionaries[task.get_encoder_lang_code(src_lang_id)]\n            target_dict = task.target_dictionaries[\n                task.get_decoder_lang_code(target_lang_id)\n            ]\n        else:\n            src_dict = task.source_dictionary\n            target_dict = task.target_dictionary\n\n        # Either retrieve the original sentences or regenerate them from tokens.\n        if align_dict is not None:\n            src_str = dataset.src.get_original_text(sample_id)\n            target_str = dataset.tgt.get_original_text(sample_id)\n        else:\n            src_str = src_dict.string(src_tokens, args.remove_bpe)\n            target_str = target_dict.string(\n                target_tokens, args.remove_bpe, escape_unk=True\n            )\n\n        if not args.quiet:\n            print(f""S-{sample_id}\\t{src_str}"")\n            print(f""T-{sample_id}\\t{target_str}"")\n\n        # used for oracle evaluation (args.report_oracle_bleu)\n        best_hypo_tokens = None\n        best_hypo_score = 0\n        collect_oracle_hypos = args.report_oracle_bleu or (\n            args.output_hypos_binary_path and args.nbest > 0\n        )\n\n        # Process top predictions\n        for i, hypo in enumerate(hypos[: min(len(hypos), args.nbest)]):\n            hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                hypo_tokens=hypo[""tokens""].int().cpu(),\n                src_str=src_str,\n                alignment=hypo[""alignment""].int().cpu()\n                if align_dict is not None\n                else None,\n                align_dict=align_dict,\n                tgt_dict=task.target_dictionary,\n                remove_bpe=args.remove_bpe,\n            )\n\n            if not args.quiet:\n                print(f""H-{sample_id}\\t{hypo[\'score\']}\\t{hypo_str}"")\n                if alignment is not None:\n                    print(\n                        ""A-{}\\t{}"".format(\n                            sample_id,\n                            "" "".join(map(lambda x: str(utils.item(x)), alignment)),\n                        )\n                    )\n\n            if collect_oracle_hypos:\n                score = smoothed_sentence_bleu(task, target_tokens, hypo_tokens)\n                if score > best_hypo_score:\n                    best_hypo_tokens = hypo_tokens\n                    best_hypo_score = score\n\n            if i == 0:\n                if align_dict is not None or args.remove_bpe is not None:\n                    # Convert back to tokens for evaluation with unk replacement\n                    # and/or without BPE\n                    target_tokens = task.target_dictionary.encode_line(\n                        target_str, add_if_not_exist=modify_target_dict\n                    )\n                # The probs score for the hypo_str; whether it\'s normalized by\n                # sequence length or not depends on normalize_scores, which is\n                # set by arg.nonormalize.\n                # However, as I tried, whether normalize_scores is set or not,\n                # the returned scores are the same (to be investigated).\n                # Here, the probs are normalized by hypo length so the value\n                # is big enough to be used as weights for backtranslations in\n                # dual learning.\n                hypo_score = (\n                    hypo[""score""] / len(hypo_tokens) if len(hypo_tokens) > 0 else 0.0\n                )\n                top_hypo_tokens = hypo_tokens\n                top_hypo_str = hypo_str\n\n        if not collect_oracle_hypos:\n            best_hypo_tokens = top_hypo_tokens\n\n        yield TranslationInfo(\n            sample_id=sample_id,\n            src_tokens=src_tokens,\n            target_tokens=target_tokens,\n            hypo_tokens=top_hypo_tokens,\n            src_str=src_str,\n            target_str=target_str,\n            hypo_str=top_hypo_str,\n            hypo_score=hypo_score,\n            best_hypo_tokens=best_hypo_tokens,\n            hypos=hypos,\n        )\n\n\ndef add_args(parser):\n    group = parser.add_argument_group(""Generation"")\n    group.add_argument(\n        ""--word-reward"",\n        type=float,\n        default=0.0,\n        help=(\n            ""Value to add to (log-prob) score for each token except EOS. ""\n            ""IMPORTANT NOTE: higher values of --length-penalty and --word-reward ""\n            ""both encourage longer translations, while higher values of ""\n            ""--unk-reward penalize UNKs more.""\n        ),\n    )\n    group.add_argument(\n        ""--model-weights"",\n        default="""",\n        help=(\n            ""Interpolation weights for ensembles. Comma-separated list of ""\n            ""floats with length equal to the number of models in the ensemble.""\n        ),\n    )\n\n\ndef get_parser_with_args():\n    parser = options.get_parser(""Generation"", default_task=""pytorch_translate"")\n    pytorch_translate_options.add_verbosity_args(parser)\n    pytorch_translate_options.add_dataset_args(parser, gen=True)\n    generation_group = options.add_generation_args(parser)\n    pytorch_translate_options.expand_generation_args(generation_group)\n\n    # Adds args used by the standalone generate binary.\n    generation_group.add_argument(\n        ""--source-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file representing the Dictionary to use."",\n    )\n    generation_group.add_argument(\n        ""--char-source-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=(\n            ""Same as --source-vocab-file except using characters. ""\n            ""(For use with char_source and char_aware models only.)""\n        ),\n    )\n    generation_group.add_argument(\n        ""--target-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file representing the Dictionary to use."",\n    )\n    generation_group.add_argument(\n        ""--char-target-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=(\n            ""Same as --target-vocab-file except using characters. ""\n            ""(For use with char_aware models only.)""\n        ),\n    )\n    generation_group.add_argument(\n        ""--source-text-file"",\n        default="""",\n        nargs=""+"",\n        metavar=""FILE"",\n        help=""Path to raw text file containing examples in source dialect. ""\n        ""This overrides what would be loaded from the data dir. ""\n        ""You can specify multiple source files (eg. for use in combination ""\n        ""with --source-ensembling). By default this will only translate the ""\n        ""first source file"",\n    )\n    generation_group.add_argument(\n        ""--target-text-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to raw text file containing examples in target dialect. ""\n        ""This overrides what would be loaded from the data dir."",\n    )\n    generation_group.add_argument(\n        ""--source-binary-file"",\n        default="""",\n        help=""Path for the binary file containing source eval examples. ""\n        ""(Overrides --source-text-file. Must be used in conjunction with ""\n        ""--target-binary-file)."",\n    )\n    generation_group.add_argument(\n        ""--target-binary-file"",\n        default="""",\n        help=""Path for the binary file containing target eval examples. ""\n        ""(Overrides --target-text-file. Must be used in conjunction with ""\n        ""--source-binary-file)."",\n    )\n    generation_group.add_argument(\n        ""--translation-output-file"",\n        default="""",\n        type=str,\n        metavar=""FILE"",\n        help=""Path to text file to store the output of the model. "",\n    )\n    generation_group.add_argument(\n        ""--translation-probs-file"",\n        default="""",\n        type=str,\n        metavar=""FILE"",\n        help=""Path to text file to store the probs of translation output. "",\n    )\n    generation_group.add_argument(\n        ""--multiling-source-lang"",\n        action=""append"",\n        metavar=""SRC"",\n        help=(\n            ""Must be set for decoding with multilingual models. ""\n            ""Must match an entry from --multiling-encoder-lang from training.""\n        ),\n    )\n    generation_group.add_argument(\n        ""--multiling-target-lang"",\n        action=""append"",\n        metavar=""TARGET"",\n        help=(\n            ""Must be set for decoding with multilingual models. ""\n            ""Must match an entry from --multiling-decoder-lang from training.""\n        ),\n    )\n    generation_group.add_argument(\n        ""--source-ensembling"",\n        action=""store_true"",\n        help=""If this flag is present, the model will ensemble the predictions ""\n        ""conditioned on multiple source sentences (one per source-text-file)"",\n    )\n    generation_group.add_argument(\n        ""--competing-completed-beam-search"",\n        action=""store_true"",\n        help=""If this flag is present, use the alternative beam search ""\n        ""implementation in research/beam_search. This beam search keeps completed ""\n        ""hypos in the beam and let them compete against hypo expansions in the ""\n        ""next time step."",\n    )\n    generation_group.add_argument(\n        ""--iterative-refinement"",\n        action=""store_true"",\n        help=""If this flag is present, use iterative refinement."",\n    )\n\n    return parser\n\n\ndef main():\n    parser = get_parser_with_args()\n    args = options.parse_args_and_arch(parser)\n    validate_args(args)\n    generate(args)\n\n\ndef validate_args(args):\n    pytorch_translate_options.validate_generation_args(args)\n\n    assert args.path is not None, ""--path required for generation!""\n    if args.source_binary_file != """":\n        assert args.target_binary_file != """"\n        assert os.path.isfile(args.source_binary_file)\n        assert os.path.isfile(args.target_binary_file)\n    else:\n        assert all(\n            (src_file and os.path.isfile(src_file))\n            for src_file in args.source_text_file\n        ), ""Please specify a valid file for --source-text-file""\n        assert args.target_text_file and os.path.isfile(\n            args.target_text_file\n        ), ""Please specify a valid file for --target-text-file""\n\n\ndef generate(args):\n    pytorch_translate_options.print_args(args)\n\n    models, model_args, task = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n        args.path.split(CHECKPOINT_PATHS_DELIMITER)\n    )\n    args.source_lang = model_args[0].source_lang\n    args.target_lang = model_args[0].target_lang\n\n    append_eos_to_source = model_args[0].append_eos_to_source\n    reverse_source = model_args[0].reverse_source\n    assert all(\n        a.append_eos_to_source == append_eos_to_source\n        and a.reverse_source == reverse_source\n        for a in model_args\n    )\n    if args.source_binary_file != """":\n        assert args.target_binary_file != """"\n        if isinstance(task, PytorchTranslateTask):\n            task.load_dataset(\n                args.gen_subset,\n                args.source_binary_file,\n                args.target_binary_file,\n                is_npz=args.is_npz,\n            )\n        else:\n            task.load_dataset(\n                args.gen_subset, args.source_binary_file, args.target_binary_file\n            )\n    elif pytorch_translate_data.is_multilingual_many_to_one(args):\n        task.set_encoder_langs(model_args[0].multiling_encoder_lang)\n        task.set_decoder_langs(model_args[0].multiling_decoder_lang)\n        task.load_dataset_from_text_multilingual(\n            args.gen_subset,\n            source_text_file=args.source_text_file[0],\n            target_text_file=args.target_text_file,\n            source_lang_id=task.get_encoder_lang_id(args.multiling_source_lang[0]),\n            target_lang_id=task.get_decoder_lang_id(args.multiling_target_lang[0]),\n            append_eos=append_eos_to_source,\n            reverse_source=reverse_source,\n        )\n    elif args.source_ensembling:\n        task.load_multisource_dataset_from_text(\n            args.gen_subset,\n            source_text_files=args.source_text_file,\n            target_text_file=args.target_text_file,\n            append_eos=append_eos_to_source,\n            reverse_source=reverse_source,\n        )\n    else:\n        task.load_dataset_from_text(\n            args.gen_subset,\n            source_text_file=args.source_text_file[0],\n            target_text_file=args.target_text_file,\n            append_eos=append_eos_to_source,\n            reverse_source=reverse_source,\n        )\n\n    lang_pair = None\n    if isinstance(task, PyTorchTranslateMultiTask):\n        if args.source_lang and args.target_lang:\n            lang_pair = args.source_lang + ""-"" + args.target_lang\n        else:\n            lang_pair = ""src-tgt""\n    scorer, num_sentences, gen_timer, _ = generate_score(\n        args=args,\n        task=task,\n        dataset=task.dataset(args.gen_subset),\n        lang_pair=lang_pair,\n        models=models,\n    )\n    print(\n        f""| Translated {num_sentences} sentences ({gen_timer.n} tokens) ""\n        f""in {gen_timer.sum:.1f}s ({1. / gen_timer.avg:.2f} tokens/s)""\n    )\n    print(\n        f""| Generate {args.gen_subset} with beam={args.beam}: ""\n        f""{scorer.result_string()}""\n    )\n    return scorer.score()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_translate/hybrid_transformer_rnn.py,10,"b'#!/usr/bin/env python3\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.models import (\n    FairseqEncoderDecoderModel,\n    FairseqIncrementalDecoder,\n    register_model,\n    register_model_architecture,\n    transformer as fairseq_transformer,\n)\nfrom fairseq.modules import MultiheadAttention\nfrom pytorch_translate import (\n    multilingual_model,\n    transformer as pytorch_translate_transformer,\n    vocab_reduction,\n)\nfrom pytorch_translate.utils import torch_find\n\n\n@register_model(""hybrid_transformer_rnn"")\nclass HybridTransformerRNNModel(FairseqEncoderDecoderModel):\n    def __init__(self, task, encoder, decoder):\n        super().__init__(encoder, decoder)\n        self.task = task\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--dropout"", type=float, metavar=""D"", help=""dropout probability""\n        )\n        parser.add_argument(\n            ""--attention-dropout"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability for attention weights"",\n        )\n        parser.add_argument(\n            ""--relu-dropout"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability after ReLU in FFN"",\n        )\n        parser.add_argument(\n            ""--encoder-pretrained-embed"",\n            type=str,\n            metavar=""STR"",\n            help=""path to pre-trained encoder embedding"",\n        )\n        parser.add_argument(\n            ""--encoder-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""encoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--encoder-ffn-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""encoder embedding dimension for FFN"",\n        )\n        parser.add_argument(\n            ""--encoder-freeze-embed"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether to freeze the encoder embedding or allow it to be ""\n                ""updated during training""\n            ),\n        )\n        parser.add_argument(\n            ""--encoder-layers"", type=int, metavar=""N"", help=""num encoder layers""\n        )\n        parser.add_argument(\n            ""--encoder-attention-heads"",\n            type=int,\n            metavar=""N"",\n            help=""num encoder attention heads"",\n        )\n        parser.add_argument(\n            ""--encoder-normalize-before"",\n            default=False,\n            action=""store_true"",\n            help=""apply layernorm before each encoder block"",\n        )\n        parser.add_argument(\n            ""--encoder-learned-pos"",\n            default=False,\n            action=""store_true"",\n            help=""use learned positional embeddings in the encoder"",\n        )\n        parser.add_argument(\n            ""--decoder-pretrained-embed"",\n            type=str,\n            metavar=""STR"",\n            help=""path to pre-trained decoder embedding"",\n        )\n        parser.add_argument(\n            ""--decoder-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""decoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--decoder-freeze-embed"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether to freeze the encoder embedding or allow it to be ""\n                ""updated during training""\n            ),\n        )\n        parser.add_argument(\n            ""--decoder-layers"", type=int, metavar=""N"", help=""num decoder layers""\n        )\n        parser.add_argument(\n            ""--decoder-attention-heads"",\n            type=int,\n            metavar=""N"",\n            help=""num decoder attention heads"",\n        )\n        parser.add_argument(\n            ""--decoder-reduced-attention-dim"",\n            type=int,\n            default=None,\n            metavar=""N"",\n            help=""if specified, computes attention with this dimensionality ""\n            ""(instead of using encoder output dims)"",\n        )\n        parser.add_argument(\n            ""--decoder-lstm-units"",\n            type=int,\n            metavar=""N"",\n            help=""num LSTM units for each decoder layer"",\n        )\n        parser.add_argument(\n            ""--decoder-out-embed-dim"",\n            default=None,\n            type=int,\n            metavar=""N"",\n            help=""decoder output embedding dimension"",\n        )\n\n        # Args for vocab reduction\n        vocab_reduction.add_args(parser)\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure that all args are properly defaulted\n        # (in case there are any new ones)\n        base_architecture(args)\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        encoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=src_dict,\n            embed_dim=args.encoder_embed_dim,\n            path=args.encoder_pretrained_embed,\n            freeze=args.encoder_freeze_embed,\n        )\n        decoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=tgt_dict,\n            embed_dim=args.decoder_embed_dim,\n            path=args.decoder_pretrained_embed,\n            freeze=args.decoder_freeze_embed,\n        )\n\n        encoder = HybridTransformerRNNModel.build_encoder(\n            args, src_dict, encoder_embed_tokens, proj_to_decoder=False\n        )\n        decoder = HybridTransformerRNNModel.build_decoder(\n            args, src_dict, tgt_dict, decoder_embed_tokens\n        )\n        return HybridTransformerRNNModel(task, encoder, decoder)\n\n    def get_targets(self, sample, net_output):\n        targets = sample[""target""].view(-1)\n        possible_translation_tokens = net_output[-1]\n        if possible_translation_tokens is not None:\n            targets = torch_find(\n                possible_translation_tokens, targets, len(self.task.target_dictionary)\n            )\n        return targets\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens, proj_to_decoder=False):\n        return pytorch_translate_transformer.TransformerEncoder(\n            args, src_dict, embed_tokens, proj_to_decoder=False\n        )\n\n    @classmethod\n    def build_decoder(cls, args, src_dict, tgt_dict, embed_tokens):\n        return HybridRNNDecoder(args, src_dict, tgt_dict, embed_tokens)\n\n\nclass HybridRNNDecoder(FairseqIncrementalDecoder):\n    """"""\n    Decoder with general structure of Chen et al., The Best of Both Worlds:\n    Combining Recent Advances in Neural Machine Translation, 2018.\n    https://arxiv.org/abs/1804.09849\n    """"""\n\n    def _init_dims(self, args, src_dict, dst_dict, embed_tokens):\n        self.dropout = args.dropout\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_tokens = embed_tokens\n\n        self.lstm_units = args.decoder_lstm_units\n        self.num_layers = args.decoder_layers\n        self.initial_input_dim = embed_dim\n\n        self.encoder_output_dim = args.encoder_embed_dim\n        if args.decoder_reduced_attention_dim is None:\n            self.attention_dim = self.encoder_output_dim\n        else:\n            self.attention_dim = args.decoder_reduced_attention_dim\n        self.input_dim = self.lstm_units + self.attention_dim\n\n        self.num_attention_heads = args.decoder_attention_heads\n        self.bottleneck_dim = args.decoder_out_embed_dim\n\n    def _init_components(self, args, src_dict, dst_dict, embed_tokens):\n        self.initial_rnn_layer = nn.LSTM(\n            input_size=self.initial_input_dim, hidden_size=self.lstm_units\n        )\n\n        self.proj_encoder_layer = None\n        if self.attention_dim != self.encoder_output_dim:\n            self.proj_encoder_layer = fairseq_transformer.Linear(\n                self.encoder_output_dim, self.attention_dim\n            )\n\n        self.proj_layer = None\n        if self.lstm_units != self.attention_dim:\n            self.proj_layer = fairseq_transformer.Linear(\n                self.lstm_units, self.attention_dim\n            )\n\n        self.attention = MultiheadAttention(\n            self.attention_dim,\n            self.num_attention_heads,\n            dropout=args.attention_dropout,\n            encoder_decoder_attention=True,\n        )\n\n        self.extra_rnn_layers = nn.ModuleList([])\n        for _ in range(self.num_layers - 1):\n            self.extra_rnn_layers.append(\n                nn.LSTM(input_size=self.input_dim, hidden_size=self.lstm_units)\n            )\n\n        self.bottleneck_layer = None\n        if self.bottleneck_dim is not None:\n            self.out_embed_dim = self.bottleneck_dim\n            self.bottleneck_layer = fairseq_transformer.Linear(\n                self.input_dim, self.out_embed_dim\n            )\n        else:\n            self.out_embed_dim = self.input_dim\n\n        self.embed_out = nn.Parameter(torch.Tensor(len(dst_dict), self.out_embed_dim))\n        nn.init.normal_(self.embed_out, mean=0, std=self.out_embed_dim ** -0.5)\n\n        self.vocab_reduction_module = None\n        if args.vocab_reduction_params:\n            self.vocab_reduction_module = vocab_reduction.VocabReduction(\n                src_dict, dst_dict, args.vocab_reduction_params, fp16=args.fp16\n            )\n\n        self.onnx_trace = False\n\n    def __init__(self, args, src_dict, dst_dict, embed_tokens):\n        super().__init__(dst_dict)\n        self._init_dims(args, src_dict, dst_dict, embed_tokens)\n        self._init_components(args, src_dict, dst_dict, embed_tokens)\n\n    # Enable dependency injection by subclasses\n    def _unpack_encoder_out(self, encoder_out):\n        """""" Allow taking encoder_out from different architecture which\n        may have different formats.\n        """"""\n        return encoder_out\n\n    def _init_hidden(self, encoder_out, batch_size):\n        """""" Initialize with latent code if available otherwise zeros.""""""\n        return torch.zeros([1, batch_size, self.lstm_units])\n\n    def _concat_latent_code(self, x, encoder_out):\n        """""" Concat latent code, if available in encoder_out, which is the\n        case in subclass.\n        """"""\n        return x\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def _embed_prev_outputs(self, prev_output_tokens, incremental_state=None):\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n        x = self.embed_tokens(prev_output_tokens)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        return x, prev_output_tokens\n\n    def forward(\n        self,\n        prev_output_tokens,\n        encoder_out,\n        incremental_state=None,\n        possible_translation_tokens=None,\n        timestep=None,\n    ):\n        x, prev_output_tokens = self._embed_prev_outputs(\n            prev_output_tokens=prev_output_tokens, incremental_state=incremental_state\n        )\n        return self._forward_given_embeddings(\n            embed_out=x,\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out,\n            incremental_state=incremental_state,\n            possible_translation_tokens=possible_translation_tokens,\n            timestep=timestep,\n        )\n\n    def _forward_given_embeddings(\n        self,\n        embed_out,\n        prev_output_tokens,\n        encoder_out,\n        incremental_state=None,\n        possible_translation_tokens=None,\n        timestep=None,\n    ):\n        x = embed_out\n        (encoder_x, src_tokens, encoder_padding_mask) = self._unpack_encoder_out(\n            encoder_out\n        )\n        bsz, seqlen = prev_output_tokens.size()\n\n        state_outputs = []\n        if incremental_state is not None:\n            prev_states = utils.get_incremental_state(\n                self, incremental_state, ""cached_state""\n            )\n            if prev_states is None:\n                prev_states = self._init_prev_states(encoder_out)\n\n            # final 2 states of list are projected key and value\n            saved_state = {""prev_key"": prev_states[-2], ""prev_value"": prev_states[-1]}\n            self.attention._set_input_buffer(incremental_state, saved_state)\n\n        if incremental_state is not None:\n            # first num_layers pairs of states are (prev_hidden, prev_cell)\n            # for each layer\n            h_prev = prev_states[0]\n            c_prev = prev_states[1]\n        else:\n            h_prev = self._init_hidden(encoder_out, bsz).type_as(x)\n            c_prev = torch.zeros([1, bsz, self.lstm_units]).type_as(x)\n\n        x = self._concat_latent_code(x, encoder_out)\n        x, (h_next, c_next) = self.initial_rnn_layer(x, (h_prev, c_prev))\n        if incremental_state is not None:\n            state_outputs.extend([h_next, c_next])\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        if self.proj_encoder_layer is not None:\n            encoder_x = self.proj_encoder_layer(encoder_x)\n\n        attention_in = x\n        if self.proj_layer is not None:\n            attention_in = self.proj_layer(x)\n\n        attention_out, attention_weights = self.attention(\n            query=attention_in,\n            key=encoder_x,\n            value=encoder_x,\n            key_padding_mask=encoder_padding_mask,\n            incremental_state=incremental_state,\n            static_kv=True,\n            need_weights=(not self.training),\n        )\n\n        for i, layer in enumerate(self.extra_rnn_layers):\n            residual = x\n            rnn_input = torch.cat([x, attention_out], dim=2)\n            rnn_input = self._concat_latent_code(rnn_input, encoder_out)\n\n            if incremental_state is not None:\n                # first num_layers pairs of states are (prev_hidden, prev_cell)\n                # for each layer\n                h_prev = prev_states[2 * i + 2]\n                c_prev = prev_states[2 * i + 3]\n            else:\n                h_prev = self._init_hidden(encoder_out, bsz).type_as(x)\n                c_prev = torch.zeros([1, bsz, self.lstm_units]).type_as(x)\n\n            x, (h_next, c_next) = layer(rnn_input, (h_prev, c_prev))\n            if incremental_state is not None:\n                state_outputs.extend([h_next, c_next])\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = x + residual\n\n        x = torch.cat([x, attention_out], dim=2)\n        x = self._concat_latent_code(x, encoder_out)\n        if self.bottleneck_layer is not None:\n            x = self.bottleneck_layer(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if (\n            self.vocab_reduction_module is not None\n            and possible_translation_tokens is None\n        ):\n            decoder_input_tokens = prev_output_tokens.contiguous()\n            possible_translation_tokens = self.vocab_reduction_module(\n                src_tokens, decoder_input_tokens=decoder_input_tokens\n            )\n\n        output_weights = self.embed_out\n        if possible_translation_tokens is not None:\n            output_weights = output_weights.index_select(\n                dim=0, index=possible_translation_tokens\n            )\n\n        logits = F.linear(x, output_weights)\n\n        if incremental_state is not None:\n            # encoder projections can be reused at each incremental step\n            state_outputs.extend([prev_states[-2], prev_states[-1]])\n            utils.set_incremental_state(\n                self, incremental_state, ""cached_state"", state_outputs\n            )\n\n        return logits, attention_weights, possible_translation_tokens\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n    def _init_prev_states(self, encoder_out):\n        """"""\n        Initial (hidden, cell) values for LSTM layers are zero.\n\n        For encoder-decoder attention, key and value are computed once from\n        the encoder outputs and stay the same throughout decoding.\n        """"""\n        (encoder_x, src_tokens, encoder_padding_mask) = self._unpack_encoder_out(\n            encoder_out\n        )\n        batch_size = torch.onnx.operators.shape_as_tensor(encoder_x)[1]\n\n        if self.proj_encoder_layer is not None:\n            encoder_x = self.proj_encoder_layer(encoder_x)\n\n        states = []\n        for _ in range(self.num_layers):\n            hidden = self._init_hidden(encoder_out, batch_size).type_as(encoder_x)\n            cell = torch.zeros([1, batch_size, self.lstm_units]).type_as(encoder_x)\n            states.extend([hidden, cell])\n\n        # (key, value) for encoder-decoder attention computed from encoder\n        # output and remain the same throughout decoding\n        key = self.attention.k_proj(encoder_x)\n        value = self.attention.v_proj(encoder_x)\n\n        # (key, value) kept in shape (bsz, num_heads, seq_len, head_dim)\n        # to avoid repeated transpose operations\n        seq_len, batch_size_int, _ = encoder_x.shape\n        num_heads = self.attention.num_heads\n        head_dim = self.attention.head_dim\n        key = (\n            key.view(seq_len, batch_size_int * num_heads, head_dim)\n            .transpose(0, 1)\n            .view(batch_size_int, num_heads, seq_len, head_dim)\n        )\n        value = (\n            value.view(seq_len, batch_size_int * num_heads, head_dim)\n            .transpose(0, 1)\n            .view(batch_size_int, num_heads, seq_len, head_dim)\n        )\n        states.extend([key, value])\n\n        return states\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        # parent reorders attention model\n        super().reorder_incremental_state(incremental_state, new_order)\n\n        cached_state = utils.get_incremental_state(\n            self, incremental_state, ""cached_state""\n        )\n        if cached_state is None:\n            return\n\n        # Last 2 elements of prev_states are encoder projections\n        # used for ONNX export\n        for i, state in enumerate(cached_state[:-2]):\n            cached_state[i] = state.index_select(1, new_order)\n\n        utils.set_incremental_state(\n            self, incremental_state, ""cached_state"", cached_state\n        )\n\n\n@register_model_architecture(""hybrid_transformer_rnn"", ""hybrid_transformer_rnn"")\ndef base_architecture(args):\n    args.encoder_pretrained_embed = getattr(args, ""encoder_pretrained_embed"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_freeze_embed = getattr(args, ""encoder_freeze_embed"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.decoder_pretrained_embed = getattr(args, ""decoder_pretrained_embed"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_layers = getattr(args, ""decoder_layers"", 2)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_reduced_attention_dim = getattr(\n        args, ""decoder_reduced_attention_dim"", None\n    )\n    args.decoder_lstm_units = getattr(args, ""decoder_lstm_units"", 512)\n    args.decoder_out_embed_dim = getattr(args, ""decoder_out_embed_dim"", 256)\n    args.decoder_freeze_embed = getattr(args, ""decoder_freeze_embed"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.relu_dropout = getattr(args, ""relu_dropout"", 0.0)\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    vocab_reduction.set_arg_defaults(args)\n\n\n@register_model(""multilingual_hybrid_transformer_rnn"")\nclass MultilingualHybridTransformerModel(multilingual_model.MultilingualModel):\n    single_model_cls = HybridTransformerRNNModel\n\n    @staticmethod\n    def add_args(parser):\n        HybridTransformerRNNModel.add_args(parser)\n        multilingual_model.MultilingualModel.add_args(parser)\n\n\n@register_model_architecture(\n    ""multilingual_hybrid_transformer_rnn"", ""multilingual_hybrid_transformer_rnn""\n)\ndef semi_supervised_transformer(args):\n    base_architecture(args)\n    multilingual_model.MultilingualModel.set_multilingual_arch_args(args)\n'"
pytorch_translate/model_constants.py,0,"b'#!/usr/bin/env python3\n\n# Pretrained model params\nPRETRAINED_CHAR_EMBED_DIM = 16\nPRETRAINED_CHAR_CNN_PARAMS = [\n    (32, 1),\n    (32, 2),\n    (64, 3),\n    (128, 4),\n    (256, 5),\n    (512, 6),\n    (1024, 7),\n]\nPRETRAINED_NUM_HIGHWAY_LAYERS = 2\nPRETRAINED_CHAR_CNN_NONLINEAR_FN = ""relu""\nPRETRAINED_CHAR_CNN_OUTPUT_DIM = 512\n'"
pytorch_translate/multi_model.py,27,"b'#!/usr/bin/env python3\n\nimport abc\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq.models import FairseqEncoder, FairseqIncrementalDecoder\nfrom pytorch_translate import rnn, vocab_reduction\nfrom pytorch_translate.common_layers import Linear, NonlinearLayer, OutputProjection\nfrom pytorch_translate.utils import average_tensors, maybe_cuda\nfrom torch.serialization import default_restore_location\n\n\ndef unfreeze_nth_component(components, unfreeze_idx=-1):\n    """"""Freeze weights in all components except the `unfreeze_idx`-th one.""""""\n    for idx, component in enumerate(components):\n        for p in component.parameters():\n            p.requires_grad = idx == unfreeze_idx\n\n\nclass MultiEncoder(FairseqEncoder):\n    """"""Concatenates the outputs of multiple encoders.""""""\n\n    def __init__(self, dictionary, encoders, training_schedule=""complete""):\n        super().__init__(dictionary)\n        self.encoders = nn.ModuleList(encoders)\n        self.unfreeze_single = False\n        self.unfreeze_idx = -1\n        if self.training:\n            if training_schedule in [""freeze_all"", ""freeze_all_encoders""]:\n                unfreeze_nth_component(self.encoders)\n            elif training_schedule.startswith(\n                ""unfreeze_enc_""\n            ) or training_schedule.startswith(""unfreeze_encdec_""):\n                _, _, n = training_schedule.split(""_"")\n                unfreeze_nth_component(self.encoders, int(n))\n            elif training_schedule in [""unfreeze_single"", ""unfreeze_single_encoder""]:\n                self.unfreeze_single = True\n                self.unfreeze_mod = len(encoders)\n            elif training_schedule == ""separate"":\n                self.unfreeze_single = True\n                self.unfreeze_mod = len(encoders) + 1\n            elif training_schedule != ""complete"":\n                raise RuntimeError(f""Unknown training schedule \'{training_schedule}\'"")\n\n    def forward(self, src_tokens, src_lengths):\n        if self.unfreeze_single:\n            self.unfreeze_idx = (self.unfreeze_idx + 1) % self.unfreeze_mod\n            unfreeze_nth_component(self.encoders, self.unfreeze_idx)\n        all_encoder_outs = [\n            encoder(src_tokens, src_lengths) for encoder in self.encoders\n        ]\n        combined_encoder_outs = []\n        for i in range(3):\n            combined_encoder_outs.append(\n                torch.cat([e[i] for e in all_encoder_outs], dim=2)\n            )\n        # src_tokens and src_lengths are taken from the first encoder.\n        combined_encoder_outs.extend(all_encoder_outs[0][3:])\n        return tuple(combined_encoder_outs)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""Reorder all outputs according to new_order.""""""\n        return rnn.reorder_encoder_output(encoder_out, new_order)\n\n\nclass MultiDecoderCombinationStrategy(nn.Module):\n    """"""Strategy for combining decoder networks.\n\n    This is an abstract strategy (GoF) which defines the mapping from multiple\n    (unprojected) decoder outputs to the fully expanded logits.\n    """"""\n\n    def __init__(self, out_embed_dims, vocab_size, vocab_reduction_module=None):\n        super().__init__()\n        self.out_embed_dims = out_embed_dims\n        self.vocab_size = vocab_size\n        self.vocab_reduction_module = vocab_reduction_module\n\n    @abc.abstractmethod\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        """"""Combine decoder outputs and project.\n\n        Args:\n            unprojected_outs (list): List of tensors with the same length as\n                self.out_embed_dims containing the unprojected decoder outputs\n                from each decoder network.\n            src_tokens (Tensor): Tensor with source sentence tokens for vocab\n                reduction.\n            input_tokens (Tensor): Tensor with target-side decoder input tokens\n                for vocab reduction.\n            possible_translation_tokens: For vocab reduction.\n            select_single (None or int): Only use the n-th decoder output.\n\n        Return:\n            A tuple (logits, possible_translation_tokens), where logits is a\n            [batch_size, seq_len, vocab_size] tensor with the final combined\n            output logits, and possible_translation_tokens the short list from\n            vocab reduction.\n        """"""\n        raise NotImplementedError()\n\n\nclass UniformStrategy(MultiDecoderCombinationStrategy):\n    """"""Uniform averaging of model predictions.""""""\n\n    def __init__(\n        self,\n        out_embed_dims,\n        vocab_size,\n        vocab_reduction_module=None,\n        norm_fn=None,\n        to_log=False,\n    ):\n        super().__init__(out_embed_dims, vocab_size)\n        assert vocab_reduction_module is None\n        self.output_projections = nn.ModuleList(\n            [OutputProjection(dim, vocab_size) for dim in out_embed_dims]\n        )\n        self.to_log = to_log\n        self.norm_fn = norm_fn\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        assert possible_translation_tokens is None\n        if select_single is not None:\n            return self.output_projections[select_single](\n                unprojected_outs[select_single]\n            )\n        logits = [p(o)[0] for p, o in zip(self.output_projections, unprojected_outs)]\n        avg = average_tensors(logits, norm_fn=self.norm_fn)\n        if self.to_log:\n            avg.log_()\n        return avg, None\n\n\nclass UnprojectedStrategy(MultiDecoderCombinationStrategy):\n    """"""Average decoder outputs, share output projection layer.""""""\n\n    def __init__(self, out_embed_dims, vocab_size, vocab_reduction_module=None):\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        out_embed_dim = out_embed_dims[0]\n        assert all(d == out_embed_dim for d in out_embed_dims)\n        self.output_projection = OutputProjection(\n            out_embed_dim, vocab_size, vocab_reduction_module\n        )\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        return self.output_projection(\n            average_tensors(unprojected_outs)\n            if select_single is None\n            else unprojected_outs[select_single],\n            src_tokens,\n            input_tokens,\n            possible_translation_tokens,\n        )\n\n\nclass MaxUnprojectedStrategy(MultiDecoderCombinationStrategy):\n    """"""Element-wise max of decoder outputs, share output projection layer.""""""\n\n    def __init__(self, out_embed_dims, vocab_size, vocab_reduction_module=None):\n        out_embed_dim = out_embed_dims[0]\n        assert all(d == out_embed_dim for d in out_embed_dims)\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        self.output_projection = OutputProjection(\n            out_embed_dim, vocab_size, vocab_reduction_module\n        )\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        if select_single is None:\n            proj_input, _ = torch.max(torch.stack(unprojected_outs), dim=0)\n        else:\n            proj_input = unprojected_outs[select_single]\n        return self.output_projection(\n            proj_input, src_tokens, input_tokens, possible_translation_tokens\n        )\n\n\nclass MultiplicativeUnprojectedStrategy(MultiDecoderCombinationStrategy):\n    """"""Element-wise product of decoder out, share output projection layer.""""""\n\n    def __init__(self, out_embed_dims, vocab_size, vocab_reduction_module=None):\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        out_embed_dim = out_embed_dims[0]\n        assert all(d == out_embed_dim for d in out_embed_dims)\n        self.output_projection = OutputProjection(\n            out_embed_dim, vocab_size, vocab_reduction_module\n        )\n        self.activation = nn.ReLU()\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        stacked = (\n            torch.stack(unprojected_outs)\n            if select_single is None\n            else torch.unsqueeze(unprojected_outs[select_single], 0)\n        )\n        return self.output_projection(\n            torch.prod(self.activation(stacked), dim=0),\n            src_tokens,\n            input_tokens,\n            possible_translation_tokens,\n        )\n\n\nclass DeepFusionStrategy(MultiDecoderCombinationStrategy):\n    """"""Deep fusion following https://arxiv.org/pdf/1503.03535.pdf.\n\n    The first decoder is assumed to be the language model.\n    """"""\n\n    def __init__(self, out_embed_dims, vocab_size, vocab_reduction_module=None):\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        self.gating_network = NonlinearLayer(\n            out_embed_dims[0], 1, bias=True, activation_fn=nn.Sigmoid\n        )\n        self.output_projection = OutputProjection(\n            sum(out_embed_dims), vocab_size, vocab_reduction_module\n        )\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        assert select_single is None\n        g = self.gating_network(unprojected_outs[0])\n        unprojected_outs[0] = g * unprojected_outs[0]\n        return self.output_projection(\n            torch.cat(unprojected_outs, 2),\n            src_tokens,\n            input_tokens,\n            possible_translation_tokens,\n        )\n\n\nclass ColdFusionStrategy(MultiDecoderCombinationStrategy):\n    """"""Cold fusion following https://arxiv.org/pdf/1708.06426.pdf.\n\n    The first decoder is assumed to be the language model.\n    """"""\n\n    def __init__(\n        self,\n        out_embed_dims,\n        vocab_size,\n        vocab_reduction_module=None,\n        hidden_layer_size=256,\n    ):\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        self.hidden_layer = NonlinearLayer(\n            vocab_size, hidden_layer_size, bias=False, activation_fn=nn.ReLU\n        )\n        trans_dim = sum(out_embed_dims[1:])\n        self.gating_network = NonlinearLayer(\n            hidden_layer_size + trans_dim,\n            hidden_layer_size,\n            bias=True,\n            activation_fn=nn.Sigmoid,\n        )\n\n        # output_projections is [LM projection, Joint projection]. This is a\n        # trick to load pretrained LM projection.\n        self.output_projections = nn.ModuleList(\n            [\n                OutputProjection(out_embed_dims[0], vocab_size),\n                OutputProjection(\n                    hidden_layer_size + trans_dim, vocab_size, vocab_reduction_module\n                ),\n            ]\n        )\n        self.pre_softmax_activation = nn.ReLU()\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        assert select_single is None\n        l_lm, _ = self.output_projections[0](\n            unprojected_outs[0], src_tokens, input_tokens, possible_translation_tokens\n        )\n        l_lm_max, _ = torch.max(l_lm, dim=2, keepdim=True)\n        l_lm = l_lm - l_lm_max\n        h_lm = self.hidden_layer(l_lm)\n        s = torch.cat(unprojected_outs[1:], 2)\n        g = self.gating_network(torch.cat([s, h_lm], 2))\n        s_cf = torch.cat([s, g * h_lm], 2)\n        logits, possible_translation_tokens = self.output_projections[1](s_cf)\n        logits = self.pre_softmax_activation(logits)\n        return logits, possible_translation_tokens\n\n\nclass ConcatStrategy(MultiDecoderCombinationStrategy):\n    """"""Concatenates decoder outputs.""""""\n\n    def __init__(self, out_embed_dims, vocab_size, vocab_reduction_module=None):\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        self.output_projection = OutputProjection(\n            sum(out_embed_dims), vocab_size, vocab_reduction_module\n        )\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        assert select_single is None\n        return self.output_projection(\n            torch.cat(unprojected_outs, 2),\n            src_tokens,\n            input_tokens,\n            possible_translation_tokens,\n        )\n\n\nclass BottleneckStrategy(MultiDecoderCombinationStrategy):\n    """"""Concatenation of decoder outputs followed by a bottleneck layer.""""""\n\n    def __init__(self, out_embed_dims, vocab_size, vocab_reduction_module=None):\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        dim = out_embed_dims[0]\n        self.bottleneck = Linear(sum(out_embed_dims), dim)\n        self.output_projection = OutputProjection(\n            dim, vocab_size, vocab_reduction_module\n        )\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        assert select_single is None\n        return self.output_projection(\n            self.bottleneck(torch.cat(unprojected_outs, 2)),\n            src_tokens,\n            input_tokens,\n            possible_translation_tokens,\n        )\n\n\nclass DeepBottleneckStrategy(MultiDecoderCombinationStrategy):\n    """"""Bottleneck strategy with an additional non-linear layer.""""""\n\n    def __init__(\n        self,\n        out_embed_dims,\n        vocab_size,\n        vocab_reduction_module=None,\n        activation_fn=torch.nn.ReLU,\n    ):\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        dim = out_embed_dims[0]\n        self.bottleneck = nn.Sequential(\n            Linear(sum(out_embed_dims), dim, bias=True),\n            activation_fn(),\n            Linear(dim, dim, bias=True),\n        )\n        self.output_projection = OutputProjection(\n            dim, vocab_size, vocab_reduction_module\n        )\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        assert select_single is None\n        return self.output_projection(\n            self.bottleneck(torch.cat(unprojected_outs, 2)),\n            src_tokens,\n            input_tokens,\n            possible_translation_tokens,\n        )\n\n\nclass BaseWeightedStrategy(MultiDecoderCombinationStrategy):\n    """"""Base class for strategies with explicitly learned weights.""""""\n\n    def __init__(\n        self,\n        out_embed_dims,\n        vocab_size,\n        vocab_reduction_module=None,\n        fixed_weights=None,\n        hidden_layer_size=32,\n        activation_fn=torch.nn.ReLU,\n        logit_fn=torch.exp,\n    ):\n        """"""Initializes a combination strategy with explicit weights.\n\n        Args:\n            out_embed_dims (list): List of output dimensionalities of the\n                decoders.\n            vocab_size (int): Size of the output projection.\n            vocab_reduction_module: For vocabulary reduction\n            fixed_weights (list): If not None, use these fixed weights rather\n                than a gating network.\n            hidden_layer_size (int): Size of the hidden layer of the gating\n                network.\n            activation_fn: Non-linearity at the hidden layer.\n            norm_fn: Function to use for normalization (exp or sigmoid).\n        """"""\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        if fixed_weights is None:\n            self.fixed_weights = None\n            self.gating_network = nn.Sequential(\n                Linear(sum(out_embed_dims), hidden_layer_size, bias=True),\n                activation_fn(),\n                Linear(hidden_layer_size, len(out_embed_dims), bias=True),\n            )\n            self.logit_fn = logit_fn\n        else:\n            assert len(fixed_weights) == len(out_embed_dims)\n            self.fixed_weights = maybe_cuda(torch.Tensor(fixed_weights).view(1, 1, -1))\n\n    def compute_weights(self, unprojected_outs, select_single=None):\n        """"""Derive interpolation weights from unprojected decoder outputs.\n\n        Args:\n            unprojected_outs: List of [batch_size, seq_len, out_embed_dim]\n                tensors with unprojected decoder outputs.\n            select_single: If not None, put all weighton n-th model.\n\n        Returns:\n            A [batch_size, seq_len, num_decoders] float32 tensor with\n            normalized decoder interpolation weights.\n        """"""\n        if select_single is not None:\n            sz = unprojected_outs[0].size()\n            ret = maybe_cuda(torch.zeros((sz[0], sz[1], len(unprojected_outs))))\n            ret[:, :, select_single] = 1.0\n            return ret\n        if self.fixed_weights is not None:\n            return self.fixed_weights\n        logits = self.logit_fn(self.gating_network(torch.cat(unprojected_outs, 2)))\n        return torch.clamp(logits / torch.sum(logits, dim=2, keepdim=True), 0.0, 1.0)\n\n\nclass WeightedStrategy(BaseWeightedStrategy):\n    """"""Weighted average of full logits.""""""\n\n    def __init__(\n        self,\n        out_embed_dims,\n        vocab_size,\n        vocab_reduction_module=None,\n        norm_fn=None,\n        to_log=False,\n        fixed_weights=None,\n    ):\n        super().__init__(out_embed_dims, vocab_size, fixed_weights=fixed_weights)\n        assert vocab_reduction_module is None\n        self.output_projections = nn.ModuleList(\n            [OutputProjection(dim, vocab_size) for dim in out_embed_dims]\n        )\n        self.norm_fn = norm_fn\n        self.n_systems = len(out_embed_dims)\n        self.to_log = to_log\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        assert possible_translation_tokens is None\n        weights = self.compute_weights(unprojected_outs, select_single)\n        weights = [weights[:, :, i : i + 1] for i in range(self.n_systems)]\n        logits = [p(o)[0] for p, o in zip(self.output_projections, unprojected_outs)]\n        avg = average_tensors(logits, weights=weights, norm_fn=self.norm_fn)\n        if self.to_log:\n            avg.log_()\n        return avg, None\n\n\nclass WeightedUnprojectedStrategy(BaseWeightedStrategy):\n    """"""Weighted average of decoder outputs, shared projection layer.""""""\n\n    def __init__(self, out_embed_dims, vocab_size, vocab_reduction_module=None):\n        super().__init__(out_embed_dims, vocab_size, vocab_reduction_module)\n        out_embed_dim = out_embed_dims[0]\n        assert all(d == out_embed_dim for d in out_embed_dims)\n        self.output_projection = OutputProjection(\n            out_embed_dim, vocab_size, vocab_reduction_module\n        )\n\n    def forward(\n        self,\n        unprojected_outs,\n        src_tokens=None,\n        input_tokens=None,\n        possible_translation_tokens=None,\n        select_single=None,\n    ):\n        weights = self.compute_weights(unprojected_outs, select_single)\n        weights = [weights[:, :, i : i + 1] for i in range(self.n_systems)]\n        averaged_unprojected = average_tensors(unprojected_outs, weights=weights)\n        return self.output_projections[0](\n            averaged_unprojected, src_tokens, input_tokens, possible_translation_tokens\n        )\n\n\ndef parse_strategy_name(strategy_name, n_models):\n    modifier_idx = None\n    if ""-"" in strategy_name:\n        strategy_name, strategy_modifier = strategy_name.split(""-"")\n        if ""_"" in strategy_modifier:\n            strategy_modifier, modifier_idx = strategy_modifier.split(""_"")\n            modifier_idx = int(modifier_idx)\n    else:\n        strategy_modifier = None\n    norm_fn = None\n    to_log = False\n    if strategy_modifier == ""probspace"":\n        norm_fn = F.softmax\n        to_log = True\n    elif strategy_modifier == ""logprobspace"":\n        norm_fn = F.log_softmax\n    if modifier_idx is not None:\n        norm_fn_list = [None] * n_models\n        norm_fn_list[modifier_idx] = norm_fn\n        norm_fn = norm_fn_list\n    return strategy_name, strategy_modifier, norm_fn, to_log\n\n\ndef create_strategy(\n    strategy_name, out_embed_dims, vocab_size, vocab_reduction_module, fixed_weights\n):\n    strategy_name, strategy_modifier, norm_fn, to_log = parse_strategy_name(\n        strategy_name, len(out_embed_dims)\n    )\n    if strategy_name == ""uniform"":\n        return UniformStrategy(\n            out_embed_dims,\n            vocab_size,\n            vocab_reduction_module,\n            norm_fn=norm_fn,\n            to_log=to_log,\n        )\n    elif strategy_name == ""weighted"":\n        return WeightedStrategy(\n            out_embed_dims,\n            vocab_size,\n            vocab_reduction_module,\n            norm_fn=norm_fn,\n            to_log=to_log,\n            fixed_weights=fixed_weights,\n        )\n    elif strategy_name == ""unprojected"":\n        return UnprojectedStrategy(out_embed_dims, vocab_size, vocab_reduction_module)\n    elif strategy_name == ""max"" and strategy_modifier == ""unprojected"":\n        return MaxUnprojectedStrategy(\n            out_embed_dims, vocab_size, vocab_reduction_module\n        )\n    elif strategy_name == ""concat"":\n        return ConcatStrategy(out_embed_dims, vocab_size, vocab_reduction_module)\n    elif strategy_name == ""deepfusion"":\n        return DeepFusionStrategy(out_embed_dims, vocab_size, vocab_reduction_module)\n    elif strategy_name == ""coldfusion"":\n        return ColdFusionStrategy(out_embed_dims, vocab_size, vocab_reduction_module)\n    elif strategy_name == ""bottleneck"":\n        return BottleneckStrategy(out_embed_dims, vocab_size, vocab_reduction_module)\n    elif strategy_name == ""deep_bottleneck"":\n        return DeepBottleneckStrategy(\n            out_embed_dims, vocab_size, vocab_reduction_module\n        )\n    elif strategy_name == ""multiplicative"" and strategy_modifier == ""unprojected"":\n        return MultiplicativeUnprojectedStrategy(\n            out_embed_dims, vocab_size, vocab_reduction_module\n        )\n    raise RuntimeError(f""Unknown combination strategy \'{strategy_name}\'"")\n\n\nclass MultiDecoder(FairseqIncrementalDecoder):\n    """"""Multi-decoder ensembles.\n\n    Combines multiple decoders. See the `MultiDecoderCombinationStrategy`\n    implementations on how the decoder outputs are combined.\n    """"""\n\n    def __init__(\n        self,\n        src_dict,\n        dst_dict,\n        decoders,\n        combination_strategy,\n        is_lm=None,\n        split_encoder=False,\n        vocab_reduction_params=None,\n        training_schedule=""complete"",\n        fixed_weights=None,\n    ):\n        """"""Create a new multi-decoder instance.\n\n        Args:\n            src_dict (Dictionary): Source language dictionary.\n            dst_dict (Dictionary): Target language dictionary.\n            decoders (list): List of DecoderWithOutputProjection.\n            combination_strategy (string): Name of the combination strategy.\n                Passed through to `create_strategy()`.\n            is_lm (list): List of booleans determining whether the n-th\n                decoder is a language model. If None, none of the decoders are\n                considered an LM.\n            split_encoder (bool): If true, split encoder output, each decoder\n                gets its own split.\n            vocab_reduction_params: For vocabular reduction.\n            training_schedule (str): Training strategy.\n            fixed_weights (list): None or list of floats. If specified, use\n                these fixed model weights in weighted* combination strategies.\n        """"""\n        super().__init__(dst_dict)\n        if is_lm is None:\n            is_lm = [False] * len(decoders)\n        assert not any(decoder.project_output for decoder in decoders)\n        assert len(is_lm) == len(decoders)\n        self.attentive_decoder_ids = [i for i, b in enumerate(is_lm) if not b]\n        self.decoders_is_lm = is_lm\n        self.decoders = nn.ModuleList(decoders)\n        vocab_reduction_module = None\n        if vocab_reduction_params:\n            vocab_reduction_module = vocab_reduction.VocabReduction(\n                src_dict, dst_dict, vocab_reduction_params\n            )\n        self.combi_strat = create_strategy(\n            combination_strategy,\n            [decoder.out_embed_dim for decoder in decoders],\n            len(dst_dict),\n            vocab_reduction_module,\n            fixed_weights,\n        )\n        self.split_encoder = split_encoder\n        self.unfreeze_single = False\n        self.separate_training = False\n        self.unfreeze_idx = -1\n        if self.training:\n            if training_schedule in [""freeze_all"", ""freeze_all_decoders""]:\n                self.freeze_decoders()\n            elif training_schedule.startswith(\n                ""unfreeze_dec_""\n            ) or training_schedule.startswith(""unfreeze_encdec_""):\n                _, _, n = training_schedule.split(""_"")\n                self.freeze_decoders(int(n))\n            elif training_schedule in [""unfreeze_single"", ""unfreeze_single_decoder""]:\n                self.unfreeze_single = True\n                self.unfreeze_mod = len(decoders)\n            elif training_schedule == ""separate"":\n                self.unfreeze_single = True\n                self.unfreeze_mod = len(decoders) + 1\n                self.separate_training = True\n            elif training_schedule != ""complete"":\n                raise RuntimeError(f""Unknown training schedule \'{training_schedule}\'"")\n\n    def freeze_decoders(self, except_idx=-1):\n        """"""Freezes weights in all decoders except `except_idx`.""""""\n        unfreeze_nth_component(self.decoders, except_idx)\n        try:\n            unfreeze_nth_component(self.combi_strat.output_projections, except_idx)\n        except AttributeError:\n            pass  # combi_strat does not have multiple output projections\n\n    def forward(\n        self,\n        input_tokens,\n        encoder_out,\n        incremental_state=None,\n        possible_translation_tokens=None,\n    ):\n        if self.unfreeze_single:\n            self.unfreeze_idx = (self.unfreeze_idx + 1) % self.unfreeze_mod\n            if self.separate_training:\n                unfreeze_combi_strat = len(self.decoders) == self.unfreeze_idx\n                for p in self.combi_strat.parameters():\n                    p.requires_grad = unfreeze_combi_strat\n            self.freeze_decoders(self.unfreeze_idx)\n        if incremental_state is None:\n            incremental_state = {\n                decoder_id: None for decoder_id in range(len(self.decoders))\n            }\n        decoder_outs = []\n        decoder_contexts = self._get_contexts(encoder_out)\n        for decoder_id, decoder in enumerate(self.decoders):\n            if decoder_id not in incremental_state:\n                incremental_state[decoder_id] = {}\n            decoder_outs.append(\n                decoder.forward_unprojected(\n                    input_tokens,\n                    decoder_contexts[decoder_id],\n                    incremental_state=incremental_state[decoder_id],\n                )\n            )\n        mean_attn_scores = average_tensors(\n            [decoder_outs[decoder_id][1] for decoder_id in self.attentive_decoder_ids]\n        )\n        select_single = None\n        if self.separate_training and not unfreeze_combi_strat:\n            select_single = self.unfreeze_idx\n        logits, possible_translation_tokens = self.combi_strat(\n            [x for x, _ in decoder_outs],\n            src_tokens=encoder_out[4],\n            input_tokens=input_tokens if self.training else None,\n            possible_translation_tokens=possible_translation_tokens,\n            select_single=select_single,\n        )\n        return logits, mean_attn_scores, possible_translation_tokens\n\n    def _get_contexts(self, encoder_out):\n        encoder_outs, final_hidden, final_cell, src_lengths, src_tokens = encoder_out\n        if self.split_encoder:\n            split_encoder_outs = []\n            offset = 0\n            for decoder in self.decoders:\n                next_offset = offset + decoder.encoder_hidden_dim\n                split_encoder_outs.append(\n                    (\n                        encoder_outs[:, :, offset:next_offset],\n                        final_hidden[:, :, offset:next_offset],\n                        final_cell[:, :, offset:next_offset],\n                        src_lengths,\n                        src_tokens,\n                    )\n                )\n                offset = next_offset\n            assert offset == encoder_outs.size(2)\n        else:\n            split_encoder_outs = [encoder_out] * len(self.decoders)\n        if any(self.decoders_is_lm):\n            num_layers, bsz, _ = final_cell.size()\n            ones = torch.ones((num_layers, bsz, 1)).type_as(final_cell)\n            dummy_out = torch.ones((1, bsz, 1)).type_as(final_cell)\n            lm_encoder_outs = dummy_out, ones, ones, src_lengths, src_tokens\n            for decoder_id, is_lm in enumerate(self.decoders_is_lm):\n                if is_lm:\n                    split_encoder_outs[decoder_id] = lm_encoder_outs\n        return split_encoder_outs\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        """"""Reorder buffered internal state (for incremental generation).""""""\n        if not incremental_state:\n            return\n        for decoder_id, decoder in enumerate(self.decoders):\n            decoder.reorder_incremental_state(incremental_state[decoder_id], new_order)\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n\ndef import_individual_models(restore_files, trainer):\n    param2size = {}\n    for name, param in trainer.model.named_parameters():\n        param2size[name] = param.size()\n    cuda_device = torch.cuda.current_device()\n    model_state = {}\n    for idx, filename in enumerate(restore_files):\n        sub_state = torch.load(\n            filename,\n            map_location=lambda s, l: default_restore_location(\n                s, ""cuda:{}"".format(cuda_device)\n            ),\n        )\n        for name, value in sub_state[""model""].items():\n            new_name = None\n            if name.startswith(""encoder.""):\n                subname = name[8:]\n                new_name = f""encoder.encoders.{idx}.{subname}""\n            elif name == ""decoder.output_projection_w"":\n                new_name = (\n                    f""decoder.combi_strat.output_projections.{idx}.""\n                    f""output_projection_w""\n                )\n            elif name == ""decoder.output_projection_b"":\n                new_name = (\n                    f""decoder.combi_strat.output_projections.{idx}.""\n                    f""output_projection_b""\n                )\n            elif name.startswith(""decoder.""):\n                subname = name[8:]\n                new_name = f""decoder.decoders.{idx}.{subname}""\n            if new_name is None:\n                print(f""WARN: Ignoring {name} in {filename} (no match)"")\n            elif new_name not in param2size:\n                print(f""WARN: Could not find {new_name}. Check architectures"")\n            elif value.size() != param2size[new_name]:\n                print(\n                    f""WARN: Tried to map {name} to {new_name}, but sizes do not match ""\n                    f""({value.size()} != {param2size[new_name]})""\n                )\n            else:\n                model_state[new_name] = value\n    trainer.model.load_state_dict(model_state, strict=False)\n    print(f""|  Imported {len(model_state)} parameters."")\n    trainer._optim_history = []\n'"
pytorch_translate/multilingual.py,18,"b'#!/usr/bin/env python3\n\nimport torch\nimport torch.nn as nn\nfrom fairseq.models import FairseqEncoder, FairseqIncrementalDecoder\nfrom pytorch_translate import utils\nfrom pytorch_translate.data import data as pytorch_translate_data\n\n\ndef rescale_grad_hook(module, idx, grad):\n    lang_bsz = module.last_lang_bszs[idx]\n    if lang_bsz > 0:\n        return grad * float(module.last_bsz) / float(lang_bsz)\n    return grad\n\n\ndef create_hook_fn(module, idx):\n    return lambda grad: rescale_grad_hook(module, idx, grad)\n\n\ndef register_hooks(module, submodules):\n    for idx, submodule in enumerate(submodules):\n        for p in submodule.parameters():\n            p.register_hook(create_hook_fn(module, idx))\n\n\nclass MultilingualEncoder(FairseqEncoder):\n    """"""Multilingual encoder.\n\n    This encoder consists of n separate encoders. A language token ID at the\n    begin of the source sentence selects the encoder to use.\n    """"""\n\n    def __init__(\n        self,\n        dictionary,\n        encoders,\n        hidden_dim,\n        num_layers,\n        embed_dim,\n        rescale_grads=False,\n    ):\n        super().__init__(dictionary)\n        self.dictionary = dictionary\n        self.encoders = nn.ModuleList(encoders)\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.word_dim = embed_dim\n        if rescale_grads and len(encoders) > 1:\n            register_hooks(self, encoders)\n\n    def forward(self, src_tokens, src_lengths):\n        # Fetch language IDs and remove them from src_tokens\n        # Language IDs are on the right\n        lang_ids = (\n            src_tokens[:, -1] - pytorch_translate_data.MULTILING_DIALECT_ID_OFFSET\n        )\n        src_tokens = src_tokens[:, :-1]\n        src_lengths -= 1\n        # Create tensors for collecting encoder outputs\n        bsz, seq_len = src_tokens.size()[:2]\n        all_encoder_outs = utils.maybe_cuda(torch.zeros(seq_len, bsz, self.hidden_dim))\n        all_final_hidden = utils.maybe_cuda(\n            torch.zeros(self.num_layers, bsz, self.hidden_dim)\n        )\n        all_final_cell = utils.maybe_cuda(\n            torch.zeros(self.num_layers, bsz, self.hidden_dim)\n        )\n        # We cannot use zeros_like() for src_lengths because dtype changes\n        # from LongInt to Int\n        all_src_lengths = utils.maybe_cuda(torch.zeros(bsz, dtype=torch.int))\n        all_src_tokens = torch.zeros_like(src_tokens)\n        all_embedded_words = utils.maybe_cuda(torch.zeros(seq_len, bsz, self.word_dim))\n        self.last_bsz = bsz\n        self.last_lang_bszs = []\n        for lang_id, encoder in enumerate(self.encoders):\n            if encoder is None:\n                continue\n            indices = torch.nonzero(lang_ids == lang_id)\n            lang_bsz = indices.size(0)\n            self.last_lang_bszs.append(lang_bsz)\n            if lang_bsz == 0:  # Language not in this batch\n                for p in encoder.parameters():\n                    p.grad = torch.zeros_like(p.data)\n                continue\n            indices = indices.squeeze(1)\n            (\n                lang_encoder_outs,\n                lang_final_hidden,\n                lang_final_cell,\n                lang_src_lengths,\n                lang_src_tokens,\n                lang_embedded_words,\n            ) = encoder(src_tokens[indices], src_lengths[indices])\n            lang_seq_len = lang_encoder_outs.size(0)\n            all_encoder_outs[:lang_seq_len, indices, :] = lang_encoder_outs\n            all_final_hidden[:, indices, :] = lang_final_hidden\n            all_final_cell[:, indices, :] = lang_final_cell\n            all_src_lengths[indices] = lang_src_lengths\n            all_src_tokens[indices] = lang_src_tokens\n            all_embedded_words[:, indices, :] = lang_embedded_words\n        return (\n            all_encoder_outs,\n            all_final_hidden,\n            all_final_cell,\n            all_src_lengths,\n            all_src_tokens,\n            all_embedded_words,\n        )\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""Reorder all outputs according to new_order.""""""\n        # assume we can use any of the encoders to do the reordering\n        populated_encoders = [\n            encoder for encoder in self.encoders if encoder is not None\n        ]\n        assert len(populated_encoders) > 0\n        return populated_encoders[0].reorder_encoder_out(encoder_out, new_order)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n\nclass MultilingualDecoder(FairseqIncrementalDecoder):\n    """"""Multilingual decoder.""""""\n\n    def __init__(self, dictionary, decoders, hidden_dim, rescale_grads=False):\n        super().__init__(dictionary)\n        self.decoders = nn.ModuleList(decoders)\n        self.hidden_dim = hidden_dim\n        self.max_vocab_size = len(dictionary)\n        if rescale_grads and len(decoders) > 1:\n            register_hooks(self, decoders)\n\n    def forward(\n        self,\n        input_tokens,\n        encoder_out,\n        incremental_state=None,\n        possible_translation_tokens=None,\n    ):\n        if input_tokens.size(1) <= 1:\n            # This happens in the first time step of beam search. We return\n            # flat scores, and wait for the real work in the next time step\n            bsz = input_tokens.size(0)\n            return (\n                utils.maybe_cuda(torch.zeros(bsz, 1, self.max_vocab_size)),\n                utils.maybe_cuda(torch.zeros(bsz, 1, encoder_out[0].size(0))),\n                None,\n            )\n        # Vocab reduction not implemented\n        assert possible_translation_tokens is None\n        # Fetch language IDs and remove them from input_tokens\n        # Token sequences start with <GO-token> <lang-id> ...\n        lang_ids = (\n            input_tokens[:, 1] - pytorch_translate_data.MULTILING_DIALECT_ID_OFFSET\n        )\n        if input_tokens.size(1) > 2:\n            input_tokens = torch.cat([input_tokens[:, :1], input_tokens[:, 2:]], dim=1)\n        else:\n            input_tokens = input_tokens[:, :1]\n\n        bsz, seq_len = input_tokens.size()[:2]\n        if incremental_state is None:\n            incremental_state = {lang_id: None for lang_id in range(len(self.decoders))}\n        else:\n            seq_len = 1\n        # Create tensors for collecting encoder outputs\n        # +1 for language ID\n        all_logits = utils.maybe_cuda(\n            torch.zeros(bsz, seq_len + 1, self.max_vocab_size)\n        )\n        all_attn_scores = utils.maybe_cuda(\n            torch.zeros(bsz, seq_len, encoder_out[0].size(0))\n        )\n        self.last_bsz = bsz\n        self.last_lang_bszs = []\n        for lang_id, decoder in enumerate(self.decoders):\n            if decoder is None:\n                continue\n            if lang_id not in incremental_state:\n                incremental_state[lang_id] = {}\n            indices = torch.nonzero(lang_ids == lang_id)\n            lang_bsz = indices.size(0)\n            self.last_lang_bszs.append(lang_bsz)\n            if lang_bsz == 0:  # Language not in this batch\n                for p in decoder.parameters():\n                    p.grad = torch.zeros_like(p.data)\n                continue\n            indices = indices.squeeze(1)\n            max_source_length = torch.max(encoder_out[3][indices])\n            lang_encoder_out = (\n                encoder_out[0][:max_source_length, indices, :],\n                encoder_out[1][:, indices, :],\n                encoder_out[2][:, indices, :],\n                encoder_out[3][indices],\n                encoder_out[4][indices, :max_source_length],\n                encoder_out[5][:max_source_length, indices, :],\n            )\n\n            lang_logits, lang_attn_scores, _ = decoder(\n                input_tokens[indices], lang_encoder_out, incremental_state[lang_id]\n            )\n            all_attn_scores[indices, :, :max_source_length] = lang_attn_scores\n            all_logits[indices, 1:, : lang_logits.size(2)] = lang_logits\n        incremental_state[""lang_ids""] = lang_ids\n        return all_logits, all_attn_scores, None\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        """"""Reorder buffered internal state (for incremental generation).""""""\n        if not incremental_state:\n            return\n        bsz = new_order.size(0)\n        for lang_id, decoder in enumerate(self.decoders):\n            if decoder is None:\n                continue\n            indices = torch.nonzero(incremental_state[""lang_ids""] == lang_id)\n            lang_bsz = indices.size(0)\n            if lang_bsz > 0:\n                if lang_bsz == bsz:\n                    lang_new_order = new_order\n                else:\n                    lang_new_order = utils.densify(new_order[indices.squeeze(1)])\n                decoder.reorder_incremental_state(\n                    incremental_state[lang_id], lang_new_order\n                )\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return int(1e5)  # an arbitrary large number\n'"
pytorch_translate/multilingual_model.py,1,"b'#!/usr/bin/env python3\n\nfrom collections import OrderedDict\n\nimport torch.nn as nn\nfrom fairseq.models import FairseqMultiModel, register_model\nfrom pytorch_translate import common_layers, utils\n\n\n@register_model(""multilingual"")\nclass MultilingualModel(FairseqMultiModel):\n    """"""\n    To use, you must extend this class and define single_model_cls as a class\n    variable. Example:\n\n        @register_model(""multilingual_transformer"")\n        class MultilingualTransformerModel(MultilingualModel):\n            single_model_cls = TransformerModel\n\n            @staticmethod\n            def add_args(parser):\n                TransformerModel.add_args(parser)\n                MultilingualModel.add_args(parser)\n    """"""\n\n    def __init__(self, task, encoders, decoders):\n        super().__init__(encoders, decoders)\n        self.task = task\n        self.models = nn.ModuleDict(\n            {\n                key: self.__class__.single_model_cls(task, encoders[key], decoders[key])\n                for key in self.keys\n            }\n        )\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--share-encoder-embeddings"",\n            action=""store_true"",\n            help=""share encoder embeddings across languages"",\n        )\n        parser.add_argument(\n            ""--share-decoder-embeddings"",\n            action=""store_true"",\n            help=""share decoder embeddings across languages"",\n        )\n        parser.add_argument(\n            ""--share-encoders"",\n            action=""store_true"",\n            help=""share encoders across languages"",\n        )\n        parser.add_argument(\n            ""--share-decoders"",\n            action=""store_true"",\n            help=""share decoders across languages"",\n        )\n\n    @staticmethod\n    def set_multilingual_arch_args(args):\n        args.share_encoder_embeddings = getattr(args, ""share_encoder_embeddings"", False)\n        args.share_decoder_embeddings = getattr(args, ""share_decoder_embeddings"", False)\n        args.share_encoders = getattr(args, ""share_encoders"", False)\n        args.share_decoders = getattr(args, ""share_decoders"", False)\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        if not hasattr(args, ""max_source_positions""):\n            args.max_source_positions = 1024\n        if not hasattr(args, ""max_target_positions""):\n            args.max_target_positions = 1024\n\n        src_langs = [lang_pair.split(""-"")[0] for lang_pair in task.lang_pairs]\n        tgt_langs = [lang_pair.split(""-"")[1] for lang_pair in task.lang_pairs]\n\n        if args.share_encoders:\n            args.share_encoder_embeddings = True\n        if args.share_decoders:\n            args.share_decoder_embeddings = True\n\n        # encoders/decoders for each language\n        lang_encoders, lang_decoders = {}, {}\n\n        def get_encoder(lang, shared_encoder_embed_tokens=None):\n            if lang not in lang_encoders:\n                src_dict = task.dicts[lang]\n                if shared_encoder_embed_tokens is None:\n                    encoder_embed_tokens = common_layers.Embedding(\n                        num_embeddings=len(src_dict),\n                        embedding_dim=args.encoder_embed_dim,\n                        padding_idx=src_dict.pad(),\n                        freeze_embed=args.encoder_freeze_embed,\n                        normalize_embed=getattr(args, ""encoder_normalize_embed"", False),\n                    )\n                    utils.load_embedding(\n                        embedding=encoder_embed_tokens,\n                        dictionary=src_dict,\n                        pretrained_embed=args.encoder_pretrained_embed,\n                    )\n                else:\n                    encoder_embed_tokens = shared_encoder_embed_tokens\n                lang_encoders[lang] = cls.single_model_cls.build_encoder(\n                    args, src_dict, embed_tokens=encoder_embed_tokens\n                )\n            return lang_encoders[lang]\n\n        def get_decoder(lang, shared_decoder_embed_tokens=None):\n            """"""\n            Fetch decoder for the input `lang`, which denotes the target\n            language of the model\n            """"""\n            if lang not in lang_decoders:\n                tgt_dict = task.dicts[lang]\n                if shared_decoder_embed_tokens is None:\n                    decoder_embed_tokens = common_layers.Embedding(\n                        num_embeddings=len(tgt_dict),\n                        embedding_dim=args.decoder_embed_dim,\n                        padding_idx=tgt_dict.pad(),\n                        freeze_embed=args.decoder_freeze_embed,\n                    )\n                    utils.load_embedding(\n                        embedding=decoder_embed_tokens,\n                        dictionary=tgt_dict,\n                        pretrained_embed=args.decoder_pretrained_embed,\n                    )\n                else:\n                    decoder_embed_tokens = shared_decoder_embed_tokens\n                lang_decoders[lang] = cls.single_model_cls.build_decoder(\n                    args, task.dicts[lang], tgt_dict, embed_tokens=decoder_embed_tokens\n                )\n            return lang_decoders[lang]\n\n        # shared encoders/decoders (if applicable)\n        shared_encoder, shared_decoder = None, None\n        if args.share_encoders:\n            shared_encoder = get_encoder(src_langs[0])\n        if args.share_decoders:\n            shared_decoder = get_decoder(tgt_langs[0])\n\n        shared_encoder_embed_tokens, shared_decoder_embed_tokens = None, None\n        if args.share_encoder_embeddings:\n            shared_encoder_embed_tokens = FairseqMultiModel.build_shared_embeddings(\n                dicts=task.dicts,\n                langs=src_langs,\n                embed_dim=args.encoder_embed_dim,\n                build_embedding=common_layers.build_embedding,\n                pretrained_embed_path=None,\n            )\n        if args.share_decoder_embeddings:\n            shared_decoder_embed_tokens = FairseqMultiModel.build_shared_embeddings(\n                dicts=task.dicts,\n                langs=tgt_langs,\n                embed_dim=args.decoder_embed_dim,\n                build_embedding=common_layers.build_embedding,\n                pretrained_embed_path=None,\n            )\n        encoders, decoders = OrderedDict(), OrderedDict()\n        for lang_pair, src_lang, tgt_lang in zip(task.lang_pairs, src_langs, tgt_langs):\n            encoders[lang_pair] = (\n                shared_encoder\n                if shared_encoder is not None\n                else get_encoder(\n                    src_lang, shared_encoder_embed_tokens=shared_encoder_embed_tokens\n                )\n            )\n            decoders[lang_pair] = (\n                shared_decoder\n                if shared_decoder is not None\n                else get_decoder(\n                    tgt_lang, shared_decoder_embed_tokens=shared_decoder_embed_tokens\n                )\n            )\n\n        return cls(task, encoders, decoders)\n'"
pytorch_translate/multilingual_utils.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport os\nfrom collections import defaultdict\nfrom typing import Dict, List, Optional, Tuple\n\nfrom pytorch_translate.data import dictionary as pytorch_translate_dictionary\n\n\ndef get_source_langs(lang_pairs: List[str]) -> List[str]:\n    """"""\n    Return list of source languages from args.lang_pairs\n    lang_pairs: List[str] where each element is a str with comma separated list\n    of language pairs\n    """"""\n    return [lang_pair.split(""-"")[0] for lang_pair in lang_pairs]\n\n\ndef get_target_langs(lang_pairs: List[str]) -> List[str]:\n    """"""\n    Return list of target languages from args.lang_pairs\n    lang_pairs: List[str] where each element is a str with comma separated list\n    of language pairs\n    """"""\n    return [lang_pair.split(""-"")[1] for lang_pair in lang_pairs]\n\n\ndef default_binary_path(save_dir: str, lang_pair: str, lang: str, split: str) -> str:\n    return os.path.join(save_dir, f""{split}-binary-{lang_pair}.{lang}"")\n\n\ndef get_dict_paths(\n    vocabulary_args: Optional[List[str]], langs: List[str], save_dir: str\n) -> Dict[str, str]:\n    """"""\n    Extract dictionary files based on --vocabulary argument, for the given\n    languages `langs`.\n    vocabulary_arg: Optional[List[str]] where each element is a str with the format\n    ""lang:vocab_file""\n    """"""\n    dicts = {}\n    if vocabulary_args is not None:\n        for vocab_config in vocabulary_args:\n            # vocab_config is in the format ""lang:vocab_file""\n            lang, vocab = vocab_config.split("":"")\n            if lang in langs:\n                dicts[lang] = vocab\n    for lang in langs:\n        if lang not in dicts:\n            dicts[lang] = pytorch_translate_dictionary.default_dictionary_path(\n                save_dir=save_dir, dialect=lang\n            )\n    return dicts\n\n\ndef get_corpora_for_lang(parallel_corpora: List[str], lang: str) -> List[str]:\n    """"""\n    Fetches list of corpora that belong to given lang\n    parallel_corpora: List[str] where each element is a str with the format\n    ""src_lang-tgt_lang:src_corpus,tgt_corpus""\n\n    Returns [] if corpora for lang is not found\n    """"""\n    corpora = []\n    for parallel_corpus_config in parallel_corpora:\n        lang_pair, parallel_corpus = parallel_corpus_config.split("":"")\n        src_lang, tgt_lang = lang_pair.split(""-"")\n        if src_lang == lang:\n            corpora.append(parallel_corpus.split("","")[0])\n        if tgt_lang == lang:\n            corpora.append(parallel_corpus.split("","")[1])\n    return corpora\n\n\ndef get_parallel_corpus_for_lang_pair(\n    parallel_corpora: List[str], lang_pair: str\n) -> Tuple[str, str]:\n    """"""\n    Fetches parallel corpus that belong to given lang_pair\n    parallel_corpora: List[str] where each element is a str with the format\n    ""src_lang-tgt_lang:src_corpus,tgt_corpus""\n\n    Returns None if parallel corpora for lang_pair is not found\n    """"""\n    for parallel_corpus_config in parallel_corpora:\n        corpus_lang_pair, parallel_corpus = parallel_corpus_config.split("":"")\n        if corpus_lang_pair == lang_pair:\n            return tuple(parallel_corpus.split("",""))\n    return None\n\n\ndef prepare_dicts(\n    args: argparse.Namespace, langs: List[str]\n) -> Tuple[Dict[str, str], Dict[str, pytorch_translate_dictionary.Dictionary]]:\n    """"""\n    Uses multilingual train corpora specified in args.multilingual_train_text_file\n    to build dictionaries for languages specified in `langs`.\n    Vocab size is defined by args.target_max_vocab_size if lang is in the set\n    of target languages, otherwise it is decided by args.target_max_vocab_size\n    """"""\n    tgt_langs = get_target_langs(args.lang_pairs.split("",""))\n    dict_paths = get_dict_paths(args.vocabulary, langs, args.save_dir)\n    lang2corpus = defaultdict(list)\n    for lang in langs:\n        lang2corpus[lang] = get_corpora_for_lang(\n            args.multilingual_train_text_file, lang\n        )\n    dict_objects = {\n        lang: pytorch_translate_dictionary.Dictionary.build_vocab_file_if_nonexistent(\n            corpus_files=lang2corpus[lang],\n            vocab_file=dict_paths[lang],\n            max_vocab_size=(\n                args.target_max_vocab_size\n                if lang in tgt_langs\n                else args.source_max_vocab_size\n            ),\n            tokens_with_penalty=None,\n        )\n        for lang in langs\n    }\n    return dict_paths, dict_objects\n'"
pytorch_translate/ngram.py,3,"b'#!/usr/bin/env python3\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom pytorch_translate import attention, utils as pytorch_translate_utils\nfrom pytorch_translate.common_layers import (\n    DecoderWithOutputProjection,\n    Embedding,\n    Linear,\n    NonlinearLayer,\n)\nfrom pytorch_translate.utils import maybe_cat\n\n\nclass NGramDecoder(DecoderWithOutputProjection):\n    """"""n-gram decoder.\n\n    This decoder implementation does not condition on the full target-side\n    history. Instead, predictions only depend on the target n-gram history and\n    the full source sentence via attention over encoder outputs. The decoder\n    network is a feedforward network with source context as additional input.\n    """"""\n\n    def __init__(\n        self,\n        src_dict,\n        dst_dict,\n        vocab_reduction_params=None,\n        n=4,\n        encoder_hidden_dim=512,\n        embed_dim=512,\n        freeze_embed=False,\n        hidden_dim=512,\n        out_embed_dim=512,\n        num_layers=1,\n        dropout_in=0.1,\n        dropout_out=0.1,\n        attention_type=""dot"",\n        residual_level=None,\n        activation_fn=nn.ReLU,\n        project_output=True,\n        pretrained_embed=None,\n        projection_pretrained_embed=None,\n    ):\n        super().__init__(\n            src_dict,\n            dst_dict,\n            vocab_reduction_params,\n            out_embed_dim,\n            project_output=project_output,\n            pretrained_embed=projection_pretrained_embed,\n        )\n        self.history_len = n - 1\n        self.encoder_hidden_dim = encoder_hidden_dim\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.out_embed_dim = out_embed_dim\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.attention_type = attention_type\n        self.residual_level = residual_level\n        self.dst_dict = dst_dict\n        self.activation_fn = activation_fn\n\n        num_embeddings = len(dst_dict)\n        padding_idx = dst_dict.pad()\n        self.embed_tokens = Embedding(\n            num_embeddings=num_embeddings,\n            embedding_dim=embed_dim,\n            padding_idx=padding_idx,\n            freeze_embed=freeze_embed,\n        )\n        pytorch_translate_utils.load_embedding(\n            embedding=self.embed_tokens,\n            dictionary=dst_dict,\n            pretrained_embed=pretrained_embed,\n        )\n\n        self.history_conv = nn.Sequential(\n            torch.nn.Conv1d(embed_dim, hidden_dim, self.history_len), activation_fn()\n        )\n\n        self.hidden_dim = hidden_dim\n        self.layers = nn.ModuleList(\n            [\n                NonlinearLayer(hidden_dim, hidden_dim, activation_fn=activation_fn)\n                for _ in range(num_layers)\n            ]\n        )\n\n        self.attention = attention.build_attention(\n            attention_type=attention_type,\n            decoder_hidden_state_dim=hidden_dim,\n            context_dim=encoder_hidden_dim,\n            force_projection=True,\n        )\n        self.combined_output_and_context_dim = self.attention.context_dim + hidden_dim\n        if self.combined_output_and_context_dim != out_embed_dim:\n            self.additional_fc = Linear(\n                self.combined_output_and_context_dim, out_embed_dim\n            )\n\n    def forward_unprojected(self, input_tokens, encoder_out, incremental_state=None):\n        padded_tokens = F.pad(\n            input_tokens,\n            (self.history_len - 1, 0, 0, 0),\n            ""constant"",\n            self.dst_dict.eos(),\n        )\n        # We use incremental_state only to check whether we are decoding or not\n        # self.training is false even for the forward pass through validation\n        if incremental_state is not None:\n            padded_tokens = padded_tokens[:, -self.history_len :]\n        utils.set_incremental_state(self, incremental_state, ""incremental_marker"", True)\n\n        bsz, seqlen = padded_tokens.size()\n        seqlen -= self.history_len - 1\n\n        # get outputs from encoder\n        (encoder_outs, final_hidden, _, src_lengths, _) = encoder_out\n\n        # padded_tokens has shape [batch_size, seq_len+history_len]\n        x = self.embed_tokens(padded_tokens)\n        x = F.dropout(x, p=self.dropout_in, training=self.training)\n\n        # Convolution needs shape [batch_size, channels, seq_len]\n        x = self.history_conv(x.transpose(1, 2)).transpose(1, 2)\n        x = F.dropout(x, p=self.dropout_out, training=self.training)\n\n        # x has shape [batch_size, seq_len, channels]\n        for i, layer in enumerate(self.layers):\n            prev_x = x\n            x = layer(x)\n            x = F.dropout(x, p=self.dropout_out, training=self.training)\n            if self.residual_level is not None and i >= self.residual_level:\n                x = x + prev_x\n\n        # Attention\n        attn_out, attn_scores = self.attention(\n            x.transpose(0, 1).contiguous().view(-1, self.hidden_dim),\n            encoder_outs.repeat(1, seqlen, 1),\n            src_lengths.repeat(seqlen),\n        )\n        if attn_out is not None:\n            attn_out = attn_out.view(seqlen, bsz, -1).transpose(1, 0)\n        attn_scores = attn_scores.view(-1, seqlen, bsz).transpose(0, 2)\n        x = maybe_cat((x, attn_out), dim=2)\n\n        # bottleneck layer\n        if hasattr(self, ""additional_fc""):\n            x = self.additional_fc(x)\n            x = F.dropout(x, p=self.dropout_out, training=self.training)\n        return x, attn_scores\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return int(1e5)  # an arbitrary large number\n'"
pytorch_translate/options.py,1,"b'#!/usr/bin/env python3\n\nimport os\n\nimport torch\nfrom fairseq.data.indexed_dataset import get_available_dataset_impl\nfrom pytorch_translate import constants, utils\n\n\ndef check_unsupported_fairseq_flags(args):\n    UNSUPPORTED_FAIRSEQ_FLAGS = [\n        # Use --save-interval-updates instead.\n        (""save_interval"", 1),\n        # Look at --num-avg-checkpoints and --auto-clear-checkpoints.\n        (""keep_interval_updates"", -1),\n        # We always save checkpoints at the end of an epoch.\n        (""no_epoch_checkpoints"", False),\n        # We always save checkpoints.\n        (""no_save"", False),\n        # We run validation every time we save a checkpoint, so this is effectively\n        # controlled by --save-interval-updates as well.\n        (""validate_interval"", 1),\n    ]\n\n    for (flag_name, default_value) in UNSUPPORTED_FAIRSEQ_FLAGS:\n        if hasattr(args, flag_name):\n            if getattr(args, flag_name) != default_value:\n                raise ValueError(\n                    f""Found unsupported Fairseq flag ""\n                    f""--{flag_name.replace(\'_\', \'-\')} with non-default value ""\n                    f""of {getattr(args, flag_name)} ""\n                )\n            setattr(args, flag_name, None)\n\n\ndef add_dataset_args(parser, train=False, gen=False):\n    """"""Same as fairseq.options.add_dataset_args but without\n    the ""data"" argument""""""\n    group = parser.add_argument_group(""Dataset and data loading"")\n    group.add_argument(\n        ""data"",\n        metavar=""DIR"",\n        nargs=""?"",\n        help=""path to data directory. ""\n        ""This is not needed but kept for backward compatibility"",\n    )\n    group.add_argument(\n        ""--num-workers"",\n        default=0,\n        type=int,\n        metavar=""N"",\n        help=""how many subprocesses to use for data loading"",\n    )\n    group.add_argument(\n        ""--skip-invalid-size-inputs-valid-test"",\n        action=""store_true"",\n        help=""Ignore too long or too short lines in valid and test set"",\n    )\n    group.add_argument(\n        ""--max-tokens"",\n        default=5000,\n        type=int,\n        metavar=""N"",\n        help=""maximum number of tokens in a batch"",\n    )\n    group.add_argument(\n        ""--max-sentences"",\n        ""--batch-size"",\n        type=int,\n        metavar=""N"",\n        help=""maximum number of sentences in a batch"",\n    )\n    group.add_argument(\n        ""--dataset-impl"",\n        metavar=""FORMAT"",\n        choices=get_available_dataset_impl(),\n        help=""output dataset implementation"",\n    )\n    if train:\n        group.add_argument(\n            ""--train-subset"",\n            default=""train"",\n            metavar=""SPLIT"",\n            choices=[""train"", ""valid"", ""test""],\n            help=""data subset to use for training (train, valid, test)"",\n        )\n        group.add_argument(\n            ""--valid-subset"",\n            default=""valid"",\n            metavar=""SPLIT"",\n            help=""comma separated list of data subsets to use""\n            "" for validation (train, valid, valid1,test, test1)"",\n        )\n        group.add_argument(\n            ""--max-sentences-valid"",\n            type=int,\n            metavar=""N"",\n            help=""maximum number of sentences in a validation batch""\n            "" (defaults to --max-sentences)"",\n        )\n    if gen:\n        group.add_argument(\n            ""--gen-subset"",\n            default=""test"",\n            metavar=""SPLIT"",\n            help=""data subset to generate (train, valid, test)"",\n        )\n        group.add_argument(\n            ""--num-shards"",\n            default=1,\n            type=int,\n            metavar=""N"",\n            help=""shard generation over N shards"",\n        )\n        group.add_argument(\n            ""--shard-id"",\n            default=0,\n            type=int,\n            metavar=""ID"",\n            help=""id of the shard to generate (id < num_shards)"",\n        )\n    return group\n\n\ndef add_preprocessing_args(parser):\n    # Args related to dataset.\n    group = parser.add_argument_group(""Preprocess data"")\n    group.add_argument(\n        ""--source-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file representing the dictionary of tokens to use. ""\n        ""If the file does not exist, the dict is auto-generated from source ""\n        ""training data and saved as that file."",\n    )\n    group.add_argument(\n        ""--source-max-vocab-size"",\n        default=-1,\n        type=int,\n        metavar=""N"",\n        help=""If a new vocab file needs to be generated, restrict it to the ""\n        ""top N most common words. If we re-use an existing vocab file, this ""\n        ""flag will have no effect. A value of < 0 means no max size."",\n    )\n    group.add_argument(\n        ""--source-bpe-cont-marker"",\n        default=None,\n        type=str,\n        metavar=""CONT"",\n        help=""Source BPE continuation marker. You should only specify this if ""\n        ""you are using a BPE source vocab that has an continuation marker ""\n        ""suffix. Note that this is the default BPE format in fairseq. Ex: \'@@\'"",\n    )\n    group.add_argument(\n        ""--source-bpe-end-marker"",\n        default=None,\n        type=str,\n        metavar=""END"",\n        help=""Source BPE end marker. You should only specify this if you are ""\n        ""using a BPE source vocab that has an end marker suffix. Ex: \'_EOW\'"",\n    )\n    group.add_argument(\n        ""--char-source-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Same as --source-vocab-file except using characters."",\n    )\n    group.add_argument(\n        ""--char-target-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Same as --target-vocab-file except using characters."",\n    )\n    group.add_argument(\n        ""--embed-bytes"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=""If specified along with a character model and set to True, ""\n        ""then we embed bytes instead of characters."",\n    )\n    group.add_argument(\n        ""--char-source-max-vocab-size"",\n        default=-1,\n        type=int,\n        metavar=""N"",\n        help=""Same as --source-max-vocab-size except using characters."",\n    )\n    group.add_argument(\n        ""--char-target-max-vocab-size"",\n        default=-1,\n        type=int,\n        metavar=""N"",\n        help=""Same as --target-max-vocab-size except using characters."",\n    )\n    group.add_argument(\n        ""--target-vocab-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file representing the fairseq Dictionary to use. ""\n        ""If the file does not exist, the dict is auto-generated from target ""\n        ""training data and saved as that file."",\n    )\n    group.add_argument(\n        ""--target-max-vocab-size"",\n        default=-1,\n        type=int,\n        metavar=""N"",\n        help=""If a new vocab file needs to be generated, restrict it to the ""\n        ""top N most common words. If we re-use an existing vocab file, this ""\n        ""flag will have no effect. A value of < 0 means no max size."",\n    )\n    group.add_argument(\n        ""--target-bpe-cont-marker"",\n        default=None,\n        type=str,\n        metavar=""CONT"",\n        help=""Target BPE continuation marker. You should only specify this if ""\n        ""you are using a BPE target vocab that has an continuation marker ""\n        ""suffix. Note that this is the default BPE format in fairseq. Ex: \'@@\'"",\n    )\n    group.add_argument(\n        ""--target-bpe-end-marker"",\n        default=None,\n        type=str,\n        metavar=""END"",\n        help=""Target BPE end marker. You should only specify this if you are ""\n        ""using a BPE target vocab that has an end marker suffix. Ex: \'_EOW\'"",\n    )\n    group.add_argument(\n        ""--fairseq-data-format"",\n        type=bool,\n        default=False,\n        help=""binary paths are prefixes for .bin/.idx mmap datasets"",\n    )\n    group.add_argument(\n        ""--train-source-text-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file containing source training examples."",\n    )\n    group.add_argument(\n        ""--train-target-text-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file containing target training examples."",\n    )\n    group.add_argument(\n        ""--eval-source-text-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file containing source eval examples for ""\n        ""calculating validation loss and BLEU eval scores."",\n    )\n    group.add_argument(\n        ""--eval-target-text-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file containing target eval examples for ""\n        ""calculating validation loss and BLEU eval scores."",\n    )\n    group.add_argument(\n        ""--train-source-binary-path"",\n        default="""",\n        help=""Path for the binary file containing source training examples."",\n    )\n    group.add_argument(\n        ""--train-target-binary-path"",\n        default="""",\n        help=""Path for the binary file containing target training examples."",\n    )\n    group.add_argument(\n        ""--train-weights-path"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file of weight (0 to 1) for each train example..""\n        ""If left empty, all examples will receive equal weights."",\n    )\n    group.add_argument(\n        ""--eval-source-binary-path"",\n        default="""",\n        help=""Path for the binary file containing source eval examples for ""\n        ""calculating validation loss and BLEU scores."",\n    )\n    group.add_argument(\n        ""--eval-target-binary-path"",\n        default="""",\n        help=""Path for the binary file containing target eval examples for ""\n        ""calculating validation loss and BLEU scores."",\n    )\n    group.add_argument(\n        ""--train-mono-source-binary-path"",\n        default="""",\n        help=""Path for the binary file containing source side monolingual data"",\n    )\n    group.add_argument(\n        ""--train-mono-target-binary-path"",\n        default="""",\n        help=""Path for the binary file containing target side monolingual data"",\n    )\n\n    # TODO(T43045193): Move this to multilingual_task.py eventually\n    group.add_argument(\n        ""--multiling-encoder-lang"",\n        action=""append"",\n        metavar=""SRC"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify a list of encoder languages. The multilingual model contains ""\n        ""a separate encoder for each language in this list."",\n    )\n    group.add_argument(\n        ""--multiling-decoder-lang"",\n        action=""append"",\n        metavar=""TARGET"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify a list of decoder languages. The multilingual model contains ""\n        ""a separate decoder for each language in this list."",\n    )\n    group.add_argument(\n        ""--multiling-source-lang"",\n        action=""append"",\n        metavar=""SRC"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify a list of corpus source languages, where the n-th language is ""\n        ""the source language of the n-th training corpus. Each entry must be ""\n        ""in --multiling-encoder-lang."",\n    )\n    group.add_argument(\n        ""--multiling-target-lang"",\n        action=""append"",\n        metavar=""TARGET"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify a list of corpus target languages, where the n-th language is ""\n        ""the target language of the n-th training corpus. Each entry must be ""\n        ""in --multiling-decoder-lang."",\n    )\n    group.add_argument(\n        ""--multiling-source-vocab-file"",\n        action=""append"",\n        metavar=""FILE"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify the path to the dictionary for the n-th entry in ""\n        ""--multiling-encoder-lang"",\n    )\n    group.add_argument(\n        ""--multiling-target-vocab-file"",\n        action=""append"",\n        metavar=""FILE"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify the path to the dictionary for the n-th entry in ""\n        ""--multiling-decoder-lang"",\n    )\n    group.add_argument(\n        ""--multiling-train-source-text-file"",\n        action=""append"",\n        metavar=""FILE"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify paths to training source samples. The n-th entry should be ""\n        ""in the n-th language in --multiling-source-lang."",\n    )\n    group.add_argument(\n        ""--multiling-train-target-text-file"",\n        action=""append"",\n        metavar=""FILE"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify paths to training target samples. The n-th entry should be ""\n        ""in the n-th language in --multiling-target-lang."",\n    )\n    group.add_argument(\n        ""--multiling-train-oversampling"",\n        action=""append"",\n        type=int,\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""oversample corpora. The n-th training corpus is oversampled by the n-""\n        ""the entry. No oversampling if not specified."",\n    )\n    group.add_argument(\n        ""--multiling-eval-source-text-file"",\n        action=""append"",\n        metavar=""FILE"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify paths to eval source samples. The n-th entry should be ""\n        ""in the n-th language in --multiling-source-lang."",\n    )\n    group.add_argument(\n        ""--multiling-eval-target-text-file"",\n        action=""append"",\n        metavar=""FILE"",\n        help=""For multilingual models only. Use this argument repeatedly to ""\n        ""specify paths to eval target samples. The n-th entry should be ""\n        ""in the n-th language in --multiling-target-lang."",\n    )\n    group.add_argument(\n        ""--multiling-rescale-grads"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=(\n            ""If true, rescale gradients based on the number of training ""\n            ""samples a specific component has received in a training batch.""\n        ),\n    )\n\n    group.add_argument(\n        ""--penalized-target-tokens-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""Path to text file of tokens to receive a penalty in decoding.""\n        ""If left empty, no penalty will be applied"",\n    )\n\n    group.add_argument(\n        ""--append-eos-to-source"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=(""If true, append EOS to source sentences (instead of just target).""),\n    )\n    group.add_argument(\n        ""--reverse-source"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=True,\n        help=(""If true, feed source sentence to model in reverse order.""),\n    )\n    group.add_argument(\n        ""--reverse-target"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=(""If true, feed target sentence to model in reverse order.""),\n    )\n\n\ndef validate_preprocessing_args(args):\n    if (\n        not (\n            (\n                getattr(args, ""train_source_text_file"", None)\n                or getattr(args, ""train_source_binary_path"", None)\n                or getattr(args, ""multiling_train_source_text_file"", None)\n                or getattr(args, ""multilingual_train_text_file"", None)\n                or getattr(args, ""multilingual_train_binary_path"", None)\n            )\n            and (\n                getattr(args, ""train_target_text_file"", None)\n                or getattr(args, ""train_target_binary_path"", None)\n                or getattr(args, ""multiling_train_target_text_file"", None)\n                or getattr(args, ""multilingual_train_text_file"", None)\n                or getattr(args, ""multilingual_train_binary_path"", None)\n            )\n            and (\n                getattr(args, ""eval_source_text_file"", None)\n                or getattr(args, ""eval_source_binary_path"", None)\n                or getattr(args, ""multiling_eval_source_text_file"", None)\n                or getattr(args, ""multilingual_eval_text_file"", None)\n                or getattr(args, ""multilingual_eval_binary_path"", None)\n            )\n            and (\n                getattr(args, ""eval_target_text_file"", None)\n                or getattr(args, ""eval_target_binary_path"", None)\n                or getattr(args, ""multiling_eval_target_text_file"", None)\n                or getattr(args, ""multilingual_eval_text_file"", None)\n                or getattr(args, ""multilingual_eval_binary_path"", None)\n            )\n        )\n        and args.task != ""dual_learning_task""\n    ):\n        raise ValueError(\n            ""At least one of --*_text_file or --*_binary_path flags must be ""\n            ""specified for each of --{train, eval}_{source, target}_*""\n        )\n\n    for file_type in (\n        ""train_source_text_file"",\n        ""train_target_text_file"",\n        ""eval_source_text_file"",\n        ""eval_target_text_file"",\n    ):\n        text_file = getattr(args, file_type)\n        if text_file and not os.path.isfile(text_file):\n            raise ValueError(\n                f""Please specify an existing text file for ""\n                f""--{file_type}={text_file}""\n            )\n\n    for file_type in (""source_vocab_file"", ""target_vocab_file""):\n        vocab_file = getattr(args, file_type)\n        if not vocab_file:\n            raise ValueError(\n                f""--{file_type} must be specified - even if you don\'t have ""\n                f""a vocab file, you must still specify a location ""\n                f""for it to be written to.""\n            )\n\n    if args.task == constants.SEMI_SUPERVISED_TASK and not (\n        getattr(args, ""train_mono_source_binary_path"", None)\n        or getattr(args, ""train_mono_target_binary_path"", None)\n        or getattr(args, ""train_mono_source_text_file"", None)\n    ):\n        raise ValueError(\n            ""For semisupervised training, at least one of --*_text_file or ""\n            ""--*_binary_path flags must be specified for at least one of ""\n            ""--train_mono_{source, target}_*""\n        )\n\n        for file_type in (""train_mono_source_text_file"", ""train_mono_target_text_file""):\n            file_path = getattr(args, file_type)\n            if file_path and not os.path.isfile(file_path):\n                raise ValueError(\n                    f""Please specify an existing text file for --{file_type}=""\n                    f""{file_path}""\n                )\n\n\ndef expand_optimization_args(group):\n    """"""Expands the optimization related arguments with pytorch_translate\n    specific arguments""""""\n    group.add_argument(\n        ""--local-num-gpus"",\n        default=torch.cuda.device_count(),\n        type=int,\n        metavar=""N"",\n        help=(\n            ""The number of local GPUs to use for training on this machine. ""\n            ""Defaults to using all visible GPUs. This should be ""\n            ""<= --distributed-world-size.""\n        ),\n    )\n    group.add_argument(\n        ""--stop-time-hr"",\n        default=-1.0,\n        type=float,\n        metavar=""N"",\n        help=""Stops training after N hours have elapsed. Use decimal values ""\n        ""for sub-hourly granularity. A value of < 0 disables this."",\n    )\n    group.add_argument(\n        ""--stop-no-best-validate-loss"",\n        default=-1,\n        type=int,\n        metavar=""N"",\n        help=""Stops training after N validations have been run without ""\n        ""achieving a better loss than before. Note that this is affected by ""\n        ""--save-interval-updates in how frequently we run validation in the ""\n        ""first place. A value of < 0 disables this."",\n    )\n    group.add_argument(\n        ""--stop-no-best-bleu-eval"",\n        default=-1,\n        type=int,\n        metavar=""N"",\n        help=""Stops training after N evals have been run without ""\n        ""achieving a better BLEU score than before. Note that this is affected ""\n        ""by --save-interval-updates in how frequently we run BLEU eval ""\n        ""in the first place. A value of < 0 disables this."",\n    )\n    group.add_argument(\n        ""--shrink-lr-no-best-tune-loss"",\n        default=5,\n        type=int,\n        metavar=""N"",\n        help=""Decay learning rate after N evals have been run without ""\n        ""achieving a lower tune loss than before. This is to achieve ""\n        ""decay lr within an epoch, independent of lr_scheduler. ""\n        ""Note that this is affected by --save-interval-updates in ""\n        ""how frequently we run BLEU eval in the first place. ""\n        ""A value of < 0 disables this."",\n    )\n    group.add_argument(\n        ""--pruning-percentile"",\n        type=int,\n        default=0,\n        help=""Proportion of weights to prune. A value <=0 disables pruning.""\n        "" By default, prunes weights uniformly and ignores bias terms."",\n    )\n    group.add_argument(\n        ""--parameters-to-prune"",\n        default=""all"",\n        help=""Names of layers to prune. Layers are pruned if the argument is ""\n        ""a substring of the layer name. Options are \'all\', \'embed\', \'lstm\'. "",\n    )\n    group.add_argument(\n        ""--loss-beam"",\n        type=int,\n        default=0,\n        help=""Beam size to use for \'sequence_nll\' loss and \'sequence_risk\' ""\n        ""loss. If zero, use --beam."",\n    )\n    group.add_argument(\n        ""--disable-eval-bleu"",\n        nargs=""?"",\n        const=True,\n        default=False,\n        type=utils.bool_flag,\n        help=(""disable bleu score evaluation on tune dataset""),\n    )\n\n    return group\n\n\ndef expand_checkpointing_args(group):\n    """"""Expands the checkpointing related arguments with pytorch_translate\n    specific arguments""""""\n    group.add_argument(\n        ""--auto-clear-checkpoints"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=True,\n        help=(\n            ""If True, we keep only the last --num-avg-checkpoints checkpoints ""\n            ""on disk and delete all older checkpoints.""\n        ),\n    )\n    group.add_argument(\n        ""--num-avg-checkpoints"",\n        default=1,\n        type=int,\n        metavar=""N"",\n        help=(\n            ""Average over the last N checkpoints when saving ""\n            ""averaged checkpoints and when doing BLEU eval. Must be >=1.""\n        ),\n    )\n    group.add_argument(\n        ""--pretrained-checkpoint-file"",\n        default="""",\n        type=str,\n        metavar=""FILE"",\n        help=(\n            ""Allows the user to resume or fine-tune training (possibly with ""\n            ""different parameter) from another run\'s checkpoint. Note that ""\n            ""if there\'s already an existing checkpoint at ""\n            ""os.path.join(--save-dir, --restore-file), this ""\n            ""flag will have no effect. We prefer using --save-dir over this ""\n            ""flag to ensure that if user resumes training on a model that ""\n            ""was originaly initialized from another run\'s checkpoint, we won\'t ""\n            ""wipe out all progress in --save-dir by re-loading the previous ""\n            ""run\'s checkpoint.""\n        ),\n    )\n    group.add_argument(\n        ""--load-pretrained-checkpoint-state"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=(\n            ""Whether to also load optimizer extra_state (epoch, ""\n            ""training time, etc) when loading from --pretrained-checkpoint-file. ""\n            ""If false, only model weights are loaded from that file. ""\n            ""Note that this has no effect when restoring from --restore-file.""\n        ),\n    )\n\n    return group\n\n\ndef expand_generation_args(group, train=False):\n    """"""Expands the generation related arguments with pytorch_translate\n    specific arguments""""""\n    group.add_argument(\n        ""--word-reward"",\n        type=float,\n        default=0.0,\n        help=(\n            ""Value to add to (log-prob) score for each token except EOS. ""\n            ""Value < 0 encourages shorter translations, while > 0 ""\n            ""(the usual case) encourages longer translations ""\n            ""(similar to --length-penalty).""\n        ),\n    )\n    group.add_argument(\n        ""--unk-reward"",\n        type=float,\n        default=0.0,\n        help=(\n            ""Value to add to (log-prob) score for UNK tokens. ""\n            ""Value < 0 (the usual case) encourages fewer UNKs, while > 0 ""\n            ""encourages more UNKs.""\n        ),\n    )\n    group.add_argument(\n        ""--length-penalty"",\n        type=float,\n        default=0.0,\n        help=(\n            ""When >0 scores are normalized according to length (divided by ""\n            ""length^length_penalty). Effectively overrides word_reward when""\n            ""in use. NOTE: supersedes --lenpen.""\n        ),\n    )\n    group.add_argument(\n        ""--model-weights"",\n        default="""",\n        help=(\n            ""Interpolation weights for ensembles. Comma-separated list of ""\n            ""floats with length equal to the number of models in the ensemble.""\n        ),\n    )\n    group.add_argument(\n        ""--report-oracle-bleu"",\n        type=utils.bool_flag,\n        nargs=""?"",\n        const=True,\n        default=False,\n        help=(\n            ""During evaluation, determine best among top-k outputs (where k ""\n            ""is controlled by --nbest) for each sentence by smoothed ""\n            ""sentence-level BLEU and report overall BLEU score for these ""\n            ""sentences.""\n        ),\n    )\n    group.add_argument(\n        ""--output-hypos-binary-path"",\n        default=None,\n        type=str,\n        help=(\n            ""Optional filename to save output hypotheses (binary format ""\n            ""and EOS-terminated, suitable for use as training targets)""\n        ),\n    )\n    group.add_argument(\n        ""--max-examples-to-evaluate"",\n        default=-1,\n        type=int,\n        help=(\n            ""If >0 and smaller than size of evaluation data set, randomly ""\n            ""sample this many examples to evaluate""\n        ),\n    )\n\n    group.add_argument(\n        ""--max-examples-to-evaluate-seed"",\n        default=-1,\n        type=int,\n        help=(\n            ""If not -1, set seed for random sample as the given seed value, ""\n            ""so we can replicate result on random sample.""\n        ),\n    )\n\n    group.add_argument(\n        ""--output-source-binary-path"",\n        default=None,\n        type=str,\n        help=(\n            ""Optional filename to save binarized source after evaluation ""\n            ""(primary use case being that this source dataset will be after ""\n            ""any filtering due to --max-examples-to-evaluate)""\n        ),\n    )\n    group.add_argument(\n        ""--translation-info-export-path"",\n        default=None,\n        type=str,\n        help=(""Optional path to save translation info output in pickled format""),\n    )\n    group.add_argument(\n        ""--diversity-sibling-gamma"",\n        type=float,\n        default=0.0,\n        help=(""The diversity rate of sibling_rank for generating diverse beams""),\n    )\n    group.add_argument(\n        ""--hypotheses-export-path"",\n        default=None,\n        type=str,\n        help=(""Optional path to save all generated hypotheses to external file""),\n    )\n\n    # These arguments are only used during training\n    if train:\n        group.add_argument(\n            ""--multi-model-restore-files"",\n            default=None,\n            type=str,\n            nargs=""+"",\n            help=(\n                ""If --multi-encoder = --multi-decoder > 1, this option makes ""\n                ""it possible to initialize individual model weights from ""\n                ""existing checkpoints of separate training runs.""\n            ),\n        )\n    return group\n\n\ndef validate_generation_args(args):\n    assert args.unkpen == 0, (\n        ""PyTorch Translate does not use fairseq\'s --unkpen flag. ""\n        ""Use --unk-reward instead, and check the flag description regarding ""\n        ""sign polarity meaning.""\n    )\n    assert args.lenpen == 1, (\n        ""Argument --lenpen is IGNORED by pytorch_translate. Use ""\n        ""--length-penalty instead.""\n    )\n    if ""num_avg_checkpoints"" in args:\n        assert args.num_avg_checkpoints >= 1, ""--num-avg-checkpoints must be >= 1.""\n\n\ndef add_verbosity_args(parser, train=False):\n    verbosity_group = parser.add_argument_group(""Verbosity"")\n    verbosity_group.add_argument(\n        ""--log-verbose"",\n        action=""store_true"",\n        help=""Whether to output more verbose logs for debugging/profiling."",\n    )\n    verbosity_group.add_argument(\n        ""--args-verbosity"",\n        default=1,\n        type=int,\n        choices=[0, 1, 2],\n        help=""Level of verbosity when printing the arguments (0: don\'t print ""\n        ""the arguments; 1: print the Namespace object; 2: print all the ""\n        ""arguments, one per line). The default is 1.""\n        ""one per line)"",\n    )\n    return verbosity_group\n\n\ndef print_args(args):\n    args_verbosity = getattr(args, ""args_verbosity"", 1)\n    if args_verbosity == 2:\n        args_sorted = sorted(vars(args).items())\n        for name, value in args_sorted:\n            print(f""{name}={value}"")\n    elif args_verbosity == 1:\n        print(args)\n    elif args_verbosity == 0:\n        return\n    else:\n        raise ValueError(""Please specify an argument verbosity level between 0 and 2"")\n'"
pytorch_translate/preprocess.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport tempfile\nfrom typing import Dict, List, Optional\n\nfrom pytorch_translate import (\n    constants,\n    multilingual_utils,\n    options as pytorch_translate_options,\n    utils,\n)\nfrom pytorch_translate.data import char_data, data as pytorch_translate_data\nfrom pytorch_translate.data.dictionary import Dictionary\n\n\ndef maybe_generate_temp_file_path(output_path=None):\n    """"""\n    This function generates a temp file path if output_path is empty or None.\n    This is useful to do before calling any preprocessing function that has a\n    precondition that data path arguments are set to valid filepaths.\n    """"""\n    if not output_path:\n        fd, output_path = tempfile.mkstemp()\n        # We only need a unique file name since the helper functions\n        # take care of actually creating the file.\n        os.close(fd)\n    # numpy silently appends this suffix if it is not present, so this ensures\n    # that the correct path is returned\n    if not output_path.endswith("".npz""):\n        output_path += "".npz""\n    return output_path\n\n\ndef binarize_text_file(\n    text_file: str,\n    dictionary: Dictionary,\n    output_path: str,\n    append_eos: bool,\n    reverse_order: bool,\n    embed_bytes: bool = False,\n    char_dictionary: Optional[Dictionary] = None,\n    already_numberized: bool = False,\n) -> str:\n    output_path = maybe_generate_temp_file_path(output_path)\n    if char_dictionary is not None:\n        dataset = char_data.InMemoryNumpyWordCharDataset()\n        dataset.parse(\n            path=text_file,\n            word_dict=dictionary,\n            char_dict=char_dictionary,\n            embed_bytes=embed_bytes,\n            reverse_order=reverse_order,\n            append_eos=append_eos,\n        )\n    else:\n        dataset = pytorch_translate_data.InMemoryIndexedDataset()\n        dataset.parse(\n            path=text_file,\n            dictionary=dictionary,\n            reverse_order=reverse_order,\n            append_eos=append_eos,\n            already_numberized=already_numberized,\n        )\n    dataset.save(output_path)\n    return output_path\n\n\ndef make_multiling_corpus_configs(\n    language_ids,\n    text_files,\n    dictionaries,\n    char_dictionaries=None,\n    oversampling_rates=None,\n):\n    if not oversampling_rates:\n        oversampling_rates = [1] * len(language_ids)\n    if char_dictionaries is None:\n        char_dictionaries = [None] * len(language_ids)\n    assert len(language_ids) == len(text_files)\n    assert len(language_ids) == len(dictionaries)\n    assert len(language_ids) == len(oversampling_rates)\n    return [\n        pytorch_translate_data.MultilingualCorpusConfig(\n            dialect_id=None\n            if i is None\n            else i + pytorch_translate_data.MULTILING_DIALECT_ID_OFFSET,\n            data_file=p,\n            dict=d,\n            char_dict=cd,\n            oversampling=o,\n        )\n        for i, p, d, cd, o in zip(\n            language_ids,\n            text_files,\n            dictionaries,\n            char_dictionaries,\n            oversampling_rates,\n        )\n    ]\n\n\ndef binarize_text_file_multilingual(\n    corpus_configs: List[pytorch_translate_data.MultilingualCorpusConfig],\n    output_path: str,\n    append_eos: bool,\n    reverse_order: bool,\n    prepend_language_id: bool,\n    use_char_data: bool = False,\n    embed_bytes: bool = False,\n    already_numberized: bool = False,\n) -> str:\n    output_path = maybe_generate_temp_file_path(output_path)\n    if use_char_data:\n        dataset = char_data.InMemoryNumpyWordCharDataset()\n        dataset.parse_multilingual(\n            corpus_configs,\n            reverse_order=reverse_order,\n            append_eos=append_eos,\n            embed_bytes=embed_bytes,\n            prepend_language_id=prepend_language_id,\n            already_numberized=already_numberized,\n        )\n    else:\n        dataset = pytorch_translate_data.InMemoryIndexedDataset()\n        dataset.parse_multilingual(\n            corpus_configs,\n            append_eos=append_eos,\n            reverse_order=reverse_order,\n            prepend_language_id=prepend_language_id,\n            already_numberized=already_numberized,\n        )\n    dataset.save(output_path)\n    return output_path\n\n\ndef preprocess_corpora(args, dictionary_cls=Dictionary):\n    if pytorch_translate_data.is_latent_variable(args):\n        return\n    if (\n        args.train_source_binary_path is not None\n        and args.train_target_binary_path is not None\n    ):\n        if (\n            isinstance(\n                utils.maybe_parse_collection_argument(args.train_source_binary_path),\n                str,\n            )\n            and isinstance(\n                utils.maybe_parse_collection_argument(args.train_target_binary_path),\n                str,\n            )\n            and not args.fairseq_data_format\n        ):\n            args.train_source_binary_path = maybe_generate_temp_file_path(\n                args.train_source_binary_path\n            )\n            args.train_target_binary_path = maybe_generate_temp_file_path(\n                args.train_target_binary_path\n            )\n    if not args.fairseq_data_format:\n        args.eval_source_binary_path = maybe_generate_temp_file_path(\n            args.eval_source_binary_path\n        )\n        args.eval_target_binary_path = maybe_generate_temp_file_path(\n            args.eval_target_binary_path\n        )\n\n    # Additional text preprocessing options could be added here before\n    # binarizing.\n    if pytorch_translate_data.is_multilingual(args):\n        preprocess_corpora_multilingual(args)\n    elif pytorch_translate_data.is_multilingual_many_to_one(args):\n        preprocess_corpora_multilingual_many_to_one(args, dictionary_cls)\n    else:\n\n        # Vocabs are built before preprocessing because we might need to use\n        # both monolingual and bilingual corpora sources to build the vocab\n        # (in the case of semisupervised training)\n        dictionaries = build_vocabs(args=args, dictionary_cls=dictionary_cls)\n        source_dict = dictionaries[""source_dict""]\n        char_source_dict = dictionaries[""char_source_dict""]\n        target_dict = dictionaries[""target_dict""]\n        char_target_dict = dictionaries[""char_target_dict""]\n\n        if char_target_dict is not None:\n            print(""char_target_dict is not None --> should use it!"")\n\n        preprocess_bilingual_corpora(\n            args=args,\n            source_dict=source_dict,\n            char_source_dict=char_source_dict,\n            target_dict=target_dict,\n            char_target_dict=char_target_dict,\n        )\n        # Binarize additional monolingual corpora for the semisupervised translation\n        # task\n        if (\n            args.task == constants.SEMI_SUPERVISED_TASK\n            or args.task == constants.DENOISING_AUTOENCODER_TASK\n        ):\n            args.train_mono_source_binary_path = maybe_generate_temp_file_path(\n                output_path=getattr(args, ""train_mono_source_binary_path"", None)\n            )\n            args.train_mono_target_binary_path = maybe_generate_temp_file_path(\n                output_path=getattr(args, ""train_mono_target_binary_path"", None)\n            )\n            preprocess_monolingual_corpora(\n                args,\n                source_dict=source_dict,\n                char_source_dict=char_source_dict,\n                target_dict=target_dict,\n                char_target_dict=char_target_dict,\n            )\n\n\ndef preprocess_monolingual_corpora(\n    args: argparse.Namespace,\n    source_dict: Dictionary,\n    char_source_dict: Optional[Dictionary],\n    target_dict: Dictionary,\n    char_target_dict: Optional[Dictionary],\n):\n    """"""\n    Preprocess source and target monolingual datasets\n    Prerequisite: Vocabs are already built (see build_vocabs)\n    """"""\n    embed_bytes = getattr(args, ""embed_bytes"", False)\n    if getattr(args, ""train_mono_source_text_file"", None):\n        args.train_mono_source_binary_path = binarize_text_file(\n            text_file=args.train_mono_source_text_file,\n            dictionary=source_dict,\n            output_path=args.train_mono_source_binary_path,\n            append_eos=args.append_eos_to_source,\n            reverse_order=args.reverse_source,\n            embed_bytes=embed_bytes,\n            char_dictionary=char_source_dict,\n        )\n\n    # For target sentences, we always append EOS tokens, and never reverse\n    # their order.\n    if getattr(args, ""train_mono_target_text_file"", None):\n        args.train_mono_target_binary_path = binarize_text_file(\n            text_file=args.train_mono_target_text_file,\n            dictionary=target_dict,\n            output_path=args.train_mono_target_binary_path,\n            # We always append EOS to the target sentence since we still want\n            # the model to output an indication the sentence has finished, even\n            # if we don\'t append the EOS symbol to the source sentence\n            # (to prevent the model from misaligning UNKs or other words\n            # to the frequently occurring EOS).\n            append_eos=True,\n            # We don\'t reverse the order of the target sentence, since\n            # even if the source sentence is fed to the model backwards,\n            # we still want the model to start outputting from the first word.\n            reverse_order=False,\n            embed_bytes=embed_bytes,\n            char_dictionary=char_target_dict,\n        )\n\n\ndef build_vocabs(\n    args: argparse.Namespace, dictionary_cls=Dictionary\n) -> Dict[str, Dictionary]:\n    """"""\n    Builds vocabs or loads them from existing vocab files. If args.task\n    is pytorch_translate_semi_supervised, we use the monolingual corpora in\n    addition to the parallel corpora for building source and target vocabs.\n    """"""\n    source_files = [args.train_source_text_file]\n    target_files = [args.train_target_text_file]\n\n    if args.task == constants.SEMI_SUPERVISED_TASK and getattr(\n        args, ""add_monolingual_data_for_vocab_building"", None\n    ):\n        if getattr(args, ""train_mono_source_text_file"", None):\n            source_files.append(args.train_mono_source_text_file)\n        if getattr(args, ""train_mono_target_text_file"", None):\n            target_files.append(args.train_mono_target_text_file)\n\n    source_dict = dictionary_cls.build_vocab_file_if_nonexistent(\n        corpus_files=source_files,\n        vocab_file=args.source_vocab_file,\n        max_vocab_size=args.source_max_vocab_size,\n        tokens_with_penalty=None,\n    )\n\n    use_char_source = (args.char_source_vocab_file != """") or getattr(\n        args, ""arch"", """"\n    ) in constants.ARCHS_FOR_CHAR_SOURCE\n\n    embed_bytes = getattr(args, ""embed_bytes"", False)\n    char_source_dict = None\n    if use_char_source:\n        char_source_dict = dictionary_cls.build_vocab_file_if_nonexistent(\n            corpus_files=source_files,\n            vocab_file=args.char_source_vocab_file,\n            max_vocab_size=args.char_source_max_vocab_size,\n            tokens_with_penalty=None,\n            is_char_vocab=True,\n            embed_bytes=embed_bytes,\n        )\n\n    target_dict = dictionary_cls.build_vocab_file_if_nonexistent(\n        corpus_files=target_files,\n        vocab_file=args.target_vocab_file,\n        max_vocab_size=args.target_max_vocab_size,\n        tokens_with_penalty=args.penalized_target_tokens_file,\n    )\n\n    use_char_target = (args.char_target_vocab_file != """") or getattr(\n        args, ""arch"", """"\n    ) in constants.ARCHS_FOR_CHAR_TARGET\n\n    char_target_dict = None\n    if use_char_target:\n        char_target_dict = dictionary_cls.build_vocab_file_if_nonexistent(\n            corpus_files=target_files,\n            vocab_file=args.char_target_vocab_file,\n            max_vocab_size=args.char_target_max_vocab_size,\n            tokens_with_penalty=None,\n            is_char_vocab=True,\n            embed_bytes=embed_bytes,\n        )\n    return {\n        ""source_dict"": source_dict,\n        ""char_source_dict"": char_source_dict,\n        ""target_dict"": target_dict,\n        ""char_target_dict"": char_target_dict,\n    }\n\n\ndef preprocess_bilingual_corpora(\n    args: argparse.Namespace,\n    source_dict: Dictionary,\n    char_source_dict: Optional[Dictionary],\n    target_dict: Dictionary,\n    char_target_dict: Optional[Dictionary],\n):\n    """"""\n    Preprocess source and target parallel datasets\n    Prerequisite: Vocabs are already built (see build_vocabs)\n    """"""\n    embed_bytes = getattr(args, ""embed_bytes"", False)\n    if args.train_source_text_file:\n        args.train_source_binary_path = binarize_text_file(\n            text_file=args.train_source_text_file,\n            dictionary=source_dict,\n            output_path=args.train_source_binary_path,\n            append_eos=args.append_eos_to_source,\n            reverse_order=args.reverse_source,\n            embed_bytes=embed_bytes,\n            char_dictionary=char_source_dict,\n        )\n    if args.eval_source_text_file:\n        args.eval_source_binary_path = binarize_text_file(\n            text_file=args.eval_source_text_file,\n            dictionary=source_dict,\n            output_path=args.eval_source_binary_path,\n            append_eos=args.append_eos_to_source,\n            reverse_order=args.reverse_source,\n            embed_bytes=embed_bytes,\n            char_dictionary=char_source_dict,\n        )\n\n    # For target sentences, we always append EOS tokens, and never reverse\n    # their order.\n    if args.train_target_text_file:\n        args.train_target_binary_path = binarize_text_file(\n            text_file=args.train_target_text_file,\n            dictionary=target_dict,\n            output_path=args.train_target_binary_path,\n            # We always append EOS to the target sentence since we still want\n            # the model to output an indication the sentence has finished, even\n            # if we don\'t append the EOS symbol to the source sentence\n            # (to prevent the model from misaligning UNKs or other words\n            # to the frequently occurring EOS).\n            append_eos=True,\n            # We don\'t reverse the order of the target sentence, since\n            # even if the source sentence is fed to the model backwards,\n            # we still want the model to start outputting from the first word.\n            reverse_order=False,\n            embed_bytes=embed_bytes,\n            char_dictionary=char_target_dict,\n        )\n    if args.eval_target_text_file:\n        args.eval_target_binary_path = binarize_text_file(\n            text_file=args.eval_target_text_file,\n            dictionary=target_dict,\n            output_path=args.eval_target_binary_path,\n            append_eos=True,\n            reverse_order=False,\n            embed_bytes=embed_bytes,\n            char_dictionary=char_target_dict,\n        )\n\n\ndef build_vocab_multicorpus(\n    corpus_langs,\n    corpus_files,\n    vocab_langs,\n    vocab_files,\n    max_vocab_size,\n    tokens_with_penalty=None,\n    dictionary_cls=Dictionary,\n):\n    lang2corpus = {lang: [] for lang in vocab_langs}\n    for lang, corpus_file in zip(corpus_langs, corpus_files):\n        lang2corpus[lang].append(corpus_file)\n    return {\n        lang: dictionary_cls.build_vocab_file_if_nonexistent(\n            corpus_files=lang2corpus[lang],\n            vocab_file=vocab_file,\n            max_vocab_size=max_vocab_size,\n            tokens_with_penalty=tokens_with_penalty,\n        )\n        for lang, vocab_file in zip(vocab_langs, vocab_files)\n    }\n\n\ndef preprocess_corpora_multilingual(args):\n    source_langs = multilingual_utils.get_source_langs(args.lang_pairs.split("",""))\n    target_langs = multilingual_utils.get_target_langs(args.lang_pairs.split("",""))\n\n    dict_paths, dict_objects = multilingual_utils.prepare_dicts(\n        args, list(set(source_langs + target_langs))\n    )\n    train_binary_path_config = []\n    eval_binary_path_config = []\n    for lang_pair in args.lang_pairs.split("",""):\n        source_lang, target_lang = lang_pair.split(""-"")\n        (\n            source_corpus,\n            target_corpus,\n        ) = multilingual_utils.get_parallel_corpus_for_lang_pair(\n            args.multilingual_train_text_file, lang_pair\n        )\n        source_binary_path = maybe_generate_temp_file_path(\n            multilingual_utils.default_binary_path(\n                args.save_dir, lang_pair, source_lang, ""train""\n            )\n        )\n        binarize_text_file(\n            text_file=source_corpus,\n            dictionary=dict_objects[source_lang],\n            output_path=source_binary_path,\n            append_eos=args.append_eos_to_source,\n            reverse_order=args.reverse_source,\n        )\n        target_binary_path = maybe_generate_temp_file_path(\n            multilingual_utils.default_binary_path(\n                args.save_dir, lang_pair, target_lang, ""train""\n            )\n        )\n        binarize_text_file(\n            text_file=target_corpus,\n            dictionary=dict_objects[target_lang],\n            output_path=target_binary_path,\n            append_eos=True,\n            reverse_order=False,\n        )\n        train_binary_path_config.append(\n            f""{lang_pair}:{source_binary_path},{target_binary_path}""\n        )\n        (\n            source_corpus,\n            target_corpus,\n        ) = multilingual_utils.get_parallel_corpus_for_lang_pair(\n            args.multilingual_eval_text_file, lang_pair\n        )\n        source_binary_path = maybe_generate_temp_file_path(\n            multilingual_utils.default_binary_path(\n                args.save_dir, lang_pair, source_lang, ""eval""\n            )\n        )\n        binarize_text_file(\n            text_file=source_corpus,\n            dictionary=dict_objects[source_lang],\n            output_path=source_binary_path,\n            append_eos=args.append_eos_to_source,\n            reverse_order=args.reverse_source,\n        )\n        target_binary_path = maybe_generate_temp_file_path(\n            multilingual_utils.default_binary_path(\n                args.save_dir, lang_pair, target_lang, ""eval""\n            )\n        )\n        binarize_text_file(\n            text_file=target_corpus,\n            dictionary=dict_objects[target_lang],\n            output_path=target_binary_path,\n            append_eos=True,\n            reverse_order=False,\n        )\n        eval_binary_path_config.append(\n            f""{lang_pair}:{source_binary_path},{target_binary_path}""\n        )\n    args.vocabulary = [f""{lang}:{dict_paths[lang]}"" for lang in dict_paths]\n    args.multilingual_train_binary_path = train_binary_path_config\n    args.multilingual_eval_binary_path = eval_binary_path_config\n\n\ndef preprocess_corpora_multilingual_many_to_one(args, dictionary_cls=Dictionary):\n    source_dicts = build_vocab_multicorpus(\n        args.multiling_source_lang,\n        args.multiling_train_source_text_file,\n        args.multiling_encoder_lang,\n        args.multiling_source_vocab_file,\n        args.source_max_vocab_size,\n        dictionary_cls,\n    )\n    source_corpus_lang_ids = [\n        args.multiling_encoder_lang.index(l) for l in args.multiling_source_lang\n    ]\n    source_corpus_dicts = [source_dicts[l] for l in args.multiling_source_lang]\n    binarize_text_file_multilingual(\n        corpus_configs=make_multiling_corpus_configs(\n            source_corpus_lang_ids,\n            args.multiling_train_source_text_file,\n            source_corpus_dicts,\n            args.multiling_train_oversampling,\n        ),\n        output_path=args.train_source_binary_path,\n        append_eos=args.append_eos_to_source,\n        reverse_order=args.reverse_source,\n        prepend_language_id=False,\n    )\n    binarize_text_file_multilingual(\n        corpus_configs=make_multiling_corpus_configs(\n            source_corpus_lang_ids,\n            args.multiling_eval_source_text_file,\n            source_corpus_dicts,\n            args.multiling_train_oversampling,\n        ),\n        output_path=args.eval_source_binary_path,\n        append_eos=args.append_eos_to_source,\n        reverse_order=args.reverse_source,\n        prepend_language_id=False,\n    )\n\n    target_dicts = build_vocab_multicorpus(\n        args.multiling_target_lang,\n        args.multiling_train_target_text_file,\n        args.multiling_decoder_lang,\n        args.multiling_target_vocab_file,\n        args.target_max_vocab_size,\n        args.penalized_target_tokens_file,\n        dictionary_cls,\n    )\n    target_corpus_lang_ids = [\n        args.multiling_decoder_lang.index(l) for l in args.multiling_target_lang\n    ]\n    target_corpus_dicts = [target_dicts[l] for l in args.multiling_target_lang]\n    binarize_text_file_multilingual(\n        corpus_configs=make_multiling_corpus_configs(\n            target_corpus_lang_ids,\n            args.multiling_train_target_text_file,\n            target_corpus_dicts,\n            args.multiling_train_oversampling,\n        ),\n        output_path=args.train_target_binary_path,\n        append_eos=True,\n        reverse_order=False,\n        prepend_language_id=True,\n    )\n    binarize_text_file_multilingual(\n        corpus_configs=make_multiling_corpus_configs(\n            target_corpus_lang_ids,\n            args.multiling_eval_target_text_file,\n            target_corpus_dicts,\n            args.multiling_train_oversampling,\n        ),\n        output_path=args.eval_target_binary_path,\n        append_eos=True,\n        reverse_order=False,\n        prepend_language_id=True,\n    )\n\n\ndef preprocess_corpora_latent_variable(args):\n    pass\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""PyTorch Translate - preprocessing"")\n    pytorch_translate_options.add_verbosity_args(parser)\n    pytorch_translate_options.add_preprocessing_args(parser)\n    args = parser.parse_args()\n    pytorch_translate_options.validate_preprocessing_args(args)\n    pytorch_translate_options.print_args(args)\n    preprocess_corpora(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_translate/rnn.py,17,"b'#!/usr/bin/env python3\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.onnx.operators\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import AdaptiveSoftmax\nfrom pytorch_translate import rnn_cell  # noqa\nfrom pytorch_translate import (\n    attention,\n    utils as pytorch_translate_utils,\n    vocab_reduction,\n)\nfrom pytorch_translate.common_layers import (\n    ContextEmbedding,\n    DecoderWithOutputProjection,\n    Embedding,\n    Linear,\n    RNNLayer,\n    VariableTracker,\n)\nfrom pytorch_translate.data import data as pytorch_translate_data\nfrom pytorch_translate.multi_model import MultiDecoder, MultiEncoder\nfrom pytorch_translate.multilingual import MultilingualDecoder, MultilingualEncoder\nfrom pytorch_translate.ngram import NGramDecoder\nfrom pytorch_translate.semi_supervised import SemiSupervisedModel\nfrom pytorch_translate.utils import maybe_cat, maybe_cuda, torch_find\nfrom torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence\n\n\ndef reorder_encoder_output(encoder_out, new_order):\n    """"""Reorder all outputs according to new_order.""""""\n    (\n        unpacked_output,\n        final_hiddens,\n        final_cells,\n        src_lengths,\n        src_tokens,\n        src_embeddings,\n    ) = encoder_out\n    unpacked_output = unpacked_output.index_select(1, new_order)\n    final_hiddens = final_hiddens.index_select(1, new_order)\n    final_cells = final_cells.index_select(1, new_order)\n    src_lengths = src_lengths.index_select(0, new_order)\n    src_tokens = pytorch_translate_utils.get_source_tokens_tensor(\n        src_tokens\n    ).index_select(0, new_order)\n    src_embeddings = src_embeddings.index_select(1, new_order)\n    return (\n        unpacked_output,\n        final_hiddens,\n        final_cells,\n        src_lengths,\n        src_tokens,\n        src_embeddings,\n    )\n\n\nclass DummyPyTextRNNPointerModel:\n    # We want to ship a PyText Seq2Seq model we\'re calling RNNPointerModel.\n    # We also need to be able to export this model in PyText which relies\n    # on logic in ensemble_export. Problem is the logic in ensemble_export\n    # is model dependent, so if our RNNPointer implementation is in\n    # PyText we need to somehow forward define it in the translate library.\n    # This is the purpose of this class. In PyText our implementation will\n    # inherit from this class and therefore we can have model specific logic\n    # of a PyText model in translate, without having to directly import a class\n    # from PyText.\n    pass\n\n\n@register_model(""rnn"")\nclass RNNModel(FairseqEncoderDecoderModel):\n    def __init__(self, task, encoder, decoder):\n        super().__init__(encoder, decoder)\n        self.task = task\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument(\n            ""--language-model-only"",\n            default=False,\n            action=""store_true"",\n            help=""whether to train a language model only where no encoder is used"",\n        )\n        parser.add_argument(\n            ""--dropout"",\n            default=0.1,\n            type=float,\n            metavar=""D"",\n            help=""dropout probability"",\n        )\n        parser.add_argument(\n            ""--encoder-embed-dim"",\n            default=0,\n            type=int,\n            metavar=""N"",\n            help=""encoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--encoder-pretrained-embed"",\n            default=None,\n            metavar=""FILE"",\n            help=""path to pre-trained encoder embedding"",\n        )\n        parser.add_argument(\n            ""--encoder-freeze-embed"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether to freeze the encoder embedding or allow it to be ""\n                ""updated during training""\n            ),\n        )\n        parser.add_argument(\n            ""--encoder-normalize-embed"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether to normalize the encoder embeddings to have zero mean ""\n                ""and unit variance (weighted by token frequency)""\n            ),\n        )\n        parser.add_argument(\n            ""--encoder-hidden-dim"", type=int, metavar=""N"", help=""encoder cell num units""\n        )\n        parser.add_argument(\n            ""--encoder-layers"", type=int, metavar=""N"", help=""number of encoder layers""\n        )\n        parser.add_argument(\n            ""--encoder-bidirectional"",\n            action=""store_true"",\n            help=""whether the first layer is bidirectional or not"",\n        )\n        parser.add_argument(\n            ""--averaging-encoder"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether use mean encoder hidden states as decoder initial ""\n                ""states or not""\n            ),\n        )\n        parser.add_argument(\n            ""--decoder-embed-dim"",\n            default=0,\n            type=int,\n            metavar=""N"",\n            help=""decoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--decoder-pretrained-embed"",\n            default=None,\n            metavar=""FILE"",\n            help=""path to pre-trained decoder embedding"",\n        )\n        parser.add_argument(\n            ""--decoder-freeze-embed"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether to freeze the decoder embedding or allow it to be ""\n                ""updated during training""\n            ),\n        )\n        parser.add_argument(\n            ""--decoder-hidden-dim"", type=int, metavar=""N"", help=""decoder cell num units""\n        )\n        parser.add_argument(\n            ""--decoder-layers"", type=int, metavar=""N"", help=""number of decoder layers""\n        )\n        parser.add_argument(\n            ""--decoder-out-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""decoder output embedding dimension"",\n        )\n        parser.add_argument(\n            ""--decoder-out-pretrained-embed"",\n            default=None,\n            metavar=""FILE"",\n            help=""path to pre-trained decoder output embedding"",\n        )\n        parser.add_argument(\n            ""--out-embed-norm"",\n            default=None,\n            type=float,\n            help=""norm for output projection weights"",\n        )\n        parser.add_argument(\n            ""--decoder-tie-embeddings"",\n            default=False,\n            action=""store_true"",\n            help=""tie the decoder word embeddings with the output projection ""\n            ""weights (requires that the embedding dims be of the same size)"",\n        )\n        parser.add_argument(\n            ""--attention-type"",\n            type=str,\n            metavar=""EXPR"",\n            help=""decoder attention, defaults to dot"",\n        )\n        parser.add_argument(\n            ""--attention-heads"",\n            default=8,\n            type=int,\n            metavar=""N"",\n            help=""number of encoder-decoder attention heads, used when attention""\n            "" type is multihead. Ignored unless attention type is multihead."",\n        )\n        parser.add_argument(\n            ""--first-layer-attention"",\n            default=False,\n            action=""store_true"",\n            help=""calculates attention after decoder\'s first RNN layer and""\n            ""concatenates it in input of every subsequent layer"",\n        )\n        parser.add_argument(\n            ""--residual-level"",\n            default=None,\n            type=int,\n            help=(\n                ""First layer where to apply a residual connection. ""\n                ""The value should be greater than 0 and smaller than the number of ""\n                ""layers.""\n            ),\n        )\n        parser.add_argument(\n            ""--cell-type"",\n            default=""lstm"",\n            type=str,\n            metavar=""EXPR"",\n            help=""cell type, defaults to lstm, values:lstm, milstm, layer_norm_lstm"",\n        )\n\n        # Granular dropout settings (if not specified these default to --dropout)\n        parser.add_argument(\n            ""--encoder-dropout-in"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability for encoder input embedding"",\n        )\n        parser.add_argument(\n            ""--encoder-dropout-out"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability for encoder output"",\n        )\n        parser.add_argument(\n            ""--decoder-dropout-in"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability for decoder input embedding"",\n        )\n        parser.add_argument(\n            ""--decoder-dropout-out"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability for decoder output"",\n        )\n        parser.add_argument(\n            ""--sequence-lstm"",\n            action=""store_true"",\n            help=""use nn.LSTM implementation for encoder"",\n        )\n        parser.add_argument(\n            ""--ngram-decoder"",\n            default=None,\n            type=int,\n            nargs=""+"",\n            help=(\n                ""A single integer, or a list of integers. If ""\n                ""positive, the decoder is not recurrent but a feedforward ""\n                ""network with target-side n-gram history as input. The decoder ""\n                ""is still conditioned on the source side via attention. If ""\n                ""this parameter is a list of integers, the n-th entry applies ""\n                ""to the n-th decoder (for multilingual models and ""\n                ""multi-decoders)""\n            ),\n        )\n        parser.add_argument(\n            ""--ngram-activation-type"",\n            default=""relu"",\n            type=str,\n            metavar=""EXPR"",\n            help=(\n                ""Activation in FF layers of the ngram decoder, defaults to ""\n                ""relu, values: relu, tanh""\n            ),\n        )\n        parser.add_argument(\n            ""--multi-encoder"",\n            default=None,\n            type=int,\n            help=(\n                ""If this is positive, train n encoder networks rather than ""\n                ""only one. The outputs of the encoders are concatenated before ""\n                ""passing them through to the decoder.""\n            ),\n        )\n        parser.add_argument(\n            ""--multi-decoder"",\n            default=None,\n            type=int,\n            help=(\n                ""If this is positive, train n decoder networks rather than ""\n                ""only one. The predictions are combined via the method in ""\n                ""--multi-decoder-combination-strategy.""\n            ),\n        )\n        parser.add_argument(\n            ""--multi-decoder-combination-strategy"",\n            default=""bottleneck"",\n            type=str,\n            metavar=""EXPR"",\n            help=(\n                ""Only used if --multi-decoder is positive. Controls how the ""\n                ""decoders are combined with each other.\\n""\n                ""- uniform: Separate projection layers, average predictions\\n""\n                ""- uniform-probspace: Separate projection layers, average ""\n                ""in probability space.\\n""\n                ""- uniform-logprobspace: Separate projection layers, average ""\n                ""in log-probability space.\\n""\n                ""- unprojected: Shared projection layer, unprojected ""\n                ""decoder outputs are averaged.\\n""\n                ""- deepfusion: cf. https://arxiv.org/pdf/1503.03535.pdf \\n""\n                ""- coldfusion: cf. https://arxiv.org/pdf/1708.06426.pdf \\n""\n                ""- weighted: Separate projection layers, weighted average ""\n                ""of logits. Weights are learned from unprojected decoder ""\n                ""outputs.\\n""\n                ""- weighted-probspace: Like \'weighted\', but average in ""\n                ""probability space.\\n""\n                ""- weighted-logprobspace: Like \'weighted\', but average in ""\n                ""log-probability space.\\n""\n                ""- weighted-unprojected: Shared projection layer, weighted ""\n                ""average of decoder outputs. Weights are learned from ""\n                ""unprojected decoder outputs.\\n""\n                ""- concat: Shared projection layer, decoder outputs are ""\n                ""concatenated.\\n""\n                ""- bottleneck: Like \'concat\' but with an additional ""\n                ""bottleneck layer to reduce the size of the output embedding ""\n                ""matrix.\\n""\n                ""- deep_bottleneck: Like \'bottleneck\' but with an additional ""\n                ""non-linear layer.\\n""\n                ""- multiplicative-unprojected: Shared projection layer, element""\n                ""-wise product of decoder outputs after ReLU.\\n""\n                ""- max-unprojected: Shared projection layer, element""\n                ""-wise max of decoder outputs.\\n""\n            ),\n        )\n        parser.add_argument(\n            ""--multi-model-fixed-weights"",\n            default=None,\n            type=float,\n            nargs=""+"",\n            help=(\n                ""Used for weighted* combination strategies. If specified, use ""\n                ""these fixed model weights rather than a gating network.""\n            ),\n        )\n        parser.add_argument(\n            ""--multi-model-training-schedule"",\n            default=""complete"",\n            type=str,\n            metavar=""EXPR"",\n            help=(\n                ""Only used if --multi-decoder is positive.\\n""\n                ""- \'complete\': Jointly train entire network on all batches.\\n""\n                ""- \'unfreeze_single\': Freeze all submodels except one for each ""\n                ""training batch.\\n""\n                ""- \'unfreeze_single_encoder\': Freeze all encoders except one ""\n                ""for each training batch.\\n""\n                ""- \'unfreeze_single_decoder\': Freeze all decoders except one ""\n                ""for each training batch.\\n""\n                ""- \'unfreeze_enc_N\': Freeze N-th encoder.\\n""\n                ""- \'unfreeze_dec_N\': Freeze N-th decoder.\\n""\n                ""- \'unfreeze_encdec_N\': Freeze N-th encoder and N-th decoder.\\n""\n                ""- \'freeze_all\': Freeze all submodels, only train combination ""\n                ""strategy.\\n""\n                ""- \'freeze_all_encoders\': Freeze all encoders.\\n""\n                ""- \'freeze_all_decoders\': Freeze all decoders.\\n""\n                ""- \'separate\': Each training batch is used for only one of the ""\n                ""following: Train the n-th submodel, or train combination ""\n                ""strategy.""\n            ),\n        )\n        parser.add_argument(\n            ""--multi-decoder-is-lm"",\n            default=None,\n            type=int,\n            nargs=""+"",\n            help=(\n                ""If specified, sets --attention-type=no and --encoder-hidden-dim=0""\n                ""for the n-th decoder in an adaptive ensemble.""\n            ),\n        )\n        parser.add_argument(\n            ""--att-weighted-source-embeds"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether use attention weighted src embeddings to improve rare ""\n                ""words translation or not""\n            ),\n        )\n        parser.add_argument(\n            ""--encoder-context-embed"",\n            default=False,\n            help=(\n                ""whether to use context-dependent source embeddings in the encoder ""\n                ""for word disambiguation""\n            ),\n            action=""store_true"",\n        )\n        parser.add_argument(\n            ""--att-weighted-activation-type"",\n            default=""tanh"",\n            type=str,\n            metavar=""EXPR"",\n            help=(\n                ""Activation in FF layers of the attention weighted src embeddings, ""\n                ""defaults to relu, values: relu, tanh""\n            ),\n        )\n        parser.add_argument(\n            ""--adaptive-softmax-cutoff"",\n            metavar=""EXPR"",\n            help=""comma separated list of adaptive softmax cutoff points. ""\n            ""Must be used with adaptive_loss criterion"",\n        )\n\n        # Args for vocab reduction\n        vocab_reduction.add_args(parser)\n\n    @staticmethod\n    def build_single_encoder(args, src_dict, embed_tokens):\n        if args.language_model_only:\n            return DummyEncoder(src_dict, num_layers=args.encoder_layers)\n        if args.sequence_lstm:\n            encoder_class = LSTMSequenceEncoder\n        else:\n            encoder_class = RNNEncoder\n        return encoder_class(\n            src_dict,\n            embed_tokens=embed_tokens,\n            embed_dim=args.encoder_embed_dim,\n            cell_type=args.cell_type,\n            num_layers=args.encoder_layers,\n            hidden_dim=args.encoder_hidden_dim,\n            dropout_in=args.encoder_dropout_in,\n            dropout_out=args.encoder_dropout_out,\n            residual_level=args.residual_level,\n            bidirectional=bool(args.encoder_bidirectional),\n            left_pad=args.left_pad_source,\n            encoder_context_embed=args.encoder_context_embed,\n        )\n\n    @staticmethod\n    def build_single_decoder(\n        args,\n        src_dict,\n        dst_dict,\n        embed_tokens,\n        ngram_decoder=None,\n        project_output=True,\n        is_lm=False,\n    ):\n        if args.adaptive_softmax_cutoff is not None:\n            project_output = False\n        attention_type = args.attention_type\n        encoder_hidden_dim = args.encoder_hidden_dim\n        if is_lm:\n            attention_type = ""no""\n            encoder_hidden_dim = 0\n        if ngram_decoder:\n            if args.ngram_activation_type == ""relu"":\n                activation_fn = nn.ReLU\n            elif args.ngram_activation_type == ""tanh"":\n                activation_fn = nn.Tanh\n            else:\n                raise Exception(\n                    ""ngram_activation_type \'%s\' not implemented""\n                    % args.ngram_activation_type\n                )\n            decoder = NGramDecoder(\n                src_dict=src_dict,\n                dst_dict=dst_dict,\n                n=ngram_decoder,\n                encoder_hidden_dim=encoder_hidden_dim,\n                embed_dim=args.decoder_embed_dim,\n                freeze_embed=args.decoder_freeze_embed,\n                out_embed_dim=args.decoder_out_embed_dim,\n                num_layers=args.decoder_layers,\n                hidden_dim=args.decoder_hidden_dim,\n                attention_type=attention_type,\n                dropout_in=args.decoder_dropout_in,\n                dropout_out=args.decoder_dropout_out,\n                residual_level=args.residual_level,\n                activation_fn=activation_fn,\n                project_output=project_output,\n                pretrained_embed=args.decoder_pretrained_embed,\n                projection_pretrained_embed=args.decoder_out_pretrained_embed,\n            )\n        else:\n            decoder = RNNDecoder(\n                src_dict=src_dict,\n                dst_dict=dst_dict,\n                embed_tokens=embed_tokens,\n                vocab_reduction_params=args.vocab_reduction_params,\n                encoder_hidden_dim=encoder_hidden_dim,\n                embed_dim=args.decoder_embed_dim,\n                out_embed_dim=args.decoder_out_embed_dim,\n                cell_type=args.cell_type,\n                num_layers=args.decoder_layers,\n                hidden_dim=args.decoder_hidden_dim,\n                attention_type=attention_type,\n                attention_heads=args.attention_heads,\n                first_layer_attention=bool(args.first_layer_attention),\n                dropout_in=args.decoder_dropout_in,\n                dropout_out=args.decoder_dropout_out,\n                residual_level=args.residual_level,\n                averaging_encoder=args.averaging_encoder,\n                project_output=project_output,\n                pretrained_embed=args.decoder_pretrained_embed,\n                projection_pretrained_embed=args.decoder_out_pretrained_embed,\n                out_embed_norm=args.out_embed_norm,\n                tie_embeddings=args.decoder_tie_embeddings,\n                att_weighted_src_embeds=args.att_weighted_src_embeds,\n                src_embed_dim=args.encoder_embed_dim,\n                att_weighted_activation_type=args.att_weighted_activation_type,\n                fp16=args.fp16,\n            )\n\n        # Being able to use adaptive softmax for RNN decoder\n        decoder.adaptive_softmax = None\n\n        if args.adaptive_softmax_cutoff is not None:\n            decoder.adaptive_softmax = AdaptiveSoftmax(\n                len(dst_dict),\n                args.decoder_out_embed_dim or args.decoder_hidden_dim,\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.dropout,\n            )\n        return decoder\n\n    @classmethod\n    def build_embed_tokens(cls, args, src_dict, dst_dict):\n        """"""Builds encoder and decoder token embeddings. If pretrained embeddings\n        are specified, load them.""""""\n        encoder_embed_tokens = Embedding(\n            num_embeddings=len(src_dict),\n            embedding_dim=args.encoder_embed_dim,\n            padding_idx=src_dict.pad(),\n            freeze_embed=args.encoder_freeze_embed,\n            normalize_embed=args.encoder_normalize_embed,\n        )\n        pytorch_translate_utils.load_embedding(\n            embedding=encoder_embed_tokens,\n            dictionary=src_dict,\n            pretrained_embed=args.encoder_pretrained_embed,\n        )\n        decoder_embed_tokens = Embedding(\n            num_embeddings=len(dst_dict),\n            embedding_dim=args.decoder_embed_dim,\n            padding_idx=dst_dict.pad(),\n            freeze_embed=args.decoder_freeze_embed,\n        )\n\n        pytorch_translate_utils.load_embedding(\n            embedding=decoder_embed_tokens,\n            dictionary=dst_dict,\n            pretrained_embed=args.decoder_pretrained_embed,\n        )\n        return encoder_embed_tokens, decoder_embed_tokens\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        """"""Build a new (multi-)encoder instance.""""""\n        if args.multi_encoder is not None:\n            encoders = [\n                RNNModel.build_single_encoder(args, src_dict, embed_tokens=embed_tokens)\n                for _ in range(args.multi_encoder)\n            ]\n            encoder = MultiEncoder(\n                src_dict, encoders, training_schedule=args.multi_model_training_schedule\n            )\n        else:\n            encoder = RNNModel.build_single_encoder(\n                args, src_dict, embed_tokens=embed_tokens\n            )\n        return encoder\n\n    @classmethod\n    def build_decoder(cls, args, src_dict, dst_dict, embed_tokens):\n        """"""Build a new (multi-)decoder instance.""""""\n        if args.multi_decoder is not None:\n            ngram_decoder_args = [None] * args.multi_decoder\n            if args.ngram_decoder is not None:\n                ngram_decoder_args = args.ngram_decoder\n                if len(ngram_decoder_args) == 1:\n                    ngram_decoder_args = [ngram_decoder_args[0]] * args.multi_decoder\n                assert len(ngram_decoder_args) == args.multi_decoder\n            is_lm_args = [False] * args.multi_decoder\n            if args.multi_decoder_is_lm is not None:\n                is_lm_args = list(map(bool, args.multi_decoder_is_lm))\n            assert len(is_lm_args) == args.multi_decoder\n            decoders = [\n                RNNModel.build_single_decoder(\n                    args,\n                    src_dict,\n                    dst_dict,\n                    embed_tokens=embed_tokens,\n                    ngram_decoder=n,\n                    project_output=False,\n                    is_lm=is_lm,\n                )\n                for is_lm, n in zip(is_lm_args, ngram_decoder_args)\n            ]\n            decoder = MultiDecoder(\n                src_dict,\n                dst_dict,\n                decoders=decoders,\n                combination_strategy=args.multi_decoder_combination_strategy,\n                is_lm=is_lm_args,\n                split_encoder=args.multi_encoder is not None,\n                vocab_reduction_params=args.vocab_reduction_params,\n                training_schedule=args.multi_model_training_schedule,\n                fixed_weights=args.multi_model_fixed_weights,\n            )\n        else:\n            if args.multi_encoder:\n                args.encoder_hidden_dim *= args.multi_encoder\n            n = args.ngram_decoder[0] if args.ngram_decoder else None\n            decoder = RNNModel.build_single_decoder(\n                args, src_dict, dst_dict, embed_tokens=embed_tokens, ngram_decoder=n\n            )\n        return decoder\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        base_architecture(args)\n        # set default value for old checkpoints\n        args.left_pad_source = getattr(args, ""left_pad_source"", False)\n        if pytorch_translate_data.is_multilingual_many_to_one(args):\n            return RNNModel.build_model_multilingual(args, task)\n        src_dict, dst_dict = task.source_dictionary, task.target_dictionary\n\n        encoder_embed_tokens, decoder_embed_tokens = RNNModel.build_embed_tokens(\n            args, src_dict, dst_dict\n        )\n        encoder = RNNModel.build_encoder(\n            args, src_dict, embed_tokens=encoder_embed_tokens\n        )\n        decoder = RNNModel.build_decoder(\n            args, src_dict, dst_dict, embed_tokens=decoder_embed_tokens\n        )\n        return cls(task, encoder, decoder)\n\n    @classmethod\n    def build_model_multilingual(cls, args, task):\n        """"""Build a new multilingual model instance.""""""\n        encoders = []\n        for lang in args.multiling_encoder_lang:\n            d = task.source_dictionaries.get(lang, None)\n            if d is not None:\n                encoder_embed_tokens = Embedding(\n                    num_embeddings=len(d),\n                    embedding_dim=args.encoder_embed_dim,\n                    padding_idx=d.pad(),\n                    freeze_embed=args.encoder_freeze_embed,\n                    normalize_embed=args.encoder_normalize_embed,\n                )\n                pytorch_translate_utils.load_embedding(\n                    embedding=encoder_embed_tokens,\n                    dictionary=d,\n                    pretrained_embed=args.encoder_pretrained_embed,\n                )\n                encoders.append(\n                    RNNModel.build_encoder(args, d, embed_tokens=encoder_embed_tokens)\n                )\n            else:\n                encoders.append(None)\n        encoder = MultilingualEncoder(\n            task.source_dictionary,\n            encoders,\n            hidden_dim=args.encoder_hidden_dim,\n            num_layers=args.encoder_layers,\n            embed_dim=args.encoder_embed_dim,\n            rescale_grads=args.multiling_rescale_grads,\n        )\n        decoders = []\n        for lang in args.multiling_decoder_lang:\n            d = task.target_dictionaries.get(lang, None)\n            if d is not None:\n                decoder_embed_tokens = Embedding(\n                    num_embeddings=len(d),\n                    embedding_dim=args.decoder_embed_dim,\n                    padding_idx=d.pad(),\n                    freeze_embed=args.decoder_freeze_embed,\n                )\n\n                pytorch_translate_utils.load_embedding(\n                    embedding=decoder_embed_tokens,\n                    dictionary=d,\n                    pretrained_embed=args.decoder_pretrained_embed,\n                )\n                decoders.append(\n                    RNNModel.build_decoder(\n                        args, None, d, embed_tokens=decoder_embed_tokens\n                    )\n                )\n            else:\n                decoders.append(None)\n        decoder = MultilingualDecoder(\n            task.target_dictionary,\n            decoders,\n            hidden_dim=args.encoder_hidden_dim,\n            rescale_grads=args.multiling_rescale_grads,\n        )\n        return cls(task, encoder, decoder)\n\n    def get_targets(self, sample, net_output):\n        targets = sample[""target""].view(-1)\n        possible_translation_tokens = net_output[-1]\n        if possible_translation_tokens is not None:\n            # pad_mask is used to ensure padding IDs remain the same regardless\n            # of where torch_find() finds it in possible_translation_tokens.\n            # This is important since FairseqCriterion has special logic to\n            # ignore padding IDs when calculating loss.\n            pad_mask = targets == self.decoder.padding_idx\n            targets = torch_find(\n                possible_translation_tokens, targets, len(self.task.target_dictionary)\n            )\n            targets[pad_mask] = self.decoder.padding_idx\n        return targets\n\n\nclass LSTMSequenceEncoder(FairseqEncoder):\n    """"""RNN encoder using nn.LSTM for cuDNN support / ONNX exportability.""""""\n\n    @staticmethod\n    def LSTM(input_size, hidden_size, **kwargs):\n        m = nn.LSTM(input_size, hidden_size, **kwargs)\n        for name, param in m.named_parameters():\n            if ""weight"" in name or ""bias"" in name:\n                param.data.uniform_(-0.1, 0.1)\n        return m\n\n    def __init__(\n        self,\n        dictionary,\n        embed_tokens,\n        embed_dim=512,\n        cell_type=""lstm"",\n        hidden_dim=512,\n        num_layers=1,\n        dropout_in=0.1,\n        dropout_out=0.1,\n        residual_level=None,\n        bidirectional=False,\n        left_pad=True,\n        encoder_context_embed=False,\n    ):\n        assert cell_type == ""lstm"", \'sequence-lstm requires cell_type=""lstm""\'\n\n        super().__init__(dictionary)\n        self.dictionary = dictionary\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.residual_level = residual_level\n        self.hidden_dim = hidden_dim\n        self.bidirectional = bidirectional\n        self.padding_idx = dictionary.pad()\n        self.left_pad = left_pad\n        self.embed_tokens = embed_tokens\n\n        if encoder_context_embed:\n            self.embed_tokens_context = ContextEmbedding(embed_dim=embed_dim)\n        self.encoder_context_embed = encoder_context_embed\n\n        self.word_dim = embed_dim\n\n        self.bilstm = BiLSTM(\n            num_layers=num_layers,\n            bidirectional=bidirectional,\n            embed_dim=embed_dim,\n            hidden_dim=hidden_dim,\n            dropout=dropout_out,\n            residual_level=residual_level,\n        )\n\n        # Variable tracker\n        self.tracker = VariableTracker()\n\n        # Initialize adversarial mode\n        self.set_gradient_tracking_mode(False)\n        self.set_embed_noising_mode(False)\n\n    def forward(self, src_tokens, src_lengths):\n        if self.left_pad:\n            # convert left-padding to right-padding\n            src_tokens = utils.convert_padding_direction(\n                src_tokens, self.padding_idx, left_to_right=True\n            )\n\n        # If we\'re generating adversarial examples we need to keep track of\n        # some internal variables\n        self.tracker.reset()\n\n        bsz, seqlen = pytorch_translate_utils.get_source_tokens_tensor(\n            src_tokens\n        ).size()\n\n        # embed tokens\n        x = self.embed_tokens(src_tokens)\n        if self.encoder_context_embed:\n            x = self.embed_tokens_context(x)\n\n        # Apply feature level noising is specified\n        if self.embed_noising_mode and self.embed_noising_layer is not None:\n            x = self.embed_noising_layer(x)\n        # Track token embeddings for generation white-box adversarial example\n        self.tracker.track(x, ""token_embeddings"", retain_grad=self.track_gradients)\n\n        x = F.dropout(x, p=self.dropout_in, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        embedded_words = x\n\n        # Allows compatibility with Caffe2 inputs for tracing (int32)\n        # as well as the current format of Fairseq-Py inputs (int64)\n        if src_lengths.dtype is torch.int64:\n            src_lengths = src_lengths.int()\n\n        unpacked_output, final_hiddens, final_cells = self.bilstm(\n            embeddings=x, lengths=src_lengths\n        )\n\n        return (\n            unpacked_output,\n            final_hiddens,\n            final_cells,\n            src_lengths,\n            src_tokens,\n            embedded_words,\n        )\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""Reorder all outputs according to new_order.""""""\n        return reorder_encoder_output(encoder_out, new_order)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n    def set_gradient_tracking_mode(self, mode=True):\n        self.tracker.reset()\n        self.track_gradients = mode\n\n    def set_embed_noising_mode(self, mode=True):\n        """"""This allows adversarial trainer to turn on and off embedding noising\n        layers. In regular training, this mode is off, and it is not included\n        in forward pass.\n        """"""\n        self.embed_noising_mode = mode\n\n\nclass DummyEncoder(FairseqEncoder):\n    """"""Dummy encoder which outputs None. Used for LM training.""""""\n\n    def __init__(self, dictionary, num_layers=1):\n        super().__init__(dictionary)\n        self.num_layers = num_layers\n\n    def forward(self, src_tokens, src_lengths):\n        bsz = src_lengths.size(0)\n        ones = maybe_cuda(torch.ones((self.num_layers, bsz, 1)))\n        dummy_out = maybe_cuda(torch.ones((1, bsz, 1)))\n\n        # ones are returned so that FC layer corresponds to learned initial\n        # state for language model\n        return dummy_out, ones, ones, src_lengths, src_tokens, dummy_out\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        return reorder_encoder_output(encoder_out, new_order)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n\nclass RNNEncoder(FairseqEncoder):\n    """"""RNN encoder.""""""\n\n    def __init__(\n        self,\n        dictionary,\n        embed_tokens,\n        embed_dim=512,\n        hidden_dim=512,\n        num_layers=1,\n        cell_type=""lstm"",\n        dropout_in=0.1,\n        dropout_out=0.1,\n        residual_level=None,\n        bidirectional=False,\n        left_pad=True,\n        encoder_context_embed=False,\n    ):\n        super().__init__(dictionary)\n        self.dictionary = dictionary\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.residual_level = residual_level\n        self.hidden_dim = hidden_dim\n        self.output_units = hidden_dim  # fairseq LSTM compatibility\n        self.bidirectional = bidirectional\n        self.padding_idx = dictionary.pad()\n        self.left_pad = left_pad\n        self.embed_tokens = embed_tokens\n\n        if encoder_context_embed:\n            self.embed_tokens = ContextEmbedding(\n                embed_dim=self.embed_dim\n            ).encoder_embed_fn\n        self.encoder_context_embed = encoder_context_embed\n        self.word_dim = embed_dim\n\n        self.cell_type = cell_type\n        self.layers = nn.ModuleList([])\n        for layer in range(num_layers):\n            self.layers.append(\n                RNNLayer(\n                    self.word_dim if layer == 0 else hidden_dim,\n                    hidden_dim,\n                    self.cell_type,\n                    True if bidirectional and layer == 0 else False,\n                )\n            )\n\n        self.num_layers = len(self.layers)\n\n    def forward(self, src_tokens, src_lengths):\n        if self.left_pad:\n            # convert left-padding to right-padding\n            src_tokens = utils.convert_padding_direction(\n                src_tokens, self.padding_idx, left_to_right=True\n            )\n        bsz, seqlen = pytorch_translate_utils.get_source_tokens_tensor(\n            src_tokens\n        ).size()\n\n        # embed tokens\n        x = self.embed_tokens(src_tokens)\n        if self.encoder_context_embed:\n            x = self.embed_tokens_context(x)\n        x = F.dropout(x, p=self.dropout_in, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        embedded_words = x\n\n        # Generate packed seq to deal with varying source seq length\n        packed_input, batch_sizes, _, _ = pack_padded_sequence(x, src_lengths)\n        final_hiddens, final_cells = [], []\n        next_hiddens = []\n        for i, rnn_layer in enumerate(self.layers):\n            current_hidden_size = (\n                self.hidden_dim // 2 if rnn_layer.is_bidirectional else self.hidden_dim\n            )\n\n            if self.cell_type in [""lstm"", ""milstm"", ""layer_norm_lstm""]:\n                prev_hidden = (\n                    x.new(bsz, current_hidden_size).zero_(),\n                    x.new(bsz, current_hidden_size).zero_(),\n                )\n            else:\n                raise Exception(f""{self.cell_type} not implemented"")\n\n            hidden, current_output = rnn_layer.forward(\n                packed_input, prev_hidden, batch_sizes\n            )\n            next_hiddens.append(hidden)\n            prev_hidden = next_hiddens[-1]\n\n            if self.dropout_out != 0:\n                current_output = F.dropout(\n                    current_output, p=self.dropout_out, training=self.training\n                )\n\n            if self.residual_level is not None and i >= self.residual_level:\n                packed_input = packed_input.clone() + current_output\n            else:\n                packed_input = current_output\n\n        final_hiddens, final_cells = zip(*next_hiddens)\n        # Reshape to [num_layer, batch_size, hidden_dim]\n        final_hiddens = torch.cat(final_hiddens, dim=0).view(\n            self.num_layers, *final_hiddens[0].size()\n        )\n        final_cells = torch.cat(final_cells, dim=0).view(\n            self.num_layers, *final_cells[0].size()\n        )\n\n        #  [max_seqlen, batch_size, hidden_dim]\n        unpacked_output, _ = pad_packed_sequence(\n            PackedSequence(packed_input, batch_sizes)\n        )\n\n        return (\n            unpacked_output,\n            final_hiddens,\n            final_cells,\n            src_lengths,\n            src_tokens,\n            embedded_words,\n        )\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        """"""Reorder all outputs according to new_order.""""""\n        return reorder_encoder_output(encoder_out, new_order)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n\nclass RNNDecoder(DecoderWithOutputProjection):\n    """"""\n    RNN decoder with multihead attention. Attention is calculated using encoder\n    output and output of decoder\'s first RNN layerself. Attention is applied\n    after first RNN layer and concatenated to input of subsequent layers.\n    """"""\n\n    def __init__(\n        self,\n        src_dict,\n        dst_dict,\n        embed_tokens,\n        vocab_reduction_params=None,\n        encoder_hidden_dim=512,\n        embed_dim=512,\n        hidden_dim=512,\n        out_embed_dim=512,\n        cell_type=""lstm"",\n        num_layers=1,\n        dropout_in=0.1,\n        dropout_out=0.1,\n        attention_type=""dot"",\n        attention_heads=8,\n        first_layer_attention=False,\n        residual_level=None,\n        averaging_encoder=False,\n        project_output=True,\n        out_embed_norm=None,\n        tie_embeddings=False,\n        pretrained_embed=None,\n        projection_pretrained_embed=None,\n        att_weighted_src_embeds=False,\n        src_embed_dim=512,\n        att_weighted_activation_type=""tanh"",\n        predictor=None,\n        fp16: bool = False,\n    ):\n        super().__init__(\n            src_dict,\n            dst_dict,\n            vocab_reduction_params,\n            out_embed_dim,\n            project_output=project_output,\n            pretrained_embed=projection_pretrained_embed,\n            out_embed_norm=out_embed_norm,\n            att_weighted_src_embeds=att_weighted_src_embeds,\n            src_embed_dim=src_embed_dim,\n            att_weighted_activation_type=att_weighted_activation_type,\n            predictor=predictor,\n            fp16=fp16,\n        )\n        encoder_hidden_dim = max(1, encoder_hidden_dim)\n        self.encoder_hidden_dim = encoder_hidden_dim\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.out_embed_dim = out_embed_dim\n        self.dropout_in = dropout_in\n        self.dropout_out = dropout_out\n        self.attention_type = attention_type\n        self.residual_level = residual_level\n        self.tie_embeddings = tie_embeddings\n        self.attention_heads = attention_heads\n        self.first_layer_attention = first_layer_attention\n        self.padding_idx = dst_dict.pad()\n        self.embed_tokens = embed_tokens\n\n        if self.tie_embeddings:\n            assert self.embed_dim == self.out_embed_dim, (\n                ""Input embeddings and output projections must have the same ""\n                ""dimension for the weights to be tied""\n            )\n            self.embed_tokens.weight = self.output_projection_w\n\n        self.hidden_dim = hidden_dim\n        self.averaging_encoder = averaging_encoder\n\n        if cell_type == ""lstm"":\n            cell_class = rnn_cell.LSTMCell\n        elif cell_type == ""milstm"":\n            cell_class = rnn_cell.MILSTMCell\n        elif cell_type == ""layer_norm_lstm"":\n            cell_class = rnn_cell.LayerNormLSTMCell\n\n        if hidden_dim != encoder_hidden_dim:\n            hidden_init_fc_list = []\n            cell_init_fc_list = []\n            for _ in range(num_layers):\n                hidden_init_fc_list.append(Linear(encoder_hidden_dim, hidden_dim))\n                cell_init_fc_list.append(Linear(encoder_hidden_dim, hidden_dim))\n            self.hidden_init_fc_list = nn.ModuleList(hidden_init_fc_list)\n            self.cell_init_fc_list = nn.ModuleList(cell_init_fc_list)\n\n        self.attention = attention.build_attention(\n            attention_type=attention_type,\n            decoder_hidden_state_dim=hidden_dim,\n            context_dim=encoder_hidden_dim,\n            nheads=attention_heads,  # specific to multihead_attention\n        )\n\n        if self.attention.context_dim:\n            self.initial_attn_context = nn.Parameter(\n                torch.Tensor(self.attention.context_dim).zero_()\n            )\n        self.combined_output_and_context_dim = self.attention.context_dim + hidden_dim\n\n        layers = []\n        for layer in range(num_layers):\n            if layer == 0:\n                cell_input_dim = embed_dim\n            else:\n                cell_input_dim = hidden_dim\n\n            # attention applied to first layer always.\n            if self.first_layer_attention or layer == 0:\n                cell_input_dim += self.attention.context_dim\n            layers.append(cell_class(input_dim=cell_input_dim, hidden_dim=hidden_dim))\n        self.layers = nn.ModuleList(layers)\n\n        if self.combined_output_and_context_dim != out_embed_dim:\n            self.additional_fc = Linear(\n                self.combined_output_and_context_dim, out_embed_dim\n            )\n\n    def forward_unprojected(self, input_tokens, encoder_out, incremental_state=None):\n        if incremental_state is not None:\n            input_tokens = input_tokens[:, -1:]\n        bsz, seqlen = input_tokens.size()\n\n        # get outputs from encoder\n        (\n            encoder_outs,\n            final_hidden,\n            final_cell,\n            src_lengths,\n            src_tokens,\n            _,\n        ) = encoder_out\n\n        # embed tokens\n        x = self.embed_tokens(input_tokens)\n        x = F.dropout(x, p=self.dropout_in, training=self.training)\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # initialize previous states (or get from cache during incremental generation)\n        cached_state = utils.get_incremental_state(\n            self, incremental_state, ""cached_state""\n        )\n        input_feed = None\n        if cached_state is not None:\n            prev_hiddens, prev_cells, input_feed = cached_state\n        else:\n            # first time step, initialize previous states\n            init_prev_states = self._init_prev_states(encoder_out)\n            prev_hiddens = []\n            prev_cells = []\n\n            # init_prev_states may or may not include initial attention context\n            for (h, c) in zip(init_prev_states[0::2], init_prev_states[1::2]):\n                prev_hiddens.append(h)\n                prev_cells.append(c)\n            if self.attention.context_dim:\n                input_feed = self.initial_attn_context.expand(\n                    bsz, self.attention.context_dim\n                )\n\n        attn_scores_per_step = []\n        outs = []\n        step_attn_scores = None\n        for j in range(seqlen):\n            # input feeding: concatenate context vector from previous time step\n            step_input = maybe_cat((x[j, :, :], input_feed), dim=1)\n            previous_layer_input = step_input\n            for i, rnn in enumerate(self.layers):\n                # recurrent cell\n                hidden, cell = rnn(step_input, (prev_hiddens[i], prev_cells[i]))\n\n                if self.first_layer_attention and i == 0:\n                    # tgt_len is 1 in decoder and squeezed for both matrices\n                    # input_feed.shape = tgt_len X bsz X embed_dim\n                    # step_attn_scores.shape = src_len X tgt_len X bsz\n                    input_feed, step_attn_scores = self.attention(\n                        hidden, encoder_outs, src_lengths\n                    )\n\n                # hidden state becomes the input to the next layer\n                layer_output = F.dropout(\n                    hidden, p=self.dropout_out, training=self.training\n                )\n\n                if self.residual_level is not None and i >= self.residual_level:\n                    # TODO add an assert related to sizes here\n                    step_input = layer_output + previous_layer_input\n                else:\n                    step_input = layer_output\n\n                if self.first_layer_attention:\n                    step_input = maybe_cat((step_input, input_feed), dim=1)\n                previous_layer_input = step_input\n\n                # save state for next time step\n                prev_hiddens[i] = hidden\n                prev_cells[i] = cell\n\n            if not self.first_layer_attention:\n                input_feed, step_attn_scores = self.attention(\n                    hidden, encoder_outs, src_lengths\n                )\n\n            attn_scores_per_step.append(step_attn_scores.unsqueeze(1))\n            attn_scores = torch.cat(attn_scores_per_step, dim=1)\n            # srclen x tgtlen x bsz -> bsz x tgtlen x srclen\n            attn_scores = attn_scores.transpose(0, 2)\n            combined_output_and_context = maybe_cat((hidden, input_feed), dim=1)\n            # save final output\n            outs.append(combined_output_and_context)\n\n        # cache previous states (no-op except during incremental generation)\n        utils.set_incremental_state(\n            self,\n            incremental_state,\n            ""cached_state"",\n            (prev_hiddens, prev_cells, input_feed),\n        )\n\n        # collect outputs across time steps\n        x = torch.cat(outs, dim=0).view(\n            seqlen, bsz, self.combined_output_and_context_dim\n        )\n\n        # T x B x C -> B x T x C\n        x = x.transpose(1, 0)\n\n        # bottleneck layer\n        if hasattr(self, ""additional_fc""):\n            x = self.additional_fc(x)\n            x = F.dropout(x, p=self.dropout_out, training=self.training)\n        return x, attn_scores\n\n    def reorder_incremental_state(self, incremental_state, new_order):\n        """"""Reorder buffered internal state (for incremental generation).""""""\n        cached_state = utils.get_incremental_state(\n            self, incremental_state, ""cached_state""\n        )\n        if cached_state is None:\n            return\n\n        def reorder_state(state):\n            if state is None:\n                return None\n            if isinstance(state, list):\n                return [reorder_state(state_i) for state_i in state]\n            return state.index_select(0, new_order)\n\n        new_state = tuple(map(reorder_state, cached_state))\n        utils.set_incremental_state(self, incremental_state, ""cached_state"", new_state)\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return int(1e5)  # an arbitrary large number\n\n    def _init_prev_states(self, encoder_out):\n        (\n            encoder_output,\n            final_hiddens,\n            final_cells,\n            src_lengths,\n            src_tokens,\n            _,\n        ) = encoder_out\n        num_layers = len(self.layers)\n        if self.averaging_encoder:\n            # Use mean encoder hidden states\n            prev_hiddens = [torch.mean(encoder_output, 0)] * num_layers\n        else:\n            # Simply return the final state of each layer\n            prev_hiddens = [final_hiddens[i] for i in range(num_layers)]\n        prev_cells = [final_cells[i] for i in range(num_layers)]\n\n        if hasattr(self, ""hidden_init_fc_list""):\n            for i in range(num_layers):\n                prev_hiddens[i] = self.hidden_init_fc_list[i](prev_hiddens[i])\n                prev_cells[i] = self.cell_init_fc_list[i](prev_cells[i])\n\n        prev_states = []\n        for h, c in zip(prev_hiddens, prev_cells):\n            prev_states.extend([h, c])\n        if self.attention.context_dim:\n            prev_states.append(self.initial_attn_context)\n\n        return prev_states\n\n\nclass BiLSTM(nn.Module):\n    """"""Wrapper for nn.LSTM\n\n    Differences include:\n    * weight initialization\n    * the bidirectional option makes the first layer bidirectional only\n    (and in that case the hidden dim is divided by 2)\n    """"""\n\n    @staticmethod\n    def LSTM(input_size, hidden_size, **kwargs):\n        m = nn.LSTM(input_size, hidden_size, **kwargs)\n        for name, param in m.named_parameters():\n            if ""weight"" in name or ""bias"" in name:\n                param.data.uniform_(-0.1, 0.1)\n        return m\n\n    def __init__(\n        self, num_layers, bidirectional, embed_dim, hidden_dim, dropout, residual_level\n    ):\n        super().__init__()\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        if bidirectional:\n            assert hidden_dim % 2 == 0, ""hidden_dim should be even if bidirectional""\n        self.hidden_dim = hidden_dim\n        self.residual_level = residual_level\n        self.layers = nn.ModuleList([])\n        for layer in range(num_layers):\n            is_layer_bidirectional = bidirectional and layer == 0\n            if is_layer_bidirectional:\n                assert hidden_dim % 2 == 0, (\n                    ""hidden_dim must be even if bidirectional ""\n                    ""(to be divided evenly between directions)""\n                )\n            self.layers.append(\n                BiLSTM.LSTM(\n                    embed_dim if layer == 0 else hidden_dim,\n                    hidden_dim // 2 if is_layer_bidirectional else hidden_dim,\n                    num_layers=1,\n                    dropout=dropout,\n                    bidirectional=is_layer_bidirectional,\n                )\n            )\n\n    def forward(self, embeddings, lengths, enforce_sorted=True):\n        # enforce_sorted is set to True by default to force input lengths\n        # are sorted in a descending order when pack padded sequence.\n        bsz = embeddings.size()[1]\n\n        # Generate packed seq to deal with varying source seq length\n        # packed_input is of type PackedSequence, which consists of:\n        # element [0]: a tensor, the packed data, and\n        # element [1]: a list of integers, the batch size for each step\n        packed_input = pack_padded_sequence(\n            embeddings, lengths, enforce_sorted=enforce_sorted\n        )\n\n        final_hiddens, final_cells = [], []\n        for i, rnn_layer in enumerate(self.layers):\n            if self.bidirectional and i == 0:\n                h0 = embeddings.new(2, bsz, self.hidden_dim // 2).zero_()\n                c0 = embeddings.new(2, bsz, self.hidden_dim // 2).zero_()\n            else:\n                h0 = embeddings.new(1, bsz, self.hidden_dim).zero_()\n                c0 = embeddings.new(1, bsz, self.hidden_dim).zero_()\n\n            # apply LSTM along entire sequence\n            current_output, (h_last, c_last) = rnn_layer(packed_input, (h0, c0))\n\n            # final state shapes: (bsz, hidden_dim)\n            if self.bidirectional and i == 0:\n                # concatenate last states for forward and backward LSTM\n                h_last = torch.cat((h_last[0, :, :], h_last[1, :, :]), dim=1)\n                c_last = torch.cat((c_last[0, :, :], c_last[1, :, :]), dim=1)\n            else:\n                h_last = h_last.squeeze(dim=0)\n                c_last = c_last.squeeze(dim=0)\n\n            final_hiddens.append(h_last)\n            final_cells.append(c_last)\n\n            if self.residual_level is not None and i >= self.residual_level:\n                packed_input[0] = packed_input.clone()[0] + current_output[0]\n            else:\n                packed_input = current_output\n\n        # Reshape to [num_layer, batch_size, hidden_dim]\n        final_hiddens = torch.cat(final_hiddens, dim=0).view(\n            self.num_layers, *final_hiddens[0].size()\n        )\n        final_cells = torch.cat(final_cells, dim=0).view(\n            self.num_layers, *final_cells[0].size()\n        )\n\n        #  [max_seqlen, batch_size, hidden_dim]\n        unpacked_output, _ = pad_packed_sequence(packed_input)\n\n        return (unpacked_output, final_hiddens, final_cells)\n\n\n@register_model(""semi_supervised_rnn"")\nclass SemisupervisedRNNModel(SemiSupervisedModel):\n    single_model_cls = RNNModel\n\n    @staticmethod\n    def add_args(parser):\n        RNNModel.add_args(parser)\n        SemiSupervisedModel.add_args(parser)\n\n\n@register_model_architecture(""rnn"", ""rnn"")\ndef base_architecture(args):\n    # default architecture\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 1)\n    args.encoder_hidden_dim = getattr(args, ""encoder_hidden_dim"", 512)\n    args.encoder_bidirectional = getattr(args, ""encoder_bidirectional"", False)\n    args.encoder_dropout_in = getattr(args, ""encoder_dropout_in"", args.dropout)\n    args.encoder_dropout_out = getattr(args, ""encoder_dropout_out"", args.dropout)\n    args.encoder_pretrained_embed = getattr(args, ""encoder_pretrained_embed"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", 512)\n    args.decoder_layers = getattr(args, ""decoder_layers"", 1)\n    args.decoder_hidden_dim = getattr(args, ""decoder_hidden_dim"", 512)\n    args.decoder_pretrained_embed = getattr(args, ""decoder_pretrained_embed"", None)\n    args.decoder_out_embed_dim = getattr(args, ""decoder_out_embed_dim"", 512)\n    args.decoder_out_pretrained_embed = getattr(\n        args, ""decoder_out_pretrained_embed"", None\n    )\n    args.attention_type = getattr(args, ""attention_type"", ""dot"")\n    args.attention_heads = getattr(args, ""attention_heads"", 8)\n    args.first_layer_attention = getattr(args, ""first_layer_attention"", False)\n    args.decoder_dropout_in = getattr(args, ""decoder_dropout_in"", args.dropout)\n    args.decoder_dropout_out = getattr(args, ""decoder_dropout_out"", args.dropout)\n    args.averaging_encoder = getattr(args, ""averaging_encoder"", False)\n    args.encoder_freeze_embed = getattr(args, ""encoder_freeze_embed"", False)\n    args.encoder_normalize_embed = getattr(args, ""encoder_normalize_embed"", False)\n    args.decoder_freeze_embed = getattr(args, ""decoder_freeze_embed"", False)\n    args.ngram_decoder = getattr(args, ""ngram_decoder"", None)\n    args.multi_encoder = getattr(args, ""multi_encoder"", None)\n    args.multi_decoder = getattr(args, ""multi_decoder"", None)\n    args.multi_decoder_is_lm = getattr(args, ""multi_decoder_is_lm"", None)\n    args.multiling_encoder_lang = getattr(args, ""multiling_encoder_lang"", None)\n    args.multi_model_training_schedule = getattr(\n        args, ""multi_model_training_schedule"", ""complete""\n    )\n    args.multi_model_fixed_weights = getattr(args, ""multi_model_fixed_weights"", None)\n    args.cell_type = getattr(args, ""cell_type"", ""lstm"")\n    args.ngram_activation_type = getattr(args, ""ngram_activation_type"", ""relu"")\n    vocab_reduction.set_arg_defaults(args)\n    args.sequence_lstm = getattr(args, ""sequence_lstm"", False)\n    args.decoder_tie_embeddings = getattr(args, ""decoder_tie_embeddings"", False)\n    args.encoder_pretrained_embed = getattr(args, ""encoder_pretrained_embed"", None)\n    args.decoder_pretrained_embed = getattr(args, ""decoder_pretrained_embed"", None)\n    args.decoder_out_pretrained_embed = getattr(\n        args, ""decoder_out_pretrained_embed"", None\n    )\n    args.out_embed_norm = getattr(args, ""out_embed_dim"", None)\n    args.att_weighted_src_embeds = getattr(args, ""att_weighted_source_embeds"", False)\n    args.att_weighted_activation_type = getattr(\n        args, ""att_weighted_activation_type"", ""tanh""\n    )\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.language_model_only = getattr(args, ""language_model_only"", False)\n    args.encoder_context_embed = getattr(args, ""encoder_context_embed"", False)\n\n\n@register_model_architecture(""rnn"", ""rnn_big_test"")\ndef rnn_big_test(args):\n    base_architecture(args)\n    args.encoder_embed_dim = 1024\n    args.encoder_layers = 6\n    args.encoder_hidden_dim = 1024\n    args.decoder_embed_dim = 1024\n    args.decoder_layers = 6\n    args.decoder_hidden_dim = 1024\n    args.decoder_out_embed_dim = 1024\n\n\n@register_model_architecture(""semi_supervised_rnn"", ""semi_supervised_rnn"")\ndef semi_supervised_rnn(args):\n    base_architecture(args)\n    SemiSupervisedModel.set_semi_supervised_arch_args(args)\n'"
pytorch_translate/rnn_cell.py,8,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef LSTMCell(input_dim, hidden_dim, **kwargs):\n    m = nn.LSTMCell(input_dim, hidden_dim, **kwargs)\n    for name, param in m.named_parameters():\n        if ""weight"" in name or ""bias"" in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m\n\n\nclass MILSTMCellBackend(nn.RNNCell):\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(MILSTMCellBackend, self).__init__(input_size, hidden_size, bias=False)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = nn.Parameter(torch.Tensor(4 * hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(4 * hidden_size))\n        else:\n            self.register_parameter(""bias"", None)\n        self.alpha = nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.beta_h = nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.beta_i = nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, x, hidden):\n        # get prev_t, cell_t from states\n        hx, cx = hidden\n        Wx = F.linear(x, self.weight_ih)\n        Uz = F.linear(hx, self.weight_hh)\n\n        # Section 2.1 in https://arxiv.org/pdf/1606.06630.pdf\n        gates = self.alpha * Wx * Uz + self.beta_i * Wx + self.beta_h * Uz + self.bias\n\n        # Same as LSTMCell after this point\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = F.sigmoid(ingate)\n        forgetgate = F.sigmoid(forgetgate)\n        cellgate = F.tanh(cellgate)\n        outgate = F.sigmoid(outgate)\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n        hy = outgate * F.tanh(cy)\n\n        return hy, cy\n\n\ndef MILSTMCell(input_dim, hidden_dim, **kwargs):\n    m = MILSTMCellBackend(input_dim, hidden_dim, **kwargs)\n    for name, param in m.named_parameters():\n        if ""weight"" in name or ""bias"" in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m\n\n\nclass LayerNormLSTMCellBackend(nn.LSTMCell):\n    def __init__(self, input_dim, hidden_dim, bias=True, epsilon=0.00001):\n        super(LayerNormLSTMCellBackend, self).__init__(input_dim, hidden_dim, bias)\n        self.epsilon = epsilon\n\n    def _layerNormalization(self, x):\n        mean = x.mean(1, keepdim=True).expand_as(x)\n        std = x.std(1, keepdim=True).expand_as(x)\n        return (x - mean) / (std + self.epsilon)\n\n    def forward(self, x, hidden):\n        hx, cx = hidden\n        gates = F.linear(x, self.weight_ih, self.bias_ih) + F.linear(\n            hx, self.weight_hh, self.bias_hh\n        )\n\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = F.sigmoid(self._layerNormalization(ingate))\n        forgetgate = F.sigmoid(self._layerNormalization(forgetgate))\n        cellgate = F.tanh(self._layerNormalization(cellgate))\n        outgate = F.sigmoid(self._layerNormalization(outgate))\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n\n        hy = outgate * F.tanh(cy)\n\n        return hy, cy\n\n\ndef LayerNormLSTMCell(input_dim, hidden_dim, **kwargs):\n    m = LayerNormLSTMCellBackend(input_dim, hidden_dim, **kwargs)\n    for name, param in m.named_parameters():\n        if ""weight"" in name or ""bias"" in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m\n'"
pytorch_translate/semi_supervised.py,1,"b'#!/usr/bin/env python3\n\nimport copy\nfrom collections import OrderedDict\n\nimport torch.nn as nn\nfrom fairseq.models import (\n    FairseqMultiModel,\n    register_model,\n    register_model_architecture,\n)\nfrom pytorch_translate import common_layers, constants, utils\n\n\n@register_model(""semi_supervised"")\nclass SemiSupervisedModel(FairseqMultiModel):\n    """"""\n    To use, you must extend this class and define single_model_cls as a class\n    variable. Example:\n\n        @register_model(""semi_supervised_transformer"")\n        class SemiSupervisedTransformerModel(SemiSupervisedModel):\n            # We can\'t use `self.single_model_cls` because at this point\n            # `__init__` hasn\'t run. single_model_cls is a static class variable\n            # that is meant to be constant.\n            single_model_cls = TransformerModel\n\n            @staticmethod\n            def add_args(parser):\n                TransformerModel.add_args(parser)\n                SemiSupervisedModel.add_args(parser)\n    """"""\n\n    def __init__(self, task, encoders, decoders):\n        super().__init__(encoders, decoders)\n        self.task = task\n        self.models = nn.ModuleDict(\n            {\n                key: self.__class__.single_model_cls(task, encoders[key], decoders[key])\n                for key in self.keys\n            }\n        )\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--share-encoder-embeddings"",\n            action=""store_true"",\n            help=""share encoder embeddings across languages"",\n        )\n        parser.add_argument(\n            ""--share-decoder-embeddings"",\n            action=""store_true"",\n            help=""share decoder embeddings across languages"",\n        )\n        parser.add_argument(\n            ""--share-encoders"",\n            action=""store_true"",\n            help=""share encoders across languages"",\n        )\n        parser.add_argument(\n            ""--share-decoders"",\n            action=""store_true"",\n            help=""share decoders across languages"",\n        )\n        parser.add_argument(\n            ""--remove-vr-if-same-lang-at-enc-and-dec"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=True,\n            help=""Whether to remove vocab reduction in the decoder for src-src ""\n            ""and tgt-tgt models. Note this should be True unless you use a ""\n            ""joint vocab AND your lexical dictionaries are built to ""\n            ""accommodate the same language at source and target."",\n        )\n\n    @staticmethod\n    def set_semi_supervised_arch_args(args):\n        args.share_encoder_embeddings = getattr(args, ""share_encoder_embeddings"", False)\n        args.share_decoder_embeddings = getattr(args, ""share_decoder_embeddings"", False)\n        args.share_encoders = getattr(args, ""share_encoders"", False)\n        args.share_decoders = getattr(args, ""share_decoders"", False)\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        if not hasattr(args, ""max_source_positions""):\n            args.max_source_positions = 1024\n        if not hasattr(args, ""max_target_positions""):\n            args.max_target_positions = 1024\n\n        src_langs = [lang_pair.split(""-"")[0] for lang_pair in task.lang_pairs]\n        tgt_langs = [lang_pair.split(""-"")[1] for lang_pair in task.lang_pairs]\n\n        if args.share_encoders:\n            args.share_encoder_embeddings = True\n        if args.share_decoders:\n            args.share_decoder_embeddings = True\n\n        # encoders/decoders for each language\n        lang_encoders, lang_decoders = {}, {}\n\n        def strip_suffix(lang):\n            """"""\n            Both ""lang"" and ""lang_mono"" languages share the same encoder/decoder\n            since they belong to the same language but use bilingual and monolingual\n            corpora respectively to train\n            So use ""lang"" as model key for both ""lang"" and ""lang_mono"" by stripping\n            the suffix ""_mono"" if it exists\n            """"""\n            if f""_{constants.MONOLINGUAL_DATA_IDENTIFIER}"" in lang:\n                lang = lang[: -(len(f""_{constants.MONOLINGUAL_DATA_IDENTIFIER}""))]\n            return lang\n\n        def get_encoder(lang):\n            lang = strip_suffix(lang)\n            if lang not in lang_encoders:\n                src_dict = task.dicts[lang]\n                encoder_embed_tokens = common_layers.Embedding(\n                    num_embeddings=len(src_dict),\n                    embedding_dim=args.encoder_embed_dim,\n                    padding_idx=src_dict.pad(),\n                    freeze_embed=args.encoder_freeze_embed,\n                    normalize_embed=getattr(args, ""encoder_normalize_embed"", False),\n                )\n                utils.load_embedding(\n                    embedding=encoder_embed_tokens,\n                    dictionary=src_dict,\n                    pretrained_embed=args.encoder_pretrained_embed,\n                )\n                lang_encoders[lang] = cls.single_model_cls.build_encoder(\n                    args, src_dict, embed_tokens=encoder_embed_tokens\n                )\n            return lang_encoders[lang]\n\n        def get_decoder(lang_pair, shared_decoder_embed_tokens=None):\n            if args.share_decoders:\n                args.remove_vr_if_same_lang_at_enc_and_dec = False\n            """"""\n            Fetch decoder for the input `lang_pair`, which denotes the target\n            language of the model\n            """"""\n            source_lang, target_lang = (\n                strip_suffix(lang) for lang in lang_pair.split(""-"")\n            )\n            if target_lang not in lang_decoders:\n                # hack to prevent VR for denoising autoencoder. We remove vocab\n                # reduction params if we have lang-lang_any_suffix\n                args_maybe_modified = copy.deepcopy(args)\n                if (\n                    source_lang == target_lang\n                    and not args.remove_vr_if_same_lang_at_enc_and_dec\n                ):\n                    args_maybe_modified.vocab_reduction_params = None\n                tgt_dict = task.dicts[target_lang]\n                if shared_decoder_embed_tokens is None:\n                    decoder_embed_tokens = common_layers.Embedding(\n                        num_embeddings=len(tgt_dict),\n                        embedding_dim=args.decoder_embed_dim,\n                        padding_idx=tgt_dict.pad(),\n                        freeze_embed=args.decoder_freeze_embed,\n                    )\n\n                    utils.load_embedding(\n                        embedding=decoder_embed_tokens,\n                        dictionary=tgt_dict,\n                        pretrained_embed=args.decoder_pretrained_embed,\n                    )\n                else:\n                    decoder_embed_tokens = shared_decoder_embed_tokens\n                lang_decoders[target_lang] = cls.single_model_cls.build_decoder(\n                    args_maybe_modified,\n                    task.dicts[target_lang],\n                    tgt_dict,\n                    embed_tokens=decoder_embed_tokens,\n                )\n            return lang_decoders[target_lang]\n\n        # shared encoders/decoders (if applicable)\n        shared_encoder, shared_decoder, shared_decoder_embed_tokens = None, None, None\n        if args.share_encoders:\n            shared_encoder = get_encoder(src_langs[0])\n        if args.share_decoders:\n            shared_decoder = get_decoder(tgt_langs[0])\n\n        if args.share_decoder_embeddings:\n            shared_decoder_embed_tokens = FairseqMultiModel.build_shared_embeddings(\n                dicts=task.dicts,\n                langs=[strip_suffix(tgt_lang) for tgt_lang in tgt_langs],\n                embed_dim=args.decoder_embed_dim,\n                build_embedding=common_layers.build_embedding,\n                pretrained_embed_path=None,\n            )\n        encoders, decoders = OrderedDict(), OrderedDict()\n        for lang_pair, src in zip(task.lang_pairs, src_langs):\n            encoders[lang_pair] = (\n                shared_encoder if shared_encoder is not None else get_encoder(src)\n            )\n            decoders[lang_pair] = (\n                shared_decoder\n                if shared_decoder is not None\n                else get_decoder(\n                    lang_pair, shared_decoder_embed_tokens=shared_decoder_embed_tokens\n                )\n            )\n\n        return cls(task, encoders, decoders)\n'"
pytorch_translate/sequence_criterions.py,8,"b'#!/usr/bin/env python3\n\nimport copy\nimport math\n\nimport torch\nfrom fairseq import bleu, utils\nfrom fairseq.criterions import LegacyFairseqCriterion, register_criterion\nfrom pytorch_translate import generate\n\n\n""""""Sequence-level losses from Edunov et al., 2017\n(https://arxiv.org/pdf/1711.04956.pdf).\n""""""\n\n\nclass BaseSequenceLossCriterion(LegacyFairseqCriterion):\n    """"""Base class for criteria with need to run beam search.""""""\n\n    def __init__(self, args, src_dict, dst_dict):\n        super().__init__(args, src_dict, dst_dict)\n        self.translator = None\n        self.scorer = bleu.Scorer(dst_dict.pad(), dst_dict.eos(), dst_dict.unk())\n\n    def get_translator(self, model):\n        """"""Get lazy singleton translator instance.""""""\n        if self.translator is None:\n            args_clone = copy.copy(self.args)\n            if self.args.loss_beam:  # Override beam size if necessary\n                args_clone.beam = self.args.loss_beam\n            self.translator = generate.build_sequence_generator(args_clone, [model])\n        return self.translator\n\n    def generate_translations(self, model, sample):\n        """"""Run beam search to generate translations from the current model.\n\n        Args:\n            model: FairseqModel to use (passed via FairseqCriterion.forward())\n            sample: Training batch (passed via FairseqCriterion.forward())\n\n        Returns:\n            A tuple (tokens, bleu) of tensors. `tokens` is a [bsz, beam_size,\n            max_translation_length] int tensor with generated translations (with\n            EOS). `bleu` is a [bsz, beam_size] float tensor with BLEU scores.\n            `tokens` corresponds to U(x) in Edunov et al., 2017.\n        """"""\n        model.eval()  # Set to eval mode\n        enc_input = (\n            sample[""net_input""][""src_tokens""],\n            sample[""net_input""][""src_lengths""],\n        )\n        trg_lens = sample[""target""].size(1) - torch.sum(\n            sample[""target""] <= self.scorer.eos, dim=1\n        )\n        max_trans_len = 0\n        all_translations = []\n        for batch_idx, batch_trans in enumerate(\n            self._batch_translations(model, enc_input)\n        ):\n            ref = sample[""target""][batch_idx, : trg_lens[batch_idx] + 1].int().cpu()\n            batch_translations = []\n            for trans in batch_trans:\n                max_trans_len = max(max_trans_len, trans[""tokens""].size(0))\n                self.scorer.reset()\n                self.scorer.add(ref, trans[""tokens""].int().cpu())\n                batch_translations.append((self.scorer.score(), trans[""tokens""]))\n            batch_translations.sort(key=lambda x: -x[0])\n            all_translations.append(batch_translations)\n        bsz = len(all_translations)\n        beam_size = self.get_translator(model).beam_size\n        tokens_tensor = trans[""tokens""].new_full(\n            (bsz, beam_size, max_trans_len), self.scorer.pad\n        )\n        bleu_tensor = torch.zeros(bsz, beam_size)\n        for batch_idx, batch_translations in enumerate(all_translations):\n            for trans_idx, (bleu_score, tokens) in enumerate(batch_translations):\n                tokens_tensor[batch_idx, trans_idx, : tokens.size(0)] = tokens\n                bleu_tensor[batch_idx, trans_idx] = bleu_score\n        model.train()  # Set back to train mode\n        return tokens_tensor, bleu_tensor\n\n    def _batch_translations(self, model, enc_input):\n        """"""Use beam_size times smaller batch size for translation.""""""\n        translator = self.get_translator(model)\n        bsz = enc_input[1].size(0)\n        gen_bsz = bsz // translator.beam_size + 1\n        for f in range(0, bsz, gen_bsz):\n            t = min(f + gen_bsz, bsz)\n            for trans in translator.generate((enc_input[0][f:t], enc_input[1][f:t])):\n                yield trans\n\n    def compute_nll(self, model, sample, translations):\n        """"""Compute negative log-likelihoods for the translations.\n\n        This function computes p(u|x) for each element in U(x) (see Edunov et\n        al., 2017 for notation) for computing gradients.\n\n        Args:\n            model: FairseqModel to use (passed via FairseqCriterion.forward())\n            sample: Training batch (passed via FairseqCriterion.forward())\n            translations: A [batch_size, beam_size, max_trg_len] int tensor of\n                generated translation (as produced by generate_translations())\n\n        Returns:\n            A [bsz, beam_size] float tensor containing the negative loglikelihoods\n            of the sentences in `translations`.\n        """"""\n        bsz, beam_size, max_trans_len = translations.size()\n        total = bsz * beam_size\n        translations = translations.view(total, max_trans_len)\n        prev_output_tokens = translations.new(total, max_trans_len)\n        prev_output_tokens[:, 0] = self.scorer.eos\n        prev_output_tokens[:, 1:] = translations[:, :-1]\n        src_tokens = (\n            sample[""net_input""][""src_tokens""]\n            .unsqueeze(1)\n            .repeat(1, beam_size, 1)\n            .view(total, -1)\n        )\n        src_lengths = (\n            sample[""net_input""][""src_lengths""]\n            .unsqueeze(1)\n            .repeat(1, beam_size)\n            .view(total)\n        )\n        all_losses = []\n        for f in range(0, total, bsz):\n            t = min(f + bsz, total)\n            net_output = model(\n                src_tokens=src_tokens[f:t],\n                src_lengths=src_lengths[f:t],\n                prev_output_tokens=prev_output_tokens[f:t],\n            )\n            lprobs = model.get_normalized_probs(net_output, log_probs=True)\n            nll_loss = -lprobs.gather(dim=-1, index=translations[f:t].unsqueeze(2))\n            non_pad_mask = translations[f:t].ne(self.padding_idx).float()\n            all_losses.append(torch.sum(nll_loss.squeeze(2) * non_pad_mask, dim=1))\n        return torch.cat(all_losses).view(bsz, beam_size)\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        sample_size = sum(log.get(""sample_size"", 0) for log in logging_outputs)\n        ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n        nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n        return {\n            ""loss"": sum(log.get(""loss"", 0) for log in logging_outputs)\n            / sample_size\n            / math.log(2),\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""sample_size"": sample_size,\n        }\n\n\n@register_criterion(""sequence_nll"")\nclass SequenceNegativeLoglikelihoodCriterion(BaseSequenceLossCriterion):\n    """"""SeqNLL loss from https://arxiv.org/pdf/1711.04956.pdf.""""""\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss, as a Variable\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        translations, bleu_scores = self.generate_translations(model, sample)\n        nll_loss = self.compute_nll(model, sample, translations)\n        loss = nll_loss[:, 0] + torch.logsumexp(-nll_loss, 1)\n        if reduce:\n            loss = loss.sum()\n        sample_size = (\n            sample[""target""].size(0) if self.args.sentence_avg else sample[""ntokens""]\n        )\n        logging_output = {\n            ""loss"": utils.item(loss.data) if reduce else loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n        }\n        return loss, sample_size, logging_output\n\n\n@register_criterion(""sequence_risk"")\nclass SequenceRiskCriterion(BaseSequenceLossCriterion):\n    """"""Risk loss from https://arxiv.org/pdf/1711.04956.pdf.""""""\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss, as a Variable\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        translations, bleu_scores = self.generate_translations(model, sample)\n        nll_loss = self.compute_nll(model, sample, translations)\n        partition = torch.logsumexp(-nll_loss, 1)\n        probs = torch.exp(-nll_loss - partition.unsqueeze(1))\n        loss = torch.sum((1 - bleu_scores.cuda() / 100) * probs, dim=1)\n        if reduce:\n            loss = loss.sum()\n        sample_size = (\n            sample[""target""].size(0) if self.args.sentence_avg else sample[""ntokens""]\n        )\n        logging_output = {\n            ""loss"": utils.item(loss.data) if reduce else loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n        }\n        return loss, sample_size, logging_output\n'"
pytorch_translate/torchscript_export.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\n\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate.constants import CHECKPOINT_PATHS_DELIMITER\nfrom pytorch_translate.ensemble_export import BeamSearch\n\n\ndef get_parser_with_args():\n    parser = argparse.ArgumentParser(\n        description=(""Export PyTorch-trained FBTranslate models"")\n    )\n    parser.add_argument(\n        ""--path"",\n        ""--checkpoint"",\n        metavar=""FILE"",\n        help=""path(s) to model file(s), colon separated"",\n    )\n    parser.add_argument(\n        ""--output-file"",\n        default="""",\n        help=""File name to which to save beam search network"",\n    )\n    parser.add_argument(\n        ""--output-graph-file"",\n        default="""",\n        help=""File name to which to save the beam search graph for debugging"",\n    )\n    parser.add_argument(\n        ""--source-vocab-file"",\n        required=True,\n        help=""File encoding PyTorch dictionary for source language"",\n    )\n    parser.add_argument(\n        ""--target-vocab-file"",\n        required=True,\n        help=""File encoding PyTorch dictionary for source language"",\n    )\n    parser.add_argument(\n        ""--beam-size"",\n        type=int,\n        default=6,\n        help=""Number of top candidates returned by each decoder step"",\n    )\n    parser.add_argument(\n        ""--word-reward"",\n        type=float,\n        default=0.0,\n        help=""Value to add for each word (besides EOS)"",\n    )\n    parser.add_argument(\n        ""--unk-reward"",\n        type=float,\n        default=0.0,\n        help=""Value to add for each word UNK token"",\n    )\n\n    return parser\n\n\ndef main():\n    parser = get_parser_with_args()\n    args = parser.parse_args()\n\n    if args.output_file == """":\n        print(""No action taken. Need output_file to be specified."")\n        parser.print_help()\n        return\n\n    checkpoint_filenames = args.path.split(CHECKPOINT_PATHS_DELIMITER)\n\n    beam_search = BeamSearch.build_from_checkpoints(\n        checkpoint_filenames=checkpoint_filenames,\n        src_dict_filename=args.src_dict,\n        dst_dict_filename=args.dst_dict,\n        beam_size=args.beam_size,\n        word_reward=args.word_reward,\n        unk_reward=args.unk_reward,\n    )\n    beam_search.save_to_pytorch(output_path=args.output_file)\n    if args.output_graph_file:\n        with open(args.output_graph_file.path, ""w"") as f:\n            f.write(str(beam_search.graph))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_translate/train.py,10,"b'#!/usr/bin/env python3\n\nimport builtins as __builtin__\nimport collections\nimport datetime\nimport faulthandler\nimport math\nimport multiprocessing.queues as mp_queues\nimport os\nimport queue\nimport random\nimport time\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom fairseq import data, distributed_utils, options, progress_bar, tasks, utils\nfrom fairseq.meters import AverageMeter, StopwatchMeter\nfrom fairseq.trainer import Trainer\nfrom pytorch_translate import char_aware_hybrid  # noqa\nfrom pytorch_translate import char_source_hybrid  # noqa\nfrom pytorch_translate import hybrid_transformer_rnn  # noqa\nfrom pytorch_translate import sequence_criterions  # noqa\nfrom pytorch_translate import transformer  # noqa\nfrom pytorch_translate import transformer_aan  # noqa\nfrom pytorch_translate import weighted_criterions  # noqa\nfrom pytorch_translate import (\n    checkpoint,\n    constants,\n    evals,\n    multi_model,\n    options as pytorch_translate_options,\n    preprocess,\n)\nfrom pytorch_translate.data import dictionary as pytorch_translate_dictionary\nfrom pytorch_translate.dual_learning import dual_learning_criterion  # noqa\nfrom pytorch_translate.dual_learning import dual_learning_task  # noqa\n\n# TODO(T55884145): Replace with\n# from fvcore.common.file_io import PathManager\nfrom pytorch_translate.file_io import PathManager\nfrom pytorch_translate.research.deliberation_networks import (  # noqa\n    deliberation_networks,\n)\nfrom pytorch_translate.research.knowledge_distillation import (  # noqa\n    dual_decoder_kd_loss,\n    dual_decoder_kd_model,\n    hybrid_dual_decoder_kd_model,\n    knowledge_distillation_loss,\n)\nfrom pytorch_translate.research.rescore import (  # noqa\n    cloze_transformer_model,\n    rescoring_criterion,\n)\nfrom pytorch_translate.word_prediction import word_prediction_criterion  # noqa\nfrom pytorch_translate.word_prediction import word_prediction_model  # noqa\n\n\ntry:\n    from pytorch_translate import latent_var_criterion  # noqa\n    from pytorch_translate import latent_var_models  # noqa\n    from pytorch_translate import latent_var_task  # noqa\nexcept ImportError:\n    pass\n\n\nfrom pytorch_translate import rnn  # noqa; noqa\n\n# we import semi_supervised here so that the model gets registered in fairseq\n# model registry\nfrom pytorch_translate import semi_supervised  # noqa; noqa\n\n\nfrom pytorch_translate import char_source_model  # noqa; noqa\nfrom pytorch_translate import char_source_transformer_model  # noqa; noqa\n\n\ndef get_parser_with_args(default_task=""pytorch_translate""):\n    parser = options.get_parser(""Trainer"", default_task=default_task)\n    pytorch_translate_options.add_verbosity_args(parser, train=True)\n    pytorch_translate_options.add_dataset_args(parser, train=True, gen=True)\n    options.add_distributed_training_args(parser)\n    # Adds args related to training (validation and stopping criterions).\n    optimization_group = options.add_optimization_args(parser)\n    pytorch_translate_options.expand_optimization_args(optimization_group)\n    # Adds args related to checkpointing.\n    checkpointing_group = options.add_checkpoint_args(parser)\n    pytorch_translate_options.expand_checkpointing_args(checkpointing_group)\n    # Add model related args\n    options.add_model_args(parser)\n    # Adds args for generating intermediate BLEU eval while training.\n    generation_group = options.add_generation_args(parser)\n    pytorch_translate_options.expand_generation_args(generation_group, train=True)\n    # Adds args related to input data files (preprocessing, numberizing, and\n    # binarizing text files; creating vocab files)\n    pytorch_translate_options.add_preprocessing_args(parser)\n    return parser\n\n\ndef default_extra_state(args) -> Dict[str, Any]:\n    return {\n        ""epoch"": 1,\n        ""batch_offset"": 0,\n        ""start_time"": time.time(),\n        ""previous_training_time"": 0.0,\n        ""tune_eval"": {\n            ""loss"": None,\n            ""perplexity"": None,\n            ""lowest_loss"": None,\n            ""num_since_best"": 0,\n        },\n        # The list of checkpoint files is actually managed by the\n        # CheckpointManager, which overwrites this placeholder when it saves\n        # checkpoints.\n        ""checkpoint_files"": [],\n        ""training_progress"": [],\n    }\n\n\ndef update_output(\n    args,\n    extra_state: Dict[str, Any],\n    output_queue: Optional[mp_queues.Queue],\n    num_updates: int,\n    train_ppl: float,\n    wps: Optional[float],\n):\n    if distributed_utils.is_master(args) and output_queue is not None:\n        progress_output: Tuple[int, Dict] = (\n            num_updates,\n            {\n                ""train_ppl"": train_ppl,\n                ""tune_loss"": utils.item(extra_state[""tune_eval""][""loss""]),\n                ""tune_ppl"": extra_state[""tune_eval""][""perplexity""],\n                ""wps"": utils.item(wps),\n                # translation_samples isn\'t currently used by the queue reader,\n                # so just pass None for now until we start needing it.\n                ""translation_samples"": None,\n            },\n        )\n        output_queue.put_nowait(progress_output)\n        extra_state[""training_progress""].append(progress_output)\n\n    return extra_state\n\n\ndef clear_per_step_extra_state(extra_state: Dict[str, Any]) -> Dict[str, Any]:\n    """"""\n    Clear values in extra_state that are technically only true for a specific\n    step (ex: the eval tune loss calculated after 5 train steps is no longer\n    accurate after 7 train steps, but might not get updated since we might not\n    be doing eval after every step).\n    """"""\n    extra_state[""tune_eval""][""loss""] = None\n    extra_state[""tune_eval""][""perplexity""] = None\n    return extra_state\n\n\ndef validate_args(args):\n    pytorch_translate_options.check_unsupported_fairseq_flags(args)\n    pytorch_translate_options.validate_preprocessing_args(args)\n    pytorch_translate_options.validate_generation_args(args)\n\n\ndef set_default_args(args):\n    # Prevents generate from printing individual translated sentences when\n    # calculating BLEU score.\n    args.quiet = True\n\n    # Set default init method for multi-GPU training if the user didn\'t specify\n    # them.\n    if args.distributed_world_size > 1:\n        args.distributed_init_method = (\n            f""tcp://localhost:{random.randint(10000, 20000)}""\n            if not args.distributed_init_method\n            else args.distributed_init_method\n        )\n\n        if args.local_num_gpus > args.distributed_world_size:\n            raise ValueError(\n                f""--local-num-gpus={args.local_num_gpus} must be ""\n                f""<= --distributed-world-size={args.distributed_world_size}.""\n            )\n        if args.local_num_gpus > torch.cuda.device_count():\n            raise ValueError(\n                f""--local-num-gpus={args.local_num_gpus} must be ""\n                f""<= the number of GPUs: {torch.cuda.device_count()}.""\n            )\n\n    if args.fp16 and getattr(args, ""adversary"", False):\n        print(\n            ""Warning: disabling fp16 training since it\'s not supported by AdversarialTrainer.""\n        )\n        args.fp16 = False\n\n    if not args.source_vocab_file:\n        args.source_vocab_file = pytorch_translate_dictionary.default_dictionary_path(\n            save_dir=args.save_dir, dialect=args.source_lang\n        )\n    if not args.target_vocab_file:\n        args.target_vocab_file = pytorch_translate_dictionary.default_dictionary_path(\n            save_dir=args.save_dir, dialect=args.target_lang\n        )\n\n    if args.arch in constants.ARCHS_FOR_CHAR_SOURCE and not args.char_source_vocab_file:\n        args.char_source_vocab_file = pytorch_translate_dictionary.default_char_dictionary_path(\n            save_dir=args.save_dir, dialect=args.source_lang\n        )\n    if args.arch in constants.ARCHS_FOR_CHAR_TARGET and not args.char_target_vocab_file:\n        args.char_target_vocab_file = pytorch_translate_dictionary.default_char_dictionary_path(\n            save_dir=args.save_dir, dialect=args.target_lang\n        )\n\n    if args.multiling_encoder_lang and not args.multiling_source_vocab_file:\n        args.multiling_source_vocab_file = [\n            pytorch_translate_dictionary.default_dictionary_path(\n                save_dir=args.save_dir, dialect=f""src-{l}""\n            )\n            for l in args.multiling_encoder_lang\n        ]\n    if args.multiling_decoder_lang and not args.multiling_target_vocab_file:\n        args.multiling_target_vocab_file = [\n            pytorch_translate_dictionary.default_dictionary_path(\n                save_dir=args.save_dir, dialect=f""trg-{l}""\n            )\n            for l in args.multiling_decoder_lang\n        ]\n\n\ndef validate_and_set_default_args(args):\n    set_default_args(args)\n    validate_args(args)\n\n\ndef create_task_and_model(args):\n    task = tasks.setup_task(args)\n    model = task.build_model(args)\n    return task, model\n\n\ndef get_num_model_params(model):\n    """"""Get the total number of parameters in a model""""""\n    return sum(p.numel() for p in model.parameters())\n\n\ndef setup_training_model(args):\n    """"""Parse args, load dataset, and build model with criterion.""""""\n    if not torch.cuda.is_available():\n        print(""Warning: training without CUDA is likely to be slow!"")\n    else:\n        torch.cuda.set_device(args.device_id)\n    torch.manual_seed(args.seed)\n\n    # Setup task and load dataset\n    task = tasks.setup_task(args)\n\n    # Build model and criterion\n    model = task.build_model(args)\n    print(""| building criterion"")\n    criterion = task.build_criterion(args)\n    print(f""| model {args.arch}, criterion {criterion.__class__.__name__}"")\n    print(\n        f""| num. model params: \\\n        {sum(p.numel() for p in model.parameters())}""\n    )\n\n    if args.task == constants.SEMI_SUPERVISED_TASK:\n        # TODO(T35638969): hide this inside the task itself, just use self.args\n        task.load_dataset(\n            split=args.train_subset,\n            src_bin_path=args.train_source_binary_path,\n            tgt_bin_path=args.train_target_binary_path,\n            forward_model=task.forward_model,\n            backward_model=task.backward_model,\n        )\n    elif args.task == ""pytorch_translate_denoising_autoencoder"":\n        task.load_dataset(\n            split=args.train_subset,\n            src_bin_path=args.train_source_binary_path,\n            tgt_bin_path=args.train_target_binary_path,\n            seed=args.seed,\n            use_noiser=True,\n        )\n    elif args.task == ""dual_learning_task"":\n        task.load_dataset(split=args.train_subset, seed=args.seed)\n    elif args.task == ""pytorch_translate_knowledge_distillation"":\n        task.load_dataset(\n            split=args.train_subset,\n            src_bin_path=args.train_source_binary_path,\n            tgt_bin_path=args.train_target_binary_path,\n            weights_file=getattr(args, ""train_weights_path"", None),\n            is_train=True,\n        )\n    elif args.task == ""pytorch_translate_cross_lingual_lm"":\n        task.load_dataset(args.train_subset, combine=True, epoch=0)\n    elif args.task == ""pytorch_translate"":\n        # Support both single and multi path loading for now\n        task.load_dataset(\n            split=args.train_subset,\n            src_bin_path=args.train_source_binary_path,\n            tgt_bin_path=args.train_target_binary_path,\n            weights_file=getattr(args, ""train_weights_path"", None),\n            is_npz=not args.fairseq_data_format,\n        )\n    else:\n        # Support both single and multi path loading for now\n        task.load_dataset(\n            split=args.train_subset,\n            src_bin_path=args.train_source_binary_path,\n            tgt_bin_path=args.train_target_binary_path,\n            weights_file=getattr(args, ""train_weights_path"", None),\n        )\n\n    if args.task == ""dual_learning_task"":\n        task.load_dataset(split=args.valid_subset, seed=args.seed)\n    elif args.task == ""pytorch_translate_cross_lingual_lm"":\n        task.load_dataset(args.valid_subset, combine=True, epoch=0)\n    elif args.task == ""pytorch_translate"":\n        task.load_dataset(\n            split=args.valid_subset,\n            src_bin_path=args.eval_source_binary_path,\n            tgt_bin_path=args.eval_target_binary_path,\n            is_npz=not args.fairseq_data_format,\n        )\n    else:\n        task.load_dataset(\n            split=args.valid_subset,\n            src_bin_path=args.eval_source_binary_path,\n            tgt_bin_path=args.eval_target_binary_path,\n        )\n\n    return task, model, criterion\n\n\ndef setup_training_state(args, trainer, task, epoch_itr):\n    """"""Set up the directory for saving checkpoints.\n    Load pretrained model if specified.""""""\n    PathManager.mkdirs(args.save_dir)\n\n    # If --restore-file is already present under --save-dir, use that one\n    # instead of --pretrained-checkpoint-file. The idea is that\n    # --pretrained-checkpoint-file allows the user to specify restoring from a\n    # different run\'s checkpoint (possibly with different training params),\n    # while not polluting the previous run\'s checkpoint directory\n    # with new checkpoints. However, if training gets interrupted\n    # and the user restarts training, we want to resume from\n    # the checkpoints under --save-dir, instead of\n    # restarting again from the old run\'s checkpoint at\n    # --pretrained-checkpoint-file.\n    #\n    # Note that if args.restore_file is an absolute path, os.path.join() will\n    # ignore previous directory args and just use the absolute path as is.\n    checkpoint_path = os.path.join(args.save_dir, args.restore_file)\n    restore_state = True\n    if PathManager.isfile(checkpoint_path):\n        print(\n            f""| Using --save-dir={args.save_dir}, --restore-file={args.restore_file}.""\n        )\n    elif args.pretrained_checkpoint_file and PathManager.isfile(\n        args.pretrained_checkpoint_file\n    ):\n        checkpoint_path = args.pretrained_checkpoint_file\n        restore_state = args.load_pretrained_checkpoint_state\n        print(\n            f""| Using --pretrained-checkpoint-file={args.pretrained_checkpoint_file}, ""\n            f""--load-pretrained-checkpoint-state={args.load_pretrained_checkpoint_state}.""\n        )\n\n    extra_state = default_extra_state(args)\n    if not PathManager.isfile(checkpoint_path) and args.multi_model_restore_files:\n        print(f""| Restoring individual models from {args.multi_model_restore_files}"")\n        multi_model.import_individual_models(args.multi_model_restore_files, trainer)\n    else:\n        loaded, loaded_extra_state = checkpoint.load_existing_checkpoint(\n            checkpoint_path=checkpoint_path,\n            trainer=trainer,\n            restore_state=restore_state,\n        )\n        if loaded_extra_state:\n            extra_state.update(loaded_extra_state)\n\n    # Reset the start time for the current training run.\n    extra_state[""start_time""] = time.time()\n\n    # Skips printing all training progress to prevent log spam.\n    training_progress = extra_state[""training_progress""]\n    extra_state[""training_progress""] = (\n        [""...truncated..."", training_progress[-1]] if len(training_progress) > 0 else []\n    )\n    print(f""| extra_state: {extra_state}"")\n    extra_state[""training_progress""] = training_progress\n\n    epoch = extra_state[""epoch""]\n    if extra_state[""batch_offset""] == 0:\n        epoch -= 1  # this will be incremented when we call epoch_itr.next_epoch_itr()\n    epoch_itr.load_state_dict(\n        {""epoch"": epoch, ""iterations_in_epoch"": extra_state[""batch_offset""]}\n    )\n\n    checkpoint_manager = None\n    if distributed_utils.is_master(args):\n        checkpoint_manager = checkpoint.CheckpointManager(\n            num_avg_checkpoints=args.num_avg_checkpoints,\n            auto_clear_checkpoints=args.auto_clear_checkpoints,\n            log_verbose=args.log_verbose,\n            checkpoint_files=extra_state[""checkpoint_files""],\n        )\n\n    return extra_state, epoch_itr, checkpoint_manager\n\n\ndef build_trainer(args, task, model, criterion, trainer_class):\n    """""" Build trainer with provided trainer_class, and set up training state.\n    """"""\n    # Build trainer\n    trainer = trainer_class(args, task, model, criterion)\n\n    print(\n        f""| training on {args.distributed_world_size} total GPUs ""\n        f""({torch.cuda.device_count()} GPUs locally on this machine).\\n""\n        f""| max tokens per GPU = {args.max_tokens} and \\\n        max sentences per GPU = {args.max_sentences}"",\n        flush=True,\n    )\n\n    epoch_itr = task.get_batch_iterator(\n        dataset=task.dataset(args.train_subset),\n        max_tokens=args.max_tokens,\n        max_sentences=args.max_sentences,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), model.max_positions()\n        ),\n        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=8,\n        seed=args.seed,\n        num_shards=args.distributed_world_size,\n        shard_id=args.distributed_rank,\n        num_workers=args.num_workers,\n    )\n    return trainer, epoch_itr\n\n\ndef setup_training(args, trainer_class=None):\n    """""" Perform several steps:\n    - build model using provided criterion and task\n    - load data\n    - build trainer\n    """"""\n\n    # Overrides the default print() to always prepend the timestamp for more\n    # informative logging.\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        if ""file"" not in kwargs:\n            builtin_print(\n                f""[{datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S.%f\')}]"",\n                *args,\n                **kwargs,\n            )\n        else:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n    task, model, criterion = setup_training_model(args)\n    if trainer_class is None:\n        trainer_class = Trainer\n\n    trainer, epoch_itr = build_trainer(\n        args=args,\n        task=task,\n        model=model,\n        criterion=criterion,\n        trainer_class=trainer_class,\n    )\n\n    return trainer, task, epoch_itr\n\n\ndef create_prune_masks(args, trainer):\n    """"""Generates binary masks for setting model weights to zero""""""\n    assert (\n        args.pruning_percentile > 0 and args.pruning_percentile < 100\n    ), ""--pruning-percentile must be in (0, 100)""\n    all_params = []\n    if args.parameters_to_prune == ""all"":\n        parameter_name = ""weight""\n    elif args.parameters_to_prune == ""embed"":\n        parameter_name = ""embed_tokens""\n    elif args.parameters_to_prune == ""lstm"":\n        parameter_name = ""weight_""\n    for name, params in trainer.model.named_parameters():\n        if parameter_name in name:\n            all_params.append(np.abs(np.reshape(params.data, (-1, 1))))\n    threshold = np.percentile(np.vstack(all_params), args.pruning_percentile)\n\n    prune_masks = {}\n    for name, params in trainer.model.named_parameters():\n        if parameter_name in name:\n            prune_masks[name] = np.abs(params.data) < threshold\n\n    return prune_masks\n\n\ndef apply_prune_masks(prune_masks, trainer):\n    """"""Selectively sets model weights to zero using a binary mask.""""""\n\n    for name, params in trainer.model.named_parameters():\n        if name in prune_masks:\n            params.data[prune_masks[name]] = 0.0\n\n\ndef train(\n    args,\n    extra_state: Dict[str, Any],\n    trainer,\n    task,\n    epoch_itr,\n    checkpoint_manager: Optional[checkpoint.CheckpointManager],\n    output_queue: Optional[mp_queues.Queue] = None,\n    **train_step_kwargs,\n):\n    # offset for current epoch (may be different from checkpoint offset)\n    starting_offset = extra_state[""batch_offset""]\n\n    # Train until the learning rate gets too small\n    max_epoch = args.max_epoch or math.inf\n    lr = trainer.get_lr()\n    train_meter = StopwatchMeter()\n    train_meter.start()\n    stop_training_mid_epoch = False\n    stop_training_end_of_epoch = False\n\n    do_prune = args.pruning_percentile > 0\n    if do_prune:\n        prune_masks = create_prune_masks(args, trainer)\n        apply_prune_masks(prune_masks, trainer)\n\n    while lr > args.min_lr and extra_state[""epoch""] <= max_epoch:\n        """"""Train the model for one epoch.""""""\n\n        itr, progress, extra_meters = setup_epoch(\n            args=args, epoch_itr=epoch_itr, trainer=trainer\n        )\n\n        for i, samples in enumerate(progress, start=starting_offset):\n            clear_per_step_extra_state(extra_state)\n            extra_state[""num_iterations""] = extra_state.get(""num_iterations"", 0) + 1\n            if (\n                train_step_kwargs is not None\n                and ""augment_adv"" in train_step_kwargs.keys()\n            ):\n                train_step_kwargs[""augment_adv""] = (\n                    extra_state[""num_iterations""] > args.warmup_steps\n                )\n            try:\n                log_output = trainer.train_step(samples, **train_step_kwargs)\n            # Fairseq\'s fp16_trainer raises this uncommon error to indicate\n            # that we should stop training.\n            except FloatingPointError as e:\n                print(f""Stopping training due to: {e}."")\n                stop_training_mid_epoch = True\n                break\n\n            if do_prune:\n                apply_prune_masks(prune_masks, trainer)\n\n            if i == starting_offset:\n                # ignore the first mini-batch in words-per-second calculation\n                trainer.get_meter(""wps"").reset()\n\n            # Clear any remaining metrics from previous steps. This should already\n            # have been done before, but just in case - to make sure we catch\n            # any case where extra_case does not get populated correctly.\n            extra_state = clear_per_step_extra_state(extra_state)\n            extra_state[""batch_offset""] = i + 1\n            extra_state, stop_training_mid_epoch = evals.save_and_eval(\n                args=args,\n                trainer=trainer,\n                task=task,\n                extra_state=extra_state,\n                checkpoint_manager=checkpoint_manager,\n            )\n\n            # This should come after save_and_eval. Even if log_output is None,\n            # meaning that there was an overflow,  We should still run\n            # save_and_eval to sync all_reduce and then skip the batch.\n            if log_output is None:\n                # This indicates that the batch was skipped, typically\n                # because of OOM or FP16 overflow.\n                continue\n\n            train_stats = evals.log_mid_epoch_stats(\n                trainer=trainer,\n                progress=progress,\n                extra_meters=extra_meters,\n                log_output=log_output,\n            )\n            extra_state = update_output(\n                args=args,\n                extra_state=extra_state,\n                output_queue=output_queue,\n                num_updates=trainer.get_num_updates(),\n                train_ppl=train_stats[""ppl""],\n                # We only report wps at the end of an epoch, since\n                # the meter gets reset at the start of every epoch.\n                wps=None,\n            )\n\n            if (\n                hasattr(args, ""lr_shrink"")\n                and args.save_interval_updates > 0\n                and extra_state[""num_iterations""] % args.save_interval_updates == 0\n                and args.shrink_lr_no_best_tune_loss > 0\n                and extra_state[""tune_eval""][""num_since_best""]\n                > args.shrink_lr_no_best_tune_loss\n            ):\n                current_lr = trainer.optimizer.get_lr()\n                trainer.optimizer.set_lr(current_lr * args.lr_shrink)\n                lr = trainer.optimizer.get_lr()\n                print(f""Decayed lr from {current_lr} to {lr}."")\n\n            if stop_training_mid_epoch:\n                break\n\n        # log end-of-epoch stats\n        train_stats = evals.log_end_epoch_stats(\n            trainer=trainer, progress=progress, extra_meters=extra_meters\n        )\n\n        # batch_offset being None denotes the end of an epoch.\n        extra_state[""batch_offset""] = None\n        extra_state, stop_training_end_of_epoch = evals.save_and_eval(\n            args=args,\n            trainer=trainer,\n            task=task,\n            extra_state=extra_state,\n            end_of_epoch=True,\n            checkpoint_manager=checkpoint_manager,\n        )\n        extra_state = update_output(\n            args=args,\n            extra_state=extra_state,\n            output_queue=output_queue,\n            num_updates=trainer.get_num_updates(),\n            train_ppl=train_stats[""ppl""],\n            wps=train_stats[""wps""],\n        )\n\n        if stop_training_mid_epoch or stop_training_end_of_epoch:\n            break\n\n        lr = trainer.lr_step(extra_state[""epoch""], extra_state[""tune_eval""][""loss""])\n        extra_state[""epoch""] += 1\n        extra_state[""batch_offset""] = 0\n        starting_offset = 0\n\n    train_meter.stop()\n    print(f""| done training in {train_meter.sum:.1f} seconds"")\n\n    # the checkpoint manager may be None\n    if checkpoint_manager:\n        checkpoint_manager.remove_all_checkpoints()\n\n\ndef setup_epoch(args, epoch_itr, trainer):\n    """"""Sets up data and progress meters for one epoch.""""""\n    # Update parameters every N batches\n    if epoch_itr.epoch <= len(args.update_freq):\n        update_freq = args.update_freq[epoch_itr.epoch - 1]\n    else:\n        update_freq = args.update_freq[-1]\n\n    # Initialize dataloader, starting at batch_offset\n    itr = epoch_itr.next_epoch_itr()\n    itr = data.iterators.GroupedIterator(itr, update_freq)\n    progress = progress_bar.build_progress_bar(\n        args, itr, epoch_itr.epoch, no_progress_bar=""simple""\n    )\n\n    # reset training meters\n    for k in [\n        ""train_loss"",\n        ""train_nll_loss"",\n        ""wps"",\n        ""ups"",\n        ""wpb"",\n        ""bsz"",\n        ""gnorm"",\n        ""clip"",\n    ]:\n        meter = trainer.get_meter(k)\n        if meter is not None:\n            meter.reset()\n\n    extra_meters = collections.defaultdict(lambda: AverageMeter())\n    return itr, progress, extra_meters\n\n\ndef single_process_main(args, trainer_class=Trainer, **train_step_kwargs):\n    """"""Train the model for multiple epochs.""""""\n    pytorch_translate_options.print_args(args)\n    trainer, task, epoch_itr = setup_training(args, trainer_class)\n    extra_state, epoch_itr, checkpoint_manager = setup_training_state(\n        args=args, trainer=trainer, task=task, epoch_itr=epoch_itr\n    )\n    train(\n        args=args,\n        extra_state=extra_state,\n        trainer=trainer,\n        task=task,\n        epoch_itr=epoch_itr,\n        checkpoint_manager=checkpoint_manager,\n        **train_step_kwargs,\n    )\n\n\ndef multi_process_train(\n    device_id: int,\n    args,\n    output_queue: Optional[mp_queues.Queue],\n    start_rank: int = 0,\n    init_fn: Optional[Callable[[], None]] = None,\n    trainer_class=None,\n    train_step_kwargs=None,\n):\n    # Enable faulthandler for better Python tracebacks upon segfaults under\n    # multiprocessing. Without this, the stack trace only shows the\n    # SpawnContext.join() call, rather than the actual line where the child\n    # process segfaulted.\n    faulthandler.enable(all_threads=True)\n\n    if init_fn:\n        init_fn()\n    args.device_id = device_id\n    args.distributed_rank = start_rank + device_id\n\n    if torch.cuda.is_available():\n        torch.cuda.set_device(args.device_id)\n\n    trainer, task, epoch_itr = setup_training(args, trainer_class)\n    # Distributed_init does initialization and works as a barrier.\n    # Therefore, any expensive data preprocessing should happen before.\n    if args.distributed_world_size > 1:\n        args.distributed_rank = distributed_utils.distributed_init(args)\n    extra_state, epoch_itr, checkpoint_manager = setup_training_state(\n        args=args, trainer=trainer, task=task, epoch_itr=epoch_itr\n    )\n\n    # Replay previous training progress so the output_queue contains all\n    # previous training progress even when we resume training from an existing\n    # checkpoint.\n    if distributed_utils.is_master(args) and output_queue is not None:\n        for progress_output in extra_state[""training_progress""]:\n            output_queue.put_nowait(progress_output)\n\n    train(\n        args=args,\n        extra_state=extra_state,\n        trainer=trainer,\n        task=task,\n        epoch_itr=epoch_itr,\n        checkpoint_manager=checkpoint_manager,\n        output_queue=output_queue,\n        **train_step_kwargs,\n    )\n\n\ndef multi_process_main(\n    args: Any,\n    start_rank: int = 0,\n    init_fn: Optional[Callable[[], None]] = None,\n    trainer_class=None,\n    **train_step_kwargs,\n):\n    pytorch_translate_options.print_args(args)\n    output_queue = torch.multiprocessing.get_context(""spawn"").Queue()\n    # Train with multiprocessing.\n    spawn_context = torch.multiprocessing.spawn(\n        fn=multi_process_train,\n        args=(\n            args,\n            output_queue,\n            start_rank,\n            init_fn,\n            trainer_class,\n            train_step_kwargs,\n        ),\n        nprocs=args.local_num_gpus,\n        # We don\'t block here to allow caller to process output_queue in\n        # parallel with training.\n        join=False,\n    )\n    return (spawn_context, output_queue)\n\n\ndef main(args, trainer_class=Trainer, **train_step_kwargs):\n    # We preprocess the data (generating vocab files and binarized data files\n    # if needed) outside of the train processes to prevent them from having to\n    # wait while the master process is doing this.\n    preprocess.preprocess_corpora(args)\n\n    if args.distributed_world_size == 1:\n        single_process_main(args, trainer_class, **train_step_kwargs)\n    else:\n        spawn_context, output_queue = multi_process_main(args=args, start_rank=0)\n\n        while not spawn_context.join(timeout=30):\n            # Periodically clears the output queue to ensure that the processes\n            # don\'t deadlock due to queue buffer being full. This is also\n            # necessary to ensure that processes join correctly, since a process\n            # may not terminate until all items it put on the queue have been\n            # consumed (per\n            # https://docs.python.org/3/library/multiprocessing.html#all-start-methods).\n            try:\n                while True:\n                    output_queue.get_nowait()\n            except queue.Empty:\n                pass\n\n\ndef _main():\n    parser = get_parser_with_args()\n    args = options.parse_args_and_arch(parser)\n    validate_and_set_default_args(args)\n    main(args)\n\n\nif __name__ == ""__main__"":\n    _main()\n'"
pytorch_translate/transformer.py,21,"b'#!/usr/bin/env python3\n\nimport math\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqEncoderDecoderModel,\n    FairseqIncrementalDecoder,\n    register_model,\n    register_model_architecture,\n    transformer as fairseq_transformer,\n)\nfrom fairseq.modules import (\n    AdaptiveSoftmax,\n    LayerNorm,\n    MultiheadAttention,\n    SinusoidalPositionalEmbedding,\n)\nfrom pytorch_translate import utils as pytorch_translate_utils, vocab_reduction\nfrom pytorch_translate.average_attention import AverageAttention\nfrom pytorch_translate.common_layers import (\n    TransformerEmbedding,\n    TransformerEncoderGivenEmbeddings,\n    TransformerTokenEmbedding,\n    VariableTracker,\n)\nfrom pytorch_translate.semi_supervised import SemiSupervisedModel\nfrom pytorch_translate.utils import torch_find\n\n\ndef build_embedding(dictionary, embed_dim, path=None, freeze=False):\n    num_embeddings = len(dictionary)\n    padding_idx = dictionary.pad()\n    emb = TransformerTokenEmbedding(num_embeddings, embed_dim, padding_idx, freeze)\n    # if provided, load from preloaded dictionaries\n    if path:\n        embed_dict = utils.parse_embedding(path)\n        utils.load_embedding(embed_dict, dictionary, emb)\n    return emb\n\n\n@register_model(""ptt_transformer"")\nclass TransformerModel(FairseqEncoderDecoderModel):\n    def __init__(self, task, encoder, decoder):\n        super().__init__(encoder, decoder)\n        self.task = task\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--dropout"", type=float, metavar=""D"", help=""dropout probability""\n        )\n        parser.add_argument(\n            ""--attention-dropout"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability for attention weights"",\n        )\n        parser.add_argument(\n            ""--relu-dropout"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability after ReLU in FFN"",\n        )\n        parser.add_argument(\n            ""--encoder-pretrained-embed"",\n            type=str,\n            metavar=""STR"",\n            help=""path to pre-trained encoder embedding"",\n        )\n        parser.add_argument(\n            ""--encoder-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""encoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--encoder-ffn-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""encoder embedding dimension for FFN"",\n        )\n        parser.add_argument(\n            ""--encoder-freeze-embed"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether to freeze the encoder embedding or allow it to be ""\n                ""updated during training""\n            ),\n        )\n        parser.add_argument(\n            ""--encoder-layers"", type=int, metavar=""N"", help=""num encoder layers""\n        )\n        parser.add_argument(\n            ""--encoder-attention-heads"",\n            type=int,\n            metavar=""N"",\n            help=""num encoder attention heads"",\n        )\n        parser.add_argument(\n            ""--encoder-normalize-before"",\n            default=False,\n            action=""store_true"",\n            help=""apply layernorm before each encoder block"",\n        )\n        parser.add_argument(\n            ""--encoder-learned-pos"",\n            default=False,\n            action=""store_true"",\n            help=""use learned positional embeddings in the encoder"",\n        )\n        parser.add_argument(\n            ""--decoder-pretrained-embed"",\n            type=str,\n            metavar=""STR"",\n            help=""path to pre-trained decoder embedding"",\n        )\n        parser.add_argument(\n            ""--decoder-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""decoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--decoder-ffn-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""decoder embedding dimension for FFN"",\n        )\n        parser.add_argument(\n            ""--decoder-freeze-embed"",\n            default=False,\n            action=""store_true"",\n            help=(\n                ""whether to freeze the encoder embedding or allow it to be ""\n                ""updated during training""\n            ),\n        )\n        parser.add_argument(\n            ""--decoder-layers"", type=int, metavar=""N"", help=""num decoder layers""\n        )\n        parser.add_argument(\n            ""--decoder-attention-heads"",\n            type=int,\n            metavar=""N"",\n            help=""num decoder attention heads"",\n        )\n        parser.add_argument(\n            ""--decoder-learned-pos"",\n            default=False,\n            action=""store_true"",\n            help=""use learned positional embeddings in the decoder"",\n        )\n        parser.add_argument(\n            ""--decoder-normalize-before"",\n            default=False,\n            action=""store_true"",\n            help=""apply layernorm before each decoder block"",\n        )\n        parser.add_argument(\n            ""--decoder-layerdrop"",\n            type=float,\n            metavar=""D"",\n            default=0,\n            help=""LayerDrop probability for decoder"",\n        )\n        parser.add_argument(\n            ""--decoder-layers-to-keep"",\n            default=None,\n            help=""which layers to *keep* when pruning as a comma-separated list"",\n        )\n        parser.add_argument(\n            ""--share-decoder-input-output-embed"",\n            default=False,\n            action=""store_true"",\n            help=""share decoder input and output embeddings"",\n        )\n        parser.add_argument(\n            ""--share-all-embeddings"",\n            default=False,\n            action=""store_true"",\n            help=""share encoder, decoder and output embeddings""\n            "" (requires shared dictionary and embed dim)"",\n        )\n        parser.add_argument(\n            ""--adaptive-softmax-cutoff"",\n            default=None,\n            metavar=""EXPR"",\n            help=""comma separated list of adaptive softmax cutoff points. ""\n            ""Must be used with adaptive_loss criterion"",\n        )\n        parser.add_argument(\n            ""--decoder-out-embed-dim"",\n            default=None,\n            type=int,\n            metavar=""N"",\n            help=""decoder output embedding dimension (bottleneck layer before""\n            ""output layer if specified.)"",\n        )\n        parser.add_argument(\n            ""--aan"",\n            default=False,\n            action=""store_true"",\n            help=""use average attention network (AAN) instead of decoder ""\n            ""self-attention"",\n        )\n\n        # Args for vocab reduction\n        vocab_reduction.add_args(parser)\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure that all args are properly defaulted\n        # (in case there are any new ones)\n        base_architecture(args)\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        if args.share_all_embeddings:\n            if src_dict != tgt_dict:\n                raise RuntimeError(\n                    ""--share-all-embeddings requires a joined dictionary""\n                )\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise RuntimeError(\n                    ""--share-all-embeddings requires --encoder-embed-dim ""\n                    ""to match --decoder-embed-dim""\n                )\n            if args.decoder_pretrained_embed and (\n                args.decoder_pretrained_embed != args.encoder_pretrained_embed\n            ):\n                raise RuntimeError(\n                    ""--share-all-embeddings not compatible with ""\n                    ""--decoder-pretrained-embed""\n                )\n            encoder_embed_tokens = build_embedding(\n                dictionary=src_dict,\n                embed_dim=args.encoder_embed_dim,\n                path=args.encoder_pretrained_embed,\n                freeze=args.encoder_freeze_embed,\n            )\n            decoder_embed_tokens = encoder_embed_tokens\n            args.share_decoder_input_output_embed = True\n        else:\n            encoder_embed_tokens = build_embedding(\n                dictionary=src_dict,\n                embed_dim=args.encoder_embed_dim,\n                path=args.encoder_pretrained_embed,\n                freeze=args.encoder_freeze_embed,\n            )\n            decoder_embed_tokens = build_embedding(\n                dictionary=tgt_dict,\n                embed_dim=args.decoder_embed_dim,\n                path=args.decoder_pretrained_embed,\n                freeze=args.decoder_freeze_embed,\n            )\n\n        encoder = TransformerModel.build_encoder(\n            args, src_dict, embed_tokens=encoder_embed_tokens\n        )\n        decoder = TransformerModel.build_decoder(\n            args, src_dict, tgt_dict, embed_tokens=decoder_embed_tokens\n        )\n        return TransformerModel(task, encoder, decoder)\n\n    def get_targets(self, sample, net_output):\n        targets = sample[""target""].view(-1)\n        possible_translation_tokens = net_output[-1]\n        if possible_translation_tokens is not None:\n            targets = torch_find(\n                possible_translation_tokens, targets, len(self.task.target_dictionary)\n            )\n        return targets\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return TransformerEncoder(args, src_dict, embed_tokens=embed_tokens)\n\n    @classmethod\n    def build_decoder(cls, args, src_dict, dst_dict, embed_tokens):\n        return TransformerDecoder(args, src_dict, dst_dict, embed_tokens=embed_tokens)\n\n\nclass TransformerEncoder(FairseqEncoder):\n    """"""Transformer encoder.""""""\n\n    def __init__(self, args, dictionary, embed_tokens, proj_to_decoder=True):\n        super().__init__(dictionary)\n        self.transformer_embedding = TransformerEmbedding(\n            args=args, embed_tokens=embed_tokens\n        )\n\n        self.transformer_encoder_given_embeddings = TransformerEncoderGivenEmbeddings(\n            args=args, proj_to_decoder=proj_to_decoder\n        )\n\n        # Variable tracker\n        self.tracker = VariableTracker()\n\n        # Initialize adversarial mode\n        self.set_gradient_tracking_mode(False)\n        self.set_embed_noising_mode(False)\n\n    def forward(self, src_tokens, src_lengths):\n        # Initialize the tracker to keep track of internal variables\n        self.tracker.reset()\n        x, encoder_padding_mask, positions = self.transformer_embedding(\n            src_tokens=src_tokens, src_lengths=src_lengths\n        )\n        # Track token embeddings\n        self.tracker.track(x, ""token_embeddings"", retain_grad=self.track_gradients)\n\n        x = self.transformer_encoder_given_embeddings(\n            x=x, positions=positions, encoder_padding_mask=encoder_padding_mask\n        )\n\n        # TODO(jamesreed): this is kinda a hack because we can\'t annotate an\n        # Optional[Tensor] output for encoder_padding_mask\n        if encoder_padding_mask is None:\n            encoder_padding_mask = torch.empty([])\n\n        return x, src_tokens, encoder_padding_mask\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        (x, src_tokens, encoder_padding_mask) = encoder_out\n        src_tokens_tensor = pytorch_translate_utils.get_source_tokens_tensor(src_tokens)\n        if x is not None:\n            x = x.index_select(1, new_order)\n        if src_tokens_tensor is not None:\n            src_tokens_tensor = src_tokens_tensor.index_select(0, new_order)\n        if encoder_padding_mask.shape == torch.Size([]):\n            encoder_padding_mask = None\n        if encoder_padding_mask is not None:\n            encoder_padding_mask = encoder_padding_mask.index_select(0, new_order)\n        return (x, src_tokens_tensor, encoder_padding_mask)\n\n    def max_positions(self):\n        """"""Maximum input length supported by the encoder.""""""\n        return self.transformer_embedding.embed_positions.max_positions\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        if isinstance(\n            self.transformer_embedding.embed_positions, SinusoidalPositionalEmbedding\n        ):\n            if f""{name}.transformer_embedding.embed_positions.weights"" in state_dict:\n                del state_dict[f""{name}.transformer_embedding.embed_positions.weights""]\n            state_dict[\n                f""{name}.transformer_embedding.embed_positions._float_tensor""\n            ] = torch.FloatTensor(1)\n        self.transformer_encoder_given_embeddings.upgrade_state_dict_named(\n            state_dict, f""{name}.transformer_encoder_given_embeddings""\n        )\n        return state_dict\n\n    def set_gradient_tracking_mode(self, mode=True):\n        self.tracker.reset()\n        self.track_gradients = mode\n\n    def set_embed_noising_mode(self, mode=True):\n        """"""This allows adversarial trainer to turn on and off embedding noising\n        layers. In regular training, this mode is off, and it is not included\n        in forward pass.\n        """"""\n        self.embed_noising_mode = mode\n\n\nclass TransformerDecoder(FairseqIncrementalDecoder):\n    """"""Transformer decoder.""""""\n\n    def __init__(self, args, src_dict, dst_dict, embed_tokens):\n        super().__init__(dst_dict)\n        self.dropout = args.dropout\n        self.decoder_layerdrop = 0\n        if hasattr(args, ""decoder_layerdrop"") and args.decoder_layerdrop > 0:\n            self.decoder_layerdrop = args.decoder_layerdrop\n\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n\n        embed_dim = embed_tokens.embedding_dim\n        padding_idx = embed_tokens.padding_idx\n\n        self.embed_tokens = embed_tokens\n        self.embed_scale = math.sqrt(embed_dim)\n        self.embed_positions = fairseq_transformer.PositionalEmbedding(\n            1024, embed_dim, padding_idx, learned=args.decoder_learned_pos\n        )\n\n        self.aan = args.aan\n        decoder_layer_class = (\n            AANDecoderLayer if self.aan else fairseq_transformer.TransformerDecoderLayer\n        )\n\n        self.layers = nn.ModuleList([])\n        self.layers.extend(\n            [decoder_layer_class(args) for i in range(args.decoder_layers)]\n        )\n        if hasattr(args, ""decoder_layers_to_keep"") and args.decoder_layers_to_keep:\n            layers_to_keep = sorted(\n                int(x) for x in args.decoder_layers_to_keep.split("","")\n            )\n            self.decoder_layers_to_keep = {\n                layer_id: layer_idx for layer_idx, layer_id in enumerate(layers_to_keep)\n            }\n\n        self.adaptive_softmax = None\n\n        self.bottleneck_layer = None\n        out_embed_dim = embed_dim\n        if args.decoder_out_embed_dim is not None:\n            assert (\n                not args.share_all_embeddings\n                and not args.share_decoder_input_output_embed\n            ), ""--decoder-out-embed-dim is incompatible with sharing output embeddings!""\n            self.bottleneck_layer = fairseq_transformer.Linear(\n                embed_dim, args.decoder_out_embed_dim\n            )\n            out_embed_dim = args.decoder_out_embed_dim\n\n        if args.adaptive_softmax_cutoff is not None:\n            self.adaptive_softmax = AdaptiveSoftmax(\n                len(dst_dict),\n                out_embed_dim,\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.dropout,\n            )\n        elif not self.share_input_output_embed:\n            self.embed_out = nn.Parameter(torch.Tensor(len(dst_dict), out_embed_dim))\n            nn.init.normal_(self.embed_out, mean=0, std=out_embed_dim ** -0.5)\n\n        self.vocab_reduction_module = None\n        if args.vocab_reduction_params:\n            assert (\n                self.adaptive_softmax is None\n            ), ""vocabulary reduction not compatible with adaptive softmax!""\n            self.vocab_reduction_module = vocab_reduction.VocabReduction(\n                src_dict, dst_dict, args.vocab_reduction_params, fp16=args.fp16\n            )\n\n        self.onnx_trace = False\n\n        # Use quantizable nn.Linear for output projection instead of F.linear\n        self.output_projection = None\n        if self.vocab_reduction_module is None:\n            if self.share_input_output_embed:\n                self.output_projection = nn.Linear(\n                    self.embed_tokens.weight.shape[1], self.embed_tokens.weight.shape[0]\n                )\n                self.output_projection.weight = self.embed_tokens.weight\n            else:\n                self.output_projection = nn.Linear(\n                    self.embed_out.shape[1], self.embed_out.shape[0]\n                )\n                self.output_projection.weight = self.embed_out\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def forward(\n        self,\n        prev_output_tokens,\n        encoder_out,\n        incremental_state=None,\n        possible_translation_tokens=None,\n        timestep=None,\n    ):\n        (encoder_x, src_tokens, encoder_padding_mask) = encoder_out\n\n        # embed positions\n        positions = self.embed_positions(\n            prev_output_tokens, incremental_state=incremental_state, timestep=timestep\n        )\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            positions = positions[:, -1:]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n        x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        # decoder layers\n        state_outputs = []  # onnx_trace only\n        attn = None\n\n        if self.onnx_trace:\n            for i, layer in enumerate(self.layers):\n                if hasattr(self, ""decoder_layers_to_keep""):\n                    if i not in self.decoder_layers_to_keep.keys():\n                        continue\n                    else:\n                        i = self.decoder_layers_to_keep[i]\n                # (prev_key, prev_value)\n                self_attn_input = incremental_state[4 * i : 4 * i + 2]\n                attn_state = incremental_state[4 * i + 2 : 4 * i + 4]\n                x, attn, self_attn_out = layer(\n                    x,\n                    encoder_x,\n                    encoder_padding_mask,\n                    incremental_state={},\n                    prev_self_attn_state=self_attn_input\n                    if len(incremental_state) > 0\n                    else None,\n                    prev_attn_state=attn_state if len(incremental_state) > 0 else None,\n                )\n                state_outputs.extend(self_attn_out)\n                state_outputs.extend(attn_state)  # unchanged\n        else:\n            for i, layer in enumerate(self.layers):\n                if hasattr(self, ""decoder_layers_to_keep""):\n                    dropout_probability = random.uniform(0, 1)\n                    if (\n                        self.training\n                        and dropout_probability < self.decoder_layerdrop\n                        or (\n                            not self.training\n                            and i not in self.decoder_layers_to_keep.keys()\n                        )\n                    ):\n                        continue\n                x, attn, _ = layer(\n                    x,\n                    encoder_x,\n                    encoder_padding_mask,\n                    incremental_state,\n                    self_attn_mask=self.buffered_future_mask(x)\n                    if incremental_state is None\n                    else None,\n                )\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.bottleneck_layer is not None:\n            x = self.bottleneck_layer(x)\n\n        if self.adaptive_softmax is not None:\n            return x, attn, None\n\n        # project back to size of vocabulary\n        if self.share_input_output_embed:\n            output_weights = self.embed_tokens.weight\n        else:\n            output_weights = self.embed_out\n\n        if (\n            self.vocab_reduction_module is not None\n            and possible_translation_tokens is None\n        ):\n            decoder_input_tokens = prev_output_tokens.contiguous()\n            possible_translation_tokens = self.vocab_reduction_module(\n                src_tokens, decoder_input_tokens=decoder_input_tokens\n            )\n        if possible_translation_tokens is not None:\n            output_weights = output_weights.index_select(\n                dim=0, index=possible_translation_tokens\n            )\n\n        # Use quantizable self.output_projection if vocab reduction is not used\n        if self.vocab_reduction_module is None:\n            logits = self.output_projection(x)\n        else:\n            logits = F.linear(x, output_weights)\n\n        if self.onnx_trace:\n            return logits, attn, possible_translation_tokens, state_outputs\n\n        return logits, attn, possible_translation_tokens\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        return self.embed_positions.max_positions\n\n    def buffered_future_mask(self, tensor):\n        dim = tensor.size(0)\n        if (\n            not hasattr(self, ""_future_mask"")\n            or self._future_mask is None\n            or self._future_mask.device != tensor.device\n        ):\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(tensor.new(dim, dim)), 1\n            )\n        if self._future_mask.size(0) < dim:\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(self._future_mask.resize_(dim, dim)), 1\n            )\n        return self._future_mask[:dim, :dim]\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            if f""{name}.embed_positions.weights"" in state_dict:\n                del state_dict[f""{name}.embed_positions.weights""]\n            state_dict[f""{name}.embed_positions._float_tensor""] = torch.FloatTensor(1)\n\n        if self.vocab_reduction_module is None:\n            prefix = name + ""."" if name != """" else """"\n            items_to_add = {}\n            keys_to_remove = []\n            for k in state_dict.keys():\n                if self.share_input_output_embed is True:\n                    if k.endswith(prefix + ""embed_tokens.weight""):\n                        items_to_add[prefix + ""output_projection.weight""] = state_dict[\n                            k\n                        ]\n                        bias = nn.Parameter(torch.Tensor(state_dict[k].shape[0]))\n                        nn.init.constant_(bias, 0.0)\n                        items_to_add[prefix + ""output_projection.bias""] = bias\n                else:\n                    if k.endswith(prefix + ""embed_out""):\n                        items_to_add[prefix + ""output_projection.weight""] = state_dict[\n                            k\n                        ]\n                        bias = nn.Parameter(torch.Tensor(state_dict[k].shape[0]))\n                        nn.init.constant_(bias, 0.0)\n                        items_to_add[prefix + ""output_projection.bias""] = bias\n\n            for k in keys_to_remove:\n                del state_dict[k]\n\n            for key, value in items_to_add.items():\n                state_dict[key] = value\n\n        return state_dict\n\n    def _init_prev_states(self, encoder_out):\n        """"""\n        For self-attention, initial (prev_key, prev_value) are dummy tensors\n        with a zero-size sequence dimension.\n        For encoder-decoder attention, key and value are computed once from\n        the encoder outputs and stay the same throughout decoding.\n        """"""\n        encoder_x, src_tokens, encoder_padding_mask = encoder_out\n        batch_size = torch.onnx.operators.shape_as_tensor(encoder_x)[1]\n        states = []\n        for i, layer in enumerate(self.layers):\n            if hasattr(self, ""decoder_layers_to_keep"") and (\n                i not in self.decoder_layers_to_keep.keys()\n            ):\n                continue\n            if self.aan:\n                # (bsz, channel)\n                prev_sum = torch.zeros([1, layer.avg_attn.embed_dim])\n                prev_pos = torch.zeros([1, 1])\n                states.extend([prev_sum, prev_pos])\n            else:\n                # dummy initial (prev_key, prev_value) for self-attention\n                for _ in range(2):\n                    dummy_state_shape = torch.cat(\n                        [\n                            batch_size.view(1),\n                            torch.LongTensor([layer.self_attn.num_heads]),\n                            torch.LongTensor([0]),\n                            torch.LongTensor([layer.self_attn.head_dim]),\n                        ]\n                    )\n                    dummy_state = torch.zeros(\n                        [1, layer.self_attn.num_heads, 0, layer.self_attn.head_dim]\n                    )\n                    reshaped_dummy_state = torch.onnx.operators.reshape_from_tensor_shape(\n                        dummy_state, dummy_state_shape\n                    )\n                    states.append(reshaped_dummy_state)\n\n            # (key, value) for encoder-decoder attention computed from encoder\n            # output and remain the same throughout decoding\n            key = layer.encoder_attn.k_proj(encoder_x)\n            value = layer.encoder_attn.v_proj(encoder_x)\n\n            # (key, value) kept in shape (bsz, num_heads, seq_len, head_dim)\n            # to avoid repeated transpose operations\n            seq_len, batch_size_int, _ = encoder_x.shape\n            num_heads = layer.encoder_attn.num_heads\n            head_dim = layer.encoder_attn.head_dim\n            key = (\n                key.view(seq_len, batch_size_int * num_heads, head_dim)\n                .transpose(0, 1)\n                .view(batch_size_int, num_heads, seq_len, head_dim)\n            )\n            value = (\n                value.view(seq_len, batch_size_int * num_heads, head_dim)\n                .transpose(0, 1)\n                .view(batch_size_int, num_heads, seq_len, head_dim)\n            )\n\n            states.extend([key, value])\n        return states\n\n\nclass AANDecoderLayer(nn.Module):\n    """"""\n    Based on https://arxiv.org/abs/1805.00631\n    """"""\n\n    def __init__(self, args):\n        super().__init__()\n        self.embed_dim = args.decoder_embed_dim\n        self.cross_self_attention = getattr(args, ""cross_self_attention"", False)\n\n        self.avg_attn = AverageAttention(self.embed_dim, dropout=args.attention_dropout)\n\n        # differently than original paper, we use a single gate\n        self.aan_gating_fc = fairseq_transformer.Linear(\n            self.embed_dim * 2, self.embed_dim\n        )\n\n        self.dropout = args.dropout\n        self.activation_fn = utils.get_activation_fn(\n            activation=getattr(args, ""activation_fn"", ""relu"")\n        )\n        self.activation_dropout = getattr(args, ""activation_dropout"", 0)\n        if self.activation_dropout == 0:\n            # for backwards compatibility with models that use args.relu_dropout\n            self.activation_dropout = getattr(args, ""relu_dropout"", 0)\n        self.normalize_before = args.decoder_normalize_before\n\n        # use layerNorm rather than FusedLayerNorm for exporting.\n        # char_inputs can be used to determint this.\n        # TODO  remove this once we update apex with the fix\n        export = getattr(args, ""char_inputs"", False)\n        self.avg_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        self.encoder_attn = MultiheadAttention(\n            self.embed_dim,\n            args.decoder_attention_heads,\n            kdim=getattr(args, ""encoder_embed_dim"", None),\n            vdim=getattr(args, ""encoder_embed_dim"", None),\n            dropout=args.attention_dropout,\n            encoder_decoder_attention=True,\n        )\n        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        self.fc1 = fairseq_transformer.Linear(\n            self.embed_dim, args.decoder_ffn_embed_dim\n        )\n        self.fc2 = fairseq_transformer.Linear(\n            args.decoder_ffn_embed_dim, self.embed_dim\n        )\n\n        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n        self.need_attn = True\n\n        self.onnx_trace = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def forward(\n        self,\n        x,\n        encoder_out=None,\n        encoder_padding_mask=None,\n        incremental_state=None,\n        prev_self_attn_state=None,\n        prev_attn_state=None,\n        self_attn_mask=None,\n        self_attn_padding_mask=None,\n        need_attn=False,\n        need_head_weights=False,\n    ):\n        """"""\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor, optional): binary\n                ByteTensor of shape `(batch, src_len)` where padding\n                elements are indicated by ``1``.\n            need_attn (bool, optional): return attention weights\n            need_head_weights (bool, optional): return attention weights\n                for each head (default: return average over heads).\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n\n        The following are used for export tracing:\n            prev_self_attn_state: [prev_sum, prev_pos]\n                assumes AverageAttention without mask trick\n            prev_attn_state: [prev_key, prev_value]\n        """"""\n        if need_head_weights:\n            need_attn = True\n\n        residual = x\n        x = self.maybe_layer_norm(self.avg_attn_layer_norm, x, before=True)\n\n        if prev_self_attn_state is not None:\n            if incremental_state is None:\n                incremental_state = {}\n            prev_sum, prev_pos = prev_self_attn_state\n            # (batch, embed) -> (seq, batch, embed)\n            prev_sum = prev_sum.unsqueeze(0)\n            saved_state = {""prev_sum"": prev_sum, ""prev_pos"": prev_pos}\n            self.avg_attn._set_input_buffer(incremental_state, saved_state)\n\n        x, _ = self.avg_attn(\n            value=x,\n            mask_future_timesteps=True,\n            incremental_state=incremental_state,\n            mask_trick=self.training,\n        )\n\n        # differently than original paper, we use a single gate\n        gate = torch.sigmoid(self.aan_gating_fc(torch.cat([residual, x], dim=-1)))\n        x = gate * x + (1 - gate) * residual\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(self.avg_attn_layer_norm, x, after=True)\n\n        if self.encoder_attn is not None:\n            residual = x\n            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, before=True)\n            if prev_attn_state is not None:\n                if incremental_state is None:\n                    incremental_state = {}\n                prev_key, prev_value = prev_attn_state[:2]\n                saved_state = {""prev_key"": prev_key, ""prev_value"": prev_value}\n                if len(prev_attn_state) >= 3:\n                    saved_state[""prev_key_padding_mask""] = prev_attn_state[2]\n                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n\n            x, attn = self.encoder_attn(\n                query=x,\n                key=encoder_out,\n                value=encoder_out,\n                key_padding_mask=encoder_padding_mask,\n                incremental_state=incremental_state,\n                static_kv=True,\n                need_weights=need_attn or (not self.training and self.need_attn),\n                need_head_weights=need_head_weights,\n            )\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = residual + x\n            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, after=True)\n\n        residual = x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n\n        if self.onnx_trace and incremental_state is not None:\n            saved_state = self.avg_attn._get_input_buffer(incremental_state)\n            # remove sequence axis for export\n            prev_sum = saved_state[""prev_sum""]\n            # (seq, batch, embed) -> (batch, embed)\n            prev_sum = prev_sum.squeeze(0)\n            prev_pos = saved_state[""prev_pos""]\n            self_attn_state = prev_sum, prev_pos\n            return x, attn, self_attn_state\n\n        return x, attn, None\n\n    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n        assert before ^ after\n        if after ^ self.normalize_before:\n            return layer_norm(x)\n        else:\n            return x\n\n    def make_generation_fast_(self, need_attn=False, **kwargs):\n        self.need_attn = need_attn\n\n\n@register_model(""semi_supervised_transformer"")\nclass SemiSupervisedTransformerModel(SemiSupervisedModel):\n    """"""\n    We can\'t use `self.single_model_cls` because at this point `__init__` hasn\'t\n    run. single_model_cls is a static class variable that is meant to be\n    constant\n    """"""\n\n    single_model_cls = TransformerModel\n\n    @staticmethod\n    def add_args(parser):\n        TransformerModel.add_args(parser)\n        SemiSupervisedModel.add_args(parser)\n\n\n@register_model_architecture(""ptt_transformer"", ""ptt_transformer"")\ndef base_architecture(args):\n    args.encoder_pretrained_embed = getattr(args, ""encoder_pretrained_embed"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 256)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 512)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 3)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 4)\n    args.encoder_freeze_embed = getattr(args, ""encoder_freeze_embed"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.decoder_pretrained_embed = getattr(args, ""decoder_pretrained_embed"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 3)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 4)\n    args.decoder_out_embed_dim = getattr(args, ""decoder_out_embed_dim"", None)\n    args.decoder_freeze_embed = getattr(args, ""decoder_freeze_embed"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_layerdrop = getattr(args, ""decoder_layerdrop"", 0)\n    args.decoder_layers_to_keep = getattr(args, ""decoder_layers_to_keep"", None)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.relu_dropout = getattr(args, ""relu_dropout"", 0.0)\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.aan = getattr(args, ""aan"", False)\n    vocab_reduction.set_arg_defaults(args)\n\n\n@register_model_architecture(\n    ""semi_supervised_transformer"", ""semi_supervised_transformer""\n)\ndef semi_supervised_transformer(args):\n    base_architecture(args)\n    SemiSupervisedModel.set_semi_supervised_arch_args(args)\n'"
pytorch_translate/transformer_aan.py,12,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqEncoderDecoderModel,\n    FairseqIncrementalDecoder,\n    register_model,\n    register_model_architecture,\n    transformer as fairseq_transformer,\n)\nfrom fairseq.modules import (\n    AdaptiveSoftmax,\n    LearnedPositionalEmbedding,\n    MultiheadAttention,\n    SinusoidalPositionalEmbedding,\n)\nfrom pytorch_translate import (\n    transformer as pytorch_translate_transformer,\n    vocab_reduction,\n)\nfrom pytorch_translate.average_attention import AverageAttention, AverageWindowAttention\nfrom pytorch_translate.utils import torch_find\n\n\n@register_model(""transformer_aan"")\nclass TransformerAANModel(FairseqEncoderDecoderModel):\n    """"""\n    Transformer model from `""Attention Is All You Need"" (Vaswani, et al, 2017)\n    <https://arxiv.org/abs/1706.03762>`_.\n    """"""\n\n    def __init__(self, task, encoder, decoder):\n        super().__init__(encoder, decoder)\n        self.task = task\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--dropout"", type=float, metavar=""D"", help=""dropout probability""\n        )\n        parser.add_argument(\n            ""--attention-dropout"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability for attention weights"",\n        )\n        parser.add_argument(\n            ""--relu-dropout"",\n            type=float,\n            metavar=""D"",\n            help=""dropout probability after ReLU in FFN"",\n        )\n        parser.add_argument(\n            ""--encoder-embed-path"",\n            type=str,\n            metavar=""STR"",\n            help=""path to pre-trained encoder embedding"",\n        )\n        parser.add_argument(\n            ""--encoder-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""encoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--encoder-ffn-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""encoder embedding dimension for FFN"",\n        )\n        parser.add_argument(\n            ""--encoder-layers"", type=int, metavar=""N"", help=""num encoder layers""\n        )\n        parser.add_argument(\n            ""--encoder-attention-heads"",\n            type=int,\n            metavar=""N"",\n            help=""num encoder attention heads"",\n        )\n        parser.add_argument(\n            ""--encoder-normalize-before"",\n            action=""store_true"",\n            help=""apply layernorm before each encoder block"",\n        )\n        parser.add_argument(\n            ""--encoder-learned-pos"",\n            action=""store_true"",\n            help=""use learned positional embeddings in the encoder"",\n        )\n        parser.add_argument(\n            ""--decoder-embed-path"",\n            type=str,\n            metavar=""STR"",\n            help=""path to pre-trained decoder embedding"",\n        )\n        parser.add_argument(\n            ""--decoder-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""decoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--decoder-ffn-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""decoder embedding dimension for FFN"",\n        )\n        parser.add_argument(\n            ""--decoder-layers"", type=int, metavar=""N"", help=""num decoder layers""\n        )\n        parser.add_argument(\n            ""--decoder-attention-heads"",\n            type=int,\n            metavar=""N"",\n            help=""num decoder attention heads"",\n        )\n        parser.add_argument(\n            ""--decoder-learned-pos"",\n            action=""store_true"",\n            help=""use learned positional embeddings in the decoder"",\n        )\n        parser.add_argument(\n            ""--decoder-normalize-before"",\n            action=""store_true"",\n            help=""apply layernorm before each decoder block"",\n        )\n        parser.add_argument(\n            ""--share-decoder-input-output-embed"",\n            action=""store_true"",\n            help=""share decoder input and output embeddings"",\n        )\n        parser.add_argument(\n            ""--share-all-embeddings"",\n            action=""store_true"",\n            help=""share encoder, decoder and output embeddings""\n            "" (requires shared dictionary and embed dim)"",\n        )\n        parser.add_argument(\n            ""--adaptive-softmax-cutoff"",\n            metavar=""EXPR"",\n            help=""comma separated list of adaptive softmax cutoff points. ""\n            ""Must be used with adaptive_loss criterion"",\n        ),\n        parser.add_argument(\n            ""--adaptive-softmax-dropout"",\n            type=float,\n            metavar=""D"",\n            help=""sets adaptive softmax dropout for the tail projections"",\n        )\n        # AAN only\n        parser.add_argument(\n            ""--decoder-attn-window-size"",\n            default=0,\n            type=int,\n            help=""attention window size of the decoder (default: 0 (unlimited))"",\n        )\n        parser.add_argument(\n            ""--no-decoder-aan-ffn"",\n            default=False,\n            action=""store_true"",\n            help=""no FFN in the AAN block"",\n        )\n        parser.add_argument(\n            ""--no-decoder-aan-gating"",\n            default=False,\n            action=""store_true"",\n            help=""no Gating in the AAN block"",\n        )\n        parser.add_argument(\n            ""--decoder-aan-ffn-use-embed-dim"",\n            default=False,\n            action=""store_true"",\n            help=""""""using decoder_embed_dim instead of decoder_ffn_embed_dim \\\n            as the hidden size of the FFN in AAN"""""",\n        )\n        parser.add_argument(\n            ""--decoder-aan-more-dropouts"",\n            type=lambda x: set(x.split("","")),\n            help=""""""places to add more dropout in AAN, accepting multiple values in \\\n            [residual/after_avg/after_aan] separated by commas"""""",\n        )\n        parser.add_argument(\n            ""--decoder-out-embed-dim"",\n            default=None,\n            type=int,\n            metavar=""N"",\n            help=""decoder output embedding dimension (bottleneck layer before""\n            ""output layer if specified.)"",\n        )\n\n        # Args for vocab reduction\n        vocab_reduction.add_args(parser)\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present in older models\n        base_architecture(args)\n\n        if not hasattr(args, ""max_source_positions""):\n            args.max_source_positions = 1024\n        if not hasattr(args, ""max_target_positions""):\n            args.max_target_positions = 1024\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        def build_embedding(dictionary, embed_dim, path=None):\n            num_embeddings = len(dictionary)\n            padding_idx = dictionary.pad()\n            emb = Embedding(num_embeddings, embed_dim, padding_idx)\n            # if provided, load from preloaded dictionaries\n            if path:\n                embed_dict = utils.parse_embedding(path)\n                utils.load_embedding(embed_dict, dictionary, emb)\n            return emb\n\n        if args.share_all_embeddings:\n            if src_dict != tgt_dict:\n                raise RuntimeError(\n                    ""--share-all-embeddings requires a joined dictionary""\n                )\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise RuntimeError(\n                    """"""--share-all-embeddings requires --encoder-embed-dim \\\n                    to match --decoder-embed-dim""""""\n                )\n            if args.decoder_embed_path and (\n                args.decoder_embed_path != args.encoder_embed_path\n            ):\n                raise RuntimeError(\n                    ""--share-all-embeddings not compatible with --decoder-embed-path""\n                )\n            encoder_embed_tokens = build_embedding(\n                src_dict, args.encoder_embed_dim, args.encoder_embed_path\n            )\n            decoder_embed_tokens = encoder_embed_tokens\n            args.share_decoder_input_output_embed = True\n        else:\n            encoder_embed_tokens = build_embedding(\n                src_dict, args.encoder_embed_dim, args.encoder_embed_path\n            )\n            decoder_embed_tokens = build_embedding(\n                tgt_dict, args.decoder_embed_dim, args.decoder_embed_path\n            )\n\n        encoder = pytorch_translate_transformer.TransformerEncoder(\n            args, src_dict, encoder_embed_tokens\n        )\n        decoder = TransformerAANDecoder(args, src_dict, tgt_dict, decoder_embed_tokens)\n        return TransformerAANModel(task, encoder, decoder)\n\n    def get_targets(self, sample, net_output):\n        targets = sample[""target""].view(-1)\n        possible_translation_tokens = net_output[-1]\n        if possible_translation_tokens is not None:\n            targets = torch_find(\n                possible_translation_tokens, targets, len(self.task.target_dictionary)\n            )\n        return targets\n\n\nclass TransformerAANDecoder(FairseqIncrementalDecoder):\n    """"""\n    Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`TransformerAANDecoderLayer`.\n    """"""\n\n    def __init__(\n        self,\n        args,\n        src_dict,\n        dst_dict,\n        embed_tokens,\n        no_encoder_attn=False,\n        left_pad=False,\n        final_norm=True,\n    ):\n        super().__init__(dst_dict)\n        self.dropout = args.dropout\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n\n        input_embed_dim = embed_tokens.embedding_dim\n        embed_dim = args.decoder_embed_dim\n\n        padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = args.max_target_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_scale = math.sqrt(embed_dim)  # todo: try with input_embed_dim\n\n        self.project_in_dim = (\n            Linear(input_embed_dim, embed_dim, bias=False)\n            if embed_dim != input_embed_dim\n            else None\n        )\n\n        self.embed_positions = fairseq_transformer.PositionalEmbedding(\n            1024, embed_dim, padding_idx, learned=args.decoder_learned_pos\n        )\n\n        self.layers = nn.ModuleList([])\n        self.layers.extend(\n            [\n                TransformerAANDecoderLayer(args, no_encoder_attn)\n                for _ in range(args.decoder_layers)\n            ]\n        )\n\n        self.adaptive_softmax = None\n\n        self.bottleneck_layer = None\n        out_embed_dim = embed_dim\n        if args.decoder_out_embed_dim is not None:\n            assert (\n                not args.share_all_embeddings\n                and not args.share_decoder_input_output_embed\n            ), ""--decoder-out-embed-dim is incompatible with sharing output embeddings!""\n            self.bottleneck_layer = Linear(embed_dim, args.decoder_out_embed_dim)\n            out_embed_dim = args.decoder_out_embed_dim\n\n        if args.adaptive_softmax_cutoff is not None:\n            self.adaptive_softmax = AdaptiveSoftmax(\n                len(dst_dict),\n                out_embed_dim,\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.adaptive_softmax_dropout,\n                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,\n                factor=args.adaptive_softmax_factor,\n                tie_proj=args.tie_adaptive_proj,\n            )\n        elif not self.share_input_output_embed:\n            self.embed_out = nn.Parameter(torch.Tensor(len(dst_dict), out_embed_dim))\n            nn.init.normal_(self.embed_out, mean=0, std=out_embed_dim ** -0.5)\n        self.register_buffer(""version"", torch.Tensor([2]))\n        self.normalize = args.decoder_normalize_before and final_norm\n        if self.normalize:\n            self.layer_norm = LayerNorm(embed_dim)\n\n        self.vocab_reduction_module = None\n        if args.vocab_reduction_params:\n            assert (\n                self.adaptive_softmax is None\n            ), ""vocabulary reduction not compatible with adaptive softmax!""\n            self.vocab_reduction_module = vocab_reduction.VocabReduction(\n                src_dict, dst_dict, args.vocab_reduction_params, fp16=args.fp16\n            )\n\n        self.onnx_trace = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def forward(\n        self,\n        prev_output_tokens,\n        encoder_out=None,\n        incremental_state=None,\n        possible_translation_tokens=None,\n        timestep=None,\n    ):\n        (encoder_x, src_tokens, encoder_padding_mask) = encoder_out\n\n        positions = self.embed_positions(\n            prev_output_tokens, incremental_state=incremental_state, timestep=timestep\n        )\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            positions = positions[:, -1:]\n\n            if self.onnx_trace:\n                assert type(incremental_state) is list\n                assert timestep is not None\n\n                state_list = incremental_state\n                incremental_state = {}\n                state_index = 0\n\n                for layer in self.layers:\n                    prev_sum = state_list[state_index]\n                    state_index += 1\n                    prev_pos = timestep.float()\n                    layer.avg_attn._set_input_buffer(\n                        incremental_state, {""prev_sum"": prev_sum, ""prev_pos"": prev_pos}\n                    )\n\n                    if layer.encoder_attn is not None:\n                        utils.set_incremental_state(\n                            layer.encoder_attn,\n                            incremental_state,\n                            ""prev_key"",\n                            state_list[state_index],\n                        )\n                        utils.set_incremental_state(\n                            layer.encoder_attn,\n                            incremental_state,\n                            ""prev_value"",\n                            state_list[state_index + 1],\n                        )\n                        state_index += 2\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        attn = None\n\n        inner_states = [x]\n\n        # decoder layers\n        for layer in self.layers:\n            x, attn = layer(\n                x,\n                encoder_x,\n                encoder_padding_mask,\n                incremental_state,\n                self_attn_mask=self.buffered_future_mask(x)\n                if incremental_state is None\n                else None,\n            )\n            inner_states.append(x)\n\n        if self.normalize:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.bottleneck_layer is not None:\n            x = self.bottleneck_layer(x)\n\n        # project back to size of vocabulary\n        if self.share_input_output_embed:\n            output_weights = self.embed_tokens.weight\n        else:\n            output_weights = self.embed_out\n\n        if (\n            self.vocab_reduction_module is not None\n            and possible_translation_tokens is None\n        ):\n            decoder_input_tokens = prev_output_tokens.contiguous()\n            possible_translation_tokens = self.vocab_reduction_module(\n                src_tokens, decoder_input_tokens=decoder_input_tokens\n            )\n        if possible_translation_tokens is not None:\n            output_weights = output_weights.index_select(\n                dim=0, index=possible_translation_tokens\n            )\n\n        if self.adaptive_softmax is None:\n            logits = F.linear(x, output_weights)\n        else:\n            assert (\n                possible_translation_tokens is None\n            ), ""vocabulary reduction and adaptive softmax are incompatible!""\n            logits = x\n\n        if self.onnx_trace:\n            state_outputs = []\n            for layer in self.layers:\n                saved_state = layer.avg_attn._get_input_buffer(incremental_state)\n                assert (\n                    ""prev_sum"" in saved_state\n                ), ""No prev_sum found while tracing average attention layer!""\n                # remove sequence axis\n                prev_sum = saved_state[""prev_sum""].squeeze(0)\n                state_outputs.append(prev_sum)\n\n                if layer.encoder_attn is not None:\n                    # prev_key and prev_value remain unchanged\n                    prev_key = state_list[len(state_outputs)]\n                    prev_value = state_list[len(state_outputs) + 1]\n                    state_outputs.extend([prev_key, prev_value])\n\n            return logits, attn, possible_translation_tokens, state_outputs\n\n        return logits, attn, possible_translation_tokens\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return min(self.max_target_positions, self.embed_positions.max_positions)\n\n    def buffered_future_mask(self, tensor):\n        dim = tensor.size(0)\n        if (\n            not hasattr(self, ""_future_mask"")\n            or self._future_mask is None\n            or self._future_mask.device != tensor.device\n        ):\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(tensor.new(dim, dim)), 1\n            )\n        if self._future_mask.size(0) < dim:\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(self._future_mask.resize_(dim, dim)), 1\n            )\n        return self._future_mask[:dim, :dim]\n\n    def upgrade_state_dict(self, state_dict):\n        """"""Upgrade a (possibly old) state dict for new versions of fairseq.""""""\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            if ""decoder.embed_positions.weights"" in state_dict:\n                del state_dict[""decoder.embed_positions.weights""]\n            state_dict[""decoder.embed_positions._float_tensor""] = torch.FloatTensor(1)\n\n        for i in range(len(self.layers)):\n            # update layer norms\n            layer_norm_map = {\n                ""0"": ""aan_layer_norm"",\n                ""1"": ""encoder_attn_layer_norm"",\n                ""2"": ""final_layer_norm"",\n            }\n            for old, new in layer_norm_map.items():\n                for m in (""weight"", ""bias""):\n                    k = ""decoder.layers.{}.layer_norms.{}.{}"".format(i, old, m)\n                    if k in state_dict:\n                        state_dict[\n                            ""decoder.layers.{}.{}.{}"".format(i, new, m)\n                        ] = state_dict[k]\n                        del state_dict[k]\n        if utils.item(state_dict.get(""decoder.version"", torch.Tensor([1]))[0]) < 2:\n            # earlier checkpoints did not normalize after the stack of layers\n            self.layer_norm = None\n            self.normalize = False\n            state_dict[""decoder.version""] = torch.Tensor([1])\n\n        return state_dict\n\n    def _init_prev_states(self, encoder_out):\n        """"""\n        For average attention, prev_sum is initialized to zero.\n        For encoder-decoder attention, key and value are computed once from\n        the encoder outputs and stay the same throughout decoding.\n        """"""\n        encoder_x, src_tokens, encoder_padding_mask = encoder_out\n        states = []\n        for layer in self.layers:\n            # (bsz, channel)\n            prev_sum = torch.zeros([1, layer.avg_attn.embed_dim])\n            states.append(prev_sum)\n\n            # (key, value) for encoder-decoder attention computed from encoder\n            # output and remain the same throughout decoding\n            key = layer.encoder_attn.k_proj(encoder_x)\n            value = layer.encoder_attn.v_proj(encoder_x)\n\n            # (key, value) kept in shape (bsz, num_heads, seq_len, head_dim)\n            # to avoid repeated transpose operations\n            seq_len, batch_size_int, _ = encoder_x.shape\n            num_heads = layer.encoder_attn.num_heads\n            head_dim = layer.encoder_attn.head_dim\n            key = (\n                key.view(seq_len, batch_size_int * num_heads, head_dim)\n                .transpose(0, 1)\n                .view(batch_size_int, num_heads, seq_len, head_dim)\n            )\n            value = (\n                value.view(seq_len, batch_size_int * num_heads, head_dim)\n                .transpose(0, 1)\n                .view(batch_size_int, num_heads, seq_len, head_dim)\n            )\n\n            states.extend([key, value])\n\n        return states\n\n\nclass TransformerAANDecoderLayer(nn.Module):\n    """"""Decoder layer block.\n    In the original paper each operation (multi-head attention, encoder\n    attention or FFN) is postprocessed with: `dropout -> add residual ->\n    layernorm`. In the tensor2tensor code they suggest that learning is more\n    robust when preprocessing each layer with layernorm and postprocessing with:\n    `dropout -> add residual`. We default to the approach in the paper, but the\n    tensor2tensor approach can be enabled by setting\n    *args.decoder_normalize_before* to ``True``.\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        no_encoder_attn (bool, optional): whether to attend to encoder outputs.\n            Default: ``False``\n    """"""\n\n    def __init__(self, args, no_encoder_attn=False):\n        super().__init__()\n        self.embed_dim = args.decoder_embed_dim\n        self.dropout = args.dropout\n        self.relu_dropout = args.relu_dropout\n        self.more_dropouts = args.decoder_aan_more_dropouts\n        if args.decoder_attn_window_size <= 0:\n            self.avg_attn = AverageAttention(\n                self.embed_dim, dropout=args.attention_dropout\n            )\n        else:\n            self.avg_attn = AverageWindowAttention(\n                self.embed_dim,\n                dropout=args.attention_dropout,\n                window_size=args.decoder_attn_window_size,\n            )\n        # self.activation = getattr(args, ""decoder_ffn_activation"", ""relu"")\n        self.aan_layer_norm = LayerNorm(self.embed_dim)\n        if args.no_decoder_aan_ffn:\n            self.aan_ffn = None\n        else:\n            aan_ffn_hidden_dim = (\n                args.decoder_ffn_embed_dim\n                if args.decoder_aan_ffn_use_embed_dim\n                else args.decoder_ffn_embed_dim\n            )\n            self.aan_ffn = FeedForwardNetwork(\n                self.embed_dim,\n                aan_ffn_hidden_dim,\n                self.embed_dim,\n                num_layers=2,\n                dropout=args.relu_dropout,\n            )\n\n        if args.no_decoder_aan_gating:\n            self.aan_gating_fc = None\n        else:\n            self.aan_gating_fc = Linear(self.embed_dim * 2, self.embed_dim * 2)\n        self.normalize_before = args.decoder_normalize_before\n\n        if no_encoder_attn:\n            self.encoder_attn = None\n            self.encoder_attn_layer_norm = None\n        else:\n            self.encoder_attn = MultiheadAttention(\n                self.embed_dim,\n                args.decoder_attention_heads,\n                kdim=args.encoder_embed_dim,\n                vdim=args.encoder_embed_dim,\n                dropout=args.attention_dropout,\n                encoder_decoder_attention=True,\n            )\n            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n\n        self.ffn = FeedForwardNetwork(\n            self.embed_dim,\n            args.decoder_ffn_embed_dim,\n            self.embed_dim,\n            num_layers=2,\n            dropout=args.relu_dropout,\n        )\n\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n        self.need_attn = True\n\n        self.onnx_trace = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def forward(\n        self,\n        x,\n        encoder_out,\n        encoder_padding_mask,\n        incremental_state,\n        prev_self_attn_state=None,\n        prev_attn_state=None,\n        self_attn_mask=None,\n        self_attn_padding_mask=None,\n    ):\n        """"""\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        """"""\n        residual = x\n        if ""residual"" in self.more_dropouts:\n            residual = F.dropout(residual, p=self.dropout, training=self.training)\n\n        if prev_self_attn_state is not None:\n            if incremental_state is None:\n                incremental_state = {}\n            prev_key, prev_value = prev_self_attn_state\n            saved_state = {""prev_key"": prev_key, ""prev_value"": prev_value}\n            self.avg_attn._set_input_buffer(incremental_state, saved_state)\n        x, _ = self.avg_attn(\n            value=x,\n            mask_future_timesteps=True,\n            incremental_state=incremental_state,\n            mask_trick=self.training,\n        )\n        if ""after_avg"" in self.more_dropouts:\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        if self.aan_layer_norm is not None:\n            x = self.maybe_layer_norm(self.aan_layer_norm, x, before=True)\n\n        if self.aan_ffn is not None:\n            x = self.aan_ffn(x)\n            if ""after_ffn"" in self.more_dropouts:\n                x = F.dropout(x, p=self.dropout, training=self.training)\n\n        if self.aan_gating_fc is not None:\n            i, f = self.aan_gating_fc(torch.cat([residual, x], dim=-1)).chunk(2, dim=-1)\n            x = torch.sigmoid(f) * residual + torch.sigmoid(i) * x\n            if ""after_gating"" in self.more_dropouts:\n                x = F.dropout(x, p=self.dropout, training=self.training)\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n\n        if self.aan_layer_norm is not None:\n            x = self.maybe_layer_norm(self.aan_layer_norm, x, after=True)\n\n        attn = None\n        if self.encoder_attn is not None:\n            residual = x\n            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, before=True)\n            if prev_attn_state is not None:\n                if incremental_state is None:\n                    incremental_state = {}\n                prev_key, prev_value = prev_attn_state\n                saved_state = {""prev_key"": prev_key, ""prev_value"": prev_value}\n                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n            x, attn = self.encoder_attn(\n                query=x,\n                key=encoder_out,\n                value=encoder_out,\n                key_padding_mask=encoder_padding_mask,\n                incremental_state=incremental_state,\n                static_kv=True,\n                need_weights=(not self.training and self.need_attn),\n            )\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = residual + x\n            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, after=True)\n\n        residual = x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n        x = self.ffn(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n\n        return x, attn\n\n    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n        assert before ^ after\n        if after ^ self.normalize_before:\n            return layer_norm(x)\n        else:\n            return x\n\n    def make_generation_fast_(self, need_attn=False, **kwargs):\n        self.need_attn = need_attn\n\n    def extra_repr(self):\n        return ""dropout={}, more_dropouts={}"".format(self.dropout, self.more_dropouts)\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n\ndef LayerNorm(embedding_dim):\n    m = nn.LayerNorm(embedding_dim)\n    return m\n\n\ndef Linear(in_features, out_features, bias=True):\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.0)\n    return m\n\n\ndef PositionalEmbeddingCreator(\n    num_embeddings, embedding_dim, padding_idx, left_pad, learned=False\n):\n    if learned:\n        m = LearnedPositionalEmbedding(\n            num_embeddings + padding_idx + 1, embedding_dim, padding_idx\n        )\n        nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n        nn.init.constant_(m.weight[padding_idx], 0)\n    else:\n        m = SinusoidalPositionalEmbedding(\n            embedding_dim=embedding_dim,\n            padding_idx=padding_idx,\n            init_size=num_embeddings + padding_idx + 1,\n        )\n    return m\n\n\nclass FeedForwardNetwork(nn.Module):\n    def __init__(\n        self, input_size, hidden_size=None, output_size=None, num_layers=2, dropout=0.0\n    ):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = self.input_size if output_size is None else output_size\n        self.hidden_size = self.output_size if hidden_size is None else hidden_size\n        self.num_layers = num_layers\n        self.activation_type = ""relu""\n        self.dropout = dropout\n        self.layers = nn.ModuleList()\n        if num_layers == 1:\n            self.layers.append(Linear(self.input_size, self.output_size))\n        else:\n            self.layers.append(Linear(self.input_size, self.hidden_size))\n            for _ in range(1, num_layers - 1):\n                self.layers.append(Linear(self.hidden_size, self.hidden_size))\n            self.layers.append(Linear(self.hidden_size, self.output_size))\n\n    def forward(self, x):\n        x = self.layers[0](x)\n        for layer in self.layers[1:]:\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n            x = layer(x)\n        return x\n\n    def extra_repr(self):\n        return ""activation_type={}, dropout={}"".format(\n            self.activation_type, self.dropout\n        )\n\n\n@register_model_architecture(""transformer_aan"", ""transformer_aan"")\ndef base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.relu_dropout = getattr(args, ""relu_dropout"", 0.0)\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.decoder_out_embed_dim = getattr(args, ""decoder_out_embed_dim"", None)\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n\n    args.decoder_aan_ffn = getattr(args, ""decoder_aan_ffn"", True)\n    args.decoder_aan_ffn_use_embed_dim = getattr(\n        args, ""decoder_aan_ffn_use_embed_dim"", False\n    )\n    args.decoder_aan_gating = getattr(args, ""decoder_aan_gating"", True)\n    args.decoder_aan_more_dropouts = getattr(args, ""decoder_aan_more_dropouts"", """")\n    vocab_reduction.set_arg_defaults(args)\n'"
pytorch_translate/utils.py,13,"b'#!/usr/bin/env python3\n\nimport argparse\nimport ast\nimport os\nimport signal\nimport threading\nimport time\nfrom typing import Dict, List, Optional, Union\n\nimport torch\nfrom fairseq import distributed_utils, tasks, utils\nfrom pytorch_translate.file_io import PathManager\n\n\n# Helper type for argparse to enable flippable boolean flags. For example,\n# group.add_argument(""--foo"", type=utils.bool_flag, nargs=""?"", const=True,\n#                    default=False)\n# creates a --foo flag that defaults to False and can be set to True by\n# specifying `--foo True` or just `--foo`. Some other --bar flag that defaults\n# to True can be set to False by specifying `--bar False`.\ndef bool_flag(value):\n    if value.lower() in (""true"", ""t"", ""1""):\n        return True\n    elif value.lower() in (""false"", ""f"", ""0""):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\n            f""Expected boolean string such as \'true\'/\'false\' instead of {value}.""\n        )\n\n\n# Variation on the fairseq StopwatchMeter that separates statistics by number\n# of tokens. Sentences longer than max_length are stored in the last bucket.\nclass BucketStopwatchMeter(object):\n    def __init__(self, increment, max_length, sentences_per_batch):\n        self.increment = increment\n        self.n_buckets = max_length // increment + 1\n        self.sentences_per_batch = sentences_per_batch\n        self.reset()\n\n    def start(self):\n        self.start_time = time.time()\n\n    def stop(self, n=1):\n        if self.start_time is not None:\n            delta = time.time() - self.start_time\n            bucket_id = min(self.n_buckets - 1, n // self.increment)\n            self.sum[bucket_id] += delta\n            self.n[bucket_id] += n\n            self.count[bucket_id] += 1\n            self.start_time = None\n\n    def reset(self):\n        self.sum = [0] * self.n_buckets\n        self.n = [0] * self.n_buckets\n        self.count = [0] * self.n_buckets\n        self.start_time = None\n\n    def reset_bucket(self, bucket_id):\n        if self.start_time is None:\n            self.sum[bucket_id] = 0\n            self.n[bucket_id] = 0\n            self.count[bucket_id] = 0\n\n    @property\n    def avg(self):\n        return sum(self.sum) / sum(self.n)\n\n    @property\n    def avgs(self):\n        result = [0] * self.n_buckets\n        for i in range(self.n_buckets):\n            if self.n[i] != 0:\n                result[i] = self.sum[i] / self.n[i]\n            else:\n                result[i] = 0\n        return result\n\n\ndef load_diverse_ensemble_for_inference(\n    filenames: List[str], task: Optional[tasks.FairseqTask] = None\n):\n    """"""Load an ensemble of diverse models for inference.\n\n    This method is similar to fairseq.utils.load_ensemble_for_inference\n    but allows to load diverse models with non-uniform args.\n\n    Args:\n        filenames: List of file names to checkpoints\n        task: Optional[FairseqTask]. If this isn\'t provided, we setup the task\n            using the first checkpoint\'s model args loaded from the saved state.\n\n    Return:\n        models, args: Tuple of lists. models contains the loaded models, args\n            the corresponding configurations.\n        task: Either the input task or the task created within this function\n            using args\n    """"""\n\n    # load model architectures and weights\n    checkpoints_data = []\n    for filename in filenames:\n        if not PathManager.exists(filename):\n            raise IOError(""Model file not found: {}"".format(filename))\n        with PathManager.open(filename, ""rb"") as f:\n            checkpoints_data.append(\n                torch.load(\n                    f,\n                    map_location=lambda s, l: torch.serialization.default_restore_location(\n                        s, ""cpu""\n                    ),\n                )\n            )\n    # build ensemble\n    ensemble = []\n    if task is None:\n        if hasattr(checkpoints_data[0][""args""], ""mode""):\n            checkpoints_data[0][""args""].mode = ""eval""\n        task = tasks.setup_task(checkpoints_data[0][""args""])\n    for checkpoint_data in checkpoints_data:\n        model = task.build_model(checkpoint_data[""args""])\n        model.load_state_dict(checkpoint_data[""model""])\n        ensemble.append(model)\n    args_list = [s[""args""] for s in checkpoints_data]\n    return ensemble, args_list, task\n\n\ndef densify(t):\n    """"""Removes holes in an array.\n\n    This function converts a 1-dimensional tensor of length n without duplicates\n    to a 1-dimensional tensor of the same length with all elements less than n\n    while preserving the order. For example,\n\n        [1, 0, 4, 5, 10, 9] -> [1, 0, 2, 3, 5, 4]\n    """"""\n    _, sorted_indices = torch.sort(t)\n    _, dense_t = torch.sort(sorted_indices)\n    return dense_t\n\n\ndef maybe_cat(tensors, dim, nullable=None):\n    """"""Like torch.cat, but skips elements in `tensors` which are None.\n\n    Args:\n        tensors: List of tensors (compare torch.cat())\n        dim: Dimension along which to concatenate (compare to torch.cat())\n        nullable: List of the same length as `tensors`. If specified, throw\n            a RuntimeError if the i-th element in `tensors` is None and the\n            i-th element in nullable is False.\n\n    Returns:\n        Concatenation of all tensors in `tensors` along `dim` which are not\n        None.\n\n    Throws:\n        RuntimeError is `nullable` constraint is violated or all alements in\n        `tensors` are None.\n    """"""\n    if nullable is not None and any(\n        (t is None) and not n for t, n in zip(tensors, nullable)\n    ):\n        raise RuntimeError(""Unexpected element in tensors is None."")\n    filtered = [t for t in tensors if t is not None]\n    if len(filtered) == 1:\n        return filtered[0]\n    return torch.cat(filtered, dim=dim)\n\n\ndef maybe_cuda(t):\n    """"""Calls `cuda()` on `t` if cuda is available.""""""\n    if torch.cuda.is_available():\n        return t.cuda()\n    return t\n\n\ndef average_tensors(tensor_list, norm_fn=None, weights=None):\n    """"""Averages a list of tensors.\n\n    Average the elements in tensor_list as follows:\n      w1*norm_fn(t1) + w2*norm_fn(t2) + ...\n    The default behavior corresponds to a [weighted] mean. You can set norm_fn\n    to F.softmax or F.log_softmax to average in probability or logprob space.\n\n    Note: This implementation favours memory efficiency over numerical\n    stability, and iterates through `tensor_list` in a Python for-loop rather\n    than stacking it to a PyTorch tensor.\n\n    Arguments:\n        tensor_list (list): Python list of tensors of the same size and same type\n        norm_fn (function): If set, apply norm_fn() to elements in `tensor_list`\n            before averaging. If list of functions, apply n-th function to\n            n-th tensor.\n        weights (list): List of tensors or floats to use to weight models. Must\n            be of the same length as `tensor_list`. If none, use uniform weights.\n\n    Returns:\n        Average of the tensors in `tensor_list`\n    """"""\n    n_tensors = len(tensor_list)\n    if weights is None:\n        weights = [1.0 / float(n_tensors)] * n_tensors\n    if not isinstance(norm_fn, list):\n        norm_fn = [norm_fn] * n_tensors\n    assert n_tensors == len(weights)\n    assert n_tensors == len(norm_fn)\n\n    def id_fn(x, dim):\n        return x\n\n    norm_fn = [id_fn if f is None else f for f in norm_fn]\n    acc = torch.zeros_like(tensor_list[0])\n    for f, w, t in zip(norm_fn, weights, tensor_list):\n        acc += w * f(t, dim=-1)\n    return acc\n\n\ndef load_embedding(embedding, dictionary, pretrained_embed):\n    """"""Loads pretrained embeddings.\n\n    Loads pretrained embeddings into a nn.Embedding layer. pretrained_embed\n    can either be a nn.Embedding layer, in which case the embedding is set\n    to the pretrained_embed argument, or a path to an embedding file.\n\n    Arguments:\n        embedding (pytorch_translate.common_layers.Embedding):\n            Embedding layer whose weights are to be set.\n        dictionary (fairseq.data.dictionary.Dictionary): dictionary with the\n            same vocabulary size as the embedding argument.\n        pretrained_embed (Union(string, nn.Embedding)): source of the\n            weights to be loaded.\n    """"""\n    if pretrained_embed is None:\n        return\n\n    if isinstance(pretrained_embed, torch.nn.Embedding):\n        embedding.weight = pretrained_embed.weight\n    else:\n        embed_dict = utils.parse_embedding(pretrained_embed)\n        utils.load_embedding(embed_dict, dictionary, embedding)\n\n    embedding.init_normalization_if_needed()\n\n\ndef torch_find(index, query, vocab_size):\n    """"""\n    Finds elements of query from index, outputting the last (max) index for each\n    query.\n    preconditions:  (1) index and query are flat arrays (can be different sizes)\n                    (2) all tokens in index and query have values < vocab_size\n    """"""\n    full_to_index = maybe_cuda(torch.zeros(vocab_size).long())\n    index_shape_range = maybe_cuda(torch.arange(index.shape[0]).long())\n    full_to_index[index] = index_shape_range\n    result = full_to_index[query]\n    return result\n\n\ndef all_gather_from_master(args, data: List) -> List:\n    if args.distributed_world_size == 1:\n        return data\n\n    gathered_data = distributed_utils.all_gather_list(data)\n    # Converts [[x0, y0, z0, ...], [x1, y1, z1, ...], [x2, y2, z2, ...], ...]\n    # to [[x0, x1, x2, ...], [y0, y1, y2, ...], [z0, z1, z2, ...], ...]\n    gathered_data_list = list(zip(*gathered_data))\n\n    output_data = []\n    for data_index, all_data in enumerate(gathered_data_list):\n        # The master\'s (process 0) data is guaranteed to be in position 0.\n        master_data = all_data[0]\n        # Sanity check that only the master returned any result.\n        if master_data is None:\n            raise RuntimeError(\n                f""Input data element {data_index} of all_gather_from_master ""\n                f""returned None from master. Results from all processes: {all_data}""\n            )\n        for i in range(1, len(all_data)):\n            if all_data[i] is not None:\n                raise RuntimeError(\n                    f""Input data element {data_index} of all_gather_from_master ""\n                    f""should have returned None from non-master process {i}. ""\n                    f""Results from all processes: {all_data}""\n                )\n        output_data.append(master_data)\n    return output_data\n\n\ndef get_source_tokens_tensor(src_tokens):\n    """"""\n    To enable integration with PyText, src_tokens should be able to support\n    more features than just token embeddings. Hence when dictionary features are\n    passed from PyText it will be passed as a tuple\n    (token_embeddings, dict_feat, ..). Thus, in this case where we need the source\n    tokens tensor (eg to calculate batch size = source_tokens_tensor.size(0)),\n    we get the first element on the tuple which is always guaranteed\n    to be source tokens and do the necessary operation.\n    eg : bsz, _ = get_source_tokens_tensor(source_tokens)[0].size(0)\n    """"""\n    if type(src_tokens) is tuple:\n        return src_tokens[0]\n    else:\n        return src_tokens\n\n\n# TODO: Remove when gluster is deprecated (T48002528)\ndef maybe_remove_gluster_path_prefix(path: str) -> str:\n    if ""gluster:///"" in path:\n        return path[10:]\n    else:\n        return path\n\n\ndef maybe_parse_collection_argument(path: str) -> Union[str, Dict]:\n    try:\n        path_dict = ast.literal_eval(path)\n        path_dict = {\n            key: maybe_remove_gluster_path_prefix(value)\n            for key, value in path_dict.items()\n        }\n    except (ValueError, SyntaxError):\n        return maybe_remove_gluster_path_prefix(path)\n    return path_dict\n'"
pytorch_translate/vocab_constants.py,0,"b'#!/usr/bin/env python3\n\nMAX_SPECIAL_TOKENS = 100\n\n# Number of Byte indices is always fixed at 256 (0-255). The additional 5 indices\n# correpsond to the special tokens for byte numberization including\n# padding, start and end of word, start and end of sentence. These are\n# separate from the special tokens in the dict and match up with the indices\n# used by pre-trained ELMo.\nNUM_BYTE_INDICES = 261\n\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\nMASK_ID = 4\n'"
pytorch_translate/vocab_reduction.py,7,"b'#!/usr/bin/env python3\n\nimport codecs\nimport logging\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\nlogger = logging.getLogger(__name__)\n\n# Vocab reduction default params when only lexical dictionaries is provided\nNUM_TOP_WORDS_DEFAULT = 2000\n\nMAX_TRANSLATION_CANDIDATES_PER_WORD_DEFAULT = 30\n\n\ndef add_args(parser):\n    parser.add_argument(\n        ""--lexical-dictionary"",\n        type=str,\n        metavar=""EXPR"",\n        action=""append"",\n        help=(""lexical dictionary(ies) for vocab reduction""),\n    )\n    parser.add_argument(\n        ""--num-top-words"",\n        type=int,\n        metavar=""N"",\n        help=""num top words for vocab reduction"",\n    )\n    parser.add_argument(\n        ""--max-translation-candidates-per-word"",\n        type=int,\n        metavar=""N"",\n        help=""max translation candidates per word for vocab reduction"",\n    )\n\n\ndef set_arg_defaults(args):\n    # lexical_dictionaries is the only required argument for vocab reduction\n    lexical_dictionaries = getattr(args, ""lexical_dictionary"", None)\n    if hasattr(args, ""vocab_reduction_params""):\n        # We\'ve already created the vocab reduction params from the bottom-level\n        # lexical_dictionaries, num_top_words and\n        # max_translation_candidates_per_word args\n        return args.vocab_reduction_params\n    args.vocab_reduction_params = None\n    if lexical_dictionaries is not None:\n        num_top_words = getattr(args, ""num_top_words"", NUM_TOP_WORDS_DEFAULT)\n        max_translation_candidates_per_word = getattr(\n            args,\n            ""max_translation_candidates_per_word"",\n            MAX_TRANSLATION_CANDIDATES_PER_WORD_DEFAULT,\n        )\n        args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": num_top_words,\n            ""max_translation_candidates_per_word"": max_translation_candidates_per_word,\n        }\n        # For less redundant logging when we print out the args Namespace,\n        # delete the bottom-level args, since we\'ll just be dealing with\n        # args.vocab_reduction_params from now on\n        delattr(args, ""lexical_dictionary"")\n        if hasattr(args, ""num_top_words""):\n            delattr(args, ""num_top_words"")\n        if hasattr(args, ""max_translation_candidates_per_word""):\n            delattr(args, ""max_translation_candidates_per_word"")\n\n\ndef select_top_candidate_per_word(\n    source_index,\n    target_indices_with_prob,\n    counter_per_word,\n    max_translation_candidates_per_word,\n    translation_candidates,\n    translation_candidates_set,\n):\n    translation_candidates_saved = 0\n    target_indices_with_prob.sort(key=lambda x: x[1], reverse=True)\n    for target_index_with_prob in target_indices_with_prob:\n        if counter_per_word[source_index] >= max_translation_candidates_per_word:\n            # don\'t save more than max_translation_candidates_per_word\n            # translation candidates for any one source token\n            break\n\n        # update translation candidates matrix at [source index, running counter\n        # per source token] to = target index\n        translation_candidates[\n            source_index, counter_per_word[source_index]\n        ] = target_index_with_prob[0]\n        translation_candidates_set.update((source_index, target_index_with_prob[0]))\n        counter_per_word[source_index] += 1\n        translation_candidates_saved += 1\n    return translation_candidates_saved\n\n\ndef get_translation_candidates(\n    src_dict,\n    dst_dict,\n    lexical_dictionaries,\n    num_top_words,\n    max_translation_candidates_per_word,\n):\n    """"""\n    Reads a lexical dictionary file, where each line is (source token, possible\n    translation of source token, probability). The file is generally grouped\n    by source tokens, but within the group, the probabilities are not\n    necessarily sorted.\n\n    A a 0.3\n    A c 0.1\n    A e 0.05\n    A f 0.01\n    B b 0.6\n    B b 0.2\n    A z 0.001\n    A y 0.002\n    ...\n\n    Returns: translation_candidates\n        Matrix of shape (src_dict, max_translation_candidates_per_word) where\n        each row corresponds to a source word in the vocab and contains token\n        indices of translation candidates for that source word\n    """"""\n\n    translation_candidates = np.zeros(\n        [len(src_dict), max_translation_candidates_per_word], dtype=np.int32\n    )\n\n    # running count of translation candidates per source word\n    counter_per_word = np.zeros(len(src_dict), dtype=np.int32)\n\n    # tracks if we\'ve already seen some (source token, target token) pair so we\n    # ignore duplicate lines\n    translation_candidates_set = set()\n\n    for lexical_dictionary in lexical_dictionaries:\n        logger.info(f""Processing dictionary file {lexical_dictionary}"")\n        translation_candidates_saved = 0\n\n        with codecs.open(lexical_dictionary, ""r"", ""utf-8"") as lexical_dictionary_file:\n            current_source_index = None\n            current_target_indices = []\n            for line in lexical_dictionary_file.readlines():\n                alignment_data = line.split()\n                if len(alignment_data) != 3:\n                    logger.warning(f""Malformed line in lexical dictionary: {line}"")\n                    continue\n                source_word, target_word, prob = alignment_data\n                prob = float(prob)\n                source_index = src_dict.index(source_word)\n                target_index = dst_dict.index(target_word)\n                if (\n                    source_index not in src_dict.lexicon_indices\n                    and target_index in dst_dict.lexicon_indices\n                ):\n                    continue\n\n                if source_index is not None and target_index is not None:\n                    if source_index != current_source_index:\n                        # We\'ve finished processing the possible translation\n                        # candidates for this source token group, so save the\n                        # extracted translation candidates\n                        translation_candidates_saved += select_top_candidate_per_word(\n                            current_source_index,\n                            current_target_indices,\n                            counter_per_word,\n                            max_translation_candidates_per_word,\n                            translation_candidates,\n                            translation_candidates_set,\n                        )\n                        current_source_index = source_index\n                        current_target_indices = []\n\n                    if (\n                        target_index >= num_top_words\n                        and (source_index, target_index)\n                        not in translation_candidates_set\n                    ):\n                        current_target_indices.append((target_index, prob))\n        # Save the extracted translation candidates for the last source token\n        # group\n        translation_candidates_saved += select_top_candidate_per_word(\n            current_source_index,\n            current_target_indices,\n            counter_per_word,\n            max_translation_candidates_per_word,\n            translation_candidates,\n            translation_candidates_set,\n        )\n        logger.info(\n            f""Loaded {translation_candidates_saved} translation""\n            f""candidates from dictionary {lexical_dictionary}""\n        )\n    return translation_candidates\n\n\nclass VocabReduction(nn.Module):\n    def __init__(\n        self,\n        src_dict,\n        dst_dict,\n        vocab_reduction_params,\n        predictor=None,\n        fp16: bool = False,\n    ):\n        super().__init__()\n        self.src_dict = src_dict\n        self.dst_dict = dst_dict\n        self.vocab_reduction_params = vocab_reduction_params\n        self.predictor = predictor\n        self.fp16 = fp16\n        self.translation_candidates = None\n\n        if (\n            self.vocab_reduction_params is not None\n            and self.vocab_reduction_params[""max_translation_candidates_per_word""] > 0\n        ):\n            translation_candidates = get_translation_candidates(\n                self.src_dict,\n                self.dst_dict,\n                self.vocab_reduction_params[""lexical_dictionaries""],\n                self.vocab_reduction_params[""num_top_words""],\n                self.vocab_reduction_params[""max_translation_candidates_per_word""],\n            )\n            self.translation_candidates = nn.Parameter(\n                torch.Tensor(translation_candidates).long(), requires_grad=False\n            )\n\n    # encoder_output is default None for backwards compatibility\n    def forward(self, src_tokens, encoder_output=None, decoder_input_tokens=None):\n        assert self.dst_dict.pad() == 0, (\n            f""VocabReduction only works correctly when the padding ID is 0 ""\n            ""(to ensure its position in possible_translation_tokens is also 0), ""\n            f""instead of {self.dst_dict.pad()}.""\n        )\n        vocab_list = [src_tokens.new_tensor([self.dst_dict.pad()])]\n\n        if decoder_input_tokens is not None:\n            flat_decoder_input_tokens = decoder_input_tokens.view(-1)\n            vocab_list.append(flat_decoder_input_tokens)\n\n        if self.translation_candidates is not None:\n            reduced_vocab = self.translation_candidates.index_select(\n                dim=0, index=src_tokens.view(-1)\n            ).view(-1)\n            vocab_list.append(reduced_vocab)\n        if (\n            self.vocab_reduction_params is not None\n            and self.vocab_reduction_params[""num_top_words""] > 0\n        ):\n            top_words = torch.arange(\n                self.vocab_reduction_params[""num_top_words""],\n                device=vocab_list[0].device,\n            ).long()\n            vocab_list.append(top_words)\n\n        # Get bag of words predicted by word predictor\n        if self.predictor is not None:\n            assert encoder_output is not None\n            pred_output = self.predictor(encoder_output)\n            # [batch, k]\n            topk_indices = self.predictor.get_topk_predicted_tokens(\n                pred_output, src_tokens, log_probs=True\n            )\n            # flatten indices for entire batch [1, batch * k]\n            topk_indices = topk_indices.view(-1)\n            vocab_list.append(topk_indices.detach())\n\n        all_translation_tokens = torch.cat(vocab_list, dim=0)\n        possible_translation_tokens = torch.unique(\n            all_translation_tokens,\n            # Sorting helps ensure that the padding ID (0) remains in position 0.\n            sorted=True,\n            # The decoder_input_tokens used here are very close to the targets\n            # tokens that we also need to map to the reduced vocab space later\n            # on, except that decoder_input_tokens have <eos> prepended, while\n            # the targets will have <eos> at the end of the sentence. This\n            # prevents us from being able to directly use the inverse indices\n            # that torch.unique can return.\n            return_inverse=False,\n        ).type_as(src_tokens)\n\n        # Pad to a multiple of 8 to ensure training with fp16 will activate\n        # NVIDIA Tensor Cores.\n        len_mod_eight = possible_translation_tokens.shape[0] % 8\n        if self.training and self.fp16 and len_mod_eight != 0:\n            possible_translation_tokens = torch.cat(\n                [\n                    possible_translation_tokens,\n                    possible_translation_tokens.new_tensor(\n                        [self.dst_dict.pad()] * (8 - len_mod_eight)\n                    ),\n                ]\n            )\n\n        return possible_translation_tokens\n'"
pytorch_translate/weighted_criterions.py,0,"b'#!/usr/bin/env python3\n\nfrom fairseq import utils\nfrom fairseq.criterions import LegacyFairseqCriterion, register_criterion\nfrom fairseq.criterions.label_smoothed_cross_entropy import (\n    LabelSmoothedCrossEntropyCriterion,\n)\n\n\n@register_criterion(""weighted_label_smoothed_cross_entropy"")\nclass WeightedLabelSmoothedCrossEntropyCriterion(LegacyFairseqCriterion):\n    def __init__(self, args, task):\n        super().__init__(args, task)\n        self.eps = args.label_smoothing\n\n    @classmethod\n    def add_args(cls, parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--label-smoothing"",\n            default=0.0,\n            type=float,\n            metavar=""D"",\n            help=""epsilon for label smoothing, 0 means no label smoothing"",\n        )\n\n    def forward(self, model, sample, reduce=True):\n        net_output = model(**sample[""net_input""])\n        lprobs = model.get_normalized_probs(net_output, log_probs=True)\n        assert ""weights"" in sample, ""Need to specify weights for examples.""\n        weights = sample[""weights""].unsqueeze(1).unsqueeze(2)\n        lprobs = lprobs * weights\n\n        lprobs = lprobs.view(-1, lprobs.size(-1))\n        target = model.get_targets(sample, net_output).view(-1, 1)\n        non_pad_mask = target.ne(self.padding_idx)\n        nll_loss = -lprobs.gather(dim=-1, index=target)[non_pad_mask]\n        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)[non_pad_mask]\n        if reduce:\n            nll_loss = nll_loss.sum()\n            smooth_loss = smooth_loss.sum()\n        eps_i = self.eps / lprobs.size(-1)\n        loss = (1.0 - self.eps) * nll_loss + eps_i * smooth_loss\n\n        sample_size = (\n            sample[""target""].size(0) if self.args.sentence_avg else sample[""ntokens""]\n        )\n        logging_output = {\n            ""loss"": utils.item(loss.data) if reduce else loss.data,\n            ""nll_loss"": utils.item(nll_loss.data) if reduce else loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @classmethod\n    def aggregate_logging_outputs(cls, logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        return LabelSmoothedCrossEntropyCriterion.aggregate_logging_outputs(\n            logging_outputs\n        )\n'"
pytorch_translate/attention/__init__.py,0,"b'#!/usr/bin/env python3\n\nimport importlib\nimport os\n\nfrom pytorch_translate.attention.base_attention import BaseAttention\n\n\nATTENTION_REGISTRY = {}\n\n\ndef build_attention(attention_type, decoder_hidden_state_dim, context_dim, **kwargs):\n    return ATTENTION_REGISTRY[attention_type](\n        decoder_hidden_state_dim, context_dim, **kwargs\n    )\n\n\ndef register_attention(name):\n    """"""Decorator to register a new attention type.""""""\n\n    def register_attention_cls(cls):\n        if name in ATTENTION_REGISTRY:\n            raise ValueError(""Cannot register duplicate attention ({})"".format(name))\n        if not issubclass(cls, BaseAttention):\n            raise ValueError(\n                ""Attention ({} : {}) must extend BaseAttention"".format(\n                    name, cls.__name__\n                )\n            )\n        ATTENTION_REGISTRY[name] = cls\n        return cls\n\n    return register_attention_cls\n\n\n# automatically import any Python files in the attention/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith("".py"") and not file.startswith(""_""):\n        module = file[: file.find("".py"")]\n        importlib.import_module(""pytorch_translate.attention.{}"".format(module))\n'"
pytorch_translate/attention/attention_utils.py,2,"b'#!/usr/bin/env python3\n\nfrom typing import Dict, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\ndef create_src_lengths_mask(\n    batch_size: int, src_lengths: Tensor, max_src_len: Optional[int] = None\n):\n    """"""\n    Generate boolean mask to prevent attention beyond the end of source\n\n    Inputs:\n      batch_size : int\n      src_lengths : [batch_size] of sentence lengths\n      max_src_len: Optionally override max_src_len for the mask\n\n    Outputs:\n      [batch_size, max_src_len]\n    """"""\n    if max_src_len is None:\n        max_src_len = int(src_lengths.max())\n    src_indices = torch.arange(0, max_src_len).unsqueeze(0).type_as(src_lengths)\n    src_indices = src_indices.expand(batch_size, max_src_len)\n    src_lengths = src_lengths.unsqueeze(dim=1).expand(batch_size, max_src_len)\n    # returns [batch_size, max_seq_len]\n    return (src_indices < src_lengths).int().detach()\n\n\ndef masked_softmax(scores, src_lengths, src_length_masking=True):\n    """"""Apply source length masking then softmax.\n    Input and output have shape bsz x src_len""""""\n    if src_length_masking:\n        bsz, max_src_len = scores.size()\n        # compute masks\n        src_mask = create_src_lengths_mask(bsz, src_lengths)\n        # Fill pad positions with -inf\n        scores = scores.masked_fill(src_mask == 0, -np.inf)\n\n    # Cast to float and then back again to prevent loss explosion under fp16.\n    return F.softmax(scores.float(), dim=-1).type_as(scores)\n'"
pytorch_translate/attention/base_attention.py,1,"b'#!/usr/bin/env python3\n\nimport torch.nn as nn\n\n\nclass BaseAttention(nn.Module):\n    def __init__(self, decoder_hidden_state_dim, context_dim):\n        super().__init__()\n        self.decoder_hidden_state_dim = decoder_hidden_state_dim\n        self.context_dim = context_dim\n\n    def forward(self, decoder_state, source_hids, src_lengths):\n        """"""\n        Input\n            decoder_state: bsz x decoder_hidden_state_dim\n            source_hids: srclen x bsz x context_dim\n            src_lengths: bsz x 1, actual sequence lengths\n        Output\n            output: bsz x context_dim\n            attn_scores: max_src_len x bsz\n        """"""\n        raise NotImplementedError\n'"
pytorch_translate/attention/dot_attention.py,2,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_translate.attention import (\n    BaseAttention,\n    attention_utils,\n    register_attention,\n)\nfrom pytorch_translate.common_layers import Linear\n\n\n@register_attention(""dot"")\nclass DotAttention(BaseAttention):\n    def __init__(self, decoder_hidden_state_dim, context_dim, **kwargs):\n        super().__init__(decoder_hidden_state_dim, context_dim)\n\n        self.input_proj = None\n        force_projection = kwargs.get(""force_projection"", False)\n        if force_projection or decoder_hidden_state_dim != context_dim:\n            self.input_proj = Linear(decoder_hidden_state_dim, context_dim, bias=True)\n        self.src_length_masking = kwargs.get(""src_length_masking"", True)\n\n    def prepare_for_onnx_export_(self, **kwargs):\n        self.src_length_masking = False\n\n    def forward(self, decoder_state, source_hids, src_lengths):\n        # Reshape to bsz x src_len x context_dim\n        source_hids = source_hids.transpose(0, 1)\n        # decoder_state: bsz x context_dim\n        if self.input_proj is not None:\n            decoder_state = self.input_proj(decoder_state)\n        # compute attention (bsz x src_len x context_dim) * (bsz x context_dim x 1)\n        attn_scores = torch.bmm(source_hids, decoder_state.unsqueeze(2)).squeeze(2)\n\n        # Mask + softmax (bsz x src_len)\n        normalized_masked_attn_scores = attention_utils.masked_softmax(\n            attn_scores, src_lengths, self.src_length_masking\n        )\n\n        # Sum weighted sources\n        attn_weighted_context = (\n            (source_hids * normalized_masked_attn_scores.unsqueeze(2))\n            .contiguous()\n            .sum(1)\n        )\n\n        return attn_weighted_context, normalized_masked_attn_scores.t()\n'"
pytorch_translate/attention/mlp_attention.py,1,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_translate.attention import (\n    BaseAttention,\n    attention_utils,\n    register_attention,\n)\nfrom pytorch_translate.common_layers import Linear\n\n\n@register_attention(""mlp"")\nclass MLPAttention(BaseAttention):\n    """"""The original attention from Badhanau et al. (2014)\n    https://arxiv.org/abs/1409.0473 based on a Multi-Layer Perceptron.\n\n    The attention score between position i in the encoder and position j in the\n    decoder is:\n    alpha_ij = V_a * tanh(W_ae * enc_i + W_ad * dec_j + b_a)\n    """"""\n\n    def __init__(self, decoder_hidden_state_dim, context_dim, **kwargs):\n        super().__init__(decoder_hidden_state_dim, context_dim)\n\n        self.context_dim = context_dim\n        self.attention_dim = kwargs.get(""attention_dim"", context_dim)\n        # W_ae and b_a\n        self.encoder_proj = Linear(context_dim, self.attention_dim, bias=True)\n        # W_ad\n        self.decoder_proj = Linear(\n            decoder_hidden_state_dim, self.attention_dim, bias=False\n        )\n        # V_a\n        self.to_scores = Linear(self.attention_dim, 1, bias=False)\n        self.src_length_masking = kwargs.get(""src_length_masking"", True)\n\n    def prepare_for_onnx_export_(self, **kwargs):\n        self.src_length_masking = False\n\n    def forward(self, decoder_state, source_hids, src_lengths):\n        """"""The expected input dimensions are:\n\n        decoder_state: bsz x decoder_hidden_state_dim\n        source_hids: src_len x bsz x context_dim\n        src_lengths: bsz\n        """"""\n        src_len, bsz, _ = source_hids.size()\n        # (src_len*bsz) x context_dim (to feed through linear)\n        flat_source_hids = source_hids.view(-1, self.context_dim)\n        # (src_len*bsz) x attention_dim\n        encoder_component = self.encoder_proj(flat_source_hids)\n        # src_len x bsz x attention_dim\n        encoder_component = encoder_component.view(src_len, bsz, self.attention_dim)\n        # 1 x bsz x attention_dim\n        decoder_component = self.decoder_proj(decoder_state).unsqueeze(0)\n        # Sum with broadcasting and apply the non linearity\n        # src_len x bsz x attention_dim\n        hidden_att = F.tanh(\n            (decoder_component + encoder_component).view(-1, self.attention_dim)\n        )\n        # Project onto the reals to get attentions scores (bsz x src_len)\n        attn_scores = self.to_scores(hidden_att).view(src_len, bsz).t()\n\n        # Mask + softmax (src_len x bsz)\n        normalized_masked_attn_scores = attention_utils.masked_softmax(\n            attn_scores, src_lengths, self.src_length_masking\n        ).t()\n\n        # Sum weighted sources (bsz x context_dim)\n        attn_weighted_context = (\n            source_hids * normalized_masked_attn_scores.unsqueeze(2)\n        ).sum(0)\n\n        return attn_weighted_context, normalized_masked_attn_scores\n'"
pytorch_translate/attention/multihead_attention.py,0,"b'#!/usr/bin/env python3\n\nfrom typing import Optional\n\nfrom fairseq.modules import multihead_attention as fair_multihead\nfrom pytorch_translate.attention import (\n    BaseAttention,\n    attention_utils,\n    register_attention,\n)\nfrom torch import Tensor\n\n\n@register_attention(""multihead"")\nclass MultiheadAttention(BaseAttention):\n    """"""\n    Multiheaded Scaled Dot Product Attention\n\n    Implements equation:\n    MultiHead(Q, K, V) = Concat(head_1,...,head_h)W^O\n        where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n\n    Similarly to the above, d_k = d_v = d_model / h\n    In this implementation, keys and values are both set to encoder output\n\n    Inputs\n      init:\n        decoder_hidden_state_dim : dimensionality of decoder hidden state\n        context_dim : dimensionality of encoder output\n        kwargs :\n          nheads : integer # of attention heads\n          unseen_mask: if True, only attend to previous sequence positions\n          src_lengths_mask: if True, mask padding based on src_lengths\n\n      forward:\n        decoder_state : [batch size, d_model]\n        source_hids : [sequence length, batch size, d_model]\n        src_lengths : [batch size]\n\n      forward:\n        query : [sequence length, batch size, d_model]\n        key: [sequence length, batch size, d_model]\n        value: [sequence length, batch size, d_model]\n\n    Output\n      result : [batch_size,  d_model]\n    """"""\n\n    def __init__(\n        self,\n        decoder_hidden_state_dim,\n        context_dim,\n        *,\n        nheads=1,\n        unseen_mask=False,\n        src_length_mask=True\n    ):\n        super().__init__(decoder_hidden_state_dim, context_dim)\n        assert decoder_hidden_state_dim == context_dim\n        d_model = decoder_hidden_state_dim  # for brevity\n        assert d_model % nheads == 0\n\n        if unseen_mask:\n            raise NotImplementedError(\n                ""Unseen mask not supported with sequential decoding""\n            )\n        self._fair_attn = fair_multihead.MultiheadAttention(d_model, nheads)\n        self.use_src_length_mask = src_length_mask\n\n    def forward(\n        self,\n        decoder_state,\n        source_hids,\n        src_lengths,\n        squeeze: bool = True,\n        max_src_len: Optional[int] = None,\n    ):\n        """"""\n        Computes MultiheadAttention with respect to either a vector\n        or a tensor\n\n        Inputs:\n            decoder_state: (bsz x decoder_hidden_state_dim) or\n                (bsz x T x decoder_hidden_state_dim)\n            source_hids: srclen x bsz x context_dim\n            src_lengths: bsz x 1, actual sequence lengths\n            squeeze: Whether or not to squeeze on the time dimension.\n                Even if decoder_state.dim() is 2 dimensional an\n                explicit time step dimension will be unsqueezed.\n            max_src_len: Optionally override the max_src_len otherwise\n                inferred from src_lengths. Useful during beam search when we\n                might have already finalized the longest src_sequence\n        Outputs:\n          [batch_size, max_src_len] if decoder_state.dim() == 2 & squeeze\n            or\n          [batch_size, 1, max_src_len] if decoder_state.dim() == 2 & !squeeze\n            or\n          [batch_size, T, max_src_len] if decoder_state.dim() == 3 & !squeeze\n            or\n          [batch_size, T, max_src_len] if decoder_state.dim() == 3 & squeeze & T != 1\n            or\n          [batch_size, max_src_len] if decoder_state.dim() == 3 & squeeze & T == 1\n        """"""\n        batch_size = decoder_state.shape[0]\n        if decoder_state.dim() == 3:\n            query = decoder_state\n        elif decoder_state.dim() == 2:\n            query = decoder_state.unsqueeze(1)\n        else:\n            raise ValueError(""decoder state must be either 2 or 3 dimensional"")\n        query = query.transpose(0, 1)\n        value = key = source_hids\n\n        src_len_mask: Optional[Tensor] = None\n        if src_lengths is not None and self.use_src_length_mask:\n            # [batch_size, 1, seq_len]\n            src_len_mask_int = attention_utils.create_src_lengths_mask(\n                batch_size=batch_size, src_lengths=src_lengths, max_src_len=max_src_len\n            )\n            src_len_mask = src_len_mask_int != 1\n        attn, attn_weights = self._fair_attn.forward(\n            query, key, value, key_padding_mask=src_len_mask, need_weights=True\n        )\n        # attn.shape = T X bsz X embed_dim\n        # attn_weights.shape = bsz X T X src_len\n        if attn_weights is not None:\n            attn_weights = attn_weights.transpose(0, 2)\n        # attn_weights.shape = src_len X T X bsz\n\n        if squeeze:\n            attn = attn.squeeze(0)\n            # attn.shape = squeeze(T) X bsz X embed_dim\n            if attn_weights is not None:\n                attn_weights = attn_weights.squeeze(1)\n            # attn_weights.shape = src_len X squeeze(T) X bsz\n            return attn, attn_weights\n        return attn, attn_weights\n'"
pytorch_translate/attention/no_attention.py,1,"b'#!/usr/bin/env python3\n\nimport torch\nfrom pytorch_translate.attention import BaseAttention, register_attention\nfrom pytorch_translate.utils import maybe_cuda\n\n\n@register_attention(""no"")\nclass NoAttention(BaseAttention):\n    def __init__(self, decoder_hidden_state_dim, context_dim, **kwargs):\n        super().__init__(decoder_hidden_state_dim, 0)\n\n    def forward(self, decoder_state, source_hids, src_lengths):\n        return None, maybe_cuda(torch.zeros(1, src_lengths.shape[0]))\n'"
pytorch_translate/attention/pooling_attention.py,2,"b'#!/usr/bin/env python3\n\nimport torch\nfrom pytorch_translate.attention import (\n    BaseAttention,\n    attention_utils,\n    register_attention,\n)\nfrom torch.autograd import Variable\n\n\n@register_attention(""pooling"")\nclass PoolingAttention(BaseAttention):\n    def __init__(self, decoder_hidden_state_dim, context_dim, **kwargs):\n        super().__init__(decoder_hidden_state_dim, context_dim)\n\n        self.pool_type = kwargs.get(""pool_type"", ""mean"")\n\n    def forward(self, decoder_state, source_hids, src_lengths):\n        assert self.decoder_hidden_state_dim == self.context_dim\n        max_src_len = source_hids.size()[0]\n        assert max_src_len == src_lengths.data.max()\n        batch_size = source_hids.size()[1]\n\n        src_mask = (\n            attention_utils.create_src_lengths_mask(batch_size, src_lengths)\n            .type_as(source_hids)\n            .t()\n            .unsqueeze(2)\n        )\n\n        if self.pool_type == ""mean"":\n            # need to make src_lengths a 3-D tensor to normalize masked_hiddens\n            denom = src_lengths.view(1, batch_size, 1).type_as(source_hids)\n            masked_hiddens = source_hids * src_mask\n            context = (masked_hiddens / denom).sum(dim=0)\n        elif self.pool_type == ""max"":\n            masked_hiddens = source_hids - 10e6 * (1 - src_mask)\n            context = masked_hiddens.max(dim=0)[0]\n        else:\n            raise ValueError(f""Pooling type {self.pool_type} is not supported."")\n        attn_scores = Variable(\n            torch.ones(src_mask.shape[1], src_mask.shape[0]).type_as(source_hids.data),\n            requires_grad=False,\n        ).t()\n\n        return context, attn_scores\n\n\n@register_attention(""max"")\nclass MaxPoolingAttention(PoolingAttention):\n    def __init__(self, decoder_hidden_state_dim, context_dim, **kwargs):\n        super().__init__(decoder_hidden_state_dim, context_dim, pool_type=""max"")\n\n\n@register_attention(""mean"")\nclass MeanPoolingAttention(PoolingAttention):\n    def __init__(self, decoder_hidden_state_dim, context_dim, **kwargs):\n        super().__init__(decoder_hidden_state_dim, context_dim, pool_type=""mean"")\n'"
pytorch_translate/data/__init__.py,0,b''
pytorch_translate/data/char_data.py,12,"b'#!/usr/bin/env python3\n\nfrom typing import Any, Dict\n\nimport numpy as np\nimport torch\nfrom fairseq import data, tokenizer\nfrom fvcore.common.file_io import PathManager\nfrom pytorch_translate import vocab_constants\nfrom pytorch_translate.data.dictionary import TAGS\n\n\nclass InMemoryNumpyWordCharDataset(data.indexed_dataset.IndexedDataset):\n    """"""analogous to fairseq.data.IndexedCachedDataset""""""\n\n    def __init__(self, ignore_chars_for_unks=False):\n        """"""\n        Initialize empty dataset\n        Args:\n        ignore_chars_for_unks: If true, ignore character sequences of\n        unknown words.\n        """"""\n        self.word_buffer = None\n        self.word_offsets = None\n        self.char_buffer = None\n        self.char_offsets = None\n        self.ignore_chars_for_unks = ignore_chars_for_unks\n        self.sizes = None\n\n    def get_tokens(self, i):\n        """"""Get tensor of token indices for example i""""""\n        assert i < self.__len__(), f""index {i} out of range!""\n        a = self.word_buffer[self.word_offsets[i] : self.word_offsets[i + 1]]\n        return torch.from_numpy(a)\n\n    def get_chars_list(self, i):\n        """"""Get list of tensors of character indices for example i""""""\n        result = []\n        for word_index in range(self.word_offsets[i], self.word_offsets[i + 1]):\n            char_indices = self.char_buffer[\n                self.char_offsets[word_index] : self.char_offsets[word_index + 1]\n            ]\n            result.append(torch.from_numpy(char_indices))\n        return result\n\n    def __len__(self):\n        # offsets includes 0 and end indices for each example\n        return self.word_offsets.size - 1\n\n    def __del__(self):\n        pass\n\n    def save(self, path):\n        assert self.word_buffer is not None\n        assert self.word_offsets is not None\n        assert self.char_buffer is not None\n        assert self.char_offsets is not None\n        with PathManager.open(path, ""wb"") as f:\n            np.savez(\n                f,\n                word_buffer=self.word_buffer,\n                word_offsets=self.word_offsets,\n                char_buffer=self.char_buffer,\n                char_offsets=self.char_offsets,\n            )\n\n    def load(self, path):\n        with PathManager.open(path, ""rb"") as f:\n            npz = np.load(f)\n            if ""char_buffer"" not in npz or ""char_offsets"" not in npz:\n                raise RuntimeError(f""{path} does not appear to be a word-char dataset!"")\n            self.word_buffer = npz[""word_buffer""]\n            self.word_offsets = npz[""word_offsets""]\n            self.sizes = self.word_offsets[1:] - self.word_offsets[:-1]\n            self.char_buffer = npz[""char_buffer""]\n            self.char_offsets = npz[""char_offsets""]\n\n    def _sent_to_word_ids(\n        self, sent, word_dict, reverse_order, prepend_inds, append_inds\n    ):\n        """"""\n        Extract the word ids for words associated with the input sentence.\n        """"""\n        words = tokenizer.tokenize_line(sent)\n        if reverse_order:\n            words.reverse()\n        word_inds = [word_dict.index(w) for w in words]\n        word_inds = prepend_inds + word_inds + append_inds\n        return words, word_inds\n\n    def _word_to_char_ids(self, word, word_dict, char_dict, embed_bytes):\n        """"""\n        Extract the char/byte ids for char/bytes associated with the input word.\n        """"""\n        if embed_bytes:\n            # The byte_id needs to be incremented by 1 to account for the\n            # padding id (0) in the embedding table\n            char_inds = (\n                [vocab_constants.NUM_BYTE_INDICES + TAGS.index(word) + 1]\n                if word in TAGS\n                else [byte_id + 1 for byte_id in word.encode(""utf8"", ""ignore"")]\n            )\n        else:\n            if word in word_dict or not self.ignore_chars_for_unks:\n                chars = [word] if word in TAGS else list(word)\n                char_inds = [char_dict.index(c) for c in chars]\n            else:\n                char_inds = [char_dict.eos_index]\n        return char_inds\n\n    def parse(\n        self,\n        path,\n        word_dict,\n        char_dict,\n        embed_bytes=False,\n        reverse_order=False,\n        append_eos=False,\n    ):\n        word_array_list = []\n        word_offsets = [0]\n        char_array_list = []\n        char_offsets = [0]\n        sizes = []\n        prepend_inds = []\n        append_inds = []\n        if append_eos:\n            append_inds.append(word_dict.eos_index)\n        with PathManager.open(path, ""r"") as f:\n            for line in f:\n                words, word_inds = self._sent_to_word_ids(\n                    sent=line,\n                    word_dict=word_dict,\n                    reverse_order=reverse_order,\n                    prepend_inds=prepend_inds,\n                    append_inds=append_inds,\n                )\n                word_array_list.append(np.array(word_inds, dtype=np.int32))\n                word_offsets.append(word_offsets[-1] + len(word_inds))\n                sizes.append(len(word_inds))\n\n                for word in words:\n                    char_inds = self._word_to_char_ids(\n                        word, word_dict, char_dict, embed_bytes\n                    )\n                    char_array_list.append(np.array(char_inds, dtype=np.int32))\n                    char_offsets.append(char_offsets[-1] + len(char_inds))\n                if append_eos:\n                    char_inds = [char_dict.eos_index]\n                    char_array_list.append(np.array(char_inds, dtype=np.int32))\n                    char_offsets.append(char_offsets[-1] + len(char_inds))\n\n        self.word_buffer = np.concatenate(word_array_list)\n        self.word_offsets = np.array(word_offsets, dtype=np.int64)\n        self.char_buffer = np.concatenate(char_array_list)\n        self.char_offsets = np.array(char_offsets, dtype=np.int64)\n        self.sizes = np.array(sizes, dtype=np.int32)\n\n        del word_array_list, word_offsets, char_array_list, char_offsets, sizes\n\n    def parse_multilingual(\n        self,\n        corpora,\n        reverse_order,\n        append_eos,\n        embed_bytes,\n        prepend_language_id,\n        already_numberized,\n    ):\n        word_array_list = []\n        word_offsets = [0]\n        char_array_list = []\n        char_offsets = [0]\n        sizes = []\n        for corpus_config in corpora:\n            prepend_inds = []\n            append_inds = []\n            if append_eos:\n                append_inds.append(corpus_config.dict.eos_index)\n            if corpus_config.dialect_id is not None:\n                if prepend_language_id:\n                    prepend_inds.append(corpus_config.dialect_id)\n                else:\n                    append_inds.append(corpus_config.dialect_id)\n            with PathManager.open(corpus_config.data_file, ""r"") as f:\n                for line in f:\n                    words, word_inds = self._sent_to_word_ids(\n                        sent=line,\n                        word_dict=corpus_config.dict,\n                        reverse_order=reverse_order,\n                        prepend_inds=prepend_inds,\n                        append_inds=append_inds,\n                    )\n\n                    word_array_list.append(np.array(word_inds, dtype=np.int32))\n                    word_offsets.append(word_offsets[-1] + len(word_inds))\n                    sizes.append(len(word_inds))\n\n                    for word in words:\n                        char_inds = self._word_to_char_ids(\n                            word=word,\n                            word_dict=corpus_config.dict,\n                            char_dict=corpus_config.char_dict,\n                            embed_bytes=embed_bytes,\n                        )\n                        char_array_list.append(np.array(char_inds, dtype=np.int32))\n                        char_offsets.append(char_offsets[-1] + len(char_inds))\n                    if append_eos:\n                        char_inds = [corpus_config.char_dict.eos_index]\n                        char_array_list.append(np.array(char_inds, dtype=np.int32))\n                        char_offsets.append(char_offsets[-1] + len(char_inds))\n\n        self.word_buffer = np.concatenate(word_array_list)\n        self.word_offsets = np.array(word_offsets, dtype=np.int32)\n        self.char_buffer = np.concatenate(char_array_list)\n        self.char_offsets = np.array(char_offsets, dtype=np.int32)\n        self.sizes = np.array(sizes, dtype=np.int32)\n\n        del word_array_list, word_offsets, char_array_list, char_offsets, sizes\n\n    @staticmethod\n    def create_from_file(path):\n        result = InMemoryNumpyWordCharDataset()\n        result.load(path)\n        return result\n\n    def subsample(self, indices):\n        """"""\n        Subsample dataset to include only those items indexed by input\n        argument indices.\n        """"""\n        word_array_list = []\n        word_offsets = [0]\n        char_array_list = []\n        char_offsets = [0]\n        sizes = []\n        for i in indices:\n            word_inds = self.word_buffer[\n                self.word_offsets[i] : self.word_offsets[i + 1]\n            ]\n            word_array_list.append(word_inds)\n            word_offsets.append(word_offsets[-1] + len(word_inds))\n            sizes.append(len(word_inds))\n\n            for word_index in range(self.word_offsets[i], self.word_offsets[i + 1]):\n                char_inds = self.char_buffer[\n                    self.char_offsets[word_index] : self.char_offsets[word_index + 1]\n                ]\n                char_array_list.append(char_inds)\n                char_offsets.append(char_offsets[-1] + len(char_inds))\n\n        self.word_buffer = np.concatenate(word_array_list)\n        self.word_offsets = np.array(word_offsets, dtype=np.int32)\n        self.char_buffer = np.concatenate(char_array_list)\n        self.char_offsets = np.array(char_offsets, dtype=np.int32)\n        self.sizes = np.array(sizes, dtype=np.int32)\n\n\nclass LanguagePairSourceCharDataset(data.LanguagePairDataset):\n    """"""\n    Version of fairseq.data.LanguagePairDataset which represents source\n    sentences as sequences of words, each represented as a sequence of\n    characters (with numberized indices for both words and characters).\n    Right-padded only.\n    """"""\n\n    def __init__(\n        self,\n        src,\n        src_sizes,\n        src_dict,\n        tgt=None,\n        tgt_sizes=None,\n        tgt_dict=None,\n        weights=None,\n    ):\n        """"""\n        src : InMemoryNumpyWordCharDataset\n        tgt : InMemoryIndexedDataset\n        weights: Optional[IndexedInMemoryDataset]\n        """"""\n        super().__init__(\n            src,\n            src_sizes,\n            src_dict,\n            tgt,\n            tgt_sizes,\n            tgt_dict,\n            left_pad_source=False,\n            left_pad_target=False,\n        )\n        self.pad_idx = src_dict.pad()\n        self.eos_idx = src_dict.eos()\n        self.weights = weights\n\n    def get_src_maybe_with_weights(self, i):\n        example = {\n            ""id"": i,\n            ""source_tokens"": self.src.get_tokens(i).long(),\n            ""source_chars_list"": self.src.get_chars_list(i),\n        }\n        if self.weights:\n            """"""\n            If weight for example is missing, use last seen weight. Sometimes we\n            just want to assign a weight to the entire dataset with a single value\n            but also maintain the IndexedInMemoryDataset convention of weights.\n            This way, even if we don\'t care/know about dataset size, we can\n            assign same weight to all examples.\n            """"""\n            if len(self.weights) <= i:\n                example[""weight""] = self.weights[-1]\n            else:\n                example[""weight""] = self.weights[i]\n        else:\n            example[""weight""] = 1.0\n\n        return example\n\n    def __getitem__(self, i):\n        example = self.get_src_maybe_with_weights(i)\n        if self.tgt:\n            example[""target""] = self.tgt[i].long()\n        return example\n\n    def __len__(self):\n        """"""Length in words""""""\n        return len(self.src)\n\n    def collate_source(self, samples) -> Dict[str, Any]:\n        # sort in order of descending number of words\n        samples.sort(key=lambda s: len(s[""source_tokens""]), reverse=True)\n        max_words = len(samples[0][""source_tokens""])\n\n        id = torch.LongTensor([s[""id""] for s in samples])\n        src_lengths = torch.LongTensor([len(s[""source_tokens""]) for s in samples])\n\n        weights = torch.FloatTensor([s[""weight""] for s in samples])\n\n        word_lengths = torch.LongTensor(len(samples), max_words).fill_(0)\n        for i, s in enumerate(samples):\n            word_lengths_array = np.array([len(w) for w in s[""source_chars_list""]])\n            word_lengths[i, : word_lengths_array.size] = torch.LongTensor(\n                word_lengths_array\n            )\n        max_word_length = int(word_lengths.max())\n\n        src_tokens = (\n            samples[0][""source_tokens""].new(len(samples), max_words).fill_(self.pad_idx)\n        )\n        for i, s in enumerate(samples):\n            src_tokens[i, : len(s[""source_tokens""])] = s[""source_tokens""]\n\n        char_inds = (\n            samples[0][""source_chars_list""][0]\n            .new(len(samples), max_words, max_word_length)\n            .long()\n            .fill_(self.pad_idx)\n        )\n\n        for i, s in enumerate(samples):\n            chars_list = s[""source_chars_list""]\n            for j, chars in enumerate(chars_list):\n                char_inds[i, j, : word_lengths[i, j]] = chars\n        return {\n            ""id"": id,\n            ""src_tokens"": src_tokens,\n            ""src_lengths"": src_lengths,\n            ""char_inds"": char_inds,\n            ""word_lengths"": word_lengths,\n            ""weights"": weights,\n        }\n\n    def collate_targets(self, samples):\n        def merge(move_eos_to_beginning=False):\n            return data.data_utils.collate_tokens(\n                [s[""target""] for s in samples],\n                self.pad_idx,\n                self.eos_idx,\n                left_pad=False,\n                move_eos_to_beginning=move_eos_to_beginning,\n            )\n\n        target = merge(move_eos_to_beginning=False)\n        prev_output_tokens = merge(move_eos_to_beginning=True)\n\n        ntokens = sum(len(s[""target""]) for s in samples)\n\n        return target, prev_output_tokens, ntokens\n\n    def collater(self, samples):\n        if len(samples) == 0:\n            return {}\n        source_data = self.collate_source(samples)\n        target, prev_output_tokens, ntokens = None, None, None\n        if self.tgt:\n            target, prev_output_tokens, ntokens = self.collate_targets(samples)\n\n        return {\n            ""id"": source_data[""id""],\n            ""ntokens"": ntokens,\n            ""net_input"": {\n                ""src_tokens"": source_data[""src_tokens""],\n                ""src_lengths"": source_data[""src_lengths""],\n                ""char_inds"": source_data[""char_inds""],\n                ""word_lengths"": source_data[""word_lengths""],\n                ""prev_output_tokens"": prev_output_tokens,\n            },\n            ""target"": target,\n            ""weights"": source_data[""weights""],\n        }\n\n\nclass LanguagePairCharDataset(LanguagePairSourceCharDataset):\n    """"""\n    Version of fairseq.data.LanguagePairDataset which represents source\n    and target sentences as sequences of words, each represented as a\n    sequence of characters (with numberized indices for both words and\n    characters).\n    Right-padded only.\n    """"""\n\n    def __init__(\n        self,\n        src: InMemoryNumpyWordCharDataset,\n        src_sizes,\n        src_dict,\n        tgt: InMemoryNumpyWordCharDataset = None,\n        tgt_sizes=None,\n        tgt_dict=None,\n        weights=None,\n    ):\n        super().__init__(src, src_sizes, src_dict, tgt, tgt_sizes, tgt_dict)\n\n    def __getitem__(self, i):\n        example = self.get_src_maybe_with_weights(i)\n        if self.tgt:\n            example[""target""] = self.tgt.get_tokens(i).long()\n            example[""target_chars_list""] = self.tgt.get_chars_list(i)\n\n        return example\n\n    def collate_tgt_chars(self, samples) -> Dict[str, Any]:\n        max_tgt_words = max(len(s[""target""]) for s in samples)\n        tgt_word_lengths = torch.LongTensor(len(samples), max_tgt_words).fill_(0)\n        for i, s in enumerate(samples):\n            word_lengths_array = np.array([len(w) for w in s[""target_chars_list""]])\n            tgt_word_lengths[i, : word_lengths_array.size] = torch.LongTensor(\n                word_lengths_array\n            )\n        max_tgt_word_length = int(tgt_word_lengths.max())\n\n        tgt_char_inds = (\n            samples[0][""target_chars_list""][0]\n            .new(len(samples), max_tgt_words, max_tgt_word_length)\n            .long()\n            .fill_(self.pad_idx)\n        )\n        prev_tgt_char_inds = (\n            samples[0][""target_chars_list""][0]\n            .new(len(samples), max_tgt_words, max_tgt_word_length)\n            .long()\n            .fill_(self.pad_idx)\n        )\n        eos_tensor = torch.tensor([self.eos_idx])\n        for i, s in enumerate(samples):\n            chars_list = s[""target_chars_list""]\n            prev_tgt_char_inds[i, 0, :1] = eos_tensor\n            for j, chars in enumerate(chars_list):\n                tgt_char_inds[i, j, : tgt_word_lengths[i, j]] = chars\n                if j < prev_tgt_char_inds.size()[1] - 1:\n                    # We skip previous characters for the last word.\n                    prev_tgt_char_inds[i, j + 1, : tgt_word_lengths[i, j]] = chars\n\n        prev_tgt_word_lengths = torch.cat(\n            (torch.ones((len(samples), 1), dtype=torch.long), tgt_word_lengths), dim=1\n        )\n\n        return {\n            ""prev_tgt_char_inds"": prev_tgt_char_inds,\n            ""tgt_char_inds"": tgt_char_inds,\n            ""tgt_word_lengths"": tgt_word_lengths,\n            ""prev_tgt_word_lengths"": prev_tgt_word_lengths,\n        }\n\n    def collater(self, samples):\n        if len(samples) == 0:\n            return {}\n        source_data = self.collate_source(samples)\n        target_toks, prev_output_tokens, ntokens = None, None, None\n        prev_tgt_char_inds, tgt_char_inds, tgt_word_lengths = None, None, None\n        prev_tgt_word_lengths = None\n        if self.tgt:\n            target_toks, prev_output_tokens, ntokens = self.collate_targets(samples)\n            tgt_char_data = self.collate_tgt_chars(samples)\n            prev_tgt_char_inds = tgt_char_data[""prev_tgt_char_inds""]\n            tgt_char_inds = tgt_char_data[""tgt_char_inds""]\n            tgt_word_lengths = tgt_char_data[""tgt_word_lengths""]\n            prev_tgt_word_lengths = tgt_char_data[""prev_tgt_word_lengths""]\n        return {\n            ""id"": source_data[""id""],\n            ""ntokens"": ntokens,\n            ""net_input"": {\n                ""src_tokens"": source_data[""src_tokens""],\n                ""src_lengths"": source_data[""src_lengths""],\n                ""char_inds"": source_data[""char_inds""],\n                ""word_lengths"": source_data[""word_lengths""],\n                ""prev_output_tokens"": prev_output_tokens,\n                ""prev_output_chars"": prev_tgt_char_inds,\n                ""prev_output_word_lengths"": prev_tgt_word_lengths,\n            },\n            ""target"": target_toks,\n            ""target_char_inds"": tgt_char_inds,\n            ""tgt_word_lengths"": tgt_word_lengths,\n            ""weights"": source_data[""weights""],\n        }\n'"
pytorch_translate/data/data.py,4,"b'#!/usr/bin/env python3\n\nimport os\nimport shutil\nimport tempfile\nfrom typing import Dict, NamedTuple, Optional\n\nimport numpy as np\nimport torch\nfrom fairseq import data, tokenizer\nfrom fvcore.common.file_io import PathManager\nfrom pytorch_translate import constants\nfrom pytorch_translate.data import dictionary as pytorch_translate_dictionary\n\n\n# The n-th source|target language is represented with the token\n# n+MULTILING_DIALECT_ID_OFFSET in the source|target token sequence.\nMULTILING_DIALECT_ID_OFFSET = 10\n\n# Read bigger arrays from disc instead of memory\nARRAY_SIZE_LIMIT_FOR_MEMORY = 10 ** 10  # 10GB\n\n\nclass CorpusConfig(NamedTuple):\n    dialect: str\n    data_file: str\n\n\nclass MultilingualCorpusConfig(NamedTuple):\n    dialect_id: Optional[int]\n    data_file: str\n    dict: pytorch_translate_dictionary.Dictionary\n    oversampling: int\n    char_dict: Optional[pytorch_translate_dictionary.Dictionary] = None\n\n\nclass ParallelCorpusConfig(NamedTuple):\n    source: CorpusConfig\n    target: CorpusConfig\n    weights_file: Optional[str] = None\n\n\nclass ParallelCorporaMapConfig(NamedTuple):\n    src_files: Dict[str, str]\n    tgt_files: Dict[str, str]\n\n\nclass InMemoryIndexedDataset(data.indexed_dataset.IndexedDataset):\n    """"""analogous to fairseq.data.indexed_dataset.IndexedCachedDataset.\n       Support loading .idx + .bin as fairseq does and also .npz. In self\n       initializer if\n       (1) path is passed in: call the initializer of parent class and loads\n       idx + bin from the path.\n       (2) path is None: Initialize an empty class. Call load(path) to load\n       .npz data.\n       When self.is_npz=True, use the implementation in this class which overrides\n       parent methods. Otherwise use parent functions in fairseq.""""""\n\n    def __init__(self, path=None, fix_lua_indexing=False, read_data=True):\n        if path is None:\n            self.buffer = None\n            self.offsets = None\n            self.sizes = None\n            self.is_npz = True\n        else:\n            self.is_npz = False\n            super().__init__(path, fix_lua_indexing)\n\n    def __getitem__(self, i):\n        if self.is_npz:\n            assert i < self.__len__(), f""index {i} out of range!""\n            a = self.buffer[self.offsets[i] : self.offsets[i + 1]]\n            return torch.from_numpy(a).long()\n        else:\n            return super().__getitem__(i)\n\n    def __len__(self):\n        if self.is_npz:\n            # offsets includes 0 and end indices for each example\n            return self.offsets.size - 1\n        else:\n            return super().__len__()\n\n    def __del__(self):\n        if self.is_npz:\n            if isinstance(self.buffer, np.memmap):\n                os.remove(self.buffer.filename)\n        else:\n            super().__del__()\n\n    def save(self, path):\n        assert self.buffer is not None\n        assert self.offsets is not None\n        with PathManager.open(path, ""wb"") as f:\n            np.savez(f, buffer=self.buffer, offsets=self.offsets)\n\n    def reverse(self, eos_token=True):\n        for i in range(len(self.offsets) - 1):\n            start_offset = self.offsets[i]\n            end_offset = self.offsets[i + 1] - 1 if eos_token else self.offsets[i + 1]\n\n            self.buffer[start_offset:end_offset] = self.buffer[start_offset:end_offset][\n                ::-1\n            ]\n\n    def load(self, path, num_examples_limit: Optional[int] = None):\n        with PathManager.open(path, ""rb"") as f:\n            npz = np.load(f)\n\n            # For big input data, we don\'t want the cpu to OOM.\n            # Therefore, we are loading the huge buffer array into disc\n            # and reading it from disc instead of memory.\n            if npz[""buffer""].nbytes > ARRAY_SIZE_LIMIT_FOR_MEMORY:\n                self.buffer = np.memmap(\n                    tempfile.NamedTemporaryFile().name,\n                    dtype=""float32"",\n                    mode=""w+"",\n                    shape=npz[""buffer""].shape,\n                )\n                self.buffer[:] = npz[""buffer""][:]\n            else:\n                self.buffer = npz[""buffer""]\n            self.offsets = npz[""offsets""]\n\n        if num_examples_limit is not None and len(self.offsets) > num_examples_limit:\n            self.offsets = self.offsets[: num_examples_limit + 1]\n            self.buffer = self.buffer[: self.offsets[-1]]\n        self.sizes = self.offsets[1:] - self.offsets[:-1]\n\n    def parse(\n        self,\n        path,\n        dictionary,\n        reverse_order=False,\n        append_eos=False,\n        already_numberized=False,\n    ):\n        self.parse_multilingual(\n            [\n                MultilingualCorpusConfig(\n                    dialect_id=None, data_file=path, dict=dictionary, oversampling=1\n                )\n            ],\n            reverse_order=reverse_order,\n            append_eos=append_eos,\n            already_numberized=already_numberized,\n        )\n\n    def parse_multilingual(\n        self,\n        corpora,\n        reverse_order=False,\n        append_eos=False,\n        prepend_language_id=True,\n        already_numberized=False,\n    ):\n        """"""Add sentences from text files to the dataset.\n\n        This method reads pairs of text files containing source and target\n        sides of a bitext. Sentences are converted to integer sequences by\n        tokenization and dictionary look-up. Note that this method removes all\n        sentences which have been previously added to the data set.\n\n        Example (single sentence):\n            token_sequence = [123, 234, 345]\n            dict.eos_idx = 2\n            dialect_id = 10\n            Result:\n                reverse_order=False, append_eos=True, prepend_language_id=True:\n                    [10, 123, 234, 345, 2]\n                reverse_order=False, append_eos=True, prepend_language_id=False:\n                    [123, 234, 345, 2, 10]\n                reverse_order=True, append_eos=True, prepend_language_id=True:\n                    [10, 345, 234, 123, 2]\n                reverse_order=True, append_eos=True, prepend_language_id=False:\n                    [345, 234, 123, 2, 10]\n\n        Args:\n            corpora: List of MultilingualCorpusConfig. If dialect_id is not\n                None, it is added to the token sequence.\n            reverse_order (bool): Whether to reverse the integer token sequence.\n            append_eos (bool): Whether to add the end-of-sentence symbol to each\n                sentence.\n            prepend_language_id (bool): Only used if dialect_id is not None. If\n                true, add ID at the begin of the token sequence. Otherwise, add\n                it at the end of the token sequence.\n            already_numberized (bool): If data_file contains lines of\n                numberized tokens, then already_numberized should be set to True\n                If data_file contains raw text sentences, then\n                already_numberized should be False (default) -- in which case\n                each line is tokenized with tokenizer then numberized with the\n                dictionary before being added to the output buffer.\n\n        """"""\n        array_list = []\n        offsets = [0]\n        sizes = []\n        print(corpora)\n        for corpus_config in corpora:\n            print(corpus_config)\n            print(corpus_config.data_file)\n            prepend_inds = []\n            append_inds = []\n            if append_eos:\n                append_inds.append(corpus_config.dict.eos_index)\n            if corpus_config.dialect_id is not None:\n                if prepend_language_id:\n                    prepend_inds.append(corpus_config.dialect_id)\n                else:\n                    append_inds.append(corpus_config.dialect_id)\n            with PathManager.open(corpus_config.data_file, ""r"") as f:\n                for line in f:\n                    if already_numberized:\n                        inds = line.strip().split()\n                        inds = [int(ind) for ind in inds]\n                    else:\n                        words = tokenizer.tokenize_line(line)\n                        inds = [corpus_config.dict.index(w) for w in words]\n\n                    if reverse_order:\n                        inds.reverse()\n                    inds = prepend_inds + inds + append_inds\n                    for _ in range(corpus_config.oversampling):\n                        array_list.append(np.array(inds, dtype=np.int32))\n                        offsets.append(offsets[-1] + len(inds))\n                        sizes.append(len(inds))\n\n        self.buffer = np.concatenate(array_list)\n        self.offsets = np.array(offsets, dtype=np.int64)\n        self.sizes = np.array(sizes, dtype=np.int32)\n        del array_list\n        del offsets\n        del sizes\n\n    def load_from_sequences(self, sequences):\n        """"""\n        Load data set from a list of sequences, each a list or numpy array of\n        indices. Note that this method removes all sentences which have been\n        previously added to the data set.\n        """"""\n        array_list = []\n        offsets = [0]\n        sizes = []\n        for inds in sequences:\n            array_list.append(np.array(inds, dtype=np.int32))\n            offsets.append(offsets[-1] + len(inds))\n            sizes.append(len(inds))\n\n        self.buffer = np.concatenate(array_list)\n        self.offsets = np.array(offsets, dtype=np.int32)\n        self.sizes = np.array(sizes, dtype=np.int32)\n        del array_list\n        del offsets\n        del sizes\n\n    @staticmethod\n    def create_from_file(path, is_npz=True, num_examples_limit: Optional[int] = None):\n        if is_npz:\n            # npz format\n            result = InMemoryIndexedDataset()\n            result.load(path, num_examples_limit=num_examples_limit)\n            return result\n        else:\n            # idx, bin format\n            if path.startswith(""manifold://""):\n                tempdir = tempfile.mkdtemp()\n                basename = os.path.basename(path)\n                prefix = os.path.join(tempdir, basename)\n                for suffix in (""bin"", ""idx""):\n                    remote_path = f""{path}.{suffix}""\n                    local_path = PathManager.get_local_path(remote_path)\n                    shutil.copy(local_path, f""{prefix}.{suffix}"")\n                path = prefix\n            return data.indexed_dataset.MMapIndexedDataset(path)\n\n    def subsample(self, indices):\n        """"""\n        Subsample dataset to include only those items indexed by input\n        argument indices.\n        """"""\n        array_list = []\n        offsets = [0]\n        sizes = []\n        for i in indices:\n            array_list.append(self[i])\n            offsets.append(offsets[-1] + len(array_list[-1]))\n            sizes.append(len(array_list[-1]))\n        self.buffer = np.concatenate(array_list)\n        self.offsets = np.array(offsets, dtype=np.int32)\n        self.sizes = np.array(sizes, dtype=np.int32)\n\n\ndef is_multilingual_many_to_one(args):\n    """"""\n    Checks whether we would be using the multilingual implementation in\n    pytorch_translate/multilingual.py. This is currently used as the default\n    many-to-one multilingual architecture.\n    """"""\n    if hasattr(args, ""multiling_encoder_lang""):\n        return bool(args.multiling_encoder_lang)\n    return args.multiling_source_lang is not None\n\n\ndef is_multilingual(args):\n    """"""\n    Checks whether we would be using the multilingual implementation that\n    extends fariseq\'s MultilingualTranslationTask and FairseqMultiModel\n    """"""\n    return args.task == constants.MULTILINGUAL_TRANSLATION_TASK\n\n\ndef is_latent_variable(args):\n    return args.task == constants.LATENT_VARIABLE_TASK\n\n\nclass IndexedRawTextDatasetWithLangId(data.IndexedRawTextDataset):\n    """"""Adds language IDs to an IndexedRawTextDataset""""""\n\n    def __init__(\n        self,\n        path,\n        dictionary,\n        lang_id,\n        append_eos=True,\n        reverse_order=False,\n        prepend_language_id=True,\n    ):\n        self.lang_id = lang_id\n        self.prepend_language_id = prepend_language_id\n        super(IndexedRawTextDatasetWithLangId, self).__init__(\n            path=path,\n            dictionary=dictionary,\n            append_eos=append_eos,\n            reverse_order=reverse_order,\n        )\n\n    def read_data(self, path, dictionary):\n        super(IndexedRawTextDatasetWithLangId, self).read_data(path, dictionary)\n        # Postprocess self.tokens_list and self.sizes\n        self.sizes += 1\n        lang_id_tensor = torch.LongTensor([self.lang_id + MULTILING_DIALECT_ID_OFFSET])\n\n        def add_lang_id(tokens):\n            if self.prepend_language_id:\n                return torch.cat([lang_id_tensor, tokens])\n            return torch.cat([tokens, lang_id_tensor])\n\n        self.tokens_list = [add_lang_id(t) for t in self.tokens_list]\n\n\ndef subsample_pair_dataset(dataset, num_samples, seed_num=-1):\n    if len(dataset) <= num_samples:\n        return\n    if seed_num != -1:\n        np.random.seed(seed_num)\n    indices = np.random.permutation(len(dataset))[:num_samples]\n    dataset.src.subsample(indices)\n    dataset.src_sizes = dataset.src.sizes\n    dataset.tgt.subsample(indices)\n    dataset.tgt_sizes = dataset.tgt.sizes\n'"
pytorch_translate/data/dictionary.py,0,"b'#!/usr/bin/env python3\n\nimport os\nimport re\nfrom typing import Dict, List, Optional, Set\n\nfrom fairseq.data import dictionary\n\n# TODO(T55884145): Replace with\n# from fvcore.common.file_io import PathManager\nfrom fairseq.file_io import PathManager\nfrom pytorch_translate import vocab_constants\n\n\nTAGS = [\n    ""@DIGITS"",\n    ""@EMOTICON"",\n    ""@FBENTITY"",\n    ""@MULTIPUNCT"",\n    ""@NOTRANSLATE"",\n    ""@PERSON"",\n    ""@PLAIN"",\n    ""@URL"",\n    ""@USERNAME"",\n]\n\nSPACE_NORMALIZER = re.compile(r""\\s+"")\n\n\ndef default_dictionary_path(save_dir: str, dialect: str) -> str:\n    return os.path.join(save_dir, f""dict.{dialect}.txt"")\n\n\ndef default_char_dictionary_path(save_dir: str, dialect: str) -> str:\n    return os.path.join(save_dir, f""char-dict.{dialect}.txt"")\n\n\ndef tokenize_line(line, embed_bytes=False):\n    line = SPACE_NORMALIZER.sub("" "", line)\n    line = line.strip()\n    return line.split()\n\n\ndef char_tokenize_line(line):\n    words = tokenize_line(line)\n    chars = []\n    for word in words:\n        if word in TAGS:\n            chars.append(word)\n        else:\n            chars.extend(c for c in word)\n    return chars\n\n\ndef add_file_to_dictionary(filename, dict, tokenize):\n    with PathManager.open(filename, ""r"", encoding=""utf-8"") as f:\n        for line in f:\n            for word in tokenize(line):\n                dict.add_symbol(word)\n            dict.add_symbol(dict.eos_word)\n\n\nclass Dictionary(dictionary.Dictionary):\n    """"""A mapping from symbols to consecutive integers""""""\n\n    def __init__(\n        self,\n        pad: str = ""<pad>"",\n        eos: str = ""</s>"",\n        unk: str = ""<unk>"",\n        bos: str = ""<s>"",\n        max_special_tokens: int = vocab_constants.MAX_SPECIAL_TOKENS,\n    ) -> None:\n        self.unk_word, self.pad_word, self.eos_word = unk, pad, eos\n        self.symbols: List[str] = []\n        self.count: List[int] = []\n        self.indices: Dict[str, int] = {}\n        self.lexicon_indices: Set[int] = set()\n\n        self.pad_index = self.add_symbol(pad)\n        assert self.pad_index == vocab_constants.PAD_ID\n\n        # Adds a junk symbol for vocab_constants\' GO_ID\n        self.add_symbol(""<reserved>"")\n\n        self.eos_index = self.add_symbol(eos)\n        assert self.eos_index == vocab_constants.EOS_ID\n\n        self.unk_index = self.add_symbol(unk)\n        assert self.unk_index == vocab_constants.UNK_ID\n        self.bos_index = self.add_symbol(bos)\n\n        # Adds junk symbols to pad up to the number of special tokens.\n        num_reserved = max_special_tokens - len(self.symbols)\n        for i in range(num_reserved):\n            self.add_symbol(f""<reserved_{i}>"")\n\n        self.nspecial = len(self.symbols)\n        assert self.nspecial == max_special_tokens\n\n    def lexicon_indices_list(self) -> List[int]:\n        return list(self.lexicon_indices)\n\n    @classmethod\n    def build_vocab_file(\n        cls,\n        corpus_files: List[str],\n        vocab_file: str,\n        max_vocab_size: int,\n        tokens_with_penalty: Optional[str] = None,\n        is_char_vocab: bool = False,\n        embed_bytes: bool = False,\n        padding_factor: int = 8,\n    ) -> ""Dictionary"":  # https://www.python.org/dev/peps/pep-0484/#forward-references\n        d = cls()\n\n        tokenize = char_tokenize_line if is_char_vocab else tokenize_line\n        embed_bytes = embed_bytes and is_char_vocab\n\n        # if we are embedding byte ids then no need to add these to the dict\n        # the ids an be obtained directly from the character\n        if not embed_bytes:\n            for corpus_file in corpus_files:\n                add_file_to_dictionary(filename=corpus_file, dict=d, tokenize=tokenize)\n\n        # Set indices to receive penalty\n        if tokens_with_penalty:\n            # Assume input tokens are unique\n            lexicon = []\n            with PathManager.open(tokens_with_penalty, ""r"", encoding=""utf-8"") as f:\n                for line in f:\n                    tokens = line.strip().split()\n                    if len(tokens) == 1:\n                        lexicon.append(tokens[0])\n\n            for token, token_index in d.indices.items():\n                if token in lexicon:\n                    d.lexicon_indices.add(token_index)\n\n        nwords = -1 if max_vocab_size <= 0 else max_vocab_size + d.nspecial\n        d.finalize(nwords=nwords, padding_factor=padding_factor)\n        d.save(vocab_file)\n        print(f""Generated new vocab file saved at {vocab_file}."")\n        if max_vocab_size < 0:\n            print(""No maximum vocab sized enforced."")\n        else:\n            print(f""Maximum vocab size {max_vocab_size}"")\n        return d\n\n    @classmethod\n    def build_vocab_file_if_nonexistent(\n        cls,\n        corpus_files: List[str],\n        vocab_file: str,\n        max_vocab_size: int,\n        tokens_with_penalty: Optional[str] = None,\n        is_char_vocab: bool = False,\n        embed_bytes: bool = False,\n        padding_factor: int = 8,\n    ) -> ""Dictionary"":  # https://www.python.org/dev/peps/pep-0484/#forward-references\n        if PathManager.isfile(vocab_file):\n            d = cls.load(vocab_file)\n            print(\n                f""Re-using existing vocab file {vocab_file}. Specified ""\n                f""max vocab size of {max_vocab_size} may not be enforced.""\n            )\n            return d\n\n        print(\n            f""Vocab file {vocab_file} does not exist. ""\n            ""Creating new vocab file at that path.""\n        )\n        return cls.build_vocab_file(\n            corpus_files=corpus_files,\n            vocab_file=vocab_file,\n            max_vocab_size=max_vocab_size,\n            tokens_with_penalty=tokens_with_penalty,\n            is_char_vocab=is_char_vocab,\n            embed_bytes=embed_bytes,\n            padding_factor=padding_factor,\n        )\n\n\nclass CharDictionary(Dictionary):\n    """"""Character vocab with its additonal special tokens.""""""\n\n    def __init__(self, word_delim=""<space>"", **kwargs):\n        super().__init__(**kwargs)\n        self.word_delim = word_delim\n        self.bow_index = self.add_symbol(""<bow>"")\n        self.eow_index = self.add_symbol(""<eow>"")\n        self.word_delim_index = self.add_symbol(word_delim)\n        self.nspecial += 3\n\n\nclass MaxVocabDictionary(Dictionary):\n    """"""This dictionary takes the form of the largest dictionary supplied via push().""""""\n\n    def push(self, d: Dictionary):\n        if len(d) > len(self):\n            self.copy_from(d)\n\n    def copy_from(self, d: dictionary.Dictionary):\n        """"""Makes self a shallow copy of d.""""""\n        self.unk_word = d.unk_word\n        self.pad_word = d.pad_word\n        self.eos_word = d.eos_word\n        self.symbols = d.symbols\n        self.count = d.count\n        self.indices = d.indices\n        self.pad_index = d.pad_index\n        self.eos_index = d.eos_index\n        self.unk_index = d.unk_index\n        self.nspecial = d.nspecial\n        self.lexicon_indices = d.lexicon_indices\n'"
pytorch_translate/data/iterators.py,2,"b'#!/usr/bin/env python3\n\nfrom fairseq.data import RoundRobinZipDatasets, iterators\nfrom pytorch_translate.data import weighted_data\n\n\nclass WeightedEpochBatchIterator(iterators.EpochBatchIterator):\n    def __init__(\n        self,\n        dataset,\n        collate_fn,\n        batch_sampler,\n        seed=1,\n        num_shards=1,\n        shard_id=0,\n        num_workers=0,\n        weights=None,\n    ):\n        """"""\n        Extension of fairseq.iterators.EpochBatchIterator to use an additional\n        weights structure. This weighs datasets as a function of epoch value.\n\n        Args:\n            dataset (~torch.utils.data.Dataset): dataset from which to load the data\n            collate_fn (callable): merges a list of samples to form a mini-batch\n            batch_sampler (~torch.utils.data.Sampler): an iterator over batches of\n                indices\n            seed (int, optional): seed for random number generator for\n                reproducibility (default: 1).\n            num_shards (int, optional): shard the data iterator into N\n                shards (default: 1).\n            shard_id (int, optional): which shard of the data iterator to\n                return (default: 0).\n            num_workers (int, optional): how many subprocesses to use for data\n                loading. 0 means the data will be loaded in the main process\n                (default: 0).\n            weights: is of the format [(epoch, {dataset: weight})]\n        """"""\n        super().__init__(\n            dataset=dataset,\n            collate_fn=collate_fn,\n            batch_sampler=batch_sampler,\n            seed=seed,\n            num_shards=num_shards,\n            shard_id=shard_id,\n            num_workers=num_workers,\n        )\n        self.weights = weights\n\n    def next_epoch_itr(self, shuffle=True, fix_batches_to_gpus=False):\n        """"""Return a new iterator over the dataset.\n\n        Args:\n            shuffle (bool, optional): shuffle batches before returning the\n                iterator. Default: ``True``\n            fix_batches_to_gpus: ensure that batches are always\n                allocated to the same shards across epochs. Requires\n                that :attr:`dataset` supports prefetching. Default:\n                ``False``\n        """"""\n        if self.weights and isinstance(self.dataset, RoundRobinZipDatasets):\n            """"""\n            Set dataset weight based on schedule and current epoch\n            """"""\n            prev_scheduled_epochs = 0\n            dataset_weights_map = None\n            for schedule in self.weights:\n                # schedule looks like (num_epochs, {dataset: weight})\n                if self.epoch <= schedule[0] + prev_scheduled_epochs:\n                    dataset_weights_map = schedule[1]\n                    break\n                prev_scheduled_epochs += schedule[0]\n            # Use last weights map if weights map is not specified for the current epoch\n            if dataset_weights_map is None:\n                dataset_weights_map = self.weights[-1][1]\n            for dataset_name in self.dataset.datasets:\n                if dataset_name in dataset_weights_map:\n                    assert isinstance(\n                        self.dataset.datasets[dataset_name],\n                        weighted_data.WeightedLanguagePairDataset,\n                    ) or isinstance(\n                        self.dataset.datasets[dataset_name],\n                        weighted_data.WeightedBacktranslationDataset,\n                    )\n                    self.dataset.datasets[dataset_name].weights = [\n                        dataset_weights_map[dataset_name]\n                    ]\n        if self._next_epoch_itr is not None:\n            self._cur_epoch_itr = self._next_epoch_itr\n            self._next_epoch_itr = None\n        else:\n            self.epoch += 1\n            self._cur_epoch_itr = self._get_iterator_for_epoch(\n                self.epoch, shuffle, fix_batches_to_gpus=fix_batches_to_gpus\n            )\n        return self._cur_epoch_itr\n'"
pytorch_translate/data/language_pair_upsampling_dataset.py,0,"b'#!/usr/bin/env python3\n\nimport numpy as np\nfrom fairseq.data.concat_dataset import ConcatDataset\n\n\nclass LanguagePairUpsamplingDataset(ConcatDataset):\n    def __init__(self, datasets, sample_ratios=1):\n        super(LanguagePairUpsamplingDataset, self).__init__(datasets, sample_ratios)\n        if isinstance(sample_ratios, float):\n            self.memoized_sizes = [self.size(idx) for idx in range(len(self))]\n        else:\n            self.memoized_sizes = np.concatenate(\n                [\n                    np.tile(ds.src_sizes, sr)\n                    for ds, sr in zip(self.datasets, self.sample_ratios)\n                ]\n            )\n\n    @property\n    def sizes(self):\n        return self.memoized_sizes\n'"
pytorch_translate/data/masked_lm_dictionary.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\n\nfrom typing import Dict, List, Set\n\nfrom pytorch_translate import vocab_constants\nfrom pytorch_translate.data.dictionary import Dictionary\n\n\nclass MaskedLMDictionary(Dictionary):\n    """"""\n    Dictionary for Masked Language Modelling tasks. This extends Dictionary by\n    adding the mask symbol.\n    """"""\n\n    def __init__(\n        self,\n        pad=""<pad>"",\n        eos=""</s>"",\n        unk=""<unk>"",\n        mask=""<mask>"",\n        max_special_tokens: int = vocab_constants.MAX_SPECIAL_TOKENS,\n    ):\n        self.symbols: List[str] = []\n        self.count: List[int] = []\n        self.indices: Dict[str, int] = {}\n        self.lexicon_indices: Set[int] = set()\n\n        self.pad_word = pad\n        self.pad_index = self.add_symbol(pad)\n        assert self.pad_index == vocab_constants.PAD_ID\n\n        # Adds a junk symbol for vocab_constants\' GO_ID\n        self.add_symbol(""<reserved>"")\n\n        self.eos_word = eos\n        self.eos_index = self.add_symbol(eos)\n        assert self.eos_index == vocab_constants.EOS_ID\n\n        self.unk_word = unk\n        self.unk_index = self.add_symbol(unk)\n        assert self.unk_index == vocab_constants.UNK_ID\n\n        self.mask_word = mask\n        self.mask_index = self.add_symbol(mask)\n        assert self.mask_index == vocab_constants.MASK_ID\n\n        # Adds junk symbols to pad up to the number of special tokens.\n        num_reserved = max_special_tokens - len(self.symbols)\n        for i in range(num_reserved):\n            self.add_symbol(f""<reserved_{i}>"")\n\n        self.nspecial = len(self.symbols)\n        assert self.nspecial == max_special_tokens\n\n    def mask(self):\n        """"""Helper to get index of mask symbol""""""\n        return self.mask_index\n'"
pytorch_translate/data/utils.py,0,"b'#!/usr/bin/env python3\n\nfrom typing import Optional\n\nfrom fvcore.common.file_io import PathManager\nfrom pytorch_translate.data import (\n    char_data,\n    data as pytorch_translate_data,\n    weighted_data,\n)\n\n\ndef load_parallel_dataset(\n    source_lang,\n    target_lang,\n    src_bin_path,\n    tgt_bin_path,\n    source_dictionary,\n    target_dictionary,\n    split,\n    remove_eos_from_source,\n    append_eos_to_target=True,\n    char_source_dict=None,\n    log_verbose=True,\n):\n    corpus = pytorch_translate_data.ParallelCorpusConfig(\n        source=pytorch_translate_data.CorpusConfig(\n            dialect=source_lang, data_file=src_bin_path\n        ),\n        target=pytorch_translate_data.CorpusConfig(\n            dialect=target_lang, data_file=tgt_bin_path\n        ),\n        weights_file=None,\n    )\n\n    if log_verbose:\n        print(""Starting to load binarized data files."", flush=True)\n    validate_corpus_exists(corpus=corpus, split=split)\n\n    tgt_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n        corpus.target.data_file\n    )\n    if char_source_dict is not None:\n        src_dataset = char_data.InMemoryNumpyWordCharDataset.create_from_file(\n            corpus.source.data_file\n        )\n    else:\n        src_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n            corpus.source.data_file\n        )\n    parallel_dataset = weighted_data.WeightedLanguagePairDataset(\n        src=src_dataset,\n        src_sizes=src_dataset.sizes,\n        src_dict=source_dictionary,\n        tgt=tgt_dataset,\n        tgt_sizes=tgt_dataset.sizes,\n        tgt_dict=target_dictionary,\n        remove_eos_from_source=remove_eos_from_source,\n        append_eos_to_target=append_eos_to_target,\n    )\n    return parallel_dataset, src_dataset, tgt_dataset\n\n\ndef load_monolingual_dataset(\n    bin_path,\n    is_source=False,\n    char_source_dict=None,\n    log_verbose=True,\n    num_examples_limit: Optional[int] = None,\n):\n    if log_verbose:\n        print(""Starting to load binarized monolingual data file."", flush=True)\n\n    if not PathManager.exists(bin_path):\n        raise ValueError(f""Monolingual binary path {bin_path} not found!"")\n\n    if char_source_dict is not None and is_source:\n        dataset = char_data.InMemoryNumpyWordCharDataset.create_from_file(path=bin_path)\n\n    else:\n        dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n            path=bin_path, num_examples_limit=num_examples_limit\n        )\n\n    if log_verbose:\n        print(f""Finished loading dataset {bin_path}"", flush=True)\n\n    print(\n        f""""""| Loaded {len(dataset)} monolingual examples for """"""\n        f""""""{""source"" if is_source else ""target""}""""""\n    )\n    return dataset\n\n\ndef validate_fairseq_dataset_exists(prefix):\n    if not PathManager.exists(f""{prefix}.idx""):\n        raise ValueError(f""{prefix}.idx not found!"")\n    if not PathManager.exists(f""{prefix}.bin""):\n        raise ValueError(f""{prefix}.bin not found!"")\n\n\ndef validate_corpus_exists(\n    corpus: pytorch_translate_data.ParallelCorpusConfig, split: str, is_npz: bool = True\n):\n    """"""\n    Makes sure that the files in the `corpus` are valid files. `split` is used\n    for logging.\n    """"""\n    if is_npz:\n        if not PathManager.exists(corpus.source.data_file):\n            raise ValueError(f""{corpus.source.data_file} for {split} not found!"")\n        if not PathManager.exists(corpus.target.data_file):\n            raise ValueError(f""{corpus.target.data_file} for {split} not found!"")\n    else:\n        validate_fairseq_dataset_exists(corpus.source.data_file)\n        validate_fairseq_dataset_exists(corpus.target.data_file)\n'"
pytorch_translate/data/weighted_data.py,4,"b'#!/usr/bin/env python3\n\nimport torch\nfrom fairseq import data\n\n\nclass IndexedWeightsDataset(data.indexed_dataset.IndexedDataset):\n    def __init__(self, path):\n        self.values = []\n        self.read_data(path)\n\n    def read_data(self, path):\n        with open(path, ""r"") as f:\n            for line in f:\n                self.values.append(float(line.strip(""\\n"")))\n            self._len = len(self.values)\n\n    def __getitem__(self, i):\n        self.check_index(i)\n        return self.values[i]\n\n    def __del__(self):\n        pass\n\n    def __len__(self):\n        return self._len\n\n\nclass WeightedLanguagePairDataset(data.language_pair_dataset.LanguagePairDataset):\n    """"""\n    Extension of fairseq.data.LanguagePairDataset where each example\n    has a weight in [0.0, 1.0], which will be used to weigh the loss.\n\n    TODO: Refactor this class to look like WeightedBacktranslationDataset.\n    We could wrap an existing dataset object and provide additional weights\n    feature. This way, it will be more composable and can be used with arbitrary\n    datasets. See D13143051.\n\n    Args:\n        weights (list): list of per example weight values; each example\n        has a weight in [0.0, 1.0]. Alternatively, when weights consists of a\n        single value, that value is broadcast as weight to all examples. [0.0]\n        gives 0 weight to all examples.\n    """"""\n\n    def __init__(\n        self,\n        src,\n        src_sizes,\n        src_dict,\n        tgt=None,\n        tgt_sizes=None,\n        tgt_dict=None,\n        weights=None,\n        **kwargs,\n    ):\n        super().__init__(src, src_sizes, src_dict, tgt, tgt_sizes, tgt_dict, **kwargs)\n        self.weights = weights\n        self.src_dict = src_dict\n\n    def __getitem__(self, i):\n        example = super().__getitem__(i)\n        if self.weights:\n            """"""\n            If weight for example is missing, use last seen weight. Sometimes we just\n            want to assign a weight to the entire dataset with a single value but also\n            maintain the list convention of weights. This way, even if we don\'t care/know\n            about dataset size, we can assign same weight to all examples.\n            """"""\n            if len(self.weights) <= i:\n                example[""weight""] = self.weights[-1]\n            else:\n                example[""weight""] = self.weights[i]\n        else:\n            example[""weight""] = 1.0\n\n        return example\n\n    def __len__(self):\n        return super().__len__()\n\n    def collater(self, samples):\n        return WeightedLanguagePairDataset.collate(\n            samples, self.src_dict.pad(), self.src_dict.eos()\n        )\n\n    @staticmethod\n    def collate(samples, pad_idx, eos_idx, left_pad_source=False):\n        if len(samples) == 0:\n            return {}\n        unweighted_data = data.language_pair_dataset.collate(\n            samples, pad_idx, eos_idx, left_pad_source\n        )\n        original_weights = torch.FloatTensor([s.get(""weight"", 1.0) for s in samples])\n        # sort by descending source length\n        src_lengths = torch.LongTensor([s[""source""].numel() for s in samples])\n        src_lengths, sort_order = src_lengths.sort(descending=True)\n        weights = original_weights.index_select(0, sort_order)\n        unweighted_data[""weights""] = weights\n        return unweighted_data\n\n\nclass WeightedBacktranslationDataset(\n    data.backtranslation_dataset.BacktranslationDataset\n):\n    """"""\n    Extension of fairseq.data.BacktranslationDataset where each example\n    has a weight in [0.0, 1.0], which will be used to weigh the loss.\n\n    Args:\n        weights (list): list of per example weight values; each example\n        has a weight in [0.0, 1.0]. Alternatively, when weights consists of a\n        single value, that value is broadcast as weight to all examples. [0.0]\n        gives 0 weight to all examples.\n    """"""\n\n    def __init__(self, dataset, weights=None, **kwargs):\n        self.weights = weights\n        self.dataset = dataset\n\n    def __getattr__(self, attr):\n        if attr in self.__dict__:\n            return getattr(self, attr)\n        return getattr(self.dataset, attr)\n\n    def __getitem__(self, i):\n        example = self.dataset.__getitem__(i)\n        if self.weights:\n            """"""\n            If weight for example is missing, use last seen weight. Sometimes we just\n            want to assign a weight to the entire dataset with a single value but also\n            maintain the list convention of weights. This way, even if we don\'t care or\n            don\'t know about dataset size, we can assign same weight to all examples.\n            """"""\n            if len(self.weights) <= i:\n                example[""weight""] = self.weights[-1]\n            else:\n                example[""weight""] = self.weights[i]\n        else:\n            example[""weight""] = 1.0\n\n        return example\n\n    def collater(self, samples):\n        if len(samples) == 0:\n            return {}\n        unweighted_data = self.dataset.collater(samples)\n        original_weights = torch.FloatTensor([s.get(""weight"", 1.0) for s in samples])\n        # sort by descending source length\n        src_lengths = torch.LongTensor([s[""source""].numel() for s in samples])\n        src_lengths, sort_order = src_lengths.sort(descending=True)\n        weights = original_weights.index_select(0, sort_order)\n        unweighted_data[""weights""] = weights\n        return unweighted_data\n'"
pytorch_translate/dual_learning/__init__.py,0,b''
pytorch_translate/dual_learning/dual_learning_criterion.py,3,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nfrom fairseq import bleu, utils\nfrom fairseq.criterions import LegacyFairseqCriterion, register_criterion\nfrom pytorch_translate import beam_decode\nfrom pytorch_translate.data.weighted_data import WeightedLanguagePairDataset\n\n\n@register_criterion(""unsupervised_criterion"")\nclass UnsupervisedCriterion(LegacyFairseqCriterion):\n    """"""This criterion computes losses from input (monolingual data in\n    translation) with two components:\n    1. Reconstruction loss:\n    2. LM loss:\n    The total loss is a weighted sum of these two components.\n    """"""\n\n    def __init__(self, args, task):\n        super().__init__(args, task)\n        self.args = args\n        self.alpha = args.reward_alpha\n        self.remove_eos_at_src = not args.append_eos_to_source\n        self.task = task\n\n    def _generate_translation(self, model, tgt_dict, sample, beam_size, **kwargs):\n        translator_class = beam_decode.SequenceGenerator\n        translator = translator_class(models=[model], tgt_dict=tgt_dict, **kwargs)\n        translator.cuda()\n        s = utils.move_to_cuda(sample)\n\n        # TODO: nbest\n        input = s[""net_input""]\n        srclen = input[""src_tokens""].size(1)\n        if self.task.use_char_source:\n            encoder_input = {\n                k: v\n                for k, v in input.items()\n                if k in [""src_tokens"", ""src_lengths"", ""char_inds"", ""word_lengths""]\n            }\n        else:\n            encoder_input = {\n                k: v for k, v in input.items() if k in [""src_tokens"", ""src_lengths""]\n            }\n        with torch.no_grad():\n            hypos = translator.generate(\n                encoder_input=encoder_input,\n                beam_size=beam_size,\n                maxlen=int(self.args.max_len_a * srclen + self.args.max_len_b),\n            )\n            for i, id in enumerate(s[""id""]):\n                # remove padding\n                src = utils.strip_pad(input[""src_tokens""][i, :], tgt_dict.pad())\n                yield id, src, hypos[i]\n\n    def _maybe_reverse_source(self, src_tokens):\n        return torch.flip(src_tokens, (0,)) if self.args.reverse_source else src_tokens\n\n    def _maybe_add_eos(self, tgt_tokens, eos_index):\n        if tgt_tokens[-1] != eos_index:\n            return torch.cat([tgt_tokens.cpu(), torch.LongTensor([eos_index])])\n        return tgt_tokens\n\n    def forward(\n        self,\n        sample,\n        forward_model,\n        forward_optimizer,\n        tgt_dict,\n        backward_model,\n        backward_optimizer,\n        src_dict,\n        lm_scorer=None,\n        reduce=True,\n        **generate_kwargs,\n    ):\n        """"""Compute the reconstruction and LM loss from forward and backward\n        models.\n\n        Args:\n            sample: original input.\n            hypos: psudo labels generated by the forward model. They are used\n                as approximation of the target space to do importance sampling.\n            forward_model: the model used to generate psuedo labels.\n            backward_model: the model to reconstruct original input using\n                psuedo labels.\n            lm_scorer: an LM model eval mode to score psuedo labels in target\n                space.\n        """"""\n        # Generate translations\n        nbest_translations = self._generate_translation(\n            forward_model, tgt_dict, sample, self.args.beam, **generate_kwargs\n        )\n\n        forward_samples = []\n        backward_samples = {}\n        # TODO (T36875783): load pretrained lm to score\n        lm_score = 0.0\n        for sample_id, src_processed, tgt_hypos in nbest_translations:\n            # compute each model\'s reward\n            forward_reward = lm_score\n            # construct the sample; compute the ce loss\n            # backward_samples need to handle EOS\n            src = self._maybe_reverse_source(src_processed)\n            src = self._maybe_add_eos(src, src_dict.eos())\n            assert len(tgt_hypos) == self.args.beam\n            for tgt_hypo_i, tgt_hypo_struct in enumerate(tgt_hypos):\n                dual_sample_id = sample_id.item() * self.args.beam + tgt_hypo_i\n                tgt_hypo = tgt_hypo_struct[""tokens""]\n                # add EOS to the target, i.e. original source, since it\'ll be used\n                # as target\n                # remove EOS in the src is optional\n                if self.remove_eos_at_src:\n                    tgt_hypo = tgt_hypo[:-1]\n                tgt_hypo_processed = self._maybe_reverse_source(tgt_hypo)\n\n                backward_sample = {\n                    ""id"": dual_sample_id,\n                    ""source"": tgt_hypo_processed.cpu(),\n                    ""target"": src.cpu(),\n                    ""weight"": 1.0 - self.alpha,\n                }\n                assert dual_sample_id not in backward_samples\n                backward_samples[dual_sample_id] = backward_sample\n\n        bwd_model_input = utils.move_to_cuda(\n            WeightedLanguagePairDataset.collate(\n                samples=list(backward_samples.values()),\n                pad_idx=src_dict.pad(),\n                eos_idx=src_dict.eos(),\n            )\n        )\n        reconstructed_source = self._generate_translation(\n            backward_model, src_dict, bwd_model_input, 1, **generate_kwargs\n        )\n        for dual_sample_id, tgt_hypo_processed, src_hypos in reconstructed_source:\n            backward_sample = backward_samples[dual_sample_id.item()]\n            src = backward_sample[""target""]\n            tgt_hypo = self._maybe_reverse_source(tgt_hypo_processed)\n\n            # use bleu score as reward\n            scorer = bleu.Scorer(src_dict.pad(), src_dict.eos(), src_dict.unk())\n            assert len(src_hypos) == 1\n            src_hypo = src_hypos[0][""tokens""][:-1]\n            scorer.add(src.int().cpu(), src_hypo.int().cpu())\n            backward_reward = (\n                scorer.score(order=self.args.reconstruction_bleu_order) / 100.0\n            )\n\n            original_stc = "" "".join(src_dict[tid] for tid in src.tolist())\n            translated_stc = "" "".join(tgt_dict[tid] for tid in tgt_hypo)\n            recon_stc = "" "".join(src_dict[tid] for tid in src_hypo.tolist())\n\n            if int(dual_sample_id / self.args.beam) % 100 == 0:\n                print(""--------"")\n                print(\n                    ""original sentence:"",\n                    original_stc.replace(self.args.source_bpe_end_marker, """"),\n                )\n                print(\n                    ""translated sentence:"",\n                    translated_stc.replace(self.args.source_bpe_end_marker, """"),\n                )\n                print(\n                    ""reconstructed sentence:"",\n                    recon_stc.replace(self.args.source_bpe_end_marker, """"),\n                )\n                print(""reward:"", backward_reward)\n                print(""--------"")\n\n            total_reward = (\n                self.alpha * forward_reward + (1.0 - self.alpha) * backward_reward\n            )\n            src_processed = self._maybe_reverse_source(src)\n            tgt_hypo = self._maybe_add_eos(tgt_hypo, tgt_dict.eos())\n            forward_samples.append(\n                {\n                    ""id"": dual_sample_id,\n                    ""source"": src_processed.cpu(),\n                    ""target"": tgt_hypo.cpu(),  # first hypo is best hypo\n                    ""weight"": total_reward,\n                }\n            )\n\n        # Now combine pseudo labelled examples to corresponding batch with\n        # rewards factored to weighting of each task\'s loss\n        agg_loss, agg_sample_size, agg_logging_output = 0.0, 0.0, {}\n        forward_model.train()\n        forward_loss, sample_size, logging_output = self.task.criterion(\n            forward_model,\n            utils.move_to_cuda(\n                WeightedLanguagePairDataset.collate(\n                    samples=forward_samples,\n                    pad_idx=tgt_dict.pad(),\n                    eos_idx=tgt_dict.eos(),\n                )\n            ),\n        )\n        agg_loss += forward_loss.detach().item()\n        agg_sample_size += sample_size\n        agg_logging_output[""primal""] = logging_output\n        # grad would be further scaled when passed back to trainer,\n        # which will do the update\n        forward_optimizer.backward(forward_loss)\n\n        backward_model.train()\n        backward_loss, sample_size, logging_output = self.task.criterion(\n            backward_model, bwd_model_input\n        )\n\n        agg_loss += backward_loss.data.item()\n        agg_sample_size += sample_size\n        agg_logging_output[""dual""] = logging_output\n        backward_optimizer.backward(backward_loss)\n        return agg_loss, agg_sample_size, agg_logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n\n        def get_logging_output(key):\n            if key in logging_outputs[0].keys():\n                return logging_outputs[0][key]\n            else:\n                return sum(\n                    log[key] if key in log else 0\n                    for _, log in logging_outputs[0].items()\n                )\n\n        loss_sum = get_logging_output(""loss"")\n        ntokens = get_logging_output(""ntokens"")\n        nsentences = get_logging_output(""nsentences"")\n        sample_size = get_logging_output(""sample_size"")\n        agg_output = {\n            ""loss"": loss_sum / sample_size / math.log(2),\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""sample_size"": sample_size,\n        }\n        if sample_size != ntokens:\n            agg_output[""nll_loss""] = loss_sum / ntokens / math.log(2)\n        return agg_output\n'"
pytorch_translate/dual_learning/dual_learning_models.py,1,"b'#!/usr/bin/env python3\n\nimport logging\n\nimport torch.nn as nn\nfrom fairseq import checkpoint_utils\nfrom fairseq.models import BaseFairseqModel, register_model\nfrom pytorch_translate import rnn\nfrom pytorch_translate.rnn import (\n    LSTMSequenceEncoder,\n    RNNDecoder,\n    RNNEncoder,\n    RNNModel,\n    base_architecture,\n)\nfrom pytorch_translate.tasks.pytorch_translate_task import PytorchTranslateTask\n\n\nlogger = logging.getLogger(__name__)\n\n\n@register_model(""dual_learning"")\nclass DualLearningModel(BaseFairseqModel):\n    """"""\n    An architecture to jointly train primal model and dual model by leveraging\n    distribution duality, which exist for both parallel data and monolingual\n    data.\n    """"""\n\n    def __init__(self, args, task, primal_model, dual_model, lm_model=None):\n        super().__init__()\n        self.args = args\n        self.task_keys = [""primal"", ""dual""]\n        self.models = nn.ModuleDict(\n            {""primal"": primal_model, ""dual"": dual_model, ""lm"": lm_model}\n        )\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens=None):\n        """"""\n        If batch is monolingual, need to run beam decoding to generate\n        fake prev_output_tokens.\n        """"""\n\n        # TODO: pass to dual model too\n        primal_encoder_out = self.models[""primal""].encoder(src_tokens, src_lengths)\n        primal_decoder_out = self.models[""primal""].decoder(\n            prev_output_tokens, primal_encoder_out\n        )\n        return primal_decoder_out\n\n    def max_positions(self):\n        return {\n            ""primal_source"": (\n                self.models[""primal""].encoder.max_positions(),\n                self.models[""primal""].decoder.max_positions(),\n            ),\n            ""dual_source"": (\n                self.models[""dual""].encoder.max_positions(),\n                self.models[""dual""].decoder.max_positions(),\n            ),\n            ""primal_parallel"": (\n                self.models[""primal""].encoder.max_positions(),\n                self.models[""primal""].decoder.max_positions(),\n            ),\n            ""dual_parallel"": (\n                self.models[""dual""].encoder.max_positions(),\n                self.models[""dual""].decoder.max_positions(),\n            ),\n        }\n\n\n@register_model(""dual_learning_rnn"")\nclass RNNDualLearningModel(DualLearningModel):\n    """"""Train two models for a task and its duality jointly.\n    This class uses RNN arch, but can be extended to take arch as an arument.\n    This class takes translation as a task, but the framework is intended\n    to be general enough to be applied to other tasks as well.\n    """"""\n\n    def __init__(self, args, task, primal_model, dual_model, lm_model=None):\n        super().__init__(args, task, primal_model, dual_model, lm_model)\n\n    @staticmethod\n    def add_args(parser):\n        rnn.RNNModel.add_args(parser)\n        parser.add_argument(\n            ""--unsupervised-dual"",\n            default=False,\n            action=""store_true"",\n            help=""Train with dual loss from monolingual data."",\n        )\n        parser.add_argument(\n            ""--supervised-dual"",\n            default=False,\n            action=""store_true"",\n            help=""Train with dual loss from parallel data."",\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """""" Build both the primal and dual models.\n        For simplicity, both models share the same arch, i.e. the same model\n        params would be used to initialize both models.\n        Support for different models/archs would be added in further iterations.\n        """"""\n        base_architecture(args)\n\n        if args.sequence_lstm:\n            encoder_class = LSTMSequenceEncoder\n        else:\n            encoder_class = RNNEncoder\n        decoder_class = RNNDecoder\n\n        encoder_embed_tokens, decoder_embed_tokens = RNNModel.build_embed_tokens(\n            args, task.primal_src_dict, task.primal_tgt_dict\n        )\n        primal_encoder = encoder_class(\n            task.primal_src_dict,\n            embed_dim=args.encoder_embed_dim,\n            embed_tokens=encoder_embed_tokens,\n            cell_type=args.cell_type,\n            num_layers=args.encoder_layers,\n            hidden_dim=args.encoder_hidden_dim,\n            dropout_in=args.encoder_dropout_in,\n            dropout_out=args.encoder_dropout_out,\n            residual_level=args.residual_level,\n            bidirectional=bool(args.encoder_bidirectional),\n        )\n        primal_decoder = decoder_class(\n            src_dict=task.primal_src_dict,\n            dst_dict=task.primal_tgt_dict,\n            embed_tokens=decoder_embed_tokens,\n            vocab_reduction_params=args.vocab_reduction_params,\n            encoder_hidden_dim=args.encoder_hidden_dim,\n            embed_dim=args.decoder_embed_dim,\n            out_embed_dim=args.decoder_out_embed_dim,\n            cell_type=args.cell_type,\n            num_layers=args.decoder_layers,\n            hidden_dim=args.decoder_hidden_dim,\n            attention_type=args.attention_type,\n            dropout_in=args.decoder_dropout_in,\n            dropout_out=args.decoder_dropout_out,\n            residual_level=args.residual_level,\n            averaging_encoder=args.averaging_encoder,\n        )\n        primal_task = PytorchTranslateTask(\n            args, task.primal_src_dict, task.primal_tgt_dict\n        )\n        primal_model = rnn.RNNModel(primal_task, primal_encoder, primal_decoder)\n        if args.pretrained_forward_checkpoint:\n            pretrained_forward_state = checkpoint_utils.load_checkpoint_to_cpu(\n                args.pretrained_forward_checkpoint\n            )\n            primal_model.load_state_dict(pretrained_forward_state[""model""], strict=True)\n            print(\n                f""Loaded pretrained primal model from {args.pretrained_forward_checkpoint}""\n            )\n\n        encoder_embed_tokens, decoder_embed_tokens = RNNModel.build_embed_tokens(\n            args, task.dual_src_dict, task.dual_tgt_dict\n        )\n        dual_encoder = encoder_class(\n            task.dual_src_dict,\n            embed_dim=args.encoder_embed_dim,\n            embed_tokens=encoder_embed_tokens,\n            cell_type=args.cell_type,\n            num_layers=args.encoder_layers,\n            hidden_dim=args.encoder_hidden_dim,\n            dropout_in=args.encoder_dropout_in,\n            dropout_out=args.encoder_dropout_out,\n            residual_level=args.residual_level,\n            bidirectional=bool(args.encoder_bidirectional),\n        )\n        dual_decoder = decoder_class(\n            src_dict=task.dual_src_dict,\n            dst_dict=task.dual_tgt_dict,\n            embed_tokens=decoder_embed_tokens,\n            vocab_reduction_params=args.vocab_reduction_params,\n            encoder_hidden_dim=args.encoder_hidden_dim,\n            embed_dim=args.decoder_embed_dim,\n            out_embed_dim=args.decoder_out_embed_dim,\n            cell_type=args.cell_type,\n            num_layers=args.decoder_layers,\n            hidden_dim=args.decoder_hidden_dim,\n            attention_type=args.attention_type,\n            dropout_in=args.decoder_dropout_in,\n            dropout_out=args.decoder_dropout_out,\n            residual_level=args.residual_level,\n            averaging_encoder=args.averaging_encoder,\n        )\n        dual_task = PytorchTranslateTask(args, task.dual_src_dict, task.dual_tgt_dict)\n        dual_model = rnn.RNNModel(dual_task, dual_encoder, dual_decoder)\n        if args.pretrained_backward_checkpoint:\n            pretrained_backward_state = checkpoint_utils.load_checkpoint_to_cpu(\n                args.pretrained_backward_checkpoint\n            )\n            dual_model.load_state_dict(pretrained_backward_state[""model""], strict=True)\n            print(\n                f""Loaded pretrained dual model from {args.pretrained_backward_checkpoint}""\n            )\n\n        # TODO (T36875783): instantiate a langauge model\n        lm_model = None\n        return RNNDualLearningModel(args, task, primal_model, dual_model, lm_model)\n'"
pytorch_translate/dual_learning/dual_learning_task.py,5,"b'#!/usr/bin/env python3\n\nfrom collections import OrderedDict\n\nimport torch\nfrom fairseq import optim, utils\nfrom fairseq.criterions import CRITERION_REGISTRY\nfrom fairseq.data import LanguagePairDataset, RoundRobinZipDatasets\nfrom fairseq.tasks import FairseqTask, register_task\nfrom pytorch_translate.data import (\n    dictionary as pytorch_translate_dictionary,\n    utils as data_utils,\n)\nfrom pytorch_translate.dual_learning import dual_learning_models\nfrom pytorch_translate.dual_learning.dual_learning_criterion import (\n    UnsupervisedCriterion,\n)\nfrom pytorch_translate.tasks.pytorch_translate_task import PytorchTranslateTask\nfrom pytorch_translate.weighted_criterions import (\n    WeightedLabelSmoothedCrossEntropyCriterion,\n)\n\n\n@register_task(""dual_learning_task"")\nclass DualLearningTask(FairseqTask):\n    """"""A task for training primal model and dual models jointly.\n    It takes:\n        - unlabelled source (aka source monolingual data for translation task),\n        - unlabelled target (aka target monolingual data for translation task),\n        - labelled (source, target) (aka parallel data for translation task),\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        PytorchTranslateTask.add_args(parser)\n        """"""Add semi-supervised arguments to the parser.""""""\n        parser.add_argument(\n            ""--dual-criterion"",\n            default=""unsupervised_criterion"",\n            help=""Criterion for jointly train primal and dual models"",\n        )\n        parser.add_argument(\n            ""--reward-alpha"",\n            type=float,\n            default=0.005,\n            help=""Hyperparam to weigh two rewards"",\n        )\n        parser.add_argument(\n            ""--soft-updates"",\n            type=int,\n            metavar=""N"",\n            default=15000,\n            help=""Number of updates before training with mono"",\n        )\n        parser.add_argument(\n            ""--forward-train-source-binary-path"",\n            default="""",\n            metavar=""FILE"",\n            help=""Path for the binary file containing source training ""\n            ""examples for forward model."",\n        )\n        parser.add_argument(\n            ""--forward-train-target-binary-path"",\n            default="""",\n            metavar=""FILE"",\n            help=""Path for the binary file containing target training ""\n            ""examples for forward model."",\n        )\n        parser.add_argument(\n            ""--forward-eval-source-binary-path"",\n            default="""",\n            metavar=""FILE"",\n            help=""Path for the binary file containing source valid ""\n            ""examples for forward model."",\n        )\n        parser.add_argument(\n            ""--forward-eval-target-binary-path"",\n            default="""",\n            metavar=""FILE"",\n            help=""Path for the binary file containing target training ""\n            ""examples for forward model."",\n        )\n        parser.add_argument(\n            ""--backward-train-source-binary-path"",\n            default="""",\n            metavar=""FILE"",\n            help=""Path for the binary file containing source training ""\n            ""examples for backward model."",\n        )\n        parser.add_argument(\n            ""--backward-train-target-binary-path"",\n            default="""",\n            metavar=""FILE"",\n            help=""Path for the binary file containing target training ""\n            ""examples for backward model."",\n        )\n        parser.add_argument(\n            ""--backward-eval-source-binary-path"",\n            default="""",\n            metavar=""FILE"",\n            help=""Path for the binary file containing source valid ""\n            ""examples for backward model."",\n        )\n        parser.add_argument(\n            ""--backward-eval-target-binary-path"",\n            default="""",\n            metavar=""FILE"",\n            help=""Path for the binary file containing target training ""\n            ""examples for backward model."",\n        )\n        parser.add_argument(\n            ""--remove-eos-at-src"", action=""store_true"", help=""If True, remove eos""\n        )\n        parser.add_argument(\n            ""--pretrained-forward-checkpoint"",\n            default="""",\n            help=""Load pretrained forward model"",\n        )\n        parser.add_argument(\n            ""--pretrained-backward-checkpoint"",\n            default="""",\n            help=""Load pretrained backward model"",\n        )\n        parser.add_argument(\n            ""--reconstruction-bleu-order"",\n            default=2,\n            help=""BLEU score order to use as reward for reconstruction"",\n        )\n\n    def __init__(\n        self, args, primal_src_dict, primal_tgt_dict, dual_src_dict, dual_tgt_dict\n    ):\n        if not torch.cuda.is_available():\n            raise NotImplementedError(""Training on CPU is not supported."")\n        super().__init__(args)\n        self.primal_src_dict = primal_src_dict\n        self.primal_tgt_dict = primal_tgt_dict\n        self.dual_src_dict = dual_src_dict\n        self.dual_tgt_dict = dual_tgt_dict\n        self.use_char_source = (args.char_source_vocab_file != """") or (\n            args.char_source_vocab_file\n        )\n        self.task_criterion = UnsupervisedCriterion(args, self).cuda()\n        self.criterion = WeightedLabelSmoothedCrossEntropyCriterion(args, self).cuda()\n        self.num_update = 0\n\n    def _build_optimizer(self, model):\n        if self.args.fp16:\n            if torch.cuda.get_device_capability(0)[0] < 7:\n                print(\n                    ""| WARNING: your device does NOT support faster training ""\n                    ""with --fp16, please switch to FP32 which is likely to be""\n                    "" faster""\n                )\n            params = list(filter(lambda p: p.requires_grad, model.parameters()))\n            self._optimizer = optim.FP16Optimizer.build_optimizer(self.args, params)\n        else:\n            if torch.cuda.get_device_capability(0)[0] >= 7:\n                print(""| NOTICE: your device may support faster training with --fp16"")\n            self._optimizer = optim.build_optimizer(self.args, model.parameters())\n        return self._optimizer\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        cls.source_lang = args.source_lang or ""src""\n        cls.target_lang = args.target_lang or ""tgt""\n\n        primal_src_dict = pytorch_translate_dictionary.Dictionary.load(\n            args.source_vocab_file\n        )\n        primal_tgt_dict = pytorch_translate_dictionary.Dictionary.load(\n            args.target_vocab_file\n        )\n        dual_src_dict = pytorch_translate_dictionary.Dictionary.load(\n            args.target_vocab_file\n        )\n        dual_tgt_dict = pytorch_translate_dictionary.Dictionary.load(\n            args.source_vocab_file\n        )\n        return cls(args, primal_src_dict, primal_tgt_dict, dual_src_dict, dual_tgt_dict)\n\n    def build_criterion(self, args):\n        return CRITERION_REGISTRY[args.dual_criterion](args, self)\n\n    def build_model(self, args):\n        return self._build_model(args)\n\n    def _build_model(self, args):\n        model = dual_learning_models.RNNDualLearningModel.build_model(args, self)\n        if not isinstance(model, dual_learning_models.DualLearningModel):\n            raise ValueError(\n                ""DualLearningTask requires a DualLearningModel architecture.""\n            )\n        return model\n\n    def load_dataset(self, split, seed=None):\n        """"""Load split, which is train (monolingual data, optional parallel data),\n        or eval (always parallel data).\n        """"""\n        if split == self.args.valid_subset:\n            # tune set is always parallel\n            primal_parallel, _, _ = data_utils.load_parallel_dataset(\n                source_lang=self.source_lang,\n                target_lang=self.target_lang,\n                src_bin_path=self.args.forward_eval_source_binary_path,\n                tgt_bin_path=self.args.forward_eval_target_binary_path,\n                source_dictionary=self.primal_src_dict,\n                target_dictionary=self.primal_tgt_dict,\n                split=split,\n                remove_eos_from_source=not self.args.append_eos_to_source,\n                append_eos_to_target=True,\n                char_source_dict=None,\n                log_verbose=self.args.log_verbose,\n            )\n            # now just flip the source and target\n            dual_parallel, _, _ = data_utils.load_parallel_dataset(\n                source_lang=self.target_lang,\n                target_lang=self.source_lang,\n                src_bin_path=self.args.backward_eval_source_binary_path,\n                tgt_bin_path=self.args.backward_eval_target_binary_path,\n                source_dictionary=self.dual_src_dict,\n                target_dictionary=self.dual_tgt_dict,\n                split=split,\n                remove_eos_from_source=not self.args.append_eos_to_source,\n                append_eos_to_target=True,\n                char_source_dict=None,\n                log_verbose=self.args.log_verbose,\n            )\n            self.datasets[split] = RoundRobinZipDatasets(\n                OrderedDict(\n                    [\n                        (""primal_parallel"", primal_parallel),\n                        (""dual_parallel"", dual_parallel),\n                    ]\n                )\n            )\n        elif split == self.args.train_subset:\n            src_dataset = data_utils.load_monolingual_dataset(\n                self.args.train_mono_source_binary_path, is_source=True\n            )\n            tgt_dataset = data_utils.load_monolingual_dataset(\n                self.args.train_mono_target_binary_path, is_source=True\n            )\n            primal_source_mono = LanguagePairDataset(\n                src=src_dataset,\n                src_sizes=src_dataset.sizes,\n                src_dict=self.primal_src_dict,\n                tgt=None,\n                tgt_sizes=None,\n                tgt_dict=None,\n            )\n            dual_source_mono = LanguagePairDataset(\n                src=tgt_dataset,\n                src_sizes=tgt_dataset.sizes,\n                src_dict=self.dual_src_dict,\n                tgt=None,\n                tgt_sizes=None,\n                tgt_dict=None,\n            )\n\n            primal_parallel, _, _ = data_utils.load_parallel_dataset(\n                source_lang=self.source_lang,\n                target_lang=self.target_lang,\n                src_bin_path=self.args.forward_train_source_binary_path,\n                tgt_bin_path=self.args.forward_train_target_binary_path,\n                source_dictionary=self.primal_src_dict,\n                target_dictionary=self.primal_tgt_dict,\n                split=split,\n                remove_eos_from_source=not self.args.append_eos_to_source,\n                append_eos_to_target=True,\n                char_source_dict=None,\n                log_verbose=self.args.log_verbose,\n            )\n            dual_parallel, _, _ = data_utils.load_parallel_dataset(\n                source_lang=self.target_lang,\n                target_lang=self.source_lang,\n                src_bin_path=self.args.backward_train_source_binary_path,\n                tgt_bin_path=self.args.backward_train_target_binary_path,\n                source_dictionary=self.dual_src_dict,\n                target_dictionary=self.dual_tgt_dict,\n                split=split,\n                remove_eos_from_source=not self.args.append_eos_to_source,\n                append_eos_to_target=True,\n                char_source_dict=None,\n                log_verbose=self.args.log_verbose,\n            )\n            self.datasets[split] = RoundRobinZipDatasets(\n                OrderedDict(\n                    [\n                        (""primal_parallel"", primal_parallel),\n                        (""dual_parallel"", dual_parallel),\n                        (""primal_source"", primal_source_mono),\n                        (""dual_source"", dual_source_mono),\n                    ]\n                )\n            )\n        else:\n            raise ValueError(""Invalid data split."")\n\n    @property\n    def source_dictionary(self):\n        return self.primal_src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.primal_tgt_dict\n\n    def _prepare_sample(self, sample):\n        if sample is None or len(sample) == 0:\n            return None\n        return utils.move_to_cuda(sample)\n\n    def _get_src_dict(self, model_key):\n        if model_key == ""primal"":\n            return self.primal_src_dict\n        else:\n            return self.dual_src_dict\n\n    def _get_tgt_dict(self, model_key):\n        if model_key == ""primal"":\n            return self.primal_tgt_dict\n        else:\n            return self.dual_tgt_dict\n\n    def _get_dual(self, model_key):\n        if model_key == ""primal"":\n            return ""dual""\n        else:\n            return ""primal""\n\n    def train_step(self, sample, model, criterion, optimizer, ignore_grad=False):\n        # Apply unsupervised dual learning objectives to both types of\n        # monolingual data\n        self.num_update += 1\n\n        model.train()\n        agg_loss, agg_sample_size, agg_logging_output = 0.0, 0.0, {}\n        for model_key in model.task_keys:\n            data_keys = [""source"", ""parallel""]\n            for data_key in data_keys:\n                sample_key = f""{model_key}_{data_key}""\n                if sample[sample_key] is None:\n                    continue\n                if data_key == ""parallel"":\n                    loss, sample_size, logging_output = self.criterion(\n                        model.models[model_key], sample[sample_key]\n                    )\n                    if ignore_grad:\n                        loss *= 0\n                    optimizer.backward(loss)\n                    agg_loss += loss.detach().item()\n                    agg_sample_size += sample_size\n                    agg_logging_output[sample_key] = logging_output\n                if data_key == ""source"" and self.num_update > self.args.soft_updates:\n                    total_loss, sample_size, logging_output = self.task_criterion(\n                        sample[sample_key],\n                        model.models[model_key],\n                        optimizer,\n                        self._get_tgt_dict(model_key),\n                        model.models[self._get_dual(model_key)],\n                        optimizer,\n                        self._get_src_dict(model_key),\n                        len_penalty=self.args.length_penalty,\n                        unk_reward=self.args.unk_reward,\n                        word_reward=self.args.word_reward,\n                    )\n                    agg_loss += total_loss\n                    agg_sample_size += sample_size\n                    agg_logging_output[sample_key] = logging_output[model_key]\n\n        return agg_loss, agg_sample_size, agg_logging_output\n\n    def aggregate_logging_outputs(self, logging_outputs, criterion):\n        parallel_keys = [\n            ""primal_parallel"",\n            ""dual_parallel"",\n            ""primal_source"",\n            ""dual_source"",\n        ]\n        agg_logging_outputs = {}\n        for lang_pair in parallel_keys:\n            agg_output = []\n            for logging_output in logging_outputs:\n                if lang_pair in logging_output.keys():\n                    agg_output.append(logging_output[lang_pair])\n            if len(agg_output) > 0:\n                agg_logging_outputs[\n                    lang_pair\n                ] = criterion.__class__.aggregate_logging_outputs(agg_output)\n\n        def sum_over_languages(key):\n            return sum(\n                logging_output[key] for logging_output in agg_logging_outputs.values()\n            )\n\n        # flatten logging outputs\n        flat_logging_output = {\n            ""{}:{}"".format(lang_pair, k): v\n            for lang_pair, agg_logging_output in agg_logging_outputs.items()\n            for k, v in agg_logging_output.items()\n        }\n        flat_logging_output[""loss""] = sum_over_languages(""loss"")\n        flat_logging_output[""nll_loss""] = sum_over_languages(""nll_loss"")\n        flat_logging_output[""sample_size""] = sum_over_languages(""sample_size"")\n        flat_logging_output[""nsentences""] = sum_over_languages(""nsentences"")\n        flat_logging_output[""ntokens""] = sum_over_languages(""ntokens"")\n        return flat_logging_output\n\n    def valid_step(self, sample, model, criterion):\n        agg_loss, agg_sample_size, agg_logging_output = 0.0, 0.0, {}\n        if sample[""primal_parallel""] is not None:\n            model.eval()\n            with torch.no_grad():\n                loss, sample_size, logging_output = self.criterion(\n                    model.models[""primal""], sample[""primal_parallel""]\n                )\n                agg_loss += loss.data.item()\n                agg_sample_size += sample_size\n                agg_logging_output[""primal_parallel""] = logging_output\n        if sample[""dual_parallel""] is not None:\n            model.eval()\n            with torch.no_grad():\n                loss, sample_size, logging_output = self.criterion(\n                    model.models[""dual""], sample[""dual_parallel""]\n                )\n                agg_loss += loss.data.item()\n                agg_sample_size += sample_size\n                agg_logging_output[""dual_parallel""] = logging_output\n        return agg_loss, agg_sample_size, agg_logging_output\n'"
pytorch_translate/models/__init__.py,0,"b'#!/usr/bin/env python3\n\nimport importlib\nimport os\n\n\n# automatically import any Python files in the models/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith("".py"") and not file.startswith(""_""):\n        model_name = file[: file.find("".py"")]\n        importlib.import_module(""pytorch_translate.models."" + model_name)\n'"
pytorch_translate/models/transformer_from_pretrained_xlm.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nfrom fairseq.models import register_model, register_model_architecture\nfrom fairseq.models.transformer import (\n    base_architecture as transformer_base_architecture,\n)\nfrom fairseq.models.transformer_from_pretrained_xlm import (\n    TransformerFromPretrainedXLMModel,\n)\nfrom pytorch_translate.data.masked_lm_dictionary import MaskedLMDictionary\n\n\n@register_model(""pytorch_translate_transformer_from_pretrained_xlm"")\nclass PytorchTranslateTransformerFromPretrainedXLMModel(\n    TransformerFromPretrainedXLMModel\n):\n    @classmethod\n    def build_model(cls, args, task):\n        return super().build_model(args, task, cls_dictionary=MaskedLMDictionary)\n\n\n@register_model_architecture(\n    ""pytorch_translate_transformer_from_pretrained_xlm"",\n    ""pytorch_translate_transformer_from_pretrained_xlm"",\n)\ndef base_architecture(args):\n    transformer_base_architecture(args)\n'"
pytorch_translate/rescoring/model_scorers.py,22,"b'#!/usr/bin/env python3\n\nimport torch\nfrom pytorch_translate import utils\n\n\nclass SimpleModelScorer(object):\n    """""" Rescores source and target tokens based on a model""""""\n\n    def __init__(self, args, model_path=None, model=None, forward_task=None):\n        """""" Initialize a rescorer model\n\n        Args:\n          args: model arguments\n          model_path: checkpoint path for rescoring model\n        """"""\n        # TODO (T40938917): Allow loading of multiple rescoring models\n        # allow to create an empty scorer w/o model\n        self.args = args\n        self.forward_task = forward_task\n        self.task = None\n        self.model = None\n        # Instantiate the model\n        if model is not None:\n            self.model = model[""model""]\n            self.task = model[""task""]\n        elif model_path:\n            rescoring_model, _, task = utils.load_diverse_ensemble_for_inference(\n                [model_path]\n            )\n            self.model = rescoring_model[0]\n            self.task = task\n\n        if self.model is not None:\n            self.model.eval()\n            # Turn off gradient computation in eval mode\n            for param in self.model.parameters():\n                param.requires_grad = False\n            utils.maybe_cuda(self.model)\n\n    def convert_hypos_to_tgt_tokens(self, hypos):\n        """"""\n        hypos is a list of hypotheses containing elements of type dict.\n        each hypothesis dictionary contains target tokens.\n        we convert these target tokens to a tensor, also eos token to the\n        left and right, and padding to the end.\n        """"""\n        max_tgt_len = max(len(hypo[""tokens""]) for hypo in hypos)\n        pad = self.task.target_dictionary.pad()\n        tgt_tokens = torch.full(\n            (len(hypos), max_tgt_len + 1), fill_value=pad, dtype=torch.long\n        )\n        eos = self.task.target_dictionary.eos()\n        tgt_tokens[:, 0] = torch.tensor(eos)\n\n        for i, hypo in enumerate(hypos):\n            start = 1\n            end = start + len(hypo[""tokens""])\n            tgt_tokens[i, start:end] = hypo[""tokens""]\n\n        return tgt_tokens\n\n    def encode(self, encoder_inputs):\n        encoder_out = self.model.encoder(*encoder_inputs)\n        return [encoder_out]\n\n    def decode(self, args, model, encoder_outs, tgt_tokens):\n        """""" Run model decoder on tgt_tokens and encoder_outputs\n\n        Args:\n          args: model arguments\n          model: given rescoring model\n          encoder_outs: encoder output. list(tuple([[input_length, batch_size,\n            hidden_dim], [batch_size, input_length]))\n          tgt_tokens: target tokens to be rescored. target tokens are expected\n            to start with eos to signal start of the sentence, and expected to\n            to end with eos to score eos. therefore target_size should be equal\n            to number_of_target_tokens + 2, and should return\n            number_of_target_tokens + 1 output. [batch_size, target_length]\n\n        Returns:\n          logprobs: log probabilities for each tgt token [batch_size,\n            target_length, vocab_size]\n\n        Raises:\n          ValueError: If there is a problem with input.\n          * If tgt_tokens don\'t start and end with eos.\n        """"""\n        eos = self.task.target_dictionary.eos()\n        unk = self.task.target_dictionary.unk()\n\n        if (tgt_tokens == eos).sum() != 2 * tgt_tokens.size()[0]:\n            raise ValueError(""Each target should have 2 eos tokens"")\n\n        decoder_out = list(model.decoder(tgt_tokens, encoder_outs[0]))\n        possible_translation_tokens = decoder_out[2] if len(decoder_out) == 3 else None\n\n        logprobs = model.get_normalized_probs(decoder_out, log_probs=True)\n\n        if possible_translation_tokens is not None:\n            unk_pos = torch.nonzero(possible_translation_tokens == unk)\n            if unk_pos.size()[0] != 0:\n                # add unk_reward if unk appears in possible_translation_tokens\n                unk_index = unk_pos[0][0]\n                logprobs[:, :, unk_index] += self.args.unk_reward\n\n        return logprobs, possible_translation_tokens\n\n    def compute_scores(self, tgt_tokens, logprobs, possible_translation_tokens=None):\n        """""" logprobs have the log probabilities for each possible token\n        for each hypothesis. here, we extract the logprobs matching the\n        target tokens.\n        """"""\n\n        def clean_tgt_tokens(tgt_tokens, possible_translation_tokens):\n            for i, hypo_tokens in enumerate(tgt_tokens):\n                for j, hypo_token in enumerate(hypo_tokens):\n                    tgt_tokens[i][j] = (\n                        possible_translation_tokens == hypo_token\n                    ).nonzero()\n            return tgt_tokens\n\n        if possible_translation_tokens is not None:\n            tgt_tokens = clean_tgt_tokens(tgt_tokens, possible_translation_tokens)\n\n        tgt_tokens = tgt_tokens[:, 1:]  # get rid of initial eos token\n\n        i, j = torch.meshgrid(\n            torch.arange(0, tgt_tokens.size(0)).long(),\n            torch.arange(0, tgt_tokens.size(1)).long(),\n        )\n        hypos_tokens_probs = torch.zeros(tgt_tokens.shape)\n        hypos_tokens_probs = logprobs[:, :-1, :][i, j, tgt_tokens]\n        pad = self.task.target_dictionary.pad()\n        mask = 1.0 - tgt_tokens.eq(pad).float()\n        hypos_scores = (hypos_tokens_probs * mask).sum(dim=1)\n        return hypos_scores\n\n    def prepare_inputs(self, src_tokens, hypos):\n        beam_size = len(hypos) // len(src_tokens)\n        bsz, src_length = src_tokens.size()\n        src_tokens_expand = (\n            src_tokens.unsqueeze(dim=1)\n            .expand(-1, beam_size, -1)\n            .contiguous()\n            .view(bsz * beam_size, -1)\n        )\n        src_lengths = (\n            torch.tensor(src_length).repeat(bsz * beam_size).type_as(src_tokens)\n        )\n        encoder_inputs = (src_tokens_expand, src_lengths)\n        tgt_tokens = self.convert_hypos_to_tgt_tokens(hypos).type_as(src_tokens)\n        return encoder_inputs, tgt_tokens\n\n    def score_tokens(self, encoder_inputs, hypos):\n        """"""calculate scores for arbitrary hypos given encoder_inputs\n           input:\n               hypos: a tensor of length bsz*beam_size\n           output:\n               hypos_scores: \\sum log prob over the entire hypo\n               encoder_outs: hidden vectors from encoder\n                             (e.g., used to stabilize rl training)\n               logprobs: log prob over all time-steps\n                         (e.g., used for word-level rl training)\n        """"""\n        encoder_outs = self.encode(encoder_inputs)\n        logprobs, possible_translation_tokens = self.decode(\n            self.args, self.model, encoder_outs, hypos\n        )\n        hypos_scores = self.compute_scores(hypos, logprobs, possible_translation_tokens)\n        return hypos_scores, encoder_outs, logprobs\n\n    def score(self, src_tokens, hypos):\n        """""" Rescores hypotheses based on a given model and input tokens.\n            src_tokens: a tensor with size bsz x max_src_len\n            hypos: a list with length of bsz * beam_size\n        """"""\n        if self.model is None:\n            return\n\n        # if cuda is available, we convert src_tokens to cuda. all other\n        # tensors copy src_tokens\'s type (cpu or gpu)\n        if torch.cuda.is_available():\n            src_tokens = src_tokens.cuda()\n\n        encoder_inputs, tgt_tokens = self.prepare_inputs(src_tokens, hypos)\n        hypos_scores, _, _ = self.score_tokens(encoder_inputs, tgt_tokens)\n        return hypos_scores\n\n\nclass R2LModelScorer(SimpleModelScorer):\n    """"""\n    R2L model works by reversing target tokens to right to left direction\n    """"""\n\n    def reverse_tgt_tokens(self, tgt_tokens):\n        """"""\n        tgt_tokens has paddings to the right since they are batched.\n        while reversing, we should roll first to keep paddings.\n\n        Note:\n            input:\n                [1 2 3]\n                [1 2 0]\n                [1 0 0]\n            output:\n                [3 2 1]\n                [2 1 0]\n                [1 0 0]\n        """"""\n        reversed_tgt_tokens = torch.zeros_like(tgt_tokens)\n\n        def roll(x, n):\n            return torch.cat((x[-n:], x[:-n]))\n\n        pad = self.task.tgt_dict.pad()\n        for i, row in enumerate(tgt_tokens):\n            pad_count = torch.sum(row == pad)\n            reversed_tgt_tokens[i] = reversed(roll(row, int(pad_count)))\n\n        return reversed_tgt_tokens\n\n    def prepare_inputs(self, src_tokens, hypos):\n        bsz, src_length = src_tokens.size()\n        beam_size = len(hypos) // bsz\n        src_lengths = torch.tensor([src_length]).repeat(len(hypos)).type_as(src_tokens)\n        src_tokens = (\n            src_tokens.unsqueeze(1)\n            .expand(-1, beam_size, -1)\n            .contiguous()\n            .view(-1, src_length)\n        )\n\n        encoder_inputs = (src_tokens, src_lengths)\n        tgt_tokens = self.convert_hypos_to_tgt_tokens(hypos).type_as(src_tokens)\n        tgt_tokens = self.reverse_tgt_tokens(tgt_tokens)\n\n        return encoder_inputs, tgt_tokens\n\n\nclass ReverseModelScorer(SimpleModelScorer):\n    """"""\n    Scores p(x|y) with a reverse model and switching src and tgt sentences\n    """"""\n\n    def prepare_inputs(self, src_tokens, hypos):\n        """"""\n        For reverse model, we need to switch src_tokens and tgt_tokens.\n        We also make sure source is reversed if original task vs new task\n        has different reverse_source settings.\n        """"""\n        eos = self.task.target_dictionary.eos()\n\n        # Prepare target tokens\n        # Map token ids from original dictionary to reverse model dictionary\n        src_string = self.forward_task.src_dict.string(src_tokens)\n        src_tokens_mapped = self.task.tgt_dict.encode_line(\n            src_string, add_if_not_exist=False\n        )[\n            :-1\n        ]  # remove eos\n        # Swap target and source tokens with necessary modifications\n        tgt_tokens = (\n            torch.cat(\n                (\n                    torch.tensor([eos]).type_as(src_tokens_mapped),\n                    reversed(src_tokens_mapped)\n                    if self.forward_task.args.reverse_source\n                    else src_tokens_mapped,\n                    torch.tensor([eos]).type_as(src_tokens_mapped),\n                ),\n                dim=0,\n            )\n            .view(1, -1)\n            .type_as(src_tokens)\n        )\n        # In reverse model, tgt_tokens are repeated instead of source\n        tgt_tokens = tgt_tokens.repeat(len(hypos), 1)\n\n        # Prepare source tokens\n        max_tgt_len = max(len(hypo[""tokens""]) for hypo in hypos)\n        if not self.args.append_eos_to_source:\n            max_tgt_len -= 1  # no eos\n        pad = self.task.target_dictionary.pad()\n        src_tokens = torch.full(\n            (len(hypos), max_tgt_len), fill_value=pad, dtype=torch.long\n        ).type_as(tgt_tokens)\n\n        src_lengths = torch.zeros(len(hypos)).type_as(src_tokens)\n        for i, hypo in enumerate(hypos):\n            tgt_string = self.forward_task.tgt_dict.string(hypo[""tokens""])\n            tgt_tokens_mapped = self.task.src_dict.encode_line(\n                tgt_string, add_if_not_exist=False\n            )\n            if not self.args.append_eos_to_source:\n                tgt_tokens_mapped = tgt_tokens_mapped[:-1]  # last token is eos\n\n            src_lengths[i] = len(tgt_tokens_mapped)\n            src_tokens[i, : len(tgt_tokens_mapped)] = (\n                reversed(tgt_tokens_mapped)\n                if self.task.args.reverse_source\n                else tgt_tokens_mapped\n            )\n\n        encoder_inputs = (src_tokens, src_lengths)\n        return encoder_inputs, tgt_tokens\n\n\nclass LMScorer(SimpleModelScorer):\n    def convert_hypos_to_tgt_tokens(self, hypos):\n        """"""\n        Converts target tokens from the translation model dictionary\n        to language model dictionary\n        """"""\n        # TODO: (T41818693) Map translation model vs LM model differences\n        # and come up with a solution\n        max_tgt_len = max(len(hypo[""tokens""]) for hypo in hypos)\n        pad = self.task.dictionary.pad_index\n        tgt_tokens = torch.full(\n            (len(hypos), max_tgt_len), fill_value=pad, dtype=torch.long\n        )\n\n        for i, hypo in enumerate(hypos):\n            tgt_string = self.forward_task.tgt_dict.string(hypo[""tokens""])\n            tgt_mapped = self.task.dictionary.encode_line(\n                tgt_string, add_if_not_exist=False\n            )\n            tgt_tokens[i, : len(tgt_mapped)] = tgt_mapped\n\n        return tgt_tokens\n\n    def score(self, src_tokens, hypos):\n        if self.model is None:\n            return\n\n        _, tgt_tokens = self.prepare_inputs(src_tokens, hypos)\n\n        decoder_out = self.model.decoder(tgt_tokens)\n        logprobs = self.model.get_normalized_probs(decoder_out, log_probs=True)\n        hypos_tokens_probs = logprobs.gather(\n            dim=2, index=tgt_tokens.unsqueeze(2)\n        ).squeeze(2)\n\n        pad = self.task.dictionary.pad_index\n        hypos_tokens_probs = (tgt_tokens != pad).float() * hypos_tokens_probs\n\n        return hypos_tokens_probs.sum(dim=1)\n'"
pytorch_translate/rescoring/rescorer.py,5,"b'#!/usr/bin/env python3\n\nimport argparse\nimport pickle\nfrom enum import Enum\n\nimport torch\nfrom fairseq import bleu\nfrom pytorch_translate import hybrid_transformer_rnn  # noqa\nfrom pytorch_translate import utils\nfrom pytorch_translate.rescoring.model_scorers import (\n    LMScorer,\n    R2LModelScorer,\n    ReverseModelScorer,\n    SimpleModelScorer,\n)\nfrom pytorch_translate.research.rescore import cloze_transformer_model  # noqa\nfrom pytorch_translate.tasks import pytorch_translate_task  # noqa\nfrom tqdm import tqdm\n\n\nclass FeatureList(Enum):\n    L2R_MODEL_SCORE = 0\n    R2L_MODEL_SCORE = 1\n    REVERSE_MODEL_SCORE = 2\n    LM_SCORE = 3\n    CLOZE_SCORE = 4\n\n\nclass Rescorer:\n    """"""Reranks n-best hypotheses based on extra models and parameters""""""\n\n    def __init__(self, args, forward_task=None, models=None):\n        """"""models = {\'l2r_model\': {\'model\': model, \'task\': task}, ...}""""""\n        self.args = args\n        if models is None:\n            models = {}\n        self.l2r_model_scorer = None\n        if args.l2r_model_path or models.get(""l2r_model"", None):\n            self.l2r_model_scorer = SimpleModelScorer(\n                args, args.l2r_model_path, models.get(""l2r_model"", None), forward_task\n            )\n\n        self.r2l_model_scorer = None\n        if args.r2l_model_path or models.get(""r2l_model"", None):\n            self.r2l_model_scorer = R2LModelScorer(\n                args, args.r2l_model_path, models.get(""r2l_model"", None), forward_task\n            )\n\n        self.reverse_model_scorer = None\n        if args.reverse_model_path or models.get(""reverse_model"", None):\n            self.reverse_model_scorer = ReverseModelScorer(\n                args,\n                args.reverse_model_path,\n                models.get(""reverse_model"", None),\n                forward_task,\n            )\n\n        self.lm_scorer = None\n        if args.lm_model_path or models.get(""lm_model"", None):\n            self.lm_scorer = LMScorer(\n                args, args.lm_model_path, models.get(""lm_model"", None), forward_task\n            )\n\n        self.cloze_transformer_scorer = None\n        if args.cloze_transformer_path or models.get(""cloze_model"", None):\n            self.cloze_transformer_scorer = SimpleModelScorer(\n                args,\n                args.cloze_transformer_path,\n                models.get(""cloze_model"", None),\n                forward_task,\n            )\n\n    def score(self, src_tokens, hypos):\n        """"""run models and compute scores based on p(y), p(x|y) etc.""""""\n\n        scores = torch.zeros((len(hypos), len(FeatureList)), dtype=torch.float)\n\n        self.compute_l2r_model_scores(src_tokens, hypos, scores)\n        self.compute_r2l_model_scores(src_tokens, hypos, scores)\n        self.compute_reverse_model_scores(src_tokens, hypos, scores)\n        self.compute_lm_scores(src_tokens, hypos, scores)\n        self.compute_cloze_transformer_scores(src_tokens, hypos, scores)\n\n        return scores\n\n    def compute_l2r_model_scores(self, src_tokens, hypos, scores):\n        if not self.l2r_model_scorer:\n            return\n        l2r_scores = self.l2r_model_scorer.score(src_tokens, hypos)\n        scores[:, FeatureList.L2R_MODEL_SCORE.value] = l2r_scores[:]\n\n    def compute_r2l_model_scores(self, src_tokens, hypos, scores):\n        if not self.r2l_model_scorer:\n            return\n        r2l_scores = self.r2l_model_scorer.score(src_tokens, hypos)\n        scores[:, FeatureList.R2L_MODEL_SCORE.value] = r2l_scores[:]\n\n    def compute_reverse_model_scores(self, src_tokens, hypos, scores):\n        """"""computes p(x|y) for each hypothesis. """"""\n        if not self.reverse_model_scorer:\n            return\n\n        scores[\n            :, FeatureList.REVERSE_MODEL_SCORE.value\n        ] = self.reverse_model_scorer.score(src_tokens, hypos)\n\n    def compute_lm_scores(self, src_tokens, hypos, scores):\n        """"""computes p(x|y) for each hypothesis. """"""\n        if not self.lm_scorer:\n            return\n\n        lm_scores = self.lm_scorer.score(src_tokens, hypos)\n        scores[:, FeatureList.LM_SCORE.value] = lm_scores[:]\n\n    def compute_cloze_transformer_scores(self, src_tokens, hypos, scores):\n        if not self.cloze_transformer_scorer:\n            return\n\n        cloze_scores = self.cloze_transformer_scorer.score(src_tokens, hypos)\n        scores[:, FeatureList.CLOZE_SCORE.value] = cloze_scores[:]\n\n\ndef add_args(parser):\n    """"""add rescorer specific arguments usable for training""""""\n    parser.add_argument(\n        ""--l2r-model-path"",\n        default=None,\n        type=str,\n        help=(""Provide a path for the l2r rescoring model""),\n    )\n    parser.add_argument(\n        ""--l2r-model-weight"",\n        default=1.0,\n        type=float,\n        help=(""Provide a weight for the l2r rescoring model""),\n    )\n    parser.add_argument(\n        ""--r2l-model-path"",\n        default=None,\n        type=str,\n        help=(""Provide a path for the r2l rescoring model""),\n    )\n    parser.add_argument(\n        ""--r2l-model-weight"",\n        default=1.0,\n        type=float,\n        help=(""Provide a weight for the r2l rescoring model""),\n    )\n    parser.add_argument(\n        ""--reverse-model-path"",\n        default=None,\n        type=str,\n        help=(""Provide a path for the reverse rescoring model""),\n    )\n    parser.add_argument(\n        ""--reverse-model-weight"",\n        default=1.0,\n        type=float,\n        help=(""Provide a weight for the reverse rescoring model""),\n    )\n    parser.add_argument(\n        ""--lm-model-path"",\n        default=None,\n        type=str,\n        help=(""Provide a path for the language model rescoring model""),\n    )\n    parser.add_argument(\n        ""--lm-model-weight"",\n        default=1.0,\n        type=float,\n        help=(""Provide a weight for the lm rescoring model""),\n    )\n    parser.add_argument(\n        ""--rescore-length-penalty"",\n        default=1.0,\n        type=float,\n        help=(""Provide a weight for length penalty used in rescoring""),\n    )\n    parser.add_argument(\n        ""--cloze-transformer-path"",\n        default=None,\n        type=str,\n        help=(""Provide a path for the cloze transformer rescoring model""),\n    )\n    parser.add_argument(\n        ""--cloze-transformer-weight"",\n        default=1.0,\n        type=float,\n        help=(""Provide a weight for the cloze transformer model""),\n    )\n\n\ndef add_args_rescore(parser):\n    """"""add arguments only used in rescoring mode""""""\n    parser.add_argument(\n        ""--unk-reward"",\n        default=-1.0,\n        type=float,\n        help=(\n            ""Value to add to (log-prob) score for UNK tokens. ""\n            ""Value < 0 (the usual case) encourages fewer UNKs, while > 0 ""\n            ""encourages more UNKs.""\n        ),\n    )\n    parser.add_argument(\n        ""--append-eos-to-source"",\n        default=False,\n        type=bool,\n        help=(""If true, append EOS to source sentences""),\n    )\n    parser.add_argument(\n        ""--batch-size"", default=1, type=int, help=""batch size for rescoring""\n    )\n    parser.add_argument(\n        ""--translation-info-export-path"",\n        default=None,\n        type=str,\n        help=(""Optional path to save translation info output in pickled format""),\n    )\n    parser.add_argument(\n        ""--scores-info-export-path"",\n        default=None,\n        type=str,\n        help=(""Optional path to save score output in pickled format""),\n    )\n\n\ndef combine_weighted_scores(scores, weights, src_len, tgt_len, lenpen):\n    """""" Combines scores from different models and returns\n\n    Args:\n        scores: scores for each feature and hypo [num_of_hypos, num_of_features]\n        weights: weights for each feature [num_of_features]\n        src_len: number of source sentence tokens\n        tgt_len: list of target sentence tokens lengths [num_of_hypos]\n        lenpen: float representing length penalty\n\n    Returns:\n        weighted_scores: one unified score for each hypothesis [num_of_hypos]\n    """"""\n    weighted_scores = scores.clone()\n    weighted_scores[:, FeatureList.L2R_MODEL_SCORE.value] /= tgt_len ** lenpen\n    weighted_scores[:, FeatureList.R2L_MODEL_SCORE.value] /= tgt_len ** lenpen\n    weighted_scores[:, FeatureList.REVERSE_MODEL_SCORE.value] /= src_len ** lenpen\n    weighted_scores[:, FeatureList.LM_SCORE.value] /= tgt_len ** lenpen\n    weighted_scores[:, FeatureList.CLOZE_SCORE.value] /= tgt_len ** lenpen\n\n    weighted_scores *= torch.tensor(weights)\n    # convert [num_of_hypos, num_of_features] to [num_of_hypos] and return\n    return weighted_scores.sum(dim=1)\n\n\ndef find_top_tokens(args, trans_batch_info, rescorer, pad):\n    """""" Rescore translations and combine weights to find top hypo tokens\n    """"""\n    len_src_tokens = [len(trans_info[""src_tokens""]) for trans_info in trans_batch_info]\n    bsz = len(trans_batch_info)\n    src_tokens = torch.zeros(bsz, max(len_src_tokens)).fill_(pad).long().cuda()\n    for i in range(bsz):\n        src_tokens[i, : len_src_tokens[i]] = (\n            trans_batch_info[i][""src_tokens""].view(1, -1).long().cuda()\n        )\n    hypos = [hypo for trans_info in trans_batch_info for hypo in trans_info[""hypos""]]\n\n    scores = rescorer.score(src_tokens, hypos)\n\n    # Prepare all the weights and call combine weighted scores\n    weights = [\n        args.l2r_model_weight,\n        args.r2l_model_weight,\n        args.reverse_model_weight,\n        args.lm_model_weight,\n        args.cloze_transformer_weight,\n    ]\n    bsz, src_len = src_tokens.size()\n    beam_size = len(hypos) // bsz\n    hypos = [hypos[i * beam_size : (i + 1) * beam_size] for i in range(bsz)]\n    scores_to_export = []\n    top_hypos = []\n    for i in range(bsz):\n        score = scores[i * beam_size : (i + 1) * beam_size, :]\n        tgt_len = torch.tensor(\n            [len(hypo[""tokens""]) for hypo in hypos[i]], dtype=torch.float\n        )\n        combined_scores = combine_weighted_scores(\n            score, weights, len_src_tokens[i], tgt_len, args.rescore_length_penalty\n        )\n        top_index = combined_scores.max(0)[1]\n        scores_to_export.append(\n            {\n                ""hypos"": [hypo[""tokens""].cpu().tolist() for hypo in hypos[i]],\n                ""target_tokens"": trans_batch_info[i][""target_tokens""].cpu().numpy(),\n                ""scores"": score.detach().numpy(),\n                ""src_len"": len_src_tokens[i],\n                ""tgt_len"": tgt_len.cpu().numpy(),\n            }\n        )\n        top_hypos.append(hypos[i][top_index][""tokens""])\n    return top_hypos, scores_to_export\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=(""Rescore generated hypotheses with extra models"")\n    )\n    add_args(parser)\n    add_args_rescore(parser)\n    args = parser.parse_args()\n\n    assert (\n        args.translation_info_export_path is not None\n    ), ""--translation_info_export_path is required for rescoring""\n\n    assert args.l2r_model_path is not None, ""Rescoring needs forward model""\n\n    _, _, forward_task = utils.load_diverse_ensemble_for_inference(\n        [args.l2r_model_path]\n    )\n    rescorer = Rescorer(args, forward_task)\n    dst_dict = forward_task.tgt_dict\n    base_bleu_scorer = bleu.Scorer(dst_dict.pad(), dst_dict.eos(), dst_dict.unk())\n    rescoring_bleu_scorer = bleu.Scorer(dst_dict.pad(), dst_dict.eos(), dst_dict.unk())\n\n    with open(args.translation_info_export_path, ""rb"") as file:\n        translation_info_list = pickle.load(file)\n\n    scores_to_export_list = []\n    trans_batch_info = []\n    for k in tqdm(range(0, len(translation_info_list), args.batch_size)):\n        trans_batch_info = translation_info_list[k : k + args.batch_size]\n        for j in range(len(trans_batch_info)):\n            trans_batch_info[j][""hypos""] = [\n                {""score"": hypo[""score""], ""tokens"": hypo[""tokens""].cuda()}\n                for hypo in trans_batch_info[j][""hypos""]\n            ]\n        top_tokens, scores_to_export = find_top_tokens(\n            args, trans_batch_info, rescorer, dst_dict.pad()\n        )\n        if args.scores_info_export_path is not None:\n            scores_to_export_list += scores_to_export\n\n        for i, trans_info in enumerate(trans_batch_info):\n            base_bleu_scorer.add(\n                trans_info[""target_tokens""].int().cpu(),\n                trans_info[""hypos""][0][""tokens""].int().cpu(),\n            )\n            rescoring_bleu_scorer.add(\n                trans_info[""target_tokens""].int().cpu(), top_tokens[i].int().cpu()\n            )\n        trans_batch_info = []\n\n    print(""| Base "", base_bleu_scorer.result_string())\n    print(""| Rescoring "", rescoring_bleu_scorer.result_string())\n\n    if args.scores_info_export_path is not None:\n        with open(args.scores_info_export_path, ""wb"") as file:\n            pickle.dump(scores_to_export_list, file)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_translate/rescoring/weights_search.py,5,"b'#!/usr/bin/env python3\n\nimport argparse\nimport pickle\n\nimport numpy as np\nimport torch\nfrom fairseq import bleu\nfrom pytorch_translate import vocab_constants\nfrom pytorch_translate.data.dictionary import Dictionary\nfrom pytorch_translate.generate import smoothed_sentence_bleu\n\n\ndef get_arg_parser():\n    parser = argparse.ArgumentParser(\n        description=(""Rescore generated hypotheses with extra models"")\n    )\n    parser.add_argument(\n        ""--scores-info-export-path"", type=str, help=""Model scores for weights search""\n    )\n    parser.add_argument(\n        ""--num-trials"",\n        type=int,\n        default=1000,\n        help=""Number of iterations of random search"",\n    )\n    parser.add_argument(""--report-oracle-bleu"", default=False, action=""store_true"")\n    return parser\n\n\nclass DummyTask:\n    """"""\n    Default values for pad, eos, unk\n    """"""\n\n    def __init__(self):\n        self.target_dictionary = Dictionary()\n\n\ndef evaluate_weights(scores_info, feature_weights, length_penalty):\n    scorer = bleu.Scorer(\n        vocab_constants.PAD_ID, vocab_constants.EOS_ID, vocab_constants.UNK_ID\n    )\n\n    for example in scores_info:\n        weighted_scores = (example[""scores""] * feature_weights).sum(axis=1)\n        weighted_scores /= (example[""tgt_len""] ** length_penalty) + 1e-12\n        top_hypo_ind = np.argmax(weighted_scores)\n        top_hypo = example[""hypos""][top_hypo_ind]\n        ref = example[""target_tokens""]\n        scorer.add(torch.IntTensor(ref), torch.IntTensor(top_hypo))\n\n    return scorer.score()\n\n\ndef identify_nonzero_features(scores_info):\n    nonzero_features = np.any(scores_info[0][""scores""] != 0, axis=0)\n    for example in scores_info[1:]:\n        nonzero_features |= np.any(example[""scores""] != 0, axis=0)\n\n    return np.where(nonzero_features)[0]\n\n\ndef random_search(scores_info_export_path, num_trials, report_oracle_bleu=False):\n    with open(scores_info_export_path, ""rb"") as f:\n        scores_info = pickle.load(f)\n\n    dummy_task = DummyTask()\n\n    if report_oracle_bleu:\n        oracle_scorer = bleu.Scorer(\n            vocab_constants.PAD_ID, vocab_constants.EOS_ID, vocab_constants.UNK_ID\n        )\n\n        for example in scores_info:\n            smoothed_bleu = []\n            for hypo in example[""hypos""]:\n                eval_score = smoothed_sentence_bleu(\n                    dummy_task,\n                    torch.IntTensor(example[""target_tokens""]),\n                    torch.IntTensor(hypo),\n                )\n                smoothed_bleu.append(eval_score)\n            best_hypo_ind = np.argmax(smoothed_bleu)\n            example[""best_hypo_ind""] = best_hypo_ind\n\n            oracle_scorer.add(\n                torch.IntTensor(example[""target_tokens""]),\n                torch.IntTensor(example[""hypos""][best_hypo_ind]),\n            )\n\n        print(""oracle BLEU: "", oracle_scorer.score())\n\n    num_features = scores_info[0][""scores""].shape[1]\n    assert all(\n        example[""scores""].shape[1] == num_features for example in scores_info\n    ), ""All examples must have the same number of scores!""\n    feature_weights = np.zeros(num_features)\n    feature_weights[0] = 1\n    score = evaluate_weights(scores_info, feature_weights, length_penalty=1)\n    print(""base BLEU: "", score)\n    best_score = score\n    best_weights = feature_weights\n    best_length_penalty = 0\n\n    nonzero_features = identify_nonzero_features(scores_info)\n\n    for i in range(num_trials):\n        feature_weights = np.zeros(num_features)\n        random_weights = np.random.dirichlet(np.ones(nonzero_features.size))\n        feature_weights[nonzero_features] = random_weights\n        length_penalty = 1.5 * np.random.random()\n\n        score = evaluate_weights(scores_info, feature_weights, length_penalty)\n        if score > best_score:\n            best_score = score\n            best_weights = feature_weights\n            best_length_penalty = length_penalty\n\n        print(f""\\r[{i}]  best: {best_score}"", end="""", flush=True)\n\n    print()\n    print(""best weights: "", best_weights)\n    print(""best length penalty: "", length_penalty)\n\n    return best_weights, best_length_penalty, best_score\n\n\ndef main():\n    args = get_arg_parser().parse_args()\n\n    assert (\n        args.scores_info_export_path is not None\n    ), ""--scores-info-export-path is required for weights search""\n\n    random_search(\n        args.scores_info_export_path, args.num_trials, args.report_oracle_bleu\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_translate/research/__init__.py,0,b''
pytorch_translate/tasks/__init__.py,0,"b'#!/usr/bin/env python3\n\nimport importlib\nimport os\n\n\n# automatically import any Python files in the tasks/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith("".py"") and not file.startswith(""_""):\n        task_name = file[: file.find("".py"")]\n        importlib.import_module(""pytorch_translate.tasks."" + task_name)\n'"
pytorch_translate/tasks/cross_lingual_lm.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nimport os\n\nfrom fairseq import tokenizer\nfrom fairseq.tasks import register_task\nfrom fairseq.tasks.cross_lingual_lm import CrossLingualLMTask\nfrom pytorch_translate.data.masked_lm_dictionary import MaskedLMDictionary\n\n\n@register_task(""pytorch_translate_cross_lingual_lm"")\nclass PytorchTranslateCrossLingualLMTask(CrossLingualLMTask):\n    """"""\n    Task for training cross-lingual language models.\n    For more details look at: https://arxiv.org/pdf/1901.07291.pdf\n    Args:\n        dictionary (MaskedLMDictionary): the dictionary for the input of the task\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        CrossLingualLMTask.add_args(parser)\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""-s"", ""--source-lang"", default=None, metavar=""SRC"", help=""source language""\n        )\n        parser.add_argument(\n            ""-t"",\n            ""--target-lang"",\n            default=None,\n            metavar=""TARGET"",\n            help=""target language"",\n        )\n        parser.add_argument(\n            ""--save-only"", action=""store_true"", help=""skip eval and only do save""\n        )\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        return MaskedLMDictionary.load(filename)\n\n    @classmethod\n    def build_dictionary(\n        cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8\n    ):\n        d = MaskedLMDictionary()\n        for filename in filenames:\n            MaskedLMDictionary.add_file_to_dictionary(\n                filename, d, tokenizer.tokenize_line, workers\n            )\n        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n        return d\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        """"""Setup the task.\n        """"""\n        if getattr(args, ""raw_text"", False):\n            args.dataset_impl = ""raw""\n        elif getattr(args, ""lazy_load"", False):\n            args.dataset_impl = ""lazy""\n\n        dictionary = MaskedLMDictionary.load(\n            os.path.join(args.data, args.source_vocab_file)\n        )\n\n        print(""| dictionary: {} types"".format(len(dictionary)))\n\n        return cls(args, dictionary)\n'"
pytorch_translate/tasks/denoising_autoencoder_task.py,0,"b'#!/usr/bin/env python3\n\nfrom collections import OrderedDict\n\nfrom fairseq import models\nfrom fairseq.data import RoundRobinZipDatasets, noising\nfrom fairseq.tasks import register_task\nfrom pytorch_translate import constants, utils\nfrom pytorch_translate.data import utils as data_utils, weighted_data\nfrom pytorch_translate.semi_supervised import SemiSupervisedModel\nfrom pytorch_translate.tasks.pytorch_translate_task import PytorchTranslateTask\nfrom pytorch_translate.tasks.semi_supervised_task import PytorchTranslateSemiSupervised\n\n\n@register_task(""pytorch_translate_denoising_autoencoder"")\nclass PytorchTranslateDenoisingAutoencoder(PytorchTranslateSemiSupervised):\n    def __init__(self, args, dicts, training):\n        super().__init__(args, dicts, training)\n        self.lang_pairs = [f""{self.source_lang}-{self.target_lang}""]\n        if getattr(self.args, ""denoising_source_parallel"", False):\n            self.lang_pairs.append(f""{self.source_lang}-{self.source_lang}"")\n        if getattr(self.args, ""denoising_target_parallel"", False):\n            self.lang_pairs.append(f""{self.target_lang}-{self.target_lang}"")\n        if getattr(self.args, ""denoising_source_mono"", False):\n            self.lang_pairs.append(\n                f""{self.source_lang}-{self.source_lang}_""\n                f""{constants.MONOLINGUAL_DATA_IDENTIFIER}""\n            )\n        if getattr(self.args, ""denoising_target_mono"", False):\n            self.lang_pairs.append(\n                f""{self.target_lang}-{self.target_lang}_""\n                f""{constants.MONOLINGUAL_DATA_IDENTIFIER}""\n            )\n\n        self.eval_lang_pairs = [f""{self.source_lang}-{self.target_lang}""]\n        self.model_lang_pairs = [f""{self.source_lang}-{self.target_lang}""]\n        # This is explicitly set so that we can re-use code from\n        # MultilingualTranslationTask\n        self.args.lang_pairs = self.lang_pairs\n\n    @staticmethod\n    def add_args(parser):\n        PytorchTranslateSemiSupervised.add_args(parser)\n\n        """"""\n        Add denoising autoencoder arguments to the parser.\n        Monolingual data is only required if you are adding a denoising\n        autoencoder objective to using monolingual data. It is possible to\n        just add a denoising autoencoder objective using one side (source or\n        target) of the parallel dataset.\n        """"""\n\n        # TODO(T35539829): implement a Noising registry so we can build a noiser\n        # and use the corresponding class to pass noise-type specific args\n        parser.add_argument(\n            ""--max-word-shuffle-distance"",\n            default=3,\n            type=int,\n            help=""Maximum distance to swap words."",\n        )\n        parser.add_argument(\n            ""--word-dropout-prob"",\n            default=0.2,\n            type=float,\n            help=""Probability for dropping words."",\n        )\n        parser.add_argument(\n            ""--word-blanking-prob"",\n            default=0.2,\n            type=float,\n            help=""Probability for replacing a word with an UNK token"",\n        )\n\n        parser.add_argument(\n            ""--denoising-source-parallel"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=False,\n            help=""Whether to add a denoising autoencoder objective using ""\n            ""the source side of the parallel data"",\n        )\n        parser.add_argument(\n            ""--denoising-target-parallel"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=False,\n            help=""Whether to add a denoising autoencoder objective using ""\n            ""the target side of the parallel data"",\n        )\n        parser.add_argument(\n            ""--denoising-source-mono"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=False,\n            help=""Whether to add a denoising autoencoder objective using ""\n            ""the monolingual source corpus"",\n        )\n        parser.add_argument(\n            ""--denoising-target-mono"",\n            type=utils.bool_flag,\n            nargs=""?"",\n            const=True,\n            default=False,\n            help=""Whether to add a denoising autoencoder objective using ""\n            ""the monolingual source corpus"",\n        )\n\n    def load_dataset(\n        self, split, src_bin_path, tgt_bin_path, seed=None, use_noiser=False\n    ):\n        """"""\n        Load a dataset split. Seed and noiser are only used for loading train\n        data, not eval data.\n        """"""\n        parallel_dataset, src_dataset, tgt_dataset = data_utils.load_parallel_dataset(\n            source_lang=self.source_lang,\n            target_lang=self.target_lang,\n            src_bin_path=src_bin_path,\n            tgt_bin_path=tgt_bin_path,\n            source_dictionary=self.source_dictionary,\n            target_dictionary=self.target_dictionary,\n            split=split,\n            remove_eos_from_source=not self.args.append_eos_to_source,\n            append_eos_to_target=True,\n            char_source_dict=None,\n            log_verbose=self.args.log_verbose,\n        )\n\n        dataset_map = OrderedDict(\n            [(f""{self.source_lang}-{self.target_lang}"", parallel_dataset)]\n        )\n\n        monolingual_num_examples_limit = None\n        if self.args.monolingual_ratio is not None:\n            monolingual_num_examples_limit = int(\n                self.args.monolingual_ratio * len(parallel_dataset)\n            )\n        if use_noiser:\n            if getattr(self.args, ""denoising_source_parallel"", False):\n                dataset_map[\n                    (f""{self.source_lang}-{self.source_lang}"")\n                ] = weighted_data.WeightedLanguagePairDataset(\n                    src=noising.NoisingDataset(\n                        src_dataset=src_dataset,\n                        src_dict=self.source_dictionary,\n                        seed=seed,\n                        noiser=self.source_noiser,\n                    ),\n                    tgt=src_dataset,\n                    src_sizes=src_dataset.sizes,\n                    src_dict=self.source_dictionary,\n                    remove_eos_from_source=not self.args.append_eos_to_source,\n                    append_eos_to_target=True,\n                )\n            if getattr(self.args, ""denoising_target_parallel"", False):\n                dataset_map[\n                    (f""{self.target_lang}-{self.target_lang}"")\n                ] = weighted_data.WeightedLanguagePairDataset(\n                    src=noising.NoisingDataset(\n                        src_dataset=tgt_dataset,\n                        src_dict=self.target_dictionary,\n                        seed=seed,\n                        noiser=self.target_noiser,\n                    ),\n                    tgt=tgt_dataset,\n                    src_sizes=tgt_dataset.sizes,\n                    src_dict=self.target_dictionary,\n                    remove_eos_from_source=not self.args.append_eos_to_source,\n                    append_eos_to_target=True,\n                )\n\n            if getattr(self.args, ""denoising_source_mono"", False):\n                source_mono_dataset = self.load_monolingual_dataset(\n                    bin_path=self.args.train_mono_source_binary_path,\n                    is_source=True,\n                    num_examples_limit=monolingual_num_examples_limit,\n                )\n                dataset_map[\n                    (\n                        f""{self.source_lang}-{self.source_lang}_""\n                        f""{constants.MONOLINGUAL_DATA_IDENTIFIER}""\n                    )\n                ] = weighted_data.WeightedLanguagePairDataset(\n                    src=noising.NoisingDataset(\n                        src_dataset=source_mono_dataset,\n                        src_dict=self.source_dictionary,\n                        seed=seed,\n                        noiser=self.source_noiser,\n                    ),\n                    tgt=source_mono_dataset,\n                    src_sizes=source_mono_dataset.sizes,\n                    src_dict=self.source_dictionary,\n                    remove_eos_from_source=not self.args.append_eos_to_source,\n                    append_eos_to_target=True,\n                )\n            if getattr(self.args, ""denoising_target_mono"", False):\n                target_mono_dataset = self.load_monolingual_dataset(\n                    bin_path=self.args.train_mono_target_binary_path,\n                    is_source=False,\n                    num_examples_limit=monolingual_num_examples_limit,\n                )\n                dataset_map[\n                    (\n                        f""{self.target_lang}-{self.target_lang}_""\n                        f""{constants.MONOLINGUAL_DATA_IDENTIFIER}""\n                    )\n                ] = weighted_data.WeightedLanguagePairDataset(\n                    src=noising.NoisingDataset(\n                        src_dataset=target_mono_dataset,\n                        src_dict=self.target_dictionary,\n                        seed=seed,\n                        noiser=self.target_noiser,\n                    ),\n                    tgt=target_mono_dataset,\n                    src_sizes=target_mono_dataset.sizes,\n                    src_dict=self.target_dictionary,\n                    remove_eos_from_source=not self.args.append_eos_to_source,\n                    append_eos_to_target=True,\n                )\n\n        # print before loading RoundRobinZipDatasets to help catch any bugs\n        for dataset_key, dataset in dataset_map.items():\n            print(f""| {split}: {dataset_key} {len(dataset)} examples in dataset"")\n\n        self.datasets[split] = RoundRobinZipDatasets(dataset_map)\n        print(\n            f""| {split} {len(self.datasets[split])} examples in RoundRobinZipDatasets""\n        )\n\n        if self.args.log_verbose:\n            print(""Finished loading dataset"", flush=True)\n\n        print(f""| {split} {len(self.datasets[split])} datasets"")\n\n    def build_model(self, args):\n        model = models.build_model(args, self)\n        if not isinstance(model, SemiSupervisedModel):\n            raise ValueError(\n                ""PytorchTranslateDenoisingAutoencoder task requires a ""\n                ""SemiSupervisedModel architecture""\n            )\n        # TODO(T35539829): implement a Noising registry so this can be built\n        # with any noising class as long as it has a @register_noising decorator\n        self.source_noiser = noising.UnsupervisedMTNoising(\n            dictionary=self.source_dictionary,\n            max_word_shuffle_distance=args.max_word_shuffle_distance,\n            word_dropout_prob=args.word_dropout_prob,\n            word_blanking_prob=args.word_blanking_prob,\n            bpe_cont_marker=self.args.source_bpe_cont_marker,\n            bpe_end_marker=self.args.source_bpe_end_marker,\n        )\n        self.target_noiser = noising.UnsupervisedMTNoising(\n            dictionary=self.target_dictionary,\n            max_word_shuffle_distance=args.max_word_shuffle_distance,\n            word_dropout_prob=args.word_dropout_prob,\n            word_blanking_prob=args.word_blanking_prob,\n            bpe_cont_marker=self.args.target_bpe_cont_marker,\n            bpe_end_marker=self.args.target_bpe_end_marker,\n        )\n        return model\n'"
pytorch_translate/tasks/knowledge_distillation_task.py,1,"b'#!/usr/bin/env python3\n\nfrom typing import Dict\n\nimport numpy as np\nimport torch\nfrom fairseq.tasks import register_task\nfrom pytorch_translate import constants, utils as pytorch_translate_utils\nfrom pytorch_translate.data import (\n    data as pytorch_translate_data,\n    utils as data_utils,\n    weighted_data,\n)\nfrom pytorch_translate.research.knowledge_distillation.teacher_score_data import (\n    TeacherDataset,\n)\nfrom pytorch_translate.tasks.pytorch_translate_task import PytorchTranslateTask\n\n\n@register_task(constants.KNOWLEDGE_DISTILLATION_TASK)\nclass PytorchKnowledgeDistillationTask(PytorchTranslateTask):\n    def __init__(\n        self, args, src_dict, tgt_dict, char_source_dict=None, char_target_dict=None\n    ):\n        super().__init__(\n            args,\n            src_dict=src_dict,\n            tgt_dict=tgt_dict,\n            char_source_dict=char_source_dict,\n            char_target_dict=char_target_dict,\n        )\n        self.top_k_probs_binary_file = args.top_k_probs_binary_file\n        self.top_k_teacher_tokens = args.top_k_teacher_tokens\n\n        if self.top_k_probs_binary_file is None:\n            # Load model ensemble from checkpoints\n            (\n                self.teacher_models,\n                _,\n                _,\n            ) = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n                args.teacher_path.split("":"")\n            )\n            if torch.cuda.is_available():\n                for teacher_model in self.teacher_models:\n                    teacher_model = pytorch_translate_utils.maybe_cuda(teacher_model)\n        else:\n            self.teacher_models = None\n\n        # Memoized scores for teacher models. By having this and gradually memoizing\n        # the values, we prevent the teacher model from keeping recalculating the\n        # teacher scores.\n        self.top_k_teacher_scores: Dict[int, np.ndarray] = {}\n        self.top_k_teacher_indices: Dict[int, np.ndarray] = {}\n\n    @staticmethod\n    def add_args(parser):\n        PytorchTranslateTask.add_args(parser)\n\n        """"""Add knowledge-distillation arguments to the parser.""""""\n        parser.add_argument(\n            ""--top-k-probs-binary-file"",\n            metavar=""PROBSFILE"",\n            type=str,\n            default=None,\n            help=""path to .npz file containing KD target probabilities for ""\n            ""each output token in training data."",\n        )\n        parser.add_argument(\n            ""--teacher-path"",\n            metavar=""FILE"",\n            type=str,\n            default=None,\n            help=""path(s) to teacher model file(s) colon separated"",\n        )\n        parser.add_argument(\n            ""--top-k-teacher-tokens"",\n            type=int,\n            default=8,\n            help=(\n                ""Incorporating only the top k words from the teacher model."",\n                ""We zero out all other possibilities and normalize the probabilities"",\n                ""based on the K top element."",\n                ""If top-k-teacher-tokens=0, it backs up to the original way of"",\n                ""enumerating all."",\n            ),\n        )\n\n    def load_dataset(\n        self, split, src_bin_path, tgt_bin_path, weights_file=None, is_train=False\n    ):\n        """"""\n        Currently this method does not support character models.\n        """"""\n        corpus = pytorch_translate_data.ParallelCorpusConfig(\n            source=pytorch_translate_data.CorpusConfig(\n                dialect=self.args.source_lang, data_file=src_bin_path\n            ),\n            target=pytorch_translate_data.CorpusConfig(\n                dialect=self.args.target_lang, data_file=tgt_bin_path\n            ),\n            weights_file=weights_file,\n        )\n\n        if self.args.log_verbose:\n            print(""Starting to load binarized data files."", flush=True)\n        data_utils.validate_corpus_exists(corpus=corpus, split=split)\n\n        dst_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n            corpus.target.data_file\n        )\n        src_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n            corpus.source.data_file\n        )\n        if is_train:\n            self.datasets[split] = TeacherDataset(\n                src=src_dataset,\n                src_sizes=src_dataset.sizes,\n                src_dict=self.src_dict,\n                tgt=dst_dataset,\n                tgt_sizes=dst_dataset.sizes,\n                tgt_dict=self.tgt_dict,\n                top_k_probs_binary_file=self.top_k_probs_binary_file,\n                teacher_models=self.teacher_models,\n                top_k_teacher_tokens=self.top_k_teacher_tokens,\n                top_k_teacher_scores=self.top_k_teacher_scores,\n                top_k_teacher_indices=self.top_k_teacher_indices,\n                left_pad_source=False,\n            )\n        else:\n            self.datasets[split] = weighted_data.WeightedLanguagePairDataset(\n                src=src_dataset,\n                src_sizes=src_dataset.sizes,\n                src_dict=self.src_dict,\n                tgt=dst_dataset,\n                tgt_sizes=dst_dataset.sizes,\n                tgt_dict=self.tgt_dict,\n                weights=None,\n                left_pad_source=False,\n            )\n\n        if self.args.log_verbose:\n            print(""Finished loading dataset"", flush=True)\n\n        print(f""| {split} {len(self.datasets[split])} examples"")\n'"
pytorch_translate/tasks/multilingual_task.py,0,"b'#!/usr/bin/env python3\n\nfrom collections import OrderedDict\n\nfrom fairseq import options\nfrom fairseq.data import RoundRobinZipDatasets\nfrom fairseq.tasks import register_task\nfrom pytorch_translate import constants\nfrom pytorch_translate.data import (\n    data as pytorch_translate_data,\n    utils as data_utils,\n    weighted_data,\n)\nfrom pytorch_translate.tasks import utils as tasks_utils\nfrom pytorch_translate.tasks.pytorch_translate_multi_task import (\n    PyTorchTranslateMultiTask,\n)\n\n\n@register_task(constants.MULTILINGUAL_TRANSLATION_TASK)\nclass PyTorchTranslateMultilingualTranslationTask(PyTorchTranslateMultiTask):\n    """"""\n    PyTorchTranslateMultilingualTranslationTask is eventually subclasses\n    fairseq.tasks.MultilingualTranslationTask. The major differences are-\n    - There is no --data folder containing data binaries and vocabularies,\n      instead we use paths from --vocabulary, --multilingual-*-text-file and\n      --multilingual-*-binary-path\n    - loss_weights is used to weigh losses from different datasets differently.\n      This is achieved by using pytorch_translate\'s WeightedLanguagePairDataset\n    - The dictionaries are instances of pytorch_translate\'s Dictionary class\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        PyTorchTranslateMultiTask.add_args(parser)\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--vocabulary"",\n            type=str,\n            metavar=""EXPR"",\n            action=""append"",\n            help=(\n                ""Per-language vocabulary configuration.""\n                ""Path to vocabulary file must be in the format lang:path""\n            ),\n            default=[],\n        )\n        parser.add_argument(\n            ""--multilingual-train-text-file"",\n            type=str,\n            metavar=""EXPR"",\n            action=""append"",\n            help=(\n                ""Path to train text file in the format ""\n                ""src_lang-tgt_lang:source-path,target-path""\n            ),\n        )\n        parser.add_argument(\n            ""--multilingual-eval-text-file"",\n            type=str,\n            metavar=""EXPR"",\n            action=""append"",\n            help=(\n                ""Path to eval text file in the format ""\n                ""src_lang-tgt_lang:source-path,target-path""\n            ),\n        )\n        parser.add_argument(\n            ""--multilingual-train-binary-path"",\n            type=str,\n            metavar=""EXPR"",\n            action=""append"",\n            help=(\n                ""Path to train binary file in the format ""\n                ""src_lang-tgt_lang:source-path,target-path""\n            ),\n        )\n        parser.add_argument(\n            ""--multilingual-eval-binary-path"",\n            type=str,\n            metavar=""EXPR"",\n            action=""append"",\n            help=(\n                ""Path to eval binary file in the format ""\n                ""src_lang-tgt_lang:source-path,target-path""\n            ),\n        )\n\n    def __init__(self, args, dicts, training):\n        super().__init__(args, dicts, training)\n        self.loss_weights = []\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        args.left_pad_source = options.eval_bool(args.left_pad_source)\n        args.left_pad_target = options.eval_bool(args.left_pad_target)\n\n        if args.source_lang is not None or args.target_lang is not None:\n            if args.lang_pairs is not None:\n                raise ValueError(\n                    ""--source-lang/--target-lang implies generation, which is ""\n                    ""incompatible with --lang-pairs""\n                )\n            training = False\n            args.lang_pairs = [""{}-{}"".format(args.source_lang, args.target_lang)]\n        else:\n            training = True\n            args.lang_pairs = args.lang_pairs.split("","")\n            args.source_lang, args.target_lang = args.lang_pairs[0].split(""-"")\n\n        dicts = tasks_utils.load_multilingual_vocabulary(args)\n\n        return cls(args, dicts, training)\n\n    def load_dataset(self, split, **kwargs):\n        """"""Load a dataset split.""""""\n\n        lang_pair_to_datasets = {}\n\n        binary_path_arg = (\n            ""--multilingual-train-binary-path""\n            if split == ""train""\n            else ""--multilingual-eval-binary-path""\n        )\n        binary_path_value = (\n            self.args.multilingual_train_binary_path\n            if split == ""train""\n            else self.args.multilingual_eval_binary_path\n        )\n\n        format_warning = (\n            f""{binary_path_arg} has to be in the format ""\n            "" src_lang-tgt_lang:src_dataset_path,tgt_dataset_path""\n        )\n\n        for path_config in binary_path_value:\n            # path_config: str\n            # in the format ""src_lang-tgt_lang:src_dataset_path,tgt_dataset_path""\n            assert "":"" in path_config, format_warning\n            lang_pair, dataset_paths = path_config.split("":"")\n\n            assert ""-"" in lang_pair, format_warning\n\n            assert "","" in dataset_paths, format_warning\n            src_dataset_path, tgt_dataset_path = dataset_paths.split("","")\n\n            lang_pair_to_datasets[lang_pair] = (src_dataset_path, tgt_dataset_path)\n\n        for lang_pair in self.args.lang_pairs:\n            assert (\n                lang_pair in lang_pair_to_datasets\n            ), ""Not all language pairs have dataset binary paths specified!""\n\n        datasets = {}\n        for lang_pair in self.args.lang_pairs:\n            src, tgt = lang_pair.split(""-"")\n            src_bin_path, tgt_bin_path = lang_pair_to_datasets[lang_pair]\n            corpus = pytorch_translate_data.ParallelCorpusConfig(\n                source=pytorch_translate_data.CorpusConfig(\n                    dialect=src, data_file=src_bin_path\n                ),\n                target=pytorch_translate_data.CorpusConfig(\n                    dialect=tgt, data_file=tgt_bin_path\n                ),\n            )\n            if self.args.log_verbose:\n                print(""Starting to load binarized data files."", flush=True)\n\n            data_utils.validate_corpus_exists(corpus=corpus, split=split)\n\n            tgt_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n                corpus.target.data_file\n            )\n            src_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n                corpus.source.data_file\n            )\n            datasets[lang_pair] = weighted_data.WeightedLanguagePairDataset(\n                src=src_dataset,\n                src_sizes=src_dataset.sizes,\n                src_dict=self.dicts[src],\n                tgt=tgt_dataset,\n                tgt_sizes=tgt_dataset.sizes,\n                tgt_dict=self.dicts[tgt],\n                weights=None,\n                left_pad_source=False,\n            )\n        self.datasets[split] = RoundRobinZipDatasets(\n            OrderedDict(\n                [(lang_pair, datasets[lang_pair]) for lang_pair in self.args.lang_pairs]\n            ),\n            eval_key=None\n            if self.training\n            else f""{self.args.source_lang}-{self.args.target_lang}"",\n        )\n\n        if self.args.log_verbose:\n            print(""Finished loading dataset"", flush=True)\n\n        print(f""| {split} {len(self.datasets[split])} examples"")\n'"
pytorch_translate/tasks/pytorch_translate_multi_task.py,0,"b'#!/usr/bin/env python3\n\nfrom fairseq import models\nfrom fairseq.data import FairseqDataset, data_utils\nfrom fairseq.models import FairseqMultiModel\nfrom fairseq.tasks.multilingual_translation import MultilingualTranslationTask\nfrom pytorch_translate.data import iterators as ptt_iterators\n\n\nclass PyTorchTranslateMultiTask(MultilingualTranslationTask):\n    def build_model(self, args):\n        model = models.build_model(args, self)\n        if not isinstance(model, FairseqMultiModel):\n            raise ValueError(\n                ""PyTorchTranslateMultiTask requires a FairseqMultiModel architecture""\n            )\n        return model\n\n    def get_batch_iterator(\n        self,\n        dataset,\n        max_tokens=None,\n        max_sentences=None,\n        max_positions=None,\n        ignore_invalid_inputs=False,\n        required_batch_size_multiple=1,\n        seed=1,\n        num_shards=1,\n        shard_id=0,\n        num_workers=0,\n    ):\n        assert isinstance(dataset, FairseqDataset)\n\n        # get indices ordered by example size\n        with data_utils.numpy_seed(seed):\n            indices = dataset.ordered_indices()\n\n        # filter examples that are too large\n        indices = data_utils.filter_by_size(\n            indices,\n            dataset,\n            max_positions,\n            raise_exception=(not ignore_invalid_inputs),\n        )\n\n        # create mini-batches with given size constraints\n        batch_sampler = data_utils.batch_by_size(\n            indices,\n            dataset.num_tokens,\n            max_tokens=max_tokens,\n            max_sentences=max_sentences,\n            required_batch_size_multiple=required_batch_size_multiple,\n        )\n\n        # return a reusable, sharded iterator\n        return ptt_iterators.WeightedEpochBatchIterator(\n            dataset=dataset,\n            collate_fn=dataset.collater,\n            batch_sampler=batch_sampler,\n            seed=seed,\n            num_shards=num_shards,\n            shard_id=shard_id,\n            num_workers=num_workers,\n            weights=self.loss_weights,\n        )\n\n    def max_positions(self):\n        """"""Return None to allow model to dictate max sentence length allowed""""""\n        return None\n'"
pytorch_translate/tasks/pytorch_translate_task.py,0,"b'#!/usr/bin/env python3\n\nimport os\nfrom collections import OrderedDict\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nfrom fairseq import data, options\nfrom fairseq.data import ConcatDataset, LanguagePairDataset, NoisingDataset\nfrom fairseq.data.multi_corpus_sampled_dataset import MultiCorpusSampledDataset\nfrom fairseq.data.noising import UnsupervisedMTNoising\nfrom fairseq.tasks import FairseqTask, register_task\nfrom pytorch_translate import constants, utils as pytorch_translate_utils\nfrom pytorch_translate.data import (\n    char_data,\n    data as pytorch_translate_data,\n    dictionary as pytorch_translate_dictionary,\n    utils as data_utils,\n    weighted_data,\n)\nfrom pytorch_translate.data.language_pair_upsampling_dataset import (\n    LanguagePairUpsamplingDataset,\n)\nfrom pytorch_translate.research.multisource import multisource_data\n\n\n@register_task(""pytorch_translate"")\nclass PytorchTranslateTask(FairseqTask):\n    @staticmethod\n    def add_args(parser):\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""-s"", ""--source-lang"", default=None, metavar=""SRC"", help=""source language""\n        )\n        parser.add_argument(\n            ""-t"",\n            ""--target-lang"",\n            default=None,\n            metavar=""TARGET"",\n            help=""target language"",\n        )\n        parser.add_argument(\n            ""--left-pad-source"",\n            default=False,\n            type=bool,\n            metavar=""BOOL"",\n            help=""pad the source on the left (default: False)"",\n        )\n        parser.add_argument(\n            ""--max-source-positions"",\n            default=1024,\n            type=int,\n            metavar=""N"",\n            help=""max number of tokens in the source sequence"",\n        )\n        parser.add_argument(\n            ""--max-target-positions"",\n            default=1024,\n            type=int,\n            metavar=""N"",\n            help=""max number of tokens in the target sequence"",\n        )\n        parser.add_argument(\n            ""--word-dropout-prob-map"",\n            default=None,\n            help=""Use NoisingDataset, and this argument specifies ""\n            ""the probability a token is dropped randomly"",\n        )\n        parser.add_argument(\n            ""--word-blank-prob-map"",\n            default=None,\n            help=""Use NoisingDataset, and this argument specifies ""\n            ""the probability a token is replaced by unk"",\n        )\n        parser.add_argument(\n            ""--max-word-shuffle-distance-map"",\n            default=None,\n            help=""Use NoisingDataset, and this argument specifies ""\n            ""the maximum distance a word could move during the shuffle"",\n        )\n        parser.add_argument(\n            ""--dataset-upsampling"",\n            default=None,\n            metavar=""FILE"",\n            help=""Upsampling for certain datasets, with upsampling rate ""\n            ""represented in a dictionary (dataset, rate). sampling ratio = ""\n            ""upsampling rate * number of lines of the dataset / ""\n            ""(upsampling rate * number of lines of the dataset""\n            ""+ number of total lines of other datsets). At most one of ""\n            ""dataset_upsampling / dataset_relative_ratio could be specified."",\n        )\n        parser.add_argument(\n            ""--dataset-relative-ratio"",\n            default=None,\n            metavar=""FILE"",\n            help=""Relative ratio(one-vs-rest) for certain dataset, ""\n            ""represented in (dataset, ratio) tuple. It would be the final sampling""\n            ""ratio for certain dataset. For example when r = 0.5, half of training""\n            ""corpus would come from this dataset. At most one of ""\n            ""dataset_upsampling / dataset_relative_ratio could be specified."",\n        )\n\n    def __init__(\n        self, args, src_dict, tgt_dict, char_source_dict=None, char_target_dict=None\n    ):\n        super().__init__(args)\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n        self.char_source_dict = char_source_dict\n        self.char_target_dict = char_target_dict\n\n    def build_model(self, args):\n        # set defaults for old model checkpoints\n        args.left_pad_source = getattr(args, ""left_pad_source"", False)\n        return super().build_model(args)\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        args.left_pad_source = options.eval_bool(args.left_pad_source)\n\n        assert not pytorch_translate_data.is_multilingual(\n            args\n        ), ""Must set `--task pytorch_translate_multilingual` for multilingual training""\n\n        # Load dictionaries\n        source_dict = pytorch_translate_dictionary.Dictionary.load(\n            args.source_vocab_file\n        )\n        target_dict = pytorch_translate_dictionary.Dictionary.load(\n            args.target_vocab_file\n        )\n\n        source_lang = args.source_lang or ""src""\n        target_lang = args.target_lang or ""tgt""\n\n        print(f""| [{source_lang}] dictionary: {len(source_dict)} types"")\n        print(f""| [{target_lang}] dictionary: {len(target_dict)} types"")\n\n        use_char_source = (args.char_source_vocab_file != """") or (\n            getattr(args, ""arch"", """") in constants.ARCHS_FOR_CHAR_SOURCE\n        )\n        if use_char_source:\n            char_source_dict = pytorch_translate_dictionary.Dictionary.load(\n                args.char_source_vocab_file\n            )\n            # this attribute is used for CharSourceModel construction\n            args.char_source_dict_size = len(char_source_dict)\n        else:\n            char_source_dict = None\n\n        use_char_target = (getattr(args, ""char_target_vocab_file"", """") != """") or (\n            getattr(args, ""arch"", """") in constants.ARCHS_FOR_CHAR_TARGET\n        )\n        if use_char_target:\n            char_target_dict = pytorch_translate_dictionary.Dictionary.load(\n                args.char_target_vocab_file\n            )\n            args.char_target_dict_size = len(char_target_dict)\n        else:\n            char_target_dict = None\n\n        return cls(\n            args,\n            src_dict=source_dict,\n            tgt_dict=target_dict,\n            char_source_dict=char_source_dict,\n            char_target_dict=char_target_dict,\n        )\n\n    def _load_dataset_single_path(\n        self,\n        split: str,\n        src_bin_path: str,\n        tgt_bin_path: str,\n        weights_file=None,\n        is_npz=True,\n    ):\n        corpus = pytorch_translate_data.ParallelCorpusConfig(\n            source=pytorch_translate_data.CorpusConfig(\n                dialect=self.args.source_lang, data_file=src_bin_path\n            ),\n            target=pytorch_translate_data.CorpusConfig(\n                dialect=self.args.target_lang, data_file=tgt_bin_path\n            ),\n            weights_file=weights_file,\n        )\n\n        if getattr(self.args, ""log_verbose"", False):\n            print(""Starting to load binarized data files."", flush=True)\n        append_bos = getattr(self.args, ""append_bos"", False)\n        data_utils.validate_corpus_exists(corpus=corpus, split=split, is_npz=is_npz)\n\n        if self.char_target_dict is not None:\n            dst_dataset = char_data.InMemoryNumpyWordCharDataset.create_from_file(\n                corpus.target.data_file\n            )\n        else:\n            dst_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n                corpus.target.data_file, is_npz=is_npz\n            )\n\n        if getattr(self.args, ""reverse_target"", None):\n            dst_dataset.reverse()\n        weights_dataset = None\n        if corpus.weights_file and os.path.exists(corpus.weights_file):\n            weights_dataset = weighted_data.IndexedWeightsDataset(corpus.weights_file)\n            print(\n                f""dst: {len(dst_dataset)} lines, weights: {len(weights_dataset)} lines.""\n            )\n            assert len(dst_dataset) == len(weights_dataset)\n\n        if self.char_source_dict is not None:\n            src_dataset = char_data.InMemoryNumpyWordCharDataset.create_from_file(\n                corpus.source.data_file\n            )\n            char_data_class = (\n                char_data.LanguagePairCharDataset\n                if self.char_target_dict is not None\n                else char_data.LanguagePairSourceCharDataset\n            )\n            self.datasets[split] = char_data_class(\n                src=src_dataset,\n                src_sizes=src_dataset.sizes,\n                src_dict=self.source_dictionary,\n                tgt=dst_dataset,\n                tgt_sizes=dst_dataset.sizes,\n                tgt_dict=self.target_dictionary,\n                weights=weights_dataset,\n            )\n        else:\n            src_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n                corpus.source.data_file, is_npz=is_npz\n            )\n            if getattr(self.args, ""train_weights_path"", None):\n                self.datasets[split] = weighted_data.WeightedLanguagePairDataset(\n                    src=src_dataset,\n                    src_sizes=src_dataset.sizes,\n                    src_dict=self.source_dictionary,\n                    tgt=dst_dataset,\n                    tgt_sizes=dst_dataset.sizes,\n                    tgt_dict=self.target_dictionary,\n                    weights=weights_dataset,\n                    left_pad_source=False,\n                )\n            else:\n                self.datasets[split] = LanguagePairDataset(\n                    src=src_dataset,\n                    src_sizes=src_dataset.sizes,\n                    src_dict=self.source_dictionary,\n                    tgt=dst_dataset,\n                    tgt_sizes=dst_dataset.sizes,\n                    tgt_dict=self.target_dictionary,\n                    append_bos=append_bos,\n                    left_pad_source=False,\n                )\n\n    def _normalized_weighted_sampling(self, weights: Dict[str, float]):\n        factor = 1.0 / sum(weights.values())\n        normalized_weights = {k: v * factor for k, v in weights.items()}\n\n        def sample(candidate_list):\n            v = np.random.random()\n            agg = 0\n            for key in candidate_list:\n                agg += normalized_weights[key]\n                if agg > v:\n                    return key\n\n        return sample\n\n    def _load_dataset_multi_path_helper(\n        self,\n        split: str,\n        src_multiple_bin_paths: Dict[str, str],\n        tgt_multiple_bin_paths: Dict[str, str],\n        dataset_upsampling: Optional[Dict[str, float]] = None,\n        dataset_relative_ratio: Optional[Tuple[str, float]] = None,\n        seed: Optional[int] = None,\n        noiser: Optional[Dict[str, UnsupervisedMTNoising]] = None,\n        is_npz: bool = True,\n    ):\n        corpora_map = pytorch_translate_data.ParallelCorporaMapConfig(\n            src_files=src_multiple_bin_paths, tgt_files=tgt_multiple_bin_paths\n        )\n        datasets = OrderedDict()\n        for key in corpora_map.src_files:\n            src, tgt = corpora_map.src_files[key], corpora_map.tgt_files[key]\n            if self.char_target_dict is not None:\n                tgt_dataset = char_data.InMemoryNumpyWordCharDataset.create_from_file(\n                    tgt\n                )\n            else:\n                tgt_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n                    tgt, is_npz=is_npz\n                )\n\n            if self.char_source_dict is not None:\n                src_dataset = char_data.InMemoryNumpyWordCharDataset.create_from_file(\n                    src\n                )\n            else:\n                src_dataset = pytorch_translate_data.InMemoryIndexedDataset.create_from_file(\n                    src, is_npz=is_npz\n                )\n            src_sizes = src_dataset.sizes\n            if noiser is not None and key in noiser:\n                src_dataset = NoisingDataset(\n                    src_dataset=src_dataset,\n                    src_dict=self.source_dictionary,\n                    seed=seed,\n                    noiser=noiser[key],\n                )\n            if self.char_source_dict is not None:\n                char_data_class = (\n                    char_data.LanguagePairCharDataset\n                    if self.char_target_dict is not None\n                    else char_data.LanguagePairSourceCharDataset\n                )\n                datasets[key] = char_data_class(\n                    src=src_dataset,\n                    src_sizes=src_sizes,\n                    src_dict=self.source_dictionary,\n                    tgt=tgt_dataset,\n                    tgt_sizes=tgt_dataset.sizes,\n                    tgt_dict=self.target_dictionary,\n                )\n            else:\n                datasets[key] = LanguagePairDataset(\n                    src=src_dataset,\n                    src_sizes=src_sizes,\n                    src_dict=self.source_dictionary,\n                    tgt=tgt_dataset,\n                    tgt_sizes=tgt_dataset.sizes,\n                    tgt_dict=self.target_dictionary,\n                    left_pad_source=False,\n                )\n        total_line_count = sum(len(datasets[key]) for key in datasets)\n        if dataset_relative_ratio:\n            ds, ratio = dataset_relative_ratio\n            line_count = len(datasets[ds])\n            # By definition ratio = u * line_count / sum(#lines of other datasets)\n            u = (total_line_count - line_count) / line_count * ratio\n            dataset_upsampling = {key: u}\n        elif not dataset_upsampling:\n            dataset_upsampling = {}\n\n        print(f""|dataset upsampling:{dataset_upsampling}"")\n        ds_list = []\n        sample_ratios = []\n        for key, val in datasets.items():\n            ds_list.append(val)\n            sample_ratios.append(int(dataset_upsampling.get(key, 1)))\n\n        self.datasets[split] = LanguagePairUpsamplingDataset(\n            datasets=datasets.values(), sample_ratios=sample_ratios\n        )\n\n    def _load_dataset_multi_path(\n        self, split: str, src_bin_path: str, tgt_bin_path: str, is_npz: bool = True\n    ):\n        assert type(tgt_bin_path) is not str\n        assert set(src_bin_path.keys()) == set(tgt_bin_path.keys())\n        source_lang = self.args.source_lang or ""src""\n        target_lang = self.args.target_lang or ""tgt""\n        direction = source_lang + ""-"" + target_lang\n        dataset_upsampling = (\n            pytorch_translate_utils.maybe_parse_collection_argument(\n                self.args.dataset_upsampling\n            )[direction]\n            if self.args.dataset_upsampling\n            else None\n        )\n        dataset_relative_ratio = (\n            pytorch_translate_utils.maybe_parse_collection_argument(\n                self.args.dataset_relative_ratio\n            )[direction]\n            if self.args.dataset_relative_ratio\n            else None\n        )\n        noiser = {}\n        noise_options = [\n            ""word_dropout_prob"",\n            ""max_word_shuffle_distance"",\n            ""word_blanking_prob"",\n        ]\n        for option in noise_options:\n            option_map = getattr(self.args, option + ""_map"", None)\n            if option_map:\n                option_map = pytorch_translate_utils.maybe_parse_collection_argument(\n                    option_map\n                )[direction]\n                for key in option_map:\n                    if key not in noiser:\n                        noiser[key] = {\n                            noise_option: None for noise_option in noise_options\n                        }\n                    noiser[key][option] = option_map[key]\n\n        for key in noiser:\n            noiser[key] = UnsupervisedMTNoising(\n                dictionary=self.src_dict,\n                max_word_shuffle_distance=noiser[key][""max_word_shuffle_distance""] or 0,\n                word_dropout_prob=noiser[key][""word_dropout_prob""] or 0,\n                word_blanking_prob=noiser[key][""word_blanking_prob""] or 0,\n            )\n\n        if dataset_relative_ratio is not None:\n            assert dataset_upsampling is None, ""dataset_upsampling and ""\n            ""dataset_relative_ratio couldn\'t be specified together.""\n            assert dataset_relative_ratio[0] in src_bin_path.keys()\n            self._load_dataset_multi_path_helper(\n                split=split,\n                src_multiple_bin_paths=src_bin_path,\n                tgt_multiple_bin_paths=tgt_bin_path,\n                dataset_relative_ratio=dataset_relative_ratio,\n                seed=self.args.seed,\n                noiser=noiser,\n                is_npz=is_npz,\n            )\n        elif dataset_upsampling is not None:\n            for key in dataset_upsampling.keys():\n                assert key in src_bin_path.keys()\n            self._load_dataset_multi_path_helper(\n                split=split,\n                src_multiple_bin_paths=src_bin_path,\n                tgt_multiple_bin_paths=tgt_bin_path,\n                dataset_upsampling=dataset_upsampling,\n                seed=self.args.seed,\n                noiser=noiser,\n                is_npz=is_npz,\n            )\n        else:\n            self._load_dataset_multi_path_helper(\n                split=split,\n                src_multiple_bin_paths=src_bin_path,\n                tgt_multiple_bin_paths=tgt_bin_path,\n                seed=self.args.seed,\n                noiser=noiser,\n                is_npz=is_npz,\n            )\n\n    def load_dataset(\n        self,\n        split: str,\n        src_bin_path: str,\n        tgt_bin_path: str,\n        weights_file=None,\n        is_npz=True,\n    ):\n        src_bin_path = pytorch_translate_utils.maybe_parse_collection_argument(\n            src_bin_path\n        )\n        tgt_bin_path = pytorch_translate_utils.maybe_parse_collection_argument(\n            tgt_bin_path\n        )\n        # At most one of dataset_upsampling / dataset_relative_ratio could be\n        # specified.\n        if type(src_bin_path) is str:\n            assert type(tgt_bin_path) is str\n            self._load_dataset_single_path(\n                split=split,\n                src_bin_path=src_bin_path,\n                tgt_bin_path=tgt_bin_path,\n                weights_file=weights_file,\n                is_npz=is_npz,\n            )\n        else:\n            self._load_dataset_multi_path(\n                split, src_bin_path, tgt_bin_path, is_npz=is_npz\n            )\n\n        if getattr(self.args, ""log_verbose"", False):\n            print(""Finished loading dataset"", flush=True)\n\n        print(f""| {split} {len(self.datasets[split])} examples"")\n\n    def load_dataset_from_text(\n        self,\n        split: str,\n        source_text_file: str,\n        target_text_file: str,\n        append_eos: Optional[bool] = False,\n        reverse_source: Optional[bool] = True,\n    ):\n        append_bos = getattr(self.args, ""append_bos"", False)\n        if self.char_target_dict is not None:\n            dst_dataset = char_data.InMemoryNumpyWordCharDataset()\n            dst_dataset.parse(\n                path=target_text_file,\n                word_dict=self.target_dictionary,\n                char_dict=self.char_target_dict,\n                reverse_order=False,\n                append_eos=True,\n            )\n        else:\n            dst_dataset = data.IndexedRawTextDataset(\n                path=target_text_file,\n                dictionary=self.target_dictionary,\n                # We always append EOS to the target sentence since we still want\n                # the model to output an indication the sentence has finished, even\n                # if we don\'t append the EOS symbol to the source sentence\n                # (to prevent the model from misaligning UNKs or other words\n                # to the frequently occurring EOS).\n                append_eos=True,\n                # We don\'t reverse the order of the target sentence, since\n                # even if the source sentence is fed to the model backwards,\n                # we still want the model to start outputting from the first word.\n                reverse_order=False,\n            )\n\n        if self.char_source_dict is not None:\n            src_dataset = char_data.InMemoryNumpyWordCharDataset()\n            src_dataset.parse(\n                path=source_text_file,\n                word_dict=self.source_dictionary,\n                char_dict=self.char_source_dict,\n                reverse_order=reverse_source,\n                append_eos=append_eos,\n            )\n            char_data_class = (\n                char_data.LanguagePairCharDataset\n                if self.char_target_dict is not None\n                else char_data.LanguagePairSourceCharDataset\n            )\n            self.datasets[split] = char_data_class(\n                src=src_dataset,\n                src_sizes=src_dataset.sizes,\n                src_dict=self.source_dictionary,\n                tgt=dst_dataset,\n                tgt_sizes=dst_dataset.sizes,\n                tgt_dict=self.target_dictionary,\n            )\n        else:\n            src_dataset = data.IndexedRawTextDataset(\n                path=source_text_file,\n                dictionary=self.source_dictionary,\n                append_eos=append_eos,\n                reverse_order=reverse_source,\n            )\n            self.datasets[split] = data.LanguagePairDataset(\n                src=src_dataset,\n                src_sizes=src_dataset.sizes,\n                src_dict=self.source_dictionary,\n                tgt=dst_dataset,\n                tgt_sizes=dst_dataset.sizes,\n                tgt_dict=self.target_dictionary,\n                left_pad_source=False,\n                append_bos=append_bos,\n            )\n\n        print(f""| {split} {len(self.datasets[split])} examples"")\n\n    def load_multisource_dataset_from_text(\n        self,\n        split: str,\n        source_text_files: List[str],\n        target_text_file: str,\n        append_eos: Optional[bool] = False,\n        reverse_source: Optional[bool] = True,\n    ):\n        src_dataset = multisource_data.IndexedRawTextMultisentDataset(\n            path=source_text_files,\n            dictionary=self.source_dictionary,\n            append_eos=append_eos,\n            reverse_order=reverse_source,\n        )\n        tgt_dataset = data.IndexedRawTextDataset(\n            path=target_text_file,\n            dictionary=self.target_dictionary,\n            # We always append EOS to the target sentence since we still want\n            # the model to output an indication the sentence has finished, even\n            # if we don\'t append the EOS symbol to the source sentence\n            # (to prevent the model from misaligning UNKs or other words\n            # to the frequently occurring EOS).\n            append_eos=True,\n            # We don\'t reverse the order of the target sentence, since\n            # even if the source sentence is fed to the model backwards,\n            # we still want the model to start outputting from the first word.\n            reverse_order=False,\n        )\n        self.datasets[split] = multisource_data.MultisourceLanguagePairDataset(\n            src=src_dataset,\n            src_sizes=src_dataset.sizes,\n            src_dict=self.source_dictionary,\n            tgt=tgt_dataset,\n            tgt_sizes=tgt_dataset.sizes,\n            tgt_dict=self.target_dictionary,\n        )\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.tgt_dict\n\n\n# We don\'t @register_task since this is mostly used for unit tests and export\nclass DictionaryHolderTask(FairseqTask):\n    """"""A simplified Task that just holds the dictionaries.""""""\n\n    def __init__(self, src_dict, dst_dict):\n        super().__init__(args=None)\n        self.src_dict = src_dict\n        self.dst_dict = dst_dict\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.dst_dict\n\n\n@register_task(""pytorch_translate_multilingual"")\nclass PytorchTranslateMultilingualTask(PytorchTranslateTask):\n    def __init__(self, args, source_dictionaries, target_dictionaries):\n        self.source_dictionaries = source_dictionaries\n        self.target_dictionaries = target_dictionaries\n\n        # Mapping from language IDs to language codes. During training\n        # this list is fully populated. During generation we typically\n        # have only a single source/target dictionary, thus it is important to\n        # call set_encoder/decoder_langs to properly populate these.\n        self.encoder_langs = list(source_dictionaries.keys())\n        self.decoder_langs = list(target_dictionaries.keys())\n\n        self.src_dict = pytorch_translate_dictionary.MaxVocabDictionary()\n        for d in source_dictionaries.values():\n            self.src_dict.push(d)\n        self.tgt_dict = pytorch_translate_dictionary.MaxVocabDictionary()\n        for d in target_dictionaries.values():\n            self.tgt_dict.push(d)\n\n        super().__init__(args, self.src_dict, self.tgt_dict)\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        assert pytorch_translate_data.is_multilingual(\n            args\n        ), ""Must set `--task pytorch_translate_multilingual` for multilingual training""\n        args.left_pad_source = options.eval_bool(args.left_pad_source)\n\n        def load_dicts(langs, paths):\n            dicts = OrderedDict()\n            for lang, dict_path in zip(langs, paths):\n                d = pytorch_translate_dictionary.Dictionary.load(dict_path)\n                dicts[lang] = d\n                print(f""| [{lang}] dictionary: {len(d)} types"")\n            return dicts\n\n        if not hasattr(args, ""multiling_source_vocab_file""):\n            args.multiling_encoder_lang = args.multiling_source_lang\n            args.multiling_source_vocab_file = [args.source_vocab_file]\n        if not hasattr(args, ""multiling_target_vocab_file""):\n            args.multiling_decoder_lang = args.multiling_target_lang\n            args.multiling_target_vocab_file = [args.target_vocab_file]\n\n        # Load dictionaries\n        src_dicts = load_dicts(\n            args.multiling_encoder_lang, args.multiling_source_vocab_file\n        )\n        tgt_dicts = load_dicts(\n            args.multiling_decoder_lang, args.multiling_target_vocab_file\n        )\n\n        return cls(args, src_dicts, tgt_dicts)\n\n    def load_dataset_from_text_multilingual(\n        self,\n        split: str,\n        source_text_file: str,\n        target_text_file: str,\n        source_lang_id: int,\n        target_lang_id: int,\n        append_eos: Optional[bool] = False,\n        reverse_source: Optional[bool] = True,\n    ):\n        src_dataset = pytorch_translate_data.IndexedRawTextDatasetWithLangId(\n            path=source_text_file,\n            dictionary=self.source_dictionary,\n            lang_id=source_lang_id,\n            append_eos=append_eos,\n            reverse_order=reverse_source,\n            prepend_language_id=False,\n        )\n        tgt_dataset = pytorch_translate_data.IndexedRawTextDatasetWithLangId(\n            path=target_text_file,\n            dictionary=self.target_dictionary,\n            lang_id=target_lang_id,\n            append_eos=True,\n            reverse_order=False,\n            prepend_language_id=True,\n        )\n        self.datasets[split] = data.LanguagePairDataset(\n            src=src_dataset,\n            src_sizes=src_dataset.sizes,\n            src_dict=self.source_dictionary,\n            tgt=tgt_dataset,\n            tgt_sizes=tgt_dataset.sizes,\n            tgt_dict=self.target_dictionary,\n        )\n        print(f""| {split} {len(self.datasets[split])} examples"")\n\n    def set_encoder_langs(self, encoder_langs):\n        self.encoder_langs = encoder_langs\n\n    def set_decoder_langs(self, decoder_langs):\n        self.decoder_langs = decoder_langs\n\n    def get_encoder_lang_id(self, lang_code):\n        return self.encoder_langs.index(lang_code)\n\n    def get_decoder_lang_id(self, lang_code):\n        return self.decoder_langs.index(lang_code)\n\n    def get_encoder_lang_code(self, lang_id):\n        return self.encoder_langs[lang_id]\n\n    def get_decoder_lang_code(self, lang_id):\n        return self.decoder_langs[lang_id]\n'"
pytorch_translate/tasks/semi_supervised_task.py,0,"b'#!/usr/bin/env python3\n\nimport json\nfrom collections import OrderedDict\nfrom typing import Optional\n\nfrom fairseq import models\nfrom fairseq.data import (\n    BacktranslationDataset,\n    LanguagePairDataset,\n    RoundRobinZipDatasets,\n    TransformEosDataset,\n)\nfrom fairseq.models import FairseqMultiModel\nfrom fairseq.tasks import register_task\nfrom pytorch_translate import (  # noqa F401 - removing rnn is causing test failures\n    beam_decode,\n    constants,\n    rnn,\n)\nfrom pytorch_translate.data import (\n    data as ptt_data,\n    dictionary as pytorch_translate_dictionary,\n    utils as ptt_data_utils,\n    weighted_data,\n)\nfrom pytorch_translate.tasks.pytorch_translate_multi_task import (\n    PyTorchTranslateMultiTask,\n)\nfrom pytorch_translate.tasks.pytorch_translate_task import PytorchTranslateTask\n\n\n@register_task(constants.SEMI_SUPERVISED_TASK)\nclass PytorchTranslateSemiSupervised(PyTorchTranslateMultiTask):\n    def __init__(self, args, dicts, training):\n        lang_pairs = [\n            f""{self.source_lang}-{self.target_lang}"",\n            f""{self.target_lang}-{self.source_lang}"",\n            (\n                f""{self.source_lang}-{self.target_lang}_""\n                f""{constants.MONOLINGUAL_DATA_IDENTIFIER}""\n            ),\n            (\n                f""{self.target_lang}-{self.source_lang}_""\n                f""{constants.MONOLINGUAL_DATA_IDENTIFIER}""\n            ),\n        ]\n        # This is explicitly set so that we can re-use code from\n        # MultilingualTranslationTask. During initialization, it sets\n        # args.lang_pairs as self.lang_pairs\n        args.lang_pairs = lang_pairs\n        super().__init__(args, dicts, training)\n        self.eval_lang_pairs = [\n            f""{self.source_lang}-{self.target_lang}"",\n            f""{self.target_lang}-{self.source_lang}"",\n        ]\n        self.remove_eos_from_source = not args.append_eos_to_source\n        self.args = args\n        self.model = None\n\n        """"""\n        loss_weights refers to weights given to training loss for constituent\n        models for specified number of epochs. If we don\'t specify a model, they\n        receive a weight of 1\n        The format is [(epochs, {model: weight})]\n\n        Sample input:\n        [(5, {\'src-tgt\': 1, \'src-tgt_mono\': 0}), (10, {\'src-tgt\': 0.5, \'src-tgt_mono\': 0.5})]\n        Here, we give assign weights as follows:\n        For first 5 epochs, \'src-tgt\' model gets weight 1, \'src-tgt_mono\' gets 0\n        For the next 10 epochs (till the end of training), \'src-tgt\' model gets\n            weight 0.5, \'src-tgt_mono\' gets 0.5, the rest get 1\n\n        """"""\n\n        # default loss_weights is equal weighting for all model keys\n        self.loss_weights = [\n            (100, {""src-tgt"": 1, ""src-tgt_mono"": 1, ""tgt-src"": 1, ""tgt-src_mono"": 1})\n        ]\n        if hasattr(self.args, ""loss_weights_json"") and self.args.loss_weights_json:\n            self.loss_weights = PytorchTranslateSemiSupervised.parse_loss_weights(\n                loss_weights_json=self.args.loss_weights_json\n            )\n\n    @staticmethod\n    def add_args(parser):\n        PytorchTranslateTask.add_args(parser)\n\n        """"""Add semi-supervised arguments to the parser.""""""\n        parser.add_argument(\n            ""--train-mono-source-text-file"",\n            default="""",\n            help=""Path for the text file containing monolingual source ""\n            ""training examples."",\n        )\n        parser.add_argument(\n            ""--train-mono-target-text-file"",\n            default="""",\n            help=""Path for the text file containing monolingual target ""\n            ""training examples."",\n        )\n        parser.add_argument(\n            ""--monolingual-ratio"",\n            default=None,\n            type=float,\n            metavar=""N"",\n            help=""Upper-bounds the number of monolingual examples to N times ""\n            ""the amount of parallel data."",\n        )\n        parser.add_argument(\n            ""--loss-weights-json"",\n            default="""",\n            help=""JSON representation of `loss_weights`:""\n            ""[[num_epochs, {\'model_key\': weight, ...}], ...]"",\n        )\n\n    @staticmethod\n    def parse_loss_weights(loss_weights_json: str):\n        # [[num_epochs, {\'model_key\': weight, ...}], ...]\n        loss_weights_decoded_json = json.loads(loss_weights_json.replace(""\'"", \'""\'))\n        # [(num_epochs, {\'model_key\': weight, ...}), ...]\n        loss_weights = [\n            (num_epochs, weights_dict)\n            for num_epochs, weights_dict in loss_weights_decoded_json\n        ]\n        return loss_weights\n\n    def load_monolingual_dataset(\n        self, bin_path, is_source=False, num_examples_limit=None\n    ):\n        return ptt_data_utils.load_monolingual_dataset(\n            bin_path=bin_path,\n            is_source=is_source,\n            char_source_dict=None,\n            log_verbose=self.args.log_verbose,\n            num_examples_limit=num_examples_limit,\n        )\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        # load dictionaries\n        cls.source_lang = args.source_lang or ""src""\n        cls.target_lang = args.target_lang or ""tgt""\n\n        dicts = OrderedDict()\n        dicts[cls.source_lang] = pytorch_translate_dictionary.Dictionary.load(\n            args.source_vocab_file\n        )\n        dicts[cls.target_lang] = pytorch_translate_dictionary.Dictionary.load(\n            args.target_vocab_file\n        )\n        print(f""| [{cls.source_lang}] dictionary: {len(dicts[cls.source_lang])} types"")\n        print(f""| [{cls.target_lang}] dictionary: {len(dicts[cls.target_lang])} types"")\n        # TODO: Handle task setup for evals\n        return cls(args, dicts, training=True)\n\n    def load_dataset(\n        self, split, src_bin_path, tgt_bin_path, forward_model=None, backward_model=None\n    ):\n        """"""Load a dataset split.""""""\n\n        corpus = ptt_data.ParallelCorpusConfig(\n            source=ptt_data.CorpusConfig(\n                dialect=self.source_lang, data_file=src_bin_path\n            ),\n            target=ptt_data.CorpusConfig(\n                dialect=self.target_lang, data_file=tgt_bin_path\n            ),\n            weights_file=None,\n        )\n\n        if self.args.log_verbose:\n            print(""Starting to load binarized data files."", flush=True)\n        ptt_data_utils.validate_corpus_exists(corpus=corpus, split=split)\n\n        forward_tgt_dataset = ptt_data.InMemoryIndexedDataset.create_from_file(\n            corpus.target.data_file\n        )\n        backward_tgt_dataset = ptt_data.InMemoryIndexedDataset.create_from_file(\n            corpus.source.data_file\n        )\n        forward_src_dataset = ptt_data.InMemoryIndexedDataset.create_from_file(\n            corpus.source.data_file\n        )\n        backward_src_dataset = ptt_data.InMemoryIndexedDataset.create_from_file(\n            corpus.target.data_file\n        )\n        forward_parallel_dataset = weighted_data.WeightedLanguagePairDataset(\n            src=forward_src_dataset,\n            src_sizes=forward_src_dataset.sizes,\n            src_dict=self.source_dictionary,\n            tgt=forward_tgt_dataset,\n            tgt_sizes=forward_tgt_dataset.sizes,\n            tgt_dict=self.target_dictionary,\n            remove_eos_from_source=self.remove_eos_from_source,\n            append_eos_to_target=True,\n        )\n        backward_parallel_dataset = weighted_data.WeightedLanguagePairDataset(\n            src=backward_src_dataset,\n            src_sizes=backward_src_dataset.sizes,\n            src_dict=self.target_dictionary,\n            tgt=backward_tgt_dataset,\n            tgt_sizes=backward_tgt_dataset.sizes,\n            tgt_dict=self.source_dictionary,\n            remove_eos_from_source=self.remove_eos_from_source,\n            append_eos_to_target=True,\n        )\n\n        dataset_map = OrderedDict(\n            [\n                (f""{self.source_lang}-{self.target_lang}"", forward_parallel_dataset),\n                (f""{self.target_lang}-{self.source_lang}"", backward_parallel_dataset),\n            ]\n        )\n\n        assert (forward_model and backward_model) or (\n            forward_model is None and backward_model is None\n        ), (\n            ""Only one of forward or backward models can\'t be null;""\n            "" both have to be non-null or null""\n        )\n        if forward_model and backward_model:\n            fwd_generator = beam_decode.SequenceGenerator(\n                models=[forward_model], tgt_dict=self.source_dictionary\n            )\n            bwd_generator = beam_decode.SequenceGenerator(\n                models=[backward_model], tgt_dict=self.target_dictionary\n            )\n\n            def monolingual_dataset(\n                path,\n                dictionary,\n                is_source=False,\n                num_examples_limit: Optional[int] = None,\n            ):\n                dataset = self.load_monolingual_dataset(\n                    path, is_source=is_source, num_examples_limit=num_examples_limit\n                )\n                return LanguagePairDataset(\n                    src=dataset,\n                    src_sizes=dataset.sizes,\n                    src_dict=dictionary,\n                    tgt=None,\n                    tgt_sizes=None,\n                    tgt_dict=None,\n                )\n\n            monolingual_num_examples_limit = None\n            if self.args.monolingual_ratio is not None:\n                monolingual_num_examples_limit = int(\n                    self.args.monolingual_ratio * len(forward_parallel_dataset)\n                )\n\n            src_dataset = monolingual_dataset(\n                path=self.args.train_mono_source_binary_path,\n                dictionary=self.source_dictionary,\n                is_source=True,\n                num_examples_limit=monolingual_num_examples_limit,\n            )\n            tgt_dataset = monolingual_dataset(\n                path=self.args.train_mono_target_binary_path,\n                dictionary=self.target_dictionary,\n                is_source=False,\n                num_examples_limit=monolingual_num_examples_limit,\n            )\n\n            def generate_fn(generator):\n                def _generate_fn(sample):\n                    net_input = sample[""net_input""]\n                    maxlen = int(\n                        self.args.max_len_a * net_input[""src_tokens""].size(1)\n                        + self.args.max_len_b\n                    )\n                    return generator.generate(net_input, maxlen=maxlen)\n\n                return _generate_fn\n\n            dataset_map[\n                f""{self.source_lang}-""\n                f""{self.target_lang}_{constants.MONOLINGUAL_DATA_IDENTIFIER}""\n            ] = weighted_data.WeightedBacktranslationDataset(\n                dataset=BacktranslationDataset(\n                    tgt_dataset=TransformEosDataset(\n                        dataset=tgt_dataset,\n                        eos=self.target_dictionary.eos(),\n                        # Remove EOS from the input before backtranslation.\n                        remove_eos_from_src=True,\n                    ),\n                    src_dict=self.source_dictionary,\n                    backtranslation_fn=generate_fn(bwd_generator),\n                    output_collater=TransformEosDataset(\n                        dataset=tgt_dataset,\n                        eos=self.target_dictionary.eos(),\n                        # The original input (now the target) doesn\'t have\n                        # an EOS, so we need to add one. The generated\n                        # backtranslation (now the source) will have an EOS,\n                        # so we want to remove it.\n                        append_eos_to_tgt=True,\n                        remove_eos_from_src=True,\n                    ).collater,\n                )\n            )\n            dataset_map[\n                f""{self.target_lang}-""\n                f""{self.source_lang}_{constants.MONOLINGUAL_DATA_IDENTIFIER}""\n            ] = weighted_data.WeightedBacktranslationDataset(\n                dataset=BacktranslationDataset(\n                    tgt_dataset=src_dataset,\n                    src_dict=self.source_dictionary,\n                    backtranslation_fn=generate_fn(fwd_generator),\n                    output_collater=TransformEosDataset(\n                        dataset=src_dataset,\n                        eos=self.source_dictionary.eos(),\n                        # The original input (now the target) doesn\'t have\n                        # an EOS, so we need to add one. The generated\n                        # backtranslation (now the source) will have an EOS,\n                        # so we want to remove it.\n                        append_eos_to_tgt=True,\n                        remove_eos_from_src=True,\n                    ).collater,\n                )\n            )\n\n        # print before loading RoundRobinZipDatasets to help catch any bugs\n        for dataset_key, dataset in dataset_map.items():\n            print(f""| {split}: {dataset_key} {len(dataset)} examples in dataset"")\n\n        self.datasets[split] = RoundRobinZipDatasets(dataset_map)\n        print(\n            f""| {split} {len(self.datasets[split])} examples in RoundRobinZipDatasets""\n        )\n\n        if self.args.log_verbose:\n            print(""Finished loading dataset"", flush=True)\n\n    def build_model(self, args):\n        model = models.build_model(args, self)\n        self.model = model\n        if not isinstance(model, FairseqMultiModel):\n            raise ValueError(\n                ""PytorchTranslateSemiSupervised task requires a FairseqMultiModel ""\n                ""architecture""\n            )\n        forward_pair = ""-"".join([self.source_lang, self.target_lang])\n        backward_pair = ""-"".join([self.target_lang, self.source_lang])\n        self.forward_model = model.models[forward_pair]\n        self.backward_model = model.models[backward_pair]\n        return model\n'"
pytorch_translate/tasks/translation_from_pretrained_xlm.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nfrom fairseq import options, tokenizer\nfrom fairseq.tasks import register_task\nfrom pytorch_translate import constants\nfrom pytorch_translate.data.masked_lm_dictionary import MaskedLMDictionary\nfrom pytorch_translate.tasks.pytorch_translate_task import PytorchTranslateTask\n\n\n@register_task(""pytorch_translate_translation_from_pretrained_xlm"")\nclass PytorchTranslateTranslationFromPretrainedXLMTask(PytorchTranslateTask):\n    """"""\n    Same as TranslationTask except use the MaskedLMDictionary class so that\n    we can load data that was binarized with the MaskedLMDictionary class.\n\n    This task should be used for the entire training pipeline when we want to\n    train an NMT model from a pretrained XLM checkpoint: binarizing NMT data,\n    training NMT with the pretrained XLM checkpoint, and subsequent evaluation\n    of that trained model.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        PytorchTranslateTask.add_args(parser)\n        """"""Add task-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--save-only"", action=""store_true"", help=""skip eval and only do save""\n        )\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        """"""Load the masked LM dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        """"""\n        return MaskedLMDictionary.load(filename)\n\n    @classmethod\n    def build_dictionary(\n        cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8\n    ):\n        """"""Build the dictionary\n\n        Args:\n            filenames (list): list of filenames\n            workers (int): number of concurrent workers\n            threshold (int): defines the minimum word count\n            nwords (int): defines the total number of words in the final dictionary,\n                including special symbols\n            padding_factor (int): can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        """"""\n        d = MaskedLMDictionary()\n        for filename in filenames:\n            MaskedLMDictionary.add_file_to_dictionary(\n                filename, d, tokenizer.tokenize_line, workers\n            )\n        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n        return d\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        args.left_pad_source = options.eval_bool(args.left_pad_source)\n\n        # Load dictionaries\n        source_dict = MaskedLMDictionary.load(args.source_vocab_file)\n        target_dict = MaskedLMDictionary.load(args.target_vocab_file)\n\n        source_lang = args.source_lang or ""src""\n        target_lang = args.target_lang or ""tgt""\n\n        print(f""| [{source_lang}] dictionary: {len(source_dict)} types"")\n        print(f""| [{target_lang}] dictionary: {len(target_dict)} types"")\n\n        use_char_source = (args.char_source_vocab_file != """") or (\n            getattr(args, ""arch"", """") in constants.ARCHS_FOR_CHAR_SOURCE\n        )\n        if use_char_source:\n            char_source_dict = MaskedLMDictionary.load(args.char_source_vocab_file)\n            # this attribute is used for CharSourceModel construction\n            args.char_source_dict_size = len(char_source_dict)\n        else:\n            char_source_dict = None\n\n        return cls(args, source_dict, target_dict, char_source_dict)\n'"
pytorch_translate/tasks/translation_lev_task.py,0,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nfrom fairseq import options\nfrom fairseq.tasks import register_task\nfrom fairseq.tasks.translation_lev import TranslationLevenshteinTask\nfrom pytorch_translate.data import dictionary as pytorch_translate_dictionary\nfrom pytorch_translate.tasks.pytorch_translate_task import PytorchTranslateTask\n\n\n@register_task(""ptt_translation_lev"")\nclass PytorchTranslationLevenshteinTask(PytorchTranslateTask):\n    """"""\n    Translation (Sequence Generation) task for Levenshtein Transformer\n    See `""Levenshtein Transformer"" <https://arxiv.org/abs/1905.11006>`_.\n    """"""\n\n    def __init__(\n        self, args, src_dict, tgt_dict, char_source_dict=None, char_target_dict=None\n    ):\n        super().__init__(args, src_dict, tgt_dict, char_source_dict, char_target_dict)\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n        self.char_source_dict = char_source_dict\n        self.char_target_dict = char_target_dict\n        self.trans_lev_task = TranslationLevenshteinTask(args, src_dict, tgt_dict)\n\n    @staticmethod\n    def add_args(parser):\n        TranslationLevenshteinTask.add_args(parser)\n\n    def inject_noise(self, target_tokens):\n        return self.trans_lev_task.inject_noise(target_tokens)\n\n    def build_generator(self, models, args):\n        self.trans_lev_task.build_generator(models, args)\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        args.left_pad_source = options.eval_bool(args.left_pad_source)\n        source_dict = pytorch_translate_dictionary.Dictionary.load(\n            args.source_vocab_file\n        )\n        target_dict = pytorch_translate_dictionary.Dictionary.load(\n            args.target_vocab_file\n        )\n        source_lang = args.source_lang or ""src""\n        target_lang = args.target_lang or ""tgt""\n        args.append_bos = True\n\n        print(f""| [{source_lang}] dictionary: {len(source_dict)} types"")\n        print(f""| [{target_lang}] dictionary: {len(target_dict)} types"")\n\n        return cls(args, source_dict, target_dict)\n\n    def train_step(\n        self, sample, model, criterion, optimizer, update_num, ignore_grad=False\n    ):\n        return self.trans_lev_task.train_step(\n            sample, model, criterion, optimizer, update_num, ignore_grad\n        )\n\n    def valid_step(self, sample, model, criterion):\n        return self.trans_lev_task.valid_step(sample, model, criterion)\n'"
pytorch_translate/tasks/utils.py,0,"b'#!/usr/bin/env python3\n\nfrom collections import OrderedDict\n\nfrom pytorch_translate.data import dictionary as pytorch_translate_dictionary\n\n\ndef load_multilingual_vocabulary(args):\n    dicts = OrderedDict()\n    vocabulary_list = getattr(args, ""vocabulary"", [])\n    comparison_lang = None\n    for vocabulary in vocabulary_list:\n        assert (\n            "":"" in vocabulary\n        ), ""--vocabulary must be specified in the format lang:path""\n        lang, path = vocabulary.split("":"")\n        dicts[lang] = pytorch_translate_dictionary.Dictionary.load(path)\n        if len(dicts) > 1:\n            assert dicts[lang].pad() == dicts[comparison_lang].pad()\n            assert dicts[lang].eos() == dicts[comparison_lang].eos()\n            assert dicts[lang].unk() == dicts[comparison_lang].unk()\n        else:\n            comparison_lang = lang\n        print(f""| [{lang}] dictionary: {len(dicts[lang])} types"")\n\n    return dicts\n'"
pytorch_translate/test/__init__.py,0,b''
pytorch_translate/test/test_DecoderBatchedStepEnsemble.py,13,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom pytorch_translate.beam_search_and_decode_v2 import (\n    DecoderBatchedStepEnsemble2BeamWithEOS,\n)\nfrom pytorch_translate.ensemble_export import EncoderEnsemble\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestDecoderBatchedStepEnsemble(unittest.TestCase):\n    def test_decoder_ensemble_2beam_with_eos(self):\n        """"""\n        Test the functionality of DecoderBatchedStepEnsemble2BeamWithEOS class.\n        At final step, it generates EOS tokens.\n        """"""\n        test_args = test_utils.ModelParamsDict(arch=""rnn"")\n        samples, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        model = task.build_model(test_args)\n        eos_token = tgt_dict.eos()\n\n        encoder_ensemble = EncoderEnsemble([model])\n        src_tokens = torch.LongTensor([4, 5, 6, 7, 8]).unsqueeze(1)\n        src_lengths = torch.LongTensor([5])\n        enc_inputs = (src_tokens, src_lengths)\n        encoder_outputs = encoder_ensemble(*enc_inputs)\n\n        beam_size = 8\n        word_reward = 1\n        unk_reward = -1\n        decoder_ensemble_2beam_with_eos = DecoderBatchedStepEnsemble2BeamWithEOS(\n            models=[model],\n            tgt_dict=tgt_dict,\n            beam_size=beam_size,\n            word_reward=word_reward,\n            unk_reward=unk_reward,\n        )\n\n        prev_tokens = torch.LongTensor([eos_token])\n        prev_scores = torch.FloatTensor([0.0])\n        timestep = torch.LongTensor([0])\n        final_step = torch.tensor([False], dtype=torch.bool)\n        maxLen = 5\n        active_hypos = torch.LongTensor([0])\n        num_steps = torch.LongTensor([maxLen])\n\n        decoder_with_eos_first_step_outputs = decoder_ensemble_2beam_with_eos(\n            prev_tokens,\n            prev_scores,\n            active_hypos,\n            timestep,\n            final_step,\n            *encoder_outputs,\n        )\n\n        (\n            cand_tokens,\n            cand_scores,\n            cand_prev_hypos_indices,\n            cand_attn_weights,\n            active_hypos,\n            *states_with_eos,\n        ) = decoder_with_eos_first_step_outputs\n\n        for i in range(len([model])):\n            states_with_eos[i] = states_with_eos[i].repeat(1, beam_size, 1)\n\n        for i in range(num_steps - 1):\n            decoder_step_with_eos_outputs = decoder_ensemble_2beam_with_eos(\n                cand_tokens,\n                cand_scores,\n                active_hypos,\n                torch.tensor([i + 1]),\n                final_step,\n                *states_with_eos,\n            )\n            (\n                cand_tokens,\n                cand_scores,\n                cand_prev_hypos_indices,\n                cand_attn_weights,\n                active_hypos,\n                *states_with_eos,\n            ) = decoder_step_with_eos_outputs\n\n        # Test the outputs of final tesp\n        decoder_final_with_eos_outputs = decoder_ensemble_2beam_with_eos(\n            cand_tokens,\n            cand_scores,\n            active_hypos,\n            torch.tensor([num_steps]),\n            torch.tensor([True]),\n            *states_with_eos,\n        )\n\n        np.testing.assert_array_equal(\n            decoder_final_with_eos_outputs[0],\n            torch.LongTensor([eos_token]).repeat(2 * beam_size),\n        )\n        np.testing.assert_array_equal(\n            decoder_final_with_eos_outputs[2],\n            torch.LongTensor(np.array([i for i in range(2 * beam_size)])),\n        )\n'"
pytorch_translate/test/test_attention.py,8,"b'#!/usr/bin/env python3\n\nimport random\nimport unittest\n\nimport numpy as np\nimport pytorch_translate.attention.multihead_attention as multihead_attention\nimport torch\nfrom pytorch_translate.attention import attention_utils, dot_attention, mlp_attention\n\n\nclass TestAttention(unittest.TestCase):\n    def setUp(self):\n        self.bsz = 10\n        self.src_len = 5\n        self.ctx_dim = 3\n        self.dec_dim = 4\n        self.att_dim = 2\n\n    def test_masked_softmax(self):\n        scores = torch.rand(20, 20)\n        lengths = torch.arange(start=1, end=21)\n\n        masked_normalized_scores = attention_utils.masked_softmax(\n            scores, lengths, src_length_masking=True\n        )\n\n        for i in range(20):\n            scores_sum = masked_normalized_scores[i].numpy().sum()\n            self.assertAlmostEqual(scores_sum, 1, places=6)\n\n    def _test_attention(self, attention):\n        dummy_source_hids = torch.rand(self.src_len, self.bsz, self.ctx_dim)\n        dummy_decoder_state = torch.rand(self.bsz, self.dec_dim)\n        dummy_src_lengths = torch.fmod(torch.arange(self.bsz), self.src_len) + 1\n        attention(dummy_decoder_state, dummy_source_hids, dummy_src_lengths)\n\n    def test_dot_attention(self):\n        self._test_attention(\n            dot_attention.DotAttention(\n                self.dec_dim,\n                self.ctx_dim,\n                src_length_masking=True,\n                force_projection=True,\n            )\n        )\n\n    def test_mlp_attention(self):\n        self._test_attention(\n            mlp_attention.MLPAttention(\n                self.dec_dim,\n                self.ctx_dim,\n                src_length_masking=True,\n                attention_dim=self.att_dim,\n            )\n        )\n\n\ndef _softmax(x):  # softmax over 4 dim matrix\n    """""" Numpy-based reference softmax over 4 dim matrix""""""\n    output = np.zeros(x.shape, dtype=np.float32)\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            for k in range(x.shape[2]):\n                x_curr = x[i, j, k, :]\n                e_x = np.exp(x_curr - np.amax(x_curr))\n                output[i, j, k, :] = e_x / np.sum(e_x)\n    return output\n\n\ndef _batchmatmul(a, b):  # batchmatmul over 4 dim matrix\n    """""" Numpy-based batch matrix multiply over 4 dim matrix""""""\n    assert a.shape[0] == b.shape[0]\n    assert a.shape[1] == b.shape[1]\n    retval = np.zeros(\n        (a.shape[0], a.shape[1], a.shape[2], b.shape[3]), dtype=np.float32\n    )\n    for i in range(a.shape[0]):\n        for j in range(a.shape[1]):\n            retval[i, j, :, :] = np.matmul(a[i, j, :, :], b[i, j, :, :])\n    return retval\n\n\nclass MultiheadAttentionTest(unittest.TestCase):\n    def _scaled_dot_attn_ref(self, Q, K, V, dims, unseen_mask=False, src_lengths=None):\n        """""" Numpy-based reference implementation of scaled dot attention\n        for testing""""""\n        QKT = _batchmatmul(\n            Q,\n            np.transpose(K, axes=[0, 1, 3, 2])\n            / np.sqrt(dims[3], dtype=np.float32),  # divide by sqrt(d_head)\n        )\n        if unseen_mask or src_lengths is not None:\n            b1, b2, s1, s2 = QKT.shape\n            # assert s1 == s2\n            for i in range(b1):\n                for j in range(b2):\n                    for m in range(s1):\n                        for n in range(s2):\n                            if unseen_mask and n > m:\n                                QKT[i, j, m, n] = -np.inf\n                            if src_lengths is not None and n >= src_lengths[i]:\n                                QKT[i, j, m, n] = -np.inf\n        reference = _softmax(QKT)\n        reference = _batchmatmul(reference, V)\n        return reference\n\n    def _generate_src_lengths(self, batch_size, seq_len):\n        src_lengths = np.array([random.randint(1, seq_len) for i in range(batch_size)])\n\n        # max source length has to equal seq_len, so randomly choose\n        # one example to have source length = seq_len\n        max_len_example_i = random.randint(0, batch_size - 1)\n        src_lengths[max_len_example_i] = seq_len\n\n        src_lengths_tensor = torch.from_numpy(src_lengths).int()\n        return src_lengths, src_lengths_tensor\n\n    def _split_heads_ref(self, X, dims, nheads, d_head):\n        X_split = np.reshape(X, dims[:2] + [nheads, d_head])\n        X_split_transposed = np.transpose(X_split, [0, 2, 1, 3])\n        reference = np.reshape(X_split_transposed, [dims[0], nheads, dims[1], d_head])\n        return reference\n\n    def _combine_heads_ref(self, X, dims, nheads, d_head):\n        X_transposed = np.transpose(X, [0, 2, 1, 3])\n        reference = np.reshape(X_transposed, dims[:2] + [nheads * d_head])\n        return reference\n\n    def _fc(self, X, X_name, module, start=None, end=None):\n        X_fc_b = None\n        X_fc_w = None\n        for name, param in module.named_parameters():\n            if X_name + "".weight"" in name or X_name + ""_weight"" in name:\n                if X_fc_w is not None:\n                    raise Exception(f""Duplicate FC name {name} found"")\n                X_fc_w = param.detach().numpy()\n            elif X_name + "".bias"" in name or X_name + ""_bias"" in name:\n                if X_fc_b is not None:\n                    raise Exception(f""Duplicate FC name {name} found"")\n                X_fc_b = param.detach().numpy()\n        return np.matmul(X, np.transpose(X_fc_w)) + X_fc_b\n\n    def _multihead_attn_test_helper(self, use_src_lengths):\n        for _ in range(100):\n            batch_sz, seq_len = [random.randint(2, 10) for r in range(2)]\n            d_head = random.randint(3, 10)\n            nheads = random.randint(3, 10)\n            d_model = d_head * nheads\n            dims = [batch_sz, seq_len, d_model]\n\n            src_lengths = None\n            src_lengths_tensor = None\n            if use_src_lengths:\n                src_lengths, src_lengths_tensor = self._generate_src_lengths(\n                    batch_size=batch_sz, seq_len=seq_len\n                )\n\n            decoder_state = np.random.rand(batch_sz, d_model).astype(np.float32)\n            K = np.random.rand(*dims).astype(np.float32)\n            V = K\n            Q = np.expand_dims(decoder_state, 1)\n\n            decoder_state_tensor = torch.from_numpy(decoder_state).float()\n            source_hid_tensor = torch.from_numpy(K).float().transpose(0, 1)\n\n            multihead_attn_module = multihead_attention.MultiheadAttention(\n                context_dim=d_model, decoder_hidden_state_dim=d_model, nheads=nheads\n            )\n\n            for squeeze in [True, False]:\n                result = (\n                    multihead_attn_module(\n                        decoder_state=decoder_state_tensor,\n                        source_hids=source_hid_tensor,\n                        src_lengths=src_lengths_tensor,\n                        squeeze=squeeze,\n                    )[0]\n                    .detach()\n                    .numpy()\n                )\n\n                if not squeeze:\n                    self.assertEqual(result.ndim, 3)\n                    result = np.squeeze(result, axis=0)\n\n                Q_fc = self._fc(Q, ""q_proj"", multihead_attn_module, end=d_model)\n                K_fc = self._fc(\n                    K, ""k_proj"", multihead_attn_module, start=d_model, end=2 * d_model\n                )\n                V_fc = self._fc(V, ""v_proj"", multihead_attn_module, start=2 * d_model)\n\n                Q_split = self._split_heads_ref(\n                    Q_fc, [batch_sz, 1, d_model], nheads, d_head\n                )\n                K_split = self._split_heads_ref(K_fc, dims, nheads, d_head)\n                V_split = self._split_heads_ref(V_fc, dims, nheads, d_head)\n\n                attn_heads = self._scaled_dot_attn_ref(\n                    Q=Q_split,\n                    K=K_split,\n                    V=V_split,\n                    dims=Q_split.shape,\n                    src_lengths=src_lengths,\n                )\n\n                combined_attn_heads = self._combine_heads_ref(\n                    X=attn_heads, dims=[batch_sz, 1], nheads=nheads, d_head=d_head\n                )\n\n                reference = self._fc(\n                    combined_attn_heads, ""out_proj"", multihead_attn_module\n                )\n                reference = np.squeeze(reference, axis=1)\n                self.assertEqual(tuple(result.shape), (batch_sz, d_model))\n                np.testing.assert_allclose(result, reference, atol=1e-5)\n\n    def test_multihead_attn_no_masking(self):\n        self._multihead_attn_test_helper(use_src_lengths=None)\n\n    def test_multihead_attn_with_src_lengths(self):\n        self._multihead_attn_test_helper(use_src_lengths=True)\n'"
pytorch_translate/test/test_beam_decode.py,16,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom typing import Any, List\n\nimport numpy as np\nimport torch\nfrom pytorch_translate import char_source_model  # noqa\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate import beam_decode, generate\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestBeamDecode(unittest.TestCase):\n    def test_basic_generate(self):\n        test_args = test_utils.ModelParamsDict()\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        model = task.build_model(test_args)\n        translator = beam_decode.SequenceGenerator([model], task.target_dictionary)\n        src_tokens = torch.LongTensor([[0, 0, 0], [0, 0, 0]])\n        src_lengths = torch.LongTensor([3, 3])\n        encoder_input = {""src_tokens"": src_tokens, ""src_lengths"": src_lengths}\n        translator.generate(encoder_input, maxlen=7)\n\n    def test_char_rnn_generate(self):\n        test_args = test_utils.ModelParamsDict(sequence_lstm=True)\n        test_args.arch = ""char_source""\n        test_args.char_source_dict_size = 126\n        test_args.char_embed_dim = 8\n        test_args.char_rnn_units = 12\n        test_args.char_rnn_layers = 2\n\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        model = task.build_model(test_args)\n        translator = beam_decode.SequenceGenerator(\n            [model], task.target_dictionary, use_char_source=True\n        )\n        src_tokens = torch.LongTensor([[0, 0, 0], [0, 0, 0]])\n        src_lengths = torch.LongTensor([3, 3])\n        char_inds = torch.LongTensor(np.zeros((2, 3, 5)))\n        word_lengths = torch.LongTensor([[5, 5, 5], [5, 5, 5]])\n        encoder_input = {\n            ""src_tokens"": src_tokens,\n            ""src_lengths"": src_lengths,\n            ""char_inds"": char_inds,\n            ""word_lengths"": word_lengths,\n        }\n        translator.generate(encoder_input, maxlen=7)\n\n    def test_gather_probs_with_vr(self):\n        """""" Tests gather_probs when there is vocab reduction """"""\n        all_translation_tokens: List[Any] = [\n            torch.LongTensor([3, 7, 8, 9]),\n            torch.LongTensor([0, 3, 5]),\n        ]\n        all_probs: List[Any] = [\n            torch.FloatTensor([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]]),\n            torch.FloatTensor([[0.4, 0.5, 0.1], [0.4, 0.5, 0.1]]),\n        ]\n        avg_probs, possible_translation_tokens = beam_decode.SequenceGenerator.gather_probs(\n            all_translation_tokens=all_translation_tokens, all_probs=all_probs\n        )\n        avg_probs = avg_probs.detach().cpu().numpy()\n        possible_translation_tokens = possible_translation_tokens.detach().cpu().numpy()\n\n        avg_probs_ref = sorted([0.4, 0.75, 0.1, 0.25, 0.25, 0.25])\n        possible_translation_tokens_ref = sorted([0, 3, 5, 7, 8, 9])\n\n        np.testing.assert_allclose(\n            actual=np.sort(avg_probs[0]), desired=np.array(avg_probs_ref), atol=1e-5\n        )\n        np.testing.assert_allclose(\n            actual=np.sort(possible_translation_tokens),\n            desired=np.array(possible_translation_tokens_ref),\n        )\n        np.testing.assert_allclose(\n            actual=np.sort(possible_translation_tokens),\n            desired=np.array(possible_translation_tokens_ref),\n        )\n\n    def test_gather_probs_without_vr(self):\n        """""" Tests gather_probs when there is no vocab reduction """"""\n        all_probs: List[Any] = [\n            torch.FloatTensor([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]]),\n            torch.FloatTensor([[0.4, 0.2, 0.1, 0.3], [0.4, 0.2, 0.1, 0.3]]),\n        ]\n        all_translation_tokens: List[Any] = [None, None]\n        avg_probs, possible_translation_tokens = beam_decode.SequenceGenerator.gather_probs(\n            all_translation_tokens=all_translation_tokens, all_probs=all_probs\n        )\n\n        assert possible_translation_tokens is None\n        avg_probs_ref = [0.65, 0.45, 0.35, 0.55]\n        np.testing.assert_allclose(\n            actual=avg_probs[0], desired=np.array(avg_probs_ref), atol=1e-5\n        )\n        np.testing.assert_allclose(\n            actual=avg_probs[1], desired=np.array(avg_probs_ref), atol=1e-5\n        )\n\n    def test_smoothed_sentence_bleu(self):\n        """"""\n        Testing calculation of smoothed_sentence_bleu() function.\n        Inputs:\n            target_tokens: [11, 12, 13, 14, 15]\n            hypo_tokens: [11, 12, 14, 15]\n            actual precision:\n                unigram: 4/4 = 1\n                bigram:  2/3 = 0.667\n                trigram: 0/2 = 0\n                4-gram:  0/1 = 0\n            smoothed precision:\n                unigram: 4/4    = 1\n                bigram:  2/3    = 0.667\n                trigram: 0.5/2  = 0.25\n                4-gram:  0.25/1 = 0.25\n            smoothed geom. mean: (1 * 2/3 * 1/4 * 1/4) ^ (1/4) = 0.4518\n            brevity penalty: e ^ (1 - 5/4) = 0.7788\n        Desired Output:\n            0.4518 * 0.7788 = 0.35186\n        """"""\n        test_args = test_utils.ModelParamsDict()\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        target_tokens = torch.IntTensor([11, 12, 13, 14, 15])\n        hypo_tokens = torch.IntTensor([11, 12, 14, 15])\n        smoothed_bleu = generate.smoothed_sentence_bleu(\n            task, target_tokens, hypo_tokens\n        )\n        np.testing.assert_almost_equal(smoothed_bleu, 0.35186, decimal=5)\n\n    def test_diversity_sibling_rank(self):\n        """"""\n        Testing calculation of sibling_rank() function.\n        """"""\n        test_args = test_utils.ModelParamsDict()\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        model = task.build_model(test_args)\n        translator = beam_decode.SequenceGenerator([model], task.target_dictionary)\n        logprobs = torch.FloatTensor(\n            [[[2, 1, 3, 5, 6], [0, 1, 3, 2, 4]], [[2, 3, 1, 5, 0], [3, 1, 5, 2, 0]]]\n        )\n        logprobs_out = torch.FloatTensor(\n            [\n                [[-1, -3, 1, 4, 6], [-4, -2, 2, 0, 4]],\n                [[0, 2, -2, 5, -4], [2, -2, 5, 0, -4]],\n            ]\n        )\n        logprobs = translator.diversity_sibling_rank(logprobs, 1)\n        np.testing.assert_allclose(\n            actual=logprobs_out.view(-1, 5).numpy(), desired=logprobs.numpy(), atol=1e-5\n        )\n'"
pytorch_translate/test/test_beam_search_and_decode.py,20,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate.beam_decode import BeamDecode\nfrom pytorch_translate.beam_search_and_decode_v2 import BeamDecodeWithEOS\n\n\nclass TestBeamSearchAndDecode(unittest.TestCase):\n    def test_get_all_end_states(self):\n        """"""\n        test the function of keeping nBest end_states.\n        In this example, all end_states should be: ([-4.6246, 1, 1](hit eos),\n        [-7.0106, 2, 2](hit eos), [-8.5362, 3, 0](hit maxLen),\n        [-9.4792, 3, 1](hit maxLen), [-10.4673, 3, 2](hit maxLen)),\n        nBest should be the first three.\n        """"""\n        beam_tokens = torch.tensor([[2, 2, 2], [3, 2, 4], [1, 5, 2], [6, 6, 3]])\n        beam_scores = torch.tensor(\n            [\n                [0.0000, 0.0000, 0.0000],\n                [-3.6237, -4.6246, -4.6427],\n                [-6.5691, -6.9823, -7.0106],\n                [-8.5362, -9.4792, -10.4673],\n            ]\n        )\n        beam_prev_indices = torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 2], [1, 0, 1]])\n\n        beam_decode = BeamDecode(\n            eos_token_id=2, length_penalty=0.0, nbest=3, beam_size=3, stop_at_eos=True\n        )\n\n        all_end_states = beam_decode._get_all_end_states(\n            beam_tokens, beam_scores, beam_prev_indices, num_steps=3\n        )\n        desired_end_states_1 = torch.tensor([-4.6246, 1, 1])\n        desired_end_states_2 = torch.tensor([-7.0106, 2, 2])\n        desired_end_states_3 = torch.tensor([-8.5362, 3, 0])\n        np.testing.assert_array_equal(\n            all_end_states[0, :].numpy(), desired_end_states_1.numpy()\n        )\n        np.testing.assert_array_equal(\n            all_end_states[1, :].numpy(), desired_end_states_2.numpy()\n        )\n        np.testing.assert_array_equal(\n            all_end_states[2, :].numpy(), desired_end_states_3.numpy()\n        )\n\n    def test_get_all_end_states_with_same_scores(self):\n        """"""\n        test the function of keeping nBest end_states\n        When two hypotheses have same scores, keep the first one.\n        """"""\n        beam_tokens = torch.tensor([[2, 2, 2, 2], [3, 4, 5, 6]])\n        beam_scores = torch.tensor(\n            [[0.0000, 0.0000, 0.0000, 0.0000], [-4.4000, -4.4000, -4.2000, -4.3000]]\n        )\n        beam_prev_indices = torch.tensor([[0, 0, 0, 0], [0, 0, 0, 0]])\n        beam_decode = BeamDecode(\n            eos_token_id=2, length_penalty=0.0, nbest=3, beam_size=4, stop_at_eos=True\n        )\n\n        all_end_states = beam_decode._get_all_end_states(\n            beam_tokens, beam_scores, beam_prev_indices, num_steps=1\n        )\n\n        desired_end_states_1 = torch.tensor([-4.2000, 1, 2])\n        desired_end_states_2 = torch.tensor([-4.3000, 1, 3])\n        # keep the first hypo [-4.4000, 1, 0] rather than [-4.4000, 1, 1]\n        desired_end_states_3 = torch.tensor([-4.4000, 1, 0])\n        np.testing.assert_array_equal(\n            all_end_states[0, :].numpy(), desired_end_states_1.numpy()\n        )\n        np.testing.assert_array_equal(\n            all_end_states[1, :].numpy(), desired_end_states_2.numpy()\n        )\n        np.testing.assert_array_equal(\n            all_end_states[2, :].numpy(), desired_end_states_3.numpy()\n        )\n\n    def test_beam_decode_with_eos(self):\n        beam_tokens = torch.tensor(\n            [\n                [2, 2, 2, 2],\n                [3, 4, 5, 2],\n                [2, 6, 6, 2],\n                [7, 2, 1, 8],\n                [9, 9, 8, 1],\n                [2, 2, 2, 2],\n            ]\n        )\n        beam_prev_indices = torch.tensor(\n            [\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 1, 0, 1],\n                [1, 1, 2, 2],\n                [0, 2, 0, 2],\n                [0, 1, 2, 3],\n            ]\n        )\n        beam_scores = torch.tensor(\n            [\n                [0.00, 0.00, 0.00, 0.00],\n                [-3.1, -3.2, -3.3, -4.2],\n                [-7.4, -6.3, -6.7, -7.5],\n                [-10.6, -11.7, -10.8, -10.7],\n                [-14.2, -15.5, -14.1, -14.9],\n                [-18.3, -18.1, -18.4, -18.8],\n            ]\n        )\n\n        # test end_states generation\n\n        # since we only consider eos when it\'s among the top beam_size indices,\n        # [-4.2, 1, 3], [-7.5, 2, 3] shouldn\'t be generated.\n        desired_end_states_1 = torch.tensor([-7.4, 2, 0])\n        desired_end_states_2 = torch.tensor([-11.7, 3, 1])\n        desired_end_states_3 = torch.tensor([-18.1, 5, 1])\n        desired_end_states_4 = torch.tensor([-18.3, 5, 0])\n\n        beam_decode_with_eos = BeamDecodeWithEOS(\n            eos_token_id=2, length_penalty=0.0, nbest=10, beam_size=2, stop_at_eos=True\n        )\n        all_end_states = beam_decode_with_eos._get_all_end_states(\n            beam_tokens, beam_scores, beam_prev_indices, num_steps=4\n        )\n\n        np.testing.assert_array_equal(\n            all_end_states[0, :].numpy(), desired_end_states_1.numpy()\n        )\n        np.testing.assert_array_equal(\n            all_end_states[1, :].numpy(), desired_end_states_2.numpy()\n        )\n        np.testing.assert_array_equal(\n            all_end_states[2, :].numpy(), desired_end_states_3.numpy()\n        )\n        np.testing.assert_array_equal(\n            all_end_states[3, :].numpy(), desired_end_states_4.numpy()\n        )\n\n        # test hypotheses generation\n        token_weights = torch.rand((6, 4, 5))\n\n        all_hypotheses = beam_decode_with_eos(\n            beam_tokens, beam_scores, token_weights, beam_prev_indices, num_steps=4\n        )\n\n        np.testing.assert_array_equal([3, 2], all_hypotheses[0][0])\n        np.testing.assert_array_equal([4, 6, 2], all_hypotheses[1][0])\n        np.testing.assert_array_equal([3, 6, 1, 9, 2], all_hypotheses[2][0])\n        np.testing.assert_array_equal([4, 6, 7, 9, 2], all_hypotheses[3][0])\n'"
pytorch_translate/test/test_bleu_significance.py,0,"b'#!/usr/bin/env python3\n\nimport tempfile\nimport unittest\n\nimport sacrebleu\nfrom pytorch_translate import bleu_significance\n\n\nclass BleuSignificanceTest(unittest.TestCase):\n    def _setup_files(self, reference_file, baseline_file, new_file):\n        reference_file.write(\n            r""""""Lorem ipsum dolor sit amet , consectetur adipiscing elit , sed do eiusmod tempor incididunt ut labore et dolore magna aliqua .\nUt enim ad minim veniam , quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat .\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur .\nExcepteur sint occaecat cupidatat non proident , sunt in culpa qui officia deserunt mollit anim id est laborum .\nShort sentence .\nComplete gibberish""""""\n        )\n        reference_file.flush()\n        baseline_file.write(\n            r""""""Lorem ipsum dolor sit AAAA , consectetur adipiscing elit , sed DD eiusmod tempor incididunt ut labore et DDDDDD magna aliqua .\nUt enim ad MMMMM veniam , quis nostrud exercitation ullamco laboris NNNN ut aliquip EE ea commodo consequat .\nDuis aute irure dolor in reprehenderit in voluptate VVVVV EEEE cillum dolore eu FFFFFF nulla pariatur .\nExcepteur sint occaecat cupidatat non proident , SSSS in culpa QQQ officia deserunt mollit AAAA id EEE laborum .\nShort phrase .\nO_o""""""\n        )\n        baseline_file.flush()\n        new_file.write(\n            r""""""Lorem IIIII dolor sit amet , consectetur adipiscing EEEE , sed do eiusmod tempor incididunt UU labore et dolore magna aliqua .\nUt enim ad MMMMM veniam , QQQQ nostrud exercitation ullamco laboris nisi ut aliquip ex EE commodo consequat .\nDDDD aute irure dolor in reprehenderit in voluptate velit esse cillum dolore EE fugiat NNNNN pariatur .\nExcepteur SSSS occaecat cupidatat NNN proident , SSSS in culpa qui officia deserunt mollit anim id EEE laborum .\nShort sentence\nHodor hodor hodor hodor hodor hodor hodor""""""\n        )\n        new_file.flush()\n\n    def test_paired_bootstrap_resample_from_files(self):\n        with tempfile.NamedTemporaryFile(\n            mode=""w+""\n        ) as reference_file, tempfile.NamedTemporaryFile(\n            mode=""w+""\n        ) as baseline_file, tempfile.NamedTemporaryFile(\n            mode=""w+""\n        ) as new_file:\n            self._setup_files(\n                reference_file=reference_file,\n                baseline_file=baseline_file,\n                new_file=new_file,\n            )\n            output = bleu_significance.paired_bootstrap_resample_from_files(\n                reference_file=reference_file.name,\n                baseline_file=baseline_file.name,\n                new_file=new_file.name,\n            )\n\n            # Sanity checks that our logic calculating BLEU score by combining\n            # sufficient statistics is the same as SacreBLEU\'s.\n            reference_file.seek(0)\n            reference_lines = [line for line in reference_file]\n            baseline_file.seek(0)\n            baseline_lines = [line for line in baseline_file]\n            new_file.seek(0)\n            new_lines = [line for line in new_file]\n\n            self.assertEqual(\n                output.baseline_bleu,\n                sacrebleu.corpus_bleu(\n                    sys_stream=baseline_lines,\n                    ref_streams=[reference_lines],\n                    lowercase=False,\n                    tokenize=""none"",\n                    use_effective_order=False,\n                ),\n            )\n            self.assertEqual(\n                output.new_bleu,\n                sacrebleu.corpus_bleu(\n                    sys_stream=new_lines,\n                    ref_streams=[reference_lines],\n                    lowercase=False,\n                    tokenize=""none"",\n                    use_effective_order=False,\n                ),\n            )\n'"
pytorch_translate/test/test_char_aware_hybrid.py,32,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom pytorch_translate import char_aware_hybrid, transformer\nfrom pytorch_translate.data.dictionary import Dictionary\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\nfrom pytorch_translate.utils import maybe_cuda\n\n\nclass TestCharAwareHybrid(unittest.TestCase):\n    def setUp(self):\n        self.src_txt, self.trg_txt = test_utils.create_test_text_files()\n        self.vocab_file_path = test_utils.make_temp_file()\n        self.word_dict = Dictionary.build_vocab_file(\n            corpus_files=[self.src_txt, self.trg_txt],\n            vocab_file=self.vocab_file_path,\n            max_vocab_size=0,\n            padding_factor=1,  # don\'t add extra padding symbols\n        )\n        self.char_dict = Dictionary.build_vocab_file(\n            corpus_files=[self.src_txt, self.trg_txt],\n            vocab_file=self.vocab_file_path,\n            max_vocab_size=0,\n            is_char_vocab=True,\n            padding_factor=1,  # don\'t add extra padding symbols\n        )\n        self.sample = self._dummy_char_data_sample(\n            src_dict=self.word_dict,\n            dst_dict=self.word_dict,\n            src_char_dict=self.char_dict,\n            dst_char_dict=self.char_dict,\n        )\n\n    def assertTensorEqual(self, x, y):\n        np.testing.assert_almost_equal(\n            x.detach().numpy(), y.detach().numpy(), decimal=6\n        )\n\n    def test_forward_training(self):\n        """"""\n        We test that if we shuffle the input sample, we will get the same\n        forward values, both in training mode (without dropout) and in\n        eval mode.\n        For the meanwhile, we use an auxiliary hybrid_transformer_rnn\n        in order to get the encoder output.\n        """"""\n        test_word_decoder_args = test_utils.ModelParamsDict(\n            arch=""hybrid_transformer_rnn""\n        )\n        self.task = tasks.DictionaryHolderTask(self.word_dict, self.word_dict)\n        word_model = maybe_cuda(self.task.build_model(test_word_decoder_args))\n        word_model.eval()  # Make sure we do not apply dropout.\n\n        test_args = test_utils.ModelParamsDict(arch=""char_aware_hybrid"")\n\n        decoder_embed_tokens = maybe_cuda(\n            transformer.build_embedding(dictionary=self.word_dict, embed_dim=10)\n        )\n        decoder = maybe_cuda(\n            char_aware_hybrid.CharAwareHybridRNNDecoder(\n                args=test_args,\n                src_dict=self.word_dict,\n                dst_dict=self.word_dict,\n                embed_tokens=decoder_embed_tokens,\n                num_chars=len(self.char_dict),\n            )\n        )\n\n        src_tokens = maybe_cuda(self.sample[""net_input""][""src_tokens""])\n        src_lengths = maybe_cuda(self.sample[""net_input""][""src_lengths""])\n        prev_output_chars = maybe_cuda(\n            self.sample[""net_input""][""prev_output_chars""][:, -1:, :].squeeze(1)\n        )\n        prev_output_tokens = maybe_cuda(\n            self.sample[""net_input""][""prev_output_tokens""][:, 0:1]\n        )\n\n        encoder_out = word_model.encoder(src_tokens, src_lengths)\n\n        embed_output = decoder._embed_prev_outputs(\n            prev_output_tokens=prev_output_tokens, prev_output_chars=prev_output_chars\n        )[0]\n        forward_output = decoder(\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out,\n            prev_output_chars=prev_output_chars,\n        )\n        output_logits = forward_output[0]\n\n        prev_output_tokens_shuffled = torch.cat(\n            [prev_output_tokens[1:], prev_output_tokens[0].unsqueeze(0)], dim=0\n        )\n        prev_output_chars_shuffled = torch.cat(\n            [prev_output_chars[1:], prev_output_chars[0].unsqueeze(0)], dim=0\n        )\n        src_tokens_shuffled = torch.cat(\n            [src_tokens[1:], src_tokens[0].unsqueeze(0)], dim=0\n        )\n\n        # Making sure shuffling is done correctly.\n        assert torch.equal(src_tokens[0], src_tokens_shuffled[2])\n        assert torch.equal(src_tokens[1], src_tokens_shuffled[0])\n        assert torch.equal(src_tokens[2], src_tokens_shuffled[1])\n        assert torch.equal(prev_output_chars[0], prev_output_chars_shuffled[2])\n        assert torch.equal(prev_output_chars[1], prev_output_chars_shuffled[0])\n        assert torch.equal(prev_output_chars[2], prev_output_chars_shuffled[1])\n        assert torch.equal(prev_output_tokens[0], prev_output_tokens_shuffled[2])\n        assert torch.equal(prev_output_tokens[1], prev_output_tokens_shuffled[0])\n        assert torch.equal(prev_output_tokens[2], prev_output_tokens_shuffled[1])\n\n        # Making sure that we embed the inputs correctly.\n        encoder_out_shuffled = word_model.encoder(src_tokens_shuffled, src_lengths)\n        embed_output_shuffled = decoder._embed_prev_outputs(\n            prev_output_tokens=prev_output_tokens_shuffled,\n            prev_output_chars=prev_output_chars_shuffled,\n        )[0]\n        self.assertTensorEqual(embed_output[0, 0], embed_output_shuffled[0, 2])\n        self.assertTensorEqual(embed_output[0, 1], embed_output_shuffled[0, 0])\n        self.assertTensorEqual(embed_output[0, 2], embed_output_shuffled[0, 1])\n\n        # Making sure the output of the forward function is correct.\n        forward_output_shuffled = decoder(\n            prev_output_tokens=prev_output_tokens_shuffled,\n            encoder_out=encoder_out_shuffled,\n            prev_output_chars=prev_output_chars_shuffled,\n        )\n        output_logits_shuffled = forward_output_shuffled[0]\n\n        self.assertTensorEqual(\n            encoder_out[0][:, 0, :], encoder_out_shuffled[0][:, 2, :]\n        )\n        self.assertTensorEqual(\n            encoder_out[0][:, 1, :], encoder_out_shuffled[0][:, 0, :]\n        )\n        self.assertTensorEqual(\n            encoder_out[0][:, 2, :], encoder_out_shuffled[0][:, 1, :]\n        )\n\n        self.assertTensorEqual(output_logits[0], output_logits_shuffled[2])\n        self.assertTensorEqual(output_logits[1], output_logits_shuffled[0])\n        self.assertTensorEqual(output_logits[2], output_logits_shuffled[1])\n\n        """"""\n        Now trying in the eval mode.\n        """"""\n        decoder.eval()\n        forward_output = decoder(\n            prev_output_tokens=prev_output_tokens,\n            encoder_out=encoder_out,\n            prev_output_chars=prev_output_chars,\n        )\n        output_logits = forward_output[0]\n        forward_output_shuffled = decoder(\n            prev_output_tokens=prev_output_tokens_shuffled,\n            encoder_out=encoder_out_shuffled,\n            prev_output_chars=prev_output_chars_shuffled,\n        )\n        output_logits_shuffled = forward_output_shuffled[0]\n        self.assertTensorEqual(output_logits[0], output_logits_shuffled[2])\n        self.assertTensorEqual(output_logits[1], output_logits_shuffled[0])\n        self.assertTensorEqual(output_logits[2], output_logits_shuffled[1])\n\n    def test_precompute(self):\n        """"""\n        We test that if we shuffle the input sample, we will get the same\n        forward values, both in training mode (without dropout) and in\n        eval mode.\n        For the meanwhile, we use an auxiliary hybrid_transformer_rnn\n        in order to get the encoder output.\n        """"""\n        test_args = test_utils.ModelParamsDict(arch=""char_aware_hybrid"")\n\n        decoder_embed_tokens = maybe_cuda(\n            transformer.build_embedding(dictionary=self.word_dict, embed_dim=10)\n        )\n\n        decoder = maybe_cuda(\n            char_aware_hybrid.CharAwareHybridRNNDecoder(\n                args=test_args,\n                src_dict=self.word_dict,\n                dst_dict=self.word_dict,\n                embed_tokens=decoder_embed_tokens,\n                num_chars=len(self.char_dict),\n            )\n        )\n        # Making sure we do not apply dropout.\n        decoder.eval()\n        decoder.precompute_char_representations(\n            char_dict=self.char_dict, embed_bytes=False, batch_size=5\n        )\n\n        first_embedding = decoder.combined_word_char_embed.weight.clone()\n        first_embedding.detach()\n        decoder.precompute_char_representations(\n            char_dict=self.char_dict, embed_bytes=False, batch_size=11\n        )\n        second_embedding = decoder.combined_word_char_embed.weight.clone()\n        second_embedding.detach()\n\n        # Due to a known issue in the char_encoder model, this does not hold for\n        # torch.equal (T53048656)\n        assert torch.allclose(first_embedding, second_embedding, rtol=1e-04, atol=1e-04)\n\n        decoder.precompute_char_representations(\n            char_dict=self.char_dict, embed_bytes=False, batch_size=23\n        )\n        third_embedding = decoder.combined_word_char_embed.weight.clone()\n        third_embedding.detach()\n        # Due to a known issue in the char_encoder model, this does not hold for\n        # torch.equal (T53048656)\n        assert torch.allclose(second_embedding, third_embedding, rtol=1e-04, atol=1e-04)\n\n    def test_forward_training_precompute(self):\n        """"""\n        We test if precomputation gives the same result.\n        """"""\n        test_args = test_utils.ModelParamsDict(arch=""char_aware_hybrid"")\n\n        decoder_embed_tokens = transformer.build_embedding(\n            dictionary=self.word_dict, embed_dim=10\n        )\n        decoder = maybe_cuda(\n            char_aware_hybrid.CharAwareHybridRNNDecoder(\n                args=test_args,\n                src_dict=self.word_dict,\n                dst_dict=self.word_dict,\n                embed_tokens=decoder_embed_tokens,\n                num_chars=len(self.char_dict),\n            )\n        )\n        decoder.eval()\n        prev_output_word_strs = self.word_dict.symbols[-3:]\n        prev_out_word_indices = [\n            self.word_dict.indices[w] for w in prev_output_word_strs\n        ]\n        prev_output_tokens = maybe_cuda(\n            torch.LongTensor(prev_out_word_indices).unsqueeze(1)\n        )\n\n        prev_output_chars = maybe_cuda(\n            torch.LongTensor(\n                [\n                    decoder._char_list_for_word(\n                        word_index=word_index, word=word, char_dict=self.char_dict\n                    )\n                    for word_index, word in zip(\n                        prev_out_word_indices, prev_output_word_strs\n                    )\n                ]\n            )\n        )\n\n        embed_output = maybe_cuda(\n            decoder._embed_prev_outputs(\n                prev_output_tokens=prev_output_tokens,\n                prev_output_chars=prev_output_chars,\n            )[0]\n        )\n\n        decoder.precompute_char_representations(\n            char_dict=self.char_dict, embed_bytes=False, batch_size=30\n        )\n        embed_output_after_precompute = decoder._embed_prev_outputs(\n            prev_output_tokens=prev_output_tokens, prev_output_chars=prev_output_chars\n        )[0]\n        # Due to a known issue in padding, we know that the results are not exactly\n        # the same.\n        assert torch.allclose(\n            embed_output, embed_output_after_precompute, rtol=1e-04, atol=1e-04\n        )\n\n    def _dummy_char_data_sample(\n        self,\n        src_dict: Dictionary,\n        dst_dict: Dictionary,\n        src_char_dict: Dictionary,\n        dst_char_dict: Dictionary,\n        batch_size: int = 3,\n        input_seq_length: int = 5,\n        output_seq_length: int = 4,\n        max_word_length: int = 7,\n    ):\n        src_tokens = torch.randint(\n            low=src_dict.nspecial,\n            high=len(src_dict),\n            size=(batch_size, input_seq_length),\n        ).long()\n\n        output_sequence = torch.randint(\n            low=dst_dict.nspecial,\n            high=len(dst_dict),\n            size=(batch_size, output_seq_length),\n        ).long()\n        eos_column = torch.LongTensor([dst_dict.eos_index] * batch_size).unsqueeze(1)\n        prev_output_tokens = torch.cat([eos_column, output_sequence], dim=1)\n        target_tokens = torch.cat([output_sequence, eos_column], dim=1)\n\n        src_char_inds = torch.randint(\n            low=src_char_dict.nspecial,\n            high=len(src_char_dict),\n            size=(batch_size, output_seq_length, max_word_length),\n        ).long()\n\n        eos_char_column = (\n            torch.LongTensor(batch_size, output_seq_length)\n            .fill_(dst_char_dict.eos_index)\n            .unsqueeze(2)\n        )\n\n        tgt_char_inds = torch.randint(\n            low=dst_char_dict.nspecial,\n            high=len(dst_char_dict),\n            size=(batch_size, output_seq_length, max_word_length),\n        ).long()\n        prev_output_chars = torch.cat([eos_char_column, tgt_char_inds], dim=2)\n        word_lengths = torch.LongTensor(batch_size, input_seq_length).fill_(\n            max_word_length\n        )\n        prev_tgt_word_lengths = torch.cat(\n            (torch.ones((batch_size, 1), dtype=torch.long), word_lengths), dim=1\n        )\n\n        sample = {\n            ""net_input"": {\n                ""src_tokens"": src_tokens,\n                ""prev_output_tokens"": prev_output_tokens,\n                ""src_lengths"": torch.LongTensor([input_seq_length] * batch_size),\n                ""char_inds"": src_char_inds,\n                ""word_lengths"": word_lengths,\n                ""prev_output_chars"": prev_output_chars,\n                ""prev_output_word_lengths"": prev_tgt_word_lengths,\n            },\n            ""target"": target_tokens,\n            ""ntokens"": target_tokens.numel(),\n            ""target_char_inds"": tgt_char_inds,\n            ""tgt_word_lengths"": word_lengths,\n            ""weights"": None,\n        }\n        return sample\n'"
pytorch_translate/test/test_checkpoint.py,17,"b'#!/usr/bin/env python3\n\nimport copy\nimport itertools\nimport os\nimport tempfile\nimport unittest\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch\nfrom pytorch_translate import checkpoint\nfrom pytorch_translate.checkpoint import CheckpointManager\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestCheckpoint(unittest.TestCase):\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""No GPU available for test."")\n    def test_load_checkpoint(self):\n        """"""Train for one step, save a checkpoint, and make sure it is loaded\n        properly.""""""\n        test_save_file = test_utils.make_temp_file()\n        test_args = test_utils.ModelParamsDict()\n        test_args.distributed_rank = 0\n        extra_state = test_utils.create_dummy_extra_state(epoch=2)\n        trainer, _ = test_utils.gpu_train_step(test_args)\n        trainer.save_checkpoint(test_save_file, extra_state)\n        loaded, extra_state = checkpoint.load_existing_checkpoint(\n            test_save_file, trainer, restore_state=True\n        )\n        # Loading checkpoint without restore state should reset extra state\n        assert loaded and extra_state[""epoch""] == 2\n        os.remove(test_save_file)\n\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""No GPU available for test."")\n    def test_load_checkpoint_no_restore_state(self):\n        """"""Train for one step, save a checkpoint, and make sure it is loaded\n        properly WITHOUT loading the extra state from the checkpoint.""""""\n        test_save_file = test_utils.make_temp_file()\n        test_args = test_utils.ModelParamsDict()\n        test_args.distributed_rank = 0\n        extra_state = test_utils.create_dummy_extra_state(epoch=2)\n        trainer, _ = test_utils.gpu_train_step(test_args)\n        trainer.save_checkpoint(test_save_file, extra_state)\n        loaded, extra_state = checkpoint.load_existing_checkpoint(\n            test_save_file, trainer, restore_state=False\n        )\n        # Loading checkpoint without restore state should reset extra state\n        assert loaded and extra_state is None\n        os.remove(test_save_file)\n\n\nclass CheckpointManagerTest(unittest.TestCase):\n    def setUp(self):\n        self._params_1 = OrderedDict(\n            [\n                (""double_tensor"", torch.DoubleTensor([100.0])),\n                (""float_tensor"", torch.FloatTensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])),\n                (""long_tensor"", torch.LongTensor([7, 8, 9])),\n                (""half_tensor"", torch.HalfTensor([10.0, 20.0])),\n            ]\n        )\n        self._params_2 = OrderedDict(\n            [\n                (""double_tensor"", torch.DoubleTensor([1.0])),\n                (""float_tensor"", torch.FloatTensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]])),\n                # Any integer tensor must remain the same in all the params.\n                (""long_tensor"", torch.LongTensor([7, 8, 9])),\n                (""half_tensor"", torch.HalfTensor([50.0, 0.0])),\n            ]\n        )\n        self._params_avg = OrderedDict(\n            [\n                (""double_tensor"", torch.DoubleTensor([50.5])),\n                (""float_tensor"", torch.FloatTensor([[1.0, 1.5, 2.0], [2.5, 3.0, 3.5]])),\n                (""long_tensor"", torch.LongTensor([7, 8, 9])),\n                # We convert fp16 to fp32 when averaging params.\n                (""half_tensor"", torch.FloatTensor([30.0, 10.0])),\n            ]\n        )\n\n        self._fd_1, self._filename_1 = tempfile.mkstemp()\n        self._fd_2, self._filename_2 = tempfile.mkstemp()\n        torch.save(OrderedDict([(""model"", self._params_1)]), self._filename_1)\n        torch.save(OrderedDict([(""model"", self._params_2)]), self._filename_2)\n\n    def tearDown(self):\n        os.close(self._fd_1)\n        os.remove(self._filename_1)\n        os.close(self._fd_2)\n        os.remove(self._filename_2)\n\n    def _check_params(self, expected_params: OrderedDict, actual_params: OrderedDict):\n        for (k_expected, v_expected), (k_actual, v_actual) in itertools.zip_longest(\n            expected_params.items(), actual_params.items()\n        ):\n            self.assertEqual(\n                k_expected,\n                k_actual,\n                (\n                    f""Key mismatch - expected {k_expected} but found {k_actual}. ""\n                    f""(Expected list of keys: {expected_params.keys()} ""\n                    f""vs actual list of keys: {actual_params.keys()})""\n                ),\n            )\n            np.testing.assert_allclose(\n                v_expected.numpy(),\n                v_actual.numpy(),\n                err_msg=f""Tensor value mismatch for key {k_expected}"",\n            )\n\n    def test_initialize(self):\n        checkpoint_manager = CheckpointManager(\n            num_avg_checkpoints=2,\n            auto_clear_checkpoints=False,\n            log_verbose=False,\n            checkpoint_files=[self._filename_1, self._filename_2],\n        )\n        checkpoint_manager._initialize()\n        self._check_params(\n            expected_params=self._params_avg,\n            actual_params=checkpoint_manager._averaged_params,\n        )\n\n        # Tests providing 3 old checkpoint files while only using\n        # num_avg_checkpoints=2. The oldest one should not be included in the\n        # average.\n        checkpoint_manager = CheckpointManager(\n            num_avg_checkpoints=2,\n            auto_clear_checkpoints=False,\n            log_verbose=False,\n            checkpoint_files=[self._filename_1, self._filename_1, self._filename_2],\n        )\n        checkpoint_manager._initialize()\n        self._check_params(\n            expected_params=self._params_avg,\n            actual_params=checkpoint_manager._averaged_params,\n        )\n\n    def test_get_averaged_params(self):\n        checkpoint_manager = CheckpointManager(\n            num_avg_checkpoints=2,\n            auto_clear_checkpoints=True,\n            log_verbose=False,\n            checkpoint_files=[],\n        )\n\n        # The first average should be the same as the first params.\n        avg_params = checkpoint_manager.get_averaged_params(new_params=self._params_1)\n        self._check_params(expected_params=self._params_1, actual_params=avg_params)\n        checkpoint_to_remove = checkpoint_manager._update_state(\n            new_params_filename=self._filename_1, new_averaged_params=avg_params\n        )\n        self.assertEqual(None, checkpoint_to_remove)\n        self._check_params(\n            expected_params=self._params_1,\n            actual_params=checkpoint_manager._averaged_params,\n        )\n\n        # Adding the second params should produce the expected average.\n        avg_params = checkpoint_manager.get_averaged_params(new_params=self._params_2)\n        self._check_params(expected_params=self._params_avg, actual_params=avg_params)\n        checkpoint_to_remove = checkpoint_manager._update_state(\n            new_params_filename=self._filename_2, new_averaged_params=avg_params\n        )\n        self.assertEqual(None, checkpoint_to_remove)\n        self._check_params(\n            expected_params=self._params_avg,\n            actual_params=checkpoint_manager._averaged_params,\n        )\n\n        # Adding the second params again should produce an average that\'s the\n        # same as the second params.\n        avg_params = checkpoint_manager.get_averaged_params(new_params=self._params_2)\n        self._check_params(expected_params=self._params_2, actual_params=avg_params)\n        checkpoint_to_remove = checkpoint_manager._update_state(\n            new_params_filename=self._filename_2, new_averaged_params=avg_params\n        )\n        # This should kick out the first params file.\n        self.assertEqual(self._filename_1, checkpoint_to_remove)\n        self._check_params(\n            expected_params=self._params_2,\n            actual_params=checkpoint_manager._averaged_params,\n        )\n\n    def test_integer_tensor_change_error(self):\n        params_invalid = copy.deepcopy(self._params_1)\n        # An integer tensor is expected to remain constant and should not change\n        # its value.\n        params_invalid[""long_tensor""] += 1\n        fd_invalid, filename_invalid = tempfile.mkstemp()\n        torch.save(OrderedDict([(""model"", params_invalid)]), filename_invalid)\n\n        # Tests trying to initialize with an invalid param.\n        with self.assertRaises(ValueError):\n            checkpoint_manager = CheckpointManager(\n                num_avg_checkpoints=2,\n                auto_clear_checkpoints=False,\n                log_verbose=False,\n                checkpoint_files=[self._filename_1, filename_invalid],\n            )\n            checkpoint_manager._initialize()\n\n        # Tests trying to get the average with an invalid new param when the\n        # queue isn\'t full.\n        checkpoint_manager = CheckpointManager(\n            num_avg_checkpoints=2,\n            auto_clear_checkpoints=False,\n            log_verbose=False,\n            checkpoint_files=[self._filename_1],\n        )\n        with self.assertRaises(ValueError):\n            checkpoint_manager.get_averaged_params(new_params=params_invalid)\n\n        # Test trying to get the average with an invalid new param when the\n        # queue is full.\n        checkpoint_manager = CheckpointManager(\n            num_avg_checkpoints=2,\n            auto_clear_checkpoints=False,\n            log_verbose=False,\n            checkpoint_files=[self._filename_1, self._filename_2],\n        )\n        with self.assertRaises(ValueError):\n            checkpoint_manager.get_averaged_params(new_params=params_invalid)\n\n        os.close(fd_invalid)\n        os.remove(filename_invalid)\n'"
pytorch_translate/test/test_data.py,9,"b'#!/usr/bin/env python3\n\nimport os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport torch\nfrom fairseq.data import LanguagePairDataset, NoisingDataset\nfrom fairseq.data.concat_dataset import ConcatDataset\nfrom fairseq.data.noising import UnsupervisedMTNoising\nfrom fairseq_cli.preprocess import (\n    binarize,\n    dataset_dest_prefix,\n    options as preprocess_options,\n)\nfrom pytorch_translate import preprocess\nfrom pytorch_translate.data import char_data, data, dictionary\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestLoadData(unittest.TestCase):\n    """"""\n    This function tests loading from .npz data.\n    """"""\n\n    def test_load_data_single_path(self):\n        test_args = test_utils.ModelParamsDict()\n        test_args.source_lang = ""en""\n        test_args.target_lang = ""fr""\n        test_args.log_verbose = False\n        src_dict, tgt_dict = test_utils.create_vocab_dictionaries()\n        src_text_file, tgt_text_file = test_utils.create_test_text_files()\n        src_bin_path = preprocess.binarize_text_file(\n            text_file=src_text_file,\n            dictionary=src_dict,\n            output_path=tempfile.NamedTemporaryFile().name,\n            append_eos=True,\n            reverse_order=False,\n        )\n        tgt_bin_path = preprocess.binarize_text_file(\n            text_file=tgt_text_file,\n            dictionary=tgt_dict,\n            output_path=tempfile.NamedTemporaryFile().name,\n            append_eos=True,\n            reverse_order=False,\n        )\n        task = tasks.PytorchTranslateTask(test_args, src_dict, tgt_dict)\n        split = ""0""\n        task.load_dataset(split, src_bin_path, tgt_bin_path)\n        self.assertEqual(len(task.datasets[split]), 4)\n        self.assertIsInstance(task.datasets[split], LanguagePairDataset)\n\n    """"""\n    This function tests loading from .idx + .bin data for fairseq compatibility.\n    """"""\n\n    def test_load_data_single_path_idx_bin(self):\n        test_args = test_utils.ModelParamsDict()\n        test_args.source_lang = ""en""\n        test_args.target_lang = ""fr""\n        test_args.log_verbose = False\n        src_dict, tgt_dict = test_utils.create_vocab_dictionaries()\n        src_text_file, tgt_text_file = test_utils.create_test_text_files()\n        task = tasks.PytorchTranslateTask(test_args, src_dict, tgt_dict)\n        with tempfile.TemporaryDirectory() as destdir:\n            preprocess_args = [\n                ""--source-lang"",\n                test_args.source_lang,\n                ""--target-lang"",\n                test_args.target_lang,\n                ""--destdir"",\n                destdir,\n            ]\n            preproc_parser = preprocess_options.get_preprocessing_parser()\n            preproc_args = preproc_parser.parse_args(preprocess_args)\n            preproc_args.dataset_impl = ""mmap""\n            split = ""train""\n            binarize(\n                preproc_args,\n                src_text_file,\n                src_dict,\n                split,\n                test_args.source_lang,\n                offset=0,\n                end=-1,\n            )\n            binarize(\n                preproc_args,\n                tgt_text_file,\n                tgt_dict,\n                split,\n                test_args.target_lang,\n                offset=0,\n                end=-1,\n            )\n            src_path = dataset_dest_prefix(preproc_args, split, test_args.source_lang)\n            tgt_path = dataset_dest_prefix(preproc_args, split, test_args.target_lang)\n            task.load_dataset(split, src_path, tgt_path, is_npz=False)\n            self.assertEqual(len(task.datasets[split]), 4)\n            self.assertIsInstance(task.datasets[split], LanguagePairDataset)\n\n    def _prepare_data_multi_path(self, num_paths):\n        test_args = test_utils.ModelParamsDict()\n        test_args.source_lang = ""en""\n        test_args.target_lang = ""fr""\n        test_args.log_verbose = False\n        test_args.dataset_upsampling = None\n        test_args.dataset_relative_ratio = None\n        src_dict, tgt_dict = test_utils.create_vocab_dictionaries()\n        src_bin_path, tgt_bin_path = {}, {}\n        for i in range(num_paths):\n            src_text_file, tgt_text_file = test_utils.create_test_text_files()\n            src_bin_path[i] = preprocess.binarize_text_file(\n                text_file=src_text_file,\n                dictionary=src_dict,\n                output_path=tempfile.NamedTemporaryFile().name,\n                append_eos=True,\n                reverse_order=False,\n            )\n            tgt_bin_path[i] = preprocess.binarize_text_file(\n                text_file=tgt_text_file,\n                dictionary=tgt_dict,\n                output_path=tempfile.NamedTemporaryFile().name,\n                append_eos=True,\n                reverse_order=False,\n            )\n        return test_args, src_dict, tgt_dict, src_bin_path, tgt_bin_path\n\n    def test_load_data_multi_path(self):\n        num_paths = 4\n        test_args, src_dict, tgt_dict, src_bin_path, tgt_bin_path = self._prepare_data_multi_path(\n            num_paths\n        )\n        task = tasks.PytorchTranslateTask(test_args, src_dict, tgt_dict)\n        split = ""1""\n        task.load_dataset(split, src_bin_path, tgt_bin_path)\n        self.assertEqual(len(task.datasets[split]), 16)\n        self.assertIsInstance(task.datasets[split], ConcatDataset)\n\n    def test_load_data_noising(self):\n        num_paths = 4\n        test_args, src_dict, tgt_dict, src_bin_path, tgt_bin_path = self._prepare_data_multi_path(\n            num_paths\n        )\n        test_args.word_dropout_prob_map = str({""en-fr"": {0: 0.1}})\n        task = tasks.PytorchTranslateTask(test_args, src_dict, tgt_dict)\n        split = ""1""\n        task.load_dataset(split, src_bin_path, tgt_bin_path)\n        self.assertEqual(len(task.datasets[split]), 16)\n        self.assertIsInstance(task.datasets[split].datasets[0].src, NoisingDataset)\n\n\nclass TestInMemoryIndexedDataset(unittest.TestCase):\n    def setUp(self):\n        self.src_txt, self.trg_txt = test_utils.create_test_text_files()\n        self.vocab_file_path = test_utils.make_temp_file()\n        self.d = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[self.src_txt, self.trg_txt],\n            vocab_file=self.vocab_file_path,\n            max_vocab_size=0,\n            padding_factor=1,  # don\'t add extra padding symbols\n        )\n        self.char_dict = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[self.src_txt, self.trg_txt],\n            vocab_file=self.vocab_file_path,\n            max_vocab_size=0,\n            is_char_vocab=True,\n            padding_factor=1,  # don\'t add extra padding symbols\n        )\n        # src_ref is reversed\n        self.src_ref = [\n            [106, 104, 102, 100],\n            [104, 104, 102, 102, 100, 100],\n            [102, 102, 102, 102, 100, 100, 100, 100],\n            [100, 100, 100, 100, 100, 100, 100, 100, 100, 100],\n        ]\n        self.trg_ref = [\n            [101, 101, 101, 101, 101, 101, 101, 101, 101, 101],\n            [101, 101, 101, 101, 103, 103, 103, 103],\n            [101, 101, 103, 103, 105, 105],\n            [101, 103, 105, 107],\n        ]\n        self.src_txt_numberized, self.trg_txt_numberized = test_utils.create_test_numberized_data_files(\n            self.src_ref, self.trg_ref, reverse_source=True\n        )\n        self.num_sentences = 4\n\n    def tearDown(self):\n        os.remove(self.src_txt)\n        os.remove(self.trg_txt)\n        os.remove(self.vocab_file_path)\n\n    def test_parse(self):\n        src_dataset = data.InMemoryIndexedDataset()\n        trg_dataset = data.InMemoryIndexedDataset()\n        for _ in range(2):\n            src_dataset.parse(\n                self.src_txt, self.d, reverse_order=True, append_eos=False\n            )\n            trg_dataset.parse(\n                self.trg_txt, self.d, reverse_order=False, append_eos=True\n            )\n            self.assertEqual(self.num_sentences, len(src_dataset))\n            self.assertEqual(self.num_sentences, len(trg_dataset))\n            for i in range(self.num_sentences):\n                self.assertListEqual(self.src_ref[i], src_dataset[i].tolist())\n                self.assertListEqual(\n                    self.trg_ref[i] + [self.d.eos_index], trg_dataset[i].tolist()\n                )\n\n    def test_parse_numberize(self):\n        src_dataset = data.InMemoryIndexedDataset()\n        trg_dataset = data.InMemoryIndexedDataset()\n        for _ in range(2):\n            src_dataset.parse(\n                self.src_txt_numberized,\n                self.d,\n                reverse_order=True,\n                append_eos=False,\n                already_numberized=True,\n            )\n            trg_dataset.parse(\n                self.trg_txt_numberized,\n                self.d,\n                reverse_order=False,\n                append_eos=True,\n                already_numberized=True,\n            )\n            self.assertEqual(self.num_sentences, len(src_dataset))\n            self.assertEqual(self.num_sentences, len(trg_dataset))\n            for i in range(self.num_sentences):\n                self.assertListEqual(self.src_ref[i], src_dataset[i].tolist())\n                self.assertListEqual(\n                    self.trg_ref[i] + [self.d.eos_index], trg_dataset[i].tolist()\n                )\n\n    def test_parse_oversampling(self):\n        dataset = data.InMemoryIndexedDataset()\n        factors = [(1, 0), (3, 2), (4, 4)]\n        for o1, o2 in factors:\n            corpora = [\n                data.MultilingualCorpusConfig(\n                    dialect_id=None,\n                    data_file=self.trg_txt,\n                    dict=self.d,\n                    oversampling=o1,\n                ),\n                data.MultilingualCorpusConfig(\n                    dialect_id=None,\n                    data_file=self.trg_txt,\n                    dict=self.d,\n                    oversampling=o2,\n                ),\n            ]\n            dataset.parse_multilingual(corpora)\n            self.assertEqual((o1 + o2) * self.num_sentences, len(dataset))\n\n    def test_parse_multiling(self):\n        prepend_dataset = data.InMemoryIndexedDataset()\n        append_dataset = data.InMemoryIndexedDataset()\n        corpora = [\n            data.MultilingualCorpusConfig(\n                dialect_id=10, data_file=self.trg_txt, dict=self.d, oversampling=1\n            ),\n            data.MultilingualCorpusConfig(\n                dialect_id=11, data_file=self.trg_txt, dict=self.d, oversampling=1\n            ),\n        ]\n        lang1 = corpora[0].dialect_id\n        lang2 = corpora[1].dialect_id\n        prepend_dataset.parse_multilingual(\n            corpora, reverse_order=False, append_eos=False, prepend_language_id=True\n        )\n        append_dataset.parse_multilingual(\n            corpora, reverse_order=False, append_eos=False, prepend_language_id=False\n        )\n        self.assertEqual(2 * self.num_sentences, len(prepend_dataset))\n        self.assertEqual(2 * self.num_sentences, len(append_dataset))\n        for i in range(self.num_sentences):\n            self.assertListEqual([lang1] + self.trg_ref[i], prepend_dataset[i].tolist())\n            self.assertListEqual(self.trg_ref[i] + [lang1], append_dataset[i].tolist())\n            self.assertListEqual(\n                [lang2] + self.trg_ref[i],\n                prepend_dataset[i + self.num_sentences].tolist(),\n            )\n            self.assertListEqual(\n                self.trg_ref[i] + [lang2],\n                append_dataset[i + self.num_sentences].tolist(),\n            )\n\n    def test_subsample_pair_dataset(self):\n        src_dataset = data.InMemoryIndexedDataset()\n        trg_dataset = data.InMemoryIndexedDataset()\n        for _ in range(5):\n            src_dataset.parse(\n                self.src_txt, self.d, reverse_order=True, append_eos=False\n            )\n            trg_dataset.parse(\n                self.trg_txt, self.d, reverse_order=False, append_eos=True\n            )\n\n        pair_dataset = LanguagePairDataset(\n            src=src_dataset,\n            src_sizes=src_dataset.sizes,\n            src_dict=self.d,\n            tgt=trg_dataset,\n            tgt_sizes=trg_dataset.sizes,\n            tgt_dict=self.d,\n            left_pad_source=False,\n        )\n\n        data.subsample_pair_dataset(pair_dataset, 2)\n        self.assertEqual(len(pair_dataset.src), 2)\n        self.assertEqual(pair_dataset.src_sizes.size, 2)\n        self.assertEqual(len(pair_dataset.tgt), 2)\n        self.assertEqual(pair_dataset.tgt_sizes.size, 2)\n\n    def test_subsample_dataset(self):\n        """"""\n        Test the InMemoryIndexedDataset.subsample() method, ensuring that the\n        examples produced by the dataset are correctly permuted according to\n        the indices argument.\n        """"""\n        trg_dataset = data.InMemoryIndexedDataset()\n\n        trg_dataset.parse(self.trg_txt, self.d, reverse_order=False, append_eos=True)\n\n        indices = np.random.permutation(len(trg_dataset))[:2]\n        token_samples = [trg_dataset[i] for i in indices]\n        trg_dataset.subsample(indices)\n        for i in range(2):\n            assert all(trg_dataset[i].numpy() == token_samples[i].numpy())\n\n    def test_subsample_char_dataset(self):\n        """"""\n        Test the InMemoryNumpyWordCharDataset.subsample() method, ensuring that\n        the examples produced by the dataset are correctly permuted according to\n        the indices argument.\n        """"""\n        src_dataset = char_data.InMemoryNumpyWordCharDataset()\n        src_dataset.parse(\n            self.src_txt, self.d, self.char_dict, reverse_order=True, append_eos=False\n        )\n\n        indices = np.random.permutation(len(src_dataset))[:2]\n        token_samples = [src_dataset.get_tokens(i) for i in indices]\n        char_samples = [src_dataset.get_chars_list(i) for i in indices]\n        src_dataset.subsample(indices)\n        for i in range(2):\n            assert all(src_dataset.get_tokens(i).numpy() == token_samples[i].numpy())\n            orig_chars_list = char_samples[i]\n            sampled_chars_list = src_dataset.get_chars_list(i)\n            assert len(sampled_chars_list) == len(orig_chars_list)\n            for sampled_chars, orig_chars in zip(sampled_chars_list, orig_chars_list):\n                assert all(sampled_chars.numpy() == orig_chars.numpy())\n\n    def test_collate_char_dataset(self):\n        src_dataset = char_data.InMemoryNumpyWordCharDataset()\n        src_dataset.parse(\n            self.src_txt, self.d, self.char_dict, reverse_order=True, append_eos=True\n        )\n        tgt_dataset = char_data.InMemoryNumpyWordCharDataset()\n        tgt_dataset.parse(\n            self.trg_txt, self.d, self.char_dict, reverse_order=True, append_eos=True\n        )\n        char_dataset = char_data.LanguagePairCharDataset(\n            src=src_dataset,\n            src_sizes=src_dataset.sizes,\n            src_dict=self.d,\n            tgt=tgt_dataset,\n            tgt_sizes=tgt_dataset.sizes,\n            tgt_dict=self.d,\n        )\n        samples = [char_dataset[i] for i in range(len(char_dataset))]\n        collate_data = char_dataset.collater(samples)\n        ids = collate_data[""id""]\n        ntokens = collate_data[""ntokens""]\n        assert len(ids) == 4\n        assert ntokens == 32\n        net_input = collate_data[""net_input""]\n        assert net_input[""char_inds""].size() == torch.Size([4, 11, 4])\n        assert net_input[""prev_output_chars""].size() == torch.Size([4, 11, 4])\n        assert collate_data[""target_char_inds""].size() == torch.Size([4, 11, 4])\n        assert net_input[""prev_output_word_lengths""].size() == torch.Size([4, 12])\n        for i in range(net_input[""prev_output_chars""].size()[0]):\n            assert net_input[""prev_output_chars""][i, 0, 0] == self.d.eos_index\n            # Asseting that the generated word before the first was is only eos.\n            assert net_input[""prev_output_word_lengths""][i][0] == 1\n\n    def test_collate_char_dataset_w_unk(self):\n        """"""\n        We intentionally intorduce a text that has unknown words in it.\n        """"""\n\n        # srct and trgt are unknown words.\n        src_txt = test_utils.write_lines_to_temp_file(\n            [""srcA srct srcA srcA"", ""srcA srcA srcA srcA""]\n        )\n        trg_txt = test_utils.write_lines_to_temp_file(\n            [""trgA trgA trgt trgB"", ""trgA trgB trgC trgD""]\n        )\n        src_dataset = char_data.InMemoryNumpyWordCharDataset()\n        src_dataset.parse(\n            src_txt, self.d, self.char_dict, reverse_order=True, append_eos=True\n        )\n        tgt_dataset = char_data.InMemoryNumpyWordCharDataset(ignore_chars_for_unks=True)\n        tgt_dataset.parse(\n            trg_txt, self.d, self.char_dict, reverse_order=True, append_eos=True\n        )\n\n        # Confirming that the third word in an unknown.\n        assert tgt_dataset.char_offsets[1] + 1 == tgt_dataset.char_offsets[2]\n        assert (\n            tgt_dataset.char_buffer[tgt_dataset.char_offsets[1]] == self.char_dict.eos()\n        )\n\n        char_dataset = char_data.LanguagePairCharDataset(\n            src=src_dataset,\n            src_sizes=src_dataset.sizes,\n            src_dict=self.d,\n            tgt=tgt_dataset,\n            tgt_sizes=tgt_dataset.sizes,\n            tgt_dict=self.d,\n        )\n        samples = [char_dataset[i] for i in range(len(char_dataset))]\n        collate_data = char_dataset.collater(samples)\n        ids = collate_data[""id""]\n        assert len(ids) == 2\n        net_input = collate_data[""net_input""]\n\n        assert net_input[""prev_output_word_lengths""][0][2] == 1\n        assert torch.equal(\n            net_input[""prev_output_chars""][0][2], torch.tensor([2, 0, 0, 0])\n        )\n        assert torch.equal(\n            collate_data[""target_char_inds""][0][1], torch.tensor([2, 0, 0, 0])\n        )\n\n    def test_collate_char_dataset_no_tgt(self):\n        src_dataset = char_data.InMemoryNumpyWordCharDataset()\n        src_dataset.parse(\n            self.src_txt, self.d, self.char_dict, reverse_order=True, append_eos=True\n        )\n        char_dataset = char_data.LanguagePairCharDataset(\n            src=src_dataset, src_sizes=src_dataset.sizes, src_dict=self.d\n        )\n        samples = [char_dataset[i] for i in range(len(char_dataset))]\n        collate_data = char_dataset.collater(samples)\n        ids = collate_data[""id""]\n        ntokens = collate_data[""ntokens""]\n        assert len(ids) == 4\n        assert ntokens is None\n        net_input = collate_data[""net_input""]\n        assert net_input[""char_inds""].size() == torch.Size([4, 11, 4])\n        assert net_input[""prev_output_chars""] is None\n        assert collate_data[""target_char_inds""] is None\n        assert net_input[""prev_output_word_lengths""] is None\n'"
pytorch_translate/test/test_dictionary.py,0,"b'#!/usr/bin/env python3\n\nimport os\nimport unittest\n\nfrom pytorch_translate.data import dictionary\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestDictionary(unittest.TestCase):\n    def test_base(self):\n        d = dictionary.Dictionary()\n        self.assertEqual(len(d), d.nspecial)\n\n    def test_build_vocab_file(self):\n        src_txt, trg_txt = test_utils.create_test_text_files()\n        tmp_prefix = test_utils.make_temp_file()\n        src_dict1 = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt], vocab_file=f""{tmp_prefix}.src1"", max_vocab_size=1000\n        )\n        src_dict2 = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt, src_txt, src_txt],\n            vocab_file=f""{tmp_prefix}.src2"",\n            max_vocab_size=1000,\n            padding_factor=1,\n        )\n        trg_dict1 = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[trg_txt], vocab_file=f""{tmp_prefix}.trg1"", max_vocab_size=1000\n        )\n        trg_dict2 = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[trg_txt, trg_txt, trg_txt],\n            vocab_file=f""{tmp_prefix}.trg2"",\n            max_vocab_size=1000,\n            padding_factor=1,\n        )\n        srctrg_dict = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt, trg_txt],\n            vocab_file=f""{tmp_prefix}.srctrg"",\n            max_vocab_size=1000,\n            padding_factor=1,\n        )\n        nspecial = src_dict1.nspecial\n        self.assertEqual(len(src_dict1), nspecial + 4)\n        self.assertEqual(len(trg_dict1), nspecial + 4)\n        self.assertEqual(len(srctrg_dict), nspecial + 8)\n        for s in src_dict1.symbols:\n            self.assertIn(s, srctrg_dict.symbols)\n        for s in trg_dict1.symbols:\n            self.assertIn(s, srctrg_dict.symbols)\n        src_dict1_loaded = dictionary.Dictionary.load(f""{tmp_prefix}.src1"")\n        src_dict2_loaded = dictionary.Dictionary.load(f""{tmp_prefix}.src2"")\n        trg_dict1_loaded = dictionary.Dictionary.load(f""{tmp_prefix}.trg1"")\n        trg_dict2_loaded = dictionary.Dictionary.load(f""{tmp_prefix}.trg2"")\n        self._assert_vocab_equal(src_dict1, src_dict2)\n        self._assert_vocab_equal(src_dict1, src_dict1_loaded)\n        self._assert_vocab_equal(src_dict1, src_dict2_loaded)\n        self._assert_vocab_equal(trg_dict1, trg_dict2)\n        self._assert_vocab_equal(trg_dict1, trg_dict1_loaded)\n        self._assert_vocab_equal(trg_dict1, trg_dict2_loaded)\n        for c in range(nspecial, nspecial + 4):\n            self.assertEqual(src_dict1.count[c], src_dict1_loaded.count[c])\n            self.assertEqual(src_dict2.count[c], src_dict2_loaded.count[c])\n            self.assertEqual(src_dict1.count[c] * 3, src_dict2.count[c])\n            self.assertEqual(trg_dict1.count[c], trg_dict1_loaded.count[c])\n            self.assertEqual(trg_dict2.count[c], trg_dict2_loaded.count[c])\n            self.assertEqual(trg_dict1.count[c] * 3, trg_dict2.count[c])\n        os.remove(f""{tmp_prefix}.src1"")\n        os.remove(f""{tmp_prefix}.src2"")\n        os.remove(f""{tmp_prefix}.trg1"")\n        os.remove(f""{tmp_prefix}.trg2"")\n        os.remove(src_txt)\n        os.remove(trg_txt)\n\n    def test_build_vocab_file_max_vocab(self):\n        src_txt, trg_txt = test_utils.create_test_text_files()\n        tmp_prefix = test_utils.make_temp_file()\n        src_dict1 = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt],\n            vocab_file=f""{tmp_prefix}.src1"",\n            max_vocab_size=1,\n            padding_factor=1,\n        )\n        src_dict2 = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt],\n            vocab_file=f""{tmp_prefix}.src2"",\n            max_vocab_size=2,\n            padding_factor=1,\n        )\n        src_dict3 = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt],\n            vocab_file=f""{tmp_prefix}.src3"",\n            max_vocab_size=104,\n            padding_factor=1,\n        )\n        src_dict4 = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt],\n            vocab_file=f""{tmp_prefix}.src4"",\n            max_vocab_size=0,\n            padding_factor=1,\n        )\n        self.assertEqual(src_dict1.nspecial + 1, len(src_dict1))\n        self.assertEqual(src_dict2.nspecial + 2, len(src_dict2))\n        self.assertEqual(src_dict3.nspecial + 4, len(src_dict3))\n        self._assert_vocab_equal(src_dict3, src_dict4)\n        os.remove(f""{tmp_prefix}.src1"")\n        os.remove(f""{tmp_prefix}.src2"")\n        os.remove(f""{tmp_prefix}.src3"")\n        os.remove(f""{tmp_prefix}.src4"")\n        os.remove(src_txt)\n        os.remove(trg_txt)\n\n    def _assert_vocab_equal(self, d1, d2):\n        self.assertDictEqual(d1.indices, d2.indices)\n        self.assertSetEqual(d1.lexicon_indices, d2.lexicon_indices)\n        self.assertListEqual(d1.symbols, d2.symbols)\n\n\nclass TestMaxVocabDictionary(unittest.TestCase):\n    def test_push(self):\n        max_vocab_dict = dictionary.MaxVocabDictionary()\n        src_txt, trg_txt = test_utils.create_test_text_files()\n        tmp_prefix = test_utils.make_temp_file()\n        src_dict = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt], vocab_file=f""{tmp_prefix}.src"", max_vocab_size=1000\n        )\n        srctrg_dict = dictionary.Dictionary.build_vocab_file(\n            corpus_files=[src_txt, trg_txt],\n            vocab_file=f""{tmp_prefix}.srctrg"",\n            max_vocab_size=1000,\n        )\n        self.assertEqual(len(max_vocab_dict), max_vocab_dict.nspecial)\n        max_vocab_dict.push(src_dict)\n        self.assertEqual(len(max_vocab_dict), len(src_dict))\n        max_vocab_dict.push(srctrg_dict)\n        self.assertEqual(len(max_vocab_dict), len(srctrg_dict))\n        max_vocab_dict.push(src_dict)\n        self.assertEqual(len(max_vocab_dict), len(srctrg_dict))\n        os.remove(f""{tmp_prefix}.src"")\n        os.remove(f""{tmp_prefix}.srctrg"")\n        os.remove(src_txt)\n        os.remove(trg_txt)\n'"
pytorch_translate/test/test_export.py,19,"b'#!/usr/bin/env python3\n\nimport io\nimport logging\nimport unittest\n\nimport numpy as np\nimport torch\nfrom pytorch_translate import char_source_hybrid  # noqa\nfrom pytorch_translate import char_source_model  # noqa\nfrom pytorch_translate import char_source_transformer_model  # noqa\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate import transformer  # noqa\nfrom pytorch_translate import constants\nfrom pytorch_translate.ensemble_export import BeamSearch, BeamSearchAndDecode\nfrom pytorch_translate.research.knowledge_distillation import (  # noqa\n    dual_decoder_kd_model,\n    hybrid_dual_decoder_kd_model,\n)\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass TestPyTorchExport(unittest.TestCase):\n    def _test_full_beam_decoder(self, test_args, quantize=False):\n        samples, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        sample = next(samples)\n        # [seq len, batch size=1]\n        src_tokens = sample[""net_input""][""src_tokens""][0:1].t()\n        # [seq len]\n        src_lengths = sample[""net_input""][""src_lengths""][0:1].long()\n\n        num_models = 3\n        model_list = []\n        for _ in range(num_models):\n            model_list.append(task.build_model(test_args))\n\n        length, word_length = 11, 7\n        if test_args.arch in constants.ARCHS_FOR_CHAR_SOURCE:\n            char_inds = torch.LongTensor(\n                np.random.randint(0, 126, (1, length, word_length), dtype=""int64"")\n            )\n            word_lengths = torch.IntTensor(\n                np.array([word_length] * length, dtype=""int32"")\n            ).reshape((1, length))\n        else:\n            char_inds, word_lengths = None, None\n\n        beam_size = 6\n        bs = BeamSearch(\n            model_list,\n            tgt_dict,\n            src_tokens,\n            src_lengths,\n            beam_size=beam_size,\n            quantize=quantize,\n            char_inds=char_inds,\n            word_lengths=word_lengths,\n        )\n        f = io.BytesIO()\n        bs.save_to_pytorch(f)\n\n        # Test generalization with a different sequence length\n        src_tokens = torch.LongTensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]).unsqueeze(1)\n        src_lengths = torch.LongTensor([11])\n        prev_token = torch.LongTensor([0])\n        prev_scores = torch.FloatTensor([0.0])\n        attn_weights = torch.zeros(src_tokens.shape[0])\n        prev_hypos_indices = torch.zeros(beam_size, dtype=torch.int64)\n\n        outs = bs(\n            src_tokens,\n            src_lengths,\n            prev_token,\n            prev_scores,\n            attn_weights,\n            prev_hypos_indices,\n            torch.LongTensor([20]),\n            char_inds=char_inds,\n            word_lengths=word_lengths,\n        )\n\n        f.seek(0)\n        deserialized_bs = torch.jit.load(f)\n        deserialized_bs.apply(lambda s: s._unpack() if hasattr(s, ""_unpack"") else None)\n        outs_deserialized = deserialized_bs(\n            src_tokens,\n            src_lengths,\n            prev_token,\n            prev_scores,\n            attn_weights,\n            prev_hypos_indices,\n            torch.LongTensor([20]),\n            char_inds=char_inds,\n            word_lengths=word_lengths,\n        )\n\n        for a, b in zip(outs_deserialized, outs):\n            np.testing.assert_allclose(a.detach().numpy(), b.detach().numpy())\n\n    def test_full_beam_decoder(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        self._test_full_beam_decoder(test_args)\n\n    def test_full_beam_decoder_vocab_reduction(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_decoder(test_args)\n\n    def test_full_beam_decoder_quantization(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_decoder(test_args, quantize=True)\n\n    def test_full_beam_decoder_char_rnn_vocab_reduction(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n\n        test_args.arch = ""char_source""\n        test_args.char_source_dict_size = 126\n        test_args.char_embed_dim = 8\n        test_args.char_rnn_units = 12\n        test_args.char_rnn_layers = 2\n\n        self._test_full_beam_decoder(test_args)\n\n    def test_full_beam_decoder_char_cnn_vocab_reduction(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n\n        test_args.arch = ""char_source""\n        test_args.char_source_dict_size = 126\n        test_args.char_embed_dim = 8\n        test_args.char_cnn_params = ""[(10, 3), (10, 5)]""\n        test_args.char_cnn_nonlinear_fn = ""tanh""\n        test_args.char_cnn_pool_type = ""max""\n        test_args.char_cnn_num_highway_layers = 2\n\n        self._test_full_beam_decoder(test_args)\n\n    def test_full_beam_decoder_quantization_hybrid(self):\n        test_args = test_utils.ModelParamsDict(arch=""hybrid_transformer_rnn"")\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_decoder(test_args, quantize=True)\n\n    def test_full_beam_decoder_quantization_hybrid_reduced_attention(self):\n        test_args = test_utils.ModelParamsDict(arch=""hybrid_transformer_rnn"")\n        test_args.decoder_reduced_attention_dim = 10\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_decoder(test_args, quantize=True)\n\n    def test_full_beam_decoder_aan(self):\n        test_args = test_utils.ModelParamsDict(arch=""transformer"")\n        test_args.aan = True\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_decoder(test_args, quantize=True)\n\n    def test_full_beam_decoder_aan_bottlenceck(self):\n        test_args = test_utils.ModelParamsDict(arch=""transformer"")\n        test_args.aan = True\n        test_args.decoder_out_embed_dim = 5\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_decoder(test_args, quantize=True)\n\n    def test_full_beam_decoder_char_transformer(self):\n        test_args = test_utils.ModelParamsDict(arch=""char_transformer"")\n        test_args.char_source_dict_size = 126\n        test_args.char_embed_dim = 8\n        test_args.char_cnn_params = ""[(10, 3), (10, 5)]""\n        test_args.char_cnn_nonlinear_fn = ""tanh""\n        test_args.char_cnn_pool_type = ""max""\n        test_args.char_cnn_num_highway_layers = 2\n\n        self._test_full_beam_decoder(test_args)\n\n    def test_full_beam_decoder_char_source_hybrid(self):\n        test_args = test_utils.ModelParamsDict(arch=""char_source_hybrid"")\n        test_args.char_source_dict_size = 126\n        test_args.char_embed_dim = 8\n        test_args.char_cnn_params = ""[(8, 3), (8, 5)]""\n        test_args.char_cnn_nonlinear_fn = ""tanh""\n        test_args.char_cnn_pool_type = ""max""\n        test_args.char_cnn_num_highway_layers = 2\n        self._test_full_beam_decoder(test_args)\n\n\nclass TestBeamSearchAndDecodeExport(unittest.TestCase):\n    def _test_full_beam_search_decoder(self, test_args, quantize=False):\n        samples, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        sample = next(samples)\n        # [seq len, batch size=1]\n        src_tokens = sample[""net_input""][""src_tokens""][0:1].t()\n        # [seq len]\n        src_lengths = sample[""net_input""][""src_lengths""][0:1].long()\n\n        num_models = 3\n        model_list = []\n        for _ in range(num_models):\n            model_list.append(task.build_model(test_args))\n\n        eos_token_id = 8\n        length_penalty = 0.25\n        nbest = 3\n        stop_at_eos = True\n        num_steps = torch.LongTensor([20])\n\n        beam_size = 6\n        bsd = BeamSearchAndDecode(\n            model_list,\n            tgt_dict,\n            src_tokens,\n            src_lengths,\n            eos_token_id=eos_token_id,\n            length_penalty=length_penalty,\n            nbest=nbest,\n            beam_size=beam_size,\n            stop_at_eos=stop_at_eos,\n            quantize=quantize,\n        )\n        f = io.BytesIO()\n        bsd.save_to_pytorch(f)\n\n        # Test generalization with a different sequence length\n        src_tokens = torch.LongTensor([1, 2, 3, 4, 5, 6, 7, 9, 9, 10, 11]).unsqueeze(1)\n        src_lengths = torch.LongTensor([11])\n        prev_token = torch.LongTensor([0])\n        prev_scores = torch.FloatTensor([0.0])\n        attn_weights = torch.zeros(src_tokens.shape[0])\n        prev_hypos_indices = torch.zeros(beam_size, dtype=torch.int64)\n\n        outs = bsd(\n            src_tokens,\n            src_lengths,\n            prev_token,\n            prev_scores,\n            attn_weights,\n            prev_hypos_indices,\n            num_steps[0],\n        )\n\n        f.seek(0)\n        deserialized_bsd = torch.jit.load(f)\n        deserialized_bsd.apply(lambda s: s._unpack() if hasattr(s, ""_unpack"") else None)\n        outs_deserialized = deserialized_bsd(\n            src_tokens,\n            src_lengths,\n            prev_token,\n            prev_scores,\n            attn_weights,\n            prev_hypos_indices,\n            num_steps[0],\n        )\n\n        for hypo, hypo_deserialized in zip(outs, outs_deserialized):\n            np.testing.assert_array_equal(\n                hypo[0].tolist(), hypo_deserialized[0].tolist()\n            )\n            np.testing.assert_array_almost_equal(\n                hypo[2], hypo_deserialized[2], decimal=1\n            )\n            np.testing.assert_array_almost_equal(\n                hypo[3].numpy(), hypo_deserialized[3].numpy(), decimal=1\n            )\n\n    def test_full_beam_search_decoder(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        self._test_full_beam_search_decoder(test_args)\n\n    def test_full_beam_search_decoder_reverse(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True, reverse_source=True\n        )\n        self._test_full_beam_search_decoder(test_args)\n\n    def test_full_beam_search_decoder_vocab_reduction(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_search_decoder(test_args)\n\n    def test_full_beam_search_decoder_quantization(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_search_decoder(test_args, quantize=True)\n\n    def test_full_beam_search_decoder_hybrid(self):\n        test_args = test_utils.ModelParamsDict(arch=""hybrid_transformer_rnn"")\n        self._test_full_beam_search_decoder(test_args)\n\n    def test_full_beam_search_decoder_quantization_hybrid(self):\n        test_args = test_utils.ModelParamsDict(arch=""hybrid_transformer_rnn"")\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        test_args.vocab_reduction_params = {\n            ""lexical_dictionaries"": lexical_dictionaries,\n            ""num_top_words"": 10,\n            ""max_translation_candidates_per_word"": 1,\n        }\n        self._test_full_beam_search_decoder(test_args, quantize=True)\n'"
pytorch_translate/test/test_export_beam_decode.py,18,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom pytorch_translate import char_source_model  # noqa\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate.beam_decode import BeamDecode, SequenceGenerator\nfrom pytorch_translate.ensemble_export import BeamSearch\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nMAX_VOCAB_SIZE = 7\n\n\ndef prepare_test_input(src_length=10, num_step=20, beam_size=5):\n    beam_tokens = torch.randint(MAX_VOCAB_SIZE, (num_step + 1, beam_size))\n    beam_scores = torch.rand((num_step + 1, beam_size))\n    token_weights = torch.rand((num_step + 1, beam_size, src_length))\n    beam_prev_indices = torch.randint(beam_size, (num_step + 1, beam_size))\n    num_step = num_step\n    return (beam_tokens, beam_scores, token_weights, beam_prev_indices, num_step)\n\n\nclass TestExportBeamDecode(unittest.TestCase):\n    def test_beamsize_one_decode(self):\n        beam_decode = BeamDecode(\n            eos_token_id=2, length_penalty=0.25, nbest=5, beam_size=1, stop_at_eos=True\n        )\n        one_beamsize_input = prepare_test_input(beam_size=1)\n\n        beam_tokens, beam_scores, token_weights, beam_prev_indices, num_step = (\n            one_beamsize_input\n        )\n        output = beam_decode(\n            beam_tokens, beam_scores, token_weights, beam_prev_indices, num_step\n        )\n\n        top_hypothesis_tokens = output[0][0]\n\n        # 0 index row is ignored\n        beam_search_token_output = beam_tokens.reshape(-1)[1:].tolist()\n        if beam_decode.eos_token_id in beam_search_token_output:\n            index_of_eos = beam_search_token_output.index(beam_decode.eos_token_id)\n            beam_search_token_output = beam_search_token_output[: index_of_eos + 1]\n\n        np.testing.assert_array_equal(top_hypothesis_tokens, beam_search_token_output)\n\n    def test_attention_weights(self):\n        beam_decode = BeamDecode(\n            eos_token_id=2, length_penalty=0.25, nbest=5, beam_size=4, stop_at_eos=True\n        )\n        one_beamsize_input = prepare_test_input(beam_size=4)\n\n        beam_tokens, beam_scores, token_weights, beam_prev_indices, num_step = (\n            one_beamsize_input\n        )\n        output = beam_decode(\n            beam_tokens, beam_scores, token_weights, beam_prev_indices, num_step\n        )\n\n        all_end_states = beam_decode._get_all_end_states(\n            beam_tokens, beam_scores, beam_prev_indices, num_step\n        )\n        for state_idx, end_state in enumerate(all_end_states):\n            beam_indices = beam_decode._get_output_steps_to_beam_indices(\n                end_state, beam_prev_indices\n            )\n            weights_from_output = output[state_idx][3].numpy()\n            weights_from_input = []\n            for pos, beam_index in enumerate(beam_indices):\n                if pos == 0:\n                    continue\n                weights_from_input.append(token_weights[pos][beam_index])\n            weights_from_input = torch.stack(weights_from_input, dim=1).numpy()\n\n            np.testing.assert_array_equal(weights_from_output, weights_from_input)\n\n    def test_known_inputs_outputs(self):\n        ALL_TESTS = [End2EndTest()]\n\n        for test in ALL_TESTS:\n            beam_decode = BeamDecode(\n                eos_token_id=test.eos_token_id,\n                length_penalty=test.length_penalty,\n                nbest=test.nbest,\n                beam_size=test.beam_size,\n                stop_at_eos=test.stop_at_eos,\n            )\n            output = beam_decode(*test.prepare_input())\n            test.test_output(output)\n\n    def test_basic_generate(self):\n        """"""\n        A basic test that the output given by SequenceGenerator class is the same\n        """"""\n        # Setup parameters required for SequenceGenerator and BeamSeach/BeamDecode\n        TEST_ARGS = test_utils.ModelParamsDict(arch=""rnn"")\n        TEST_ARGS.sequence_lstm = True\n        BEAM_SIZE = 1\n        WORD_REWARD = 0\n        UNK_REWARD = 0\n        LENGTH_PENALTY = 0\n\n        PLACEHOLDER_SEQ_LENGTH = 5\n        NBEST = 2\n        MAX_SEQ_LEN = 7\n\n        src_tokens = torch.LongTensor([[0, 0, 0]])\n        src_lengths = torch.LongTensor([3])\n\n        # Generate values using SequenceGenerator\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(TEST_ARGS)\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        model = task.build_model(TEST_ARGS)\n        translator = SequenceGenerator(\n            [model],\n            task.target_dictionary,\n            beam_size=BEAM_SIZE,\n            word_reward=WORD_REWARD,\n            unk_reward=UNK_REWARD,\n        )\n\n        encoder_input = {""src_tokens"": src_tokens, ""src_lengths"": src_lengths}\n        top_seq_gen_hypothesis = translator.generate(encoder_input, maxlen=MAX_SEQ_LEN)[\n            0\n        ]\n\n        # Generate output using BeamSearch/BeamDecode\n        placeholder_src_tokens = torch.LongTensor(\n            np.ones((PLACEHOLDER_SEQ_LENGTH, 1), dtype=""int64"")\n        )\n        placeholder_src_lengths = torch.IntTensor(\n            np.array([PLACEHOLDER_SEQ_LENGTH], dtype=""int32"")\n        )\n\n        beam_search = BeamSearch(\n            [model],\n            tgt_dict,\n            placeholder_src_tokens,\n            placeholder_src_lengths,\n            beam_size=BEAM_SIZE,\n            word_reward=WORD_REWARD,\n            unk_reward=UNK_REWARD,\n            quantize=False,\n        )\n        beam_decode = BeamDecode(\n            eos_token_id=tgt_dict.eos(),\n            length_penalty=LENGTH_PENALTY,\n            nbest=NBEST,\n            beam_size=BEAM_SIZE,\n            stop_at_eos=True,\n        )\n\n        # Few more placeholder inputs for BeamSearch\n        prev_token = torch.LongTensor([tgt_dict.eos()])\n        prev_scores = torch.FloatTensor([0.0])\n        attn_weights = torch.zeros(src_lengths[0].item())\n        prev_hypos_indices = torch.zeros(1, dtype=torch.int64)\n        num_steps = torch.LongTensor([MAX_SEQ_LEN])\n\n        all_tokens, all_scores, all_weights, all_prev_indices = beam_search(\n            src_tokens.transpose(0, 1),\n            src_lengths,\n            prev_token,\n            prev_scores,\n            attn_weights,\n            prev_hypos_indices,\n            num_steps,\n        )\n        beam_decode_output = beam_decode(\n            all_tokens, all_scores, all_weights, all_prev_indices, num_steps[0]\n        )\n\n        for hyp_index in range(\n            min(len(beam_decode_output), len(top_seq_gen_hypothesis))\n        ):\n            top_beam_decode_hypothesis = beam_decode_output[hyp_index]\n\n            # Compare two outputs\n            # We always look only from 0 to MAX_SEQ_LEN, because sequence generator\n            # adds an EOS at the end after MAX_SEQ_LEN\n            ## Compare two hypothesis\n            np.testing.assert_array_equal(\n                top_seq_gen_hypothesis[hyp_index][""tokens""].tolist()[0:MAX_SEQ_LEN],\n                top_beam_decode_hypothesis[0].tolist()[0:MAX_SEQ_LEN],\n            )\n            ## Compare token level scores\n            np.testing.assert_array_almost_equal(\n                top_seq_gen_hypothesis[hyp_index][""positional_scores""].tolist()[\n                    0:MAX_SEQ_LEN\n                ],\n                top_beam_decode_hypothesis[2][0:MAX_SEQ_LEN],\n                decimal=1,\n            )\n\n            ## Compare attention weights\n            np.testing.assert_array_almost_equal(\n                top_seq_gen_hypothesis[hyp_index][""attention""].numpy()[\n                    :, 0:MAX_SEQ_LEN\n                ],\n                top_beam_decode_hypothesis[3].numpy()[:, 0:MAX_SEQ_LEN],\n                decimal=1,\n            )\n            ## Not testing the hypothesis score as sequence generator is adding EOS\n            ## at the end, it changes the final score\n\n\nclass BeamDecodeTestBase(object):\n    def __init__(self):\n        self.eos_token_id = 2\n        self.length_penalty = 0.25\n        self.nbest = 2\n        self.beam_size = 3\n        self.stop_at_eos = True\n\n    def prepare_input(self):\n        pass\n\n    def test_output(self):\n        pass\n\n\nclass End2EndTest(BeamDecodeTestBase):\n    def prepare_input(self):\n        self.src_length = 3\n        self.num_steps = 5\n\n        beam_tokens = torch.tensor(\n            [[0, 0, 0], [0, 4, 1], [6, 2, 3], [1, 1, 0], [2, 1, 5], [3, 5, 5]]\n        )\n        beam_scores = torch.tensor(\n            [\n                [0.2322, 0.3070, 0.3755],\n                [0.8971, 0.0688, 0.9484],\n                [0.2515, 0.2836, 0.2849],\n                [0.1710, 0.6512, 0.6899],\n                [0.1411, 0.1590, 0.0182],\n                [0.262, 0.193, 0.015],\n            ]\n        )\n        token_weights = torch.tensor(\n            [\n                [\n                    [0.0378, 0.1570, 0.4723],\n                    [0.6100, 0.0196, 0.9939],\n                    [0.2908, 0.1187, 0.3387],\n                ],\n                [\n                    [0.1265, 0.7551, 0.1549],\n                    [0.8359, 0.3574, 0.2500],\n                    [0.4979, 0.0281, 0.8424],\n                ],\n                [\n                    [0.9921, 0.0731, 0.5912],\n                    [0.0423, 0.4134, 0.9655],\n                    [0.9652, 0.0211, 0.7168],\n                ],\n                [\n                    [0.8314, 0.9484, 0.4877],\n                    [0.8744, 0.2184, 0.6247],\n                    [0.4664, 0.3436, 0.1755],\n                ],\n                [\n                    [0.2532, 0.3367, 0.3911],\n                    [0.2439, 0.8911, 0.7692],\n                    [0.9397, 0.1810, 0.3092],\n                ],\n                [\n                    [0.3943, 0.1062, 0.4711],\n                    [0.0387, 0.0143, 0.0836],\n                    [0.7161, 0.6038, 0.4064],\n                ],\n            ]\n        )\n        beam_prev_indices = torch.tensor(\n            [[1, 1, 2], [2, 0, 0], [1, 0, 2], [2, 2, 0], [2, 0, 0], [0, 2, 0]]\n        )\n        return (\n            beam_tokens,\n            beam_scores,\n            token_weights,\n            beam_prev_indices,\n            self.num_steps,\n        )\n\n    def test_output(self, output):\n        np.testing.assert_array_equal(output[0][0].numpy(), np.array([0, 2]))\n        np.testing.assert_array_equal(output[1][0].numpy(), np.array([1, 3, 1, 5, 5]))\n        # Third best candidate is [4, 6, 0, 2]\n'"
pytorch_translate/test/test_integration.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nimport contextlib\nimport os\nimport tempfile\nimport unittest\nfrom io import StringIO\n\nimport torch\nfrom fairseq import options\nfrom pytorch_translate import generate, models, train  # noqa need to load models\nfrom pytorch_translate.test.utils import (\n    create_dummy_data,\n    create_dummy_multilingual_data,\n    generate_main,\n    train_translation_model,\n)\n\n\nclass TestTranslation(unittest.TestCase):\n    def test_rnn(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_rnn"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""rnn"",\n                        ""--cell-type"",\n                        ""lstm"",\n                        ""--sequence-lstm"",\n                        ""--reverse-source"",\n                        ""--encoder-bidirectional"",\n                        ""--encoder-layers"",\n                        ""2"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                        ""--attention-type"",\n                        ""dot"",\n                    ],\n                )\n                generate_main(data_dir)\n\n    def test_char_rnn(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_char_rnn"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""char_source"",\n                        ""--char-embed-dim"",\n                        ""4"",\n                        ""--char-rnn-units"",\n                        ""8"",\n                        ""--char-rnn-layers"",\n                        ""1"",\n                        ""--char-source-max-vocab-size"",\n                        ""26"",\n                        ""--cell-type"",\n                        ""lstm"",\n                        ""--sequence-lstm"",\n                        ""--encoder-dropout-in"",\n                        ""0"",\n                        ""--encoder-dropout-out"",\n                        ""0"",\n                        ""--decoder-dropout-in"",\n                        ""0.2"",\n                        ""--decoder-dropout-out"",\n                        ""0.2"",\n                        ""--encoder-layers"",\n                        ""2"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                        ""--attention-type"",\n                        ""dot"",\n                    ],\n                )\n                generate_main(\n                    data_dir,\n                    [\n                        ""--char-source-vocab-file"",\n                        os.path.join(data_dir, ""char-dictionary-in.txt""),\n                    ],\n                )\n\n    @unittest.skip(""it\'s failing (T40139796)"")\n    def test_multilingual(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_multilingual"") as data_dir:\n                create_dummy_multilingual_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--task"",\n                        ""pytorch_translate_multilingual"",\n                        ""--arch"",\n                        ""rnn"",\n                        ""--cell-type"",\n                        ""lstm"",\n                        ""--sequence-lstm"",\n                        ""--reverse-source"",\n                        ""--encoder-bidirectional"",\n                        ""--encoder-layers"",\n                        ""2"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                        ""--attention-type"",\n                        ""dot"",\n                        ""--multiling-encoder-lang"",\n                        ""xh"",\n                        ""--multiling-encoder-lang"",\n                        ""zu"",\n                        ""--multiling-encoder-lang"",\n                        ""en"",\n                        ""--multiling-decoder-lang"",\n                        ""xh"",\n                        ""--multiling-decoder-lang"",\n                        ""en"",\n                        ""--multiling-source-lang"",\n                        ""xh"",\n                        ""--multiling-target-lang"",\n                        ""en"",\n                        ""--multiling-train-source-text-file"",\n                        os.path.join(data_dir, ""train.xhen.xh""),\n                        ""--multiling-train-target-text-file"",\n                        os.path.join(data_dir, ""train.xhen.en""),\n                        ""--multiling-eval-source-text-file"",\n                        os.path.join(data_dir, ""tune.xhen.xh""),\n                        ""--multiling-eval-target-text-file"",\n                        os.path.join(data_dir, ""tune.xhen.en""),\n                        ""--multiling-source-lang"",\n                        ""zu"",\n                        ""--multiling-target-lang"",\n                        ""en"",\n                        ""--multiling-train-source-text-file"",\n                        os.path.join(data_dir, ""train.zuen.zu""),\n                        ""--multiling-train-target-text-file"",\n                        os.path.join(data_dir, ""train.zuen.en""),\n                        ""--multiling-eval-source-text-file"",\n                        os.path.join(data_dir, ""tune.zuen.zu""),\n                        ""--multiling-eval-target-text-file"",\n                        os.path.join(data_dir, ""tune.zuen.en""),\n                        ""--multiling-source-lang"",\n                        ""en"",\n                        ""--multiling-target-lang"",\n                        ""xh"",\n                        ""--multiling-train-source-text-file"",\n                        os.path.join(data_dir, ""train.xhen.en""),\n                        ""--multiling-train-target-text-file"",\n                        os.path.join(data_dir, ""train.xhen.xh""),\n                        ""--multiling-eval-source-text-file"",\n                        os.path.join(data_dir, ""tune.xhen.en""),\n                        ""--multiling-eval-target-text-file"",\n                        os.path.join(data_dir, ""tune.xhen.xh""),\n                        # set these to empty to satisfy argument validation\n                        ""--train-source-text-file"",\n                        """",\n                        ""--train-target-text-file"",\n                        """",\n                        ""--eval-source-text-file"",\n                        """",\n                        ""--eval-target-text-file"",\n                        """",\n                    ],\n                )\n                for langpair, src, tgt in [\n                    (""xhen"", ""xh"", ""en""),\n                    (""zuen"", ""zu"", ""en""),\n                    (""xhen"", ""en"", ""xh""),\n                ]:\n                    generate_main(\n                        data_dir,\n                        [\n                            ""--task"",\n                            ""pytorch_translate_multilingual"",\n                            ""--multiling-source-lang"",\n                            src,\n                            ""--multiling-target-lang"",\n                            tgt,\n                            ""--source-vocab-file"",\n                            os.path.join(data_dir, f""dictionary-src-{src}.txt""),\n                            ""--target-vocab-file"",\n                            os.path.join(data_dir, f""dictionary-trg-{tgt}.txt""),\n                            ""--source-text-file"",\n                            os.path.join(data_dir, f""tune.{langpair}.{src}""),\n                            ""--target-text-file"",\n                            os.path.join(data_dir, f""tune.{langpair}.{tgt}""),\n                        ],\n                    )\n\n    def test_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_transformer"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""ptt_transformer"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--encoder-attention-heads"",\n                        ""4"",\n                        ""--encoder-layers"",\n                        ""3"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--decoder-attention-heads"",\n                        ""4"",\n                        ""--decoder-layers"",\n                        ""3"",\n                    ],\n                )\n                generate_main(data_dir)\n\n    def test_char_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_char_transformer"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""char_source_transformer"",\n                        ""--char-embed-dim"",\n                        ""4"",\n                        ""--char-cnn-params"",\n                        ""[(10, 1), (20, 2)]"",\n                        ""--char-cnn-nonlinear-fn"",\n                        ""relu"",\n                        ""--char-cnn-num-highway-layers"",\n                        ""2"",\n                        ""--char-source-max-vocab-size"",\n                        ""26"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--encoder-attention-heads"",\n                        ""4"",\n                        ""--encoder-layers"",\n                        ""3"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--decoder-attention-heads"",\n                        ""4"",\n                        ""--decoder-layers"",\n                        ""3"",\n                    ],\n                )\n                generate_main(\n                    data_dir,\n                    [\n                        ""--char-source-vocab-file"",\n                        os.path.join(data_dir, ""char-dictionary-in.txt""),\n                    ],\n                )\n\n    def test_char_source_hybrid(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_char_rnn"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""char_source_hybrid"",\n                        ""--char-embed-dim"",\n                        ""4"",\n                        ""--char-cnn-params"",\n                        ""[(10, 1), (20,2)]"",\n                        ""--char-cnn-nonlinear-fn"",\n                        ""relu"",\n                        ""--char-cnn-num-highway-layers"",\n                        ""2"",\n                        ""--char-source-max-vocab-size"",\n                        ""26"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--encoder-attention-heads"",\n                        ""4"",\n                        ""--encoder-layers"",\n                        ""3"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-attention-heads"",\n                        ""4"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-lstm-units"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                    ],\n                )\n                generate_main(\n                    data_dir,\n                    [\n                        ""--char-source-vocab-file"",\n                        os.path.join(data_dir, ""char-dictionary-in.txt""),\n                    ],\n                )\n\n    def test_char_aware_hybrid(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_char_aware"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""char_aware_hybrid"",\n                        ""--char-embed-dim"",\n                        ""4"",\n                        ""--char-cnn-params"",\n                        ""[(10, 1), (20,2)]"",\n                        ""--char-cnn-nonlinear-fn"",\n                        ""relu"",\n                        ""--char-cnn-num-highway-layers"",\n                        ""2"",\n                        ""--char-source-max-vocab-size"",\n                        ""26"",\n                        ""--char-target-max-vocab-size"",\n                        ""26"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--encoder-attention-heads"",\n                        ""4"",\n                        ""--encoder-layers"",\n                        ""3"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-attention-heads"",\n                        ""4"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-lstm-units"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                    ],\n                )\n                generate_main(\n                    data_dir,\n                    [\n                        ""--char-source-vocab-file"",\n                        os.path.join(data_dir, ""char-src-dictionary-in.txt""),\n                        ""--char-target-vocab-file"",\n                        os.path.join(data_dir, ""char-tgt-dictionary-in.txt""),\n                    ],\n                )\n\n    def test_semi_supervised_rnn(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_rnn"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--task"",\n                        ""pytorch_translate_semi_supervised"",\n                        ""--train-mono-source-text-file"",\n                        os.path.join(data_dir, ""train.in""),\n                        ""--train-mono-target-text-file"",\n                        os.path.join(data_dir, ""train.out""),\n                        ""--arch"",\n                        ""semi_supervised_rnn"",\n                        ""--cell-type"",\n                        ""lstm"",\n                        ""--sequence-lstm"",\n                        ""--reverse-source"",\n                        ""--encoder-bidirectional"",\n                        ""--encoder-layers"",\n                        ""2"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                        ""--attention-type"",\n                        ""dot"",\n                    ],\n                )\n\n    def test_semi_supervised_transformer(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_transformer"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""semi_supervised_transformer"",\n                        # semi-supervised task args:\n                        ""--task"",\n                        ""pytorch_translate_semi_supervised"",\n                        ""--train-mono-source-text-file"",\n                        os.path.join(data_dir, ""train.in""),\n                        ""--train-mono-target-text-file"",\n                        os.path.join(data_dir, ""train.out""),\n                        # transformer args:\n                        ""--encoder-embed-dim"",\n                        ""4"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""4"",\n                        ""--encoder-attention-heads"",\n                        ""2"",\n                        ""--encoder-layers"",\n                        ""1"",\n                        ""--decoder-embed-dim"",\n                        ""4"",\n                        ""--decoder-ffn-embed-dim"",\n                        ""4"",\n                        ""--decoder-attention-heads"",\n                        ""2"",\n                        ""--decoder-layers"",\n                        ""1"",\n                    ],\n                )\n\n    def test_denoising_autoencoder(self):\n        """"""\n        Tests denoising autoencoder task. Important flags:\n        `--train-mono-*-text-file`, `--task`, `--arch`, and\n        `--denoising-target-mono`.\n        """"""\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_rnn"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--task"",\n                        ""pytorch_translate_denoising_autoencoder"",\n                        ""--train-mono-source-text-file"",\n                        os.path.join(data_dir, ""train.in""),\n                        ""--train-mono-target-text-file"",\n                        os.path.join(data_dir, ""train.out""),\n                        ""--arch"",\n                        ""semi_supervised_rnn"",\n                        ""--denoising-target-mono"",\n                        ""--cell-type"",\n                        ""lstm"",\n                        ""--sequence-lstm"",\n                        ""--reverse-source"",\n                        ""--encoder-bidirectional"",\n                        ""--encoder-layers"",\n                        ""2"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                        ""--attention-type"",\n                        ""dot"",\n                    ],\n                )\n\n    def test_multilingual_hybrid(self):\n        """"""\n        Tests multilingual translation task. Important flags:\n        `--multilingual-*-binary-path`, `--task`, `--arch`,\n        `--source-vocabulary`, `--target-vocabulary`, `--vocabulary`.\n        """"""\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_multilingual_hybrid"") as data_dir:\n                create_dummy_multilingual_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--task"",\n                        ""pytorch_translate_multilingual_task"",\n                        ""--arch"",\n                        ""multilingual_hybrid_transformer_rnn"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--encoder-attention-heads"",\n                        ""4"",\n                        ""--encoder-layers"",\n                        ""3"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-attention-heads"",\n                        ""4"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-lstm-units"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                        ""--lang-pairs"",\n                        ""xh-en,zu-en"",\n                        ""--multilingual-train-text-file"",\n                        (\n                            ""xh-en:""\n                            f""{os.path.join(data_dir, \'train.xhen.xh\')},""\n                            f""{os.path.join(data_dir, \'train.xhen.en\')}""\n                        ),\n                        ""--multilingual-eval-text-file"",\n                        (\n                            ""xh-en:""\n                            f""{os.path.join(data_dir, \'tune.xhen.xh\')},""\n                            f""{os.path.join(data_dir, \'tune.xhen.en\')}""\n                        ),\n                        ""--multilingual-train-text-file"",\n                        (\n                            ""zu-en:""\n                            f""{os.path.join(data_dir, \'train.zuen.zu\')},""\n                            f""{os.path.join(data_dir, \'train.zuen.en\')}""\n                        ),\n                        ""--multilingual-eval-text-file"",\n                        (\n                            ""zu-en:""\n                            f""{os.path.join(data_dir, \'tune.zuen.zu\')},""\n                            f""{os.path.join(data_dir, \'tune.zuen.en\')}""\n                        ),\n                        # set these to empty to satisfy argument validation\n                        ""--train-source-text-file"",\n                        """",\n                        ""--train-target-text-file"",\n                        """",\n                        ""--eval-source-text-file"",\n                        """",\n                        ""--eval-target-text-file"",\n                        """",\n                    ],\n                    # fairseq MultlilingualTranslationTask expects mandatory\n                    # data directory positional argument\n                    set_empty_data_positional_arg=True,\n                    set_lang_args=False,\n                )\n\n    def test_word_prediction(self):\n        """""" Tests a word prediction model, which will use a learned vocab\n        reduction via the word prediction model. It uses a custom criterion\n        (word_prediction) on top of label_smoothed_cross_entropy so we pass the\n        corresponding word_prediction criterion flag in during training.\n        """"""\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_word_pred"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir=data_dir,\n                    extra_flags=[\n                        ""--arch"",\n                        ""rnn_word_pred"",\n                        ""--predictor-hidden-dim"",\n                        ""32"",\n                        ""--topk-labels-per-source-token"",\n                        ""5"",\n                        ""--cell-type"",\n                        ""lstm"",\n                        ""--sequence-lstm"",\n                        ""--reverse-source"",\n                        ""--encoder-bidirectional"",\n                        ""--encoder-layers"",\n                        ""2"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                        ""--attention-type"",\n                        ""dot"",\n                    ],\n                    criterion=[\n                        ""--criterion"",\n                        ""word_prediction"",\n                        ""--label-smoothing"",\n                        ""0.1"",\n                    ],\n                )\n                generate_main(data_dir)\n\n    def test_pretrained_masked_lm_for_translation(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_mlm"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""xlm_base"",\n                        data_dir,\n                        # semi-supervised task args:\n                        ""--task"",\n                        ""pytorch_translate_cross_lingual_lm"",\n                        # transformer args:\n                        ""--encoder-embed-dim"",\n                        ""4"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""4"",\n                        ""--encoder-attention-heads"",\n                        ""2"",\n                        ""--encoder-layers"",\n                        ""1"",\n                        # dict files\n                        ""--source-vocab-file"",\n                        os.path.join(data_dir, ""dictionary-in.txt""),\n                        ""--target-vocab-file"",\n                        os.path.join(data_dir, ""dictionary-out.txt""),\n                        # additoinal ones\n                        ""--dataset-impl"",\n                        ""raw"",\n                        ""--monolingual-langs"",\n                        ""in,out"",\n                        ""--save-only"",\n                        ""--masked-lm-only"",\n                        ""--num-segment"",\n                        ""2"",\n                    ],\n                    criterion=[""--criterion"", ""legacy_masked_lm_loss""],\n                )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
pytorch_translate/test/test_multilingual_utils.py,0,b''
pytorch_translate/test/test_options.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\nimport unittest\n\nfrom pytorch_translate import constants, options\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestOptions(unittest.TestCase):\n    def get_common_data_args_namespace(self):\n        args = argparse.Namespace()\n        args.train_source_text_file = test_utils.make_temp_file()\n        args.train_target_text_file = test_utils.make_temp_file()\n        args.eval_source_text_file = test_utils.make_temp_file()\n        args.eval_target_text_file = test_utils.make_temp_file()\n        args.source_vocab_file = test_utils.make_temp_file()\n        args.target_vocab_file = test_utils.make_temp_file()\n        args.task = ""pytorch_translate""\n        return args\n\n    def test_validate_preprocesing_args(self):\n        """""" Make sure we validation passes with the minimum args required. """"""\n        args = self.get_common_data_args_namespace()\n        options.validate_preprocessing_args(args)\n\n    def test_validate_fails_for_missing_preprocessing_arg(self):\n        """"""\n        We expect a ValueError when filepaths for a certain data type is\n        missing. In this case, train source is not set at all -- no text file or\n        binary path corresponds to this required data.\n        """"""\n        args = self.get_common_data_args_namespace()\n        args.train_source_text_file = None\n        self.assertRaises(ValueError, options.validate_preprocessing_args, args)\n\n    def test_validate_fails_for_invalid_file(self):\n        """""" We expect a ValueError when a filepath is invalid """"""\n        args = self.get_common_data_args_namespace()\n        args.train_source_text_file = ""nonexistent_file_path""\n        self.assertRaises(ValueError, options.validate_preprocessing_args, args)\n\n    def test_validate_preprocessing_args_monolingual(self):\n        """"""\n        Make sure we pass validation with the semisupervised training\n        task when the required monolingual source and target data is\n        set.\n        """"""\n        args = self.get_common_data_args_namespace()\n        args.task = constants.SEMI_SUPERVISED_TASK\n        args.train_mono_source_binary_path = test_utils.make_temp_file()\n        args.train_mono_target_text_file = test_utils.make_temp_file()\n        options.validate_preprocessing_args(args)\n\n    def test_validate_preprocessing_args_monolingual_source_only(self):\n        """"""\n        Make sure we pass validation with the semisupervised training\n        task when we only have monolingual source data.\n        """"""\n        args = self.get_common_data_args_namespace()\n        args.task = constants.SEMI_SUPERVISED_TASK\n        args.train_mono_source_binary_path = test_utils.make_temp_file()\n        options.validate_preprocessing_args(args)\n\n    def test_validate_preprocessing_args_monolingual_target_only(self):\n        """"""\n        Make sure we pass validation with the semisupervised training\n        task when we only have monolingual source data.\n        """"""\n        args = self.get_common_data_args_namespace()\n        args.task = constants.SEMI_SUPERVISED_TASK\n        args.train_mono_target_binary_path = test_utils.make_temp_file()\n        options.validate_preprocessing_args(args)\n\n    def test_validate_preprocessing_args_monolingual_fails_with_missing(self):\n        """"""\n        We expect a ValueError with the semisupervised task if there\'s\n        no monolingual data at all.\n        """"""\n        args = self.get_common_data_args_namespace()\n        args.task = constants.SEMI_SUPERVISED_TASK\n        self.assertRaises(ValueError, options.validate_preprocessing_args, args)\n'"
pytorch_translate/test/test_preprocess.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport unittest\n\nfrom pytorch_translate import constants, preprocess\nfrom pytorch_translate.data.dictionary import Dictionary\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestPreprocess(unittest.TestCase):\n    def setUp(self):\n        self.source_text_file, self.target_text_file = (\n            test_utils.create_test_text_files()\n        )\n\n    def get_common_data_args_namespace(self):\n        args = argparse.Namespace()\n\n        args.train_source_text_file = self.source_text_file\n        args.train_target_text_file = self.target_text_file\n        args.eval_source_text_file = self.source_text_file\n        args.eval_target_text_file = self.target_text_file\n\n        # The idea is to have these filled in during preprocessing\n        args.train_source_binary_path = """"\n        args.train_target_binary_path = """"\n        args.eval_source_binary_path = """"\n        args.eval_target_binary_path = """"\n\n        # Required data preprocessing args\n        args.append_eos_to_source = False\n        args.reverse_source = True\n        args.fairseq_data_format = False\n\n        args.multiling_source_lang = None  # Indicates no multilingual data\n        args.penalized_target_tokens_file = """"\n\n        args.source_vocab_file = test_utils.make_temp_file()\n        args.source_max_vocab_size = None\n        args.target_vocab_file = test_utils.make_temp_file()\n        args.target_max_vocab_size = None\n        args.char_source_vocab_file = """"\n        args.char_target_vocab_file = """"\n\n        args.task = ""pytorch_translate""\n        return args\n\n    def test_build_vocabs_char(self):\n        args = self.get_common_data_args_namespace()\n        args.arch = ""char_aware_hybrid""\n        args.char_source_max_vocab_size = 30\n        args.char_target_max_vocab_size = 30\n        args.char_source_vocab_file = test_utils.make_temp_file()\n        args.char_target_vocab_file = test_utils.make_temp_file()\n        dictionaries = preprocess.build_vocabs(args, Dictionary)\n        assert len(dictionaries[""char_source_dict""]) > 0\n        assert len(dictionaries[""char_target_dict""]) > 0\n\n    def test_build_vocabs_no_char(self):\n        args = self.get_common_data_args_namespace()\n        args.arch = ""rnn""\n        dictionaries = preprocess.build_vocabs(args, Dictionary)\n        dictionaries[""char_source_dict""] is None\n        dictionaries[""char_target_dict""] is None\n\n    def test_preprocess(self):\n        """"""\n        This is just a correctness test to make sure no errors are thrown when\n        all the required args are passed. Actual parsing code is tested by\n        test_data.py\n        """"""\n        args = self.get_common_data_args_namespace()\n        preprocess.preprocess_corpora(args)\n        for file_type in (\n            ""train_source_binary_path"",\n            ""train_target_binary_path"",\n            ""eval_source_binary_path"",\n            ""eval_target_binary_path"",\n        ):\n            file = getattr(args, file_type)\n            assert file and os.path.isfile(file)\n            assert file.endswith("".npz"")\n\n    def test_preprocess_with_monolingual(self):\n        """"""\n        This is just a correctness test to make sure no errors are thrown when\n        all the required args are passed. Actual parsing code is tested by\n        test_data.py\n        """"""\n        args = self.get_common_data_args_namespace()\n        args.task = constants.SEMI_SUPERVISED_TASK\n        args.train_mono_source_text_file = self.source_text_file\n        args.train_mono_target_text_file = self.target_text_file\n        preprocess.preprocess_corpora(args)\n        for file_type in (\n            ""train_source_binary_path"",\n            ""train_target_binary_path"",\n            ""eval_source_binary_path"",\n            ""eval_target_binary_path"",\n            ""train_mono_source_binary_path"",\n            ""train_mono_target_binary_path"",\n        ):\n            file_path = getattr(args, file_type)\n            assert file_path and os.path.isfile(file_path)\n            assert file_path.endswith("".npz"")\n\n    def test_preprocess_with_monolingual_with_tgt_chars(self):\n        """"""\n        This is just a correctness test to make sure no errors are thrown when\n        all the required args are passed. Actual parsing code is tested by\n        test_data.py\n        """"""\n        args = self.get_common_data_args_namespace()\n        args.task = constants.SEMI_SUPERVISED_TASK\n        args.train_mono_source_text_file = self.source_text_file\n        args.train_mono_target_text_file = self.target_text_file\n        args.arch = ""char_aware_hybrid""\n        args.char_source_max_vocab_size = 30\n        args.char_target_max_vocab_size = 30\n        args.char_source_vocab_file = test_utils.make_temp_file()\n        args.char_target_vocab_file = test_utils.make_temp_file()\n\n        preprocess.preprocess_corpora(args)\n        for file_type in (\n            ""train_source_binary_path"",\n            ""train_target_binary_path"",\n            ""eval_source_binary_path"",\n            ""eval_target_binary_path"",\n            ""train_mono_source_binary_path"",\n            ""train_mono_target_binary_path"",\n        ):\n            file_path = getattr(args, file_type)\n            assert file_path and os.path.isfile(file_path)\n            assert file_path.endswith("".npz"")\n'"
pytorch_translate/test/test_semi_supervised_task.py,0,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom itertools import zip_longest\n\nfrom pytorch_translate.tasks.semi_supervised_task import PytorchTranslateSemiSupervised\n\n\nclass TestSemiSupervisedTask(unittest.TestCase):\n    def test_parse_loss_weights(self):\n        """""" Make sure we can decode a loss_weights json. """"""\n        loss_weights_json = """"""[\n            [5, {\'src-tgt\': 1, \'src-tgt_mono\': 0, \'tgt-src\': 1, \'tgt-src_mono\': 0}],\n            [5, {\'src-tgt\': 1, \'src-tgt_mono\': 0.5, \'tgt-src\': 1, \'tgt-src_mono\': 0.5}],\n            [100, {\'src-tgt\': 1, \'src-tgt_mono\': 1, \'tgt-src\': 1, \'tgt-src_mono\': 1}]\n        ]""""""\n        expected_loss_weights = [\n            (5, {""src-tgt"": 1, ""src-tgt_mono"": 0, ""tgt-src"": 1, ""tgt-src_mono"": 0}),\n            (5, {""src-tgt"": 1, ""src-tgt_mono"": 0.5, ""tgt-src"": 1, ""tgt-src_mono"": 0.5}),\n            (100, {""src-tgt"": 1, ""src-tgt_mono"": 1, ""tgt-src"": 1, ""tgt-src_mono"": 1}),\n        ]\n        parsed_loss_weights = PytorchTranslateSemiSupervised.parse_loss_weights(\n            loss_weights_json=loss_weights_json\n        )\n        for parsed_schedule, expected_schedule in zip_longest(\n            parsed_loss_weights, expected_loss_weights\n        ):\n            self.assertEquals(parsed_schedule[0], expected_schedule[0])\n            self.assertDictEqual(parsed_schedule[1], expected_schedule[1])\n'"
pytorch_translate/test/test_train.py,6,"b'#!/usr/bin/env python3\n\nimport os\nimport unittest\n\nimport numpy as np\nimport torch\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate import train\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestRNNModel(unittest.TestCase):\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""No GPU available for test."")\n    def test_gpu_train_step(self):\n        test_args = test_utils.ModelParamsDict()\n        trainer, _ = test_utils.gpu_train_step(test_args)\n        assert trainer.get_meter(""gnorm"").avg > 0\n\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""No GPU available for test."")\n    def test_gpu_freeze_embedding(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_freeze_embed=True, decoder_freeze_embed=True\n        )\n        test_utils.gpu_train_step(test_args)\n\n    def test_load_pretrained_embedding(self):\n        test_args = test_utils.ModelParamsDict()\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        encoder_embed_path, embed_weights = test_utils.create_pretrained_embed(\n            src_dict, test_args.encoder_hidden_dim\n        )\n        test_args.encoder_pretrained_embed = encoder_embed_path\n        task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        model = task.build_model(test_args)\n        assert np.allclose(\n            model.encoder.embed_tokens.weight.data.numpy(), embed_weights\n        )\n        os.remove(encoder_embed_path)\n\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""No GPU available for test."")\n    def test_milstm_cell(self):\n        test_args = test_utils.ModelParamsDict(cell_type=""milstm"")\n        trainer, _ = test_utils.gpu_train_step(test_args)\n        assert trainer.get_meter(""gnorm"").avg > 0\n\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""No GPU available for test."")\n    def test_sequence_lstm_encoder(self):\n        test_args = test_utils.ModelParamsDict(\n            encoder_bidirectional=True, sequence_lstm=True\n        )\n        trainer, _ = test_utils.gpu_train_step(test_args)\n        assert trainer.get_meter(""gnorm"").avg > 0\n\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""No GPU available for test."")\n    def test_layer_norm_lstm_cell(self):\n        test_args = test_utils.ModelParamsDict(cell_type=""layer_norm_lstm"")\n        trainer, _ = test_utils.gpu_train_step(test_args)\n        assert trainer.get_meter(""gnorm"").avg > 0\n\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""No GPU available for test."")\n    def test_first_layer_multihead_attention_(self):\n        test_args = test_utils.ModelParamsDict(\n            attention_type=""multihead"", attention_heads=2, first_layer_attention=True\n        )\n        trainer, _ = test_utils.gpu_train_step(test_args)\n        assert trainer.get_meter(""gnorm"").avg > 0\n'"
pytorch_translate/test/test_utils.py,12,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy.testing as npt\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_translate import utils as pytorch_utils\n\n\nclass TestAverageTensors(unittest.TestCase):\n    def test_mean(self):\n        a = torch.Tensor([[0.0, 2.0, 5.0], [5.0, -5.0, 6.0]])\n        b = torch.Tensor([[4.0, 2.0, -1.0], [5.0, 10.0, 6.0]])\n        c = torch.Tensor([[-1.0, 2.0, 5.0], [2.0, 10.0, 6.0]])\n        expected = torch.Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n        npt.assert_allclose(pytorch_utils.average_tensors([a, b, c]), expected)\n        third = 1.0 / 3.0\n        npt.assert_allclose(\n            pytorch_utils.average_tensors([a, b, c], weights=[third, third, third]),\n            expected,\n        )\n        npt.assert_allclose(\n            pytorch_utils.average_tensors([a, b, c], weights=[1, 1, 1]), 3 * expected\n        )\n        npt.assert_allclose(\n            pytorch_utils.average_tensors([a, b, c], weights=[1, 0, 0]), a\n        )\n\n    def test_prob_space(self):\n        a = torch.Tensor([[5.0, 5.0], [6.0, 2.0], [2.0, 6.0]])\n        b = torch.Tensor([[0.0, 0.0], [2.0, 6.0], [6.0, 2.0]])\n        expected = torch.Tensor([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]])\n        npt.assert_allclose(\n            pytorch_utils.average_tensors([a, b], norm_fn=F.softmax), expected\n        )\n\n\nclass TestMaybeCat(unittest.TestCase):\n    def test_cat(self):\n        a = torch.IntTensor([[1, 2, 3], [4, 5, 6]])\n        b = torch.IntTensor([[11, 12, 13], [14, 15, 16]])\n        ab = torch.IntTensor([[1, 2, 3, 11, 12, 13], [4, 5, 6, 14, 15, 16]])\n        npt.assert_array_equal(pytorch_utils.maybe_cat([a, b], dim=1), ab)\n        npt.assert_array_equal(\n            pytorch_utils.maybe_cat([a, None, b, None, None], dim=1), ab\n        )\n        npt.assert_array_equal(pytorch_utils.maybe_cat([None, None, a, None], dim=1), a)\n\n    def test_nullable(self):\n        a = torch.IntTensor([[1, 2, 3], [4, 5, 6]])\n        pytorch_utils.maybe_cat([a, None], 1)\n        pytorch_utils.maybe_cat([a, None], 1, nullable=[True, True])\n        pytorch_utils.maybe_cat([a, None], 1, nullable=[False, True])\n        with self.assertRaises(RuntimeError):\n            pytorch_utils.maybe_cat([a, None], 1, nullable=[False, False])\n        with self.assertRaises(RuntimeError):\n            pytorch_utils.maybe_cat([None, None], 1)\n        with self.assertRaises(RuntimeError):\n            pytorch_utils.maybe_cat([], 1)\n'"
pytorch_translate/test/test_vocab_reduction.py,0,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy as np\nfrom pytorch_translate import vocab_reduction\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestVocabReduction(unittest.TestCase):\n    def test_get_translation_candidates(self):\n        lexical_dictionaries = test_utils.create_lexical_dictionaries()\n        src_dict, dst_dict = test_utils.create_vocab_dictionaries()\n        translation_candidates = vocab_reduction.get_translation_candidates(\n            src_dict=src_dict,\n            dst_dict=dst_dict,\n            lexical_dictionaries=lexical_dictionaries,\n            num_top_words=10,\n            max_translation_candidates_per_word=1,\n        )\n\n        translation_candidates_ref = test_utils.create_vocab_reduction_expected_array(\n            src_dict\n        )\n        assert translation_candidates.size != 0\n        np.testing.assert_array_equal(\n            translation_candidates, translation_candidates_ref\n        )\n'"
pytorch_translate/test/utils.py,10,"b'#!/usr/bin/env python3\n\nimport codecs\nimport os\nimport random\nimport tempfile\nfrom typing import Any, Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom fairseq import data, options\nfrom fairseq.trainer import Trainer\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate import generate, models, train, vocab_constants\nfrom pytorch_translate.data import (\n    data as pytorch_translate_data,\n    dictionary as pytorch_translate_dictionary,\n)\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\n\n\nclass ModelParamsDict:\n    def __init__(self, arch=""rnn"", **kwargs):\n        # Model params\n        if arch == ""transformer"":\n            self.arch = ""ptt_transformer""\n            self.encoder_embed_dim = 10\n            self.encoder_ffn_embed_dim = 16\n            self.encoder_layers = 2\n            self.encoder_attention_heads = 2\n            self.decoder_embed_dim = 10\n            self.decoder_ffn_embed_dim = 16\n            self.decoder_layers = 2\n            self.decoder_attention_heads = 2\n            self.aan = False\n        elif arch == ""hybrid_transformer_rnn"":\n            self.arch = ""hybrid_transformer_rnn""\n            self.encoder_embed_dim = 6\n            self.encoder_ffn_embed_dim = 16\n            self.encoder_layers = 2\n            self.encoder_attention_heads = 2\n            self.decoder_embed_dim = 10\n            self.decoder_lstm_units = 13\n            self.decoder_layers = 2\n            self.decoder_attention_heads = 2\n        elif arch == ""char_source_transformer"":\n            self.arch = ""char_source_transformer""\n            self.encoder_embed_dim = 10\n            self.encoder_ffn_embed_dim = 16\n            self.encoder_layers = 2\n            self.encoder_attention_heads = 2\n            self.decoder_embed_dim = 10\n            self.decoder_ffn_embed_dim = 16\n            self.decoder_layers = 2\n            self.decoder_attention_heads = 2\n        elif arch == ""char_source_hybrid"":\n            self.arch = ""char_source_hybrid""\n            self.encoder_embed_dim = 6\n            self.encoder_ffn_embed_dim = 16\n            self.encoder_layers = 2\n            self.encoder_attention_heads = 2\n            self.decoder_embed_dim = 10\n            self.decoder_lstm_units = 13\n            self.decoder_layers = 2\n            self.decoder_attention_heads = 2\n        elif arch == ""char_aware_hybrid"":\n            self.arch = ""char_aware_hybrid""\n            self.attention_dropout = 0\n            self.decoder_embed_dim = 10\n            self.decoder_lstm_units = 13\n            self.decoder_layers = 2\n            self.decoder_attention_heads = 2\n            self.dropout = 0\n            self.decoder_reduced_attention_dim = None\n            self.decoder_out_embed_dim = None\n            self.encoder_embed_dim = 6\n            self.encoder_ffn_embed_dim = 16\n            self.encoder_layers = 2\n            self.encoder_attention_heads = 2\n        elif arch == ""dual_decoder_kd"":\n            self.arch = ""dual_decoder_kd""\n            self.encoder_embed_dim = 10\n            self.encoder_ffn_embed_dim = 16\n            self.encoder_layers = 2\n            self.encoder_attention_heads = 2\n            self.decoder_embed_dim = 10\n            self.decoder_ffn_embed_dim = 16\n            self.decoder_layers = 2\n            self.decoder_attention_heads = 2\n            self.student_decoder_embed_dim = 5\n            self.student_decoder_layers = 2\n            self.student_decoder_attention_heads = 2\n            self.student_decoder_lstm_units = 7\n            self.student_decoder_out_embed_dim = 4\n            self.student_decoder_reduced_attention_dim = 8\n        elif arch == ""hybrid_dual_decoder_kd"":\n            self.arch = ""hybrid_dual_decoder_kd""\n            self.encoder_embed_dim = 10\n            self.encoder_ffn_embed_dim = 16\n            self.encoder_layers = 2\n            self.encoder_attention_heads = 2\n            self.decoder_embed_dim = 10\n            self.decoder_lstm_units = 16\n            self.decoder_layers = 2\n            self.decoder_attention_heads = 2\n            self.student_decoder_embed_dim = 5\n            self.student_decoder_layers = 2\n            self.student_decoder_attention_heads = 2\n            self.student_decoder_lstm_units = 7\n            self.student_decoder_out_embed_dim = 4\n            self.student_decoder_reduced_attention_dim = 8\n        elif arch == ""latent_var_transformer"":\n            self.arch = ""latent_var_transformer""\n            self.decoder_attention_heads = 8\n            self.decoder_embed_dim = 512\n            self.decoder_ffn_embed_dim = 2048\n            self.decoder_input_dim = 512\n            self.decoder_layers = 6\n            self.decoder_out_embed_dim = 512\n            self.decoder_output_dim = 512\n            self.encoder_attention_heads = 8\n            self.encoder_embed_dim = 512\n            self.encoder_ffn_embed_dim = 2048\n            self.encoder_layers = 6\n        else:\n            self.arch = ""rnn""\n            self.encoder_embed_dim = 10\n            self.encoder_embed_path = None\n            self.encoder_freeze_embed = False\n            self.encoder_hidden_dim = 10\n            self.encoder_layers = 2\n            self.encoder_bidirectional = False\n            self.encoder_dropout_in = 0\n            self.encoder_dropout_out = 0\n            self.encoder_context_embed = False\n            self.decoder_embed_dim = 10\n            self.decoder_embed_path = None\n            self.decoder_freeze_embed = False\n            self.decoder_hidden_dim = 10\n            self.decoder_out_embed_dim = 5\n            self.decoder_out_embed_path = None\n            self.out_embed_norm = None\n            self.decoder_layers = 2\n            self.dropout = 0\n            self.decoder_dropout_in = 0\n            self.decoder_dropout_out = 0\n            self.attention_type = ""dot""\n            self.attention_heads = 8\n            self.first_layer_attention = False\n            self.residual_level = None\n            self.averaging_encoder = False\n            self.cell_type = ""lstm""\n            self.sequence_lstm = False\n            self.decoder_tie_embeddings = False\n            self.language_model_only = False\n        # Training params\n        self.unk_only_char_encoding = False\n        self.criterion = ""cross_entropy""\n        self.lr = [0.1]\n        self.optimizer = ""sgd""\n        self.momentum = 0\n        self.label_smoothing = None\n        self.weight_decay = 0.0\n        self.lr_scheduler = ""fixed""\n        self.force_anneal = 0\n        self.lr_shrink = 0\n        self.sentence_avg = True\n        self.clip_norm = 5.0\n        self.batch_size = 4\n        self.vocab_reduction_params = None\n        self.distributed_world_size = 1\n        self.seed = 1\n        self.left_pad_source = False\n        self.fp16 = False\n        self.cpu = None\n        self.reverse_source = False\n        self.append_eos_to_source = False\n        self.word_reward = 0.0\n        self.length_penalty = 0.0\n        self.use_bmuf = False\n        self.no_save_optimizer_state = False\n        self.fast_stat_sync = False\n        self.empty_cache_freq = 0\n        # Rescoring params\n        self.enable_rescoring = False\n        self.l2r_model_path = None\n        self.l2r_model_weight = None\n        self.enable_r2l_rescoring = False\n        self.r2l_model_path = None\n        self.r2l_model_weight = None\n        self.enable_reverse_rescoring = False\n        self.reverse_model_path = None\n        self.reverse_model_weight = None\n        self.cloze_transformer_weight = None\n        self.enable_lm_rescoring = False\n        self.lm_model_path = None\n        self.lm_model_weight = None\n        self.cloze_transformer_path = None\n        self.cloze_transformer_weight = None\n        # Modified params\n        for param, value in kwargs.items():\n            assert hasattr(\n                self, param\n            ), f""Tried to specify value for nonexistent property {param}.""\n            self.__setattr__(param, value)\n\n\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return len(self.data)\n\n\ndef dummy_dictionary(\n    dummy_tokens=3,\n    additional_token_list=None,\n    dictionary_cls=pytorch_translate_dictionary.Dictionary,\n):\n    """"""First adds the amount of dummy_tokens that you specify, then\n    finally the additional_token_list, which is a list of string token values""""""\n    d = dictionary_cls()\n    for i in range(dummy_tokens):\n        token = f""token_{i}""\n        d.add_symbol(token)\n    if additional_token_list is not None:\n        for token in additional_token_list:\n            d.add_symbol(token)\n    d.finalize(padding_factor=-1)\n    return d\n\n\ndef prepare_inputs(\n    test_args, source_vocab_size=103, target_vocab_size=103, is_variable_seqlen=False\n):\n    # first 100 indices are reserved for special tokens\n    src_dict = dummy_dictionary(dummy_tokens=source_vocab_size - 100)\n    tgt_dict = dummy_dictionary(dummy_tokens=source_vocab_size - 100)\n\n    def get_single_example(sample_id, src_sentence_length, tgt_sentence_length):\n        non_special_start = 4\n        example = {\n            ""id"": sample_id,\n            # Note: both source and target-side sentences are expected\n            # to end in the EOS marker. LanguagePairDataset then:\n            # (1) moves the EOS to the start of the target, for input feeding\n            # (2) it also handles left (right) padding of the source (target)\n            ""source"": torch.LongTensor(\n                np.random.randint(\n                    low=non_special_start,\n                    high=len(src_dict.symbols),\n                    size=src_sentence_length,\n                ).tolist()\n                + [src_dict.eos()]\n            ),\n            ""target"": torch.LongTensor(\n                np.random.randint(\n                    low=non_special_start,\n                    high=len(tgt_dict.symbols),\n                    size=tgt_sentence_length,\n                ).tolist()\n                + [tgt_dict.eos()]\n            ),\n        }\n        return example\n\n    min_sent_len = 7\n    max_sent_len = 12\n    fixed_tgt_length = 12\n    dataset = TestDataset(\n        [\n            get_single_example(\n                example_id,\n                np.random.randint(low=min_sent_len, high=max_sent_len, size=1)\n                if is_variable_seqlen\n                else 10,\n                fixed_tgt_length,\n            )\n            for example_id in range(test_args.batch_size)\n        ]\n    )\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=test_args.batch_size,\n        collate_fn=(\n            lambda samples: data.language_pair_dataset.collate(\n                samples, src_dict.pad(), src_dict.eos()\n            )\n        ),\n    )\n    data_iterator = iter(dataloader)\n    return data_iterator, src_dict, tgt_dict\n\n\ndef create_dummy_extra_state(**kwargs):\n    extra_state = {\n        ""epoch"": 1,\n        ""batch_offset"": 0,\n        ""val_loss"": None,\n        ""start_time"": 0,\n        ""last_bleu_eval"": 0,\n    }\n    for param, value in kwargs.items():\n        assert (\n            param in extra_state\n        ), f""Tried to specify value for nonexistent property {param}.""\n        extra_state[param] = value\n    return extra_state\n\n\ndef create_lexical_dictionaries():\n    lexical_dictionary_path = write_lines_to_temp_file(\n        [\n            ""a A 0.7"",\n            ""a B 0.3"",\n            ""b C 0.1"",\n            ""b D 0.8"",\n            ""b E 0.1"",\n            ""c A 0.3"",\n            ""c B 0.4"",\n            ""c C 0.3"",\n            ""d D 0.4"",\n            ""d E 0.3"",\n            ""d A 0.2"",\n            ""d B 0.1"",\n            ""e C 1.0"",\n        ]\n    )\n    return [lexical_dictionary_path]\n\n\ndef create_pretrained_embed(dictionary, embed_dim):\n    """"""Creates a dummy embedding file in the format accepted by fairseq. An\n    embedding file has the following format: the first line has vocabulary size\n    and dimension. The following lines contain word and space-separated\n    embedding values.\n\n    Example:\n        2 5\n        the -0.0230 -0.0264  0.0287  0.0171  0.1403\n        at -0.0395 -0.1286  0.0275  0.0254 -0.0932\n\n    Arguments:\n        dictionary (fairseq.data.dictionary.Dictionary): dictionary with\n            sample tokens as entries\n        embed_dim (int): embedding dimension to be generated\n\n    Returns:\n        Path to a text file with dummy embeddings in the format described above.\n    """"""\n\n    embed_weights = np.random.random((len(dictionary), embed_dim))\n    pretrained_embed_path = write_lines_to_temp_file(\n        [""{} {}"".format(len(dictionary), embed_dim)]\n        + [\n            ""{} {}"".format(token, "" "".join([str(val) for val in embedding]))\n            for token, embedding in zip(dictionary.symbols, embed_weights)\n        ]\n    )\n    return pretrained_embed_path, embed_weights\n\n\ndef create_test_text_files():\n    src = write_lines_to_temp_file(\n        [\n            ""srcA srcB srcC srcD"",\n            ""srcA srcA srcB srcB srcC srcC"",\n            ""srcA srcA srcA srcA srcB srcB srcB srcB"",\n            ""srcA srcA srcA srcA srcA srcA srcA srcA srcA srcA"",\n        ]\n    )\n    trg = write_lines_to_temp_file(\n        [\n            ""trgA trgA trgA trgA trgA trgA trgA trgA trgA trgA"",\n            ""trgA trgA trgA trgA trgB trgB trgB trgB"",\n            ""trgA trgA trgB trgB trgC trgC"",\n            ""trgA trgB trgC trgD"",\n        ]\n    )\n    return src, trg\n\n\ndef create_test_numberized_data_files(src_ref, trg_ref, reverse_source=True):\n    """"""\n    Reformat ref from [[#, #, #], [#, #, #]] --> [""# # #"", ""# # #""]\n    """"""\n    if reverse_source:\n        src_ref = [reversed(line) for line in src_ref]\n\n    # during parsing\n    src = write_lines_to_temp_file(\n        ["" "".join([str(ind) for ind in line]) for line in src_ref]\n    )\n    trg = write_lines_to_temp_file(\n        ["" "".join([str(ind) for ind in line]) for line in trg_ref]\n    )\n    return src, trg\n\n\ndef write_lines_to_temp_file(lines):\n    temp_file_path = make_temp_file()\n    with codecs.open(temp_file_path, ""w"", ""utf-8"") as temp_file:\n        temp_file.write(""\\n"".join(lines) + ""\\n"")\n    return temp_file_path\n\n\ndef make_temp_file():\n    temp_file = tempfile.NamedTemporaryFile(mode=""w"", delete=False, dir=""/tmp"")\n    temp_file_path = temp_file.name\n    temp_file.close()\n    return temp_file_path\n\n\ndef create_vocab_dictionaries():\n    additional_special_tokens = (\n        vocab_constants.MAX_SPECIAL_TOKENS\n        - pytorch_translate_dictionary.Dictionary().nspecial\n    )\n    src_dict = dummy_dictionary(\n        dummy_tokens=additional_special_tokens,\n        additional_token_list=[""a"", ""b"", ""c"", ""d"", ""e""],\n    )\n    tgt_dict = dummy_dictionary(\n        dummy_tokens=additional_special_tokens,\n        additional_token_list=[""A"", ""B"", ""C"", ""D"", ""E""],\n    )\n    return src_dict, tgt_dict\n\n\ndef create_vocab_reduction_expected_array(\n    src_dict, max_translation_candidates_per_word=1\n):\n    expected_translation_candidates = np.zeros(\n        [len(src_dict), max_translation_candidates_per_word], dtype=np.int32\n    )\n\n    expected_translation_candidates[100][0] = 100\n    expected_translation_candidates[101][0] = 103\n    expected_translation_candidates[102][0] = 101\n    expected_translation_candidates[103][0] = 103\n    expected_translation_candidates[104][0] = 102\n\n    return expected_translation_candidates\n\n\ndef gpu_train_step(test_args: ModelParamsDict) -> Tuple[Trainer, Dict[Any, Any]]:\n    """"""Sets up inputs from test_args then executes a single train step. A train\n    step always requires a GPU.""""""\n    samples, src_dict, tgt_dict = prepare_inputs(test_args)\n    task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n    model = task.build_model(test_args)\n    criterion = task.build_criterion(test_args)\n    sample = next(samples)\n    trainer = Trainer(test_args, task, model, criterion)\n    logging_dict = trainer.train_step([sample])\n    return trainer, logging_dict\n\n\ndef write_dummy_file(filename, num_examples, maxlen):\n    rng_state = torch.get_rng_state()\n    torch.manual_seed(0)\n    data = torch.rand(num_examples * maxlen)\n    data = 97 + torch.floor(26 * data).int()\n    with open(filename, ""w"") as h:\n        offset = 0\n        for _ in range(num_examples):\n            ex_len = random.randint(1, maxlen)\n            ex_str = "" "".join(map(chr, data[offset : offset + ex_len]))\n            print(ex_str, file=h)\n            offset += ex_len\n    torch.set_rng_state(rng_state)\n\n\ndef create_dummy_data(data_dir, num_examples=100, maxlen=5):\n    def _create_dummy_data(filename):\n        write_dummy_file(os.path.join(data_dir, filename), num_examples, maxlen)\n\n    _create_dummy_data(""train.in"")\n    _create_dummy_data(""train.out"")\n    _create_dummy_data(""valid.in"")\n    _create_dummy_data(""valid.out"")\n    _create_dummy_data(""test.in"")\n    _create_dummy_data(""test.out"")\n\n\ndef create_dummy_multilingual_data(data_dir, num_examples=100, maxlen=5):\n    def _create_dummy_data(filename):\n        write_dummy_file(os.path.join(data_dir, filename), num_examples, maxlen)\n\n    for src, tgt in [(""xh"", ""en""), (""zu"", ""en"")]:\n        langpair = src + tgt\n        _create_dummy_data(f""train.{langpair}.{src}"")\n        _create_dummy_data(f""train.{langpair}.{tgt}"")\n        _create_dummy_data(f""tune.{langpair}.{src}"")\n        _create_dummy_data(f""tune.{langpair}.{tgt}"")\n\n\ndef generate_main(data_dir, extra_flags=None):\n    parser = generate.get_parser_with_args()\n    args = options.parse_args_and_arch(\n        parser,\n        [\n            ""--source-vocab-file"",\n            os.path.join(data_dir, ""dictionary-in.txt""),\n            ""--target-vocab-file"",\n            os.path.join(data_dir, ""dictionary-out.txt""),\n            ""--source-text-file"",\n            os.path.join(data_dir, ""test.in""),\n            ""--target-text-file"",\n            os.path.join(data_dir, ""test.out""),\n            ""--path"",\n            os.path.join(data_dir, ""checkpoint_last.pt""),\n            ""--beam"",\n            ""3"",\n            ""--length-penalty"",\n            ""0.0"",\n            ""--batch-size"",\n            ""64"",\n            ""--max-len-b"",\n            ""5"",\n            ""--no-progress-bar"",\n        ]\n        + (extra_flags or []),\n    )\n    generate.validate_args(args)\n    generate.generate(args)\n\n\ndef train_translation_model(\n    data_dir,\n    extra_flags,\n    criterion=None,\n    set_empty_data_positional_arg=False,\n    set_lang_args=True,\n    save_dir: str = None,\n):\n    parser = train.get_parser_with_args()\n    args = options.parse_args_and_arch(\n        parser,\n        ([""""] if set_empty_data_positional_arg else [])\n        + [\n            ""--save-dir"",\n            save_dir if save_dir else data_dir,\n            ""--train-source-text-file"",\n            os.path.join(data_dir, ""train.in""),\n            ""--train-target-text-file"",\n            os.path.join(data_dir, ""train.out""),\n            ""--eval-source-text-file"",\n            os.path.join(data_dir, ""valid.in""),\n            ""--eval-target-text-file"",\n            os.path.join(data_dir, ""valid.out""),\n            ""--source-max-vocab-size"",\n            ""26"",\n            ""--target-max-vocab-size"",\n            ""26"",\n            ""--max-tokens"",\n            ""500"",\n            ""--optimizer"",\n            ""sgd"",\n            ""--lr"",\n            ""0.05"",\n            ""--lr-scheduler"",\n            ""fixed"",\n            ""--lr-shrink"",\n            ""0.95"",\n            ""--momentum"",\n            ""0.0"",\n            ""--clip-norm"",\n            ""5.0"",\n            ""--sentence-avg"",\n            ""--beam"",\n            ""3"",\n            ""--stop-no-best-bleu-eval"",\n            ""5"",\n            ""--unk-reward"",\n            ""0.5"",\n            ""--num-avg-checkpoints"",\n            ""10"",\n            ""--max-epoch"",\n            ""1"",\n            ""--stop-time-hr"",\n            ""1"",\n            ""--no-progress-bar"",\n            ""--distributed-world-size"",\n            ""1"",\n            ""--local-num-gpus"",\n            ""1"" if torch.cuda.device_count() >= 1 else ""0"",\n        ]\n        + ([""--source-lang"", ""in"", ""--target-lang"", ""out""] if set_lang_args else [])\n        + (extra_flags or [])\n        + (\n            criterion\n            or [\n                ""--criterion"",\n                ""label_smoothed_cross_entropy"",\n                ""--label-smoothing"",\n                ""0.1"",\n            ]\n        ),\n    )\n    train.validate_and_set_default_args(args)\n    train.main(args)\n\n\ndef create_dummy_binarized_dataset(\n    num_sentences=13, min_length=5, max_length=10, append_eos=False\n):\n    index_sequences = []\n    for _ in range(num_sentences):\n        sequence = [\n            np.random.randint(100, 103)\n            for _ in range(np.random.randint(min_length, max_length + 1))\n        ]\n        if append_eos:\n            sequence.append(vocab_constants.EOS_ID)\n        index_sequences.append(sequence)\n\n    dataset = pytorch_translate_data.InMemoryIndexedDataset()\n    dataset.load_from_sequences(index_sequences)\n    return dataset\n'"
pytorch_translate/word_prediction/__init__.py,0,b''
pytorch_translate/word_prediction/word_prediction_criterion.py,11,"b'#!/usr/bin/env python3\n\nimport abc\nimport math\n\nimport torch\nfrom fairseq import utils\nfrom fairseq.criterions import register_criterion\nfrom fairseq.criterions.label_smoothed_cross_entropy import (\n    LabelSmoothedCrossEntropyCriterion,\n)\nfrom pytorch_translate.utils import maybe_cuda\n\n\nclass _BasePredictionCriterion(abc.ABC, LabelSmoothedCrossEntropyCriterion):\n    """"""Base class for the losses in order to combine commonly used methods.""""""\n\n    @abc.abstractmethod\n    def predictor_loss_function(self, prediction, target, *args, **kwargs):\n        """"""Pure abstract method that computes the loss.\n\n        Args:\n            prediction: Prediction that was made by the model of shape\n                        [BATCH_SIZE, N_LABELS]\n            target: Expected result of shape [BATCH_SIZE, N_OUTPUT_TOKENS]\n        Returns:\n            loss: This method should return the loss as a Tensor or Variable.\n        """"""\n        return torch.Tensor(float(""Inf""))\n\n    def forward(self, model, sample, reduce=True, *args, **kwargs):\n        """"""Computes the loss for the given sample.\n\n        This method uses the inheriting classes\' `predictor_loss_function`.\n\n        Args:\n            model: Model to use for the loss computation.\n            sample: Chosen sample as a dict with at least the following keys:\n                    \'net_input\', \'target\', \'ntokens\'\n            reduce: Boolean flag to reduce the result to per batch elements.\n            args, kwargs: Positional/Keyword arguments are passed through to the\n                          `predictor_loss_function`.\n        Returns:\n            loss: Total loss as a Variable\n            sample_size: Sample size - used for the gradient denominator.\n            logging_output: Logging outputs to display during training.\n\n        Raises:\n            AssertionError:\n                - prediction and target batch numbers are different\n                - prediction shape is not [BATCH_SIZE, N_LABELS]\n                - losses for translation and prediction are not the same shape\n        """"""\n        predictor_output, decoder_output = model(**sample[""net_input""])\n        # translation loss\n        translation_loss, nll_loss = super().compute_loss(\n            model, decoder_output, sample, reduce\n        )\n        prediction_target = model.get_target_words(sample)\n        # predictor loss\n        prediction_lprobs = model.get_predictor_normalized_probs(\n            predictor_output, log_probs=True\n        )\n        prediction_lprobs = prediction_lprobs.view(-1, prediction_lprobs.size(-1))\n\n        assert prediction_lprobs.size(0) == prediction_target.size(0)\n        assert prediction_lprobs.dim() == 2\n\n        prediction_loss = self.predictor_loss_function(\n            prediction_lprobs, prediction_target, *args, **kwargs\n        )\n\n        # prevent domination of padding idx\n        non_pad_mask = prediction_target.ne(model.encoder.padding_idx)\n        prediction_loss = prediction_loss[non_pad_mask]\n\n        # TODO: normalize , sentence avg\n        if reduce:\n            prediction_loss = prediction_loss.sum()\n        else:\n            prediction_loss = prediction_loss.sum(1)  # loss per batch element\n\n        assert translation_loss.size() == prediction_loss.size()\n        loss = translation_loss + prediction_loss\n\n        if self.sentence_avg:\n            sample_size = sample[""target""].size(0)\n        else:\n            sample_size = sample[""ntokens""]\n\n        logging_output = {\n            ""nll_loss"": nll_loss,\n            ""translation_loss"": translation_loss.data,\n            ""prediction_loss"": prediction_loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n        }\n\n        if reduce:\n            logging_output[""translation_loss""] = utils.item(\n                logging_output[""translation_loss""]\n            )\n            logging_output[""prediction_loss""] = utils.item(\n                logging_output[""prediction_loss""]\n            )\n            logging_output[""nll_loss""] = utils.item(logging_output[""nll_loss""])\n        logging_output[""loss""] = utils.item(logging_output[""translation_loss""])\n\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregates logging outputs from data parallel training.\n\n        Args:\n            logging_outputs: Output log with \'translation_loss\' and\n                             \'prediction_loss\'.\n        Returns:\n            agg_output: Aggregated logs.\n        """"""\n        ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n        nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n        sample_size = sum(log.get(""sample_size"", 0) for log in logging_outputs)\n        agg_output = {\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""sample_size"": sample_size,\n        }\n\n        for loss in [""translation_loss"", ""prediction_loss""]:\n            loss_sum = sum(log.get(loss, 0) for log in logging_outputs)\n\n            agg_output[loss] = loss_sum / sample_size / math.log(2)\n            if loss == ""translation_loss"" and sample_size != ntokens:\n                agg_output[""nll_loss""] = loss_sum / ntokens / math.log(2)\n\n        return agg_output\n\n\n@register_criterion(""word_prediction"")\nclass WordPredictionCriterion(_BasePredictionCriterion):\n    """"""Implements a combined loss from translation and target words prediction.\n    """"""\n\n    def predictor_loss_function(self, prediction, target):\n        """"""Loss function that maximizes the confidence of the true positive.\n\n        Args:\n            prediction: Prediction that was made by the model of shape\n                        [BATCH_SIZE, N_LABELS]\n            target: Expected result of shape [BATCH_SIZE, N_OUTPUT_TOKENS]\n\n        Returns:\n            loss: Loss as a torch.Variable\n        """"""\n        return -prediction.gather(dim=-1, index=target)\n\n\n@register_criterion(""warp_loss"")\nclass WARPLossCriterion(_BasePredictionCriterion):\n    """"""Implements a combined loss from translation and target words prediction.\n\n    References:\n      [1] https://research.google.com/pubs/archive/37180.pdf\n      [2] https://arxiv.org/abs/1708.01771\n      [3] https://discuss.pytorch.org/t/writing-warp-loss-layer/3715\n\n    TODO(T38581791):\n      Although this implementation is faster than [3] (3s/it vs 70s/it), it is\n      still much slower than `WordPredictionCriterion`.\n      Need to explore ways to speed it up -- it looks like the training time\n      increases after each iteration. This happens because it becomes harder\n      to search after each update.\n    """"""\n\n    def __init__(self, task, sentence_avg, label_smoothing):\n        super().__init__(task, sentence_avg, label_smoothing)\n        self.rank_weights = 0.0\n\n    def set_rank_weights(self, n_labels, rank_weights_type=""uniform""):\n        """"""Sets ranking for weights based on the number of labels.\n\n        Args:\n            n_labels: Number of labels\n            rank_weights_type: Type of the ranking.\n\n        Raises:\n            AssertionError: Number of labels <= 1\n            NotImplementedError: rank_weights_type is not \'uniform\'\n        """"""\n        assert n_labels > 1\n        if rank_weights_type == ""uniform"":\n            self.rank_weights = 1.0 / (n_labels - 1) * maybe_cuda(torch.ones(n_labels))\n        else:\n            raise NotImplementedError(\n                ""Rank weights type {} not implemented"".format(rank_weights_type)\n            )\n\n    def predictor_loss_function(self, prediction, target, rank_weights_type=""uniform""):\n        """"""Implements the WARP loss given in [1].\n\n        In its core the function computes the following:\n            loss = (X-1)/N*(xn_i - xp),\n        where `xn_i` is confidence of the ith false positive, and `xp` is the\n        true positive confidence. `X` is the total number of labels and `N` is\n        the number of steps that it takes to find a false positive.\n        Note: We might want to use ln((X-1)/N), in case N << X, which would\n              expolode the loss.\n\n        Args:\n            prediction: Prediction that was made by the model of shape\n                        [BATCH_SIZE, N_LABELS]\n            target: Expected result of shape [BATCH_SIZE, N_OUTPUT_TOKENS]\n            rank_weight_type: Argument to set the ranks of the weights.\n                              See `set_rank_weights` for more details.\n\n        Returns:\n            loss: Loss as a torch.Variable\n        """"""\n        batch_size = prediction.size()[0]\n        n_labels = prediction.size()[1]\n        n_output_tokens = target.size()[1]\n        max_num_trials = n_labels - 1\n\n        self.set_rank_weights(n_labels, rank_weights_type)\n\n        loss = maybe_cuda(torch.zeros(batch_size, n_output_tokens))\n\n        for i in range(batch_size):\n            for j in range(n_output_tokens):\n                target_idx = target[i, j]\n                neg_labels_idx = maybe_cuda(\n                    torch.tensor(\n                        list(set(range(n_labels)) - set(target[i, :].cpu().numpy()))\n                    )\n                )\n                neg_idx = torch.multinomial(neg_labels_idx.double(), 1)\n                # This is the hinge loss:\n                # sample_score_margin = \\\n                #   1 - prediction[i, target_idx] + prediction[i, neg_idx]\n                # TODO:\n                #   Since |- prediction[i, target_idx] + prediction[i, neg_idx]|\n                #   is normally around 0.01, directly using log probability in\n                #   hinge loss causes most N to be 1, thus is not a good choice.\n                # Observation: translation_loss is normally ~10, similar to\n                #              log_probs.\n                # Alternatives: scale up score difference by 100 times to match\n                #               the magnitude of 1, but we also need to consider\n                #               magnitude of weights and loss;\n                sample_score_margin = (\n                    -prediction[i, target_idx] + prediction[i, neg_idx]\n                )\n                N = 1\n                while sample_score_margin < 0 and N < max_num_trials:\n                    neg_idx = torch.multinomial(neg_labels_idx.double(), 1)\n                    N += 1\n                    sample_score_margin = (\n                        -prediction[i, target_idx] + prediction[i, neg_idx]\n                    )\n\n                k = torch.floor(torch.tensor(max_num_trials / N)).int()\n                weights = torch.sum(self.rank_weights[:k])\n                score_margins = -prediction[i, target_idx] + prediction[i, neg_idx]\n                loss[i, j] = (weights * score_margins.clamp(min=0.0)).mean()\n        return loss\n'"
pytorch_translate/word_prediction/word_prediction_model.py,0,"b'#!/usr/bin/env python3\n\nfrom fairseq.models import (\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom pytorch_translate import rnn\nfrom pytorch_translate.rnn import LSTMSequenceEncoder, RNNDecoder, RNNEncoder, RNNModel\nfrom pytorch_translate.utils import torch_find\nfrom pytorch_translate.word_prediction import word_predictor\n\n\nclass WordPredictionModel(FairseqEncoderDecoderModel):\n    """"""\n    An architecuture which jointly learns translation and target words\n    prediction, as described in http://aclweb.org/anthology/D17-1013.\n    """"""\n\n    def __init__(self, task, encoder, decoder, predictor):\n        super().__init__(encoder, decoder)\n        self.predictor = predictor\n        self.task = task\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens):\n        encoder_output = self.encoder(src_tokens, src_lengths)\n        pred_output = self.predictor(encoder_output)\n        decoder_output = self.decoder(prev_output_tokens, encoder_output)\n        return pred_output, decoder_output\n\n    def get_predictor_normalized_probs(self, pred_output, log_probs):\n        return self.predictor.get_normalized_probs(pred_output, log_probs)\n\n    def get_target_words(self, sample):\n        return sample[""target""]\n\n\n@register_model(""rnn_word_pred"")\nclass RNNWordPredictionModel(WordPredictionModel):\n    """"""\n    A subclass which adds words prediction to RNN arch.\n    """"""\n\n    @staticmethod\n    def add_args(parser):\n        rnn.RNNModel.add_args(parser)\n        parser.add_argument(\n            ""--predictor-hidden-dim"",\n            type=int,\n            metavar=""N"",\n            help=""word predictor num units"",\n        )\n\n        parser.add_argument(\n            ""--topk-labels-per-source-token"",\n            type=int,\n            metavar=""N"",\n            help=""Top k predicted words from the word predictor module for use""\n            ""as translation candidates in vocab reduction module, as a multiple""\n            ""of source tokens."",\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        src_dict, dst_dict = task.source_dictionary, task.target_dictionary\n        base_architecture_wp(args)\n\n        encoder_embed_tokens, decoder_embed_tokens = RNNModel.build_embed_tokens(\n            args, src_dict, dst_dict\n        )\n\n        if args.sequence_lstm:\n            encoder_class = LSTMSequenceEncoder\n        else:\n            encoder_class = RNNEncoder\n        decoder_class = RNNDecoder\n\n        encoder = encoder_class(\n            src_dict,\n            embed_tokens=encoder_embed_tokens,\n            embed_dim=args.encoder_embed_dim,\n            cell_type=args.cell_type,\n            num_layers=args.encoder_layers,\n            hidden_dim=args.encoder_hidden_dim,\n            dropout_in=args.encoder_dropout_in,\n            dropout_out=args.encoder_dropout_out,\n            residual_level=args.residual_level,\n            bidirectional=bool(args.encoder_bidirectional),\n        )\n        predictor = word_predictor.WordPredictor(\n            encoder_output_dim=args.encoder_hidden_dim,\n            hidden_dim=args.predictor_hidden_dim,\n            output_dim=len(dst_dict),\n            topk_labels_per_source_token=args.topk_labels_per_source_token,\n        )\n        decoder = decoder_class(\n            src_dict=src_dict,\n            dst_dict=dst_dict,\n            embed_tokens=decoder_embed_tokens,\n            vocab_reduction_params=args.vocab_reduction_params,\n            encoder_hidden_dim=args.encoder_hidden_dim,\n            embed_dim=args.decoder_embed_dim,\n            out_embed_dim=args.decoder_out_embed_dim,\n            cell_type=args.cell_type,\n            num_layers=args.decoder_layers,\n            hidden_dim=args.decoder_hidden_dim,\n            attention_type=args.attention_type,\n            dropout_in=args.decoder_dropout_in,\n            dropout_out=args.decoder_dropout_out,\n            residual_level=args.residual_level,\n            averaging_encoder=args.averaging_encoder,\n            predictor=None if args.topk_labels_per_source_token is None else predictor,\n        )\n\n        return cls(task, encoder, decoder, predictor)\n\n    def get_targets(self, sample, net_output):\n        targets = sample[""target""].view(-1)\n        possible_translation_tokens = net_output[-1]\n        if possible_translation_tokens is not None:\n            targets = torch_find(\n                possible_translation_tokens, targets, len(self.task.target_dictionary)\n            )\n        return targets\n\n\n@register_model_architecture(""rnn_word_pred"", ""rnn_word_pred"")\ndef base_architecture_wp(args):\n    # default architecture\n    rnn.base_architecture(args)\n    args.predictor_hidden_dim = getattr(args, ""predictor_hidden_dim"", 512)\n    args.topk_labels_per_source_token = getattr(\n        args, ""topk_labels_per_source_token"", None\n    )\n'"
pytorch_translate/word_prediction/word_predictor.py,8,"b'#!/usr/bin/env python3\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass WordPredictor(nn.Module):\n    def __init__(\n        self,\n        encoder_output_dim,\n        hidden_dim,\n        output_dim,\n        topk_labels_per_source_token=None,\n        use_self_attention=False,\n    ):\n        super().__init__()\n        self.encoder_output_dim = encoder_output_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.topk_labels_per_source_token = topk_labels_per_source_token\n        self.use_self_attention = use_self_attention\n\n        if self.use_self_attention:\n            self.init_layer = nn.Linear(encoder_output_dim, encoder_output_dim)\n            self.attn_layer = nn.Linear(2 * encoder_output_dim, 1)\n            self.hidden_layer = nn.Linear(2 * encoder_output_dim, hidden_dim)\n            self.output_layer = nn.Linear(hidden_dim, output_dim)\n        else:\n            self.hidden_layer = nn.Linear(encoder_output_dim, hidden_dim)\n            self.output_layer = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, encoder_output):\n        # [source_length, batch_size, encoder_output_dim]\n        encoder_hiddens, *_ = encoder_output\n        assert encoder_hiddens.dim()\n\n        if self.use_self_attention:\n            # [batch_size, hidden_dim]\n            init_state = self._get_init_state(encoder_hiddens)\n            # [source_length, batch_size, 1]\n            attn_scores = self._attention(encoder_hiddens, init_state)\n            # [batch_size, hidden_dim]\n            attned_state = (encoder_hiddens * attn_scores).sum(0)\n\n            pred_input = torch.cat([init_state, attned_state], 1)\n            pred_hidden = F.relu(self.hidden_layer(pred_input))\n            # [batch_size, vocab_size]\n            logits = self.output_layer(pred_hidden)\n        else:\n            # [source_length, batch_size, hidden_dim]\n            hidden = F.relu(self.hidden_layer(encoder_hiddens))\n            # [batch_size, hidden_dim]\n            mean_hidden = torch.mean(hidden, 0)\n            max_hidden = torch.max(hidden, 0)[0]\n            # [batch_size, vocab_size]\n            logits = self.output_layer(mean_hidden + max_hidden)\n\n        return logits\n\n    def _get_init_state(self, encoder_hiddens):\n        x = torch.mean(encoder_hiddens, 0)\n        x = F.relu(self.init_layer(x))\n        return x\n\n    def _attention(self, encoder_hiddens, init_state):\n        init_state = init_state.unsqueeze(0).expand_as(encoder_hiddens)\n        attn_input = torch.cat([init_state, encoder_hiddens], 2)\n        attn_scores = F.relu(self.attn_layer(attn_input))\n        attn_scores = F.softmax(attn_scores, 0)\n        return attn_scores\n\n    def get_normalized_probs(self, net_output, log_probs):\n        """"""Get normalized probabilities (or log probs) from a net\'s output.""""""\n        logits = net_output  # [batch, vocab]\n        if log_probs:\n            return F.log_softmax(logits, dim=1)\n        else:\n            return F.softmax(logits, dim=1)\n\n    def get_topk_predicted_tokens(self, net_output, src_tokens, log_probs: bool):\n        """"""\n        Get self.topk_labels_per_source_token top predicted words for vocab\n        reduction (per source token).\n        """"""\n        assert (\n            isinstance(self.topk_labels_per_source_token, int)\n            and self.topk_labels_per_source_token > 0\n        ), ""topk_labels_per_source_token must be a positive int, or None""\n\n        # number of labels to predict for each example in batch\n        k = src_tokens.size(1) * self.topk_labels_per_source_token\n        # [batch_size, vocab_size]\n        probs = self.get_normalized_probs(net_output, log_probs)\n        _, topk_indices = torch.topk(probs, k, dim=1)\n\n        return topk_indices\n'"
pytorch_translate/rescoring/test/test_model_scorers.py,57,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom unittest.mock import patch\n\nimport torch\nfrom pytorch_translate.rescoring.model_scorers import (\n    R2LModelScorer,\n    ReverseModelScorer,\n    SimpleModelScorer,\n)\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestModelScorers(unittest.TestCase):\n    def setUp(self):\n        self.args = test_utils.ModelParamsDict()\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(self.args)\n        self.task = tasks.PytorchTranslateTask(self.args, src_dict, tgt_dict)\n        self.model = self.task.build_model(self.args)\n\n    def test_reverse_tgt_tokens(self):\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([self.model], self.args, self.task),\n        ):\n            scorer = R2LModelScorer(self.args, ""/tmp/model_path.txt"")\n            pad = self.task.tgt_dict.pad()\n            tgt_tokens = torch.Tensor([[1, 2, 3], [1, 2, pad], [1, pad, pad]])\n            expected_tokens = torch.Tensor([[3, 2, 1], [2, 1, pad], [1, pad, pad]])\n            reversed_tgt_tokens = scorer.reverse_tgt_tokens(tgt_tokens)\n            assert torch.equal(reversed_tgt_tokens, expected_tokens)\n\n    def test_convert_hypos_to_tgt_tokens(self):\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([self.model], self.args, self.task),\n        ):\n            scorer = SimpleModelScorer(self.args, ""/tmp/model_path.txt"")\n            hypos = [\n                {""tokens"": torch.Tensor([1, 2, 3, 4, 5])},\n                {""tokens"": torch.Tensor([1, 2, 3, 4])},\n                {""tokens"": torch.Tensor([1, 2, 3])},\n                {""tokens"": torch.Tensor([1, 2])},\n                {""tokens"": torch.Tensor([1])},\n            ]\n            tgt_tokens = scorer.convert_hypos_to_tgt_tokens(hypos)\n\n            pad = self.task.tgt_dict.pad()\n            eos = self.task.tgt_dict.eos()\n            expected_tgt_tokens = torch.Tensor(\n                [\n                    [eos, 1, 2, 3, 4, 5],\n                    [eos, 1, 2, 3, 4, pad],\n                    [eos, 1, 2, 3, pad, pad],\n                    [eos, 1, 2, pad, pad, pad],\n                    [eos, 1, pad, pad, pad, pad],\n                ]\n            ).type_as(tgt_tokens)\n            assert torch.equal(tgt_tokens, expected_tgt_tokens)\n\n    def test_compute_scores(self):\n        # TODO(halilakin): Verify behaviour in batch mode\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([self.model], self.args, self.task),\n        ):\n            scorer = SimpleModelScorer(self.args, ""/tmp/model_path.txt"")\n            tgt_tokens = torch.tensor([[2, 11, 22, 0], [2, 33, 44, 55]])\n            logprobs = torch.zeros(\n                tgt_tokens.shape[0], tgt_tokens.shape[1], len(self.task.tgt_dict)\n            )\n            logprobs[0, 0, 11] = 0.5\n            logprobs[0, 1, 22] = 1.5\n            logprobs[0, 3, :] = 5\n\n            logprobs[1, 0, 33] = 0.5\n            logprobs[1, 1, 44] = 1.5\n            logprobs[1, 2, 55] = 2.5\n\n            hypos_scores = scorer.compute_scores(tgt_tokens, logprobs)\n            assert hypos_scores[0] == 2.0\n            assert hypos_scores[1] == 4.5\n\n    def test_simple_scorer_prepare_inputs(self):\n        pad = self.task.tgt_dict.pad()\n        eos = self.task.tgt_dict.eos()\n\n        src_tokens = torch.tensor([[6, 7, 8]], dtype=torch.int)\n        hypos = [\n            {""tokens"": torch.tensor([12, 13, 14, eos], dtype=torch.int)},\n            {""tokens"": torch.tensor([22, 23, eos], dtype=torch.int)},\n        ]\n\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([self.model], self.args, self.task),\n        ):\n            scorer = SimpleModelScorer(\n                self.args, ""/tmp/model_path.txt"", None, self.task\n            )\n            (encoder_inputs, tgt_tokens) = scorer.prepare_inputs(src_tokens, hypos)\n\n            # Test encoder inputs\n            assert torch.equal(\n                encoder_inputs[0], torch.tensor([[6, 7, 8], [6, 7, 8]], dtype=torch.int)\n            ), ""Encoder inputs are not as expected""\n            assert torch.equal(\n                encoder_inputs[1], torch.tensor([3, 3], dtype=torch.int)\n            ), ""Src lengths are not as expected""\n\n            # Test target tokens\n            assert torch.equal(\n                tgt_tokens,\n                torch.tensor(\n                    [[eos, 12, 13, 14, eos], [eos, 22, 23, eos, pad]], dtype=torch.int\n                ),\n            ), ""Target tokens are not as expected""\n\n    def test_reverse_scorer_prepare_inputs(self):\n        self.args.append_eos_to_source = True\n        pad = self.task.tgt_dict.pad()\n        eos = self.task.tgt_dict.eos()\n\n        src_tokens = torch.tensor([6, 7, 8], dtype=torch.int)\n        hypos = [\n            {""tokens"": torch.tensor([12, 13, 14, eos], dtype=torch.int)},\n            {""tokens"": torch.tensor([22, 23, eos], dtype=torch.int)},\n        ]\n\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([self.model], self.args, self.task),\n        ):\n            scorer = ReverseModelScorer(\n                self.args, ""/tmp/model_path.txt"", None, self.task\n            )\n            (encoder_inputs, tgt_tokens) = scorer.prepare_inputs(src_tokens, hypos)\n\n            # Test encoder inputs\n            assert torch.equal(\n                encoder_inputs[0],\n                torch.tensor([[12, 13, 14, eos], [22, 23, eos, pad]], dtype=torch.int),\n            ), ""Encoder inputs are not as expected""\n            max_tgt_len = max(len(hypo[""tokens""]) for hypo in hypos)\n            assert encoder_inputs[1][0] == max_tgt_len, "" Src length is not as expected""\n\n            # Test target tokens\n            assert torch.equal(\n                tgt_tokens,\n                torch.tensor(\n                    [[eos, 6, 7, 8, eos], [eos, 6, 7, 8, eos]], dtype=torch.int\n                ),\n            ), ""Target tokens are not as expected""\n\n            # Turn off append_eos_to_source and verify outputs again\n            self.args.append_eos_to_source = False\n            (encoder_inputs, tgt_tokens) = scorer.prepare_inputs(src_tokens, hypos)\n\n            # Test encoder inputs\n            assert torch.equal(\n                encoder_inputs[0],\n                torch.tensor([[12, 13, 14], [22, 23, pad]], dtype=torch.int),\n            ), ""Encoder inputs are not as expected""\n            max_tgt_len = max(len(hypo[""tokens""]) for hypo in hypos)\n            assert (\n                encoder_inputs[1][0] == max_tgt_len - 1\n            ), "" Src length is not as expected""\n\n            # Test target tokens\n            assert torch.equal(\n                tgt_tokens,\n                torch.tensor(\n                    [[eos, 6, 7, 8, eos], [eos, 6, 7, 8, eos]], dtype=torch.int\n                ),\n            ), ""Target tokens are not as expected""\n\n    def test_r2l_scorer_prepare_inputs(self):\n        eos = self.task.tgt_dict.eos()\n        src_tokens = torch.tensor([[6, 7, 8], [1, 2, 3]], dtype=torch.int)\n        hypos = [\n            {""tokens"": torch.tensor([12, 13, 14, eos], dtype=torch.int)},\n            {""tokens"": torch.tensor([22, 23, eos], dtype=torch.int)},\n            {""tokens"": torch.tensor([12, 13, 14, eos], dtype=torch.int)},\n            {""tokens"": torch.tensor([22, 23, eos], dtype=torch.int)},\n        ]\n\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([self.model], self.args, self.task),\n        ):\n            scorer = R2LModelScorer(self.args, ""/tmp/model_path.txt"", None, self.task)\n            (encoder_inputs, tgt_tokens) = scorer.prepare_inputs(src_tokens, hypos)\n            # Test encoder inputs\n            assert torch.equal(\n                encoder_inputs[0],\n                torch.tensor(\n                    [[6, 7, 8], [6, 7, 8], [1, 2, 3], [1, 2, 3]], dtype=torch.int\n                ),\n            ), ""Encoder inputs are not as expected""\n\n    def test_padding(self):\n        """"""Same sentence should produce the same score with or without padding\n        """"""\n        eos = self.task.tgt_dict.eos()\n\n        src_tokens = torch.tensor([[6, 7, 8]], dtype=torch.long)\n        hypos_with_padding = [\n            {""tokens"": torch.tensor([12, 13, 14, 15, 16, eos], dtype=torch.long)},\n            {""tokens"": torch.tensor([22, 23, 24, 25, eos], dtype=torch.long)},\n            {""tokens"": torch.tensor([32, 33, eos], dtype=torch.long)},\n        ]\n        hypos_without_padding = [\n            {""tokens"": torch.tensor([22, 23, 24, 25, eos], dtype=torch.long)},\n            {""tokens"": torch.tensor([32, 33, eos], dtype=torch.long)},\n        ]\n\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([self.model], self.args, self.task),\n        ):\n            scorer = SimpleModelScorer(\n                self.args, ""/tmp/model_path.txt"", None, self.task\n            )\n            scores_with_padding = scorer.score(src_tokens, hypos_with_padding)\n            scores_without_padding = scorer.score(src_tokens, hypos_without_padding)\n            assert (\n                scores_with_padding[1] == scores_without_padding[0]\n                and scores_with_padding[2] == scores_without_padding[1]\n            ), ""Scores with and without padding should not be different""\n\n    def test_reverse_model_scorer(self):\n        """"""Verify that reverse model is working correctly, by having one\n        forward and one backward scorers, and asserting that we get the same\n        scores from two scorers when source and targets are reversed\n\n        """"""\n        eos = self.task.tgt_dict.eos()\n\n        src_tokens = torch.tensor([[6, 7, 8]], dtype=torch.long)\n        hypos = [\n            {""tokens"": torch.tensor([12, 13, 14, 15, eos], dtype=torch.long)},\n            {""tokens"": torch.tensor([22, 23, 24, eos], dtype=torch.long)},\n            {""tokens"": torch.tensor([32, 33, eos], dtype=torch.long)},\n        ]\n\n        reverse_src_tokens_0 = torch.tensor([[12, 13, 14, 15]], dtype=torch.long)\n        reverse_src_tokens_1 = torch.tensor([[22, 23, 24]], dtype=torch.long)\n        reverse_src_tokens_2 = torch.tensor([[32, 33]], dtype=torch.long)\n        reverse_hypos = [{""tokens"": torch.tensor([6, 7, 8, eos], dtype=torch.long)}]\n\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([self.model], self.args, self.task),\n        ):\n            scorer = SimpleModelScorer(\n                self.args, ""/tmp/model_path.txt"", None, self.task\n            )\n            reverse_scorer = ReverseModelScorer(\n                self.args, ""/tmp/model_path.txt"", None, self.task\n            )\n            forward_scores = scorer.score(src_tokens, hypos)\n            reverse_score_0 = reverse_scorer.score(reverse_src_tokens_0, reverse_hypos)\n            reverse_score_1 = reverse_scorer.score(reverse_src_tokens_1, reverse_hypos)\n            reverse_score_2 = reverse_scorer.score(reverse_src_tokens_2, reverse_hypos)\n\n            assert forward_scores[0] == reverse_score_0[0]\n            assert forward_scores[1] == reverse_score_1[0]\n            assert forward_scores[2] == reverse_score_2[0]\n'"
pytorch_translate/rescoring/test/test_rescorer.py,20,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom unittest.mock import patch\n\nimport torch\nfrom pytorch_translate.rescoring.rescorer import Rescorer, combine_weighted_scores\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestRescorer(unittest.TestCase):\n    def test_combine_weighted_scores(self):\n        test_args = test_utils.ModelParamsDict()\n        test_args.enable_rescoring = True\n        test_args.length_penalty = 1\n        test_args.l2r_model_path = """"\n        test_args.l2r_model_weight = 1.0\n        test_args.r2l_model_weight = 0.0\n        test_args.reverse_model_weight = 0.0\n        test_args.cloze_transformer_weight = 0.0\n        test_args.lm_model_weight = 1.01\n        test_args.length_penalty = 1.0\n\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.PytorchTranslateTask(test_args, src_dict, tgt_dict)\n        model = task.build_model(test_args)\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([model], test_args, task),\n        ):\n            scores = torch.tensor(\n                [[80, 0, 0, 0, 0], [0, 0, 0, 80, 0]], dtype=torch.float\n            )\n            src_tokens = torch.tensor([[1, 2, 3, 4, 5]])\n            hypos = [{""tokens"": torch.tensor([1, 2])}, {""tokens"": torch.tensor([1, 2])}]\n\n            src_len = len(src_tokens)\n\n            tgt_len = torch.tensor(\n                [len(hypo[""tokens""]) for hypo in hypos], dtype=torch.float\n            )\n            weights = [\n                test_args.l2r_model_weight,\n                test_args.r2l_model_weight,\n                test_args.reverse_model_weight,\n                test_args.lm_model_weight,\n                test_args.cloze_transformer_weight,\n            ]\n            combined_scores = combine_weighted_scores(\n                scores, weights, src_len, tgt_len, 1\n            )\n\n            # 80/(2^1), 0, 0, 80*1.01/(2^1)\n            expected = torch.tensor([40.0, 40.4], dtype=torch.float)\n            assert torch.equal(combined_scores, expected)\n\n    def test_model_passing_as_parameter(self):\n        test_args = test_utils.ModelParamsDict(""transformer"")\n        test_args.enable_rescoring = True\n        test_args.length_penalty = 1\n        test_args.l2r_model_weight = 1.0\n        test_args.r2l_model_weight = 0.0\n        test_args.reverse_model_weight = 0.0\n        test_args.lm_model_weight = 1.01\n        test_args.cloze_transformer_weight = 1.0\n        test_args.length_penalty = 1.0\n\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.PytorchTranslateTask(test_args, src_dict, tgt_dict)\n        model = task.build_model(test_args)\n        src_tokens = torch.tensor([[1, 2, 3, 4, 5]])\n        hypos = [{""tokens"": torch.tensor([1, 2])}, {""tokens"": torch.tensor([1, 2])}]\n        rescorer = Rescorer(\n            test_args, task, {""l2r_model"": {""model"": model, ""task"": task}}\n        )\n        scores = rescorer.score(src_tokens, hypos)\n        assert scores.size()[1] == 5\n\n    def test_batch_computation(self):\n        test_args = test_utils.ModelParamsDict(""transformer"")\n        test_args.enable_rescoring = True\n        test_args.length_penalty = 1\n        test_args.l2r_model_path = ""/tmp/test_rescorer_model.pt""\n        test_args.l2r_model_weight = 1.0\n        test_args.r2l_model_weight = 0.0\n        test_args.reverse_model_weight = 0.0\n        test_args.cloze_transformer_weight = 1.0\n        test_args.lm_model_weight = 0.0\n        test_args.length_penalty = 1.0\n\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        task = tasks.PytorchTranslateTask(test_args, src_dict, tgt_dict)\n        model = task.build_model(test_args)\n        torch.save(model, test_args.l2r_model_path)\n        with patch(\n            ""pytorch_translate.utils.load_diverse_ensemble_for_inference"",\n            return_value=([model], test_args, task),\n        ):\n            rescorer = Rescorer(test_args)\n            src_tokens = torch.tensor([[1, 3, 3, 4, 2], [1, 3, 2, 0, 0]])\n            hypos = [\n                {""tokens"": torch.tensor([1, 5, 2])},\n                {""tokens"": torch.tensor([6, 3, 5, 2])},\n                {""tokens"": torch.tensor([1, 2])},\n                {""tokens"": torch.tensor([1, 5, 6, 2])},\n            ]\n            scores = rescorer.score(src_tokens, hypos)\n\n            src_tokens = torch.tensor([[1, 3, 3, 4, 2]])\n            hypos = [\n                {""tokens"": torch.tensor([1, 5, 2])},\n                {""tokens"": torch.tensor([6, 3, 5, 2])},\n            ]\n            scores_single = rescorer.score(src_tokens, hypos)\n\n            assert torch.equal(scores[0], scores_single[0])\n'"
pytorch_translate/research/attention/multihead_attention.py,6,"b'#!/usr/bin/env python3\n\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\ndef create_src_lengths_mask(batch_size, src_lengths):\n    max_srclen = src_lengths.max()\n    src_indices = torch.arange(0, max_srclen).unsqueeze(0).type_as(src_lengths)\n    src_indices = src_indices.expand(batch_size, max_srclen)\n    src_lengths = src_lengths.unsqueeze(dim=1).expand(batch_size, max_srclen)\n    # returns [batch_size, max_seq_len]\n    return (src_indices < src_lengths).int().detach()\n\n\ndef apply_masks(scores, batch_size, unseen_mask, src_lengths):\n    seq_len = scores.shape[-1]\n\n    # [1, seq_len, seq_len]\n    sequence_mask = torch.ones(seq_len, seq_len).unsqueeze(0).int()\n    if unseen_mask:\n        # [1, seq_len, seq_len]\n        sequence_mask = (\n            torch.tril(torch.ones(seq_len, seq_len), diagonal=0).unsqueeze(0).int()\n        )\n\n    if src_lengths is not None:\n        # [batch_size, 1, seq_len]\n        src_lengths_mask = create_src_lengths_mask(\n            batch_size=batch_size, src_lengths=src_lengths\n        ).unsqueeze(-2)\n\n        # [batch_size, seq_len, seq_len]\n        sequence_mask = sequence_mask & src_lengths_mask\n\n    # [batch_size, 1, seq_len, seq_len]\n    sequence_mask = sequence_mask.unsqueeze(1)\n\n    scores = scores.masked_fill(sequence_mask == 0, -np.inf)\n    return scores\n\n\ndef scaled_dot_prod_attn(query, key, value, unseen_mask=False, src_lengths=None):\n    """"""\n    Scaled Dot Product Attention\n\n    Implements equation:\n    Attention(Q, K, V) = softmax(QK^T/\\sqrt{d_k})V\n\n    Inputs:\n      query : [batch size, nheads, sequence length, d_k]\n      key : [batch size, nheads, sequence length, d_k]\n      value : [batch size, nheads, sequence length, d_v]\n      unseen_mask: if True, only attend to previous sequence positions\n      src_lengths_mask: if True, mask padding based on src_lengths\n\n    Outputs:\n      attn: [batch size, sequence length, d_v]\n\n    Note that in this implementation d_q = d_k = d_v = dim\n    """"""\n    d_k = query.shape[-1]\n    scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(d_k)\n    if unseen_mask or src_lengths is not None:\n        scores = apply_masks(\n            scores=scores,\n            batch_size=query.shape[0],\n            unseen_mask=unseen_mask,\n            src_lengths=src_lengths,\n        )\n    p_attn = F.softmax(scores, dim=-1)\n    return torch.matmul(p_attn, value), p_attn\n\n\ndef split_heads(X, nheads):\n    """"""\n    Split heads:\n    1) Split (reshape) last dimension (size d_model) into nheads, d_head\n    2) Transpose X from (batch size, sequence length, nheads, d_head) to\n        (batch size, nheads, sequence length, d_head)\n\n    Inputs:\n      X : [batch size, sequence length, nheads * d_head]\n      nheads : integer\n    Outputs:\n      [batch size,  nheads, sequence length, d_head]\n\n    """"""\n    last_dim = X.shape[-1]\n    assert last_dim % nheads == 0\n    X_last_dim_split = X.view(list(X.shape[:-1]) + [nheads, last_dim // nheads])\n    return X_last_dim_split.transpose(1, 2)\n\n\ndef combine_heads(X):\n    """"""\n    Combine heads (the inverse of split heads):\n    1) Transpose X from (batch size, nheads, sequence length, d_head) to\n        (batch size, sequence length, nheads, d_head)\n    2) Combine (reshape) last 2 dimensions (nheads, d_head) into 1 (d_model)\n\n    Inputs:\n      X : [batch size * nheads, sequence length, d_head]\n      nheads : integer\n      d_head : integer\n\n    Outputs:\n      [batch_size, seq_len, d_model]\n\n    """"""\n    X = X.transpose(1, 2)\n    nheads, d_head = X.shape[-2:]\n    return X.contiguous().view(list(X.shape[:-2]) + [nheads * d_head])\n\n\nclass MultiheadAttention(nn.Module):\n    """"""\n    Multiheaded Scaled Dot Product Attention\n\n    Implements equation:\n    MultiHead(Q, K, V) = Concat(head_1,...,head_h)W^O\n        where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n\n    Similarly to the above, d_k = d_v = d_model / h\n\n    Inputs\n      init:\n        nheads : integer # of attention heads\n        d_model : model dimensionality\n        d_head : dimensionality of a single head\n\n      forward:\n        query : [batch size, sequence length, d_model]\n        key: [batch size, sequence length, d_model]\n        value: [batch size, sequence length, d_model]\n      unseen_mask: if True, only attend to previous sequence positions\n      src_lengths_mask: if True, mask padding based on src_lengths\n\n    Output\n      result : [batch_size, sequence length, d_model]\n    """"""\n\n    def __init__(self, nheads, d_model):\n        ""Take in model size and number of heads.""\n        super(MultiheadAttention, self).__init__()\n        assert d_model % nheads == 0\n        self.d_head = d_model // nheads\n        self.nheads = nheads\n        self.Q_fc = nn.Linear(d_model, d_model, bias=False)\n        self.K_fc = nn.Linear(d_model, d_model, bias=False)\n        self.V_fc = nn.Linear(d_model, d_model, bias=False)\n        self.output_fc = nn.Linear(d_model, d_model, bias=False)\n        self.attn = None\n\n    def forward(self, query, key, value, unseen_mask=False, src_lengths=None):\n        # 1. Fully-connected layer on q, k, v then\n        # 2. Split heads on q, k, v\n        # (batch_size, seq_len, d_model) -->\n        # (batch_size, nheads, seq_len, d_head)\n        query = split_heads(self.Q_fc(query), self.nheads)\n        key = split_heads(self.K_fc(key), self.nheads)\n        value = split_heads(self.V_fc(value), self.nheads)\n\n        # 4. Scaled dot product attention\n        # (batch_size, nheads, seq_len, d_head)\n        x, self.attn = scaled_dot_prod_attn(\n            query=query,\n            key=key,\n            value=value,\n            unseen_mask=unseen_mask,\n            src_lengths=src_lengths,\n        )\n\n        # 5. Combine heads\n        x = combine_heads(x)\n\n        # 6. Fully-connected layer for output\n        return self.output_fc(x)\n'"
pytorch_translate/research/beam_search/__init__.py,0,b''
pytorch_translate/research/beam_search/competing_completed.py,12,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nfrom fairseq.models import FairseqIncrementalDecoder\nfrom pytorch_translate.beam_decode import SequenceGenerator\n\n\nclass CompetingCompletedSequenceGenerator(SequenceGenerator):\n    """"""Beam search which keeps completed hypotheses in the beam.\n\n    This is an alternative beam search implementation which is more similar to\n    some of the beam search implementations in the literature (cf. Nematus,\n    Blocks, tensor2tensor, TF NMT tutorial...). This implementation keeps\n    completed hypotheses as active hypotheses in the next time step. Thus, they\n    will keep competing against longer hypotheses. Beam search terminates if all\n    active hypotheses are completed (i.e. end with EOS).\n\n    Experiments have shown that this implementation and the main implementation\n    in `beam_search.py` produce almost the same translations. We\'re leaving this\n    under research/ because experimentation did not show a noticeable difference\n    in translation quality from the existing implementation.\n    """"""\n\n    def _generate(\n        self,\n        encoder_input,\n        beam_size=None,\n        maxlen=None,\n        prefix_tokens=None,\n        extra_info=True,\n    ):\n        """"""Run beam search.\n\n        Args:\n            encoder_input: 2-tuple (tokens, length) of int tensors containing\n                the source tokens and the source sentence lengths.\n            beam_size: beam size (if None, use self.beam_size)\n            maxlen: Maximum target sentence length (if None, use self.maxlen)\n            prefix_tokens: None or [bsz, prefix_length] int tensor with\n                translation prefixes. All generated translations will be\n                constrained to these prefixes.\n            extra_info: If true, output additional information like alignment,\n                attentions, and positional scores.\n\n        Returns:\n            A list of lists, containing the n-best translations for each\n            batch entry. The translations are represented as dictionary with\n            keys tokens, score, attention, alignment, positional_scores.\n        """"""\n        src_tokens = encoder_input[0]\n        bsz, srclen = src_tokens.size()\n        maxlen = min(maxlen, self.maxlen) if maxlen is not None else self.maxlen\n        beam_size = beam_size if beam_size is not None else self.beam_size\n\n        # Encode\n        encoder_outs, incremental_states = self._encode(encoder_input, beam_size)\n\n        hypo_tokens = src_tokens.new(bsz * beam_size, maxlen + 2).fill_(self.pad)\n        hypo_tokens_buf = hypo_tokens.clone()\n        hypo_tokens[:, 0] = self.eos\n        cand_scores = src_tokens.new(bsz, beam_size).float().fill_(0)\n        attn_list = []\n        cand_scores_list = []\n        cand_beams_list = []\n        new_order = None\n        # For example, new_order_offsets for bsz=3, beam_size=5 is\n        # [0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10]\n        new_order_offsets = (\n            torch.arange(0, bsz * beam_size, step=beam_size, dtype=torch.long)\n            .view(bsz, 1)\n            .repeat(1, beam_size)\n            .view(-1)\n            .cuda()\n        )\n        for step in range(maxlen + 1):  # one extra step for EOS marker\n            num_finished = 0\n            if step > 0:\n                eos_indices = torch.nonzero(hypo_tokens[:, step] == self.eos)\n                num_finished = eos_indices.size(0)\n                if num_finished == bsz * beam_size:\n                    break\n            self.reorder_states(new_order, incremental_states)\n            word_scores, avg_attn, possible_translation_tokens = self._decode(\n                hypo_tokens[:, : step + 1], encoder_outs, incremental_states\n            )\n            attn_list.append(avg_attn.view(bsz, beam_size, -1))\n            self.add_rewards(word_scores, step, possible_translation_tokens)\n            if step == 0:  # Use only first beam in first time step\n                word_scores.view(bsz, beam_size, -1)[:, 1:, :] = -math.inf\n            elif num_finished > 0:  # make sure EOS is continued with EOS\n                word_scores[eos_indices] = -math.inf\n                word_scores[eos_indices, self.eos] = 0.0\n            if prefix_tokens is not None and step < prefix_tokens.size(1):\n                self.constrain_tokens(word_scores, prefix_tokens[step])\n            if step >= maxlen:  # Force EOS on all hypos\n                self.constrain_tokens(\n                    word_scores, hypo_tokens.new_full((bsz * beam_size,), self.eos)\n                )\n            word_scores.add_(cand_scores.view(-1, 1))\n            cand_scores, cand_indices, cand_beams = self.select_next_words(\n                word_scores, bsz, beam_size, possible_translation_tokens\n            )\n            new_order = cand_beams.view(-1) + new_order_offsets\n            torch.index_select(hypo_tokens, dim=0, index=new_order, out=hypo_tokens_buf)\n            hypo_tokens_buf[:, step + 1] = cand_indices.view(-1)\n            cand_scores_list.append(cand_scores)\n            cand_beams_list.append(cand_beams)\n            hypo_tokens, hypo_tokens_buf = hypo_tokens_buf, hypo_tokens\n        build_hypo_fn = self.build_hypos if extra_info else self.build_hypos_fast\n        return build_hypo_fn(\n            hypo_tokens.view(bsz, beam_size, -1),\n            cand_scores_list,\n            attn_list,\n            cand_beams_list,\n        )\n\n    def build_hypos(self, hypo_tokens, cand_scores_list, attn_list, cand_beams_list):\n        bsz, beam_size, maxlen = hypo_tokens.size()\n        seqlens = maxlen - torch.sum(hypo_tokens <= self.eos, dim=2)  # eos and pad\n        all_hypos = []\n        for batch_idx in range(bsz):\n            hypos = []\n            batch_cand_scores = self.backtrace(\n                batch_idx, cand_beams_list, cand_scores_list\n            )\n            batch_attns = self.backtrace(batch_idx, cand_beams_list, attn_list)\n            for i in range(beam_size):\n                seqlen = seqlens[batch_idx, i]\n                this_attns = torch.stack(batch_attns[i], dim=1)[:, : seqlen + 1]\n                _, alignment = this_attns.max(dim=0)\n                pos_scores = []\n                prev = 0.0\n                for pos in range(seqlen + 1):\n                    pos_scores.append(batch_cand_scores[i][pos] - prev)\n                    prev = batch_cand_scores[i][pos]\n                hypos.append(\n                    {\n                        ""tokens"": hypo_tokens[batch_idx, i, 1 : seqlen + 2],\n                        ""score"": cand_scores_list[-1][batch_idx, i],\n                        ""attention"": this_attns,\n                        ""alignment"": alignment,\n                        ""positional_scores"": pos_scores,\n                    }\n                )\n            all_hypos.append(hypos)\n        return all_hypos\n\n    def build_hypos_fast(\n        self, hypo_tokens, cand_scores_list, attn_list, cand_beams_list\n    ):\n        bsz, beam_size, maxlen = hypo_tokens.size()\n        seqlens = maxlen - torch.sum(hypo_tokens <= self.eos, dim=2)  # eos and pad\n        all_hypos = []\n        dummy_alignment = torch.LongTensor([1, 2, 3])\n        for batch_idx in range(bsz):\n            hypos = []\n            for i in range(beam_size):\n                seqlen = seqlens[batch_idx, i]\n                hypos.append(\n                    {\n                        ""tokens"": hypo_tokens[batch_idx, i, 1 : seqlen + 2],\n                        ""score"": cand_scores_list[-1][batch_idx, i],\n                        ""attention"": None,\n                        ""alignment"": dummy_alignment,\n                        ""positional_scores"": None,\n                    }\n                )\n            all_hypos.append(hypos)\n        return all_hypos\n\n    def backtrace(self, batch_idx, backpointers_list, elements_list):\n        beam_size = backpointers_list[0].size(1)\n        backtraced = [[elements_list[-1][batch_idx, i]] for i in range(beam_size)]\n        hypo_ptrs = list(range(beam_size))\n        pos = -1\n        for backpointers in reversed(backpointers_list[1:]):\n            pos -= 1\n            for i in range(beam_size):\n                hypo_ptrs[i] = backpointers[batch_idx][hypo_ptrs[i]]\n                backtraced[i].append(elements_list[pos][batch_idx, hypo_ptrs[i]])\n        for l in backtraced:\n            l.reverse()\n        return backtraced\n\n    def select_next_words(\n        self, word_scores, bsz, beam_size, possible_translation_tokens\n    ):\n        cand_scores, cand_indices = torch.topk(word_scores.view(bsz, -1), k=beam_size)\n        possible_tokens_size = self.vocab_size\n        if possible_translation_tokens is not None:\n            possible_tokens_size = possible_translation_tokens.size(0)\n        cand_beams = torch.div(cand_indices, possible_tokens_size)\n        cand_indices.fmod_(possible_tokens_size)\n        # Handle vocab reduction\n        if possible_translation_tokens is not None:\n            possible_translation_tokens = possible_translation_tokens.view(\n                1, possible_tokens_size\n            ).expand(cand_indices.size(0), possible_tokens_size)\n            cand_indices = torch.gather(\n                possible_translation_tokens, dim=1, index=cand_indices, out=cand_indices\n            )\n        return cand_scores, cand_indices, cand_beams\n\n    def constrain_tokens(self, word_scores, permitted_tokens):\n        """"""Modifies word_scores such that hypos are expanded with the tokens\n        in `permitted_tokens`.\n\n        Args:\n            word_scores: [bsz*beam_size, vocab_size] float tensor with\n                cumulative word scores (logprobs plus hypo scores plus rewards)\n            permitted_tokens: [bsz*beam_size] int tensor of tokens. We set all\n                entries in `word_scores` to -inf except the permitted tokens.\n        """"""\n        permitted_tokens = permitted_tokens.unsqueeze(1)\n        permitted_scores = torch.gather(word_scores, dim=1, index=permitted_tokens)\n        word_scores[:, :] = -math.inf\n        word_scores.scatter_(dim=1, index=permitted_tokens, src=permitted_scores)\n\n    def reorder_states(self, new_order, incremental_states):\n        if new_order is None:\n            return\n        for model in self.models:\n            if isinstance(model.decoder, FairseqIncrementalDecoder):\n                model.decoder.reorder_incremental_state(\n                    incremental_states[model], new_order\n                )\n\n    def add_rewards(self, word_scores, step, possible_translation_tokens):\n        """"""Compute the accumulated scores for each word.\n\n        Args:\n            word_scores: [bsz*beam_size, vocab_size] float tensor with cumulative\n                word scores (logprobs plus hypo scores)\n            step (int): time step\n            possible_translation_tokens: For vocab reduction\n        """"""\n        word_scores[:, self.pad] = -math.inf  # never select pad\n\n        # apply unk reward\n        if possible_translation_tokens is None:\n            unk_index = self.unk\n        else:\n            unk_index = torch.nonzero(possible_translation_tokens == self.unk)[0, 0]\n        word_scores[:, unk_index] += self.unk_reward\n\n        # external lexicon reward\n        word_scores[:, self.lexicon_indices] += self.lexicon_reward\n\n        word_scores += self.word_reward\n        if step >= self.minlen:\n            word_scores[:, self.eos] -= self.word_reward\n        else:\n            word_scores[:, self.eos] = -math.inf\n'"
pytorch_translate/research/deliberation_networks/deliberation_networks.py,12,"b'#!/usr/bin/env python3\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import options, utils\nfrom fairseq.models import (\n    FairseqEncoderDecoderModel,\n    FairseqIncrementalDecoder,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.transformer import (\n    Embedding,\n    Linear,\n    TransformerDecoder,\n    TransformerEncoder,\n)\nfrom fairseq.modules import (\n    AdaptiveSoftmax,\n    LayerNorm,\n    MultiheadAttention,\n    PositionalEmbedding,\n    SinusoidalPositionalEmbedding,\n)\n\n\nDEFAULT_MAX_SOURCE_POSITIONS = 1024\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\n\n@register_model(""two_phase_transformer"")\nclass TwoPhaseTransformerModel(FairseqEncoderDecoderModel):\n    """"""\n    Two phase decoding:\n    www.microsoft.com/en-us/research/uploads/prod/2018/03/final-achieving-human.pdf\n    Args:\n        encoder (TransformerEncoder): the encoder\n        decoder (TransformerTwoPhaseDecoder): the decoder\n    """"""\n\n    def __init__(self, encoder, decoder):\n        super().__init__(encoder, decoder)\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        # fmt: off\n        parser.add_argument(\'--activation-fn\',\n                            choices=utils.get_available_activation_fns(),\n                            help=\'activation function to use\')\n        parser.add_argument(\'--dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability\')\n        parser.add_argument(\'--attention-dropout\', type=float, metavar=\'D\',\n                            help=\'dropout probability for attention weights\')\n        parser.add_argument(\'--activation-dropout\', \'--relu-dropout\', type=float,\n                            metavar=\'D\',\n                            help=\'dropout probability after activation in FFN.\')\n        parser.add_argument(\'--encoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained encoder embedding\')\n        parser.add_argument(\'--encoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension\')\n        parser.add_argument(\'--encoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'encoder embedding dimension for FFN\')\n        parser.add_argument(\'--encoder-layers\', type=int, metavar=\'N\',\n                            help=\'num encoder layers\')\n        parser.add_argument(\'--encoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num encoder attention heads\')\n        parser.add_argument(\'--encoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each encoder block\')\n        parser.add_argument(\'--encoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the encoder\')\n        parser.add_argument(\'--decoder-embed-path\', type=str, metavar=\'STR\',\n                            help=\'path to pre-trained decoder embedding\')\n        parser.add_argument(\'--decoder-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension\')\n        parser.add_argument(\'--decoder-ffn-embed-dim\', type=int, metavar=\'N\',\n                            help=\'decoder embedding dimension for FFN\')\n        parser.add_argument(\'--decoder-layers\', type=int, metavar=\'N\',\n                            help=\'num decoder layers\')\n        parser.add_argument(\'--decoder-attention-heads\', type=int, metavar=\'N\',\n                            help=\'num decoder attention heads\')\n        parser.add_argument(\'--decoder-learned-pos\', action=\'store_true\',\n                            help=\'use learned positional embeddings in the decoder\')\n        parser.add_argument(\'--decoder-normalize-before\', action=\'store_true\',\n                            help=\'apply layernorm before each decoder block\')\n        parser.add_argument(\'--share-decoder-input-output-embed\', action=\'store_true\',\n                            help=\'share decoder input and output embeddings\')\n        parser.add_argument(\'--share-all-embeddings\', action=\'store_true\',\n                            help=\'share encoder, decoder and output embeddings\'\n                                 \' (requires shared dictionary and embed dim)\')\n        parser.add_argument(\'--no-token-positional-embeddings\',\n                            default=False, action=\'store_true\',\n                            help=\'if set, disables positional embeddings\'\n                                    \'(outside self attention)\')\n        parser.add_argument(\'--adaptive-softmax-cutoff\', metavar=\'EXPR\',\n                            help=\'comma separated list of \'\n                                \'adaptive softmax cutoff points. \'\n                                 \'Must be used with adaptive_loss criterion\'),\n        parser.add_argument(\'--adaptive-softmax-dropout\', type=float, metavar=\'D\',\n                            help=\'sets adaptive softmax dropout\'\n                                    \' for the tail projections\')\n        # fmt: on\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n\n        # make sure all arguments are present in older models\n        base_architecture(args)\n\n        if not hasattr(args, ""max_source_positions""):\n            args.max_source_positions = DEFAULT_MAX_SOURCE_POSITIONS\n        if not hasattr(args, ""max_target_positions""):\n            args.max_target_positions = DEFAULT_MAX_TARGET_POSITIONS\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        def build_embedding(dictionary, embed_dim, path=None):\n            num_embeddings = len(dictionary)\n            padding_idx = dictionary.pad()\n            emb = Embedding(num_embeddings, embed_dim, padding_idx)\n            # if provided, load from preloaded dictionaries\n            if path:\n                embed_dict = utils.parse_embedding(path)\n                utils.load_embedding(embed_dict, dictionary, emb)\n            return emb\n\n        if args.share_all_embeddings:\n            if src_dict != tgt_dict:\n                raise ValueError(""--share-all-embeddings requires a joined dictionary"")\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise ValueError(\n                    ""--share-all-embeddings requires --encoder-embed-dim ""\n                    ""to match --decoder-embed-dim""\n                )\n            if args.decoder_embed_path and (\n                args.decoder_embed_path != args.encoder_embed_path\n            ):\n                raise ValueError(\n                    ""--share-all-embeddings not compatible with --decoder-embed-path""\n                )\n            encoder_embed_tokens = build_embedding(\n                src_dict, args.encoder_embed_dim, args.encoder_embed_path\n            )\n            decoder_embed_tokens = encoder_embed_tokens\n            args.share_decoder_input_output_embed = True\n        else:\n            encoder_embed_tokens = build_embedding(\n                src_dict, args.encoder_embed_dim, args.encoder_embed_path\n            )\n            decoder_embed_tokens = build_embedding(\n                tgt_dict, args.decoder_embed_dim, args.decoder_embed_path\n            )\n\n        encoder = cls.build_encoder(args, src_dict, encoder_embed_tokens)\n        decoder = cls.build_decoder(args, tgt_dict, decoder_embed_tokens)\n        return TwoPhaseTransformerModel(encoder, decoder)\n\n    @classmethod\n    def build_encoder(cls, args, src_dict, embed_tokens):\n        return TransformerEncoder(args, src_dict, embed_tokens)\n\n    @classmethod\n    def build_decoder(cls, args, tgt_dict, embed_tokens):\n        return TransformerTwoPhaseDecoder(args, tgt_dict, embed_tokens)\n\n\nclass TransformerTwoPhaseDecoder(FairseqIncrementalDecoder):\n    """"""Two phase decoder.\n    First pass will be transofrmer decoder and second pass will be\n    another transformer decoder with inputs of first pass decoder and\n    encoder.\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): encoding dictionary\n        embed_tokens (torch.nn.Embedding): input embedding\n        no_encoder_attn_phase1: if encoder attention should be there in phase 1\n        no_encoder_decoder_attn_phase2: If encoder and decoder attention should\n                                        be there in phase 2\n    """"""\n\n    def __init__(\n        self,\n        args,\n        dictionary,\n        embed_tokens,\n        no_encoder_attn_phase1=None,\n        no_encoder_decoder_attn_phase2=None,\n    ):\n        super().__init__(dictionary)\n        self.register_buffer(""version"", torch.Tensor([3]))\n\n        self.decoder_phase1 = TransformerDecoder(\n            args, dictionary, embed_tokens, no_encoder_attn_phase1\n        )\n        self.decoder_phase2 = TransformerDecoderPhase2(\n            args, dictionary, embed_tokens, no_encoder_decoder_attn_phase2\n        )\n\n    def forward(\n        self, prev_output_tokens, encoder_out=None, incremental_state=None, **unused\n    ):\n        decoder_phase1_output = self.decoder_phase1(\n            prev_output_tokens, encoder_out, incremental_state\n        )\n        decoder_phase2_output = self.decoder_phase2(\n            prev_output_tokens,\n            encoder_out,\n            decoder_phase1_output[1][""inner_states""][-1],\n            incremental_state,\n        )\n        return decoder_phase2_output\n\n\nclass TransformerDecoderPhase2(FairseqIncrementalDecoder):\n    """"""Second phase of decoder layer block\n        Each layer will be of type TransformerDecoderLayerPhase2\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~fairseq.data.Dictionary): encoding dictionary\n        embed_tokens (torch.nn.Embedding): input embedding\n        no_encoder_decoder_attn: If encoder decoder attention is required or not\n    """"""\n\n    def __init__(self, args, dictionary, embed_tokens, no_encoder_decoder_attn=False):\n        super().__init__(dictionary)\n        self.register_buffer(""version"", torch.Tensor([3]))\n\n        self.dropout = args.dropout\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n\n        input_embed_dim = embed_tokens.embedding_dim\n        embed_dim = args.decoder_embed_dim\n        self.output_embed_dim = args.decoder_output_dim\n\n        padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = args.max_target_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_scale = math.sqrt(embed_dim)  # todo: try with input_embed_dim\n\n        self.project_in_dim = (\n            Linear(input_embed_dim, embed_dim, bias=False)\n            if embed_dim != input_embed_dim\n            else None\n        )\n\n        self.embed_positions = (\n            PositionalEmbedding(\n                args.max_target_positions,\n                embed_dim,\n                padding_idx,\n                learned=args.decoder_learned_pos,\n            )\n            if not args.no_token_positional_embeddings\n            else None\n        )\n\n        self.layers = nn.ModuleList([])\n        self.layers.extend(\n            [\n                TransformerDecoderLayerPhase2(args, no_encoder_decoder_attn)\n                for _ in range(args.decoder_layers)\n            ]\n        )\n\n        self.adaptive_softmax = None\n\n        self.project_out_dim = (\n            Linear(embed_dim, self.output_embed_dim, bias=False)\n            if embed_dim != self.output_embed_dim and not args.tie_adaptive_weights\n            else None\n        )\n\n        if args.adaptive_softmax_cutoff is not None:\n            self.adaptive_softmax = AdaptiveSoftmax(\n                len(dictionary),\n                self.output_embed_dim,\n                options.eval_str_list(args.adaptive_softmax_cutoff, type=int),\n                dropout=args.adaptive_softmax_dropout,\n                adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None,\n                factor=args.adaptive_softmax_factor,\n                tie_proj=args.tie_adaptive_proj,\n            )\n        elif not self.share_input_output_embed:\n            self.embed_out = nn.Parameter(\n                torch.Tensor(len(dictionary), self.output_embed_dim)\n            )\n            nn.init.normal_(self.embed_out, mean=0, std=self.output_embed_dim ** -0.5)\n\n        if args.decoder_normalize_before and not getattr(\n            args, ""no_decoder_final_norm"", False\n        ):\n            self.layer_norm = LayerNorm(embed_dim)\n        else:\n            self.layer_norm = None\n\n    def forward(\n        self,\n        prev_output_tokens,\n        encoder_out=None,\n        decoder_out=None,\n        incremental_state=None,\n        **unused\n    ):\n        """"""\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for input feeding/teacher forcing\n            encoder_out (Tensor, optional): output from the encoder, used for\n                encoder-side attention\n            decoder_out (Tensor, optional): output from the decoder phase1, used for\n                decoder-side attention\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n\n        Returns:\n            tuple:\n                - the decoder\'s output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        """"""\n        x, extra = self.extract_features(\n            prev_output_tokens, encoder_out, decoder_out, incremental_state\n        )\n        x = self.output_layer(x)\n        return x, extra\n\n    def extract_features(\n        self,\n        prev_output_tokens,\n        encoder_out=None,\n        decoder_out=None,\n        incremental_state=None,\n        **unused\n    ):\n        """"""\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder\'s features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        """"""\n        # embed positions\n        positions = (\n            self.embed_positions(\n                prev_output_tokens, incremental_state=incremental_state\n            )\n            if self.embed_positions is not None\n            else None\n        )\n\n        if incremental_state is not None:\n            prev_output_tokens = prev_output_tokens[:, -1:]\n            if positions is not None:\n                positions = positions[:, -1:]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n\n        if self.project_in_dim is not None:\n            x = self.project_in_dim(x)\n\n        if positions is not None:\n            x += positions\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n        attn = None\n\n        inner_states = [x]\n\n        # decoder layers\n        for layer in self.layers:\n            x, attn = layer(\n                x,\n                encoder_out[""encoder_out""] if encoder_out is not None else None,\n                encoder_out[""encoder_padding_mask""]\n                if encoder_out is not None\n                else None,\n                decoder_out,\n                incremental_state,\n                self_attn_mask=self.buffered_future_mask(x)\n                if incremental_state is None\n                else None,\n            )\n            inner_states.append(x)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if self.project_out_dim is not None:\n            x = self.project_out_dim(x)\n\n        return x, {""attn"": attn, ""inner_states"": inner_states}\n\n    def output_layer(self, features, **kwargs):\n        """"""Project features to the vocabulary size.""""""\n        if self.adaptive_softmax is None:\n            # project back to size of vocabulary\n            if self.share_input_output_embed:\n                return F.linear(features, self.embed_tokens.weight)\n            else:\n                return F.linear(features, self.embed_out)\n        else:\n            return features\n\n    def max_positions(self):\n        """"""Maximum output length supported by the decoder.""""""\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return min(self.max_target_positions, self.embed_positions.max_positions)\n\n    def buffered_future_mask(self, tensor):\n        dim = tensor.size(0)\n        if (\n            not hasattr(self, ""_future_mask"")\n            or self._future_mask is None\n            or self._future_mask.device != tensor.device\n        ):\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(tensor.new(dim, dim)), 1\n            )\n        if self._future_mask.size(0) < dim:\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(self._future_mask.resize_(dim, dim)), 1\n            )\n        return self._future_mask[:dim, :dim]\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        """"""Upgrade a (possibly old) state dict for new versions of fairseq.""""""\n        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):\n            weights_key = ""{}.embed_positions.weights"".format(name)\n            if weights_key in state_dict:\n                del state_dict[weights_key]\n            state_dict[\n                ""{}.embed_positions._float_tensor"".format(name)\n            ] = torch.FloatTensor(1)\n\n        for i in range(len(self.layers)):\n            # update layer norms\n            layer_norm_map = {\n                ""0"": ""self_attn_layer_norm"",\n                ""1"": ""encoder_attn_layer_norm"",\n                ""2"": ""final_layer_norm"",\n            }\n            for old, new in layer_norm_map.items():\n                for m in (""weight"", ""bias""):\n                    k = ""{}.layers.{}.layer_norms.{}.{}"".format(name, i, old, m)\n                    if k in state_dict:\n                        state_dict[\n                            ""{}.layers.{}.{}.{}"".format(name, i, new, m)\n                        ] = state_dict[k]\n                        del state_dict[k]\n\n        version_key = ""{}.version"".format(name)\n        if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) < 2:\n            # earlier checkpoints did not normalize after the stack of layers\n            self.layer_norm = None\n            self.normalize = False\n            state_dict[version_key] = torch.Tensor([1])\n\n        return state_dict\n\n\nclass TransformerDecoderLayerPhase2(nn.Module):\n    """"""Second phase of decoder layer block\n    This layer will take the input from the ecoder and phirst pass decoder.\n    papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf\n    """"""\n\n    def __init__(\n        self,\n        args,\n        no_encoder_decoder_attn=False,\n        add_bias_kv=False,\n        add_zero_attn=False,\n    ):\n        super().__init__()\n        self.embed_dim = args.decoder_embed_dim\n        self.self_attn = MultiheadAttention(\n            embed_dim=self.embed_dim,\n            num_heads=args.decoder_attention_heads,\n            dropout=args.attention_dropout,\n            add_bias_kv=add_bias_kv,\n            add_zero_attn=add_zero_attn,\n            self_attention=True,\n        )\n        self.dropout = args.dropout\n        self.activation_fn = utils.get_activation_fn(\n            activation=getattr(args, ""activation_fn"", ""relu"")\n        )\n        self.activation_dropout = getattr(args, ""activation_dropout"", 0)\n        if self.activation_dropout == 0:\n            # for backwards compatibility with models that use args.relu_dropout\n            self.activation_dropout = getattr(args, ""relu_dropout"", 0)\n        self.normalize_before = args.decoder_normalize_before\n\n        # use layerNorm rather than FusedLayerNorm for exporting.\n        # char_inputs can be used to determint this.\n        # TODO  remove this once we update apex with the fix\n        export = getattr(args, ""char_inputs"", False)\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        if no_encoder_decoder_attn:\n            self.encoder_attn = None\n            self.decoder_attn = None\n            self.encoder_layer_norm = None\n            self.decoder_layer_norm = None\n        else:\n            self.encoder_attn = MultiheadAttention(\n                self.embed_dim,\n                args.decoder_attention_heads,\n                dropout=args.attention_dropout,\n                encoder_decoder_attention=True,\n            )\n            self.decoder_attn = MultiheadAttention(\n                self.embed_dim,\n                args.decoder_attention_heads,\n                dropout=args.attention_dropout,\n                encoder_decoder_attention=True,\n            )\n            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n            self.decoder_attn_layer_norm = LayerNorm(self.embed_dim, export=export)\n\n        self.fc1 = Linear(self.embed_dim, args.decoder_ffn_embed_dim)\n        self.fc2 = Linear(args.decoder_ffn_embed_dim, self.embed_dim)\n\n        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n        self.need_attn = True\n\n        self.onnx_trace = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def forward(\n        self,\n        x,\n        encoder_out=None,\n        encoder_padding_mask=None,\n        decoder_out=None,\n        incremental_state=None,\n        prev_self_attn_state=None,\n        prev_encoder_attn_state=None,\n        prev_decoder_attn_state=None,\n        self_attn_mask=None,\n        self_attn_padding_mask=None,\n    ):\n        """"""\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n\n        Returns:\n            encoded output of shape `(batch, src_len, embed_dim)`\n        """"""\n        residual = x\n        x_self_attention = self.maybe_layer_norm(\n            self.self_attn_layer_norm, x, before=True\n        )\n        if prev_self_attn_state is not None:\n            if incremental_state is None:\n                incremental_state = {}\n            prev_key, prev_value = prev_self_attn_state\n            saved_state = {""prev_key"": prev_key, ""prev_value"": prev_value}\n            self.self_attn._set_input_buffer(incremental_state, saved_state)\n        x_self_attention, attn = self.self_attn(\n            query=x,\n            key=x,\n            value=x,\n            key_padding_mask=self_attn_padding_mask,\n            incremental_state=incremental_state,\n            need_weights=False,\n            attn_mask=self_attn_mask,\n        )\n        x_self_attention = F.dropout(\n            x_self_attention, p=self.dropout, training=self.training\n        )\n        x_self_attention = residual + x_self_attention\n        x_self_attention = self.maybe_layer_norm(\n            self.self_attn_layer_norm, x_self_attention, after=True\n        )\n\n        if self.encoder_attn is not None:\n            residual = x\n            x_encoder_attention = self.maybe_layer_norm(\n                self.encoder_attn_layer_norm, x, before=True\n            )\n            if prev_encoder_attn_state is not None:\n                if incremental_state is None:\n                    incremental_state = {}\n                prev_key, prev_value = prev_encoder_attn_state\n                saved_state = {""prev_key"": prev_key, ""prev_value"": prev_value}\n                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n            x_encoder_attention, attn = self.encoder_attn(\n                query=x_encoder_attention,\n                key=encoder_out,\n                value=encoder_out,\n                key_padding_mask=encoder_padding_mask,\n                incremental_state=incremental_state,\n                static_kv=True,\n                need_weights=(not self.training and self.need_attn),\n            )\n            x_encoder_attention = F.dropout(\n                x_encoder_attention, p=self.dropout, training=self.training\n            )\n            x_encoder_attention = residual + x_encoder_attention\n            x_encoder_attention = self.maybe_layer_norm(\n                self.encoder_attn_layer_norm, x_encoder_attention, after=True\n            )\n\n        if self.decoder_attn is not None:\n            residual = x\n            x_decoder_attention = self.maybe_layer_norm(\n                self.decoder_attn_layer_norm, x, before=True\n            )\n            if prev_decoder_attn_state is not None:\n                if incremental_state is None:\n                    incremental_state = {}\n                prev_key, prev_value = prev_decoder_attn_state\n                saved_state = {""prev_key"": prev_key, ""prev_value"": prev_value}\n                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n            x_decoder_attention, attn = self.decoder_attn(\n                query=x_decoder_attention,\n                key=decoder_out,\n                value=decoder_out,\n                incremental_state=incremental_state,\n                static_kv=True,\n                need_weights=(not self.training and self.need_attn),\n            )\n            x_decoder_attention = F.dropout(\n                x_decoder_attention, p=self.dropout, training=self.training\n            )\n            x_decoder_attention = residual + x_decoder_attention\n            x_decoder_attention = self.maybe_layer_norm(\n                self.encoder_attn_layer_norm, x_decoder_attention, after=True\n            )\n        x = x_self_attention + x_encoder_attention + x_decoder_attention\n\n        residual = x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n        if self.onnx_trace and incremental_state is not None:\n            saved_state = self.self_attn._get_input_buffer(incremental_state)\n            self_attn_state = saved_state[""prev_key""], saved_state[""prev_value""]\n            return x, attn, self_attn_state\n        return x, attn\n\n    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n        assert before ^ after\n        if after ^ self.normalize_before:\n            return layer_norm(x)\n        else:\n            return x\n\n    def make_generation_fast_(self, need_attn=False, **kwargs):\n        self.need_attn = need_attn\n\n\n@register_model_architecture(""two_phase_transformer"", ""two_phase_transformer"")\ndef base_architecture(args):\n    args.encoder_embed_path = getattr(args, ""encoder_embed_path"", None)\n    args.encoder_embed_dim = getattr(args, ""encoder_embed_dim"", 512)\n    args.encoder_ffn_embed_dim = getattr(args, ""encoder_ffn_embed_dim"", 2048)\n    args.encoder_layers = getattr(args, ""encoder_layers"", 6)\n    args.encoder_attention_heads = getattr(args, ""encoder_attention_heads"", 8)\n    args.encoder_normalize_before = getattr(args, ""encoder_normalize_before"", False)\n    args.encoder_learned_pos = getattr(args, ""encoder_learned_pos"", False)\n    args.decoder_embed_path = getattr(args, ""decoder_embed_path"", None)\n    args.decoder_embed_dim = getattr(args, ""decoder_embed_dim"", args.encoder_embed_dim)\n    args.decoder_ffn_embed_dim = getattr(\n        args, ""decoder_ffn_embed_dim"", args.encoder_ffn_embed_dim\n    )\n    args.decoder_layers = getattr(args, ""decoder_layers"", 6)\n    args.decoder_attention_heads = getattr(args, ""decoder_attention_heads"", 8)\n    args.decoder_normalize_before = getattr(args, ""decoder_normalize_before"", False)\n    args.decoder_learned_pos = getattr(args, ""decoder_learned_pos"", False)\n    args.attention_dropout = getattr(args, ""attention_dropout"", 0.0)\n    args.activation_dropout = getattr(args, ""activation_dropout"", 0.0)\n    args.activation_fn = getattr(args, ""activation_fn"", ""relu"")\n    args.dropout = getattr(args, ""dropout"", 0.1)\n    args.adaptive_softmax_cutoff = getattr(args, ""adaptive_softmax_cutoff"", None)\n    args.adaptive_softmax_dropout = getattr(args, ""adaptive_softmax_dropout"", 0)\n    args.share_decoder_input_output_embed = getattr(\n        args, ""share_decoder_input_output_embed"", False\n    )\n    args.share_all_embeddings = getattr(args, ""share_all_embeddings"", False)\n    args.no_token_positional_embeddings = getattr(\n        args, ""no_token_positional_embeddings"", False\n    )\n    args.adaptive_input = getattr(args, ""adaptive_input"", False)\n\n    args.decoder_output_dim = getattr(\n        args, ""decoder_output_dim"", args.decoder_embed_dim\n    )\n    args.decoder_input_dim = getattr(args, ""decoder_input_dim"", args.decoder_embed_dim)\n\n\n@register_model_architecture(\n    ""two_phase_transformer"", ""two_phase_transformer_architecture""\n)\ndef twophasetransformer_architecture(args):\n    base_architecture(args)\n'"
pytorch_translate/research/knowledge_distillation/__init__.py,0,b''
pytorch_translate/research/knowledge_distillation/collect_top_k_probs.py,4,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import options, progress_bar, utils\nfrom pytorch_translate import hybrid_transformer_rnn  # noqa\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate import transformer  # noqa\nfrom pytorch_translate import (\n    options as pytorch_translate_options,\n    utils as pytorch_translate_utils,\n)\nfrom pytorch_translate.constants import CHECKPOINT_PATHS_DELIMITER\nfrom pytorch_translate.tasks.pytorch_translate_multi_task import (  # noqa\n    PyTorchTranslateMultiTask,\n)\n\n\ndef compute_top_k(\n    task,\n    models,\n    dataset,\n    k,\n    use_cuda,\n    max_tokens=None,\n    max_sentences=None,\n    progress_bar_args=None,\n):\n    """"""\n    This function runs forward computation on an ensemble of trained models\n    using binarized parallel training data and returns the top-k probabilities\n    and their corresponding token indices for each output step.\n\n    Returns: (top_k_scores, top_k_indices)\n        Each a NumPy array of size (total_target_tokens, k)\n    """"""\n    top_k_scores_list = [None for _ in range(len(dataset))]\n    top_k_indices_list = [None for _ in range(len(dataset))]\n\n    itr = task.get_batch_iterator(\n        dataset=dataset, max_tokens=max_tokens, max_sentences=max_sentences\n    ).next_epoch_itr(shuffle=False)\n    if progress_bar_args is not None:\n        itr = progress_bar.build_progress_bar(\n            args=progress_bar_args,\n            iterator=itr,\n            prefix=f""top-k probs eval"",\n            no_progress_bar=""simple"",\n        )\n\n    for sample in itr:\n        sentence_ids = sample[""id""]\n        target_lengths = (\n            (sample[""net_input""][""prev_output_tokens""] != dataset.tgt_dict.pad())\n            .sum(axis=1)\n            .numpy()\n        )\n        if use_cuda:\n            sample = utils.move_to_cuda(sample)\n        avg_probs = None\n        for model in models:\n            with torch.no_grad():\n                net_output = model(**sample[""net_input""])\n                probs = model.get_normalized_probs(net_output, log_probs=False)\n            if avg_probs is None:\n                avg_probs = probs\n            else:\n                avg_probs.add_(probs)\n        avg_probs.div_(len(models))\n\n        top_k_avg_probs, indices = torch.topk(avg_probs, k=k)\n\n        top_k_probs_normalized = F.normalize(top_k_avg_probs, p=1, dim=2).cpu()\n        indices = indices.cpu()\n\n        for i, sentence_id in enumerate(sentence_ids):\n            length = target_lengths[i]\n            top_k_scores_list[sentence_id] = top_k_probs_normalized[i][:length].numpy()\n            top_k_indices_list[sentence_id] = indices[i][:length].numpy()\n\n    assert all(\n        top_k_scores is not None for top_k_scores in top_k_scores_list\n    ), ""scores not calculated for all examples!""\n    assert all(\n        top_k_indices is not None for top_k_indices in top_k_indices_list\n    ), ""indices not calculated for all examples!""\n\n    top_k_scores = np.concatenate(top_k_scores_list, axis=0)\n    top_k_indices = np.concatenate(top_k_indices_list, axis=0)\n\n    return top_k_scores, top_k_indices\n\n\ndef save_top_k(args):\n    """"""\n    This function runs forward computation on an ensemble of trained models\n    using binarized parallel training data and saves the top-k probabilities\n    and their corresponding token indices for each output step.\n\n    Note that the Python binary accepts all generation params, but ignores\n    inapplicable ones (such as those related to output length). --max-tokens\n    is of particular importance to prevent memory errors.\n    """"""\n    pytorch_translate_options.print_args(args)\n    use_cuda = torch.cuda.is_available() and not getattr(args, ""cpu"", False)\n\n    (\n        models,\n        model_args,\n        task,\n    ) = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n        args.path.split(CHECKPOINT_PATHS_DELIMITER)\n    )\n    for model in models:\n        model.eval()\n        if use_cuda:\n            model.cuda()\n\n    append_eos_to_source = model_args[0].append_eos_to_source\n    reverse_source = model_args[0].reverse_source\n    assert all(\n        a.append_eos_to_source == append_eos_to_source\n        and a.reverse_source == reverse_source\n        for a in model_args\n    )\n    assert (\n        args.source_binary_file != """" and args.target_binary_file != """"\n    ), ""collect_top_k_probs requires binarized data.""\n    task.load_dataset(args.gen_subset, args.source_binary_file, args.target_binary_file)\n\n    assert (\n        args.top_k_probs_binary_file != """"\n    ), ""must specify output file (--top-k-probs-binary-file)!""\n    output_path = args.top_k_probs_binary_file\n\n    dataset = task.dataset(args.gen_subset)\n\n    top_k_scores, top_k_indices = compute_top_k(\n        task=task,\n        models=models,\n        dataset=dataset,\n        k=args.k_probs_to_collect,\n        use_cuda=use_cuda,\n        max_tokens=args.teacher_max_tokens,\n        max_sentences=args.max_sentences,\n        progress_bar_args=args,\n    )\n\n    np.savez(output_path, top_k_scores=top_k_scores, top_k_indices=top_k_indices)\n    print(\n        f""Saved top {top_k_scores.shape[1]} probs for a total of ""\n        f""{top_k_scores.shape[0]} tokens to file {output_path}""\n    )\n\n\ndef get_parser_with_args():\n    parser = options.get_parser(""Collect Top-K Probs"", default_task=""pytorch_translate"")\n    pytorch_translate_options.add_verbosity_args(parser)\n    pytorch_translate_options.add_dataset_args(parser, gen=True)\n    generation_group = options.add_generation_args(parser)\n\n    generation_group.add_argument(\n        ""--source-binary-file"",\n        default="""",\n        help=""Path for the binary file containing source eval examples. ""\n        ""(Overrides --source-text-file. Must be used in conjunction with ""\n        ""--target-binary-file)."",\n    )\n    generation_group.add_argument(\n        ""--target-binary-file"",\n        default="""",\n        help=""Path for the binary file containing target eval examples. ""\n        ""(Overrides --target-text-file. Must be used in conjunction with ""\n        ""--source-binary-file)."",\n    )\n    generation_group.add_argument(\n        ""--k-probs-to-collect"",\n        type=int,\n        default=8,\n        help=""Number of probabilities to collect for each output step."",\n    )\n    generation_group.add_argument(\n        ""--top-k-probs-binary-file"",\n        type=str,\n        default="""",\n        help=""File into which to save top-K probabilities for each token."",\n    )\n    generation_group.add_argument(\n        ""--teacher-max-tokens"",\n        type=int,\n        default=1000,\n        help=""Maximum number of words in minibatch for teacher to score."",\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser_with_args()\n    args = options.parse_args_and_arch(parser)\n    save_top_k(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pytorch_translate/research/knowledge_distillation/dual_decoder_kd_loss.py,0,"b'#!/usr/bin/env python3\n\nimport math\n\nfrom fairseq import utils\nfrom fairseq.criterions import LegacyFairseqCriterion, register_criterion\n\n\n@register_criterion(""dual_decoder_kd_loss"")\nclass DualDecoderCriterion(LegacyFairseqCriterion):\n    def __init__(self, args, task):\n        super().__init__(args, task)\n        self.eps = args.label_smoothing\n        self.kd_weight = args.kd_weight\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--label-smoothing"",\n            default=0.0,\n            type=float,\n            metavar=""D"",\n            help=""[teacher decoder only] epsilon for label smoothing, 0 means ""\n            ""no label smoothing."",\n        )\n        parser.add_argument(\n            ""--kd-weight"",\n            type=float,\n            default=0.5,\n            help=(\n                ""[student decoder only] mixture weight between the knowledge ""\n                ""distillation and negative log likelihood losses. Must be in ""\n                ""[0.0, 1.0]."",\n            ),\n        )\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n\n        src_tokens = sample[""net_input""][""src_tokens""]\n        src_lengths = sample[""net_input""][""src_lengths""]\n        prev_output_tokens = sample[""net_input""][""prev_output_tokens""]\n\n        encoder_out = model.encoder(src_tokens, src_lengths)\n        student_output = model.student_decoder(prev_output_tokens, encoder_out)\n        teacher_output = model.teacher_decoder(prev_output_tokens, encoder_out)\n\n        teacher_loss, teacher_nll_loss, teacher_probs = self.compute_teacher_loss(\n            model, teacher_output, sample, reduce=reduce\n        )\n\n        # do not propagate gradient from student loss to teacher output\n        teacher_probs = teacher_probs.detach()\n        student_loss, student_nll_loss = self.compute_student_loss(\n            model, student_output, sample, teacher_probs, reduce=reduce\n        )\n\n        total_loss = student_loss + teacher_loss\n\n        sample_size = (\n            sample[""target""].size(0) if self.args.sentence_avg else sample[""ntokens""]\n        )\n        logging_output = {\n            ""teacher_loss"": utils.item(teacher_loss.data)\n            if reduce\n            else teacher_loss.data,\n            ""teacher_nll_loss"": utils.item(teacher_nll_loss.data)\n            if reduce\n            else teacher_nll_loss.data,\n            ""student_loss"": utils.item(student_loss.data)\n            if reduce\n            else student_loss.data,\n            ""student_nll_loss"": utils.item(student_nll_loss.data)\n            if reduce\n            else student_nll_loss.data,\n            ""loss"": utils.item(total_loss.data) if reduce else total_loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n        }\n        return total_loss, sample_size, logging_output\n\n    def compute_teacher_loss(self, model, net_output, sample, reduce=True):\n        probs = model.get_normalized_probs(net_output, log_probs=False)\n        probs = probs.view(-1, probs.size(-1))\n        lprobs = probs.log()\n        target = model.get_targets(sample, net_output).view(-1, 1)\n        non_pad_mask = target.ne(self.padding_idx)\n        nll_loss = -lprobs.gather(dim=-1, index=target)[non_pad_mask]\n        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)[non_pad_mask]\n        if reduce:\n            nll_loss = nll_loss.sum()\n            smooth_loss = smooth_loss.sum()\n        eps_i = self.eps / lprobs.size(-1)\n        loss = (1.0 - self.eps) * nll_loss + eps_i * smooth_loss\n        return loss, nll_loss, probs\n\n    def compute_student_loss(\n        self, model, net_output, sample, teacher_probs, reduce=True\n    ):\n        lprobs = model.get_normalized_probs(net_output, log_probs=True)\n        lprobs = lprobs.view(-1, lprobs.size(-1))\n        target = model.get_targets(sample, net_output).view(-1, 1)\n        non_pad_mask = target.ne(self.padding_idx)\n        nll_loss = -lprobs.gather(dim=-1, index=target)[non_pad_mask]\n        kd_loss = (teacher_probs * -lprobs).sum(dim=-1, keepdim=True)[non_pad_mask]\n        if reduce:\n            nll_loss = nll_loss.sum()\n            kd_loss = kd_loss.sum()\n        loss = (1.0 - self.kd_weight) * nll_loss + self.kd_weight * kd_loss\n        return loss, nll_loss\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n        nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n        sample_size = sum(log.get(""sample_size"", 0) for log in logging_outputs)\n        return {\n            ""student_loss"": sum(log.get(""student_loss"", 0) for log in logging_outputs)\n            / sample_size\n            / math.log(2),\n            ""student_nll_loss"": sum(\n                log.get(""student_nll_loss"", 0) for log in logging_outputs\n            )\n            / ntokens\n            / math.log(2),\n            ""teacher_loss"": sum(log.get(""teacher_loss"", 0) for log in logging_outputs)\n            / sample_size\n            / math.log(2),\n            ""teacher_nll_loss"": sum(\n                log.get(""teacher_nll_loss"", 0) for log in logging_outputs\n            )\n            / ntokens\n            / math.log(2),\n            ""loss"": sum(log.get(""loss"", 0) for log in logging_outputs)\n            / sample_size\n            / math.log(2),\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""sample_size"": sample_size,\n        }\n'"
pytorch_translate/research/knowledge_distillation/dual_decoder_kd_model.py,0,"b'#!/usr/bin/env python3\n\nfrom fairseq.models import (\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom pytorch_translate import (\n    hybrid_transformer_rnn,\n    transformer as pytorch_translate_transformer,\n)\nfrom pytorch_translate.utils import torch_find\n\n\n@register_model(""dual_decoder_kd"")\nclass DualDecoderKDModel(FairseqEncoderDecoderModel):\n    def __init__(self, task, encoder, teacher_decoder, student_decoder):\n        super().__init__(encoder, student_decoder)\n        self.teacher_decoder = teacher_decoder\n        self.student_decoder = student_decoder\n        self.using_teacher = True\n        self.task = task\n\n    def get_teacher_model(self):\n        return pytorch_translate_transformer.TransformerModel(\n            self.task, self.encoder, self.teacher_decoder\n        )\n\n    def get_student_model(self):\n        return hybrid_transformer_rnn.HybridTransformerRNNModel(\n            self.task, self.encoder, self.student_decoder\n        )\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n\n        # command-line args for transformer model are used to build\n        # encoder and teacher decoder\n        pytorch_translate_transformer.TransformerModel.add_args(parser)\n\n        # distinct args for student decoder\n        parser.add_argument(\n            ""--student-decoder-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] decoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--student-decoder-layers"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] num decoder layers"",\n        )\n        parser.add_argument(\n            ""--student-decoder-attention-heads"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] num decoder attention heads"",\n        )\n        parser.add_argument(\n            ""--student-decoder-lstm-units"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] num LSTM units for each decoder layer"",\n        )\n        parser.add_argument(\n            ""--student-decoder-out-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] decoder output embedding dimension"",\n        )\n        parser.add_argument(\n            ""--student-decoder-reduced-attention-dim"",\n            type=int,\n            default=None,\n            metavar=""N"",\n            help=""if specified, computes attention with this dimensionality ""\n            ""in the student decoder (instead of using encoder output dims)"",\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure that all args are properly defaulted\n        # (in case there are any new ones)\n        base_architecture(args)\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        encoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=src_dict,\n            embed_dim=args.encoder_embed_dim,\n            path=args.encoder_pretrained_embed,\n            freeze=args.encoder_freeze_embed,\n        )\n\n        teacher_decoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=tgt_dict,\n            embed_dim=args.decoder_embed_dim,\n            path=args.decoder_pretrained_embed,\n            freeze=args.decoder_freeze_embed,\n        )\n\n        student_decoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=tgt_dict, embed_dim=args.student_decoder_embed_dim\n        )\n\n        encoder = pytorch_translate_transformer.TransformerEncoder(\n            args, src_dict, encoder_embed_tokens, proj_to_decoder=True\n        )\n\n        teacher_decoder = pytorch_translate_transformer.TransformerModel.build_decoder(\n            args, src_dict, tgt_dict, embed_tokens=teacher_decoder_embed_tokens\n        )\n\n        student_decoder = StudentHybridRNNDecoder(\n            args, src_dict, tgt_dict, student_decoder_embed_tokens\n        )\n\n        return DualDecoderKDModel(\n            task=task,\n            encoder=encoder,\n            teacher_decoder=teacher_decoder,\n            student_decoder=student_decoder,\n        )\n\n    def get_targets(self, sample, net_output):\n        targets = sample[""target""].view(-1)\n        possible_translation_tokens = net_output[-1]\n        if possible_translation_tokens is not None:\n            targets = torch_find(\n                possible_translation_tokens, targets, len(self.task.target_dictionary)\n            )\n        return targets\n\n\nclass StudentHybridRNNDecoder(hybrid_transformer_rnn.HybridRNNDecoder):\n    """"""\n    Subclass which constructs RNN decoder from student arguments.\n    (dropout, attention_dropout, and vocab reduction params shared with teacher.)\n    """"""\n\n    def _init_dims(self, args, src_dict, dst_dict, embed_tokens):\n        self.dropout = args.dropout\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_tokens = embed_tokens\n\n        self.lstm_units = args.student_decoder_lstm_units\n        self.num_layers = args.student_decoder_layers\n        self.initial_input_dim = embed_dim\n\n        # for compatibility with transformer dimensions in encoder\n        # and teacher decoder are different\n        self.encoder_output_dim = args.decoder_embed_dim\n        if args.student_decoder_reduced_attention_dim is None:\n            self.attention_dim = self.encoder_output_dim\n        else:\n            self.attention_dim = args.student_decoder_reduced_attention_dim\n        self.input_dim = self.lstm_units + self.attention_dim\n\n        self.num_attention_heads = args.student_decoder_attention_heads\n        self.bottleneck_dim = args.student_decoder_out_embed_dim\n\n\n@register_model_architecture(""dual_decoder_kd"", ""dual_decoder_kd"")\ndef base_architecture(args):\n    pytorch_translate_transformer.base_architecture(args)\n    args.student_decoder_embed_dim = getattr(args, ""student_decoder_embed_dim"", 128)\n    args.student_decoder_layers = getattr(args, ""student_decoder_layers"", 3)\n    args.student_decoder_attention_heads = getattr(\n        args, ""student_decoder_attention_heads"", 8\n    )\n    args.student_decoder_lstm_units = getattr(args, ""student_decoder_lstm_units"", 128)\n    args.student_decoder_out_embed_dim = getattr(\n        args, ""student_decoder_out_embed_dim"", 128\n    )\n    args.student_decoder_reduced_attention_dim = getattr(\n        args, ""student_decoder_reduced_attention_dim"", None\n    )\n'"
pytorch_translate/research/knowledge_distillation/hybrid_dual_decoder_kd_model.py,0,"b'#!/usr/bin/env python3\n\nfrom fairseq.models import (\n    FairseqEncoderDecoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom pytorch_translate import (\n    hybrid_transformer_rnn,\n    transformer as pytorch_translate_transformer,\n)\nfrom pytorch_translate.utils import torch_find\n\n\n@register_model(""hybrid_dual_decoder_kd"")\nclass HybridDualDecoderKDModel(FairseqEncoderDecoderModel):\n    def __init__(self, task, encoder, teacher_decoder, student_decoder):\n        super().__init__(encoder, student_decoder)\n        self.teacher_decoder = teacher_decoder\n        self.student_decoder = student_decoder\n        self.using_teacher = True\n        self.task = task\n\n    def get_teacher_model(self):\n        return hybrid_transformer_rnn.HybridTransformerRNNModel(\n            self.task, self.encoder, self.teacher_decoder\n        )\n\n    def get_student_model(self):\n        return hybrid_transformer_rnn.HybridTransformerRNNModel(\n            self.task, self.encoder, self.student_decoder\n        )\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add model-specific arguments to the parser.""""""\n\n        # command-line args for hybrid_transformer_rnn model are used to build\n        # encoder and teacher decoder\n        hybrid_transformer_rnn.HybridTransformerRNNModel.add_args(parser)\n\n        # distinct args for student decoder\n        parser.add_argument(\n            ""--student-decoder-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] decoder embedding dimension"",\n        )\n        parser.add_argument(\n            ""--student-decoder-layers"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] num decoder layers"",\n        )\n        parser.add_argument(\n            ""--student-decoder-attention-heads"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] num decoder attention heads"",\n        )\n        parser.add_argument(\n            ""--student-decoder-lstm-units"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] num LSTM units for each decoder layer"",\n        )\n        parser.add_argument(\n            ""--student-decoder-out-embed-dim"",\n            type=int,\n            metavar=""N"",\n            help=""[student RNN] decoder output embedding dimension"",\n        )\n        parser.add_argument(\n            ""--student-decoder-reduced-attention-dim"",\n            type=int,\n            default=None,\n            metavar=""N"",\n            help=""if specified, computes attention with this dimensionality ""\n            ""in the student decoder (instead of using encoder output dims)"",\n        )\n\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure that all args are properly defaulted\n        # (in case there are any new ones)\n        base_architecture(args)\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        encoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=src_dict,\n            embed_dim=args.encoder_embed_dim,\n            path=args.encoder_pretrained_embed,\n            freeze=args.encoder_freeze_embed,\n        )\n\n        teacher_decoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=tgt_dict, embed_dim=args.decoder_embed_dim\n        )\n\n        student_decoder_embed_tokens = pytorch_translate_transformer.build_embedding(\n            dictionary=tgt_dict, embed_dim=args.student_decoder_embed_dim\n        )\n\n        encoder = pytorch_translate_transformer.TransformerEncoder(\n            args, src_dict, encoder_embed_tokens, proj_to_decoder=False\n        )\n\n        teacher_decoder = hybrid_transformer_rnn.HybridRNNDecoder(\n            args, src_dict, tgt_dict, teacher_decoder_embed_tokens\n        )\n\n        student_decoder = StudentHybridRNNDecoder(\n            args, src_dict, tgt_dict, student_decoder_embed_tokens\n        )\n\n        return HybridDualDecoderKDModel(\n            task=task,\n            encoder=encoder,\n            teacher_decoder=teacher_decoder,\n            student_decoder=student_decoder,\n        )\n\n    def get_targets(self, sample, net_output):\n        targets = sample[""target""].view(-1)\n        possible_translation_tokens = net_output[-1]\n        if possible_translation_tokens is not None:\n            targets = torch_find(\n                possible_translation_tokens, targets, len(self.task.target_dictionary)\n            )\n        return targets\n\n\nclass StudentHybridRNNDecoder(hybrid_transformer_rnn.HybridRNNDecoder):\n    """"""\n    Subclass which constructs RNN decoder from student arguments.\n    (dropout, attention_dropout, and vocab reduction params shared with teacher.)\n    """"""\n\n    def _init_dims(self, args, src_dict, dst_dict, embed_tokens):\n        self.dropout = args.dropout\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_tokens = embed_tokens\n\n        self.lstm_units = args.student_decoder_lstm_units\n        self.num_layers = args.student_decoder_layers\n        self.initial_input_dim = embed_dim\n\n        self.encoder_output_dim = args.encoder_embed_dim\n        if args.student_decoder_reduced_attention_dim is None:\n            self.attention_dim = self.encoder_output_dim\n        else:\n            self.attention_dim = args.student_decoder_reduced_attention_dim\n        self.input_dim = self.lstm_units + self.attention_dim\n\n        self.num_attention_heads = args.student_decoder_attention_heads\n        self.bottleneck_dim = args.student_decoder_out_embed_dim\n\n\n@register_model_architecture(""hybrid_dual_decoder_kd"", ""hybrid_dual_decoder_kd"")\ndef base_architecture(args):\n    hybrid_transformer_rnn.base_architecture(args)\n    args.student_decoder_embed_dim = getattr(args, ""student_decoder_embed_dim"", 128)\n    args.student_decoder_layers = getattr(args, ""student_decoder_layers"", 3)\n    args.student_decoder_attention_heads = getattr(\n        args, ""student_decoder_attention_heads"", 8\n    )\n    args.student_decoder_lstm_units = getattr(args, ""student_decoder_lstm_units"", 128)\n    args.student_decoder_out_embed_dim = getattr(\n        args, ""student_decoder_out_embed_dim"", 128\n    )\n'"
pytorch_translate/research/knowledge_distillation/knowledge_distillation_loss.py,3,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.criterions import LegacyFairseqCriterion, register_criterion\nfrom pytorch_translate import utils as pytorch_translate_utils\n\n\n@register_criterion(""word_knowledge_distillation"")\nclass KnowledgeDistillationCriterion(LegacyFairseqCriterion):\n    def __init__(self, args, task):\n        """"""\n        This code is for word-level knowledge distillation. Most of the algorithm\n        is inspired from the Kim and Rush (2016) paper:\n        http://www.aclweb.org/anthology/D16-1139\n        """"""\n        super().__init__(args, task)\n        self.kd_weight = getattr(args, ""kd_weight"", 0)\n        if self.kd_weight < 0 or self.kd_weight > 1:\n            raise ValueError(f""--kd-weight ({self.kd_weight}) must be in [0, 1]"")\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        parser.add_argument(\n            ""--kd-weight"",\n            type=float,\n            default=0.0,\n            help=(\n                ""mixture weight between the knowledge distillation and"",\n                ""negative log likelihood losses. Must be in [0.0, 1.0]"",\n            ),\n        )\n\n    def get_kd_loss(self, sample, student_lprobs, lprobs):\n        """"""\n        The second return argument is used for unit testing.\n\n        Args:\n            * sample: batched sample that has teacher score keys (top_k_scores and\n             top_k_indices)\n            * student_lprobs: tensor of student log probabilities\n            * lprobs: flat version of student_lprobs\n        """"""\n        top_k_teacher_probs_normalized = sample[""top_k_scores""]\n        indices = sample[""top_k_indices""]\n\n        assert indices.shape[0:1] == student_lprobs.shape[0:1]\n\n        kd_loss = -torch.sum(\n            torch.gather(student_lprobs, 2, indices)\n            * top_k_teacher_probs_normalized.float()\n        )\n        return kd_loss\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss, as a Variable\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n\n        # 1. Generate translation using student model\n        net_output = model(**sample[""net_input""])\n        student_lprobs = model.get_normalized_probs(net_output, log_probs=True)\n        # [bsz, seqlen, vocab] -> [bsz*seqlen, vocab]\n        lprobs = student_lprobs.view(-1, student_lprobs.size(-1))\n\n        # 2. Get translation from teacher models and calulate KD loss.\n        kd_loss = None\n        if ""top_k_scores"" in sample:\n            # top_k_scores is not present in the validation data.\n            kd_loss = self.get_kd_loss(sample, student_lprobs, lprobs)\n\n        # 3. Compute NLL loss with respect to the ground truth\n        target = model.get_targets(sample, net_output).view(-1)\n        nll_loss = F.nll_loss(\n            lprobs,\n            target,\n            size_average=False,\n            ignore_index=self.padding_idx,\n            reduce=reduce,\n        )\n\n        # 4. Linearly interpolate between NLL and KD loss\n        if kd_loss is not None:\n            loss = kd_loss * self.kd_weight + nll_loss * (1 - self.kd_weight)\n        else:\n            loss = nll_loss\n\n        if self.args.sentence_avg:\n            sample_size = sample[""target""].size(0)\n        else:\n            sample_size = sample[""ntokens""]\n        if self.args.sentence_avg:\n            sample_size = sample[""target""].size(0)\n        else:\n            sample_size = sample[""ntokens""]\n        logging_output = {\n            ""loss"": utils.item(loss.data) if reduce else loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsamples"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(""loss"", 0) for log in logging_outputs)\n        ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n        nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n        sample_size = sum(log.get(""sample_size"", 0) for log in logging_outputs)\n        agg_output = {\n            ""loss"": loss_sum / sample_size / math.log(2),\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""sample_size"": sample_size,\n        }\n        if sample_size != ntokens:\n            agg_output[""nll_loss""] = loss_sum / ntokens / math.log(2)\n        return agg_output\n'"
pytorch_translate/research/knowledge_distillation/teacher_score_data.py,6,"b'#!/usr/bin/env python3\n\nimport math\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import data, utils\nfrom pytorch_translate import utils as pytorch_translate_utils\n\n\nMEM_SPLIT_SIZE = 10\n\n\nclass TeacherDataset(data.language_pair_dataset.LanguagePairDataset):\n    """"""\n    Extension of fairseq.data.LanguagePairDataset where each example\n    has a score from a ensemble of teacher models.\n    """"""\n\n    def __init__(\n        self,\n        src,\n        src_sizes,\n        src_dict,\n        tgt=None,\n        tgt_sizes=None,\n        tgt_dict=None,\n        top_k_probs_binary_file: Optional[str] = None,\n        teacher_models: List[Any] = None,\n        top_k_teacher_scores: Dict[int, np.ndarray] = None,\n        top_k_teacher_indices: Dict[int, np.ndarray] = None,\n        top_k_teacher_tokens=8,\n        **kwargs,\n    ):\n        """"""\n        Args:\n            task: The task for which the teacher models are expected to make\n                prediction.\n            teacher_models: A list of preloaded teacher models.\n            top_k_teacher_tokens: The number of top word-level prediction that\n                want to save from the teacher predictions.\n            top_k_teacher_scores: A dictionary for memoization that is passed\n                between the dataset and PytorchKnowledgeDistillationTask.\n            top_k_teacher_scores:  A  dictionary for memoization indices of scores;\n                similar to the previous argument, it is passed between the dataset\n                and PytorchKnowledgeDistillationTask.\n        """"""\n        super().__init__(src, src_sizes, src_dict, tgt, tgt_sizes, tgt_dict, **kwargs)\n        self.src_dict = src_dict\n\n        self.top_k_probs_binary_file = top_k_probs_binary_file\n        self.teacher_models = teacher_models\n        self.top_k_teacher_tokens = top_k_teacher_tokens\n        self.top_k_teacher_scores = top_k_teacher_scores\n        self.top_k_teacher_indices = top_k_teacher_indices\n\n        self.all_sen_ids_memoized = False\n\n        if self.top_k_probs_binary_file is not None:\n            npz = np.load(self.top_k_probs_binary_file)\n            flat_scores = npz[""top_k_scores""]\n            flat_indices = npz[""top_k_indices""]\n\n            offset = 0\n            for i, length in enumerate(tgt_sizes):\n                scores = flat_scores[offset : offset + length, :]\n                indices = flat_indices[offset : offset + length, :]\n                offset += length\n                self.top_k_teacher_scores[i] = torch.Tensor(scores)\n                self.top_k_teacher_indices[i] = torch.LongTensor(indices)\n\n    def __len__(self):\n        return super().__len__()\n\n    def collater(self, dataset_samples):\n        return TeacherDataset.collate(\n            dataset_samples,\n            self.teacher_models,\n            self.top_k_teacher_tokens,\n            self.src_dict.pad(),\n            self.src_dict.eos(),\n            self.top_k_teacher_scores,\n            self.top_k_teacher_indices,\n        )\n\n    @staticmethod\n    def collate(\n        dataset,\n        teacher_models,\n        top_k_teacher_tokens,\n        pad_idx,\n        eos_idx,\n        top_k_teacher_scores: Dict[int, np.ndarray],\n        top_k_teacher_indices: Dict[int, np.ndarray],\n        left_pad_source=False,\n    ):\n        if len(dataset) == 0:\n            return {}\n\n        batched_samples = data.language_pair_dataset.collate(\n            dataset, pad_idx, eos_idx, left_pad_source\n        )\n\n        sen_ids = batched_samples[""id""].numpy()\n\n        if teacher_models is not None:\n            all_sen_ids_memoized = all(id in top_k_teacher_scores for id in sen_ids)\n\n            if not all_sen_ids_memoized:\n                # Because there is a high chance that the batches do not fit into memory\n                # for big batches, we have to split them into smaller batches and\n                # memoize their values separately.\n                smaller_datasets = []\n\n                chunk_size = max(1, math.ceil(len(dataset) / MEM_SPLIT_SIZE))\n                for i in range(MEM_SPLIT_SIZE):\n                    small_data = dataset[\n                        chunk_size * i : min(len(dataset), (i + 1) * chunk_size)\n                    ]\n                    if len(small_data) > 0:\n                        smaller_datasets.append(small_data)\n\n                for smaller_data in smaller_datasets:\n                    smaller_batch = data.language_pair_dataset.collate(\n                        smaller_data, pad_idx, eos_idx, left_pad_source\n                    )\n\n                    sen_ids_this_batch = smaller_batch[""id""].numpy()\n\n                    # smaller_batch is natively on CPU. We want to make sure that\n                    # the teacher models run on GPU.\n                    net_input = {\n                        key: pytorch_translate_utils.maybe_cuda(\n                            smaller_batch[""net_input""][key]\n                        )\n                        for key in smaller_batch[""net_input""].keys()\n                    }\n\n                    teacher_output = teacher_models[0](**net_input)\n                    avg_teacher_probs = teacher_models[0].get_normalized_probs(\n                        teacher_output, log_probs=False\n                    )\n\n                    for i in range(1, len(teacher_models)):\n                        teacher_output = teacher_models[i](**net_input)\n                        probs = teacher_models[i].get_normalized_probs(\n                            teacher_output, log_probs=False\n                        )\n                        avg_teacher_probs.add_(probs)\n\n                    avg_teacher_probs.div_(len(teacher_models))\n                    avg_teacher_probs = avg_teacher_probs.detach()\n\n                    # Getting the topk probabilities, masking others,\n                    # normalizing the topk probabilities.\n                    top_k_teacher_tokens_avg_probs, indices = torch.topk(\n                        avg_teacher_probs, k=top_k_teacher_tokens\n                    )\n                    top_k_teacher_probs_normalized = F.normalize(\n                        top_k_teacher_tokens_avg_probs, p=1, dim=2\n                    ).cpu()\n                    indices = indices.cpu()\n\n                    # Memoization\n                    for id_index, id in enumerate(sen_ids_this_batch):\n                        target_length = sum(\n                            (batched_samples[""target""][id_index] != pad_idx).numpy()\n                        )\n                        if id not in top_k_teacher_scores:\n                            top_k_teacher_scores[id] = top_k_teacher_probs_normalized[\n                                id_index\n                            ][:target_length, :]\n                            top_k_teacher_indices[id] = indices[id_index][\n                                :target_length, :\n                            ]\n            else:\n                # We assume that when there is a batch which is entirely memoized\n                # that means we do not need the teacher models anymore, and\n                # it is better to remove them from memory.\n                if len(teacher_models) > 0:\n                    del teacher_models[:]\n\n        # Now we assume that all values are already memoized.\n        # Preparing all zero scores and gradually filling them in.\n        max_ntokens = batched_samples[""target""].shape[1]\n        memoized_probs = torch.zeros(len(sen_ids), max_ntokens, top_k_teacher_tokens)\n        memoized_prob_idx = torch.zeros(\n            len(sen_ids), max_ntokens, top_k_teacher_tokens\n        ).long()\n\n        for idx, id in enumerate(sen_ids):\n            memoized_probs[idx][\n                : top_k_teacher_scores[id].shape[0]\n            ] = top_k_teacher_scores[id]\n            memoized_prob_idx[idx][\n                : top_k_teacher_indices[id].shape[0]\n            ] = top_k_teacher_indices[id]\n        batched_samples[""top_k_scores""] = memoized_probs\n        batched_samples[""top_k_indices""] = memoized_prob_idx\n        return batched_samples\n'"
pytorch_translate/research/lexical_choice/__init__.py,0,b''
pytorch_translate/research/lexical_choice/lexical_translation.py,4,"b'#!/usr/bin/env python3\n\nimport torch\n\n\ndef attention_weighted_src_embedding(\n    src_embedding, attn_scores, activation_fn=torch.tanh\n):\n    """"""\n    use the attention weights to form a weighted average of embeddings\n    parameters:\n        src_embedding:  srclen x bsz x embeddim\n        attn_scores: bsz x tgtlen x srclen\n    return:\n        lex: bsz x tgtlen x embeddim\n    """"""\n    # lexical choice varying lengths: T x B x C -> B x T x C\n    src_embedding = src_embedding.transpose(0, 1)\n\n    lex = torch.bmm(attn_scores, src_embedding)\n    lex = activation_fn(lex)\n    return lex\n\n\ndef lex_logits(lex_h, output_projection_w_lex, output_projection_b_lex, logits_shape):\n    """"""\n    calculate the logits of lexical layer output\n    """"""\n    projection_lex_flat = torch.matmul(output_projection_w_lex, lex_h.t()).t()\n\n    logits = (\n        torch.onnx.operators.reshape_from_tensor_shape(\n            projection_lex_flat, logits_shape\n        )\n        + output_projection_b_lex\n    )\n    return logits\n'"
pytorch_translate/research/multisource/__init__.py,0,b''
pytorch_translate/research/multisource/multisource_data.py,2,"b'#!/usr/bin/env python3\n\nimport numpy as np\nimport torch\nfrom fairseq import data\n\n\nclass MultisourceLanguagePairDataset(data.LanguagePairDataset):\n    """"""A language pair dataset with multiple source sentences for\n    each target sentence.""""""\n\n    def __getitem__(self, i):\n        source = [src_sent.long() for src_sent in self.src[i]]\n        res = {""id"": i, ""source"": source}\n        if self.tgt:\n            res[""target""] = self.tgt[i].long()\n\n        return res\n\n    def collater(self, samples):\n        return MultisourceLanguagePairDataset.collate(\n            samples,\n            self.src_dict.pad(),\n            self.src_dict.eos(),\n            self.tgt is not None,\n            self.left_pad_source,\n            self.left_pad_target,\n        )\n\n    @staticmethod\n    def collate(\n        samples,\n        pad_idx,\n        eos_idx,\n        has_target=True,\n        left_pad_source=True,\n        left_pad_target=False,\n    ):\n        if len(samples) == 0:\n            return {}\n\n        n_sources = len(samples[0][""source""])\n        assert all(\n            len(sample[""source""]) == n_sources for sample in samples\n        ), ""All samples in a batch must have the same number of source sentences.""\n\n        def merge(key, left_pad, source=False, move_eos_to_beginning=False):\n            if source:\n                # Collate source sentences all source sentences together. Each\n                return data.data_utils.collate_tokens(\n                    [s[key][src_id] for s in samples for src_id in range(n_sources)],\n                    pad_idx,\n                    eos_idx,\n                    left_pad,\n                    move_eos_to_beginning,\n                )\n            else:\n                return data.data_utils.collate_tokens(\n                    [s[key] for s in samples],\n                    pad_idx,\n                    eos_idx,\n                    left_pad,\n                    move_eos_to_beginning,\n                )\n\n        id = torch.LongTensor([s[""id""] for s in samples])\n        src_tokens = merge(""source"", left_pad=left_pad_source, source=True)\n        # We sort all source sentences from each batch element by length\n        src_lengths = torch.LongTensor(\n            [\n                s[""source""][src_id].numel()\n                for s in samples\n                for src_id in range(n_sources)\n            ]\n        )\n        src_lengths, sort_order = src_lengths.sort(descending=True)\n        # id = id.index_select(0, sort_order)\n        src_tokens = src_tokens.index_select(0, sort_order)\n        # Record which sentence corresponds to which source and sample\n        _, rev_order = sort_order.sort()\n        # srcs_ids[k] contains the indices of kth source sentences of each\n        # sample in src_tokens\n        srcs_ids = [rev_order[k::n_sources] for k in range(n_sources)]\n\n        prev_output_tokens = None\n        target = None\n        ntokens = None\n        if has_target:\n            target = merge(""target"", left_pad=left_pad_target)\n            # we create a shifted version of targets for feeding the\n            # previous output token(s) into the next decoder step\n            prev_output_tokens = merge(\n                ""target"", left_pad=left_pad_target, move_eos_to_beginning=True\n            )\n            ntokens = sum(len(s[""target""]) for s in samples)\n\n        return {\n            ""id"": id,\n            ""ntokens"": ntokens,\n            ""net_input"": {\n                ""src_tokens"": src_tokens,\n                ""src_lengths"": src_lengths,\n                ""src_ids"": srcs_ids,\n                ""prev_output_tokens"": prev_output_tokens,\n            },\n            ""target"": target,\n        }\n\n\nclass IndexedRawTextMultisentDataset(data.IndexedRawTextDataset):\n    """"""Takes a list of text file as input and binarizes them in memory at\n    instantiation. Original lines are also kept in memory""""""\n\n    def read_data(self, paths, dictionary):\n        for path in paths:\n            with open(path, ""r"") as f:\n                file_lines = []\n                file_tokens_list = []\n                file_sizes = []\n                for line in f:\n                    file_lines.append(line.strip(""\\n""))\n                    tokens = dictionary.encode_line(\n                        line,\n                        add_if_not_exist=False,\n                        append_eos=self.append_eos,\n                        reverse_order=self.reverse_order,\n                    )\n                    file_tokens_list.append(tokens)\n                    file_sizes.append(len(tokens))\n                self.lines.append(file_lines)\n                self.tokens_list.append(file_tokens_list)\n                self.sizes.append(file_sizes)\n        # Zip all sentences for each sample together\n        self.lines = list(zip(*self.lines))\n        self.tokens_list = list(zip(*self.tokens_list))\n        # Sum sentence sizes for each sample\n        self.sizes = np.asarray(self.sizes).sum(axis=0)\n'"
pytorch_translate/research/multisource/multisource_decode.py,23,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nfrom fairseq import utils\nfrom fairseq.models import FairseqIncrementalDecoder\n\n\nclass MultiSourceSequenceGenerator(torch.nn.Module):\n    # We use the 1st source sentence as reference alignment\n    align_to = 0\n\n    def __init__(\n        self,\n        models,\n        tgt_dict,\n        beam_size=1,\n        minlen=1,\n        maxlen=None,\n        stop_early=True,\n        normalize_scores=True,\n        len_penalty=0,\n        unk_reward=0,\n        lexicon_reward=0,\n        retain_dropout=False,\n        word_reward=0,\n        model_weights=None,\n        use_char_source=False,\n        align_to=1,\n    ):\n        """"""Generates translations from multiple source sentences\n\n        This only supports one model for now.\n\n        Args:\n            models: List of FairseqModel objects. Each one must implement\n                expand_encoder_output() method to replicate encoder outputs.\n                For now only one model is supported\n            min/maxlen: The length of the generated output will be bounded by\n                minlen and maxlen (not including the end-of-sentence marker).\n            stop_early: Stop generation immediately after we finalize beam_size\n                hypotheses, even though longer hypotheses might have better\n                normalized scores.\n            normalize_scores: Normalize scores by the length of the output.\n            word_reward: add this value to score each token except EOS\n                (an alternative method to len_penalty for encouraging longer\n                output)\n            model_weights: None or list of Python floats of the same length as\n                `models` with ensemble interpolation weights.\n            use_char_source: if True, encoder inputs consist of (src_tokens,\n                src_lengths, char_inds, word_lengths)\n        """"""\n        self.models = models\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.beam_size = beam_size\n        self.minlen = minlen\n        max_decoder_len = min(m.max_decoder_positions() for m in self.models)\n        self.maxlen = (\n            max_decoder_len if maxlen is None else min(maxlen, max_decoder_len)\n        )\n        self.stop_early = stop_early\n        self.normalize_scores = normalize_scores\n        self.len_penalty = len_penalty\n        self.unk_reward = unk_reward\n        self.lexicon_reward = lexicon_reward\n        self.lexicon_indices = tgt_dict.lexicon_indices_list()\n        self.retain_dropout = retain_dropout\n        self.word_reward = word_reward\n        if model_weights is not None:\n            assert len(models) == len(model_weights)\n            self.model_weights = model_weights\n        else:\n            self.model_weights = [1.0 / len(models)] * len(models)\n        self.use_char_source = use_char_source\n\n    def cuda(self):\n        for model in self.models:\n            model.cuda()\n        return self\n\n    def generate_batched_itr(\n        self,\n        data_itr,\n        beam_size=None,\n        maxlen_a=0.0,\n        maxlen_b=None,\n        cuda=False,\n        timer=None,\n        prefix_size=0,\n    ):\n        """"""Iterate over a batched dataset and yield individual translations.\n\n        Args:\n            maxlen_a/b: generate sequences of maximum length ax + b,\n                where x is the source sentence length.\n            cuda: use GPU for generation\n            timer: StopwatchMeter for timing generations.\n        """"""\n        if maxlen_b is None:\n            maxlen_b = self.maxlen\n\n        for sample in data_itr:\n            if cuda:\n                s = utils.move_to_cuda(sample)\n            input = s[""net_input""]\n            # Take the max source length to compute the max target length\n            srclen = input[""src_tokens""].size(1)\n            # FIXME: handle characters properly\n            if self.use_char_source:\n                raise ValueError(\n                    ""Character level encoder is not supported yet for ""\n                    ""multisource sentences.""\n                )\n            encoder_inputs = (input[""src_tokens""], input[""src_lengths""])\n            if timer is not None:\n                timer.start()\n            with torch.no_grad():\n                hypos = self.generate(\n                    encoder_inputs,\n                    srcs_ids=input[""src_ids""],\n                    beam_size=beam_size,\n                    maxlen=int(maxlen_a * srclen + maxlen_b),\n                    prefix_tokens=s[""target""][:, :prefix_size]\n                    if prefix_size > 0\n                    else None,\n                )\n            if timer is not None:\n                timer.stop(s[""ntokens""])\n            for i, id in enumerate(s[""id""]):\n                src = input[""src_tokens""].index_select(\n                    0, input[""src_ids""][self.align_to]\n                )\n                # remove padding from ref\n                ref = utils.strip_pad(s[""target""][i, :], self.pad)\n                yield id, src, ref, hypos[i]\n\n    def generate(\n        self,\n        encoder_inputs,\n        srcs_ids,\n        beam_size=None,\n        maxlen=None,\n        prefix_tokens=None,\n        src_weights=None,\n    ):\n        """"""Generate a batch of translations.""""""\n        with torch.no_grad():\n            return self._generate(\n                encoder_inputs, srcs_ids, beam_size, maxlen, prefix_tokens, src_weights\n            )\n\n    def _generate(\n        self,\n        encoder_inputs,\n        srcs_ids,\n        beam_size=None,\n        maxlen=None,\n        prefix_tokens=None,\n        src_weights=None,\n    ):\n        """"""Generates a translation from multiple source sentences""""""\n        n_srcs = len(srcs_ids)\n        srcs_tokens = encoder_inputs[0]\n        align_src_tokens = srcs_tokens.index_select(0, srcs_ids[self.align_to])\n\n        bsz, srclen = align_src_tokens.size()\n        maxlen = min(maxlen, self.maxlen) if maxlen is not None else self.maxlen\n\n        # the max beam size is the dictionary size - 1, since we never select pad\n        beam_size = beam_size if beam_size is not None else self.beam_size\n        assert (\n            beam_size < self.vocab_size\n        ), ""Beam size must be smaller than target vocabulary""\n\n        # Encode\n        encoder_outs = self._encode(encoder_inputs, beam_size, srcs_ids)\n        incremental_states = self._init_incremental_states(n_srcs)\n\n        # initialize buffers\n        scores = align_src_tokens.new(bsz * beam_size, maxlen + 1).float().fill_(0)\n        scores_buf = scores.clone()\n        tokens = align_src_tokens.new(bsz * beam_size, maxlen + 2).fill_(self.pad)\n        tokens_buf = tokens.clone()\n        tokens[:, 0] = self.eos\n\n        # may differ from input length\n        src_encoding_len = encoder_outs[self.align_to][0][0].size(0)\n\n        attn = scores.new(bsz * beam_size, src_encoding_len, maxlen + 2)\n        attn_buf = attn.clone()\n\n        # list of completed sentences\n        finalized = [[] for i in range(bsz)]\n        finished = [False for i in range(bsz)]\n        worst_finalized = [{""idx"": None, ""score"": -math.inf} for i in range(bsz)]\n        num_remaining_sent = bsz\n\n        # number of candidate hypos per step\n        cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n\n        # offset arrays for converting between different indexing schemes\n        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n        cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n\n        # helper function for allocating buffers on the fly\n        buffers = {}\n\n        def buffer(name, type_of=tokens):  # noqa\n            if name not in buffers:\n                buffers[name] = type_of.new()\n            return buffers[name]\n\n        def is_finished(sent, step, unfinalized_scores=None):\n            """"""\n            Check whether we\'ve finished generation for a given sentence, by\n            comparing the worst score among finalized hypotheses to the best\n            possible score among unfinalized hypotheses.\n            """"""\n            assert len(finalized[sent]) <= beam_size\n            if len(finalized[sent]) == beam_size:\n                if self.stop_early or step == maxlen or unfinalized_scores is None:\n                    return True\n                # stop if the best unfinalized score is worse than the worst\n                # finalized one\n                best_unfinalized_score = unfinalized_scores[sent].max()\n                if self.normalize_scores:\n                    best_unfinalized_score /= (maxlen + 1) ** self.len_penalty\n                if worst_finalized[sent][""score""] >= best_unfinalized_score:\n                    return True\n            return False\n\n        def finalize_hypos(step, bbsz_idx, eos_scores, unfinalized_scores=None):\n            """"""\n            Finalize the given hypotheses at this step, while keeping the total\n            number of finalized hypotheses per sentence <= beam_size.\n\n            Note: the input must be in the desired finalization order, so that\n            hypotheses that appear earlier in the input are preferred to those\n            that appear later.\n\n            Args:\n                step: current time step\n                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n                    indicating which hypotheses to finalize\n                eos_scores: A vector of the same size as bbsz_idx containing\n                    scores for each hypothesis\n                unfinalized_scores: A vector containing scores for all\n                    unfinalized hypotheses\n            """"""\n            assert bbsz_idx.numel() == eos_scores.numel()\n\n            # clone relevant token and attention tensors\n            tokens_clone = tokens.index_select(0, bbsz_idx)\n            tokens_clone = tokens_clone[\n                :, 1 : step + 2\n            ]  # skip the first index, which is EOS\n            tokens_clone[:, step] = self.eos\n            attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1 : step + 2]\n\n            # compute scores per token position\n            pos_scores = scores.index_select(0, bbsz_idx)[:, : step + 1]\n            pos_scores[:, step] = eos_scores\n            # convert from cumulative to per-position scores\n            pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n\n            # normalize sentence-level scores\n            if self.normalize_scores:\n                eos_scores /= (step + 1) ** self.len_penalty\n\n            sents_seen = set()\n            for i, (idx, score) in enumerate(\n                zip(bbsz_idx.tolist(), eos_scores.tolist())\n            ):\n                sent = idx // beam_size\n                sents_seen.add(sent)\n\n                def get_hypo():\n                    _, alignment = attn_clone[i].max(dim=0)\n                    return {\n                        ""tokens"": tokens_clone[i],\n                        ""score"": score,\n                        ""attention"": attn_clone[i],  # src_len x tgt_len\n                        ""alignment"": alignment,\n                        ""positional_scores"": pos_scores[i],\n                    }\n\n                if len(finalized[sent]) < beam_size:\n                    finalized[sent].append(get_hypo())\n                elif not self.stop_early and score > worst_finalized[sent][""score""]:\n                    # replace worst hypo for this sentence with new/better one\n                    worst_idx = worst_finalized[sent][""idx""]\n                    if worst_idx is not None:\n                        finalized[sent][worst_idx] = get_hypo()\n\n                    # find new worst finalized hypo for this sentence\n                    idx, s = min(\n                        enumerate(finalized[sent]), key=lambda r: r[1][""score""]\n                    )\n                    worst_finalized[sent] = {""score"": s[""score""], ""idx"": idx}\n\n            # return number of hypotheses finished this step\n            num_finished = 0\n            for sent in sents_seen:\n                # check termination conditions for this sentence\n                if not finished[sent] and is_finished(sent, step, unfinalized_scores):\n                    finished[sent] = True\n                    num_finished += 1\n            return num_finished\n\n        reorder_state = None\n        for step in range(maxlen + 1):  # one extra step for EOS marker\n            # reorder decoder internal states based on the prev choice of beams\n            if reorder_state is not None:\n                for model_id, model in enumerate(self.models):\n                    if isinstance(model.decoder, FairseqIncrementalDecoder):\n                        for src_id in range(n_srcs):\n                            model.decoder.reorder_incremental_state(\n                                incremental_states[(src_id, model_id)], reorder_state\n                            )\n            # Run decoder for one step\n            logprobs, avg_attn, possible_translation_tokens = self._decode(\n                tokens[:, : step + 1], encoder_outs, incremental_states, n_srcs\n            )\n\n            if step == 0:\n                # at the first step all hypotheses are equally likely, so use\n                # only the first beam\n                logprobs = logprobs.unfold(0, 1, beam_size).squeeze(2).contiguous()\n                scores = scores.type_as(logprobs)\n                scores_buf = scores_buf.type_as(logprobs)\n            else:\n                # make probs contain cumulative scores for each hypothesis\n                logprobs.add_(scores[:, step - 1].view(-1, 1))\n            logprobs[:, self.pad] = -math.inf  # never select pad\n\n            # apply unk reward\n            if possible_translation_tokens is None:\n                unk_index = self.unk\n            else:\n                unk_index = torch.nonzero(possible_translation_tokens == self.unk)[0, 0]\n            logprobs[:, unk_index] += self.unk_reward\n\n            # external lexicon reward\n            logprobs[:, self.lexicon_indices] += self.lexicon_reward\n\n            logprobs += self.word_reward\n            logprobs[:, self.eos] -= self.word_reward\n\n            # Record attention scores\n            attn[:, :, step + 1].copy_(avg_attn)\n\n            cand_scores = buffer(""cand_scores"", type_of=scores)\n            cand_indices = buffer(""cand_indices"")\n            cand_beams = buffer(""cand_beams"")\n            eos_bbsz_idx = buffer(""eos_bbsz_idx"")\n            eos_scores = buffer(""eos_scores"", type_of=scores)\n            if step < maxlen:\n                if prefix_tokens is not None and step < prefix_tokens.size(1):\n                    logprobs_slice = logprobs.view(bsz, -1, logprobs.size(-1))[:, 0, :]\n                    cand_scores = torch.gather(\n                        logprobs_slice, dim=1, index=prefix_tokens[:, step].view(-1, 1)\n                    ).expand(-1, cand_size)\n                    cand_indices = (\n                        prefix_tokens[:, step].view(-1, 1).expand(bsz, cand_size)\n                    )\n                    cand_beams.resize_as_(cand_indices).fill_(0)\n                else:\n                    # take the best 2 x beam_size predictions. We\'ll choose the first\n                    # beam_size of these which don\'t predict eos to continue with.\n                    torch.topk(\n                        logprobs.view(bsz, -1),\n                        k=min(\n                            cand_size, logprobs.view(bsz, -1).size(1) - 1\n                        ),  # -1 so we never select pad\n                        out=(cand_scores, cand_indices),\n                    )\n\n                    possible_tokens_size = self.vocab_size\n                    if possible_translation_tokens is not None:\n                        possible_tokens_size = possible_translation_tokens.size(0)\n                    # cand_indices has values in [0, vocab_size * beam_size]\n                    # the following does euclidean division bu vocab_size\n                    # to retrieve the beam and word id of each candidate\n                    torch.div(cand_indices, possible_tokens_size, out=cand_beams)\n                    cand_indices.fmod_(possible_tokens_size)\n                    # Handle vocab reduction\n                    if possible_translation_tokens is not None:\n                        possible_translation_tokens = possible_translation_tokens.view(\n                            1, possible_tokens_size\n                        ).expand(cand_indices.size(0), possible_tokens_size)\n                        cand_indices = torch.gather(\n                            possible_translation_tokens,\n                            dim=1,\n                            index=cand_indices,\n                            out=cand_indices,\n                        )\n            else:\n                # finalize all active hypotheses once we hit maxlen\n                # pick the hypothesis with the highest log prob of EOS right now\n                torch.sort(\n                    logprobs[:, self.eos],\n                    descending=True,\n                    out=(eos_scores, eos_bbsz_idx),\n                )\n                num_remaining_sent -= finalize_hypos(step, eos_bbsz_idx, eos_scores)\n                assert num_remaining_sent == 0\n                break\n\n            # cand_bbsz_idx contains beam indices for the top candidate\n            # hypotheses, with a range of values: [0, bsz*beam_size),\n            # and dimensions: [bsz, cand_size]\n            cand_bbsz_idx = cand_beams.add_(bbsz_offsets)\n\n            # finalize hypotheses that end in eos\n            eos_mask = cand_indices.eq(self.eos)\n            if step >= self.minlen:\n                # only consider eos when it\'s among the top beam_size indices\n                torch.masked_select(\n                    cand_bbsz_idx[:, :beam_size],\n                    mask=eos_mask[:, :beam_size],\n                    out=eos_bbsz_idx,\n                )\n                if eos_bbsz_idx.numel() > 0:\n                    torch.masked_select(\n                        cand_scores[:, :beam_size],\n                        mask=eos_mask[:, :beam_size],\n                        out=eos_scores,\n                    )\n                    num_remaining_sent -= finalize_hypos(\n                        step, eos_bbsz_idx, eos_scores, cand_scores\n                    )\n\n            assert num_remaining_sent >= 0\n            if num_remaining_sent == 0:\n                break\n            assert step < maxlen\n\n            # set active_mask so that values > cand_size indicate eos hypos\n            # and values < cand_size indicate candidate active hypos.\n            # After, the min values per row are the top candidate active hypos\n            active_mask = buffer(""active_mask"")\n            torch.add(\n                eos_mask.type_as(cand_offsets) * cand_size,\n                cand_offsets[: eos_mask.size(1)],\n                out=active_mask,\n            )\n\n            # get the top beam_size active hypotheses, which are just the hypos\n            # with the smallest values in active_mask\n            active_hypos, _ignore = buffer(""active_hypos""), buffer(""_ignore"")\n            torch.topk(\n                active_mask,\n                k=beam_size,\n                dim=1,\n                largest=False,\n                out=(_ignore, active_hypos),\n            )\n            active_bbsz_idx = buffer(""active_bbsz_idx"")\n            torch.gather(cand_bbsz_idx, dim=1, index=active_hypos, out=active_bbsz_idx)\n            active_scores = torch.gather(\n                cand_scores,\n                dim=1,\n                index=active_hypos,\n                out=scores[:, step].view(bsz, beam_size),\n            )\n            active_bbsz_idx = active_bbsz_idx.view(-1)\n            active_scores = active_scores.view(-1)\n\n            # copy tokens and scores for active hypotheses\n            torch.index_select(\n                tokens[:, : step + 1],\n                dim=0,\n                index=active_bbsz_idx,\n                out=tokens_buf[:, : step + 1],\n            )\n            torch.gather(\n                cand_indices,\n                dim=1,\n                index=active_hypos,\n                out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1],\n            )\n            if step > 0:\n                torch.index_select(\n                    scores[:, :step],\n                    dim=0,\n                    index=active_bbsz_idx,\n                    out=scores_buf[:, :step],\n                )\n            torch.gather(\n                cand_scores,\n                dim=1,\n                index=active_hypos,\n                out=scores_buf.view(bsz, beam_size, -1)[:, :, step],\n            )\n\n            # copy attention for active hypotheses\n            torch.index_select(\n                attn[:, :, : step + 2],\n                dim=0,\n                index=active_bbsz_idx,\n                out=attn_buf[:, :, : step + 2],\n            )\n\n            # swap buffers\n            tokens, tokens_buf = tokens_buf, tokens\n            scores, scores_buf = scores_buf, scores\n            attn, attn_buf = attn_buf, attn\n\n            # reorder incremental state in decoder\n            reorder_state = active_bbsz_idx\n\n        # sort by score descending\n        for sent in range(bsz):\n            finalized[sent] = sorted(\n                finalized[sent], key=lambda r: r[""score""], reverse=True\n            )\n\n        return finalized\n\n    def _init_incremental_states(self, n_srcs):\n        incremental_states = {}\n        for src_id in range(n_srcs):\n            for model_id, model in enumerate(self.models):\n                if isinstance(model.decoder, FairseqIncrementalDecoder):\n                    incremental_states[(src_id, model_id)] = {}\n                else:\n                    incremental_states[(src_id, model_id)] = None\n        return incremental_states\n\n    def _encode(self, encoder_inputs, beam_size, srcs_ids):\n        encoder_outs = [[] for _ in range(len(srcs_ids))]\n\n        def pick_src_encodings(encoder_out, src_ids):\n            (\n                unpacked_output,\n                final_hiddens,\n                final_cells,\n                src_lengths,\n                src_tokens,\n            ) = encoder_out\n            unpacked_output = unpacked_output.index_select(1, src_ids)\n            final_hiddens = final_hiddens.index_select(1, src_ids)\n            final_cells = final_cells.index_select(1, src_ids)\n            src_lengths = src_lengths.index_select(0, src_ids)\n            src_tokens = src_tokens.index_select(0, src_ids)\n            max_src_len = src_lengths.data.max()\n            return (\n                unpacked_output[:max_src_len, :, :],\n                final_hiddens,\n                final_cells,\n                src_lengths,\n                src_tokens[:, :max_src_len],\n            )\n\n        # Enumerate all input/model pair\n        # At this point encoder_input is a tuple of srcs_tokens and srcs_lengths\n        # So we call zip(*) on it to get a list of token, length tuples\n        for model in self.models:\n            if not self.retain_dropout:\n                model.eval()\n            encoder_out = model.encoder(*encoder_inputs)\n            for k, src_ids in enumerate(srcs_ids):\n                # Get the encodings corresponding to each source sentence\n                encoder_out_k = pick_src_encodings(encoder_out, src_ids)\n                # expand outputs for each example beam_size times\n                encoder_out_k = model.expand_encoder_output(encoder_out_k, beam_size)\n                encoder_outs[k].append(encoder_out_k)\n        return encoder_outs\n\n    def _decode(self, tokens, encoder_outs, incremental_states, n_srcs=1):\n        # Source sentences are weighted equally (for now)\n        srcs_weights = [1 / n_srcs] * n_srcs\n\n        avg_probs = None\n        avg_attn = None\n        for src_id, src_weight in enumerate(srcs_weights):\n            for model_id, (model_weight, model) in enumerate(\n                zip(self.model_weights, self.models)\n            ):\n                with torch.no_grad():\n                    encoder_out = encoder_outs[src_id][model_id]\n                    incremental_state = incremental_states[(src_id, model_id)]\n                    decoder_out = list(\n                        model.decoder(tokens, encoder_out, incremental_state)\n                    )\n                    decoder_out[0] = decoder_out[0][:, -1, :]\n                    attn = decoder_out[1]\n                    if len(decoder_out) == 3:\n                        possible_translation_tokens = decoder_out[2]\n                    else:\n                        possible_translation_tokens = None\n                probs = (\n                    src_weight\n                    * model_weight\n                    * model.get_normalized_probs(decoder_out, log_probs=False)\n                )\n                if avg_probs is None:\n                    avg_probs = probs\n                else:\n                    avg_probs.add_(probs)\n                if attn is not None and src_id == self.align_to:\n                    attn = attn[:, -1, :]\n                    if avg_attn is None:\n                        avg_attn = attn\n                    else:\n                        avg_attn.add_(attn)\n        avg_probs.log_()\n        if avg_attn is not None:\n            avg_attn.div_(len(self.models))\n\n        return avg_probs, avg_attn, possible_translation_tokens\n'"
pytorch_translate/research/rescore/__init__.py,0,b''
pytorch_translate/research/rescore/cloze_transformer_model.py,4,"b'#!/usr/bin/env python3\n\nimport torch\nfrom fairseq import utils\nfrom fairseq.models import register_model, register_model_architecture\nfrom pytorch_translate.transformer import (\n    TransformerDecoder,\n    TransformerModel,\n    base_architecture,\n    build_embedding,\n)\n\n\n@register_model(""cloze_transformer"")\nclass ClozeTransformerModel(TransformerModel):\n    @classmethod\n    def build_model(cls, args, task):\n        """"""Build a new model instance.""""""\n        # make sure that all args are properly defaulted\n        # (in case there are any new ones)\n        cloze_transformer_architecture(args)\n\n        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n\n        if args.share_all_embeddings:\n            if src_dict != tgt_dict:\n                raise RuntimeError(\n                    ""--share-all-embeddings requires a joined dictionary""\n                )\n            if args.encoder_embed_dim != args.decoder_embed_dim:\n                raise RuntimeError(\n                    ""--share-all-embeddings requires --encoder-embed-dim ""\n                    ""to match --decoder-embed-dim""\n                )\n            if args.decoder_pretrained_embed and (\n                args.decoder_pretrained_embed != args.encoder_pretrained_embed\n            ):\n                raise RuntimeError(\n                    ""--share-all-embeddings not compatible with ""\n                    ""--decoder-pretrained-embed""\n                )\n            encoder_embed_tokens = build_embedding(\n                dictionary=src_dict,\n                embed_dim=args.encoder_embed_dim,\n                path=args.encoder_pretrained_embed,\n                freeze=args.encoder_freeze_embed,\n            )\n            decoder_embed_tokens = encoder_embed_tokens\n            args.share_decoder_input_output_embed = True\n        else:\n            encoder_embed_tokens = build_embedding(\n                dictionary=src_dict,\n                embed_dim=args.encoder_embed_dim,\n                path=args.encoder_pretrained_embed,\n                freeze=args.encoder_freeze_embed,\n            )\n            decoder_embed_tokens = build_embedding(\n                dictionary=tgt_dict,\n                embed_dim=args.decoder_embed_dim,\n                path=args.decoder_pretrained_embed,\n                freeze=args.decoder_freeze_embed,\n            )\n\n        encoder = ClozeTransformerModel.build_encoder(\n            args, src_dict, embed_tokens=encoder_embed_tokens\n        )\n        decoder = ClozeTransformerModel.build_decoder(\n            args, src_dict, tgt_dict, embed_tokens=decoder_embed_tokens\n        )\n        return ClozeTransformerModel(task, encoder, decoder)\n\n    @classmethod\n    def build_decoder(cls, args, src_dict, dst_dict, embed_tokens):\n        return ClozeTransformerDecoder(\n            args, src_dict, dst_dict, embed_tokens=embed_tokens\n        )\n\n\nclass ClozeTransformerDecoder(TransformerDecoder):\n    """"""Cloze-Transformer decoder.""""""\n\n    def __init__(self, args, src_dict, dst_dict, embed_tokens, left_pad=False):\n        super().__init__(args, src_dict, dst_dict, embed_tokens)\n        assert args.decoder_layers == 1\n\n    def buffered_future_mask(self, tensor):\n        """"""attend all surounding words except itself\n           [[0, -inf, 0]\n            [0,  0, -inf]\n            [0,  0,   0]]\n        The attention map is not ture diagonal since we predict y_{t+1} at time-step t\n        """"""\n        dim = tensor.size(0)\n        if (\n            not hasattr(self, ""_future_mask"")\n            or self._future_mask is None\n            or self._future_mask.device != tensor.device\n        ):\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(tensor.new(dim, dim)), 1\n            )\n            self._future_mask = torch.tril(self._future_mask, 1)\n        if self._future_mask.size(0) < dim:\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(self._future_mask.resize_(dim, dim)), 1\n            )\n            self._future_mask = torch.tril(self._future_mask, 1)\n        return self._future_mask[:dim, :dim]\n\n\n@register_model_architecture(""cloze_transformer"", ""cloze_transformer"")\ndef cloze_transformer_architecture(args):\n    base_architecture(args)\n'"
pytorch_translate/research/rescore/rescoring_criterion.py,9,"b'# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport math\n\nimport torch\nfrom fairseq import utils\nfrom fairseq.criterions import LegacyFairseqCriterion, register_criterion\nfrom pytorch_translate import generate, utils as pytorch_translate_utils\nfrom pytorch_translate.rescoring.model_scorers import SimpleModelScorer\nfrom pytorch_translate.rescoring.rescorer import (  # noqa\n    Rescorer,\n    add_args as rescore_add_args,\n    combine_weighted_scores,\n)\n\n\n@register_criterion(""rescoring_criterion"")\nclass RescoringCriterion(LegacyFairseqCriterion):\n    def __init__(self, args, task):\n        super().__init__(args, task)\n        self.self_rescorer = SimpleModelScorer(args, None, None, task)\n        self.rescore_models = self.load_rescore_models(args)\n        self.args = args\n        self.task = task\n\n    @staticmethod\n    def add_args(parser):\n        """"""Add criterion-specific arguments to the parser.""""""\n        rescore_add_args(parser)\n        parser.add_argument(\n            ""--rl-weight"",\n            type=float,\n            default=0.1,\n            help=""trade-off coefficient of rl loss"",\n        )\n        parser.add_argument(\n            ""--rl-num-trajectory"",\n            type=int,\n            default=3,\n            help=""num trajectory in rl training"",\n        )\n        parser.add_argument(\n            ""--topk-words"",\n            type=int,\n            default=8,\n            help=""match topk words at each time step"",\n        )\n        parser.add_argument(\n            ""--word-weight"", type=float, default=1.0, help=""weight for word level""\n        )\n        parser.add_argument(\n            ""--word-model"",\n            type=str,\n            default=""cloze_model"",\n            help=""word-level teacher model"",\n        )\n\n    def forward(self, model, sample, reduce=True):\n        """"""Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        """"""\n        src_tokens = sample[""net_input""][""src_tokens""]\n        beam_size = self.args.rl_num_trajectory\n        bsz, srclen = src_tokens.size()\n        encoder_input = {\n            ""src_tokens"": sample[""net_input""][""src_tokens""],\n            ""src_lengths"": sample[""net_input""][""src_lengths""],\n        }\n\n        # 1) Generate hypos\n        translator = generate.build_sequence_generator(self.args, self.task, [model])\n        with torch.no_grad():\n            seq_hypos = translator.generate(\n                encoder_input,\n                beam_size,\n                maxlen=int(self.args.max_len_a * srclen + self.args.max_len_b),\n            )\n\n        word_hypos = [[] for j in range(bsz)]\n        for k in range(bsz):\n            word_hypos[k] = [{""tokens"": sample[""target""][k]}]\n\n        ## Mix sequence, word-level hypos\n        hypos = [seq_hypos[j] + word_hypos[j] for j in range(bsz)]\n        hypos = [hypo for _ in hypos for hypo in _]\n        hypos_len = (\n            torch.tensor([len(hypo[""tokens""]) for hypo in hypos])\n            .type_as(src_tokens)\n            .float()\n        )\n        # mask index for word-level hypos, e.g., target sentence\n        mask_index = torch.arange(beam_size, (beam_size + 1) * bsz, beam_size + 1).view(\n            -1\n        )\n\n        # 2) Compute (log)-probs via forward models\n        self.self_rescorer.model = model\n        self.self_rescorer.task = self.task\n        model.train()\n        assert self.self_rescorer.model.training, ""model should be in training phase""\n\n        hypo_encoder_inputs, hypo_tokens = self.self_rescorer.prepare_inputs(\n            src_tokens, hypos\n        )\n        hypo_logprobs, hypo_encoder_outs, forward_logprobs = self.self_rescorer.score_tokens(\n            hypo_encoder_inputs, hypo_tokens\n        )\n        hypo_logprobs /= hypos_len ** self.args.rescore_length_penalty\n\n        # 3) Sequence level\n        seq_loss = torch.zeros(1).type_as(hypo_logprobs)\n        if self.args.rl_weight > 0.0:\n            ## 3.1) Compute seq-level rewards\n            with torch.no_grad():\n                rescorer = Rescorer(self.args, self.task, self.rescore_models)\n                scores = rescorer.score(src_tokens, hypos)\n                rewards = self.combine_score(src_tokens, hypos, hypos_len, scores)\n            assert not rewards.requires_grad, ""no grads flow back to generation""\n            ## 3.2) Compute Policy Gradient loss\n            rewards = rewards.type_as(hypo_logprobs)\n            seq_mask = hypo_logprobs.new_ones(hypo_logprobs.size())\n            seq_mask[mask_index] = 0.0\n            seq_loss = -1.0 * (seq_mask * hypo_logprobs * rewards).sum()\n\n        # 4) Word-level\n        word_loss = torch.zeros(1).type_as(hypo_logprobs)\n        if self.args.word_weight > 0.0:\n            ## 4.1) Compute word-level rewards from a left-right rescoring model\n            with torch.no_grad():\n                teacher_model = self.rescore_models[self.args.word_model]\n                teacher = SimpleModelScorer(self.args, None, teacher_model, self.task)\n                _, _, teacher_logprobs = teacher.score_tokens(\n                    hypo_encoder_inputs, hypo_tokens\n                )\n            ## 4.2) Compute word-level loss\n            f_logprob, f_index = forward_logprobs.topk(self.args.topk_words)\n            word_mask = f_logprob.new_zeros(f_logprob.size())\n            word_mask[mask_index, :, :] = 1.0\n            ## KL(p_s || p_t) = \\sum p_s log p_s - \\sum p_s log p_t, aka RL + maxEnt\n            word_loss = (\n                word_mask\n                * f_logprob.exp()\n                * (f_logprob - 1.0 * teacher_logprobs.gather(-1, f_index))\n            ).sum()\n\n        # 5) Compute Cross-entropy loss\n        eos = self.task.target_dictionary.eos()\n        target_tokens = torch.cat(\n            (\n                torch.zeros(bsz, 1).fill_(eos).type_as(sample[""target""]),\n                sample[""target""],\n            ),\n            dim=1,\n        )\n        target_encoder_inputs = (\n            encoder_input[""src_tokens""],\n            [encoder_input[""src_lengths""][0].item()],\n        )\n        target_logprobs, target_encoder_out, _ = self.self_rescorer.score_tokens(\n            target_encoder_inputs, target_tokens\n        )\n        nll_loss = -1.0 * target_logprobs.sum()\n\n        # 6) Gather losses\n        loss = (\n            self.args.rl_weight * seq_loss\n            + self.args.word_weight * word_loss\n            + nll_loss\n        )\n\n        # Logging\n        sample_size = (\n            sample[""target""].size(0) if self.args.sentence_avg else sample[""ntokens""]\n        )\n        logging_output = {\n            ""loss"": utils.item(loss.data) if reduce else loss.data,\n            ""nll_loss"": utils.item(nll_loss.data) if reduce else nll_loss.data,\n            ""ntokens"": sample[""ntokens""],\n            ""nsentences"": sample[""target""].size(0),\n            ""sample_size"": sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        """"""Aggregate logging outputs from data parallel training.""""""\n        loss_sum = sum(log.get(""loss"", 0) for log in logging_outputs)\n        nll_loss_sum = sum(log.get(""nll_loss"", 0) for log in logging_outputs)\n        ntokens = sum(log.get(""ntokens"", 0) for log in logging_outputs)\n        nsentences = sum(log.get(""nsentences"", 0) for log in logging_outputs)\n        sample_size = sum(log.get(""sample_size"", 0) for log in logging_outputs)\n        return {\n            ""loss"": loss_sum / sample_size if sample_size > 0 else 0.0,\n            ""nll_loss"": nll_loss_sum / ntokens / math.log(2),\n            ""ntokens"": ntokens,\n            ""nsentences"": nsentences,\n            ""sample_size"": sample_size,\n        }\n\n    def combine_score(self, src_tokens, hypos, hypos_len, scores):\n        """""" Rescore translations and combine weights to find top hypo tokens\n        """"""\n        # Prepare all the weights and call combine weighted scores\n        args = self.args\n        weights = [\n            args.l2r_model_weight,\n            args.r2l_model_weight,\n            args.reverse_model_weight,\n            args.lm_model_weight,\n            args.cloze_transformer_weight,\n        ]\n        bsz, src_len = src_tokens.size()\n        hypos_len = hypos_len.type_as(scores)\n        combined_scores = combine_weighted_scores(\n            scores, weights, src_len, hypos_len, args.length_penalty\n        )\n        return combined_scores\n\n    def load_rescore_models(self, args):\n        """"""load rescoring models""""""\n        models = {}\n        if args.l2r_model_path:\n            l2r_model, _, l2r_task = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n                [args.l2r_model_path]\n            )\n            models[""l2r_model""] = {""model"": l2r_model[0], ""task"": l2r_task}\n        #\n        if args.r2l_model_path:\n            r2l_model, _, r2l_task = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n                [args.r2l_model_path]\n            )\n            models[""r2l_model""] = {""model"": r2l_model[0], ""task"": r2l_task}\n        #\n        if args.reverse_model_path:\n            reverse_model, _, reverse_task = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n                [args.reverse_model_path]\n            )\n            models[""reverse_model""] = {""model"": reverse_model[0], ""task"": reverse_task}\n        #\n        if args.lm_model_path:\n            lm_model, _, lm_task = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n                [args.lm_model_path]\n            )\n            models[""lm_model""] = {""model"": lm_model[0], ""task"": lm_task}\n        #\n        if args.cloze_transformer_path:\n            cloze_model, _, cloze_task = pytorch_translate_utils.load_diverse_ensemble_for_inference(\n                [args.cloze_transformer_path]\n            )\n            models[""cloze_model""] = {""model"": cloze_model[0], ""task"": cloze_task}\n        return models\n'"
pytorch_translate/research/test/__init__.py,0,b''
pytorch_translate/research/test/test_knowledge_distillation.py,13,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.data import language_pair_dataset\nfrom pytorch_translate import char_source_model  # noqa\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate.research.knowledge_distillation import (\n    collect_top_k_probs,\n    dual_decoder_kd_loss,\n    dual_decoder_kd_model,\n    knowledge_distillation_loss,\n)\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestKnowledgeDistillation(unittest.TestCase):\n    def test_topk_kd_loss(self):\n        """"""\n        Makes sure that we can build KD loss without problem.\n        """"""\n        test_args = test_utils.ModelParamsDict()\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        self.task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        sample = self._dummy_sample()\n        model = self.task.build_model(test_args)\n        net_output = model(**sample[""net_input""])\n        student_lprobs = model.get_normalized_probs(net_output, log_probs=True)\n        # [bsz, seqlen, vocab] -> [bsz*seqlen, vocab]\n        lprobs = student_lprobs.view(-1, student_lprobs.size(-1))\n\n        teacher_model = self.task.build_model(test_args)\n        teacher_probs = teacher_model.get_normalized_probs(net_output, log_probs=False)\n        top_k_teacher_probs, indices = torch.topk(teacher_probs, k=3)\n        top_k_teacher_probs_normalized = F.normalize(\n            top_k_teacher_probs, p=1, dim=2\n        ).detach()\n        sample[""top_k_scores""] = top_k_teacher_probs_normalized\n        sample[""top_k_indices""] = indices\n\n        kd_criterion = knowledge_distillation_loss.KnowledgeDistillationCriterion.build_criterion(\n            test_args, self.task\n        )\n        kd_loss = kd_criterion.get_kd_loss(sample, student_lprobs, lprobs)\n\n        # Calculate kd_loss using full matrix and compare\n        topk_mask = torch.zeros(student_lprobs.shape).type_as(student_lprobs)\n        topk_probs = topk_mask.scatter(\n            2, indices, top_k_teacher_probs_normalized.float()\n        )\n        topk_probs_flat = topk_probs.view(-1, topk_probs.size(-1))\n        kd_loss_2 = -torch.sum(topk_probs_flat * lprobs)\n        np.testing.assert_almost_equal(kd_loss.item(), kd_loss_2.item(), decimal=4)\n        assert kd_loss >= 0\n\n    def test_dual_decoder_kd_loss(self):\n        test_args = test_utils.ModelParamsDict(arch=""dual_decoder_kd"")\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        self.task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        sample = self._dummy_sample()\n        model = self.task.build_model(test_args)\n\n        test_args.kd_weight = 0.5\n        test_args.label_smoothing = 0.1\n        criterion = dual_decoder_kd_loss.DualDecoderCriterion.build_criterion(\n            test_args, self.task\n        )\n\n        src_tokens = sample[""net_input""][""src_tokens""]\n        src_lengths = sample[""net_input""][""src_lengths""]\n        prev_output_tokens = sample[""net_input""][""prev_output_tokens""]\n\n        encoder_out = model.encoder(src_tokens, src_lengths)\n        student_output = model.student_decoder(prev_output_tokens, encoder_out)\n        teacher_output = model.teacher_decoder(prev_output_tokens, encoder_out)\n\n        teacher_loss, teacher_nll_loss, teacher_probs = criterion.compute_teacher_loss(\n            model, teacher_output, sample, reduce=True\n        )\n\n        # probabilities for each label should sum to one\n        assert all((teacher_probs.sum(dim=1) - 1.0).abs() < 1e-6)\n\n        student_loss, student_nll_loss = criterion.compute_student_loss(\n            model, student_output, sample, teacher_probs, reduce=True\n        )\n\n    def _dummy_sample(self, batch_size=3, input_seq_length=5, output_seq_length=4):\n\n        output_sequence = torch.randint(\n            low=self.task.dst_dict.nspecial,\n            high=len(self.task.dst_dict),\n            size=(batch_size, output_seq_length),\n        ).long()\n        eos_column = torch.LongTensor(\n            [self.task.dst_dict.eos_index] * batch_size\n        ).unsqueeze(1)\n        prev_output_tokens = torch.cat([eos_column, output_sequence], dim=1)\n        target_tokens = torch.cat([output_sequence, eos_column], dim=1)\n\n        sample = {\n            ""net_input"": {\n                ""src_tokens"": torch.randint(\n                    low=self.task.src_dict.nspecial,\n                    high=len(self.task.src_dict),\n                    size=(batch_size, input_seq_length),\n                ).long(),\n                ""prev_output_tokens"": prev_output_tokens,\n                ""src_lengths"": torch.LongTensor([input_seq_length] * batch_size),\n            },\n            ""target"": target_tokens,\n            ""ntokens"": target_tokens.numel(),\n        }\n        return sample\n\n    def test_dual_decoder_args(self):\n        test_args = test_utils.ModelParamsDict(arch=""dual_decoder_kd"")\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        self.task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        model = self.task.build_model(test_args)\n\n        assert (\n            model.encoder.transformer_embedding.embed_tokens.embedding_dim\n            == test_args.encoder_embed_dim\n        )\n        assert (\n            model.encoder.transformer_encoder_given_embeddings.layers[\n                0\n            ].fc1.out_features\n            == test_args.encoder_ffn_embed_dim\n        )\n        assert (\n            len(model.encoder.transformer_encoder_given_embeddings.layers)\n            == test_args.encoder_layers\n        )\n        assert (\n            model.encoder.transformer_encoder_given_embeddings.layers[\n                0\n            ].self_attn.num_heads\n            == test_args.encoder_attention_heads\n        )\n        assert (\n            model.teacher_decoder.embed_tokens.embedding_dim\n            == test_args.decoder_embed_dim\n        )\n        assert (\n            model.teacher_decoder.layers[0].fc1.out_features\n            == test_args.decoder_ffn_embed_dim\n        )\n        assert len(model.teacher_decoder.layers) == test_args.decoder_layers\n        assert (\n            model.teacher_decoder.layers[0].self_attn.num_heads\n            == test_args.decoder_attention_heads\n        )\n        assert (\n            model.student_decoder.embed_tokens.embedding_dim\n            == test_args.student_decoder_embed_dim\n        )\n        assert model.student_decoder.num_layers == test_args.student_decoder_layers\n        assert (\n            model.student_decoder.num_attention_heads\n            == test_args.student_decoder_attention_heads\n        )\n        assert model.student_decoder.lstm_units == test_args.student_decoder_lstm_units\n        assert (\n            model.student_decoder.out_embed_dim\n            == test_args.student_decoder_out_embed_dim\n        )\n\n    def test_collect_top_k_probs(self):\n        test_args = test_utils.ModelParamsDict(arch=""hybrid_transformer_rnn"")\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        self.task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n        model = self.task.build_model(test_args)\n\n        use_cuda = torch.cuda.is_available()\n        if use_cuda:\n            model.cuda()\n        model.eval()\n\n        binarized_source = test_utils.create_dummy_binarized_dataset()\n        binarized_target = test_utils.create_dummy_binarized_dataset(append_eos=True)\n        dataset = language_pair_dataset.LanguagePairDataset(\n            src=binarized_source,\n            src_sizes=binarized_source.sizes,\n            src_dict=self.task.src_dict,\n            tgt=binarized_target,\n            tgt_sizes=binarized_target.sizes,\n            tgt_dict=self.task.dst_dict,\n            left_pad_source=False,\n        )\n\n        top_k_scores, top_k_indices = collect_top_k_probs.compute_top_k(\n            task=self.task,\n            models=[model],\n            dataset=dataset,\n            k=3,\n            use_cuda=use_cuda,\n            max_tokens=None,\n            max_sentences=None,\n            progress_bar_args=None,\n        )\n\n        batch = language_pair_dataset.collate(\n            [dataset[0]],\n            pad_idx=self.task.src_dict.pad(),\n            eos_idx=self.task.src_dict.eos(),\n            left_pad_source=False,\n        )\n\n        sample = batch[""net_input""]\n        if use_cuda:\n            sample = utils.move_to_cuda(sample)\n\n        with torch.no_grad():\n            net_output = model(**sample)\n            probs = model.get_normalized_probs(net_output, log_probs=False)\n\n        top_probs, top_indices = torch.topk(probs[0, 0], k=3)\n        if use_cuda:\n            top_probs = top_probs.cpu()\n            top_indices = top_indices.cpu()\n\n        np.testing.assert_array_equal(top_k_indices[0], top_indices.numpy())\n        normalized_probs = (top_probs / top_probs.sum()).numpy()\n        np.testing.assert_almost_equal(top_k_scores[0], normalized_probs)\n'"
pytorch_translate/research/test/test_teacher_score_dataset.py,8,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy as np\nimport torch\nfrom pytorch_translate import char_source_model  # noqa\nfrom pytorch_translate import rnn  # noqa\nfrom pytorch_translate import utils as pytorch_translate_utils\nfrom pytorch_translate.research.knowledge_distillation.teacher_score_data import (\n    TeacherDataset,\n)\nfrom pytorch_translate.tasks import pytorch_translate_task as tasks\nfrom pytorch_translate.test import utils as test_utils\n\n\nclass TestTeacherScoreDataSet(unittest.TestCase):\n    def test_collate(self):\n        """"""\n        Makes sure that we can memoize in collate if we give a particular data index\n        in different orders.\n        """"""\n        test_args = test_utils.ModelParamsDict()\n        _, src_dict, tgt_dict = test_utils.prepare_inputs(test_args)\n        self.task = tasks.DictionaryHolderTask(src_dict, tgt_dict)\n\n        teacher_model = pytorch_translate_utils.maybe_cuda(\n            self.task.build_model(test_args)\n        )\n        teacher_models = [teacher_model]\n\n        d0, d1, d2, d3 = self._dummy_datasets(src_dict.eos(), tgt_dict.eos())\n        dataset1 = [d0, d1]\n        dataset2 = [d2, d3]\n        dataset3 = [d3, d0]\n        dataset4 = [d1, d2]\n\n        top_k_teacher_scores = {}\n        top_k_teacher_indices = {}\n        b1 = TeacherDataset.collate(\n            dataset1,\n            teacher_models,\n            3,\n            src_dict.pad(),\n            src_dict.eos(),\n            top_k_teacher_scores,\n            top_k_teacher_indices,\n        )\n        TeacherDataset.collate(\n            dataset2,\n            teacher_models,\n            3,\n            src_dict.pad(),\n            src_dict.eos(),\n            top_k_teacher_scores,\n            top_k_teacher_indices,\n        )\n        before_scores = [top_k_teacher_scores[i].cpu().numpy() for i in range(4)]\n        before_indices = [top_k_teacher_indices[i].cpu().numpy() for i in range(4)]\n\n        TeacherDataset.collate(\n            dataset3,\n            teacher_models,\n            3,\n            src_dict.pad(),\n            src_dict.eos(),\n            top_k_teacher_scores,\n            top_k_teacher_indices,\n        )\n        TeacherDataset.collate(\n            dataset4,\n            teacher_models,\n            3,\n            src_dict.pad(),\n            src_dict.eos(),\n            top_k_teacher_scores,\n            top_k_teacher_indices,\n        )\n        after_scores = [top_k_teacher_scores[i].cpu().numpy() for i in range(4)]\n        after_indices = [top_k_teacher_indices[i].cpu().numpy() for i in range(4)]\n\n        for i in range(4):\n            np.array_equal(after_scores[i], before_scores[i])\n            np.array_equal(after_indices[i], before_indices[i])\n\n        b5 = TeacherDataset.collate(\n            dataset1,\n            teacher_models,\n            3,\n            src_dict.pad(),\n            src_dict.eos(),\n            top_k_teacher_scores,\n            top_k_teacher_indices,\n        )\n\n        assert len(teacher_models) == 0\n        probs_before = b1[""top_k_scores""].numpy()\n        indices_before = b1[""top_k_indices""].numpy()\n        probs_after = b5[""top_k_scores""].numpy()\n        indices_after = b5[""top_k_indices""].numpy()\n\n        # The first one has a different length, does the last two values in the\n        # before value has irrelevant values.abs\n        assert np.array_equal(probs_before[0][:-4], probs_after[0][:-4]) is True\n        assert np.array_equal(indices_before[0][:-4], indices_after[0][:-4]) is True\n        assert np.array_equal(probs_after[0][-4:], np.zeros((4, 3))) is True\n        assert np.array_equal(indices_after[0][-4:], np.zeros((4, 3))) is True\n\n        assert np.array_equal(probs_before[1], probs_after[1]) is True\n        assert np.array_equal(indices_before[1], indices_after[1]) is True\n\n    def _dummy_datasets(self, src_eos_idx, tgt_eos_idx):\n        d0 = {\n            ""id"": 0,\n            ""source"": torch.LongTensor(np.array([1, 9, 3, 4, 5, 6, 7, 8, src_eos_idx])),\n            ""target"": torch.LongTensor(np.array([1, 9, 3, 4, tgt_eos_idx])),\n        }\n        d1 = {\n            ""id"": 1,\n            ""source"": torch.LongTensor(np.array([11, 9, 13, 4, 15, 6, src_eos_idx])),\n            ""target"": torch.LongTensor(\n                np.array([1, 9, 3, 4, 15, 6, 17, 18, tgt_eos_idx])\n            ),\n        }\n        d2 = {\n            ""id"": 2,\n            ""source"": torch.LongTensor(\n                np.array([11, 9, 3, 4, 25, 6, 7, 28, src_eos_idx])\n            ),\n            ""target"": torch.LongTensor(\n                np.array([21, 9, 3, 24, 5, 6, 27, 28, tgt_eos_idx])\n            ),\n        }\n        d3 = {\n            ""id"": 3,\n            ""source"": torch.LongTensor(np.array([9, 9, 9, 4, 25, 9, 7, src_eos_idx])),\n            ""target"": torch.LongTensor(np.array([21, 9, 3, 9, 5, 9, tgt_eos_idx])),\n        }\n        return d0, d1, d2, d3\n'"
pytorch_translate/research/test/test_unsupervised_morphology.py,0,"b'#!/usr/bin/env python3\n\nimport math\nimport random\nimport shutil\nimport tempfile\nimport unittest\nfrom os import path\nfrom unittest.mock import Mock, patch\n\nfrom pytorch_translate.research.unsupervised_morphology import unsupervised_morphology\n\n\ndef get_all_substrings(string):\n    length = len(string)\n    for i in range(length):\n        for j in range(i + 1, length + 1):\n            yield (string[i:j])\n\n\nclass TestUnsupervisedMorphology(unittest.TestCase):\n    def test_morph_init(self):\n        morph_hmm_model = unsupervised_morphology.MorphologyHMMParams()\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            morph_hmm_model.init_params_from_data(""no_exist_file.txt"")\n\n            assert len(morph_hmm_model.morph_emit_probs) == 51\n            assert round(morph_hmm_model.morph_emit_probs[""1234""], 3) == round(\n                0.014141414141414142, 3\n            )\n\n    def test_zero_out_params(self):\n        morph_hmm_model = unsupervised_morphology.MorphologyHMMParams()\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            morph_hmm_model.init_params_from_data(""no_exist_file.txt"")\n            for morph in morph_hmm_model.morph_emit_probs.keys():\n                assert morph_hmm_model.morph_emit_probs[morph] > 0\n\n            morph_hmm_model.zero_out_params()\n            for morph in morph_hmm_model.morph_emit_probs.keys():\n                assert morph_hmm_model.morph_emit_probs[morph] == 0\n\n    def test_emission_probs(self):\n        morph_hmm_model = unsupervised_morphology.MorphologyHMMParams()\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            morph_hmm_model.init_params_from_data(""no_exist_file.txt"")\n\n            # todo add more tests\n            e = 0.014141414141414142\n            e_r = e * math.exp(-9)\n            assert round(morph_hmm_model.emission_prob(""1234""), 3) == round(e_r, 3)\n\n    def test_emission_log_prob(self):\n        morph_hmm_model = unsupervised_morphology.MorphologyHMMParams()\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            morph_hmm_model.init_params_from_data(""no_exist_file.txt"")\n\n            # todo add more tests\n            e = 0.014141414141414142\n            e_r = e * math.exp(-9)\n            assert round(morph_hmm_model.emission_log_prob(""1234""), 3) == round(\n                math.log(e_r), 3\n            )\n\n    def test_segment_viterbi_no_smoothing(self):\n        morph_hmm_model = unsupervised_morphology.MorphologyHMMParams(\n            smoothing_const=0.0\n        )\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            morph_hmm_model.init_params_from_data(""no_exist_file.txt"")\n\n            segmentor = unsupervised_morphology.MorphologySegmentor(morph_hmm_model)\n            assert segmentor.segment_viterbi(""123123789"") == [0, 2, 3, 5, 6, 9]\n\n    def test_segment_viterbi_w_smoothing(self):\n        morph_hmm_model = unsupervised_morphology.MorphologyHMMParams()\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            morph_hmm_model.init_params_from_data(""no_exist_file.txt"")\n\n            segmentor = unsupervised_morphology.MorphologySegmentor(morph_hmm_model)\n            assert segmentor.segment_viterbi(""123123789"") == [0, 2, 3, 5, 6, 9]\n\n    def test_segment_word_no_smoothing(self):\n        morph_hmm_model = unsupervised_morphology.MorphologyHMMParams(\n            smoothing_const=0.0\n        )\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            morph_hmm_model.init_params_from_data(""no_exist_file.txt"")\n\n            segmentor = unsupervised_morphology.MorphologySegmentor(morph_hmm_model)\n            assert segmentor.segment_word(""123123789789"") == ""12 3 12 3 789 789""\n\n    def check_emission_after_forward_backward(self, str, e, expected_morphemes):\n        for str in get_all_substrings(str):\n            if str in expected_morphemes:\n                assert e[str] > 0\n            else:\n                assert e[str] == 0\n\n    def test_forward_backward(self):\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [""123 12123""]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            unsupervised_model = unsupervised_morphology.UnsupervisedMorphology(\n                ""no_exist_file.txt"", smoothing_const=0.0\n            )\n            e = unsupervised_model.forward_backward(""123"")\n\n            # checking emission parameters\n            self.check_emission_after_forward_backward(\n                ""123"", e, get_all_substrings(""123"")\n            )\n\n    def test_forward_backward_long_str(self):\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            unsupervised_model = unsupervised_morphology.UnsupervisedMorphology(\n                ""no_exist_file.txt"", smoothing_const=0.0\n            )\n            e = unsupervised_model.forward_backward(""1232345"")\n            expected_morphs = {\n                ""1"",\n                ""2"",\n                ""3"",\n                ""4"",\n                ""5"",\n                ""12"",\n                ""23"",\n                ""34"",\n                ""45"",\n                ""123"",\n                ""234"",\n                ""345"",\n                ""2345"",\n            }\n            self.check_emission_after_forward_backward(""1232345"", e, expected_morphs)\n\n    def test_EM_alg(self):\n        txt_content = [\n            ""work"",\n            ""works"",\n            ""worked"",\n            ""working"",\n            ""go"",\n            ""goes"",\n            ""gone"",\n            ""going"",\n            ""do"",\n            ""does"",\n            ""did"",\n            ""doing"",\n            ""see"",\n            ""saw"",\n            ""seen"",\n            ""seeing"",\n        ]\n        # Running with forward-backward.\n        with patch(""builtins.open"") as mock_open:\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            unsupervised_model = unsupervised_morphology.UnsupervisedMorphology(\n                ""no_exist_file.txt"", smoothing_const=0.0\n            )\n            unsupervised_model.expectation_maximization(10, 10)\n\n        # Running with Viterbi-EM.\n        with patch(""builtins.open"") as mock_open:\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            unsupervised_model = unsupervised_morphology.UnsupervisedMorphology(\n                ""no_exist_file.txt"", smoothing_const=0.0, use_hardEM=True\n            )\n            unsupervised_model.expectation_maximization(10, 10)\n\n    def test_get_expectations_from_viterbi(self):\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""123 124 234 345"",\n                ""112 122 123 345"",\n                ""123456789"",\n                ""123456 456789"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n\n            um = unsupervised_morphology.UnsupervisedMorphology(\n                ""no_exist_file.txt"", smoothing_const=0.0, use_hardEM=True\n            )\n            assert um.segmentor.segment_viterbi(""123123789"") == [0, 2, 3, 5, 6, 9]\n            e = um.get_expectations_from_viterbi(""123123789"")\n            assert e[""12""] == 2\n            assert e[""789""] == 1\n            assert e[""89""] == 0\n\n    def test_save_load(self):\n        with patch(""builtins.open"") as mock_open:\n            txt_content = [\n                ""work"",\n                ""works"",\n                ""worked"",\n                ""working"",\n                ""go"",\n                ""goes"",\n                ""gone"",\n                ""going"",\n                ""do"",\n                ""does"",\n                ""did"",\n                ""doing"",\n                ""see"",\n                ""saw"",\n                ""seen"",\n                ""seeing"",\n            ]\n            mock_open.return_value.__enter__ = mock_open\n            mock_open.return_value.__iter__ = Mock(return_value=iter(txt_content))\n            unsupervised_model = unsupervised_morphology.UnsupervisedMorphology(\n                ""no_exist_file.txt"", smoothing_const=0.0\n            )\n            unsupervised_model.expectation_maximization(3, 2)\n\n        test_dir = tempfile.mkdtemp()\n        unsupervised_model.params.save(path.join(test_dir, ""test.pickle""))\n\n        loaded_params = unsupervised_morphology.MorphologyHMMParams.load(\n            path.join(test_dir, ""test.pickle"")\n        )\n\n        assert (\n            unsupervised_model.params.morph_emit_probs == loaded_params.morph_emit_probs\n        )\n        assert unsupervised_model.params.word_counts == loaded_params.word_counts\n        assert (\n            unsupervised_model.params.smoothing_const == loaded_params.smoothing_const\n        )\n        assert unsupervised_model.params.SMALL_CONST == loaded_params.SMALL_CONST\n        assert unsupervised_model.params.len_cost_pow == loaded_params.len_cost_pow\n        shutil.rmtree(test_dir)\n'"
pytorch_translate/research/tune_ensemble_weights/tune_model_weights.py,0,"b'#!/usr/bin/env python3\n\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nfrom fairseq import options\nfrom pytorch_translate import generate\nfrom pytorch_translate.constants import CHECKPOINT_PATHS_DELIMITER\n\n\ndef add_tune_args(parser):\n    group = parser.add_argument_group(""Tune parameter parser."")\n    group.add_argument(\n        ""--n-grid"",\n        default=6,\n        type=int,\n        metavar=""N"",\n        help=""how many grid added to tune for each weight."",\n    )\n    group.add_argument(\n        ""--weight-lower-bound"",\n        default=0.0,\n        type=float,\n        help=""lower bound for each weight."",\n    )\n    group.add_argument(\n        ""--weight-upper-bound"",\n        default=1.0,\n        type=float,\n        help=""upper bound for each weight."",\n    )\n    group.add_argument(\n        ""--output-file-name"",\n        default=""output.csv"",\n        type=str,\n        help=""name of output file."",\n    )\n    return parser\n\n\ndef tune_model_weights():\n    parser = generate.get_parser_with_args()\n    parser = add_tune_args(parser)\n    args = options.parse_args_and_arch(parser)\n    print(args.model_weights)\n    n_models = len(args.path.split(CHECKPOINT_PATHS_DELIMITER))\n    print(n_models)\n\n    weight_grid = np.linspace(\n        args.weight_lower_bound, args.weight_upper_bound, args.n_grid + 1\n    )\n    weight_vec_aux = list(itertools.product(weight_grid, weight_grid))\n    weight_vec = []\n    for w1, w2 in weight_vec_aux:\n        weight_sum = w1 + w2\n        if weight_sum <= 1:\n            w3 = 1 - weight_sum\n            weight_vec.append(str(w1) + "","" + str(w2) + "","" + str(w3))\n\n    print(len(weight_vec))\n    output = pd.DataFrame()\n    for weight in weight_vec:\n        args.model_weights = weight\n        print(args.model_weights)\n        generate.validate_args(args)\n        score = generate.generate(args)\n        print(score)\n        output = output.append(\n            {""weight"": args.model_weights, ""bleu_score"": score}, ignore_index=True\n        )\n        output.to_csv(args.output_file_name)\n    return output\n\n\nif __name__ == ""__main__"":\n    tune_model_weights()\n'"
pytorch_translate/research/tune_ensemble_weights/tune_model_weights_with_ax.py,0,"b'#!/usr/bin/env python3\nimport json\n\nfrom ax.service.managed_loop import optimize\nfrom fairseq import options\nfrom pytorch_translate import generate\nfrom pytorch_translate.constants import CHECKPOINT_PATHS_DELIMITER\n\n\ndef add_tune_args(parser):\n    group = parser.add_argument_group(""Tune parameter parser."")\n    group.add_argument(\n        ""--n-grid"",\n        default=6,\n        type=int,\n        metavar=""N"",\n        help=""how many grid added to tune for each weight."",\n    )\n    group.add_argument(\n        ""--weight-lower-bound"",\n        default=0.0,\n        type=float,\n        help=""lower bound for each weight."",\n    )\n    group.add_argument(\n        ""--weight-upper-bound"",\n        default=1.0,\n        type=float,\n        help=""upper bound for each weight."",\n    )\n    group.add_argument(\n        ""--num-trails-ax-opt"",\n        default=5,\n        type=int,\n        help=""number of trials in AX optimization."",\n    )\n    group.add_argument(\n        ""--output-json-best-parameters"",\n        default=""best_parameters.json"",\n        type=str,\n        help=""name of output file for the best parameters."",\n    )\n    group.add_argument(\n        ""--output-json-best-value"",\n        default=""best_value.json"",\n        type=str,\n        help=""name of output file for the best value of the evaluation function."",\n    )\n    return parser\n\n\ndef tune_model_weights():\n    parser = generate.get_parser_with_args()\n    parser = add_tune_args(parser)\n    args = options.parse_args_and_arch(parser)\n    n_models = len(args.path.split(CHECKPOINT_PATHS_DELIMITER))\n    print(n_models)\n    print(args.weight_lower_bound)\n    print(args.weight_upper_bound)\n    print(args.output_json_best_parameters)\n    print(args.output_json_best_value)\n    print(args.num_trails_ax_opt)\n\n    def evaluation_function(parameterization):\n        w1 = parameterization.get(""w1"")\n        w2 = parameterization.get(""w2"")\n        w3 = parameterization.get(""w3"")\n        weight = str(w1) + "","" + str(w2) + "","" + str(w3)\n        args.model_weights = weight\n        generate.validate_args(args)\n        score = generate.generate(args)\n        return {""bleu_score"": (score, 0.0)}\n\n    lower_bound = args.weight_lower_bound\n    upper_bound = args.weight_upper_bound\n    best_parameters, values, experiment, model = optimize(\n        parameters=[\n            {\n                ""name"": ""w1"",\n                ""type"": ""range"",\n                ""bounds"": [lower_bound, upper_bound],\n                ""value_type"": ""float"",\n            },\n            {""name"": ""w2"", ""type"": ""range"", ""bounds"": [lower_bound, upper_bound]},\n            {""name"": ""w3"", ""type"": ""range"", ""bounds"": [lower_bound, upper_bound]},\n        ],\n        experiment_name=""tune_model_weights"",\n        objective_name=""bleu_score"",\n        evaluation_function=evaluation_function,\n        minimize=True,  # Optional, defaults to False.\n        parameter_constraints=[\n            ""w1 + w2 + w3 <= 1"",\n            ""w1 + w2 + w3 >= 0.99"",\n        ],  # Optional.\n        total_trials=args.num_trails_ax_opt,  # Optional.\n    )\n\n    json_file = json.dumps(best_parameters)\n    with open(args.output_json_best_parameters, ""w"") as f:\n        f.write(json_file)\n        f.close()\n\n    json_file = json.dumps(values)\n    with open(args.output_json_best_value, ""w"") as f:\n        f.write(json_file)\n        f.close()\n    return best_parameters, values\n\n\nif __name__ == ""__main__"":\n    tune_model_weights()\n'"
pytorch_translate/test/gpu/__init__.py,0,b''
pytorch_translate/test/gpu/test_integration_gpu.py,5,"b'#!/usr/bin/env python3\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n#\n# GPU integration tests; those that don\'t need GPUs should go in\n# test_integration.py instead\n\nimport contextlib\nimport tempfile\nimport unittest\nfrom io import StringIO\n\nimport torch\nfrom pytorch_translate import generate, models, train  # noqa need to load models\nfrom pytorch_translate.test.utils import (\n    create_dummy_data,\n    generate_main,\n    train_translation_model,\n)\n\n\nclass TestTranslation(unittest.TestCase):\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""Test only supports GPU training."")\n    def test_rnn_fp16(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_rnn_fp16"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--fp16"",\n                        ""--arch"",\n                        ""rnn"",\n                        ""--cell-type"",\n                        ""lstm"",\n                        ""--sequence-lstm"",\n                        ""--reverse-source"",\n                        ""--encoder-bidirectional"",\n                        ""--encoder-layers"",\n                        ""2"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-layers"",\n                        ""2"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-hidden-dim"",\n                        ""16"",\n                        ""--decoder-out-embed-dim"",\n                        ""8"",\n                        ""--attention-type"",\n                        ""dot"",\n                    ],\n                )\n                generate_main(data_dir)\n\n    @unittest.skipIf(torch.cuda.device_count() < 1, ""Test only supports GPU training."")\n    def test_transformer_fp_16(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_transformer"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--fp16"",\n                        ""--arch"",\n                        ""ptt_transformer"",\n                        ""--encoder-embed-dim"",\n                        ""8"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--encoder-attention-heads"",\n                        ""4"",\n                        ""--encoder-layers"",\n                        ""3"",\n                        ""--decoder-embed-dim"",\n                        ""8"",\n                        ""--decoder-ffn-embed-dim"",\n                        ""16"",\n                        ""--decoder-attention-heads"",\n                        ""4"",\n                        ""--decoder-layers"",\n                        ""3"",\n                    ],\n                )\n                generate_main(data_dir)\n\n    @unittest.skipIf(\n        torch.cuda.device_count() < 2, ""Test only supports multi-GPU training.""\n    )\n    def test_transformer_multigpu(self):\n        with contextlib.redirect_stdout(StringIO()):\n            with tempfile.TemporaryDirectory(""test_transformer"") as data_dir:\n                create_dummy_data(data_dir)\n                train_translation_model(\n                    data_dir,\n                    [\n                        ""--arch"",\n                        ""ptt_transformer"",\n                        ""--encoder-embed-dim"",\n                        ""256"",\n                        ""--encoder-ffn-embed-dim"",\n                        ""512"",\n                        ""--encoder-attention-heads"",\n                        ""4"",\n                        ""--encoder-layers"",\n                        ""3"",\n                        ""--decoder-embed-dim"",\n                        ""256"",\n                        ""--decoder-ffn-embed-dim"",\n                        ""512"",\n                        ""--decoder-attention-heads"",\n                        ""4"",\n                        ""--decoder-layers"",\n                        ""3"",\n                        ""--distributed-world-size"",\n                        str(torch.cuda.device_count()),\n                        ""--local-num-gpus"",\n                        str(torch.cuda.device_count()),\n                    ],\n                )\n                generate_main(data_dir)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
