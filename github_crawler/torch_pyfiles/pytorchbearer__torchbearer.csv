file_path,api_count,code
setup.py,0,"b'from setuptools import setup\n\nversion_dict = {}\nexec(open(""./torchbearer/version.py"").read(), version_dict)\n\nimport sys\nif sys.version_info[0] >= 3:\n    from os import path\n    this_directory = path.abspath(path.dirname(__file__))\n    with open(path.join(this_directory, \'README.md\'), encoding=\'utf-8\') as f:\n        long_description = f.read()\nelse:\n    long_description = \'A model training library for pytorch\'\n\nsetup(\n    name=\'torchbearer\',\n    version=version_dict[\'__version__\'],\n    packages=[\'torchbearer\', \'torchbearer.metrics\', \'torchbearer.callbacks\', \'torchbearer.callbacks.imaging\', \'tests\', \'tests.metrics\', \'tests.callbacks\', \'tests.callbacks.imaging\'],\n    url=\'https://github.com/pytorchbearer/torchbearer\',\n    download_url=\'https://github.com/pytorchbearer/torchbearer/archive/\' + version_dict[\'__version__\'] + \'.tar.gz\',\n    classifiers=[\n        ""License :: OSI Approved :: MIT License""\n    ],\n    author=\'Matt Painter\',\n    author_email=\'mp2u16@ecs.soton.ac.uk\',\n    description=\'A model training library for pytorch\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    install_requires=[\'torch>=1.0.0\', \'numpy\', \'tqdm\'],\n    python_requires=\'>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*\',\n)\n'"
docs/conf.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# torchbearer documentation build configuration file, created by\n# sphinx-quickstart on Mon Jul 16 14:35:09 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nautodoc_mock_imports = [\'torch\', \'torch.nn.utils.clip_grad_norm\', \'torchvision\', \'torchvision.utils\', \'torchvision.datasets\', \'torchvision.datasets.folder\', \'torch.nn\', \'torch.nn.functional\', \'torch.nn.modules\', \'torch.optim\', \'torch.distributions.utils\', \'torch.distributions\', \'torch.utils\', \'torch.utils.data\', \'numpy\', \'sklearn\', \'sklearn.metrics\', \'tqdm\', \'tensorboardX\', \'tensorboardX.torchvis\', \'livelossplot\', \'IPython\']\n\nsys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.abspath(\'../\'))\nsys.path.insert(0, os.path.abspath(\'../torchbearer\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.napoleon\', \'sphinx.ext.mathjax\', \'sphinx.ext.autodoc\', \'sphinx.ext.viewcode\', \'sphinx.ext.intersphinx\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'torchbearer\'\ncopyright = \'Torchbearer Contributors\'\nauthor = \'Torchbearer Contributors\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n\nversion_dict = {}\nexec(open(""../torchbearer/version.py"").read(), version_dict)\n\n# The short X.Y version.\nversion = version_dict[\'__version__\']\n# The full version, including alpha/beta/rc tags.\nrelease = version_dict[\'__version__\']\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n    ]\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'torchbearerdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'torchbearer.tex\', \'torchbearer Documentation\',\n     \'Torchbearer Contributors\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'torchbearer\', \'torchbearer Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'torchbearer\', \'torchbearer Documentation\',\n     author, \'torchbearer\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\nhtml_scaled_image_link = False\n\n\n'"
tests/__init__.py,0,b''
tests/test_bases.py,2,"b'import unittest\nfrom mock import Mock, MagicMock, create_autospec, patch\n\nimport torch\n\nimport torchbearer\nfrom torchbearer.bases import Metric, Callback, base_closure\nfrom torchbearer.metrics.primitives import CategoricalAccuracy\n\n\nclass TestMetric(unittest.TestCase):\n    def setUp(self):\n        self._state = {\n            torchbearer.Y_TRUE: torch.LongTensor([0, 1, 2, 2, 1]),\n            torchbearer.Y_PRED: torch.FloatTensor([\n                [0.9, 0.1, 0.1], # Correct\n                [0.1, 0.9, 0.1], # Correct\n                [0.1, 0.1, 0.9], # Correct\n                [0.9, 0.1, 0.1], # Incorrect\n                [0.9, 0.1, 0.1], # Incorrect\n            ])\n        }\n        self._state[torchbearer.Y_PRED].requires_grad = True\n        self._targets = [1, 1, 1, 0, 0]\n        self._metric = CategoricalAccuracy().root\n\n    def test_requires_grad(self):\n        result = self._metric.process(self._state)\n        self.assertTrue(self._state[torchbearer.Y_PRED].requires_grad is True)\n        self.assertTrue(result.requires_grad is False)\n\n    def test_empty_methods(self):\n        metric = Metric(name=\'test\')\n        self.assertTrue(metric.process() is None)\n        self.assertTrue(metric.process_final() is None)\n\n\nclass TestCallback(unittest.TestCase):\n    def test_state_dict(self):\n        callback = Callback()\n\n        self.assertEqual(callback.state_dict(), {})\n        self.assertEqual(callback.load_state_dict({}), callback)\n\n    def test_str(self):\n        callback = Callback()\n        self.assertEqual(str(callback).strip(), ""torchbearer.bases.Callback"")\n\n    def test_empty_methods(self):\n        callback = Callback()\n\n        self.assertIsNone(callback.on_init({}))\n        self.assertIsNone(callback.on_start({}))\n        self.assertIsNone(callback.on_start_epoch({}))\n        self.assertIsNone(callback.on_start_training({}))\n        self.assertIsNone(callback.on_sample({}))\n        self.assertIsNone(callback.on_forward({}))\n        self.assertIsNone(callback.on_criterion({}))\n        self.assertIsNone(callback.on_backward({}))\n        self.assertIsNone(callback.on_step_training({}))\n        self.assertIsNone(callback.on_end_training({}))\n        self.assertIsNone(callback.on_end_epoch({}))\n        self.assertIsNone(callback.on_checkpoint({}))\n        self.assertIsNone(callback.on_end({}))\n        self.assertIsNone(callback.on_start_validation({}))\n        self.assertIsNone(callback.on_sample_validation({}))\n        self.assertIsNone(callback.on_forward_validation({}))\n        self.assertIsNone(callback.on_end_validation({}))\n        self.assertIsNone(callback.on_step_validation({}))\n        self.assertIsNone(callback.on_criterion_validation({}))\n\n\nclass TestBaseCrit(unittest.TestCase):\n\n    def state_model_with_e(self, e):\n        def model(x, state):\n            raise e\n            return x\n        return model\n\n    def stateless_model_with_e(self, e):\n        def model(x):\n            raise e\n            return x\n        return model\n\n    def optional_model_with_e(self, e):\n        def model(x, state=None):\n            raise e\n            return x\n        return model\n\n    def test_opt(self):\n        opt = Mock()\n        opt.zero_grad = Mock()\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: None, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda x,y: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(opt.zero_grad.call_count == 1)\n\n    def test_forward_x(self):\n        opt = Mock()\n\n        def model_forward(x):\n            return None\n        model = create_autospec(model_forward)\n\n        x = \'test\'\n\n        state = {torchbearer.X: x, torchbearer.MODEL: model, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda x,y: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(model.call_args[0][0] == x)\n\n    def test_forward_multiple_x(self):\n        opt = Mock()\n\n        def model_forward(x1, x2):\n            return None\n        model = create_autospec(model_forward)\n\n        x1 = \'test1\'\n        x2 = \'test2\'\n\n        state = {torchbearer.X: [x1, x2], torchbearer.MODEL: model, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda x,y: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(model.call_args[0][0] == x1)\n        self.assertTrue(model.call_args[0][1] == x2)\n\n    def test_forward_state(self):\n        opt = Mock()\n\n        def model_forward(x, state):\n            return None\n        model = create_autospec(model_forward)\n\n        x = \'test\'\n\n        state = {torchbearer.X: x, torchbearer.MODEL: model, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda x,y: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(model.call_args[0][0] == x)\n        self.assertDictEqual(model.call_args[1][\'state\'], state)\n\n    def test_forward_multiple_x_and_state(self):\n        opt = Mock()\n\n        def model_forward(x1, x2, state):\n            return None\n        model = create_autospec(model_forward)\n\n        x1 = \'test1\'\n        x2 = \'test2\'\n\n        state = {torchbearer.X: [x1, x2], torchbearer.MODEL: model, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda x,y: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(model.call_args[0][0] == x1)\n        self.assertTrue(model.call_args[0][1] == x2)\n        self.assertDictEqual(model.call_args[1][\'state\'], state)\n\n    def test_loss_no_state(self):\n        opt = Mock()\n\n        y_pred = \'yp\'\n        y_true = \'yt\'\n        def loss_sig(y_pred, y_true):\n            return None\n        crit = create_autospec(loss_sig)\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: y_pred, torchbearer.Y_TRUE: y_true,\n                 torchbearer.CRITERION: crit, torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(crit.call_args[0] == (y_pred, y_true))\n\n    def test_loss_multiple_output_no_state(self):\n        opt = Mock()\n\n        y_pred1 = \'yp1\'\n        y_pred2 = \'yp2\'\n        y_true1 = \'yt1\'\n        y_true2 = \'yt2\'\n        def loss_sig(y_pred1, y_pred2, y_true1, y_true2):\n            return None\n        crit = create_autospec(loss_sig)\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: (y_pred1, y_pred2), torchbearer.Y_TRUE: (y_true1, y_true2),\n                 torchbearer.CRITERION: crit, torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(crit.call_args[0] == (y_pred1, y_pred2, y_true1, y_true2))\n\n    def test_loss_state(self):\n        opt = Mock()\n\n        y_pred = \'yp\'\n        y_true = \'yt\'\n        def loss_sig(state):\n            return None\n        crit = create_autospec(loss_sig)\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: y_pred, torchbearer.Y_TRUE: y_true,\n                 torchbearer.CRITERION: crit, torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertDictEqual(crit.call_args[0][0], state)\n\n    def test_backward(self):\n        opt = Mock()\n        loss = Mock()\n        loss.backward = Mock()\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: None, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda state: loss, torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(loss.backward.call_count == 1)\n\n    def test_callback_list(self):\n        opt = Mock()\n        callback_list = Mock()\n        callback_list.on_forward = Mock()\n        callback_list.on_criterion = Mock()\n        callback_list.on_backward = Mock()\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: None, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda state: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: callback_list, torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n        closure(state)\n        self.assertTrue(callback_list.on_forward.call_count == 1)\n        self.assertTrue(callback_list.on_criterion.call_count == 1)\n        self.assertTrue(callback_list.on_backward.call_count == 1)\n\n\n    def test_stateless_type_error(self):\n        state = {torchbearer.X: None, torchbearer.MODEL: self.stateless_model_with_e(TypeError(\'test\')), torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 2)\n\n    def test_stateless_exception(self):\n        state = {torchbearer.X: None, torchbearer.MODEL: self.stateless_model_with_e(Exception(\'test\')), torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 1)\n        self.assertTrue(\'test\' in context.exception.args[0][0].args)\n\n    def test_exception_exception(self):\n\n        state = {torchbearer.X: None, torchbearer.MODEL: self.optional_model_with_e(Exception(\'test\')),\n                 torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 2)\n        self.assertTrue(\'test\' in context.exception.args[0][0].args)\n        self.assertTrue(\'test\' in context.exception.args[0][1].args)\n\n\n    def test_state_exception(self):\n        state = {torchbearer.X: None, torchbearer.MODEL: self.state_model_with_e(Exception(\'test\')), torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 1)\n        self.assertTrue(\'test\' in context.exception.args[0][0].args)\n\n    def test_state_type_error(self):\n        state = {torchbearer.X: None, torchbearer.MODEL: self.state_model_with_e(TypeError(\'test\')), torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                               torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 2)\n\n\nclass TestApexCrit(unittest.TestCase):\n    def setUp(self):\n        super(TestApexCrit, self).setUp()\n\n        import types\n        import sys\n\n        module_name = \'apex\'\n        bogus_module = types.ModuleType(module_name)\n        self.old_module = sys.modules[module_name] if module_name in sys.modules else None\n        sys.modules[module_name] = bogus_module\n        self.mock_amp = MagicMock(name=module_name + \'.amp\')\n        bogus_module.amp = self.mock_amp\n\n        from torchbearer.bases import apex_closure\n        self.closure = apex_closure()\n\n    def tearDown(self):\n        super(TestApexCrit, self).tearDown()\n        if self.old_module is not None:\n            import sys\n            sys.modules[\'apex\'] = self.old_module\n\n    def state_model_with_e(self, e):\n        def model(x, state):\n            raise e\n            return x\n        return model\n\n    def stateless_model_with_e(self, e):\n        def model(x):\n            raise e\n            return x\n        return model\n\n    def test_opt(self):\n        opt = Mock()\n        opt.zero_grad = Mock()\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: None, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda x,y: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n        closure(state)\n        self.assertTrue(opt.zero_grad.call_count == 1)\n\n    def test_forward_x(self):\n        opt = Mock()\n\n        def model_forward(x):\n            return None\n        model = create_autospec(model_forward)\n\n        x = \'test\'\n\n        state = {torchbearer.X: x, torchbearer.MODEL: model, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda x,y: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n        closure(state)\n        self.assertTrue(model.call_args[0][0] == x)\n\n    def test_forward_state(self):\n        opt = Mock()\n\n        def model_forward(x, state):\n            return None\n        model = create_autospec(model_forward)\n\n        x = \'test\'\n\n        state = {torchbearer.X: x, torchbearer.MODEL: model, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda x,y: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n        closure(state)\n        self.assertTrue(model.call_args[0][0] == x)\n        self.assertDictEqual(model.call_args[1][\'state\'], state)\n\n    def test_loss_no_state(self):\n        opt = Mock()\n\n        y_pred = \'yp\'\n        y_true = \'yt\'\n        def loss_sig(y_pred, y_true):\n            return None\n        crit = create_autospec(loss_sig)\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: y_pred, torchbearer.Y_TRUE: y_true,\n                 torchbearer.CRITERION: crit, torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n        closure(state)\n        self.assertTrue(crit.call_args[0] == (y_pred, y_true))\n\n    def test_loss_state(self):\n        opt = Mock()\n\n        y_pred = \'yp\'\n        y_true = \'yt\'\n        def loss_sig(state):\n            return None\n        crit = create_autospec(loss_sig)\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: y_pred, torchbearer.Y_TRUE: y_true,\n                 torchbearer.CRITERION: crit, torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n        closure(state)\n        self.assertDictEqual(crit.call_args[0][0], state)\n\n    def test_backward(self):\n        opt = Mock()\n        loss = Mock()\n        loss.backward = Mock()\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: None, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda state: loss, torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: MagicMock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n        closure(state)\n        self.assertTrue(\'backward\' in self.mock_amp.scale_loss.mock_calls[2][0])\n\n    def test_callback_list(self):\n        opt = Mock()\n        callback_list = Mock()\n        callback_list.on_forward = Mock()\n        callback_list.on_criterion = Mock()\n        callback_list.on_backward = Mock()\n\n        state = {torchbearer.X: None, torchbearer.MODEL: lambda x: None, torchbearer.Y_TRUE: None,\n                 torchbearer.CRITERION: lambda state: MagicMock(), torchbearer.LOSS: None, torchbearer.OPTIMIZER: opt,\n                 torchbearer.CALLBACK_LIST: callback_list, torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n        closure(state)\n        self.assertTrue(callback_list.on_forward.call_count == 1)\n        self.assertTrue(callback_list.on_criterion.call_count == 1)\n        self.assertTrue(callback_list.on_backward.call_count == 1)\n\n\n    def test_stateless_type_error(self):\n        state = {torchbearer.X: None, torchbearer.MODEL: self.stateless_model_with_e(TypeError(\'test\')), torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 2)\n\n    def test_stateless_exception(self):\n        state = {torchbearer.X: None, torchbearer.MODEL: self.stateless_model_with_e(Exception(\'test\')), torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 1)\n        self.assertTrue(\'test\' in context.exception.args[0][0].args)\n\n    def test_state_exception(self):\n        state = {torchbearer.X: None, torchbearer.MODEL: self.state_model_with_e(Exception(\'test\')), torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 1)\n        self.assertTrue(\'test\' in context.exception.args[0][0].args)\n\n    def test_state_type_error(self):\n        state = {torchbearer.X: None, torchbearer.MODEL: self.state_model_with_e(TypeError(\'test\')), torchbearer.CRITERION: lambda state: MagicMock(),\n                 torchbearer.OPTIMIZER: Mock(), torchbearer.CALLBACK_LIST: Mock(), torchbearer.BACKWARD_ARGS: {}}\n\n        closure = self.closure\n\n        with self.assertRaises(Exception) as context:\n            closure(state)\n\n        self.assertTrue(len(context.exception.args[0]) == 2)'"
tests/test_cv_utils.py,28,"b""import unittest\n\nfrom mock import Mock\n\nimport torchbearer\nfrom torchbearer.cv_utils import *\n\n\nclass TestCVUtils(unittest.TestCase):\n\n    def test_train_valid_splitter_sizes(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = 0.1\n        shuffle = False\n\n        x, y, x_val, y_val = train_valid_splitter(x, y, valid_split, shuffle)\n        self.assertTrue(x.size()[0] == 90)\n        self.assertTrue(y.size()[0] == 90)\n        self.assertTrue(x_val.size()[0] == 10)\n        self.assertTrue(y_val.size()[0] == 10)\n\n    def test_train_valid_splitter_sizes_2(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = 0.4\n        shuffle = False\n\n        x, y, x_val, y_val = train_valid_splitter(x, y, valid_split, shuffle)\n        self.assertTrue(x.size()[0] == 60)\n        self.assertTrue(y.size()[0] == 60)\n        self.assertTrue(x_val.size()[0] == 40)\n        self.assertTrue(y_val.size()[0] == 40)\n\n    def test_train_valid_splitter_sizes_2d(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        x = torch.stack([x, x], -1)\n        y = torch.stack([y, y], -1)\n\n        valid_split = 0.1\n        shuffle = False\n\n        x, y, x_val, y_val = train_valid_splitter(x, y, valid_split, shuffle)\n        self.assertTrue(list(x.size()) == [90, 2])\n        self.assertTrue(list(y.size()) == [90, 2])\n        self.assertTrue(list(x_val.size()) == [10, 2])\n        self.assertTrue(list(y_val.size()) == [10, 2])\n\n    def test_train_valid_splitter_order(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = 0.1\n        shuffle = False\n\n        x, y, x_val, y_val = train_valid_splitter(x, y, valid_split, shuffle)\n        self.assertTrue(list(x.numpy()) == list(range(11, 101)))\n        self.assertTrue(list(y.numpy()) == list(range(11, 101)))\n        self.assertTrue(list(x_val.numpy()) == list(range(1, 11)))\n        self.assertTrue(list(y_val.numpy()) == list(range(1, 11)))\n\n    def test_train_valid_splitter_split_negative(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = -0.1\n        shuffle = False\n\n        x, y, x_val, y_val = train_valid_splitter(x, y, valid_split, shuffle)\n        self.assertTrue(list(x.numpy()) == list(range(91, 101)))\n        self.assertTrue(list(y.numpy()) == list(range(91, 101)))\n        self.assertTrue(list(x_val.numpy()) == list(range(1, 91)))\n        self.assertTrue(list(y_val.numpy()) == list(range(1, 91)))\n\n    def test_train_valid_splitter_split_zero(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = 0\n        shuffle = False\n\n        x, y, x_val, y_val = train_valid_splitter(x, y, valid_split, shuffle)\n        self.assertTrue(list(x.numpy()) == list(range(1, 101)))\n        self.assertTrue(list(y.numpy()) == list(range(1, 101)))\n        self.assertTrue(list(x_val.numpy()) == list(range(0, 0)))\n        self.assertTrue(list(y_val.numpy()) == list(range(0, 0)))\n\n    def test_train_valid_splitter_split_too_big(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = 1.8\n        shuffle = False\n\n        x, y, x_val, y_val = train_valid_splitter(x, y, valid_split, shuffle)\n        self.assertTrue(list(x.numpy()) == list(range(0, 0)))\n        self.assertTrue(list(y.numpy()) == list(range(0, 0)))\n        self.assertTrue(list(x_val.numpy()) == list(range(1, 101)))\n        self.assertTrue(list(y_val.numpy()) == list(range(1, 101)))\n\n    def test_train_valid_splitter_shuffle_size(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = 0.1\n        shuffle = True\n\n        x, y, x_val, y_val = train_valid_splitter(x, y, valid_split, shuffle)\n        self.assertTrue(x.size()[0] == 90)\n        self.assertTrue(y.size()[0] == 90)\n        self.assertTrue(x_val.size()[0] == 10)\n        self.assertTrue(y_val.size()[0] == 10)\n\n    def test_get_train_valid_sets_splitter_args(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = 0.1\n        shuffle = True\n\n        torchbearer.cv_utils.train_valid_splitter = Mock(return_value=(x,y,x,y))\n        tvs = torchbearer.cv_utils.train_valid_splitter\n\n        trainset, valset = get_train_valid_sets(x, y, None, valid_split, shuffle)\n        self.assertEqual(tvs.call_count, 1)\n        self.assertTrue(tvs.call_args[0][-1] == valid_split)\n        self.assertTrue(list(tvs.call_args[0][0].numpy()) == list(x.numpy()))\n        self.assertTrue(list(tvs.call_args[0][1].numpy()) == list(y.numpy()))\n        self.assertTrue(tvs.call_args[1]['shuffle'] == shuffle)\n\n    def test_get_train_valid_sets_given_valid_data(self):\n        x = range(1, 101)\n        y = range(1, 101)\n        x_val = range(101, 121)\n        y_val = range(101, 121)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n        x_val = torch.Tensor(x_val)\n        y_val = torch.Tensor(y_val)\n\n        valid_split = 0.1\n        shuffle = False\n\n        trainset, valset = get_train_valid_sets(x, y, (x_val, y_val), valid_split, shuffle)\n        self.assertTrue(len(valset) == len(x_val))\n\n    def test_get_train_valid_sets_no_valid(self):\n        x = range(1, 101)\n        y = range(1, 101)\n\n        x = torch.Tensor(x)\n        y = torch.Tensor(y)\n\n        valid_split = None\n        shuffle = False\n\n        trainset, valset = get_train_valid_sets(x, y, None, valid_split, shuffle)\n        self.assertTrue(valset is None)\n        self.assertTrue(len(trainset) == len(x))\n\n    def test_DatasetValidationSplitter(self):\n        data = torch.Tensor(list(range(1000)))\n        dataset = TensorDataset(data)\n\n        splitter = DatasetValidationSplitter(len(dataset), 0.1)\n        trainset = splitter.get_train_dataset(dataset)\n        validset = splitter.get_val_dataset(dataset)\n\n        self.assertTrue(len(trainset) == 900)\n        self.assertTrue(len(validset) == 100)\n\n        # Check for ids in both train and validation set\n        collision = False\n        for id in trainset:\n            if id in validset.ids:\n                collision = True\n        self.assertFalse(collision)\n\n    def test_DatasetValidationSplitter_seed(self):\n        data = torch.Tensor(list(range(1000)))\n        dataset = TensorDataset(data)\n\n        splitter_1 = DatasetValidationSplitter(len(dataset), 0.1, shuffle_seed=1)\n        trainset_1 = splitter_1.get_train_dataset(dataset)\n        validset_1 = splitter_1.get_val_dataset(dataset)\n\n        splitter_2 = DatasetValidationSplitter(len(dataset), 0.1, shuffle_seed=1)\n        trainset_2 = splitter_2.get_train_dataset(dataset)\n        validset_2 = splitter_2.get_val_dataset(dataset)\n\n        self.assertTrue(trainset_1.ids[0] == trainset_2.ids[0])\n        self.assertTrue(validset_1.ids[0] == validset_2.ids[0])\n\n\n"""
tests/test_end_to_end.py,25,"b'import unittest\n\nimport torch\nfrom torch.nn import Module, Linear\nimport torch.nn.init as init\n\nimport torchbearer\nfrom torchbearer import callbacks as c\n\n\nclass Net(Module):\n    def __init__(self, x):\n        super(Net, self).__init__()\n        self.pars = torch.nn.Parameter(x)\n\n    def f(self):\n        """"""\n        function to be minimised:\n        f(x) = (x[0]-5)^2 + x[1]^2 + (x[2]-1)^2\n        Solution:\n        x = [5,0,1]\n        """"""\n        out = torch.zeros_like(self.pars)\n        out[0] = self.pars[0]-5\n        out[1] = self.pars[1]\n        out[2] = self.pars[2]-1\n        return torch.sum(out**2)\n\n    def forward(self, _):\n        return self.f()\n\n\nclass NetWithState(Net):\n    def forward(self, _, state=None):\n        if state is None:\n            raise ValueError\n        return super(NetWithState, self).forward(_)\n\n\ndef loss(y_pred, y_true):\n    return y_pred\n\n\nclass TestEndToEnd(unittest.TestCase):\n    def test_basic_opt(self):\n        p = torch.tensor([2.0, 1.0, 10.0])\n        training_steps = 1000\n\n        model = NetWithState(p)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01)\n\n        trial = torchbearer.Trial(model, optim, loss).for_train_steps(training_steps).for_val_steps(1).for_test_steps(1)\n        trial.run()\n        trial.predict()\n        trial.evaluate()\n\n        self.assertAlmostEqual(model.pars[0].item(), 5.0, places=4)\n        self.assertAlmostEqual(model.pars[1].item(), 0.0, places=4)\n        self.assertAlmostEqual(model.pars[2].item(), 1.0, places=4)\n\n    def test_callbacks(self):\n        from torch.utils.data import TensorDataset\n        traingen = TensorDataset(torch.rand(10, 1, 3), torch.rand(10, 1))\n        valgen = TensorDataset(torch.rand(10, 1, 3), torch.rand(10, 1))\n        testgen = TensorDataset(torch.rand(10, 1, 3), torch.rand(10, 1))\n\n        model = torch.nn.Linear(3, 1)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01)\n        cbs = []\n        cbs.extend([c.EarlyStopping(), c.GradientClipping(10, model.parameters()), c.Best(\'test.pt\'),\n                    c.MostRecent(\'test.pt\'), c.ReduceLROnPlateau(), c.CosineAnnealingLR(0.1, 0.01),\n                    c.ExponentialLR(1), c.Interval(\'test.pt\'), c.CSVLogger(\'test_csv.pt\'),\n                    c.L1WeightDecay(), c.L2WeightDecay(), c.TerminateOnNaN(monitor=\'fail_metric\')])\n\n        trial = torchbearer.Trial(model, optim, torch.nn.MSELoss(), metrics=[\'loss\'], callbacks=cbs)\n        trial = trial.with_generators(traingen, valgen, testgen)\n        trial.run(2)\n        trial.predict()\n        trial.evaluate(data_key=torchbearer.TEST_DATA)\n        trial.evaluate()\n\n        import os\n        os.remove(\'test.pt\')\n        os.remove(\'test_csv.pt\')\n\n    def test_zero_model(self):\n        model = Linear(3, 1)\n        init.constant_(model.weight, 0)\n        init.constant_(model.bias, 0)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01)\n\n        trial = torchbearer.Trial(model, optim, loss)\n        trial.with_test_data(torch.rand(10, 3), batch_size=3)\n        preds = trial.predict()\n\n        for i in range(len(preds)):\n            self.assertAlmostEqual(preds[i], 0)\n\n    def test_basic_checkpoint(self):\n        p = torch.tensor([2.0, 1.0, 10.0])\n        training_steps = 500\n\n        model = Net(p)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01)\n\n        trial = torchbearer.Trial(model, optim, loss, callbacks=[torchbearer.callbacks.MostRecent(filepath=\'test.pt\')]).for_train_steps(training_steps).for_val_steps(1)\n        trial.run(2)  # Simulate 2 \'epochs\'\n\n        # Reload\n        p = torch.tensor([2.0, 1.0, 10.0])\n        model = Net(p)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01)\n\n        trial = torchbearer.Trial(model, optim, loss, callbacks=[torchbearer.callbacks.MostRecent(filepath=\'test.pt\')]).for_train_steps(training_steps)\n        trial.load_state_dict(torch.load(\'test.pt\'))\n        self.assertEqual(len(trial.state[torchbearer.HISTORY]), 2)\n        self.assertAlmostEqual(model.pars[0].item(), 5.0, places=4)\n        self.assertAlmostEqual(model.pars[1].item(), 0.0, places=4)\n        self.assertAlmostEqual(model.pars[2].item(), 1.0, places=4)\n\n        import os\n        os.remove(\'test.pt\')\n\n    def test_with_loader(self):\n        p = torch.tensor([2.0, 1.0, 10.0])\n        training_steps = 2\n\n        model = Net(p)\n        optim = torch.optim.SGD(model.parameters(), lr=0.01)\n        test_var = {\'loaded\': False}\n\n        def custom_loader(state):\n            state[torchbearer.X], state[torchbearer.Y_TRUE] = None, None\n            test_var[\'loaded\'] = True\n\n        trial = torchbearer.Trial(model, optim, loss, callbacks=[torchbearer.callbacks.MostRecent(filepath=\'test.pt\')]).for_train_steps(training_steps).for_val_steps(1)\n        trial.with_loader(custom_loader)\n        self.assertTrue(not test_var[\'loaded\'])\n        trial.run(1)\n        self.assertTrue(test_var[\'loaded\'])\n\n        import os\n        os.remove(\'test.pt\')\n\n    def test_only_model(self):\n        p = torch.tensor([2.0, 1.0, 10.0])\n        model = Net(p)\n        trial = torchbearer.Trial(model)\n        self.assertListEqual(trial.run(), [])\n\n    def test_no_model(self):\n        trial = torchbearer.Trial(None)\n        trial.run()\n        self.assertTrue(torchbearer.trial.MockModel()(torch.rand(1)) is None)\n\n    def test_no_train_steps(self):\n        trial = torchbearer.Trial(None)\n        trial.for_val_steps(10)\n        trial.run()'"
tests/test_magics.py,0,"b""import unittest\n\n\nclass TestMagics(unittest.TestCase):\n    def test_torchbearer_fn(self):\n        from torchbearer.magics import torchbearer as tb\n        from torchbearer.magics import is_notebook\n\n        self.assertFalse(is_notebook())\n        tb('notebook')\n        self.assertTrue(is_notebook())\n        tb('normal')\n        self.assertFalse(is_notebook())\n\n"""
tests/test_state.py,0,"b""import unittest\nimport warnings\n\nimport torchbearer\nfrom torchbearer.state import State\n\n\nclass TestStateKey(unittest.TestCase):\n    def test_key_metric(self):\n        key = torchbearer.state_key('test')\n        state = {key: 4}\n\n        self.assertDictEqual(key.process(state), {str(key): 4})\n        self.assertDictEqual(key.process_final(state), {str(key): 4})\n\n    def test_key_call(self):\n        key = torchbearer.state_key('call_test')\n        state = {key: 'test'}\n\n        self.assertEqual(key(state), 'test')\n\n    def test_key_repr(self):\n        key = torchbearer.state_key('repr_test')\n        self.assertEqual(str(key), 'repr_test')\n        self.assertEqual(repr(key), 'repr_test')\n\n    def test_key_added(self):\n        key = torchbearer.state_key('key')\n        self.assertTrue('key' in torchbearer.state.__keys__)\n\n    def test_collision(self):\n        _ = torchbearer.state_key('test')\n        key_1 = torchbearer.state_key('test')\n        key_2 = torchbearer.state_key('test')\n\n        self.assertTrue('test' != str(key_1))\n        self.assertTrue('test' != str(key_2))\n\n    def test_duplicate_string(self):\n        _ = torchbearer.state_key('test_dup')\n        key_1 = torchbearer.state_key('test_dup')\n        key_2 = torchbearer.state_key('test_dup')\n\n        self.assertTrue('test_dup_1' == str(key_1))\n        self.assertTrue('test_dup_2' == str(key_2))\n\n    def test_compare_to_statekey(self):\n        key_1 = torchbearer.state_key('test_compare_sk')\n        key_2 = torchbearer.state_key('test_compare_sk_2')\n        # Simulates same key in different sessions where the object hash is changed\n        key_2.key = 'test_compare_sk'\n        self.assertEqual(key_1, key_2)\n\n    def test_compare_to_string(self):\n        key_1 = torchbearer.state_key('test_compare')\n        self.assertEqual(key_1, 'test_compare')\n\n\nclass TestState(unittest.TestCase):\n    def test_contains(self):\n        s = State()\n\n        key1 = torchbearer.state_key('test_a')\n        key2 = torchbearer.state_key('test_b')\n\n        s[key1] = 1\n        s[key2] = 2\n\n        self.assertTrue(s.__contains__(key1))\n\n    def test_delete(self):\n        s = State()\n\n        key1 = torchbearer.state_key('test_a')\n        key2 = torchbearer.state_key('test_b')\n\n        s[key1] = 1\n        s[key2] = 2\n\n        self.assertTrue(s.__contains__(key1))\n        s.__delitem__(key1)\n        self.assertFalse(s.__contains__(key1))\n\n    def test_update(self):\n        s = State()\n\n        key1 = torchbearer.state_key('test_a')\n        key2 = torchbearer.state_key('test_b')\n\n        new_s = {key1: 1, key2: 2}\n        s.update(new_s)\n\n        self.assertTrue(s.__contains__(key1))\n        self.assertTrue(s[key1] == 1)\n\n    def test_update_state(self):\n        s = State()\n        new_s = State()\n\n        key1 = torchbearer.state_key('test_a')\n        key2 = torchbearer.state_key('test_b')\n\n        new_s_dict = {key1: 1, key2: 2}\n        new_s.update(new_s_dict)\n\n        s.update(new_s)\n        self.assertTrue(s.__contains__(key1))\n        self.assertTrue(s[key1] == 1)\n\n    def test_warn(self):\n        s = State()\n\n        key1 = torchbearer.state_key('test_a')\n        key2 = torchbearer.state_key('test_b')\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter('always')\n\n            s[key1] = 'key_1'\n            s[key2] = 'key_2'\n            s['bad_key'] = 'bad_key'\n            self.assertTrue(len(w) == 1)\n            self.assertTrue('State was accessed with a string' in str(w[-1].message))\n"""
tests/test_trial.py,201,"b'from unittest import TestCase\nfrom mock import MagicMock, Mock, patch, ANY, create_autospec\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport torchbearer\nimport torchbearer.callbacks as callbacks\nfrom torchbearer import Trial, State\nfrom torchbearer.metrics import Metric\nfrom torchbearer.trial import deep_to, load_batch_none, load_batch_predict, load_batch_standard, load_batch_infinite, update_device_and_dtype, CallbackListInjection\n\n\nclass _StateMaker(object):\n    def __getitem__(self, keys):\n        if not isinstance(keys, tuple):\n            keys = (keys,)\n        assert all(isinstance(key, slice) for key in keys)\n        state = State()\n        for k in keys:\n            state[k.start] = k.stop\n        return state\n\n\nmake_state = _StateMaker()\n\n\nclass TestMockOptimizer(TestCase):\n    @patch(\'torchbearer.trial.Optimizer\')\n    def test_mock_optimizer(self, mock_opt):\n        mock_opt.add_param_group = Mock()\n        mock_opt.load_state_dict = Mock()\n        mock_opt.state_dict = Mock()\n        mock_opt.step = Mock()\n        mock_opt.zero_grad = Mock()\n\n        opt = torchbearer.trial.MockOptimizer()\n\n        self.assertIsNone(opt.add_param_group({}))\n        mock_opt.add_param_group.assert_not_called()\n\n        self.assertIsNone(opt.load_state_dict({}))\n        mock_opt.load_state_dict.assert_not_called()\n\n        self.assertDictEqual(opt.state_dict(), {})\n        mock_opt.state_dict.assert_not_called()\n\n        self.assertIsNone(opt.step())\n        mock_opt.step.assert_not_called()\n\n        self.assertIsNone(opt.zero_grad())\n        mock_opt.zero_grad.assert_not_called()\n\n    def test_mock_optimizer_closure(self):\n        t = Trial(None)\n        closure = Mock()\n        opt = t.state[torchbearer.OPTIMIZER]\n        opt.step(closure)\n        self.assertTrue(closure.call_count == 1)\n\n\nclass TestCallbackListInjection(TestCase):\n    def test_pass_through(self):\n        mock = MagicMock()\n        injection = CallbackListInjection(None, mock)\n\n        # state_dict\n        mock.state_dict.return_value = \'test\'\n        self.assertEqual(injection.state_dict(), \'test\')\n        self.assertEqual(mock.state_dict.call_count, 1)\n\n        # load_state_dict\n        injection.load_state_dict(\'test\')\n        mock.load_state_dict.assert_called_once_with(\'test\')\n\n        # iter\n        mock.__iter__.return_value = [\'iterator\']\n        self.assertEqual(next(injection.__iter__()), \'iterator\')\n        self.assertEqual(mock.__iter__.call_count, 1)\n\n        # copy\n        mock.copy.return_value = \'copy\'\n        self.assertEqual(injection.copy(), \'copy\')\n\n        # append\n        injection.append(\'stuff to append\')\n        mock.append.assert_called_once_with(\'stuff to append\')\n\n    def test_order(self):\n        d = {\'my_number\': 10}\n\n        @callbacks.on_start\n        def set_one(state):\n            d[\'my_number\'] = 1\n\n        set_one.on_end = Mock()\n\n        @callbacks.on_start\n        def set_two(state):\n            d[\'my_number\'] = 2\n\n        set_two.on_end = Mock()\n\n        injection = CallbackListInjection(set_one, callbacks.CallbackList([set_two]))\n\n        injection.on_end({})\n        self.assertEqual(set_one.on_end.call_count, 1)\n        self.assertEqual(set_two.on_end.call_count, 1)\n\n        injection.on_start({})\n        self.assertEqual(d[\'my_number\'], 2)\n\n\nclass TestWithGenerators(TestCase):\n    def test_with_train_generator_state_filled(self):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_train_generator(generator, 1)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_GENERATOR] == generator)\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == 1)\n\n    @patch(\'warnings.warn\')\n    def test_with_train_generator_too_many_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_train_generator(generator, 10)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == 10)\n\n    @patch(\'warnings.warn\')\n    def test_with_train_generator_inf_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_train_generator(generator, -1)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == -1)\n\n    @patch(\'warnings.warn\')\n    def test_with_train_generator_fractional_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_train_generator(generator, 1.5)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == 1)\n\n    @patch(\'warnings.warn\')\n    def test_with_train_generator_negative_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_train_generator(generator, -2)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == -2)\n\n    @patch(\'warnings.warn\')\n    def test_with_train_generator_none_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_train_generator(generator, None)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == 2)\n\n    @patch(\'warnings.warn\')\n    def test_with_train_generator_old_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_train_steps(100)\n        torchbearertrial.with_train_generator(generator, None)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == 100)\n\n    def test_with_val_generator_state_filled(self):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_val_generator(generator, 1)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_GENERATOR] == generator)\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] == 1)\n\n    @patch(\'warnings.warn\')\n    def test_with_val_generator_too_many_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_val_generator(generator, 10)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] == 10)\n\n    @patch(\'warnings.warn\')\n    def test_with_val_generator_fractional_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_val_generator(generator, 1.5)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] == 1)\n\n    @patch(\'warnings.warn\')\n    def test_with_val_generator_negative_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_val_generator(generator, -2)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] == -2)\n\n    @patch(\'warnings.warn\')\n    def test_with_val_generator_none_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_val_generator(generator, None)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] == 2)\n\n    @patch(\'warnings.warn\')\n    def test_with_val_generator_old_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_val_steps(100)\n        torchbearertrial.with_val_generator(generator, None)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] == 100)\n\n    def test_with_test_generator_state_filled(self):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_test_generator(generator, 1)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_GENERATOR] == generator)\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == 1)\n\n    @patch(\'warnings.warn\')\n    def test_with_test_generator_too_many_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_test_generator(generator, 10)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == 10)\n\n    @patch(\'warnings.warn\')\n    def test_with_test_generator_fractional_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_test_generator(generator, 1.5)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == 1)\n\n    @patch(\'warnings.warn\')\n    def test_with_test_generator_negative_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_test_generator(generator, -2)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == -2)\n\n    @patch(\'warnings.warn\')\n    def test_with_test_generator_none_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_test_generator(generator, None)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == 2)\n\n    @patch(\'warnings.warn\')\n    def test_with_test_generator_old_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_test_steps(100)\n        torchbearertrial.with_test_generator(generator, None)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == 100)\n\n    @patch(\'warnings.warn\')\n    def test_for_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n\n        train_steps = 1\n\n        val_steps = 2\n\n        test_steps = 3\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        trainstep = torchbearertrial.for_train_steps = MagicMock()\n        valstep = torchbearertrial.for_val_steps = MagicMock()\n        teststep = torchbearertrial.for_test_steps = MagicMock()\n\n        torchbearertrial.for_steps(train_steps, val_steps, test_steps)\n        trainstep.assert_called_once_with(train_steps)\n        valstep.assert_called_once_with(val_steps)\n        teststep.assert_called_once_with(test_steps)\n\n    @patch(\'warnings.warn\')\n    def test_with_generators(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n\n        train_generator = MagicMock()\n        train_generator.__len__.return_value = 2\n        train_steps = 1\n\n        val_generator = MagicMock()\n        val_generator.__len__.return_value = 3\n        val_steps = 2\n\n        test_generator = MagicMock()\n        test_generator.__len__.return_value = 4\n        test_steps = 3\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        traingen = torchbearertrial.with_train_generator = MagicMock()\n        valgen = torchbearertrial.with_val_generator = MagicMock()\n        testgen = torchbearertrial.with_test_generator = MagicMock()\n\n        torchbearertrial.with_generators(train_generator, val_generator, test_generator, train_steps, val_steps, test_steps)\n        traingen.assert_called_once_with(train_generator, train_steps)\n        valgen.assert_called_once_with(val_generator, val_steps)\n        testgen.assert_called_once_with(test_generator, test_steps)\n\n    @patch(\'warnings.warn\')\n    def test_for_inf_train_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_inf_train_steps()\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == -1)\n\n    @patch(\'warnings.warn\')\n    def test_for_inf_val_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_inf_val_steps()\n\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] == -1)\n\n\n    @patch(\'warnings.warn\')\n    def test_for_inf_test_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_inf_test_steps()\n\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == -1)\n\n    @patch(\'warnings.warn\')\n    def test_for_inf_steps(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_inf_steps(True, True, True)\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == -1)\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] == -1)\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == -1)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_inf_steps(True, False, True)\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == -1)\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] != -1)\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] == -1)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_inf_steps(True, False, False)\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] == -1)\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] != -1)\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] != -1)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.for_inf_steps(False, False, False)\n        self.assertTrue(torchbearertrial.state[torchbearer.TRAIN_STEPS] != -1)\n        self.assertTrue(torchbearertrial.state[torchbearer.VALIDATION_STEPS] != -1)\n        self.assertTrue(torchbearertrial.state[torchbearer.TEST_STEPS] != -1)\n\n    @patch(\'warnings.warn\')\n    def test_with_inf_train_loader(self, _):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_inf_train_loader()\n\n        self.assertTrue(torchbearertrial.state[torchbearer.INF_TRAIN_LOADING])\n\n\nclass TestWithData(TestCase):\n    @patch(\'torchbearer.trial.TensorDataset\')\n    @patch(\'torchbearer.trial.DataLoader\')\n    def test_with_train_data(self, d, td):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        x = torch.rand(1,5)\n        y = torch.rand(1,5)\n        d.return_value = -1\n        steps = 4\n        shuffle = False\n        num_workers = 1\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_train_generator = MagicMock()\n        torchbearertrial.with_train_data(x, y, 1, shuffle=shuffle, num_workers=num_workers, steps=steps)\n\n        d.assert_called_once_with(ANY, 1, shuffle=shuffle, num_workers=num_workers)\n        torchbearertrial.with_train_generator.assert_called_once_with(-1, steps=4)\n        td.assert_called_once_with(x,y)\n\n    @patch(\'torchbearer.trial.TensorDataset\')\n    @patch(\'torchbearer.trial.DataLoader\')\n    def test_with_val_data(self, d, td):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        x = torch.rand(1,5)\n        y = torch.rand(1,5)\n        d.return_value = -1\n        steps = 4\n        shuffle = False\n        num_workers = 1\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_val_generator = MagicMock()\n        torchbearertrial.with_val_data(x, y, 1, shuffle=shuffle, num_workers=num_workers, steps=steps)\n\n        d.assert_called_once_with(ANY, 1, shuffle=shuffle, num_workers=num_workers)\n        torchbearertrial.with_val_generator.assert_called_once_with(-1, steps=4)\n        td.assert_called_once_with(x,y)\n\n    @patch(\'torchbearer.trial.TensorDataset\')\n    @patch(\'torchbearer.trial.DataLoader\')\n    def test_with_test_data(self, d, td):\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n\n        optimizer = MagicMock()\n        metric = Metric(\'test\')\n        criterion = None\n        generator = MagicMock()\n        generator.__len__.return_value = 2\n\n        x = torch.rand(1,5)\n        y = torch.rand(1,5)\n        d.return_value = -1\n        steps = 4\n        num_workers = 1\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric])\n        torchbearertrial.with_test_generator = MagicMock()\n        torchbearertrial.with_test_data(x, 1, num_workers=num_workers, steps=steps)\n\n        d.assert_called_once_with(ANY, 1, num_workers=num_workers)\n        torchbearertrial.with_test_generator.assert_called_once_with(-1, steps=4)\n        td.assert_called_once_with(x)\n\n    def test_with_data(self):\n        trial = Trial(None)\n        mock_train_data, mock_val_data, mock_test_data = Mock(), Mock(), Mock()\n        trial.with_train_data = mock_train_data\n        trial.with_val_data = mock_val_data\n        trial.with_test_data = mock_test_data\n        shuffle = True\n        batch_size = 30\n\n        one_tensor = torch.Tensor([1])\n        target_tensor = torch.Tensor([10])\n        trial.with_data(one_tensor, target_tensor, one_tensor*2, target_tensor*2, one_tensor*3, batch_size,\n                        train_steps=100, val_steps=200, test_steps=300, shuffle=shuffle)\n\n        self.assertTrue(mock_train_data.call_args[0] == (one_tensor, target_tensor, 30, shuffle, 1, 100))\n        self.assertTrue(mock_val_data.call_args[0] == (one_tensor*2, target_tensor*2, 30, shuffle, 1, 200))\n        self.assertTrue(mock_test_data.call_args[0] == (one_tensor*3, 30, 1, 300))\n\n\nclass TestWithClosureAndLoader(TestCase):\n    def test_with_closure(self):\n        def closure():\n            return \'test\'\n        t = Trial(None)\n        t.with_closure(closure)\n        self.assertTrue(t.closure() == \'test\')\n\n    def test_with_loader(self):\n        def loader(state):\n            print(\'test\')\n        t = Trial(None)\n        t.with_loader(loader)\n        self.assertTrue(t.state[torchbearer.LOADER] == loader)\n\n\nclass TestRun(TestCase):\n    def test_run_callback_calls(self):\n        metric = Metric(\'test\')\n        metric.process = Mock(return_value={\'test\': 0})\n        metric.process_final = Mock(return_value={\'test\': 0})\n        metric.reset = Mock(return_value=None)\n\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n\n        epochs = 1\n\n        callback = MagicMock()\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric], callbacks=[callback])\n        torchbearertrial._fit_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial._validation_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial.with_train_generator(generator, steps=train_steps)\n        torchbearertrial.run(epochs=epochs, verbose=0)\n\n        self.assertEqual(callback.on_start.call_count, 1)\n        self.assertEqual(callback.on_start_epoch.call_count, 1)\n        self.assertEqual(callback.on_end_epoch.call_count, 1)\n        self.assertEqual(callback.on_end.call_count, 1)\n\n    def test_run_epochs_ran_normal(self):\n        metric = Metric(\'test\')\n        metric.process = Mock(return_value={\'test\': 0})\n        metric.process_final = Mock(return_value={\'test\': 0})\n        metric.reset = Mock(return_value=None)\n\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n\n        epochs = 4\n\n        callback = MagicMock()\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric], callbacks=[callback])\n        torchbearertrial._fit_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial._validation_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial.with_train_generator(generator, steps=train_steps)\n        torchbearertrial.run(epochs=epochs, verbose=0)\n\n        self.assertTrue(torchbearertrial._fit_pass.call_count == epochs)\n\n    def test_run_epochs_ran_negative(self):\n        metric = Metric(\'test\')\n        metric.process = Mock(return_value={\'test\': 0})\n        metric.process_final = Mock(return_value={\'test\': 0})\n        metric.reset = Mock(return_value=None)\n\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n\n        epochs = -1\n\n        callback = MagicMock()\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric], callbacks=[callback])\n        torchbearertrial._fit_pass = Mock()\n        torchbearertrial._validation_pass = Mock()\n        torchbearertrial.with_train_generator(generator, steps=train_steps)\n        torchbearertrial.run(epochs=epochs, verbose=0)\n\n        self.assertTrue(torchbearertrial._fit_pass.call_count == 0)\n\n    def test_run_epochs_history_populated(self):\n        metric = Metric(\'test\')\n        metric.process = Mock(return_value={\'test\': 0})\n        metric.process_final = Mock(return_value={\'test\': 0})\n        metric.reset = Mock(return_value=None)\n\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n\n        epochs = 10\n\n        callback = MagicMock()\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric], callbacks=[callback])\n        torchbearertrial._fit_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial._validation_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial.with_train_generator(generator, steps=train_steps)\n        torchbearertrial.state[torchbearer.HISTORY] = [1,2,3,4,5]\n        torchbearertrial.run(epochs=epochs, verbose=0)\n\n        self.assertTrue(torchbearertrial._fit_pass.call_count == 5)\n\n    @patch(\'warnings.warn\')\n    def test_run_fit_pass_Args(self, _):\n        metric = Metric(\'test\')\n        metric.process = Mock(return_value={\'test\': 0})\n        metric.process_final = Mock(return_value={\'test\': 0})\n        metric.reset = Mock(return_value=None)\n\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n\n        epochs = 1\n        torchmodel = 1\n\n        torchbearertrial = Trial(torchmodel, None, None, [], callbacks=[])\n        torchbearertrial._fit_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial._validation_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial.with_train_generator(generator, steps=train_steps)\n        torchbearertrial.run(epochs=epochs, verbose=0)\n\n        self.assertEqual(torchbearertrial._fit_pass.call_count, 1)\n\n    def test_run_stop_training(self):\n        metric = Metric(\'test\')\n        metric.process = Mock(return_value={\'test\': 0})\n        metric.process_final = Mock(return_value={\'test\': 0})\n        metric.reset = Mock(return_value=None)\n\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n\n        epochs = 10\n\n        callback = MagicMock()\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric], callbacks=[callback])\n        torchbearertrial._fit_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial._validation_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial.with_train_generator(generator, steps=train_steps)\n        torchbearertrial.state[torchbearer.STOP_TRAINING] = True\n        torchbearertrial.run(epochs=epochs, verbose=0)\n\n        self.assertEqual(callback.on_start_epoch.call_count, 1)\n        self.assertTrue(callback.on_end_epoch.call_count == 0)\n        self.assertEqual(callback.on_end.call_count, 1)\n\n    def test_run_stop_training_second(self):\n        metric = Metric(\'test\')\n        metric.process = Mock(return_value={\'test\': 0})\n        metric.process_final = Mock(return_value={\'test\': 0})\n        metric.reset = Mock(return_value=None)\n\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n\n        epochs = 10\n        callback = MagicMock()\n\n        @torchbearer.callbacks.on_end_epoch\n        def stop_callback(state):\n            state[torchbearer.STOP_TRAINING] = True\n\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric], callbacks=[stop_callback, callback])\n        torchbearertrial._fit_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial._validation_pass = Mock(return_value={torchbearer.METRICS: {}})\n        torchbearertrial.with_train_generator(generator, steps=train_steps)\n        torchbearertrial.run(epochs=epochs, verbose=0)\n\n        self.assertEqual(callback.on_start_epoch.call_count, 1)\n        self.assertEqual(callback.on_end_epoch.call_count, 1)\n        self.assertEqual(callback.on_end.call_count, 1)\n\n    def test_run_history_metrics(self):\n        metric = Metric(\'test\')\n        metric.process = Mock(return_value={\'test\': 0})\n        metric.process_final = Mock(return_value={\'test\': 0})\n        metric.reset = Mock(return_value=None)\n\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n\n        epochs = 1\n\n        callback = MagicMock()\n        torchmodel = MagicMock()\n        torchmodel.forward = Mock(return_value=1)\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [metric], callbacks=[callback])\n        torchbearertrial._fit_pass = Mock(return_value={torchbearer.METRICS: {\'fit_test\': 1}})\n        torchbearertrial._validation_pass = Mock(return_value={\'val_test\': 2})\n        torchbearertrial.with_train_generator(generator, steps=train_steps)\n        history = torchbearertrial.run(epochs=epochs, verbose=0)\n        self.assertDictEqual(history[0], {\'train_steps\': train_steps, \'validation_steps\': None, \'fit_test\': 1, \'val_test\': 2})\n\n\nclass TestFitPass(TestCase):\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_train_called(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion, torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0, torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.BACKWARD_ARGS: {},\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertEqual(torchbearertrial.train.call_count, 1)\n\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_metrics_reset(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion, torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0, torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.BACKWARD_ARGS: {},\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertEqual(metric_list.reset.call_count, 1)\n\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_callback_calls(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        optimizer = MagicMock()\n        optimizer.step = lambda closure: closure()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion, torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0, torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.BACKWARD_ARGS: {}\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertEqual(callback_list.on_start_training.call_count, 1)\n        self.assertTrue(callback_list.on_sample.call_count == 3)\n        self.assertTrue(callback_list.on_forward.call_count == 3)\n        self.assertTrue(callback_list.on_criterion.call_count == 3)\n        self.assertTrue(callback_list.on_backward.call_count == 3)\n        self.assertTrue(callback_list.on_step_training.call_count == 3)\n        self.assertEqual(callback_list.on_end_training.call_count, 1)\n\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_optimizer_calls(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        optimizer = MagicMock()\n        optimizer.step = Mock(side_effect=lambda closure: closure())\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion, torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0, torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.BACKWARD_ARGS: {}\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertTrue(optimizer.zero_grad.call_count == 3)\n        self.assertTrue(optimizer.step.call_count == 3)\n\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_forward_call_no_state(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        optimizer = MagicMock()\n        optimizer.step = lambda closure: closure()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion, torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0, torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.BACKWARD_ARGS: {}\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertTrue(torchmodel.call_count == 3)\n        self.assertTrue(torchmodel.call_args_list[0][0][0].item() == 1)\n\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_forward_call_with_state(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        optimizer = MagicMock()\n        optimizer.step = lambda closure: closure()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0,\n            torchbearer.BACKWARD_ARGS: {}\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = True\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertTrue(torchmodel.call_count == 3)\n        self.assertTrue(len(torchmodel.call_args_list[0][1]) == 1)\n\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_criterion(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n        optimizer.step = lambda closure: closure()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        def crit_sig(y_pred, y_true):\n            return loss\n        criterion = create_autospec(crit_sig)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer, torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0,\n            torchbearer.BACKWARD_ARGS: {}, torchbearer.GENERATOR: generator\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = True\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None,\n                                  torchbearer.GENERATOR: generator}\n        torchbearertrial._fit_pass(state)\n        self.assertTrue(criterion.call_count == 3)\n        self.assertTrue(criterion.call_args_list[0][0][0] == 5)\n        self.assertTrue(criterion.call_args_list[0][0][1].item() == 1.0)\n\n    def test_fit_criterion_passed_state(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n        optimizer.step = lambda closure: closure()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        def crit_sig(state):\n            return loss\n        criterion = create_autospec(crit_sig)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer, torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0,\n            torchbearer.BACKWARD_ARGS: {}\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = True\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list, torchbearer.LOADER: None,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False,}\n\n        torchbearertrial._fit_pass(state)\n        self.assertTrue(criterion.call_count == 3)\n        self.assertTrue(criterion.call_args_list[0][0][0] == state)\n\n\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_backward(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n        optimizer.step = lambda closure: closure()\n\n        loss = MagicMock()\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer, torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.LOADER: None,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0,\n            torchbearer.BACKWARD_ARGS: {}\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = True\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.INF_TRAIN_LOADING: False, torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertTrue(loss.backward.call_count == 3)\n\n    @patch(\'torchbearer.CallbackListInjection\')\n    def test_fit_metrics_process(self, mock_inj):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        callback_list = MagicMock()\n        mock_inj.return_value = callback_list\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer, torchbearer.INF_TRAIN_LOADING: False, torchbearer.BACKWARD_ARGS: {},\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = True\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertTrue(metric_list.process.call_count == 3)\n\n    def test_fit_metrics_final(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer, torchbearer.INF_TRAIN_LOADING: False, torchbearer.BACKWARD_ARGS: {},\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float,\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = True\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        history = torchbearertrial._fit_pass(state)[torchbearer.METRICS]\n        self.assertEqual(metric_list.process_final.call_count, 1)\n        self.assertTrue(history[\'test\'] == 2)\n\n    def test_fit_stop_training(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        train_steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: True, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer, torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.BACKWARD_ARGS: {},\n            torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: train_steps, torchbearer.EPOCH: 0\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = True\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, train_steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        torchbearertrial._fit_pass(state)\n        self.assertEqual(metric_list.process.call_count, 1)\n\n    def test_fit_iterator_none(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = 1\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: True, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer, torchbearer.BACKWARD_ARGS: {},\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: None, torchbearer.TRAIN_STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1]\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: None, torchbearer.CALLBACK_LIST: callback_list, torchbearer.TRAIN_DATA: (None, steps), torchbearer.LOADER: None}\n\n        state = torchbearertrial._fit_pass(state)\n        self.assertTrue(state[torchbearer.ITERATOR] is None)\n\n    def test_fit_state_values(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = 1\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n        optimizer.step = lambda closure: closure()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: True, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.TRAIN_GENERATOR: generator, torchbearer.TRAIN_STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.BACKWARD_ARGS: {}\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.TRAIN_GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list,\n                                  torchbearer.TRAIN_DATA: (generator, steps), torchbearer.INF_TRAIN_LOADING: False, torchbearer.LOADER: None}\n\n        state = torchbearertrial._fit_pass(state)\n        self.assertTrue(state[torchbearer.ITERATOR] is not None)\n        self.assertTrue(state[torchbearer.Y_PRED] == 5)\n        self.assertTrue(state[torchbearer.LOSS].item() == 2)\n        self.assertTrue(state[torchbearer.METRICS][\'test\'] == 2)\n\n\nclass TestTestPass(TestCase):\n    def test_metric_reset(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_none\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        torchbearertrial._test_pass(state)\n        self.assertEqual(metric_list.reset.call_count, 1)\n\n    def test_callback_calls(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_none\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        torchbearertrial._test_pass(state)\n        self.assertEqual(callback_list.on_start_validation.call_count, 1)\n        self.assertTrue(callback_list.on_sample_validation.call_count == 3)\n        self.assertTrue(callback_list.on_forward_validation.call_count == 3)\n        self.assertTrue(callback_list.on_criterion_validation.call_count == 3)\n        self.assertTrue(callback_list.on_step_validation.call_count == 3)\n        self.assertEqual(callback_list.on_end_validation.call_count, 1)\n\n    def test_forward_no_state(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_standard\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        torchbearertrial._test_pass(state)\n        self.assertTrue(torchmodel.call_count == 3)\n        self.assertTrue(torchmodel.call_args_list[0][0][0].item() == 1)\n\n    def test_forward_with_state(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_none\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = True\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        torchbearertrial._test_pass(state)\n        self.assertTrue(torchmodel.call_count == 3)\n        self.assertTrue(len(torchmodel.call_args_list[0][1]) == 1)\n\n    def test_criterion(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        def spec_crit(y_pred, y_true):\n            pass\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = create_autospec(spec_crit)\n        criterion.return_value = loss\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_standard\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        torchbearertrial._test_pass(state)\n        self.assertTrue(criterion.call_count == 3)\n        self.assertTrue(criterion.call_args_list[0][0][0] == 5)\n        self.assertTrue(criterion.call_args_list[0][0][1].item() == 1.0)\n\n    def test_criterion_multiple_outputs(self):\n        data = [(torch.Tensor([1]), (torch.Tensor([1]), torch.Tensor([1]))),\n                (torch.Tensor([2]), (torch.Tensor([2]), torch.Tensor([2]))),\n                (torch.Tensor([3]), (torch.Tensor([3]), torch.Tensor([3]))), ]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = [5, 5]\n        optimizer = MagicMock()\n\n        def spec_crit(y_pred1, y_pred2, y_true1, y_true2):\n            pass\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = create_autospec(spec_crit)\n        criterion.return_value = loss\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_standard\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        torchbearertrial._test_pass(state)\n        self.assertTrue(criterion.call_count == 3)\n        self.assertTrue(criterion.call_args_list[0][0][0] == 5)\n        self.assertTrue(criterion.call_args_list[0][0][1] == 5)\n        self.assertTrue(criterion.call_args_list[0][0][2].item() == 1.0)\n        self.assertTrue(criterion.call_args_list[0][0][3].item() == 1.0)\n\n    def test_criterion_passed_state(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        def spec_crit(state):\n            pass\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = create_autospec(spec_crit)\n        criterion.return_value = loss\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\', torchbearer.LOADER: None,\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_standard\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list, torchbearer.LOADER: None,}\n\n        torchbearertrial._test_pass(state)\n        self.assertTrue(criterion.call_count == 3)\n        self.assertTrue(criterion.call_args_list[0][0][0] == state)\n\n    def test_metric_process(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_none\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        torchbearertrial._test_pass(state)\n        self.assertTrue(metric_list.process.call_count == 3)\n\n    def test_metric_final(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: False, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_none\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        history = torchbearertrial._test_pass(state)\n        self.assertEqual(metric_list.process_final.call_count, 1)\n        self.assertTrue(history[torchbearer.METRICS][\'test\'] == 2)\n\n    def test_stop_training(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = len(data)\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: True, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_none\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        torchbearertrial._test_pass(state)\n        self.assertEqual(metric_list.process.call_count, 1)\n\n    def test_iterator_none(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = 1\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: True, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: None, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_none\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: None, torchbearer.CALLBACK_LIST: callback_list}\n\n        state = torchbearertrial._test_pass(state)\n        self.assertTrue(state[torchbearer.ITERATOR] is None)\n\n    def test_state_values(self):\n        data = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])),\n                (torch.Tensor([3]), torch.Tensor([3]))]\n        generator = DataLoader(data)\n        steps = 1\n        epochs = 1\n        torchmodel = MagicMock()\n        torchmodel.return_value = 5\n        optimizer = MagicMock()\n\n        loss = torch.tensor([2.0], requires_grad=True)\n        criterion = Mock(return_value=loss)\n\n        metric_list = MagicMock()\n        metric_list.process.return_value = {\'test\': 0}\n        metric_list.process_final.return_value = {\'test\': 2}\n        callback_list = MagicMock()\n        torchbearer.CallbackListInjection = Mock(return_value=callback_list)\n\n        state = make_state[\n            torchbearer.MAX_EPOCHS: epochs, torchbearer.STOP_TRAINING: True, torchbearer.MODEL: torchmodel, torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer,\n            torchbearer.METRIC_LIST: metric_list, torchbearer.CALLBACK_LIST: callback_list, torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float, torchbearer.HISTORY: [], torchbearer.GENERATOR: generator, torchbearer.STEPS: steps, torchbearer.EPOCH: 0,\n            torchbearer.X: data[0][0], torchbearer.Y_TRUE: data[0][1], torchbearer.SAMPLER: load_batch_none\n        ]\n\n        torchbearertrial = Trial(torchmodel, optimizer, criterion, [], callbacks=[])\n        torchbearertrial.train = Mock()\n        torchbearertrial.pass_state = False\n        torchbearertrial.state = {torchbearer.GENERATOR: generator, torchbearer.CALLBACK_LIST: callback_list}\n\n        state = torchbearertrial._test_pass(state)\n        self.assertTrue(state[torchbearer.ITERATOR] is not None)\n        self.assertTrue(state[torchbearer.Y_PRED] == 5)\n        self.assertTrue(state[torchbearer.LOSS].item() == 2)\n        self.assertTrue(state[torchbearer.METRICS][\'test\'] == 2)\n\n\nclass TestTrialValEvalPred(TestCase):\n    def test_validation_pass(self):\n        generator = MagicMock()\n        steps = 5\n        torchbearer.CallbackListInjection = Mock()\n\n        state = {torchbearer.VALIDATION_GENERATOR: generator, torchbearer.VALIDATION_STEPS: steps, torchbearer.METRICS: 1}\n        t = Trial(MagicMock())\n        eval_mock = t.eval = Mock()\n        test_pass_mock = t._test_pass = Mock()\n        t.state = {torchbearer.VALIDATION_GENERATOR: generator, torchbearer.CALLBACK_LIST: None,\n                   torchbearer.VALIDATION_DATA: (generator, steps), torchbearer.LOADER: None}\n        metrics = t._validation_pass(state)\n\n        self.assertEqual(eval_mock.call_count, 1)\n        self.assertEqual(test_pass_mock.call_count, 1)\n        test_pass_state = test_pass_mock.call_args[0][0]\n        self.assertTrue(test_pass_state[torchbearer.GENERATOR] == generator)\n        self.assertTrue(test_pass_state[torchbearer.STEPS] == steps)\n        self.assertTrue(metrics == 1)\n\n    def test_validation_pass_none(self):\n        generator = None\n        steps = None\n        torchbearer.CallbackListInjection = Mock()\n\n        state = {torchbearer.VALIDATION_GENERATOR: generator, torchbearer.VALIDATION_STEPS: steps, torchbearer.METRICS: 1}\n        t = Trial(MagicMock())\n        eval_mock = t.eval = Mock()\n        t._test_pass = Mock()\n        t.state = {torchbearer.VALIDATION_GENERATOR: generator, torchbearer.CALLBACK_LIST: None,\n                   torchbearer.VALIDATION_DATA: (generator, steps), torchbearer.LOADER: None}\n        t._validation_pass(state)\n\n        self.assertTrue(eval_mock.call_count == 0)\n\n    def test_evaluate(self):\n        generator = MagicMock()\n        steps = 5\n        torchbearer.CallbackListInjection = Mock()\n\n        t = Trial(MagicMock())\n        eval_mock = t.eval = Mock()\n        clist = MagicMock()\n        state = {torchbearer.HISTORY: [{\'train_steps\': \'steps\', \'train_metric\': 2}], torchbearer.VALIDATION_GENERATOR: generator,\n                 torchbearer.CALLBACK_LIST: clist, torchbearer.VALIDATION_STEPS: steps, torchbearer.VALIDATION_DATA: (generator, steps),\n                 torchbearer.METRICS: {\'val_metric\': 1}, torchbearer.LOADER: None}\n        test_pass_mock = t._test_pass = Mock(return_value=state)\n        t.state = state\n        metrics = t.evaluate()\n\n        self.assertEqual(clist.on_start.call_count, 1)\n        self.assertEqual(clist.on_start_epoch.call_count, 1)\n        self.assertEqual(clist.on_end_epoch.call_count, 1)\n        self.assertEqual(clist.on_end.call_count, 1)\n        self.assertEqual(eval_mock.call_count, 1)\n        self.assertEqual(test_pass_mock.call_count, 1)\n        test_pass_state = test_pass_mock.call_args[0][0]\n        self.assertTrue(test_pass_state[torchbearer.GENERATOR] == generator)\n        self.assertTrue(test_pass_state[torchbearer.STEPS] == steps)\n        self.assertEqual(metrics[\'val_metric\'], 1)\n        self.assertDictEqual(state[torchbearer.HISTORY][0], {\'train_steps\': \'steps\', \'train_metric\': 2, \'val_metric\': 1})\n\n    def test_evaluate_none(self):\n        generator = None\n        steps = None\n        torchbearer.CallbackListInjection = Mock()\n\n        t = Trial(MagicMock())\n        eval_mock = t.eval = Mock()\n        test_pass_mock = t._test_pass = Mock(return_value={torchbearer.METRICS: 1})\n        t.state = {torchbearer.VALIDATION_GENERATOR: generator, torchbearer.CALLBACK_LIST: None,\n                   torchbearer.VALIDATION_STEPS: steps, torchbearer.VALIDATION_DATA: (generator, steps), torchbearer.LOADER: None}\n        metrics = t.evaluate()\n\n        self.assertTrue(eval_mock.call_count == 0)\n\n    def test_predict(self):\n        generator = MagicMock()\n        steps = 5\n        torchbearer.CallbackListInjection = Mock()\n\n        state = {torchbearer.TEST_GENERATOR: generator, torchbearer.TEST_STEPS: steps, torchbearer.METRICS: 1}\n        t = Trial(MagicMock())\n        eval_mock = t.eval = Mock()\n        test_pass_mock = t._test_pass = Mock(return_value={torchbearer.FINAL_PREDICTIONS: 1})\n        clist = MagicMock()\n        t.state = {torchbearer.TEST_GENERATOR: generator, torchbearer.CALLBACK_LIST: clist, torchbearer.TEST_STEPS: steps,\n                   torchbearer.TEST_DATA: (generator, steps), torchbearer.LOADER: None}\n        metrics = t.predict()\n\n        self.assertEqual(clist.on_start.call_count, 1)\n        self.assertEqual(clist.on_start_epoch.call_count, 1)\n        self.assertEqual(clist.on_end_epoch.call_count, 1)\n        self.assertEqual(clist.on_end.call_count, 1)\n        self.assertEqual(eval_mock.call_count, 1)\n        self.assertEqual(test_pass_mock.call_count, 1)\n        test_pass_state = test_pass_mock.call_args[0][0]\n        self.assertTrue(test_pass_state[torchbearer.GENERATOR] == generator)\n        self.assertTrue(test_pass_state[torchbearer.STEPS] == steps)\n        self.assertTrue(metrics == 1)\n\n    def test_predict_none(self):\n        generator = None\n        steps = None\n        torchbearer.CallbackListInjection = Mock()\n\n        state = {torchbearer.TEST_GENERATOR: generator, torchbearer.TEST_STEPS: steps, torchbearer.METRICS: 1}\n        t = Trial(MagicMock())\n        eval_mock = t.eval = Mock()\n        test_pass_mock = t._test_pass = Mock(return_value={torchbearer.FINAL_PREDICTIONS: 1})\n        t.state = {torchbearer.TEST_GENERATOR: generator, torchbearer.CALLBACK_LIST: None, torchbearer.TEST_STEPS: steps,\n                   torchbearer.TEST_DATA: (generator, steps), torchbearer.LOADER: None}\n        metrics = t.predict()\n\n        self.assertTrue(eval_mock.call_count == 0)\n\n\nclass TestReplay(TestCase):\n    @patch(\'torchbearer.trial.Tqdm\')\n    def test_replay_tqdm(self, tq):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n        history = [{\'train_steps\': 10, \'validation_steps\': 5, \'test\': i, \'val_test2\': i+1} for i in range(10)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback])\n        self.assertEqual(tq.call_count, 1)\n\n    @patch(\'torchbearer.trial.Tqdm\')\n    def test_replay_no_tqdm(self, tq):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n        history = [{\'train_steps\': 10, \'validation_steps\': 5, \'test\': i, \'val_test2\': i+1} for i in range(10)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback], verbose=0)\n        tq.assert_not_called()\n\n    @patch(\'torchbearer.trial.Tqdm\')\n    def test_replay_multi_call(self, mock_tqdm):\n        t = Trial(MagicMock())\n        history = [{\'train_steps\': 10, \'validation_steps\': 5, \'test\': i, \'val_test2\': i + 1} for i in range(1)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(verbose=2)\n        mock_tqdm.reset_mock()\n        callback = MagicMock()\n        t.replay(callbacks=[callback], verbose=0)\n        mock_tqdm.assert_not_called()\n\n    def test_replay_callback_calls(self):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n        history = [{\'train_steps\': 10, \'validation_steps\': 5, \'test\': i, \'val_test2\': i+1} for i in range(10)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback], verbose=0)\n        self.assertEqual(callback.on_start.call_count, 1)\n        self.assertTrue(callback.on_sample.call_count == 100)\n        self.assertTrue(callback.on_sample_validation.call_count == 50)\n\n    def test_replay_none_train_steps(self):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n        history = [{\'train_steps\': None, \'validation_steps\': 5, \'test\': i, \'val_test2\': i+1} for i in range(10)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback], verbose=0)\n        self.assertEqual(callback.on_start.call_count, 1)\n        self.assertTrue(callback.on_sample.call_count == 0)\n        self.assertTrue(callback.on_sample_validation.call_count == 50)\n\n    def test_replay_none_validation_steps(self):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n        history = [{\'train_steps\': 10, \'validation_steps\': None, \'test\': i} for i in range(10)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback], verbose=0)\n        self.assertEqual(callback.on_start.call_count, 1)\n        self.assertTrue(callback.on_sample.call_count == 100)\n        self.assertTrue(callback.on_sample_validation.call_count == 0)\n\n    def test_replay_one_batch_true(self):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n        history = [{\'train_steps\': 10, \'validation_steps\': 5, \'test\': i, \'val_test2\': i+1} for i in range(1)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback], verbose=0, one_batch=True)\n        self.assertTrue(callback.on_start.call_count == 1)\n        self.assertTrue(callback.on_sample.call_count == 1)\n        self.assertTrue(callback.on_sample_validation.call_count == 1)\n\n    def test_replay_metrics(self):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n        history = [{\'train_steps\': 10, \'validation_steps\': 5, \'test\': i, \'val_test2\': i+1} for i in range(10)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback], verbose=0)\n\n        self.assertTrue(callback.on_sample.call_args_list[0][0][0][torchbearer.METRICS][\'test\'] == 9)\n        self.assertTrue(callback.on_sample_validation.call_args_list[0][0][0][torchbearer.METRICS][\'val_test2\'] == 10)\n\n    def test_replay_stop_training(self):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n\n        @torchbearer.callbacks.on_sample\n        def stop_training(state):\n            state[torchbearer.STOP_TRAINING] = True\n\n        history = [{\'train_steps\': 10, \'validation_steps\': 5, \'test\': i, \'val_test2\': i+1} for i in range(10)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback, stop_training], verbose=0)\n\n        self.assertTrue(callback.on_sample.call_count == 10)\n        callback.on_sample_validation.assert_not_called()\n\n    def test_replay_stop_training_on_validation(self):\n        t = Trial(MagicMock())\n        callback = MagicMock()\n\n        @torchbearer.callbacks.on_sample_validation\n        def stop_training(state):\n            state[torchbearer.STOP_TRAINING] = True\n\n        history = [{\'train_steps\': 10, \'validation_steps\': 5, \'test\': i, \'val_test2\': i+1} for i in range(10)]\n\n        t.state[torchbearer.HISTORY] = history\n        t.replay(callbacks=[callback, stop_training], verbose=0)\n\n        self.assertTrue(callback.on_sample_validation.call_count == 1)\n\n\nclass TestTrialMembers(TestCase):\n    def test_init_none_criterion(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        optimizer = MagicMock()\n        metric = MagicMock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, None, [metric], []).to(\'cpu\', torch.float64)\n        loss = torchbearertrial.state[torchbearer.CRITERION](None, None)\n        self.assertTrue(str(loss.device) == \'cpu\')\n        self.assertTrue(loss.dtype == torch.float64)\n        self.assertTrue(torch.is_tensor(loss))\n        self.assertTrue(loss.shape == torch.Size([1]))\n        self.assertTrue(loss.item() == 0)\n\n    def test_init_none_criterion_add(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        optimizer = MagicMock()\n        metric = MagicMock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, None, [metric], []).to(\'cpu\', torch.float64)\n        loss = torchbearertrial.state[torchbearer.CRITERION](None, None)\n        loss = loss + 1\n        self.assertTrue(str(loss.device) == \'cpu\')\n        self.assertTrue(loss.dtype == torch.float64)\n        self.assertTrue(torch.is_tensor(loss))\n        self.assertTrue(loss.shape == torch.Size([1]))\n        self.assertTrue(loss.item() == 1)\n\n    def test_str(self):\n        torchmodel = ""mod""\n        optimizer = ""opt""\n        metric = torchbearer.metrics.Metric(\'met\')\n        cb = torchbearer.callbacks.Callback()\n        cb.on_init = Mock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, ""crit"", [metric], [cb])\n        correct_string = ""--------------------- OPTIMZER ---------------------\\nopt\\n\\n-------------------- CRITERION ---------------------\\ncrit\\n\\n--------------------- METRICS ----------------------\\n[\'met\']\\n\\n-------------------- CALLBACKS ---------------------\\n[\'torchbearer.bases.Callback\']\\n\\n---------------------- MODEL -----------------------\\nmod\\n\\n""\n        self.assertEqual(str(torchbearertrial), correct_string)\n        self.assertEqual(cb.on_init.call_count, 1)\n\n    def test_repr(self):\n        torchmodel = ""mod""\n        optimizer = ""opt""\n        metric = torchbearer.metrics.Metric(\'met\')\n\n        torchbearertrial = Trial(torchmodel, optimizer, ""crit"", [metric], [torchbearer.callbacks.Callback()])\n        self.assertEqual(str(torchbearertrial), repr(torchbearertrial))\n\n    def test_train(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        optimizer = MagicMock()\n        metric = MagicMock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, None, [metric], [])\n        torchbearertrial.train()\n        self.assertTrue(torchbearertrial.state[torchbearer.MODEL].training == True)\n        self.assertEqual(metric.train.call_count, 1)\n\n    def test_eval(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        optimizer = MagicMock()\n        metric = MagicMock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, None, [metric], [])\n        torchbearertrial.eval()\n        self.assertTrue(torchbearertrial.state[torchbearer.MODEL].training == False)\n        self.assertEqual(metric.eval.call_count, 1)\n\n    def test_to_both_args(self):\n        dev = \'cuda:1\'\n        dtype = torch.float16\n\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.to = Mock()\n        optimizer = torch.optim.Adam(torchmodel.parameters(), 0.1)\n        state_tensor = torch.Tensor([1])\n        state_tensor.to = Mock()\n        optimizer.state = {\'test\': {\'test\': state_tensor}}\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.to(dev, dtype)\n\n        self.assertTrue(torchmodel.to.call_args[0][0] == dev)\n        self.assertTrue(torchmodel.to.call_args[0][1] == dtype)\n        self.assertTrue(state_tensor.to.call_args[0][0] == dev)\n        self.assertTrue(state_tensor.to.call_args[0][1] == dtype)\n\n    def test_to_only_device(self):\n        dev = \'cuda:1\'\n\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.to = Mock()\n        optimizer = torch.optim.Adam(torchmodel.parameters(), 0.1)\n        state_tensor = torch.Tensor([1])\n        state_tensor.to = Mock()\n        optimizer.state = {\'test\': {\'test\': state_tensor}}\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.to(dev)\n\n        self.assertTrue(torchmodel.to.call_args[0][0] == dev)\n        self.assertTrue(state_tensor.to.call_args[0][0] == dev)\n\n    def test_to_only_dtype(self):\n        dtype = torch.float16\n\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.to = Mock()\n        optimizer = torch.optim.Adam(torchmodel.parameters(), 0.1)\n        state_tensor = torch.Tensor([1])\n        state_tensor.to = Mock()\n        optimizer.state = {\'test\': {\'test\': state_tensor}}\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.to(dtype)\n\n        self.assertTrue(torchmodel.to.call_args[0][0] == dtype)\n        self.assertTrue(state_tensor.to.call_args[0][0] == dtype)\n\n    def test_to_kwargs(self):\n        dev = \'cuda:1\'\n        dtype = torch.float16\n\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.to = Mock()\n        optimizer = torch.optim.Adam(torchmodel.parameters(), 0.1)\n        state_tensor = torch.Tensor([1])\n        state_tensor.to = Mock()\n        optimizer.state = {\'test\': {\'test\': state_tensor}}\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.to(device=dev, dtype=dtype)\n\n        self.assertTrue(torchmodel.to.call_args[1][\'device\'] == dev)\n        self.assertTrue(torchmodel.to.call_args[1][\'dtype\'] == dtype)\n        self.assertTrue(state_tensor.to.call_args[1][\'device\'] == dev)\n        self.assertTrue(state_tensor.to.call_args[1][\'dtype\'] == dtype)\n\n    @patch(\'torch.cuda.current_device\')\n    def test_cuda_no_device(self, device_mock):\n        device_mock.return_value = 111\n\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.load_state_dict = Mock()\n\n        optimizer = torch.optim.SGD(torchmodel.parameters(), 0.1)\n        optimizer.load_state_dict = Mock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.to = Mock()\n        torchbearertrial.cuda()\n\n        self.assertTrue(torchbearertrial.to.call_args[0][0] == \'cuda:\' + str(111))\n\n    def test_cuda_with_device(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.load_state_dict = Mock()\n\n        optimizer = torch.optim.SGD(torchmodel.parameters(), 0.1)\n        optimizer.load_state_dict = Mock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.to = Mock()\n        torchbearertrial.cuda(device=\'2\')\n\n        self.assertTrue(torchbearertrial.to.call_args[0][0] == \'cuda:2\')\n\n    def test_cpu(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.load_state_dict = Mock()\n\n        optimizer = torch.optim.SGD(torchmodel.parameters(), 0.1)\n        optimizer.load_state_dict = Mock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.to = Mock()\n        torchbearertrial.cpu()\n\n        self.assertTrue(torchbearertrial.to.call_args[0][0] == \'cpu\')\n\n    def test_load_state_dict_resume(self):\n        key_words = {\'strict\': True}\n\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.load_state_dict = Mock()\n        torch_state = torchmodel.state_dict()\n\n        optimizer = torch.optim.SGD(torchmodel.parameters(), 0.1)\n        optimizer.load_state_dict = Mock()\n        optimizer_state = optimizer.state_dict()\n\n        callback_list = MagicMock()\n        callback_list.state_dict = Mock(return_value = 1)\n\n        history = [\'test\']\n\n        torchbearertrial = Trial(torchmodel, optimizer, None, [], [])\n        torchbearertrial.state[torchbearer.CALLBACK_LIST] = callback_list\n        torchbearertrial.state[torchbearer.HISTORY] = history\n        torchbearer_state = torchbearertrial.state_dict()\n        torchbearertrial.state[torchbearer.HISTORY] = \'Wrong\'\n\n        torchbearertrial.load_state_dict(torchbearer_state, **key_words)\n\n        self.assertTrue(torchmodel.load_state_dict.call_args[0][0] == torch_state)\n        self.assertTrue(optimizer.load_state_dict.call_args[0][0] == optimizer_state)\n        self.assertTrue(optimizer.load_state_dict.call_args[0][0] == optimizer_state)\n        self.assertTrue(callback_list.load_state_dict.call_args[0][0] == 1)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.HISTORY] == history)\n        self.assertEqual(torchbearertrial.state[torchbearer.MODEL].load_state_dict.call_count, 1)\n        self.assertEqual(torchbearertrial.state[torchbearer.OPTIMIZER].load_state_dict.call_count, 1)\n        self.assertEqual(torchbearertrial.state[torchbearer.CALLBACK_LIST].load_state_dict.call_count, 1)\n        self.assertTrue(torchmodel.load_state_dict.call_args[1] == key_words)\n\n    def test_load_state_dict_no_resume(self):\n        key_words = {\'strict\': True}\n\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel.load_state_dict = Mock()\n        torch_state = torchmodel.state_dict()\n\n        optimizer = torch.optim.SGD(torchmodel.parameters(), 0.1)\n        optimizer.load_state_dict = Mock()\n        optimizer_state = optimizer.state_dict()\n\n        history = [\'test\']\n\n        torchbearertrial = Trial(torchmodel, optimizer, None, [], [])\n        torchbearertrial.state[torchbearer.HISTORY] = history\n        torchbearer_state = torchbearertrial.state_dict()\n        torchbearertrial.state[torchbearer.HISTORY] = \'Wrong\'\n\n        torchbearertrial.load_state_dict(torchbearer_state, resume=False, **key_words)\n\n        self.assertTrue(torchbearertrial.state[torchbearer.HISTORY] is \'Wrong\')\n        self.assertEqual(torchbearertrial.state[torchbearer.MODEL].load_state_dict.call_count, 1)\n        self.assertTrue(torchbearertrial.state[torchbearer.OPTIMIZER].load_state_dict.call_count == 0)\n        self.assertTrue(torchmodel.load_state_dict.call_args[1] == key_words)\n\n    def test_load_state_dict_wrong_version(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1, 1))\n        torchmodel.load_state_dict = Mock()\n\n        optimizer = torch.optim.SGD(torchmodel.parameters(), 0.1)\n        optimizer.load_state_dict = Mock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, None, [], [])\n\n        torchbearer_state = torchbearertrial.state_dict()\n        torchbearer_state[torchbearer.VERSION] = \'0.1.7\'  # Old version\n\n        import warnings\n        with warnings.catch_warnings(record=True) as w:\n            torchbearertrial.load_state_dict(torchbearer_state, resume=True)\n            self.assertTrue(len(w) == 1)\n            self.assertTrue(issubclass(w[-1].category, UserWarning))\n\n    def test_load_state_dict_not_torchbearer(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1, 1))\n        torchmodel.load_state_dict = Mock()\n\n        optimizer = torch.optim.SGD(torchmodel.parameters(), 0.1)\n        optimizer.load_state_dict = Mock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, None, [], [])\n\n        torchbearer_state = torchbearertrial.state_dict()\n        torchbearer_state[torchbearer.VERSION] = \'0.1.7\'  # Old version\n\n        import warnings\n        with warnings.catch_warnings(record=True) as w:\n            torchbearertrial.load_state_dict(torchbearer_state[torchbearer.MODEL])\n            self.assertTrue(len(w) == 1)\n            self.assertTrue(issubclass(w[-1].category, UserWarning))\n\n        self.assertEqual(torchmodel.load_state_dict.call_count, 1)\n        optimizer.load_state_dict.assert_not_called()\n\n    def test_state_dict(self):\n        torchmodel = torch.nn.Sequential(torch.nn.Linear(1,1))\n        torchmodel_state = torchmodel.state_dict()\n\n        optimizer = torch.optim.SGD(torchmodel.parameters(), 0.1)\n        optimizer_state = optimizer.state_dict()\n\n        callback_list = MagicMock()\n        callback_list.state_dict = Mock(return_value = 1)\n\n        history = [\'test\']\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.state[torchbearer.HISTORY] = history\n        torchbearertrial.state[torchbearer.CALLBACK_LIST] = callback_list\n        torchbearer_state = torchbearertrial.state_dict()\n\n        self.assertTrue(torchbearer_state[torchbearer.VERSION] == torchbearer.__version__.replace(\'.dev\', \'\'))\n        self.assertTrue(torchbearer_state[torchbearer.MODEL] == torchmodel_state)\n        self.assertTrue(torchbearer_state[torchbearer.OPTIMIZER] == optimizer_state)\n        self.assertTrue(torchbearer_state[torchbearer.CALLBACK_LIST] == 1)\n        self.assertTrue(torchbearer_state[torchbearer.HISTORY] == history)\n\n    def test_state_dict_kwargs(self):\n        keywords = {\'destination\': None, \'prefix\': \'\', \'keep_vars\': False}\n        torchmodel = MagicMock()\n        optimizer = MagicMock()\n\n        torchbearertrial = Trial(torchmodel, optimizer, torch.nn.L1Loss(), [])\n        torchbearertrial.state_dict(**keywords)\n\n        self.assertTrue(torchmodel.state_dict.call_args[1] == keywords)\n        self.assertTrue(optimizer.state_dict.call_args[1] == {})\n\n\nclass TestTrialFunctions(TestCase):\n    @patch(\'torchbearer.trial.Tqdm\')\n    def test_get_printer_no_tqdm(self, tq):\n        verbose = 0\n        validation_label_letter = \'v\'\n\n        printer = torchbearer.trial.get_printer(verbose=verbose, validation_label_letter=validation_label_letter)\n        tq.assert_not_called()\n\n    @patch(\'torchbearer.trial.Tqdm\')\n    def test_get_printer_verbose_1(self, tq):\n        verbose = 1\n        validation_label_letter = \'v\'\n\n        printer = torchbearer.trial.get_printer(verbose=verbose, validation_label_letter=validation_label_letter)\n        tq.assert_called_once_with(on_epoch=True, validation_label_letter=validation_label_letter)\n\n    @patch(\'torchbearer.trial.Tqdm\')\n    def test_get_printer_verbose_2(self, tq):\n        verbose = 2\n        validation_label_letter = \'v\'\n\n        printer = torchbearer.trial.get_printer(verbose=verbose, validation_label_letter=validation_label_letter)\n        tq.assert_called_once_with(validation_label_letter=validation_label_letter)\n\n    @patch(\'torchbearer.trial.Tqdm\')\n    def test_get_printer_letter(self, tq):\n        verbose = 2\n        validation_label_letter = \'r\'\n\n        printer = torchbearer.trial.get_printer(verbose=verbose, validation_label_letter=validation_label_letter)\n        tq.assert_called_once_with(validation_label_letter=validation_label_letter)\n\n    @patch(\'torchbearer.trial.get_printer\')\n    @patch(\'torchbearer.trial.CallbackListInjection\')\n    def test_inject_printer_no_tqdm(self, c_inj, get_print_mock):\n        callback_list = torchbearer.callbacks.CallbackList([])\n\n        class SomeClass:\n            @torchbearer.inject_printer(\'v\')\n            def test_func(self, verbose=0):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.CALLBACK_LIST: callback_list}\n        t.test_func(verbose=0)\n        self.assertEqual(c_inj.call_count, 1)\n        get_print_mock.assert_called_once_with(validation_label_letter=\'v\', verbose=0)\n\n    @patch(\'torchbearer.trial.get_printer\')\n    @patch(\'torchbearer.trial.CallbackListInjection\')\n    def test_inject_printer_no_kwargs(self, c_inj, get_print_mock):\n        callback_list = torchbearer.callbacks.CallbackList([])\n\n        class SomeClass:\n            @torchbearer.inject_printer(\'v\')\n            def test_func(self, verbose=0):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.CALLBACK_LIST: callback_list}\n        t.test_func(1)\n        self.assertEqual(c_inj.call_count, 1)\n        get_print_mock.assert_called_once_with(validation_label_letter=\'v\', verbose=1)\n\n    @patch(\'torchbearer.trial.get_printer\')\n    @patch(\'torchbearer.trial.CallbackListInjection\')\n    def test_inject_both(self, c_inj, get_print_mock):\n        callback_list = torchbearer.callbacks.CallbackList([])\n        generator = MagicMock()\n        steps = None\n\n        class SomeClass:\n            @torchbearer.inject_printer(\'v\')\n            @torchbearer.inject_sampler(torchbearer.GENERATOR, load_batch_standard)\n            def test_func(self, verbose=0):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.CALLBACK_LIST: callback_list, torchbearer.GENERATOR: (generator, steps), torchbearer.LOADER: None}\n        t.test_func(1)\n        self.assertEqual(c_inj.call_count, 1)\n        get_print_mock.assert_called_once_with(validation_label_letter=\'v\', verbose=1)\n\n    @patch(\'torchbearer.trial.get_printer\')\n    @patch(\'torchbearer.trial.CallbackListInjection\')\n    def test_inject_printer_tqdm_on_epoch(self, c_inj, get_print_mock):\n        callback_list = torchbearer.callbacks.CallbackList([])\n\n        class SomeClass:\n            @torchbearer.inject_printer(\'t\')\n            def test_func(self, verbose=0):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.CALLBACK_LIST: callback_list}\n        t.test_func(verbose=1)\n        self.assertEqual(c_inj.call_count, 1)\n        get_print_mock.assert_called_once_with(validation_label_letter=\'t\', verbose=1)\n\n    @patch(\'torchbearer.trial.get_printer\')\n    @patch(\'torchbearer.trial.CallbackListInjection\')\n    def test_inject_printer_tqdm_on_batch(self, c_inj, get_print_mock):\n        callback_list = torchbearer.callbacks.CallbackList([])\n\n        class SomeClass:\n            @torchbearer.inject_printer(\'t\')\n            def test_func(self, verbose=0):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.CALLBACK_LIST: callback_list}\n        t.test_func(verbose=2)\n        self.assertEqual(c_inj.call_count, 1)\n        get_print_mock.assert_called_once_with(validation_label_letter=\'t\', verbose=2)\n\n    @patch(\'torchbearer.trial.get_printer\')\n    @patch(\'torchbearer.trial.CallbackListInjection\')\n    def test_inject_printer_tqdm_default(self, c_inj, get_print_mock):\n        callback_list = torchbearer.callbacks.CallbackList([])\n\n        class SomeClass:\n            @torchbearer.inject_printer(\'t\')\n            def test_func(self, verbose=2):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.CALLBACK_LIST: callback_list}\n        t.test_func()\n        self.assertEqual(c_inj.call_count, 1)\n        get_print_mock.assert_called_once_with(validation_label_letter=\'t\', verbose=2)\n\n    @patch(\'torchbearer.trial.Tqdm\')\n    @patch(\'torchbearer.trial.CallbackListInjection\')\n    def test_inject_printer_injection(self, c_inj, tq):\n        callback_list = torchbearer.callbacks.CallbackList([])\n\n        class SomeClass:\n            @torchbearer.inject_printer(\'v\')\n            def test_func(self_inner, verbose=0):\n                self.assertEqual(c_inj.call_count, 1)\n\n        t = SomeClass()\n        t.state = {torchbearer.CALLBACK_LIST: callback_list}\n        t.test_func()\n        self.assertTrue(t.state[torchbearer.CALLBACK_LIST] == callback_list)\n\n    def test_inject_sampler_standard(self):\n        generator = MagicMock()\n        steps = None\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.GENERATOR, load_batch_standard)\n            def test_func(self):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.GENERATOR: (generator, steps), torchbearer.LOADER: None}\n        t.test_func()\n        self.assertTrue(t.state[torchbearer.SAMPLER] == torchbearer.trial.load_batch_standard)\n\n    def test_inject_sampler_none(self):\n        generator = None\n        steps = None\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.GENERATOR, load_batch_standard)\n            def test_func(self):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.GENERATOR: (generator, steps), torchbearer.LOADER: None}\n        t.test_func()\n        self.assertTrue(t.state[torchbearer.SAMPLER] == torchbearer.trial.load_batch_none)\n\n    def test_inject_sampler_predict(self):\n        generator = MagicMock()\n        steps = None\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.GENERATOR, load_batch_predict)\n            def test_func(self):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.GENERATOR: (generator, steps), torchbearer.LOADER: None}\n        t.test_func()\n        self.assertTrue(t.state[torchbearer.SAMPLER] == torchbearer.trial.load_batch_predict)\n\n    def test_inject_sampler_custom(self):\n        generator = MagicMock()\n        steps = None\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.GENERATOR, load_batch_predict)\n            def test_func(self):\n                pass\n\n        def some_loader(state):\n            return \'test\'\n\n        t = SomeClass()\n        t.state = {torchbearer.GENERATOR: (generator, steps), torchbearer.LOADER: some_loader}\n        t.test_func()\n        self.assertTrue(t.state[torchbearer.SAMPLER] == some_loader)\n\n    @patch(\'warnings.warn\')\n    @patch(\'torchbearer.trial.load_batch_infinite\')\n    def test_inject_sampler_infinite(self, mock_lbi, _):\n        generator = MagicMock()\n        steps = -1\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.GENERATOR, load_batch_predict)\n            def test_func(self):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.GENERATOR: (generator, steps), torchbearer.LOADER: None}\n        t.test_func()\n        self.assertTrue(mock_lbi.call_args[0][0] == load_batch_predict)\n\n    @patch(\'torchbearer.trial.load_batch_infinite\')\n    def test_inject_sampler_infinite_standard_loader(self, mock_lbi):\n        class EmptyObj: # Mocks don\'t play well with hasattr so need an empty object\n            def __len__(self):\n                return 100\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                return None\n\n        generator = EmptyObj()\n        steps = 10\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.TRAIN_DATA, load_batch_standard)\n            def test_func(self):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.TRAIN_DATA: (generator, steps), torchbearer.INF_TRAIN_LOADING: True, torchbearer.LOADER: None}\n        t.test_func()\n        self.assertTrue(mock_lbi.call_args[0][0] == load_batch_standard)\n        self.assertTrue(generator.tb_iter)\n\n    @patch(\'torchbearer.trial.load_batch_infinite\')\n    def test_inject_sampler_infinite_train_loading(self, mock_lbi):\n        generator = MagicMock()\n        generator.__len__.return_value = 10\n        steps = 5\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.TRAIN_DATA, load_batch_standard)\n            def test_func(self):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.TRAIN_DATA: (generator, steps), torchbearer.INF_TRAIN_LOADING: True, torchbearer.LOADER: None}\n        t.test_func()\n        self.assertTrue(mock_lbi.call_args[0][0] == load_batch_standard)\n\n    def test_inject_sampler_data_key(self):\n        generator = MagicMock()\n        test_generator = \'test\'\n        test_steps = 1\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.GENERATOR, load_batch_predict)\n            def test_func(self, data_key=None):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.GENERATOR: (generator, None), torchbearer.TEST_GENERATOR: (test_generator, test_steps), torchbearer.LOADER: None}\n        t.test_func(data_key=torchbearer.TEST_GENERATOR)\n        self.assertTrue(t.state[torchbearer.GENERATOR] == test_generator)\n        self.assertTrue(t.state[torchbearer.STEPS] == test_steps)\n\n    def test_inject_sampler_data_key_no_kwargs(self):\n        generator = MagicMock()\n        test_generator = \'test\'\n        test_steps = 1\n\n        class SomeClass:\n            @torchbearer.inject_sampler(torchbearer.GENERATOR, load_batch_predict)\n            def test_func(self, data_key=None):\n                pass\n\n        t = SomeClass()\n        t.state = {torchbearer.GENERATOR: (generator, None), torchbearer.TEST_GENERATOR: (test_generator, test_steps), torchbearer.LOADER: None}\n        t.test_func(torchbearer.TEST_GENERATOR)\n        self.assertTrue(t.state[torchbearer.GENERATOR] == test_generator)\n        self.assertTrue(t.state[torchbearer.STEPS] == test_steps)\n\n    @patch(\'torchbearer.trial.CallbackListInjection\')\n    def test_inject_callback(self, c_inj):\n        callback_list = torchbearer.callbacks.CallbackList([])\n        test_callback = MagicMock()\n\n        class SomeClass:\n            @torchbearer.inject_callback(test_callback)\n            def test_func(self_inner):\n                self.assertEqual(c_inj.call_count, 1)\n\n        t = SomeClass()\n        t.state = {torchbearer.CALLBACK_LIST: callback_list}\n        t.test_func()\n        self.assertTrue(c_inj.call_args[0][0] == test_callback)\n\n    def test_deep_to_tensor(self):\n        base_tensor = torch.Tensor([1])\n        tensor = MagicMock(spec=base_tensor)\n        new_dtype = torch.float16\n        new_device = \'cuda:1\'\n\n        deep_to(tensor, new_device, new_dtype)\n        self.assertTrue(tensor.to.call_args[0][0] == new_device)\n        self.assertTrue(tensor.to.call_args[0][1] == new_dtype)\n\n    def test_deep_to_tensor_int_dtype(self):\n        base_tensor = torch.Tensor([1])\n        tensor = MagicMock(spec=base_tensor)\n        tensor.dtype = torch.uint8\n        new_device = \'cuda:1\'\n        new_dtype = torch.uint8\n\n        deep_to(tensor, new_device, new_dtype)\n        self.assertTrue(tensor.to.call_args[0][0] == new_device)\n        self.assertTrue(len(tensor.to.call_args[0]) == 1)\n\n    def test_deep_to_list(self):\n        base_tensor = torch.Tensor([1])\n        tensor_1 = MagicMock(spec=base_tensor)\n        tensor_2 = MagicMock(spec=base_tensor)\n        tensors = [tensor_1, tensor_2]\n        new_dtype = torch.float16\n        new_device = \'cuda:1\'\n\n        deep_to(tensors, new_device, new_dtype)\n        for tensor in tensors:\n            self.assertTrue(tensor.to.call_args[0][0] == new_device)\n            self.assertTrue(tensor.to.call_args[0][1] == new_dtype)\n\n    def test_deep_to_dict(self):\n        tensor_1 = torch.Tensor([0])\n        tensor_1.to = Mock()\n        tensor_2 = torch.Tensor([0])\n        tensor_2.to = Mock()\n        tensors = {\'t1\': tensor_1, \'t2\': tensor_2}\n        new_dtype = torch.float16\n        new_device = \'cuda:1\'\n\n        deep_to(tensors, new_device, new_dtype)\n        self.assertTrue(tensor_1.to.call_args[0][0] == new_device)\n        self.assertTrue(tensor_1.to.call_args[0][1] == new_dtype)\n        self.assertTrue(tensor_2.to.call_args[0][0] == new_device)\n        self.assertTrue(tensor_2.to.call_args[0][1] == new_dtype)\n\n    def test_deep_to_unknown_object(self):\n        tensor_1 = MagicMock()\n        tensor_2 = MagicMock()\n        tensors = {\'t1\': tensor_1, \'t2\': tensor_2}\n        new_dtype = torch.float16\n        new_device = \'cuda:1\'\n\n        deep_to(tensors, new_device, new_dtype)\n        self.assertTrue(tensor_1.to.call_args is None)\n        self.assertTrue(tensor_2.to.call_args is None)\n\n    def test_load_batch_standard(self):\n        items = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2]))]\n        iterator = iter(items)\n\n        state = {torchbearer.ITERATOR: iterator, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.int}\n\n        load_batch_standard(state)\n        self.assertTrue(state[torchbearer.X].item() == items[0][0].item())\n        self.assertTrue(state[torchbearer.Y_TRUE].item() == items[0][1].item())\n\n    def test_load_batch_inf_standard_normal(self):\n        items = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        iterator = iter(items)\n        state = {torchbearer.ITERATOR: iterator, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.int}\n\n        loader = load_batch_infinite(load_batch_standard)\n\n        for i in range(2):\n            loader(state)\n        self.assertTrue(state[torchbearer.X].item() == items[1][0].item())\n        self.assertTrue(state[torchbearer.Y_TRUE].item() == items[1][1].item())\n\n    def test_load_batch_inf_standard_too_many(self):\n        items = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2])), (torch.Tensor([3]), torch.Tensor([3]))]\n        iterator = iter(items)\n\n        state = {torchbearer.GENERATOR: items, torchbearer.ITERATOR: iterator, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.int}\n\n        loader = load_batch_infinite(load_batch_standard)\n\n        for i in range(12):\n            loader(state)\n\n        self.assertTrue(state[torchbearer.X].item() == items[2][0].item())\n        self.assertTrue(state[torchbearer.Y_TRUE].item() == items[2][1].item())\n\n    def test_load_batch_none(self):\n        items = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2]))]\n        iterator = iter(items)\n\n        state = {torchbearer.ITERATOR: iterator, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.int}\n\n        load_batch_none(state)\n        self.assertTrue(state[torchbearer.X] is None)\n        self.assertTrue(state[torchbearer.Y_TRUE] is None)\n\n    def test_load_batch_predict_data(self):\n        items = [torch.Tensor([1]), torch.Tensor([2])]\n        iterator = iter(items)\n\n        state = {torchbearer.ITERATOR: iterator, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.int}\n        load_batch_predict(state)\n        self.assertTrue(state[torchbearer.X].item() == items[0].item())\n\n    def test_load_batch_predict_list(self):\n        items = [(torch.Tensor([1]), torch.Tensor([1])), (torch.Tensor([2]), torch.Tensor([2]))]\n        iterator = iter(items)\n\n        state = {torchbearer.ITERATOR: iterator, torchbearer.DEVICE: \'cpu\', torchbearer.DATA_TYPE: torch.int}\n\n        load_batch_predict(state)\n        self.assertTrue(state[torchbearer.X].item() == items[0][0].item())\n        self.assertTrue(state[torchbearer.Y_TRUE].item() == items[0][1].item())\n\n    def test_update_device_and_dtype_only_kwarg(self):\n        main_state = {}\n        dtype = torch.float16\n        dev = \'cuda:1\'\n\n        kwargs = {str(torchbearer.DEVICE): dev, str(torchbearer.DATA_TYPE): dtype}\n\n        main_state = update_device_and_dtype(main_state, **kwargs)\n\n        self.assertTrue(main_state[torchbearer.DATA_TYPE] == dtype)\n        self.assertTrue(main_state[torchbearer.DEVICE] == dev)\n\n    def test_update_device_and_dtype_only_arg(self):\n        main_state = {}\n        dtype = torch.float16\n        dev = \'cuda:1\'\n        args = (dtype, dev)\n\n        main_state = update_device_and_dtype(main_state, *args)\n\n        self.assertTrue(main_state[torchbearer.DATA_TYPE] == dtype)\n        self.assertTrue(main_state[torchbearer.DEVICE] == dev)\n\n    def test_new_iter_none(self):\n        generator = None\n        t = Trial(None)\n        out = t._new_iter(generator)\n        self.assertTrue(out is None)\n\n    def test_new_iter_standard(self):\n        class EmptyObj(object):\n            def __init__(self):\n                super(self.__class__, self).__init__()\n                self.count = 0\n\n            def __iter__(self):\n                self.count += 1\n                return iter([1,2,3])\n\n        generator = EmptyObj()\n        t = Trial(None)\n        _ = t._new_iter(generator)\n        self.assertTrue(generator.count == 1)\n        self.assertTrue(not hasattr(generator, \'inf\'))\n\n    def test_new_iter_inf(self):\n        class EmptyObj(object):\n            def __init__(self):\n                super(self.__class__, self).__init__()\n                self.count = 0\n                self.tb_iter = Mock()\n                self.inf = True\n\n            def __iter__(self):\n                self.count += 1\n                return iter([1,2,3])\n\n        generator = EmptyObj()\n        t = Trial(None)\n        out = t._new_iter(generator)\n        self.assertTrue(out == generator.tb_iter)\n        self.assertTrue(generator.count == 0)\n'"
torchbearer/__init__.py,0,"b'from .version import __version__\nfrom . import magics\nfrom .bases import no_grad, enable_grad, cite, base_closure, Callback, Metric, standard_closure\nfrom .state import *\n\nfrom . import metrics\nfrom . import callbacks\nfrom .trial import *\nfrom . import cv_utils\n'"
torchbearer/bases.py,6,"b'from distutils.version import LooseVersion\nimport functools\nimport traceback\nimport warnings\n\nimport torch\nimport torchbearer\n\nimport sys\nif sys.version_info[0] < 3:\n    def set_doc(inner, doc):\n        return None  # Not simple to do in Python 2.7 so we can leave it for now, just build docs with Python 3+\nelse:\n    def set_doc(inner, doc):\n        inner.__doc__ = doc\n\n\nclass no_grad(torch.no_grad):\n    """""" Context-manager and decorator that disables gradient calculation.\n    See `torch.autograd.no_grad <https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad>`_\n    """"""\n    def __init__(self):\n        super(no_grad, self).__init__()\n        version = torch.__version__ if str(torch.__version__) is torch.__version__ else ""0.4.1""\n        if LooseVersion(version) < LooseVersion(""0.4.1""):  # No grad is not a decorator\n            _patch_call(self, self.call)\n\n    def call(self, func):\n        @functools.wraps(func)\n        def decorate_no_grad(*args, **kwargs):\n            with self:\n                return func(*args, **kwargs)\n\n        return decorate_no_grad\n\n\ndef _patch_call(instance, func):\n    class _(type(instance)):\n        def __call__(self, *arg, **kwarg):\n            return func(*arg, **kwarg)\n    instance.__class__ = _\n\n\nclass enable_grad(torch.enable_grad):\n    """""" Context-manager and decorator that enables gradient calculation.\n    See `torch.autograd.enable_grad <https://pytorch.org/docs/stable/autograd.html#torch.autograd.enable_grad>`_\n    """"""\n    def __init__(self):\n        super(enable_grad, self).__init__()\n        version = torch.__version__ if str(torch.__version__) is torch.__version__ else ""0.4.1""\n        if LooseVersion(version) < LooseVersion(""0.4.1""):  # Enable grad is not a decorator\n            _patch_call(self, self.call)\n\n    def call(self, func):\n        @functools.wraps(func)\n        def decorate_enable_grad(*args, **kwargs):\n            with self:\n                return func(*args, **kwargs)\n\n        return decorate_enable_grad\n\n\nclass Metric(object):\n    """"""Base metric class. Process will be called on each batch, process-final at the end of each epoch.\n    The metric contract allows for metrics to take any args but not kwargs. The initial metric call will be given state,\n    however, subsequent metrics can pass any values desired.\n\n    .. note::\n\n        All metrics must extend this class.\n\n    Args:\n        name (str): The name of the metric\n    """"""\n\n    def __init__(self, name):\n        self.name = name\n\n    def __str__(self):\n        return self.name\n\n    def process(self, *args):\n        """"""Process the state and update the metric for one iteration.\n\n        Args:\n            args: Arguments given to the metric. If this is a root level metric, will be given state\n\n        Returns:\n            None, or the value of the metric for this batch\n        """"""\n        pass\n\n    def process_final(self, *args):\n        """"""Process the terminal state and output the final value of the metric.\n\n        Args:\n            args: Arguments given to the metric. If this is a root level metric, will be given state\n\n        Returns:\n            None or the value of the metric for this epoch\n        """"""\n        pass\n\n    def eval(self, data_key=None):\n        """"""Put the metric in eval mode during model validation.\n        """"""\n        pass\n\n    def train(self):\n        """"""Put the metric in train mode during model training.\n        """"""\n        pass\n\n    def reset(self, state):\n        """"""Reset the metric, called before the start of an epoch.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n\nclass Callback(object):\n    """"""Base callback class.\n\n    .. note::\n\n        All callbacks should override this class.\n\n    """"""\n\n    def state_dict(self):\n        """"""Get a dict containing the callback state.\n\n        Returns:\n            dict: A dict containing parameters and persistent buffers.\n        """"""\n        return {}\n\n    def __str__(self):\n        return str(self.__class__).replace(\'<class \', \'\').replace(\'>\', \'\').replace(""\'"", """")\n\n    def load_state_dict(self, state_dict):\n        """"""Resume this callback from the given state. Expects that this callback was constructed in the same way.\n\n        Args:\n            state_dict (dict): The state dict to reload\n\n        Returns:\n            Callback: self\n        """"""\n        return self\n\n    def on_init(self, state):\n        """"""Perform some action with the given state as context at the init of a trial instance\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_start(self, state):\n        """"""Perform some action with the given state as context at the start of a model fit.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_start_epoch(self, state):\n        """"""Perform some action with the given state as context at the start of each epoch.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_start_training(self, state):\n        """"""Perform some action with the given state as context at the start of the training loop.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_sample(self, state):\n        """"""Perform some action with the given state as context after data has been sampled from the generator.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_forward(self, state):\n        """"""Perform some action with the given state as context after the forward pass (model output) has been completed.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_criterion(self, state):\n        """"""Perform some action with the given state as context after the criterion has been evaluated.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_backward(self, state):\n        """"""Perform some action with the given state as context after backward has been called on the loss.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_step_training(self, state):\n        """"""Perform some action with the given state as context after step has been called on the optimiser.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_end_training(self, state):\n        """"""Perform some action with the given state as context after the training loop has completed.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_start_validation(self, state):\n        """"""Perform some action with the given state as context at the start of the validation loop.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_sample_validation(self, state):\n        """"""Perform some action with the given state as context after data has been sampled from the validation generator.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_forward_validation(self, state):\n        """"""Perform some action with the given state as context after the forward pass (model output) has been completed\n        with the validation data.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_criterion_validation(self, state):\n        """"""Perform some action with the given state as context after the criterion evaluation has been completed\n        with the validation data.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_step_validation(self, state):\n        """"""Perform some action with the given state as context at the end of each validation step.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_end_validation(self, state):\n        """"""Perform some action with the given state as context at the end of the validation loop.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_end_epoch(self, state):\n        """"""Perform some action with the given state as context at the end of each epoch.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_checkpoint(self, state):\n        """"""Perform some action with the state after all other callbacks have completed at the end of an epoch and the\n        history has been updated. Should only be used for taking checkpoints or snapshots and will only be called by the\n        run method of Trial.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n    def on_end(self, state):\n        """"""Perform some action with the given state as context at the end of the model fitting.\n\n        Args:\n            state (dict): The current state dict of the :class:`.Trial`.\n        """"""\n        pass\n\n\ndef _get_param_list(param):\n    if isinstance(param, list):\n        return param\n    if isinstance(param, tuple):\n        return list(param)\n    return [param]\n\n\ndef _forward_with_exceptions(x, model, y_pred, state):\n    dx = state[x]\n\n    # Forward Pass\n    try:\n        exc_info = sys.exc_info()\n        state[y_pred] = state[model](*_get_param_list(dx), state=state)\n    except Exception as e:\n        error = []\n        try:\n            state[y_pred] = state[model](*_get_param_list(dx))\n        except TypeError as e2:\n            if isinstance(e, TypeError):  # If both are type errors, show both.\n                error.append(e2)\n            error.append(e)\n            raise Exception(error)\n        except Exception as e2:\n            if not isinstance(e, TypeError):\n                error.append(e)\n            error.append(e2)\n            raise Exception(error)\n    finally:\n        print_trace = False\n        for exc in exc_info:\n            if exc is not None:\n                print_trace = True\n\n        traceback.print_exception(*exc_info) if print_trace else None\n\n\ndef base_closure(x, model, y_pred, y_true, crit, loss, opt):\n    """"""Function to create a standard pytorch closure using objects taken from state under the given keys.\n\n    Args:\n        x: State key under which the input data is stored\n        model: State key under which the pytorch model is stored\n        y_pred: State key under which the predictions will be stored\n        y_true: State key under which the targets are stored\n        crit: State key under which the criterion function is stored (function of state or (y_pred, y_true))\n        loss: State key under which the loss will be stored\n        opt: State key under which the optimsiser is stored\n\n    Returns:\n        function: Standard closure function\n    """"""\n    def closure(state):\n        # Zero grads\n        state[opt].zero_grad()\n\n        _forward_with_exceptions(x, model, y_pred, state)\n\n        state[torchbearer.CALLBACK_LIST].on_forward(state)\n\n        # Loss Calculation\n        try:\n            state[loss] = state[crit](state)\n        except TypeError:\n            loss_function_params = _get_param_list(state[y_pred]) + _get_param_list(state[y_true])\n            state[loss] = state[crit](*loss_function_params)\n\n        state[torchbearer.CALLBACK_LIST].on_criterion(state)\n\n        # Backwards pass\n        state[loss].backward(**state[torchbearer.BACKWARD_ARGS])\n\n        state[torchbearer.CALLBACK_LIST].on_backward(state)\n    return closure\n\n\nstandard_closure = lambda: base_closure(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, torchbearer.Y_TRUE,\n                                torchbearer.CRITERION, torchbearer.LOSS, torchbearer.OPTIMIZER)\n\n\ndef apex_closure():\n    from apex import amp\n\n    def _apex_closure(state):\n        # Zero grads\n        state[torchbearer.OPTIMIZER].zero_grad()\n\n        _forward_with_exceptions(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, state)\n\n        state[torchbearer.CALLBACK_LIST].on_forward(state)\n\n        # Loss Calculation\n        try:\n            state[torchbearer.LOSS] = state[torchbearer.CRITERION](state)\n        except TypeError:\n            loss_function_params = _get_param_list(state[torchbearer.Y_PRED]) + _get_param_list(state[torchbearer.Y_TRUE])\n            state[torchbearer.LOSS] = state[torchbearer.CRITERION](*loss_function_params)\n\n        state[torchbearer.CALLBACK_LIST].on_criterion(state)\n\n        # Backwards pass\n        with amp.scale_loss(state[torchbearer.LOSS], state[torchbearer.OPTIMIZER]) as scaled_loss:\n            scaled_loss.backward(**state[torchbearer.BACKWARD_ARGS])\n\n        state[torchbearer.CALLBACK_LIST].on_backward(state)\n    return _apex_closure\n\n\ndef cite(bibtex):\n    """"""A decorator which adds a reference to the **Google style** docstring of the given object. The ``Args:`` or\n    ``Returns:`` line is then prepended with the given bibtex string at runtime. Otherwise, the last line is used.\n\n    Args:\n        bibtex (str): The bibtex string to insert\n\n    Returns:\n        The decorator\n    """"""\n    def decorator(inner):\n        doc = inner.__doc__.split(\'\\n\')\n        i = 0\n        s = 0\n        for line in doc:\n            sline = line.strip()\n            if sline == \'Args:\' or sline == \'Returns:\':\n                for char in line:\n                    if char == \' \':\n                        s += 1\n                break\n            i += 1\n\n        spaces = \' \' * (s + 4)\n        to_insert = \' \' * s + \'::\\n\\n\' + spaces\n        to_insert += bibtex.strip().replace(\'\\n\', \'\\n\' + spaces).rstrip()\n\n        doc.insert(i, \'\')\n        doc.insert(i, to_insert)\n        set_doc(inner, \'\\n\'.join(doc))\n        return inner\n    return decorator\n\n\ndef get_metric(self_tag, state, metric_key):\n    if torchbearer.DATA in state and state[torchbearer.DATA] == \'test_data\' and \'val_\' in metric_key:\n        return None\n\n    if metric_key in state[torchbearer.METRICS]:\n        return state[torchbearer.METRICS][metric_key]\n    else:\n        warnings.warn(\'{}: Failed to retrieve key `{}` from metrics. \'.format(self_tag, metric_key))\n'"
torchbearer/cv_utils.py,12,"b'import math\n\nimport torch\nfrom torch.utils.data import TensorDataset, Dataset\nimport random\n\n\ndef train_valid_splitter(x, y, split, shuffle=True):\n    """""" Generate training and validation tensors from whole dataset data and label tensors\n\n    Args:\n        x (torch.Tensor): Data tensor for whole dataset\n        y (torch.Tensor): Label tensor for whole dataset\n        split (float): Fraction of dataset to be used for validation\n        shuffle (bool): If True randomize tensor order before splitting else do not randomize\n\n    Returns:\n        Training and validation tensors (training data, training labels, validation data, validation labels)\n    """"""\n    num_samples_x = x.size()[0]\n    num_valid_samples = int(math.floor(num_samples_x * split))\n\n    if shuffle:\n        indicies = torch.randperm(num_samples_x)\n        x, y = x[indicies], y[indicies]\n\n    x_val, y_val = x[:num_valid_samples], y[:num_valid_samples]\n    x, y = x[num_valid_samples:], y[num_valid_samples:]\n\n    return x, y, x_val, y_val\n\n\ndef get_train_valid_sets(x, y, validation_data, validation_split, shuffle=True):\n    """""" Generate validation and training datasets from whole dataset tensors\n\n    Args:\n        x (torch.Tensor): Data tensor for dataset\n        y (torch.Tensor): Label tensor for dataset\n        validation_data ((torch.Tensor, torch.Tensor)): Optional validation data (x_val, y_val) to be\n            used instead of splitting x and y tensors\n        validation_split (float): Fraction of dataset to be used for validation\n        shuffle (bool): If True randomize tensor order before splitting else do not randomize\n\n    Returns:\n        Training and validation datasets\n    """"""\n\n    valset = None\n\n    if validation_data is not None:\n        x_val, y_val = validation_data\n    elif isinstance(validation_split, float):\n        x, y, x_val, y_val = train_valid_splitter(x, y, validation_split, shuffle=shuffle)\n    else:\n        x_val, y_val = None, None\n\n    trainset = TensorDataset(x, y)\n    if x_val is not None and y_val is not None:\n        valset = TensorDataset(x_val, y_val)\n\n    return trainset, valset\n\n\nclass DatasetValidationSplitter:\n    """""" Generates training and validation split indicies for a given dataset length and creates training and\n    validation datasets using these splits\n\n    Args:\n        dataset_len: The length of the dataset to be split into training and validation\n        split_fraction: The fraction of the whole dataset to be used for validation\n        shuffle_seed: Optional random seed for the shuffling process\n    """"""\n    def __init__(self, dataset_len, split_fraction, shuffle_seed=None):\n        self.dataset_len = dataset_len\n        self.split_fraction = split_fraction\n        self.valid_ids = None\n        self.train_ids = None\n        self._gen_split_ids(shuffle_seed)\n\n    def _gen_split_ids(self, seed):\n        all_ids = list(range(self.dataset_len))\n\n        if seed is not None:\n            random.seed(seed)\n        random.shuffle(all_ids)\n\n        num_valid_ids = int(math.floor(self.dataset_len*self.split_fraction))\n        self.valid_ids = all_ids[:num_valid_ids]\n        self.train_ids = all_ids[num_valid_ids:]\n\n    def get_train_dataset(self, dataset):\n        """""" Creates a training dataset from existing dataset\n\n        Args:\n            dataset (torch.utils.data.Dataset): Dataset to be split into a training dataset\n\n        Returns:\n            torch.utils.data.Dataset: Training dataset split from whole dataset\n        """"""\n        return SubsetDataset(dataset, self.train_ids)\n\n    def get_val_dataset(self, dataset):\n        """""" Creates a validation dataset from existing dataset\n\n        Args:\n        dataset (torch.utils.data.Dataset): Dataset to be split into a validation dataset\n\n        Returns:\n            torch.utils.data.Dataset: Validation dataset split from whole dataset\n        """"""\n        return SubsetDataset(dataset, self.valid_ids)\n\n\nclass SubsetDataset(Dataset):\n    """""" Dataset that consists of a subset of a previous dataset\n\n    Args:\n        dataset (torch.utils.data.Dataset): Complete dataset\n        ids (list): List of subset IDs\n    """"""\n    def __init__(self, dataset, ids):\n        super(SubsetDataset, self).__init__()\n        self.dataset = dataset\n        self.ids = ids\n\n    def __getitem__(self, index):\n        return self.dataset.__getitem__(self.ids[index])\n\n    def __len__(self):\n        return len(self.ids)\n'"
torchbearer/magics.py,0,"b'# Sets global variable notebook when line magic is called with ""%torchbearer notebook""global notebook\nnotebook = {\'nb\': False}\n\n\ndef set_notebook(is_notebook):\n    notebook[\'nb\'] = is_notebook\n\n\ndef torchbearer(line):\n    if line == \'notebook\':\n        set_notebook(True)\n    elif line == \'normal\':\n        set_notebook(False)\n\n\ntry:\n    import IPython.core.magic\n    torchbearer = IPython.core.magic.register_line_magic(torchbearer)\n    set_notebook(True)\nexcept (NameError, ImportError) as e:\n    pass\n\n\ndef is_notebook():\n    return notebook[\'nb\']\n'"
torchbearer/state.py,0,"b'from torchbearer import Metric\nimport warnings\n\n__keys__ = []\n\n\ndef state_key(key):\n    """"""Computes and returns a non-conflicting key for the state dictionary when given a seed key\n\n    Args:\n        key (str): The seed key - basis for new state key\n\n    Returns:\n        StateKey: New state key\n    """"""\n    return StateKey(key)\n\n\nclass StateKey(Metric):\n    """""" StateKey class that is a unique state key based on the input string key. State keys are also metrics which\n    retrieve themselves from state.\n\n    Args:\n        key (str): Base key\n    """"""\n    def __init__(self, key):\n        self.key = self._gen_key_(key)\n        super(StateKey, self).__init__(self.key)\n\n    def process(self, state):\n        return {self.name: state[self]}\n\n    def process_final(self, state):\n        return {self.name: state[self]}\n\n    def __call__(self, state):\n        return state[self]\n\n    def _gen_key_(self, key):\n        if key in __keys__:\n            count = 1\n            my_key = key + \'_\' + str(count)\n\n            while my_key in __keys__:\n                count += 1\n                my_key = key + \'_\' + str(count)\n\n            key = my_key\n\n        __keys__.append(key)\n        return key\n\n    def __repr__(self):\n        return self.key\n\n    def __str__(self):\n        return self.key\n\n    def __eq__(self, other):\n        return self.key == str(other)\n\n    def __hash__(self):\n        return self.key.__hash__()\n\n\nclass State(dict):\n    """"""\n    State dictionary that behaves like a python dict but accepts StateKeys\n    """"""\n    def __init__(self):\n        super(State, self).__init__()\n\n    def get_key(self, statekey):\n        if isinstance(statekey, str):\n            warnings.warn(""State was accessed with a string: {}, generate keys with StateKey(str)."".format(statekey), stacklevel=3)\n        return statekey\n\n    @property\n    def data(self):\n        new_state = State()\n        for key in self.keys():\n            try:\n                new_state[key] = self[key].data\n            except AttributeError:\n                new_state[key] = self[key]\n        return new_state\n\n    def __getitem__(self, key):\n        return super(State, self).__getitem__(self.get_key(key))\n\n    def __setitem__(self, key, val):\n        super(State, self).__setitem__(self.get_key(key), val)\n\n    def __delitem__(self, val):\n        super(State, self).__delitem__(val)\n\n    def __contains__(self, o):\n        return super(State, self).__contains__(self.get_key(o))\n\n    def update(self, d):\n        new_dict = {}\n        for key in d:\n            new_dict[self.get_key(key)] = d[key]\n        super(State, self).update(new_dict)\n\n\n#: The torchbearer version\nVERSION = state_key(\'torchbearer_version\')\n\n#: The PyTorch module / model that will be trained\nMODEL = state_key(\'model\')\n\n#: The criterion to use when model fitting\nCRITERION = state_key(\'criterion\')\n\n#: The optimizer to use when model fitting\nOPTIMIZER = state_key(\'optimizer\')\n\n#: The device currently in use by the :class:`.Trial` and PyTorch model\nDEVICE = state_key(\'device\')\n\n#: The data type of tensors in use by the model, match this to avoid type issues\nDATA_TYPE = state_key(\'dtype\')\n\n#: The list of metrics in use by the :class:`.Trial`\nMETRIC_LIST = state_key(\'metric_list\')\n\n#: The metric dict from the current batch of data\nMETRICS = state_key(\'metrics\')\n\n#: A self refrence to the Trial object for persistence etc.\nSELF = state_key(\'self\')\n\n#: The current epoch number\nEPOCH = state_key(\'epoch\')\n\n#: The total number of epochs to run for\nMAX_EPOCHS = state_key(\'max_epochs\')\n\n#: The string name of the current data\nDATA = state_key(\'data\')\n\n#: The current data generator (DataLoader)\nGENERATOR = state_key(\'generator\')\n\n#: The current iterator\nITERATOR = state_key(\'iterator\')\n\n#: The current number of steps per epoch\nSTEPS = state_key(\'steps\')\n\n#: The train data generator in the Trial object\nTRAIN_GENERATOR = state_key(\'train_generator\')\n\n#: The number of train steps to take\nTRAIN_STEPS = state_key(\'train_steps\')\n\n#: The flag representing train data\nTRAIN_DATA = state_key(\'train_data\')\n\n#: Flag for refreshing of training iterator when finished instead of each epoch\nINF_TRAIN_LOADING = state_key(\'inf_train_loading\')\n\n#: The validation data generator in the Trial object\nVALIDATION_GENERATOR = state_key(\'validation_generator\')\n\n#: The number of validation steps to take\nVALIDATION_STEPS = state_key(\'validation_steps\')\n\n#: The flag representing validation data\nVALIDATION_DATA = state_key(\'validation_data\')\n\n#: The test data generator in the Trial object\nTEST_GENERATOR = state_key(\'test_generator\')\n\n#: The number of test steps to take\nTEST_STEPS = state_key(\'test_steps\')\n\n#: The flag representing test data\nTEST_DATA = state_key(\'test_data\')\n\n#: A flag that can be set to true to stop the current fit call\nSTOP_TRAINING = state_key(\'stop_training\')\n\n#: The current batch of ground truth data\nTARGET = Y_TRUE = state_key(\'y_true\')\n\n#: The current batch of predictions\nPREDICTION = Y_PRED = state_key(\'y_pred\')\n\n#: The current batch of inputs\nINPUT = X = state_key(\'x\')\n\n#: The sampler which loads data from the generator onto the correct device\nSAMPLER = state_key(\'sampler\')\n\n#: The batch loader which handles formatting data from each batch\nLOADER = state_key(\'loader\')\n\n#: The current value for the loss\nLOSS = state_key(\'loss\')\n\n#: The key which maps to the predictions over the dataset when calling predict\nFINAL_PREDICTIONS = state_key(\'final_predictions\')\n\n#: The current batch number\nBATCH = state_key(\'t\')\n\n#: The timings keys used by the timer callback\nTIMINGS = state_key(\'timings\')\n\n#: The :class:`.CallbackList` object which is called by the Trial\nCALLBACK_LIST = state_key(\'callback_list\')\n\n#: The history list of the Trial instance\nHISTORY = state_key(\'history\')\n\n#: The optional arguments which should be passed to the backward call\nBACKWARD_ARGS = state_key(\'backward_args\')\n\n#: The lambda coefficient of the linear combination of inputs\nMIXUP_LAMBDA = state_key(\'mixup_lambda\')\n\n#: The permutation of input indices for input mixup\nMIXUP_PERMUTATION = state_key(\'mixup_permutation\')\n'"
torchbearer/trial.py,69,"b'import sys\n\nif sys.version_info[0] < 3:\n    import inspect\n    def get_default(fcn, arg):\n        a = inspect.getargspec(fcn)\n        return dict(zip(a.args[-len(a.defaults):], a.defaults))[arg]\nelse:\n    from inspect import signature\n    def get_default(fcn, arg):\n        return signature(fcn).parameters[arg].default\n\nimport functools\nimport warnings\nimport itertools\n\nimport torch\nimport torch.nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import Optimizer\n\nimport torchbearer\nfrom torchbearer import cite\nfrom torchbearer import State\nfrom torchbearer.metrics import MetricList\nfrom torchbearer.callbacks import Callback, CallbackList, Tqdm, AggregatePredictions\nfrom torchbearer.bases import standard_closure, _forward_with_exceptions, _get_param_list\n\nbibtex = """"""\n@article{2018torchbearer,\n  title={Torchbearer: A Model Fitting Library for PyTorch},\n  author={Harris, Ethan and Painter, Matthew and Hare, Jonathon},\n  journal={arXiv preprint arXiv:1809.03363},\n  year={2018}\n}\n""""""\n\n\nclass MockOptimizer(Optimizer):\n    """"""The Mock Optimizer will be used inplace of an optimizer in the event that none is passed to the Trial class.\n    """"""\n\n    def __init__(self):\n        super(MockOptimizer, self).__init__([torch.zeros(1)], [])\n\n    def add_param_group(self, param_group):\n        pass  # Do Nothing\n\n    def load_state_dict(self, state_dict):\n        pass  # Do Nothing\n\n    def state_dict(self):\n        return {}  # Return Empty\n\n    def step(self, closure=None):\n        if closure is not None:\n            closure()\n\n    def zero_grad(self):\n        pass  # Do Nothing\n\n\nclass MockModel(torch.nn.Module):\n    def forward(self, x, state=None):\n        return None\n\n\nclass CallbackListInjection(CallbackList):\n    """"""This class allows for an callback to be injected into a callback list, without masking the methods available for\n    mutating the list. In this way, callbacks (such as printers) can be injected seamlessly into the methods of the\n    trial class.\n\n    Args:\n        callback (Callback): The :class:`.Callback` to inject\n        callback_list (CallbackList): The underlying :class:`.CallbackList`\n    """"""\n\n    def __init__(self, callback, callback_list):\n        super(CallbackListInjection, self).__init__([])\n\n        self.callback = callback\n        self.callback_list = callback_list\n\n    def state_dict(self):\n        return self.callback_list.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.callback_list.load_state_dict(state_dict)\n        return self\n\n    def __iter__(self):\n        return self.callback_list.__iter__()\n\n    def __copy__(self):\n        return self.callback_list.copy()\n\n    def copy(self):\n        return self.__copy__()\n\n    def append(self, callback_list):\n        self.callback_list.append(callback_list)\n\n    def _for_list(self, function):\n        function(self.callback)  # Call injected callback BEFORE the callback list\n        function(self.callback_list)\n\n\ndef inject_printer(validation_label_letter=\'v\'):\n    """"""The inject printer decorator is used to inject the appropriate printer callback, according to the verbosity level.\n\n    Args:\n        validation_label_letter (str): The validation label letter to use\n\n    Returns:\n        A decorator\n    """"""\n    from inspect import getcallargs\n\n    def decorator(func):\n        root = func if not hasattr(func, \'root\') else func.root\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            call_args = getcallargs(root, self, *args, **kwargs)\n            verbose = call_args[\'verbose\'] if \'verbose\' in call_args else get_default(func, \'verbose\')  # Populate default value\n            verbose = self.verbose if verbose == -1 else verbose\n\n            printer = get_printer(verbose=verbose, validation_label_letter=validation_label_letter)\n\n            callback_list_old = self.state[torchbearer.CALLBACK_LIST]\n\n            self.state[torchbearer.CALLBACK_LIST] = CallbackListInjection(printer, callback_list_old)\n\n            res = func(self, *args, **kwargs)\n\n            self.state[torchbearer.CALLBACK_LIST] = callback_list_old\n            return res\n        wrapper.root = root\n        return wrapper\n    return decorator\n\n\ndef get_printer(verbose, validation_label_letter):\n    if verbose >= 2:\n        printer = Tqdm(validation_label_letter=validation_label_letter)\n    elif verbose >= 1:\n        printer = Tqdm(validation_label_letter=validation_label_letter, on_epoch=True)\n    else:\n        printer = Callback()\n\n    return printer\n\n\ndef deep_to(batch, device, dtype):\n    """""" Static method to call :func:`to` on tensors, tuples or dicts. All items will have :func:`deep_to` called\n\n    Example: ::\n\n        >>> import torch\n        >>> from torchbearer import deep_to\n        >>> example_dict = {\'a\': torch.ones(5)*2.1, \'b\': torch.ones(1)*5.9}\n        >>> deep_to(example_dict, device=\'cpu\', dtype=torch.int)\n        {\'a\': tensor([2, 2, 2, 2, 2], dtype=torch.int32), \'b\': tensor([5], dtype=torch.int32)}\n\n    Args:\n        batch (tuple / list / torch.Tensor / dict): The mini-batch which requires a :func:`to` call\n        device (torch.device): The desired device of the batch\n        dtype (torch.dtype): The desired datatype of the batch\n\n    Returns:\n        tuple / list / torch.Tensor: The moved or casted batch\n    """"""\n    is_tuple = isinstance(batch, tuple)\n\n    if isinstance(batch, list) or isinstance(batch, tuple):\n        batch = list(batch)\n        for i in range(len(batch)):\n            batch[i] = deep_to(batch[i], device, dtype)\n        batch = tuple(batch) if is_tuple else batch\n    elif isinstance(batch, dict):\n        for key in batch:\n            batch[key] = deep_to(batch[key], device, dtype)\n    elif torch.is_tensor(batch):\n        if batch.dtype.is_floating_point:\n            batch = batch.to(device, dtype)\n        else:\n            batch = batch.to(device)\n\n    return batch\n\n\ndef load_batch_infinite(loader):\n    """""" Wraps a batch loader and refreshes the iterator once it has been completed.\n\n    Args:\n        loader: batch loader to wrap\n    """"""\n\n    def call(state):\n        try:\n            loader(state)\n        except StopIteration:\n            state[torchbearer.ITERATOR] = iter(state[torchbearer.GENERATOR])\n            loader(state)\n\n    return call\n\n\ndef load_batch_standard(state):\n    """""" Load a standard (input data, target) tuple mini-batch from iterator into state\n\n    Args:\n        state (dict): The current state dict of the :class:`Trial`.\n    """"""\n    state[torchbearer.X], state[torchbearer.Y_TRUE] = deep_to(next(state[torchbearer.ITERATOR]),\n                                                              state[torchbearer.DEVICE],\n                                                              state[torchbearer.DATA_TYPE])\n\n\ndef load_batch_none(state):\n    """""" Load a none (none, none) tuple mini-batch into state\n\n    Args:\n        state (dict): The current state dict of the :class:`Trial`.\n    """"""\n    state[torchbearer.X], state[torchbearer.Y_TRUE] = None, None\n\n\ndef load_batch_predict(state):\n    """""" Load a prediction (input data, target) or (input data) mini-batch from iterator into state\n\n    Args:\n        state (dict): The current state dict of the :class:`Trial`.\n    """"""\n    data = deep_to(next(state[torchbearer.ITERATOR]), state[torchbearer.DEVICE], state[torchbearer.DATA_TYPE])\n    if isinstance(data, list) or isinstance(data, tuple):\n        try:\n            state[torchbearer.X], state[torchbearer.Y_TRUE] = data\n        except ValueError:\n            state[torchbearer.X] = data[0]\n    else:\n        state[torchbearer.X] = data\n\n\ndef inject_sampler(data_key, batch_sampler):\n    """""" Decorator to inject a :class:`Sampler` into state[torchbearer.SAMPLER] along with the specified \\\n        generator into state[torchbearer.GENERATOR] and number of steps into state[torchbearer.STEPS]\n\n    Args:\n        data_key (:class:`.StateKey`): Key for the data to inject\n        batch_sampler (function): Batch sampler function that extracts batch from data loader, stores in state and sends\n        data to correct device\n\n    Returns:\n        The decorator\n    """"""\n    from inspect import getcallargs\n\n    def decorator(func):\n        root = func if not hasattr(func, \'root\') else func.root\n\n        def infinite_wrapper(self, key, generator, steps, sampler):\n            if generator is not None and steps is not None:\n                over_steps = steps > len(generator)\n                inf_steps = steps == -1\n                inf_train_loader = key == torchbearer.TRAIN_DATA and self.state[torchbearer.INF_TRAIN_LOADING]\n\n                if over_steps or inf_steps or inf_train_loader:  # Want iterator to refresh at end not per epoch\n                    if steps == -1: warnings.warn(""Trial is set to run indefinitely. ""\n                                              ""Make sure you have some method to terminate safely."")\n                    sampler = load_batch_infinite(sampler)\n\n                # Want iterator to run until end before refreshing regardless of number of train/val steps\n                if inf_train_loader and not hasattr(generator, \'tb_iter\'):\n                    generator.tb_iter = iter(generator)\n\n            return generator, sampler\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            sampler = batch_sampler\n\n            call_args = getcallargs(root, self, *args, **kwargs)\n            key = call_args[\'data_key\'] if (\'data_key\' in call_args and call_args[\'data_key\'] is not None) else data_key  # Populate default value\n            generator, steps = self.state[key] if self.state[key] is not None else (None, None)\n\n            if self.state[torchbearer.LOADER] is not None:\n                sampler = self.state[torchbearer.LOADER]\n            elif generator is None:\n                sampler = load_batch_none\n\n            generator, sampler = infinite_wrapper(self, key, generator, steps, sampler)\n\n            self.state[torchbearer.DATA] = key\n            self.state[torchbearer.SAMPLER] = sampler\n            self.state[torchbearer.GENERATOR] = generator\n            self.state[torchbearer.STEPS] = steps\n\n            res = func(self, *args, **kwargs)\n\n            return res\n        wrapper.root = root\n        return wrapper\n    return decorator\n\n\ndef inject_callback(callback):\n    """""" Decorator to inject a callback into the callback list and remove the callback after the decorated function has executed\n\n    Args:\n        callback (Callback): :class:`.Callback` to be injected\n\n    Returns:\n        The decorator\n    """"""\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            callback_list_old = self.state[torchbearer.CALLBACK_LIST]\n\n            self.state[torchbearer.CALLBACK_LIST] = CallbackListInjection(callback, callback_list_old)\n\n            res = func(self, *args, **kwargs)\n\n            self.state[torchbearer.CALLBACK_LIST] = callback_list_old\n            return res\n        return wrapper\n    return decorator\n\n\ndef update_device_and_dtype(state, *args, **kwargs):\n    """"""Function gets data type and device values from the args / kwargs and updates state.\n\n    Args:\n        state (State): The :class:`.State` to update\n        args: Arguments to the :func:`Trial.to` function\n        kwargs: Keyword arguments to the :func:`Trial.to` function\n\n    Returns:\n        state\n    """"""\n    for key, _ in kwargs.items():\n        if key == str(torchbearer.DATA_TYPE):\n            state[torchbearer.DATA_TYPE] = kwargs[\'dtype\']\n        elif str(torchbearer.DEVICE) in kwargs:\n            state[torchbearer.DEVICE] = kwargs[\'device\']\n\n    for arg in args:\n        if isinstance(arg, torch.dtype):\n            state[torchbearer.DATA_TYPE] = arg\n        else:\n            state[torchbearer.DEVICE] = arg\n\n    return state\n\n\n@cite(bibtex)\nclass Trial(object):\n    """"""\n    The trial class contains all of the required hyper-parameters for model running in torchbearer and presents an\n    API for model fitting, evaluating and predicting.\n\n    Example: ::\n\n        >>> import torch\n        >>> from torchbearer import Trial\n\n        # Example trial that attempts to minimise the output of a linear layer.\n        # Makes use of a callback to input the random data at each batch and a loss that is the absolute value of the\n        # linear layer output. Runs for 10 iterations and a single epoch.\n        >>> model = torch.nn.Linear(2,1)\n        >>> optimiser = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n        >>> @torchbearer.callbacks.on_sample\n        ... def initial_data(state):\n        ...     state[torchbearer.X] = torch.rand(1, 2)*10\n        >>> def minimise_output_loss(y_pred, y_true):\n        ...     return torch.abs(y_pred)\n        >>> trial = Trial(model, optimiser, minimise_output_loss, [\'loss\'], [initial_data]).for_steps(10).run(1)\n\n    Args:\n        model (torch.nn.Module): The base pytorch model\n        optimizer (torch.optim.Optimizer): The optimizer used for pytorch model weight updates\n        criterion (func / None): The final loss criterion that provides a loss value to the optimizer\n        metrics (list): The list of :class:`torchbearer.Metric <.Metric>` instances to process during fitting\n        callbacks (list): The list of :class:`torchbearer.Callback <.Callback>` instances to call during fitting\n        verbose (int): Global verbosity .If 2: use tqdm on batch, If 1: use tqdm on epoch, If 0: display no training\n            progress\n    """"""\n    def __init__(self, model, optimizer=None, criterion=None, metrics=[], callbacks=[], verbose=2):\n        if criterion is None:\n            def criterion(_, __):\n                return torch.zeros(1, device=self.state[torchbearer.DEVICE], dtype=self.state[torchbearer.DATA_TYPE], requires_grad=True)\n\n        self.verbose = verbose\n\n        self.closure = standard_closure()\n        self.state = State()\n        self.state.update({\n            torchbearer.MODEL: model if model is not None else MockModel(),\n            torchbearer.CRITERION: criterion,\n            torchbearer.OPTIMIZER: optimizer if optimizer is not None else MockOptimizer(),\n            torchbearer.METRIC_LIST: MetricList(metrics),\n            torchbearer.CALLBACK_LIST: CallbackList(callbacks),\n            torchbearer.DEVICE: \'cpu\',\n            torchbearer.DATA_TYPE: torch.float32,\n            torchbearer.SELF: self,\n            torchbearer.HISTORY: [],\n            torchbearer.BACKWARD_ARGS: {},\n            torchbearer.TRAIN_GENERATOR: None,\n            torchbearer.VALIDATION_GENERATOR: None,\n            torchbearer.TEST_GENERATOR: None,\n            torchbearer.TRAIN_STEPS: None,\n            torchbearer.VALIDATION_STEPS: None,\n            torchbearer.TEST_STEPS: None,\n            torchbearer.TRAIN_DATA: None,\n            torchbearer.VALIDATION_DATA: None,\n            torchbearer.TEST_DATA: None,\n            torchbearer.INF_TRAIN_LOADING: False,\n            torchbearer.LOADER: None\n        })\n\n        self.state[torchbearer.CALLBACK_LIST].on_init(self.state)\n\n    def __str__(self):\n        def state_string(name, state_key):\n            import math\n            N = (50-len(name))/2\n            res = ""-"" * int(math.floor(N)) + "" "" + name.upper() + "" "" + ""-"" * int(math.ceil(N))\n            res = res + ""-"" if len(res) < 52 else res\n            return res + ""\\n"" + str(self.state[state_key]) + ""\\n\\n""\n\n        optim_str = state_string(\'Optimzer\', torchbearer.OPTIMIZER)\n        crit_str = state_string(""Criterion"", torchbearer.CRITERION)\n        metrics_str = state_string(""Metrics"", torchbearer.METRIC_LIST)\n        callbacks_str = state_string(""Callbacks"", torchbearer.CALLBACK_LIST)\n        model_str = state_string(""Model"", torchbearer.MODEL)\n\n        return optim_str + crit_str + metrics_str + callbacks_str + model_str\n\n    def __repr__(self):\n        return str(self)\n\n    # Data addition\n\n    def for_train_steps(self, steps):\n        """"""Run this trial for the given number of training steps. Note that the generator will output (None, None) if it\n        has not been set. Useful for differentiable programming. Returns self so that methods can be chained for\n        convenience. If steps is larger than dataset size then loader will be refreshed like if it was a new epoch. If\n        steps is -1 then loader will be refreshed until stopped by STOP_TRAINING flag or similar.\n\n        Example: ::\n\n            # Simple trial that runs for 100 training iterations, in this case optimising nothing\n            >>> from torchbearer import Trial\n            >>> trial = Trial(None).for_train_steps(100)\n\n        Args:\n            steps (int): The number of training steps per epoch to run.\n\n        Returns:\n            Trial: self\n        """"""\n        if not isinstance(steps, int):\n            warnings.warn(""Number of training steps is not an int, casting to int"")\n            steps = int(steps)\n        self.state[torchbearer.TRAIN_STEPS] = steps\n        self.state[torchbearer.TRAIN_DATA] = (self.state[torchbearer.TRAIN_GENERATOR], self.state[torchbearer.TRAIN_STEPS])\n\n        return self\n\n    def with_train_generator(self, generator, steps=None):\n        """"""Use this trial with the given train generator. Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs for 100 training iterations on the MNIST dataset\n            >>> from torchbearer import Trial\n            >>> from torchvision.datasets import MNIST\n            >>> from torch.utils.data import DataLoader\n            >>> dataloader = DataLoader(MNIST(\'./data/\', train=True))\n            >>> trial = Trial(None).with_train_generator(dataloader).for_steps(100).run(1)\n\n        Args:\n            generator: The train data generator to use during calls to :meth:`.run`\n            steps (int): The number of steps per epoch to take when using this generator.\n\n        Returns:\n            Trial: self\n        """"""\n        self.state[torchbearer.TRAIN_GENERATOR] = generator\n        steps = self.state[torchbearer.TRAIN_STEPS] if steps is None else steps\n        steps = len(generator) if steps is None else steps\n        self.for_train_steps(steps)\n\n        return self\n\n    def with_train_data(self, x, y, batch_size=1, shuffle=True, num_workers=1, steps=None):\n        """"""Use this trial with the given train data. Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs for 10 training iterations on some random data\n            >>> from torchbearer import Trial\n            >>> data = torch.rand(10, 1)\n            >>> targets = torch.rand(10, 1)\n            >>> trial = Trial(None).with_val_data(data, targets).for_steps(10).run(1)\n\n        Args:\n            x (torch.Tensor): The train x data to use during calls to :meth:`.run`\n            y (torch.Tensor): The train labels to use during calls to :meth:`.run`\n            batch_size (int): The size of each batch to sample from the data\n            shuffle (bool): If True, then data will be shuffled each epoch\n            num_workers (int): Number of worker threads to use in the data loader\n            steps (int): The number of steps per epoch to take when using this data\n\n        Returns:\n            Trial: self\n        """"""\n        dataset = TensorDataset(x, y)\n        dataloader = DataLoader(dataset, batch_size, shuffle=shuffle, num_workers=num_workers)\n        self.with_train_generator(dataloader, steps=steps)\n\n        return self\n\n    def for_val_steps(self, steps):\n        """"""Run this trial for the given number of validation steps. Note that the generator will output (None, None) if\n        it has not been set. Useful for differentiable programming. Returns self so that methods can be chained for\n        convenience. If steps larger than dataset size then loader will be refreshed like if it was a new epoch. If\n        steps -1 then loader will be refreshed until stopped by STOP_TRAINING flag or similar.\n\n        Example: ::\n\n            # Simple trial that runs for 10 validation iterations on no data\n            >>> from torchbearer import Trial\n            >>> data = torch.rand(10, 1)\n            >>> trial = Trial(None).for_val_steps(10).run(1)\n\n        Args:\n            steps (int): The number of validation steps per epoch to run\n\n        Returns:\n            Trial: self\n        """"""\n        if not isinstance(steps, int):\n            warnings.warn(""Number of validation steps is not an int, casting to int"")\n            steps = int(steps)\n        self.state[torchbearer.VALIDATION_STEPS] = steps\n        self.state[torchbearer.VALIDATION_DATA] = (self.state[torchbearer.VALIDATION_GENERATOR], self.state[torchbearer.VALIDATION_STEPS])\n\n        return self\n\n    def with_val_generator(self, generator, steps=None):\n        """"""Use this trial with the given validation generator. Returns self so that methods can be chained for\n        convenience.\n\n        Example: ::\n\n            # Simple trial that runs for 100 validation iterations on the MNIST dataset\n            >>> from torchbearer import Trial\n            >>> from torchvision.datasets import MNIST\n            >>> from torch.utils.data import DataLoader\n            >>> dataloader = DataLoader(MNIST(\'./data/\', train=False))\n            >>> trial = Trial(None).with_val_generator(dataloader).for_steps(100).run(1)\n\n        Args:\n            generator: The validation data generator to use during calls to :meth:`.run` and :meth:`.evaluate`\n            steps (int): The number of steps per epoch to take when using this generator\n\n        Returns:\n            Trial: self\n        """"""\n        self.state[torchbearer.VALIDATION_GENERATOR] = generator\n        steps = self.state[torchbearer.VALIDATION_STEPS] if steps is None else steps\n        steps = len(generator) if steps is None else steps\n        self.for_val_steps(steps)\n\n        return self\n\n    def with_val_data(self, x, y, batch_size=1, shuffle=True, num_workers=1, steps=None):\n        """"""Use this trial with the given validation data. Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs for 10 validation iterations on some random data\n            >>> from torchbearer import Trial\n            >>> data = torch.rand(10, 1)\n            >>> targets = torch.rand(10, 1)\n            >>> trial = Trial(None).with_val_data(data, targets).for_steps(10).run(1)\n\n        Args:\n            x (torch.Tensor): The validation x data to use during calls to :meth:`.run` and :meth:`.evaluate`\n            y (torch.Tensor): The validation labels to use during calls to :meth:`.run` and :meth:`.evaluate`\n            batch_size (int): The size of each batch to sample from the data\n            shuffle (bool): If True, then data will be shuffled each epoch\n            num_workers (int): Number of worker threads to use in the data loader\n            steps (int): The number of steps per epoch to take when using this data\n\n        Returns:\n            Trial: self\n        """"""\n        dataset = TensorDataset(x, y)\n        dataloader = DataLoader(dataset, batch_size, shuffle=shuffle, num_workers=num_workers)\n        self.with_val_generator(dataloader, steps=steps)\n\n        return self\n\n    def for_test_steps(self, steps):\n        """"""Run this trial for the given number of test steps. Note that the generator will output (None, None) if\n        it has not been set. Useful for differentiable programming. Returns self so that methods can be chained for\n        convenience. If steps larger than dataset size then loader will be refreshed like if it was a new epoch. If\n        steps -1 then loader will be refreshed until stopped by STOP_TRAINING flag or similar.\n\n        Example: ::\n\n            # Simple trial that runs for 10 test iterations on some random data\n            >>> from torchbearer import Trial\n            >>> data = torch.rand(10, 1)\n            >>> trial = Trial(None).with_test_data(data).for_test_steps(10).run(1)\n\n        Args:\n            steps (int): The number of test steps per epoch to run (when using :meth:`.predict`)\n\n        Returns:\n            Trial: self\n        """"""\n        if not isinstance(steps, int):\n            warnings.warn(""Number of test steps is not an int, casting to int"")\n            steps = int(steps)\n        self.state[torchbearer.TEST_STEPS] = steps\n        self.state[torchbearer.TEST_DATA] = (self.state[torchbearer.TEST_GENERATOR], self.state[torchbearer.TEST_STEPS])\n\n        return self\n\n    def with_test_generator(self, generator, steps=None):\n        """"""Use this trial with the given test generator. Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs for 10 test iterations on no data\n            >>> from torchbearer import Trial\n            >>> data = torch.rand(10, 1)\n            >>> trial = Trial(None).with_test_data(data).for_test_steps(10).run(1)\n\n        Args:\n            generator: The test data generator to use during calls to :meth:`.predict`\n            steps (int): The number of steps per epoch to take when using this generator\n\n        Returns:\n            Trial: self\n        """"""\n        self.state[torchbearer.TEST_GENERATOR] = generator\n        steps = self.state[torchbearer.TEST_STEPS] if steps is None else steps\n        steps = len(generator) if steps is None else steps\n        self.for_test_steps(steps)\n\n        return self\n\n    def with_test_data(self, x, batch_size=1, num_workers=1, steps=None):\n        """"""Use this trial with the given test data. Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs for 10 test iterations on some random data\n            >>> from torchbearer import Trial\n            >>> data = torch.rand(10, 1)\n            >>> trial = Trial(None).with_test_data(data).for_test_steps(10).run(1)\n\n        Args:\n            x (torch.Tensor): The test x data to use during calls to :meth:`.predict`\n            batch_size (int): The size of each batch to sample from the data\n            num_workers (int): Number of worker threads to use in the data loader\n            steps (int): The number of steps per epoch to take when using this data\n\n        Returns:\n            Trial: self\n        """"""\n        dataset = TensorDataset(x)\n        dataloader = DataLoader(dataset, batch_size, num_workers=num_workers)\n        self.with_test_generator(dataloader, steps=steps)\n\n        return self\n\n    def for_steps(self, train_steps=None, val_steps=None, test_steps=None):\n        """"""Use this trial for the given number of train, val and test steps. Returns self so that methods can be chained\n        for convenience. If steps larger than dataset size then loader will be refreshed like if it was a new epoch. If\n        steps -1 then loader will be refreshed until stopped by STOP_TRAINING flag or similar.\n\n        Example: ::\n\n            # Simple trial that runs for 10 training, validation and test iterations on some random data\n            >>> from torchbearer import Trial\n            >>> train_data = torch.rand(10, 1)\n            >>> val_data = torch.rand(10, 1)\n            >>> test_data = torch.rand(10, 1)\n            >>> trial = Trial(None).with_train_data(train_data).with_val_data(val_data).with_test_data(test_data)\n            >>> trial.for_steps(10, 10, 10).run(1)\n\n        Args:\n            train_steps (int): The number of training steps per epoch to run\n            val_steps (int): The number of validation steps per epoch to run\n            test_steps (int): The number of test steps per epoch to run (when using :meth:`.predict`)\n\n        Returns:\n            Trial: self\n        """"""\n        if train_steps is not None:\n            self.for_train_steps(train_steps)\n        if val_steps is not None:\n            self.for_val_steps(val_steps)\n        if test_steps is not None:\n            self.for_test_steps(test_steps)\n\n        return self\n\n    def with_generators(self, train_generator=None, val_generator=None, test_generator=None, train_steps=None, val_steps=None, test_steps=None):\n        """"""Use this trial with the given generators. Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs for 100 steps from a training and validation data generator\n            >>> from torchbearer import Trial\n            >>> from torchvision.datasets import MNIST\n            >>> from torch.utils.data import DataLoader\n            >>> trainloader = DataLoader(MNIST(\'./data/\', train=True))\n            >>> valloader = DataLoader(MNIST(\'./data/\', train=False))\n            >>> trial = Trial(None).with_generators(trainloader, valloader, train_steps=100, val_steps=100).run(1)\n\n        Args:\n            train_generator: The training data generator to use during calls to :meth:`.run`\n            val_generator: The validation data generator to use during calls to :meth:`.run` and :meth:`.evaluate`\n            test_generator: The testing data generator to use during calls to :meth:`.predict`\n            train_steps (int): The number of steps per epoch to take when using the training generator\n            val_steps (int): The number of steps per epoch to take when using the validation generator\n            test_steps (int): The number of steps per epoch to take when using the testing generator\n\n        Returns:\n            Trial: self\n        """"""\n        if train_generator is not None:\n            self.with_train_generator(train_generator, train_steps)\n        if val_generator is not None:\n            self.with_val_generator(val_generator, val_steps)\n        if test_generator is not None:\n            self.with_test_generator(test_generator, test_steps)\n\n        return self\n\n    def with_data(self, x_train=None, y_train=None, x_val=None, y_val=None, x_test=None, batch_size=1,\n                  num_workers=1, train_steps=None, val_steps=None, test_steps=None, shuffle=True):\n        """"""Use this trial with the given data. Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs for 10 test iterations on some random data\n            >>> from torchbearer import Trial\n            >>> data = torch.rand(10, 1)\n            >>> targets = torch.rand(10, 1)\n            >>> test_data = torch.rand(10, 1)\n            >>> trial = Trial(None).with_data(x_train=data, y_train=targets, x_test=test_data)\n            >>> trial.for_test_steps(10).run(1)\n\n        Args:\n            x_train (torch.Tensor): The training data to use\n            y_train (torch.Tensor): The training targets to use\n            x_val (torch.Tensor): The validation data to use\n            y_val (torch.Tensor): The validation targets to use\n            x_test (torch.Tensor): The test data to use\n            batch_size (int): Batch size to use in mini-batching\n            num_workers (int): Number of workers to use for data loading and batching\n            train_steps (int): Number of steps for each training pass\n            val_steps (int): Number of steps for each validation pass\n            test_steps (int): Number of steps for each test pass\n            shuffle (bool): If True, shuffle training and validation data.\n\n        Returns:\n            Trial: self\n        """"""\n        self.with_train_data(x_train, y_train, batch_size, shuffle, num_workers, train_steps)\n        self.with_val_data(x_val, y_val, batch_size, shuffle, num_workers, val_steps)\n        self.with_test_data(x_test, batch_size, num_workers, test_steps)\n        return self\n\n    # Infinite steps and loading\n\n    def for_inf_train_steps(self):\n        """"""Use this trial with an infinite number of training steps (until stopped via STOP_TRAINING flag or similar). \n        Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs training data until stopped\n            >>> from torchbearer import Trial\n            >>> from torchvision.datasets import MNIST\n            >>> from torch.utils.data import DataLoader\n            >>> trainloader = DataLoader(MNIST(\'./data/\', train=True))\n            >>> trial = Trial(None).with_train_generator(trainloader).for_inf_train_steps()\n            >>> trial.run(1)\n\n        Returns:\n            Trial: self\n        """"""\n        self.for_train_steps(-1)\n        return self\n\n    def for_inf_val_steps(self):\n        """"""Use this trial with an infinite number of validation steps (until stopped via STOP_TRAINING flag or similar).\n        Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs validation data until stopped\n            >>> from torchbearer import Trial\n            >>> from torchvision.datasets import MNIST\n            >>> from torch.utils.data import DataLoader\n            >>> valloader = DataLoader(MNIST(\'./data/\', train=False))\n            >>> trial = Trial(None).with_val_generator(valloader).for_inf_val_steps()\n            >>> trial.run(1)\n\n        Returns:\n            Trial: self\n        """"""\n        self.for_val_steps(-1)\n        return self\n\n    def for_inf_test_steps(self):\n        """"""Use this trial with an infinite number of test steps (until stopped via STOP_TRAINING flag or similar). \n        Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs test data until stopped\n            >>> from torchbearer import Trial\n            >>> test_data = torch.rand(1000, 10)\n            >>> trial = Trial(None).with_test_data(test_data).for_inf_test_steps()\n            >>> trial.run(1)\n\n        Returns:\n            Trial: self\n        """"""\n        self.for_test_steps(-1)\n        return self\n\n    def for_inf_steps(self, train=True, val=True, test=True):\n        """"""Use this trail with infinite steps. Returns self so that methods can be chained for convenience.\n\n        Example: ::\n\n            # Simple trial that runs training and test data until stopped\n            >>> from torchbearer import Trial\n            >>> from torchvision.datasets import MNIST\n            >>> from torch.utils.data import DataLoader\n            >>> trainloader = DataLoader(MNIST(\'./data/\', train=True))\n            >>> valloader = DataLoader(MNIST(\'./data/\', train=False))\n            >>> trial = Trial(None).with_train_generator(trainloader).for_inf_steps(valloader)\n            >>> trial.with_inf_test_loader(True, False, True).run(1)\n\n        Args:\n            train (bool): Use an infinite number of training steps\n            val (bool): Use an infinite number of validation steps\n            test (bool): Use an infinite number of test steps\n\n        Returns:\n            Trial: self\n        """"""\n        if train: self.for_inf_train_steps()\n        if val: self.for_inf_val_steps()\n        if test: self.for_inf_test_steps()\n\n        return self\n\n    def with_inf_train_loader(self):\n        """"""Use this trial with a training iterator that refreshes when it finishes instead of each epoch. \n        This allows for setting training steps less than the size of the generator and model will still be trained on \n        all training samples if enough ""epochs"" are run.\n\n        Example: ::\n\n            # Simple trial that runs 10 epochs of 100 iterations of a training generator without reshuffling until all data has been seen\n            >>> from torchbearer import Trial\n            >>> from torchvision.datasets import MNIST\n            >>> from torch.utils.data import DataLoader\n            >>> trainloader = DataLoader(MNIST(\'./data/\', train=True))\n            >>> trial = Trial(None).with_train_generator(trainloader).with_inf_train_loader()\n            >>> trial.run(10)\n\n        Returns:\n            Trial: self:\n        """"""\n        self.state[torchbearer.INF_TRAIN_LOADING] = True\n\n        return self\n\n    # Customise training loop\n\n    def with_loader(self, batch_loader):\n        """"""Use this trial with custom batch loader. Usually calls next on state[torchbearer.ITERATOR] and populates\n        state[torchbearer.X] and state[torchbearer.Y_TRUE]\n\n        Example: ::\n\n            # Simple trial that runs with a custom loader function that populates X and Y_TRUE in state with random data\n            >>> from torchbearer import Trial\n            >>> def custom_loader(state):\n            ...     state[X], state[Y_TRUE] = torch.rand(5, 5), torch.rand(5, 5)\n            >>> trial = Trial(None).with_loader(custom_loader)\n            >>> trial.run(10)\n\n        Args:\n            batch_loader (function): Function of state that extracts data from data loader (stored under\n                torchbearer.ITERATOR), stores it in state and sends it to the correct device\n\n        Returns:\n            Trial: self:\n        """"""\n        self.state[torchbearer.LOADER] = batch_loader\n        return self\n\n    def with_closure(self, closure):\n        """"""Use this trial with custom closure\n\n        Example: ::\n\n            # Simple trial that runs with a custom closure\n            >>> from torchbearer import Trial\n            >>> def custom_closure(state):\n            ...     print(state[torchbearer.BATCH])\n            >>> trial = Trial(None).with_closure(custom_closure).for_steps(3)\n            >>> _ = trial.run(1)\n            0\n            1\n            2\n\n        Args:\n            closure (function): Function of state that defines the custom closure\n\n        Returns:\n            Trial: self:\n        """"""\n        self.closure = closure\n\n        return self\n\n    # Run\n\n    @inject_printer()\n    def run(self, epochs=1, verbose=-1):\n        r""""""Run this trial for the given number of epochs, starting from the last trained epoch.\n\n        Example: ::\n\n            # Simple trial that runs with a custom closure\n            >>> from torchbearer import Trial\n            >>> trial = Trial(None).for_steps(100)\n            >>> _ = trial.run(1)\n\n        Args:\n            epochs (int, optional): The number of epochs to run for\n            verbose (int, optional): If 2: use tqdm on batch, If 1: use tqdm on epoch, If 0: display no training\n                progress, If -1: Automatic\n\n        State Requirements:\n            - :attr:`torchbearer.state.MODEL`: Model should be callable and not none, set on Trial init\n\n        Returns:\n            list: The model history (list of tuple of steps summary and epoch metric dicts)\n        """"""\n        state = State()\n        state.update({\n            torchbearer.MAX_EPOCHS: epochs,\n            torchbearer.STOP_TRAINING: False,\n        })\n\n        if self.state[torchbearer.MODEL] is None or not callable(self.state[torchbearer.MODEL]):\n            warnings.warn(\'The Model is None or not callable which may cause issues if not deliberate\')\n            self.state[torchbearer.MODEL] = MockModel()\n\n        state.update(self.state)  # TODO: Swap this for something which makes `self.state` still mutable\n\n        if state[torchbearer.TRAIN_GENERATOR] is not None \\\n                or state[torchbearer.TRAIN_STEPS] is not None \\\n                or state[torchbearer.VALIDATION_GENERATOR] is not None \\\n                or state[torchbearer.VALIDATION_STEPS] is not None:\n\n            state[torchbearer.CALLBACK_LIST].on_start(state)\n\n            for state[torchbearer.EPOCH] in range(len(state[torchbearer.HISTORY]), state[torchbearer.MAX_EPOCHS]):\n                state[torchbearer.CALLBACK_LIST].on_start_epoch(state)\n\n                final_metrics = self._fit_pass(state)[torchbearer.METRICS]\n\n                if state[torchbearer.STOP_TRAINING]:\n                    break\n\n                final_metrics.update(self._validation_pass(state))\n                state[torchbearer.METRICS] = final_metrics\n                state[torchbearer.CALLBACK_LIST].on_end_epoch(state)\n                steps_summary = {str(torchbearer.TRAIN_STEPS): state[torchbearer.TRAIN_STEPS], str(torchbearer.VALIDATION_STEPS): state[torchbearer.VALIDATION_STEPS]}\n                self.state[torchbearer.HISTORY].append(dict(state[torchbearer.METRICS], **steps_summary))\n                state[torchbearer.CALLBACK_LIST].on_checkpoint(state)\n\n                if state[torchbearer.STOP_TRAINING]:\n                    break\n\n            state[torchbearer.CALLBACK_LIST].on_end(state)\n\n        return self.state[torchbearer.HISTORY]\n\n    @staticmethod\n    def _new_iter(generator):\n        if generator is None:\n            return None\n        if hasattr(generator, \'inf\') and generator.inf:  # Inf train loader deals with the iterator itself\n            return generator.tb_iter\n        else:\n            return iter(generator)\n\n    @inject_sampler(torchbearer.TRAIN_DATA, load_batch_standard)\n    def _fit_pass(self, state):\n        state.update(self.state)  # TODO: Hack to make injection work, should be removed if `self.state` is mutable\n        self.train()\n\n        state[torchbearer.ITERATOR] = Trial._new_iter(state[torchbearer.GENERATOR])\n\n        state[torchbearer.METRIC_LIST].reset(state)\n        state[torchbearer.METRICS] = {}\n\n        state[torchbearer.STEPS] = 0 if state[torchbearer.STEPS] is None else state[torchbearer.STEPS]\n        state[torchbearer.CALLBACK_LIST].on_start_training(state)\n        for state[torchbearer.BATCH] in (range(state[torchbearer.STEPS]) if state[torchbearer.STEPS] != -1 else itertools.count()):\n            state[torchbearer.SAMPLER](state)\n            state[torchbearer.CALLBACK_LIST].on_sample(state)\n\n            # Update parameters\n            state[torchbearer.OPTIMIZER].step(lambda: self.closure(state))\n\n            state[torchbearer.METRICS] = state[torchbearer.METRIC_LIST].process(state.data)\n            state[torchbearer.CALLBACK_LIST].on_step_training(state)\n\n            if state[torchbearer.STOP_TRAINING]:\n                break\n\n        state[torchbearer.METRICS].update(state[torchbearer.METRIC_LIST].process_final(state.data))\n\n        state[torchbearer.CALLBACK_LIST].on_end_training(state)\n        return state\n\n    def _test_pass(self, state):\n        with torch.no_grad():\n            state[torchbearer.ITERATOR] = Trial._new_iter(state[torchbearer.GENERATOR])\n\n            state[torchbearer.METRIC_LIST].reset(state)\n            state[torchbearer.METRICS] = {}\n\n            state[torchbearer.CALLBACK_LIST].on_start_validation(state)\n\n            state[torchbearer.STEPS] = 0 if state[torchbearer.STEPS] is None else state[torchbearer.STEPS]\n            for state[torchbearer.BATCH] in range(state[torchbearer.STEPS]):\n                state[torchbearer.SAMPLER](state)\n                state[torchbearer.CALLBACK_LIST].on_sample_validation(state)\n\n                _forward_with_exceptions(torchbearer.X, torchbearer.MODEL, torchbearer.Y_PRED, state)\n\n                state[torchbearer.CALLBACK_LIST].on_forward_validation(state)\n\n                # Loss and metrics\n                if torchbearer.Y_TRUE in state:\n                    # Loss Calculation\n                    try:\n                        state[torchbearer.LOSS] = state[torchbearer.CRITERION](state)\n                    except TypeError:\n                        loss_function_params = _get_param_list(state[torchbearer.Y_PRED]) + _get_param_list(\n                            state[torchbearer.Y_TRUE])\n                        state[torchbearer.LOSS] = state[torchbearer.CRITERION](*loss_function_params)\n                    state[torchbearer.CALLBACK_LIST].on_criterion_validation(state)\n                    state[torchbearer.METRICS] = state[torchbearer.METRIC_LIST].process(state.data)\n\n                state[torchbearer.CALLBACK_LIST].on_step_validation(state)\n                if state[torchbearer.STOP_TRAINING]:\n                    break\n\n            if torchbearer.Y_TRUE in state:\n                state[torchbearer.METRICS].update(state[torchbearer.METRIC_LIST].process_final(state.data))\n            state[torchbearer.CALLBACK_LIST].on_end_validation(state)\n        return state\n\n    @inject_sampler(torchbearer.VALIDATION_DATA, load_batch_standard)\n    def _validation_pass(self, state):\n        state.update(self.state)  # TODO: Hack to make injection work, should be removed if `self.state` is mutable\n\n        if state[torchbearer.VALIDATION_GENERATOR] is not None or state[torchbearer.VALIDATION_STEPS] is not None:\n            self.eval()\n\n            self._test_pass(state)\n        return state[torchbearer.METRICS]\n\n    @inject_sampler(torchbearer.VALIDATION_DATA, load_batch_standard)\n    @inject_printer(validation_label_letter=\'e\')\n    def evaluate(self, verbose=-1, data_key=None):  # Note: kwargs appear unused but are inspected in inject_sampler\n        """"""Evaluate this trial on the validation data.\n\n        Example: ::\n\n            # Simple trial to evaluate on both validation and test data\n            >>> from torchbearer import Trial\n            >>> test_data = torch.rand(5, 5)\n            >>> val_data = torch.rand(5, 5)\n            >>> t = Trial(None).with_val_data(val_data).with_test_data(test_data)\n            >>> t.evaluate(data_key=torchbearer.VALIDATION_DATA).evaluate(data_key=torchbearer.TEST_DATA)\n\n        Args:\n            verbose (int): If 2: use tqdm on batch, If 1: use tqdm on epoch, If 0: display no training progress, If -1: Automatic\n            data_key (StateKey): Optional :class:`.StateKey` for the data to evaluate on. Default: torchbearer.VALIDATION_DATA\n\n        Returns:\n            dict: The final metric values\n        """"""\n        state = State()\n        state.update({\n            torchbearer.MAX_EPOCHS: 1,\n            torchbearer.EPOCH: 0,\n            torchbearer.STOP_TRAINING: False\n        })\n        state.update(self.state)  # TODO: Hack to make injection work, should be removed if `self.state` is mutable\n\n        if state[torchbearer.GENERATOR] is not None or state[torchbearer.STEPS] is not None:\n            state[torchbearer.CALLBACK_LIST].on_start(state)\n            state[torchbearer.CALLBACK_LIST].on_start_epoch(state)\n\n            self.eval()\n            state = self._test_pass(state)\n\n            state[torchbearer.CALLBACK_LIST].on_end_epoch(state)\n\n            if len(self.state[torchbearer.HISTORY]) != 0:\n                self.state[torchbearer.HISTORY][-1].update(state[torchbearer.METRICS])\n\n            state[torchbearer.CALLBACK_LIST].on_end(state)\n            return state[torchbearer.METRICS]\n        return {}\n\n    @inject_callback(AggregatePredictions())\n    @inject_sampler(torchbearer.TEST_DATA, load_batch_predict)\n    @inject_printer(validation_label_letter=\'p\')\n    def predict(self, verbose=-1, data_key=None):  # Note: kwargs appear unused but are inspected in inject_sampler\n        """"""Determine predictions for this trial on the test data.\n\n        Example: ::\n\n            # Simple trial to predict on some validation and test data\n            >>> from torchbearer import Trial\n            >>> val_data = torch.rand(5, 5)\n            >>> test_data = torch.rand(5, 5)\n            >>> t = Trial(None).with_test_data(test_data)\n            >>> test_predictions = t.predict(data_key=torchbearer.TEST_DATA)\n\n        Args:\n            verbose (int): If 2: use tqdm on batch, If 1: use tqdm on epoch, If 0: display no training progress, If -1: Automatic\n            data_key (StateKey): Optional :class:`.StateKey` for the data to predict on. Default: torchbearer.TEST_DATA\n\n        Returns:\n            list: Model outputs as a list\n        """"""\n        state = State()\n        state.update({\n            torchbearer.MAX_EPOCHS: 1,\n            torchbearer.EPOCH: 0,\n            torchbearer.STOP_TRAINING: False\n        })\n\n        state.update(self.state)  # TODO: Hack to make injection work, should be removed if `self.state` is mutable\n\n        if state[torchbearer.GENERATOR] is not None or state[torchbearer.STEPS] is not None:\n            state[torchbearer.CALLBACK_LIST].on_start(state)\n            state[torchbearer.CALLBACK_LIST].on_start_epoch(state)\n\n            self.eval()\n            res = self._test_pass(state)[torchbearer.FINAL_PREDICTIONS]\n\n            state[torchbearer.CALLBACK_LIST].on_end_epoch(state)\n            state[torchbearer.CALLBACK_LIST].on_end(state)\n            return res\n        return []\n\n    def replay(self, callbacks=None, verbose=2, one_batch=False):  # TODO: Should we track if testing passes have happened?\n        """""" Replay the fit passes stored in history with given callbacks, useful when reloading a saved Trial. Note that only progress and metric information is populated in state during a replay.\n\n        Example: ::\n\n            >>> from torchbearer import Trial\n            >>> state = torch.load(\'some_state.pt\')\n            >>> t = Trial(None).load_state_dict(state)\n            >>> t.replay()\n\n        Args:\n            callbacks (list): List of callbacks to be run during the replay\n            verbose (int): If 2: use tqdm on batch, If 1: use tqdm on epoch, If 0: display no training progress\n            one_batch (bool): If True, only one batch per epoch is replayed. If False, all batches are replayed\n\n        Returns:\n            Trial: self\n        """"""\n        if callbacks is None:\n            callbacks = []\n        history = self.state[torchbearer.HISTORY]\n        callbacks.append(get_printer(verbose=verbose, validation_label_letter=\'v\'))\n        callbacks = CallbackList(callbacks)\n\n        state = State()\n        state.update(self.state)\n        state[torchbearer.STOP_TRAINING] = False\n        state[torchbearer.MAX_EPOCHS] = len(history)\n\n        callbacks.on_start(state)\n        for i in range(len(history)):\n            metrics = dict(history[i])\n            state[torchbearer.EPOCH] = i\n            if not one_batch:\n                state[torchbearer.TRAIN_STEPS], state[torchbearer.VALIDATION_STEPS] = metrics[str(torchbearer.TRAIN_STEPS)], metrics[str(torchbearer.VALIDATION_STEPS)]\n            else:\n                state[torchbearer.TRAIN_STEPS], state[torchbearer.VALIDATION_STEPS] =\\\n                    1 if metrics[str(torchbearer.TRAIN_STEPS)] is not None else None,\\\n                    1 if metrics[str(torchbearer.VALIDATION_STEPS)] is not None else None\n            del metrics[str(torchbearer.TRAIN_STEPS)]\n            del metrics[str(torchbearer.VALIDATION_STEPS)]\n            state[torchbearer.METRICS] = metrics\n\n            self._replay_pass(state, callbacks)\n        callbacks.on_end(state)\n\n        return self\n\n    def _replay_pass(self, state, callback_list):\n        callback_list.on_start_epoch(state)\n        all_metrics = state[torchbearer.METRICS]\n\n        if state[torchbearer.TRAIN_STEPS] is not None:\n            # Training pass\n            state[torchbearer.STEPS] = state[torchbearer.TRAIN_STEPS]\n            state[torchbearer.METRICS] = {key: all_metrics[key] for key in all_metrics.keys() if ""val_"" not in key}\n            callback_list.on_start_training(state)\n            for state[torchbearer.BATCH] in range(state[torchbearer.STEPS]):\n                callback_list.on_sample(state)\n                callback_list.on_forward(state)\n                callback_list.on_criterion(state)\n                callback_list.on_backward(state)\n                callback_list.on_step_training(state)\n                if state[torchbearer.STOP_TRAINING]:\n                    break\n            callback_list.on_end_training(state)\n\n        if state[torchbearer.VALIDATION_STEPS] is not None:\n            # Validation pass\n            if not state[torchbearer.STOP_TRAINING]:\n                state[torchbearer.STEPS] = state[torchbearer.VALIDATION_STEPS]\n                state[torchbearer.METRICS] = {key: all_metrics[key] for key in all_metrics.keys() if ""val_"" in key}\n                callback_list.on_start_validation(state)\n                for state[torchbearer.BATCH] in range(state[torchbearer.STEPS]):\n                    callback_list.on_sample_validation(state)\n                    callback_list.on_forward_validation(state)\n                    callback_list.on_criterion_validation(state)\n                    callback_list.on_step_validation(state)\n                    if state[torchbearer.STOP_TRAINING]:\n                        break\n                callback_list.on_end_validation(state)\n\n        state[torchbearer.METRICS] = all_metrics\n        callback_list.on_end_epoch(state)\n\n        return self\n\n    # Device management\n\n    def train(self):\n        """"""Set model and metrics to training mode.\n\n        Example: ::\n            >>> from torchbearer import Trial\n            >>> t = Trial(None).train()\n\n        Returns:\n            Trial: self\n        """"""\n        self.state[torchbearer.MODEL].train()\n        self.state[torchbearer.METRIC_LIST].train()\n\n        return self\n\n    def eval(self):\n        """"""Set model and metrics to evaluation mode\n\n        Example: ::\n            >>> from torchbearer import Trial\n            >>> t = Trial(None).eval()\n\n        Returns:\n            Trial: self\n        """"""\n        self.state[torchbearer.MODEL].eval()\n        if torchbearer.DATA in self.state:\n            self.state[torchbearer.METRIC_LIST].eval(data_key=self.state[torchbearer.DATA])\n        else:\n            self.state[torchbearer.METRIC_LIST].eval()\n\n        return self\n\n    def to(self, *args, **kwargs):\n        """""" Moves and/or casts the parameters and buffers.\n\n        Example: ::\n            >>> from torchbearer import Trial\n            >>> t = Trial(None).to(\'cuda:1\')\n\n        Args:\n            args: See: `torch.nn.Module.to <https://pytorch.org/docs/stable/nn.html?highlight=#torch.nn.Module.to>`_\n            kwargs: See: `torch.nn.Module.to <https://pytorch.org/docs/stable/nn.html?highlight=#torch.nn.Module.to>`_\n\n        Returns:\n            Trial: self\n        """"""\n        self.state[torchbearer.MODEL].to(*args, **kwargs)\n\n        for state in self.state[torchbearer.OPTIMIZER].state.values():\n            for k, v in state.items():\n                if torch.is_tensor(v):\n                    state[k] = v.to(*args, **kwargs)\n\n        self.state = update_device_and_dtype(self.state, *args, **kwargs)\n\n        return self\n\n    def cuda(self, device=None):\n        """""" Moves all model parameters and buffers to the GPU.\n\n        Example: ::\n            >>> from torchbearer import Trial\n            >>> t = Trial(None).cuda()\n\n        Args:\n            device (int): if specified, all parameters will be copied to that device\n\n        Returns:\n            Trial: self\n        """"""\n        if device is None:\n            device = torch.cuda.current_device()\n        self.to(\'cuda:\' + str(device))\n\n        return self\n\n    def cpu(self):\n        """""" Moves all model parameters and buffers to the CPU.\n\n        Example: ::\n            >>> from torchbearer import Trial\n            >>> t = Trial(None).cpu()\n\n        Returns:\n            Trial: self\n        """"""\n        self.to(\'cpu\')\n\n        return self\n\n    # States\n\n    def state_dict(self, **kwargs):\n        """"""Get a dict containing the model and optimizer states, as well as the model history.\n\n        Example: ::\n            >>> from torchbearer import Trial\n            >>> t = Trial(None)\n            >>> state = t.state_dict() # State dict that can now be saved with torch.save\n\n        Args:\n            kwargs: See: `torch.nn.Module.state_dict <https://pytorch.org/docs/stable/nn.html?highlight=#torch.nn.Module.state_dict>`_\n\n        Returns:\n            dict: A dict containing parameters and persistent buffers.\n        """"""\n        state_dict = {\n            torchbearer.VERSION: torchbearer.__version__.replace(\'.dev\', \'\'),\n            torchbearer.MODEL: self.state[torchbearer.MODEL].state_dict(**kwargs),\n            torchbearer.OPTIMIZER: self.state[torchbearer.OPTIMIZER].state_dict(),\n            torchbearer.HISTORY: self.state[torchbearer.HISTORY],\n            torchbearer.CALLBACK_LIST: self.state[torchbearer.CALLBACK_LIST].state_dict()\n        }\n        return state_dict\n\n    def load_state_dict(self, state_dict, resume=True, **kwargs):\n        """"""Resume this trial from the given state. Expects that this trial was constructed in the same way. Optionally,\n        just load the model state when resume=False.\n\n        Example: ::\n            >>> from torchbearer import Trial\n            >>> t = Trial(None)\n            >>> state = torch.load(\'some_state.pt\')\n            >>> t.load_state_dict(state)\n\n        Args:\n            state_dict (dict): The state dict to reload\n            resume (bool): If True, resume from the given state. Else, just load in the model weights.\n            kwargs: See: `torch.nn.Module.load_state_dict <https://pytorch.org/docs/stable/nn.html?highlight=#torch.nn.Module.load_state_dict>`_\n\n        Returns:\n            Trial: self\n        """"""\n        if resume and torchbearer.MODEL in state_dict:  # torchbearer dict\n            if torchbearer.VERSION in state_dict and state_dict[torchbearer.VERSION] != torchbearer.__version__.replace(\'.dev\', \'\'):\n                warnings.warn(\'This state dict was saved with a different torchbearer version, loading available keys. Consider setting resume=False\')\n\n            if torchbearer.MODEL in state_dict:\n                self.state[torchbearer.MODEL].load_state_dict(state_dict[torchbearer.MODEL], **kwargs)\n\n            if torchbearer.OPTIMIZER in state_dict:\n                self.state[torchbearer.OPTIMIZER].load_state_dict(state_dict[torchbearer.OPTIMIZER])\n\n            if torchbearer.HISTORY in state_dict:\n                self.state[torchbearer.HISTORY] = state_dict[torchbearer.HISTORY]\n\n            if torchbearer.CALLBACK_LIST in state_dict:\n                self.state[torchbearer.CALLBACK_LIST].load_state_dict(state_dict[torchbearer.CALLBACK_LIST])\n        elif torchbearer.MODEL in state_dict:\n            self.state[torchbearer.MODEL].load_state_dict(state_dict[torchbearer.MODEL], **kwargs)\n        else:  # something else\n            warnings.warn(\'Not a torchbearer state dict, passing to model\')\n            self.state[torchbearer.MODEL].load_state_dict(state_dict, **kwargs)\n\n        return self\n'"
torchbearer/version.py,0,"b""__version__ = '0.5.3.dev'\n"""
tests/callbacks/__init__.py,0,b''
tests/callbacks/test_aggregate_predictions.py,5,"b'from unittest import TestCase\nimport torch\nimport warnings\n\nfrom torchbearer.callbacks import AggregatePredictions\nimport torchbearer\n\n\nclass TestAggregatePredictions(TestCase):\n\n    def test_aggreate_predictions(self):\n        aggregator = AggregatePredictions()\n\n        y_pred_1 = torch.Tensor([1,2,3])\n        y_pred_2 = torch.Tensor([3,4,5])\n\n        state_1 = {torchbearer.Y_PRED: y_pred_1}\n        state_2 = {torchbearer.Y_PRED: y_pred_2}\n        final_state = {}\n\n        aggregator.on_step_validation(state_1)\n        self.assertTrue(list(aggregator.predictions_list[0].numpy()) == list(y_pred_1.numpy()))\n\n        aggregator.on_step_validation(state_2)\n        self.assertTrue(list(aggregator.predictions_list[1].numpy()) == list(y_pred_2.numpy()))\n\n        aggregate = torch.cat([y_pred_1, y_pred_2])\n        aggregator.on_end_validation(final_state)\n        self.assertTrue(list(final_state[torchbearer.FINAL_PREDICTIONS].numpy()) == list(aggregate.numpy()))\n\n    def test_aggreate_predictions_multiple_calls(self):\n        aggregator = AggregatePredictions()\n\n        y_pred_1 = torch.Tensor([1,2,3])\n        y_pred_2 = torch.Tensor([3,4,5])\n\n        state_1 = {torchbearer.Y_PRED: y_pred_1}\n        state_2 = {torchbearer.Y_PRED: y_pred_2}\n\n        aggregator.on_step_validation(state_1)\n        self.assertTrue(list(aggregator.predictions_list[0].numpy()) == list(y_pred_1.numpy()))\n\n        aggregator.on_step_validation(state_2)\n        self.assertTrue(list(aggregator.predictions_list[1].numpy()) == list(y_pred_2.numpy()))\n\n        aggregator.on_end_epoch(state_2)\n        self.assertTrue(list(aggregator.predictions_list) == [])\n\n    def test_none_predictions(self):\n        aggregator = AggregatePredictions()\n\n        with warnings.catch_warnings(record=True) as w:\n            state_1 = {torchbearer.Y_PRED: [None]}\n\n            aggregator.on_step_validation(state_1)\n            aggregator.on_step_validation(state_1)\n\n            self.assertTrue(list(aggregator.predictions_list) == [[None], [None]])\n\n            aggregator.on_end_validation(state_1)\n            self.assertTrue(state_1[torchbearer.FINAL_PREDICTIONS] == [[None], [None]])'"
tests/callbacks/test_between_class.py,12,"b""from unittest import TestCase\n\nimport torch\n\nimport torchbearer\nfrom torchbearer.callbacks import BCPlus\n\n\nclass TestBCPlus(TestCase):\n    def test_on_val(self):\n        bcplus = BCPlus(classes=4)\n        state = {torchbearer.TARGET: torch.tensor([1, 3, 2])}\n        bcplus.on_sample_validation(state)\n        self.assertTrue((state[torchbearer.TARGET] -\n                         torch.tensor([[0, 1, 0, 0],\n                                       [0, 0, 0, 1],\n                                       [0, 0, 1, 0]]).float()).abs().lt(1e-4).all())\n\n        bcplus = BCPlus(classes=4)\n        state = {torchbearer.TARGET: torch.tensor([[0, 1, 0, 0],\n                                                   [0, 0, 0, 1],\n                                                   [0, 0, 1, 0]])}\n        bcplus.on_sample_validation(state)\n        self.assertTrue((state[torchbearer.TARGET] -\n                         torch.tensor([[0, 1, 0, 0],\n                                       [0, 0, 0, 1],\n                                       [0, 0, 1, 0]]).float()).abs().lt(1e-4).all())\n\n    def test_bc_loss(self):\n        prediction = torch.tensor([[10.0, 0.01]])\n        target = torch.tensor([[0., 0.8]])\n        loss = BCPlus.bc_loss({torchbearer.PREDICTION: prediction, torchbearer.TARGET: target})\n        self.assertTrue((loss - 7.81).abs().le(1e-2).all())\n\n    def test_sample_targets(self):\n        # Test mixup\n        bcplus = BCPlus(classes=4, mixup_loss=True)\n        state = {torchbearer.INPUT: torch.zeros(3, 10, 10), torchbearer.TARGET: torch.tensor([1, 3, 2]), torchbearer.DEVICE: 'cpu'}\n        bcplus.on_sample(state)\n\n        self.assertTrue(torchbearer.MIXUP_LAMBDA in state)\n        self.assertTrue(torchbearer.MIXUP_PERMUTATION in state)\n        self.assertTrue(len(state[torchbearer.TARGET]) == 2)\n\n        # Test bcplus\n        bcplus = BCPlus(classes=4)\n        state = {torchbearer.INPUT: torch.zeros(3, 10, 10), torchbearer.TARGET: torch.tensor([1, 3, 2]),\n                 torchbearer.DEVICE: 'cpu'}\n        bcplus.on_sample(state)\n\n        self.assertTrue(state[torchbearer.TARGET].dim() == 2)\n        self.assertTrue(not (torchbearer.MIXUP_PERMUTATION in state))\n\n    def test_sample_inputs(self):\n        torch.manual_seed(7)\n\n        batch = torch.tensor([[\n            [0.1, 0.5, 0.6],\n            [0.8, 0.6, 0.5],\n            [0.2, 0.4, 0.7]\n        ]])\n        target = torch.tensor([1])\n        state = {torchbearer.INPUT: batch, torchbearer.TARGET: target, torchbearer.DEVICE: 'cpu'}\n\n        bcplus = BCPlus(classes=4)\n        bcplus.on_sample(state)\n\n        lam = torch.ones(1) * 0.2649\n\n        self.assertTrue(((state[torchbearer.INPUT] * (lam.pow(2) + (1 - lam).pow(2)).sqrt()) - (batch - batch.mean())).abs().le(1e-4).all())\n"""
tests/callbacks/test_callbacks.py,0,"b""import warnings\n\nfrom unittest import TestCase\nfrom mock import MagicMock, Mock\n\nimport torchbearer\nfrom torchbearer.callbacks import CallbackList, Tqdm, TensorBoard\n\n\nclass TestCallbackList(TestCase):\n    def __init__(self, methodName='runTest'):\n        super(TestCallbackList, self).__init__(methodName)\n        self.callback_1 = MagicMock(spec=torchbearer.callbacks.printer.Tqdm())\n        self.callback_2 = MagicMock(spec=torchbearer.callbacks.tensor_board.TensorBoard())\n        callbacks = [self.callback_1, self.callback_2]\n        self.list = CallbackList(callbacks)\n\n    def test_state_dict(self):\n        self.callback_1.state_dict = Mock(return_value='test_1')\n        self.callback_2.state_dict = Mock(return_value='test_2')\n\n        state = self.list.state_dict()\n\n        self.assertEqual(self.callback_1.state_dict.call_count, 1)\n        self.assertEqual(self.callback_2.state_dict.call_count, 1)\n        self.assertEqual(state[CallbackList.CALLBACK_STATES][0], 'test_1')\n        self.assertEqual(state[CallbackList.CALLBACK_STATES][1], 'test_2')\n        self.assertEqual(state[CallbackList.CALLBACK_TYPES][0], Tqdm().__class__)\n        self.assertEqual(state[CallbackList.CALLBACK_TYPES][1], TensorBoard().__class__)\n\n    def test_load_state_dict(self):\n        self.callback_1.load_state_dict = Mock(return_value='test_1')\n        self.callback_2.load_state_dict = Mock(return_value='test_2')\n\n        self.callback_1.state_dict = Mock(return_value='test_1')\n        self.callback_2.state_dict = Mock(return_value='test_2')\n\n        state = self.list.state_dict()\n        self.list.load_state_dict(state)\n\n        self.callback_1.load_state_dict.assert_called_once_with('test_1')\n        self.callback_2.load_state_dict.assert_called_once_with('test_2')\n\n        state = self.list.state_dict()\n        state[CallbackList.CALLBACK_TYPES] = list(reversed(state[CallbackList.CALLBACK_TYPES]))\n\n        with warnings.catch_warnings(record=True) as w:\n            self.list.load_state_dict(state)\n            self.assertTrue(len(w) == 1)\n            self.assertTrue(issubclass(w[-1].category, UserWarning))\n            self.assertTrue('Callback classes did not match, expected: [\\'TensorBoard\\', \\'Tqdm\\']' in str(w[-1].message))\n\n    def test_for_list(self):\n        self.list.on_start({})\n        self.assertTrue(self.callback_1.method_calls[0][0] == 'on_start')\n        self.assertTrue(self.callback_2.method_calls[0][0] == 'on_start')\n\n    def test_list_in_list(self):\n        callback = 'test'\n        clist = CallbackList([callback])\n        clist2 = CallbackList([clist])\n        self.assertTrue(clist2.callback_list[0] == 'test')\n\n    def test_iter_copy(self):\n        callback = 'test'\n        clist = CallbackList([callback])\n        cpy = clist.__copy__()\n        self.assertTrue(cpy.callback_list[0] == 'test')\n        self.assertTrue(cpy is not clist)\n        cpy = clist.copy()\n        self.assertTrue(cpy.callback_list[0] == 'test')\n        self.assertTrue(cpy is not clist)\n        for cback in clist:\n            self.assertTrue(cback == 'test')\n"""
tests/callbacks/test_checkpointers.py,8,"b'from unittest import TestCase\nfrom mock import patch, Mock\n\nimport torchbearer\nfrom torchbearer import Trial\nfrom torchbearer.callbacks.checkpointers import _Checkpointer, ModelCheckpoint, MostRecent, Interval, Best\nimport warnings\n\n\nclass TestCheckpointer(TestCase):\n    @patch(\'os.makedirs\')\n    def test_make_dirs(self, mock_dirs):\n        _Checkpointer(\'thisdirectoryshouldntexist/norshouldthis/model.pt\')\n        mock_dirs.assert_called_once_with(\'thisdirectoryshouldntexist/norshouldthis\')\n\n    @patch(\'torch.save\')\n    @patch(\'os.makedirs\')\n    def test_no_existing_file(self, mock_dirs, mock_save):\n        check = _Checkpointer(\'thisdirectoryshouldntexist/norshouldthis/model.pt\')\n        check.most_recent = \'thisfiledoesnotexist.pt\'\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\'always\')\n            check.save_checkpoint({torchbearer.METRICS: {}, torchbearer.SELF: Mock()}, True)\n            self.assertTrue(len(w) == 1)\n            self.assertTrue(\'Failed to delete old file\' in str(w[-1].message))\n\n    @patch(""torch.save"")\n    def test_save_checkpoint_save_filename(self, mock_save):\n        torchmodel = Mock()\n        optim = Mock()\n        state = {\n            torchbearer.SELF: Trial(torchmodel, optim, None, []),\n            torchbearer.METRICS: {}\n        }\n\n        file_format = \'test_file.pt\'\n        check = _Checkpointer(file_format)\n        check.save_checkpoint(state)\n        self.assertEqual(mock_save.call_count, 1)\n\n        self.assertTrue(mock_save.call_args[0][1] == \'test_file.pt\')\n\n    @patch(""torch.save"")\n    def test_save_checkpoint_formatting(self, mock_save):\n        torchmodel = Mock()\n        optim = Mock()\n        state = {\n            torchbearer.SELF: Trial(torchmodel, optim, None, []),\n            torchbearer.METRICS: {},\n            torchbearer.EPOCH: 2\n        }\n\n        file_format = \'test_file_{epoch}.pt\'\n        check = _Checkpointer(file_format)\n        check.save_checkpoint(state)\n        self.assertEqual(mock_save.call_count, 1)\n\n        self.assertTrue(mock_save.call_args[0][1] == \'test_file_2.pt\')\n\n    @patch(""torch.save"")\n    def test_save_checkpoint_formatting_metric(self, mock_save):\n        torchmodel = Mock()\n        optim = Mock()\n        state = {\n            torchbearer.SELF: Trial(torchmodel, optim, None, []),\n            torchbearer.METRICS: {\'test_metric\': 0.001},\n            torchbearer.EPOCH: 2\n        }\n\n        file_format = \'test_file_{test_metric}.pt\'\n        check = _Checkpointer(file_format)\n        check.save_checkpoint(state)\n        self.assertEqual(mock_save.call_count, 1)\n\n        self.assertTrue(mock_save.call_args[0][1] == \'test_file_0.001.pt\')\n\n    @patch(""torch.save"")\n    def test_save_checkpoint_subformatting(self, mock_save):\n        torchmodel = Mock()\n        optim = Mock()\n        state = {\n            torchbearer.SELF: Trial(torchmodel, optim, None, []),\n            torchbearer.METRICS: {\'test_metric\': 0.001},\n            torchbearer.EPOCH: 2\n        }\n\n        file_format = \'test_file_{test_metric:.01f}.pt\'\n        check = _Checkpointer(file_format)\n        check.save_checkpoint(state)\n        self.assertEqual(mock_save.call_count, 1)\n\n        self.assertTrue(mock_save.call_args[0][1] == \'test_file_0.0.pt\')\n\n    @patch(""torch.save"")\n    def test_save_checkpoint_model_only(self, mock_save):\n        torchmodel = Mock()\n        optim = Mock()\n        state = {\n            torchbearer.SELF: Trial(torchmodel, optim, None, []),\n            torchbearer.METRICS: {\'test_metric\': 0.001},\n            torchbearer.EPOCH: 2,\n            torchbearer.MODEL: torchmodel,\n        }\n\n        file_format = \'test_file_{test_metric:.01f}.pt\'\n        check = _Checkpointer(file_format, save_model_params_only=True)\n        check.save_checkpoint(state)\n        self.assertEqual(mock_save.call_count, 1)\n        self.assertTrue(mock_save.call_args[0][0] == torchmodel.state_dict())\n        self.assertTrue(mock_save.call_args[0][1] == \'test_file_0.0.pt\')\n\n\n    @patch(""torch.save"")\n    def test_save_checkpoint_wrong_format(self, _):\n        torchmodel = Mock()\n        optim = Mock()\n        state = {\n            torchbearer.SELF: Trial(torchmodel, optim, None, []),\n            torchbearer.METRICS: {\'test_metric\': 0.001},\n            torchbearer.EPOCH: 2\n        }\n\n        file_format = \'test_file_{test_metric:d}.pt\'\n        check = _Checkpointer(file_format)\n        try:\n            check.save_checkpoint(state)\n        except:\n            return\n\n        self.fail(\'No error was thrown when wrong format chosen for save file format\')\n\n    @patch(\'os.remove\')\n    @patch(""torch.save"")\n    def test_save_checkpoint_overwrite_recent(self, _, __):\n        torchmodel = Mock()\n        optim = Mock()\n        state = {\n            torchbearer.SELF: Trial(torchmodel, optim, None, []),\n            torchbearer.EPOCH: 0,\n            torchbearer.METRICS: {}\n        }\n\n        file_format = \'test_file_{epoch}.pt\'\n        check = _Checkpointer(file_format)\n        check.save_checkpoint(state, True)\n        self.assertTrue(check.most_recent == \'test_file_0.pt\')\n\n        state[torchbearer.EPOCH] = 1\n        check.save_checkpoint(state, True)\n        self.assertTrue(check.most_recent == \'test_file_1.pt\')\n\n\nclass TestModelCheckpoint(TestCase):\n    def test_best_only(self):\n        self.assertTrue(isinstance(ModelCheckpoint(save_best_only=True), Best))\n\n    def test_not_best_only(self):\n        self.assertTrue(isinstance(ModelCheckpoint(save_best_only=False), Interval))\n\n\nclass TestMostRecent(TestCase):\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_save(self, mock_save_check):\n        state = {}\n        check = MostRecent(\'test_file.pt\')\n\n        check.on_checkpoint(state)\n        check.on_checkpoint(state)\n\n        self.assertTrue(mock_save_check.call_count == 2)\n\n\n# TODO: Negative and fractional interval test and decide how to handle\nclass TestInterval(TestCase):\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_interval_is_1(self, mock_save_check):\n        state = {}\n        check = Interval(\'test_file\', period=1)\n\n        check.on_checkpoint(state)\n        check.on_checkpoint(state)\n\n        self.assertTrue(mock_save_check.call_count == 2)\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_interval_is_more_than_1(self, mock_save_check):\n        state = {}\n        check = Interval(\'test_file\', period=4)\n\n        for i in range(13):\n            check.on_checkpoint(state)\n            if i == 3:\n                self.assertTrue(mock_save_check.call_count == 1)\n            elif i == 6:\n                self.assertFalse(mock_save_check.call_count == 2)\n            elif i == 7:\n                self.assertTrue(mock_save_check.call_count == 2)\n\n        self.assertTrue(mock_save_check.call_count == 3)\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_interval_on_batch(self, mock_save_check):\n        state = {}\n        check = Interval(\'test_file\', period=4, on_batch=True)\n\n        for i in range(13):\n            check.on_step_training(state)\n            if i == 3:\n                self.assertTrue(mock_save_check.call_count == 1)\n            elif i == 6:\n                self.assertFalse(mock_save_check.call_count == 2)\n            elif i == 7:\n                self.assertTrue(mock_save_check.call_count == 2)\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save_check.call_count == 3)\n\n    def test_state_dict(self):\n        check = Interval(\'test\')\n        check.epochs_since_last_save = 10\n\n        state = check.state_dict()\n\n        check = Interval(\'test\')\n        check.load_state_dict(state)\n\n        self.assertEqual(check.epochs_since_last_save, 10)\n\n\nclass TestBest(TestCase):\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_min_with_increasing(self, mock_save):\n        state = {torchbearer.METRICS: {\'val_loss\': 0.1}}\n\n        file_path = \'test_file_{val_loss:.2f}\'\n        check = Best(file_path, mode=\'min\')\n        check.on_start(state)\n\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n        state = {torchbearer.METRICS: {\'val_loss\': 0.2}}\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_min_with_decreasing(self, mock_save):\n        state = {torchbearer.METRICS: {\'val_loss\': 0.1}}\n\n        file_path = \'test_file_{val_loss:.2f}\'\n        check = Best(file_path, mode=\'min\')\n        check.on_start(state)\n\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n        state = {torchbearer.METRICS: {\'val_loss\': 0.001}}\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 2)\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_max_with_increasing(self, mock_save):\n        state = {torchbearer.METRICS: {\'val_loss\': 0.1}}\n\n        file_path = \'test_file_{val_loss:.2f}\'\n        check = Best(file_path, mode=\'max\')\n        check.on_start(state)\n\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n        state = {torchbearer.METRICS: {\'val_loss\': 0.2}}\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 2)\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_max_with_decreasing(self, mock_save):\n        state = {torchbearer.METRICS: {\'val_loss\': 0.1}}\n\n        file_path = \'test_file_{val_loss:.2f}\'\n        check = Best(file_path, mode=\'max\')\n        check.on_start(state)\n\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n        state = {torchbearer.METRICS: {\'val_loss\': 0.001}}\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_min_delta_no_save(self, mock_save):\n        state = {torchbearer.METRICS: {\'val_loss\': 0.1}}\n\n        file_path = \'test_file_{val_loss:.2f}\'\n        check = Best(file_path, mode=\'min\', min_delta=0.1)\n        check.on_start(state)\n\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n        state = {torchbearer.METRICS: {\'val_loss\': 0.001}}\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_min_delta_save(self, mock_save):\n        state = {torchbearer.METRICS: {\'val_loss\': 0.1}}\n\n        file_path = \'test_file_{val_loss:.2f}\'\n        check = Best(file_path, mode=\'min\', min_delta=0.1)\n        check.on_start(state)\n\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 1)\n\n        state = {torchbearer.METRICS: {\'val_loss\': -0.001}}\n        check.on_checkpoint(state)\n        self.assertTrue(mock_save.call_count == 2)\n        \n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_auto_shoud_be_min(self, _):\n        state = {torchbearer.METRICS: {\'val_loss\': 0.1}}\n\n        file_path = \'test_file_{val_loss:.2f}\'\n        check = Best(file_path, monitor=\'val_loss\')\n        check.on_start(state)\n\n        check.on_checkpoint(state)\n        self.assertTrue(check.mode == \'min\')\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_auto_shoud_be_max(self, _):\n        state = {torchbearer.METRICS: {\'acc_loss\': 0.1}}\n\n        file_path = \'test_file_{acc_loss:.2f}\'\n        check = Best(file_path, monitor=\'acc_loss\')\n        check.on_start(state)\n\n        check.on_checkpoint(state)\n        self.assertTrue(check.mode == \'max\')\n\n    @patch(\'torchbearer.callbacks.checkpointers._Checkpointer.save_checkpoint\')\n    def test_bad_monitor(self, _):\n        state = {torchbearer.METRICS: {\'acc_loss\': 0.1}}\n\n        file_path = \'test_file_{acc_loss:.2f}\'\n        check = Best(file_path, monitor=\'test_fail\')\n        check.on_start(state)\n\n        with warnings.catch_warnings(record=True) as w:\n            check.on_checkpoint(state)\n            self.assertTrue(len(w) == 1)\n\n    def test_state_dict(self):\n        check = Best(\'test\')\n        check.best = \'temp2\'\n        check.epochs_since_last_save = 10\n\n        state = check.state_dict()\n\n        check = Best(\'test\')\n        check.load_state_dict(state)\n\n        self.assertEqual(check.best, \'temp2\')\n        self.assertEqual(check.epochs_since_last_save, 10)\n'"
tests/callbacks/test_csv_logger.py,0,"b'from unittest import TestCase\r\nfrom mock import patch, mock_open\r\n\r\nimport torchbearer\r\nfrom torchbearer.callbacks import CSVLogger\r\n\r\n\r\nclass TestCSVLogger(TestCase):\r\n\r\n    @patch(""torchbearer.callbacks.csv_logger.open"", new_callable=mock_open)\r\n    def test_write_header(self, mock_open):\r\n        state = {\r\n            torchbearer.EPOCH: 0,\r\n            torchbearer.BATCH: 1,\r\n            torchbearer.METRICS: {\'test_metric_1\': 0.1, \'test_metric_2\': 5}\r\n        }\r\n\r\n        logger = CSVLogger(\'test_file.log\')\r\n        logger.on_start(state)\r\n        logger.on_step_training(state)\r\n        logger.on_end_epoch(state)\r\n        logger.on_end(state)\r\n\r\n        header = mock_open.mock_calls[1][1][0]\r\n        self.assertTrue(\'epoch\' in header)\r\n        self.assertTrue(\'test_metric_1\' in header)\r\n        self.assertTrue(\'test_metric_2\' in header)\r\n\r\n    @patch(""torchbearer.callbacks.csv_logger.open"", new_callable=mock_open)\r\n    def test_write_no_header(self, mock_open):\r\n        state = {\r\n            torchbearer.EPOCH: 0,\r\n            torchbearer.BATCH: 1,\r\n            torchbearer.METRICS: {\'test_metric_1\': 0.1, \'test_metric_2\': 5}\r\n        }\r\n\r\n        logger = CSVLogger(\'test_file.log\', write_header=False)\r\n        logger.on_start(state)\r\n        logger.on_step_training(state)\r\n        logger.on_end_epoch(state)\r\n        logger.on_end(state)\r\n\r\n        header = mock_open.mock_calls[1][1][0]\r\n        self.assertTrue(\'epoch\' not in header)\r\n        self.assertTrue(\'test_metric_1\' not in header)\r\n        self.assertTrue(\'test_metric_2\' not in header)\r\n\r\n    @patch(""torchbearer.callbacks.csv_logger.open"", new_callable=mock_open)\r\n    def test_csv_closed(self, mock_open):\r\n        state = {\r\n            torchbearer.EPOCH: 0,\r\n            torchbearer.BATCH: 1,\r\n            torchbearer.METRICS: {\'test_metric_1\': 0.1, \'test_metric_2\': 5}\r\n        }\r\n\r\n        logger = CSVLogger(\'test_file.log\', write_header=False)\r\n        logger.on_start(state)\r\n        logger.on_step_training(state)\r\n        logger.on_end_epoch(state)\r\n        logger.on_end(state)\r\n\r\n        self.assertTrue(mock_open.return_value.close.called)\r\n\r\n    @patch(""torchbearer.callbacks.csv_logger.open"", new_callable=mock_open)\r\n    def test_append(self, mock_open):\r\n        state = {\r\n            torchbearer.EPOCH: 0,\r\n            torchbearer.BATCH: 1,\r\n            torchbearer.METRICS: {\'test_metric_1\': 0.1, \'test_metric_2\': 5}\r\n        }\r\n\r\n        logger = CSVLogger(\'test_file.log\', append=True)\r\n        logger.on_start(state)\r\n        logger.on_step_training(state)\r\n        logger.on_end_epoch(state)\r\n        logger.on_end(state)\r\n\r\n        import sys\r\n        if sys.version_info[0] < 3:\r\n            self.assertTrue(mock_open.call_args[0][1] == \'ab\')\r\n        else:\r\n            self.assertTrue(mock_open.call_args[0][1] == \'a\')\r\n\r\n    @patch(""torchbearer.callbacks.csv_logger.open"", new_callable=mock_open)\r\n    def test_get_field_dict(self, mock_open):\r\n        state = {\r\n            torchbearer.EPOCH: 0,\r\n            torchbearer.BATCH: 1,\r\n            torchbearer.METRICS: {\'test_metric_1\': 0.1, \'test_metric_2\': 5}\r\n        }\r\n        correct_fields_dict = {\r\n            \'epoch\': 0,\r\n            \'batch\': 1,\r\n            \'test_metric_1\': 0.1,\r\n            \'test_metric_2\': 5\r\n        }\r\n\r\n        logger = CSVLogger(\'test_file.log\', batch_granularity=True)\r\n\r\n        logger_fields_dict = logger._get_field_dict(state)\r\n\r\n        self.assertDictEqual(logger_fields_dict, correct_fields_dict)\r\n\r\n    @patch(\'torchbearer.callbacks.CSVLogger._write_to_dict\')\r\n    @patch(""torchbearer.callbacks.csv_logger.open"", new_callable=mock_open)\r\n    def test_write_on_epoch(self, mock_open, mock_write):\r\n        state = {\r\n            torchbearer.EPOCH: 0,\r\n            torchbearer.BATCH: 1,\r\n            torchbearer.METRICS: {\'test_metric_1\': 0.1, \'test_metric_2\': 5}\r\n        }\r\n\r\n        logger = CSVLogger(\'test_file.log\')\r\n        logger.on_start(state)\r\n        logger.on_step_training(state)\r\n        logger.on_end_epoch(state)\r\n        logger.on_end(state)\r\n\r\n        self.assertEqual(mock_write.call_count, 1)\r\n\r\n    @patch(\'torchbearer.callbacks.CSVLogger._write_to_dict\')\r\n    @patch(""torchbearer.callbacks.csv_logger.open"", new_callable=mock_open)\r\n    def test_batch_granularity(self, mock_open, mock_write):\r\n        state = {\r\n            torchbearer.EPOCH: 0,\r\n            torchbearer.BATCH: 1,\r\n            torchbearer.METRICS: {\'test_metric_1\': 0.1, \'test_metric_2\': 5}\r\n        }\r\n\r\n        logger = CSVLogger(\'test_file.log\', batch_granularity=True)\r\n        logger.on_start(state)\r\n        logger.on_step_training(state)\r\n        logger.on_step_training(state)\r\n        logger.on_end_epoch(state)\r\n        logger.on_end(state)\r\n\r\n        self.assertTrue(mock_write.call_count == 3)\r\n'"
tests/callbacks/test_cutout.py,26,"b""from unittest import TestCase\n\nimport torch\n\nimport torchbearer\nfrom torchbearer.callbacks.cutout import Cutout, RandomErase, CutMix\n\n\nclass TestCutOut(TestCase):\n    def test_cutout(self):\n        random_image = torch.rand(2, 3, 100, 100)\n        torch.manual_seed(7)\n        co = Cutout(1, 10)\n        state = {torchbearer.X: random_image}\n        co.on_sample(state)\n        reg_img = state[torchbearer.X].view(-1)\n\n        x = [21, 86]\n        y = [15, 92]\n\n        known_cut = random_image.clone().numpy()\n        known_cut[0, :, y[0]-10//2:y[0]+10//2, x[0]-10//2:x[0]+10//2] = 0\n        known_cut[1, :, y[1]-10//2:y[1]+10//2, x[1]-10//2:x[1]+10//2] = 0\n        known_cut = torch.from_numpy(known_cut)\n        known_cut = known_cut.view(-1)\n\n        diff = (torch.abs(known_cut-reg_img) > 1e-4).any()\n        self.assertTrue(diff.item() == 0)\n\n    def test_cutout_constant(self):\n        random_image = torch.rand(2, 3, 100, 100)\n        torch.manual_seed(7)\n        co = Cutout(1, 10, constant=0.5)\n        state = {torchbearer.X: random_image}\n        co.on_sample(state)\n        reg_img = state[torchbearer.X].view(-1)\n\n        x = [21, 86]\n        y = [15, 92]\n\n        known_cut = random_image.clone().numpy()\n        known_cut[0, :, y[0]-10//2:y[0]+10//2, x[0]-10//2:x[0]+10//2] = 0.5\n        known_cut[1, :, y[1]-10//2:y[1]+10//2, x[1]-10//2:x[1]+10//2] = 0.5\n        known_cut = torch.from_numpy(known_cut)\n        known_cut = known_cut.view(-1)\n\n        diff = (torch.abs(known_cut-reg_img) > 1e-4).any()\n        self.assertTrue(diff.item() == 0)\n\n    # TODO: Find a better test for this\n    def test_random_erase(self):\n        random_image = torch.rand(2, 3, 100, 100)\n        torch.manual_seed(7)\n        co = RandomErase(1, 10)\n        state = {torchbearer.X: random_image}\n        co.on_sample(state)\n        reg_img = state[torchbearer.X].view(-1)\n\n        x = [21, 86]\n        y = [15, 92]\n\n        known_cut = random_image.clone().numpy()\n        known_cut[0, :, y[0]-10//2:y[0]+10//2, x[0]-10//2:x[0]+10//2] = 0\n        known_cut[1, :, y[1]-10//2:y[1]+10//2, x[1]-10//2:x[1]+10//2] = 0\n        known_cut = torch.from_numpy(known_cut)\n\n        known_cut = known_cut.view(-1)\n        masked_pix = known_cut == 0\n\n        diff = (torch.abs(known_cut[masked_pix]-reg_img[masked_pix]) > 1e-4).any()\n        self.assertTrue(diff.item() > 0)\n\n    def test_cutmix(self):\n        random_image = torch.rand(5, 3, 100, 100)\n        state = {torchbearer.X: random_image, torchbearer.Y_TRUE: torch.randint(10, (5,)).long(), torchbearer.DEVICE: 'cpu'}\n        torch.manual_seed(7)\n        co = CutMix(0.25, classes=10)\n        co.on_sample(state)\n        reg_img = state[torchbearer.X].view(-1)\n\n        x = [72, 83, 18, 96, 40]\n        y = [8, 17, 62, 30, 66]\n        perm = [0, 4, 3, 2, 1]\n        sz = 3\n\n        rnd = random_image.clone().numpy()\n        known_cut = random_image.clone().numpy()\n        known_cut[0, :, y[0]-sz//2:y[0]+sz//2, x[0]-sz//2:x[0]+sz//2] = rnd[perm[0], :, y[0]-sz//2:y[0]+sz//2, x[0]-sz//2:x[0]+sz//2]\n        known_cut[1, :, y[1]-sz//2:y[1]+sz//2, x[1]-sz//2:x[1]+sz//2] = rnd[perm[1], :, y[1]-sz//2:y[1]+sz//2, x[1]-sz//2:x[1]+sz//2]\n        known_cut[2, :, y[2]-sz//2:y[2]+sz//2, x[2]-sz//2:x[2]+sz//2] = rnd[perm[2], :, y[2]-sz//2:y[2]+sz//2, x[2]-sz//2:x[2]+sz//2]\n        known_cut[3, :, y[3]-sz//2:y[3]+sz//2, x[3]-sz//2:x[3]+sz//2] = rnd[perm[3], :, y[3]-sz//2:y[3]+sz//2, x[3]-sz//2:x[3]+sz//2]\n        known_cut[4, :, y[4]-sz//2:y[4]+sz//2, x[4]-sz//2:x[4]+sz//2] = rnd[perm[4], :, y[4]-sz//2:y[4]+sz//2, x[4]-sz//2:x[4]+sz//2]\n        known_cut = torch.from_numpy(known_cut)\n        known_cut = known_cut.view(-1)\n\n        diff = (torch.abs(known_cut-reg_img) > 1e-4).any()\n        self.assertTrue(diff.item() == 0)\n\n    def test_cutmix_targets(self):\n        random_image = torch.rand(2, 3, 100, 100)\n        torch.manual_seed(7)\n        co = CutMix(1.0, classes=4)\n        target = torch.tensor([\n            [0., 1., 0., 0.],\n            [0., 0., 0., 1.]\n        ])\n        state = {torchbearer.X: random_image, torchbearer.Y_TRUE: torch.tensor([1, 3]).long(), torchbearer.DEVICE: 'cpu'}\n        co.on_sample(state)\n        self.assertTrue(((state[torchbearer.TARGET] - target).abs() < 0.00001).all())\n        state = {torchbearer.X: random_image, torchbearer.Y_TRUE: torch.tensor([1, 3]).long()}\n        co.on_sample_validation(state)\n        self.assertTrue(((state[torchbearer.TARGET] - target).abs() < 0.00001).all())\n        state = {torchbearer.X: random_image, torchbearer.Y_TRUE: target.long()}\n        co.on_sample_validation(state)\n        self.assertTrue(((state[torchbearer.TARGET] - target).abs() < 0.00001).all())\n\n    def test_target(self):\n        mixup = CutMix(-0.1, classes=2, mixup_loss=True)\n        X = torch.rand(2, 3, 100, 100)\n        Y_true = torch.Tensor([0., 1.])\n\n        state = {\n            torchbearer.X : X,\n            torchbearer.Y_TRUE : Y_true,\n            torchbearer.DEVICE: 'cpu'\n        }\n\n        mixup.on_sample(state)\n\n        self.assertTrue((state[torchbearer.Y_TRUE][0] == torch.Tensor([0., 1.])).all())\n        self.assertTrue((state[torchbearer.Y_TRUE][1] == torch.Tensor([0., 1.])).all() or (state[torchbearer.Y_TRUE][1] == torch.Tensor([1., 0.])).all())\n"""
tests/callbacks/test_decorators.py,0,"b""import unittest\n\nimport torchbearer.callbacks as callbacks\nimport torchbearer\n\n\nclass TestDecorators(unittest.TestCase):\n\n    def test_multi(self):\n        def example(state):\n            return state\n        state = 'test'\n        c = callbacks.on_backward(callbacks.on_sample(callbacks.on_start(example)))\n        self.assertTrue(c.on_backward(state) == state)\n        self.assertTrue(c.on_sample(state) == state)\n        self.assertTrue(c.on_start(state) == state)\n\n    def test_target(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.bind_to(callbacks.on_start)(example).on_start(state) == state)\n\n    def test_on_init(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_init(example).on_init(state) == state)\n\n    def test_on_start(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_start(example).on_start(state) == state)\n\n    def test_on_start_epoch(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_start_epoch(example).on_start_epoch(state) == state)\n\n    def test_on_start_training(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_start_training(example).on_start_training(state) == state)\n\n    def test_on_sample(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_sample(example).on_sample(state) == state)\n\n    def test_on_forward(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_forward(example).on_forward(state) == state)\n\n    def test_on_criterion(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_criterion(example).on_criterion(state) == state)\n\n    def test_on_backward(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_backward(example).on_backward(state) == state)\n\n    def test_on_step_training(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_step_training(example).on_step_training(state) == state)\n\n    def test_on_end_training(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_end_training(example).on_end_training(state) == state)\n\n    def test_on_end_epoch(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_end_epoch(example).on_end_epoch(state) == state)\n\n    def test_on_end(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_end(example).on_end(state) == state)\n\n    def test_on_start_validation(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_start_validation(example).on_start_validation(state) == state)\n\n    def test_on_sample_validation(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_sample_validation(example).on_sample_validation(state) == state)\n\n    def test_on_forward_validation(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_forward_validation(example).on_forward_validation(state) == state)\n\n    def test_on_criterion_validation(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_criterion_validation(example).on_criterion_validation(state) == state)\n\n    def test_on_end_validation(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_end_validation(example).on_end_validation(state) == state)\n\n    def test_on_step_validation(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_step_validation(example).on_step_validation(state) == state)\n\n    def test_on_checkpoint(self):\n        def example(state):\n            return state\n        state = 'test'\n        self.assertTrue(callbacks.on_checkpoint(example).on_checkpoint(state) == state)\n\n    def test_add_to_loss(self):\n        def example(state):\n            return 1\n        state = {'test': 'test', torchbearer.LOSS: 0}\n        callbacks.add_to_loss(example).on_criterion(state)\n        self.assertTrue(state[torchbearer.LOSS] == 1)\n        callbacks.add_to_loss(example).on_criterion_validation(state)\n        self.assertTrue(state[torchbearer.LOSS] == 2)\n\n    def test_once(self):\n        class Example(callbacks.Callback):\n            @callbacks.once\n            def on_step_validation(self, state):\n                state['value'] += 1\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb = Example()\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb2 = Example()\n\n        cb2.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb2.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        state[torchbearer.EPOCH] += 1\n        cb2.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n    def test_once_per_epoch(self):\n        class Example(callbacks.Callback):\n            @callbacks.once_per_epoch\n            def on_step_validation(self, state):\n                state['value'] += 1\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb = Example()\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb2 = Example()\n\n        cb2.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb2.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        state[torchbearer.EPOCH] += 1\n        cb2.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n    def test_only_if(self):\n        class Example(callbacks.Callback):\n            @callbacks.only_if(lambda s: s[torchbearer.EPOCH] % 2 == 0)\n            def on_step_validation(self, state):\n                state['value'] += 1\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb = Example()\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 3)\n\n    def test_once_lambda(self):\n        @callbacks.once\n        @callbacks.on_step_validation\n        def callback_func(state):\n            state['value'] += 1\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb = callback_func\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n    def test_once_per_epoch_lambda(self):\n        @callbacks.once_per_epoch\n        @callbacks.on_step_validation\n        def callback_func(state):\n            state['value'] += 1\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb = callback_func\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n    def test_only_if_lambda(self):\n        @callbacks.only_if(lambda s: s[torchbearer.EPOCH] % 2 == 0)\n        @callbacks.on_step_validation\n        def callback_func(state):\n            state['value'] += 1\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb = callback_func\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 3)\n\n    def test_lambda_only_if(self):\n        @callbacks.on_step_validation\n        @callbacks.only_if(lambda s: s[torchbearer.EPOCH] % 2 == 0)\n        def callback_func(state):\n            state['value'] += 1\n\n        state = {torchbearer.EPOCH: 0, 'value': 0}\n\n        cb = callback_func\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 1)\n\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 2)\n\n        state[torchbearer.EPOCH] += 1\n        cb.on_step_validation(state)\n        self.assertTrue(state['value'] == 3)"""
tests/callbacks/test_early_stopping.py,0,"b""from unittest import TestCase\nfrom mock import MagicMock\n\nimport torchbearer\nfrom torchbearer.callbacks import EarlyStopping\n\n\nclass TestEarlyStopping(TestCase):\n    def test_step_on_batch(self):\n        stopper = EarlyStopping(monitor='test_metric', mode='min', step_on_batch=True)\n\n        stopper.step = MagicMock()\n\n        stopper.on_step_training('test')\n        self.assertTrue(stopper.step.call_count == 1)\n\n        stopper.on_end_epoch('test')\n        self.assertTrue(stopper.step.call_count == 1)\n\n    def test_min_should_stop(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', mode='min')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric'] = 0.01\n        stopper.on_end_epoch(state)\n\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_min_should_continue(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', mode='min')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric'] = 0.0001\n\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n    def test_max_should_stop(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric',  mode='max')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric'] = 0.0001\n        stopper.on_end_epoch(state)\n\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_max_should_continue(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', mode='max')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric'] = 0.01\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n    def test_max_equal_should_stop(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', mode='max')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        stopper.on_end_epoch(state)\n\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_in_equal_should_stop(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', mode='min')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        stopper.on_end_epoch(state)\n\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_patience_should_stop(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', patience=3)\n\n        stopper.on_start(state)\n\n        for i in range(3):\n            stopper.on_end_epoch(state)\n            self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        stopper.on_end_epoch(state)\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_patience_should_continue(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', patience=3)\n\n        stopper.on_start(state)\n\n        for i in range(3):\n            stopper.on_end_epoch(state)\n            self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric'] = 0.0001\n        stopper.on_end_epoch(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n    def test_min_delta_should_continue(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', mode='max', min_delta=0.1)\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric'] = 0.102\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n    def test_min_delta_should_stop(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric', mode='max', min_delta=0.1)\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric'] = 0.10\n        stopper.on_end_epoch(state)\n\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_auto_should_be_min(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertTrue(stopper.mode == 'min')\n\n    def test_auto_should_be_max(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'acc_metric': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='acc_metric')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertTrue(stopper.mode == 'max')\n\n    def test_monitor_should_continue(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric_1': 0.001, 'test_metric_2': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric_2', mode='max')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric_1'] = 0.0001\n        state[torchbearer.METRICS]['test_metric_2'] = 0.01\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n    def test_monitor_should_stop(self):\n        state = {\n            torchbearer.EPOCH: 1,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'test_metric_1': 0.001, 'test_metric_2': 0.001}\n        }\n\n        stopper = EarlyStopping(monitor='test_metric_2', mode='max')\n\n        stopper.on_start(state)\n        stopper.on_end_epoch(state)\n\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n        state[torchbearer.METRICS]['test_metric_1'] = 0.1\n        state[torchbearer.METRICS]['test_metric_2'] = 0.0001\n        stopper.on_end_epoch(state)\n\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_state_dict(self):\n        stopper = EarlyStopping(monitor='test_metric_1')\n        stopper.wait = 10\n        stopper.best = 20\n        state = stopper.state_dict()\n\n        stopper = EarlyStopping(monitor='test_metric_1')\n        self.assertNotEqual(stopper.wait, 10)\n\n        stopper.load_state_dict(state)\n        self.assertEqual(stopper.wait, 10)\n        self.assertEqual(stopper.best, 20)\n"""
tests/callbacks/test_gradient_clipping.py,7,"b""from unittest import TestCase\nfrom mock import patch, Mock\n\nimport torchbearer\nfrom torchbearer.callbacks import GradientNormClipping, GradientClipping\nimport torch.nn as nn\n\n\nclass TestGradientNormClipping(TestCase):\n    @patch('torch.nn.utils.clip_grad_norm_')\n    def test_not_given_params(self, mock_clip):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        param = Mock(return_value=-1)\n        param.requires_grad = Mock(return_value=True)\n        model.parameters = Mock(return_value=[param])\n        state = {torchbearer.MODEL: model}\n\n        clipper = GradientNormClipping(5)\n\n        clipper.on_start(state)\n        clipper.on_backward(state)\n\n        self.assertTrue(next(iter(mock_clip.mock_calls[0][1][0]))() == -1)\n\n    @patch('torch.nn.utils.clip_grad_norm_')\n    def test_given_params(self, mock_clip):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=-1)\n        state = {torchbearer.MODEL: model}\n\n        clipper = GradientNormClipping(5, params=model.parameters())\n\n        clipper.on_start(state)\n        clipper.on_backward(state)\n\n        self.assertTrue(mock_clip.mock_calls[0][1][0] == -1)\n\n    @patch('torch.nn.utils.clip_grad_norm_')\n    def test_passed_norm(self, mock_clip):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=-1)\n        state = {torchbearer.MODEL: model}\n\n        clipper = GradientNormClipping(5, 2, params=model.parameters())\n\n        clipper.on_start(state)\n        clipper.on_backward(state)\n\n        self.assertTrue(mock_clip.mock_calls[0][1][1] == 5)\n        self.assertTrue(mock_clip.mock_calls[0][2]['norm_type'] == 2)\n\n    @patch('torch.nn.utils.clip_grad_norm_')\n    def test_default_norm_type(self, mock_clip):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=-1)\n        state = {torchbearer.MODEL: model}\n\n        clipper = GradientNormClipping(5, params=model.parameters())\n\n        clipper.on_start(state)\n        clipper.on_backward(state)\n\n        self.assertTrue(mock_clip.mock_calls[0][1][1] == 5)\n        self.assertTrue(mock_clip.mock_calls[0][2]['norm_type'] == 2)\n\n\nclass TestGradientClipping(TestCase):\n    @patch('torch.nn.utils.clip_grad_value_')\n    def test_not_given_params(self, mock_clip):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        param = Mock(return_value=-1)\n        param.requires_grad = Mock(return_value=True)\n        model.parameters = Mock(return_value=[param])\n        state = {torchbearer.MODEL: model}\n\n        clipper = GradientClipping(5)\n\n        clipper.on_start(state)\n        clipper.on_backward(state)\n\n        self.assertTrue(next(iter(mock_clip.mock_calls[0][1][0]))() == -1)\n\n    @patch('torch.nn.utils.clip_grad_value_')\n    def test_given_params(self, mock_clip):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=-1)\n        state = {torchbearer.MODEL: model}\n\n        clipper = GradientClipping(5, params=model.parameters())\n\n        clipper.on_start(state)\n        clipper.on_backward(state)\n\n        self.assertTrue(mock_clip.mock_calls[0][1][0] == -1)\n"""
tests/callbacks/test_init.py,12,"b""import torch\nfrom unittest import TestCase\n\nfrom mock import MagicMock, patch\n\nimport torchbearer\nimport torchbearer.callbacks.init as init\n\n\nclass TestWeightInit(TestCase):\n    def test_modules_from_state(self):\n        callback = init.WeightInit(targets=['Mock'])\n        model = MagicMock()\n        state = {torchbearer.MODEL: model}\n        callback.on_init(state)\n        self.assertTrue(model.modules.call_count == 1)\n\n    def test_filter(self):\n        mock = MagicMock()\n        callback = init.WeightInit(initialiser=lambda m: m.test(), modules=[mock], targets=['Mock'])\n        callback.on_init({})\n        self.assertTrue(mock.test.call_count == 1)\n\n        mock = MagicMock()\n        callback = init.WeightInit(initialiser=lambda m: m.test(), modules=[mock], targets=['Not'])\n        callback.on_init({})\n        self.assertTrue(mock.test.call_count == 0)\n\n    def test_module_list(self):\n        mock = MagicMock()\n        callback = init.WeightInit(initialiser=lambda m: m.test(), modules=[mock], targets=['Mock'])\n        model = MagicMock()\n        state = {torchbearer.MODEL: model}\n        callback.on_init(state)\n        self.assertTrue(model.modules.call_count == 0)\n\n\nclass TestSimpleInits(TestCase):\n    @patch('torchbearer.callbacks.init.init')\n    def test_kaiming(self, nn_init):\n        callback = init.KaimingNormal(a=1, mode='test', nonlinearity='test2')\n        mock = MagicMock()\n        callback.initialiser(mock)\n        nn_init.kaiming_normal_.assert_called_once_with(mock.weight.data, a=1, mode='test', nonlinearity='test2')\n\n        callback = init.KaimingUniform(a=1, mode='test', nonlinearity='test2')\n        mock = MagicMock()\n        callback.initialiser(mock)\n        nn_init.kaiming_uniform_.assert_called_once_with(mock.weight.data, a=1, mode='test', nonlinearity='test2')\n\n    @patch('torchbearer.callbacks.init.init')\n    def test_xavier(self, nn_init):\n        callback = init.XavierNormal(gain=100)\n        mock = MagicMock()\n        callback.initialiser(mock)\n        nn_init.xavier_normal_.assert_called_once_with(mock.weight.data, gain=100)\n\n        callback = init.XavierUniform(gain=100)\n        mock = MagicMock()\n        callback.initialiser(mock)\n        nn_init.xavier_uniform_.assert_called_once_with(mock.weight.data, gain=100)\n\n    def test_bias(self):\n        callback = init.ZeroBias()\n        mock = MagicMock()\n        callback.initialiser(mock)\n        self.assertTrue(mock.bias.data.zero_.call_count == 1)\n\n\nclass TestLsuv(TestCase):\n    def test_end_to_end(self):\n        import numpy as np\n        from torchbearer.callbacks.init import ZeroBias\n\n        np.random.seed(7)\n        torch.manual_seed(7)\n\n        class Flatten(torch.nn.Module):\n            def forward(self, x):\n                return x.view(x.shape[0], -1)\n\n        model = torch.nn.Sequential(\n            torch.nn.Conv2d(1,1,1),\n            Flatten(),\n            torch.nn.Linear(4, 2),\n        )\n\n        state = {torchbearer.MODEL: model}\n        data = torch.rand(2, 1, 2, 2)\n        ZeroBias(model.modules()).on_init(state)  # LSUV expects biases to be zero\n        init.LsuvInit(data).on_init(state)\n\n        correct_conv_weight = torch.FloatTensor([[[[3.2236]]]])\n        correct_linear_weight = torch.FloatTensor([[-0.3414, -0.5503, -0.4402, -0.4367],\n                                                   [0.3425, -0.0697, -0.6646, 0.4900]])\n\n        conv_weight = list(model.modules())[1].weight\n        linear_weight = list(model.modules())[3].weight\n        diff_conv = (conv_weight-correct_conv_weight) < 0.0001\n        diff_linear = (linear_weight - correct_linear_weight) < 0.0001\n        self.assertTrue(diff_conv.all().item())\n        self.assertTrue(diff_linear.all().item())\n\n    def test_break(self):\n        import numpy as np\n        from torchbearer.callbacks.init import ZeroBias\n\n        np.random.seed(7)\n        torch.manual_seed(7)\n\n        model = torch.nn.Sequential(\n            torch.nn.Conv2d(1,1,1),\n        )\n\n        with patch('torchbearer.callbacks.lsuv.LSUV.apply_weights_correction') as awc:\n            state = {torchbearer.MODEL: model}\n            data = torch.rand(2, 1, 2, 2)\n            ZeroBias(model.modules()).on_init(state)  # LSUV expects biases to be zero\n            init.LsuvInit(data, std_tol=1e-20, max_attempts=0, do_orthonorm=False).on_init(state)\n\n            # torchbearer.callbacks.lsuv.apply_weights_correction = old_fun\n            self.assertTrue(awc.call_count == 2)\n\n"""
tests/callbacks/test_label_smoothing.py,4,"b'from unittest import TestCase\n\nimport torch\n\nimport torchbearer\nfrom torchbearer.callbacks import LabelSmoothingRegularisation\n\n\nclass TestLabelSmoothingRegularisation(TestCase):\n    def test_with_long_target(self):\n        callback = LabelSmoothingRegularisation(epsilon=0.4, classes=4)\n        state = {torchbearer.TARGET: torch.Tensor([1, 3]).long()}\n\n        target = torch.Tensor([\n            [0.1, 0.7, 0.1, 0.1],\n            [0.1, 0.1, 0.1, 0.7]\n        ])\n\n        callback.on_sample(state)\n        self.assertTrue(((state[torchbearer.TARGET] - target).abs() < 0.00001).all())\n\n    def test_with_hot_target(self):\n        callback = LabelSmoothingRegularisation(epsilon=0.4)\n        state = {torchbearer.TARGET: torch.Tensor([\n            [0, 1, 0, 0],\n            [0, 0, 0, 1]\n        ]).long()}\n\n        target = torch.Tensor([\n            [0.1, 0.7, 0.1, 0.1],\n            [0.1, 0.1, 0.1, 0.7]\n        ])\n\n        callback.on_sample(state)\n        self.assertTrue(((state[torchbearer.TARGET] - target).abs() < 0.00001).all())\n'"
tests/callbacks/test_live_loss_plot.py,0,"b""from unittest import TestCase\n\nfrom mock import patch, MagicMock\n\nimport torchbearer\nfrom torchbearer.callbacks import LiveLossPlot\n\n\nclass TestLiveLossPlot(TestCase):\n    @patch('livelossplot.PlotLosses')\n    def test_on_start(self, llp_mock):\n        llp = LiveLossPlot(True, 1, True, False)\n        llp.on_start({})\n        self.assertTrue(llp_mock.call_count == 2)\n\n    def test_on_batch(self):\n        llp = LiveLossPlot(True, 1, False, False)\n        llp.batch_plt = MagicMock()\n        llp.plt = MagicMock()\n        state = {torchbearer.BATCH: 1, torchbearer.METRICS: {'test': 1}}\n        llp.on_step_training(state)\n        llp.on_step_training(state)\n\n        self.assertTrue(llp.batch_plt.update.call_count == 2)\n        self.assertTrue(llp.plt.update.call_count == 0)\n\n    def test_on_batch_steps(self):\n        llp = LiveLossPlot(True, 2, False, False)\n        llp.batch_plt = MagicMock()\n        llp.plt = MagicMock()\n        state = {torchbearer.BATCH: 1, torchbearer.METRICS: {'test': 1}}\n        llp.on_step_training(state)\n        state = {torchbearer.BATCH: 2, torchbearer.METRICS: {'test': 1}}\n        llp.on_step_training(state)\n        state = {torchbearer.BATCH: 3, torchbearer.METRICS: {'test': 1}}\n        llp.on_step_training(state)\n        state = {torchbearer.BATCH: 4, torchbearer.METRICS: {'test': 1}}\n        llp.on_step_training(state)\n\n        self.assertTrue(llp.batch_plt.draw.call_count == 2)\n        self.assertTrue(llp.plt.draw.call_count == 0)\n\n    def test_not_on_batch(self):\n        llp = LiveLossPlot(False, 10, False, False)\n        llp.batch_plt = MagicMock()\n        llp.plt = MagicMock()\n        state = {torchbearer.BATCH: 1, torchbearer.METRICS: {'test': 1}}\n        llp.on_step_training(state)\n        llp.on_step_training(state)\n\n        self.assertTrue(llp.batch_plt.update.call_count == 0)\n\n    def test_on_epoch(self):\n        llp = LiveLossPlot(False, 10, True, False)\n        llp.batch_plt = MagicMock()\n        llp.plt = MagicMock()\n        state = {torchbearer.BATCH: 1, torchbearer.METRICS: {'test': 1}}\n        llp.on_end_epoch(state)\n        llp.on_end_epoch(state)\n\n        self.assertTrue(llp.batch_plt.update.call_count == 0)\n        self.assertTrue(llp.plt.update.call_count == 2)\n\n    def test_draw_once(self):\n        llp = LiveLossPlot(True, 1, True, True)\n        llp.batch_plt = MagicMock()\n        llp.plt = MagicMock()\n        state = {torchbearer.BATCH: 1, torchbearer.METRICS: {'test': 1}}\n        llp.on_end_epoch(state)\n        llp.on_end_epoch(state)\n        llp.on_end(state)\n\n        self.assertTrue(llp.plt.draw.call_count == 1)\n        self.assertTrue(llp.batch_plt.draw.call_count == 1)\n"""
tests/callbacks/test_manifold_mixup.py,11,"b""from unittest import TestCase\nfrom mock import patch, Mock\n\nfrom torch import nn\nfrom torchbearer.callbacks.manifold_mixup import ManifoldMixup\nimport torchbearer\nimport torch\n\n\nclass TestModule(nn.Module):\n    def __init__(self):\n        super(TestModule, self).__init__()\n        self.conv = nn.Conv1d(1, 1, 1)\n        self.relu = nn.ReLU()\n        self.bn = nn.BatchNorm1d(1)\n\n    def forward(self, x):\n        x = self.conv(x.view(-1, 1, 1))\n        x = self.relu(x)\n        x = self.bn(x)\n        return x\n\n\nclass TestModule2(nn.Module):\n    def __init__(self):\n        super(TestModule2, self).__init__()\n        self.layer1 = TestModule()\n\n    def forward(self, x):\n        return self.layer1(x)\n\n\nclass TestModel(nn.Module):\n    def __init__(self):\n        super(TestModel, self).__init__()\n        self.fc1 = nn.Linear(1, 1)\n        self.conv1 = nn.Conv1d(1, 1, 1)\n        self.relu = nn.ReLU()\n        self.layer1 = TestModule()\n        self.layer2 = TestModule2()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.conv1(x.view(-1,1,1))\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return x\n\n\nclass TestManifoldMixup(TestCase):\n    def setUp(self):\n        super(TestManifoldMixup, self).setUp()\n        self.model = TestModel()\n\n    def test_depth_none(self):\n        mm = ManifoldMixup().at_depth(None)\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n\n        self.assertTrue(len(mm._layers) == 12)\n        \n    def test_depth_0(self):\n        mm = ManifoldMixup().at_depth(0)\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n\n        checks = [\n            self.model.fc1 in mm._layers,\n            self.model.conv1 in mm._layers,\n            self.model.relu in mm._layers,\n            self.model.layer1 in mm._layers,\n            self.model.layer2 in mm._layers,\n            ]\n\n        self.assertTrue(all(checks))  # Top level modules in\n        self.assertFalse(self.model.layer1.conv in mm._layers)  # Depth 1 modules not in\n\n    def test_depth_1(self):\n        mm = ManifoldMixup().at_depth(1)\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n\n        top_checks = [\n            self.model.fc1 in mm._layers,\n            self.model.conv1 in mm._layers,\n            self.model.relu in mm._layers,\n            self.model.layer1 in mm._layers,\n            self.model.layer2 in mm._layers,\n        ]\n\n        first_checks = [\n            self.model.layer1.conv in mm._layers,\n            self.model.layer1.relu in mm._layers,\n            self.model.layer1.bn in mm._layers,\n            self.model.layer2.layer1 in mm._layers,\n        ]\n\n        self.assertFalse(any(top_checks))  # Top level modules not in\n        self.assertTrue(all(first_checks))  # Depth 1 modules in\n\n    def test_depth_2(self):\n        mm = ManifoldMixup().at_depth(2)\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n\n        top_checks = [\n            self.model.fc1 in mm._layers,\n            self.model.conv1 in mm._layers,\n            self.model.relu in mm._layers,\n            self.model.layer1 in mm._layers,\n            self.model.layer2 in mm._layers,\n        ]\n\n        first_checks = [\n            self.model.layer1.conv in mm._layers,\n            self.model.layer1.relu in mm._layers,\n            self.model.layer1.bn in mm._layers,\n            self.model.layer2.layer1 in mm._layers,\n        ]\n\n        second_checks = [\n            self.model.layer2.layer1.conv in mm._layers,\n            self.model.layer2.layer1.relu in mm._layers,\n            self.model.layer2.layer1.bn in mm._layers,\n        ]\n\n        self.assertFalse(any(top_checks))  # Top level modules not in\n        self.assertFalse(any(first_checks))  # Depth 1 modules not in\n        self.assertTrue(all(second_checks))  # Depth 2 modules in\n\n    def test_for_layers(self):\n        mm = ManifoldMixup().at_depth(None).for_layers(['conv1', 'layer1_conv', 'layer2_layer1_conv'])\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n        \n        self.assertTrue(self.model.conv1 in mm._layers, self.model.layer1.conv in mm._layers and self.model.layer2.layer1.conv in mm._layers)\n        self.assertTrue(len(mm._layers) == 3)\n\n    def test_get_selected_layers(self):\n        mm = ManifoldMixup().at_depth(None).for_layers(['conv1', 'layer1_conv', 'layer2_layer1_conv'])\n        found_layers = mm.get_selected_layers(self.model)\n        self.assertTrue(len(found_layers) == 3)\n        self.assertTrue('conv1' in found_layers)\n        self.assertTrue('layer1_conv' in found_layers)\n        self.assertTrue('layer2_layer1_conv' in found_layers)\n\n    def test_layer_filter(self):\n        mm = ManifoldMixup().at_depth(None).with_layer_filter(['conv1', 'layer1_conv', 'layer2_layer1_conv'])\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n\n        self.assertFalse(self.model.conv1 in mm._layers,\n                        self.model.layer1.conv in mm._layers and self.model.layer2.layer1.conv in mm._layers)\n        self.assertTrue(len(mm._layers) == 12-3)\n\n    def test_layer_type_filter(self):\n        mm = ManifoldMixup().at_depth(None).with_layer_type_filter([nn.Conv1d])\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n\n        self.assertFalse(self.model.conv1 in mm._layers,\n                        self.model.layer1.conv in mm._layers and self.model.layer2.layer1.conv in mm._layers)\n        self.assertTrue(len(mm._layers) == 12-3)\n\n    def test_wrap(self):\n        mm = ManifoldMixup().at_depth(None).for_layers(['conv1', 'layer1_relu', 'layer2_layer1_conv'])\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n\n        self.model.conv1.mixup()\n        self.model.layer1.relu.mixup()\n        self.model.layer2.layer1.conv.mixup()\n\n        self.assertRaises(AttributeError, lambda: self.model.relu.mixup())\n\n    @patch('torchbearer.callbacks.manifold_mixup._mixup_inputs', side_effect=lambda x, _: x)\n    def test_call_mix(self, _):\n        mm = ManifoldMixup().at_depth(None).for_layers(['conv1', 'layer1_relu', 'layer2_layer1_conv'])\n\n        state = {torchbearer.MODEL: self.model}\n        mm.on_start(state)\n\n        self.model.conv1.mixup()\n        self.assertTrue(self.model.conv1.do_mixup)\n        self.model(torch.rand(3, 1))\n        self.assertFalse(self.model.conv1.do_mixup)\n\n    @patch('torchbearer.callbacks.manifold_mixup._mixup')\n    def test_on_sample(self, mix):\n        mm = ManifoldMixup().at_depth(None).for_layers(['conv1', 'layer1_relu', 'layer2_layer1_conv'])\n\n        state = {torchbearer.MODEL: self.model, torchbearer.X: torch.rand(3, 1), torchbearer.Y_TRUE: torch.rand(3, 1)}\n        mm.on_start(state)\n\n        mm.on_sample(state)\n        self.assertTrue(mix.call_count == 1)\n        \n        self.assertTrue(torchbearer.MIXUP_PERMUTATION in state)\n        self.assertTrue(torchbearer.MIXUP_LAMBDA in state)\n\n        state = {torchbearer.MODEL: self.model, torchbearer.X: torch.rand(3, 1), torchbearer.Y_TRUE: torch.rand(3, 1)}\n        mm.on_sample(state)\n        self.assertTrue(mix.call_count == 2)\n\n    @patch('torchbearer.callbacks.manifold_mixup._mixup_inputs', side_effect=lambda x, _: x)\n    def test_eval(self, mix):\n        mm = ManifoldMixup().at_depth(None).for_layers(['conv1', 'layer1_relu', 'layer2_layer1_conv'])\n\n        self.model.eval()\n        state = {torchbearer.MODEL: self.model, torchbearer.X: torch.rand(3, 1), torchbearer.Y_TRUE: torch.rand(3, 1)}\n        mm.on_start(state)\n\n        mm.on_sample(state)\n        self.model(torch.rand(3, 1))\n        self.assertTrue(mix.call_count == 0)\n\n        state = {torchbearer.MODEL: self.model, torchbearer.X: torch.rand(3, 1), torchbearer.Y_TRUE: torch.rand(3, 1)}\n        mm.on_sample(state)\n        self.model = self.model.train()\n        self.model(torch.rand(3, 1))\n        self.assertTrue(mix.call_count == 1)\n\n    def test_mixup_inputs(self):\n        from torchbearer.callbacks.manifold_mixup import _mixup_inputs\n\n        x = torch.Tensor([[1, 2], [2, 3]])\n        perm = torch.Tensor([1, 0]).long()\n        lam = torch.Tensor([0.1])\n\n        state = {torchbearer.X: x, torchbearer.MIXUP_PERMUTATION: perm, torchbearer.MIXUP_LAMBDA: lam}\n        mixed = _mixup_inputs(x, state)\n\n        self.assertFalse((mixed - torch.Tensor([[1.9, 2.9], [1.1, 2.1]]) > 1e-6).any())\n\n    @patch('torchbearer.callbacks.manifold_mixup.Beta')\n    def test_sample_lam_random(self, beta):\n        mm = ManifoldMixup()\n        sl = mm._sample_lam\n        sl()\n\n        self.assertTrue(beta.mock_calls[0][1] == (1., 1.))\n        self.assertTrue(beta.mock_calls[1][0] == '().sample')\n\n    def test_sample_lam_negative(self):\n        mm = ManifoldMixup(alpha=-1)\n        sl = mm._sample_lam\n        lam = sl()\n\n        self.assertTrue(lam == 1.)\n\n    def test_sample_lam_fixed(self):\n        mm = ManifoldMixup(lam=2.)\n        sl = mm._sample_lam\n        lam = sl()\n\n        self.assertTrue(lam == 2.)\n        \n    def test_single_to_list(self):\n        mm = ManifoldMixup()\n        sl = mm._single_to_list\n\n        item = 1.\n        self.assertTrue(sl(item) == [item, ])\n\n\n\n\n"""
tests/callbacks/test_mixup.py,13,"b""from unittest import TestCase\nfrom mock import patch, call, MagicMock\nimport torch\n\nimport torchbearer\nfrom torchbearer.callbacks import Mixup, MixupAcc\n\n\nclass TestMixupInputs(TestCase):\n    def test_input(self):\n        mixup = Mixup()\n        X = torch.Tensor([[1., 2., 3.], [6., 7., 8.]])\n        Y_true = torch.Tensor([0., 1.])\n\n        state = {\n            torchbearer.X : X,\n            torchbearer.Y_TRUE : Y_true\n        }\n\n        mixup.on_sample(state)\n        self.assertTrue((torch.eq(state[torchbearer.X], X * state[torchbearer.MIXUP_LAMBDA] + X[state[torchbearer.MIXUP_PERMUTATION], :] * (1-state[torchbearer.MIXUP_LAMBDA]))).all())\n\n    def test_alpha(self):\n        mixup = Mixup(-0.1)\n        X = torch.Tensor([[1., 2., 3.], [6., 7., 8.]])\n        Y_true = torch.Tensor([0., 1.])\n\n        state = {\n            torchbearer.X : X,\n            torchbearer.Y_TRUE : Y_true\n        }\n\n        mixup.on_sample(state)\n        self.assertTrue(state[torchbearer.MIXUP_LAMBDA] == 1.0)\n\n    def test_fixed_lambda(self):\n        mixup = Mixup(-0.1, 0.3)\n        X = torch.Tensor([[1., 2., 3.], [6., 7., 8.]])\n        Y_true = torch.Tensor([0., 1.])\n        lam = 0.3\n\n        state = {\n            torchbearer.X : X,\n            torchbearer.Y_TRUE : Y_true,\n            torchbearer.MIXUP_LAMBDA : lam\n        }\n\n        mixup.on_sample(state)\n        self.assertTrue(state[torchbearer.X][0][1] == 0.3 * 2 + 0.7 * 2 or state[torchbearer.X][0][1] == 0.3 * 2 + 0.7 * 7)\n\n    def test_target(self):\n        mixup = Mixup(-0.1)\n        X = torch.Tensor([[1., 2., 3.], [6., 7., 8.]])\n        Y_true = torch.Tensor([0., 1.])\n\n        state = {\n            torchbearer.X : X,\n            torchbearer.Y_TRUE : Y_true\n        }\n\n        mixup.on_sample(state)\n\n        self.assertTrue((state[torchbearer.Y_TRUE][0] == torch.Tensor([0., 1.])).all())\n        self.assertTrue((state[torchbearer.Y_TRUE][1] == torch.Tensor([0., 1.])).all() or (state[torchbearer.Y_TRUE][1] == torch.Tensor([1., 0.])).all())\n\n    @patch('torchbearer.callbacks.mixup.F.cross_entropy')\n    def test_loss(self, mock_cross_entropy):\n        mock_cross_entropy.return_value = 1.0\n\n        loss = Mixup.mixup_loss\n        res = loss({torchbearer.DATA: torchbearer.TRAIN_DATA, torchbearer.Y_PRED: 'test1', torchbearer.Y_TRUE: ('target1', 'target2'), torchbearer.MIXUP_LAMBDA: 0.7})\n\n        mock_cross_entropy.assert_has_calls([call('test1', 'target1'), call('test1', 'target2')])\n        self.assertTrue(res == 1.0)\n\nclass TestMixupAcc(TestCase):\n    def setUp(self):\n        self.lam = 0.8\n\n        self._state = {\n            torchbearer.Y_TRUE: (torch.LongTensor([0, 1, 2, 2, 1]), torch.LongTensor([1, 2, 1, 0, 1])),\n            torchbearer.Y_PRED: torch.FloatTensor([\n                [0.9, 0.1, 0.1],  # Correct\n                [0.1, 0.9, 0.1],  # Correct\n                [0.1, 0.1, 0.9],  # Correct\n                [0.9, 0.1, 0.1],  # Incorrect\n                [0.9, 0.1, 0.1],  # Incorrect\n            ]),\n            torchbearer.MIXUP_LAMBDA: self.lam,\n        }\n        self._targets = [0.8, 0.8, 0.8, 0.2, 0]\n        self._true_targets = [1, 1, 1, 0, 0]\n        self._metric = MixupAcc().root  # Get root node of Tree for testing\n\n    def test_train_process(self):\n        self._metric.train()\n        result = self._metric.process_train(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_val_process(self):\n        self._metric.train()\n        _state = self._state.copy()\n        _state[torchbearer.Y_TRUE] = _state[torchbearer.Y_TRUE][0]\n        result = self._metric.process_validate(_state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._true_targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_reset(self):\n        self._metric.cat_acc = MagicMock()\n        self._metric.reset({})\n        self.assertTrue(self._metric.cat_acc.reset.call_count == 1)"""
tests/callbacks/test_printer.py,0,"b""from unittest import TestCase\n\nfrom mock import patch, MagicMock\n\nimport torchbearer\nfrom torchbearer.callbacks import Tqdm, ConsolePrinter\n\n\nclass TestFormatMetrics(TestCase):\n    def test_precision(self):\n        metrics = {'test': 1.2345}\n        res = torchbearer.callbacks.printer._format_metrics(metrics, lambda x: round(x, 3))\n        self.assertEqual('test=1.234', res)\n\n    def test_string(self):\n        metrics = {'test': '1.2345'}\n        res = torchbearer.callbacks.printer._format_metrics(metrics, lambda x: round(x, 3))\n        self.assertEqual(res, 'test=1.2345')\n\n    def test_not_string(self):\n        metrics = {'test': {'hello': 2}}\n        res = torchbearer.callbacks.printer._format_metrics(metrics, lambda x: round(x, 3))\n        self.assertEqual(res, 'test={\\'hello\\': 2}')\n\n\nclass TestConsolePrinter(TestCase):\n    @patch('torchbearer.callbacks.printer.print')\n    def test_console_printer(self, mock_print):\n        state = {torchbearer.BATCH: 5, torchbearer.EPOCH: 1, torchbearer.MAX_EPOCHS: 10, torchbearer.TRAIN_STEPS: 100, torchbearer.VALIDATION_STEPS: 101, torchbearer.METRICS: {'test': 0.99456}}\n        printer = ConsolePrinter(validation_label_letter='e')\n        state[torchbearer.STEPS] = state[torchbearer.TRAIN_STEPS]\n\n        printer.on_step_training(state)\n        mock_print.assert_called_once_with('\\r1/10(t): 5/100 test=0.9946', end='')\n        mock_print.reset_mock()\n\n        printer.on_end_training(state)\n        mock_print.assert_called_once_with('\\r1/10(t): test=0.9946')\n        mock_print.reset_mock()\n\n        state[torchbearer.STEPS] = state[torchbearer.VALIDATION_STEPS]\n        printer.on_step_validation(state)\n        mock_print.assert_called_once_with('\\r1/10(e): 5/101 test=0.9946', end='')\n        mock_print.reset_mock()\n\n        printer.on_end_validation(state)\n        mock_print.assert_called_once_with('\\r1/10(e): test=0.9946')\n        mock_print.reset_mock()\n\n\nclass TestTqdm(TestCase):\n    def test_tqdm(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.MAX_EPOCHS: 10, torchbearer.TRAIN_STEPS: 100, torchbearer.VALIDATION_STEPS: 101, torchbearer.METRICS: {'test': 0.99456}}\n        tqdm = Tqdm(validation_label_letter='e')\n        tqdm.tqdm_module = MagicMock()\n        mock_tqdm = tqdm.tqdm_module\n        state[torchbearer.STEPS] = state[torchbearer.TRAIN_STEPS]\n\n        tqdm.on_start_training(state)\n        mock_tqdm.assert_called_once_with(total=100, desc='1/10(t)')\n\n        tqdm.on_step_training(state)\n        mock_tqdm.return_value.set_postfix_str.assert_called_once_with('test=0.9946')\n        mock_tqdm.return_value.update.assert_called_once_with(1)\n        mock_tqdm.return_value.set_postfix_str.reset_mock()\n\n        tqdm.on_end_training(state)\n        mock_tqdm.return_value.set_postfix_str.assert_called_once_with('test=0.9946')\n        self.assertEqual(mock_tqdm.return_value.close.call_count, 1)\n\n        mock_tqdm.reset_mock()\n        mock_tqdm.return_value.set_postfix_str.reset_mock()\n        mock_tqdm.return_value.update.reset_mock()\n        mock_tqdm.return_value.close.reset_mock()\n\n        state[torchbearer.STEPS] = state[torchbearer.VALIDATION_STEPS]\n        tqdm.on_start_validation(state)\n        mock_tqdm.assert_called_once_with(total=101, desc='1/10(e)')\n\n        tqdm.on_step_validation(state)\n        mock_tqdm.return_value.set_postfix_str.assert_called_once_with('test=0.9946')\n        mock_tqdm.return_value.update.assert_called_once_with(1)\n        mock_tqdm.return_value.set_postfix_str.reset_mock()\n\n        tqdm.on_end_validation(state)\n        mock_tqdm.return_value.set_postfix_str.assert_called_once_with('test=0.9946')\n        self.assertEqual(mock_tqdm.return_value.close.call_count, 1)\n\n    def test_tqdm_custom_args(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.MAX_EPOCHS: 10, torchbearer.TRAIN_STEPS: 100,\n                 torchbearer.VALIDATION_STEPS: 101, torchbearer.METRICS: {'test': 10}}\n        state[torchbearer.HISTORY] = [dict(state[torchbearer.METRICS], train_steps=None, validation_steps=None)]\n        tqdm = Tqdm(ascii=True)\n        state[torchbearer.STEPS] = state[torchbearer.TRAIN_STEPS]\n\n        tqdm.tqdm_module = MagicMock()\n        mock_tqdm = tqdm.tqdm_module\n\n        tqdm.on_start_training(state)\n        mock_tqdm.assert_called_once_with(total=100, desc='1/10(t)', ascii=True)\n\n        tqdm = Tqdm(on_epoch=True, ascii=True)\n\n        tqdm.tqdm_module = MagicMock()\n        mock_tqdm = tqdm.tqdm_module\n\n        tqdm.on_start(state)\n        mock_tqdm.assert_called_once_with(initial=1, total=10, ascii=True)\n\n    def test_tqdm_on_epoch(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.MAX_EPOCHS: 10, torchbearer.HISTORY: [0, {'train_steps': 1, 'validation_steps': None, 'test': 0.99456}],\n                 torchbearer.METRICS: {'test': 0.99456}}\n        tqdm = Tqdm(validation_label_letter='e', on_epoch=True)\n        tqdm.tqdm_module = MagicMock()\n        mock_tqdm = tqdm.tqdm_module\n\n        tqdm.on_start(state)\n        mock_tqdm.assert_called_once_with(initial=2, total=10)\n        mock_tqdm.return_value.set_postfix_str.assert_called_once_with('test=0.9946')\n        mock_tqdm.return_value.update.assert_called_once_with(1)\n        mock_tqdm.return_value.set_postfix_str.reset_mock()\n        mock_tqdm.return_value.update.reset_mock()\n\n        tqdm.on_end_epoch(state)\n        mock_tqdm.return_value.set_postfix_str.assert_called_once_with('test=0.9946')\n        mock_tqdm.return_value.update.assert_called_once_with(1)\n        mock_tqdm.return_value.set_postfix_str.reset_mock()\n\n        tqdm.on_end(state)\n        mock_tqdm.return_value.set_postfix_str.assert_called_once_with('test=0.9946')\n        self.assertEqual(mock_tqdm.return_value.close.call_count, 1)\n\n    def test_tqdm_keys(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.MAX_EPOCHS: 10, torchbearer.HISTORY: [0, {'test': 0.99456}],\n                 torchbearer.METRICS: {'test': 0.99456}}\n        tqdm = Tqdm(validation_label_letter='e', on_epoch=True)\n        tqdm.tqdm_module = MagicMock()\n\n        tqdm.on_start(state)\n\n    @patch('torchbearer.magics.is_notebook')\n    def test_tqdm_module_init_notebook(self, mock_is_notebook):\n        from tqdm import tqdm_notebook\n        mock_is_notebook.return_value = True\n        tqdm = Tqdm(validation_label_letter='e', on_epoch=True)\n        self.assertTrue(tqdm.tqdm_module == tqdm_notebook)\n\n    @patch('torchbearer.magics.is_notebook')\n    def test_tqdm_module_init_not_notebook(self, mock_is_notebook):\n        from tqdm import tqdm as base_tqdm\n        mock_is_notebook.return_value = False\n        tqdm = Tqdm(validation_label_letter='e', on_epoch=True)\n        self.assertTrue(tqdm.tqdm_module == base_tqdm)\n"""
tests/callbacks/test_pycm.py,1,"b""import sys\nfrom unittest import TestCase\n\nfrom mock import MagicMock, patch, ANY\n\nimport torchbearer\nfrom torchbearer.callbacks import PyCM\n\nimport matplotlib.pyplot as plt  # Import so that it can be mocked\nplt.ioff()\n\n\nclass TestHandlers(TestCase):\n    @patch('matplotlib.pyplot')\n    def test_to_pyplot(self, mock_pyplot):\n        if sys.version_info[0] >= 3:\n            import pycm\n\n            handler = torchbearer.callbacks.pycm._to_pyplot(True, 'test {epoch}')\n\n            y_true = [2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2]\n            y_pred = [0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]\n            cm = pycm.ConfusionMatrix(y_true, y_pred)\n            handler(cm, {torchbearer.EPOCH: 3})\n\n            self.assertTrue(mock_pyplot.imshow.call_args[0][0].max() == 1)\n            mock_pyplot.title.assert_called_once_with('test 3')\n\n            handler = torchbearer.callbacks.pycm._to_pyplot(False)\n\n            y_true = [2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2]\n            y_pred = [0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]\n            cm = pycm.ConfusionMatrix(y_true, y_pred)\n            handler(cm, {})\n\n            self.assertTrue(mock_pyplot.imshow.call_args[0][0].max() > 1)\n\n\nclass TestPyCM(TestCase):\n    def test_exception(self):\n        if sys.version_info[0] < 3:\n            self.assertRaises(Exception, PyCM)\n\n    @patch('torchbearer.callbacks.pycm.EpochLambda')\n    def test_make_cm(self, emock_lambda):\n        if sys.version_info[0] >= 3:\n            with patch('pycm.ConfusionMatrix') as confusion_mocktrix:\n                confusion_mocktrix.return_value = 'test'\n                handler = MagicMock()\n                callback = PyCM(test=10).with_handler(handler)\n                state = {torchbearer.METRIC_LIST: None}\n\n                callback._add_metric(state)\n                emock_lambda.assert_called_once_with('pycm', ANY, False)\n\n                make_cm = emock_lambda.call_args[0][1]\n\n                import torch\n\n                y_pred = torch.rand(5, 2) / 2\n                y_pred[:, 1] = 1\n                y_true = MagicMock()\n\n                make_cm(y_pred, y_true)\n\n                self.assertTrue(y_true.cpu.call_count == 1)\n                self.assertTrue(y_true.cpu().numpy.call_count == 1)\n                confusion_mocktrix.assert_called_once_with(y_true.cpu().numpy(), ANY, test=10)\n                self.assertTrue(confusion_mocktrix.call_args[0][1].sum() == 5)\n                handler.assert_called_once_with('test', state)\n\n    def test_on_train(self):\n        if sys.version_info[0] >= 3:\n            callback = PyCM().on_train()\n            state = {torchbearer.METRIC_LIST: None}\n            callback.on_start_training(state)\n            self.assertTrue(state[torchbearer.METRIC_LIST] is not None)\n\n    def test_on_val(self):\n        if sys.version_info[0] >= 3:\n            callback = PyCM().on_val()\n            state = {torchbearer.METRIC_LIST: None, torchbearer.DATA: torchbearer.VALIDATION_DATA}\n            callback.on_start_validation(state)\n            self.assertTrue(state[torchbearer.METRIC_LIST] is not None)\n\n    def test_on_test(self):\n        if sys.version_info[0] >= 3:\n            callback = PyCM().on_test()\n            state = {torchbearer.METRIC_LIST: None, torchbearer.DATA: torchbearer.TEST_DATA}\n            callback.on_start_validation(state)\n            self.assertTrue(state[torchbearer.METRIC_LIST] is not None)\n\n    def test_with_handler(self):\n        if sys.version_info[0] >= 3:\n            callback = PyCM()\n            callback.with_handler('test')\n            self.assertTrue('test' in callback._handlers)\n\n    def test_to_state(self):\n        if sys.version_info[0] >= 3:\n            callback = PyCM()\n            callback.to_state('test')\n            out = {}\n            callback._handlers[0]('cm', out)\n            self.assertTrue('test' in out)\n            self.assertTrue(out['test'] == 'cm')\n\n    @patch('torchbearer.callbacks.pycm.print')\n    def test_to_console(self, mock_print):\n        if sys.version_info[0] >= 3:\n            callback = PyCM()\n            callback.to_console()\n            callback._handlers[0]('cm', {})\n            mock_print.assert_called_once_with('cm')\n\n    def test_to_file(self):\n        if sys.version_info[0] >= 3:\n            callback = PyCM()\n            callback.to_pycm_file('test {epoch}')\n            cm = MagicMock()\n            callback._handlers[0](cm, {torchbearer.EPOCH: 1})\n\n            cm.save_stat.assert_called_once_with('test 1')\n\n            callback = PyCM()\n            callback.to_html_file('test {epoch}')\n            cm = MagicMock()\n            callback._handlers[0](cm, {torchbearer.EPOCH: 2})\n\n            cm.save_html.assert_called_once_with('test 2')\n\n            callback = PyCM()\n            callback.to_csv_file('test {epoch}')\n            cm = MagicMock()\n            callback._handlers[0](cm, {torchbearer.EPOCH: 3})\n\n            cm.save_csv.assert_called_once_with('test 3')\n\n            callback = PyCM()\n            callback.to_obj_file('test {epoch}')\n            cm = MagicMock()\n            callback._handlers[0](cm, {torchbearer.EPOCH: 4})\n\n            cm.save_obj.assert_called_once_with('test 4')\n\n    @patch('torchbearer.callbacks.pycm._to_pyplot')\n    def test_to_pyplot(self, mock_to_pyplot):\n        if sys.version_info[0] >= 3:\n            PyCM().to_pyplot(True, 'test', 'test2')\n            mock_to_pyplot.assert_called_once_with(normalize=True, title='test', cmap='test2')"""
tests/callbacks/test_sample_pairing.py,2,"b""from unittest import TestCase\nfrom mock import patch, call\nimport torch\n\nimport torchbearer\nfrom torchbearer.callbacks import SamplePairing\n\n\nclass TestSamplePairing(TestCase):\n    @patch('torchbearer.callbacks.sample_pairing.torch')\n    def test_on_sample(self, mock_torch):\n        mock_torch.randperm.return_value = torch.Tensor([3, 2, 1, 0]).long()\n        callback = SamplePairing(policy=SamplePairing.default_policy(0, 10, 10, 10))\n\n        state = {torchbearer.INPUT: torch.Tensor([0.25, 0.5, 0.75, 1]), torchbearer.EPOCH: 0}\n        callback.on_sample(state)\n        self.assertTrue(((state[torchbearer.INPUT] - 0.625).abs() < 0.0001).all())\n\n    def test_default_policy(self):\n        policy = SamplePairing.default_policy(10, 80, 3, 2)\n\n        state = {torchbearer.EPOCH: 0}\n        self.assertFalse(policy(state))\n        state = {torchbearer.EPOCH: 10}\n        self.assertTrue(policy(state))\n\n        for i in range(2):\n            state = {torchbearer.EPOCH: 11 + i}\n            policy(state)\n        state = {torchbearer.EPOCH: 14}\n        self.assertFalse(policy(state))\n        state = {torchbearer.EPOCH: 15}\n        self.assertFalse(policy(state))\n        state = {torchbearer.EPOCH: 16}\n        self.assertTrue(policy(state))\n"""
tests/callbacks/test_tensor_board.py,42,"b'import os\nfrom unittest import TestCase\nimport warnings\n\nimport torch\nimport torch.nn as nn\nfrom mock import patch, Mock, ANY, MagicMock\n\nimport torchbearer\nfrom torchbearer.callbacks import TensorBoard, TensorBoardImages, TensorBoardProjector, TensorBoardText\n\n\nclass TestTensorBoard(TestCase):\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_add_metric_single(self, _, __, writer):\n        mock_fn = MagicMock()\n\n        def fn_test(ex, types):\n            def fn_test_1(tag, metric, *args, **kwargs):\n                if type(metric) in types:\n                    raise ex\n                else:\n                    mock_fn(tag, metric)\n            return fn_test_1\n\n        tb = TensorBoard()\n        state = {torchbearer.METRICS: {\'test\': 1, \'test2\': [1, 2, 3], \'test3\': [[1], [2], [3, 4]]}}\n        tb.add_metric(fn_test(NotImplementedError, [list]), \'single\', state[torchbearer.METRICS][\'test\'])\n\n        self.assertTrue(mock_fn.call_args_list[0][0] == (\'single\', 1))\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_add_metric_list(self, _, __, writer):\n        mock_fn = MagicMock()\n\n        def fn_test(ex, types):\n            def fn_test_1(tag, metric, *args, **kwargs):\n                if type(metric) in types:\n                    raise ex\n                else:\n                    mock_fn(tag, metric)\n            return fn_test_1\n\n        tb = TensorBoard()\n        state = {torchbearer.METRICS: {\'test\': 1, \'test2\': [1, 2, 3], \'test3\': [[1], [2], [3, 4]]}}\n        tb.add_metric(fn_test(NotImplementedError, [list]), \'single\', state[torchbearer.METRICS][\'test2\'])\n\n        self.assertTrue(mock_fn.call_args_list[0][0] == (\'single_0\', 1))\n        self.assertTrue(mock_fn.call_args_list[1][0] == (\'single_1\', 2))\n        self.assertTrue(mock_fn.call_args_list[2][0] == (\'single_2\', 3))\n\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_add_metric_list_of_list(self, _, __, writer):\n        mock_fn = MagicMock()\n\n        def fn_test(ex, types):\n            def fn_test_1(tag, metric, *args, **kwargs):\n                if type(metric) in types:\n                    raise ex\n                else:\n                    mock_fn(tag, metric)\n            return fn_test_1\n\n        tb = TensorBoard()\n        state = {torchbearer.METRICS: {\'test\': 1, \'test2\': [1, 2, 3], \'test3\': [[1], 2, [3, 4]]}}\n        tb.add_metric(fn_test(NotImplementedError, [list]), \'single\', state[torchbearer.METRICS][\'test3\'])\n\n        self.assertTrue(mock_fn.call_args_list[0][0] == (\'single_0_0\', 1))\n        self.assertTrue(mock_fn.call_args_list[1][0] == (\'single_1\', 2))\n        self.assertTrue(mock_fn.call_args_list[2][0] == (\'single_2_0\', 3))\n        self.assertTrue(mock_fn.call_args_list[3][0] == (\'single_2_1\', 4))\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_add_metric_dict(self, _, __, writer):\n        mock_fn = MagicMock()\n\n        def fn_test(ex, types):\n            def fn_test_1(tag, metric, *args, **kwargs):\n                if type(metric) in types:\n                    raise ex\n                else:\n                    mock_fn(tag, metric)\n            return fn_test_1\n\n        tb = TensorBoard()\n        state = {torchbearer.METRICS: {\'test\': {\'key1\': 2, \'key2\': 3}}}\n        tb.add_metric(fn_test(NotImplementedError, [list, dict]), \'single\', state[torchbearer.METRICS][\'test\'])\n\n        call_args = list(mock_fn.call_args_list)\n        call_args.sort()\n        self.assertTrue(call_args[0][0] == (\'single_key1\', 2))\n        self.assertTrue(call_args[1][0] == (\'single_key2\', 3))\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_add_metric_dict_and_list(self, _, __, writer):\n        mock_fn = MagicMock()\n\n        def fn_test(ex, types):\n            def fn_test_1(tag, metric, *args, **kwargs):\n                if type(metric) in types:\n                    raise ex\n                else:\n                    mock_fn(tag, metric)\n            return fn_test_1\n\n        tb = TensorBoard()\n        state = {torchbearer.METRICS: {\'test\': {\'key1\': 2, \'key2\': [3, 4]}}}\n        tb.add_metric(fn_test(NotImplementedError, [list, dict]), \'single\', state[torchbearer.METRICS][\'test\'])\n\n        call_args = list(mock_fn.call_args_list)\n        call_args.sort()\n        self.assertTrue(call_args[0][0] == (\'single_key1\', 2))\n        self.assertTrue(call_args[1][0] == (\'single_key2_0\', 3))\n        self.assertTrue(call_args[2][0] == (\'single_key2_1\', 4))\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_add_metric_fail_iterable(self, _, __, writer):\n        mock_fn = MagicMock()\n\n        def fn_test(ex, types):\n            def fn_test_1(tag, metric, *args, **kwargs):\n                if type(metric) in types:\n                    raise ex\n                else:\n                    mock_fn(tag, metric)\n            return fn_test_1\n\n        tb = TensorBoard()\n        state = {torchbearer.METRICS: {\'test\': 0.1}}\n        with warnings.catch_warnings(record=True) as w:\n            tb.add_metric(fn_test(NotImplementedError, [list, dict, float]), \'single\', state[torchbearer.METRICS][\'test\'])\n            self.assertTrue(len(w) == 1)\n\n        call_args = list(mock_fn.call_args_list)\n        call_args.sort()\n        self.assertTrue(len(call_args) == 0)\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_add_metric_fail(self, _, __, writer):\n        mock_fn = MagicMock()\n\n        def fn_test(ex, types):\n            def fn_test_1(tag, metric, *args, **kwargs):\n                if type(metric) in types:\n                    raise ex\n                else:\n                    mock_fn(tag, metric)\n            return fn_test_1\n\n        tb = TensorBoard()\n        state = {torchbearer.METRICS: {\'test\': 0.1}}\n        with warnings.catch_warnings(record=True) as w:\n            tb.add_metric(fn_test(Exception, [float]), \'single\', state[torchbearer.METRICS][\'test\'])\n            self.assertTrue(len(w) == 1)\n\n        call_args = list(mock_fn.call_args_list)\n        call_args.sort()\n        self.assertTrue(len(call_args) == 0)\n\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'visdom.Visdom\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_get_writer_oserror(self, mockdirs, isdir, _, __):\n        from torchbearer.callbacks.tensor_board import get_writer\n        import sys\n\n        isdir.return_value = True\n        mockdirs.side_effect = OSError\n\n        self.assertRaises(OSError, lambda: get_writer(\'test\', \'nothing\', visdom=True))\n        if sys.version_info[0] >= 3:\n            mockdirs.assert_called_once_with(\'test\', exist_ok=True)\n        else:\n            mockdirs.assert_called_once_with(\'test\')\n\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'visdom.Visdom\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.path.isdir\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    def test_get_writer_oserror_eexist(self, mockdirs, isdir, _, __):\n        from torchbearer.callbacks.tensor_board import get_writer\n        import sys\n        import errno\n\n        class MyError(OSError):\n            def __init__(self):\n                self.errno = errno.EEXIST\n\n        isdir.return_value = True\n        mockdirs.side_effect = MyError\n\n        get_writer(\'test\', \'nothing\', visdom=True)\n        if sys.version_info[0] >= 3:\n            mockdirs.assert_called_once_with(\'test\', exist_ok=True)\n        else:\n            mockdirs.assert_called_once_with(\'test\')\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_log_dir(self, mock_board, _):\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoard(write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_end(state)\n\n        mock_board.assert_called_once_with(log_dir=os.path.join(\'./logs\', \'Sequential_torchbearer\'))\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_log_dir_visdom(self, mock_visdom, mock_writer, _):\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n        mock_writer.__delete__ = Mock()\n\n        tboard = TensorBoard(visdom=True, write_epoch_metrics=False)\n\n        tboard.on_start(state)\n        tboard.on_end(state)\n\n        self.assertEqual(mock_visdom.call_count, 1)\n        self.assertTrue(mock_visdom.call_args[1][\'log_to_filename\'] == os.path.join(\'./logs\', \'Sequential_torchbearer\',\n                                                                                    \'log.log\'))\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_batch_log_dir(self, mock_board, _):\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.EPOCH: 0}\n\n        tboard = TensorBoard(write_batch_metrics=True, write_graph=False, write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_end_epoch(state)\n        tboard.on_end(state)\n\n        mock_board.assert_called_with(log_dir=os.path.join(\'./logs\', \'Sequential_torchbearer\', \'epoch-0\'))\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_batch_log_dir_visdom(self, mock_visdom, mock_writer, _):\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 0, torchbearer.METRICS: {\'test\': 1}, torchbearer.BATCH: 0}\n\n        tboard = TensorBoard(visdom=True, write_batch_metrics=True, write_graph=False, write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_end_epoch(state)\n        tboard.on_end(state)\n\n        self.assertTrue(mock_visdom.call_args[1][\'log_to_filename\'] == os.path.join(\'./logs\', \'Sequential_torchbearer\', \'epoch\', \'log.log\'))\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    @patch(\'torch.rand\')\n    def test_write_graph(self, mock_rand, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_graph = Mock()\n        mock_rand.return_value = 1\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.X: torch.zeros(1, 1, 9, 9)}\n\n        tboard = TensorBoard(write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_sample(state)\n        tboard.on_end(state)\n\n        mock_rand.assert_called_once_with(state[torchbearer.X].size(), requires_grad=False)\n        self.assertEqual(mock_board.return_value.add_graph.call_count, 1)\n        self.assertEqual(str(state[torchbearer.MODEL]), str(mock_board.return_value.add_graph.call_args_list[0][0][0]))\n        self.assertNotEqual(state[torchbearer.MODEL], mock_board.return_value.add_graph.call_args_list[0][0][0])\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_writer_closed_on_end(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.close = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoard(write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_end({})\n        self.assertEqual(mock_board.return_value.close.call_count, 1)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_writer_closed_on_end_visdom(self, mock_visdom, mock_writer, _):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.close = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoard(visdom=True, write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_end({})\n        self.assertEqual(mock_writer.return_value.close.call_count, 1)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_batch_writer_closed_on_end_epoch(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.close = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.EPOCH: 0}\n\n        tboard = TensorBoard(write_batch_metrics=True, write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_end_epoch({})\n        self.assertEqual(mock_board.return_value.close.call_count, 1)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_batch_writer_closed_on_end_epoch_visdom(self, mock_visdom, mock_writer, _):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.close = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.EPOCH: 0}\n\n        tboard = TensorBoard(visdom=True, write_batch_metrics=True, write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_end_epoch({})\n        tboard.on_end(state)\n        self.assertTrue(mock_writer.return_value.close.call_count == 2)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_batch_metrics(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_scalar = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 0, torchbearer.METRICS: {\'test\': 1}, torchbearer.BATCH: 0}\n\n        tboard = TensorBoard(write_batch_metrics=True, write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_step_training(state)\n        mock_board.return_value.add_scalar.assert_called_once_with(\'batch/test\', 1, 0)\n        mock_board.return_value.add_scalar.reset_mock()\n        tboard.on_step_validation(state)\n        mock_board.return_value.add_scalar.assert_called_once_with(\'batch/test\', 1, 0)\n        tboard.on_end_epoch(state)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_batch_metrics_visdom(self, mock_visdom, mock_writer, _):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.add_scalar = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 0, torchbearer.METRICS: {\'test\': 1}, torchbearer.BATCH: 0, torchbearer.TRAIN_STEPS: 0}\n\n        tboard = TensorBoard(visdom=True, write_batch_metrics=True, write_epoch_metrics=False)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_step_training(state)\n        mock_writer.return_value.add_scalar.assert_called_once_with(\'test\', 1, 0, main_tag=\'batch\')\n        mock_writer.return_value.add_scalar.reset_mock()\n        tboard.on_step_validation(state)\n        mock_writer.return_value.add_scalar.assert_called_once_with(\'test\', 1, 0, main_tag=\'batch\')\n        tboard.on_end_epoch(state)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_epoch_metrics(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_scalar = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.EPOCH: 0,\n                 torchbearer.METRICS: {\'test\': 1}}\n\n        tboard = TensorBoard(write_batch_metrics=False, write_epoch_metrics=True)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_end_epoch(state)\n        mock_board.return_value.add_scalar.assert_called_once_with(\'epoch/test\', 1, 0)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_epoch_metrics_visdom(self, mock_visdom, mock_writer, _):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.add_scalar = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.EPOCH: 0,\n                 torchbearer.METRICS: {\'test\': 1}}\n\n        tboard = TensorBoard(visdom=True, write_batch_metrics=False, write_epoch_metrics=True)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_end_epoch(state)\n        mock_writer.return_value.add_scalar.assert_called_once_with(\'test\', 1, 0, main_tag=\'epoch\')\n        tboard.on_end(state)\n\n\nclass TestTensorBoardImages(TestCase):\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_log_dir(self, mock_board, _):\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(log_dir=\'./test\', comment=\'torchbearer\')\n        tboard.on_start(state)\n        tboard.on_end(state)\n\n        mock_board.assert_called_once_with(log_dir=os.path.join(\'./test\', \'Sequential_torchbearer\'))\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_log_dir_visdom(self, mock_visdom, mock_writer, _):\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n        mock_writer.__delete__ = Mock()\n\n        tboard = TensorBoardImages(visdom=True, log_dir=\'./test\', comment=\'torchbearer\')\n\n        tboard.on_start(state)\n        tboard.on_end(state)\n\n        self.assertEqual(mock_visdom.call_count, 1)\n        self.assertTrue(mock_visdom.call_args[1][\'log_to_filename\'] == os.path.join(\'./test\', \'Sequential_torchbearer\',\n                                                                                    \'log.log\'))\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_writer_closed_on_end(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.close = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages()\n        tboard.on_start(state)\n        tboard.on_end({})\n        self.assertEqual(mock_board.return_value.close.call_count, 1)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_writer_closed_on_end_visdom_visdom(self, mock_visdom, mock_writer, _):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.close = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoard(visdom=True)\n        tboard.on_start(state)\n        tboard.on_end({})\n        self.assertEqual(mock_writer.return_value.close.call_count, 1)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_simple_case(self, mock_board, mock_grid, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(name=\'test\', key=\'x\', write_each_epoch=False, num_images=18, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_board.return_value.add_image.assert_called_once_with(\'test\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == state[\'x\'].size())\n        tboard.on_end({})\n\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_simple_case_visdom(self, mock_visdom, mock_writer, _, mock_grid):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(visdom=True, name=\'test\', key=\'x\', write_each_epoch=False, num_images=18, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_writer.return_value.add_image.assert_called_once_with(\'test1\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == state[\'x\'].size())\n        tboard.on_end({})\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_multi_batch(self, mock_board, mock_grid, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(name=\'test\', key=\'x\', write_each_epoch=False, num_images=36, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_board.return_value.add_image.assert_called_once_with(\'test\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == torch.ones(36, 3, 10, 10).size())\n        tboard.on_end({})\n\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_multi_batch_visdom(self, mock_visdom, mock_writer, _, mock_grid):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(visdom=True, name=\'test\', key=\'x\', write_each_epoch=False, num_images=36, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_writer.return_value.add_image.assert_called_once_with(\'test1\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == torch.ones(36, 3, 10, 10).size())\n        tboard.on_end({})\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_multi_epoch(self, mock_board, mock_grid, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(name=\'test\', key=\'x\', write_each_epoch=True, num_images=36, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n        tboard.on_end_epoch(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_board.return_value.add_image.assert_called_once_with(\'test\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == torch.ones(36, 3, 10, 10).size())\n        tboard.on_end({})\n\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_multi_epoch_visdom(self, mock_visdom, mock_writer, _, mock_grid):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(visdom=True, name=\'test\', key=\'x\', write_each_epoch=True, num_images=36, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n        tboard.on_end_epoch(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_writer.return_value.add_image.assert_called_once_with(\'test1\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == torch.ones(36, 3, 10, 10).size())\n        tboard.on_end({})\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_single_channel(self, mock_board, mock_grid, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(name=\'test\', key=\'x\', write_each_epoch=True, num_images=18, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_board.return_value.add_image.assert_called_once_with(\'test\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == torch.ones(18, 1, 10, 10).size())\n        tboard.on_end({})\n\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_single_channel_visdom(self, mock_visdom, mock_writer, _, mock_grid):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(visdom=True, name=\'test\', key=\'x\', write_each_epoch=True, num_images=18, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_writer.return_value.add_image.assert_called_once_with(\'test1\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == torch.ones(18, 1, 10, 10).size())\n        tboard.on_end({})\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_odd_batches(self, mock_board, mock_grid, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(name=\'test\', key=\'x\', write_each_epoch=True, num_images=40, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n        tboard.on_step_validation(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_board.return_value.add_image.assert_called_once_with(\'test\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == torch.ones(40, 3, 10, 10).size())\n        tboard.on_end({})\n\n    @patch(\'torchvision.utils.make_grid\')\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_odd_batches_visdom(self, mock_visdom, mock_writer, _, mock_grid):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.add_image = Mock()\n\n        mock_grid.return_value = 10\n\n        state = {\'x\': torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 1,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardImages(visdom=True, name=\'test\', key=\'x\', write_each_epoch=True, num_images=40, nrow=9, padding=3,\n                                   normalize=True, norm_range=\'tmp\', scale_each=True, pad_value=1)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n        tboard.on_step_validation(state)\n        tboard.on_step_validation(state)\n\n        mock_grid.assert_called_once_with(ANY, nrow=9, padding=3, normalize=True, range=\'tmp\', scale_each=True,\n                                          pad_value=1)\n        mock_writer.return_value.add_image.assert_called_once_with(\'test1\', 10, 1)\n        self.assertTrue(mock_grid.call_args[0][0].size() == torch.ones(40, 3, 10, 10).size())\n        tboard.on_end({})\n\n\nclass TestTensorBoardProjector(TestCase):\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_log_dir(self, mock_board, _):\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardProjector(log_dir=\'./test\', comment=\'torchbearer\')\n        tboard.on_start(state)\n        tboard.on_end(state)\n\n        mock_board.assert_called_once_with(log_dir=os.path.join(\'./test\', \'Sequential_torchbearer\'))\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_writer_closed_on_end(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.close = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3))}\n\n        tboard = TensorBoardProjector()\n        tboard.on_start(state)\n        tboard.on_end({})\n        self.assertEqual(mock_board.return_value.close.call_count, 1)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_simple_case(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_embedding = Mock()\n\n        state = {torchbearer.X: torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 0,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.Y_TRUE: torch.ones(18),\n                 torchbearer.BATCH: 0}\n\n        tboard = TensorBoardProjector(num_images=18, avg_data_channels=False, write_data=False,\n                                      features_key=torchbearer.Y_TRUE)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n\n        mock_board.return_value.add_embedding.assert_called_once_with(ANY, metadata=ANY, label_img=ANY, tag=\'features\',\n                                                                      global_step=0)\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[0][0].size() == state[torchbearer.Y_TRUE].unsqueeze(\n                1).size())\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'metadata\'].size() == state[torchbearer.Y_TRUE].size())\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'label_img\'].size() == state[torchbearer.X].size())\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_multi_epoch(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_embedding = Mock()\n\n        state = {torchbearer.X: torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 0,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.Y_TRUE: torch.ones(18),\n                 torchbearer.BATCH: 0}\n\n        tboard = TensorBoardProjector(num_images=18, avg_data_channels=False, write_data=False,\n                                      features_key=torchbearer.Y_TRUE)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n\n        mock_board.return_value.add_embedding.assert_called_once_with(ANY, metadata=ANY, label_img=ANY, tag=\'features\',\n                                                                      global_step=0)\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[0][0].size() == state[torchbearer.Y_TRUE].unsqueeze(\n                1).size())\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'metadata\'].size() == state[torchbearer.Y_TRUE].size())\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'label_img\'].size() == state[torchbearer.X].size())\n\n        tboard.on_end_epoch({})\n        mock_board.return_value.add_embedding.reset_mock()\n\n        tboard.on_step_validation(state)\n\n        mock_board.return_value.add_embedding.assert_called_once_with(ANY, metadata=ANY, label_img=ANY, tag=\'features\',\n                                                                      global_step=0)\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[0][0].size() == state[torchbearer.Y_TRUE].unsqueeze(\n                1).size())\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'metadata\'].size() == state[torchbearer.Y_TRUE].size())\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'label_img\'].size() == state[torchbearer.X].size())\n        tboard.on_end({})\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_multi_batch(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_embedding = Mock()\n\n        state = {torchbearer.X: torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 0,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.Y_TRUE: torch.ones(18),\n                 torchbearer.BATCH: 0}\n\n        tboard = TensorBoardProjector(num_images=45, avg_data_channels=False, write_data=False,\n                                      features_key=torchbearer.Y_TRUE)\n\n        tboard.on_start(state)\n        for i in range(3):\n            state[torchbearer.BATCH] = i\n            tboard.on_step_validation(state)\n\n        mock_board.return_value.add_embedding.assert_called_once_with(ANY, metadata=ANY, label_img=ANY, tag=\'features\',\n                                                                      global_step=0)\n        self.assertTrue(mock_board.return_value.add_embedding.call_args[0][0].size() == torch.Size([45, 1]))\n        self.assertTrue(mock_board.return_value.add_embedding.call_args[1][\'metadata\'].size() == torch.Size([45]))\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'label_img\'].size() == torch.Size([45, 3, 10, 10]))\n        tboard.on_end({})\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_multi_batch_data(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_embedding = Mock()\n\n        state = {torchbearer.X: torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 0,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.Y_TRUE: torch.ones(18),\n                 torchbearer.BATCH: 0}\n\n        tboard = TensorBoardProjector(num_images=45, avg_data_channels=False, write_data=True, write_features=False)\n\n        tboard.on_start(state)\n        for i in range(3):\n            state[torchbearer.BATCH] = i\n            tboard.on_step_validation(state)\n\n        mock_board.return_value.add_embedding.assert_called_once_with(ANY, metadata=ANY, label_img=ANY, tag=\'data\',\n                                                                      global_step=-1)\n        self.assertTrue(mock_board.return_value.add_embedding.call_args[0][0].size() == torch.Size([45, 300]))\n        self.assertTrue(mock_board.return_value.add_embedding.call_args[1][\'metadata\'].size() == torch.Size([45]))\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'label_img\'].size() == torch.Size([45, 3, 10, 10]))\n        tboard.on_end({})\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_channel_average(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_embedding = Mock()\n\n        state = {torchbearer.X: torch.ones(18, 3, 10, 10), torchbearer.EPOCH: 0,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.Y_TRUE: torch.ones(18),\n                 torchbearer.BATCH: 0}\n\n        tboard = TensorBoardProjector(num_images=18, avg_data_channels=True, write_data=True, write_features=False)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n\n        mock_board.return_value.add_embedding.assert_called_once_with(ANY, metadata=ANY, label_img=ANY, tag=\'data\',\n                                                                      global_step=-1)\n        self.assertTrue(mock_board.return_value.add_embedding.call_args[0][0].size() == torch.Size([18, 100]))\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'metadata\'].size() == state[torchbearer.Y_TRUE].size())\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'label_img\'].size() == state[torchbearer.X].size())\n        tboard.on_end({})\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_no_channels(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_embedding = Mock()\n\n        state = {torchbearer.X: torch.ones(18, 10, 10), torchbearer.EPOCH: 0,\n                 torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)), torchbearer.Y_TRUE: torch.ones(18),\n                 torchbearer.BATCH: 0}\n\n        tboard = TensorBoardProjector(num_images=18, avg_data_channels=False, write_data=True, write_features=False)\n\n        tboard.on_start(state)\n        tboard.on_step_validation(state)\n\n        mock_board.return_value.add_embedding.assert_called_once_with(ANY, metadata=ANY, label_img=ANY, tag=\'data\',\n                                                                      global_step=-1)\n        self.assertTrue(mock_board.return_value.add_embedding.call_args[0][0].size() == torch.Size([18, 100]))\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'metadata\'].size() == state[torchbearer.Y_TRUE].size())\n        self.assertTrue(\n            mock_board.return_value.add_embedding.call_args[1][\'label_img\'].size() == torch.Size([18, 1, 10, 10]))\n        tboard.on_end({})\n\n\nclass TestTensorbardText(TestCase):\n    def test_table_formatter_one_metric(self):\n        tf = TensorBoardText.table_formatter\n\n        metrics = str({\'test_metric_1\': 1})\n        table = tf(metrics).replace("" "", """")\n\n        correct_table = \'<table><th>Metric</th><th>Value</th><tr><td>test_metric_1</td><td>1</td></tr></table>\'\n        self.assertEqual(table, correct_table)\n\n    def test_table_formatter_two_metrics(self):\n        tf = TensorBoardText.table_formatter\n\n        metrics = str({\'test_metric_1\': 1, \'test_metric_2\': 2})\n        table = tf(metrics).replace("" "", """")\n\n        self.assertIn(\'<tr><td>test_metric_1</td><td>1</td></tr>\', table)\n        self.assertIn(\'<tr><td>test_metric_2</td><td>2</td></tr>\', table)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_epoch_writer(self, mock_writer, _):\n        tboard = TensorBoardText(log_trial_summary=False)\n\n        metrics = {\'test_metric_1\': 1, \'test_metric_2\': 1}\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 1, torchbearer.METRICS: metrics}\n        metric_string = TensorBoardText.table_formatter(str(metrics))\n\n        tboard.on_start(state)\n        tboard.on_start_training(state)\n        tboard.on_start_epoch(state)\n        tboard.on_end_epoch(state)\n        mock_writer.return_value.add_text.assert_called_once_with(\'epoch\', metric_string, 1)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_epoch_writer_visdom(self, mock_visdom, mock_writer, _):\n        tboard = TensorBoardText(visdom=True, log_trial_summary=False)\n\n        metrics = {\'test_metric_1\': 1, \'test_metric_2\': 1}\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 1, torchbearer.METRICS: metrics}\n        metric_string = TensorBoardText.table_formatter(str(metrics))\n\n        tboard.on_start(state)\n        tboard.on_start_training(state)\n        tboard.on_start_epoch(state)\n        tboard.on_end_epoch(state)\n        mock_writer.return_value.add_text.assert_called_once_with(\'epoch\', \'<h4>Epoch 1</h4>\'+metric_string, 1)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_batch_writer(self, mock_writer, _):\n        tboard = TensorBoardText(write_epoch_metrics=False, write_batch_metrics=True, log_trial_summary=False)\n\n        metrics = {\'test_metric_1\': 1, \'test_metric_2\': 1}\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 1, torchbearer.BATCH: 100, torchbearer.METRICS: metrics}\n        metric_string = TensorBoardText.table_formatter(str(metrics))\n\n        tboard.on_start(state)\n        tboard.on_start_training(state)\n        tboard.on_start_epoch(state)\n        tboard.on_step_training(state)\n        mock_writer.return_value.add_text.assert_called_once_with(\'batch\', metric_string, 100)\n        tboard.on_end_epoch(state)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_batch_writer_visdom(self, mock_visdom, mock_writer, _):\n        tboard = TensorBoardText(visdom=True, write_epoch_metrics=False, write_batch_metrics=True, log_trial_summary=False)\n\n        metrics = {\'test_metric_1\': 1, \'test_metric_2\': 1}\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 1, torchbearer.BATCH: 100, torchbearer.METRICS: metrics}\n        metric_string = TensorBoardText.table_formatter(str(metrics))\n        metric_string = \'<h3>Epoch {} - Batch {}</h3>\'.format(state[torchbearer.EPOCH], state[torchbearer.BATCH])+metric_string\n\n        tboard.on_start(state)\n        tboard.on_start_training(state)\n        tboard.on_start_epoch(state)\n        tboard.on_step_training(state)\n        mock_writer.return_value.add_text.assert_called_once_with(\'batch\', metric_string, 1)\n        tboard.on_end_epoch(state)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_batch_metrics(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_text = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 0, torchbearer.METRICS: {\'test\': 1}, torchbearer.BATCH: 0}\n\n        tboard = TensorBoardText(write_batch_metrics=True, write_epoch_metrics=False, log_trial_summary=False)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_step_training(state)\n        mock_board.return_value.add_text.assert_called_once_with(\'batch\', TensorBoardText.table_formatter(str(state[torchbearer.METRICS])), 0)\n        mock_board.return_value.add_text.reset_mock()\n        tboard.on_end_epoch(state)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.torchvis.VisdomWriter\')\n    @patch(\'visdom.Visdom\')\n    def test_batch_metrics_visdom(self, mock_visdom, mock_writer, _):\n        mock_writer.return_value = Mock()\n        mock_writer.return_value.add_text = Mock()\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 0, torchbearer.METRICS: {\'test\': 1}, torchbearer.BATCH: 0, torchbearer.TRAIN_STEPS: 0}\n\n        tboard = TensorBoardText(visdom=True, write_batch_metrics=True, write_epoch_metrics=False, log_trial_summary=False)\n        tboard.on_start(state)\n        tboard.on_start_epoch(state)\n        tboard.on_step_training(state)\n        mock_writer.return_value.add_text.assert_called_once_with(\'batch\', \'<h3>Epoch {} - Batch {}</h3>\'.format(state[torchbearer.EPOCH], state[torchbearer.BATCH])+TensorBoardText.table_formatter(str(state[torchbearer.METRICS])), 1)\n        mock_writer.return_value.add_text.reset_mock()\n        tboard.on_step_validation(state)\n        tboard.on_end(state)\n\n    @patch(\'torchbearer.callbacks.tensor_board.os.makedirs\')\n    @patch(\'tensorboardX.SummaryWriter\')\n    def test_log_summary(self, mock_board, _):\n        mock_board.return_value = Mock()\n        mock_board.return_value.add_text = Mock()\n        mock_self = \'test\'\n\n        state = {torchbearer.MODEL: nn.Sequential(nn.Conv2d(3, 3, 3)),\n                 torchbearer.EPOCH: 0, torchbearer.METRICS: {\'test\': 1}, torchbearer.BATCH: 0, torchbearer.SELF: mock_self}\n        tboard = TensorBoardText(write_batch_metrics=False, write_epoch_metrics=False, log_trial_summary=True)\n        tboard.on_start(state)\n        self.assertEqual(mock_board.return_value.add_text.call_args[0][0], \'trial\')\n        self.assertEqual(mock_board.return_value.add_text.call_args[0][1], str(mock_self))\n'"
tests/callbacks/test_terminate_on_nan.py,0,"b""from unittest import TestCase\n\nimport torchbearer\nfrom torchbearer.callbacks import TerminateOnNaN\n\n\nclass TestTerminateOnNaN(TestCase):\n    def test_should_terminate(self):\n        state = {\n            torchbearer.EPOCH: 0,\n            torchbearer.BATCH: 0,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'running_loss': float('nan')}\n        }\n\n        terminator = TerminateOnNaN()\n\n        terminator.on_step_training(state)\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n        terminator.on_step_validation(state)\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n        terminator.on_end_epoch(state)\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_should_not_terminate(self):\n        state = {\n            torchbearer.EPOCH: 0,\n            torchbearer.BATCH: 0,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'running_loss': 0.01}\n        }\n\n        terminator = TerminateOnNaN()\n\n        terminator.on_step_training(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n        terminator.on_step_validation(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n        terminator.on_end_epoch(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n    def test_monitor_should_terminate(self):\n        state = {\n            torchbearer.EPOCH: 0,\n            torchbearer.BATCH: 0,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'running_loss': 0.001, 'test_metric': float('nan')}\n        }\n\n        terminator = TerminateOnNaN(monitor='test_metric')\n\n        terminator.on_step_training(state)\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n        terminator.on_step_validation(state)\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n        terminator.on_end_epoch(state)\n        self.assertTrue(state[torchbearer.STOP_TRAINING])\n\n    def test_monitor_should_not_terminate(self):\n        state = {\n            torchbearer.EPOCH: 0,\n            torchbearer.BATCH: 0,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'running_loss': 0.01, 'test_metric': 0.001}\n        }\n\n        terminator = TerminateOnNaN(monitor='test_metric')\n\n        terminator.on_step_training(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n        terminator.on_step_validation(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n        terminator.on_end_epoch(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n\n    def test_not_found_metric(self):\n        state = {\n            torchbearer.EPOCH: 0,\n            torchbearer.BATCH: 0,\n            torchbearer.STOP_TRAINING: False,\n            torchbearer.METRICS: {'running_loss': 0.001, 'test_metric': float('nan')}\n        }\n\n        terminator = TerminateOnNaN(monitor='test')\n\n        terminator.on_step_training(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n        terminator.on_step_validation(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n        terminator.on_end_epoch(state)\n        self.assertFalse(state[torchbearer.STOP_TRAINING])\n"""
tests/callbacks/test_torch_scheduler.py,8,"b'from unittest import TestCase\nfrom mock import patch, Mock\nimport warnings\n\nimport torchbearer\nfrom torchbearer.callbacks import TorchScheduler, LambdaLR, StepLR, MultiStepLR, ExponentialLR, CosineAnnealingLR,\\\n    ReduceLROnPlateau, CyclicLR\n\n\nclass TestTorchScheduler(TestCase):\n    def setUp(self):\n        super(TestTorchScheduler, self).setUp()\n        warnings.filterwarnings(\'always\')\n\n    def tearDown(self):\n        super(TestTorchScheduler, self).tearDown()\n        warnings.filterwarnings(\'default\')\n\n    def test_torch_scheduler_on_batch_with_monitor(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.METRICS: {\'test\': 101}, torchbearer.OPTIMIZER: \'optimizer\'}\n        mock_scheduler = Mock()\n        mock_scheduler.return_value = mock_scheduler\n\n        torch_scheduler = TorchScheduler(lambda opt: mock_scheduler(opt), monitor=\'test\', step_on_batch=True)\n\n        torch_scheduler.on_start(state)\n        mock_scheduler.assert_called_once_with(\'optimizer\')\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_start_training(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_sample(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_step_training(state)\n        mock_scheduler.step.assert_called_once_with(101)\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_end_epoch(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n    def test_torch_scheduler_on_epoch_with_monitor(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.METRICS: {\'test\': 101}, torchbearer.OPTIMIZER: \'optimizer\',\n                 torchbearer.DATA: None}\n        mock_scheduler = Mock()\n        mock_scheduler.return_value = mock_scheduler\n\n        torch_scheduler = TorchScheduler(lambda opt: mock_scheduler(opt), monitor=\'test\', step_on_batch=False)\n\n        torch_scheduler.on_start(state)\n        mock_scheduler.assert_called_once_with(\'optimizer\')\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_start_training(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_sample(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_step_training(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_end_epoch(state)\n        mock_scheduler.step.assert_called_once_with(101, epoch=1)\n        mock_scheduler.reset_mock()\n\n    def test_torch_scheduler_on_batch_no_monitor(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.OPTIMIZER: \'optimizer\'}\n        mock_scheduler = Mock()\n        mock_scheduler.return_value = mock_scheduler\n\n        torch_scheduler = TorchScheduler(lambda opt: mock_scheduler(opt), monitor=None, step_on_batch=True)\n\n        torch_scheduler.on_start(state)\n        mock_scheduler.assert_called_once_with(\'optimizer\')\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_start_training(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_sample(state)\n        mock_scheduler.step.assert_called_once_with()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_step_training(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_end_epoch(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n    def test_torch_scheduler_on_epoch_no_monitor(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.OPTIMIZER: \'optimizer\', torchbearer.METRICS: {}}\n        mock_scheduler = Mock()\n        mock_scheduler.return_value = mock_scheduler\n\n        torch_scheduler = TorchScheduler(lambda opt: mock_scheduler(opt), monitor=None, step_on_batch=False)\n\n        torch_scheduler.on_start(state)\n        mock_scheduler.assert_called_once_with(\'optimizer\')\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_sample(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_step_training(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n        torch_scheduler.on_end_epoch(state)\n        mock_scheduler.assert_not_called()\n        mock_scheduler.reset_mock()\n\n    def test_monitor_not_found(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.OPTIMIZER: \'optimizer\', torchbearer.METRICS: {\'not_test\': 1.}}\n        mock_scheduler = Mock()\n        mock_scheduler.return_value = mock_scheduler\n\n        torch_scheduler = TorchScheduler(lambda opt: mock_scheduler(opt), monitor=\'test\', step_on_batch=False)\n        torch_scheduler.on_start(state)\n\n        with warnings.catch_warnings(record=True) as w:\n            torch_scheduler.on_start_validation(state)\n            self.assertTrue(len(w) == 0)\n\n        with warnings.catch_warnings(record=True) as w:\n            torch_scheduler.on_end_epoch(state)\n            self.assertTrue(\'Failed to retrieve key `test`\' in str(w[0].message))\n\n    def test_monitor_found(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.OPTIMIZER: \'optimizer\', torchbearer.METRICS: {\'test\': 1.}}\n        mock_scheduler = Mock()\n        mock_scheduler.return_value = mock_scheduler\n\n        torch_scheduler = TorchScheduler(lambda opt: mock_scheduler(opt), monitor=\'test\', step_on_batch=False)\n        torch_scheduler.on_start(state)\n        with warnings.catch_warnings(record=True) as w:\n            torch_scheduler.on_start_training(state)\n            self.assertTrue(len(w) == 0)\n\n        with warnings.catch_warnings(record=True) as w:\n            torch_scheduler.on_start_validation(state)\n            self.assertTrue(len(w) == 0)\n\n        with warnings.catch_warnings(record=True) as w:\n            torch_scheduler.on_end_epoch(state)\n            self.assertTrue(len(w) == 0)\n\n    def test_batch_monitor_not_found(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.OPTIMIZER: \'optimizer\', torchbearer.METRICS: {\'not_test\': 1.}}\n        mock_scheduler = Mock()\n        mock_scheduler.return_value = mock_scheduler\n\n        torch_scheduler = TorchScheduler(lambda opt: mock_scheduler(opt), monitor=\'test\', step_on_batch=True)\n        torch_scheduler.on_start(state)\n\n        with warnings.catch_warnings(record=True) as w:\n            torch_scheduler.on_step_training(state)\n            self.assertTrue(\'Failed to retrieve key `test`\' in str(w[0].message))\n\n    def test_batch_monitor_found(self):\n        state = {torchbearer.EPOCH: 1, torchbearer.OPTIMIZER: \'optimizer\', torchbearer.METRICS: {\'test\': 1.}}\n        mock_scheduler = Mock()\n        mock_scheduler.return_value = mock_scheduler\n\n        torch_scheduler = TorchScheduler(lambda opt: mock_scheduler(opt), monitor=\'test\', step_on_batch=True)\n        torch_scheduler.on_start(state)\n\n        with warnings.catch_warnings(record=True) as w:\n            torch_scheduler.on_step_training(state)\n            self.assertTrue(len(w) == 0)\n\n\nclass TestLambdaLR(TestCase):\n    @patch(\'torch.optim.lr_scheduler.LambdaLR\')\n    def test_lambda_lr(self, lr_mock):\n        state = {torchbearer.OPTIMIZER: \'optimizer\'}\n\n        scheduler = LambdaLR(0.1, last_epoch=-4, step_on_batch=\'batch\')\n        scheduler.on_start(state)\n\n        lr_mock.assert_called_once_with(\'optimizer\', 0.1, last_epoch=-4)\n        self.assertTrue(scheduler._step_on_batch == \'batch\')\n\n\nclass TestStepLR(TestCase):\n    @patch(\'torch.optim.lr_scheduler.StepLR\')\n    def test_lambda_lr(self, lr_mock):\n        state = {torchbearer.OPTIMIZER: \'optimizer\'}\n\n        scheduler = StepLR(10, gamma=0.4, last_epoch=-4, step_on_batch=\'batch\')\n        scheduler.on_start(state)\n\n        lr_mock.assert_called_once_with(\'optimizer\', 10, gamma=0.4, last_epoch=-4)\n        self.assertTrue(scheduler._step_on_batch == \'batch\')\n\n\nclass TestMultiStepLR(TestCase):\n    @patch(\'torch.optim.lr_scheduler.MultiStepLR\')\n    def test_lambda_lr(self, lr_mock):\n        state = {torchbearer.OPTIMIZER: \'optimizer\'}\n\n        scheduler = MultiStepLR(10, gamma=0.4, last_epoch=-4, step_on_batch=\'batch\')\n        scheduler.on_start(state)\n\n        lr_mock.assert_called_once_with(\'optimizer\', 10, gamma=0.4, last_epoch=-4)\n        self.assertTrue(scheduler._step_on_batch == \'batch\')\n\n\nclass TestExponentialLR(TestCase):\n    @patch(\'torch.optim.lr_scheduler.ExponentialLR\')\n    def test_lambda_lr(self, lr_mock):\n        state = {torchbearer.OPTIMIZER: \'optimizer\'}\n\n        scheduler = ExponentialLR(0.4, last_epoch=-4, step_on_batch=\'batch\')\n        scheduler.on_start(state)\n\n        lr_mock.assert_called_once_with(\'optimizer\', 0.4, last_epoch=-4)\n        self.assertTrue(scheduler._step_on_batch == \'batch\')\n\n\nclass TestCosineAnnealingLR(TestCase):\n    @patch(\'torch.optim.lr_scheduler.CosineAnnealingLR\')\n    def test_lambda_lr(self, lr_mock):\n        state = {torchbearer.OPTIMIZER: \'optimizer\'}\n\n        scheduler = CosineAnnealingLR(4, eta_min=10, last_epoch=-4, step_on_batch=\'batch\')\n        scheduler.on_start(state)\n\n        lr_mock.assert_called_once_with(\'optimizer\', 4, eta_min=10, last_epoch=-4)\n        self.assertTrue(scheduler._step_on_batch == \'batch\')\n\n\nclass TestReduceLROnPlateau(TestCase):\n    @patch(\'torch.optim.lr_scheduler.ReduceLROnPlateau\')\n    def test_lambda_lr(self, lr_mock):\n        state = {torchbearer.OPTIMIZER: \'optimizer\'}\n\n        scheduler = ReduceLROnPlateau(monitor=\'test\', mode=\'max\', factor=0.2, patience=100, verbose=True, threshold=10,\n                                      threshold_mode=\'thresh\', cooldown=5, min_lr=0.1, eps=1e-4, step_on_batch=\'batch\')\n        scheduler.on_start(state)\n\n        lr_mock.assert_called_once_with(\'optimizer\', mode=\'max\', factor=0.2, patience=100, verbose=True, threshold=10,\n                                        threshold_mode=\'thresh\', cooldown=5, min_lr=0.1, eps=1e-4)\n        self.assertTrue(scheduler._step_on_batch == \'batch\')\n        self.assertTrue(scheduler._monitor == \'test\')\n\n\nclass TestCyclicLR(TestCase):\n    def test_lambda_lr(self):\n        from distutils.version import LooseVersion\n        import torch\n        version = torch.__version__ if str(torch.__version__) is torch.__version__ else ""0.4.0""\n        if LooseVersion(version) > LooseVersion(""1.0.0""): # CyclicLR is implemented\n            with patch(\'torch.optim.lr_scheduler.CyclicLR\') as lr_mock:\n                state = {torchbearer.OPTIMIZER: \'optimizer\'}\n\n                scheduler = CyclicLR(0.01, 0.1, monitor=\'test\', step_size_up=200, step_size_down=None, mode=\'triangular\',\n                         gamma=2., scale_fn=None, scale_mode=\'cycle\', cycle_momentum=False, base_momentum=0.7, max_momentum=0.9,\n                         last_epoch=-1, step_on_batch=\'batch\')\n                scheduler.on_start(state)\n\n                lr_mock.assert_called_once_with(\'optimizer\', 0.01, 0.1, step_size_up=200, step_size_down=None, mode=\'triangular\',\n                         gamma=2., scale_fn=None, scale_mode=\'cycle\', cycle_momentum=False, base_momentum=0.7, max_momentum=0.9,\n                         last_epoch=-1)\n                self.assertTrue(scheduler._step_on_batch == \'batch\')\n                self.assertTrue(scheduler._monitor == \'test\')\n        else:\n            self.assertRaises(NotImplementedError, lambda: CyclicLR(0.01, 0.1, monitor=\'test\', step_size_up=200, step_size_down=None, mode=\'triangular\',\n                         gamma=2., scale_fn=None, scale_mode=\'cycle\', cycle_momentum=False, base_momentum=0.7, max_momentum=0.9,\n                         last_epoch=-1, step_on_batch=\'batch\'))\n'"
tests/callbacks/test_unpack_state.py,0,"b""import torch\nfrom unittest import TestCase\n\nfrom mock import MagicMock, patch\n\nimport torchbearer\nfrom torchbearer.callbacks import UnpackState\n\n\nclass TestUnpackState(TestCase):\n    def reset_state(self):\n        return {\n            torchbearer.X: 'data',\n            torchbearer.Y_TRUE: 'targets'\n        }\n\n    def test_sample(self):\n        packer = UnpackState(keys=[torchbearer.X, torchbearer.Y_TRUE])\n        state = self.reset_state()\n\n        packer.on_sample(state)\n        self.assertTrue(state[torchbearer.X] == {torchbearer.X: 'data', torchbearer.Y_TRUE: 'targets'})\n\n        state = self.reset_state()\n        packer.on_sample_validation(state)\n        self.assertTrue(state[torchbearer.X] == {torchbearer.X: 'data', torchbearer.Y_TRUE: 'targets'})\n\n    def test_no_key(self):\n        packer = UnpackState()\n        state = self.reset_state()\n\n        packer.on_sample(state)\n        self.assertTrue(state[torchbearer.X] == 'data')\n\n        state = self.reset_state()\n        packer.on_sample_validation(state)\n        self.assertTrue(state[torchbearer.X] == 'data')\n\n    def test_fill_X(self):\n        packer = UnpackState(keys=[torchbearer.Y_TRUE])\n        state = self.reset_state()\n\n        packer.on_sample(state)\n        self.assertTrue(state[torchbearer.X] == {torchbearer.X: 'data', torchbearer.Y_TRUE: 'targets'})\n\n        state = self.reset_state()\n        packer.on_sample_validation(state)\n        self.assertTrue(state[torchbearer.X] == {torchbearer.X: 'data', torchbearer.Y_TRUE: 'targets'})\n\n    def test_forward_no_dict(self):\n        packer = UnpackState(keys=[torchbearer.Y_TRUE])\n        state = self.reset_state()\n\n        state[torchbearer.Y_PRED] = 1\n        packer.on_forward(state)\n        self.assertTrue(state[torchbearer.Y_PRED] == 1)\n\n        state = self.reset_state()\n\n        state[torchbearer.Y_PRED] = 1\n        packer.on_forward_validation(state)\n        self.assertTrue(state[torchbearer.Y_PRED] == 1)\n\n    def test_forward_list(self):\n        packer = UnpackState(keys=[torchbearer.Y_TRUE])\n        state = self.reset_state()\n\n        state[torchbearer.Y_PRED] = [1, 2, 3]\n        packer.on_forward(state)\n        self.assertTrue(state[torchbearer.Y_PRED] == [1, 2, 3])\n\n        state = self.reset_state()\n\n        state[torchbearer.Y_PRED] = [1, 2, 3]\n        packer.on_forward_validation(state)\n        self.assertTrue(state[torchbearer.Y_PRED] == [1, 2, 3])\n\n    def test_forward_dict_no_y_pred(self):\n        packer = UnpackState(keys=[torchbearer.Y_TRUE])\n        state = self.reset_state()\n\n        state[torchbearer.Y_PRED] = {'one': 1, 'two': 2}\n        packer.on_forward(state)\n        self.assertTrue(state[torchbearer.Y_PRED] == {'one': 1, 'two': 2})\n\n        state = self.reset_state()\n\n        state[torchbearer.Y_PRED] = {'one': 1, 'two': 2}\n        packer.on_forward_validation(state)\n        self.assertTrue(state[torchbearer.Y_PRED] == {'one': 1, 'two': 2})\n\n    def test_forward_dict_y_pred(self):\n        packer = UnpackState(keys=[torchbearer.Y_TRUE])\n        state = self.reset_state()\n\n        state[torchbearer.Y_PRED] = {torchbearer.Y_PRED: 1, 'two': 2}\n        packer.on_forward(state)\n        self.assertTrue(state[torchbearer.Y_PRED] == 1)\n        self.assertTrue(state['two'] == 2)\n\n        state = self.reset_state()\n\n        state[torchbearer.Y_PRED] = {torchbearer.Y_PRED: 1, 'two': 2}\n        packer.on_forward_validation(state)\n        self.assertTrue(state[torchbearer.Y_PRED] == 1)\n        self.assertTrue(state['two'] == 2)"""
tests/callbacks/test_weight_decay.py,5,"b""from unittest import TestCase\n\nimport torch\nimport torch.nn as nn\nfrom mock import Mock, patch\n\nimport torchbearer\nfrom torchbearer.callbacks import WeightDecay, L1WeightDecay, L2WeightDecay\n\n\nclass TestWeightDecay(TestCase):\n\n    def test_single_parameter(self):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=[nn.Parameter(torch.Tensor([2.0]))])\n        state = {\n            torchbearer.MODEL: model,\n            torchbearer.LOSS: 1\n        }\n\n        decay = WeightDecay(1, 1.0)\n        decay.on_start(state)\n        decay.on_criterion(state)\n\n        self.assertTrue(state[torchbearer.LOSS].item() == 3)\n\n    def test_multiple_parameters(self):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=[nn.Parameter(torch.Tensor([2.0, 1.0]))])\n        state = {\n            torchbearer.MODEL: model,\n            torchbearer.LOSS: 1\n        }\n\n        decay = WeightDecay(1, 1.0)\n        decay.on_start(state)\n        decay.on_criterion(state)\n\n        self.assertTrue(state[torchbearer.LOSS].item() == 4)\n\n    def test_rate(self):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=[nn.Parameter(torch.Tensor([2.0]))])\n        state = {\n            torchbearer.MODEL: model,\n            torchbearer.LOSS: 1\n        }\n\n        decay = WeightDecay(0.5, 1.0)\n        decay.on_start(state)\n        decay.on_criterion(state)\n\n        self.assertTrue(state[torchbearer.LOSS].item() == 2)\n\n    def test_pow(self):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=[nn.Parameter(torch.Tensor([2.0]))])\n        state = {\n            torchbearer.MODEL: model,\n            torchbearer.LOSS: 1\n        }\n\n        decay = WeightDecay(2, 2)\n        decay.on_start(state)\n        decay.on_criterion(state)\n\n        self.assertTrue(state[torchbearer.LOSS].item() == 5)\n\n    def test_given_params(self):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=-1)\n        state = {\n            torchbearer.MODEL: model,\n            torchbearer.LOSS: 1\n        }\n\n        decay = WeightDecay(2, 2, params=model.parameters())\n        decay.on_start(state)\n\n        self.assertTrue(decay.params == -1)\n\n    @patch('torchbearer.callbacks.weight_decay.WeightDecay.__init__')\n    def test_L1(self, mock_wd):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=-1)\n        state = {\n            torchbearer.MODEL: model,\n            torchbearer.LOSS: 1\n        }\n\n        decay = L1WeightDecay()\n        self.assertTrue(mock_wd.call_args[1]['p'] == 1)\n\n    @patch('torchbearer.callbacks.weight_decay.WeightDecay.__init__')\n    def test_L2(self, mock_wd):\n        model = nn.Sequential(nn.Conv2d(3, 3, 3))\n        model.parameters = Mock(return_value=-1)\n        state = {\n            torchbearer.MODEL: model,\n            torchbearer.LOSS: 1\n        }\n\n        decay = L2WeightDecay()\n        self.assertTrue(mock_wd.call_args[1]['p'] == 2)\n"""
tests/metrics/__init__.py,0,b''
tests/metrics/test_aggregators.py,35,"b""import unittest\n\nfrom mock import Mock, call\n\nfrom torchbearer.metrics import RunningMean, Metric, RunningMetric, Mean, Std, Var\n\nimport torch\n\n\nclass TestVar(unittest.TestCase):\n    def test_variance_dim(self):\n        var = Var('test', dim=0)\n        var.process(torch.Tensor([[1., 2.], [3., 4.]]))\n        var.process(torch.Tensor([[4., 3.], [2., 1.]]))\n        var.process(torch.Tensor([[1., 1.], [1., 1.]]))\n\n        res = var.process_final()\n        self.assertTrue(len(res) == 2)\n        for m in res:\n            self.assertTrue(abs(m - 1.6000) < 0.0001)\n\n\nclass TestStd(unittest.TestCase):\n    def setUp(self):\n        self._metric = Metric('test')\n        self._metric.process = Mock()\n        self._metric.process.side_effect = [torch.zeros(torch.Size([])),\n                                            torch.FloatTensor([0.1, 0.2, 0.3]),\n                                            torch.FloatTensor([0.4, 0.5, 0.6]),\n                                            torch.FloatTensor([0.7, 0.8, 0.9]),\n                                            torch.ones(torch.Size([]))]\n\n        self._std = Std('test', unbiased=False)\n        self._std.reset({})\n        self._target = 0.31622776601684\n\n    def test_train(self):\n        self.setUp()\n        self._std.train()\n        for i in range(5):\n            self._std.process(self._metric.process())\n        result = self._std.process_final({})\n        self.assertAlmostEqual(self._target, result, places=5)\n\n    def test_validate(self):\n        self.setUp()\n        self._std.eval()\n        for i in range(5):\n            self._std.process(self._metric.process())\n        result = self._std.process_final({})\n        self.assertAlmostEqual(self._target, result, places=5)\n\n    def test_precision_error(self):\n        self.setUp()\n        self._std.train()\n        val = torch.tensor([0.55])\n        for i in range(2):\n            self._std.process(val)\n\n        result = self._std.process_final({})\n        self.assertEqual(0, result)\n\n    def setUpMoreDims(self):\n        self._metric = Metric('test')\n        self._metric.process = Mock()\n        self._metric.process.side_effect = [torch.zeros(torch.Size([])),\n                                            torch.FloatTensor([[0.1, 0.2, 0.3], [1.1, 1.2, 1.3]]),\n                                            torch.FloatTensor([[0.4, 0.5, 0.6], [1.4, 1.5, 1.6]]),\n                                            torch.FloatTensor([[0.7, 0.8, 0.9], [1.7, 1.8, 1.9]]),\n                                            torch.ones(torch.Size([]))]\n        self._std = Std('test', unbiased=False)\n        self._std.reset({})\n        self._target = 0.57662804083742\n\n    def test_more_dims(self):\n        self.setUpMoreDims()\n        for i in range(5):\n            self._std.process(self._metric.process())\n        result = self._std.process_final({})\n        self.assertAlmostEqual(self._target, result, places=5)\n\n    def test_std_dim(self):\n        std = Std('test', dim=0)\n        std.process(torch.Tensor([[1., 2.], [3., 4.]]))\n        std.process(torch.Tensor([[4., 3.], [2., 1.]]))\n        std.process(torch.Tensor([[1., 1.], [1., 1.]]))\n\n        res = std.process_final()\n        self.assertTrue(len(res) == 2)\n        for m in res:\n            self.assertTrue(abs(m - 1.2649) < 0.0001)\n\n\nclass TestMean(unittest.TestCase):\n    def setUp(self):\n        self._metric = Metric('test')\n        self._metric.process = Mock()\n        self._metric.process.side_effect = [torch.zeros(torch.Size([])),\n                                            torch.FloatTensor([0.1, 0.2, 0.3]),\n                                            torch.FloatTensor([0.4, 0.5, 0.6]),\n                                            torch.FloatTensor([0.7, 0.8, 0.9]),\n                                            torch.ones(torch.Size([]))]\n\n        self._mean = Mean('test')\n        self._mean.reset({})\n        self._target = 0.5\n\n    def test_train_dict(self):\n        self.setUp()\n        self._mean.train()\n        for i in range(5):\n            self._mean.process(self._metric.process())\n        result = self._mean.process_final({})\n        self.assertAlmostEqual(self._target, result, places=5)\n\n    def test_validate_dict(self):\n        self.setUp()\n        self._mean.eval()\n        for i in range(5):\n            self._mean.process(self._metric.process())\n        result = self._mean.process_final({})\n        self.assertAlmostEqual(self._target, result, places=5)\n\n    def setUpMoreDims(self):\n        self._metric = Metric('test')\n        self._metric.process = Mock()\n        self._metric.process.side_effect = [torch.zeros(torch.Size([])),\n                                            torch.FloatTensor([[0.1, 0.2, 0.3], [1.1, 1.2, 1.3]]),\n                                            torch.FloatTensor([[0.4, 0.5, 0.6], [1.4, 1.5, 1.6]]),\n                                            torch.FloatTensor([[0.7, 0.8, 0.9], [1.7, 1.8, 1.9]]),\n                                            torch.ones(torch.Size([]))]\n        self._mean = Mean('test')\n        self._mean.reset({})\n        self._target = 0.95\n\n    def test_more_dims(self):\n        self.setUpMoreDims()\n        for i in range(5):\n            self._mean.process(self._metric.process())\n        result = self._mean.process_final({})\n        self.assertAlmostEqual(self._target, result, places=5)\n\n    def test_mean_dim(self):\n        mean = Mean('test', dim=0)\n        mean.process(torch.Tensor([[1., 2.], [3., 4.]]))\n        mean.process(torch.Tensor([[4., 3.], [2., 1.]]))\n        mean.process(torch.Tensor([[1., 1.], [1., 1.]]))\n\n        res = mean.process_final()\n        self.assertTrue(len(res) == 2)\n        for m in res:\n            self.assertTrue(abs(m - 2.0) < 0.0001)\n\n\nclass TestRunningMetric(unittest.TestCase):\n    def setUp(self):\n        self._metric = RunningMetric('test', batch_size=5, step_size=5)\n        self._metric.reset({})\n        self._metric._process_train = Mock(return_value=3)\n        self._metric._step = Mock(return_value='output')\n\n    def test_train_called_with_state(self):\n        self._metric.train()\n        self._metric.process({'test': -1})\n        self._metric._process_train.assert_called_with({'test': -1})\n\n    def test_cache_one_step(self):\n        self._metric.train()\n        for i in range(6):\n            self._metric.process({})\n        self._metric._step.assert_has_calls([call([3]), call([3, 3, 3, 3, 3])])\n\n    def test_empty_methods(self):\n        metric = RunningMetric('test')\n        self.assertRaises(NotImplementedError, lambda: metric._step(['test']) is None)\n        self.assertRaises(NotImplementedError, lambda: metric._process_train(['test']) is None)\n\n\nclass TestRunningMean(unittest.TestCase):\n    def setUp(self):\n        self._metric = Metric('test')\n        self._mean = RunningMean('test')\n        self._cache = [torch.Tensor([1.0]), torch.Tensor([1.5]), torch.Tensor([2.0])]\n        self._target = 1.5\n\n    def test_train(self):\n        result = self._mean._process_train(torch.FloatTensor([1.0, 1.5, 2.0]))\n        self.assertAlmostEqual(self._target, result, 3, 0.002)\n\n    def test_step(self):\n        result = self._mean._step(self._cache)\n        self.assertEqual(self._target, result)\n\n    def test_dims(self):\n        mean = RunningMean('test', dim=0)\n        cache = [mean._process_train(torch.Tensor([[1., 2.], [3., 4.]])),\n                 mean._process_train(torch.Tensor([[4., 3.], [2., 1.]])),\n                 mean._process_train(torch.Tensor([[1., 1.], [1., 1.]]))]\n\n        res = mean._step(cache)\n        self.assertTrue(len(res) == 2)\n        for m in res:\n            self.assertTrue(abs(m - 2.0) < 0.0001)\n"""
tests/metrics/test_decorators.py,0,"b""import unittest\n\nfrom mock import Mock\n\nimport torchbearer.metrics as metrics\nfrom torchbearer.metrics import default_for_key, lambda_metric, EpochLambda, BatchLambda\n\n\nclass TestDecorators(unittest.TestCase):\n    def test_default_for_key_class(self):\n        metric = metrics.Loss\n        metric = default_for_key('test')(metric)\n        self.assertTrue(metrics.get_default('test').name == 'loss')\n        self.assertTrue(metric == metrics.Loss)\n\n    def test_default_for_key_metric(self):\n        metric = metrics.Loss()\n        metric = default_for_key('test')(metric)\n        self.assertTrue(metrics.get_default('test').name == 'loss')\n        self.assertTrue(metric.name == 'loss')\n\n    def test_default_for_key_args(self):\n        mock = Mock()\n\n        class MyMetric(metrics.Metric):\n            def __init__(self, *args, **kwargs):\n                super(MyMetric, self).__init__('test')\n                mock(*args, **kwargs)\n\n        default_for_key('test', 10, some_arg='a test')(MyMetric)\n        metrics.get_default('test')\n        mock.assert_called_once_with(10, some_arg='a test')\n\n    def test_lambda_metric_epoch(self):\n        metric = 'test'\n        metric = lambda_metric('test', on_epoch=True)(metric)\n        self.assertTrue(isinstance(metric, EpochLambda))\n        self.assertTrue(metric._final == 'test')\n\n    def test_lambda_metric_batch(self):\n        metric = 'test'\n        metric = lambda_metric('test')(metric)\n        self.assertTrue(isinstance(metric, BatchLambda))\n        self.assertTrue(metric._metric_function == 'test')\n\n    def test_to_dict_metric_class(self):\n        metric = metrics.Metric\n        out = metrics.to_dict(metric)('test')\n        self.assertTrue(out.metric.name == 'test')\n        self.assertTrue(isinstance(out, metrics.ToDict))\n\n    def test_to_dict_metric_instance(self):\n        metric = metrics.Metric('test')\n        out = metrics.to_dict(metric)\n        self.assertTrue(out.metric.name == 'test')\n        self.assertTrue(isinstance(out, metrics.ToDict))\n\n    def test_mean_metric_class(self):\n        metric = metrics.Metric\n        out = metrics.mean(dim=10)(metric)('test')\n        self.assertTrue(isinstance(out, metrics.MetricTree))\n        self.assertTrue(isinstance(out.children[0], metrics.ToDict))\n        self.assertTrue(isinstance(out.children[0].metric, metrics.Mean))\n        self.assertTrue(out.children[0].metric._kwargs['dim'] == 10)\n        self.assertTrue(out.children[0].metric.name == 'test')\n        self.assertTrue(out.root.name == 'test')\n\n    def test_mean_metric_instance(self):\n        metric = metrics.Metric('test')\n        out = metrics.mean(metric, dim=10)\n        self.assertTrue(isinstance(out, metrics.MetricTree))\n        self.assertTrue(isinstance(out.children[0], metrics.ToDict))\n        self.assertTrue(isinstance(out.children[0].metric, metrics.Mean))\n        self.assertTrue(out.children[0].metric._kwargs['dim'] == 10)\n        self.assertTrue(out.children[0].metric.name == 'test')\n        self.assertTrue(out.root.name == 'test')\n\n    def test_std_metric_class(self):\n        metric = metrics.Metric\n        out = metrics.std(dim=10, unbiased=False)(metric)('test')\n        self.assertTrue(isinstance(out, metrics.MetricTree))\n        self.assertTrue(isinstance(out.children[0], metrics.ToDict))\n        self.assertTrue(isinstance(out.children[0].metric, metrics.Std))\n        self.assertTrue(out.children[0].metric._kwargs['dim'] == 10)\n        self.assertTrue(not out.children[0].metric._unbiased)\n        self.assertTrue(out.children[0].metric.name == 'test_std')\n        self.assertTrue(out.root.name == 'test')\n\n    def test_std_metric_instance(self):\n        metric = metrics.Metric('test')\n        out = metrics.std(metric, dim=10, unbiased=False)\n        self.assertTrue(isinstance(out, metrics.MetricTree))\n        self.assertTrue(isinstance(out.children[0], metrics.ToDict))\n        self.assertTrue(isinstance(out.children[0].metric, metrics.Std))\n        self.assertTrue(out.children[0].metric._kwargs['dim'] == 10)\n        self.assertTrue(not out.children[0].metric._unbiased)\n        self.assertTrue(out.children[0].metric.name == 'test_std')\n        self.assertTrue(out.root.name == 'test')\n\n    def test_var_metric_class(self):\n        metric = metrics.Metric\n        out = metrics.var(dim=10, unbiased=False)(metric)('test')\n        self.assertTrue(isinstance(out, metrics.MetricTree))\n        self.assertTrue(isinstance(out.children[0], metrics.ToDict))\n        self.assertTrue(isinstance(out.children[0].metric, metrics.Var))\n        self.assertTrue(out.children[0].metric._kwargs['dim'] == 10)\n        self.assertTrue(not out.children[0].metric._unbiased)\n        self.assertTrue(out.children[0].metric.name == 'test_var')\n        self.assertTrue(out.root.name == 'test')\n\n    def test_var_metric_instance(self):\n        metric = metrics.Metric('test')\n        out = metrics.var(metric, dim=10, unbiased=False)\n        self.assertTrue(isinstance(out, metrics.MetricTree))\n        self.assertTrue(isinstance(out.children[0], metrics.ToDict))\n        self.assertTrue(isinstance(out.children[0].metric, metrics.Var))\n        self.assertTrue(out.children[0].metric._kwargs['dim'] == 10)\n        self.assertTrue(not out.children[0].metric._unbiased)\n        self.assertTrue(out.children[0].metric.name == 'test_var')\n        self.assertTrue(out.root.name == 'test')\n\n    def test_running_mean_metric_class(self):\n        metric = metrics.Metric\n        out = metrics.running_mean(batch_size=40, step_size=20, dim=10)(metric)('test')\n        self.assertTrue(isinstance(out, metrics.MetricTree))\n        self.assertTrue(isinstance(out.children[0], metrics.ToDict))\n        self.assertTrue(isinstance(out.children[0].metric, metrics.RunningMean))\n        self.assertTrue(out.children[0].metric._batch_size == 40)\n        self.assertTrue(out.children[0].metric._step_size == 20)\n        self.assertTrue(out.children[0].metric._kwargs['dim'] == 10)\n        self.assertTrue(out.children[0].metric.name == 'running_test')\n        self.assertTrue(out.root.name == 'test')\n\n    def test_running_mean_metric_instance(self):\n        metric = metrics.Metric('test')\n        out = metrics.running_mean(batch_size=40, step_size=20, dim=10)(metric)\n        self.assertTrue(isinstance(out, metrics.MetricTree))\n        self.assertTrue(isinstance(out.children[0], metrics.ToDict))\n        self.assertTrue(isinstance(out.children[0].metric, metrics.RunningMean))\n        self.assertTrue(out.children[0].metric._batch_size == 40)\n        self.assertTrue(out.children[0].metric._step_size == 20)\n        self.assertTrue(out.children[0].metric._kwargs['dim'] == 10)\n        self.assertTrue(out.children[0].metric.name == 'running_test')\n        self.assertTrue(out.root.name == 'test')\n"""
tests/metrics/test_default.py,2,"b""import unittest\nfrom mock import Mock, patch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchbearer\nfrom torchbearer.metrics import DefaultAccuracy\n\n\nclass TestDefaultAccuracy(unittest.TestCase):\n    def test_defaults(self):\n\n        state = {torchbearer.CRITERION: 'not a criterion'}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'acc')\n\n        state = {torchbearer.CRITERION: nn.CrossEntropyLoss()}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'acc')\n\n        state = {torchbearer.CRITERION: nn.NLLLoss()}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'acc')\n\n        state = {torchbearer.CRITERION: F.cross_entropy}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'acc')\n\n        state = {torchbearer.CRITERION: F.nll_loss}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'acc')\n\n        state = {torchbearer.CRITERION: nn.MSELoss()}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'mse')\n\n        state = {torchbearer.CRITERION: F.mse_loss}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'mse')\n\n        state = {torchbearer.CRITERION: nn.BCELoss()}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'binary_acc')\n\n        state = {torchbearer.CRITERION: nn.BCEWithLogitsLoss()}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'binary_acc')\n\n        state = {torchbearer.CRITERION: F.binary_cross_entropy}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'binary_acc')\n\n        state = {torchbearer.CRITERION: F.binary_cross_entropy_with_logits}\n        metric = DefaultAccuracy()\n        metric.reset(state)\n        self.assertEqual(metric.name, 'binary_acc')\n\n    @patch('torchbearer.metrics.default.CategoricalAccuracy')\n    def test_pass_through(self, cat_acc):\n        mock = Mock()\n        cat_acc.return_value = mock\n        mock.reset = Mock()\n        mock.process = Mock()\n        mock.process_final = Mock()\n        mock.eval = Mock()\n        mock.train = Mock()\n\n        metric = DefaultAccuracy()\n        metric.reset({torchbearer.CRITERION: None})\n        metric.process(1, 2, 3)\n        metric.process_final(4, 5, 6)\n        metric.eval()\n        metric.train()\n\n        self.assertEqual(cat_acc.call_count, 1)\n        mock.reset.assert_called_once_with({torchbearer.CRITERION: None})\n        mock.process.assert_called_once_with(1, 2, 3)\n        mock.process_final.assert_called_once_with(4, 5, 6)\n        self.assertEqual(mock.eval.call_count, 1)\n        self.assertEqual(mock.train.call_count, 1)\n\n    @patch('torchbearer.metrics.default.CategoricalAccuracy')\n    def test_reset_after_eval(self, cat_acc):\n        metric = DefaultAccuracy()\n        self.assertTrue(cat_acc.call_count == 1)\n        cat_acc.reset_mock()\n        metric.eval()\n\n        state = {torchbearer.CRITERION: F.cross_entropy, torchbearer.DATA: 'test'}\n        mock = Mock()\n        mock.eval = Mock()\n        torchbearer.metrics.default.__loss_map__[F.cross_entropy.__name__] = lambda: mock\n\n        metric.reset(state)\n\n        mock.eval.assert_called_once_with(data_key='test')\n        self.assertTrue(mock.eval.call_count == 1)\n"""
tests/metrics/test_lr.py,2,"b""import unittest\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchbearer\nfrom torchbearer.metrics import LR\n\n\nclass TestLR(unittest.TestCase):\n    def test_simple(self):\n        state = {torchbearer.OPTIMIZER: optim.SGD(nn.Linear(10, 10).parameters(), lr=0.01)}\n        metric = LR()\n        metric.reset(state)\n        self.assertDictEqual(metric.process(state), {'lr': 0.01})\n        self.assertDictEqual(metric.process_final(state), {'lr': 0.01})\n\n    def test_groups(self):\n        state = {\n            torchbearer.OPTIMIZER: optim.SGD([\n                {'params': nn.Linear(10, 10).parameters()},\n                {'params': nn.Linear(10, 10).parameters(), 'lr': 1e-3}\n            ], lr=1e-2, momentum=0.9)\n        }\n        metric = LR()\n        metric.reset(state)\n        self.assertListEqual(metric.process(state)['lr'], [1e-2, 1e-3])\n        self.assertListEqual(metric.process_final(state)['lr'], [1e-2, 1e-3])\n"""
tests/metrics/test_metrics.py,0,"b""import unittest\nfrom mock import Mock\n\nfrom torchbearer.metrics import MetricList, Metric, MetricTree, AdvancedMetric\n\n\nclass TestMetricTree(unittest.TestCase):\n    def test_dict_return(self):\n        root = Metric('test')\n        root.process = Mock(return_value={0: 'test', 1: 'something else'})\n        leaf1 = Metric('test')\n        leaf1.process = Mock(return_value={'test': 10})\n        leaf2 = Metric('test')\n        leaf2.process = Mock(return_value=None)\n\n        tree = MetricTree(root)\n        tree.add_child(leaf1)\n        tree.add_child(leaf2)\n\n        self.assertTrue(tree.process('args') == {'test': 10})\n\n        root.process.assert_called_once_with('args')\n        leaf1.process.assert_called_once_with('test')\n        leaf2.process.assert_called_once_with('test')\n\n    def test_process(self):\n        root = Metric('test')\n        root.process = Mock(return_value='test')\n        leaf1 = Metric('test')\n        leaf1.process = Mock(return_value={'test': 10})\n        leaf2 = Metric('test')\n        leaf2.process = Mock(return_value=None)\n\n        tree = MetricTree(root)\n        tree.add_child(leaf1)\n        tree.add_child(leaf2)\n\n        self.assertTrue(tree.process('args') == {'test': 10})\n\n        root.process.assert_called_once_with('args')\n        leaf1.process.assert_called_once_with('test')\n        leaf2.process.assert_called_once_with('test')\n\n    def test_process_final(self):\n        root = Metric('test')\n        root.process_final = Mock(return_value='test')\n        leaf1 = Metric('test')\n        leaf1.process_final = Mock(return_value={'test': 10})\n        leaf2 = Metric('test')\n        leaf2.process_final = Mock(return_value=None)\n\n        tree = MetricTree(root)\n        tree.add_child(leaf1)\n        tree.add_child(leaf2)\n\n        self.assertTrue(tree.process_final('args') == {'test': 10})\n\n        root.process_final.assert_called_once_with('args')\n        leaf1.process_final.assert_called_once_with('test')\n        leaf2.process_final.assert_called_once_with('test')\n\n    def test_train(self):\n        root = Metric('test')\n        root.train = Mock()\n        leaf = Metric('test')\n        leaf.train = Mock()\n\n        tree = MetricTree(root)\n        tree.add_child(leaf)\n\n        tree.train()\n\n        self.assertEqual(root.train.call_count, 1)\n        self.assertEqual(leaf.train.call_count, 1)\n\n    def test_eval(self):\n        root = Metric('test')\n        root.eval = Mock()\n        leaf = Metric('test')\n        leaf.eval = Mock()\n\n        tree = MetricTree(root)\n        tree.add_child(leaf)\n\n        tree.eval()\n\n        self.assertEqual(root.eval.call_count, 1)\n        self.assertEqual(leaf.eval.call_count, 1)\n\n    def test_reset(self):\n        root = Metric('test')\n        root.reset = Mock()\n        leaf = Metric('test')\n        leaf.reset = Mock()\n\n        tree = MetricTree(root)\n        tree.add_child(leaf)\n\n        tree.reset({})\n        root.reset.assert_called_once_with({})\n        leaf.reset.assert_called_once_with({})\n\n    def test_string(self):\n        root = Metric('test')\n        tree = MetricTree(root)\n        self.assertEqual(str(root), str(tree))\n\n\nclass TestMetricList(unittest.TestCase):\n    def test_list_in_list(self):\n        metric = MetricList(['acc', MetricList(['loss'])])\n        self.assertTrue(metric.metric_list[0].name == 'acc')\n        self.assertTrue(metric.metric_list[1].name == 'loss')\n\n    def test_default_acc(self):\n        metric = MetricList(['acc'])\n        self.assertTrue(metric.metric_list[0].name == 'acc', msg='acc not in: ' + str(metric.metric_list))\n\n    def test_default_loss(self):\n        metric = MetricList(['loss'])\n        self.assertTrue(metric.metric_list[0].name == 'loss', msg='loss not in: ' + str(metric.metric_list))\n\n    def test_default_epoch(self):\n        metric = MetricList(['epoch'])\n        self.assertTrue(metric.metric_list[0].name == 'epoch', msg='loss not in: ' + str(metric.metric_list))\n\n    def test_process(self):\n        my_mock = Metric('test')\n        my_mock.process = Mock(return_value={'test': -1})\n        metric = MetricList([my_mock])\n        result = metric.process({'state': -1})\n        self.assertEqual({'test': -1}, result)\n        my_mock.process.assert_called_once_with({'state': -1})\n\n    def test_process_final(self):\n        my_mock = Metric('test')\n        my_mock.process_final = Mock(return_value={'test': -1})\n        metric = MetricList([my_mock])\n        result = metric.process_final({'state': -1})\n        self.assertEqual({'test': -1}, result)\n        my_mock.process_final.assert_called_once_with({'state': -1})\n\n    def test_train(self):\n        my_mock = Metric('test')\n        my_mock.train = Mock(return_value=None)\n        metric = MetricList([my_mock])\n        metric.train()\n        self.assertEqual(my_mock.train.call_count, 1)\n\n    def test_eval(self):\n        my_mock = Metric('test')\n        my_mock.eval = Mock(return_value=None)\n        metric = MetricList([my_mock])\n        metric.eval()\n        self.assertEqual(my_mock.eval.call_count, 1)\n\n    def test_reset(self):\n        my_mock = Metric('test')\n        my_mock.reset = Mock(return_value=None)\n        metric = MetricList([my_mock])\n        metric.reset({'state': -1})\n        my_mock.reset.assert_called_once_with({'state': -1})\n\n\nclass TestAdvancedMetric(unittest.TestCase):\n    def test_empty_methods(self):\n        metric = AdvancedMetric('test')\n\n        self.assertTrue(metric.process_train() is None)\n        self.assertTrue(metric.process_final_train() is None)\n        self.assertTrue(metric.process_validate() is None)\n        self.assertTrue(metric.process_final_validate() is None)\n\n    def test_train(self):\n        metric = AdvancedMetric('test')\n        metric.process_train = Mock()\n        metric.process_final_train = Mock()\n\n        metric.train()\n        metric.process('testing')\n        metric.process_train.assert_called_once_with('testing')\n\n        metric.process_final('testing')\n        metric.process_final_train.assert_called_once_with('testing')\n\n    def test_eval(self):\n        metric = AdvancedMetric('test')\n        metric.process_validate = Mock()\n        metric.process_final_validate = Mock()\n\n        metric.eval()\n        metric.process('testing')\n        metric.process_validate.assert_called_once_with('testing')\n\n        metric.process_final('testing')\n        metric.process_final_validate.assert_called_once_with('testing')\n"""
tests/metrics/test_primitives.py,16,"b""import unittest\n\nimport torch\nfrom torch.autograd import Variable\n\nimport torchbearer\nfrom torchbearer.metrics import Loss, Epoch, CategoricalAccuracy, TopKCategoricalAccuracy, BinaryAccuracy, MeanSquaredError\n\n\nclass TestLoss(unittest.TestCase):\n    def setUp(self):\n        with torch.no_grad():\n            self._state = {\n                torchbearer.LOSS: torch.FloatTensor([2.35])\n            }\n        self._metric = Loss().root  # Get root node of Tree for testing\n\n    def test_train_process(self):\n        self._metric.train()\n        result = self._metric.process(self._state)\n        self.assertAlmostEqual(2.35, result[0], 3, 0.002)\n\n    def test_validate_process(self):\n        self._metric.eval()\n        result = self._metric.process(self._state)\n        self.assertAlmostEqual(2.35, result[0], 3, 0.002)\n\n\nclass TestEpoch(unittest.TestCase):\n    def setUp(self):\n        self._state = {\n            torchbearer.EPOCH: 101\n        }\n        self._metric = Epoch().metric  # Get wrapped metric for testing\n\n    def test_process(self):\n        result = self._metric.process(self._state)\n        self.assertEqual(101, result)\n\n    def test_process_final(self):\n        result = self._metric.process_final(self._state)\n        self.assertEqual(101, result)\n\n\nclass TestBinaryAccuracy(unittest.TestCase):\n    def setUp(self):\n        self._state = {\n            torchbearer.Y_TRUE: torch.LongTensor([\n                [1, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1],\n                [0, 0, 1],\n                [0, 1, 0]\n            ]),\n            torchbearer.Y_PRED: torch.FloatTensor([\n                [0.9, 0.1, 0.1],  # Correct\n                [0.1, 0.9, 0.1],  # Correct\n                [0.1, 0.1, 0.9],  # Correct\n                [0.9, 0.1, 0.1],  # Incorrect\n                [0.9, 0.1, 0.1]  # Incorrect\n            ])\n        }\n        self._targets = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]\n        self._metric = BinaryAccuracy().root  # Get root node of Tree for testing\n\n    def test_train_process(self):\n        self._metric.train()\n        result = self._metric.process(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_validate_process(self):\n        self._metric.eval()\n        result = self._metric.process(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_weird_types(self):\n        state = {\n            torchbearer.Y_TRUE: torch.LongTensor([\n                [1, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1],\n                [0, 0, 1],\n                [0, 1, 0]\n            ]).byte(),\n            torchbearer.Y_PRED: torch.FloatTensor([\n                [0.9, 0.1, 0.1],  # Correct\n                [0.1, 0.9, 0.1],  # Correct\n                [0.1, 0.1, 0.9],  # Correct\n                [0.9, 0.1, 0.1],  # Incorrect\n                [0.9, 0.1, 0.1]  # Incorrect\n            ]).double()\n        }\n\n        self._metric.train()\n        result = self._metric.process(state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_threshold(self):\n        state = {\n            torchbearer.Y_TRUE: torch.FloatTensor([\n                [0.9, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1],\n                [0, 0.3, 1],\n                [0, 0.6, 0]\n            ]),\n            torchbearer.Y_PRED: torch.FloatTensor([\n                [0.9, 0.1, 0.1],  # Correct\n                [0.1, 0.9, 0.1],  # Correct\n                [0.1, 0.1, 0.9],  # Correct\n                [0.9, 0.1, 0.1],  # Incorrect\n                [0.9, 0.1, 0.1]  # Incorrect\n            ])\n        }\n\n        metric = BinaryAccuracy(threshold=0.4).root  # Get root node of Tree for testing\n        metric.train()\n        result = metric.process(state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n\nclass TestCategoricalAccuracy(unittest.TestCase):\n    def setUp(self):\n        self._state = {\n            torchbearer.Y_TRUE: Variable(torch.LongTensor([0, 1, 2, 2, 1])),\n            torchbearer.Y_PRED: Variable(torch.FloatTensor([\n                [0.9, 0.1, 0.1], # Correct\n                [0.1, 0.9, 0.1], # Correct\n                [0.1, 0.1, 0.9], # Correct\n                [0.9, 0.1, 0.1], # Incorrect\n                [0.9, 0.1, 0.1], # Incorrect\n            ])),\n        }\n        self._targets = [1, 1, 1, 0, 0]\n        self._metric = CategoricalAccuracy().root  # Get root node of Tree for testing\n\n    def test_ignore_index(self):\n        metric = CategoricalAccuracy(ignore_index=1).root  # Get root node of Tree for testing\n        targets = [1, 1, 0]\n\n        metric.train()\n        result = metric.process(self._state)\n        for i in range(0, len(targets)):\n            self.assertEqual(result[i], targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_train_process(self):\n        self._metric.train()\n        result = self._metric.process(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_train_process_soft(self):\n        self._metric.train()\n        soft_targets = torch.FloatTensor([\n                [0.98, 0.01, 0.01], # Correct\n                [0.01, 0.98, 0.01], # Correct\n                [0.01, 0.01, 0.98], # Correct\n                [0.01, 0.01, 0.98], # Incorrect\n                [0.01, 0.98, 0.01], # Incorrect\n            ])\n        state = self._state.copy()\n        state[torchbearer.Y_TRUE] = soft_targets\n        result = self._metric.process(state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_validate_process(self):\n        self._metric.eval()\n        result = self._metric.process(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n\nclass TestTopKCategoricalAccuracy(unittest.TestCase):\n    def setUp(self):\n        self._state = {\n            torchbearer.Y_TRUE: Variable(torch.LongTensor([0, 5, 2, 3, 1])),\n            torchbearer.Y_PRED: Variable(torch.FloatTensor([\n                [0.9, 0.8, 0.7, 0.6, 0.5, 0.4],  # Correct\n                [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],  # Correct\n                [0.6, 0.5, 0.4, 0.7, 0.8, 0.9],  # Incorrect\n                [0.6, 0.5, 0.7, 0.4, 0.8, 0.9],  # Incorrect\n                [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # Correct\n            ]))\n        }\n        self._targets = [1, 1, 0, 0, 1]\n        self._metric = TopKCategoricalAccuracy(k=5).root  # Get root node of Tree for testing\n\n    def test_ignore_index(self):\n        metric = TopKCategoricalAccuracy(ignore_index=1).root  # Get root node of Tree for testing\n        targets = [1, 1, 0, 0]\n\n        metric.train()\n        result = metric.process(self._state)\n        for i in range(0, len(targets)):\n            self.assertEqual(result[i], targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_train_process(self):\n        self._metric.train()\n        result = self._metric.process(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_validate_process(self):\n        self._metric.eval()\n        result = self._metric.process(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertEqual(result[i], self._targets[i],\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_top_ten_default(self):\n        metric = torchbearer.metrics.get_default('top_10_acc').root\n        self.assertEqual(metric.k, 10)\n\n\nclass TestMeanSquaredError(unittest.TestCase):\n    def setUp(self):\n        self._state = {\n            torchbearer.Y_TRUE: Variable(torch.FloatTensor(\n                [0.8, 0.2, 0.0, 0.4, 0.3, 0.7]\n            )),\n            torchbearer.Y_PRED: Variable(torch.FloatTensor(\n                [0.9, 0.1, 0.1, 0.7, 0.5, 0.6]\n            ))\n        }\n        self._targets = [0.01, 0.01, 0.01, 0.09, 0.04, 0.01]\n        self._metric = MeanSquaredError().root  # Get root node of Tree for testing\n\n    def test_train_process(self):\n        self._metric.train()\n        result = self._metric.process(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertAlmostEqual(result[i].item(), self._targets[i], places=3,\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))\n\n    def test_validate_process(self):\n        self._metric.eval()\n        result = self._metric.process(self._state)\n        for i in range(0, len(self._targets)):\n            self.assertAlmostEqual(result[i].item(), self._targets[i], places=3,\n                             msg='returned: ' + str(result[i]) + ' expected: ' + str(self._targets[i])\n                                 + ' in: ' + str(result))"""
tests/metrics/test_roc_auc_score.py,12,"b""import unittest\n\nfrom mock import Mock, patch\n\nimport torchbearer\nfrom torchbearer.metrics import RocAucScore, MetricList\n\nimport torch\n\nimport numpy as np\n\n\nclass TestRocAucScore(unittest.TestCase):\n    @patch('sklearn.metrics')\n    def test_one_hot(self, mock_sklearn_metrics):\n        mock_sklearn_metrics.roc_auc_score = Mock()\n        metric = RocAucScore(one_hot_classes=3, one_hot_offset=1)\n        metric.reset({torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32})\n        res = metric.process({torchbearer.BATCH: 0, torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32,\n                        torchbearer.Y_TRUE: torch.LongTensor([1, 2, 3]),\n                        torchbearer.Y_PRED: torch.FloatTensor([[0.0, 0.0, 0.0], [1.1, 1.1, 1.1], [2.2, 2.2, 2.2]])})\n        self.assertTrue('roc_auc_score' in res)\n        self.assertEqual(mock_sklearn_metrics.roc_auc_score.call_count, 1)\n        self.assertTrue(np.array_equal(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n                                       mock_sklearn_metrics.roc_auc_score.call_args_list[0][0][0]))\n        try:\n            np.testing.assert_array_almost_equal(np.array([[0.0, 0.0, 0.0], [1.1, 1.1, 1.1], [2.2, 2.2, 2.2]]),\n                                                                                   mock_sklearn_metrics.roc_auc_score.call_args_list[0][0][1])\n        except AssertionError:\n            self.fail('y_pred not correctly passed to sklearn')\n\n    @patch('sklearn.metrics')\n    def test_non_one_hot_grad(self, mock_sklearn_metrics):\n        mock_sklearn_metrics.roc_auc_score = Mock()\n        metric = RocAucScore(one_hot_labels=False)\n        metric.reset({torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32})\n        state = {torchbearer.BATCH: 0, torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32,\n                              torchbearer.Y_TRUE: torch.LongTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]]),\n                              torchbearer.Y_PRED: torch.FloatTensor(\n                                  [[0.0, 0.0, 0.0], [1.1, 1.1, 1.1], [2.2, 2.2, 2.2]])}\n        state[torchbearer.Y_PRED].requires_grad = True\n        res = metric.process(state)\n        self.assertTrue('roc_auc_score' in res)\n        self.assertEqual(mock_sklearn_metrics.roc_auc_score.call_count, 1)\n        self.assertTrue(np.array_equal(np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]),\n                                       mock_sklearn_metrics.roc_auc_score.call_args_list[0][0][0]))\n        try:\n            np.testing.assert_array_almost_equal(np.array([[0.0, 0.0, 0.0], [1.1, 1.1, 1.1], [2.2, 2.2, 2.2]]),\n                                                 mock_sklearn_metrics.roc_auc_score.call_args_list[0][0][1])\n        except AssertionError:\n            self.fail('y_pred not correctly passed to sklearn')\n\n    @patch('sklearn.metrics')\n    def test_non_one_hot(self, mock_sklearn_metrics):\n        mock_sklearn_metrics.roc_auc_score = Mock()\n        metric = RocAucScore(one_hot_labels=False)\n        metric.reset({torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32})\n        res = metric.process({torchbearer.BATCH: 0, torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32,\n                        torchbearer.Y_TRUE: torch.LongTensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]]),\n                        torchbearer.Y_PRED: torch.FloatTensor([[0.0, 0.0, 0.0], [1.1, 1.1, 1.1], [2.2, 2.2, 2.2]])})\n        self.assertTrue('roc_auc_score' in res)\n        self.assertEqual(mock_sklearn_metrics.roc_auc_score.call_count, 1)\n        self.assertTrue(np.array_equal(np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]),\n                                       mock_sklearn_metrics.roc_auc_score.call_args_list[0][0][0]))\n        try:\n            np.testing.assert_array_almost_equal(np.array([[0.0, 0.0, 0.0], [1.1, 1.1, 1.1], [2.2, 2.2, 2.2]]),\n                                                                                   mock_sklearn_metrics.roc_auc_score.call_args_list[0][0][1])\n        except AssertionError:\n            self.fail('y_pred not correctly passed to sklearn')\n\n    def test_default_roc(self):\n        mlist = MetricList(['roc_auc'])\n        self.assertTrue(mlist.metric_list[0].name == 'roc_auc_score')\n\n        mlist = MetricList(['roc_auc_score'])\n        self.assertTrue(mlist.metric_list[0].name == 'roc_auc_score')\n"""
tests/metrics/test_timer.py,0,"b""from unittest import TestCase\n\nfrom mock import Mock, MagicMock, patch\n\nimport torchbearer\nfrom torchbearer.metrics.timer import TimerMetric, _TimerMetric\n\n\nclass TestTimer(TestCase):\n    def test_update_time(self):\n        timer = TimerMetric('test')\n        timerMetric = TimerMetric('test2')\n        timerMetric.process = Mock(return_value=1)\n        timer.update_time('test', timerMetric, {})\n        self.assertTrue(timer.get_timings()['test'] == 1)\n\n        timerMetric.process = Mock(return_value=2)\n        timer.update_time('test_2', timerMetric, {})\n        self.assertTrue(timer.get_timings()['test_2'] == 2)\n\n    def test_calls(self):\n        timer = TimerMetric('test')\n        timer.batch_timer = MagicMock()\n        timer.epoch_timer = MagicMock()\n        timer.train_timer = MagicMock()\n        timer.total_timer = MagicMock()\n        timer.valid_timer = MagicMock()\n\n        timer.on_start({})\n        timer.on_start_training({})\n        timer.on_start_epoch({})\n        timer.on_sample({})\n        timer.on_forward({})\n        timer.on_criterion({})\n        timer.on_backward({})\n        timer.on_step_training({})\n        timer.on_start_validation({})\n        timer.on_sample_validation({})\n        timer.on_forward_validation({})\n        timer.on_criterion_validation({})\n        timer.on_step_validation({})\n        timer.on_end_training({})\n        timer.on_end_validation({})\n        timer.on_end_epoch({})\n        timer.on_end({})\n\n        self.assertTrue(timer.batch_timer.process.call_count == 11)\n        self.assertTrue(timer.total_timer.process.call_count == 1)\n        self.assertTrue(timer.epoch_timer.process.call_count == 1)\n        self.assertTrue(timer.train_timer.process.call_count == 2)\n        self.assertTrue(timer.valid_timer.process.call_count == 1)\n\n    def test_process(self):\n        timer = TimerMetric((torchbearer.metrics.timer.ON_FORWARD, ))\n        timer.time_dict = {torchbearer.metrics.timer.ON_FORWARD: 1, 'test': 2}\n        self.assertTrue(timer.process({})[torchbearer.metrics.timer.ON_FORWARD] == 1)\n\n    def test_reset(self):\n        state = {torchbearer.CALLBACK_LIST: torchbearer.callbacks.CallbackList([])}\n\n        timer = TimerMetric()\n        self.assertTrue(state[torchbearer.CALLBACK_LIST].callback_list == [])\n        timer.reset(state)\n        self.assertIsInstance(state[torchbearer.CALLBACK_LIST].callback_list[0], TimerMetric)\n\n        timer.reset(state)\n        self.assertTrue(len(state[torchbearer.CALLBACK_LIST].callback_list) == 1)\n\n\nclass TestTimerMetric(TestCase):\n    @patch('time.time')\n    def test_process(self, time):\n        time.return_value = 1\n        timer_metric = _TimerMetric('test')\n        time.return_value = 2\n        dt = timer_metric.process({})\n\n        self.assertTrue(dt == 1)\n\n    @patch('time.time')\n    def test_reset(self, time):\n        time.return_value = 1\n        timer_metric = _TimerMetric('test')\n\n        time.return_value = 3\n        timer_metric.reset({})\n        self.assertTrue(timer_metric.t == 3)\n\n"""
tests/metrics/test_wrappers.py,19,"b""import unittest\n\nimport torch\nfrom mock import Mock, call\nfrom torch.autograd import Variable\n\nimport torchbearer\nfrom torchbearer.metrics import Metric, BatchLambda, EpochLambda, ToDict\n\n\nclass TestToDict(unittest.TestCase):\n    def setUp(self):\n        self._metric = Metric('test')\n        self._metric.train = Mock()\n        self._metric.eval = Mock()\n        self._metric.reset = Mock()\n        self._metric.process = Mock(return_value='process')\n        self._metric.process_final = Mock(return_value='process_final')\n\n        self._to_dict = ToDict(self._metric)\n\n    def test_train_process(self):\n        self._to_dict.train()\n        self.assertEqual(self._metric.train.call_count, 1)\n\n        self.assertTrue(self._to_dict.process('input') == {'test': 'process'})\n        self._metric.process.assert_called_once_with('input')\n\n    def test_train_process_final(self):\n        self._to_dict.train()\n        self.assertEqual(self._metric.train.call_count, 1)\n\n        self.assertTrue(self._to_dict.process_final('input') == {'test': 'process_final'})\n        self._metric.process_final.assert_called_once_with('input')\n\n    def test_eval_process(self):\n        self._to_dict.eval()\n        self.assertEqual(self._metric.eval.call_count, 1)\n\n        self.assertTrue(self._to_dict.process('input') == {'val_test': 'process'})\n        self._metric.process.assert_called_once_with('input')\n\n    def test_eval_process_final(self):\n        self._to_dict.eval()\n        self.assertEqual(self._metric.eval.call_count, 1)\n\n        self.assertTrue(self._to_dict.process_final('input') == {'val_test': 'process_final'})\n        self._metric.process_final.assert_called_once_with('input')\n\n    def test_eval_train(self):\n        self._to_dict.eval(data_key=torchbearer.TRAIN_DATA)\n        self.assertEqual(self._metric.eval.call_count, 1)\n\n        self.assertTrue(self._to_dict.process_final('input') == {'train_test': 'process_final'})\n        self._metric.process_final.assert_called_once_with('input')\n\n    def test_eval_test(self):\n        self._to_dict.eval(data_key=torchbearer.TEST_DATA)\n        self.assertEqual(self._metric.eval.call_count, 1)\n\n        self.assertTrue(self._to_dict.process_final('input') == {'test_test': 'process_final'})\n        self._metric.process_final.assert_called_once_with('input')\n\n    def test_reset(self):\n        self._to_dict.reset('test')\n        self._metric.reset.assert_called_once_with('test')\n\n\nclass TestBatchLambda(unittest.TestCase):\n    def setUp(self):\n        self._metric_function = Mock(return_value='test')\n        self._metric = BatchLambda('test', self._metric_function)\n        self._states = [{torchbearer.Y_TRUE: Variable(torch.FloatTensor([1])), torchbearer.Y_PRED: Variable(torch.FloatTensor([2]))},\n                        {torchbearer.Y_TRUE: Variable(torch.FloatTensor([3])), torchbearer.Y_PRED: Variable(torch.FloatTensor([4]))},\n                        {torchbearer.Y_TRUE: Variable(torch.FloatTensor([5])), torchbearer.Y_PRED: Variable(torch.FloatTensor([6]))}]\n\n    def test_train(self):\n        self._metric.train()\n        calls = []\n        for i in range(len(self._states)):\n            self._metric.process(self._states[i])\n            calls.append(call(self._states[i][torchbearer.Y_PRED].data, self._states[i][torchbearer.Y_TRUE].data))\n        self._metric_function.assert_has_calls(calls)\n\n    def test_validate(self):\n        self._metric.eval()\n        calls = []\n        for i in range(len(self._states)):\n            self._metric.process(self._states[i])\n            calls.append(call(self._states[i][torchbearer.Y_PRED].data, self._states[i][torchbearer.Y_TRUE].data))\n        self._metric_function.assert_has_calls(calls)\n\n\nclass TestEpochLambda(unittest.TestCase):\n    def setUp(self):\n        self._metric_function = Mock(return_value='test')\n        self._metric = EpochLambda('test', self._metric_function, step_size=3)\n        self._metric.reset({torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32})\n        self._states = [{torchbearer.BATCH: 0, torchbearer.Y_TRUE: torch.LongTensor([0]), torchbearer.Y_PRED: torch.FloatTensor([0.0]), torchbearer.DEVICE: 'cpu'},\n                        {torchbearer.BATCH: 1, torchbearer.Y_TRUE: torch.LongTensor([1]), torchbearer.Y_PRED: torch.FloatTensor([0.1]), torchbearer.DEVICE: 'cpu'},\n                        {torchbearer.BATCH: 2, torchbearer.Y_TRUE: torch.LongTensor([2]), torchbearer.Y_PRED: torch.FloatTensor([0.2]), torchbearer.DEVICE: 'cpu'},\n                        {torchbearer.BATCH: 3, torchbearer.Y_TRUE: torch.LongTensor([3]), torchbearer.Y_PRED: torch.FloatTensor([0.3]), torchbearer.DEVICE: 'cpu'},\n                        {torchbearer.BATCH: 4, torchbearer.Y_TRUE: torch.LongTensor([4]), torchbearer.Y_PRED: torch.FloatTensor([0.4]), torchbearer.DEVICE: 'cpu'}]\n\n    def test_train(self):\n        self._metric.train()\n        calls = [[torch.FloatTensor([0.0]), torch.LongTensor([0])],\n                 [torch.FloatTensor([0.0, 0.1, 0.2, 0.3]), torch.LongTensor([0, 1, 2, 3])]]\n        for i in range(len(self._states)):\n            self._metric.process(self._states[i])\n        self.assertEqual(2, len(self._metric_function.call_args_list))\n        for i in range(len(self._metric_function.call_args_list)):\n            self.assertTrue(torch.eq(self._metric_function.call_args_list[i][0][0], calls[i][0]).all)\n            self.assertTrue(torch.lt(torch.abs(torch.add(self._metric_function.call_args_list[i][0][1], -calls[i][1])), 1e-12).all)\n        self._metric_function.reset_mock()\n        self._metric.process_final({})\n\n        self.assertEqual(self._metric_function.call_count, 1)\n        self.assertTrue(torch.eq(self._metric_function.call_args_list[0][0][1], torch.LongTensor([0, 1, 2, 3, 4])).all)\n        self.assertTrue(torch.lt(torch.abs(torch.add(self._metric_function.call_args_list[0][0][0], -torch.FloatTensor([0.0, 0.1, 0.2, 0.3, 0.4]))), 1e-12).all)\n\n    def test_validate(self):\n        self._metric.eval()\n        for i in range(len(self._states)):\n            self._metric.process(self._states[i])\n        self._metric_function.assert_not_called()\n        self._metric.process_final_validate({})\n\n        self.assertEqual(self._metric_function.call_count, 1)\n        self.assertTrue(torch.eq(self._metric_function.call_args_list[0][0][1], torch.LongTensor([0, 1, 2, 3, 4])).all)\n        self.assertTrue(torch.lt(torch.abs(torch.add(self._metric_function.call_args_list[0][0][0], -torch.FloatTensor([0.0, 0.1, 0.2, 0.3, 0.4]))), 1e-12).all)\n\n    def test_not_running(self):\n        metric = EpochLambda('test', self._metric_function, running=False, step_size=6)\n        metric.reset({torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32})\n        metric.train()\n\n        for i in range(12):\n            metric.process(self._states[0])\n\n        self._metric_function.assert_not_called()\n"""
torchbearer/callbacks/__init__.py,0,"b'from torchbearer import Callback\nfrom .callbacks import *\nfrom .unpack_state import *\nfrom .cutout import Cutout, RandomErase, CutMix\nfrom .lsuv import LSUV\nfrom .checkpointers import *\nfrom .csv_logger import *\nfrom .early_stopping import *\nfrom .gradient_clipping import *\nfrom .printer import ConsolePrinter, Tqdm\nfrom .tensor_board import TensorBoard, TensorBoardImages, TensorBoardProjector, TensorBoardText\nfrom .terminate_on_nan import *\nfrom .torch_scheduler import *\nfrom .weight_decay import *\nfrom .aggregate_predictions import *\nfrom .decorators import *\nfrom .live_loss_plot import LiveLossPlot\nfrom . import init\nfrom . import imaging\nfrom .pycm import PyCM\nfrom .mixup import Mixup, MixupAcc\nfrom .sample_pairing import SamplePairing\nfrom .label_smoothing import LabelSmoothingRegularisation\nfrom .between_class import BCPlus\n'"
torchbearer/callbacks/aggregate_predictions.py,2,"b""import torchbearer\nfrom torchbearer.callbacks import Callback\nimport torch\nimport warnings\n\n\nclass AggregatePredictions(Callback):\n    def __init__(self):\n        super(AggregatePredictions, self).__init__()\n        self.predictions_list = []\n\n    def on_step_validation(self, state):\n        super(AggregatePredictions, self).on_step_validation(state)\n        self.predictions_list.append(state[torchbearer.Y_PRED])\n\n    def on_end_validation(self, state):\n        try:\n            if len(self.predictions_list) == 1 and type(self.predictions_list[0]) is torch.Tensor:\n                state[torchbearer.FINAL_PREDICTIONS] = self.predictions_list[0]\n            else:\n                state[torchbearer.FINAL_PREDICTIONS] = torch.cat(self.predictions_list, 0)\n        except:\n            warnings.warn('Failed to format predictions as tensor, returning as list.')\n            state[torchbearer.FINAL_PREDICTIONS] = self.predictions_list\n\n    def on_end_epoch(self, state):\n        super(AggregatePredictions, self).on_end_epoch(state)\n        self.predictions_list = []\n"""
torchbearer/callbacks/between_class.py,5,"b'import torchbearer\nfrom torchbearer import Callback\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions import Beta\n\nfrom torchbearer.bases import cite\n\nbc = """"""\n@inproceedings{tokozume2018between,\n  title={Between-class learning for image classification},\n  author={Tokozume, Yuji and Ushiku, Yoshitaka and Harada, Tatsuya},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={5486--5494},\n  year={2018}\n}\n""""""\n\n\n@cite(bc)\nclass BCPlus(Callback):\n    """"""BC+ callback which mixes images by treating them as waveforms. For standard BC, see :class:`.Mixup`.\n    This callback can optionally convert labels to one hot before combining them according to the lambda parameters,\n    sampled from a beta distribution, use alpha=1 to replicate the paper. Use with :meth:`BCPlus.bc_loss` or set\n    `mixup_loss = True` and use :meth:`.Mixup.mixup_loss`.\n\n    .. note::\n\n       This callback first sets all images to have zero mean. Consider adding an offset (e.g. 0.5) back before\n       visualising.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import BCPlus\n\n        # Example Trial which does BCPlus regularisation\n        >>> bcplus = BCPlus(classes=10)\n        >>> trial = Trial(None, criterion=BCPlus.bc_loss, callbacks=[bcplus], metrics=[\'acc\'])\n\n    Args:\n        mixup_loss (bool): If True, the lambda and targets will be stored for use with the mixup loss function.\n        alpha (float): The alpha value for the beta distribution.\n        classes (int): The number of classes for conversion to one hot.\n\n    State Requirements:\n        - :attr:`torchbearer.state.X`: State should have the current data stored and correctly normalised\n        - :attr:`torchbearer.state.Y_TRUE`: State should have the current data stored\n    """"""\n\n    def __init__(self, mixup_loss=False, alpha=1, classes=-1):\n        super(BCPlus, self).__init__()\n        self.mixup_loss = mixup_loss\n        self.classes = classes\n        self.dist = Beta(torch.tensor([float(alpha)]), torch.tensor([float(alpha)]))\n\n    @staticmethod\n    def bc_loss(state):\n        """"""The KL divergence between the outputs of the model and the ratio labels. Model ouputs should be un-normalised\n        logits as this function performs a log_softmax.\n\n        Args:\n            state: The current :class:`Trial` state.\n        """"""\n        prediction, target = state[torchbearer.Y_PRED], state[torchbearer.Y_TRUE]\n\n        entropy = - (target[target.nonzero().split(1, dim=1)] * target[target.nonzero().split(1, dim=1)].log()).sum()\n        cross = - (target * F.log_softmax(prediction, dim=1)).sum()\n\n        return (cross - entropy) / prediction.size(0)\n\n    def _to_one_hot(self, target):\n        if target.dim() == 1:\n            target = target.unsqueeze(1)\n            one_hot = torch.zeros_like(target).repeat(1, self.classes)\n            one_hot.scatter_(1, target, 1)\n            return one_hot\n        return target.float()\n\n    def on_sample(self, state):\n        super(BCPlus, self).on_sample(state)\n\n        lam = self.dist.sample().to(state[torchbearer.DEVICE])\n\n        permutation = torch.randperm(state[torchbearer.X].size(0))\n\n        batch1 = state[torchbearer.X]\n        batch1 = batch1 - batch1.view(batch1.size(0), -1).mean(1, keepdim=True).view(*tuple([batch1.size(0)] + [1] * (batch1.dim() - 1)))\n        g1 = batch1.view(batch1.size(0), -1).std(1, keepdim=True).view(*tuple([batch1.size(0)] + [1] * (batch1.dim() - 1)))\n\n        batch2 = batch1[permutation]\n        g2 = g1[permutation]\n\n        p = 1. / (1 + ((g1 / g2) * ((1 - lam) / lam)))\n\n        state[torchbearer.X] = (batch1 * p + batch2 * (1 - p)) / (p.pow(2) + (1 - p).pow(2)).sqrt()\n\n        if not self.mixup_loss:\n            target = self._to_one_hot(state[torchbearer.TARGET]).float()\n            state[torchbearer.Y_TRUE] = lam * target + (1 - lam) * target[permutation]\n        else:\n            state[torchbearer.MIXUP_LAMBDA] = lam\n            state[torchbearer.MIXUP_PERMUTATION] = permutation\n            state[torchbearer.Y_TRUE] = (state[torchbearer.Y_TRUE], state[torchbearer.Y_TRUE][state[torchbearer.MIXUP_PERMUTATION]])\n\n    def on_sample_validation(self, state):\n        super(BCPlus, self).on_sample_validation(state)\n        if not self.mixup_loss:\n            state[torchbearer.TARGET] = self._to_one_hot(state[torchbearer.TARGET]).float()\n'"
torchbearer/callbacks/callbacks.py,0,"b'from torchbearer import Callback\n\n\nclass CallbackList(Callback):\n    """"""The :class:`CallbackList` class is a wrapper for a list of callbacks which acts as a single :class:`.Callback` and\n    internally calls each :class:`.Callback` in the given list in turn.\n\n    Args:\n        callback_list (list): The list of callbacks to be wrapped. If the list contains a :class:`CallbackList`, this\n            will be unwrapped.\n    """"""\n\n    CALLBACK_STATES = \'callback_states\'\n    CALLBACK_TYPES = \'callback_types\'\n\n    def __init__(self, callback_list):\n        super(CallbackList, self).__init__()\n        self.callback_list = []\n        self.append(callback_list)\n\n    def state_dict(self):\n        """"""Get a dict containing all of the callback states.\n\n        Returns:\n            dict: A dict containing parameters and persistent buffers.\n        """"""\n        state_dict = {\n            CallbackList.CALLBACK_STATES: [],\n            CallbackList.CALLBACK_TYPES: []\n        }\n\n        def to_state(callback):\n            state_dict[CallbackList.CALLBACK_STATES].append(callback.state_dict())\n            state_dict[CallbackList.CALLBACK_TYPES].append(callback.__class__)\n\n        self._for_list(to_state)\n\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        """"""Resume this callback list from the given state. Callbacks must be given in the same order for this to work.\n\n        Args:\n            state_dict (dict): The state dict to reload\n\n        Returns:\n            CallbackList: self\n        """"""\n\n        t_iter = iter(state_dict[CallbackList.CALLBACK_TYPES])\n        s_iter = iter(state_dict[CallbackList.CALLBACK_STATES])\n\n        def from_state(callback):\n            if callback.__class__ == next(t_iter):\n                callback.load_state_dict(next(s_iter))\n            else:\n                import warnings\n                warnings.warn(\'Callback classes did not match, expected: \' + str([c.__name__ for c in state_dict[CallbackList.CALLBACK_TYPES]]))\n\n        self._for_list(from_state)\n\n        return self\n\n    def _for_list(self, function):\n        for callback in self.callback_list:\n            function(callback)\n\n    def __str__(self):\n        return str([str(c) for c in self.callback_list])\n\n    def __iter__(self):\n        return self.callback_list.__iter__()\n\n    def __copy__(self):\n        return CallbackList(self.callback_list)\n\n    def copy(self):\n        return self.__copy__()\n\n    def append(self, callback_list):\n        for callback in callback_list:\n            if isinstance(callback, CallbackList):\n                self.callback_list = self.callback_list + callback.callback_list\n            else:\n                self.callback_list.append(callback)\n\n    def on_init(self, state):\n        """"""Call on_init on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_init(state))\n\n    def on_start(self, state):\n        """"""Call on_start on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_start(state))\n\n    def on_start_epoch(self, state):\n        """"""Call on_start_epoch on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_start_epoch(state))\n\n    def on_start_training(self, state):\n        """"""Call on_start_training on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_start_training(state))\n\n    def on_sample(self, state):\n        """"""Call on_sample on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_sample(state))\n\n    def on_forward(self, state):\n        """"""Call on_forward on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_forward(state))\n\n    def on_criterion(self, state):\n        """"""Call on_criterion on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_criterion(state))\n\n    def on_backward(self, state):\n        """"""Call on_backward on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_backward(state))\n\n    def on_step_training(self, state):\n        """"""Call on_step_training on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_step_training(state))\n\n    def on_end_training(self, state):\n        """"""Call on_end_training on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_end_training(state))\n\n    def on_start_validation(self, state):\n        """"""Call on_start_validation on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_start_validation(state))\n\n    def on_sample_validation(self, state):\n        """"""Call on_sample_validation on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_sample_validation(state))\n\n    def on_forward_validation(self, state):\n        """"""Call on_forward_validation on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_forward_validation(state))\n\n    def on_criterion_validation(self, state):\n        """"""Call on_criterion_validation on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_criterion_validation(state))\n\n    def on_step_validation(self, state):\n        """"""Call on_step_validation on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_step_validation(state))\n\n    def on_end_validation(self, state):\n        """"""Call on_end_validation on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_end_validation(state))\n\n    def on_end_epoch(self, state):\n        """"""Call on_end_epoch on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_end_epoch(state))\n\n    def on_checkpoint(self, state):\n        """"""Call on_checkpoint on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_checkpoint(state))\n\n    def on_end(self, state):\n        """"""Call on_end on each callback in turn with the given state.\n\n        Args:\n            state (dict[str,any]): The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda callback: callback.on_end(state))\n'"
torchbearer/callbacks/checkpointers.py,17,"b'import torchbearer\n\nimport torch\n\nfrom torchbearer.callbacks.callbacks import Callback\nimport os\nimport warnings\nfrom torchbearer.bases import get_metric\n\n\nclass _Checkpointer(Callback):\n    def __init__(self, fileformat, save_model_params_only=False, pickle_module=torch.serialization.pickle, pickle_protocol=torch.serialization.DEFAULT_PROTOCOL):\n        super(_Checkpointer, self).__init__()\n        self.fileformat = fileformat\n\n        self.pickle_module = pickle_module\n        self.pickle_protocol = pickle_protocol\n\n        self.save_model_params_only = save_model_params_only\n\n        self.most_recent = None\n\n        if fileformat.__contains__(os.sep) and not os.path.exists(os.path.dirname(fileformat)):\n            os.makedirs(os.path.dirname(fileformat))\n\n    def save_checkpoint(self, model_state, overwrite_most_recent=False):\n        state = {}\n        state.update(model_state)\n        state.update(model_state[torchbearer.METRICS])\n\n        string_state = {str(key): state[key] for key in state.keys()}\n        filepath = self.fileformat.format(**string_state)\n\n        if self.most_recent is not None and overwrite_most_recent:\n            try:\n                os.remove(self.most_recent)\n            except OSError:\n                warnings.warn(\'Failed to delete old file. Are you running two checkpointers with the same filename?\')\n\n        if self.save_model_params_only:\n            torch.save(model_state[torchbearer.MODEL].state_dict(), filepath, pickle_module=self.pickle_module,\n                       pickle_protocol=self.pickle_protocol)\n        else:\n            torch.save(model_state[torchbearer.SELF].state_dict(), filepath, pickle_module=self.pickle_module,\n                       pickle_protocol=self.pickle_protocol)\n\n        self.most_recent = filepath\n\n\ndef ModelCheckpoint(filepath=\'model.{epoch:02d}-{val_loss:.2f}.pt\', save_model_params_only=False,\n        monitor=\'val_loss\', save_best_only=False, mode=\'auto\', period=1, min_delta=0):\n    """"""Save the model after every epoch. `filepath` can contain named formatting options, which will be filled any\n    values from state. For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}`, then the model checkpoints\n    will be saved with the epoch number and the validation loss in the filename. The torch :class:`.Trial` will be\n    saved to filename.\n\n\n    Example: ::\n\n        >>> from torchbearer.callbacks import ModelCheckpoint\n        >>> from torchbearer import Trial\n        >>> import torch\n\n        # Example Trial (without optimiser or loss criterion) which uses this checkpointer\n        >>> model = torch.nn.Linear(1,1)\n        >>> checkpoint = ModelCheckpoint(\'my_path.pt\', monitor=\'val_acc\', mode=\'max\')\n        >>> trial = Trial(model, callbacks=[checkpoint], metrics=[\'acc\'])\n\n    Args:\n        filepath (str): Path to save the model file\n        save_model_params_only (bool): If `save_model_params_only=True`, only model parameters will be saved so that\n            the results can be loaded into a PyTorch nn.Module. The other option, `save_model_params_only=False`,\n            should be used only if the results will be loaded into a Torchbearer Trial object later.\n        monitor (str): Quantity to monitor\n        save_best_only (bool): If `save_best_only=True`, the latest best model according to the quantity\n            monitored will not be overwritten\n        mode (str): One of {auto, min, max}. If `save_best_only=True`, the decision to overwrite the current\n            save file is made based on either the maximization or the minimization of the monitored quantity. For\n            `val_acc`, this should be `max`, for `val_loss` this should be `min`, etc. In `auto` mode, the direction is\n            automatically inferred from the name of the monitored quantity.\n        period (int): Interval (number of epochs) between checkpoints\n        min_delta (float): If `save_best_only=True`, this is the minimum improvement required to trigger a save\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `state_dict` method\n        - :attr:`torchbearer.state.METRICS`: Metrics dictionary should exist\n        - :attr:`torchbearer.state.SELF`: Self should be the :attr:`torchbearer.Trial` which is running this callback\n    """"""\n    if save_best_only:\n        check = Best(filepath, save_model_params_only, monitor, mode, period, min_delta)\n    else:\n        check = Interval(filepath, save_model_params_only, period)\n\n    return check\n\n\nclass MostRecent(_Checkpointer):\n    """"""Model checkpointer which saves the most recent model to a given filepath. `filepath` can contain named\n    formatting options, which will be filled any values from state. For example: if `filepath` is\n    `weights.{epoch:02d}-{val_loss:.2f}`, then the model checkpoints will be saved with the epoch number and the\n    validation loss in the filename.\n\n    Example: ::\n\n        >>> from torchbearer.callbacks import MostRecent\n        >>> from torchbearer import Trial\n        >>> import torch\n\n        # Example Trial (without optimiser or loss criterion) which uses this checkpointer\n        >>> model = torch.nn.Linear(1,1)\n        >>> checkpoint = MostRecent(\'my_path.pt\')\n        >>> trial = Trial(model, callbacks=[checkpoint], metrics=[\'acc\'])\n\n    Args:\n        filepath (str): Path to save the model file\n        save_model_params_only (bool): If `save_model_params_only=True`, only model parameters will be saved so that\n            the results can be loaded into a PyTorch nn.Module. The other option, `save_model_params_only=False`,\n            should be used only if the results will be loaded into a Torchbearer Trial object later.\n        pickle_module (module): The pickle module to use, default is \'torch.serialization.pickle\'\n        pickle_protocol (int): The pickle protocol to use, default is \'torch.serialization.DEFAULT_PROTOCOL\'\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `state_dict` method\n        - :attr:`torchbearer.state.METRICS`: Metrics dictionary should exist\n        - :attr:`torchbearer.state.SELF`: Self should be the :attr:`torchbearer.Trial` which is running this callback\n    """"""\n\n    def __init__(self, filepath=\'model.{epoch:02d}-{val_loss:.2f}.pt\', save_model_params_only=False,\n                 pickle_module=torch.serialization.pickle, pickle_protocol=torch.serialization.DEFAULT_PROTOCOL):\n\n        super(MostRecent, self).__init__(filepath, save_model_params_only=save_model_params_only,\n                                         pickle_module=pickle_module, pickle_protocol=pickle_protocol)\n        self.filepath = filepath\n\n    def on_checkpoint(self, state):\n        super(MostRecent, self).on_end_epoch(state)\n        self.save_checkpoint(state, overwrite_most_recent=True)\n\n\nclass Best(_Checkpointer):\n    """"""Model checkpointer which saves the best model according to the given configurations. `filepath` can contain\n    named formatting options, which will be filled any values from state. For example: if `filepath` is\n    `weights.{epoch:02d}-{val_loss:.2f}`, then the model checkpoints will be saved with the epoch number and the\n    validation loss in the filename.\n\n    Example: ::\n\n        >>> from torchbearer.callbacks import Best\n        >>> from torchbearer import Trial\n        >>> import torch\n\n        # Example Trial (without optimiser or loss criterion) which uses this checkpointer\n        >>> model = torch.nn.Linear(1,1)\n        >>> checkpoint = Best(\'my_path.pt\', monitor=\'val_acc\', mode=\'max\')\n        >>> trial = Trial(model, callbacks=[checkpoint], metrics=[\'acc\'])\n\n    Args:\n        filepath (str): Path to save the model file\n        save_model_params_only (bool): If `save_model_params_only=True`, only model parameters will be saved so that\n            the results can be loaded into a PyTorch nn.Module. The other option, `save_model_params_only=False`,\n            should be used only if the results will be loaded into a Torchbearer Trial object later.\n        monitor (str): Quantity to monitor\n        mode (str): One of {auto, min, max}. If `save_best_only=True`, the decision to overwrite the current save file\n            is made based on either the maximization or the minimization of the monitored quantity. For `val_acc`, this\n            should be `max`, for `val_loss` this should be `min`, etc. In `auto` mode, the direction is automatically\n            inferred from the name of the monitored quantity.\n        period (int): Interval (number of epochs) between checkpoints\n        min_delta (float): If `save_best_only=True`, this is the minimum improvement required to trigger a save\n        pickle_module (module): The pickle module to use, default is \'torch.serialization.pickle\'\n        pickle_protocol (int): The pickle protocol to use, default is \'torch.serialization.DEFAULT_PROTOCOL\'\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `state_dict` method\n        - :attr:`torchbearer.state.METRICS`: Metrics dictionary should exist, with the `monitor` key populated\n        - :attr:`torchbearer.state.SELF`: Self should be the :attr:`torchbearer.Trial` which is running this callback\n    """"""\n\n    def __init__(self, filepath=\'model.{epoch:02d}-{val_loss:.2f}.pt\', save_model_params_only=False, monitor=\'val_loss\',\n                 mode=\'auto\', period=1, min_delta=0, pickle_module=torch.serialization.pickle,\n                 pickle_protocol=torch.serialization.DEFAULT_PROTOCOL):\n\n        super(Best, self).__init__(filepath, save_model_params_only=save_model_params_only,\n                                   pickle_module=pickle_module, pickle_protocol=pickle_protocol)\n        self.min_delta = min_delta\n        self.mode = mode\n        self.monitor = monitor\n        self.period = period\n        self.epochs_since_last_save = 0\n\n        if self.mode not in [\'min\', \'max\']:\n            if \'acc\' in self.monitor:\n                self.mode = \'max\'\n            else:\n                self.mode = \'min\'\n\n        if self.mode == \'min\':\n            self.min_delta *= -1\n            self.monitor_op = lambda x1, x2: (x1-self.min_delta) < x2\n        elif self.mode == \'max\':\n            self.min_delta *= 1\n            self.monitor_op = lambda x1, x2: (x1-self.min_delta) > x2\n\n        self.best = None\n\n    def state_dict(self):\n        state_dict = super(Best, self).state_dict()\n        state_dict[\'epochs\'] = self.epochs_since_last_save\n        state_dict[\'best\'] = self.best\n\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        super(Best, self).load_state_dict(state_dict)\n        self.epochs_since_last_save = state_dict[\'epochs\']\n        self.best = state_dict[\'best\']\n\n        return self\n\n    def on_start(self, state):\n        if self.best is None:\n            self.best = float(\'inf\') if self.mode == \'min\' else -float(\'inf\')\n\n    def on_checkpoint(self, state):\n        super(Best, self).on_end_epoch(state)\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n\n            current = get_metric(\'Best Checkpoint\', state, self.monitor)\n            if current is None:\n                return\n\n            if self.monitor_op(current, self.best):\n                self.best = current\n                self.save_checkpoint(state, overwrite_most_recent=True)\n\n\nclass Interval(_Checkpointer):\n    """"""Model checkpointer which which saves the model every \'period\' epochs to the given filepath. `filepath` can\n    contain named formatting options, which will be filled any values from state. For example: if `filepath` is\n    `weights.{epoch:02d}-{val_loss:.2f}`, then the model checkpoints will be saved with the epoch number and the\n    validation loss in the filename.\n\n    Example: ::\n\n        >>> from torchbearer.callbacks import Interval\n        >>> from torchbearer import Trial\n        >>> import torch\n\n        # Example Trial (without optimiser or loss criterion) which uses this checkpointer\n        >>> model = torch.nn.Linear(1,1)\n        >>> checkpoint = Interval(\'my_path.pt\', period=100, on_batch=True)\n        >>> trial = Trial(model, callbacks=[checkpoint], metrics=[\'acc\'])\n\n    Args:\n        filepath (str): Path to save the model file\n        save_model_params_only (bool): If `save_model_params_only=True`, only model parameters will be saved so that\n            the results can be loaded into a PyTorch nn.Module. The other option, `save_model_params_only=False`,\n            should be used only if the results will be loaded into a Torchbearer Trial object later.\n        period (int): Interval (number of steps) between checkpoints\n        on_batch (bool): If true step each batch, if false step each epoch.\n        period (int): Interval (number of epochs) between checkpoints\n        pickle_module (module): The pickle module to use, default is \'torch.serialization.pickle\'\n        pickle_protocol (int): The pickle protocol to use, default is \'torch.serialization.DEFAULT_PROTOCOL\'\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `state_dict` method\n        - :attr:`torchbearer.state.METRICS`: Metrics dictionary should exist\n        - :attr:`torchbearer.state.SELF`: Self should be the :attr:`torchbearer.Trial` which is running this callback\n    """"""\n\n    def __init__(self, filepath=\'model.{epoch:02d}-{val_loss:.2f}.pt\', save_model_params_only=False, period=1, on_batch=False, pickle_module=torch.serialization.pickle, pickle_protocol=torch.serialization.DEFAULT_PROTOCOL):\n\n        super(Interval, self).__init__(filepath, save_model_params_only=save_model_params_only,\n                                       pickle_module=pickle_module, pickle_protocol=pickle_protocol)\n        self.period = period\n        self.epochs_since_last_save = 0\n\n        if on_batch:\n            self.on_step_training = self.on_checkpoint\n            self.on_checkpoint = lambda _: None\n\n    def state_dict(self):\n        state_dict = super(Interval, self).state_dict()\n        state_dict[\'epochs\'] = self.epochs_since_last_save\n\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        super(Interval, self).load_state_dict(state_dict)\n        self.epochs_since_last_save = state_dict[\'epochs\']\n\n        return self\n\n    def on_checkpoint(self, state):\n        super(Interval, self).on_end_epoch(state)\n\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n            self.save_checkpoint(state)\n'"
torchbearer/callbacks/csv_logger.py,0,"b'import sys\n\nimport torchbearer\n\nfrom torchbearer.callbacks import Callback\nimport csv\n\n\nclass CSVLogger(Callback):\n    """"""Callback to log metrics to a given csv file.\n\n    Example: ::\n\n        >>> from torchbearer.callbacks import CSVLogger\n        >>> from torchbearer import Trial\n        >>> import torch\n\n        # Example Trial (without optimiser or loss criterion) which writes metrics to a csv file appending to previous content\n        >>> logger = CSVLogger(\'my_path.pt\', separator=\',\', append=True)\n        >>> trial = Trial(None, callbacks=[logger], metrics=[\'acc\'])\n\n    Args:\n        filename (str): The name of the file to output to\n        separator (str): The delimiter to use (e.g. comma, tab etc.)\n        batch_granularity (bool): If True, write on each batch, else on each epoch\n        write_header (bool): If True, write the CSV header at the beginning of training\n        append (bool): If True, append to the file instead of replacing it\n\n    State Requirements:\n        - :attr:`torchbearer.state.EPOCH`: State should have the current epoch stored\n        - :attr:`torchbearer.state.METRICS`: Metrics dictionary should exist\n        - :attr:`torchbearer.state.BATCH`: State should have the current batch stored if using `batch_granularity`\n    """"""\n\n    def __init__(self, filename, separator=\',\', batch_granularity=False, write_header=True, append=False):\n\n        super(CSVLogger, self).__init__()\n        self.batch_granularity = batch_granularity\n        self.filename = filename\n        self.separator = separator\n        if append:\n            filemode = \'a\'\n        else:\n            filemode = \'w\'\n        self.filemode = filemode\n\n        self.write_header = write_header\n\n    def on_start(self, state):\n        if sys.version_info[0] < 3:\n            self.filemode += \'b\'\n            self.csvfile = open(self.filename, self.filemode)\n        else:\n            self.csvfile = open(self.filename, self.filemode, newline=\'\')\n\n    def on_step_training(self, state):\n        super(CSVLogger, self).on_step_training(state)\n        if self.batch_granularity:\n            self._write_to_dict(state)\n\n    def on_end_epoch(self, state):\n        super(CSVLogger, self).on_end_training(state)\n        self._write_to_dict(state)\n\n    def on_end(self, state):\n        super(CSVLogger, self).on_end(state)\n        self.csvfile.close()\n\n    def _write_to_dict(self, state):\n        fields = self._get_field_dict(state)\n        self.writer = csv.DictWriter(self.csvfile, fieldnames=fields.keys(), delimiter=self.separator)\n\n        if self.write_header:\n            self.writer.writeheader()\n            self.write_header = False\n\n        self.writer.writerow(fields)\n        self.csvfile.flush()\n\n    def _get_field_dict(self, state):\n        fields = {\'epoch\': state[torchbearer.EPOCH]}\n\n        if self.batch_granularity:\n            fields.update({\'batch\': state[torchbearer.BATCH]})\n\n        fields.update(state[torchbearer.METRICS])\n\n        return fields\n'"
torchbearer/callbacks/cutout.py,9,"b'import torchbearer\nfrom torchbearer import Callback\nimport torch\nfrom torch.distributions import Beta\n\nfrom torchbearer.bases import cite\n\ncutout = """"""\n@article{devries2017improved,\n  title={Improved regularization of convolutional neural networks with Cutout},\n  author={DeVries, Terrance and Taylor, Graham W},\n  journal={arXiv preprint arXiv:1708.04552},\n  year={2017}\n}\n""""""\n\nrandom_erase = """"""\n@article{zhong2017random,\n  title={Random erasing data augmentation},\n  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},\n  journal={arXiv preprint arXiv:1708.04896},\n  year={2017}\n}\n""""""\n\n\ncutmix = """"""\n@article{yun2019cutmix,\n  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},\n  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},\n  journal={arXiv preprint arXiv:1905.04899},\n  year={2019}\n}\n""""""\n\n\n@cite(cutout)\nclass Cutout(Callback):\n    """""" Cutout callback which randomly masks out patches of image data. Implementation a modified version of the code\n    found `here <https://github.com/uoguelph-mlrg/Cutout>`_.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import Cutout\n\n        # Example Trial which does Cutout regularisation\n        >>> cutout = Cutout(1, 10)\n        >>> trial = Trial(None, callbacks=[cutout], metrics=[\'acc\'])\n\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n        constant (float): Constant value for each square patch\n\n    State Requirements:\n        - :attr:`torchbearer.state.X`: State should have the current data stored\n    """"""\n    def __init__(self, n_holes, length, constant=0.):\n        super(Cutout, self).__init__()\n        self.constant = constant\n        self.cutter = BatchCutout(n_holes, length, length)\n\n    def on_sample(self, state):\n        super(Cutout, self).on_sample(state)\n        mask = self.cutter(state[torchbearer.X])\n        erase_locations = mask == 0\n        constant = torch.ones_like(state[torchbearer.X]) * self.constant\n        state[torchbearer.X][erase_locations] = constant[erase_locations]\n\n\n@cite(random_erase)\nclass RandomErase(Callback):\n    """""" Random erase callback which replaces random patches of image data with random noise.\n    Implementation a modified version of the cutout code found\n    `here <https://github.com/uoguelph-mlrg/Cutout>`_.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import RandomErase\n\n        # Example Trial which does Cutout regularisation\n        >>> erase = RandomErase(1, 10)\n        >>> trial = Trial(None, callbacks=[erase], metrics=[\'acc\'])\n\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n\n    State Requirements:\n        - :attr:`torchbearer.state.X`: State should have the current data stored\n    """"""\n    def __init__(self, n_holes, length):\n        super(RandomErase, self).__init__()\n        self.cutter = BatchCutout(n_holes, length, length)\n\n    def on_sample(self, state):\n        super(RandomErase, self).on_sample(state)\n        mask = self.cutter(state[torchbearer.X])\n        erase_locations = mask == 0\n        random = torch.rand_like(state[torchbearer.X])\n        state[torchbearer.X][erase_locations] = random[erase_locations]\n\n\n@cite(cutmix)\nclass CutMix(Callback):\n    """""" Cutmix callback which replaces a random patch of image data with the corresponding patch from another image.\n    This callback also converts labels to one hot before combining them according to the lambda parameters, sampled from\n    a beta distribution as is done in the paper.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import CutMix\n\n        # Example Trial which does CutMix regularisation\n        >>> cutmix = CutMix(1, classes=10)\n        >>> trial = Trial(None, callbacks=[cutmix], metrics=[\'acc\'])\n\n    Args:\n        alpha (float): The alpha value for the beta distribution.\n        classes (int): The number of classes for conversion to one hot.\n\n    State Requirements:\n        - :attr:`torchbearer.state.X`: State should have the current data stored\n        - :attr:`torchbearer.state.Y_TRUE`: State should have the current data stored\n    """"""\n    def __init__(self, alpha, classes=-1, mixup_loss=False):\n        super(CutMix, self).__init__()\n        self.classes = classes\n        self.dist = Beta(torch.tensor([float(alpha)]), torch.tensor([float(alpha)]))\n        self.mixup_loss = mixup_loss\n\n    def _to_one_hot(self, target):\n        if target.dim() == 1:\n            target = target.unsqueeze(1)\n            one_hot = torch.zeros_like(target).repeat(1, self.classes)\n            one_hot.scatter_(1, target, 1)\n            return one_hot\n        return target\n\n    def on_sample(self, state):\n        super(CutMix, self).on_sample(state)\n\n        lam = self.dist.sample().to(state[torchbearer.DEVICE])\n        length = (1 - lam).sqrt()\n        cutter = BatchCutout(1, (length * state[torchbearer.X].size(-1)).round().item(), (length * state[torchbearer.X].size(-2)).round().item())\n        mask = cutter(state[torchbearer.X])\n        erase_locations = mask == 0\n\n        permutation = torch.randperm(state[torchbearer.X].size(0))\n        if self.mixup_loss:\n            state[torchbearer.MIXUP_PERMUTATION] = permutation\n            state[torchbearer.MIXUP_LAMBDA] = lam\n\n        state[torchbearer.X][erase_locations] = state[torchbearer.X][permutation][erase_locations]\n\n        if self.mixup_loss:\n            state[torchbearer.TARGET] = (state[torchbearer.TARGET], state[torchbearer.TARGET][state[torchbearer.MIXUP_PERMUTATION]])\n        else:\n            target = self._to_one_hot(state[torchbearer.TARGET]).float()\n            state[torchbearer.TARGET] = lam * target + (1 - lam) * target[permutation]\n\n\n    def on_sample_validation(self, state):\n        super(CutMix, self).on_sample_validation(state)\n        if not self.mixup_loss:\n            state[torchbearer.TARGET] = self._to_one_hot(state[torchbearer.TARGET]).float()\n\n\nclass BatchCutout(object):\n    """"""Randomly mask out one or more patches from a batch of images.\n\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        width (int): The width (in pixels) of each square patch.\n        height (int): The height (in pixels) of each square patch.\n    """"""\n    def __init__(self, n_holes, width, height):\n        self.n_holes = n_holes\n        self.width = width\n        self.height = height\n\n    def __call__(self, img):\n        """"""\n\n        Args:\n            img (Tensor): Tensor image of size (B, C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        """"""\n        b = img.size(0)\n        c = img.size(1)\n        h = img.size(-2)\n        w = img.size(-1)\n\n        mask = torch.ones((b, h, w), device=img.device)\n\n        for n in range(self.n_holes):\n            y = torch.randint(h, (b,)).long()\n            x = torch.randint(w, (b,)).long()\n\n            y1 = (y - self.height // 2).clamp(0, h).int()\n            y2 = (y + self.height // 2).clamp(0, h).int()\n            x1 = (x - self.width // 2).clamp(0, w).int()\n            x2 = (x + self.width // 2).clamp(0, w).int()\n\n            for batch in range(b):\n                mask[batch, y1[batch]: y2[batch], x1[batch]: x2[batch]] = 0\n\n        mask = mask.unsqueeze(1).repeat(1, c, 1, 1)\n\n        return mask\n'"
torchbearer/callbacks/decorators.py,1,"b'import sys\nif sys.version_info[0] < 3:\n    import inspect\n    def count_args(fcn):\n        return len(inspect.getargspec(fcn).args)\nelse:\n    from inspect import signature\n    def count_args(fcn):\n        return len(signature(fcn).parameters)\n\nimport types\n\nimport torchbearer\nfrom torchbearer.callbacks import Callback\n\n\nclass LambdaCallback(Callback):\n    def __init__(self, func):\n        self.func = func\n\n    def on_lambda(self, state):\n        return self.func(state)\n\n\ndef bind_to(target):\n    def decorator(func):\n        if isinstance(func, LambdaCallback):\n            callback = func\n        else:\n            callback = LambdaCallback(func)\n        setattr(callback, target.__name__, types.MethodType(lambda self, state: self.on_lambda(state), callback))\n        return callback\n    return decorator\n\n\ndef on_init(func):\n    """""" The :func:`on_init` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_init`\n    calling the decorated function\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_init\n\n        # Example callback on start\n        >>> @on_init\n        ... def print_callback(state):\n        ...     print(\'Initialised trial.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Initialised trial.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_init` calling func\n    """"""\n    return bind_to(Callback.on_init)(func)\n\n\ndef on_start(func):\n    """""" The :func:`on_start` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_start`\n    calling the decorated function\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_start\n\n        # Example callback on start\n        >>> @on_start\n        ... def print_callback(state):\n        ...     print(\'Starting training.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Starting training.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_start` calling func\n    """"""\n    return bind_to(Callback.on_start)(func)\n\n\ndef on_start_epoch(func):\n    """""" The :func:`on_start_epoch` decorator is used to initialise a :class:`.Callback` with\n    :meth:`~.Callback.on_start_epoch` calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_start_epoch\n\n        # Example callback running at start of each epoch\n        >>> @on_start_epoch\n        ... def print_callback(state):\n        ...     print(\'Starting epoch {}.\'.format(state[torchbearer.EPOCH]))\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Starting epoch 0.\n\n        Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_start_epoch` calling func\n    """"""\n    return bind_to(Callback.on_start_epoch)(func)\n\n\ndef on_start_training(func):\n    """""" The :func:`on_start_training` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_start_training`\n    calling the decorated function\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_start_training\n\n        # Example callback running at start of the training pass\n        >>> @on_start_training\n        ... def print_callback(state):\n        ...     print(\'Starting training.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Starting training.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_start_training` calling func\n    """"""\n    return bind_to(Callback.on_start_training)(func)\n\n\ndef on_sample(func):\n    """""" The :func:`on_sample` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_sample`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_sample\n\n        # Example callback running each time a sample is taken from the dataset\n        >>> @on_sample\n        ... def print_callback(state):\n        ...     print(\'Current sample {}.\'.format(state[torchbearer.X]))\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Current sample None.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_sample` calling func\n    """"""\n    return bind_to(Callback.on_sample)(func)\n\n\ndef on_forward(func):\n    """""" The :func:`on_forward` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_forward`\n    calling the decorated function\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_forward\n\n        # Example callback running after each training forward pass of the torch model\n        >>> @on_forward\n        ... def print_callback(state):\n        ...     print(\'Evaluated training batch.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Evaluated training batch.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_forward` calling func\n    """"""\n    return bind_to(Callback.on_forward)(func)\n\n\ndef on_criterion(func):\n    """""" The :func:`on_criterion` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_criterion`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_criterion\n\n        # Example callback running after each evaluation of the loss\n        >>> @on_criterion\n        ... def print_callback(state):\n        ...     print(\'Current loss {}.\'.format(state[torchbearer.LOSS].item()))\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Current loss 0.0.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_criterion` calling func\n    """"""\n    return bind_to(Callback.on_criterion)(func)\n\n\ndef on_backward(func):\n    """""" The :func:`on_backward` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_backward`\n    calling the decorated function\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_backward\n\n        # Example callback running after each backward pass of the torch model\n        >>> @on_backward\n        ... def print_callback(state):\n        ...     print(\'Doing backward.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Doing backward.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_backward` calling func\n    """"""\n    return bind_to(Callback.on_backward)(func)\n\n\ndef on_step_training(func):\n    """""" The :func:`on_step_training` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_step_training`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_step_training\n\n        # Example callback running after each training step\n        >>> @on_step_training\n        ... def print_callback(state):\n        ...     print(\'Step {}.\'.format(state[torchbearer.BATCH]))\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Step 0.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_step_training` calling func\n    """"""\n    return bind_to(Callback.on_step_training)(func)\n\n\ndef on_end_training(func):\n    """""" The :func:`on_end_training` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_end_training`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_end_training\n\n        # Example callback running after each training pass\n        >>> @on_end_training\n        ... def print_callback(state):\n        ...     print(\'Finished training pass.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Finished training pass.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_end_training` calling func\n    """"""\n    return bind_to(Callback.on_end_training)(func)\n\n\ndef on_end_epoch(func):\n    """""" The :func:`on_end_epoch` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_end_epoch`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_end_epoch\n\n        # Example callback running each epoch\n        >>> @on_end_epoch\n        ... def print_callback(state):\n        ...     print(\'Finished epoch {}.\'.format(state[torchbearer.EPOCH]))\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Finished epoch 0.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_end_epoch` calling func\n    """"""\n    return bind_to(Callback.on_end_epoch)(func)\n\n\ndef on_end(func):\n    """""" The :func:`on_end` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_end`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_end\n\n        # Example callback running after all training is finished.\n        >>> @on_end\n        ... def print_callback(state):\n        ...     print(\'Finished training model.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Finished training model.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_end` calling func\n    """"""\n    return bind_to(Callback.on_end)(func)\n\n\ndef on_start_validation(func):\n    """""" The :func:`on_start_validation` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_start_validation`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_start_validation\n\n        # Example callback running when each validation pass starts.\n        >>> @on_start_validation\n        ... def print_callback(state):\n        ...     print(\'Starting validation.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).for_val_steps(1).run()\n        Starting validation.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_start_validation` calling func\n    """"""\n    return bind_to(Callback.on_start_validation)(func)\n\n\ndef on_sample_validation(func):\n    """""" The :func:`on_sample_validation` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_sample_validation`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_sample_validation\n\n        # Example callback running after each validation sample is drawn.\n        >>> @on_sample_validation\n        ... def print_callback(state):\n        ...     print(\'Sampled validation data {}.\'.format(state[torchbearer.X]))\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).for_val_steps(1).run()\n        Sampled validation data None.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_sample_validation` calling func\n    """"""\n    return bind_to(Callback.on_sample_validation)(func)\n\n\ndef on_forward_validation(func):\n    """""" The :func:`on_forward_validation` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_forward_validation`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_forward_validation\n\n        # Example callback running after each torch model forward pass in validation.\n        >>> @on_forward_validation\n        ... def print_callback(state):\n        ...     print(\'Evaluated validation batch.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).for_val_steps(1).run()\n        Evaluated validation batch.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_forward_validation` calling func\n    """"""\n    return bind_to(Callback.on_forward_validation)(func)\n\n\ndef on_criterion_validation(func):\n    """""" The :func:`on_criterion_validation` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_criterion_validation`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_criterion_validation\n\n        # Example callback running after each criterion evaluation in validation.\n        >>> @on_criterion_validation\n        ... def print_callback(state):\n        ...     print(\'Current val loss {}.\'.format(state[torchbearer.LOSS].item()))\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).for_val_steps(1).run()\n        Current val loss 0.0.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_criterion_validation` calling func\n    """"""\n    return bind_to(Callback.on_criterion_validation)(func)\n\n\ndef on_end_validation(func):\n    """""" The :func:`on_end_validation` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_end_validation`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_end_validation\n\n        # Example callback running at the end of each validation pass.\n        >>> @on_end_validation\n        ... def print_callback(state):\n        ...     print(\'Finished validating.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).for_val_steps(1).run()\n        Finished validating.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_end_validation` calling func\n    """"""\n    return bind_to(Callback.on_end_validation)(func)\n\n\ndef on_step_validation(func):\n    """""" The :func:`on_step_validation` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_step_validation`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_step_validation\n\n        # Example callback running at the end of each validation step.\n        >>> @on_step_validation\n        ... def print_callback(state):\n        ...     print(\'Validation step {}.\'.format(state[torchbearer.BATCH]))\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).for_val_steps(1).run()\n        Validation step 0.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_step_validation` calling func\n    """"""\n    return bind_to(Callback.on_step_validation)(func)\n\n\ndef on_checkpoint(func):\n    """""" The :func:`on_checkpoint` decorator is used to initialise a :class:`.Callback` with :meth:`~.Callback.on_checkpoint`\n    calling the decorated function\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import on_checkpoint\n\n        # Example callback running at checkpoint time.\n        >>> @on_checkpoint\n        ... def print_callback(state):\n        ...     print(\'Checkpointing.\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        Checkpointing.\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback with :meth:`~.Callback.on_checkpoint` calling func\n    """"""\n    return bind_to(Callback.on_checkpoint)(func)\n\n\ndef add_to_loss(func):\n    """""" The :func:`add_to_loss` decorator is used to initialise a :class:`.Callback` with the value returned from func\n    being added to the loss\n\n    Example: ::\n\n        >>> import torch\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import add_to_loss\n\n        # Example callback to add a quantity to the loss each step.\n        >>> @add_to_loss\n        ... def loss_callback(state):\n        ...     return torch.Tensor([1.125])\n\n        >>> trial = Trial(None, callbacks=[loss_callback], metrics=[\'loss\']).for_steps(1).run()\n        >>> print(trial[0][1][\'loss\'])\n        1.125\n\n\n    Args:\n        func (function): The function(state) to *decorate*\n\n    Returns:\n        Callback: Initialised callback which adds the returned value from func to the loss\n    """"""\n    @on_criterion\n    @on_criterion_validation\n    def add_to_loss_callback(state):\n        state[torchbearer.LOSS] = state[torchbearer.LOSS] + func(state)\n\n    return add_to_loss_callback\n\n\ndef once(fcn):\n    """"""\n    Decorator to fire a callback once in the lifetime of the callback. If the callback is a class method, each\n    instance of the class will fire only once. For functions, only the first instance will fire (even if more than\n    one function is present in the callback list).\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import once, on_step_training\n\n        # Example callback to be called exactly once on the very first training step\n        >>> @once\n        ... @on_step_training\n        ... def print_callback(state):\n        ...     print(\'This happens once ever\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        This happens once ever\n\n    Args:\n        fcn (function): the `torchbearer callback` function to decorate.\n\n    Returns:\n        the decorator\n    """"""\n    def _once(self, _):\n        try:\n            return not self.__done__\n        except AttributeError:\n            self.__done__ = True\n            return True\n\n    return only_if(_once)(fcn)\n\n\ndef once_per_epoch(fcn):\n    """"""Decorator to fire a callback once (on the first call) in any given epoch. If the callback is a class method, each\n    instance of the class will fire once per epoch. For functions, only the first instance will fire (even if more than\n    one function is present in the callback list).\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import once_per_epoch, on_step_training\n\n        # Example callback to be called exactly once per epoch, on the first training step\n        >>> @once_per_epoch\n        ... @on_step_training\n        ... def print_callback(state):\n        ...     print(\'This happens once per epoch\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(1).run()\n        This happens once per epoch\n\n    .. note::\n        The decorated callback may exhibit unusual behaviour if it is reused\n\n    Args:\n        fcn (function): the `torchbearer callback` function to decorate.\n\n    Returns:\n        the decorator\n    """"""\n    def ope(self, state):\n        try:\n            if state[torchbearer.EPOCH] != self.__last_epoch__:\n                self.__last_epoch__ = state[torchbearer.EPOCH]\n                return True\n            return False\n        except AttributeError:\n            self.__last_epoch__ = state[torchbearer.EPOCH]\n            return True\n\n    return only_if(ope)(fcn)\n\n\ndef only_if(condition_expr):\n    """"""\n    Decorator to fire a callback only if the given conditional expression function returns True. The conditional\n    expression can be a function of state or self and state. If the decorated function is not a class method (i.e. it\n    does not take state) the decorated function will be passed instead. This enables the storing of temporary variables.\n\n    Example: ::\n\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import only_if, on_step_training\n\n        # Example callback to be called only when the given condition is true on each training step\n        >>> @only_if(lambda state: state[torchbearer.BATCH] == 100)\n        ... @on_step_training\n        ... def print_callback(state):\n        ...     print(\'This is the 100th batch\')\n\n        >>> trial = Trial(None, callbacks=[print_callback]).for_steps(101).run()\n        This is the 100th batch\n\n    Args:\n        condition_expr (function(self, state) or function(self)): a function/lambda which takes state and optionally\\\n                        self that must evaluate to true for the decorated `torchbearer callback` to be called. The\\\n                        `state` object passed to the callback will be passed as an argument to the condition function.\n\n    Returns:\n        the decorator\n    """"""\n    def condition_decorator(fcn):\n        if isinstance(fcn, LambdaCallback):\n            fcn.func = condition_decorator(fcn.func)\n            return fcn\n        else:\n            count = count_args(fcn)\n            if count == 2 and not hasattr(fcn, \'__self__\'):  # Assume Class method\n                def decfcn(o, state):\n                    try:\n                        res = condition_expr(o, state)\n                    except TypeError:\n                        res = condition_expr(state)\n                    if res:\n                        return fcn(o, state)\n            else:  # Assume function of state\n                def id_fcn(state):\n                    return fcn(state)  # Hack to allow setting attributes of bound methods\n\n                def decfcn(state):\n                    try:\n                        res = condition_expr(id_fcn, state)\n                    except TypeError:\n                        res = condition_expr(state)\n                    if res:\n                        return id_fcn(state)\n            return decfcn\n    return condition_decorator\n'"
torchbearer/callbacks/early_stopping.py,0,"b'from __future__ import print_function\nimport torchbearer\n\nfrom torchbearer.callbacks import Callback\nfrom .decorators import only_if\nfrom torchbearer.bases import get_metric\n\n\nclass EarlyStopping(Callback):\n    """"""Callback to stop training when a monitored quantity has stopped improving.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import EarlyStopping\n\n        # Example Trial which does early stopping if the validation accuracy drops below the max seen for 5 epochs in a row\n        >>> stopping = EarlyStopping(monitor=\'val_acc\', patience=5, mode=\'max\')\n        >>> trial = Trial(None, callbacks=[stopping], metrics=[\'acc\'])\n\n    Args:\n        monitor (str): Name of quantity in metrics to be monitored\n        min_delta (float): Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute\n            change of less than min_delta, will count as no improvement.\n        patience (int): Number of epochs with no improvement after which training will be stopped.\n        mode (str): One of {auto, min, max}. In `min` mode, training will stop when the quantity monitored has stopped\n            decreasing; in `max` mode it will stop when the quantity monitored has stopped increasing; in `auto` mode,\n            the direction is automatically inferred from the name of the monitored quantity.\n\n    State Requirements:\n        - :attr:`torchbearer.state.METRICS`: Metrics should be a dict containing the given monitor key as a minimum\n    """"""\n\n    def __init__(self, monitor=\'val_loss\',\n                 min_delta=0, patience=0, mode=\'auto\', step_on_batch=False):\n\n        super(EarlyStopping, self).__init__()\n\n        self.monitor = monitor\n        self.min_delta = min_delta\n        self.patience = patience\n        self.mode = mode\n        self.step_on_batch = step_on_batch\n\n        if self.mode not in [\'min\', \'max\']:\n            if \'acc\' in self.monitor:\n                self.mode = \'max\'\n            else:\n                self.mode = \'min\'\n\n        if self.mode == \'min\':\n            self.min_delta *= -1\n            self.monitor_op = lambda x1, x2: x1 < x2\n        elif self.mode == \'max\':\n            self.min_delta *= 1\n            self.monitor_op = lambda x1, x2: x1 > x2\n\n        self.wait = 0\n        self.best = float(\'inf\') if self.mode == \'min\' else -float(\'inf\')\n\n    def state_dict(self):\n        state_dict = {\n            \'wait\': self.wait,\n            \'best\': self.best\n        }\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        self.wait = state_dict[\'wait\']\n        self.best = state_dict[\'best\']\n\n    def step(self, state):\n        current = get_metric(\'Early Stopping\', state, self.monitor)\n        if current is None:\n            return\n\n        if self.monitor_op(current - self.min_delta, self.best):\n            self.best = current\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                state[torchbearer.STOP_TRAINING] = True\n\n    @only_if(lambda self, _: self.step_on_batch)\n    def on_step_training(self, state):\n        self.step(state)\n\n    @only_if(lambda self, _: not self.step_on_batch)\n    def on_end_epoch(self, state):\n        self.step(state)\n'"
torchbearer/callbacks/gradient_clipping.py,8,"b'import torchbearer\n\nfrom torchbearer.callbacks import Callback\n\nimport torch\n\n\nclass GradientNormClipping(Callback):\n    """"""GradientNormClipping callback, which uses \'torch.nn.utils.clip_grad_norm\\_\' to clip the gradient norms to the\n    given value. If params is None they will be retrieved from state.\n\n    Example: ::\n\n        >>> import torch.nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import GradientNormClipping\n\n        # Example Trial which clips all model gradients norms at 2 under the L1 norm.\n        >>> model = torch.nn.Linear(1,1)\n        >>> clip = GradientNormClipping(2, 1)\n        >>> trial = Trial(model, callbacks=[clip], metrics=[\'acc\'])\n\n    Args:\n        max_norm (float or int): max norm of the gradients\n        norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\n            infinity norm.\n        params (Iterable[Tensor] or Tensor, optional): an iterable of Tensors or a\n            single Tensor that will have gradients normalized, otherwise this is retrieved from state\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `parameters` method\n    """"""\n    def __init__(self, max_norm, norm_type=2, params=None):\n        super(GradientNormClipping, self).__init__()\n\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.params = params\n\n    def on_start(self, state):\n        """"""If params is None then retrieve from the model.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if self.params is None:\n            self.params = filter(lambda p: p.requires_grad, state[torchbearer.MODEL].parameters())\n\n    def on_backward(self, state):\n        """"""Between the backward pass (which computes the gradients) and the step call (which updates the parameters),\n        clip the gradient.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        torch.nn.utils.clip_grad_norm_(self.params, self.max_norm, norm_type=self.norm_type)\n\n\nclass GradientClipping(Callback):\n    """"""GradientClipping callback, which uses \'torch.nn.utils.clip_grad_value\\_\' to clip the gradients of the given\n    parameters to the given value. If params is None they will be retrieved from state.\n\n    Example: ::\n\n        >>> import torch.nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import GradientClipping\n\n        # Example Trial which clips all model gradients at 2 under the L1 norm.\n        >>> model = torch.nn.Linear(1,1)\n        >>> clip = GradientNormClipping(2, 1)\n        >>> trial = Trial(model, callbacks=[clip], metrics=[\'acc\'])\n\n    Args:\n        clip_value (float or int): maximum allowed value of the gradients\n            The gradients are clipped in the range [-clip_value, clip_value]\n        params (Iterable[Tensor] or Tensor, optional): an iterable of Tensors or a\n            single Tensor that will have gradients normalized, otherwise this is retrieved from state\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `parameters` method\n    """"""\n    def __init__(self, clip_value, params=None):\n\n        super(GradientClipping, self).__init__()\n\n        self.clip_value = clip_value\n        self.params = params\n\n    def on_start(self, state):\n        """"""If params is None then retrieve from the model.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if self.params is None:\n            self.params = filter(lambda p: p.requires_grad, state[torchbearer.MODEL].parameters())\n\n    def on_backward(self, state):\n        """"""Between the backward pass (which computes the gradients) and the step call (which updates the parameters),\n        clip the gradient.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        torch.nn.utils.clip_grad_value_(self.params, self.clip_value)\n'"
torchbearer/callbacks/init.py,22,"b'import torchbearer\nfrom torchbearer import cite\nfrom torchbearer.callbacks import Callback\n\nimport torch.nn.init as init\n\n__kaiming__ = """"""\n@inproceedings{he2015delving,\n  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n  booktitle={Proceedings of the IEEE international conference on computer vision},\n  pages={1026--1034},\n  year={2015}\n}""""""\n\n__xavier__ = """"""\n@inproceedings{glorot2010understanding,\n  title={Understanding the difficulty of training deep feedforward neural networks},\n  author={Glorot, Xavier and Bengio, Yoshua},\n  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},\n  pages={249--256},\n  year={2010}\n}\n""""""\n\n__lsuv__ = """"""\n@article{mishkin2015all,\n  title={All you need is a good init},\n  author={Mishkin, Dmytro and Matas, Jiri},\n  journal={arXiv preprint arXiv:1511.06422},\n  year={2015}\n}\n""""""\n\n\nclass WeightInit(Callback):\n    """"""Base class for weight initialisations. Performs the provided function for each module when on_init is\n    called.\n\n    Args:\n        initialiser (lambda): a function which initialises an nn.Module **inplace**\n        modules (Iterable[nn.Module] or nn.Module, optional): an iterable of nn.Modules or a\n            single nn.Module that will have weights initialised, otherwise this is retrieved from the model\n        targets (list[String]): A list of lookup strings to match which modules will be initialised\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `modules` method if modules is None\n    """"""\n    def __init__(self, initialiser=lambda module: module, modules=None, targets=[\'Conv\', \'Linear\', \'Bilinear\']):\n        self.initialiser = initialiser\n        self.modules = modules\n        self.targets = targets\n\n    def on_init(self, state):\n        if self.modules is None:\n            self.modules = state[torchbearer.MODEL].modules()\n\n        for m in self.modules:\n            if len(list(filter(lambda target: target in m.__class__.__name__, self.targets))) > 0:\n                self.initialiser(m)\n\n\n@cite(__lsuv__)\nclass LsuvInit(Callback):\n    """"""Layer-sequential unit-variance (LSUV) initialization as described in\n    `All you need is a good init <https://arxiv.org/abs/1511.06422>`_ and\n    modified from the code by  `ducha-aiki <https://github.com/ducha-aiki/LSUV-pytorch>`__.\n    To be consistent with the paper, LsuvInit should be preceeded by a ZeroBias init on the Linear and Conv layers.\n\n\n    Example: ::\n\n        >>> import torch\n        >>> import torch.nn as nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks.init import LsuvInit\n\n        # 100 random data points\n        >>> data = torch.rand(100, 3, 5, 5)\n        >>> example_batch = data[:3]\n        >>> lsuv = LsuvInit(example_batch)\n\n        # Model and trail using lsuv init for some random data\n        >>> model = nn.Sequential(nn.Conv2d(3, 1, 3), nn.ReLU())\n        >>> trial = Trial(model, callbacks=[lsuv]).with_train_data(data, data+5)\n\n    Args:\n        data_item (torch.Tensor): A representative data item to put through the model\n        weight_lambda (lambda): A function that takes a module and returns the weight attribute. If none defaults to \n            module.weight.\n        needed_std: See `paper <https://arxiv.org/abs/1511.06422>`__, where needed_std is always 1.0\n        std_tol: See `paper <https://arxiv.org/abs/1511.06422>`__, Tol_{var}\n        max_attempts: See `paper <https://arxiv.org/abs/1511.06422>`__, T_{max}\n        do_orthonorm: See `paper <https://arxiv.org/abs/1511.06422>`__, first pre-initialise with orthonormal matricies\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `modules` method if modules is None\n    """"""\n    def __init__(self, data_item, weight_lambda=None, needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=True):\n        from torchbearer.callbacks.lsuv import LSUV\n        self.lsuv_init = LSUV\n        self.data = data_item\n        self.needed_std = needed_std\n        self.std_tol = std_tol\n        self.max_attempts = max_attempts\n        self.do_arthonorm = do_orthonorm\n        self.weight_lambda = weight_lambda\n\n    def on_init(self, state):\n        lsuv = self.lsuv_init()\n        state[torchbearer.MODEL] = lsuv.init_model(state[torchbearer.MODEL], self.data, self.weight_lambda, self.needed_std,\n                                                  self.std_tol, self.max_attempts, self.do_arthonorm)\n\n\n@cite(__kaiming__)\nclass KaimingNormal(WeightInit):\n    """"""Kaiming Normal weight initialisation. Uses `torch.nn.init.kaiming_normal_` on the ``weight`` attribute of the\n    filtered modules.\n\n    Example: ::\n\n        >>> import torch\n        >>> import torch.nn as nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks.init import KaimingNormal\n\n        # 100 random data points\n        >>> data = torch.rand(100, 3, 5, 5)\n        >>> example_batch = data[:3]\n        >>> initialiser = KaimingNormal()\n\n        # Model and trail using kaiming init for some random data\n        >>> model = nn.Sequential(nn.Conv2d(3, 1, 3), nn.ReLU())\n        >>> trial = Trial(model, callbacks=[initialiser]).with_train_data(data, data+5)\n\n    Args:\n        a (int): See `PyTorch kaiming_normal_ <https://pytorch.org/docs/stable/nn.html#torch.nn.init.kaiming_normal_>`_\n        mode (str): See `PyTorch kaiming_normal_`_\n        nonlinearity (str): See `PyTorch kaiming_normal_`_\n        modules (Iterable[nn.Module] or nn.Module, optional): an iterable of nn.Modules or a\n            single nn.Module that will have weights initialised, otherwise this is retrieved from the model\n        targets (list[String]): A list of lookup strings to match which modules will be initialised\n\n    See:\n        `PyTorch kaiming_normal_`_\n    """"""\n    def __init__(self, a=0, mode=\'fan_in\', nonlinearity=\'leaky_relu\', modules=None,\n                 targets=[\'Conv\', \'Linear\', \'Bilinear\']):\n        def initialiser(module):\n            init.kaiming_normal_(module.weight.data, a=a, mode=mode, nonlinearity=nonlinearity)\n\n        super(KaimingNormal, self).__init__(initialiser, modules=modules, targets=targets)\n\n\n@cite(__kaiming__)\nclass KaimingUniform(WeightInit):\n    """"""Kaiming Uniform weight initialisation. Uses `torch.nn.init.kaiming_uniform_` on the ``weight`` attribute of the\n    filtered modules.\n\n    Example: ::\n\n        >>> import torch\n        >>> import torch.nn as nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks.init import KaimingUniform\n\n        # 100 random data points\n        >>> data = torch.rand(100, 3, 5, 5)\n        >>> example_batch = data[:3]\n        >>> initialiser = KaimingUniform()\n\n        # Model and trail using kaiming init for some random data\n        >>> model = nn.Sequential(nn.Conv2d(3, 1, 3), nn.ReLU())\n        >>> trial = Trial(model, callbacks=[initialiser]).with_train_data(data, data+5)\n\n    Args:\n        a (int): See `PyTorch kaiming_uniform_ <https://pytorch.org/docs/stable/nn.html#torch.nn.init.kaiming_uniform_>`_\n        mode (str): See `PyTorch kaiming_uniform_`_\n        nonlinearity (str): See `PyTorch kaiming_uniform_`_\n        modules (Iterable[nn.Module] or nn.Module, optional): an iterable of nn.Modules or a\n            single nn.Module that will have weights initialised, otherwise this is retrieved from the model\n        targets (list[String]): A list of lookup strings to match which modules will be initialised\n\n    See:\n        `PyTorch kaiming_uniform_`_\n    """"""\n    def __init__(self, a=0, mode=\'fan_in\', nonlinearity=\'leaky_relu\', modules=None,\n                 targets=[\'Conv\', \'Linear\', \'Bilinear\']):\n        def initialiser(module):\n            init.kaiming_uniform_(module.weight.data, a=a, mode=mode, nonlinearity=nonlinearity)\n\n        super(KaimingUniform, self).__init__(initialiser, modules=modules, targets=targets)\n\n\n@cite(__xavier__)\nclass XavierNormal(WeightInit):\n    """"""Xavier Normal weight initialisation. Uses ``torch.nn.init.xavier_normal_`` on the ``weight`` attribute of the\n    filtered modules.\n\n    Example: ::\n\n        >>> import torch\n        >>> import torch.nn as nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks.init import XavierNormal\n\n        # 100 random data points\n        >>> data = torch.rand(100, 3, 5, 5)\n        >>> example_batch = data[:3]\n        >>> initialiser = XavierNormal()\n\n        # Model and trail using Xavier init for some random data\n        >>> model = nn.Sequential(nn.Conv2d(3, 1, 3), nn.ReLU())\n        >>> trial = Trial(model, callbacks=[initialiser]).with_train_data(data, data+5)\n\n    Args:\n        gain (int): See `PyTorch xavier_normal_ <https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_normal_>`_\n        modules (Iterable[nn.Module] or nn.Module, optional): an iterable of nn.Modules or a\n            single nn.Module that will have weights initialised, otherwise this is retrieved from the model\n        targets (list[String]): A list of lookup strings to match which modules will be initialised\n\n    See:\n        `PyTorch xavier_normal_`_\n    """"""\n    def __init__(self, gain=1, modules=None, targets=[\'Conv\', \'Linear\', \'Bilinear\']):\n        def initialiser(module):\n            init.xavier_normal_(module.weight.data, gain=gain)\n\n        super(XavierNormal, self).__init__(initialiser, modules=modules, targets=targets)\n\n\n@cite(__xavier__)\nclass XavierUniform(WeightInit):\n    """"""Xavier Uniform weight initialisation. Uses ``torch.nn.init.xavier_uniform_`` on the ``weight`` attribute of the\n    filtered modules.\n\n    Example: ::\n\n        >>> import torch\n        >>> import torch.nn as nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks.init import XavierUniform\n\n        # 100 random data points\n        >>> data = torch.rand(100, 3, 5, 5)\n        >>> example_batch = data[:3]\n        >>> initialiser = XavierUniform()\n\n        # Model and trail using Xavier init for some random data\n        >>> model = nn.Sequential(nn.Conv2d(3, 1, 3), nn.ReLU())\n        >>> trial = Trial(model, callbacks=[initialiser]).with_train_data(data, data+5)\n\n    Args:\n        gain (int): See `PyTorch xavier_uniform_ <https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_>`_\n        modules (Iterable[nn.Module] or nn.Module, optional): an iterable of nn.Modules or a\n            single nn.Module that will have weights initialised, otherwise this is retrieved from the model\n        targets (list[String]): A list of lookup strings to match which modules will be initialised\n\n    See:\n        `PyTorch xavier_uniform_`_\n    """"""\n    def __init__(self, gain=1, modules=None, targets=[\'Conv\', \'Linear\', \'Bilinear\']):\n        def initialiser(module):\n            init.xavier_uniform_(module.weight.data, gain=gain)\n\n        super(XavierUniform, self).__init__(initialiser, modules=modules, targets=targets)\n\n\nclass ZeroBias(WeightInit):\n    """"""Zero initialisation for the ``bias`` attributes of filtered modules. This is recommended for use in conjunction\n    with weight initialisation schemes.\n\n    Example: ::\n\n        >>> import torch\n        >>> import torch.nn as nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks.init import ZeroBias\n\n        # 100 random data points\n        >>> data = torch.rand(100, 3, 5, 5)\n        >>> example_batch = data[:3]\n        >>> initialiser = ZeroBias()\n\n        # Model and trail using zero bias init for some random data\n        >>> model = nn.Sequential(nn.Conv2d(3, 1, 3), nn.ReLU())\n        >>> trial = Trial(model, callbacks=[initialiser]).with_train_data(data, data+5)\n\n    Args:\n        modules (Iterable[nn.Module] or nn.Module, optional): an iterable of nn.Modules or a\n            single nn.Module that will have weights initialised, otherwise this is retrieved from the model\n        targets (list[String]): A list of lookup strings to match which modules will be initialised\n    """"""\n    def __init__(self, modules=None, targets=[\'Conv\', \'Linear\', \'Bilinear\']):\n        def initialiser(module):\n            module.bias.data.zero_()\n\n        super(ZeroBias, self).__init__(initialiser, modules=modules, targets=targets)\n'"
torchbearer/callbacks/label_smoothing.py,1,"b'import torch\n\nimport torchbearer\nfrom torchbearer import cite\nfrom torchbearer.callbacks import Callback\n\n\nbibtex = """"""\n@article{szegedy2015rethinking,\n  title={Rethinking the inception architecture for computer vision. arXiv 2015},\n  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},\n  journal={arXiv preprint arXiv:1512.00567},\n  volume={1512},\n  year={2015}\n}\n""""""\n\n\n@cite(bibtex)\nclass LabelSmoothingRegularisation(Callback):\n    """"""Perform Label Smoothing Regularisation (LSR) on the targets during training. This involves converting the target\n    to a one-hot vector and smoothing according to the value epsilon.\n\n    .. note::\n\n        Requires a multi-label loss, such as nn.BCELoss\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import LabelSmoothingRegularisation\n\n        # Example Trial which does label smoothing regularisation\n        >>> smoothing = LabelSmoothingRegularisation()\n        >>> trial = Trial(None, criterion=nn.BCELoss(), callbacks=[smoothing], metrics=[\'acc\'])\n\n    Args:\n        epsilon (float): The epsilon parameter from the paper\n        classes (int): The number of target classes, not required if the target is already one-hot encoded\n    """"""\n    def __init__(self, epsilon, classes=-1):\n        self.epsilon = epsilon\n        self.classes = classes\n\n    def to_one_hot(self, state):\n        target = state[torchbearer.TARGET]\n\n        if target.dim() == 1:\n            target = target.unsqueeze(1)\n            one_hot = torch.zeros_like(target).repeat(1, self.classes)\n            one_hot.scatter_(1, target, 1)\n            target = one_hot\n        return target\n\n    def on_sample(self, state):\n        target = self.to_one_hot(state)\n        target = (1 - self.epsilon) * target.float() + (self.epsilon / target.size(1))\n        state[torchbearer.TARGET] = target\n\n    def on_sample_validation(self, state):\n        target = self.to_one_hot(state)\n        state[torchbearer.TARGET] = target.float()\n'"
torchbearer/callbacks/live_loss_plot.py,2,"b'import sys\nimport os\n\nimport torchbearer\nfrom torchbearer.callbacks import Callback\nfrom torchbearer.bases import get_metric\n\n\nclass no_print:\n    def __init__(self):\n        pass\n\n    def __enter__(self):\n        self.stdout = sys.stdout\n        sys.stdout = open(os.devnull, \'w\')\n        return self\n\n    def __exit__(self, *exc):\n        sys.stdout = self.stdout\n        return False\n\n\nclass LiveLossPlot(Callback):\n    """"""\n    Callback to write metrics to `LiveLossPlot <https://github.com/stared/livelossplot>`_, a library for visualisation in notebooks\n\n    Example: ::\n\n        >>> import torch.nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import LiveLossPlot\n\n        # Example Trial which clips all model gradients norms at 2 under the L1 norm.\n        >>> model = torch.nn.Linear(1,1)\n        >>> live_loss_plot = LiveLossPlot()\n        >>> trial = Trial(model, callbacks=[live_loss_plot], metrics=[\'acc\'])\n\n    Args:\n        on_batch (bool): If True, batch metrics will be logged. Else batch metrics will not be logged\n        batch_step_size (int): The number of batches between logging metrics\n        on_epoch (bool): If True, epoch metrics will be logged every epoch. Else epoch metrics will not be logged\n        draw_once (bool): If True, draw the plot only at the end of training. Else draw every time metrics are logged\n        kwargs: Keyword arguments for livelossplot.PlotLosses\n\n    State Requirements:\n        - :attr:`torchbearer.state.METRICS`: Metrics should be a dict containing the metrics to be plotted\n        - :attr:`torchbearer.state.BATCH`: Batch should be the current batch or iteration number in the epoch\n    """"""\n    def __init__(self, on_batch=False, batch_step_size=10, on_epoch=True, draw_once=False, **kwargs):\n        super(LiveLossPlot, self).__init__()\n        self._kwargs = kwargs\n\n        self.on_batch = on_batch\n        self.on_epoch = on_epoch\n        self.draw_once = draw_once\n        self.batch_step_size = batch_step_size\n\n        if on_batch:\n            self.on_step_training = self._on_step_training\n\n        if on_epoch:\n            self.on_end_epoch = self._on_end_epoch\n\n    def on_start(self, state):\n        from livelossplot import PlotLosses\n        self.plt = PlotLosses(**self._kwargs)\n        self.batch_plt = PlotLosses(**self._kwargs)\n\n    def _on_step_training(self, state):\n        # These checks shouldn\'t fail\n        self.batch_plt.update({k: get_metric(\'LiveLossPlot\', state, k) for k in state[torchbearer.METRICS]})\n\n        if state[torchbearer.BATCH] % self.batch_step_size == 0 and not self.draw_once:\n            with no_print():\n                self.batch_plt.draw()\n\n    def _on_end_epoch(self, state):\n        self.plt.update({k: get_metric(\'LiveLossPlot\', state, k) for k in state[torchbearer.METRICS]})\n        if not self.draw_once:\n            with no_print():\n                self.plt.draw()\n\n    def on_end(self, state):\n        if self.draw_once:\n            with no_print():\n                self.batch_plt.draw()\n                self.plt.draw()\n'"
torchbearer/callbacks/lsuv.py,9,"b'# Copyright (C) 2017, Dmytro Mishkin\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the\n#    distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# This file has been modified from https://github.com/ducha-aiki/LSUV-pytorch\n\nimport numpy as np\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\n\n\nclass LSUV(object):\n    """"""Initialisation from the paper `All you need is a good init <https://arxiv.org/abs/1511.06422>`__. Call\n    LSUV.init_model(...) on a torch model to perform the initialisation. Implementation based off of the PyTorch\n    implementation `here <https://github.com/ducha-aiki/LSUV-pytorch>`__.\n\n    Example: ::\n\n        >>> import torch.nn\n        >>> from torchbearer.callbacks import LSUV\n\n        # Example of initialising a torch model with LSUV\n        >>> model = torch.nn.Linear(1,1)\n        >>> data = torch.rand(100, 1)\n        >>> lsuv = LSUV(model, data)\n        >>> model = lsuv.init_model(model, data)\n\n    """"""\n    def __init__(self):\n        super(LSUV, self).__init__()\n        self.gg = self.reset_parameters()\n\n    def reset_parameters(self):\n        self.gg = {\n            \'hook_position\': 0,\n            \'total_fc_conv_layers\': 0,\n            \'done_counter\': -1,\n            \'hook\': None,\n            \'act_dict\': {},\n            \'counter_to_apply_correction\': 0,\n            \'correction_needed\': False,\n            \'current_coef\': 1.0,\n            \'weight_lambda\': lambda m: m.weight,\n        }\n        return self.gg\n        \n    def svd_orthonormal(self, w):\n        shape = w.shape\n        flat_shape = (shape[0], np.prod(shape[1:]))\n        a = torch.rand(flat_shape, device=w.device)\n        u, _, v = torch.svd(a, some=True)\n        q = u if u.shape == flat_shape else v.t()\n        q = q.view(shape)\n        return q.to(torch.float)\n    \n    def add_current_hook(self, m):\n        if self.gg[\'hook\'] is not None:\n            return\n        if (isinstance(m, nn.Conv2d)) or (isinstance(m, nn.Linear)):\n            if self.gg[\'hook_position\'] > self.gg[\'done_counter\']:\n                self.gg[\'hook\'] = m.register_forward_hook(self.store_activations_wrapper())\n            else:\n                self.gg[\'hook_position\'] += 1\n    \n    def count_conv_fc_layers(self, m):\n        if (isinstance(m, nn.Conv2d)) or (isinstance(m, nn.Linear)):\n            self.gg[\'total_fc_conv_layers\'] += 1\n    \n    def orthogonal_weights_init(self, m):\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n            weight = self.gg[\'weight_lambda\'](m)\n            w_ortho = self.svd_orthonormal(weight.data)\n            m.weight.data = w_ortho.data\n    \n    def apply_weights_correction(self, m):\n        if self.gg[\'hook\'] is None or not self.gg[\'correction_needed\']:\n            return\n        if (isinstance(m, nn.Conv2d)) or (isinstance(m, nn.Linear)):\n            if self.gg[\'counter_to_apply_correction\'] < self.gg[\'hook_position\']:\n                self.gg[\'counter_to_apply_correction\'] += 1\n            else:\n                weight = self.gg[\'weight_lambda\'](m)\n                weight.data *= float(self.gg[\'current_coef\'])\n                self.gg[\'correction_needed\'] = False\n    \n    def init_model(self, model, data, weight_lambda=None, needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=True):\n        self.gg = self.reset_parameters()\n        train = True if model.training else False\n        self.gg[\'weight_lambda\'] = self.gg[\'weight_lambda\'] if weight_lambda is None else weight_lambda\n    \n        model.eval()\n        model.apply(self.count_conv_fc_layers)\n        if do_orthonorm:\n            model.apply(self.orthogonal_weights_init)\n        for layer_idx in range(self.gg[\'total_fc_conv_layers\']):\n            model.apply(self.add_current_hook)\n            _ = model(data)\n            current_std = self.gg[\'act_dict\'].std()\n            attempts = 0\n            while torch.abs(current_std - needed_std).item() > std_tol:\n                self.gg[\'current_coef\'] = needed_std / (current_std + 1e-8)\n                self.gg[\'correction_needed\'] = True\n                model.apply(self.apply_weights_correction)\n                _ = model(data)\n                current_std = self.gg[\'act_dict\'].std()\n                attempts += 1\n                if attempts > max_attempts:\n                    break\n            if self.gg[\'hook\'] is not None:\n                self.gg[\'hook\'].remove()\n            self.gg[\'done_counter\'] += 1\n            self.gg[\'counter_to_apply_correction\'] = 0\n            self.gg[\'hook_position\'] = 0\n            self.gg[\'hook\'] = None\n              \n        if train:\n            model.train()\n        return model\n\n    def store_activations_wrapper(self):\n        gg = self.gg\n        def store_activations(self, input, output):\n            gg[\'act_dict\'] = output.data\n        return store_activations\n'"
torchbearer/callbacks/manifold_mixup.py,2,"b'from torchbearer import Callback\nimport torchbearer\nfrom torch.distributions.beta import Beta\nimport random\nimport torch\nimport types\nfrom torchbearer import cite\n\n\nmanifold_mixup = """"""\n@inproceedings{verma2019manifold,\n  title={Manifold Mixup: Better Representations by Interpolating Hidden States},\n  author={Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua},\n  booktitle={International Conference on Machine Learning},\n  pages={6438--6447},\n  year={2019}\n}\n""""""\n\n\n@cite(manifold_mixup)\nclass ManifoldMixup(Callback):\n    """""" Performs manifold mixup on desired layers. Requires use of :meth:`MixupInputs.loss`, otherwise lambdas can be found in\n    state under :attr:`.MIXUP_LAMBDA`. Model targets will be a tuple containing the original target and permuted target.\n\n    Args:\n        lam: Mixup lambda. If RANDOM, choose lambda from Beta(alpha, alpha). Else, lambda=lam\n        alpha: Mixup alpha. Alpha used to sample lambda from Beta(alpha, alpha)\n    """"""\n    RANDOM = -10.0\n\n    def __init__(self, alpha=1.0, lam=RANDOM):\n        super(ManifoldMixup, self).__init__()\n        self._layers = []\n        self._mixup_layers = None\n        self.alpha = alpha\n        self.lam = lam\n        self.distrib = Beta(self.alpha, self.alpha)\n        self.layer_names = []\n        self.depth = 0\n        self._layer_filter = []\n        self._layer_types = []\n\n    def _sample_lam(self):\n        if self.lam is self.RANDOM:\n            if self.alpha > 0:\n                lam = self.distrib.sample()\n            else:\n                lam = 1.0\n        else:\n            lam = self.lam\n        return lam\n\n    def _single_to_list(self, item):\n        if not isinstance(item, list) or isinstance(item, tuple):\n            item = [item, ]\n        return item\n\n    def for_layers(self, layers):\n        """""" Specify the layer names on which to apply manifold mixup.\n\n        Args:\n            layers (list or str): Layer names, eg [\'conv1\', \'fc1\']\n\n        Returns: self\n        """"""\n        layers = self._single_to_list(layers)\n        self._mixup_layers = layers\n        return self\n\n    def with_layer_filter(self, layer_filter=()):\n        """""" Specify layer names to exclude from manifold mixup.\n\n        Args:\n            layer_filter (list or str): Layer filter, eg [\'conv1\', \'fc1\']\n\n        Returns: self\n        """"""\n        layer_filter = self._single_to_list(layer_filter)\n        self._layer_filter = layer_filter\n        return self\n\n    def with_layer_type_filter(self, layer_types=()):\n        """""" Specify the layer types to exclude from manifold mixup\n\n        Args:\n            layer_types (list or str): Layer types, eg [nn.RelU]:\n\n        Returns: self\n        """"""\n        layer_types = self._single_to_list(layer_types)\n        self._layer_types = layer_types\n        return self\n\n    def at_depth(self, N):\n        """""" Specify the module depth at which to search for layers. Top level modules are at level 0. \n        Submodules of these are at level 1, etc. \n        To search all depths, set N=None.\n\n        Args:\n            N (int or None): Module depth\n\n        Returns: self\n        """"""\n        self.depth = N\n        return self\n\n    def _wrap_layers(self, model, state):\n        # Wrap the chosen layers with redefined forward that does mixup\n        self._recursive_wrap(model, \'\', state, 0)\n\n    def on_start(self, state):\n        super(ManifoldMixup, self).on_start(state)\n        self._wrap_layers(state[torchbearer.MODEL], state)\n\n    def on_sample(self, state):\n        lam = self._sample_lam()\n        state[torchbearer.MIXUP_LAMBDA] = lam\n\n        layer = random.choice(self._layers)\n        layer.mixup()\n\n        state[torchbearer.MIXUP_PERMUTATION] = torch.randperm(state[torchbearer.X].size(0))\n        state[torchbearer.Y_TRUE] = (state[torchbearer.Y_TRUE], state[torchbearer.Y_TRUE][state[torchbearer.MIXUP_PERMUTATION]])\n        \n    def _wrap_layer_check(self, module, name, nname):\n        # Check for exclusions\n        name_check = self._mixup_layers is None or nname in self._mixup_layers\n\n        filters = [\n            any([f == nname for f in self._layer_filter]),\n            any([isinstance(module, t) for t in self._layer_types]),\n        ]\n        return name_check and not any(filters)\n    \n    def get_selected_layers(self, model):\n        layer_names = []\n        return self._recursive_name_seach(layer_names, model, \'\', 0)\n\n    def _recursive_name_seach(self, layer_names, layer, pre_name, depth):\n        for name, module in layer.named_children():\n            nname = pre_name + \'_\' + name if pre_name != \'\' else name\n            if depth == self.depth or self.depth is None:\n                if self._wrap_layer_check(module, name, nname):\n                    layer_names.append(nname)\n                \n            if self.depth is None or depth <= self.depth:\n                if len(list(layer.named_children())) > 0:\n                    self._recursive_name_seach(layer_names, module, nname, depth+1)\n        return layer_names\n\n    def _recursive_wrap(self, layer, pre_name, state, depth):\n        for name, module in layer.named_children():\n            nname = pre_name + \'_\' + name if pre_name != \'\' else name\n\n            def new_forward(old_forward):\n                def new_forward_1(self, *args, **kwargs):\n                    o = old_forward(*args, **kwargs)\n                    \n                    if self.do_mixup and self.training:\n                        o = _mixup_inputs(o, state)\n\n                    self.do_mixup = False\n                    return o\n\n                return new_forward_1\n\n            if depth == self.depth or self.depth is None:\n                self.layer_names.append(nname)\n\n                if self._wrap_layer_check(module, name, nname):\n                    self._layers.append(module)\n                    nf = new_forward(module.forward)\n\n                    bound_forward = types.MethodType(nf, module)\n                    bound_mixup = types.MethodType(_mixup, module)\n\n                    module.__setattr__(\'forward\', bound_forward)\n                    module.__setattr__(\'mixup\', bound_mixup)\n                    module.__setattr__(\'state\', state)\n                    module.__setattr__(\'do_mixup\', False)\n\n            if self.depth is None or depth <= self.depth:\n                if len(list(layer.named_children())) > 0:\n                    self._recursive_wrap(module, nname, state, depth+1)\n\n\ndef _mixup(self):\n    self.do_mixup = True\n\n\ndef _mixup_inputs(x, state):\n    index = state[torchbearer.MIXUP_PERMUTATION]\n    lam = state[torchbearer.MIXUP_LAMBDA]\n    mixed_x = lam * x + (1 - lam) * x[index,:]\n    return mixed_x\n\n'"
torchbearer/callbacks/mixup.py,3,"b'import torch\nimport torch.nn.functional as F\nfrom torch.distributions.beta import Beta\n\nimport torchbearer\nfrom torchbearer import cite\nfrom torchbearer.callbacks import Callback\nfrom torchbearer.metrics import CategoricalAccuracy, AdvancedMetric, running_mean, mean, super\n\n\nmixup= """"""\n@inproceedings{zhang2018mixup,\n  title={mixup: Beyond Empirical Risk Minimization},\n  author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},\n  booktitle={International Conference on Learning Representations},\n  year={2018}\n}\n""""""\n\n\n@running_mean\n@mean\nclass MixupAcc(AdvancedMetric):\n    def __init__(self):\n        super(MixupAcc, self).__init__(\'mixup_acc\')\n        self.cat_acc = CategoricalAccuracy().root\n\n    def process_train(self, *args):\n        super(MixupAcc, self).process_train(*args)\n        state = args[0]\n        \n        target1, target2 = state[torchbearer.Y_TRUE]\n        _state = args[0].copy()\n        _state[torchbearer.Y_TRUE] = target1\n        acc1 = self.cat_acc.process(_state)\n\n        _state = args[0].copy()\n        _state[torchbearer.Y_TRUE] = target2\n        acc2 = self.cat_acc.process(_state)\n\n        return acc1 * state[torchbearer.MIXUP_LAMBDA] + acc2 * (1-state[torchbearer.MIXUP_LAMBDA])\n\n    def process_validate(self, *args):\n        super(MixupAcc, self).process_validate(*args)\n\n        return self.cat_acc.process(*args)\n\n    def reset(self, state):\n        self.cat_acc.reset(state)\n\n\n@cite(mixup)\nclass Mixup(Callback):\n    """"""Perform mixup on the model inputs. Requires use of :meth:`MixupInputs.loss`, otherwise lambdas can be found in\n    state under :attr:`.MIXUP_LAMBDA`. Model targets will be a tuple containing the original target and permuted target.\n\n    .. note::\n\n        The accuracy metric for mixup is different on training to deal with the different targets,\n        but for validation it is exactly the categorical accuracy, despite being called ""val_mixup_acc""\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import Mixup\n\n        # Example Trial which does Mixup regularisation\n        >>> mixup = Mixup(0.9)\n        >>> trial = Trial(None, criterion=Mixup.mixup_loss, callbacks=[mixup], metrics=[\'acc\'])\n\n    Args:\n        lam (float): Mixup inputs by fraction lam. If RANDOM, choose lambda from Beta(alpha, alpha). Else, lambda=lam\n        alpha (float): The alpha value to use in the beta distribution.\n    """"""\n    RANDOM = -10.0\n\n    def __init__(self, alpha=1.0, lam=RANDOM):\n        super(Mixup, self).__init__()\n        self.alpha = alpha\n        self.lam = lam\n        self.distrib = Beta(self.alpha, self.alpha)\n\n    @staticmethod\n    def mixup_loss(state):\n        """"""The standard cross entropy loss formulated for mixup (weighted combination of `F.cross_entropy`).\n\n        Args:\n            state: The current :class:`Trial` state.\n        """"""\n        input, target = state[torchbearer.Y_PRED], state[torchbearer.Y_TRUE]\n\n        if state[torchbearer.DATA] is torchbearer.TRAIN_DATA:\n            y1, y2 = target\n            return F.cross_entropy(input, y1) * state[torchbearer.MIXUP_LAMBDA] + F.cross_entropy(input, y2) * (1-state[torchbearer.MIXUP_LAMBDA])\n        else:\n            return F.cross_entropy(input, target)\n\n    def on_sample(self, state):\n        if self.lam is Mixup.RANDOM:\n            if self.alpha > 0:\n                lam = self.distrib.sample()\n            else:\n                lam = 1.0\n        else:\n            lam = self.lam\n\n        state[torchbearer.MIXUP_LAMBDA] = lam\n\n        state[torchbearer.MIXUP_PERMUTATION] = torch.randperm(state[torchbearer.X].size(0))\n        state[torchbearer.X] = state[torchbearer.X] * state[torchbearer.MIXUP_LAMBDA] + state[torchbearer.X][state[torchbearer.MIXUP_PERMUTATION], :] * (1-state[torchbearer.MIXUP_LAMBDA])\n        state[torchbearer.Y_TRUE] = (state[torchbearer.Y_TRUE], state[torchbearer.Y_TRUE][state[torchbearer.MIXUP_PERMUTATION]])\n\n\nfrom torchbearer.metrics import default as d\nd.__loss_map__[Mixup.mixup_loss.__name__] = MixupAcc\n'"
torchbearer/callbacks/printer.py,2,"b'from __future__ import print_function\nfrom collections import OrderedDict\nfrom functools import partial\n\nfrom tqdm import tqdm\n\nimport torchbearer\nfrom torchbearer.callbacks import Callback\n\n\ndef _format_metrics(metrics, rounder):\n    # Adapted from https://github.com/tqdm/tqdm\n    postfix = OrderedDict([])\n    for key in sorted(metrics.keys()):\n        postfix[key] = metrics[key]\n\n    for key in postfix.keys():\n        try:\n            postfix[key] = str(rounder(postfix[key]))\n        except TypeError:\n            try:\n                postfix[key] = str(list(map(rounder, postfix[key])))\n            except TypeError:\n                postfix[key] = str(postfix[key])\n    postfix = \', \'.join(key + \'=\' + postfix[key].strip() for key in postfix.keys())\n    return postfix\n\n\nclass ConsolePrinter(Callback):\n    """"""The ConsolePrinter callback simply outputs the training metrics to the console.\n\n    Example: ::\n\n        >>> import torch.nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import ConsolePrinter\n\n        # Example Trial which forgoes the usual printer for a console printer\n        >>> printer = ConsolePrinter()\n        >>> trial = Trial(None, callbacks=[printer], verbose=0).for_steps(1).run()\n        0/1(t):\n\n    Args:\n        validation_label_letter (str): This is the letter displayed after the epoch number indicating the current phase\n            of training\n        precision (int): Precision of the number format in decimal places\n\n    State Requirements:\n        - :attr:`torchbearer.state.EPOCH`: The current epoch number\n        - :attr:`torchbearer.state.MAX_EPOCHS`: The total number of epochs for this run\n        - :attr:`torchbearer.state.BATCH`: The current batch / iteration number\n        - :attr:`torchbearer.state.STEPS`: The total number of steps / batches / iterations for this epoch\n        - :attr:`torchbearer.state.METRICS`: The metrics dict to print\n    """"""\n    def __init__(self, validation_label_letter=\'v\', precision=4):\n        super(ConsolePrinter, self).__init__()\n        self.validation_label = validation_label_letter\n        self.rounder = partial(round, ndigits=precision)\n\n    def _step(self, state, letter, steps):\n        epoch_str = \'{:d}/{:d}({:s}): \'.format(state[torchbearer.EPOCH], state[torchbearer.MAX_EPOCHS], letter)\n        batch_str = \'{:d}/{:d} \'.format(state[torchbearer.BATCH], steps)\n        stats_str = _format_metrics(state[torchbearer.METRICS], self.rounder)\n        print(\'\\r\' + epoch_str + batch_str + stats_str, end=\'\')\n\n    def _end(self, state, letter):\n        epoch_str = \'{:d}/{:d}({:s}): \'.format(state[torchbearer.EPOCH], state[torchbearer.MAX_EPOCHS], letter)\n        stats_str = _format_metrics(state[torchbearer.METRICS], self.rounder)\n        print(\'\\r\' + epoch_str + stats_str)\n\n    def on_step_training(self, state):\n        self._step(state, \'t\', state[torchbearer.STEPS])\n\n    def on_end_training(self, state):\n        self._end(state, \'t\')\n\n    def on_step_validation(self, state):\n        self._step(state, self.validation_label, state[torchbearer.STEPS])\n\n    def on_end_validation(self, state):\n        self._end(state, self.validation_label)\n\n\nclass Tqdm(Callback):\n    """"""The Tqdm callback outputs the progress and metrics for training and validation loops to the console using TQDM.\n    The given key is used to label validation output.\n\n    Example: ::\n\n        >>> import torch.nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import Tqdm\n\n        # Example Trial which forgoes the usual printer for a customised tqdm printer.\n        >>> printer = Tqdm(precision=8)\n        # Note that outputs are written to stderr, not stdout as shown in this example\n        >>> trial = Trial(None, callbacks=[printer], verbose=0).for_steps(1).run(1)\n        0/1(t): 100%|...| 1/1 [00:00<00:00, 29.40it/s]\n\n    Args:\n        tqdm_module: The tqdm module to use. If none, defaults to tqdm or tqdm_notebook if in notebook\n        validation_label_letter (str): The letter to use for validation outputs.\n        precision (int): Precision of the number format in decimal places\n        on_epoch (bool): If True, output a single progress bar which tracks epochs\n        tqdm_args: Any extra keyword args provided here will be passed through to the tqdm module constructor.\n            See `github.com/tqdm/tqdm#parameters <https://github.com/tqdm/tqdm#parameters>`_ for more details.\n\n    State Requirements:\n        - :attr:`torchbearer.state.EPOCH`: The current epoch number\n        - :attr:`torchbearer.state.MAX_EPOCHS`: The total number of epochs for this run\n        - :attr:`torchbearer.state.STEPS`: The total number of steps / batches / iterations for this epoch\n        - :attr:`torchbearer.state.METRICS`: The metrics dict to print\n        - :attr:`torchbearer.state.HISTORY`: The history of the :class:`.Trial` object\n    """"""\n    def __init__(self, tqdm_module=None, validation_label_letter=\'v\', precision=4, on_epoch=False, **tqdm_args):\n        if torchbearer.magics.is_notebook() and tqdm_module is None:\n            from tqdm import tqdm_notebook\n            self.tqdm_module = tqdm_notebook\n        else:\n            self.tqdm_module = tqdm if tqdm_module is None else tqdm_module\n\n        self._loader = None\n        self.validation_label = validation_label_letter\n        self.rounder = partial(round, ndigits=precision)\n        self._on_epoch = on_epoch\n        self.tqdm_args = tqdm_args\n\n    def _on_start(self, state, letter):\n        bar_desc = \'{:d}/{:d}({:s})\'.format(state[torchbearer.EPOCH], state[torchbearer.MAX_EPOCHS], letter)\n        self._loader = self.tqdm_module(total=state[torchbearer.STEPS], desc=bar_desc, **self.tqdm_args)\n\n    def _update(self, state):\n        self._loader.update(1)\n        self._loader.set_postfix_str(_format_metrics(state[torchbearer.METRICS], self.rounder))\n\n    def _close(self, state):\n        self._loader.set_postfix_str(_format_metrics(state[torchbearer.METRICS], self.rounder))\n        self._loader.close()\n\n    def on_start(self, state):\n        if self._on_epoch:\n\n            n = len(state[torchbearer.HISTORY])\n            self._loader = self.tqdm_module(initial=n, total=state[torchbearer.MAX_EPOCHS], **self.tqdm_args)\n\n            if n > 0:\n                metrics = dict(state[torchbearer.HISTORY][-1])\n                try:\n                    del metrics[str(torchbearer.TRAIN_STEPS)]\n                except KeyError:\n                    pass\n\n                try:\n                    del metrics[str(torchbearer.VALIDATION_STEPS)]\n                except KeyError:\n                    pass\n                state[torchbearer.METRICS] = metrics\n                self._update(state)\n\n    def on_end_epoch(self, state):\n        if self._on_epoch:\n            self._update(state)\n\n    def on_end(self, state):\n        if self._on_epoch:\n            self._close(state)\n\n    def on_start_training(self, state):\n        """"""Initialise the TQDM bar for this training phase.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if not self._on_epoch:\n            self._on_start(state, \'t\')\n\n    def on_step_training(self, state):\n        """"""Update the bar with the metrics from this step.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if not self._on_epoch:\n            self._update(state)\n\n    def on_end_training(self, state):\n        """"""Update the bar with the terminal training metrics and then close.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if not self._on_epoch:\n            self._close(state)\n\n    def on_start_validation(self, state):\n        """"""Initialise the TQDM bar for this validation phase.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if not self._on_epoch:\n            self._on_start(state, self.validation_label)\n\n    def on_step_validation(self, state):\n        """"""Update the bar with the metrics from this step.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if not self._on_epoch:\n            self._update(state)\n\n    def on_end_validation(self, state):\n        """"""Update the bar with the terminal validation metrics and then close.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if not self._on_epoch:\n            self._close(state)\n'"
torchbearer/callbacks/pycm.py,1,"b'from __future__ import print_function\n\nimport torch\n\nimport torchbearer\nfrom torchbearer import Callback\nfrom torchbearer.bases import cite\nfrom torchbearer.metrics import EpochLambda, MetricList\n\n\npycm_bib = """"""\n@article{Haghighi2018,\n  doi = {10.21105/joss.00729},\n  url = {https://doi.org/10.21105/joss.00729},\n  year  = {2018},\n  month = {may},\n  publisher = {The Open Journal},\n  volume = {3},\n  number = {25},\n  pages = {729},\n  author = {Sepand Haghighi and Masoomeh Jasemi and Shaahin Hessabi and Alireza Zolanvari},\n  title = {{PyCM}: Multiclass confusion matrix library in Python},\n  journal = {Journal of Open Source Software}\n}\n""""""\n\n\ndef _to_pyplot(normalize=False, title=\'Confusion matrix\', cmap=None):\n    """"""\n    This function modified to plots the ConfusionMatrix object.\n    Normalization can be applied by setting `normalize=True`.\n\n    Code Reference :\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n    """"""\n    import matplotlib.pyplot as plt\n    import itertools\n    import numpy as np\n\n    if cmap is None:\n        cmap = plt.cm.Blues\n\n    def handler(cm, state):\n        string_state = {str(key): state[key] for key in state.keys()}\n        plt_cm = []\n        for i in cm.classes:\n            row = []\n            for j in cm.classes:\n                row.append(cm.table[i][j])\n            plt_cm.append(row)\n        plt_cm = np.array(plt_cm)\n        if normalize:\n            plt_cm = plt_cm.astype(\'float\') / plt_cm.sum(axis=1)[:, np.newaxis]\n        plt.imshow(plt_cm, interpolation=\'nearest\', cmap=cmap)\n        plt.title(title.format(**string_state))\n        plt.colorbar()\n        tick_marks = np.arange(len(cm.classes))\n        plt.xticks(tick_marks, cm.classes, rotation=45)\n        plt.yticks(tick_marks, cm.classes)\n\n        fmt = \'.2f\' if normalize else \'d\'\n        thresh = plt_cm.max() / 2.\n        for i, j in itertools.product(range(plt_cm.shape[0]), range(plt_cm.shape[1])):\n            plt.text(j, i, format(plt_cm[i, j], fmt),\n                     horizontalalignment=""center"",\n                     color=""white"" if plt_cm[i, j] > thresh else ""black"")\n\n        plt.tight_layout()\n        plt.ylabel(\'Actual\')\n        plt.xlabel(\'Predict\')\n        plt.show()\n    return handler\n\n\n@cite(pycm_bib)\nclass PyCM(Callback):\n    """"""Create confusion matrices using the `PyCM library <https://github.com/sepandhaghighi/pycm>`_.\n\n    Args:\n        kwargs: Additional keyword args to pass to the `ConfusionMatrix` call\n    """"""\n    def __init__(self, **kwargs):\n        import sys\n        if sys.version_info[0] < 3:\n            raise Exception(\'PyCM requires Python>=3!\')\n        self._handlers = []\n\n        self.kwargs = kwargs\n\n    def _add_metric(self, state):\n        from pycm import ConfusionMatrix\n\n        def make_cm(y_pred, y_true):\n            _, y_pred = torch.max(y_pred, 1)\n            cm = ConfusionMatrix(y_true.cpu().numpy(), y_pred.cpu().numpy(), **self.kwargs)\n            for handler in self._handlers:\n                handler(cm, state)\n\n        my_metric = EpochLambda(\'pycm\', make_cm, False)\n        my_metric.reset(state)\n        state[torchbearer.METRIC_LIST] = MetricList([state[torchbearer.METRIC_LIST], my_metric])\n\n    def on_train(self):\n        """"""Process this callback for training batches\n\n        Returns:\n            PyCM: self\n        """"""\n        _old_start_training = self.on_start_training\n\n        def wrapper(state):\n            _old_start_training(state)\n            self._add_metric(state)\n\n        self.on_start_training = wrapper\n        return self\n\n    def on_val(self):\n        """"""Process this callback for validation batches\n\n        Returns:\n            PyCM: self\n        """"""\n        _old_start_validation = self.on_start_validation\n\n        def wrapper(state):\n            _old_start_validation(state)\n            if state[torchbearer.DATA] is torchbearer.VALIDATION_DATA:\n                self._add_metric(state)\n\n        self.on_start_validation = wrapper\n        return self\n\n    def on_test(self):\n        """"""Process this callback for test batches\n\n        Returns:\n            PyCM: self\n        """"""\n        _old_start_validation = self.on_start_validation\n\n        def wrapper(state):\n            _old_start_validation(state)\n            if state[torchbearer.DATA] is torchbearer.TEST_DATA:\n                self._add_metric(state)\n\n        self.on_start_validation = wrapper\n        return self\n\n    def with_handler(self, handler):\n        """"""Append the given output handler to the list of handlers\n\n        Args:\n            handler: A function of confusion and matrix and state\n\n        Returns:\n            PyCM: self\n        """"""\n        self._handlers.append(handler)\n        return self\n\n    def to_state(self, key):\n        """"""Send `ConfusionMatrix` objects from this callback to the given state key\n\n        Args:\n            key (StateKey): The key to store the confusion matrix in\n\n        Returns:\n            PyCM: self\n        """"""\n        def handler(cm, state):\n            state[key] = cm\n        return self.with_handler(handler)\n\n    def to_console(self):\n        """"""Print `ConfusionMatrix` objects from this callback to the console\n\n        Returns:\n            PyCM: self\n        """"""\n        return self.with_handler(lambda cm, _: print(cm))\n\n    def to_pycm_file(self, filename, *args, **kwargs):\n        """"""Save `ConfusionMatrix` objects from this callback to `.pycm` files\n\n        Args:\n            filename (str): The name of the file, will be formatted with state to create unique filenames if desired\n\n        See:\n            `PyCM Source (save_stat) <https://github.com/sepandhaghighi/pycm/blob/master/pycm/pycm_obj.py>`_\n\n        Returns:\n            PyCM: self\n        """"""\n        def handler(cm, state):\n            string_state = {str(key): state[key] for key in state.keys()}\n            cm.save_stat(filename.format(**string_state), *args, **kwargs)\n        return self.with_handler(handler)\n\n    def to_html_file(self, filename, *args, **kwargs):\n        """"""Save `ConfusionMatrix` objects from this callback to `.html` files\n\n        Args:\n            filename (str): The name of the file, will be formatted with state to create unique filenames if desired\n\n        See:\n            `PyCM Source (save_html) <https://github.com/sepandhaghighi/pycm/blob/master/pycm/pycm_obj.py>`_\n\n        Returns:\n            PyCM: self\n        """"""\n        def handler(cm, state):\n            string_state = {str(key): state[key] for key in state.keys()}\n            cm.save_html(filename.format(**string_state), *args, **kwargs)\n        return self.with_handler(handler)\n\n    def to_csv_file(self, filename, *args, **kwargs):\n        """"""Save `ConfusionMatrix` objects from this callback to `.csv` files\n\n        Args:\n            filename (str): The name of the file, will be formatted with state to create unique filenames if desired\n\n        See:\n            `PyCM Source (save_csv) <https://github.com/sepandhaghighi/pycm/blob/master/pycm/pycm_obj.py>`_\n\n        Returns:\n            PyCM: self\n        """"""\n        def handler(cm, state):\n            string_state = {str(key): state[key] for key in state.keys()}\n            cm.save_csv(filename.format(**string_state), *args, **kwargs)\n        return self.with_handler(handler)\n\n    def to_obj_file(self, filename, *args, **kwargs):\n        """"""Save `ConfusionMatrix` objects from this callback to `.obj` files\n\n        Args:\n            filename (str): The name of the file, will be formatted with state to create unique filenames if desired\n\n        See:\n            `PyCM Source (save_obj) <https://github.com/sepandhaghighi/pycm/blob/master/pycm/pycm_obj.py>`_\n\n        Returns:\n            PyCM: self\n        """"""\n        def handler(cm, state):\n            string_state = {str(key): state[key] for key in state.keys()}\n            cm.save_obj(filename.format(**string_state), *args, **kwargs)\n        return self.with_handler(handler)\n\n    def to_pyplot(self, normalize=False, title=\'Confusion matrix\', cmap=None):\n        """"""Plot `ConfusionMatrix` objects from this callback with `matplotlib.pyplot`\n\n        Args:\n            normalize (bool): If true, normalize the confusion matrix\n            title (str): Title to use for the plot, will be formatted with state to create unique titles if desired\n            cmap: Colour map object to use for the plot, defaults to `plt.cm.Blues` if None\n\n        See:\n            `PyCM Source <https://github.com/sepandhaghighi/pycm/blob/master/pycm/pycm_obj.py>`_\n\n        Returns:\n            PyCM: self\n        """"""\n        return self.with_handler(_to_pyplot(normalize=normalize, title=title, cmap=cmap))\n'"
torchbearer/callbacks/sample_pairing.py,1,"b'import torch\n\nimport torchbearer\nfrom torchbearer import cite\nfrom torchbearer.callbacks import Callback, only_if, once_per_epoch\n\n\nbibtex = """"""\n@article{inoue2018data,\n  title={Data augmentation by pairing samples for images classification},\n  author={Inoue, Hiroshi},\n  journal={arXiv preprint arXiv:1801.02929},\n  year={2018}\n}\n""""""\n\n\n@cite(bibtex)\nclass SamplePairing(Callback):\n    """"""Perform SamplePairing on the model inputs. This is the process of averaging each image with another random image\n    without changing the targets. The key here is to use the policy function to only do this some of the time.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import SamplePairing\n\n        # Example Trial which does Sample Pairing regularisation with the policy from the paper\n        >>> pairing = SamplePairing()\n        >>> trial = Trial(None, criterion=Mixup.loss, callbacks=[pairing], metrics=[\'acc\'])\n\n    Args:\n        policy: A function of state which returns True if the current batch should be paired.\n    """"""\n\n    @staticmethod\n    def default_policy(start_epoch, end_epoch, on_epochs, off_epochs):\n        """"""Return a policy which performs sample pairing according to the process defined in the paper.\n\n        Args:\n            start_epoch (int): Epoch to start pairing on\n            end_epoch (int): Epoch to end pairing on (and begin fine-tuning)\n            on_epochs (int): Number of epochs to run sample pairing for before a break\n            off_epochs (int): Number of epochs to break for\n\n        Returns:\n            A policy function\n        """"""\n        cache = {\'tick\': 0, \'on\': False}\n\n        @once_per_epoch\n        def compute(state):\n            if start_epoch <= state[torchbearer.EPOCH] < end_epoch:\n                if cache[\'tick\'] < on_epochs:\n                    cache[\'on\'] = True\n                else:\n                    cache[\'on\'] = False\n\n                cache[\'tick\'] = cache[\'tick\'] + 1\n                if cache[\'tick\'] == (on_epochs + off_epochs):\n                    cache[\'tick\'] = 0\n                    cache[\'on\'] = False\n            else:\n                cache[\'on\'] = False\n\n        def policy(state):\n            compute(state)\n            return cache[\'on\']\n\n        return policy\n\n    def __init__(self, policy=None):\n        self.on_sample = only_if(SamplePairing.default_policy(100, 800, 8, 2) if policy is None else policy)(self.on_sample)\n\n    def on_sample(self, state):\n        permutation = torch.randperm(state[torchbearer.X].size(0))\n        state[torchbearer.INPUT] = (state[torchbearer.INPUT] + state[torchbearer.INPUT][permutation]) / 2.\n'"
torchbearer/callbacks/tensor_board.py,14,"b'import copy\nimport os\nimport warnings\n\nimport torch\nimport torch.nn.functional as F\n\nimport torchbearer\nfrom torchbearer.callbacks import Callback\n\n__writers__ = dict()\n\n\nclass VisdomParams:\n    """"""\n    Class to hold visdom client arguments. Modify member variables before initialising tensorboard callbacks for custom\n    arguments. See: `visdom <https://github.com/facebookresearch/visdom#visdom-arguments-python-only>`_\n    """"""\n    SERVER = \'http://localhost\'\n    ENDPOINT = \'events\'\n    PORT = 8097\n    IPV6 = True\n    HTTP_PROXY_HOST = None\n    HTTP_PROXY_PORT = None\n    ENV = \'main\'\n    SEND = True\n    RAISE_EXCEPTIONS = None\n    USE_INCOMING_SOCKET = True\n    LOG_TO_FILENAME = None\n\n    def __to_dict__(self):\n        base_params = {e.lower(): VisdomParams.__dict__[e] for e in VisdomParams.__dict__ if \'__\' not in e}\n        new_params = {e.lower(): self.__dict__[e] for e in self.__dict__ if \'__\' not in e}\n        base_params.update(new_params)\n        return base_params\n\n\ndef get_writer(log_dir, logger, visdom=False, visdom_params=None):\n    """"""\n    Get the writer assigned to the given log directory.\n    If the writer doesn\'t exist it will be created, and a reference to the logger added.\n\n    Args:\n        log_dir (str): the log directory\n        logger: the object requesting the writer. That object should call `close_writer` when its finished\n        visdom (bool): if true VisdomWriter is returned instead of tensorboard SummaryWriter\n        visdom_params (VisdomParams): Visdom parameter settings object, uses default if None\n\n    Returns:\n        the `SummaryWriter` or `VisdomWriter` object\n    """"""\n    import tensorboardX\n    from tensorboardX import SummaryWriter\n    import sys\n    import errno\n    kwargs = {}\n    if sys.version_info[0] >= 3:\n        kwargs[\'exist_ok\'] = True\n\n    writer_key = \'writer\'\n    if visdom:\n        writer_key = \'writer_visdom\'\n\n    if log_dir not in __writers__ or writer_key not in __writers__[log_dir]:\n        if visdom:\n            w = tensorboardX.torchvis.VisdomWriter()\n            from visdom import Visdom\n            try:\n                os.makedirs(log_dir, **kwargs)\n            except OSError as exc:\n                if exc.errno == errno.EEXIST and os.path.isdir(log_dir):\n                    pass\n                else:\n                    raise exc\n            if visdom_params is None:\n                visdom_params = VisdomParams()\n                visdom_params.LOG_TO_FILENAME = os.path.join(log_dir, \'log.log\')\n            w.vis = Visdom(**visdom_params.__to_dict__())\n        else:\n            w = SummaryWriter(log_dir=log_dir)\n        __writers__[log_dir] = {writer_key: w, \'references\': set()}\n\n    __writers__[log_dir][\'references\'].add(logger)\n    return __writers__[log_dir][writer_key]\n\n\ndef close_writer(log_dir, logger):\n    """"""\n    Decrement the reference count for a writer belonging to a specific log directory.\n    If the reference count gets to zero, the writer will be closed and removed.\n\n    Args:\n        log_dir (str): the log directory\n        logger: the object releasing the writer\n    """"""\n    if log_dir in __writers__:\n        __writers__[log_dir][\'references\'].discard(logger)\n\n        if len(__writers__[log_dir][\'references\']) is 0:\n            if \'writer\' in __writers__[log_dir]:\n                __writers__[log_dir][\'writer\'].close()\n\n            if \'writer_visdom\' in __writers__[log_dir]:\n                __writers__[log_dir][\'writer_visdom\'].close()\n\n        del __writers__[log_dir]\n\n\nclass AbstractTensorBoard(Callback):\n    """"""TensorBoard callback which writes metrics to the given log directory. Requires the TensorboardX library for python.\n\n    Args:\n        log_dir (str): The tensorboard log path for output\n        comment (str): Descriptive comment to append to path\n        visdom (bool): If true, log to visdom instead of tensorboard\n        visdom_params (VisdomParams): Visdom parameter settings object, uses default if None\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: PyTorch model\n    """"""\n\n    def __init__(self, log_dir=\'./logs\',\n                 comment=\'torchbearer\', visdom=False, visdom_params=None):\n        super(AbstractTensorBoard, self).__init__()\n\n        self.raw_log_dir = log_dir\n        self.log_dir = log_dir\n        self.comment = comment\n        self.writer = None\n        self.visdom = visdom\n        self.visdom_params = visdom_params\n\n    def get_writer(self, log_dir=None, visdom=False, visdom_params=None):\n        """"""\n        Get a SummaryWriter for the given directory (or the default writer if the directory is not given).\n        If you are getting a `SummaryWriter` for a custom directory, it is your responsibility to close\n        it using `close_writer`.\n\n        Args:\n            log_dir (str): the (optional) directory\n            visdom (bool): If true, return VisdomWriter, if false return tensorboard SummaryWriter\n            visdom_params (VisdomParams): Visdom parameter settings object, uses default if None\n\n        Returns:\n            the `SummaryWriter` or `VisdomWriter`\n        """"""\n        if log_dir is None:\n            self.writer = get_writer(self.log_dir, self, visdom=visdom, visdom_params=visdom_params)\n            return self.writer\n        else:\n            return get_writer(log_dir, self, visdom=visdom, visdom_params=visdom_params)\n\n    def close_writer(self, log_dir=None):\n        """"""\n        Decrement the reference count for a writer belonging to the given log directory\n        (or the default writer if the directory is not given). If the reference count gets to zero,\n        the writer will be closed and removed.\n\n        Args:\n            log_dir (str): the (optional) directory\n        """"""\n        if log_dir is None:\n            close_writer(self.log_dir, self)\n        else:\n            close_writer(log_dir, self)\n\n    def on_start(self, state):\n        self.log_dir = os.path.join(self.log_dir, state[torchbearer.MODEL].__class__.__name__ + \'_\' + self.comment)\n        self.writer = self.get_writer(visdom=self.visdom, visdom_params=self.visdom_params)\n\n    @staticmethod\n    def add_metric(add_fn, tag, metric, *args, **kwargs):\n        """""" Static method that recurses through `metric` until the `add_fn` can be applied. Useful when metric is an\n        iterable of tensors so that the tensors can  all be passed to an `add_fn` such as writer.add_scalar.\n        For example, if passed `metric` as [[A, B], [C, ], D, {\'E\': E}] then `add_fn` would be called on A, B, C, D and\n        E and the respective tags (with base tag \'met\') would be: met_0_0, met_0_1, met_1_0, met_2, met_E. Throws a\n        warning if `add_fn` fails to parse a metric.\n\n        Args:\n            add_fn: Function to be called to log a metric, e.g. SummaryWriter.add_scalar\n            tag: Tag under which to log the metric\n            metric: Iterable of metrics.\n            *args: Args for `add_fn`\n            **kwargs: Keyword args for `add_fn`\n\n        Returns:\n\n        """"""\n        try:\n            add_fn(tag, metric, *args, **kwargs)\n        except NotImplementedError:\n            try:\n                for key, met in enumerate(metric):\n                    if isinstance(metric, dict):\n                        key, met = met, metric[met]\n\n                    AbstractTensorBoard.add_metric(add_fn, tag+\'_{}\'.format(key), met, *args, **kwargs)\n            except TypeError as e:\n                warnings.warn(\'Failed to log metric to tensorboard with error: {}\'.format(e))\n        except Exception as e:\n            warnings.warn(\'Failed to log metric to tensorboard with error: {}\'.format(e))\n\n    def on_end(self, state):\n        self.close_writer()\n\n\nclass TensorBoard(AbstractTensorBoard):\n    """"""TensorBoard callback which writes metrics to the given log directory. Requires the TensorboardX library for python.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import TensorBoard\n        >>> import datetime\n        >>> current_time = datetime.now().strftime(\'%b%d_%H-%M-%S\')\n        # Callback that will log to tensorboard under ""(model name)_(current time)""\n        >>> tb = TensorBoard(log_dir=\'./logs\', write_graph=False, comment=current_time)\n        # Trial that will run the callback and log accuracy and loss metrics\n        >>> t = Trial(None, callbacks=[tb], metrics=[\'acc\', \'loss\'])\n\n    Args:\n        log_dir (str): The tensorboard log path for output\n        write_graph (bool): If True, the model graph will be written using the TensorboardX library\n        write_batch_metrics (bool): If True, batch metrics will be written\n        batch_step_size (int): The step size to use when writing batch metrics, make this larger to reduce latency\n        write_epoch_metrics (bool): If True, metrics from the end of the epoch will be written\n        comment (str): Descriptive comment to append to path\n        visdom (bool): If true, log to visdom instead of tensorboard\n        visdom_params (VisdomParams): Visdom parameter settings object, uses default if None\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: PyTorch model\n        - :attr:`torchbearer.state.EPOCH`: State should have the current epoch stored\n        - :attr:`torchbearer.state.X`: State should have the current data stored if a model graph is to be built\n        - :attr:`torchbearer.state.BATCH`: State should have the current batch number stored if logging batch metrics\n        - :attr:`torchbearer.state.TRAIN_STEPS`: State should have the number of training steps stored\n        - :attr:`torchbearer.state.METRICS`: State should have a dictionary of metrics stored\n    """"""\n\n    def __init__(self, log_dir=\'./logs\',\n                 write_graph=True,\n                 write_batch_metrics=False,\n                 batch_step_size=10,\n                 write_epoch_metrics=True,\n                 comment=\'torchbearer\',\n                 visdom=False,\n                 visdom_params=None):\n        super(TensorBoard, self).__init__(log_dir, comment, visdom, visdom_params)\n\n        self.write_graph = write_graph\n        self.write_batch_metrics = write_batch_metrics\n        self.batch_step_size = batch_step_size\n        self.write_epoch_metrics = write_epoch_metrics\n        self.visdom = visdom\n\n        if self.write_graph and not visdom:\n            def handle_graph(state):\n                dummy = torch.rand(state[torchbearer.X].size(), requires_grad=False)\n                model = copy.deepcopy(state[torchbearer.MODEL]).to(\'cpu\')\n                self.writer.add_graph(model, (dummy,))\n                self._handle_graph = lambda _: None\n\n            self._handle_graph = handle_graph\n        else:\n            self._handle_graph = lambda _: None\n\n        self.batch_log_dir = None\n        self.batch_writer = None\n\n    def on_start_epoch(self, state):\n        if self.write_batch_metrics:\n            if self.visdom:\n                self.batch_log_dir = os.path.join(self.log_dir, \'epoch/\')\n            else:\n                self.batch_log_dir = os.path.join(self.log_dir, \'epoch-\' + str(state[torchbearer.EPOCH]))\n            self.batch_writer = self.get_writer(self.batch_log_dir, visdom=self.visdom)\n\n    def on_sample(self, state):\n        self._handle_graph(state)\n\n    def on_step_training(self, state):\n        if self.write_batch_metrics and state[torchbearer.BATCH] % self.batch_step_size == 0:\n            for metric in state[torchbearer.METRICS]:\n                if self.visdom:\n                    self.add_metric(self.batch_writer.add_scalar, metric, state[torchbearer.METRICS][metric],\n                                                 state[torchbearer.EPOCH] * state[torchbearer.TRAIN_STEPS] + state[\n                                                     torchbearer.BATCH], main_tag=\'batch\')\n                else:\n                    self.add_metric(self.batch_writer.add_scalar, \'batch/\' + metric, state[torchbearer.METRICS][metric], state[torchbearer.BATCH])\n\n    def on_step_validation(self, state):\n        if self.write_batch_metrics and state[torchbearer.BATCH] % self.batch_step_size == 0:\n            for metric in state[torchbearer.METRICS]:\n                if self.visdom:\n                    self.add_metric(self.batch_writer.add_scalar, metric, state[torchbearer.METRICS][metric],\n                                                 state[torchbearer.EPOCH] * state[torchbearer.TRAIN_STEPS] + state[\n                                                     torchbearer.BATCH], main_tag=\'batch\')\n                else:\n                    self.add_metric(self.batch_writer.add_scalar, \'batch/\' + metric, state[torchbearer.METRICS][metric], state[torchbearer.BATCH])\n\n    def on_end_epoch(self, state):\n        if self.write_batch_metrics and not self.visdom:\n            self.close_writer(self.batch_log_dir)\n\n        if self.write_epoch_metrics:\n            for metric in state[torchbearer.METRICS]:\n                if self.visdom:\n                    self.add_metric(self.writer.add_scalar, metric, state[torchbearer.METRICS][metric], state[torchbearer.EPOCH],\n                                           main_tag=\'epoch\')\n                else:\n                    self.add_metric(self.writer.add_scalar, \'epoch/\' + metric, state[torchbearer.METRICS][metric], state[torchbearer.EPOCH])\n\n    def on_end(self, state):\n        super(TensorBoard, self).on_end(state)\n        if self.write_batch_metrics and self.visdom:\n            self.close_writer(self.batch_log_dir)\n\n\nclass TensorBoardText(AbstractTensorBoard):\n    """"""TensorBoard callback which writes metrics as text to the given log directory. Requires the TensorboardX library\n    for python.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import TensorBoardText\n        >>> import datetime\n        >>> current_time = datetime.now().strftime(\'%b%d_%H-%M-%S\')\n        # Callback that will log to tensorboard under ""(model name)_(current time)""\n        >>> tb = TensorBoardText(comment=current_time)\n        # Trial that will run the callback and log accuracy and loss metrics as text to tensorboard\n        >>> t = Trial(None, callbacks=[tb], metrics=[\'acc\', \'loss\'])\n\n    Args:\n        log_dir (str): The tensorboard log path for output\n        write_epoch_metrics (bool): If True, metrics from the end of the epoch will be written\n        log_trial_summary (bool): If True logs a string summary of the Trial\n        batch_step_size (int): The step size to use when writing batch metrics, make this larger to reduce latency\n        comment (str): Descriptive comment to append to path\n        visdom (bool): If true, log to visdom instead of tensorboard\n        visdom_params (VisdomParams): Visdom parameter settings object, uses default if None\n\n    State Requirements:\n        - :attr:`torchbearer.state.SELF`: The :attr:`torchbearer.Trial` running this callback\n        - :attr:`torchbearer.state.EPOCH`: State should have the current epoch stored\n        - :attr:`torchbearer.state.BATCH`: State should have the current batch number stored if logging batch metrics\n        - :attr:`torchbearer.state.METRICS`: State should have a dictionary of metrics stored\n    """"""\n\n    def __init__(self, log_dir=\'./logs\',\n                 write_epoch_metrics=True,\n                 write_batch_metrics=False,\n                 log_trial_summary=True,\n                 batch_step_size=100,\n                 comment=\'torchbearer\',\n                 visdom=False,\n                 visdom_params=None):\n        super(TensorBoardText, self).__init__(log_dir, comment, visdom, visdom_params)\n        self.write_epoch_metrics = write_epoch_metrics\n        self.write_batch_metrics = write_batch_metrics\n        self.log_trial_summary = log_trial_summary\n        self.batch_step_size = batch_step_size\n        self.visdom = visdom\n        self.batch_log_dir = None\n        self.batch_writer = None\n        self.logged_summary = False\n\n    @staticmethod\n    def table_formatter(string):\n        table = \'<table><th>Metric</th><th>Value</th>\'\n        string = string.replace(\'{\', \'\').replace(\'}\', \'\').replace(""\'"", """")  # TODO: Replace this with single pass regex\n\n        def cell(string):\n            return \'<td>\' + string + \'</td>\'\n\n        def row(string):\n            return \'<tr>\' + string + \'</tr>\'\n\n        metrics = string.split(\',\')\n        for _, metric in enumerate(metrics):\n            items = metric.split(\':\')\n            name, value = items[0], items[1]\n            table = table + row(cell(name) + cell(value))\n\n        return table + \'</table>\'\n\n    def on_start(self, state):\n        super(TensorBoardText, self).on_start(state)\n        if self.log_trial_summary and not self.logged_summary:\n            self.logged_summary = True\n            self.writer.add_text(\'trial\', str(state[torchbearer.SELF]).replace(\'\\n\', \'\\n \\n\'), 1)\n\n    def on_start_epoch(self, state):\n        if self.write_batch_metrics:\n            if self.visdom:\n                self.batch_log_dir = os.path.join(self.log_dir, \'epoch/\')\n                batch_params = self.visdom_params if self.visdom_params is not None else VisdomParams()\n                batch_params.ENV = batch_params.ENV + \'-batch\'\n                self.batch_writer = self.get_writer(self.batch_log_dir, visdom=self.visdom, visdom_params=batch_params)\n            else:\n                self.batch_log_dir = os.path.join(self.log_dir, \'epoch-\' + str(state[torchbearer.EPOCH]))\n                self.batch_writer = self.get_writer(self.batch_log_dir)\n\n    def on_step_training(self, state):\n        if self.write_batch_metrics and state[torchbearer.BATCH] % self.batch_step_size == 0:\n            if self.visdom:\n                self.batch_writer.add_text(\'batch\', \'<h3>Epoch {} - Batch {}</h3>\'.format(state[torchbearer.EPOCH], state[torchbearer.BATCH])+self.table_formatter(str(state[torchbearer.METRICS])), 1)\n            else:\n                self.batch_writer.add_text(\'batch\', self.table_formatter(str(state[torchbearer.METRICS])), state[torchbearer.BATCH])\n\n    def on_end_epoch(self, state):\n        if self.write_epoch_metrics:\n            if self.visdom:\n                self.writer.add_text(\'epoch\', \'<h4>Epoch {}</h4>\'.format(state[torchbearer.EPOCH])+self.table_formatter(str(state[torchbearer.METRICS])), 1)\n            else:\n                self.writer.add_text(\'epoch\', self.table_formatter(str(state[torchbearer.METRICS])), state[torchbearer.EPOCH])\n\n    def on_end(self, state):\n        super(TensorBoardText, self).on_end(state)\n        if self.write_batch_metrics and self.visdom:\n            self.close_writer(self.batch_log_dir)\n\n\nclass TensorBoardImages(AbstractTensorBoard):\n    """"""The TensorBoardImages callback will write a selection of images from the validation pass to tensorboard using the\n    TensorboardX library and torchvision.utils.make_grid (requires torchvision). Images are selected from the given key and saved to the given\n    path. Full name of image sub directory will be model name + _ + comment.\n\n    Example: ::\n\n        >>> from torchbearer import Trial, state_key\n        >>> from torchbearer.callbacks import TensorBoardImages\n        >>> import datetime\n        >>> current_time = datetime.now().strftime(\'%b%d_%H-%M-%S\')\n        >>> IMAGE_KEY = state_key(\'image_key\')\n\n        >>> # Callback that will log to tensorboard under ""(model name)_(current time)""\n        >>> tb = TensorBoardImages(comment=current_time, name=\'test_image\', key=IMAGE_KEY)\n        >>> # Trial that will run log to tensorboard images stored under IMAGE_KEY\n        >>> t = Trial(None, callbacks=[tb], metrics=[\'acc\', \'loss\'])\n\n    Args:\n        log_dir (str): The tensorboard log path for output\n        comment (str): Descriptive comment to append to path\n        name (str): The name of the image\n        key (StateKey): The key in state containing image data (tensor of size [c, w, h] or [b, c, w, h])\n        write_each_epoch (bool): If True, write data on every epoch, else write only for the first epoch.\n        num_images (int): The number of images to write\n        nrow: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        padding: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        normalize: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        norm_range: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        scale_each: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        pad_value: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        visdom (bool): If true, log to visdom instead of tensorboard\n        visdom_params (VisdomParams): Visdom parameter settings object, uses default if None\n\n    State Requirements:\n        - :attr:`torchbearer.state.EPOCH`: State should have the current epoch stored\n        - `key`: State should have images stored under the given state key\n    """"""\n\n    def __init__(self, log_dir=\'./logs\',\n                 comment=\'torchbearer\',\n                 name=\'Image\',\n                 key=torchbearer.Y_PRED,\n                 write_each_epoch=True,\n                 num_images=16,\n                 nrow=8,\n                 padding=2,\n                 normalize=False,\n                 norm_range=None,\n                 scale_each=False,\n                 pad_value=0,\n                 visdom=False,\n                 visdom_params=None):\n        super(TensorBoardImages, self).__init__(log_dir, comment, visdom=visdom, visdom_params=visdom_params)\n        self.name = name\n        self.key = key\n        self.write_each_epoch = write_each_epoch\n        self.num_images = num_images\n        self.nrow = nrow\n        self.padding = padding\n        self.normalize = normalize\n        self.norm_range = norm_range\n        self.scale_each = scale_each\n        self.pad_value = pad_value\n\n        self._data = None\n        self.done = False\n\n    def on_step_validation(self, state):\n        if not self.done:\n            import torchvision.utils as utils\n            data = state[self.key].clone()\n\n            if len(data.size()) == 3:\n                data = data.unsqueeze(1)\n\n            if self._data is None:\n                remaining = self.num_images if self.num_images < data.size(0) else data.size(0)\n\n                self._data = data[:remaining].to(\'cpu\')\n            else:\n                remaining = self.num_images - self._data.size(0)\n\n                if remaining > data.size(0):\n                    remaining = data.size(0)\n\n                self._data = torch.cat((self._data, data[:remaining].to(\'cpu\')), dim=0)\n\n            if self._data.size(0) >= self.num_images:\n                image = utils.make_grid(\n                    self._data,\n                    nrow=self.nrow,\n                    padding=self.padding,\n                    normalize=self.normalize,\n                    range=self.norm_range,\n                    scale_each=self.scale_each,\n                    pad_value=self.pad_value\n                )\n                if self.visdom:\n                    name = self.name + str(state[torchbearer.EPOCH])\n                else:\n                    name = self.name\n                self.writer.add_image(name, image, state[torchbearer.EPOCH])\n                self.done = True\n                self._data = None\n\n    def on_end_epoch(self, state):\n        if self.write_each_epoch:\n            self.done = False\n\n\nclass TensorBoardProjector(AbstractTensorBoard):\n    """"""The TensorBoardProjector callback is used to write images from the validation pass to Tensorboard using the\n    TensorboardX library. Images are written to the given directory and, if required, so are associated features.\n\n    Args:\n        log_dir (str): The tensorboard log path for output\n        comment (str): Descriptive comment to append to path\n        num_images (int): The number of images to write\n        avg_pool_size (int): Size of the average pool to perform on the image. This is recommended to reduce the overall\n            image sizes and improve latency\n        avg_data_channels (bool): If True, the image data will be averaged in the channel dimension\n        write_data (bool): If True, the raw data will be written as an embedding\n        write_features (bool): If True, the image features will be written as an embedding\n        features_key (StateKey): The key in state to use for the embedding. Typically model output but can be used to show\n            features from any layer of the model.\n\n    State Requirements:\n        - :attr:`torchbearer.state.EPOCH`: State should have the current epoch stored\n        - :attr:`torchbearer.state.X`: State should have the current data stored\n        - :attr:`torchbearer.state.Y_TRUE`: State should have the current targets stored\n    """"""\n\n    def __init__(self, log_dir=\'./logs\',\n                 comment=\'torchbearer\',\n                 num_images=100,\n                 avg_pool_size=1,\n                 avg_data_channels=True,\n                 write_data=True,\n                 write_features=True,\n                 features_key=torchbearer.Y_PRED):\n        super(TensorBoardProjector, self).__init__(log_dir, comment)\n        self.num_images = num_images\n        self.avg_pool_size = avg_pool_size\n        self.avg_data_channels = avg_data_channels\n        self.write_data = write_data\n        self.write_features = write_features\n        self.features_key = features_key\n        self.done = False\n        self._images = None\n        self._labels = None\n        self._features = None\n        self._data = None\n\n    def on_step_validation(self, state):\n        if not self.done:\n            x = state[torchbearer.X].data.clone()\n\n            if len(x.size()) == 3:\n                x = x.unsqueeze(1)\n\n            x = F.avg_pool2d(x, self.avg_pool_size).data\n\n            data = None\n\n            if state[torchbearer.EPOCH] == 0 and self.write_data:\n                if self.avg_data_channels:\n                    data = torch.mean(x, 1)\n                else:\n                    data = x\n\n                data = data.view(data.size(0), -1)\n\n            feature = None\n\n            if self.write_features:\n                feature = state[self.features_key].data.clone()\n                feature = feature.view(feature.size(0), -1)\n\n            label = state[torchbearer.Y_TRUE].data.clone()\n\n            if state[torchbearer.BATCH] == 0:\n                remaining = self.num_images if self.num_images < label.size(0) else label.size(0)\n\n                self._images = x[:remaining].to(\'cpu\')\n                self._labels = label[:remaining].to(\'cpu\')\n\n                if data is not None:\n                    self._data = data[:remaining].to(\'cpu\')\n\n                if feature is not None:\n                    self._features = feature[:remaining].to(\'cpu\')\n            else:\n                remaining = self.num_images - self._labels.size(0)\n\n                if remaining > label.size(0):\n                    remaining = label.size(0)\n\n                self._images = torch.cat((self._images, x[:remaining].to(\'cpu\')), dim=0)\n                self._labels = torch.cat((self._labels, label[:remaining].to(\'cpu\')), dim=0)\n\n                if data is not None:\n                    self._data = torch.cat((self._data, data[:remaining].to(\'cpu\')), dim=0)\n\n                if feature is not None:\n                    self._features = torch.cat((self._features, feature[:remaining].to(\'cpu\')), dim=0)\n\n            if self._labels.size(0) >= self.num_images:\n                if state[torchbearer.EPOCH] == 0 and self.write_data:\n                    self.writer.add_embedding(self._data, metadata=self._labels, label_img=self._images, tag=\'data\',\n                                              global_step=-1)\n                if self.write_features:\n                    self.writer.add_embedding(self._features, metadata=self._labels, label_img=self._images,\n                                              tag=\'features\', global_step=state[torchbearer.EPOCH])\n                self.done = True\n\n    def on_end_epoch(self, state):\n        if self.write_features:\n            self.done = False\n'"
torchbearer/callbacks/terminate_on_nan.py,2,"b'from __future__ import print_function\nimport torchbearer\n\nfrom torchbearer.callbacks import Callback\nfrom torchbearer.bases import get_metric\n\nimport math\n\n\nclass TerminateOnNaN(Callback):\n    """"""Callback which montiors the given metric and halts training if its value is nan or inf.\n\n    Example: ::\n\n        >>> import torch.nn\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import TerminateOnNaN\n\n        # Example Trial which terminates on a NaN, forced by a separate callback. Terminates on the 11th batch since\n        the running loss only updates every 10 iterations.\n        >>> term = TerminateOnNaN(monitor=\'running_loss\')\n        >>> @torchbearer.callbacks.on_criterion\n        ... def force_terminate(state):\n        ...     if state[torchbearer.BATCH] == 5:\n        ...         state[torchbearer.LOSS] = state[torchbearer.LOSS] * torch.Tensor([float(\'NaN\')])\n        >>> trial = Trial(None, callbacks=[term, force_terminate], metrics=[\'loss\'], verbose=2).for_steps(30).run(1)\n        Invalid running_loss, terminating\n\n    Args:\n        monitor (str): The name of the metric to monitor\n\n    State Requirements:\n        - :attr:`torchbearer.state.METRICS`: Metrics should be a dict containing at least the key `monitor`\n    """"""\n    def __init__(self, monitor=\'running_loss\'):\n        super(TerminateOnNaN, self).__init__()\n        self._monitor = monitor\n\n    def _check(self, state):\n        value = get_metric(\'TerminateOnNaN\', state, self._monitor)\n        if value is not None:\n            if math.isnan(value) or math.isinf(value):\n                print(\'Invalid \' + self._monitor + \', terminating\')\n                state[torchbearer.STOP_TRAINING] = True\n\n    def on_step_training(self, state):\n        self._check(state)\n\n    def on_end_epoch(self, state):\n        self._check(state)\n\n    def on_step_validation(self, state):\n        self._check(state)\n'"
torchbearer/callbacks/torch_scheduler.py,15,"b'import torchbearer\nfrom torchbearer.callbacks import Callback\nfrom torchbearer.bases import get_metric\n\nimport torch\n\n\nclass TorchScheduler(Callback):\n    def __init__(self, scheduler_builder, monitor=None, step_on_batch=False):\n        self._scheduler_builder = scheduler_builder\n        self._monitor = monitor\n        self._scheduler = None\n        self._step_on_batch = step_on_batch\n\n    def on_start(self, state):\n        self._scheduler = self._scheduler_builder(state[torchbearer.OPTIMIZER])\n\n    def on_sample(self, state):\n        if self._step_on_batch and self._monitor is None:\n            self._scheduler.step()\n\n    def on_step_training(self, state):\n        if self._step_on_batch:\n            if self._monitor is not None:\n                current = get_metric(\'Scheduler\', state, self._monitor)\n                if current is None:\n                    return\n                self._scheduler.step(current)\n            else:\n                self._scheduler.step()\n\n    def on_end_epoch(self, state):\n        if not self._step_on_batch:\n            if self._monitor is not None:\n                current = get_metric(\'Scheduler\', state, self._monitor)\n                if current is None:\n                    return\n                self._scheduler.step(current, epoch=state[torchbearer.EPOCH])\n            else:\n                self._scheduler.step(epoch=state[torchbearer.EPOCH])\n\n\nclass LambdaLR(TorchScheduler):\n    """"""\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import LambdaLR\n\n        # Example Trial which performs the two learning rate lambdas from the PyTorch docs\n        >>> lambda1 = lambda epoch: epoch // 30\n        >>> lambda2 = lambda epoch: 0.95 ** epoch\n        >>> scheduler = LambdaLR(lr_lambda=[lambda1, lambda2])\n        >>> trial = Trial(None, callbacks=[scheduler], metrics=[\'loss\'], verbose=2).for_steps(10).run(1)\n\n    Args:\n        step_on_batch (bool): If True, step will be called on each training iteration rather than on each epoch\n\n    See:\n        `PyTorch LambdaLR <http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.LambdaLR>`_\n    """"""\n    def __init__(self, lr_lambda, last_epoch=-1, step_on_batch=False):\n        super(LambdaLR, self).__init__(lambda opt:\n                                       torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda, last_epoch=last_epoch),\n                                       step_on_batch=step_on_batch)\n\n\nclass StepLR(TorchScheduler):\n    """"""\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import StepLR\n\n        >>> # Assuming optimizer uses lr = 0.05 for all groups\n        >>> # lr = 0.05     if epoch < 30\n        >>> # lr = 0.005    if 30 <= epoch < 60\n        >>> # lr = 0.0005   if 60 <= epoch < 90\n        >>> scheduler = StepLR(step_size=30, gamma=0.1)\n        >>> trial = Trial(None, callbacks=[scheduler], metrics=[\'loss\'], verbose=2).for_steps(10).run(1)\n\n    Args:\n        step_on_batch (bool): If True, step will be called on each training iteration rather than on each epoch\n\n    See:\n        `PyTorch StepLR <http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.StepLR>`_\n    """"""\n    def __init__(self, step_size, gamma=0.1, last_epoch=-1, step_on_batch=False):\n        super(StepLR, self).__init__(lambda opt:\n                                     torch.optim.lr_scheduler.StepLR(opt, step_size, gamma=gamma,\n                                                                     last_epoch=last_epoch),\n                                     step_on_batch=step_on_batch)\n\n\nclass MultiStepLR(TorchScheduler):\n    """"""\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import MultiStepLR\n\n        >>> # Assuming optimizer uses lr = 0.05 for all groups\n        >>> # lr = 0.05     if epoch < 30\n        >>> # lr = 0.005    if 30 <= epoch < 80\n        >>> # lr = 0.0005   if epoch >= 80\n        >>> scheduler = MultiStepLR(milestones=[30,80], gamma=0.1)\n        >>> trial = Trial(None, callbacks=[scheduler], metrics=[\'loss\'], verbose=2).for_steps(10).run(1)\n\n    Args:\n        step_on_batch (bool): If True, step will be called on each training iteration rather than on each epoch\n\n    See:\n        `PyTorch MultiStepLR <http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.MultiStepLR>`_\n    """"""\n    def __init__(self, milestones, gamma=0.1, last_epoch=-1, step_on_batch=False):\n        super(MultiStepLR, self).__init__(lambda opt:\n                                          torch.optim.lr_scheduler.MultiStepLR(opt, milestones, gamma=gamma,\n                                                                               last_epoch=last_epoch),\n                                          step_on_batch=step_on_batch)\n\n\nclass ExponentialLR(TorchScheduler):\n    """"""\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import ExponentialLR\n\n        >>> # Example scheduler which multiplies the learning rate by 0.1 every epoch\n        >>> scheduler = ExponentialLR(gamma=0.1)\n        >>> trial = Trial(None, callbacks=[scheduler], metrics=[\'loss\'], verbose=2).for_steps(10).run(1)\n\n    Args:\n        step_on_batch (bool): If True, step will be called on each training iteration rather than on each epoch\n\n    See:\n        `PyTorch ExponentialLR <http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.ExponentialLR>`_\n    """"""\n    def __init__(self, gamma, last_epoch=-1, step_on_batch=False):\n        super(ExponentialLR, self).__init__(lambda opt:\n                                            torch.optim.lr_scheduler.ExponentialLR(opt, gamma, last_epoch=last_epoch),\n                                            step_on_batch=step_on_batch)\n\n\nclass CosineAnnealingLR(TorchScheduler):\n    """"""\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import CosineAnnealingLR\n\n        >>> # Example scheduler which uses cosine learning rate annealing - see PyTorch docs\n        >>> scheduler = MultiStepLR(milestones=[30,80], gamma=0.1)\n        >>> trial = Trial(None, callbacks=[scheduler], metrics=[\'loss\'], verbose=2).for_steps(10).run(1)\n\n    Args:\n        step_on_batch (bool): If True, step will be called on each training iteration rather than on each epoch\n\n    See:\n        `PyTorch CosineAnnealingLR <http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR>`_\n    """"""\n    def __init__(self, T_max, eta_min=0, last_epoch=-1, step_on_batch=False):\n        super(CosineAnnealingLR, self).__init__(lambda opt:\n                                                torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max, eta_min=eta_min,\n                                                                                           last_epoch=last_epoch),\n                                                step_on_batch=step_on_batch)\n\n\nclass ReduceLROnPlateau(TorchScheduler):\n    """"""\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import ReduceLROnPlateau\n\n        >>> # Example scheduler which divides the learning rate by 10 on plateaus of 5 epochs without significant\n        >>> # validation loss decrease, in order to stop overshooting the local minima. new_lr = lr * factor\n        >>> scheduler = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=5)\n        >>> trial = Trial(None, callbacks=[scheduler], metrics=[\'loss\'], verbose=2).for_steps(10).for_val_steps(10).run(1)\n\n    Args:\n        monitor (str): The name of the quantity in metrics to monitor. (Default value = \'val_loss\')\n        step_on_batch (bool): If True, step will be called on each training iteration rather than on each epoch\n\n    See:\n        `PyTorch ReduceLROnPlateau <http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau>`_\n    """"""\n    def __init__(self,  monitor=\'val_loss\', mode=\'min\', factor=0.1, patience=10, verbose=False, threshold=1e-4,\n                 threshold_mode=\'rel\', cooldown=0, min_lr=0, eps=1e-8, step_on_batch=False):\n        super(ReduceLROnPlateau, self).__init__(lambda opt:\n                                                torch.optim.lr_scheduler.ReduceLROnPlateau(\n                                                    opt, mode=mode, factor=factor, patience=patience, verbose=verbose,\n                                                    threshold=threshold, threshold_mode=threshold_mode,\n                                                    cooldown=cooldown, min_lr=min_lr, eps=eps), monitor=monitor,\n                                                step_on_batch=step_on_batch)\n\n\nclass CyclicLR(TorchScheduler):\n    """"""\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import CyclicLR\n\n        >>> # Example scheduler which cycles the learning rate between 0.01 and 0.1\n        >>> scheduler = CyclicLR(0.01, 0.1)\n        >>> trial = Trial(None, callbacks=[scheduler], metrics=[\'loss\'], verbose=2).for_steps(10).for_val_steps(10).run(1)\n\n    Args:\n        monitor (str): The name of the quantity in metrics to monitor. (Default value = \'val_loss\')\n        step_on_batch (bool): If True, step will be called on each training iteration rather than on each epoch\n\n    See:\n        `PyTorch ReduceLROnPlateau <http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau>`_\n    """"""\n    def __init__(self,  base_lr, max_lr, monitor=\'val_loss\', step_size_up=2000, step_size_down=None, mode=\'triangular\',\n                 gamma=1., scale_fn=None, scale_mode=\'cycle\', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9,\n                 last_epoch=-1, step_on_batch=False):\n        from distutils.version import LooseVersion\n        version = torch.__version__ if str(torch.__version__) is torch.__version__ else ""0.4.0""\n        if LooseVersion(version) > LooseVersion(""1.0.0""):  # CyclicLR is implemented\n            super(CyclicLR, self).__init__(lambda opt:\n                                                    torch.optim.lr_scheduler.CyclicLR(\n                                                        opt, base_lr, max_lr, step_size_up=step_size_up,\n                                                        step_size_down=step_size_down, mode=mode, gamma=gamma,\n                                                        scale_fn=scale_fn, scale_mode=scale_mode,\n                                                        cycle_momentum=cycle_momentum, base_momentum=base_momentum,\n                                                        max_momentum=max_momentum, last_epoch=last_epoch),\n                                           monitor=monitor, step_on_batch=step_on_batch)\n        else:\n            raise NotImplementedError(\'CyclicLR scheduler was not implemented in PyTorch versions less than 1.1.0. \'\n                                      \'Update PyTorch or use the CyclicLR callback from an older Torchbearer version.\')\n'"
torchbearer/callbacks/unpack_state.py,2,"b'import torchbearer\nfrom torchbearer import Callback\n\n\nclass UnpackState(Callback):\n    """"""Callback that unpacks a number of items from the state dictionary and passes it to the model forward as a separate\n    dictionary (under the same keys).\n\n    Note that torchbearer.X is always passed in the dictionary, if given as a key or not.\n\n    Note that, if output_to_state is set then when the model outputs a dictionary, the main\n    torchbearer state will be updated with this dictionary. So, if model outputs are\n    {torchbearer.Y_PRED: 1, SOME_KEY: 10} then the main torchbearer state will look like:\n    {..., torchbearer.Y_PRED: 1, SOME_KEY: 10, ...} after the forward pass. If Y_PRED is not a key in the model output\n    dict or the output is not a dict then main state will look like: {..., torchbearer.Y_PRED: (model_output), ...}\n\n    This is useful when using a DataParallel model where DataParallel cannot pass the main torchbearer state dictionary\n    directly.\n\n    Example: ::\n        >>> import torch\n        >>> from torch.nn import Module, DataParallel\n        >>> import torchbearer\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import UnpackState\n\n        >>> class TestModel(Module):\n        ...     def forward(self, x):\n        ...         print(x)\n        ...         return x\n\n        >>> A_KEY = torchbearer.state_key(\'a_key\')\n        >>> unpacker = UnpackState(keys=[torchbearer.X, A_KEY])\n        >>> t = Trial(TestModel(), callbacks=[unpacker])\n        >>> t.state[A_KEY] = \'test\'\n        >>> t.with_train_data(torch.ones(10, 1), torch.ones(10, 1), batch_size=2, steps=1).run()\n        {x: tensor([[1.],\n                [1.]]), a_key: \'test\'}\n        [((1, None), {})]\n\n\n    Args:\n        keys (list / tuple): List of keys to unpack from state and pass to the model on forward\n        output_to_state (bool): If True and torchbearer.Y_PRED in model_outputs, main torchbearer state will be updated\n        (state.update(model_outputs)). If False then model outputs will be stored in the main state under\n        torchbeaerer.Y_PRED.\n    """"""\n    def __init__(self, keys=None, output_to_state=True):\n        super(UnpackState, self).__init__()\n        self.output_to_state = output_to_state\n        self.keys = keys if keys is not None else [torchbearer.X]\n        if torchbearer.X not in self.keys:\n            self.keys.insert(0, torchbearer.X)\n\n    def on_sample(self, state):\n        if self.keys != [torchbearer.X]:\n            state[torchbearer.X] = {k: state[k] for k in self.keys}\n\n    def on_sample_validation(self, state):\n        self.on_sample(state)\n\n    def on_forward(self, state):\n        if self.output_to_state and isinstance(state[torchbearer.Y_PRED], dict):\n            outputs = state[torchbearer.Y_PRED]\n            state.update(outputs)\n            state[torchbearer.Y_PRED] = outputs[torchbearer.Y_PRED] if torchbearer.Y_PRED in outputs else outputs\n\n    def on_forward_validation(self, state):\n        self.on_forward(state)\n'"
torchbearer/callbacks/weight_decay.py,1,"b'import torchbearer\n\nfrom torchbearer.callbacks import Callback\n\nimport torch\n\n\nclass WeightDecay(Callback):\n    """"""Create a WeightDecay callback which uses the given norm on the given parameters and with the given decay rate.\n    If params is None (default) then the parameters will be retrieved from the model.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import WeightDecay\n\n        # Example Trial which runs a trial with weight decay on the model\n        >>> decay = WeightDecay()\n        >>> trial = Trial(None, callbacks=[decay], metrics=[\'loss\'], verbose=2).for_steps(10).run(1)\n\n    Args:\n        rate (float): The decay rate or lambda\n        p (int): The norm level\n        params (Iterable[Tensor] or Tensor, optional): an iterable of Tensors or a\n            single Tensor that will have gradients normalized, otherwise this is retrieved from state\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `parameters` method\n        - :attr:`torchbearer.state.LOSS`: Loss should be a tensor that can be incremented\n    """"""\n    def __init__(self, rate=5e-4, p=2, params=None):\n        super(WeightDecay, self).__init__()\n\n        self.p = p\n        self.params = params\n        self.rate = rate\n\n    def on_start(self, state):\n        """"""Retrieve params from state[\'model\'] if required.\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        if self.params is None:\n            self.params = state[torchbearer.MODEL].parameters()\n\n    def on_criterion(self, state):\n        """"""Calculate the decay term and add to state[\'loss\'].\n\n        Args:\n            state (dict): The :class:`.Trial` state\n        """"""\n        for param in self.params:\n            state[torchbearer.LOSS] += self.rate * torch.norm(param, self.p)\n\n\nclass L1WeightDecay(WeightDecay):\n    """"""WeightDecay callback which uses an L1 norm with the given rate and parameters. If params is None (default) then\n    the parameters will be retrieved from the model.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import L1WeightDecay\n\n        # Example Trial which runs a trial with weight decay on the model using an L1 norm\n        >>> decay = L1WeightDecay()\n        >>> trial = Trial(None, callbacks=[decay], metrics=[\'loss\'], verbose=2).for_steps(10).run(1)\n\n    Args:\n        rate (float): The decay rate or lambda\n        params (Iterable[Tensor] or Tensor, optional): an iterable of Tensors or a\n            single Tensor that will have gradients normalized, otherwise this is retrieved from state\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `parameters` method\n        - :attr:`torchbearer.state.LOSS`: Loss should be a tensor that can be incremented\n    """"""\n    def __init__(self, rate=5e-4, params=None):\n        super(L1WeightDecay, self).__init__(rate=rate, p=1, params=params)\n\n\nclass L2WeightDecay(WeightDecay):\n    """"""WeightDecay callback which uses an L2 norm with the given rate and parameters. If params is None (default) then\n    the parameters will be retrieved from the model.\n\n    Example: ::\n\n        >>> from torchbearer import Trial\n        >>> from torchbearer.callbacks import L2WeightDecay\n\n        # Example Trial which runs a trial with weight decay on the model using an L2 norm\n        >>> decay = L2WeightDecay()\n        >>> trial = Trial(None, callbacks=[decay], metrics=[\'loss\'], verbose=2).for_steps(10).run(1)\n\n    Args:\n        rate (float): The decay rate or lambda\n        params (Iterable[Tensor] or Tensor, optional): an iterable of Tensors or a\n            single Tensor that will have gradients normalized, otherwise this is retrieved from state\n\n    State Requirements:\n        - :attr:`torchbearer.state.MODEL`: Model should have the `parameters` method\n        - :attr:`torchbearer.state.LOSS`: Loss should be a tensor that can be incremented\n    """"""\n    def __init__(self, rate=5e-4, params=None):\n        super(L2WeightDecay, self).__init__(rate=rate, p=2, params=params)\n'"
torchbearer/metrics/__init__.py,0,b'from torchbearer import Metric\nfrom .metrics import *\nfrom .wrappers import *\nfrom .aggregators import *\nfrom .decorators import *\nfrom .roc_auc_score import *\nfrom .primitives import *\nfrom .timer import TimerMetric\nfrom .default import DefaultAccuracy\nfrom .lr import LR\n'
torchbearer/metrics/aggregators.py,1,"b'""""""\nAggregators are a special kind of :class:`.Metric` which takes as input, the output from a previous metric or metrics.\nAs a result, via a :class:`.MetricTree`, a series of aggregators can collect statistics such as Mean or Standard\nDeviation without needing to compute the underlying metric multiple times. This can, however, make the aggregators\ncomplex to use. It is therefore typically better to use the :mod:`decorator API<.metrics.decorators>`.\n""""""\nfrom collections import deque\n\nimport torch\n\nfrom torchbearer.bases import Metric\nfrom .metrics import AdvancedMetric\n\n\nclass RunningMetric(AdvancedMetric):\n    """"""A metric which aggregates batches of results and presents a method to periodically process these into a value.\n\n    .. note::\n\n       Running metrics only provide output during training.\n\n    Args:\n        name (str): The name of the metric.\n        batch_size (int): The size of the deque to store of previous results.\n        step_size (int): The number of iterations between aggregations.\n    """"""\n    def __init__(self, name, batch_size=50, step_size=10):\n        super(RunningMetric, self).__init__(name)\n        self._batch_size = batch_size\n        self._step_size = step_size\n        self._cache = deque()\n        self._result = {}\n\n    def _process_train(self, *args):\n        """"""Process the metric for a single train step.\n\n        Args:\n            args: The output of some :class:`.Metric`\n\n        Returns:\n            The metric value.\n        """"""\n        raise NotImplementedError\n\n    def _step(self, cache):\n        """"""Aggregate the cache to produce a single metric value.\n\n        Args:\n            cache (list): The current stored metric cache.\n\n        Returns:\n            The new metric value.\n        """"""\n        raise NotImplementedError\n\n    def process_train(self, *args):\n        """"""Add the current metric value to the cache and call \'_step\' is needed.\n\n        Args:\n            args: The output of some :class:`.Metric`\n\n        Returns:\n            The current metric value.\n        """"""\n        self._cache.append(self._process_train(*args))\n        if len(self._cache) > self._batch_size:\n            self._cache.popleft()\n        if self._i % self._step_size == 0:\n            self._result = self._step(list(self._cache))\n        self._i += 1\n        return self._result\n\n    def reset(self, state):\n        """"""Reset the step counter. Does not clear the cache.\n\n        Args:\n            state (dict): The current model state.\n        """"""\n        self._i = 0\n\n\nclass RunningMean(RunningMetric):\n    """"""A :class:`RunningMetric` which outputs the running mean of its input tensors over the course of an epoch.\n\n    Args:\n        name (str): The name of this running mean.\n        batch_size (int): The size of the deque to store of previous results.\n        step_size (int): The number of iterations between aggregations.\n        dim (int, tuple): The dimension(s) on which to perform the mean. If left as None, this will mean over the whole\n            Tensor\n    """"""\n\n    def __init__(self, name, batch_size=50, step_size=10, dim=None):\n        super(RunningMean, self).__init__(name, batch_size=batch_size, step_size=step_size)\n        self._kwargs = {\'dim\': dim} if dim is not None else {}\n\n    def _process_train(self, *args):\n        data = args[0]\n        res = data.mean(**self._kwargs)\n        return res\n\n    def _step(self, cache):\n        res = sum(cache) / float(len(cache))\n        return res.item() if res.numel() <= 1 else res.tolist()\n\n\nclass Var(Metric):\n    """"""Metric aggregator which calculates the **sample** variance of process outputs between calls to reset.\n    Optionally calculate the population variance if ``unbiased = False``.\n\n    Args:\n        name (str): The name of this metric.\n        unbiased (bool): If True (default), calculates the sample variance, else, the population variance\n        dim (int, tuple): The dimension(s) on which to compute the std. If left as None, this will operate over the\n            whole Tensor\n    """"""\n\n    def __init__(self, name, unbiased=True, dim=None):\n        super(Var, self).__init__(name)\n        self._unbiased = unbiased\n        self._kwargs = {\'dim\': dim} if dim is not None else {}\n        self._sum = 0.0\n        self._sum_sq = 0.0\n        self._count = 0\n\n    def process(self, *args):\n        """"""Compute values required for the variance from the input. The input should be a torch Tensor. The sum and sum\n        of squares will be computed over the provided dimension.\n\n        Args:\n            args (torch.Tensor):  The output of some previous call to :meth:`.Metric.process`.\n        """"""\n        data = args[0]\n        tot = data.sum(**self._kwargs)\n        self._sum += tot\n        self._sum_sq += data.pow(2).sum(**self._kwargs)\n        self._count += data.numel() / tot.numel()\n\n    def _process_final(self):\n        mean = self._sum / self._count\n        mean = mean.pow(2)\n        variance = ((self._sum_sq / self._count) - mean).clamp(min=0)\n        if self._unbiased:\n            variance = variance * (self._count / (self._count - 1.0))\n        return variance\n\n    def process_final(self, *args):\n        """"""Compute and return the final variance.\n\n        Returns:\n            The variance of each observation since the last reset call.\n        """"""\n        res = self._process_final()\n        return res.item() if res.numel() <= 1 else res.tolist()\n\n    def reset(self, state):\n        """"""Reset the statistics to compute the next variance.\n\n        Args:\n            state (dict): The model state.\n        """"""\n        super(Var, self).reset(state)\n        self._sum = 0.0\n        self._sum_sq = 0.0\n        self._count = 0\n\n\nclass Std(Var):\n    """"""Metric aggregator which calculates the **sample** standard deviation of process outputs between calls to reset.\n    Optionally calculate the population std if ``unbiased = False``.\n\n    Args:\n        name (str): The name of this metric.\n        unbiased (bool): If True (default), calculates the sample standard deviation, else, the population standard\n            deviation\n        dim (int, tuple): The dimension(s) on which to compute the std. If left as None, this will operate over the\n            whole Tensor\n    """"""\n\n    def __init__(self, name, unbiased=True, dim=None):\n        super(Std, self).__init__(name, unbiased=unbiased, dim=dim)\n\n    def process_final(self, *args):\n        """"""Compute and return the final standard deviation.\n\n        Returns:\n            The standard deviation of each observation since the last reset call.\n        """"""\n        res = super(Std, self)._process_final().sqrt()\n        return res.item() if res.numel() <= 1 else res.tolist()\n\n\nclass Mean(Metric):\n    """"""Metric aggregator which calculates the mean of process outputs between calls to reset.\n\n    Args:\n        name (str): The name of this metric.\n        dim (int, tuple): The dimension(s) on which to perform the mean. If left as None, this will mean over the whole\n            Tensor\n    """"""\n\n    def __init__(self, name, dim=None):\n        super(Mean, self).__init__(name)\n        self._kwargs = {\'dim\': dim} if dim is not None else {}\n        self._sum = 0.0\n        self._count = 0\n\n    def process(self, *args):\n        """"""Add the input to the rolling sum. Input must be a torch tensor.\n\n        Args:\n            args:  The output of some previous call to :meth:`.Metric.process`.\n        """"""\n        data = args[0]\n        tot = data.sum(**self._kwargs)\n        self._sum += tot\n        self._count += data.numel() / tot.numel()\n\n    def process_final(self, *args):\n        """"""Compute and return the mean of all metric values since the last call to reset.\n\n        Returns:\n            The mean of the metric values since the last call to reset.\n        """"""\n        res = self._sum / self._count\n        return res.item() if res.numel() <= 1 else res.tolist()\n\n    def reset(self, state):\n        """"""Reset the running count and total.\n\n        Args:\n            state (dict): The model state.\n        """"""\n        super(Mean, self).reset(state)\n        self._sum = 0.0\n        self._count = 0\n'"
torchbearer/metrics/decorators.py,12,"b'""""""\nThe decorator API is the core way to interact with metrics in torchbearer. All of the classes and functionality handled\nhere can be reproduced by manually interacting with the classes if necessary. Broadly speaking, the decorator API is\nused to construct a :class:`.MetricFactory` which will build a :class:`.MetricTree` that handles data flow between\ninstances of :class:`.Mean`, :class:`.RunningMean`, :class:`.Std` etc.\n""""""\nimport inspect\n\nfrom torchbearer.metrics import EpochLambda, BatchLambda, ToDict, Mean, MetricTree, Std, Var, RunningMean\nfrom .metrics import add_default\n\n\ndef default_for_key(key, *args, **kwargs):\n    """"""The :func:`default_for_key` decorator will register the given metric in the global metric dict\n    (`metrics.DEFAULT_METRICS`) so that it can be referenced by name in instances of :class:`.MetricList` such as in the\n    list given to the :class:`.torchbearer.Model`.\n\n    Example: ::\n\n        @default_for_key(\'acc\')\n        class CategoricalAccuracy(metrics.BatchLambda):\n            ...\n\n    Args:\n        key (str): The key to use when referencing the metric\n        args: Any args to pass to the underlying metric when constructed\n        kwargs: Any keyword args to pass to the underlying metric when constructed\n    """"""\n    def decorator(arg):\n        add_default(key, arg, *args, **kwargs)\n        return arg\n    return decorator\n\n\ndef lambda_metric(name, on_epoch=False):\n    """"""The :func:`lambda_metric` decorator is used to convert a lambda function `y_pred, y_true` into a :class:`.Metric`\n    instance. This can be used as in the following example: ::\n\n        @metrics.lambda_metric(\'my_metric\')\n        def my_metric(y_pred, y_true):\n            ... # Calculate some metric\n\n        model = Model(metrics=[my_metric])\n\n    Args:\n        name (str): The name of the metric (e.g. \'loss\')\n        on_epoch (bool): If True the metric will be an instance of :class:`.EpochLambda` instead of :class:`.BatchLambda`\n\n    Returns:\n        A decorator which replaces a function with a :class:`.Metric`\n    """"""\n    def decorator(metric_function):\n        if on_epoch:\n            return EpochLambda(name, metric_function)\n        else:\n            return BatchLambda(name, metric_function)\n    return decorator\n\n\ndef to_dict(clazz):\n    """"""The :func:`to_dict` decorator is used to wrap either a :class:`.Metric` class or a :class:`.Metric` instance with\n    a :class:`.ToDict` instance. The result is that future output will be wrapped in a `dict[name, value]`.\n\n    Example: ::\n\n        >>> from torchbearer import metrics\n\n        >>> @metrics.lambda_metric(\'my_metric\')\n        ... def my_metric(y_pred, y_true):\n        ...     return y_pred + y_true\n        ...\n        >>> my_metric.process({\'y_pred\':4, \'y_true\':5})\n        9\n\n        >>> @metrics.to_dict\n        ... @metrics.lambda_metric(\'my_metric\')\n        ... def my_metric(y_pred, y_true):\n        ...     return y_pred + y_true\n        ...\n        >>> my_metric.process({\'y_pred\':4, \'y_true\':5})\n        {\'my_metric\': 9}\n\n    Args:\n        clazz: The class to *decorate*\n\n    Returns:\n        A :class:`.ToDict` instance or a :class:`.ToDict` wrapper of the given class\n    """"""\n    if inspect.isclass(clazz):\n        class Wrapper(ToDict):\n            __doc__ = clazz.__doc__\n\n            def __init__(self, *args, **kwargs):\n                super(Wrapper, self).__init__(clazz(*args, **kwargs))\n        Wrapper.__name__ = clazz.__name__\n        return Wrapper\n    else:\n        return ToDict(clazz)\n\n\ndef _wrap_and_add_to_tree(clazz, child_func):\n    if inspect.isclass(clazz):\n        class Wrapper(MetricTree):\n            __doc__ = clazz.__doc__\n\n            def __init__(self, *args, **kwargs):\n                inner = clazz(*args, **kwargs)\n                if isinstance(inner, MetricTree):\n                    super(Wrapper, self).__init__(inner.root)\n                    self.children = inner.children\n                else:\n                    super(Wrapper, self).__init__(inner)\n\n                self.add_child(child_func(self.root))\n        Wrapper.__name__ = clazz.__name__\n        return Wrapper\n    else:\n        inner = clazz\n        if not isinstance(inner, MetricTree):\n            inner = MetricTree(inner)\n        inner.add_child(child_func(inner))\n        return inner\n\n\ndef mean(clazz=None, dim=None):\n    """"""The :func:`mean` decorator is used to add a :class:`.Mean` to the :class:`.MetricTree` which will will output a\n    mean value at the end of each epoch. At build time, if the inner class is not a :class:`.MetricTree`, one will be\n    created. The :class:`.Mean` will also be wrapped in a :class:`.ToDict` for simplicity.\n\n    Example: ::\n\n        >>> import torch\n        >>> from torchbearer import metrics\n\n        >>> @metrics.mean\n        ... @metrics.lambda_metric(\'my_metric\')\n        ... def metric(y_pred, y_true):\n        ...     return y_pred + y_true\n        ...\n        >>> metric.reset({})\n        >>> metric.process({\'y_pred\':torch.Tensor([2]), \'y_true\':torch.Tensor([2])}) # 4\n        {}\n        >>> metric.process({\'y_pred\':torch.Tensor([3]), \'y_true\':torch.Tensor([3])}) # 6\n        {}\n        >>> metric.process({\'y_pred\':torch.Tensor([4]), \'y_true\':torch.Tensor([4])}) # 8\n        {}\n        >>> metric.process_final()\n        {\'my_metric\': 6.0}\n\n    Args:\n        clazz: The class to *decorate*\n        dim (int, tuple): See :class:`.Mean`\n\n    Returns:\n        A :class:`.MetricTree` with a :class:`.Mean` appended or a wrapper class that extends :class:`.MetricTree`\n    """"""\n    if clazz is None:\n        def decorator(clazz):\n            return mean(clazz, dim=dim)\n        return decorator\n\n    return _wrap_and_add_to_tree(clazz, lambda metric: ToDict(Mean(metric.name, dim=dim)))\n\n\ndef std(clazz=None, unbiased=True, dim=None):\n    """"""The :func:`std` decorator is used to add a :class:`.Std` to the :class:`.MetricTree` which will will output a\n    sample standard deviation value at the end of each epoch. At build time, if the inner class is not a\n    :class:`.MetricTree`, one will be created. The :class:`.Std` will also be wrapped in a :class:`.ToDict` (with \'_std\'\n    appended) for simplicity.\n\n    Example: ::\n\n        >>> import torch\n        >>> from torchbearer import metrics\n\n        >>> @metrics.std\n        ... @metrics.lambda_metric(\'my_metric\')\n        ... def metric(y_pred, y_true):\n        ...     return y_pred + y_true\n        ...\n        >>> metric.reset({})\n        >>> metric.process({\'y_pred\':torch.Tensor([2]), \'y_true\':torch.Tensor([2])}) # 4\n        {}\n        >>> metric.process({\'y_pred\':torch.Tensor([3]), \'y_true\':torch.Tensor([3])}) # 6\n        {}\n        >>> metric.process({\'y_pred\':torch.Tensor([4]), \'y_true\':torch.Tensor([4])}) # 8\n        {}\n        >>> \'%.4f\' % metric.process_final()[\'my_metric_std\']\n        \'2.0000\'\n\n    Args:\n        clazz: The class to *decorate*\n        unbiased (bool): See :class:`.Std`\n        dim (int, tuple): See :class:`.Std`\n\n    Returns:\n        A :class:`.MetricTree` with a :class:`.Std` appended or a wrapper class that extends :class:`.MetricTree`\n    """"""\n    if clazz is None:\n        def decorator(clazz):\n            return std(clazz, unbiased=unbiased, dim=dim)\n        return decorator\n\n    return _wrap_and_add_to_tree(clazz, lambda metric: ToDict(Std(metric.name + \'_std\', unbiased=unbiased, dim=dim)))\n\n\ndef var(clazz=None, unbiased=True, dim=None):\n    """"""The :func:`var` decorator is used to add a :class:`.Var` to the :class:`.MetricTree` which will will output a\n    sample variance value at the end of each epoch. At build time, if the inner class is not a :class:`.MetricTree`, one\n    will be created. The :class:`.Var` will also be wrapped in a :class:`.ToDict` (with \'_var\' appended) for simplicity.\n\n    Example: ::\n\n        >>> import torch\n        >>> from torchbearer import metrics\n\n        >>> @metrics.var\n        ... @metrics.lambda_metric(\'my_metric\')\n        ... def metric(y_pred, y_true):\n        ...     return y_pred + y_true\n        ...\n        >>> metric.reset({})\n        >>> metric.process({\'y_pred\':torch.Tensor([2]), \'y_true\':torch.Tensor([2])}) # 4\n        {}\n        >>> metric.process({\'y_pred\':torch.Tensor([3]), \'y_true\':torch.Tensor([3])}) # 6\n        {}\n        >>> metric.process({\'y_pred\':torch.Tensor([4]), \'y_true\':torch.Tensor([4])}) # 8\n        {}\n        >>> \'%.4f\' % metric.process_final()[\'my_metric_var\']\n        \'4.0000\'\n\n    Args:\n        clazz: The class to *decorate*\n        unbiased (bool): See :class:`.Var`\n        dim (int, tuple): See :class:`.Var`\n\n    Returns:\n        A :class:`.MetricTree` with a :class:`.Var` appended or a wrapper class that extends :class:`.MetricTree`\n    """"""\n    if clazz is None:\n        def decorator(clazz):\n            return var(clazz, unbiased=unbiased, dim=dim)\n        return decorator\n\n    return _wrap_and_add_to_tree(clazz, lambda metric: ToDict(Var(metric.name + \'_var\', unbiased=unbiased, dim=dim)))\n\n\ndef running_mean(clazz=None, batch_size=50, step_size=10, dim=None):\n    """"""The :func:`running_mean` decorator is used to add a :class:`.RunningMean` to the :class:`.MetricTree`. If the\n    inner class is not a :class:`.MetricTree` then one will be created. The :class:`.RunningMean` will be wrapped in a\n    :class:`.ToDict` (with \'running\\_\' prepended to the name) for simplicity.\n\n    .. note::\n        The decorator function does not need to be called if not desired, both: `@running_mean` and `@running_mean()`\n        are acceptable.\n\n    Example: ::\n\n        >>> import torch\n        >>> from torchbearer import metrics\n\n        >>> @metrics.running_mean(step_size=2) # Update every 2 steps\n        ... @metrics.lambda_metric(\'my_metric\')\n        ... def metric(y_pred, y_true):\n        ...     return y_pred + y_true\n        ...\n        >>> metric.reset({})\n        >>> metric.process({\'y_pred\':torch.Tensor([2]), \'y_true\':torch.Tensor([2])}) # 4\n        {\'running_my_metric\': 4.0}\n        >>> metric.process({\'y_pred\':torch.Tensor([3]), \'y_true\':torch.Tensor([3])}) # 6\n        {\'running_my_metric\': 4.0}\n        >>> metric.process({\'y_pred\':torch.Tensor([4]), \'y_true\':torch.Tensor([4])}) # 8, triggers update\n        {\'running_my_metric\': 6.0}\n\n    Args:\n        clazz: The class to *decorate*\n        batch_size (int): See :class:`.RunningMean`\n        step_size (int): See :class:`.RunningMean`\n        dim (int, tuple): See :class:`.RunningMean`\n\n    Returns:\n        decorator or :class:`.MetricTree` instance or wrapper\n    """"""\n    if clazz is None:\n        def decorator(clazz):\n            return running_mean(clazz, batch_size=batch_size, step_size=step_size, dim=dim)\n        return decorator\n\n    return _wrap_and_add_to_tree(clazz, lambda metric: ToDict(RunningMean(\'running_\' + metric.name, batch_size=batch_size, step_size=step_size, dim=dim)))\n'"
torchbearer/metrics/default.py,2,"b'""""""\nBase metrics are the base classes which represent the metrics supplied with torchbearer. They all use the\n:func:`.default_for_key` decorator so that they can be accessed in the call to :class:`.torchbearer.Model` via the\nfollowing strings:\n\n- \'`acc`\' or \'`accuracy`\': The :class:`.DefaultAccuracy` metric\n- \'`binary_acc`\' or \'`binary_accuracy`\': The :class:`.BinaryAccuracy` metric\n- \'`cat_acc`\' or \'`cat_accuracy`\': The :class:`.CategoricalAccuracy` metric\n- \'`top_5_acc`\' or \'`top_5_accuracy`\': The :class:`.TopKCategoricalAccuracy` metric\n- \'`top_10_acc`\' or \'`top_10_accuracy`\': The :class:`.TopKCategoricalAccuracy` metric with k=10\n- \'`mse`\': The :class:`.MeanSquaredError` metric\n- \'`loss`\': The :class:`.Loss` metric\n- \'`epoch`\': The :class:`.Epoch` metric\n- \'`lr`\': The :class:`.LR` metric\n- \'`roc_auc`\' or \'`roc_auc_score`\': The :class:`.RocAucScore` metric\n""""""\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchbearer\nfrom torchbearer.metrics import default_for_key, Metric, CategoricalAccuracy, MeanSquaredError, BinaryAccuracy\n\ntry:\n    __loss_map__ = {\n        # NN\n        nn.CrossEntropyLoss.__name__: CategoricalAccuracy,\n        nn.NLLLoss.__name__: CategoricalAccuracy,\n        nn.MSELoss.__name__: MeanSquaredError,\n        nn.BCELoss.__name__: BinaryAccuracy,\n        nn.BCEWithLogitsLoss.__name__:  BinaryAccuracy,\n        # Functional\n        F.cross_entropy.__name__: CategoricalAccuracy,\n        F.nll_loss.__name__: CategoricalAccuracy,\n        F.mse_loss.__name__: MeanSquaredError,\n        F.binary_cross_entropy.__name__: BinaryAccuracy,\n        F.binary_cross_entropy_with_logits.__name__: BinaryAccuracy\n    }\nexcept AttributeError:  # Thrown when building the docs with mocked pytorch\n    __loss_map__ = {}\n\n\n@default_for_key(\'accuracy\')\n@default_for_key(\'acc\')\nclass DefaultAccuracy(Metric):\n    """"""The default accuracy metric loads in a different accuracy metric depending on the loss function or criterion in\n    use at the start of training. Default for keys: `acc`, `accuracy`. The following bindings are in place for both nn\n    and functional variants:\n\n    - cross entropy loss -> :class:`.CategoricalAccuracy` [DEFAULT]\n    - nll loss -> :class:`.CategoricalAccuracy`\n    - mse loss -> :class:`.MeanSquaredError`\n    - bce loss -> :class:`.BinaryAccuracy`\n    - bce loss with logits -> :class:`.BinaryAccuracy`\n    """"""\n    def __init__(self):\n        super(DefaultAccuracy, self).__init__(\'placeholder\')  # Don\'t set yet, wait for reset\n\n        self.metric = CategoricalAccuracy()  # Default to CategoricalAccuracy\n        self.name = self.metric.name\n        self._loaded = False\n        self._train = True\n\n    def train(self):\n        self._train = True\n        return self.metric.train()\n\n    def eval(self, data_key=None):\n        self._train = False\n        return self.metric.eval(data_key=data_key)\n\n    def process(self, *args):\n        return self.metric.process(*args)\n\n    def process_final(self, *args):\n        return self.metric.process_final(*args)\n\n    def reset(self, state):\n        if not self._loaded:\n            criterion = state[torchbearer.CRITERION]\n\n            name = None\n\n            if hasattr(criterion, \'__name__\'):\n                name = criterion.__name__\n            elif hasattr(criterion, \'__class__\'):\n                name = criterion.__class__.__name__\n\n            if name is not None and name in __loss_map__:\n                self.metric = __loss_map__[name]()\n                self.name = self.metric.name\n                if self._train:\n                    self.metric.train()\n                else:\n                    self.metric.eval(data_key=state[torchbearer.DATA])\n\n            self._loaded = True\n\n        return self.metric.reset(state)\n'"
torchbearer/metrics/lr.py,0,"b'""""""\n    .. autoclass:: LR()\n""""""\nimport torchbearer\nfrom .metrics import AdvancedMetric\nfrom .decorators import default_for_key, to_dict\n\nold_super = super\n\n\ndef super(_, obj):\n    return old_super(obj.__class__, obj)\n\n\ndef _get_lr(optimizer):\n    lrs = []\n    for group in optimizer.param_groups:\n        lrs.append(group[\'lr\'])\n\n    if len(lrs) == 1:\n        return lrs[0]\n    return lrs\n\n\n@default_for_key(\'lr\')\n@to_dict\nclass LR(AdvancedMetric):\n    """"""Returns the learning rate/s from the optimizer group/s. Use this to log the current LR when using decay.\n    Default for key \'lr\'\n\n    State Requirements:\n        - :attr:`torchbearer.state.OPTIMIZER`: The optimizer in state will be used to retrieve the learning rate.\n    """"""\n\n    def __init__(self):\n        super(LR, self).__init__(\'lr\')\n\n    def process_train(self, *args):\n        state = args[0]\n        return _get_lr(state[torchbearer.OPTIMIZER])\n\n    def process_final_train(self, *args):\n        state = args[0]\n        return _get_lr(state[torchbearer.OPTIMIZER])\n'"
torchbearer/metrics/metrics.py,0,"b'import inspect\nfrom torchbearer import Metric, no_grad\n\n__defaults__ = {}\n\n\ndef add_default(key, metric, *args, **kwargs):\n    __defaults__[key] = (metric, args, kwargs)\n\n\ndef get_default(key):\n    metric, args, kwargs = __defaults__[key]\n    if inspect.isclass(metric):\n        metric = metric(*args, **kwargs)\n    return metric\n\n\nclass MetricTree(Metric):\n    """"""A tree structure which has a node :class:`.Metric` and some children. Upon execution, the node is called with the\n    input and its output is passed to each of the children. A dict is updated with the results.\n\n    .. note::\n\n       If the node output is already a dict (i.e. the node is a standalone metric), this is unwrapped before passing the\n       **first** value to the children.\n\n    Args:\n        metric (Metric): The metric to act as the root node of the tree / subtree\n    """"""\n    def __init__(self, metric):\n        super(MetricTree, self).__init__(metric.name)\n        self.root = metric\n        self.children = []\n\n    def __str__(self):\n        return str(self.root)\n\n    def add_child(self, child):\n        """"""Add a child to this node of the tree\n\n        Args:\n            child (Metric): The child to add\n        """"""\n        self.children.append(child)\n\n    def _for_tree(self, function, *args):\n        result = {}\n        node_value = function(self.root, *args)\n        if isinstance(node_value, dict):\n            node_value = next(iter(node_value.values()))\n\n        for subtree in self.children:\n            out = function(subtree, node_value)\n            if out is not None:\n                result.update(out)\n\n        return result\n\n    def process(self, *args):\n        """"""Process this node and then pass the output to each child.\n\n        Returns:\n            A dict containing all results from the children\n        """"""\n        return self._for_tree(lambda metric, *in_args: metric.process(*in_args), *args)\n\n    def process_final(self, *args):\n        """"""Process this node and then pass the output to each child.\n\n        Returns:\n            A dict containing all results from the children\n        """"""\n        return self._for_tree(lambda metric, *in_args: metric.process_final(*in_args), *args)\n\n    def eval(self, data_key=None):\n        self.root.eval(data_key=data_key)\n\n        for subtree in self.children:\n            subtree.eval(data_key=data_key)\n\n    def train(self):\n        self.root.train()\n\n        for subtree in self.children:\n            subtree.train()\n\n    def reset(self, state):\n        self.root.reset(state)\n\n        for subtree in self.children:\n            subtree.reset(state)\n\n\nclass MetricList(Metric):\n    """"""The :class:`MetricList` class is a wrapper for a list of metrics which acts as a single metric and produces a\n    dictionary of outputs.\n\n    Args:\n        metric_list (list): The list of metrics to be wrapped. If the list contains a :class:`MetricList`, this will be\n            unwrapped. Any strings in the list will be retrieved from metrics.DEFAULT_METRICS.\n    """"""\n\n    def __init__(self, metric_list):\n        super(MetricList, self).__init__(\'metric_list\')\n\n        self.metric_list = []\n\n        for metric in metric_list:\n\n            if isinstance(metric, str):\n                metric = get_default(metric)\n\n            if isinstance(metric, MetricList):\n                self.metric_list = self.metric_list + metric.metric_list\n            else:\n                self.metric_list.append(metric)\n\n    def _for_list(self, function):\n        result = {}\n        for metric in self.metric_list:\n            out = function(metric)\n            if out is not None:\n                result.update(out)\n        return result\n\n    @no_grad()\n    def process(self, *args):\n        """"""Process each metric an wrap in a dictionary which maps metric names to values.\n\n        Returns:\n            dict[str,any]: A dictionary which maps metric names to values.\n        """"""\n        return self._for_list(lambda metric: metric.process(*args))\n\n    @no_grad()\n    def process_final(self, *args):\n        """"""Process each metric an wrap in a dictionary which maps metric names to values.\n\n        Returns:\n            dict[str,any]: A dictionary which maps metric names to values.\n\n        """"""\n        return self._for_list(lambda metric: metric.process_final(*args))\n\n    def train(self):\n        """"""Put each metric in train mode.\n        """"""\n        self._for_list(lambda metric: metric.train())\n\n    def eval(self, data_key=None):\n        """"""Put each metric in eval mode\n        """"""\n        self._for_list(lambda metric: metric.eval(data_key=data_key))\n\n    def reset(self, state):\n        """"""Reset each metric with the given state.\n\n        Args:\n            state: The current state dict of the :class:`.Trial`.\n        """"""\n        self._for_list(lambda metric: metric.reset(state))\n\n    def __str__(self):\n        return str([str(m) for m in self.metric_list])\n\n\nclass AdvancedMetric(Metric):\n    """"""The :class:`AdvancedMetric` class is a metric which provides different process methods for training and\n    validation. This enables running metrics which do not output intermediate steps during validation.\n\n    Args:\n        name (str): The name of the metric.\n    """"""\n\n    def __init__(self, name):\n        super(AdvancedMetric, self).__init__(name)\n        self._process = self.process_train\n        self._process_final = self.process_final_train\n\n    def process_train(self, *args):\n        """"""Process the given state and return the metric value for a training iteration.\n\n        Returns:\n            The metric value for a training iteration.\n        """"""\n        pass\n\n    def process_validate(self, *args):\n        """"""Process the given state and return the metric value for a validation iteration.\n\n        Returns:\n            The metric value for a validation iteration.\n        """"""\n        pass\n\n    def process_final_train(self, *args):\n        """"""Process the given state and return the final metric value for a training iteration.\n\n        Returns:\n            The final metric value for a training iteration.\n        """"""\n        pass\n\n    def process_final_validate(self, *args):\n        """"""Process the given state and return the final metric value for a validation iteration.\n\n        Returns:\n            The final metric value for a validation iteration.\n        """"""\n        pass\n\n    def process(self, *args):\n        """"""Depending on the current mode, return the result of either \'process_train\' or \'process_validate\'.\n\n        Returns:\n            The metric value.\n        """"""\n        return self._process(*args)\n\n    def process_final(self, *args):\n        """"""Depending on the current mode, return the result of either \'process_final_train\' or \'process_final_validate\'.\n\n        Returns:\n            The final metric value.\n        """"""\n        return self._process_final(*args)\n\n    def eval(self, data_key=None):\n        """"""Put the metric in eval mode.\n\n        Args:\n            data_key (StateKey): The torchbearer data_key, if used\n        """"""\n        self._process = self.process_validate\n        self._process_final = self.process_final_validate\n\n    def train(self):\n        """"""Put the metric in train mode.\n        """"""\n        self._process = self.process_train\n        self._process_final = self.process_final_train\n'"
torchbearer/metrics/primitives.py,11,"b'""""""\n    .. autoclass:: BinaryAccuracy()\n    .. autoclass:: CategoricalAccuracy(ignore_index=-100)\n    .. autoclass:: TopKCategoricalAccuracy(k=5, ignore_index=-100)\n    .. autoclass:: MeanSquaredError()\n    .. autoclass:: Loss()\n    .. autoclass:: Epoch()\n""""""\nimport torchbearer\nfrom torchbearer import Metric\nfrom .decorators import default_for_key, running_mean, mean, std, to_dict\n\nimport torch\n\nold_super = super\n\n\ndef super(_, obj):\n    return old_super(obj.__class__, obj)\n\n\n@default_for_key(\'binary_accuracy\')\n@default_for_key(\'binary_acc\')\n@running_mean\n@mean\nclass BinaryAccuracy(Metric):\n    """"""Binary accuracy metric. Uses torch.eq to compare predictions to targets. Decorated with a mean and running_mean.\n    Default for key: \'binary_acc\'.\n\n    Args:\n        pred_key (StateKey): The key in state which holds the predicted values\n        target_key (StateKey): The key in state which holds the target values\n        threshold (float): value between 0 and 1 to use as a threshold when binarizing predictions and targets\n    """"""\n\n    def __init__(self, pred_key=torchbearer.Y_PRED, target_key=torchbearer.Y_TRUE, threshold=0.5):\n        super(BinaryAccuracy, self).__init__(\'binary_acc\')\n        self.pred_key = pred_key\n        self.target_key = target_key\n\n        self.threshold = threshold\n\n    def process(self, *args):\n        state = args[0]\n        y_pred = (state[self.pred_key].float() > self.threshold).long()\n        y_true = (state[self.target_key].float() > self.threshold).long()\n\n        return torch.eq(y_pred, y_true).view(-1).float()\n\n\n@default_for_key(\'cat_accuracy\')\n@default_for_key(\'cat_acc\')\n@running_mean\n@mean\nclass CategoricalAccuracy(Metric):\n    """"""Categorical accuracy metric. Uses torch.max to determine predictions and compares to targets. Decorated with a\n    mean, running_mean and std. Default for key: \'cat_acc\'\n\n    Args:\n        pred_key (StateKey): The key in state which holds the predicted values\n        target_key (StateKey): The key in state which holds the target values\n        ignore_index (int): Specifies a target value that is ignored and does not contribute to the metric output.\n            See `<https://pytorch.org/docs/stable/nn.html#crossentropyloss>`_\n    """"""\n\n    def __init__(self, pred_key=torchbearer.Y_PRED, target_key=torchbearer.Y_TRUE, ignore_index=-100):\n        super(Metric, self).__init__(\'acc\')\n        self.pred_key = pred_key\n        self.target_key = target_key\n\n        self.ignore_index = ignore_index\n\n    def process(self, *args):\n        state = args[0]\n        y_pred = state[self.pred_key]\n        y_true = state[self.target_key]\n\n        if len(y_true.shape) == 2:\n            _, y_true = torch.max(y_true, 1)\n\n        mask = y_true.eq(self.ignore_index).eq(0)\n        y_pred = y_pred[mask]\n        y_true = y_true[mask]\n        _, y_pred = torch.max(y_pred, 1)\n        return (y_pred == y_true).float()\n\n\n@default_for_key(\'top_10_accuracy\', k=10)\n@default_for_key(\'top_5_accuracy\')\n@default_for_key(\'top_10_acc\', k=10)\n@default_for_key(\'top_5_acc\')\n@running_mean\n@mean\nclass TopKCategoricalAccuracy(Metric):\n    """"""Top K Categorical accuracy metric. Uses torch.topk to determine the top k predictions and compares to targets.\n    Decorated with a mean, running_mean and std. Default for keys: \'top_5_acc\', \'top_10_acc\'.\n\n    Args:\n        pred_key (StateKey): The key in state which holds the predicted values\n        target_key (StateKey): The key in state which holds the target values\n        ignore_index (int): Specifies a target value that is ignored and does not contribute to the metric output.\n            See `<https://pytorch.org/docs/stable/nn.html#crossentropyloss>`_\n    """"""\n\n    def __init__(self, pred_key=torchbearer.Y_PRED, target_key=torchbearer.Y_TRUE, k=5, ignore_index=-100):\n        super(TopKCategoricalAccuracy, self).__init__(\'top_\' + str(k) + \'_acc\')\n        self.pred_key = pred_key\n        self.target_key = target_key\n\n        self.k = k\n        self.ignore_index = ignore_index\n\n    def process(self, *args):\n        state = args[0]\n        y_pred = state[self.pred_key]\n        y_true = state[self.target_key]\n        mask = y_true.eq(self.ignore_index).eq(0)\n        y_pred = y_pred[mask]\n        y_true = y_true[mask]\n\n        sorted_indices = torch.topk(y_pred, self.k, dim=1)[1]\n        expanded_y = y_true.view(-1, 1).expand(-1, self.k)\n        return torch.sum(torch.eq(sorted_indices, expanded_y), dim=1).float()\n\n\n@default_for_key(\'mse\')\n@running_mean\n@mean\nclass MeanSquaredError(Metric):\n    """"""Mean squared error metric. Computes the pixelwise squared error which is then averaged with decorators.\n    Decorated with a mean and running_mean. Default for key: \'mse\'.\n\n    Args:\n        pred_key (StateKey): The key in state which holds the predicted values\n        target_key (StateKey): The key in state which holds the target values\n    """"""\n\n    def __init__(self, pred_key=torchbearer.Y_PRED, target_key=torchbearer.Y_TRUE):\n        super(MeanSquaredError, self).__init__(\'mse\')\n        self.pred_key = pred_key\n        self.target_key = target_key\n\n    def process(self, *args):\n        state = args[0]\n        y_pred = state[self.pred_key]\n        y_true = state[self.target_key]\n        return torch.pow(y_pred - y_true.view_as(y_pred), 2).data\n\n\n@default_for_key(\'loss\')\n@running_mean\n@mean\nclass Loss(Metric):\n    """"""Simply returns the \'loss\' value from the model state. Decorated with a mean, running_mean and std. Default for\n    key: \'loss\'.\n\n    State Requirements:\n        - :attr:`torchbearer.state.LOSS`: This key should map to the loss for the current batch\n    """"""\n\n    def __init__(self):\n        super(Loss, self).__init__(\'loss\')\n\n    def process(self, *args):\n        state = args[0]\n        return state[torchbearer.LOSS]\n\n\n@default_for_key(\'epoch\')\n@to_dict\nclass Epoch(Metric):\n    """"""Returns the \'epoch\' from the model state. Default for key: \'epoch\'.\n\n    State Requirements:\n        - :attr:`torchbearer.state.EPOCH`: This key should map to the number of the current epoch\n    """"""\n\n    def __init__(self):\n        super(Epoch, self).__init__(\'epoch\')\n\n    def process_final(self, *args):\n        state = args[0]\n        return self._process(state)\n\n    def process(self, *args):\n        state = args[0]\n        return self._process(state)\n\n    def _process(self, state):\n        return state[torchbearer.EPOCH]\n'"
torchbearer/metrics/roc_auc_score.py,0,"b'""""""\n    .. autoclass:: RocAucScore(one_hot_labels=True, one_hot_offset=0, one_hot_classes=10)\n""""""\nfrom .decorators import default_for_key, to_dict\nfrom .wrappers import EpochLambda\n\nold_super = super\n\n\ndef super(_, obj):\n    return old_super(obj.__class__, obj)\n\n\n@default_for_key(\'roc_auc\')\n@default_for_key(\'roc_auc_score\')\n@to_dict\nclass RocAucScore(EpochLambda):\n    """"""Area Under ROC curve metric. Default for keys: \'roc_auc\', \'roc_auc_score\'.\n\n    .. note::\n\n        Requires :mod:`sklearn.metrics`.\n\n    Args:\n        one_hot_labels (bool): If True, convert the labels to a one hot encoding. Required if they are not already.\n        one_hot_offset (int): Subtracted from class labels, use if not already zero based.\n        one_hot_classes (int): Number of classes for the one hot encoding.\n    """"""\n\n    def __init__(self, one_hot_labels=True, one_hot_offset=0, one_hot_classes=10):\n        import sklearn.metrics\n        import numpy as np\n\n        def to_categorical(y):\n            return np.eye(one_hot_classes, dtype=\'uint8\')[y - one_hot_offset]\n\n        if one_hot_labels:\n            process = to_categorical\n        else:\n            process = lambda y: y\n\n        super(RocAucScore, self).__init__(\'roc_auc_score\', lambda y_pred, y_true: sklearn.metrics.roc_auc_score(process(y_true.cpu().numpy()), y_pred.cpu().detach().numpy()))\n'"
torchbearer/metrics/timer.py,0,"b'from __future__ import print_function\nimport sys\nif sys.version_info[0] < 3:\n    old_super = super\n\n    def super(_, obj):\n        return old_super(obj.__class__, obj)\n\nimport time\nfrom torchbearer.callbacks import Callback\nimport torchbearer\nfrom torchbearer.metrics import Metric\n\nON_START_TRAINING = \'on_start_training\'\nON_START_EPOCH = \'on_start_epoch\'\nON_SAMPLE = \'on_sample\'\nON_FORWARD = \'on_forward\'\nON_CRITERION = \'on_criterion\'\nON_BACKWARD = \'on_backward\'\nON_STEP_TRAINING = \'on_step_training\'\nON_START_VALIDATION = \'on_start_validation\'\nON_SAMPLE_VALIDATION = \'on_sample_validation\'\nON_FORWARD_VALIDATION = \'on_forward_validaiton\'\nON_CRITERION_VALIDATION = \'on_criterion_validation\'\nON_STEP_VALIDATION = \'on_step_validation\'\nTRAIN_TIME = \'train_time\'\nTOTAL_TIME = \'total_time\'\nVALIDATION_TIME = \'validation_time\'\n\n\nclass TimerMetric(Callback, Metric):\n    def __init__(self, time_keys=()):\n        """""" Timer callback that aggregates timings for each stage of model execution\n        """"""\n        super(TimerMetric, self).__init__(name=\'timer\')\n        self.t0 = time.time()\n        self.time_dict = {}\n        # self.init_keys()\n        self.batch_timer = _TimerMetric(\'t_batch\')\n        self.epoch_timer = _TimerMetric(\'t_epoch\')\n        self.train_timer = _TimerMetric(\'t_train\')\n        self.valid_timer = _TimerMetric(\'t_valid\')\n        self.total_timer = _TimerMetric(\'t_total\')\n        self.time_keys = time_keys\n        self.added_callback = False\n\n    def update_time(self, text, metric, state):\n        self.time_dict[text] = metric.process(state)\n        state[torchbearer.TIMINGS] = self.time_dict\n\n    def process(self, *args):\n        super(TimerMetric, self).process(*args)\n        d_out = {key: self.time_dict[key] for key in self.time_keys if key in self.time_dict}\n        return d_out\n\n    def reset(self, state):\n        super(TimerMetric, self).reset(state)\n        if not self.added_callback:\n            state[torchbearer.CALLBACK_LIST].append([self])\n            self.added_callback = True\n\n    def on_start(self, state):\n        self.t0 = time.time()\n        self.batch_timer.reset(state)\n        self.epoch_timer.reset(state)\n        self.train_timer.reset(state)\n        self.valid_timer.reset(state)\n        self.total_timer.reset(state)\n\n    def on_start_training(self, state):\n        super(TimerMetric, self).on_start_training(state)\n        self.update_time(ON_START_TRAINING, self.batch_timer, state)\n        self.update_time(ON_START_TRAINING, self.train_timer, state)\n\n    def on_start_epoch(self, state):\n        super(TimerMetric, self).on_start_epoch(state)\n        self.update_time(ON_START_EPOCH, self.epoch_timer, state)\n\n    def on_sample(self, state):\n        super(TimerMetric, self).on_sample(state)\n        self.update_time(ON_SAMPLE, self.batch_timer, state)\n\n    def on_forward(self, state):\n        super(TimerMetric, self).on_forward(state)\n        self.update_time(ON_FORWARD, self.batch_timer, state)\n\n    def on_criterion(self, state):\n        super(TimerMetric, self).on_criterion(state)\n        self.update_time(ON_CRITERION, self.batch_timer, state)\n\n    def on_backward(self, state):\n        super(TimerMetric, self).on_backward(state)\n        self.update_time(ON_BACKWARD, self.batch_timer, state)\n\n    def on_step_training(self, state):\n        super(TimerMetric, self).on_step_training(state)\n        self.update_time(ON_STEP_TRAINING, self.batch_timer, state)\n\n    def on_start_validation(self, state):\n        super(TimerMetric, self).on_start_validation(state)\n        self.update_time(ON_START_VALIDATION, self.batch_timer, state)\n\n    def on_sample_validation(self, state):\n        super(TimerMetric, self).on_sample_validation(state)\n        self.update_time(ON_SAMPLE_VALIDATION, self.batch_timer, state)\n\n    def on_forward_validation(self, state):\n        super(TimerMetric, self).on_forward_validation(state)\n        self.update_time(ON_FORWARD_VALIDATION, self.batch_timer, state)\n\n    def on_criterion_validation(self, state):\n        super(TimerMetric, self).on_criterion_validation(state)\n        self.update_time(ON_CRITERION_VALIDATION, self.batch_timer, state)\n\n    def on_step_validation(self, state):\n        super(TimerMetric, self).on_step_validation(state)\n        self.update_time(ON_STEP_VALIDATION, self.batch_timer, state)\n\n    def on_end_training(self, state):\n        super(TimerMetric, self).on_end_training(state)\n        self.valid_timer.reset(state)\n        self.batch_timer.reset(state)\n        self.update_time(TRAIN_TIME, self.train_timer, state)\n\n    def on_end_epoch(self, state):\n        super(TimerMetric, self).on_end_epoch(state)\n        self.batch_timer.reset(state)\n        self.train_timer.reset(state)\n\n    def on_end(self, state):\n        super(TimerMetric, self).on_end(state)\n        self.update_time(TOTAL_TIME, self.total_timer, state)\n        print(str(self.time_dict))\n\n    def on_end_validation(self, state):\n        super(TimerMetric, self).on_end_validation(state)\n        self.update_time(VALIDATION_TIME, self.valid_timer, state)\n\n    def get_timings(self):\n        return self.time_dict\n\n\nclass _TimerMetric(Metric):\n    def __init__(self, name):\n        super(_TimerMetric, self).__init__(name)\n        self.t = time.time()\n\n    def process(self, *args):\n        super(_TimerMetric, self).process(*args)\n        dt = time.time() - self.t\n        self.t = time.time()\n        return dt\n\n    def reset(self, state):\n        super(_TimerMetric, self).reset(state)\n        self.t = time.time()\n'"
torchbearer/metrics/wrappers.py,4,"b'""""""\nMetric wrappers are classes which wrap instances of :class:`.Metric` or, in the case of :class:`EpochLambda` and\n:class:`BatchLambda`, functions. Typically, these should **not** be used directly (although this is entirely possible),\nbut via the :mod:`decorator API<.metrics.decorators>`.\n""""""\nimport torchbearer\nfrom torchbearer.bases import Metric\nfrom .metrics import AdvancedMetric\n\nimport torch\n\n\nclass ToDict(AdvancedMetric):\n    """"""The :class:`ToDict` class is an :class:`.AdvancedMetric` which will put output from the inner :class:`.Metric` in\n    a dict (mapping metric name to value) before returning. When in `eval` mode, \'val\\_\' will be prepended to the metric\n    name.\n\n    Example: ::\n\n        >>> from torchbearer import metrics\n\n        >>> @metrics.lambda_metric(\'my_metric\')\n        ... def my_metric(y_pred, y_true):\n        ...     return y_pred + y_true\n        ...\n        >>> metric = metrics.ToDict(my_metric().build())\n        >>> metric.process({\'y_pred\': 4, \'y_true\': 5})\n        {\'my_metric\': 9}\n        >>> metric.eval()\n        >>> metric.process({\'y_pred\': 4, \'y_true\': 5})\n        {\'val_my_metric\': 9}\n\n    Args:\n        metric (Metric): The :class:`.Metric` instance to *wrap*.\n    """"""\n\n    def __init__(self, metric):\n        super(ToDict, self).__init__(metric.name)\n\n        self.eval_flag = \'val\'\n        self.metric = metric\n\n    def process_train(self, *args):\n        val = self.metric.process(*args)\n        if val is not None:\n            return {self.metric.name: val}\n\n    def process_validate(self, *args):\n        val = self.metric.process(*args)\n        if val is not None:\n            return {self.eval_flag + \'_\' + self.metric.name: val}\n\n    def process_final_train(self, *args):\n        val = self.metric.process_final(*args)\n        if val is not None:\n            return {self.metric.name: val}\n\n    def process_final_validate(self, *args):\n        val = self.metric.process_final(*args)\n        if val is not None:\n            return {self.eval_flag + \'_\' + self.metric.name: val}\n\n    def eval(self, data_key=None):\n        super(ToDict, self).eval(data_key=data_key)\n        if data_key == torchbearer.TEST_DATA:\n            self.eval_flag = \'test\'\n        elif data_key == torchbearer.TRAIN_DATA:\n            self.eval_flag = \'train\'\n        else:\n            self.eval_flag = \'val\'\n        self.metric.eval(data_key=data_key)\n\n    def train(self):\n        super(ToDict, self).train()\n        self.metric.train()\n\n    def reset(self, state):\n        super(ToDict, self).reset(state)\n        self.metric.reset(state)\n\n\nclass BatchLambda(Metric):\n    """"""A metric which returns the output of the given function on each batch.\n\n    Args:\n        name (str): The name of the metric.\n        metric_function (func): A metric function(\'y_pred\', \'y_true\') to wrap.\n    """"""\n\n    def __init__(self, name, metric_function):\n        super(BatchLambda, self).__init__(name)\n        self._metric_function = metric_function\n\n    def process(self, *args):\n        """"""Return the output of the wrapped function.\n\n        Args:\n            args: The :class:`.torchbearer.Trial` state.\n\n        Returns:\n            The value of the metric function(\'y_pred\', \'y_true\').\n        """"""\n        state = args[0]\n        return self._metric_function(state[torchbearer.Y_PRED], state[torchbearer.Y_TRUE])\n\n\nclass EpochLambda(AdvancedMetric):\n    """"""A metric wrapper which computes the given function for concatenated values of \'y_true\' and \'y_pred\' each epoch.\n    Can be used as a running metric which computes the function for batches of outputs with a given step size during\n    training.\n\n    Args:\n        name (str): The name of the metric.\n        metric_function (func): The function(\'y_pred\', \'y_true\') to use as the metric.\n        running (bool): True if this should act as a running metric.\n        step_size (int): Step size to use between calls if running=True.\n    """"""\n\n    def __init__(self, name, metric_function, running=True, step_size=50):\n        super(EpochLambda, self).__init__(name)\n        self._step = metric_function\n        self._final = metric_function\n        self._step_size = step_size\n        self._result = 0.0\n\n        if not running:\n            self._step = lambda y_pred, y_true: None\n\n    def process_train(self, *args):\n        """"""Concatenate the \'y_true\' and \'y_pred\' from the state along the 0 dimension, this must be the batch dimension.\n        If this is a running metric, evaluates the function every number of steps.\n\n        Args:\n            args: The :class:`.torchbearer.Trial` state.\n\n        Returns:\n            The current running result.\n        """"""\n        state = args[0]\n        if (self._y_pred is None) or (self._y_true is None):\n            self._y_true = state[torchbearer.Y_TRUE]\n            self._y_pred = state[torchbearer.Y_PRED]\n        else:\n            self._y_true = torch.cat((self._y_true, state[torchbearer.Y_TRUE]), dim=0)\n            self._y_pred = torch.cat((self._y_pred, state[torchbearer.Y_PRED]), dim=0)\n        if state[torchbearer.BATCH] % self._step_size == 0:\n            self._result = self._step(self._y_pred, self._y_true)\n        return self._result\n\n    def process_final_train(self, *args):\n        """"""Evaluate the function with the aggregated outputs.\n\n        Returns:\n            The result of the function.\n        """"""\n        return self._final(self._y_pred, self._y_true)\n\n    def process_validate(self, *args):\n        """"""During validation, just concatenate \'y_true\' and y_pred\'.\n\n        Args:\n            args: The :class:`.torchbearer.Trial` state.\n        """"""\n        state = args[0]\n        if (self._y_pred is None) or (self._y_true is None):\n            self._y_true = state[torchbearer.Y_TRUE]\n            self._y_pred = state[torchbearer.Y_PRED]\n        else:\n            self._y_true = torch.cat((self._y_true, state[torchbearer.Y_TRUE]), dim=0)\n            self._y_pred = torch.cat((self._y_pred, state[torchbearer.Y_PRED]), dim=0)\n\n    def process_final_validate(self, *args):\n        """"""Evaluate the function with the aggregated outputs.\n\n        Returns:\n            The result of the function.\n        """"""\n        return self._final(self._y_pred, self._y_true)\n\n    def reset(self, state):\n        """"""Reset the \'y_true\' and \'y_pred\' caches.\n\n        Args:\n            state (dict): The :class:`.torchbearer.Trial` state.\n        """"""\n        super(EpochLambda, self).reset(state)\n        self._y_true = None\n        self._y_pred = None\n'"
docs/_static/examples/distributed_data_parallel.py,10,"b'import os\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\nimport sys\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torchbearer\nimport platform\nfrom torchvision import datasets, transforms\nimport argparse\n\n\nparser = argparse.ArgumentParser(description=\'Torchbearer Distributed Data Parallel MNIST\')\nparser.add_argument(\'--master-addr\', \'--master\', \'--host\', \'-m\', dest=\'master\', help=\'Address of master node\')\nparser.add_argument(\'--rank\', \'-r\', dest=\'rank\', help=\'Rank of this process\')\nparser.add_argument(\'--world-size\', dest=\'world_size\', default=2, help=\'World size\')\nargs = parser.parse_args()\n\n\ndef setup():\n    os.environ[\'MASTER_ADDR\'] = args.master\n    os.environ[\'MASTER_PORT\'] = \'29500\'\n\n    # initialize the process group\n    dist.init_process_group(""gloo"", rank=args.rank, world_size=args.world_size)\n\n    # Explicitly setting seed makes sure that models created in two processes\n    # start from same random weights and biases. Alternatively, sync models\n    # on start with the callback below.\n    #torch.manual_seed(42)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(784, 100)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(100, 10)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef sync_model(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.data, op=dist.ReduceOp.SUM)\n        param.data /= size\n\n\ndef average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n        param.grad.data /= size\n\n\n@torchbearer.callbacks.on_init\ndef sync(state):\n    sync_model(state[torchbearer.MODEL])\n\n\n@torchbearer.callbacks.on_backward\ndef grad(state):\n    average_gradients(state[torchbearer.MODEL])\n\n\n@torchbearer.callbacks.on_sample\ndef flatten(state):\n    state[torchbearer.X] = state[torchbearer.X].view(state[torchbearer.X].shape[0], -1)\n\n\ndef worker():\n    setup()\n    print(""Rank and node: {}-{}"".format(args.rank, platform.node()))\n\n    model = ToyModel().to(\'cpu\')\n    ddp_model = DDP(model)\n\n    kwargs = {}\n\n    ds = datasets.MNIST(\'./data/mnist/\', train=True, download=True,\n         transform=transforms.Compose([\n             transforms.ToTensor(),\n             transforms.Normalize((0.1307,), (0.3081,))\n          ]))\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(ds)\n    train_loader = torch.utils.data.DataLoader(ds,\n        batch_size=128, sampler=train_sampler, **kwargs)\n\n    test_ds = datasets.MNIST(\'./data/mnist\', train=False,\n              transform=transforms.Compose([\n                 transforms.ToTensor(),\n                 transforms.Normalize((0.1307,), (0.3081,))\n                 ]))\n    test_sampler = torch.utils.data.distributed.DistributedSampler(test_ds)\n    test_loader = torch.utils.data.DataLoader(test_ds,\n        batch_size=128, sampler=test_sampler,  **kwargs)\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    trial = torchbearer.Trial(ddp_model, optimizer, loss_fn, metrics=[\'loss\', \'acc\'],\n        callbacks=[sync, grad, flatten])\n    trial.with_train_generator(train_loader)\n    trial.run(10, verbose=2)\n\n    print(""Model hash: {}"".format(hash(model)))\n    print(\'First parameter: {}\'.format(next(model.parameters())))\n\n    cleanup()\n\n\nif __name__ == ""__main__"":\n    worker()\n    print(\'done\')\n'"
docs/_static/examples/tensorboard.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms\n\nfrom torchbearer.cv_utils import DatasetValidationSplitter\n\nBATCH_SIZE = 128\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\ndataset = torchvision.datasets.CIFAR10(root='./data/cifar', train=True, download=True,\n                                        transform=transforms.Compose([transforms.ToTensor(), normalize]))\nsplitter = DatasetValidationSplitter(len(dataset), 0.1)\ntrainset = splitter.get_train_dataset(dataset)\nvalset = splitter.get_val_dataset(dataset)\n\ntraingen = torch.utils.data.DataLoader(trainset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=10)\nvalgen = torch.utils.data.DataLoader(valset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=10)\n\n\ntestset = torchvision.datasets.CIFAR10(root='./data/cifar', train=False, download=True,\n                                       transform=transforms.Compose([transforms.ToTensor(), normalize]))\ntestgen = torch.utils.data.DataLoader(testset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=False, num_workers=10)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.convs = nn.Sequential(\n            nn.Conv2d(3, 16, stride=2, kernel_size=3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, stride=2, kernel_size=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, stride=2, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n\n        self.classifier = nn.Linear(576, 10)\n\n    def forward(self, x):\n        x = self.convs(x)\n        x = x.view(-1, 576)\n        return self.classifier(x)\n\n\nmodel = SimpleModel()\n\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\nloss = nn.CrossEntropyLoss()\n\nfrom torchbearer import Trial\nfrom torchbearer.callbacks import TensorBoard\n\ntorchbearer_trial = Trial(model, optimizer, loss, metrics=['acc', 'loss'], callbacks=[TensorBoard(write_graph=True, write_batch_metrics=False, write_epoch_metrics=False)]).to('cuda')\ntorchbearer_trial.with_generators(train_generator=traingen, val_generator=valgen)\ntorchbearer_trial.run(epochs=1)\n\ntorchbearer_trial = Trial(model, optimizer, loss, metrics=['acc', 'loss'], callbacks=[TensorBoard(write_graph=False, write_batch_metrics=True, batch_step_size=10, write_epoch_metrics=False)]).to('cuda')\ntorchbearer_trial.with_generators(train_generator=traingen, val_generator=valgen)\ntorchbearer_trial.run(epochs=10)\n\ntorchbearer_trial = Trial(model, optimizer, loss, metrics=['acc', 'loss'],  callbacks=[TensorBoard(write_graph=False, write_batch_metrics=False, write_epoch_metrics=True)]).to('cuda')\ntorchbearer_trial.with_generators(train_generator=traingen, val_generator=valgen)\ntorchbearer_trial.run(epochs=10)\n\n"""
docs/_static/examples/visdom_note.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms\n\nfrom torchbearer.cv_utils import DatasetValidationSplitter\n\nBATCH_SIZE = 128\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\ndataset = torchvision.datasets.CIFAR10(root='./data/cifar', train=True, download=True,\n                                        transform=transforms.Compose([transforms.ToTensor(), normalize]))\nsplitter = DatasetValidationSplitter(len(dataset), 0.1)\ntrainset = splitter.get_train_dataset(dataset)\nvalset = splitter.get_val_dataset(dataset)\n\ntraingen = torch.utils.data.DataLoader(trainset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=10)\nvalgen = torch.utils.data.DataLoader(valset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=10)\n\n\ntestset = torchvision.datasets.CIFAR10(root='./data/cifar', train=False, download=True,\n                                       transform=transforms.Compose([transforms.ToTensor(), normalize]))\ntestgen = torch.utils.data.DataLoader(testset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=False, num_workers=10)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.convs = nn.Sequential(\n            nn.Conv2d(3, 16, stride=2, kernel_size=3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, stride=2, kernel_size=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, stride=2, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n\n        self.classifier = nn.Linear(576, 10)\n\n    def forward(self, x):\n        x = self.convs(x)\n        x = x.view(-1, 576)\n        return self.classifier(x)\n\n\nmodel = SimpleModel()\n\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\nloss = nn.CrossEntropyLoss()\n\nfrom torchbearer import Trial\nfrom torchbearer.callbacks import TensorBoard\n\ntorchbearer_trial = Trial(model, optimizer, loss, metrics=['acc', 'loss'], callbacks=[TensorBoard(visdom=True, write_graph=True, write_batch_metrics=True, batch_step_size=10, write_epoch_metrics=True)]).to('cuda')\ntorchbearer_trial.with_generators(train_generator=traingen, val_generator=valgen)\ntorchbearer_trial.run(epochs=5)\n\nimport torchbearer.callbacks.tensor_board as tensorboard\n\ntensorboard.VisdomParams.ENV = 'Test'\ntorchbearer_trial = Trial(model, optimizer, loss, metrics=['acc', 'loss'], callbacks=[TensorBoard(visdom=True, write_graph=False, write_batch_metrics=True, batch_step_size=10, write_epoch_metrics=False)]).to('cuda')\ntorchbearer_trial.with_generators(train_generator=traingen, val_generator=valgen)\ntorchbearer_trial.run(epochs=5)\n"""
tests/callbacks/imaging/__init__.py,0,b''
tests/callbacks/imaging/test_imaging.py,6,"b""from unittest import TestCase\n\nfrom mock import MagicMock, patch, ANY\n\nimport torch\n\nimport torchbearer\nimport torchbearer.callbacks.imaging as imaging\n\nimport matplotlib.pyplot as plt  # Import so that it can be mocked\nplt.ioff()\n\n\nclass TestHandlers(TestCase):\n    @patch('PIL.Image.fromarray')\n    def test_to_file(self, pil):\n        handler = imaging.imaging._to_file('test')\n        mock = MagicMock()\n        handler(mock, 0, '')\n        mock.mul.assert_called_once_with(255)\n        mock.mul().clamp.assert_called_once_with(0, 255)\n        self.assertTrue(mock.mul().clamp().byte.call_count == 1)\n        mock.mul().clamp().byte().permute.assert_called_once_with(1, 2, 0)\n        self.assertTrue(mock.mul().clamp().byte().permute().cpu.call_count == 1)\n        self.assertTrue(mock.mul().clamp().byte().permute().cpu().numpy.call_count == 1)\n\n        pil.assert_called_once_with(mock.mul().clamp().byte().permute().cpu().numpy())\n        pil().save.assert_called_once_with('test')\n\n    @patch('matplotlib.pyplot')\n    def test_to_pyplot(self, plt):\n        handler = imaging.imaging._to_pyplot(title='test')\n        mock = MagicMock()\n        handler(mock, 0, '')\n        mock.mul.assert_called_once_with(255)\n        mock.mul().clamp.assert_called_once_with(0, 255)\n        self.assertTrue(mock.mul().clamp().byte.call_count == 1)\n        mock.mul().clamp().byte().permute.assert_called_once_with(1, 2, 0)\n        self.assertTrue(mock.mul().clamp().byte().permute().cpu.call_count == 1)\n        self.assertTrue(mock.mul().clamp().byte().permute().cpu().numpy.call_count == 1)\n\n        plt.title.assert_called_once_with('test')\n\n        plt.imshow.assert_called_once_with(mock.mul().clamp().byte().permute().cpu().numpy())\n        plt.axis.assert_called_once_with('off')\n        self.assertTrue(plt.show.call_count == 1)\n\n    @patch('torchbearer.callbacks.tensor_board')\n    def test_to_tensorboard(self, tboard):\n        handler = imaging.imaging._to_tensorboard('test', log_dir='./logs', comment='comment')\n        image = MagicMock()\n        handler(image, 0, {torchbearer.EPOCH: 1})\n        image.clamp.assert_called_once_with(0, 1)\n        tboard.get_writer.assert_called_once_with('./logs/comment', imaging.imaging._to_tensorboard)\n        tboard.get_writer().add_image.assert_called_once_with('test', image.clamp(), 1)\n        tboard.close_writer.assert_called_once_with('./logs/comment', imaging.imaging._to_tensorboard)\n\n    @patch('torchbearer.callbacks.tensor_board')\n    def test_to_visdom(self, tboard):\n        handler = imaging.imaging._to_visdom('test', log_dir='./logs', comment='comment', visdom_params='test_params')\n        image = MagicMock()\n        handler(image, 0, {torchbearer.EPOCH: 1})\n        image.clamp.assert_called_once_with(0, 1)\n        tboard.get_writer.assert_called_once_with('./logs/comment', imaging.imaging._to_visdom, visdom=True, visdom_params='test_params')\n        tboard.get_writer().add_image.assert_called_once_with('test_1', image.clamp(), 1)\n        tboard.close_writer.assert_called_once_with('./logs/comment', imaging.imaging._to_visdom)\n\n\nclass TestImagingCallback(TestCase):\n    def test_transform(self):\n        callback = imaging.ImagingCallback()\n        self.assertTrue(callback.transform('test') is 'test')\n\n        callback = imaging.ImagingCallback(transform=lambda _: 'test')\n        self.assertTrue(callback.transform('something else') is 'test')\n\n    @patch('torchbearer.callbacks.imaging.imaging._cache_images')\n    def test_cache(self, mock_cache_images):\n        callback = imaging.ImagingCallback()\n        callback.cache(10)\n        mock_cache_images.assert_called_once_with(10)\n\n    @patch('torchvision.utils.make_grid')\n    def test_make_grid(self, mock_grid):\n        callback = imaging.ImagingCallback()\n        callback.on_batch = lambda _: 10\n        callback.make_grid(1, 2, True, 4, True, 6)\n        callback.on_batch({})\n        mock_grid.assert_called_once_with(10, nrow=1, padding=2, normalize=True, range=4, scale_each=True, pad_value=6)\n\n    def test_process(self):\n        callback = imaging.ImagingCallback()\n        callback.on_batch = MagicMock()\n        handler = MagicMock()\n        callback = callback.with_handler(handler)\n        callback.transform = MagicMock()\n        state = 'state'\n        callback.process(state)\n        self.assertTrue(callback.transform.call_count == 1)\n        handler.assert_called_once_with(callback.transform()[None], 0, 'state')\n        self.assertTrue(callback.transform().dim.call_count == 1)\n\n    @patch('torchbearer.callbacks.imaging.imaging._to_visdom')\n    @patch('torchbearer.callbacks.imaging.imaging._to_tensorboard')\n    @patch('torchbearer.callbacks.imaging.imaging._to_pyplot')\n    @patch('torchbearer.callbacks.imaging.imaging._to_file')\n    def test_simple_methods(self, mock_to_file, mock_to_pyplot, mock_to_tensorboard, mock_to_visdom):\n        callback = imaging.ImagingCallback()\n        self.assertRaises(NotImplementedError, lambda: callback.on_batch('test'))\n\n        callback = imaging.ImagingCallback().to_file('test')\n        self.assertTrue(mock_to_file.call_count == 1)\n\n        callback = imaging.ImagingCallback().to_pyplot()\n        self.assertTrue(mock_to_pyplot.call_count == 1)\n\n        callback = imaging.ImagingCallback().to_state('test')\n        state = {}\n        callback._handlers[0][0]('image', 0, state)\n        self.assertTrue('test' in state)\n        self.assertTrue(state['test'] is 'image')\n\n        callback = imaging.ImagingCallback().to_state(0)\n        state = {}\n        callback._handlers[0][0]('image', 0, state)\n        self.assertTrue(0 in state)\n        self.assertTrue(state[0] is 'image')\n\n        callback = imaging.ImagingCallback().to_tensorboard()\n        self.assertTrue(mock_to_tensorboard.call_count == 1)\n\n        callback = imaging.ImagingCallback().to_visdom()\n        self.assertTrue(mock_to_visdom.call_count == 1)\n\n    @patch('torchbearer.callbacks.imaging.imaging._to_file')\n    def test_index(self, mock_to_file):\n        callback = imaging.ImagingCallback().to_file('test', index=10)\n        image = torch.zeros(11, 1, 1, 1)\n        callback.on_batch = lambda _: image\n        callback.process('state')\n        mock_to_file().assert_called_once_with(ANY, 10, 'state')\n        self.assertTrue((mock_to_file().call_args_list[0][0][0] == image[10]).all())\n        mock_to_file().reset_mock()\n\n        callback = imaging.ImagingCallback().to_file('test', index=[2, 5, 10])\n        image = torch.rand(11, 1, 1, 1)\n        callback.on_batch = lambda _: image\n        callback.process('state')\n        self.assertTrue(mock_to_file().call_count == 3)\n        self.assertTrue((mock_to_file().call_args_list[0][0][0] == image[2]).all())\n        self.assertTrue((mock_to_file().call_args_list[1][0][0] == image[5]).all())\n        self.assertTrue((mock_to_file().call_args_list[2][0][0] == image[10]).all())\n\n    @patch('torchbearer.callbacks.imaging.imaging._to_file')\n    def test_dims(self, mock_to_file):\n        callback = imaging.ImagingCallback().to_file('test')\n        image = torch.rand(1, 1, 1)\n        callback.on_batch = lambda _: image\n        callback.process('state')\n        mock_to_file().assert_called_once_with(ANY, 0, 'state')\n        self.assertTrue((mock_to_file().call_args_list[0][0][0] == image).all())\n        self.assertTrue(mock_to_file().call_args_list[0][0][0].dim() == 3)\n\n    def test_on_train(self):\n        callback = imaging.ImagingCallback()\n        mock = MagicMock()\n        callback.on_step_training = mock\n        callback.process = MagicMock()\n        callback = callback.on_train()\n        callback.on_step_training('state')\n        mock.assert_called_once_with('state')\n        callback.process.assert_called_once_with('state')\n\n    def test_on_val(self):\n        callback = imaging.ImagingCallback()\n        mock = MagicMock()\n        callback.on_step_validation = mock\n        callback.process = MagicMock()\n        callback = callback.on_val()\n        state = {torchbearer.DATA: torchbearer.TEST_DATA}\n        callback.on_step_validation(state)\n        mock.assert_called_once_with(state)\n        self.assertTrue(callback.process.call_count == 0)\n\n        mock.reset_mock()\n        callback.process.reset_mock()\n\n        state = {torchbearer.DATA: torchbearer.VALIDATION_DATA}\n        callback.on_step_validation(state)\n        mock.assert_called_once_with(state)\n        callback.process.assert_called_once_with(state)\n\n    def test_on_test(self):\n        callback = imaging.ImagingCallback()\n        mock = MagicMock()\n        callback.on_step_validation = mock\n        callback.process = MagicMock()\n        callback = callback.on_test()\n        state = {torchbearer.DATA: torchbearer.VALIDATION_DATA}\n        callback.on_step_validation(state)\n        mock.assert_called_once_with(state)\n        self.assertTrue(callback.process.call_count == 0)\n\n        mock.reset_mock()\n        callback.process.reset_mock()\n\n        state = {torchbearer.DATA: torchbearer.TEST_DATA}\n        callback.on_step_validation(state)\n        mock.assert_called_once_with(state)\n        callback.process.assert_called_once_with(state)\n\n\nclass TestCachingImagingCallback(TestCase):\n    def test_main(self):\n        callback = imaging.CachingImagingCallback(key='my_key', num_images=5)\n        self.assertRaises(NotImplementedError, lambda: callback.on_cache('', {}))\n\n        callback.on_cache = MagicMock()\n        callback.on_cache.return_value = 'image'\n\n        state = {'my_key': torch.ones(10, 3, 2, 2), torchbearer.BATCH: 0}\n        callback.on_batch(state)\n        self.assertTrue(callback.on_cache.call_args[0][0].sum() == 60)\n        self.assertTrue(callback.on_cache.call_args[0][1] == state)\n\n    def test_multi_batch(self):\n        callback = imaging.CachingImagingCallback(key='my_key', num_images=25)\n        self.assertRaises(NotImplementedError, lambda: callback.on_cache('', {}))\n\n        callback.on_cache = MagicMock()\n        callback.on_cache.return_value = 'image'\n\n        state = {'my_key': torch.ones(10, 3, 2, 2), torchbearer.BATCH: 0}\n        callback.on_batch(state)\n        state[torchbearer.BATCH] = 1\n        callback.on_batch(state)\n        state[torchbearer.BATCH] = 2\n        callback.on_batch(state)\n        self.assertTrue(callback.on_cache.call_args[0][0].sum() == 300)\n        self.assertTrue(callback.on_cache.call_args[0][1] == state)\n\n    def test_multi_epoch(self):\n        callback = imaging.CachingImagingCallback(key='my_key', num_images=5)\n        self.assertRaises(NotImplementedError, lambda: callback.on_cache('', {}))\n\n        callback.on_cache = MagicMock()\n        callback.on_cache.return_value = 'image'\n\n        state = {'my_key': torch.ones(10, 3, 2, 2), torchbearer.BATCH: 0}\n        callback.on_batch(state)\n        self.assertTrue(callback.on_cache.call_args[0][0].sum() == 60)\n        self.assertTrue(callback.on_cache.call_args[0][1] == state)\n        callback.on_cache.reset_mock()\n        callback.on_end_epoch({})\n        callback.on_batch(state)\n        self.assertTrue(callback.on_cache.call_args[0][0].sum() == 60)\n        self.assertTrue(callback.on_cache.call_args[0][1] == state)\n\n\nclass TestMakeGrid(TestCase):\n    @patch('torchvision.utils.make_grid')\n    def test_main(self, mock_grid):\n        mock_grid.return_value = 10\n\n        callback = imaging.MakeGrid(key='x', num_images=18, nrow=9, padding=3, normalize=True, norm_range='tmp',\n                                    scale_each=True, pad_value=1)\n\n        res = callback.on_cache('test', {})\n        mock_grid.assert_called_once_with('test', nrow=9, padding=3, normalize=True, range='tmp', scale_each=True,\n                                          pad_value=1)\n        self.assertTrue(res == 10)\n\n\nclass TestFromState(TestCase):\n    def test_main(self):\n        callback = imaging.FromState('test')\n\n        self.assertTrue(callback.on_batch({torchbearer.EPOCH: 0, 'test': 1}) == 1)\n        self.assertTrue(callback.on_batch({torchbearer.EPOCH: 1, 'testing': 1}) is None)\n"""
tests/callbacks/imaging/test_inside_cnns.py,12,"b""from unittest import TestCase\n\nfrom mock import MagicMock, patch, ANY\n\nimport torch\n\nimport torchbearer\nimport torchbearer.callbacks.imaging as imaging\n\n\nclass TestClassAppearanceModel(TestCase):\n    def test_targets_hot(self):\n        callback = imaging.ClassAppearanceModel(nclasses=10, input_size=(1, 1, 1), target=5)\n        targets_hot = callback._targets_hot({torchbearer.DEVICE: 'cpu'})\n        self.assertTrue(targets_hot.sum() == 1)\n        self.assertTrue(targets_hot[0][5] == 1)\n\n    def test_targets_to_key(self):\n        callback = imaging.ClassAppearanceModel(nclasses=10, input_size=(1, 1, 1), target=5)\n        callback = callback.target_to_key('test')\n        state = {torchbearer.DEVICE: 'cpu'}\n        targets_hot = callback._targets_hot(state)\n        self.assertTrue(targets_hot.sum() == 1)\n        self.assertTrue(targets_hot[0][5] == 1)\n        self.assertTrue(state['test'] == 5)\n\n    @patch('torchbearer.callbacks.imaging.inside_cnns.torch.randint')\n    def test_random_targets(self, randint):\n        randint.return_value = torch.ones(1, 1) * 6\n        callback = imaging.ClassAppearanceModel(nclasses=10, input_size=(1, 1, 1))\n        state = {torchbearer.DEVICE: 'cpu'}\n        targets_hot = callback._targets_hot(state)\n        self.assertTrue(targets_hot.sum() == 1)\n        self.assertTrue(targets_hot[0][6] == 1)\n\n    def test_cam_loss(self):\n        key = 'my_key'\n        targets_hot = torch.FloatTensor([[0, 0, 0, 1, 0]]).ge(0.5)\n        decay = 0.5\n        loss = imaging.inside_cnns._cam_loss(key, targets_hot, decay)\n\n        model = MagicMock\n        model.input_image = torch.ones(5, 5) * 2\n\n        state = {'my_key': torch.FloatTensor([[0, 0, 0, 100, 0]]), torchbearer.MODEL: model}\n        res = loss(state)\n        self.assertTrue(res.item() == -50)\n\n    def test_cam_wrapper(self):\n        model = MagicMock()\n        wrapper = imaging.inside_cnns._CAMWrapper((3, 10, 10), model)\n\n        self.assertTrue(wrapper.input_image.requires_grad)\n        self.assertTrue(wrapper.input_image.shape == torch.Size([3, 10, 10]))\n        wrapper('', 'state')\n        self.assertTrue(model.call_count == 1)\n        self.assertTrue(model.call_args[0][0].shape == torch.Size([1, 3, 10, 10]))\n        self.assertTrue(model.call_args[0][1] == 'state')\n        model.reset_mock()\n\n        model.side_effect = lambda x: x\n        wrapper('', 'state')\n        self.assertTrue(model.call_count == 2)\n        self.assertTrue(model.call)\n        self.assertTrue(model.call_args[0][0].shape == torch.Size([1, 3, 10, 10]))\n\n    @patch('torchbearer.callbacks.imaging.inside_cnns._cam_loss')\n    @patch('torchbearer.callbacks.imaging.inside_cnns._CAMWrapper')\n    @patch('torchbearer.callbacks.imaging.inside_cnns.torchbearer.Trial')\n    def test_on_batch(self, _, wrapper, loss):\n        wrapper().input_image = torch.nn.Parameter(torch.zeros(10))\n\n        factory = MagicMock()\n        callback = imaging.ClassAppearanceModel(nclasses=10, input_size=(1, 1, 1), optimizer_factory=factory)\n\n        model = MagicMock()\n        callback.on_batch({torchbearer.EPOCH: 0, torchbearer.MODEL: model, torchbearer.DEVICE: 'cpu', torchbearer.DATA_TYPE: torch.float32})\n\n        loss.assert_called_once_with(torchbearer.PREDICTION, ANY, 0.01)\n\n        self.assertTrue(model.eval.call_count == 1)\n        self.assertTrue(model.train.call_count == 1)\n        self.assertTrue(factory.call_count == 1)\n        self.assertTrue(next(iter(factory.call_args[0][0])) is wrapper().input_image)\n\n    def test_end_to_end(self):\n        with torchbearer.no_grad():\n            model = torch.nn.Linear(10, 5)\n            callback = imaging.ClassAppearanceModel(nclasses=5, input_size=(10), steps=1)\n            state = {torchbearer.EPOCH: 0, torchbearer.MODEL: model, torchbearer.DEVICE: 'cpu',\n                     torchbearer.DATA_TYPE: torch.float32}\n            callback.on_batch(state)\n"""
torchbearer/callbacks/imaging/__init__.py,0,"b'""""""\n\n""""""\nfrom .imaging import *\nfrom .inside_cnns import ClassAppearanceModel, RANDOM\n'"
torchbearer/callbacks/imaging/imaging.py,13,"b'import torchbearer\nfrom torchbearer import Callback\n\nimport torch\n\n\ndef _to_file(filename):\n    from PIL import Image\n\n    def handler(image, index, _):\n        ndarr = image.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n        im = Image.fromarray(ndarr)\n        im.save(filename.format(index=str(index)))\n\n    return handler\n\n\ndef _to_pyplot(title=None, show=True):\n    import matplotlib.pyplot as plt\n\n    def handler(image, index, _):\n        ndarr = image.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n        plt.imshow(ndarr)\n        if title is not None:\n            plt.title(title.format(index=str(index)))\n        plt.axis(\'off\')\n\n        if show:\n            plt.show()\n\n    return handler\n\n\ndef _to_tensorboard(name=\'Image\', log_dir=\'./logs\', comment=\'torchbearer\'):\n    import torchbearer.callbacks.tensor_board as tb\n    import os\n    log_dir = os.path.join(log_dir, comment)\n\n    def handler(image, index, state):\n        writer = tb.get_writer(log_dir, _to_tensorboard)\n        writer.add_image(name.format(index=str(index)), image.clamp(0, 1), state[torchbearer.EPOCH])\n        tb.close_writer(log_dir, _to_tensorboard)\n\n    return handler\n\n\ndef _to_visdom(name=\'Image\', log_dir=\'./logs\', comment=\'torchbearer\', visdom_params=None):\n    import torchbearer.callbacks.tensor_board as tb\n    import os\n    log_dir = os.path.join(log_dir, comment)\n\n    def handler(image, index, state):\n        writer = tb.get_writer(log_dir, _to_visdom, visdom=True, visdom_params=visdom_params)\n        writer.add_image(name.format(index=str(index)) + \'_\' + str(state[torchbearer.EPOCH]), image.clamp(0, 1), state[torchbearer.EPOCH])\n        tb.close_writer(log_dir, _to_visdom)\n\n    return handler\n\n\ndef _cache_images(num_images):\n    cache = {\'images\': None, \'done\': False}\n\n    def decorator(fun):\n        def step(state):\n            if state[torchbearer.BATCH] == 0:\n                cache[\'done\'] = False\n\n            if not cache[\'done\']:\n                data = fun(state)\n\n                if cache[\'images\'] is None:\n                    remaining = num_images if num_images < data.size(0) else data.size(0)\n\n                    cache[\'images\'] = data[:remaining]\n                else:\n                    remaining = num_images - cache[\'images\'].size(0)\n\n                    if remaining > data.size(0):\n                        remaining = data.size(0)\n\n                    cache[\'images\'] = torch.cat((cache[\'images\'], data[:remaining]), dim=0)\n\n                if cache[\'images\'].size(0) >= num_images:\n                    res = cache[\'images\']\n                    cache[\'done\'] = True\n                    cache[\'images\'] = None\n                    return res\n        return step\n    return decorator\n\n\nclass ImagingCallback(Callback):\n    """"""The :class:`ImagingCallback` provides a generic interface for callbacks which yield images that should be sent to\n    a file, tensorboard, visdom etc. without needing bespoke code. This allows the user to easily define custom\n    visualisations by only writing the code to produce the image.\n\n    Args:\n        transform (callable, optional): A function/transform that  takes in a Tensor and returns a transformed version.\n            This will be applied to the image before it is sent to output.\n    """"""\n    def __init__(self, transform=None):\n        self._handlers = []\n        self.transform = (lambda img: img) if transform is None else transform\n\n    def on_batch(self, state):\n        raise NotImplementedError\n\n    def process(self, state):\n        img = self.on_batch(state)\n        if img is not None:\n            img = self.transform(img)\n            for handler, index in self._handlers:\n                if img.dim() == 3:\n                    img = img.unsqueeze(0)\n                rng = range(img.size(0)) if index is None else index\n                try:\n                    for i in rng:\n                        handler(img[i], i, state)\n                except TypeError:\n                    handler(img[rng], rng, state)\n\n    def on_train(self):\n        """"""Process this callback for training batches\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        _old_step_training = self.on_step_training\n\n        def wrapper(state):\n            _old_step_training(state)\n            self.process(state)\n\n        self.on_step_training = wrapper\n        return self\n\n    def on_val(self):\n        """"""Process this callback for validation batches\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        _old_step_validation = self.on_step_validation\n\n        def wrapper(state):\n            _old_step_validation(state)\n            if state[torchbearer.DATA] is torchbearer.VALIDATION_DATA:\n                self.process(state)\n\n        self.on_step_validation = wrapper\n        return self\n\n    def on_test(self):\n        """"""Process this callback for test batches\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        _old_step_validation = self.on_step_validation\n\n        def wrapper(state):\n            _old_step_validation(state)\n            if state[torchbearer.DATA] is torchbearer.TEST_DATA:\n                self.process(state)\n\n        self.on_step_validation = wrapper\n        return self\n\n    def with_handler(self, handler, index=None):\n        """"""Append the given output handler to the list of handlers\n\n        Args:\n            handler: A function of image and state which stores the given image in some way\n            index (int or list or None): If not None, only apply the handler on this index / list of indices\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        self._handlers.append((handler, index))\n        return self\n\n    def to_file(self, filename, index=None):\n        """"""Send images from this callback to the given file\n\n        Args:\n            filename (str): The filename to store the image to\n            index (int or list or None): If not None, only apply the handler on this index / list of indices\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        return self.with_handler(_to_file(filename), index=index)\n\n    def to_pyplot(self, title=None, show=True, index=None):\n        """"""Show images from this callback with pyplot\n\n        Args:\n            title (str or None): If not None, plt.title will be called with the given string\n            show (bool): If True (default), show will be called after each image is plotted\n            index (int or list or None): If not None, only apply the handler on this index / list of indices\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        return self.with_handler(_to_pyplot(title=title, show=show), index=index)\n\n    def to_state(self, keys, index=None):\n        """"""Put images from this callback in state with the given key\n\n        Args:\n            keys (StateKey or list[StateKey]): The state key or keys to use for the images\n            index (int or list or None): If not None, only apply the handler on this index / list of indices\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        if str(keys) == keys:\n            keys = [keys]\n\n        try:\n            _ = (key for key in keys)\n        except TypeError:\n            keys = [keys]\n\n        def handler(img, i, state):\n            state[keys[i]] = img\n        return self.with_handler(handler, index=index)\n\n    def to_tensorboard(self, name=\'Image\', log_dir=\'./logs\', comment=\'torchbearer\', index=None):\n        """"""Direct images from this callback to tensorboard with the given parameters\n\n        Args:\n            name (str): The name of the image\n            log_dir (str): The tensorboard log path for output\n            comment (str): Descriptive comment to append to path\n            index (int or list or None): if not None, only apply the handler on this index / list of indices\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        return self.with_handler(_to_tensorboard(name=name, log_dir=log_dir, comment=comment), index=index)\n\n    def to_visdom(self, name=\'Image\', log_dir=\'./logs\', comment=\'torchbearer\', visdom_params=None, index=None):\n        """"""Direct images from this callback to visdom with the given parameters\n\n        Args:\n            name (str): The name of the image\n            log_dir (str): The visdom log path for output\n            comment (str): Descriptive comment to append to path\n            visdom_params (:class:`.VisdomParams`): Visdom parameter settings object, uses default if None\n            index (int or list or None): if not None, only apply the handler on this index / list of indices\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        return self.with_handler(_to_visdom(name=name, log_dir=log_dir, comment=comment, visdom_params=visdom_params), index=index)\n\n    def cache(self, num_images):\n        """"""Cache images **before** they are passed to handlers. Once per epoch, a single cache will be returned,\n        containing the first `num_images` to be returned.\n\n        Args:\n            num_images (int): The number of images to cache\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        self.on_batch = _cache_images(num_images)(self.on_batch)\n        return self\n\n    def make_grid(self, nrow=8, padding=2, normalize=False, norm_range=None, scale_each=False, pad_value=0):\n        """"""Use `torchvision.utils.make_grid` to make a grid of the images being returned by this callback. Recommended\n        for use alongside `cache`.\n\n        Args:\n            nrow: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n            padding: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n            normalize: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n            norm_range: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n            scale_each: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n            pad_value: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n\n        Returns:\n            ImagingCallback: self\n        """"""\n        import torchvision.utils as utils\n\n        def decorator(func):\n            def wrapper(state):\n                cache = func(state)\n                if cache is not None:\n                    return utils.make_grid(cache, nrow=nrow, padding=padding, normalize=normalize, range=norm_range,\n                                           scale_each=scale_each, pad_value=pad_value)\n            return wrapper\n\n        self.on_batch = decorator(self.on_batch)\n        return self\n\n\nclass FromState(ImagingCallback):\n    """"""The :class:`FromState` callback is an :class:`ImagingCallback` which retrieves and image from state when called.\n    The number of times the function is called can be controlled with a provided decorator (once_per_epoch, only_if\n    etc.)\n\n    Args:\n        key (StateKey): The :class:`.StateKey` containing the image (tensor of size [c, w, h])\n        transform (callable, optional): A function/transform that  takes in a Tensor and returns a transformed version.\n            This will be applied to the image before it is sent to output.\n        decorator: A function which will be used to wrap the callback function. once_per_epoch by default\n    """"""\n    def __init__(self, key, transform=None, decorator=None):\n        super(FromState, self).__init__(transform=transform)\n        self.key = key\n\n        if decorator is not None:\n            self.on_batch = decorator(self.on_batch)\n\n    def on_batch(self, state):\n        try:\n            return state[self.key]\n        except KeyError:\n            return None\n\n\nclass CachingImagingCallback(FromState):\n    """"""The :class:`CachingImagingCallback` is an :class:`ImagingCallback` which caches batches of images from the given\n    state key up to the required amount before passing this along with state to the implementing class, once per epoch.\n\n    Args:\n        key (StateKey): The :class:`.StateKey` containing image data (tensor of size [b, c, w, h])\n        transform (callable, optional): A function/transform that  takes in a Tensor and returns a transformed version.\n            This will be applied to the image before it is sent to output.\n        num_images: The number of images to cache\n    """"""\n    def __init__(self,\n                 key=torchbearer.INPUT,\n                 transform=None,\n                 num_images=16):\n        super(CachingImagingCallback, self).__init__(key=key, transform=transform, decorator=_cache_images(num_images))\n\n        def decorator(func):\n            def wrapper(state):\n                res = func(state)\n                if res is not None:\n                    return self.on_cache(res, state)\n            return wrapper\n\n        self.on_batch = decorator(self.on_batch)\n\n    def on_cache(self, cache, state):\n        """"""This method should be implemented by the overriding class to return an image from the cache.\n\n        Args:\n            cache (tensor): The collected cache of size (num_images, C, W, H)\n            state (dict): The trial state dict\n\n        Returns:\n            The processed image\n        """"""\n        raise NotImplementedError\n\n\nclass MakeGrid(CachingImagingCallback):\n    """"""The :class:`MakeGrid` callback is a :class:`CachingImagingCallback` which calls make grid on the cache with the\n    provided parameters.\n\n    Args:\n        key (StateKey): The :class:`.StateKey` containing image data (tensor of size [b, c, w, h])\n        transform (callable, optional): A function/transform that  takes in a Tensor and returns a transformed version.\n            This will be applied to the image before it is sent to output.\n        num_images: The number of images to cache\n        nrow: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        padding: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        normalize: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        norm_range: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        scale_each: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n        pad_value: See `torchvision.utils.make_grid <https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid>`_\n    """"""\n    def __init__(self,\n                 key=torchbearer.INPUT,\n                 transform=None,\n                 num_images=16,\n                 nrow=8,\n                 padding=2,\n                 normalize=False,\n                 norm_range=None,\n                 scale_each=False,\n                 pad_value=0):\n        super(MakeGrid, self).__init__(transform=transform, num_images=num_images, key=key)\n        self.key = key\n        self.num_images = num_images\n        self.nrow = nrow\n        self.padding = padding\n        self.normalize = normalize\n        self.norm_range = norm_range\n        self.scale_each = scale_each\n        self.pad_value = pad_value\n\n    def on_cache(self, cache, state):\n        import torchvision.utils as utils\n        return utils.make_grid(\n            cache,\n            nrow=self.nrow,\n            padding=self.padding,\n            normalize=self.normalize,\n            range=self.norm_range,\n            scale_each=self.scale_each,\n            pad_value=self.pad_value\n        )\n'"
torchbearer/callbacks/imaging/inside_cnns.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchbearer\nfrom torchbearer.callbacks.decorators import once_per_epoch\nfrom . import imaging\n\n_inside_cnns = """"""\n@article{simonyan2013deep,\n  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},\n  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},\n  journal={arXiv preprint arXiv:1312.6034},\n  year={2013}\n}\n""""""\n\nRANDOM = -10\n"""""" Flag that when passed as the target chosses a random target""""""\n\n\nclass _CAMWrapper(nn.Module):\n    def __init__(self, input_size, base_model, transform=None):\n        super(_CAMWrapper, self).__init__()\n        self.base_model = base_model\n        input_image = torch.zeros(input_size)\n\n        self.input_image = nn.Parameter(input_image, requires_grad=True)\n\n        self.transform = (lambda x: x) if transform is None else transform\n\n    def forward(self, _, state):\n        try:\n            return self.base_model(self.transform(self.input_image.sigmoid()).unsqueeze(0), state)\n        except TypeError:\n            return self.base_model(self.transform(self.input_image.sigmoid()).unsqueeze(0))\n\n\ndef _cam_loss(key, targets_hot, decay):\n    def loss(state):\n        img = state[torchbearer.MODEL].input_image\n        return - torch.masked_select(state[key], targets_hot).sum() + decay * img.pow(2).sum()\n    return loss\n\n\n@torchbearer.cite(_inside_cnns)\nclass ClassAppearanceModel(imaging.ImagingCallback):\n    """"""The :class:`.ClassAppearanceModel` callback implements Figure 1 from\n    `Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps <https://arxiv.org/abs/1312.6034>`_.\n    This is a simple gradient ascent on an image (initialised to zero) with a sum-squares regularizer. Internally this\n    creates a new :class:`.Trial` instance which then performs the optimization.\n\n    Args:\n        nclasses (int): The number of output classes\n        input_size (tuple): The size to use for the input image\n        optimizer_factory: A function of parameters which returns an optimizer to use\n        logit_key (StateKey): :class:`.StateKey` storing the class logits\n        target (int): Target class for the optimisation or RANDOM\n        steps (int): Number of optimisation steps to take\n        decay (float): Lambda for the L2 decay on the image\n        verbose (int): Verbosity level to pass to the internal :class:`.Trial` instance\n        transform (callable, optional): A function/transform that  takes in a Tensor and returns a transformed version.\n            This will be applied to the image before it is sent to output\n\n    """"""\n    def __init__(self, nclasses, input_size, optimizer_factory=lambda params: optim.Adam(params, lr=0.5), steps=256,\n                 logit_key=torchbearer.PREDICTION, target=RANDOM, decay=0.01, verbose=0, in_transform=None, transform=None):\n        super(ClassAppearanceModel, self).__init__(transform=transform)\n\n        self.nclasses = nclasses\n        self.input_size = input_size\n        self.optimizer_factory = optimizer_factory\n        self.logit_key = logit_key\n        self.target = target\n        self.steps = steps\n        self.decay = decay\n        self.verbose = verbose\n        self.in_transform = in_transform\n\n        self._target_keys = []\n\n    def target_to_key(self, key):\n        self._target_keys.append(key)\n        return self\n\n    def _targets_hot(self, state):\n        targets = torch.randint(high=self.nclasses, size=(1, 1)).long().to(state[torchbearer.DEVICE])\n        if self.target is not RANDOM:\n            targets[0][0] = self.target\n        for key in self._target_keys:\n            state[key] = targets\n        targets_hot = torch.zeros(1, self.nclasses).to(state[torchbearer.DEVICE])\n        targets_hot.scatter_(1, targets, 1)\n        targets_hot = targets_hot.ge(0.5)\n        return targets_hot\n\n    @torchbearer.enable_grad()\n    @once_per_epoch\n    def on_batch(self, state):\n        training = state[torchbearer.MODEL].training\n        state[torchbearer.MODEL].eval()\n\n        targets_hot = self._targets_hot(state)\n\n        key = self.logit_key\n\n        @torchbearer.callbacks.on_sample\n        def make_eval(_):\n            state[torchbearer.MODEL].eval()\n\n        model = _CAMWrapper(self.input_size, state[torchbearer.MODEL], transform=self.in_transform)\n        trial = torchbearer.Trial(model, self.optimizer_factory(filter(lambda p: p.requires_grad, [model.input_image])),\n                                  _cam_loss(key, targets_hot, self.decay), callbacks=[make_eval])\n        trial.for_train_steps(self.steps).to(state[torchbearer.DEVICE], state[torchbearer.DATA_TYPE])\n        trial.run(verbose=self.verbose)\n\n        if training:\n            state[torchbearer.MODEL].train()\n\n        return model.input_image.sigmoid()\n'"
