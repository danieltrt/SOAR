file_path,api_count,code
a2c_devel.py,12,"b'import gym\ngym.logger.set_level(40)\n\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\n\nfrom IPython.display import clear_output\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.switch_backend(\'agg\')\n\nfrom timeit import default_timer as timer\nfrom datetime import timedelta\nimport os\nimport glob\n\nfrom utils.wrappers import make_env_a2c_atari\nfrom utils.plot import visdom_plot\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n\nfrom utils.hyperparameters import Config\nfrom agents.PPO import Model\n\nuse_vis=True\nport=8097\nlog_dir = ""/tmp/gym/""\n\ntry:\n    os.makedirs(log_dir)\nexcept OSError:\n    files = glob.glob(os.path.join(log_dir, \'*.monitor.csv\'))\n    for f in files:\n        os.remove(f)\n\nconfig = Config()\n\n#ppo control\nconfig.ppo_epoch = 3\nconfig.num_mini_batch = 32\nconfig.ppo_clip_param = 0.1\n\n#a2c control\nconfig.num_agents=8\nconfig.rollout=128\nconfig.USE_GAE = True\nconfig.gae_tau = 0.95\n\n#misc agent variables\nconfig.GAMMA=0.99\nconfig.LR=7e-4\nconfig.entropy_loss_weight=0.01\nconfig.value_loss_weight=1.0\nconfig.grad_norm_max = 0.5\n\nconfig.MAX_FRAMES=int(1e7 / config.num_agents / config.rollout)\n\n\nif __name__==\'__main__\':\n    seed = 1\n\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\n    torch.set_num_threads(1)\n\n    if use_vis:\n        from visdom import Visdom\n        viz = Visdom(port=port)\n        win = None\n\n    env_id = ""PongNoFrameskip-v4""\n    envs = [make_env_a2c_atari(env_id, seed, i, log_dir) for i in range(config.num_agents)]\n    envs = SubprocVecEnv(envs) if config.num_agents > 1 else DummyVecEnv(envs)\n\n    obs_shape = envs.observation_space.shape\n    obs_shape = (obs_shape[0] * 4, *obs_shape[1:])\n\n    model = Model(env=envs, config=config)\n\n    current_obs = torch.zeros(config.num_agents, *obs_shape,\n                    device=config.device, dtype=torch.float)\n\n    def update_current_obs(obs):\n        shape_dim0 = envs.observation_space.shape[0]\n        obs = torch.from_numpy(obs.astype(np.float32)).to(config.device)\n        current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n        current_obs[:, -shape_dim0:] = obs\n\n    obs = envs.reset()\n    update_current_obs(obs)\n\n    model.rollouts.observations[0].copy_(current_obs)\n    \n    episode_rewards = np.zeros(config.num_agents, dtype=np.float)\n    final_rewards = np.zeros(config.num_agents, dtype=np.float)\n\n    start=timer()\n\n    print_step = 1\n    print_threshold = 10\n    \n    for frame_idx in range(1, config.MAX_FRAMES+1):\n        for step in range(config.rollout):\n            with torch.no_grad():\n                values, actions, action_log_prob = model.get_action(model.rollouts.observations[step])\n            cpu_actions = actions.view(-1).cpu().numpy()\n    \n            obs, reward, done, _ = envs.step(cpu_actions)\n\n            episode_rewards += reward\n            masks = 1. - done.astype(np.float32)\n            final_rewards *= masks\n            final_rewards += (1. - masks) * episode_rewards\n            episode_rewards *= masks\n\n            rewards = torch.from_numpy(reward.astype(np.float32)).view(-1, 1).to(config.device)\n            masks = torch.from_numpy(masks).to(config.device).view(-1, 1)\n\n            current_obs *= masks.view(-1, 1, 1, 1)\n            update_current_obs(obs)\n\n            model.rollouts.insert(current_obs, actions.view(-1, 1), action_log_prob, values, rewards, masks)\n            \n        with torch.no_grad():\n            next_value = model.get_values(model.rollouts.observations[-1])\n\n        model.rollouts.compute_returns(next_value, config.GAMMA)\n            \n        value_loss, action_loss, dist_entropy = model.update(model.rollouts)\n        \n        model.rollouts.after_update()\n\n        if frame_idx % print_threshold == 0:\n            end = timer()\n            total_num_steps = (frame_idx + 1) * config.num_agents * config.rollout\n            print(""Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}"".\n                format(frame_idx, total_num_steps,\n                       int(total_num_steps / (end - start)),\n                       np.mean(final_rewards),\n                       np.median(final_rewards),\n                       np.min(final_rewards),\n                       np.max(final_rewards), dist_entropy,\n                       value_loss, action_loss))\n\n        if use_vis and frame_idx % 100 == 0:\n            try:\n                # Sometimes monitor doesn\'t properly flush the outputs\n                win = visdom_plot(viz, win, log_dir, ""PongNoFrameskip-v4"",\n                                  \'a2c-Q\', config.MAX_FRAMES * config.num_agents * config.rollout)\n            except IOError:\n                pass\n\n    model.save_w()\n    envs.close()'"
dqn_devel.py,0,"b'import gym\nimport numpy as np\n\nfrom timeit import default_timer as timer\nfrom datetime import timedelta\nimport math\nimport glob\n\nfrom utils.wrappers import *\nfrom utils.hyperparameters import Config\nfrom agents.DQN import Model\nfrom utils.plot import plot_all_data\n\nconfig = Config()\n\n#algorithm control\nconfig.USE_NOISY_NETS      = False\nconfig.USE_PRIORITY_REPLAY = False\n\n#Multi-step returns\nconfig.N_STEPS = 1\n\n#epsilon variables\nconfig.epsilon_start    = 1.0\nconfig.epsilon_final    = 0.01\nconfig.epsilon_decay    = 30000\nconfig.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n\n#misc agent variables\nconfig.GAMMA = 0.99\nconfig.LR    = 1e-4\n\n#memory\nconfig.TARGET_NET_UPDATE_FREQ = 1000\nconfig.EXP_REPLAY_SIZE        = 100000\nconfig.BATCH_SIZE             = 32\n\nconfig.PRIORITY_ALPHA       = 0.6\nconfig.PRIORITY_BETA_START  = 0.4\nconfig.PRIORITY_BETA_FRAMES = 100000\n\n#Noisy Nets\nconfig.SIGMA_INIT = 0.5\n\n#Learning control variables\nconfig.LEARN_START = 10000\nconfig.MAX_FRAMES  = 1000000\nconfig.UPDATE_FREQ = 1\n\n#Categorical Params\nconfig.ATOMS = 51\nconfig.V_MAX = 50\nconfig.V_MIN = 0\n\n#Quantile Regression Parameters\nconfig.QUANTILES = 21\n\n#DRQN Parameters\nconfig.SEQUENCE_LENGTH = 8\n\n#data logging parameters\nconfig.ACTION_SELECTION_COUNT_FREQUENCY = 1000\n\nif __name__==\'__main__\':\n    start=timer()\n\n    log_dir = ""/tmp/gym/""\n    try:\n        os.makedirs(log_dir)\n    except OSError:\n        files = glob.glob(os.path.join(log_dir, \'*.monitor.csv\')) \\\n            + glob.glob(os.path.join(log_dir, \'*td.csv\')) \\\n            + glob.glob(os.path.join(log_dir, \'*sig_param_mag.csv\')) \\\n            + glob.glob(os.path.join(log_dir, \'*action_log.csv\'))\n        for f in files:\n            os.remove(f)\n\n    env_id = ""PongNoFrameskip-v4""\n    env    = make_atari(env_id)\n    env    = bench.Monitor(env, os.path.join(log_dir, env_id))\n    env    = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=True)\n    env    = WrapPyTorch(env)\n    model  = Model(env=env, config=config, log_dir=log_dir)\n\n    episode_reward = 0\n\n    observation = env.reset()\n    for frame_idx in range(1, config.MAX_FRAMES + 1):\n        epsilon = config.epsilon_by_frame(frame_idx)\n\n        action = model.get_action(observation, epsilon)\n        model.save_action(action, frame_idx) #log action selection\n\n        prev_observation=observation\n        observation, reward, done, _ = env.step(action)\n        observation = None if done else observation\n\n        model.update(prev_observation, action, reward, observation, frame_idx)\n        episode_reward += reward\n        \n        if done:\n            model.finish_nstep()\n            model.reset_hx()\n            observation = env.reset()\n            model.save_reward(episode_reward)\n            episode_reward = 0\n            \n        if frame_idx % 10000 == 0:\n            model.save_w()\n            try:\n                print(\'frame %s. time: %s\' % (frame_idx, timedelta(seconds=int(timer()-start))))\n                plot_all_data(log_dir, env_id, \'DRQN\', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=False)\n            except IOError:\n                pass\n\n    model.save_w()\n    env.close()'"
agents/A2C.py,6,"b""import numpy as np\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom agents.BaseAgent import BaseAgent\nfrom networks.networks import ActorCritic\nfrom utils.RolloutStorage import RolloutStorage\n\nfrom timeit import default_timer as timer\n\nclass Model(BaseAgent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        super(Model, self).__init__(config=config, env=env, log_dir=log_dir)\n        self.device = config.device\n\n        self.noisy=config.USE_NOISY_NETS\n        self.priority_replay=config.USE_PRIORITY_REPLAY\n\n        self.gamma = config.GAMMA\n        self.lr = config.LR\n        self.target_net_update_freq = config.TARGET_NET_UPDATE_FREQ\n        self.learn_start = config.LEARN_START\n        self.sigma_init= config.SIGMA_INIT\n        self.num_agents = config.num_agents\n        self.value_loss_weight = config.value_loss_weight\n        self.entropy_loss_weight = config.entropy_loss_weight\n        self.rollout = config.rollout\n        self.grad_norm_max = config.grad_norm_max\n\n        self.static_policy = static_policy\n        self.num_feats = env.observation_space.shape\n        self.num_feats = (self.num_feats[0] * 4, *self.num_feats[1:])\n        self.num_actions = env.action_space.n\n        self.env = env\n\n        self.declare_networks()\n            \n        self.optimizer = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=0.99, eps=1e-5)\n        \n        #move to correct device\n        self.model = self.model.to(self.device)\n\n        if self.static_policy:\n            self.model.eval()\n        else:\n            self.model.train()\n\n        self.rollouts = RolloutStorage(self.rollout, self.num_agents,\n            self.num_feats, self.env.action_space, self.device, config.USE_GAE, config.gae_tau)\n\n        self.value_losses = []\n        self.entropy_losses = []\n        self.policy_losses = []\n\n\n    def declare_networks(self):\n        self.model = ActorCritic(self.num_feats, self.num_actions)\n\n    def get_action(self, s, deterministic=False):\n        logits, values = self.model(s)\n        dist = torch.distributions.Categorical(logits=logits)\n\n        if deterministic:\n            actions = dist.probs.argmax(dim=1, keepdim=True)\n        else:\n            actions = dist.sample().view(-1, 1)\n\n        log_probs = F.log_softmax(logits, dim=1)\n        action_log_probs = log_probs.gather(1, actions)\n\n        return values, actions, action_log_probs\n        \n\n    def evaluate_actions(self, s, actions):\n        logits, values = self.model(s)\n\n        dist = torch.distributions.Categorical(logits=logits)\n\n        log_probs = F.log_softmax(logits, dim=1)\n        action_log_probs = log_probs.gather(1, actions)\n\n        dist_entropy = dist.entropy().mean()\n\n        return values, action_log_probs, dist_entropy\n\n    def get_values(self, s):\n        _, values = self.model(s)\n\n        return values\n\n    def compute_loss(self, rollouts):\n        obs_shape = rollouts.observations.size()[2:]\n        action_shape = rollouts.actions.size()[-1]\n        num_steps, num_processes, _ = rollouts.rewards.size()\n\n        values, action_log_probs, dist_entropy = self.evaluate_actions(\n            rollouts.observations[:-1].view(-1, *obs_shape),\n            rollouts.actions.view(-1, 1))\n\n        values = values.view(num_steps, num_processes, 1)\n        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n\n        advantages = rollouts.returns[:-1] - values\n        value_loss = advantages.pow(2).mean()\n\n        action_loss = -(advantages.detach() * action_log_probs).mean()\n\n        loss = action_loss + self.value_loss_weight * value_loss - self.entropy_loss_weight * dist_entropy\n\n        return loss, action_loss, value_loss, dist_entropy\n\n    def update(self, rollout):\n        loss, action_loss, value_loss, dist_entropy = self.compute_loss(rollout)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_norm_max)\n        self.optimizer.step()\n\n        #self.save_loss(loss.item(), action_loss.item(), value_loss.item(), dist_entropy.item())\n        #self.save_sigma_param_magnitudes()\n\n        return value_loss.item(), action_loss.item(), dist_entropy.item()\n\n    '''def save_loss(self, loss, policy_loss, value_loss, entropy_loss):\n        super(Model, self).save_td(loss)\n        self.policy_losses.append(policy_loss)\n        self.value_losses.append(value_loss)\n        self.entropy_losses.append(entropy_loss)'''\n"""
agents/BaseAgent.py,7,"b'import numpy as np\nimport pickle\nimport os.path\nimport csv\n\nimport torch\nimport torch.optim as optim\n\n\nclass BaseAgent(object):\n    def __init__(self, config, env, log_dir=\'/tmp/gym\'):\n        self.model=None\n        self.target_model=None\n        self.optimizer = None\n\n        self.log_dir = log_dir\n\n        self.rewards = []\n\n        self.action_log_frequency = config.ACTION_SELECTION_COUNT_FREQUENCY\n        self.action_selections = [0 for _ in range(env.action_space.n)]\n\n    def huber(self, x):\n        cond = (x.abs() < 1.0).float().detach()\n        return 0.5 * x.pow(2) * cond + (x.abs() - 0.5) * (1.0 - cond)\n\n    def MSE(self, x):\n        return 0.5 * x.pow(2)\n\n    def save_w(self):\n        torch.save(self.model.state_dict(), \'./saved_agents/model.dump\')\n        torch.save(self.optimizer.state_dict(), \'./saved_agents/optim.dump\')\n    \n    def load_w(self):\n        fname_model = ""./saved_agents/model.dump""\n        fname_optim = ""./saved_agents/optim.dump""\n\n        if os.path.isfile(fname_model):\n            self.model.load_state_dict(torch.load(fname_model))\n            self.target_model.load_state_dict(self.model.state_dict())\n\n        if os.path.isfile(fname_optim):\n            self.optimizer.load_state_dict(torch.load(fname_optim))\n\n    def save_replay(self):\n        pickle.dump(self.memory, open(\'./saved_agents/exp_replay_agent.dump\', \'wb\'))\n\n    def load_replay(self):\n        fname = \'./saved_agents/exp_replay_agent.dump\'\n        if os.path.isfile(fname):\n            self.memory = pickle.load(open(fname, \'rb\'))\n\n    def save_sigma_param_magnitudes(self, tstep):\n        with torch.no_grad():\n            sum_, count = 0.0, 0.0\n            for name, param in self.model.named_parameters():\n                if param.requires_grad and \'sigma\' in name:\n                    sum_+= torch.sum(param.abs()).item()\n                    count += np.prod(param.shape)\n            \n            if count > 0:\n                with open(os.path.join(self.log_dir, \'sig_param_mag.csv\'), \'a\') as f:\n                    writer = csv.writer(f)\n                    writer.writerow((tstep, sum_/count))\n\n    def save_td(self, td, tstep):\n        with open(os.path.join(self.log_dir, \'td.csv\'), \'a\') as f:\n            writer = csv.writer(f)\n            writer.writerow((tstep, td))\n\n    def save_reward(self, reward):\n        self.rewards.append(reward)\n\n    def save_action(self, action, tstep):\n        self.action_selections[int(action)] += 1.0/self.action_log_frequency\n        if (tstep+1) % self.action_log_frequency == 0:\n            with open(os.path.join(self.log_dir, \'action_log.csv\'), \'a\') as f:\n                writer = csv.writer(f)\n                writer.writerow(list([tstep]+self.action_selections))\n            self.action_selections = [0 for _ in range(len(self.action_selections))]\n'"
agents/Categorical_DQN.py,9,"b""import numpy as np\n\nimport torch\n\nfrom agents.DQN import Model as DQN_Agent\nfrom networks.network_bodies import SimpleBody, AtariBody\nfrom networks.networks import CategoricalDQN\n\n\nclass Model(DQN_Agent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        self.atoms = config.ATOMS\n        self.v_max = config.V_MAX\n        self.v_min = config.V_MIN\n        self.supports = torch.linspace(self.v_min, self.v_max, self.atoms).view(1, 1, self.atoms).to(config.device)\n        self.delta = (self.v_max - self.v_min) / (self.atoms - 1)\n\n        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n\n    def declare_networks(self):\n        self.model = CategoricalDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=self.noisy, sigma_init=self.sigma_init, atoms=self.atoms)\n        self.target_model = CategoricalDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=self.noisy, sigma_init=self.sigma_init, atoms=self.atoms)\n\n    def projection_distribution(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        with torch.no_grad():\n            max_next_dist = torch.zeros((self.batch_size, 1, self.atoms), device=self.device, dtype=torch.float) + 1. / self.atoms\n            if not empty_next_state_values:\n                max_next_action = self.get_max_next_state_action(non_final_next_states)\n                self.target_model.sample_noise()\n                max_next_dist[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n                max_next_dist = max_next_dist.squeeze()\n\n            Tz = batch_reward.view(-1, 1) + (self.gamma**self.nsteps) * self.supports.view(1, -1) * non_final_mask.to(torch.float).view(-1, 1)\n            Tz = Tz.clamp(self.v_min, self.v_max)\n            b = (Tz - self.v_min) / self.delta\n            l = b.floor().to(torch.int64)\n            u = b.ceil().to(torch.int64)\n            l[(u > 0) * (l == u)] -= 1\n            u[(l < (self.atoms - 1)) * (l == u)] += 1\n            \n            offset = torch.linspace(0, (self.batch_size - 1) * self.atoms, self.batch_size).unsqueeze(dim=1).expand(self.batch_size, self.atoms).to(batch_action)\n            m = batch_state.new_zeros(self.batch_size, self.atoms)\n            m.view(-1).index_add_(0, (l + offset).view(-1), (max_next_dist * (u.float() - b)).view(-1))  # m_l = m_l + p(s_t+n, a*)(u - b)\n            m.view(-1).index_add_(0, (u + offset).view(-1), (max_next_dist * (b - l.float())).view(-1))  # m_u = m_u + p(s_t+n, a*)(b - l)\n\n        return m\n    \n    def compute_loss(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        batch_action = batch_action.unsqueeze(dim=-1).expand(-1, -1, self.atoms)\n        batch_reward = batch_reward.view(-1, 1, 1)\n\n        # estimate\n        self.model.sample_noise()\n        current_dist = self.model(batch_state).gather(1, batch_action).squeeze()\n\n        target_prob = self.projection_distribution(batch_vars)\n          \n        loss = -(target_prob * current_dist.log()).sum(-1)\n        if self.priority_replay:\n            self.memory.update_priorities(indices, loss.detach().squeeze().abs().cpu().numpy().tolist())\n            loss = loss * weights\n        loss = loss.mean()\n\n        return loss\n\n    def get_action(self, s, eps):\n        with torch.no_grad():\n            if np.random.random() >= eps or self.static_policy or self.noisy:\n                X = torch.tensor([s], device=self.device, dtype=torch.float) \n                self.model.sample_noise()\n                a = self.model(X) * self.supports\n                a = a.sum(dim=2).max(1)[1].view(1, 1)\n                return a.item()\n            else:\n                return np.random.randint(0, self.num_actions)\n\n    def get_max_next_state_action(self, next_states):\n        next_dist = self.target_model(next_states) * self.supports\n        return next_dist.sum(dim=2).max(1)[1].view(next_states.size(0), 1, 1).expand(-1, -1, self.atoms)"""
agents/DQN.py,10,"b""import numpy as np\n\nimport torch\nimport torch.optim as optim\n\nfrom agents.BaseAgent import BaseAgent\nfrom networks.networks import DQN\nfrom networks.network_bodies import AtariBody, SimpleBody\nfrom utils.ReplayMemory import ExperienceReplayMemory, PrioritizedReplayMemory\n\nfrom timeit import default_timer as timer\n\nclass Model(BaseAgent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        super(Model, self).__init__(config=config, env=env, log_dir=log_dir)\n        self.device = config.device\n\n        self.noisy=config.USE_NOISY_NETS\n        self.priority_replay=config.USE_PRIORITY_REPLAY\n\n        self.gamma = config.GAMMA\n        self.lr = config.LR\n        self.target_net_update_freq = config.TARGET_NET_UPDATE_FREQ\n        self.experience_replay_size = config.EXP_REPLAY_SIZE\n        self.batch_size = config.BATCH_SIZE\n        self.learn_start = config.LEARN_START\n        self.update_freq = config.UPDATE_FREQ\n        self.sigma_init= config.SIGMA_INIT\n        self.priority_beta_start = config.PRIORITY_BETA_START\n        self.priority_beta_frames = config.PRIORITY_BETA_FRAMES\n        self.priority_alpha = config.PRIORITY_ALPHA\n\n        self.static_policy = static_policy\n        self.num_feats = env.observation_space.shape\n        self.num_actions = env.action_space.n\n        self.env = env\n\n        self.declare_networks()\n            \n        self.target_model.load_state_dict(self.model.state_dict())\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n        \n        #move to correct device\n        self.model = self.model.to(self.device)\n        self.target_model.to(self.device)\n\n        if self.static_policy:\n            self.model.eval()\n            self.target_model.eval()\n        else:\n            self.model.train()\n            self.target_model.train()\n\n        self.update_count = 0\n\n        self.declare_memory()\n\n        self.nsteps = config.N_STEPS\n        self.nstep_buffer = []\n\n    def declare_networks(self):\n        self.model = DQN(self.num_feats, self.num_actions, noisy=self.noisy, sigma_init=self.sigma_init, body=AtariBody)\n        self.target_model = DQN(self.num_feats, self.num_actions, noisy=self.noisy, sigma_init=self.sigma_init, body=AtariBody)\n\n    def declare_memory(self):\n        self.memory = ExperienceReplayMemory(self.experience_replay_size) if not self.priority_replay else PrioritizedReplayMemory(self.experience_replay_size, self.priority_alpha, self.priority_beta_start, self.priority_beta_frames)\n\n    def append_to_replay(self, s, a, r, s_):\n        self.nstep_buffer.append((s, a, r, s_))\n\n        if(len(self.nstep_buffer)<self.nsteps):\n            return\n        \n        R = sum([self.nstep_buffer[i][2]*(self.gamma**i) for i in range(self.nsteps)])\n        state, action, _, _ = self.nstep_buffer.pop(0)\n\n        self.memory.push((state, action, R, s_))\n\n    def prep_minibatch(self):\n        # random transition batch is taken from experience replay memory\n        transitions, indices, weights = self.memory.sample(self.batch_size)\n        \n        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n\n        shape = (-1,)+self.num_feats\n\n        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).squeeze().view(-1, 1)\n        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).squeeze().view(-1, 1)\n        \n        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n        try: #sometimes all next states are false\n            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).view(shape)\n            empty_next_state_values = False\n        except:\n            non_final_next_states = None\n            empty_next_state_values = True\n\n        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights\n\n    def compute_loss(self, batch_vars): #faster\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        #estimate\n        self.model.sample_noise()\n        current_q_values = self.model(batch_state).gather(1, batch_action)\n        \n        #target\n        with torch.no_grad():\n            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\n            if not empty_next_state_values:\n                max_next_action = self.get_max_next_state_action(non_final_next_states)\n                self.target_model.sample_noise()\n                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n            expected_q_values = batch_reward + ((self.gamma**self.nsteps)*max_next_q_values)\n\n        diff = (expected_q_values - current_q_values)\n        if self.priority_replay:\n            self.memory.update_priorities(indices, diff.detach().squeeze().abs().cpu().numpy().tolist())\n            loss = self.MSE(diff).squeeze() * weights\n        else:\n            loss = self.MSE(diff)\n        loss = loss.mean()\n\n        return loss\n\n    def update(self, s, a, r, s_, frame=0):\n        if self.static_policy:\n            return None\n\n        self.append_to_replay(s, a, r, s_)\n\n        if frame < self.learn_start or frame % self.update_freq != 0:\n            return None\n\n        batch_vars = self.prep_minibatch()\n\n        loss = self.compute_loss(batch_vars)\n\n        # Optimize the model\n        self.optimizer.zero_grad()\n        loss.backward()\n        for param in self.model.parameters():\n            param.grad.data.clamp_(-1, 1)\n        self.optimizer.step()\n\n        self.update_target_model()\n        self.save_td(loss.item(), frame)\n        self.save_sigma_param_magnitudes(frame)\n\n    def get_action(self, s, eps=0.1): #faster\n        with torch.no_grad():\n            if np.random.random() >= eps or self.static_policy or self.noisy:\n                X = torch.tensor([s], device=self.device, dtype=torch.float)\n                self.model.sample_noise()\n                a = self.model(X).max(1)[1].view(1, 1)\n                return a.item()\n            else:\n                return np.random.randint(0, self.num_actions)\n\n    def update_target_model(self):\n        self.update_count+=1\n        self.update_count = self.update_count % self.target_net_update_freq\n        if self.update_count == 0:\n            self.target_model.load_state_dict(self.model.state_dict())\n\n    def get_max_next_state_action(self, next_states):\n        return self.target_model(next_states).max(dim=1)[1].view(-1, 1)\n\n    def finish_nstep(self):\n        while len(self.nstep_buffer) > 0:\n            R = sum([self.nstep_buffer[i][2]*(self.gamma**i) for i in range(len(self.nstep_buffer))])\n            state, action, _, _ = self.nstep_buffer.pop(0)\n\n            self.memory.push((state, action, R, None))\n\n    def reset_hx(self):\n        pass"""
agents/DRQN.py,11,"b""import numpy as np\n\nimport torch\nimport torch.optim as optim\n\nfrom agents.DQN import Model as DQN_Agent\nfrom networks.networks import DRQN\nfrom utils.ReplayMemory import RecurrentExperienceReplayMemory\nfrom utils.hyperparameters import Config\nfrom networks.network_bodies import AtariBody, SimpleBody\n\nclass Model(DQN_Agent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        self.sequence_length=config.SEQUENCE_LENGTH\n\n        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n\n        self.reset_hx()\n    \n    def declare_networks(self):\n        self.model = DRQN(self.num_feats, self.num_actions, body=SimpleBody)\n        self.target_model = DRQN(self.num_feats, self.num_actions, body=SimpleBody)\n\n    def declare_memory(self):\n        self.memory = RecurrentExperienceReplayMemory(self.experience_replay_size, self.sequence_length)\n        #self.memory = ExperienceReplayMemory(self.experience_replay_size)\n\n    def prep_minibatch(self):\n        transitions, indices, weights = self.memory.sample(self.batch_size)\n\n        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n\n        shape = (self.batch_size,self.sequence_length)+self.num_feats\n\n        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).view(self.batch_size, self.sequence_length, -1)\n        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).view(self.batch_size, self.sequence_length)\n        #get set of next states for end of each sequence\n        batch_next_state = tuple([batch_next_state[i] for i in range(len(batch_next_state)) if (i+1)%(self.sequence_length)==0])\n\n        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n        try: #sometimes all next states are false, especially with nstep returns\n            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).unsqueeze(dim=1)\n            non_final_next_states = torch.cat([batch_state[non_final_mask, 1:, :], non_final_next_states], dim=1)\n            empty_next_state_values = False\n        except:\n            empty_next_state_values = True\n\n        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights\n    \n    def compute_loss(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        #estimate\n        current_q_values, _ = self.model(batch_state)\n        current_q_values = current_q_values.gather(2, batch_action).squeeze()\n        \n        #target\n        with torch.no_grad():\n            max_next_q_values = torch.zeros((self.batch_size, self.sequence_length), device=self.device, dtype=torch.float)\n            if not empty_next_state_values:\n                max_next, _ = self.target_model(non_final_next_states)\n                max_next_q_values[non_final_mask] = max_next.max(dim=2)[0]\n            expected_q_values = batch_reward + ((self.gamma**self.nsteps)*max_next_q_values)\n\n        diff = (expected_q_values - current_q_values)\n        loss = self.huber(diff)\n        loss = loss.mean()\n\n        return loss\n\n    def get_action(self, s, eps=0.1):\n        with torch.no_grad():\n            self.seq.pop(0)\n            self.seq.append(s)\n            if np.random.random() >= eps or self.static_policy or self.noisy:\n                X = torch.tensor([self.seq], device=self.device, dtype=torch.float) \n                self.model.sample_noise()\n                a, _ = self.model(X)\n                a = a[:, -1, :] #select last element of seq\n                a = a.max(1)[1]\n                return a.item()\n            else:\n                return np.random.randint(0, self.num_actions)\n\n    #def get_max_next_state_action(self, next_states, hx):\n    #    max_next, _ = self.target_model(next_states, hx)\n    #    return max_next.max(dim=1)[1].view(-1, 1)'''\n\n    def reset_hx(self):\n        #self.action_hx = self.model.init_hidden(1)\n        self.seq = [np.zeros(self.num_feats) for j in range(self.sequence_length)]\n\n    """
agents/Double_DQN.py,0,"b""import numpy as np\n\nimport torch\n\nfrom agents.DQN import Model as DQN_Agent\n\nclass Model(DQN_Agent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n\n    def get_max_next_state_action(self, next_states):\n        return self.model(next_states).max(dim=1)[1].view(-1, 1) \n"""
agents/Dueling_DQN.py,0,"b""import numpy as np\n\nimport torch\n\nfrom agents.DQN import Model as DQN_Agent\nfrom networks.networks import DuelingDQN\n\nclass Model(DQN_Agent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n\n    def declare_networks(self):\n        self.model = DuelingDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=self.noisy, sigma_init=self.sigma_init)\n        self.target_model = DuelingDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=self.noisy, sigma_init=self.sigma_init)"""
agents/PPO.py,7,"b'import numpy as np\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom agents.A2C import Model as A2C\n\n\nclass Model(A2C):\n    def __init__(self, static_policy=False, env=None, config=None):\n        super(Model, self).__init__(static_policy, env, config)\n        \n        self.num_agents = config.num_agents\n        self.value_loss_weight = config.value_loss_weight\n        self.entropy_loss_weight = config.entropy_loss_weight\n        self.rollout = config.rollout\n        self.grad_norm_max = config.grad_norm_max\n\n        self.ppo_epoch = config.ppo_epoch\n        self.num_mini_batch = config.num_mini_batch\n        self.clip_param = config.ppo_clip_param\n\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, eps=1e-5)\n\n    def compute_loss(self, sample):\n        observations_batch, actions_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ = sample\n\n        values, action_log_probs, dist_entropy = self.evaluate_actions(observations_batch, actions_batch)\n\n        ratio = torch.exp(action_log_probs - old_action_log_probs_batch)\n        surr1 = ratio * adv_targ\n        surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv_targ\n        action_loss = -torch.min(surr1, surr2).mean()\n\n        value_loss = F.mse_loss(return_batch, values)\n\n        loss = action_loss + self.value_loss_weight * value_loss - self.entropy_loss_weight * dist_entropy\n\n        return loss, action_loss, value_loss, dist_entropy\n\n    def update(self, rollout):\n        advantages = rollout.returns[:-1] - rollout.value_preds[:-1]\n        advantages = (advantages - advantages.mean()) / (\n            advantages.std() + 1e-5)\n\n\n        value_loss_epoch = 0\n        action_loss_epoch = 0\n        dist_entropy_epoch = 0\n\n        for e in range(self.ppo_epoch):\n            data_generator = rollout.feed_forward_generator(\n                advantages, self.num_mini_batch)\n\n            for sample in data_generator:\n                loss, action_loss, value_loss, dist_entropy = self.compute_loss(sample)\n\n                self.optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_norm_max)\n                self.optimizer.step()\n\n                value_loss_epoch += value_loss.item()\n                action_loss_epoch += action_loss.item()\n                dist_entropy_epoch += dist_entropy.item()\n        \n        value_loss_epoch /= (self.ppo_epoch * self.num_mini_batch)\n        action_loss_epoch /= (self.ppo_epoch * self.num_mini_batch)\n        dist_entropy_epoch /= (self.ppo_epoch * self.num_mini_batch)\n        total_loss = value_loss_epoch + action_loss_epoch + dist_entropy_epoch\n\n        #self.save_loss(total_loss, action_loss_epoch, value_loss_epoch, dist_entropy_epoch)\n        #self.save_sigma_param_magnitudes()\n\n        return action_loss_epoch, value_loss_epoch, dist_entropy_epoch'"
agents/QuantileRegression_DQN.py,6,"b""import numpy as np\n\nimport torch\n\nfrom agents.DQN import Model as DQN_Agent\nfrom networks.network_bodies import SimpleBody, AtariBody\nfrom networks.networks import QRDQN\n\nclass Model(DQN_Agent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        self.num_quantiles = config.QUANTILES\n        self.cumulative_density = torch.tensor((2 * np.arange(self.num_quantiles) + 1) / (2.0 * self.num_quantiles), device=config.device, dtype=torch.float) \n        self.quantile_weight = 1.0 / self.num_quantiles\n\n        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n    \n    \n    def declare_networks(self):\n        self.model = QRDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=self.noisy, sigma_init=self.sigma_init, quantiles=self.num_quantiles)\n        self.target_model = QRDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=self.noisy, sigma_init=self.sigma_init, quantiles=self.num_quantiles)\n\n    def next_distribution(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        with torch.no_grad():\n            quantiles_next = torch.zeros((self.batch_size, self.num_quantiles), device=self.device, dtype=torch.float)\n            if not empty_next_state_values:\n                self.target_model.sample_noise()\n                max_next_action = self.get_max_next_state_action(non_final_next_states)\n                quantiles_next[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action).squeeze(dim=1)\n\n            quantiles_next = batch_reward + (self.gamma*quantiles_next)\n\n        return quantiles_next\n    \n    def compute_loss(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        batch_action = batch_action.unsqueeze(dim=-1).expand(-1, -1, self.num_quantiles)\n\n        #estimate\n        self.model.sample_noise()\n        quantiles = self.model(batch_state)\n        quantiles = quantiles.gather(1, batch_action).squeeze(1)\n\n        quantiles_next = self.next_distribution(batch_vars)\n          \n        diff = quantiles_next.t().unsqueeze(-1) - quantiles.unsqueeze(0)\n\n        loss = self.huber(diff) * torch.abs(self.cumulative_density.view(1, -1) - (diff < 0).to(torch.float))\n        loss = loss.transpose(0,1)\n        if self.priority_replay:\n            self.memory.update_priorities(indices, loss.detach().mean(1).sum(-1).abs().cpu().numpy().tolist())\n            loss = loss * weights.view(self.batch_size, 1, 1)\n        loss = loss.mean(1).sum(-1).mean()\n\n        return loss\n\n    def get_action(self, s, eps):\n        with torch.no_grad():\n            if np.random.random() >= eps or self.static_policy or self.noisy:\n                X = torch.tensor([s], device=self.device, dtype=torch.float) \n                self.model.sample_noise()\n                a = (self.model(X) * self.quantile_weight).sum(dim=2).max(dim=1)[1]\n                return a.item()\n            else:\n                return np.random.randint(0, self.num_actions)\n\n    def get_max_next_state_action(self, next_states):\n        next_dist = self.target_model(next_states) * self.quantile_weight\n        return next_dist.sum(dim=2).max(1)[1].view(next_states.size(0), 1, 1).expand(-1, -1, self.num_quantiles)"""
agents/Quantile_Rainbow.py,6,"b""import numpy as np\n\nimport torch\n\nfrom agents.DQN import Model as DQN_Agent\nfrom networks.network_bodies import SimpleBody, AtariBody\nfrom networks.networks import DuelingQRDQN\nfrom utils.ReplayMemory import PrioritizedReplayMemory\n\nclass Model(DQN_Agent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        self.num_quantiles = config.QUANTILES\n        self.cumulative_density = torch.tensor((2 * np.arange(self.num_quantiles) + 1) / (2.0 * self.num_quantiles), device=config.device, dtype=torch.float) \n        self.quantile_weight = 1.0 / self.num_quantiles\n\n        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n\n        self.nsteps=max(self.nsteps, 3)\n    \n    \n    def declare_networks(self):\n        self.model = DuelingQRDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=True, sigma_init=self.sigma_init, quantiles=self.num_quantiles)\n        self.target_model = DuelingQRDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=True, sigma_init=self.sigma_init, quantiles=self.num_quantiles)\n\n    def declare_memory(self):\n        self.memory = PrioritizedReplayMemory(self.experience_replay_size, self.priority_alpha, self.priority_beta_start, self.priority_beta_frames)\n\n    def next_distribution(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        with torch.no_grad():\n            quantiles_next = torch.zeros((self.batch_size, self.num_quantiles), device=self.device, dtype=torch.float)\n            if not empty_next_state_values:\n                self.target_model.sample_noise()\n                max_next_action = self.get_max_next_state_action(non_final_next_states)\n                quantiles_next[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action).squeeze(dim=1)\n\n            quantiles_next = batch_reward + ((self.gamma**self.nsteps)*quantiles_next)\n\n        return quantiles_next\n    \n    def compute_loss(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        batch_action = batch_action.unsqueeze(dim=-1).expand(-1, -1, self.num_quantiles)\n\n        self.model.sample_noise()\n        quantiles = self.model(batch_state)\n        quantiles = quantiles.gather(1, batch_action).squeeze(1)\n\n        quantiles_next = self.next_distribution(batch_vars)\n          \n        diff = quantiles_next.t().unsqueeze(-1) - quantiles.unsqueeze(0)\n\n        loss = self.huber(diff) * torch.abs(self.cumulative_density.view(1, -1) - (diff < 0).to(torch.float))\n        loss = loss.transpose(0,1)\n        self.memory.update_priorities(indices, loss.detach().mean(1).sum(-1).abs().cpu().numpy().tolist())\n        loss = loss * weights.view(self.batch_size, 1, 1)\n        loss = loss.mean(1).sum(-1).mean()\n\n        return loss\n\n    def get_action(self, s, eps):\n        with torch.no_grad():\n            X = torch.tensor([s], device=self.device, dtype=torch.float) \n            self.model.sample_noise()\n            a = (self.model(X) * self.quantile_weight).sum(dim=2).max(dim=1)[1]\n            return a.item()\n\n    def get_max_next_state_action(self, next_states):\n        next_dist = self.model(next_states) * self.quantile_weight\n        return next_dist.sum(dim=2).max(1)[1].view(next_states.size(0), 1, 1).expand(-1, -1, self.num_quantiles)"""
agents/Rainbow.py,9,"b""import numpy as np\n\nimport torch\n\nfrom agents.DQN import Model as DQN_Agent\nfrom networks.networks import CategoricalDuelingDQN, CategoricalDQN\nfrom utils.ReplayMemory import PrioritizedReplayMemory\n\nclass Model(DQN_Agent):\n    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n        self.atoms=config.ATOMS\n        self.v_max=config.V_MAX\n        self.v_min=config.V_MIN\n        self.supports = torch.linspace(self.v_min, self.v_max, self.atoms).view(1, 1, self.atoms).to(config.device)\n        self.delta = (self.v_max - self.v_min) / (self.atoms - 1)\n\n        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n\n        self.nsteps=max(self.nsteps,3)\n    \n    def declare_networks(self):\n        self.model = CategoricalDuelingDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=True, sigma_init=self.sigma_init, atoms=self.atoms)\n        self.target_model = CategoricalDuelingDQN(self.env.observation_space.shape, self.env.action_space.n, noisy=True, sigma_init=self.sigma_init, atoms=self.atoms)\n\n    def declare_memory(self):\n        self.memory = PrioritizedReplayMemory(self.experience_replay_size, self.priority_alpha, self.priority_beta_start, self.priority_beta_frames)\n\n    def projection_distribution(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        with torch.no_grad():\n            max_next_dist = torch.zeros((self.batch_size, 1, self.atoms), device=self.device, dtype=torch.float) + 1./self.atoms\n            if not empty_next_state_values:\n                max_next_action = self.get_max_next_state_action(non_final_next_states)\n                self.target_model.sample_noise()\n                max_next_dist[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n                max_next_dist = max_next_dist.squeeze()\n\n\n            Tz = batch_reward.view(-1, 1) + (self.gamma**self.nsteps)*self.supports.view(1, -1) * non_final_mask.to(torch.float).view(-1, 1)\n            Tz = Tz.clamp(self.v_min, self.v_max)\n            b = (Tz - self.v_min) / self.delta\n            l = b.floor().to(torch.int64)\n            u = b.ceil().to(torch.int64)\n            l[(u > 0) * (l == u)] -= 1\n            u[(l < (self.atoms - 1)) * (l == u)] += 1\n            \n\n            offset = torch.linspace(0, (self.batch_size - 1) * self.atoms, self.batch_size).unsqueeze(dim=1).expand(self.batch_size, self.atoms).to(batch_action)\n            m = batch_state.new_zeros(self.batch_size, self.atoms)\n            m.view(-1).index_add_(0, (l + offset).view(-1), (max_next_dist * (u.float() - b)).view(-1))  # m_l = m_l + p(s_t+n, a*)(u - b)\n            m.view(-1).index_add_(0, (u + offset).view(-1), (max_next_dist * (b - l.float())).view(-1))  # m_u = m_u + p(s_t+n, a*)(b - l)\n\n        return m\n    \n    def compute_loss(self, batch_vars):\n        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n\n        batch_action = batch_action.unsqueeze(dim=-1).expand(-1, -1, self.atoms)\n        batch_reward = batch_reward.view(-1, 1, 1)\n\n        #estimate\n        self.model.sample_noise()\n        current_dist = self.model(batch_state).gather(1, batch_action).squeeze()\n\n        target_prob = self.projection_distribution(batch_vars)\n          \n        loss = -(target_prob * current_dist.log()).sum(-1)\n        self.memory.update_priorities(indices, loss.detach().squeeze().abs().cpu().numpy().tolist())\n        loss = loss * weights\n        loss = loss.mean()\n\n        return loss\n\n    def get_action(self, s, eps):\n        with torch.no_grad():\n            X = torch.tensor([s], device=self.device, dtype=torch.float)\n            self.model.sample_noise()\n            a = self.model(X) * self.supports\n            a = a.sum(dim=2).max(1)[1].view(1, 1)\n            return a.item()\n\n    def get_max_next_state_action(self, next_states):\n        next_dist = self.model(next_states) * self.supports\n        return next_dist.sum(dim=2).max(1)[1].view(next_states.size(0), 1, 1).expand(-1, -1, self.atoms)\n\n"""
agents/__init__.py,0,b''
networks/__init__.py,0,b''
networks/layers.py,11,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# Factorised NoisyLinear layer with bias\nclass NoisyLinear(nn.Module):\n    def __init__(self, in_features, out_features, std_init=0.4, factorised_noise=True):\n        super(NoisyLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.std_init = std_init\n        self.factorised_noise = factorised_noise\n        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n        self.bias_mu = nn.Parameter(torch.empty(out_features))\n        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n        self.register_buffer('bias_epsilon', torch.empty(out_features))\n        self.reset_parameters()\n        self.sample_noise()\n\n    def reset_parameters(self):\n        mu_range = 1.0 / math.sqrt(self.in_features)\n        self.weight_mu.data.uniform_(-mu_range, mu_range)\n        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n        self.bias_mu.data.uniform_(-mu_range, mu_range)\n        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n\n    def _scale_noise(self, size):\n        x = torch.randn(size)\n        return x.sign().mul_(x.abs().sqrt_())\n\n    def sample_noise(self):\n        if self.factorised_noise:\n            epsilon_in = self._scale_noise(self.in_features)\n            epsilon_out = self._scale_noise(self.out_features)\n            self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n            self.bias_epsilon.copy_(epsilon_out)\n        else:\n            self.weight_epsilon.copy_(torch.randn((self.out_features, self.in_features)))\n            self.bias_epsilon.copy_(torch.randn(self.out_features))\n\n    def forward(self, inp):\n        if self.training:\n            return F.linear(inp, self.weight_mu + self.weight_sigma * self.weight_epsilon, self.bias_mu + self.bias_sigma * self.bias_epsilon)\n        else:\n            return F.linear(inp, self.weight_mu, self.bias_mu)"""
networks/network_bodies.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom networks.layers import NoisyLinear\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\nclass AtariBody(nn.Module):\n    def __init__(self, input_shape, num_actions, noisy=False, sigma_init=0.5):\n        super(AtariBody, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_actions\n        self.noisy=noisy\n\n        self.conv1 = nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, stride=1)\n\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)\n\n        return x\n    \n    def feature_size(self):\n        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)\n\n    def sample_noise(self):\n        pass\n\n\nclass SimpleBody(nn.Module):\n    def __init__(self, input_shape, num_actions, noisy=False, sigma_init=0.5):\n        super(SimpleBody, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_actions\n        self.noisy=noisy\n\n        self.fc1 = nn.Linear(input_shape[0], 128) if not self.noisy else NoisyLinear(input_shape[0], 128, sigma_init)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return x\n\n    def feature_size(self):\n        return self.fc1(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)\n\n    def sample_noise(self):\n        if self.noisy:\n            self.fc1.sample_noise()'"
networks/networks.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom networks.layers import NoisyLinear\nfrom networks.network_bodies import SimpleBody, AtariBody\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\nclass DQN(nn.Module):\n    def __init__(self, input_shape, num_actions, noisy=False, sigma_init=0.5, body=SimpleBody):\n        super(DQN, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_actions\n        self.noisy=noisy\n\n        self.body = body(input_shape, num_actions, noisy, sigma_init)\n\n        self.fc1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.fc2 = nn.Linear(512, self.num_actions) if not self.noisy else NoisyLinear(512, self.num_actions, sigma_init)\n        \n    def forward(self, x):\n        x = self.body(x)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n\n        return x\n\n    def sample_noise(self):\n        if self.noisy:\n            self.body.sample_noise()\n            self.fc1.sample_noise()\n            self.fc2.sample_noise()\n\n\nclass DuelingDQN(nn.Module):\n    def __init__(self, input_shape, num_outputs, noisy=False, sigma_init=0.5, body=SimpleBody):\n        super(DuelingDQN, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_outputs\n        self.noisy=noisy\n\n        self.body = body(input_shape, num_outputs, noisy, sigma_init)\n\n        self.adv1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.adv2 = nn.Linear(512, self.num_actions) if not self.noisy else NoisyLinear(512, self.num_actions, sigma_init)\n\n        self.val1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.val2 = nn.Linear(512, 1) if not self.noisy else NoisyLinear(512, 1, sigma_init)\n        \n    def forward(self, x):\n        x = self.body(x)\n\n        adv = F.relu(self.adv1(x))\n        adv = self.adv2(adv)\n\n        val = F.relu(self.val1(x))\n        val = self.val2(val)\n\n        return val + adv - adv.mean()\n    \n    def sample_noise(self):\n        if self.noisy:\n            self.body.sample_noise()\n            self.adv1.sample_noise()\n            self.adv2.sample_noise()\n            self.val1.sample_noise()\n            self.val2.sample_noise()\n\nclass CategoricalDQN(nn.Module):\n    def __init__(self, input_shape, num_outputs, noisy=False, sigma_init=0.5, body=SimpleBody, atoms=51):\n        super(CategoricalDQN, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_outputs\n        self.noisy=noisy\n        self.atoms=atoms\n\n        self.body = body(input_shape, num_outputs, noisy, sigma_init)\n\n        self.fc1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.fc2 = nn.Linear(512, self.num_actions*self.atoms) if not self.noisy else NoisyLinear(512, self.num_actions*self.atoms, sigma_init)\n\n        \n    def forward(self, x):\n        x = self.body(x)\n\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n\n        return F.softmax(x.view(-1, self.num_actions, self.atoms), dim=2)\n    \n    def sample_noise(self):\n        if self.noisy:\n            self.body.sample_noise()\n            self.fc1.sample_noise()\n            self.fc2.sample_noise()\n\nclass CategoricalDuelingDQN(nn.Module):\n    def __init__(self, input_shape, num_outputs, noisy=False, sigma_init=0.5, body=SimpleBody, atoms=51):\n        super(CategoricalDuelingDQN, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_outputs\n        self.noisy=noisy\n        self.atoms=atoms\n\n        self.body = body(input_shape, num_outputs, noisy, sigma_init)\n\n        self.adv1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.adv2 = nn.Linear(512, self.num_actions*self.atoms) if not self.noisy else NoisyLinear(512, self.num_actions*self.atoms, sigma_init)\n\n        self.val1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.val2 = nn.Linear(512, 1*self.atoms) if not self.noisy else NoisyLinear(512, 1*self.atoms, sigma_init)\n\n        \n    def forward(self, x):\n        x = self.body(x)\n\n        adv = F.relu(self.adv1(x))\n        adv = self.adv2(adv).view(-1, self.num_actions, self.atoms)\n\n        val = F.relu(self.val1(x))\n        val = self.val2(val).view(-1, 1, self.atoms)\n\n        final = val + adv - adv.mean(dim=1).view(-1, 1, self.atoms)\n\n        return F.softmax(final, dim=2)\n    \n    def sample_noise(self):\n        if self.noisy:\n            self.body.sample_noise()\n            self.adv1.sample_noise()\n            self.adv2.sample_noise()\n            self.val1.sample_noise()\n            self.val2.sample_noise()\n\n\nclass QRDQN(nn.Module):\n    def __init__(self, input_shape, num_outputs, noisy=False, sigma_init=0.5, body=SimpleBody, quantiles=51):\n        super(QRDQN, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_outputs\n        self.noisy=noisy\n        self.quantiles=quantiles\n\n        self.body = body(input_shape, num_outputs, noisy, sigma_init)\n\n        self.fc1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.fc2 = nn.Linear(512, self.num_actions*self.quantiles) if not self.noisy else NoisyLinear(512, self.num_actions*self.quantiles, sigma_init)\n\n        \n    def forward(self, x):\n        x = self.body(x)\n\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n\n        return x.view(-1, self.num_actions, self.quantiles)\n    \n    def sample_noise(self):\n        if self.noisy:\n            self.body.sample_noise()\n            self.fc1.sample_noise()\n            self.fc2.sample_noise()\n\n\nclass DuelingQRDQN(nn.Module):\n    def __init__(self, input_shape, num_outputs, noisy=False, sigma_init=0.5, body=SimpleBody, quantiles=51):\n        super(DuelingQRDQN, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_outputs\n        self.noisy=noisy\n        self.quantiles=quantiles\n\n        self.body = body(input_shape, num_outputs, noisy, sigma_init)\n\n        self.adv1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.adv2 = nn.Linear(512, self.num_actions*self.quantiles) if not self.noisy else NoisyLinear(512, self.num_actions*self.quantiles, sigma_init)\n\n        self.val1 = nn.Linear(self.body.feature_size(), 512) if not self.noisy else NoisyLinear(self.body.feature_size(), 512, sigma_init)\n        self.val2 = nn.Linear(512, 1*self.quantiles) if not self.noisy else NoisyLinear(512, 1*self.quantiles, sigma_init)\n\n        \n    def forward(self, x):\n        x = self.body(x)\n\n        adv = F.relu(self.adv1(x))\n        adv = self.adv2(adv).view(-1, self.num_actions, self.quantiles)\n\n        val = F.relu(self.val1(x))\n        val = self.val2(val).view(-1, 1, self.quantiles)\n\n        final = val + adv - adv.mean(dim=1).view(-1, 1, self.quantiles)\n\n        return final\n    \n    def sample_noise(self):\n        if self.noisy:\n            self.body.sample_noise()\n            self.adv1.sample_noise()\n            self.adv2.sample_noise()\n            self.val1.sample_noise()\n            self.val2.sample_noise()\n\n\n########Recurrent Architectures#########\n\nclass DRQN(nn.Module):\n    def __init__(self, input_shape, num_actions, noisy=False, sigma_init=0.5, gru_size=512, bidirectional=False, body=SimpleBody):\n        super(DRQN, self).__init__()\n        \n        self.input_shape = input_shape\n        self.num_actions = num_actions\n        self.noisy = noisy\n        self.gru_size = gru_size\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if self.bidirectional else 1\n\n        self.body = body(input_shape, num_actions, noisy=self.noisy, sigma_init=sigma_init)\n        self.gru = nn.GRU(self.body.feature_size(), self.gru_size, num_layers=1, batch_first=True, bidirectional=bidirectional)\n        self.fc2 = nn.Linear(self.gru_size, self.num_actions) if not self.noisy else NoisyLinear(self.gru_size, self.num_actions, sigma_init)\n        \n    def forward(self, x, hx=None):\n        batch_size = x.size(0)\n        sequence_length = x.size(1)\n        \n        x = x.view((-1,)+self.input_shape)\n        \n        #format outp for batch first gru\n        feats = self.body(x).view(batch_size, sequence_length, -1)\n        hidden = self.init_hidden(batch_size) if hx is None else hx\n        out, hidden = self.gru(feats, hidden)\n        x = self.fc2(out)\n\n        return x, hidden\n\n    def init_hidden(self, batch_size):\n        return torch.zeros(1*self.num_directions, batch_size, self.gru_size, device=device, dtype=torch.float)\n    \n    def sample_noise(self):\n        if self.noisy:\n            self.body.sample_noise()\n            self.fc2.sample_noise()\n\n\n########Actor Critic Architectures#########\nclass ActorCritic(nn.Module):\n    def __init__(self, input_shape, num_actions):\n        super(ActorCritic, self).__init__()\n\n        init_ = lambda m: self.layer_init(m, nn.init.orthogonal_,\n                    lambda x: nn.init.constant_(x, 0),\n                    nn.init.calculate_gain(\'relu\'))\n\n        self.conv1 = init_(nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4))\n        self.conv2 = init_(nn.Conv2d(32, 64, kernel_size=4, stride=2))\n        self.conv3 = init_(nn.Conv2d(64, 32, kernel_size=3, stride=1))\n        self.fc1 = init_(nn.Linear(self.feature_size(input_shape), 512))\n\n        init_ = lambda m: self.layer_init(m, nn.init.orthogonal_,\n          lambda x: nn.init.constant_(x, 0))\n\n        self.critic_linear = init_(nn.Linear(512, 1))\n\n        init_ = lambda m: self.layer_init(m, nn.init.orthogonal_,\n              lambda x: nn.init.constant_(x, 0), gain=0.01)\n\n        self.actor_linear = init_(nn.Linear(512, num_actions))\n\n        self.train()\n\n    def forward(self, inputs):\n        x = F.relu(self.conv1(inputs/255.0))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)\n\n        x = F.relu(self.fc1(x))\n\n        value = self.critic_linear(x)\n        logits = self.actor_linear(x)\n\n        return logits, value\n\n    def feature_size(self, input_shape):\n        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *input_shape)))).view(1, -1).size(1)\n\n    def layer_init(self, module, weight_init, bias_init, gain=1):\n        weight_init(module.weight.data, gain=gain)\n        bias_init(module.bias.data)\n        return module\n\n    \nclass ActorCriticER(nn.Module):\n    def __init__(self, input_shape, num_actions):\n        super(ActorCriticER, self).__init__()\n\n        init_ = lambda m: self.layer_init(m, nn.init.orthogonal_,\n                    lambda x: nn.init.constant_(x, 0),\n                    nn.init.calculate_gain(\'relu\'))\n\n        self.conv1 = init_(nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4))\n        self.conv2 = init_(nn.Conv2d(32, 64, kernel_size=4, stride=2))\n        self.conv3 = init_(nn.Conv2d(64, 32, kernel_size=3, stride=1))\n        self.fc1 = init_(nn.Linear(self.feature_size(input_shape), 512))\n\n        init_ = lambda m: self.layer_init(m, nn.init.orthogonal_,\n          lambda x: nn.init.constant_(x, 0))\n\n        self.critic_linear = init_(nn.Linear(512, num_actions))\n\n        init_ = lambda m: self.layer_init(m, nn.init.orthogonal_,\n              lambda x: nn.init.constant_(x, 0), gain=0.01)\n\n        self.actor_linear = init_(nn.Linear(512, num_actions))\n\n        self.train()\n\n    def forward(self, inputs):\n        x = F.relu(self.conv1(inputs/255.0))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)\n\n        x = F.relu(self.fc1(x))\n\n        q_value = self.critic_linear(x)\n        logits = self.actor_linear(x)\n        policy = F.softmax(logits, dim=1) \n        value = (policy * q_value).sum(-1, keepdim=True)\n\n        return logits, policy, value, q_value\n\n    def feature_size(self, input_shape):\n        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *input_shape)))).view(1, -1).size(1)\n\n    def layer_init(self, module, weight_init, bias_init, gain=1):\n        weight_init(module.weight.data, gain=gain)\n        bias_init(module.bias.data)\n        return module'"
saved_agents/__init__.py,0,b''
utils/ReplayMemory.py,3,"b'import random\nimport numpy as np\nimport torch\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nfrom utils.data_structures import SegmentTree, MinSegmentTree, SumSegmentTree\n\n\nclass ExperienceReplayMemory:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n\n    def push(self, transition):\n        self.memory.append(transition)\n        if len(self.memory) > self.capacity:\n            del self.memory[0]\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size), None, None\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass PrioritizedReplayMemory(object):\n    def __init__(self, size, alpha=0.6, beta_start=0.4, beta_frames=100000):\n        """"""Create Prioritized Replay buffer.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        alpha: float\n            how much prioritization is used\n            (0 - no prioritization, 1 - full prioritization)\n        See Also\n        --------\n        ReplayBuffer.__init__\n        """"""\n        super(PrioritizedReplayMemory, self).__init__()\n        self._storage = []\n        self._maxsize = size\n        self._next_idx = 0\n\n        assert alpha >= 0\n        self._alpha = alpha\n\n        self.beta_start = beta_start\n        self.beta_frames = beta_frames\n        self.frame=1\n\n        it_capacity = 1\n        while it_capacity < size:\n            it_capacity *= 2\n\n        self._it_sum = SumSegmentTree(it_capacity)\n        self._it_min = MinSegmentTree(it_capacity)\n        self._max_priority = 1.0\n\n    def beta_by_frame(self, frame_idx):\n        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n\n    def push(self, data):\n        """"""See ReplayBuffer.store_effect""""""\n        idx = self._next_idx\n\n        if self._next_idx >= len(self._storage):\n            self._storage.append(data)\n        else:\n            self._storage[self._next_idx] = data\n        self._next_idx = (self._next_idx + 1) % self._maxsize\n\n\n        self._it_sum[idx] = self._max_priority ** self._alpha\n        self._it_min[idx] = self._max_priority ** self._alpha\n\n    def _encode_sample(self, idxes):\n        return [self._storage[i] for i in idxes]\n\n    def _sample_proportional(self, batch_size):\n        res = []\n        for _ in range(batch_size):\n            # TODO(szymon): should we ensure no repeats?\n            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n            idx = self._it_sum.find_prefixsum_idx(mass)\n            res.append(idx)\n        return res\n\n    def sample(self, batch_size):\n        """"""Sample a batch of experiences.\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        weights: np.array\n            Array of shape (batch_size,) and dtype np.float32\n            denoting importance weight of each sampled transition\n        idxes: np.array\n            Array of shape (batch_size,) and dtype np.int32\n            idexes in buffer of sampled experiences\n        """"""\n\n        idxes = self._sample_proportional(batch_size)\n\n        weights = []\n\n        #find smallest sampling prob: p_min = smallest priority^alpha / sum of priorities^alpha\n        p_min = self._it_min.min() / self._it_sum.sum()\n\n        beta = self.beta_by_frame(self.frame)\n        self.frame+=1\n        \n        #max_weight given to smallest prob\n        max_weight = (p_min * len(self._storage)) ** (-beta)\n\n        for idx in idxes:\n            p_sample = self._it_sum[idx] / self._it_sum.sum()\n            weight = (p_sample * len(self._storage)) ** (-beta)\n            weights.append(weight / max_weight)\n        weights = torch.tensor(weights, device=device, dtype=torch.float) \n        encoded_sample = self._encode_sample(idxes)\n        return encoded_sample, idxes, weights\n\n    def update_priorities(self, idxes, priorities):\n        """"""Update priorities of sampled transitions.\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`.\n        """"""\n        assert len(idxes) == len(priorities)\n        for idx, priority in zip(idxes, priorities):\n            assert 0 <= idx < len(self._storage)\n            self._it_sum[idx] = (priority+1e-5) ** self._alpha\n            self._it_min[idx] = (priority+1e-5) ** self._alpha\n\n            self._max_priority = max(self._max_priority, (priority+1e-5))\n\n\nclass RecurrentExperienceReplayMemory:\n    def __init__(self, capacity, sequence_length=10):\n        self.capacity = capacity\n        self.memory = []\n        self.seq_length=sequence_length\n\n    def push(self, transition):\n        self.memory.append(transition)\n        if len(self.memory) > self.capacity:\n            del self.memory[0]\n\n    def sample(self, batch_size):\n        finish = random.sample(range(0, len(self.memory)), batch_size)\n        begin = [x-self.seq_length for x in finish]\n        samp = []\n        for start, end in zip(begin, finish):\n            #correct for sampling near beginning\n            final = self.memory[max(start+1,0):end+1]\n            \n            #correct for sampling across episodes\n            for i in range(len(final)-2, -1, -1):\n                if final[i][3] is None:\n                    final = final[i+1:]\n                    break\n                    \n            #pad beginning to account for corrections\n            while(len(final)<self.seq_length):\n                final = [(np.zeros_like(self.memory[0][0]), 0, 0, np.zeros_like(self.memory[0][3]))] + final\n                            \n            samp+=final\n\n        #returns flattened version\n        return samp, None, None\n\n    def __len__(self):\n        return len(self.memory)\n\n\n\'\'\'class PrioritizedReplayMemory(object):  \n    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n        self.prob_alpha = alpha\n        self.capacity   = capacity\n        self.buffer     = []\n        self.pos        = 0\n        self.priorities = np.zeros((capacity,), dtype=np.float32)\n        self.frame = 1\n        self.beta_start = beta_start\n        self.beta_frames = beta_frames\n\n    def beta_by_frame(self, frame_idx):\n        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n    \n    def push(self, transition):\n        max_prio = self.priorities.max() if self.buffer else 1.0**self.prob_alpha\n        \n        if len(self.buffer) < self.capacity:\n            self.buffer.append(transition)\n        else:\n            self.buffer[self.pos] = transition\n        \n        self.priorities[self.pos] = max_prio\n\n        self.pos = (self.pos + 1) % self.capacity\n    \n    def sample(self, batch_size):\n        if len(self.buffer) == self.capacity:\n            prios = self.priorities\n        else:\n            prios = self.priorities[:self.pos]\n\n        total = len(self.buffer)\n\n        probs = prios / prios.sum()\n\n        indices = np.random.choice(total, batch_size, p=probs)\n        samples = [self.buffer[idx] for idx in indices]\n        \n        beta = self.beta_by_frame(self.frame)\n        self.frame+=1\n\n        #min of ALL probs, not just sampled probs\n        prob_min = probs.min()\n        max_weight = (prob_min*total)**(-beta)\n\n        weights  = (total * probs[indices]) ** (-beta)\n        weights /= max_weight\n        weights  = torch.tensor(weights, device=device, dtype=torch.float)\n        \n        return samples, indices, weights\n    \n    def update_priorities(self, batch_indices, batch_priorities):\n        for idx, prio in zip(batch_indices, batch_priorities):\n            self.priorities[idx] = (prio + 1e-5)**self.prob_alpha\n\n    def __len__(self):\n        return len(self.buffer)\'\'\'\n\n'"
utils/RolloutStorage.py,8,"b'import torch\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n\nclass RolloutStorage(object):\n    def __init__(self, num_steps, num_processes, obs_shape, action_space, device, USE_GAE=True, gae_tau=0.95):\n        self.observations = torch.zeros(num_steps + 1, num_processes, *obs_shape).to(device)\n        self.rewards = torch.zeros(num_steps, num_processes, 1).to(device)\n        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1).to(device)\n        self.returns = torch.zeros(num_steps + 1, num_processes, 1).to(device)\n        self.action_log_probs = torch.zeros(num_steps, num_processes, 1).to(device)\n        self.actions = torch.zeros(num_steps, num_processes, 1).to(device, torch.long)\n        self.masks = torch.ones(num_steps + 1, num_processes, 1).to(device)\n\n        self.num_steps = num_steps\n        self.step = 0\n        self.gae = USE_GAE\n        self.gae_tau = gae_tau\n\n    def insert(self, current_obs, action, action_log_prob, value_pred, reward, mask):\n        self.observations[self.step + 1].copy_(current_obs)\n        self.actions[self.step].copy_(action)\n        self.action_log_probs[self.step].copy_(action_log_prob)\n        self.value_preds[self.step].copy_(value_pred)\n        self.rewards[self.step].copy_(reward)\n        self.masks[self.step + 1].copy_(mask)\n\n        self.step = (self.step + 1) % self.num_steps\n\n    def after_update(self):\n        self.observations[0].copy_(self.observations[-1])\n        self.masks[0].copy_(self.masks[-1])\n\n    def compute_returns(self, next_value, gamma):\n        if self.gae:\n            self.value_preds[-1] = next_value\n            gae = 0\n            for step in reversed(range(self.rewards.size(0))):\n                delta = self.rewards[step] + gamma * self.value_preds[step + 1] * self.masks[step + 1] - self.value_preds[step]\n                gae = delta + gamma * self.gae_tau * self.masks[step + 1] * gae\n                self.returns[step] = gae + self.value_preds[step]\n        else:\n            self.returns[-1] = next_value\n            for step in reversed(range(self.rewards.size(0))):\n                self.returns[step] = self.returns[step + 1] * \\\n                    gamma * self.masks[step + 1] + self.rewards[step]\n\n    def feed_forward_generator(self, advantages, num_mini_batch):\n        num_steps, num_processes = self.rewards.size()[0:2]\n        batch_size = num_processes * num_steps\n        assert batch_size >= num_mini_batch, (\n            f""PPO requires the number processes ({num_processes}) ""\n            f""* number of steps ({num_steps}) = {num_processes * num_steps} ""\n            f""to be greater than or equal to the number of PPO mini batches ({num_mini_batch})."")\n        mini_batch_size = batch_size // num_mini_batch\n        sampler = BatchSampler(SubsetRandomSampler(range(batch_size)), mini_batch_size, drop_last=False)\n        for indices in sampler:\n            observations_batch = self.observations[:-1].view(-1,\n                                        *self.observations.size()[2:])[indices]\n            actions_batch = self.actions.view(-1, self.actions.size(-1))[indices]\n            return_batch = self.returns[:-1].view(-1, 1)[indices]\n            masks_batch = self.masks[:-1].view(-1, 1)[indices]\n            old_action_log_probs_batch = self.action_log_probs.view(-1, 1)[indices]\n            adv_targ = advantages.view(-1, 1)[indices]\n\n            yield observations_batch, actions_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ'"
utils/__init__.py,0,b''
utils/data_structures.py,0,"b'import random\nimport operator\n\nclass SegmentTree(object):\n    def __init__(self, capacity, operation, neutral_element):\n        """"""Build a Segment Tree data structure.\n        https://en.wikipedia.org/wiki/Segment_tree\n        Can be used as regular array, but with two\n        important differences:\n            a) setting item\'s value is slightly slower.\n               It is O(lg capacity) instead of O(1).\n            b) user has access to an efficient ( O(log segment size) )\n               `reduce` operation which reduces `operation` over\n               a contiguous subsequence of items in the array.\n        Paramters\n        ---------\n        capacity: int\n            Total size of the array - must be a power of two.\n        operation: lambda obj, obj -> obj\n            and operation for combining elements (eg. sum, max)\n            must form a mathematical group together with the set of\n            possible values for array elements (i.e. be associative)\n        neutral_element: obj\n            neutral element for the operation above. eg. float(\'-inf\')\n            for max and 0 for sum.\n        """"""\n        assert capacity > 0 and capacity & (capacity - 1) == 0, ""capacity must be positive and a power of 2.""\n        self._capacity = capacity\n        self._value = [neutral_element for _ in range(2 * capacity)]\n        self._operation = operation\n\n    def _reduce_helper(self, start, end, node, node_start, node_end):\n        if start == node_start and end == node_end:\n            return self._value[node]\n        mid = (node_start + node_end) // 2\n        if end <= mid:\n            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n        else:\n            if mid + 1 <= start:\n                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n            else:\n                return self._operation(\n                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n                )\n\n    def reduce(self, start=0, end=None):\n        """"""Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n        Parameters\n        ----------\n        start: int\n            beginning of the subsequence\n        end: int\n            end of the subsequences\n        Returns\n        -------\n        reduced: obj\n            result of reducing self.operation over the specified range of array elements.\n        """"""\n        if end is None:\n            end = self._capacity\n        if end < 0:\n            end += self._capacity\n        end -= 1\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n\n    def __setitem__(self, idx, val):\n        # index of the leaf\n        idx += self._capacity\n        self._value[idx] = val\n        idx //= 2\n        while idx >= 1:\n            self._value[idx] = self._operation(\n                self._value[2 * idx],\n                self._value[2 * idx + 1]\n            )\n            idx //= 2\n\n    def __getitem__(self, idx):\n        assert 0 <= idx < self._capacity\n        return self._value[self._capacity + idx]\n\n\nclass SumSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super(SumSegmentTree, self).__init__(\n            capacity=capacity,\n            operation=operator.add,\n            neutral_element=0.0\n        )\n\n    def sum(self, start=0, end=None):\n        """"""Returns arr[start] + ... + arr[end]""""""\n        return super(SumSegmentTree, self).reduce(start, end)\n\n    def find_prefixsum_idx(self, prefixsum):\n        """"""Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n        Parameters\n        ----------\n        perfixsum: float\n            upperbound on the sum of array prefix\n        Returns\n        -------\n        idx: int\n            highest index satisfying the prefixsum constraint\n        """"""\n        try:\n            assert 0 <= prefixsum <= self.sum() + 1e-5\n        except AssertionError:\n            print(""Prefix sum error: {}"".format(prefixsum))\n            exit()\n        idx = 1\n        while idx < self._capacity:  # while non-leaf\n            if self._value[2 * idx] > prefixsum:\n                idx = 2 * idx\n            else:\n                prefixsum -= self._value[2 * idx]\n                idx = 2 * idx + 1\n        return idx - self._capacity\n\n\nclass MinSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super(MinSegmentTree, self).__init__(\n            capacity=capacity,\n            operation=min,\n            neutral_element=float(\'inf\')\n        )\n\n    def min(self, start=0, end=None):\n        """"""Returns min(arr[start], ...,  arr[end])""""""\n\n        return super(MinSegmentTree, self).reduce(start, end)'"
utils/hyperparameters.py,1,"b'import torch\nimport math\n\n\nclass Config(object):\n    def __init__(self):\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        #PPO controls\n        self.ppo_epoch = 3\n        self.num_mini_batch = 32\n        self.ppo_clip_param = 0.1\n\n        #a2c controls\n        self.num_agents = 8\n        self.rollout = 16\n        self.value_loss_weight = 0.5\n        self.entropy_loss_weight = 0.001\n        self.grad_norm_max = 0.5\n        self.USE_GAE=True\n        self.gae_tau = 0.95\n\n        #algorithm control\n        self.USE_NOISY_NETS=False\n        self.USE_PRIORITY_REPLAY=False\n        \n        #Multi-step returns\n        self.N_STEPS = 1\n\n        #epsilon variables\n        self.epsilon_start = 1.0\n        self.epsilon_final = 0.01\n        self.epsilon_decay = 30000\n        self.epsilon_by_frame = lambda frame_idx: self.epsilon_final + (self.epsilon_start - self.epsilon_final) * math.exp(-1. * frame_idx / self.epsilon_decay)\n\n        #misc agent variables\n        self.GAMMA=0.99\n        self.LR=1e-4\n\n        #memory\n        self.TARGET_NET_UPDATE_FREQ = 1000\n        self.EXP_REPLAY_SIZE = 100000\n        self.BATCH_SIZE = 32\n        self.PRIORITY_ALPHA=0.6\n        self.PRIORITY_BETA_START=0.4\n        self.PRIORITY_BETA_FRAMES = 100000\n\n        #Noisy Nets\n        self.SIGMA_INIT=0.5\n\n        #Learning control variables\n        self.LEARN_START = 10000\n        self.MAX_FRAMES=100000\n        self.UPDATE_FREQ = 1\n\n        #Categorical Params\n        self.ATOMS = 51\n        self.V_MAX = 10\n        self.V_MIN = -10\n\n        #Quantile Regression Parameters\n        self.QUANTILES=51\n\n        #DRQN Parameters\n        self.SEQUENCE_LENGTH=8\n\n        #data logging parameters\n        self.ACTION_SELECTION_COUNT_FREQUENCY = 1000\n\n\n\'\'\'\n\n#epsilon variables\nepsilon_start = 1.0\nepsilon_final = 0.01\nepsilon_decay = 30000\nepsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n\n#misc agent variables\nGAMMA=0.99\nLR=1e-4\n\n#memory\nTARGET_NET_UPDATE_FREQ = 1000\nEXP_REPLAY_SIZE = 100000\nBATCH_SIZE = 32\nPRIORITY_ALPHA=0.6\nPRIORITY_BETA_START=0.4\nPRIORITY_BETA_FRAMES = 100000\n\n#Noisy Nets\nSIGMA_INIT=0.5\n\n#Learning control variables\nLEARN_START = 10000\nMAX_FRAMES=1000000\n\n\'\'\''"
utils/plot.py,0,"b'import numpy as np\n\n#switch backend in driver file\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport os\nimport glob\nfrom scipy.signal import medfilt\n\ndef smooth_reward_curve(x, y):\n    # Halfwidth of our smoothing convolution\n    halfwidth = min(31, int(np.ceil(len(x) / 30)))\n    k = halfwidth\n    xsmoo = x[k:-k]\n    ysmoo = np.convolve(y, np.ones(2 * k + 1), mode=\'valid\') / \\\n        np.convolve(np.ones_like(y), np.ones(2 * k + 1), mode=\'valid\')\n    downsample = max(int(np.floor(len(xsmoo) / 1e3)), 1)\n    return xsmoo[::downsample], ysmoo[::downsample]\n\n\ndef fix_point(x, y, interval):\n    np.insert(x, 0, 0)\n    np.insert(y, 0, 0)\n\n    fx, fy = [], []\n    pointer = 0\n\n    ninterval = int(max(x) / interval + 1)\n\n    for i in range(ninterval):\n        tmpx = interval * i\n\n        while pointer + 1 < len(x) and tmpx > x[pointer + 1]:\n            pointer += 1\n\n        if pointer + 1 < len(x):\n            alpha = (y[pointer + 1] - y[pointer]) / \\\n                (x[pointer + 1] - x[pointer])\n            tmpy = y[pointer] + alpha * (tmpx - x[pointer])\n            fx.append(tmpx)\n            fy.append(tmpy)\n\n    return fx, fy\n\ndef load_reward_data(indir, smooth, bin_size):\n    datas = []\n    infiles = glob.glob(os.path.join(indir, \'*.monitor.csv\'))\n\n    for inf in infiles:\n        with open(inf, \'r\') as f:\n            f.readline()\n            f.readline()\n            for line in f:\n                tmp = line.split(\',\')\n                t_time = float(tmp[2])\n                tmp = [t_time, int(tmp[1]), float(tmp[0])]\n                datas.append(tmp)\n\n    datas = sorted(datas, key=lambda d_entry: d_entry[0])\n    result = []\n    timesteps = 0\n    for i in range(len(datas)):\n        result.append([timesteps, datas[i][-1]])\n        timesteps += datas[i][1]\n\n    if len(result) < bin_size:\n        return [None, None]\n\n    x, y = np.array(result)[:, 0], np.array(result)[:, 1]\n\n    if smooth == 1:\n        x, y = smooth_reward_curve(x, y)\n\n    if smooth == 2:\n        y = medfilt(y, kernel_size=9)\n\n    x, y = fix_point(x, y, bin_size)\n    return [x, y]\n\n#TODO: only works for Experience Replay style training for now\ndef load_custom_data(indir, stat_file, smooth, bin_size):\n    datas = []\n    infiles = glob.glob(os.path.join(indir, stat_file))\n\n    for inf in infiles: #should be 1\n        with open(inf, \'r\') as f:\n            for line in f:\n                tmp = line.split(\',\')\n                tmp = [int(tmp[0]), float(tmp[1])]\n                datas.append(tmp)\n\n    datas = sorted(datas, key=lambda d_entry: d_entry[0])\n    result = []\n    for i in range(len(datas)):\n        result.append([datas[i][0], datas[i][1]])\n\n    if len(result) < bin_size:\n        return [None, None]\n\n    x, y = np.array(result)[:, 0], np.array(result)[:, 1]\n\n    if smooth == 1:\n        x, y = smooth_reward_curve(x, y)\n\n    if smooth == 2:\n        y = medfilt(y, kernel_size=9)\n\n    x, y = fix_point(x, y, bin_size)\n    return [x, y]\n\n#TODO: only works for Experience Replay style training for now\ndef load_action_data(indir, smooth, bin_size):\n    datas = []\n    infiles = glob.glob(os.path.join(indir, \'action_log.csv\'))\n\n    for inf in infiles: #should be 1\n        with open(inf, \'r\') as f:\n            for line in f:\n                tmp = line.split(\',\')\n                tmp = [int(tmp[0])] + [float(tmp[i]) for i in range(1, len(tmp))]\n                datas.append(tmp)\n\n    datas = sorted(datas, key=lambda d_entry: d_entry[0])\n    result = datas\n    #for i in range(len(datas)):\n    #    result.append([datas[i][0], datas[i][1]])\n\n    if len(result) < bin_size:\n        return [None, None]\n\n    x, y = np.array(result)[:, 0], np.array(result)[:, 1:]\n\n    \'\'\'if smooth == 1:\n        x, y = smooth_reward_curve(x, y)\n\n    if smooth == 2:\n        y = medfilt(y, kernel_size=9)\n\n    x, y = fix_point(x, y, bin_size)\'\'\'\n    return [x, np.transpose(y)]\n\ndef visdom_plot(viz, win, folder, game, name, num_steps, bin_size=100, smooth=1):\n    tx, ty = load_reward_data(folder, smooth, bin_size)\n    if tx is None or ty is None:\n        return win\n\n    fig = plt.figure()\n    plt.plot(tx, ty, label=""{}"".format(name))\n\n    tick_fractions = np.array([0.1, 0.2, 0.4, 0.6, 0.8, 1.0])\n    ticks = tick_fractions * num_steps\n    tick_names = [""{:.0e}"".format(tick) for tick in ticks]\n    plt.xticks(ticks, tick_names)\n    plt.xlim(0, num_steps * 1.01)\n\n    plt.xlabel(\'Number of Timesteps\')\n    plt.ylabel(\'Rewards\')\n\n    plt.title(game)\n    plt.legend(loc=4)\n    plt.show()\n    plt.draw()\n\n    \n    image = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3, ))\n    plt.close(fig)\n\n    #Show it in visdom\n    image = np.transpose(image, (2, 0, 1))\n\n    return viz.image(image, win=win)\n\ndef plot(folder, game, name, num_steps, bin_size=100, smooth=1):\n    matplotlib.rcParams.update({\'font.size\': 20})\n    tx, ty = load_reward_data(folder, smooth, bin_size)\n\n    if tx is None or ty is None:\n        return\n\n    fig = plt.figure(figsize=(20,5))\n    plt.plot(tx, ty, label=""{}"".format(name))\n\n    tick_fractions = np.array([0.1, 0.2, 0.4, 0.6, 0.8, 1.0])\n    ticks = tick_fractions * num_steps\n    tick_names = [""{:.0e}"".format(tick) for tick in ticks]\n    plt.xticks(ticks, tick_names)\n    plt.xlim(0, num_steps * 1.01)\n\n    plt.xlabel(\'Number of Timesteps\')\n    plt.ylabel(\'Rewards\')\n\n    plt.title(game)\n    plt.legend(loc=4)\n    plt.show()\n\ndef make_patch_spines_invisible(ax):\n    ax.set_frame_on(True)\n    ax.patch.set_visible(False)\n    for sp in ax.spines.values():\n        sp.set_visible(False)\n\ndef plot_all_data(folder, game, name, num_steps, bin_size=(10, 100, 100, 1), smooth=1, time=None, save_filename=\'results.png\', ipynb=False):\n    matplotlib.rcParams.update({\'font.size\': 20})\n    params = {\n        \'xtick.labelsize\': 20,\n        \'ytick.labelsize\': 15,\n        \'legend.fontsize\': 15\n    }\n    plt.rcParams.update(params)\n\n    tx, ty = load_reward_data(folder, smooth, bin_size[0])\n\n    if tx is None or ty is None:\n        return\n\n    if time is not None:\n        title = \'Avg. Last 10 Rewards: \' +  str(np.round(np.mean(ty[-10]))) + \' || \' +  game + \' || Elapsed Time: \' + str(time)\n    else:\n        title = \'Avg. Last 10 Rewards: \' +  str(np.round(np.mean(ty[-10]))) + \' || \' +  game\n\n    tick_fractions = np.array([0.1, 0.2, 0.4, 0.6, 0.8, 1.0])\n    ticks = tick_fractions * num_steps\n    tick_names = [""{:.0e}"".format(tick) for tick in ticks]\n\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 15), subplot_kw = dict(xticks=ticks, xlim=(0, num_steps*1.15), xlabel=\'Timestep\', title=title))\n    ax1.set_xticklabels(tick_names)\n    ax2.set_xticklabels(tick_names)\n    ax3.set_xticklabels(tick_names)\n\n    ax1.set_ylabel(\'Reward\')\n\n    p1, = ax1.plot(tx, ty, label=""Reward"")\n    #lines = [p1]\n\n    ax1.yaxis.label.set_color(p1.get_color())\n    ax1.tick_params(axis=\'y\', colors=p1.get_color())\n\n    ax1.legend([p1], [p1.get_label()], loc=4)\n\n    \n    #Load td data if it exists\n    tx, ty = load_custom_data(folder, \'td.csv\', smooth, bin_size[1])\n\n    ax2.set_title(\'Loss vs Timestep\')\n\n    if tx is not None or ty is not None:\n        ax2.set_ylabel(\'Avg .Temporal Difference\')\n        p2, = ax2.plot(tx, ty, \'r-\', label=\'Avg. TD\')\n        g2_lines = [p2]\n\n        ax2.yaxis.label.set_color(p2.get_color())\n        ax2.tick_params(axis=\'y\', colors=p2.get_color())\n\n        ax2.legend(g2_lines, [l.get_label() for l in g2_lines], loc=4)\n    \n    #Load Sigma Parameter Data if it exists\n    tx, ty = load_custom_data(folder, \'sig_param_mag.csv\', smooth, bin_size[2])\n\n    if tx is not None or ty is not None:\n        #need to update g2 title if sig data will be included\n        ax2.set_title(\'Loss/Avg. Sigma Parameter Magnitude vs Timestep\')\n\n        ax4 = ax2.twinx()\n\n        ax4.set_ylabel(\'Avg. Sigma Parameter Mag.\')\n        p4, = ax4.plot(tx, ty, \'g-\', label=\'Avg. Sigma Mag.\')\n        g2_lines += [p4]\n\n        ax4.yaxis.label.set_color(p4.get_color())\n        ax4.tick_params(axis=\'y\', colors=p4.get_color())\n\n        #ax4.spines[""right""].set_position((""axes"", 1.05))\n        #make_patch_spines_invisible(ax4)\n        #ax4.spines[""right""].set_visible(True)\n\n        ax2.legend(g2_lines, [l.get_label() for l in g2_lines], loc=4) #remake g2 legend because we have a new line\n\n    #Load action selection data if it exists\n    tx, ty = load_action_data(folder, smooth, bin_size[3])\n\n    ax3.set_title(\'Action Selection Frequency(%) vs Timestep\')\n\n    if tx is not None or ty is not None:\n        ax3.set_ylabel(\'Action Selection Frequency(%)\')\n        labels = [\'Action {}\'.format(i) for i in range(ty.shape[0])]\n        p3 = ax3.stackplot(tx, ty, labels=labels)\n\n        base = 0.0\n        for percent, index in zip(ty, range(ty.shape[0])):\n            offset = base + percent[-1]/3.0\n            ax3.annotate(str(\'{:.2f}\'.format(ty[index][-1])), xy=(tx[-1], offset), color=p3[index].get_facecolor().ravel())\n            base += percent[-1]\n\n        #ax3.yaxis.label.set_color(p3.get_color())\n        #ax3.tick_params(axis=\'y\', colors=p3.get_color())\n\n        ax3.legend(loc=4) #remake g2 legend because we have a new line\n\n    plt.tight_layout() # prevent label cutoff\n\n    if ipynb:\n        plt.show()\n    else:\n        plt.savefig(save_filename)\n    plt.clf()\n    plt.close()\n    \n    #return np.round(np.mean(ty[-10:]))\n\ndef plot_reward(folder, game, name, num_steps, bin_size=10, smooth=1, time=None, save_filename=\'results.png\', ipynb=False):\n    matplotlib.rcParams.update({\'font.size\': 20})\n    tx, ty = load_reward_data(folder, smooth, bin_size)\n\n    if tx is None or ty is None:\n        return\n\n    fig = plt.figure(figsize=(20,5))\n    plt.plot(tx, ty, label=""{}"".format(name))\n\n    tick_fractions = np.array([0.1, 0.2, 0.4, 0.6, 0.8, 1.0])\n    ticks = tick_fractions * num_steps\n    tick_names = [""{:.0e}"".format(tick) for tick in ticks]\n    plt.xticks(ticks, tick_names)\n    plt.xlim(0, num_steps * 1.01)\n\n    plt.xlabel(\'Number of Timesteps\')\n    plt.ylabel(\'Rewards\')\n\n    if time is not None:\n        plt.title(game + \' || Last 10: \' + str(np.round(np.mean(ty[-10]))) + \' || Elapsed Time: \' + str(time))\n    else:\n        plt.title(game + \' || Last 10: \' + str(np.round(np.mean(ty[-10]))))\n    plt.legend(loc=4)\n    if ipynb:\n        plt.show()\n    else:\n        plt.savefig(save_filename)\n    plt.clf()\n    plt.close()\n    \n    return np.round(np.mean(ty[-10]))\n\n\'\'\'def plot_td(folder, game, name, num_steps, bin_size=10, smooth=1, time=None, save_filename=\'td.png\', ipynb=False):\n    matplotlib.rcParams.update({\'font.size\': 20})\n    tx, ty = load_custom_data(folder, \'td.csv\', smooth, bin_size)\n\n    if tx is None or ty is None:\n        return\n\n    fig = plt.figure(figsize=(20,5))\n    plt.plot(tx, ty, label=""{}"".format(name))\n\n    tick_fractions = np.array([0.1, 0.2, 0.4, 0.6, 0.8, 1.0])\n    ticks = tick_fractions * num_steps\n    tick_names = [""{:.0e}"".format(tick) for tick in ticks]\n    plt.xticks(ticks, tick_names)\n    plt.xlim(0, num_steps * 1.01)\n\n    plt.xlabel(\'Number of Timesteps\')\n    plt.ylabel(\'Rewards\')\n\n    if time is not None:\n        plt.title(game + \' || Last 10: \' + str(np.round(np.mean(ty[-1]))) + \' || Elapsed Time: \' + str(time))\n    else:\n        plt.title(game + \' || Last 10: \' + str(np.round(np.mean(ty[-1]))))\n    plt.legend(loc=4)\n    if ipynb:\n        plt.show()\n    else:\n        plt.savefig(save_filename)\n    plt.clf()\n    plt.close()\n    \n    return np.round(np.mean(ty[-1]))\n\ndef plot_sig(folder, game, name, num_steps, bin_size=10, smooth=1, time=None, save_filename=\'sig.png\', ipynb=False):\n    matplotlib.rcParams.update({\'font.size\': 20})\n    tx, ty = load_custom_data(folder, \'sig_param_mag.csv\', smooth, bin_size)\n\n    if tx is None or ty is None:\n        return\n\n    fig = plt.figure(figsize=(20,5))\n    plt.plot(tx, ty, label=""{}"".format(name))\n\n    tick_fractions = np.array([0.1, 0.2, 0.4, 0.6, 0.8, 1.0])\n    ticks = tick_fractions * num_steps\n    tick_names = [""{:.0e}"".format(tick) for tick in ticks]\n    plt.xticks(ticks, tick_names)\n    plt.xlim(0, num_steps * 1.01)\n\n    plt.xlabel(\'Number of Timesteps\')\n    plt.ylabel(\'Rewards\')\n\n    if time is not None:\n        plt.title(game + \' || Last 10: \' + str(np.round(np.mean(ty[-1]))) + \' || Elapsed Time: \' + str(time))\n    else:\n        plt.title(game + \' || Last 10: \' + str(np.round(np.mean(ty[-1]))))\n    plt.legend(loc=4)\n    if ipynb:\n        plt.show()\n    else:\n        plt.savefig(save_filename)\n    plt.clf()\n    plt.close()\n    \n    return np.round(np.mean(ty[-1]))\'\'\''"
utils/wrappers.py,0,"b'import numpy as np\n\nimport gym\nfrom gym import spaces\nfrom gym.spaces.box import Box\n\nimport os\n\nfrom baselines import bench\nfrom baselines.common.atari_wrappers import make_atari, wrap_deepmind\n\nclass ImageToPyTorch(gym.ObservationWrapper):\n    """"""\n    Image shape to num_channels x weight x height\n    """"""\n    def __init__(self, env):\n        super(ImageToPyTorch, self).__init__(env)\n        old_shape = self.observation_space.shape\n        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n\n    def observation(self, observation):\n        return np.swapaxes(observation, 2, 0)/255.0\n    \ndef wrap_pytorch(env):\n    return ImageToPyTorch(env)\n\nclass WrapPyTorch(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        super(WrapPyTorch, self).__init__(env)\n        obs_shape = self.observation_space.shape\n        self.observation_space = Box(\n            self.observation_space.low[0, 0, 0],\n            self.observation_space.high[0, 0, 0],\n            [obs_shape[2], obs_shape[1], obs_shape[0]],\n            dtype=self.observation_space.dtype)\n\n    def observation(self, observation):\n        return observation.transpose(2, 0, 1)\n\ndef make_env_a2c_atari(env_id, seed, rank, log_dir):\n    def _thunk():\n        env = make_atari(env_id)\n        env.seed(seed + rank)\n\n        obs_shape = env.observation_space.shape\n\n        if log_dir is not None:\n            env = bench.Monitor(env, os.path.join(log_dir, str(rank)))\n\n        env = wrap_deepmind(env)\n\n        obs_shape = env.observation_space.shape\n        env = WrapPyTorch(env)\n\n        return env\n    return _thunk'"
