file_path,api_count,code
convert_to_caffe2_models.py,3,"b'from vision.ssd.vgg_ssd import create_vgg_ssd\nfrom vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd\nfrom vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite\nfrom vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite\nfrom vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite\n\nimport sys\nimport torch.onnx\nfrom caffe2.python.onnx.backend import Caffe2Backend as c2\nimport onnx\n\n\nif len(sys.argv) < 3:\n    print(\'Usage: python convert_to_caffe2_models.py <net type: mobilenet-v1-ssd|others>  <model path>\')\n    sys.exit(0)\nnet_type = sys.argv[1]\nmodel_path = sys.argv[2]\n\nlabel_path = sys.argv[3]\n\nclass_names = [name.strip() for name in open(label_path).readlines()]\nnum_classes = len(class_names)\n\nif net_type == \'vgg16-ssd\':\n    net = create_vgg_ssd(len(class_names), is_test=True)\nelif net_type == \'mb1-ssd\':\n    net = create_mobilenetv1_ssd(len(class_names), is_test=True)\nelif net_type == \'mb1-ssd-lite\':\n    net = create_mobilenetv1_ssd_lite(len(class_names), is_test=True)\nelif net_type == \'mb2-ssd-lite\':\n    net = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)\nelif net_type == \'sq-ssd-lite\':\n    net = create_squeezenet_ssd_lite(len(class_names), is_test=True)\nelse:\n    print(""The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite."")\n    sys.exit(1)\nnet.load(model_path)\nnet.eval()\n\nmodel_path = f""models/{net_type}.onnx""\ninit_net_path = f""models/{net_type}_init_net.pb""\ninit_net_txt_path = f""models/{net_type}_init_net.pbtxt""\npredict_net_path = f""models/{net_type}_predict_net.pb""\npredict_net_txt_path = f""models/{net_type}_predict_net.pbtxt""\n\ndummy_input = torch.randn(1, 3, 300, 300)\ntorch.onnx.export(net, dummy_input, model_path, verbose=False, output_names=[\'scores\', \'boxes\'])\n\nmodel = onnx.load(model_path)\ninit_net, predict_net = c2.onnx_graph_to_caffe2_net(model)\n\nprint(f""Save the model in binary format to the files {init_net_path} and {predict_net_path}."")\n\nwith open(init_net_path, ""wb"") as fopen:\n    fopen.write(init_net.SerializeToString())\nwith open(predict_net_path, ""wb"") as fopen:\n    fopen.write(predict_net.SerializeToString())\n\nprint(f""Save the model in txt format to the files {init_net_txt_path} and {predict_net_txt_path}. "")\nwith open(init_net_txt_path, \'w\') as f:\n    f.write(str(init_net))\n\nwith open(predict_net_txt_path, \'w\') as f:\n    f.write(str(predict_net))\n'"
draw_eval_results.py,0,"b'import sys\nimport cv2\nimport pandas as pd\nimport os\n\neval_result_file = sys.argv[1]\nimage_dir = sys.argv[2]\noutput_dir = sys.argv[3]\nthreshold = float(sys.argv[4])\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\nr = pd.read_csv(eval_result_file, delimiter="" "", names=[""ImageID"", ""Prob"", ""x1"", ""y1"", ""x2"", ""y2""])\nr[\'x1\'] = r[\'x1\'].astype(int)\nr[\'y1\'] = r[\'y1\'].astype(int)\nr[\'x2\'] = r[\'x2\'].astype(int)\nr[\'y2\'] = r[\'y2\'].astype(int)\n\n\nfor image_id, g in r.groupby(\'ImageID\'):\n    image = cv2.imread(os.path.join(image_dir, image_id + "".jpg""))\n    for row in g.itertuples():\n        if row.Prob < threshold:\n            continue\n        cv2.rectangle(image, (row.x1, row.y1), (row.x2, row.y2), (255, 255, 0), 4)\n        label = f""{row.Prob:.2f}""\n        cv2.putText(image, label,\n                    (row.x1 + 20, row.y1 + 40),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    1,  # font scale\n                    (255, 0, 255),\n                    2)  # line type\n    cv2.imwrite(os.path.join(output_dir, image_id + "".jpg""), image)\nprint(f""Task Done. Processed {r.shape[0]} bounding boxes."")'"
eval_ssd.py,10,"b'import torch\nfrom vision.ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor\nfrom vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd, create_mobilenetv1_ssd_predictor\nfrom vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite, create_mobilenetv1_ssd_lite_predictor\nfrom vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite, create_squeezenet_ssd_lite_predictor\nfrom vision.datasets.voc_dataset import VOCDataset\nfrom vision.datasets.open_images import OpenImagesDataset\nfrom vision.utils import box_utils, measurements\nfrom vision.utils.misc import str2bool, Timer\nimport argparse\nimport pathlib\nimport numpy as np\nimport logging\nimport sys\nfrom vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite, create_mobilenetv2_ssd_lite_predictor\n\n\nparser = argparse.ArgumentParser(description=""SSD Evaluation on VOC Dataset."")\nparser.add_argument(\'--net\', default=""vgg16-ssd"",\n                    help=""The network architecture, it should be of mb1-ssd, mb1-ssd-lite, mb2-ssd-lite or vgg16-ssd."")\nparser.add_argument(""--trained_model"", type=str)\n\nparser.add_argument(""--dataset_type"", default=""voc"", type=str,\n                    help=\'Specify dataset type. Currently support voc and open_images.\')\nparser.add_argument(""--dataset"", type=str, help=""The root directory of the VOC dataset or Open Images dataset."")\nparser.add_argument(""--label_file"", type=str, help=""The label file path."")\nparser.add_argument(""--use_cuda"", type=str2bool, default=True)\nparser.add_argument(""--use_2007_metric"", type=str2bool, default=True)\nparser.add_argument(""--nms_method"", type=str, default=""hard"")\nparser.add_argument(""--iou_threshold"", type=float, default=0.5, help=""The threshold of Intersection over Union."")\nparser.add_argument(""--eval_dir"", default=""eval_results"", type=str, help=""The directory to store evaluation results."")\nparser.add_argument(\'--mb2_width_mult\', default=1.0, type=float,\n                    help=\'Width Multiplifier for MobilenetV2\')\nargs = parser.parse_args()\nDEVICE = torch.device(""cuda:0"" if torch.cuda.is_available() and args.use_cuda else ""cpu"")\n\n\ndef group_annotation_by_class(dataset):\n    true_case_stat = {}\n    all_gt_boxes = {}\n    all_difficult_cases = {}\n    for i in range(len(dataset)):\n        image_id, annotation = dataset.get_annotation(i)\n        gt_boxes, classes, is_difficult = annotation\n        gt_boxes = torch.from_numpy(gt_boxes)\n        for i, difficult in enumerate(is_difficult):\n            class_index = int(classes[i])\n            gt_box = gt_boxes[i]\n            if not difficult:\n                true_case_stat[class_index] = true_case_stat.get(class_index, 0) + 1\n\n            if class_index not in all_gt_boxes:\n                all_gt_boxes[class_index] = {}\n            if image_id not in all_gt_boxes[class_index]:\n                all_gt_boxes[class_index][image_id] = []\n            all_gt_boxes[class_index][image_id].append(gt_box)\n            if class_index not in all_difficult_cases:\n                all_difficult_cases[class_index]={}\n            if image_id not in all_difficult_cases[class_index]:\n                all_difficult_cases[class_index][image_id] = []\n            all_difficult_cases[class_index][image_id].append(difficult)\n\n    for class_index in all_gt_boxes:\n        for image_id in all_gt_boxes[class_index]:\n            all_gt_boxes[class_index][image_id] = torch.stack(all_gt_boxes[class_index][image_id])\n    for class_index in all_difficult_cases:\n        for image_id in all_difficult_cases[class_index]:\n            all_gt_boxes[class_index][image_id] = torch.tensor(all_gt_boxes[class_index][image_id])\n    return true_case_stat, all_gt_boxes, all_difficult_cases\n\n\ndef compute_average_precision_per_class(num_true_cases, gt_boxes, difficult_cases,\n                                        prediction_file, iou_threshold, use_2007_metric):\n    with open(prediction_file) as f:\n        image_ids = []\n        boxes = []\n        scores = []\n        for line in f:\n            t = line.rstrip().split("" "")\n            image_ids.append(t[0])\n            scores.append(float(t[1]))\n            box = torch.tensor([float(v) for v in t[2:]]).unsqueeze(0)\n            box -= 1.0  # convert to python format where indexes start from 0\n            boxes.append(box)\n        scores = np.array(scores)\n        sorted_indexes = np.argsort(-scores)\n        boxes = [boxes[i] for i in sorted_indexes]\n        image_ids = [image_ids[i] for i in sorted_indexes]\n        true_positive = np.zeros(len(image_ids))\n        false_positive = np.zeros(len(image_ids))\n        matched = set()\n        for i, image_id in enumerate(image_ids):\n            box = boxes[i]\n            if image_id not in gt_boxes:\n                false_positive[i] = 1\n                continue\n\n            gt_box = gt_boxes[image_id]\n            ious = box_utils.iou_of(box, gt_box)\n            max_iou = torch.max(ious).item()\n            max_arg = torch.argmax(ious).item()\n            if max_iou > iou_threshold:\n                if difficult_cases[image_id][max_arg] == 0:\n                    if (image_id, max_arg) not in matched:\n                        true_positive[i] = 1\n                        matched.add((image_id, max_arg))\n                    else:\n                        false_positive[i] = 1\n            else:\n                false_positive[i] = 1\n\n    true_positive = true_positive.cumsum()\n    false_positive = false_positive.cumsum()\n    precision = true_positive / (true_positive + false_positive)\n    recall = true_positive / num_true_cases\n    if use_2007_metric:\n        return measurements.compute_voc2007_average_precision(precision, recall)\n    else:\n        return measurements.compute_average_precision(precision, recall)\n\n\nif __name__ == \'__main__\':\n    eval_path = pathlib.Path(args.eval_dir)\n    eval_path.mkdir(exist_ok=True)\n    timer = Timer()\n    class_names = [name.strip() for name in open(args.label_file).readlines()]\n\n    if args.dataset_type == ""voc"":\n        dataset = VOCDataset(args.dataset, is_test=True)\n    elif args.dataset_type == \'open_images\':\n        dataset = OpenImagesDataset(args.dataset, dataset_type=""test"")\n\n    true_case_stat, all_gb_boxes, all_difficult_cases = group_annotation_by_class(dataset)\n    if args.net == \'vgg16-ssd\':\n        net = create_vgg_ssd(len(class_names), is_test=True)\n    elif args.net == \'mb1-ssd\':\n        net = create_mobilenetv1_ssd(len(class_names), is_test=True)\n    elif args.net == \'mb1-ssd-lite\':\n        net = create_mobilenetv1_ssd_lite(len(class_names), is_test=True)\n    elif args.net == \'sq-ssd-lite\':\n        net = create_squeezenet_ssd_lite(len(class_names), is_test=True)\n    elif args.net == \'mb2-ssd-lite\':\n        net = create_mobilenetv2_ssd_lite(len(class_names), width_mult=args.mb2_width_mult, is_test=True)\n    else:\n        logging.fatal(""The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite."")\n        parser.print_help(sys.stderr)\n        sys.exit(1)  \n\n    timer.start(""Load Model"")\n    net.load(args.trained_model)\n    net = net.to(DEVICE)\n    print(f\'It took {timer.end(""Load Model"")} seconds to load the model.\')\n    if args.net == \'vgg16-ssd\':\n        predictor = create_vgg_ssd_predictor(net, nms_method=args.nms_method, device=DEVICE)\n    elif args.net == \'mb1-ssd\':\n        predictor = create_mobilenetv1_ssd_predictor(net, nms_method=args.nms_method, device=DEVICE)\n    elif args.net == \'mb1-ssd-lite\':\n        predictor = create_mobilenetv1_ssd_lite_predictor(net, nms_method=args.nms_method, device=DEVICE)\n    elif args.net == \'sq-ssd-lite\':\n        predictor = create_squeezenet_ssd_lite_predictor(net,nms_method=args.nms_method, device=DEVICE)\n    elif args.net == \'mb2-ssd-lite\':\n        predictor = create_mobilenetv2_ssd_lite_predictor(net, nms_method=args.nms_method, device=DEVICE)\n    else:\n        logging.fatal(""The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite."")\n        parser.print_help(sys.stderr)\n        sys.exit(1)\n\n    results = []\n    for i in range(len(dataset)):\n        print(""process image"", i)\n        timer.start(""Load Image"")\n        image = dataset.get_image(i)\n        print(""Load Image: {:4f} seconds."".format(timer.end(""Load Image"")))\n        timer.start(""Predict"")\n        boxes, labels, probs = predictor.predict(image)\n        print(""Prediction: {:4f} seconds."".format(timer.end(""Predict"")))\n        indexes = torch.ones(labels.size(0), 1, dtype=torch.float32) * i\n        results.append(torch.cat([\n            indexes.reshape(-1, 1),\n            labels.reshape(-1, 1).float(),\n            probs.reshape(-1, 1),\n            boxes + 1.0  # matlab\'s indexes start from 1\n        ], dim=1))\n    results = torch.cat(results)\n    for class_index, class_name in enumerate(class_names):\n        if class_index == 0: continue  # ignore background\n        prediction_path = eval_path / f""det_test_{class_name}.txt""\n        with open(prediction_path, ""w"") as f:\n            sub = results[results[:, 1] == class_index, :]\n            for i in range(sub.size(0)):\n                prob_box = sub[i, 2:].numpy()\n                image_id = dataset.ids[int(sub[i, 0])]\n                print(\n                    image_id + "" "" + "" "".join([str(v) for v in prob_box]),\n                    file=f\n                )\n    aps = []\n    print(""\\n\\nAverage Precision Per-class:"")\n    for class_index, class_name in enumerate(class_names):\n        if class_index == 0:\n            continue\n        prediction_path = eval_path / f""det_test_{class_name}.txt""\n        ap = compute_average_precision_per_class(\n            true_case_stat[class_index],\n            all_gb_boxes[class_index],\n            all_difficult_cases[class_index],\n            prediction_path,\n            args.iou_threshold,\n            args.use_2007_metric\n        )\n        aps.append(ap)\n        print(f""{class_name}: {ap}"")\n\n    print(f""\\nAverage Precision Across All Classes:{sum(aps)/len(aps)}"")\n\n\n\n'"
extract_tf_weights.py,0,"b'import tensorflow as tf\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.framework import tensor_util\nimport sys\nimport pickle\n\n\ndef read_weights(frozen_model):\n    weights = {}\n    with tf.Session() as sess:\n        with gfile.FastGFile(frozen_model, \'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def)\n        for n in graph_def.node:\n            if n.op == \'Const\':\n                weights[n.name] = tensor_util.MakeNdarray(n.attr[\'value\'].tensor)\n                print(""Name:"", n.name, ""Shape:"", weights[n.name].shape)\n    return weights\n\n\nif len(sys.argv)  < 3:\n    print(""Usage: python extract_tf_weights.py <frozen_model.pb> <weights_file.pickle>"")\n\nfrozen_model = sys.argv[1]\nweights_file = sys.argv[2]\n\nweights = read_weights(frozen_model)\nwith open(weights_file, ""wb"") as f:\n    pickle.dump(weights, f)\n    print(f""Saved weights to {weights_file}."")'"
open_images_downloader.py,0,"b'import time\nimport boto3\nfrom botocore import UNSIGNED\nfrom botocore.config import Config\nimport botocore\nimport logging\nfrom multiprocessing import Pool, Manager\nimport pandas as pd\nimport os\nimport argparse\nimport sys\nimport functools\nfrom urllib import request\n\n\ns3 = boto3.client(\'s3\', config=Config(signature_version=UNSIGNED))\n\n\ndef download(bucket, root, retry, counter, lock, path):\n    i = 0\n    src = path\n    dest = f""{root}/{path}""\n    while i < retry:\n        try:\n            if not os.path.exists(dest):\n                s3.download_file(bucket, src, dest)\n            else:\n                logging.info(f""{dest} already exists."")\n            with lock:\n                counter.value += 1\n                if counter.value % 100 == 0:\n                    logging.warning(f""Downloaded {counter.value} images."")\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\'Error\'][\'Code\'] == ""404"":\n                logging.warning(f""The file s3://{bucket}/{src} does not exist."")\n                return\n            i += 1\n            logging.warning(f""Sleep {i} and try again."")\n            time.sleep(i)\n    logging.warning(f""Failed to download the file s3://{bucket}/{src}. Exception: {e}"")\n\n\ndef batch_download(bucket, file_paths, root, num_workers=10, retry=10):\n    with Pool(num_workers) as p:\n        m = Manager()\n        counter = m.Value(\'i\', 0)\n        lock = m.Lock()\n        download_ = functools.partial(download, bucket, root, retry, counter, lock)\n        p.map(download_, file_paths)\n\n\ndef http_download(url, path):\n    with request.urlopen(url) as f:\n        with open(path, ""wb"") as fout:\n            buf = f.read(1024)\n            while buf:\n                fout.write(buf)\n                buf = f.read(1024)\n\n\ndef log_counts(values):\n    for k, count in values.value_counts().iteritems():\n        logging.warning(f""{k}: {count}/{len(values)} = {count/len(values):.2f}."")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Dowload open image dataset by class.\')\n\n    parser.add_argument(""--root"", type=str,\n                        help=\'The root directory that you want to store the open image data.\')\n    parser.add_argument(""include_depiction"", action=""store_true"",\n                        help=""Do you want to include drawings or depictions?"")\n    parser.add_argument(""--class_names"", type=str,\n                        help=""the classes you want to download."")\n    parser.add_argument(""--num_workers"", type=int, default=10,\n                        help=""the classes you want to download."")\n    parser.add_argument(""--retry"", type=int, default=10,\n                        help=""retry times when downloading."")\n    parser.add_argument(""--filter_file"", type=str, default="""",\n                        help=""This file specifies the image ids you want to exclude."")\n    parser.add_argument(\'--remove_overlapped\', action=\'store_true\',\n                        help=""Remove single boxes covered by group boxes."")\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(stream=sys.stdout, level=logging.WARNING,\n                        format=\'%(asctime)s - %(name)s - %(message)s\')\n\n    args = parse_args()\n    bucket = ""open-images-dataset""\n    names = [e.strip() for e in args.class_names.split("","")]\n    class_names = []\n    group_filters = []\n    percentages = []\n    for name in names:\n        t = name.split("":"")\n        class_names.append(t[0].strip())\n        if len(t) >= 2 and t[1].strip():\n            group_filters.append(t[1].strip())\n        else:\n            group_filters.append("""")\n        if len(t) >= 3 and t[2].strip():\n            percentages.append(float(t[2].strip()))\n        else:\n            percentages.append(1.0)\n\n    if not os.path.exists(args.root):\n        os.makedirs(args.root)\n\n    excluded_images = set()\n    if args.filter_file:\n        for line in open(args.filter_file):\n            img_id = line.strip()\n            if not img_id:\n                continue\n            excluded_images.add(img_id)\n\n    class_description_file = os.path.join(args.root, ""class-descriptions-boxable.csv"")\n    if not os.path.exists(class_description_file):\n        url = ""https://storage.googleapis.com/openimages/2018_04/class-descriptions-boxable.csv""\n        logging.warning(f""Download {url}."")\n        http_download(url, class_description_file)\n\n    class_descriptions = pd.read_csv(class_description_file,\n                                    names=[""id"", ""ClassName""])\n    class_descriptions = class_descriptions[class_descriptions[\'ClassName\'].isin(class_names)]\n\n    image_files = []\n    for dataset_type in [""train"", ""validation"", ""test""]:\n        image_dir = os.path.join(args.root, dataset_type)\n        os.makedirs(image_dir, exist_ok=True)\n\n        annotation_file = f""{args.root}/{dataset_type}-annotations-bbox.csv""\n        if not os.path.exists(annotation_file):\n            url = f""https://storage.googleapis.com/openimages/2018_04/{dataset_type}/{dataset_type}-annotations-bbox.csv""\n            logging.warning(f""Download {url}."")\n            http_download(url, annotation_file)\n        logging.warning(f""Read annotation file {annotation_file}"")\n        annotations = pd.read_csv(annotation_file)\n        annotations = pd.merge(annotations, class_descriptions,\n                               left_on=""LabelName"", right_on=""id"",\n                               how=""inner"")\n        if not args.include_depiction:\n            annotations = annotations.loc[annotations[\'IsDepiction\'] != 1, :]\n\n        filtered = []\n        for class_name, group_filter, percentage in zip(class_names, group_filters, percentages):\n            sub = annotations.loc[annotations[\'ClassName\'] == class_name, :]\n            excluded_images |= set(sub[\'ImageID\'].sample(frac=1 - percentage))\n\n            if group_filter == \'~group\':\n                excluded_images |= set(sub.loc[sub[\'IsGroupOf\'] == 1, \'ImageID\'])\n            elif group_filter == \'group\':\n                excluded_images |= set(sub.loc[sub[\'IsGroupOf\'] == 0, \'ImageID\'])\n            filtered.append(sub)\n\n        annotations = pd.concat(filtered)\n        annotations = annotations.loc[~annotations[\'ImageID\'].isin(excluded_images), :]\n\n\n        if args.remove_overlapped:\n            images_with_group = annotations.loc[annotations[\'IsGroupOf\'] == 1, \'ImageID\']\n            annotations = annotations.loc[~(annotations[\'ImageID\'].isin(set(images_with_group)) & (annotations[\'IsGroupOf\'] == 0)), :]\n        annotations = annotations.sample(frac=1.0)\n\n        logging.warning(f""{dataset_type} bounding boxes size: {annotations.shape[0]}"")\n        logging.warning(""Approximate Image Stats: "")\n        log_counts(annotations.drop_duplicates([""ImageID"", ""ClassName""])[""ClassName""])\n        logging.warning(""Label distribution: "")\n        log_counts(annotations[\'ClassName\'])\n\n        logging.warning(f""Shuffle dataset."")\n\n\n        sub_annotation_file = f""{args.root}/sub-{dataset_type}-annotations-bbox.csv""\n        logging.warning(f""Save {dataset_type} data to {sub_annotation_file}."")\n        annotations.to_csv(sub_annotation_file, index=False)\n        image_files.extend(f""{dataset_type}/{id}.jpg"" for id in set(annotations[\'ImageID\']))\n    logging.warning(f""Start downloading {len(image_files)} images."")\n    batch_download(bucket, image_files, args.root, args.num_workers, args.retry)\n    logging.warning(""Task Done."")\n'"
prune_alexnet.py,21,"b'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import datasets, transforms\nimport argparse\nimport logging\nimport sys\nfrom tensorboardX import SummaryWriter\n\nfrom vision.prunning.prunner import ModelPrunner\nfrom vision.utils.misc import str2bool\nfrom vision.nn.alexnet import alexnet\n\n\nparser = argparse.ArgumentParser(description=\'Demonstration of Pruning AlexNet\')\n\nparser.add_argument(""--train"", dest=""train"", action=""store_true"")\nparser.add_argument(""--prune_conv"", dest=""prune_conv"", action=""store_true"")\nparser.add_argument(""--prune_linear"", dest=""prune_linear"", action=""store_true"")\nparser.add_argument(""--trained_model"", type=str)\nparser.add_argument(\'--dataset\', type=str, help=\'Dataset directory path\')\nparser.add_argument(\'--validation_dataset\', help=\'Dataset directory path\')\nparser.add_argument(\'--batch_size\', default=12, type=int,\n                    help=\'Batch size for training\')\nparser.add_argument(\'--num_epochs\', default=25, type=int,\n                    help=\'number of batches to train\')\nparser.add_argument(\'--num_recovery_batches\', default=2, type=int,\n                    help=\'number of batches to train to recover the network\')\nparser.add_argument(\'--recovery_learning_rate\', default=1e-4, type=float,\n                    help=\'learning rate to recover the network\')\nparser.add_argument(\'--recovery_batch_size\', default=32, type=int,\n                    help=\'Batch size for training\')\n\n# Params for SGD\nparser.add_argument(\'--learning_rate\', default=1e-3, type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float,\n                    help=\'Momentum value for optim\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float,\n                    help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float,\n                    help=\'Gamma update for SGD\')\n\n# Params for Pruning\nparser.add_argument(\'--prune_conv_num\', default=1, type=int,\n                    help=\'the number of conv filters you want to prune in very iteration.\')\nparser.add_argument(\'--prune_linear_num\', default=2, type=int,\n                    help=\'the number of linear filters you want to prune in very iteration.\')\nparser.add_argument(\'--window\', default=10, type=int,\n                    help=\'Window size for tracking training accuracy.\')\n\nparser.add_argument(\'--use_cuda\', default=True, type=str2bool,\n                    help=\'Use CUDA to train model\')\n\n\nargs = parser.parse_args()\nDEVICE = torch.device(""cuda:0"" if torch.cuda.is_available() and args.use_cuda else ""cpu"")\ncpu_device = torch.device(""cpu"")\n\n\nif args.use_cuda and torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True\n\n\ndef train_epoch(net, data_iter, num_epochs=1, optimizer=None):\n    net = net.to(DEVICE)\n    net.train()\n    criterion = nn.CrossEntropyLoss()\n\n    num = 0\n    for i in range(num_epochs):\n        inputs, labels = next(data_iter)\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        if optimizer:\n            optimizer.zero_grad()\n\n        outputs = net(inputs)\n\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        if optimizer:\n            optimizer.step()\n        train_loss = loss.item() * inputs.size(0)\n        train_accuracy = torch.sum(preds == labels.data).item()\n        num += inputs.size(0)\n    train_loss /= num\n    train_accuracy /= num\n    logging.info(\'Train Epoch Loss:{:.4f}, Accuracy:{:.4f}\'.format(train_loss, train_accuracy))\n    return train_loss, train_accuracy\n\n\ndef train(net, train_loader, val_loader, num_epochs, learning_rate, save_model=True):\n    net = net.to(DEVICE)\n    net.train()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=learning_rate,\n                          momentum=args.momentum, weight_decay=args.weight_decay)\n    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n    for i in range(num_epochs):\n        net.train()\n        exp_lr_scheduler.step()\n        num = 0\n        running_loss = 0.0\n        running_corrects = 0.0\n        for inputs, labels in train_loader:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = net(inputs)\n\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data).item()\n            num += inputs.size(0)\n\n        logging.info(\'Epoch: {}, Training Loss:{:.4f}, Training Accuracy:{:.4f}\'.format(i, running_loss/num, running_corrects/num))\n        val_loss, val_accuracy = eval(net, val_loader)\n        logging.info(\'Epoch: {}, Val Loss:{:.4f}, Val Accuracy:{:.4f}\'.format(i, val_loss, val_accuracy))\n        if save_model:\n            torch.save(net.state_dict(), ""models/ant-alexnet-epoch-{}-{:.4f}.pth"".format(i, val_accuracy))\n    return val_loss, val_accuracy\n\n\ndef eval(net, loader):\n    net.eval()\n    criterion = nn.CrossEntropyLoss()\n    running_loss = 0.0\n    running_corrects = 0\n    num = 0\n    for inputs, labels in loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        with torch.set_grad_enabled(False):\n            outputs = net(inputs)\n\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data).item()\n        num += inputs.size(0)\n    running_loss /= num\n    running_corrects = running_corrects / num\n    return running_loss, running_corrects\n\n\ndef make_prunner_loader(dataset):\n    loader = torch.utils.data.DataLoader(dataset, batch_size=args.recovery_batch_size, shuffle=True, num_workers=1)\n    while True:\n        for inputs, labels in loader:\n            yield inputs, labels\n\nif __name__ == \'__main__\':\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n                        format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n\n    net = alexnet(True)\n    net.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, 2),\n        )\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    train_dataset = datasets.ImageFolder(args.dataset, train_transform)\n    val_dataset = datasets.ImageFolder(args.validation_dataset, val_transform)\n    logging.info(f""Training dataset size: {len(train_dataset)}."")\n    logging.info(f""Validation Dataset size: {len(val_dataset)}."")\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, num_workers=1)\n    writer = SummaryWriter()\n    if args.train:\n        logging.info(""Start training."")\n        train(net, train_loader, val_loader, args.num_epochs, args.learning_rate)\n    elif args.prune_conv or args.prune_linear:\n        net.load_state_dict(torch.load(args.trained_model))\n        prunner_data_iter = iter(make_prunner_loader(train_dataset))\n        prunner = ModelPrunner(net, lambda model: train_epoch(model, prunner_data_iter),\n                               ignored_paths=[(\'classifier\', \'6\')])  # do not prune the last layer.\n        num_filters = prunner.book.num_of_conv2d_filters()\n        logging.info(f""Number of Conv2d filters: {num_filters}"")\n\n        num_linear_filters = prunner.book.num_of_linear_filters()\n        logging.info(f""Number of Linear filters: {num_linear_filters}"")\n        if args.prune_conv:\n            prune_num = prunner.book.num_of_conv2d_filters() - 5 * (prunner.book.num_of_conv2d_modules())\n        else:\n            prune_num = prunner.book.num_of_linear_filters() - 5 * (prunner.book.num_of_linear_modules())\n        logging.info(f""Number of Layers to Prune: {prune_num}"")\n        i = 0\n        iteration = 0\n        train_data_iter = iter(make_prunner_loader(train_dataset))\n        optimizer = optim.SGD(net.parameters(), lr=args.recovery_learning_rate,\n                              momentum=args.momentum, weight_decay=args.weight_decay)\n        while i < prune_num:\n            if args.prune_conv:\n                prunner.prune_conv_layers(args.prune_conv_num)\n                i += args.prune_conv_num\n            else:\n                _, accuracy_gain = prunner.prune_linear_layers(args.prune_linear_num)\n                i += args.prune_linear_num\n            if iteration % 10 == 0:\n                val_loss, val_accuracy = eval(prunner.model, val_loader)\n                logging.info(f""Prune: {i}/{prune_num}, After Pruning Evaluation Accuracy:{val_accuracy:.4f}."")\n            val_loss, val_accuracy = train_epoch(prunner.model, train_data_iter, args.num_recovery_batches, optimizer)\n            for name, param in net.named_parameters():\n                writer.add_histogram(name, param.clone().cpu().data.numpy(), 10)\n            if iteration % 10 == 0:\n                dummy_input = torch.rand(1, 3, 224, 224)\n                writer.add_graph(net, dummy_input)\n                val_loss, val_accuracy = eval(prunner.model, val_loader)\n                logging.info(f""Prune: {i}/{prune_num}, After Recovery Evaluation Accuracy:{val_accuracy:.4f}."")\n                logging.info(f""Prune: {i}/{prune_num}, Iteration: {iteration}, Save model."")\n                with open(f""models/alexnet-pruned-{i}.txt"", ""w"") as f:\n                    print(prunner.model, file=f)\n                torch.save(prunner.model.state_dict(), f""models/prunned-alexnet-{i}-{prune_num}-{val_accuracy:.4f}.pth"")\n            iteration += 1\n    else:\n        logging.fatal(""You should specify --prune_conv, --prune_linear or --train."")\n\n    writer.close()\n'"
run_ssd_example.py,0,"b'from vision.ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor\nfrom vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd, create_mobilenetv1_ssd_predictor\nfrom vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite, create_mobilenetv1_ssd_lite_predictor\nfrom vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite, create_squeezenet_ssd_lite_predictor\nfrom vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite, create_mobilenetv2_ssd_lite_predictor\nfrom vision.utils.misc import Timer\nimport cv2\nimport sys\n\n\nif len(sys.argv) < 5:\n    print(\'Usage: python run_ssd_example.py <net type>  <model path> <label path> <image path>\')\n    sys.exit(0)\nnet_type = sys.argv[1]\nmodel_path = sys.argv[2]\nlabel_path = sys.argv[3]\nimage_path = sys.argv[4]\n\nclass_names = [name.strip() for name in open(label_path).readlines()]\n\nif net_type == \'vgg16-ssd\':\n    net = create_vgg_ssd(len(class_names), is_test=True)\nelif net_type == \'mb1-ssd\':\n    net = create_mobilenetv1_ssd(len(class_names), is_test=True)\nelif net_type == \'mb1-ssd-lite\':\n    net = create_mobilenetv1_ssd_lite(len(class_names), is_test=True)\nelif net_type == \'mb2-ssd-lite\':\n    net = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)\nelif net_type == \'sq-ssd-lite\':\n    net = create_squeezenet_ssd_lite(len(class_names), is_test=True)\nelse:\n    print(""The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite."")\n    sys.exit(1)\nnet.load(model_path)\n\nif net_type == \'vgg16-ssd\':\n    predictor = create_vgg_ssd_predictor(net, candidate_size=200)\nelif net_type == \'mb1-ssd\':\n    predictor = create_mobilenetv1_ssd_predictor(net, candidate_size=200)\nelif net_type == \'mb1-ssd-lite\':\n    predictor = create_mobilenetv1_ssd_lite_predictor(net, candidate_size=200)\nelif net_type == \'mb2-ssd-lite\':\n    predictor = create_mobilenetv2_ssd_lite_predictor(net, candidate_size=200)\nelif net_type == \'sq-ssd-lite\':\n    predictor = create_squeezenet_ssd_lite_predictor(net, candidate_size=200)\nelse:\n    predictor = create_vgg_ssd_predictor(net, candidate_size=200)\n\norig_image = cv2.imread(image_path)\nimage = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\nboxes, labels, probs = predictor.predict(image, 10, 0.4)\n\nfor i in range(boxes.size(0)):\n    box = boxes[i, :]\n    cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), (255, 255, 0), 4)\n    #label = f""""""{voc_dataset.class_names[labels[i]]}: {probs[i]:.2f}""""""\n    label = f""{class_names[labels[i]]}: {probs[i]:.2f}""\n    cv2.putText(orig_image, label,\n                (box[0] + 20, box[1] + 40),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                1,  # font scale\n                (255, 0, 255),\n                2)  # line type\npath = ""run_ssd_example_output.jpg""\ncv2.imwrite(path, orig_image)\nprint(f""Found {len(probs)} objects. The output image is {path}"")\n'"
run_ssd_live_caffe2.py,0,"b'import vision.utils.box_utils_numpy as box_utils\nfrom vision.utils.misc import Timer\nfrom vision.ssd.config.mobilenetv1_ssd_config import specs, center_variance, size_variance\n\n\nimport cv2\nimport sys\nfrom caffe2.python import core, workspace, net_printer\nimport numpy as np\n\npriors = box_utils.generate_ssd_priors(specs, 300)\nprint(\'priors.shape\', priors.shape)\n\n\ndef load_model(init_net_path, predict_net_path):\n    with open(init_net_path, ""rb"") as f:\n        init_net = f.read()\n    with open(predict_net_path, ""rb"") as f:\n        predict_net = f.read()\n    p = workspace.Predictor(init_net, predict_net)\n    return p\n\n\ndef predict(width, height, confidences, boxes, prob_threshold, iou_threshold=0.5, top_k=-1):\n    boxes = boxes[0]\n    confidences = confidences[0]\n    picked_box_probs = []\n    picked_labels = []\n    for class_index in range(1, confidences.shape[1]):\n        probs = confidences[:, class_index]\n        mask = probs > prob_threshold\n        probs = probs[mask]\n        if probs.shape[0] == 0:\n            continue\n        subset_boxes = boxes[mask, :]\n        box_probs = np.concatenate([subset_boxes, probs.reshape(-1, 1)], axis=1)\n        box_probs = box_utils.hard_nms(box_probs,\n                                  iou_threshold=iou_threshold,\n                                  top_k=top_k,\n                                  )\n        picked_box_probs.append(box_probs)\n        picked_labels.extend([class_index] * box_probs.shape[0])\n    if not picked_box_probs:\n        return np.array([]), np.array([]), np.array([])\n    picked_box_probs = np.concatenate(picked_box_probs)\n    picked_box_probs[:, 0] *= width\n    picked_box_probs[:, 1] *= height\n    picked_box_probs[:, 2] *= width\n    picked_box_probs[:, 3] *= height\n    return picked_box_probs[:, :4].astype(np.int32), np.array(picked_labels), picked_box_probs[:, 4]\n\n\nif len(sys.argv) < 2:\n    print(\'Usage: python run_ssd_live_caffe2.py init_net predict_net\')\n    sys.exit(0)\ninit_net_path = sys.argv[1]\npredict_net_path = sys.argv[2]\nlabel_path = sys.argv[3]\n\nclass_names = [name.strip() for name in open(label_path).readlines()]\npredictor = load_model(init_net_path, predict_net_path)\n\nif len(sys.argv) >= 5:\n    cap = cv2.VideoCapture(sys.argv[4])  # capture from file\nelse:\n    cap = cv2.VideoCapture(0)   # capture from camera\n    cap.set(3, 1920)\n    cap.set(4, 1080)\n\ntimer = Timer()\nwhile True:\n    ret, orig_image = cap.read()\n    if orig_image is None:\n        continue\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (300, 300))\n    image = image.astype(np.float32)\n    image = (image - 127) / 128\n    image = np.transpose(image, [2, 0, 1])\n    image = np.expand_dims(image, axis=0)\n    timer.start()\n    confidences, boxes = predictor.run({\'0\': image})\n    interval = timer.end()\n    print(\'Inference Time: {:.2f}s.\'.format(interval))\n    timer.start()\n    boxes, labels, probs = predict(orig_image.shape[1], orig_image.shape[0], confidences, boxes, 0.55)\n    interval = timer.end()\n    print(\'NMS Time: {:.2f}s, Detect Objects: {:d}.\'.format(interval, labels.shape[0]))\n    for i in range(boxes.shape[0]):\n        box = boxes[i, :]\n        label = f""{class_names[labels[i]]}: {probs[i]:.2f}""\n\n        cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), (255, 255, 0), 4)\n\n        cv2.putText(orig_image, label,\n                    (box[0]+20, box[1]+40),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    1,  # font scale\n                    (255, 0, 255),\n                    2)  # line type\n    cv2.imshow(\'annotated\', orig_image)\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\ncap.release()\ncv2.destroyAllWindows()\n'"
run_ssd_live_demo.py,0,"b'from vision.ssd.vgg_ssd import create_vgg_ssd, create_vgg_ssd_predictor\nfrom vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd, create_mobilenetv1_ssd_predictor\nfrom vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite, create_mobilenetv1_ssd_lite_predictor\nfrom vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite, create_squeezenet_ssd_lite_predictor\nfrom vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite, create_mobilenetv2_ssd_lite_predictor\nfrom vision.utils.misc import Timer\nimport cv2\nimport sys\n\nif len(sys.argv) < 4:\n    print(\'Usage: python run_ssd_example.py <net type>  <model path> <label path> [video file]\')\n    sys.exit(0)\nnet_type = sys.argv[1]\nmodel_path = sys.argv[2]\nlabel_path = sys.argv[3]\n\nif len(sys.argv) >= 5:\n    cap = cv2.VideoCapture(sys.argv[4])  # capture from file\nelse:\n    cap = cv2.VideoCapture(0)   # capture from camera\n    cap.set(3, 1920)\n    cap.set(4, 1080)\n\nclass_names = [name.strip() for name in open(label_path).readlines()]\nnum_classes = len(class_names)\n\n\nif net_type == \'vgg16-ssd\':\n    net = create_vgg_ssd(len(class_names), is_test=True)\nelif net_type == \'mb1-ssd\':\n    net = create_mobilenetv1_ssd(len(class_names), is_test=True)\nelif net_type == \'mb1-ssd-lite\':\n    net = create_mobilenetv1_ssd_lite(len(class_names), is_test=True)\nelif net_type == \'mb2-ssd-lite\':\n    net = create_mobilenetv2_ssd_lite(len(class_names), is_test=True)\nelif net_type == \'sq-ssd-lite\':\n    net = create_squeezenet_ssd_lite(len(class_names), is_test=True)\nelse:\n    print(""The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite."")\n    sys.exit(1)\nnet.load(model_path)\n\nif net_type == \'vgg16-ssd\':\n    predictor = create_vgg_ssd_predictor(net, candidate_size=200)\nelif net_type == \'mb1-ssd\':\n    predictor = create_mobilenetv1_ssd_predictor(net, candidate_size=200)\nelif net_type == \'mb1-ssd-lite\':\n    predictor = create_mobilenetv1_ssd_lite_predictor(net, candidate_size=200)\nelif net_type == \'mb2-ssd-lite\':\n    predictor = create_mobilenetv2_ssd_lite_predictor(net, candidate_size=200)\nelif net_type == \'sq-ssd-lite\':\n    predictor = create_squeezenet_ssd_lite_predictor(net, candidate_size=200)\nelse:\n    print(""The net type is wrong. It should be one of vgg16-ssd, mb1-ssd and mb1-ssd-lite."")\n    sys.exit(1)\n\n\ntimer = Timer()\nwhile True:\n    ret, orig_image = cap.read()\n    if orig_image is None:\n        continue\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    timer.start()\n    boxes, labels, probs = predictor.predict(image, 10, 0.4)\n    interval = timer.end()\n    print(\'Time: {:.2f}s, Detect Objects: {:d}.\'.format(interval, labels.size(0)))\n    for i in range(boxes.size(0)):\n        box = boxes[i, :]\n        label = f""{class_names[labels[i]]}: {probs[i]:.2f}""\n        cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), (255, 255, 0), 4)\n\n        cv2.putText(orig_image, label,\n                    (box[0]+20, box[1]+40),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    1,  # font scale\n                    (255, 0, 255),\n                    2)  # line type\n    cv2.imshow(\'annotated\', orig_image)\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\ncap.release()\ncv2.destroyAllWindows()\n'"
train_ssd.py,7,"b'import argparse\nimport os\nimport logging\nimport sys\nimport itertools\n\nimport torch\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n\nfrom vision.utils.misc import str2bool, Timer, freeze_net_layers, store_labels\nfrom vision.ssd.ssd import MatchPrior\nfrom vision.ssd.vgg_ssd import create_vgg_ssd\nfrom vision.ssd.mobilenetv1_ssd import create_mobilenetv1_ssd\nfrom vision.ssd.mobilenetv1_ssd_lite import create_mobilenetv1_ssd_lite\nfrom vision.ssd.mobilenet_v2_ssd_lite import create_mobilenetv2_ssd_lite\nfrom vision.ssd.squeezenet_ssd_lite import create_squeezenet_ssd_lite\nfrom vision.datasets.voc_dataset import VOCDataset\nfrom vision.datasets.open_images import OpenImagesDataset\nfrom vision.nn.multibox_loss import MultiboxLoss\nfrom vision.ssd.config import vgg_ssd_config\nfrom vision.ssd.config import mobilenetv1_ssd_config\nfrom vision.ssd.config import squeezenet_ssd_config\nfrom vision.ssd.data_preprocessing import TrainAugmentation, TestTransform\n\nparser = argparse.ArgumentParser(\n    description=\'Single Shot MultiBox Detector Training With Pytorch\')\n\nparser.add_argument(""--dataset_type"", default=""voc"", type=str,\n                    help=\'Specify dataset type. Currently support voc and open_images.\')\n\nparser.add_argument(\'--datasets\', nargs=\'+\', help=\'Dataset directory path\')\nparser.add_argument(\'--validation_dataset\', help=\'Dataset directory path\')\nparser.add_argument(\'--balance_data\', action=\'store_true\',\n                    help=""Balance training data by down-sampling more frequent labels."")\n\n\nparser.add_argument(\'--net\', default=""vgg16-ssd"",\n                    help=""The network architecture, it can be mb1-ssd, mb1-lite-ssd, mb2-ssd-lite or vgg16-ssd."")\nparser.add_argument(\'--freeze_base_net\', action=\'store_true\',\n                    help=""Freeze base net layers."")\nparser.add_argument(\'--freeze_net\', action=\'store_true\',\n                    help=""Freeze all the layers except the prediction head."")\n\nparser.add_argument(\'--mb2_width_mult\', default=1.0, type=float,\n                    help=\'Width Multiplifier for MobilenetV2\')\n\n# Params for SGD\nparser.add_argument(\'--lr\', \'--learning-rate\', default=1e-3, type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float,\n                    help=\'Momentum value for optim\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float,\n                    help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float,\n                    help=\'Gamma update for SGD\')\nparser.add_argument(\'--base_net_lr\', default=None, type=float,\n                    help=\'initial learning rate for base net.\')\nparser.add_argument(\'--extra_layers_lr\', default=None, type=float,\n                    help=\'initial learning rate for the layers not in base net and prediction heads.\')\n\n\n# Params for loading pretrained basenet or checkpoints.\nparser.add_argument(\'--base_net\',\n                    help=\'Pretrained base model\')\nparser.add_argument(\'--pretrained_ssd\', help=\'Pre-trained base model\')\nparser.add_argument(\'--resume\', default=None, type=str,\n                    help=\'Checkpoint state_dict file to resume training from\')\n\n# Scheduler\nparser.add_argument(\'--scheduler\', default=""multi-step"", type=str,\n                    help=""Scheduler for SGD. It can one of multi-step and cosine"")\n\n# Params for Multi-step Scheduler\nparser.add_argument(\'--milestones\', default=""80,100"", type=str,\n                    help=""milestones for MultiStepLR"")\n\n# Params for Cosine Annealing\nparser.add_argument(\'--t_max\', default=120, type=float,\n                    help=\'T_max value for Cosine Annealing Scheduler.\')\n\n# Train params\nparser.add_argument(\'--batch_size\', default=32, type=int,\n                    help=\'Batch size for training\')\nparser.add_argument(\'--num_epochs\', default=120, type=int,\n                    help=\'the number epochs\')\nparser.add_argument(\'--num_workers\', default=4, type=int,\n                    help=\'Number of workers used in dataloading\')\nparser.add_argument(\'--validation_epochs\', default=5, type=int,\n                    help=\'the number epochs\')\nparser.add_argument(\'--debug_steps\', default=100, type=int,\n                    help=\'Set the debug log output frequency.\')\nparser.add_argument(\'--use_cuda\', default=True, type=str2bool,\n                    help=\'Use CUDA to train model\')\n\nparser.add_argument(\'--checkpoint_folder\', default=\'models/\',\n                    help=\'Directory for saving checkpoint models\')\n\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\n                    format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\nargs = parser.parse_args()\nDEVICE = torch.device(""cuda:0"" if torch.cuda.is_available() and args.use_cuda else ""cpu"")\n\nif args.use_cuda and torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True\n    logging.info(""Use Cuda."")\n\n\ndef train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n    net.train(True)\n    running_loss = 0.0\n    running_regression_loss = 0.0\n    running_classification_loss = 0.0\n    for i, data in enumerate(loader):\n        images, boxes, labels = data\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        confidence, locations = net(images)\n        regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)  # TODO CHANGE BOXES\n        loss = regression_loss + classification_loss\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        running_regression_loss += regression_loss.item()\n        running_classification_loss += classification_loss.item()\n        if i and i % debug_steps == 0:\n            avg_loss = running_loss / debug_steps\n            avg_reg_loss = running_regression_loss / debug_steps\n            avg_clf_loss = running_classification_loss / debug_steps\n            logging.info(\n                f""Epoch: {epoch}, Step: {i}, "" +\n                f""Average Loss: {avg_loss:.4f}, "" +\n                f""Average Regression Loss {avg_reg_loss:.4f}, "" +\n                f""Average Classification Loss: {avg_clf_loss:.4f}""\n            )\n            running_loss = 0.0\n            running_regression_loss = 0.0\n            running_classification_loss = 0.0\n\n\ndef test(loader, net, criterion, device):\n    net.eval()\n    running_loss = 0.0\n    running_regression_loss = 0.0\n    running_classification_loss = 0.0\n    num = 0\n    for _, data in enumerate(loader):\n        images, boxes, labels = data\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n        num += 1\n\n        with torch.no_grad():\n            confidence, locations = net(images)\n            regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n            loss = regression_loss + classification_loss\n\n        running_loss += loss.item()\n        running_regression_loss += regression_loss.item()\n        running_classification_loss += classification_loss.item()\n    return running_loss / num, running_regression_loss / num, running_classification_loss / num\n\n\nif __name__ == \'__main__\':\n    timer = Timer()\n\n    logging.info(args)\n    if args.net == \'vgg16-ssd\':\n        create_net = create_vgg_ssd\n        config = vgg_ssd_config\n    elif args.net == \'mb1-ssd\':\n        create_net = create_mobilenetv1_ssd\n        config = mobilenetv1_ssd_config\n    elif args.net == \'mb1-ssd-lite\':\n        create_net = create_mobilenetv1_ssd_lite\n        config = mobilenetv1_ssd_config\n    elif args.net == \'sq-ssd-lite\':\n        create_net = create_squeezenet_ssd_lite\n        config = squeezenet_ssd_config\n    elif args.net == \'mb2-ssd-lite\':\n        create_net = lambda num: create_mobilenetv2_ssd_lite(num, width_mult=args.mb2_width_mult)\n        config = mobilenetv1_ssd_config\n    else:\n        logging.fatal(""The net type is wrong."")\n        parser.print_help(sys.stderr)\n        sys.exit(1)\n    train_transform = TrainAugmentation(config.image_size, config.image_mean, config.image_std)\n    target_transform = MatchPrior(config.priors, config.center_variance,\n                                  config.size_variance, 0.5)\n\n    test_transform = TestTransform(config.image_size, config.image_mean, config.image_std)\n\n    logging.info(""Prepare training datasets."")\n    datasets = []\n    for dataset_path in args.datasets:\n        if args.dataset_type == \'voc\':\n            dataset = VOCDataset(dataset_path, transform=train_transform,\n                                 target_transform=target_transform)\n            label_file = os.path.join(args.checkpoint_folder, ""voc-model-labels.txt"")\n            store_labels(label_file, dataset.class_names)\n            num_classes = len(dataset.class_names)\n        elif args.dataset_type == \'open_images\':\n            dataset = OpenImagesDataset(dataset_path,\n                 transform=train_transform, target_transform=target_transform,\n                 dataset_type=""train"", balance_data=args.balance_data)\n            label_file = os.path.join(args.checkpoint_folder, ""open-images-model-labels.txt"")\n            store_labels(label_file, dataset.class_names)\n            logging.info(dataset)\n            num_classes = len(dataset.class_names)\n\n        else:\n            raise ValueError(f""Dataset type {args.dataset_type} is not supported."")\n        datasets.append(dataset)\n    logging.info(f""Stored labels into file {label_file}."")\n    train_dataset = ConcatDataset(datasets)\n    logging.info(""Train dataset size: {}"".format(len(train_dataset)))\n    train_loader = DataLoader(train_dataset, args.batch_size,\n                              num_workers=args.num_workers,\n                              shuffle=True)\n    logging.info(""Prepare Validation datasets."")\n    if args.dataset_type == ""voc"":\n        val_dataset = VOCDataset(args.validation_dataset, transform=test_transform,\n                                 target_transform=target_transform, is_test=True)\n    elif args.dataset_type == \'open_images\':\n        val_dataset = OpenImagesDataset(dataset_path,\n                                        transform=test_transform, target_transform=target_transform,\n                                        dataset_type=""test"")\n        logging.info(val_dataset)\n    logging.info(""validation dataset size: {}"".format(len(val_dataset)))\n\n    val_loader = DataLoader(val_dataset, args.batch_size,\n                            num_workers=args.num_workers,\n                            shuffle=False)\n    logging.info(""Build network."")\n    net = create_net(num_classes)\n    min_loss = -10000.0\n    last_epoch = -1\n\n    base_net_lr = args.base_net_lr if args.base_net_lr is not None else args.lr\n    extra_layers_lr = args.extra_layers_lr if args.extra_layers_lr is not None else args.lr\n    if args.freeze_base_net:\n        logging.info(""Freeze base net."")\n        freeze_net_layers(net.base_net)\n        params = itertools.chain(net.source_layer_add_ons.parameters(), net.extras.parameters(),\n                                 net.regression_headers.parameters(), net.classification_headers.parameters())\n        params = [\n            {\'params\': itertools.chain(\n                net.source_layer_add_ons.parameters(),\n                net.extras.parameters()\n            ), \'lr\': extra_layers_lr},\n            {\'params\': itertools.chain(\n                net.regression_headers.parameters(),\n                net.classification_headers.parameters()\n            )}\n        ]\n    elif args.freeze_net:\n        freeze_net_layers(net.base_net)\n        freeze_net_layers(net.source_layer_add_ons)\n        freeze_net_layers(net.extras)\n        params = itertools.chain(net.regression_headers.parameters(), net.classification_headers.parameters())\n        logging.info(""Freeze all the layers except prediction heads."")\n    else:\n        params = [\n            {\'params\': net.base_net.parameters(), \'lr\': base_net_lr},\n            {\'params\': itertools.chain(\n                net.source_layer_add_ons.parameters(),\n                net.extras.parameters()\n            ), \'lr\': extra_layers_lr},\n            {\'params\': itertools.chain(\n                net.regression_headers.parameters(),\n                net.classification_headers.parameters()\n            )}\n        ]\n\n    timer.start(""Load Model"")\n    if args.resume:\n        logging.info(f""Resume from the model {args.resume}"")\n        net.load(args.resume)\n    elif args.base_net:\n        logging.info(f""Init from base net {args.base_net}"")\n        net.init_from_base_net(args.base_net)\n    elif args.pretrained_ssd:\n        logging.info(f""Init from pretrained ssd {args.pretrained_ssd}"")\n        net.init_from_pretrained_ssd(args.pretrained_ssd)\n    logging.info(f\'Took {timer.end(""Load Model""):.2f} seconds to load the model.\')\n\n    net.to(DEVICE)\n\n    criterion = MultiboxLoss(config.priors, iou_threshold=0.5, neg_pos_ratio=3,\n                             center_variance=0.1, size_variance=0.2, device=DEVICE)\n    optimizer = torch.optim.SGD(params, lr=args.lr, momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n    logging.info(f""Learning rate: {args.lr}, Base net learning rate: {base_net_lr}, ""\n                 + f""Extra Layers learning rate: {extra_layers_lr}."")\n\n    if args.scheduler == \'multi-step\':\n        logging.info(""Uses MultiStepLR scheduler."")\n        milestones = [int(v.strip()) for v in args.milestones.split("","")]\n        scheduler = MultiStepLR(optimizer, milestones=milestones,\n                                                     gamma=0.1, last_epoch=last_epoch)\n    elif args.scheduler == \'cosine\':\n        logging.info(""Uses CosineAnnealingLR scheduler."")\n        scheduler = CosineAnnealingLR(optimizer, args.t_max, last_epoch=last_epoch)\n    else:\n        logging.fatal(f""Unsupported Scheduler: {args.scheduler}."")\n        parser.print_help(sys.stderr)\n        sys.exit(1)\n\n    logging.info(f""Start training from epoch {last_epoch + 1}."")\n    for epoch in range(last_epoch + 1, args.num_epochs):\n        scheduler.step()\n        train(train_loader, net, criterion, optimizer,\n              device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)\n        \n        if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:\n            val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)\n            logging.info(\n                f""Epoch: {epoch}, "" +\n                f""Validation Loss: {val_loss:.4f}, "" +\n                f""Validation Regression Loss {val_regression_loss:.4f}, "" +\n                f""Validation Classification Loss: {val_classification_loss:.4f}""\n            )\n            model_path = os.path.join(args.checkpoint_folder, f""{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth"")\n            net.save(model_path)\n            logging.info(f""Saved model {model_path}"")\n'"
translate_tf_mobilenetv1.py,9,"b'import torch\nimport sys\n\nfrom vision.nn.mobilenet import MobileNetV1\nfrom extract_tf_weights import read_weights\n\n\ndef fill_weights_torch_model(weights, state_dict):\n    for name in state_dict:\n        if name == \'classifier.weight\':\n            weight = weights[\'MobilenetV1/Logits/Conv2d_1c_1x1/weights\']\n            weight = torch.tensor(weight, dtype=torch.float32).permute(3, 2, 0, 1)\n            assert state_dict[name].size() == weight.size()\n            state_dict[name] = weight\n        elif name == \'classifier.bias\':\n            bias = weights[\'MobilenetV1/Logits/Conv2d_1c_1x1/biases\']\n            bias = torch.tensor(bias, dtype=torch.float32)\n            assert state_dict[name].size() == bias.size()\n            state_dict[name] = bias\n        elif name.endswith(\'BatchNorm.weight\'):\n            key = name.replace(""features"", ""MobilenetV1"").replace(""."", ""/"").replace(\'BatchNorm/weight\', \'BatchNorm/gamma\')\n            weight = torch.tensor(weights[key], dtype=torch.float32)\n            assert weight.size() == state_dict[name].size()\n            state_dict[name] = weight\n        elif name.endswith(\'BatchNorm.bias\'):\n            key = name.replace(""features"", ""MobilenetV1"").replace(""."", ""/"").replace(\'BatchNorm/bias\', \'BatchNorm/beta\')\n            bias = torch.tensor(weights[key], dtype=torch.float32)\n            assert bias.size() == state_dict[name].size()\n            state_dict[name] = bias\n        elif name.endswith(\'running_mean\'):\n            key = name.replace(""features"", ""MobilenetV1"").replace(""."", ""/"").replace(\'running_mean\', \'moving_mean\')\n            running_mean = torch.tensor(weights[key], dtype=torch.float32)\n            assert running_mean.size() == state_dict[name].size()\n            state_dict[name] = running_mean\n        elif name.endswith(\'running_var\'):\n            key = name.replace(""features"", ""MobilenetV1"").replace(""."", ""/"").replace(\'running_var\', \'moving_variance\')\n            running_var = torch.tensor(weights[key], dtype=torch.float32)\n            assert running_var.size() == state_dict[name].size()\n            state_dict[name] = running_var\n        elif name.endswith(\'depthwise.weight\'):\n            key = name.replace(""features"", ""MobilenetV1"").replace(""."", ""/"")\n            key = key.replace(\'depthwise/weight\', \'depthwise/depthwise_weights\')\n            weight = torch.tensor(weights[key], dtype=torch.float32).permute(2, 3, 0, 1)\n            assert weight.size() == state_dict[name].size()\n            state_dict[name] = weight\n        else:\n            key = name.replace(""features"", ""MobilenetV1"").replace(""."", ""/"").replace(\'weight\', \'weights\')\n            weight = torch.tensor(weights[key], dtype=torch.float32).permute(3, 2, 0, 1)\n            assert weight.size() == state_dict[name].size()\n            state_dict[name] = weight\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 3:\n        print(""Usage: python translate_tf_modelnetv1.py <tf_model.pb> <pytorch_weights.pth>"")\n    tf_model = sys.argv[1]\n    torch_weights_path = sys.argv[2]\n    print(""Extract weights from tf model."")\n    weights = read_weights(tf_model)\n\n    net = MobileNetV1(1001)\n    states = net.state_dict()\n    print(""Translate tf weights."")\n    fill_weights_torch_model(weights, states)\n    torch.save(states, torch_weights_path)'"
visual_tf_models.py,0,"b'import tensorflow as tf\nfrom tensorflow.python.platform import gfile\nimport sys\nimport time\n\nif len(sys.argv) < 2:\n    print(""Usage: python visual_tf_model.py <model.pb>"")\n    sys.exit(0)\n\nmodel_file_name = sys.argv[1]\nwith tf.Session() as sess:\n    with gfile.FastGFile(model_file_name, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        g_in = tf.import_graph_def(graph_def)\nLOGDIR=\'log\'\ntrain_writer = tf.summary.FileWriter(LOGDIR)\ntrain_writer.add_graph(sess.graph)\n\nwhile True:\n    time.sleep(1000)'"
vision/__init__.py,0,b''
vision/datasets/__init__.py,0,b''
vision/datasets/collation.py,7,"b'import torch\nimport numpy as np\n\n\ndef object_detection_collate(batch):\n    images = []\n    gt_boxes = []\n    gt_labels = []\n    image_type = type(batch[0][0])\n    box_type = type(batch[0][1])\n    label_type = type(batch[0][2])\n    for image, boxes, labels in batch:\n        if image_type is np.ndarray:\n            images.append(torch.from_numpy(image))\n        elif image_type is torch.Tensor:\n            images.append(image)\n        else:\n            raise TypeError(f""Image should be tensor or np.ndarray, but got {image_type}."")\n        if box_type is np.ndarray:\n            gt_boxes.append(torch.from_numpy(boxes))\n        elif box_type is torch.Tensor:\n            gt_boxes.append(boxes)\n        else:\n            raise TypeError(f""Boxes should be tensor or np.ndarray, but got {box_type}."")\n        if label_type is np.ndarray:\n            gt_labels.append(torch.from_numpy(labels))\n        elif label_type is torch.Tensor:\n            gt_labels.append(labels)\n        else:\n            raise TypeError(f""Labels should be tensor or np.ndarray, but got {label_type}."")\n    return torch.stack(images), gt_boxes, gt_labels'"
vision/datasets/generate_vocdata.py,0,"b'import glob\nimport sys\nimport os\nimport xml.etree.ElementTree as ET\nfrom random import random\n\ndef main(filename):\n    # ratio to divide up the images\n    train = 0.7\n    val = 0.2\n    test = 0.1\n    if (train + test + val) != 1.0:\n        print(""probabilities must equal 1"")\n        exit()\n\n    # get the labels\n    labels = []\n    imgnames = []\n    annotations = {}\n\n    with open(filename, \'r\') as labelfile:\n        label_string = """"\n        for line in labelfile:\n                label_string += line.rstrip()\n\n    labels = label_string.split(\',\')\n    labels  = [elem.replace("" "", """") for elem in labels]\n\n    # get image names\n    for filename in os.listdir(""./JPEGImages""):\n        if filename.endswith("".jpg""):\n            img = filename.rstrip(\'.jpg\')\n            imgnames.append(img)\n\n    print(""Labels:"", labels, ""imgcnt:"", len(imgnames))\n\n    # initialise annotation list\n    for label in labels:\n        annotations[label] = []\n\n    # Scan the annotations for the labels\n    for img in imgnames:\n        annote = ""Annotations/"" + img + \'.xml\'\n        if os.path.isfile(annote):\n            tree = ET.parse(annote)\n            root = tree.getroot()\n            annote_labels = []\n            for labelname in root.findall(\'*/name\'):\n                labelname = labelname.text\n                annote_labels.append(labelname)\n                if labelname in labels:\n                    annotations[labelname].append(img)\n            annotations[img] = annote_labels\n        else:\n            print(""Missing annotation for "", annote)\n            exit() \n\n    # divvy up the images to the different sets\n    sampler = imgnames.copy()\n    train_list = []\n    val_list = []\n    test_list = []\n\n    while len(sampler) > 0:\n        dice = random()\n        elem = sampler.pop()\n\n        if dice <= test:\n            test_list.append(elem)\n        elif dice <= (test + val):\n            val_list.append(elem)\n        else:\n            train_list.append(elem) \n\n    print(""Training set:"", len(train_list), ""validation set:"", len(val_list), ""test set:"", len(test_list))\n\n\n    # create the dataset files\n    create_folder(""./ImageSets/Main/"")\n    with open(""./ImageSets/Main/train.txt"", \'w\') as outfile:\n        for name in train_list:\n            outfile.write(name + ""\\n"")\n    with open(""./ImageSets/Main/val.txt"", \'w\') as outfile:\n        for name in val_list:\n            outfile.write(name + ""\\n"")\n    with open(""./ImageSets/Main/trainval.txt"", \'w\') as outfile:\n        for name in train_list:\n            outfile.write(name + ""\\n"")\n        for name in val_list:\n            outfile.write(name + ""\\n"")\n\n    with open(""./ImageSets/Main/test.txt"", \'w\') as outfile:\n        for name in test_list:\n            outfile.write(name + ""\\n"")\n\n    # create the individiual files for each label\n    for label in labels:\n        with open(""./ImageSets/Main/""+ label +""_train.txt"", \'w\') as outfile:\n            for name in train_list:\n                if label in annotations[name]:\n                    outfile.write(name + "" 1\\n"")\n                else:\n                    outfile.write(name + "" -1\\n"")\n        with open(""./ImageSets/Main/""+ label +""_val.txt"", \'w\') as outfile:\n            for name in val_list:\n                if label in annotations[name]:\n                    outfile.write(name + "" 1\\n"")\n                else:\n                    outfile.write(name + "" -1\\n"")\n        with open(""./ImageSets/Main/""+ label +""_test.txt"", \'w\') as outfile:\n            for name in test_list:\n                if label in annotations[name]:\n                    outfile.write(name + "" 1\\n"")\n                else:\n                    outfile.write(name + "" -1\\n"")\n\ndef create_folder(foldername):\n    if os.path.exists(foldername):\n        print(\'folder already exists:\', foldername)\n    else:\n        os.makedirs(foldername)\n\nif __name__==\'__main__\':\n    if len(sys.argv) < 2:\n        print(""usage: python generate_vocdata.py <labelfile>"")\n        exit()\n    main(sys.argv[1])\n'"
vision/datasets/open_images.py,0,"b'import numpy as np\nimport pathlib\nimport cv2\nimport pandas as pd\nimport copy\n\nclass OpenImagesDataset:\n\n    def __init__(self, root,\n                 transform=None, target_transform=None,\n                 dataset_type=""train"", balance_data=False):\n        self.root = pathlib.Path(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.dataset_type = dataset_type.lower()\n\n        self.data, self.class_names, self.class_dict = self._read_data()\n        self.balance_data = balance_data\n        self.min_image_num = -1\n        if self.balance_data:\n            self.data = self._balance_data()\n        self.ids = [info[\'image_id\'] for info in self.data]\n\n        self.class_stat = None\n\n    def _getitem(self, index):\n        image_info = self.data[index]\n        image = self._read_image(image_info[\'image_id\'])\n        # duplicate boxes to prevent corruption of dataset\n        boxes = copy.copy(image_info[\'boxes\'])\n        boxes[:, 0] *= image.shape[1]\n        boxes[:, 1] *= image.shape[0]\n        boxes[:, 2] *= image.shape[1]\n        boxes[:, 3] *= image.shape[0]\n        # duplicate labels to prevent corruption of dataset\n        labels = copy.copy(image_info[\'labels\'])\n        if self.transform:\n            image, boxes, labels = self.transform(image, boxes, labels)\n        if self.target_transform:\n            boxes, labels = self.target_transform(boxes, labels)\n        return image_info[\'image_id\'], image, boxes, labels\n\n    def __getitem__(self, index):\n        _, image, boxes, labels = self._getitem(index)\n        return image, boxes, labels\n\n    def get_annotation(self, index):\n        """"""To conform the eval_ssd implementation that is based on the VOC dataset.""""""\n        image_id, image, boxes, labels = self._getitem(index)\n        is_difficult = np.zeros(boxes.shape[0], dtype=np.uint8)\n        return image_id, (boxes, labels, is_difficult)\n\n    def get_image(self, index):\n        image_info = self.data[index]\n        image = self._read_image(image_info[\'image_id\'])\n        if self.transform:\n            image, _ = self.transform(image)\n        return image\n\n    def _read_data(self):\n        annotation_file = f""{self.root}/sub-{self.dataset_type}-annotations-bbox.csv""\n        annotations = pd.read_csv(annotation_file)\n        class_names = [\'BACKGROUND\'] + sorted(list(annotations[\'ClassName\'].unique()))\n        class_dict = {class_name: i for i, class_name in enumerate(class_names)}\n        data = []\n        for image_id, group in annotations.groupby(""ImageID""):\n            boxes = group.loc[:, [""XMin"", ""YMin"", ""XMax"", ""YMax""]].values.astype(np.float32)\n            # make labels 64 bits to satisfy the cross_entropy function\n            labels = np.array([class_dict[name] for name in group[""ClassName""]], dtype=\'int64\')\n            data.append({\n                \'image_id\': image_id,\n                \'boxes\': boxes,\n                \'labels\': labels\n            })\n        return data, class_names, class_dict\n\n    def __len__(self):\n        return len(self.data)\n\n    def __repr__(self):\n        if self.class_stat is None:\n            self.class_stat = {name: 0 for name in self.class_names[1:]}\n            for example in self.data:\n                for class_index in example[\'labels\']:\n                    class_name = self.class_names[class_index]\n                    self.class_stat[class_name] += 1\n        content = [""Dataset Summary:""\n                   f""Number of Images: {len(self.data)}"",\n                   f""Minimum Number of Images for a Class: {self.min_image_num}"",\n                   ""Label Distribution:""]\n        for class_name, num in self.class_stat.items():\n            content.append(f""\\t{class_name}: {num}"")\n        return ""\\n"".join(content)\n\n    def _read_image(self, image_id):\n        image_file = self.root / self.dataset_type / f""{image_id}.jpg""\n        image = cv2.imread(str(image_file))\n        if image.shape[2] == 1:\n            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n\n    def _balance_data(self):\n        label_image_indexes = [set() for _ in range(len(self.class_names))]\n        for i, image in enumerate(self.data):\n            for label_id in image[\'labels\']:\n                label_image_indexes[label_id].add(i)\n        label_stat = [len(s) for s in label_image_indexes]\n        self.min_image_num = min(label_stat[1:])\n        sample_image_indexes = set()\n        for image_indexes in label_image_indexes[1:]:\n            image_indexes = np.array(list(image_indexes))\n            sub = np.random.permutation(image_indexes)[:self.min_image_num]\n            sample_image_indexes.update(sub)\n        sample_data = [self.data[i] for i in sample_image_indexes]\n        return sample_data\n\n\n\n\n\n'"
vision/datasets/voc_dataset.py,0,"b'import numpy as np\nimport logging\nimport pathlib\nimport xml.etree.ElementTree as ET\nimport cv2\nimport os\n\n\nclass VOCDataset:\n\n    def __init__(self, root, transform=None, target_transform=None, is_test=False, keep_difficult=False, label_file=None):\n        """"""Dataset for VOC data.\n        Args:\n            root: the root of the VOC2007 or VOC2012 dataset, the directory contains the following sub-directories:\n                Annotations, ImageSets, JPEGImages, SegmentationClass, SegmentationObject.\n        """"""\n        self.root = pathlib.Path(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        if is_test:\n            image_sets_file = self.root / ""ImageSets/Main/test.txt""\n        else:\n            image_sets_file = self.root / ""ImageSets/Main/trainval.txt""\n        self.ids = VOCDataset._read_image_ids(image_sets_file)\n        self.keep_difficult = keep_difficult\n\n        # if the labels file exists, read in the class names\n        label_file_name = self.root / ""labels.txt""\n\n        if os.path.isfile(label_file_name):\n            class_string = """"\n            with open(label_file_name, \'r\') as infile:\n                for line in infile:\n                    class_string += line.rstrip()\n\n            # classes should be a comma separated list\n            \n            classes = class_string.split(\',\')\n            # prepend BACKGROUND as first class\n            classes.insert(0, \'BACKGROUND\')\n            classes  = [ elem.replace("" "", """") for elem in classes]\n            self.class_names = tuple(classes)\n            logging.info(""VOC Labels read from file: "" + str(self.class_names))\n\n        else:\n            logging.info(""No labels file, using default VOC classes."")\n            self.class_names = (\'BACKGROUND\',\n            \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n            \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n            \'cow\', \'diningtable\', \'dog\', \'horse\',\n            \'motorbike\', \'person\', \'pottedplant\',\n            \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n\n        self.class_dict = {class_name: i for i, class_name in enumerate(self.class_names)}\n\n    def __getitem__(self, index):\n        image_id = self.ids[index]\n        boxes, labels, is_difficult = self._get_annotation(image_id)\n        if not self.keep_difficult:\n            boxes = boxes[is_difficult == 0]\n            labels = labels[is_difficult == 0]\n        image = self._read_image(image_id)\n        if self.transform:\n            image, boxes, labels = self.transform(image, boxes, labels)\n        if self.target_transform:\n            boxes, labels = self.target_transform(boxes, labels)\n        return image, boxes, labels\n\n    def get_image(self, index):\n        image_id = self.ids[index]\n        image = self._read_image(image_id)\n        if self.transform:\n            image, _ = self.transform(image)\n        return image\n\n    def get_annotation(self, index):\n        image_id = self.ids[index]\n        return image_id, self._get_annotation(image_id)\n\n    def __len__(self):\n        return len(self.ids)\n\n    @staticmethod\n    def _read_image_ids(image_sets_file):\n        ids = []\n        with open(image_sets_file) as f:\n            for line in f:\n                ids.append(line.rstrip())\n        return ids\n\n    def _get_annotation(self, image_id):\n        annotation_file = self.root / f""Annotations/{image_id}.xml""\n        objects = ET.parse(annotation_file).findall(""object"")\n        boxes = []\n        labels = []\n        is_difficult = []\n        for object in objects:\n            class_name = object.find(\'name\').text.lower().strip()\n            # we\'re only concerned with clases in our list\n            if class_name in self.class_dict:\n                bbox = object.find(\'bndbox\')\n\n                # VOC dataset format follows Matlab, in which indexes start from 0\n                x1 = float(bbox.find(\'xmin\').text) - 1\n                y1 = float(bbox.find(\'ymin\').text) - 1\n                x2 = float(bbox.find(\'xmax\').text) - 1\n                y2 = float(bbox.find(\'ymax\').text) - 1\n                boxes.append([x1, y1, x2, y2])\n\n                labels.append(self.class_dict[class_name])\n                is_difficult_str = object.find(\'difficult\').text\n                is_difficult.append(int(is_difficult_str) if is_difficult_str else 0)\n\n        return (np.array(boxes, dtype=np.float32),\n                np.array(labels, dtype=np.int64),\n                np.array(is_difficult, dtype=np.uint8))\n\n    def _read_image(self, image_id):\n        image_file = self.root / f""JPEGImages/{image_id}.jpg""\n        image = cv2.imread(str(image_file))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n\n\n\n'"
vision/nn/__init__.py,0,b''
vision/nn/alexnet.py,3,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n# copied from torchvision (https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py).\n# The forward function is modified for model pruning.\n\n__all__ = [\'AlexNet\', \'alexnet\']\n\n\nmodel_urls = {\n    \'alexnet\': \'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\',\n}\n\n\nclass AlexNet(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\ndef alexnet(pretrained=False, **kwargs):\n    r""""""AlexNet model architecture from the\n    `""One weird trick..."" <https://arxiv.org/abs/1404.5997>`_ paper.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = AlexNet(**kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'alexnet\']))\n    return model'"
vision/nn/mobilenet.py,2,"b'# borrowed from ""https://github.com/marvis/pytorch-mobilenet""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MobileNetV1(nn.Module):\n    def __init__(self, num_classes=1024):\n        super(MobileNetV1, self).__init__()\n\n        def conv_bn(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True)\n            )\n\n        def conv_dw(inp, oup, stride):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                nn.ReLU(inplace=True),\n\n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                nn.ReLU(inplace=True),\n            )\n\n        self.model = nn.Sequential(\n            conv_bn(3, 32, 2),\n            conv_dw(32, 64, 1),\n            conv_dw(64, 128, 2),\n            conv_dw(128, 128, 1),\n            conv_dw(128, 256, 2),\n            conv_dw(256, 256, 1),\n            conv_dw(256, 512, 2),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 1024, 2),\n            conv_dw(1024, 1024, 1),\n        )\n        self.fc = nn.Linear(1024, num_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = F.avg_pool2d(x, 7)\n        x = x.view(-1, 1024)\n        x = self.fc(x)\n        return x'"
vision/nn/mobilenet_v2.py,1,"b'import torch.nn as nn\nimport math\n\n# Modified from https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py.\n# In this version, Relu6 is replaced with Relu to make it ONNX compatible.\n# BatchNorm Layer is optional to make it easy do batch norm confusion.\n\n\ndef conv_bn(inp, oup, stride, use_batch_norm=True, onnx_compatible=False):\n    ReLU = nn.ReLU if onnx_compatible else nn.ReLU6\n\n    if use_batch_norm:\n        return nn.Sequential(\n            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(oup),\n            ReLU(inplace=True)\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n            ReLU(inplace=True)\n        )\n\n\ndef conv_1x1_bn(inp, oup, use_batch_norm=True, onnx_compatible=False):\n    ReLU = nn.ReLU if onnx_compatible else nn.ReLU6\n    if use_batch_norm:\n        return nn.Sequential(\n            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n            ReLU(inplace=True)\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n            ReLU(inplace=True)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio, use_batch_norm=True, onnx_compatible=False):\n        super(InvertedResidual, self).__init__()\n        ReLU = nn.ReLU if onnx_compatible else nn.ReLU6\n\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            if use_batch_norm:\n                self.conv = nn.Sequential(\n                    # dw\n                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                    nn.BatchNorm2d(hidden_dim),\n                    ReLU(inplace=True),\n                    # pw-linear\n                    nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                    nn.BatchNorm2d(oup),\n                )\n            else:\n                self.conv = nn.Sequential(\n                    # dw\n                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                    ReLU(inplace=True),\n                    # pw-linear\n                    nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                )\n        else:\n            if use_batch_norm:\n                self.conv = nn.Sequential(\n                    # pw\n                    nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                    nn.BatchNorm2d(hidden_dim),\n                    ReLU(inplace=True),\n                    # dw\n                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                    nn.BatchNorm2d(hidden_dim),\n                    ReLU(inplace=True),\n                    # pw-linear\n                    nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                    nn.BatchNorm2d(oup),\n                )\n            else:\n                self.conv = nn.Sequential(\n                    # pw\n                    nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                    ReLU(inplace=True),\n                    # dw\n                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                    ReLU(inplace=True),\n                    # pw-linear\n                    nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1., dropout_ratio=0.2,\n                 use_batch_norm=True, onnx_compatible=False):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2, onnx_compatible=onnx_compatible)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s,\n                                               expand_ratio=t, use_batch_norm=use_batch_norm,\n                                               onnx_compatible=onnx_compatible))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1,\n                                               expand_ratio=t, use_batch_norm=use_batch_norm,\n                                               onnx_compatible=onnx_compatible))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel,\n                                         use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_ratio),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n'"
vision/nn/multibox_loss.py,3,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\n\nfrom ..utils import box_utils\n\n\nclass MultiboxLoss(nn.Module):\n    def __init__(self, priors, iou_threshold, neg_pos_ratio,\n                 center_variance, size_variance, device):\n        """"""Implement SSD Multibox Loss.\n\n        Basically, Multibox loss combines classification loss\n         and Smooth L1 regression loss.\n        """"""\n        super(MultiboxLoss, self).__init__()\n        self.iou_threshold = iou_threshold\n        self.neg_pos_ratio = neg_pos_ratio\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.priors = priors\n        self.priors.to(device)\n\n    def forward(self, confidence, predicted_locations, labels, gt_locations):\n        """"""Compute classification loss and smooth l1 loss.\n\n        Args:\n            confidence (batch_size, num_priors, num_classes): class predictions.\n            locations (batch_size, num_priors, 4): predicted locations.\n            labels (batch_size, num_priors): real labels of all the priors.\n            boxes (batch_size, num_priors, 4): real boxes corresponding all the priors.\n        """"""\n        num_classes = confidence.size(2)\n        with torch.no_grad():\n            # derived from cross_entropy=sum(log(p))\n            loss = -F.log_softmax(confidence, dim=2)[:, :, 0]\n            mask = box_utils.hard_negative_mining(loss, labels, self.neg_pos_ratio)\n\n        confidence = confidence[mask, :]\n        classification_loss = F.cross_entropy(confidence.reshape(-1, num_classes), labels[mask], size_average=False)\n        pos_mask = labels > 0\n        predicted_locations = predicted_locations[pos_mask, :].reshape(-1, 4)\n        gt_locations = gt_locations[pos_mask, :].reshape(-1, 4)\n        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations, size_average=False)\n        num_pos = gt_locations.size(0)\n        return smooth_l1_loss/num_pos, classification_loss/num_pos\n'"
vision/nn/scaled_l2_norm.py,3,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\nclass ScaledL2Norm(nn.Module):\n    def __init__(self, in_channels, initial_scale):\n        super(ScaledL2Norm, self).__init__()\n        self.in_channels = in_channels\n        self.scale = nn.Parameter(torch.Tensor(in_channels))\n        self.initial_scale = initial_scale\n        self.reset_parameters()\n\n    def forward(self, x):\n        return (F.normalize(x, p=2, dim=1)\n                * self.scale.unsqueeze(0).unsqueeze(2).unsqueeze(3))\n\n    def reset_parameters(self):\n        self.scale.data.fill_(self.initial_scale)'"
vision/nn/squeezenet.py,6,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'SqueezeNet\', \'squeezenet1_0\', \'squeezenet1_1\']\n\n\nmodel_urls = {\n    \'squeezenet1_0\': \'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\',\n    \'squeezenet1_1\': \'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\',\n}\n\n\nclass Fire(nn.Module):\n\n    def __init__(self, inplanes, squeeze_planes,\n                 expand1x1_planes, expand3x3_planes):\n        super(Fire, self).__init__()\n        self.inplanes = inplanes\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n                                   kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n                                   kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n\nclass SqueezeNet(nn.Module):\n\n    def __init__(self, version=1.0, num_classes=1000):\n        super(SqueezeNet, self).__init__()\n        if version not in [1.0, 1.1]:\n            raise ValueError(""Unsupported SqueezeNet version {version}:""\n                             ""1.0 or 1.1 expected"".format(version=version))\n        self.num_classes = num_classes\n        if version == 1.0:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                Fire(128, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(256, 32, 128, 128),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n                Fire(512, 64, 256, 256),\n            )\n        else:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256),\n            )\n        # Final convolution is initialized differently form the rest\n        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            final_conv,\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d(13, stride=1)\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m is final_conv:\n                    init.normal_(m.weight, mean=0.0, std=0.01)\n                else:\n                    init.kaiming_uniform_(m.weight)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x.view(x.size(0), self.num_classes)\n\n\ndef squeezenet1_0(pretrained=False, **kwargs):\n    r""""""SqueezeNet model architecture from the `""SqueezeNet: AlexNet-level\n    accuracy with 50x fewer parameters and <0.5MB model size""\n    <https://arxiv.org/abs/1602.07360>`_ paper.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = SqueezeNet(version=1.0, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'squeezenet1_0\']))\n    return model\n\n\ndef squeezenet1_1(pretrained=False, **kwargs):\n    r""""""SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = SqueezeNet(version=1.1, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'squeezenet1_1\']))\n    return model\n'"
vision/nn/vgg.py,1,"b""import torch.nn as nn\n\n\n# borrowed from https://github.com/amdegroot/ssd.pytorch/blob/master/ssd.py\ndef vgg(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == 'C':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers"""
vision/prunning/__init__.py,0,b''
vision/prunning/prunner.py,9,"b'import torch\nimport torch.nn as nn\nimport logging\nfrom heapq import nsmallest\n\nfrom ..utils.model_book import ModelBook\n\n\nclass ModelPrunner:\n    def __init__(self, model, train_fun, ignored_paths=[]):\n        """""" Implement the pruning algorithm described in the paper https://arxiv.org/pdf/1611.06440.pdf .\n\n        The prunning criteria is dC/dh * h, while C is the cost, h is the activation.\n        """"""\n        self.model = model\n        self.train_fun = train_fun\n        self.ignored_paths = ignored_paths\n        self.book = ModelBook(self.model)\n        self.outputs = {}\n        self.grads = {}\n        self.handles = []\n        self.decendent_batch_norms = {}  # descendants impacted by the conv layers.\n        self.last_conv_path = None    # used to trace the graph\n        self.descendent_convs = {}    # descendants impacted by the conv layers.\n        self.descendent_linears = {}  # descendants impacted by the linear layers.\n        self.last_linear_path = None  # used to trace the graph\n\n    def _make_new_conv(self, conv, filter_index, channel_type=""out""):\n        if not isinstance(conv, nn.Conv2d):\n            raise TypeError(f""The module is not Conv2d, but {type(conv)}."")\n\n        if channel_type == ""out"":\n            new_conv = nn.Conv2d(conv.in_channels, conv.out_channels - 1, conv.kernel_size, conv.stride,\n                                 conv.padding, conv.dilation, conv.groups, conv.bias is not None)\n            mask = torch.ones(conv.out_channels, dtype=torch.uint8)\n            mask[filter_index] = 0\n            new_conv.weight.data = conv.weight.data[mask, :, :, :]\n            if conv.bias is not None:\n                new_conv.bias.data = conv.bias.data[mask]\n\n        elif channel_type == \'in\':\n            new_conv = nn.Conv2d(conv.in_channels - 1, conv.out_channels, conv.kernel_size, conv.stride,\n                                 conv.padding, conv.dilation, conv.groups, conv.bias is not None)\n            mask = torch.ones(conv.in_channels, dtype=torch.uint8)\n            mask[filter_index] = 0\n            new_conv.weight.data = conv.weight.data[:, mask, :, :]\n            if conv.bias is not None:\n                new_conv.bias.data = conv.bias.data\n        else:\n            raise ValueError(f""{channel_type} should be either \'in\' or \'out\'."")\n        return new_conv\n\n    def remove_conv_filter(self, path, filter_index):\n        conv = self.book.get_module(path)\n        logging.info(f\'Prune Conv: {""/"".join(path)}, Filter: {filter_index}, Layer: {conv}\')\n        new_conv = self._make_new_conv(conv, filter_index, channel_type=""out"")\n        self._update_model(path, new_conv)\n\n        next_conv_path = self.descendent_convs.get(path)\n        if next_conv_path:\n            next_conv = self.book.get_module(next_conv_path)\n            new_next_conv = self._make_new_conv(next_conv, filter_index, channel_type=""in"")\n            self._update_model(next_conv_path, new_next_conv)\n\n        # reduce the num_features of batch norm\n        batch_norm_path = self.decendent_batch_norms.get(path)\n        if batch_norm_path:\n            batch_norm = self.book.get_module(batch_norm_path)\n            new_batch_norm = nn.BatchNorm2d(batch_norm.num_features - 1)\n            self._update_model(batch_norm_path, new_batch_norm)\n\n        # reduce the in channels of linear layer\n        linear_path = self.descendent_linears.get(path)\n        if linear_path:\n            linear = self.book.get_module(linear_path)\n            new_linear = self._make_new_linear(linear, filter_index, conv, channel_type=""in"")\n            self._update_model(linear_path, new_linear)\n\n    @staticmethod\n    def _make_new_linear(linear, feature_index, conv=None, channel_type=""out""):\n        if channel_type == ""out"":\n            new_linear = nn.Linear(linear.in_features, linear.out_features - 1,\n                                   bias=linear.bias is not None)\n            mask = torch.ones(linear.out_features, dtype=torch.uint8)\n            mask[feature_index] = 0\n            new_linear.weight.data = linear.weight.data[mask, :]\n            if linear.bias is not None:\n                new_linear.bias.data = linear.bias.data[mask]\n        elif channel_type == ""in"":\n            if conv:\n                block = int(linear.in_features / conv.out_channels)\n            else:\n                block = 1\n            new_linear = nn.Linear(linear.in_features - block, linear.out_features,\n                                   bias=linear.bias is not None)\n            start_index = feature_index * block\n            end_index = (feature_index + 1) * block\n            mask = torch.ones(linear.in_features, dtype=torch.uint8)\n            mask[start_index: end_index] = 0\n            new_linear.weight.data = linear.weight.data[:, mask]\n            if linear.bias is not None:\n                new_linear.bias.data = linear.bias.data\n        else:\n            raise ValueError(f""{channel_type} should be either \'in\' or \'out\'."")\n        return new_linear\n\n    def prune_conv_layers(self, num=1):\n        """"""Prune one conv2d filter.\n        """"""\n        self.register_conv_hooks()\n        before_loss, before_accuracy = self.train_fun(self.model)\n        ranks = []\n        for path, output in self.outputs.items():\n            output = output.data\n            grad = self.grads[path].data\n            v = grad * output\n            v = v.sum(0).sum(1).sum(1)  # sum to the channel axis.\n            v = torch.abs(v)\n            v = v / torch.sqrt(torch.sum(v * v))  # normalize\n            for i, e in enumerate(v):\n                ranks.append((path, i, e))\n        to_prune = nsmallest(num, ranks, key=lambda t: t[2])\n        to_prune = sorted(to_prune, key=lambda t: (t[0], -t[1]))  # prune the filters with bigger indexes first to avoid rearrangement.\n        for path, filter_index, value in to_prune:\n            self.remove_conv_filter(path, filter_index)\n        self.deregister_hooks()\n        after_loss, after_accuracy = self.train_fun(self.model)\n        return after_loss - before_loss, after_accuracy - before_accuracy\n\n    def register_conv_hooks(self):\n        """"""Run register before training for pruning.""""""\n        self.outputs.clear()\n        self.grads.clear()\n        self.handles.clear()\n        self.last_conv_path = None\n        self.decendent_batch_norms.clear()\n        self.descendent_convs.clear()\n        self.descendent_linears.clear()\n\n        def forward_hook(m, input, output):\n            path = self.book.get_path(m)\n            if isinstance(m, nn.Conv2d):\n                if path not in self.ignored_paths:\n                    self.outputs[path] = output\n                if self.last_conv_path:\n                    self.descendent_convs[self.last_conv_path] = path\n                self.last_conv_path = path\n            elif isinstance(m, nn.BatchNorm2d):\n                if self.last_conv_path:\n                    self.decendent_batch_norms[self.last_conv_path] = path\n            elif isinstance(m, nn.Linear):\n                if self.last_conv_path:\n                    self.descendent_linears[self.last_conv_path] = path\n                self.last_conv_path = None  # after a linear layer the conv layer doesn\'t matter\n\n        def backward_hook(m, input, output):\n            path = self.book.get_path(m)\n            self.grads[path] = output[0]\n\n        for path, m in self.book.modules(module_type=(nn.Conv2d, nn.BatchNorm2d, nn.Linear)):\n            h = m.register_forward_hook(forward_hook)\n            self.handles.append(h)\n            h = m.register_backward_hook(backward_hook)\n            self.handles.append(h)\n\n    def deregister_hooks(self):\n        """"""Run degresiter before retraining to recover the model""""""\n        for handle in self.handles:\n            handle.remove()\n\n    def prune_linear_layers(self, num=1):\n        self.register_linear_hooks()\n        before_loss, before_accuracy = self.train_fun(self.model)\n        ranks = []\n        for path, output in self.outputs.items():\n            output = output.data\n            grad = self.grads[path].data\n            v = grad * output\n            v = v.sum(0)  # sum to the channel axis.\n            v = torch.abs(v)\n            v = v / torch.sqrt(torch.sum(v * v))  # normalize\n            for i, e in enumerate(v):\n                ranks.append((path, i, e))\n        to_prune = nsmallest(num, ranks, key=lambda t: t[2])\n        to_prune = sorted(to_prune, key=lambda t: (t[0], -t[1]))\n        for path, feature_index, value in to_prune:\n            self.remove_linear_feature(path, feature_index)\n        self.deregister_hooks()\n        after_loss, after_accuracy = self.train_fun(self.model)\n        return after_loss - before_loss, after_accuracy - before_accuracy\n\n    def register_linear_hooks(self):\n        self.outputs.clear()\n        self.grads.clear()\n        self.handles.clear()\n        self.descendent_linears.clear()\n        self.last_linear_path = None\n\n        def forward_hook(m, input, output):\n            path = self.book.get_path(m)\n            if path not in self.ignored_paths:\n                self.outputs[path] = output\n            if self.last_linear_path:\n                self.descendent_linears[self.last_linear_path] = path\n            self.last_linear_path = path\n\n        def backward_hook(m, input, output):\n            path = self.book.get_path(m)\n            self.grads[path] = output[0]\n\n        for _, m in self.book.linear_modules():\n            h = m.register_forward_hook(forward_hook)\n            self.handles.append(h)\n            h = m.register_backward_hook(backward_hook)\n            self.handles.append(h)\n\n    def remove_linear_feature(self, path, feature_index):\n        linear = self.book.get_module(path)\n        logging.info(f\'Prune Linear: {""/"".join(path)}, Filter: {feature_index}, Layer: {linear}\')\n        new_linear = self._make_new_linear(linear, feature_index, channel_type=""out"")\n        self._update_model(path, new_linear)\n\n        # update following linear layers\n        next_linear_path = self.descendent_linears.get(path)\n        if next_linear_path:\n            next_linear = self.book.get_module(next_linear_path)\n            new_next_linear = self._make_new_linear(next_linear, feature_index, channel_type=\'in\')\n            self._update_model(next_linear_path, new_next_linear)\n\n    def _update_model(self, path, module):\n        parent = self.book.get_module(path[:-1])\n        parent._modules[path[-1]] = module\n        self.book.update(path, module)\n'"
vision/ssd/__init__.py,0,b''
vision/ssd/data_preprocessing.py,0,"b'from ..transforms.transforms import *\n\n\nclass TrainAugmentation:\n    def __init__(self, size, mean=0, std=1.0):\n        """"""\n        Args:\n            size: the size the of final image.\n            mean: mean pixel value per channel.\n        """"""\n        self.mean = mean\n        self.size = size\n        self.augment = Compose([\n            ConvertFromInts(),\n            PhotometricDistort(),\n            Expand(self.mean),\n            RandomSampleCrop(),\n            RandomMirror(),\n            ToPercentCoords(),\n            Resize(self.size),\n            SubtractMeans(self.mean),\n            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n            ToTensor(),\n        ])\n\n    def __call__(self, img, boxes, labels):\n        """"""\n\n        Args:\n            img: the output of cv.imread in RGB layout.\n            boxes: boundding boxes in the form of (x1, y1, x2, y2).\n            labels: labels of boxes.\n        """"""\n        return self.augment(img, boxes, labels)\n\n\nclass TestTransform:\n    def __init__(self, size, mean=0.0, std=1.0):\n        self.transform = Compose([\n            ToPercentCoords(),\n            Resize(size),\n            SubtractMeans(mean),\n            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n            ToTensor(),\n        ])\n\n    def __call__(self, image, boxes, labels):\n        return self.transform(image, boxes, labels)\n\n\nclass PredictionTransform:\n    def __init__(self, size, mean=0.0, std=1.0):\n        self.transform = Compose([\n            Resize(size),\n            SubtractMeans(mean),\n            lambda img, boxes=None, labels=None: (img / std, boxes, labels),\n            ToTensor()\n        ])\n\n    def __call__(self, image):\n        image, _, _ = self.transform(image)\n        return image'"
vision/ssd/fpn_mobilenetv1_ssd.py,2,"b""import torch\nfrom torch.nn import Conv2d, Sequential, ModuleList, ReLU\nfrom ..nn.mobilenet import MobileNetV1\n\nfrom .fpn_ssd import FPNSSD\nfrom .predictor import Predictor\nfrom .config import mobilenetv1_ssd_config as config\n\n\ndef create_fpn_mobilenetv1_ssd(num_classes):\n    base_net = MobileNetV1(1001).features  # disable dropout layer\n\n    source_layer_indexes = [\n        (69, Conv2d(in_channels=512, out_channels=256, kernel_size=1)),\n        (len(base_net), Conv2d(in_channels=1024, out_channels=256, kernel_size=1)),\n    ]\n    extras = ModuleList([\n        Sequential(\n            Conv2d(in_channels=1024, out_channels=256, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        )\n    ])\n\n    regression_headers = ModuleList([\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),   #  TODO: change to kernel_size=1, padding=0?\n    ])\n\n    classification_headers = ModuleList([\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n    ])\n\n    return FPNSSD(num_classes, base_net, source_layer_indexes,\n               extras, classification_headers, regression_headers)\n\n\ndef create_fpn_mobilenetv1_ssd_predictor(net, candidate_size=200, nms_method=None, sigma=0.5, device=torch.device('cpu')):\n    predictor = Predictor(net, config.image_size, config.image_mean, config.priors,\n                          config.center_variance, config.size_variance,\n                          nms_method=nms_method,\n                          iou_threshold=config.iou_threshold,\n                          candidate_size=candidate_size,\n                          sigma=sigma,\n                          device=device)\n    return predictor\n"""
vision/ssd/fpn_ssd.py,10,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import List, Tuple\n\nfrom ..utils import box_utils\n\n\nclass FPNSSD(nn.Module):\n    def __init__(self, num_classes: int, base_net: nn.ModuleList, source_layer_indexes: List[int],\n                 extras: nn.ModuleList, classification_headers: nn.ModuleList,\n                 regression_headers: nn.ModuleList, upsample_mode=""nearest""):\n        """"""Compose a SSD model using the given components.\n        """"""\n        super(FPNSSD, self).__init__()\n\n        self.num_classes = num_classes\n        self.base_net = base_net\n        self.source_layer_indexes = source_layer_indexes\n        self.extras = extras\n        self.classification_headers = classification_headers\n        self.regression_headers = regression_headers\n        self.upsample_mode = upsample_mode\n\n        # register layers in source_layer_indexes by adding them to a module list\n        self.source_layer_add_ons = nn.ModuleList([t[1] for t in source_layer_indexes if isinstance(t, tuple)])\n        self.upsamplers = [\n            nn.Upsample(size=(19, 19), mode=\'bilinear\'),\n            nn.Upsample(size=(10, 10), mode=\'bilinear\'),\n            nn.Upsample(size=(5, 5), mode=\'bilinear\'),\n            nn.Upsample(size=(3, 3), mode=\'bilinear\'),\n            nn.Upsample(size=(2, 2), mode=\'bilinear\'),\n        ]\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        confidences = []\n        locations = []\n        start_layer_index = 0\n        header_index = 0\n        features = []\n        for end_layer_index in self.source_layer_indexes:\n\n            if isinstance(end_layer_index, tuple):\n                added_layer = end_layer_index[1]\n                end_layer_index = end_layer_index[0]\n            else:\n                added_layer = None\n            for layer in self.base_net[start_layer_index: end_layer_index]:\n                x = layer(x)\n            start_layer_index = end_layer_index\n            if added_layer:\n                y = added_layer(x)\n            else:\n                y = x\n            #confidence, location = self.compute_header(header_index, y)\n            features.append(y)\n            header_index += 1\n            # confidences.append(confidence)\n            # locations.append(location)\n\n        for layer in self.base_net[end_layer_index:]:\n            x = layer(x)\n\n        for layer in self.extras:\n            x = layer(x)\n            #confidence, location = self.compute_header(header_index, x)\n            features.append(x)\n            header_index += 1\n            # confidences.append(confidence)\n            # locations.append(location)\n\n        upstream_feature = None\n        for i in range(len(features) - 1, -1, -1):\n            feature = features[i]\n            if upstream_feature is not None:\n                upstream_feature = self.upsamplers[i](upstream_feature)\n                upstream_feature += feature\n            else:\n                upstream_feature = feature\n            confidence, location = self.compute_header(i, upstream_feature)\n            confidences.append(confidence)\n            locations.append(location)\n        confidences = torch.cat(confidences, 1)\n        locations = torch.cat(locations, 1)\n        return confidences, locations\n\n    def compute_header(self, i, x):\n        confidence = self.classification_headers[i](x)\n        confidence = confidence.permute(0, 2, 3, 1).contiguous()\n        confidence = confidence.view(confidence.size(0), -1, self.num_classes)\n\n        location = self.regression_headers[i](x)\n        location = location.permute(0, 2, 3, 1).contiguous()\n        location = location.view(location.size(0), -1, 4)\n\n        return confidence, location\n\n    def init_from_base_net(self, model):\n        self.base_net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage), strict=False)\n        self.source_layer_add_ons.apply(_xavier_init_)\n        self.extras.apply(_xavier_init_)\n        self.classification_headers.apply(_xavier_init_)\n        self.regression_headers.apply(_xavier_init_)\n\n    def init(self):\n        self.base_net.apply(_xavier_init_)\n        self.source_layer_add_ons.apply(_xavier_init_)\n        self.extras.apply(_xavier_init_)\n        self.classification_headers.apply(_xavier_init_)\n        self.regression_headers.apply(_xavier_init_)\n\n    def load(self, model):\n        self.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n\n    def save(self, model_path):\n        torch.save(self.state_dict(), model_path)\n\n\nclass MatchPrior(object):\n    def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n        self.center_form_priors = center_form_priors\n        self.corner_form_priors = box_utils.center_form_to_corner_form(center_form_priors)\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.iou_threshold = iou_threshold\n\n    def __call__(self, gt_boxes, gt_labels):\n        if type(gt_boxes) is np.ndarray:\n            gt_boxes = torch.from_numpy(gt_boxes)\n        if type(gt_labels) is np.ndarray:\n            gt_labels = torch.from_numpy(gt_labels)\n        boxes, labels = box_utils.assign_priors(gt_boxes, gt_labels,\n                                                self.corner_form_priors, self.iou_threshold)\n        boxes = box_utils.corner_form_to_center_form(boxes)\n        locations = box_utils.convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n        return locations, labels\n\n\ndef _xavier_init_(m: nn.Module):\n    if isinstance(m, nn.Conv2d):\n        nn.init.xavier_uniform_(m.weight)\n'"
vision/ssd/mobilenet_v2_ssd_lite.py,2,"b'import torch\nfrom torch.nn import Conv2d, Sequential, ModuleList, BatchNorm2d\nfrom torch import nn\nfrom ..nn.mobilenet_v2 import MobileNetV2, InvertedResidual\n\nfrom .ssd import SSD, GraphPath\nfrom .predictor import Predictor\nfrom .config import mobilenetv1_ssd_config as config\n\n\ndef SeperableConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, onnx_compatible=False):\n    """"""Replace Conv2d with a depthwise Conv2d and Pointwise Conv2d.\n    """"""\n    ReLU = nn.ReLU if onnx_compatible else nn.ReLU6\n    return Sequential(\n        Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n               groups=in_channels, stride=stride, padding=padding),\n        BatchNorm2d(in_channels),\n        ReLU(),\n        Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n    )\n\n\ndef create_mobilenetv2_ssd_lite(num_classes, width_mult=1.0, use_batch_norm=True, onnx_compatible=False, is_test=False):\n    base_net = MobileNetV2(width_mult=width_mult, use_batch_norm=use_batch_norm,\n                           onnx_compatible=onnx_compatible).features\n\n    source_layer_indexes = [\n        GraphPath(14, \'conv\', 3),\n        19,\n    ]\n    extras = ModuleList([\n        InvertedResidual(1280, 512, stride=2, expand_ratio=0.2),\n        InvertedResidual(512, 256, stride=2, expand_ratio=0.25),\n        InvertedResidual(256, 256, stride=2, expand_ratio=0.5),\n        InvertedResidual(256, 64, stride=2, expand_ratio=0.25)\n    ])\n\n    regression_headers = ModuleList([\n        SeperableConv2d(in_channels=round(576 * width_mult), out_channels=6 * 4,\n                        kernel_size=3, padding=1, onnx_compatible=False),\n        SeperableConv2d(in_channels=1280, out_channels=6 * 4, kernel_size=3, padding=1, onnx_compatible=False),\n        SeperableConv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1, onnx_compatible=False),\n        SeperableConv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1, onnx_compatible=False),\n        SeperableConv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1, onnx_compatible=False),\n        Conv2d(in_channels=64, out_channels=6 * 4, kernel_size=1),\n    ])\n\n    classification_headers = ModuleList([\n        SeperableConv2d(in_channels=round(576 * width_mult), out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=1280, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=64, out_channels=6 * num_classes, kernel_size=1),\n    ])\n\n    return SSD(num_classes, base_net, source_layer_indexes,\n               extras, classification_headers, regression_headers, is_test=is_test, config=config)\n\n\ndef create_mobilenetv2_ssd_lite_predictor(net, candidate_size=200, nms_method=None, sigma=0.5, device=torch.device(\'cpu\')):\n    predictor = Predictor(net, config.image_size, config.image_mean,\n                          config.image_std,\n                          nms_method=nms_method,\n                          iou_threshold=config.iou_threshold,\n                          candidate_size=candidate_size,\n                          sigma=sigma,\n                          device=device)\n    return predictor\n'"
vision/ssd/mobilenetv1_ssd.py,1,"b'import torch\nfrom torch.nn import Conv2d, Sequential, ModuleList, ReLU\nfrom ..nn.mobilenet import MobileNetV1\n\nfrom .ssd import SSD\nfrom .predictor import Predictor\nfrom .config import mobilenetv1_ssd_config as config\n\n\ndef create_mobilenetv1_ssd(num_classes, is_test=False):\n    base_net = MobileNetV1(1001).model  # disable dropout layer\n\n    source_layer_indexes = [\n        12,\n        14,\n    ]\n    extras = ModuleList([\n        Sequential(\n            Conv2d(in_channels=1024, out_channels=256, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        )\n    ])\n\n    regression_headers = ModuleList([\n        Conv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=1024, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n    ])\n\n    classification_headers = ModuleList([\n        Conv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=1024, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n    ])\n\n    return SSD(num_classes, base_net, source_layer_indexes,\n               extras, classification_headers, regression_headers, is_test=is_test, config=config)\n\n\ndef create_mobilenetv1_ssd_predictor(net, candidate_size=200, nms_method=None, sigma=0.5, device=None):\n    predictor = Predictor(net, config.image_size, config.image_mean,\n                          config.image_std,\n                          nms_method=nms_method,\n                          iou_threshold=config.iou_threshold,\n                          candidate_size=candidate_size,\n                          sigma=sigma,\n                          device=device)\n    return predictor\n'"
vision/ssd/mobilenetv1_ssd_lite.py,1,"b'import torch\nfrom torch.nn import Conv2d, Sequential, ModuleList, ReLU, BatchNorm2d\nfrom ..nn.mobilenet import MobileNetV1\n\nfrom .ssd import SSD\nfrom .predictor import Predictor\nfrom .config import mobilenetv1_ssd_config as config\n\n\ndef SeperableConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n    """"""Replace Conv2d with a depthwise Conv2d and Pointwise Conv2d.\n    """"""\n    return Sequential(\n        Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n               groups=in_channels, stride=stride, padding=padding),\n        ReLU(),\n        Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n    )\n\n\ndef create_mobilenetv1_ssd_lite(num_classes, is_test=False):\n    base_net = MobileNetV1(1001).model  # disable dropout layer\n\n    source_layer_indexes = [\n        12,\n        14,\n    ]\n    extras = ModuleList([\n        Sequential(\n            Conv2d(in_channels=1024, out_channels=256, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n        ),\n        Sequential(\n            Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n        )\n    ])\n\n    regression_headers = ModuleList([\n        SeperableConv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=1024, out_channels=6 * 4, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=1),\n    ])\n\n    classification_headers = ModuleList([\n        SeperableConv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=1024, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=1),\n    ])\n\n    return SSD(num_classes, base_net, source_layer_indexes,\n               extras, classification_headers, regression_headers, is_test=is_test, config=config)\n\n\ndef create_mobilenetv1_ssd_lite_predictor(net, candidate_size=200, nms_method=None, sigma=0.5, device=None):\n    predictor = Predictor(net, config.image_size, config.image_mean,\n                          config.image_std,\n                          nms_method=nms_method,\n                          iou_threshold=config.iou_threshold,\n                          candidate_size=candidate_size,\n                          sigma=sigma,\n                          device=device)\n    return predictor\n'"
vision/ssd/predictor.py,7,"b'import torch\n\nfrom ..utils import box_utils\nfrom .data_preprocessing import PredictionTransform\nfrom ..utils.misc import Timer\n\n\nclass Predictor:\n    def __init__(self, net, size, mean=0.0, std=1.0, nms_method=None,\n                 iou_threshold=0.45, filter_threshold=0.01, candidate_size=200, sigma=0.5, device=None):\n        self.net = net\n        self.transform = PredictionTransform(size, mean, std)\n        self.iou_threshold = iou_threshold\n        self.filter_threshold = filter_threshold\n        self.candidate_size = candidate_size\n        self.nms_method = nms_method\n\n        self.sigma = sigma\n        if device:\n            self.device = device\n        else:\n            self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n        self.net.to(self.device)\n        self.net.eval()\n\n        self.timer = Timer()\n\n    def predict(self, image, top_k=-1, prob_threshold=None):\n        cpu_device = torch.device(""cpu"")\n        height, width, _ = image.shape\n        image = self.transform(image)\n        images = image.unsqueeze(0)\n        images = images.to(self.device)\n        with torch.no_grad():\n            self.timer.start()\n            scores, boxes = self.net.forward(images)\n            print(""Inference time: "", self.timer.end())\n        boxes = boxes[0]\n        scores = scores[0]\n        if not prob_threshold:\n            prob_threshold = self.filter_threshold\n        # this version of nms is slower on GPU, so we move data to CPU.\n        boxes = boxes.to(cpu_device)\n        scores = scores.to(cpu_device)\n        picked_box_probs = []\n        picked_labels = []\n        for class_index in range(1, scores.size(1)):\n            probs = scores[:, class_index]\n            mask = probs > prob_threshold\n            probs = probs[mask]\n            if probs.size(0) == 0:\n                continue\n            subset_boxes = boxes[mask, :]\n            box_probs = torch.cat([subset_boxes, probs.reshape(-1, 1)], dim=1)\n            box_probs = box_utils.nms(box_probs, self.nms_method,\n                                      score_threshold=prob_threshold,\n                                      iou_threshold=self.iou_threshold,\n                                      sigma=self.sigma,\n                                      top_k=top_k,\n                                      candidate_size=self.candidate_size)\n            picked_box_probs.append(box_probs)\n            picked_labels.extend([class_index] * box_probs.size(0))\n        if not picked_box_probs:\n            return torch.tensor([]), torch.tensor([]), torch.tensor([])\n        picked_box_probs = torch.cat(picked_box_probs)\n        picked_box_probs[:, 0] *= width\n        picked_box_probs[:, 1] *= height\n        picked_box_probs[:, 2] *= width\n        picked_box_probs[:, 3] *= height\n        return picked_box_probs[:, :4], torch.tensor(picked_labels), picked_box_probs[:, 4]'"
vision/ssd/squeezenet_ssd_lite.py,2,"b'import torch\nfrom torch.nn import Conv2d, Sequential, ModuleList, ReLU\nfrom ..nn.squeezenet import squeezenet1_1\n\nfrom .ssd import SSD\nfrom .predictor import Predictor\nfrom .config import squeezenet_ssd_config as config\n\n\ndef SeperableConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n    """"""Replace Conv2d with a depthwise Conv2d and Pointwise Conv2d.\n    """"""\n    return Sequential(\n        Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n               groups=in_channels, stride=stride, padding=padding),\n        ReLU(),\n        Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n    )\n\n\ndef create_squeezenet_ssd_lite(num_classes, is_test=False):\n    base_net = squeezenet1_1(False).features  # disable dropout layer\n\n    source_layer_indexes = [\n        12\n    ]\n    extras = ModuleList([\n        Sequential(\n            Conv2d(in_channels=512, out_channels=256, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=2),\n        ),\n        Sequential(\n            Conv2d(in_channels=512, out_channels=256, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n        ),\n        Sequential(\n            Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            SeperableConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n        )\n    ])\n\n    regression_headers = ModuleList([\n        SeperableConv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=1),\n    ])\n\n    classification_headers = ModuleList([\n        SeperableConv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        SeperableConv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=1),\n    ])\n\n    return SSD(num_classes, base_net, source_layer_indexes,\n               extras, classification_headers, regression_headers, is_test=is_test, config=config)\n\n\ndef create_squeezenet_ssd_lite_predictor(net, candidate_size=200, nms_method=None, sigma=0.5, device=torch.device(\'cpu\')):\n    predictor = Predictor(net, config.image_size, config.image_mean,\n                          config.image_std,\n                          nms_method=nms_method,\n                          iou_threshold=config.iou_threshold,\n                          candidate_size=candidate_size,\n                          sigma=sigma,\n                          device=device)\n    return predictor'"
vision/ssd/ssd.py,12,"b'import torch.nn as nn\nimport torch\nimport numpy as np\nfrom typing import List, Tuple\nimport torch.nn.functional as F\n\nfrom ..utils import box_utils\nfrom collections import namedtuple\nGraphPath = namedtuple(""GraphPath"", [\'s0\', \'name\', \'s1\'])  #\n\n\nclass SSD(nn.Module):\n    def __init__(self, num_classes: int, base_net: nn.ModuleList, source_layer_indexes: List[int],\n                 extras: nn.ModuleList, classification_headers: nn.ModuleList,\n                 regression_headers: nn.ModuleList, is_test=False, config=None, device=None):\n        """"""Compose a SSD model using the given components.\n        """"""\n        super(SSD, self).__init__()\n\n        self.num_classes = num_classes\n        self.base_net = base_net\n        self.source_layer_indexes = source_layer_indexes\n        self.extras = extras\n        self.classification_headers = classification_headers\n        self.regression_headers = regression_headers\n        self.is_test = is_test\n        self.config = config\n\n        # register layers in source_layer_indexes by adding them to a module list\n        self.source_layer_add_ons = nn.ModuleList([t[1] for t in source_layer_indexes\n                                                   if isinstance(t, tuple) and not isinstance(t, GraphPath)])\n        if device:\n            self.device = device\n        else:\n            self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n        if is_test:\n            self.config = config\n            self.priors = config.priors.to(self.device)\n            \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        confidences = []\n        locations = []\n        start_layer_index = 0\n        header_index = 0\n        for end_layer_index in self.source_layer_indexes:\n            if isinstance(end_layer_index, GraphPath):\n                path = end_layer_index\n                end_layer_index = end_layer_index.s0\n                added_layer = None\n            elif isinstance(end_layer_index, tuple):\n                added_layer = end_layer_index[1]\n                end_layer_index = end_layer_index[0]\n                path = None\n            else:\n                added_layer = None\n                path = None\n            for layer in self.base_net[start_layer_index: end_layer_index]:\n                x = layer(x)\n            if added_layer:\n                y = added_layer(x)\n            else:\n                y = x\n            if path:\n                sub = getattr(self.base_net[end_layer_index], path.name)\n                for layer in sub[:path.s1]:\n                    x = layer(x)\n                y = x\n                for layer in sub[path.s1:]:\n                    x = layer(x)\n                end_layer_index += 1\n            start_layer_index = end_layer_index\n            confidence, location = self.compute_header(header_index, y)\n            header_index += 1\n            confidences.append(confidence)\n            locations.append(location)\n\n        for layer in self.base_net[end_layer_index:]:\n            x = layer(x)\n\n        for layer in self.extras:\n            x = layer(x)\n            confidence, location = self.compute_header(header_index, x)\n            header_index += 1\n            confidences.append(confidence)\n            locations.append(location)\n\n        confidences = torch.cat(confidences, 1)\n        locations = torch.cat(locations, 1)\n        \n        if self.is_test:\n            confidences = F.softmax(confidences, dim=2)\n            boxes = box_utils.convert_locations_to_boxes(\n                locations, self.priors, self.config.center_variance, self.config.size_variance\n            )\n            boxes = box_utils.center_form_to_corner_form(boxes)\n            return confidences, boxes\n        else:\n            return confidences, locations\n\n    def compute_header(self, i, x):\n        confidence = self.classification_headers[i](x)\n        confidence = confidence.permute(0, 2, 3, 1).contiguous()\n        confidence = confidence.view(confidence.size(0), -1, self.num_classes)\n\n        location = self.regression_headers[i](x)\n        location = location.permute(0, 2, 3, 1).contiguous()\n        location = location.view(location.size(0), -1, 4)\n\n        return confidence, location\n\n    def init_from_base_net(self, model):\n        self.base_net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage), strict=True)\n        self.source_layer_add_ons.apply(_xavier_init_)\n        self.extras.apply(_xavier_init_)\n        self.classification_headers.apply(_xavier_init_)\n        self.regression_headers.apply(_xavier_init_)\n\n    def init_from_pretrained_ssd(self, model):\n        state_dict = torch.load(model, map_location=lambda storage, loc: storage)\n        state_dict = {k: v for k, v in state_dict.items() if not (k.startswith(""classification_headers"") or k.startswith(""regression_headers""))}\n        model_dict = self.state_dict()\n        model_dict.update(state_dict)\n        self.load_state_dict(model_dict)\n        self.classification_headers.apply(_xavier_init_)\n        self.regression_headers.apply(_xavier_init_)\n\n    def init(self):\n        self.base_net.apply(_xavier_init_)\n        self.source_layer_add_ons.apply(_xavier_init_)\n        self.extras.apply(_xavier_init_)\n        self.classification_headers.apply(_xavier_init_)\n        self.regression_headers.apply(_xavier_init_)\n\n    def load(self, model):\n        self.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n\n    def save(self, model_path):\n        torch.save(self.state_dict(), model_path)\n\n\nclass MatchPrior(object):\n    def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n        self.center_form_priors = center_form_priors\n        self.corner_form_priors = box_utils.center_form_to_corner_form(center_form_priors)\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.iou_threshold = iou_threshold\n\n    def __call__(self, gt_boxes, gt_labels):\n        if type(gt_boxes) is np.ndarray:\n            gt_boxes = torch.from_numpy(gt_boxes)\n        if type(gt_labels) is np.ndarray:\n            gt_labels = torch.from_numpy(gt_labels)\n        boxes, labels = box_utils.assign_priors(gt_boxes, gt_labels,\n                                                self.corner_form_priors, self.iou_threshold)\n        boxes = box_utils.corner_form_to_center_form(boxes)\n        locations = box_utils.convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n        return locations, labels\n\n\ndef _xavier_init_(m: nn.Module):\n    if isinstance(m, nn.Conv2d):\n        nn.init.xavier_uniform_(m.weight)\n'"
vision/ssd/vgg_ssd.py,1,"b""import torch\nfrom torch.nn import Conv2d, Sequential, ModuleList, ReLU, BatchNorm2d\nfrom ..nn.vgg import vgg\n\nfrom .ssd import SSD\nfrom .predictor import Predictor\nfrom .config import vgg_ssd_config as config\n\n\ndef create_vgg_ssd(num_classes, is_test=False):\n    vgg_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n                  512, 512, 512]\n    base_net = ModuleList(vgg(vgg_config))\n\n    source_layer_indexes = [\n        (23, BatchNorm2d(512)),\n        len(base_net),\n    ]\n    extras = ModuleList([\n        Sequential(\n            Conv2d(in_channels=1024, out_channels=256, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=512, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3),\n            ReLU()\n        ),\n        Sequential(\n            Conv2d(in_channels=256, out_channels=128, kernel_size=1),\n            ReLU(),\n            Conv2d(in_channels=128, out_channels=256, kernel_size=3),\n            ReLU()\n        )\n    ])\n\n    regression_headers = ModuleList([\n        Conv2d(in_channels=512, out_channels=4 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=1024, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=4 * 4, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=4 * 4, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n    ])\n\n    classification_headers = ModuleList([\n        Conv2d(in_channels=512, out_channels=4 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=1024, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=4 * num_classes, kernel_size=3, padding=1),\n        Conv2d(in_channels=256, out_channels=4 * num_classes, kernel_size=3, padding=1), # TODO: change to kernel_size=1, padding=0?\n    ])\n\n    return SSD(num_classes, base_net, source_layer_indexes,\n               extras, classification_headers, regression_headers, is_test=is_test, config=config)\n\n\ndef create_vgg_ssd_predictor(net, candidate_size=200, nms_method=None, sigma=0.5, device=None):\n    predictor = Predictor(net, config.image_size, config.image_mean,\n                          nms_method=nms_method,\n                          iou_threshold=config.iou_threshold,\n                          candidate_size=candidate_size,\n                          sigma=sigma,\n                          device=device)\n    return predictor\n"""
vision/test/__init__.py,0,b''
vision/test/test_vgg_ssd.py,4,"b'from ..ssd.vgg_ssd import create_vgg_ssd\n\nimport torch\nimport tempfile\n\n\ndef test_create_vgg_ssd():\n    for num_classes in [2, 10, 21, 100]:\n        _ = create_vgg_ssd(num_classes)\n\n\ndef test_forward():\n    for num_classes in [2]:\n        net = create_vgg_ssd(num_classes)\n        net.init()\n        net.eval()\n        x = torch.randn(2, 3, 300, 300)\n        confidences, locations = net.forward(x)\n        assert confidences.size() == torch.Size([2, 8732, num_classes])\n        assert locations.size() == torch.Size([2, 8732, 4])\n        assert confidences.nonzero().size(0) != 0\n        assert locations.nonzero().size(0) != 0\n\n\ndef test_save_model():\n    net = create_vgg_ssd(10)\n    net.init()\n    with tempfile.TemporaryFile() as f:\n        net.save(f)\n\n\ndef test_save_load_model_consistency():\n    net = create_vgg_ssd(20)\n    net.init()\n    model_path = tempfile.NamedTemporaryFile().name\n    net.save(model_path)\n    net_copy = create_vgg_ssd(20)\n    net_copy.load(model_path)\n\n    net.eval()\n    net_copy.eval()\n\n    for _ in range(1):\n        x = torch.randn(1, 3, 300, 300)\n        confidences1, locations1 = net.forward(x)\n        confidences2, locations2 = net_copy.forward(x)\n        assert (confidences1 == confidences2).long().sum() == confidences2.numel()\n        assert (locations1 == locations2).long().sum() == locations2.numel()\n'"
vision/transforms/__init__.py,0,b''
vision/transforms/transforms.py,2,"b'# from https://github.com/amdegroot/ssd.pytorch\n\n\nimport torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport types\nfrom numpy import random\n\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2]-box_b[0]) *\n              (box_b[3]-box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass Compose(object):\n    """"""Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels\n\n\nclass Lambda(object):\n    """"""Applies a lambda as a transform.""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, boxes=None, labels=None):\n        return self.lambd(img, boxes, labels)\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass SubtractMeans(object):\n    def __init__(self, mean):\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image -= self.mean\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Resize(object):\n    def __init__(self, size=300):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                 self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            swap = self.perms[random.randint(len(self.perms))]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current, transform):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == \'BGR\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == \'RGB\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif self.current == \'BGR\' and self.transform == \'RGB\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif self.current == \'HSV\' and self.transform == \'BGR\':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        elif self.current == \'HSV\' and self.transform == ""RGB"":\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToCV2Image(object):\n    def __call__(self, tensor, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\n\nclass RandomSampleCrop(object):\n    """"""Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    """"""\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float(\'-inf\')\n            if max_iou is None:\n                max_iou = float(\'inf\')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n                                              :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop\'s\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels\n\n\nclass Expand(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width*ratio - width)\n        top = random.uniform(0, height*ratio - height)\n\n        expand_image = np.zeros(\n            (int(height*ratio), int(width*ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n                     int(left):int(left + width)] = image\n        image = expand_image\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass RandomMirror(object):\n    def __call__(self, image, boxes, classes):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass SwapChannels(object):\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),  # RGB\n            ConvertColor(current=""RGB"", transform=\'HSV\'),  # HSV\n            RandomSaturation(),  # HSV\n            RandomHue(),  # HSV\n            ConvertColor(current=\'HSV\', transform=\'RGB\'),  # RGB\n            RandomContrast()  # RGB\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n\n'"
vision/utils/__init__.py,0,b'from .misc import *\n'
vision/utils/box_utils.py,18,"b'import collections\nimport torch\nimport itertools\nfrom typing import List\nimport math\n\nSSDBoxSizes = collections.namedtuple(\'SSDBoxSizes\', [\'min\', \'max\'])\n\nSSDSpec = collections.namedtuple(\'SSDSpec\', [\'feature_map_size\', \'shrinkage\', \'box_sizes\', \'aspect_ratios\'])\n\n\ndef generate_ssd_priors(specs: List[SSDSpec], image_size, clamp=True) -> torch.Tensor:\n    """"""Generate SSD Prior Boxes.\n\n    It returns the center, height and width of the priors. The values are relative to the image size\n    Args:\n        specs: SSDSpecs about the shapes of sizes of prior boxes. i.e.\n            specs = [\n                SSDSpec(38, 8, SSDBoxSizes(30, 60), [2]),\n                SSDSpec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n                SSDSpec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n                SSDSpec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n                SSDSpec(3, 100, SSDBoxSizes(213, 264), [2]),\n                SSDSpec(1, 300, SSDBoxSizes(264, 315), [2])\n            ]\n        image_size: image size.\n        clamp: if true, clamp the values to make fall between [0.0, 1.0]\n    Returns:\n        priors (num_priors, 4): The prior boxes represented as [[center_x, center_y, w, h]]. All the values\n            are relative to the image size.\n    """"""\n    priors = []\n    for spec in specs:\n        scale = image_size / spec.shrinkage\n        for j, i in itertools.product(range(spec.feature_map_size), repeat=2):\n            x_center = (i + 0.5) / scale\n            y_center = (j + 0.5) / scale\n\n            # small sized square box\n            size = spec.box_sizes.min\n            h = w = size / image_size\n            priors.append([\n                x_center,\n                y_center,\n                w,\n                h\n            ])\n\n            # big sized square box\n            size = math.sqrt(spec.box_sizes.max * spec.box_sizes.min)\n            h = w = size / image_size\n            priors.append([\n                x_center,\n                y_center,\n                w,\n                h\n            ])\n\n            # change h/w ratio of the small sized box\n            size = spec.box_sizes.min\n            h = w = size / image_size\n            for ratio in spec.aspect_ratios:\n                ratio = math.sqrt(ratio)\n                priors.append([\n                    x_center,\n                    y_center,\n                    w * ratio,\n                    h / ratio\n                ])\n                priors.append([\n                    x_center,\n                    y_center,\n                    w / ratio,\n                    h * ratio\n                ])\n\n    priors = torch.tensor(priors)\n    if clamp:\n        torch.clamp(priors, 0.0, 1.0, out=priors)\n    return priors\n\n\ndef convert_locations_to_boxes(locations, priors, center_variance,\n                               size_variance):\n    """"""Convert regressional location results of SSD into boxes in the form of (center_x, center_y, h, w).\n\n    The conversion:\n        $$predicted\\_center * center_variance = \\frac {real\\_center - prior\\_center} {prior\\_hw}$$\n        $$exp(predicted\\_hw * size_variance) = \\frac {real\\_hw} {prior\\_hw}$$\n    We do it in the inverse direction here.\n    Args:\n        locations (batch_size, num_priors, 4): the regression output of SSD. It will contain the outputs as well.\n        priors (num_priors, 4) or (batch_size/1, num_priors, 4): prior boxes.\n        center_variance: a float used to change the scale of center.\n        size_variance: a float used to change of scale of size.\n    Returns:\n        boxes:  priors: [[center_x, center_y, h, w]]. All the values\n            are relative to the image size.\n    """"""\n    # priors can have one dimension less.\n    if priors.dim() + 1 == locations.dim():\n        priors = priors.unsqueeze(0)\n    return torch.cat([\n        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n        torch.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n    ], dim=locations.dim() - 1)\n\n\ndef convert_boxes_to_locations(center_form_boxes, center_form_priors, center_variance, size_variance):\n    # priors can have one dimension less\n    if center_form_priors.dim() + 1 == center_form_boxes.dim():\n        center_form_priors = center_form_priors.unsqueeze(0)\n    return torch.cat([\n        (center_form_boxes[..., :2] - center_form_priors[..., :2]) / center_form_priors[..., 2:] / center_variance,\n        torch.log(center_form_boxes[..., 2:] / center_form_priors[..., 2:]) / size_variance\n    ], dim=center_form_boxes.dim() - 1)\n\n\ndef area_of(left_top, right_bottom) -> torch.Tensor:\n    """"""Compute the areas of rectangles given two corners.\n\n    Args:\n        left_top (N, 2): left top corner.\n        right_bottom (N, 2): right bottom corner.\n\n    Returns:\n        area (N): return the area.\n    """"""\n    hw = torch.clamp(right_bottom - left_top, min=0.0)\n    return hw[..., 0] * hw[..., 1]\n\n\ndef iou_of(boxes0, boxes1, eps=1e-5):\n    """"""Return intersection-over-union (Jaccard index) of boxes.\n\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n        eps: a small number to avoid 0 as denominator.\n    Returns:\n        iou (N): IoU values.\n    """"""\n    overlap_left_top = torch.max(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = torch.min(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area / (area0 + area1 - overlap_area + eps)\n\n\ndef assign_priors(gt_boxes, gt_labels, corner_form_priors,\n                  iou_threshold):\n    """"""Assign ground truth boxes and targets to priors.\n\n    Args:\n        gt_boxes (num_targets, 4): ground truth boxes.\n        gt_labels (num_targets): labels of targets.\n        priors (num_priors, 4): corner form priors\n    Returns:\n        boxes (num_priors, 4): real values for priors.\n        labels (num_priros): labels for priors.\n    """"""\n    # size: num_priors x num_targets\n    ious = iou_of(gt_boxes.unsqueeze(0), corner_form_priors.unsqueeze(1))\n    # size: num_priors\n    best_target_per_prior, best_target_per_prior_index = ious.max(1)\n    # size: num_targets\n    best_prior_per_target, best_prior_per_target_index = ious.max(0)\n\n    for target_index, prior_index in enumerate(best_prior_per_target_index):\n        best_target_per_prior_index[prior_index] = target_index\n    # 2.0 is used to make sure every target has a prior assigned\n    best_target_per_prior.index_fill_(0, best_prior_per_target_index, 2)\n    # size: num_priors\n    labels = gt_labels[best_target_per_prior_index]\n    labels[best_target_per_prior < iou_threshold] = 0  # the backgournd id\n    boxes = gt_boxes[best_target_per_prior_index]\n    return boxes, labels\n\n\ndef hard_negative_mining(loss, labels, neg_pos_ratio):\n    """"""\n    It used to suppress the presence of a large number of negative prediction.\n    It works on image level not batch level.\n    For any example/image, it keeps all the positive predictions and\n     cut the number of negative predictions to make sure the ratio\n     between the negative examples and positive examples is no more\n     the given ratio for an image.\n\n    Args:\n        loss (N, num_priors): the loss for each example.\n        labels (N, num_priors): the labels.\n        neg_pos_ratio:  the ratio between the negative examples and positive examples.\n    """"""\n    pos_mask = labels > 0\n    num_pos = pos_mask.long().sum(dim=1, keepdim=True)\n    num_neg = num_pos * neg_pos_ratio\n\n    loss[pos_mask] = -math.inf\n    _, indexes = loss.sort(dim=1, descending=True)\n    _, orders = indexes.sort(dim=1)\n    neg_mask = orders < num_neg\n    return pos_mask | neg_mask\n\n\ndef center_form_to_corner_form(locations):\n    return torch.cat([locations[..., :2] - locations[..., 2:]/2,\n                     locations[..., :2] + locations[..., 2:]/2], locations.dim() - 1) \n\n\ndef corner_form_to_center_form(boxes):\n    return torch.cat([\n        (boxes[..., :2] + boxes[..., 2:]) / 2,\n         boxes[..., 2:] - boxes[..., :2]\n    ], boxes.dim() - 1)\n\n\ndef hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n    """"""\n\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        iou_threshold: intersection over union threshold.\n        top_k: keep top_k results. If k <= 0, keep all the results.\n        candidate_size: only consider the candidates with the highest scores.\n    Returns:\n         picked: a list of indexes of the kept boxes\n    """"""\n    scores = box_scores[:, -1]\n    boxes = box_scores[:, :-1]\n    picked = []\n    _, indexes = scores.sort(descending=True)\n    indexes = indexes[:candidate_size]\n    while len(indexes) > 0:\n        current = indexes[0]\n        picked.append(current.item())\n        if 0 < top_k == len(picked) or len(indexes) == 1:\n            break\n        current_box = boxes[current, :]\n        indexes = indexes[1:]\n        rest_boxes = boxes[indexes, :]\n        iou = iou_of(\n            rest_boxes,\n            current_box.unsqueeze(0),\n        )\n        indexes = indexes[iou <= iou_threshold]\n\n    return box_scores[picked, :]\n\n\ndef nms(box_scores, nms_method=None, score_threshold=None, iou_threshold=None,\n        sigma=0.5, top_k=-1, candidate_size=200):\n    if nms_method == ""soft"":\n        return soft_nms(box_scores, score_threshold, sigma, top_k)\n    else:\n        return hard_nms(box_scores, iou_threshold, top_k, candidate_size=candidate_size)\n\n\ndef soft_nms(box_scores, score_threshold, sigma=0.5, top_k=-1):\n    """"""Soft NMS implementation.\n\n    References:\n        https://arxiv.org/abs/1704.04503\n        https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/cython_nms.pyx\n\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        score_threshold: boxes with scores less than value are not considered.\n        sigma: the parameter in score re-computation.\n            scores[i] = scores[i] * exp(-(iou_i)^2 / simga)\n        top_k: keep top_k results. If k <= 0, keep all the results.\n    Returns:\n         picked_box_scores (K, 5): results of NMS.\n    """"""\n    picked_box_scores = []\n    while box_scores.size(0) > 0:\n        max_score_index = torch.argmax(box_scores[:, 4])\n        cur_box_prob = torch.tensor(box_scores[max_score_index, :])\n        picked_box_scores.append(cur_box_prob)\n        if len(picked_box_scores) == top_k > 0 or box_scores.size(0) == 1:\n            break\n        cur_box = cur_box_prob[:-1]\n        box_scores[max_score_index, :] = box_scores[-1, :]\n        box_scores = box_scores[:-1, :]\n        ious = iou_of(cur_box.unsqueeze(0), box_scores[:, :-1])\n        box_scores[:, -1] = box_scores[:, -1] * torch.exp(-(ious * ious) / sigma)\n        box_scores = box_scores[box_scores[:, -1] > score_threshold, :]\n    if len(picked_box_scores) > 0:\n        return torch.stack(picked_box_scores)\n    else:\n        return torch.tensor([])\n\n\n\n'"
vision/utils/box_utils_numpy.py,5,"b'from .box_utils import SSDSpec\n\nfrom typing import List\nimport itertools\nimport math\nimport numpy as np\n\n\ndef generate_ssd_priors(specs: List[SSDSpec], image_size, clamp=True):\n    """"""Generate SSD Prior Boxes.\n\n    It returns the center, height and width of the priors. The values are relative to the image size\n    Args:\n        specs: SSDSpecs about the shapes of sizes of prior boxes. i.e.\n            specs = [\n                SSDSpec(38, 8, SSDBoxSizes(30, 60), [2]),\n                SSDSpec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n                SSDSpec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n                SSDSpec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n                SSDSpec(3, 100, SSDBoxSizes(213, 264), [2]),\n                SSDSpec(1, 300, SSDBoxSizes(264, 315), [2])\n            ]\n        image_size: image size.\n        clamp: if true, clamp the values to make fall between [0.0, 1.0]\n    Returns:\n        priors (num_priors, 4): The prior boxes represented as [[center_x, center_y, w, h]]. All the values\n            are relative to the image size.\n    """"""\n    priors = []\n    for spec in specs:\n        scale = image_size / spec.shrinkage\n        for j, i in itertools.product(range(spec.feature_map_size), repeat=2):\n            x_center = (i + 0.5) / scale\n            y_center = (j + 0.5) / scale\n\n            # small sized square box\n            size = spec.box_sizes.min\n            h = w = size / image_size\n            priors.append([\n                x_center,\n                y_center,\n                w,\n                h\n            ])\n\n            # big sized square box\n            size = math.sqrt(spec.box_sizes.max * spec.box_sizes.min)\n            h = w = size / image_size\n            priors.append([\n                x_center,\n                y_center,\n                w,\n                h\n            ])\n\n            # change h/w ratio of the small sized box\n            size = spec.box_sizes.min\n            h = w = size / image_size\n            for ratio in spec.aspect_ratios:\n                ratio = math.sqrt(ratio)\n                priors.append([\n                    x_center,\n                    y_center,\n                    w * ratio,\n                    h / ratio\n                ])\n                priors.append([\n                    x_center,\n                    y_center,\n                    w / ratio,\n                    h * ratio\n                ])\n\n    priors = np.array(priors, dtype=np.float32)\n    if clamp:\n        np.clip(priors, 0.0, 1.0, out=priors)\n    return priors\n\n\ndef convert_locations_to_boxes(locations, priors, center_variance,\n                               size_variance):\n    """"""Convert regressional location results of SSD into boxes in the form of (center_x, center_y, h, w).\n\n    The conversion:\n        $$predicted\\_center * center_variance = \\frac {real\\_center - prior\\_center} {prior\\_hw}$$\n        $$exp(predicted\\_hw * size_variance) = \\frac {real\\_hw} {prior\\_hw}$$\n    We do it in the inverse direction here.\n    Args:\n        locations (batch_size, num_priors, 4): the regression output of SSD. It will contain the outputs as well.\n        priors (num_priors, 4) or (batch_size/1, num_priors, 4): prior boxes.\n        center_variance: a float used to change the scale of center.\n        size_variance: a float used to change of scale of size.\n    Returns:\n        boxes:  priors: [[center_x, center_y, h, w]]. All the values\n            are relative to the image size.\n    """"""\n    # priors can have one dimension less.\n    if len(priors.shape) + 1 == len(locations.shape):\n        priors = np.expand_dims(priors, 0)\n    return np.concatenate([\n        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n        np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n    ], axis=len(locations.shape) - 1)\n\n\ndef convert_boxes_to_locations(center_form_boxes, center_form_priors, center_variance, size_variance):\n    # priors can have one dimension less\n    if len(center_form_priors.shape) + 1 == len(center_form_boxes.shape):\n        center_form_priors = np.expand_dims(center_form_priors, 0)\n    return np.concatenate([\n        (center_form_boxes[..., :2] - center_form_priors[..., :2]) / center_form_priors[..., 2:] / center_variance,\n        np.log(center_form_boxes[..., 2:] / center_form_priors[..., 2:]) / size_variance\n    ], axis=len(center_form_boxes.shape) - 1)\n\n\ndef area_of(left_top, right_bottom):\n    """"""Compute the areas of rectangles given two corners.\n\n    Args:\n        left_top (N, 2): left top corner.\n        right_bottom (N, 2): right bottom corner.\n\n    Returns:\n        area (N): return the area.\n    """"""\n    hw = np.clip(right_bottom - left_top, 0.0, None)\n    return hw[..., 0] * hw[..., 1]\n\n\ndef iou_of(boxes0, boxes1, eps=1e-5):\n    """"""Return intersection-over-union (Jaccard index) of boxes.\n\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n        eps: a small number to avoid 0 as denominator.\n    Returns:\n        iou (N): IoU values.\n    """"""\n    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area / (area0 + area1 - overlap_area + eps)\n\n\ndef center_form_to_corner_form(locations):\n    return np.concatenate([locations[..., :2] - locations[..., 2:]/2,\n                     locations[..., :2] + locations[..., 2:]/2], len(locations.shape) - 1)\n\n\ndef corner_form_to_center_form(boxes):\n    return np.concatenate([\n        (boxes[..., :2] + boxes[..., 2:]) / 2,\n         boxes[..., 2:] - boxes[..., :2]\n    ], len(boxes.shape) - 1)\n\n\ndef hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n    """"""\n\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        iou_threshold: intersection over union threshold.\n        top_k: keep top_k results. If k <= 0, keep all the results.\n        candidate_size: only consider the candidates with the highest scores.\n    Returns:\n         picked: a list of indexes of the kept boxes\n    """"""\n    scores = box_scores[:, -1]\n    boxes = box_scores[:, :-1]\n    picked = []\n    #_, indexes = scores.sort(descending=True)\n    indexes = np.argsort(scores)\n    #indexes = indexes[:candidate_size]\n    indexes = indexes[-candidate_size:]\n    while len(indexes) > 0:\n        #current = indexes[0]\n        current = indexes[-1]\n        picked.append(current)\n        if 0 < top_k == len(picked) or len(indexes) == 1:\n            break\n        current_box = boxes[current, :]\n        #indexes = indexes[1:]\n        indexes = indexes[:-1]\n        rest_boxes = boxes[indexes, :]\n        iou = iou_of(\n            rest_boxes,\n            np.expand_dims(current_box, axis=0),\n        )\n        indexes = indexes[iou <= iou_threshold]\n\n    return box_scores[picked, :]\n\n\n# def nms(box_scores, nms_method=None, score_threshold=None, iou_threshold=None,\n#         sigma=0.5, top_k=-1, candidate_size=200):\n#     if nms_method == ""soft"":\n#         return soft_nms(box_scores, score_threshold, sigma, top_k)\n#     else:\n#         return hard_nms(box_scores, iou_threshold, top_k, candidate_size=candidate_size)\n\n#\n# def soft_nms(box_scores, score_threshold, sigma=0.5, top_k=-1):\n#     """"""Soft NMS implementation.\n#\n#     References:\n#         https://arxiv.org/abs/1704.04503\n#         https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/cython_nms.pyx\n#\n#     Args:\n#         box_scores (N, 5): boxes in corner-form and probabilities.\n#         score_threshold: boxes with scores less than value are not considered.\n#         sigma: the parameter in score re-computation.\n#             scores[i] = scores[i] * exp(-(iou_i)^2 / simga)\n#         top_k: keep top_k results. If k <= 0, keep all the results.\n#     Returns:\n#          picked_box_scores (K, 5): results of NMS.\n#     """"""\n#     picked_box_scores = []\n#     while box_scores.size(0) > 0:\n#         max_score_index = torch.argmax(box_scores[:, 4])\n#         cur_box_prob = torch.tensor(box_scores[max_score_index, :])\n#         picked_box_scores.append(cur_box_prob)\n#         if len(picked_box_scores) == top_k > 0 or box_scores.size(0) == 1:\n#             break\n#         cur_box = cur_box_prob[:-1]\n#         box_scores[max_score_index, :] = box_scores[-1, :]\n#         box_scores = box_scores[:-1, :]\n#         ious = iou_of(cur_box.unsqueeze(0), box_scores[:, :-1])\n#         box_scores[:, -1] = box_scores[:, -1] * torch.exp(-(ious * ious) / sigma)\n#         box_scores = box_scores[box_scores[:, -1] > score_threshold, :]\n#     if len(picked_box_scores) > 0:\n#         return torch.stack(picked_box_scores)\n#     else:\n#         return torch.tensor([])\n'"
vision/utils/measurements.py,0,"b'import numpy as np\n\n\ndef compute_average_precision(precision, recall):\n    """"""\n    It computes average precision based on the definition of Pascal Competition. It computes the under curve area\n    of precision and recall. Recall follows the normal definition. Precision is a variant.\n    pascal_precision[i] = typical_precision[i:].max()\n    """"""\n    # identical but faster version of new_precision[i] = old_precision[i:].max()\n    precision = np.concatenate([[0.0], precision, [0.0]])\n    for i in range(len(precision) - 1, 0, -1):\n        precision[i - 1] = np.maximum(precision[i - 1], precision[i])\n\n    # find the index where the value changes\n    recall = np.concatenate([[0.0], recall, [1.0]])\n    changing_points = np.where(recall[1:] != recall[:-1])[0]\n\n    # compute under curve area\n    areas = (recall[changing_points + 1] - recall[changing_points]) * precision[changing_points + 1]\n    return areas.sum()\n\n\ndef compute_voc2007_average_precision(precision, recall):\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n        if np.sum(recall >= t) == 0:\n            p = 0\n        else:\n            p = np.max(precision[recall >= t])\n        ap = ap + p / 11.\n    return ap\n'"
vision/utils/misc.py,3,"b'import time\nimport torch\n\n\ndef str2bool(s):\n    return s.lower() in (\'true\', \'1\')\n\n\nclass Timer:\n    def __init__(self):\n        self.clock = {}\n\n    def start(self, key=""default""):\n        self.clock[key] = time.time()\n\n    def end(self, key=""default""):\n        if key not in self.clock:\n            raise Exception(f""{key} is not in the clock."")\n        interval = time.time() - self.clock[key]\n        del self.clock[key]\n        return interval\n        \n\ndef save_checkpoint(epoch, net_state_dict, optimizer_state_dict, best_score, checkpoint_path, model_path):\n    torch.save({\n        \'epoch\': epoch,\n        \'model\': net_state_dict,\n        \'optimizer\': optimizer_state_dict,\n        \'best_score\': best_score\n    }, checkpoint_path)\n    torch.save(net_state_dict, model_path)\n        \n        \ndef load_checkpoint(checkpoint_path):\n    return torch.load(checkpoint_path)\n\n\ndef freeze_net_layers(net):\n    for param in net.parameters():\n        param.requires_grad = False\n\n\ndef store_labels(path, labels):\n    with open(path, ""w"") as f:\n        f.write(""\\n"".join(labels))\n'"
vision/utils/model_book.py,1,"b'from collections import OrderedDict\nimport torch.nn as nn\n\n\nclass ModelBook:\n    """"""Maintain the mapping between modules and their paths.\n\n    Example:\n        book = ModelBook(model_ft)\n        for p, m in book.conv2d_modules():\n            print(\'path:\', p, \'num of filters:\', m.out_channels)\n            assert m is book.get_module(p)\n    """"""\n\n    def __init__(self, model):\n        self._model = model\n        self._modules = OrderedDict()\n        self._paths = OrderedDict()\n        path = []\n        self._construct(self._model, path)\n\n    def _construct(self, module, path):\n        if not module._modules:\n            return\n        for name, m in module._modules.items():\n            cur_path = tuple(path + [name])\n            self._paths[m] = cur_path\n            self._modules[cur_path] = m\n            self._construct(m, path + [name])\n\n    def conv2d_modules(self):\n        return self.modules(nn.Conv2d)\n\n    def linear_modules(self):\n        return self.modules(nn.Linear)\n\n    def modules(self, module_type=None):\n        for p, m in self._modules.items():\n            if not module_type or isinstance(m, module_type):\n                yield p, m\n\n    def num_of_conv2d_modules(self):\n        return self.num_of_modules(nn.Conv2d)\n\n    def num_of_conv2d_filters(self):\n        """"""Return the sum of out_channels of all conv2d layers.\n\n        Here we treat the sub weight with size of [in_channels, h, w] as a single filter.\n        """"""\n        num_filters = 0\n        for _, m in self.conv2d_modules():\n            num_filters += m.out_channels\n        return num_filters\n\n    def num_of_linear_modules(self):\n        return self.num_of_modules(nn.Linear)\n\n    def num_of_linear_filters(self):\n        num_filters = 0\n        for _, m in self.linear_modules():\n            num_filters += m.out_features\n        return num_filters\n\n    def num_of_modules(self, module_type=None):\n        num = 0\n        for p, m in self._modules.items():\n            if not module_type or isinstance(m, module_type):\n                num += 1\n        return num\n\n    def get_module(self, path):\n        return self._modules.get(path)\n\n    def get_path(self, module):\n        return self._paths.get(module)\n\n    def update(self, path, module):\n        old_module = self._modules[path]\n        del self._paths[old_module]\n        self._paths[module] = path\n        self._modules[path] = module\n'"
vision/ssd/config/__init__.py,0,b''
vision/ssd/config/mobilenetv1_ssd_config.py,0,"b'import numpy as np\n\nfrom vision.utils.box_utils import SSDSpec, SSDBoxSizes, generate_ssd_priors\n\n\nimage_size = 300\nimage_mean = np.array([127, 127, 127])  # RGB layout\nimage_std = 128.0\niou_threshold = 0.45\ncenter_variance = 0.1\nsize_variance = 0.2\n\nspecs = [\n    SSDSpec(19, 16, SSDBoxSizes(60, 105), [2, 3]),\n    SSDSpec(10, 32, SSDBoxSizes(105, 150), [2, 3]),\n    SSDSpec(5, 64, SSDBoxSizes(150, 195), [2, 3]),\n    SSDSpec(3, 100, SSDBoxSizes(195, 240), [2, 3]),\n    SSDSpec(2, 150, SSDBoxSizes(240, 285), [2, 3]),\n    SSDSpec(1, 300, SSDBoxSizes(285, 330), [2, 3])\n]\n\n\npriors = generate_ssd_priors(specs, image_size)'"
vision/ssd/config/squeezenet_ssd_config.py,0,"b'import numpy as np\n\nfrom vision.utils.box_utils import SSDSpec, SSDBoxSizes, generate_ssd_priors\n\n\nimage_size = 300\nimage_mean = np.array([127, 127, 127])  # RGB layout\nimage_std = 128.0\niou_threshold = 0.45\ncenter_variance = 0.1\nsize_variance = 0.2\n\nspecs = [\n    SSDSpec(17, 16, SSDBoxSizes(60, 105), [2, 3]),\n    SSDSpec(10, 32, SSDBoxSizes(105, 150), [2, 3]),\n    SSDSpec(5, 64, SSDBoxSizes(150, 195), [2, 3]),\n    SSDSpec(3, 100, SSDBoxSizes(195, 240), [2, 3]),\n    SSDSpec(2, 150, SSDBoxSizes(240, 285), [2, 3]),\n    SSDSpec(1, 300, SSDBoxSizes(285, 330), [2, 3])\n]\n\n\npriors = generate_ssd_priors(specs, image_size)'"
vision/ssd/config/vgg_ssd_config.py,0,"b'import numpy as np\n\nfrom vision.utils.box_utils import SSDSpec, SSDBoxSizes, generate_ssd_priors\n\n\nimage_size = 300\nimage_mean = np.array([123, 117, 104])  # RGB layout\nimage_std = 1.0\n\niou_threshold = 0.45\ncenter_variance = 0.1\nsize_variance = 0.2\n\nspecs = [\n    SSDSpec(38, 8, SSDBoxSizes(30, 60), [2]),\n    SSDSpec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n    SSDSpec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n    SSDSpec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n    SSDSpec(3, 100, SSDBoxSizes(213, 264), [2]),\n    SSDSpec(1, 300, SSDBoxSizes(264, 315), [2])\n]\n\n\npriors = generate_ssd_priors(specs, image_size)'"
