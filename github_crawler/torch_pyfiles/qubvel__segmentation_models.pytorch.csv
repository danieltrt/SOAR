file_path,api_count,code
__init__.py,0,b'from segmentation_models_pytorch import *'
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note: To use the \'upload\' functionality of this file, you must:\n#   $ pip install twine\n\nimport io\nimport os\nimport sys\nfrom shutil import rmtree\n\nfrom setuptools import find_packages, setup, Command\n\n# Package meta-data.\nNAME = \'segmentation_models_pytorch\'\nDESCRIPTION = \'Image segmentation models with pre-trained backbones. PyTorch.\'\nURL = \'https://github.com/qubvel/segmentation_models.pytorch\'\nEMAIL = \'qubvel@gmail.com\'\nAUTHOR = \'Pavel Yakubovskiy\'\nREQUIRES_PYTHON = \'>=3.0.0\'\nVERSION = None\n\n# The rest you shouldn\'t have to touch too much :)\n# ------------------------------------------------\n# Except, perhaps the License and Trove Classifiers!\n# If you do change the License, remember to change the Trove Classifier for that!\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n# What packages are required for this module to be executed?\ntry:\n    with open(os.path.join(here, \'requirements.txt\'), encoding=\'utf-8\') as f:\n        REQUIRED = f.read().split(\'\\n\')\nexcept:\n    REQUIRED = []\n\n# What packages are optional?\nEXTRAS = {\n    \'test\': [\'pytest\']\n}\n\n# Import the README and use it as the long-description.\n# Note: this will only work if \'README.md\' is present in your MANIFEST.in file!\ntry:\n    with io.open(os.path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n        long_description = \'\\n\' + f.read()\nexcept FileNotFoundError:\n    long_description = DESCRIPTION\n\n# Load the package\'s __version__.py module as a dictionary.\nabout = {}\nif not VERSION:\n    with open(os.path.join(here, NAME, \'__version__.py\')) as f:\n        exec(f.read(), about)\nelse:\n    about[\'__version__\'] = VERSION\n\n\nclass UploadCommand(Command):\n    """"""Support setup.py upload.""""""\n\n    description = \'Build and publish the package.\'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        """"""Prints things in bold.""""""\n        print(s)\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\'Removing previous builds...\')\n            rmtree(os.path.join(here, \'dist\'))\n        except OSError:\n            pass\n\n        self.status(\'Building Source and Wheel (universal) distribution...\')\n        os.system(\'{0} setup.py sdist bdist_wheel --universal\'.format(sys.executable))\n\n        self.status(\'Uploading the package to PyPI via Twine...\')\n        os.system(\'twine upload dist/*\')\n\n        self.status(\'Pushing git tags...\')\n        os.system(\'git tag v{0}\'.format(about[\'__version__\']))\n        os.system(\'git push --tags\')\n\n        sys.exit()\n\n\n# Where the magic happens:\nsetup(\n    name=NAME,\n    version=about[\'__version__\'],\n    description=DESCRIPTION,\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    author=AUTHOR,\n    author_email=EMAIL,\n    python_requires=REQUIRES_PYTHON,\n    url=URL,\n    packages=find_packages(exclude=(\'tests\', \'docs\', \'images\')),\n    # If your package is a single module, use this instead of \'packages\':\n    # py_modules=[\'mypackage\'],\n\n    # entry_points={\n    #     \'console_scripts\': [\'mycli=mymodule:cli\'],\n    # },\n    install_requires=REQUIRED,\n    extras_require=EXTRAS,\n    include_package_data=True,\n    license=\'MIT\',\n    classifiers=[\n        # Trove classifiers\n        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: Implementation :: CPython\',\n        \'Programming Language :: Python :: Implementation :: PyPy\'\n    ],\n    # $ setup.py publish support.\n    cmdclass={\n        \'upload\': UploadCommand,\n    },\n)\n'"
misc/generate_table.py,0,"b'import segmentation_models_pytorch as smp\n\nencoders = smp.encoders.encoders\n\n\nWIDTH = 32\nCOLUMNS = [\n    ""Encoder"",\n    ""Weights"",\n    ""Params, M"",\n]\n\ndef wrap_row(r):\n    return ""|{}|"".format(r)\n\nheader = ""|"".join([column.ljust(WIDTH, \' \') for column in COLUMNS])\nseparator = ""|"".join([""-"" * WIDTH] + ["":"" + ""-"" * (WIDTH - 2) + "":""] * (len(COLUMNS) - 1))\n\nprint(wrap_row(header))\nprint(wrap_row(separator))\n\nfor encoder_name, encoder in encoders.items():\n    weights = ""<br>"".join(encoder[""pretrained_settings""].keys())\n    encoder_name = encoder_name.ljust(WIDTH, "" "")\n    weights = weights.ljust(WIDTH, "" "")\n\n    model = encoder[""encoder""](**encoder[""params""], depth=5)\n    params = sum(p.numel() for p in model.parameters())\n    params = str(params // 1000000) + ""M""\n    params = params.ljust(WIDTH, "" "")\n\n    row = ""|"".join([encoder_name, weights, params])\n    print(wrap_row(row))\n'"
segmentation_models_pytorch/__init__.py,0,"b'from .unet import Unet\nfrom .linknet import Linknet\nfrom .fpn import FPN\nfrom .pspnet import PSPNet\nfrom .deeplabv3 import DeepLabV3, DeepLabV3Plus\nfrom .pan import PAN\n\nfrom . import encoders\nfrom . import utils\n\nfrom .__version__ import __version__\n'"
segmentation_models_pytorch/__version__.py,0,"b""VERSION = (0, 1, 0)\n\n__version__ = '.'.join(map(str, VERSION))\n"""
tests/test_models.py,8,"b'import os\nimport sys\nimport mock\nimport pytest\nimport torch\n\n# mock detection module\nsys.modules[""torchvision._C""] = mock.Mock()\n\nimport segmentation_models_pytorch as smp\n\nIS_TRAVIS = os.environ.get(""TRAVIS"", False)\n\n\ndef get_encoders():\n    travis_exclude_encoders = [\n        ""senet154"",\n        ""resnext101_32x16d"",\n        ""resnext101_32x32d"",\n        ""resnext101_32x48d"",\n    ]\n    encoders = smp.encoders.get_encoder_names()\n    if IS_TRAVIS:\n        encoders = [e for e in encoders if e not in travis_exclude_encoders]\n    return encoders\n\n\nENCODERS = get_encoders()\nDEFAULT_ENCODER = ""resnet18""\n\n\ndef get_sample(model_class):\n    if model_class in [smp.Unet, smp.Linknet, smp.FPN, smp.PSPNet]:\n        sample = torch.ones([1, 3, 64, 64])\n    elif model_class == smp.PAN:\n        sample = torch.ones([2, 3, 256, 256])\n    elif model_class == smp.DeepLabV3:\n        sample = torch.ones([2, 3, 128, 128])\n    else:\n        raise ValueError(""Not supported model class {}"".format(model_class))\n    return sample\n\n\ndef _test_forward(model, sample, test_shape=False):\n    with torch.no_grad():\n        out = model(sample)\n    if test_shape:\n        assert out.shape[2:] == sample.shape[2:]\n\n\ndef _test_forward_backward(model, sample, test_shape=False):\n    out = model(sample)\n    out.mean().backward()\n    if test_shape:\n        assert out.shape[2:] == sample.shape[2:]\n\n\n@pytest.mark.parametrize(""encoder_name"", ENCODERS)\n@pytest.mark.parametrize(""encoder_depth"", [3, 5])\n@pytest.mark.parametrize(""model_class"", [smp.FPN, smp.PSPNet, smp.Linknet, smp.Unet])\ndef test_forward(model_class, encoder_name, encoder_depth, **kwargs):\n    if model_class is smp.Unet:\n        kwargs[""decoder_channels""] = (16, 16, 16, 16, 16)[-encoder_depth:]\n    model = model_class(\n        encoder_name, encoder_depth=encoder_depth, encoder_weights=None, **kwargs\n    )\n    sample = get_sample(model_class)\n\n    if encoder_depth == 5 and model_class != smp.PSPNet:\n        test_shape = True\n    else:\n        test_shape = False\n\n    _test_forward(model, sample, test_shape)\n\n\n@pytest.mark.parametrize(\n    ""model_class"",\n    [smp.PAN, smp.FPN, smp.PSPNet, smp.Linknet, smp.Unet, smp.DeepLabV3]\n)\ndef test_forward_backward(model_class):\n    sample = get_sample(model_class)\n    model = model_class(DEFAULT_ENCODER, encoder_weights=None)\n    _test_forward_backward(model, sample)\n\n\n@pytest.mark.parametrize(""model_class"", [smp.PAN, smp.FPN, smp.PSPNet, smp.Linknet, smp.Unet])\ndef test_aux_output(model_class):\n    model = model_class(\n        DEFAULT_ENCODER, encoder_weights=None, aux_params=dict(classes=2)\n    )\n    sample = get_sample(model_class)\n    label_size = (sample.shape[0], 2)\n    mask, label = model(sample)\n    assert label.size() == label_size\n\n\n@pytest.mark.parametrize(""upsampling"", [2, 4, 8])\n@pytest.mark.parametrize(""model_class"", [smp.FPN, smp.PSPNet])\ndef test_upsample(model_class, upsampling):\n    default_upsampling = 4 if model_class is smp.FPN else 8\n    model = model_class(DEFAULT_ENCODER, encoder_weights=None, upsampling=upsampling)\n    sample = get_sample(model_class)\n    mask = model(sample)\n    assert mask.size()[-1] / 64 == upsampling / default_upsampling\n\n\n@pytest.mark.parametrize(""model_class"", [smp.FPN])\n@pytest.mark.parametrize(""encoder_name"", ENCODERS)\n@pytest.mark.parametrize(""in_channels"", [1, 2, 4])\ndef test_in_channels(model_class, encoder_name, in_channels):\n    sample = torch.ones([1, in_channels, 64, 64])\n    model = model_class(DEFAULT_ENCODER, encoder_weights=None, in_channels=in_channels)\n    with torch.no_grad():\n        model(sample)\n\n    assert model.encoder._in_channels == in_channels\n\n\n@pytest.mark.parametrize(""encoder_name"", ENCODERS)\ndef test_dilation(encoder_name):\n    if (encoder_name in [\'inceptionresnetv2\', \'xception\', \'inceptionv4\'] or\n            encoder_name.startswith(\'vgg\') or encoder_name.startswith(\'densenet\')):\n        return\n\n    encoder = smp.encoders.get_encoder(encoder_name)\n    encoder.make_dilated(\n        stage_list=[5],\n        dilation_list=[2],\n    )\n\n    encoder.eval()\n    with torch.no_grad():\n        sample = torch.ones([1, 3, 64, 64])\n        output = encoder(sample)\n\n    shapes = [out.shape[-1] for out in output]\n    assert shapes == [64, 32, 16, 8, 4, 4]  # last downsampling replaced with dilation\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
tests/test_preprocessing.py,0,"b""import os\nimport sys\nimport mock\nimport pytest\nimport numpy as np\n\n# mock detection module \nsys.modules['torchvision._C'] = mock.Mock()\n\nimport segmentation_models_pytorch as smp\n\n\ndef _test_preprocessing(inp, out, **params):\n    preprocessed_output = smp.encoders.preprocess_input(inp, **params)\n    assert np.allclose(preprocessed_output, out)\n\n\ndef test_mean():\n    inp = np.ones((32, 32, 3))\n    out = np.zeros((32, 32, 3))\n    mean = (1, 1, 1)\n    _test_preprocessing(inp, out, mean=mean)\n\n\ndef test_std():\n    inp = np.ones((32, 32, 3)) * 255\n    out = np.ones((32, 32, 3))\n    std = (255, 255, 255)\n    _test_preprocessing(inp, out, std=std)\n\n\ndef test_input_range():\n    inp = np.ones((32, 32, 3))\n    out = np.ones((32, 32, 3))\n    _test_preprocessing(inp, out, input_range=(0, 1))\n    _test_preprocessing(inp * 255, out, input_range=(0, 1))\n    _test_preprocessing(inp * 255, out * 255, input_range=(0, 255))\n\n\ndef test_input_space():\n    inp = np.stack(\n        [np.ones((32, 32)),\n        np.zeros((32, 32))],\n        axis=-1\n    )\n    out = np.stack(\n        [np.zeros((32, 32)),\n         np.ones((32, 32))],\n        axis=-1\n    )\n    _test_preprocessing(inp, out, input_space='BGR')\n"""
segmentation_models_pytorch/base/__init__.py,0,"b'from .model import SegmentationModel\n\nfrom .modules import (\n    Conv2dReLU,\n    Attention,\n)\n\nfrom .heads import (\n    SegmentationHead,\n    ClassificationHead,\n)'"
segmentation_models_pytorch/base/heads.py,1,"b'import torch.nn as nn\nfrom .modules import Flatten, Activation\n\n\nclass SegmentationHead(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n        activation = Activation(activation)\n        super().__init__(conv2d, upsampling, activation)\n\n\nclass ClassificationHead(nn.Sequential):\n\n    def __init__(self, in_channels, classes, pooling=""avg"", dropout=0.2, activation=None):\n        if pooling not in (""max"", ""avg""):\n            raise ValueError(""Pooling should be one of (\'max\', \'avg\'), got {}."".format(pooling))\n        pool = nn.AdaptiveAvgPool2d(1) if pooling == \'avg\' else nn.AdaptiveMaxPool2d(1)\n        flatten = Flatten()\n        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n        linear = nn.Linear(in_channels, classes, bias=True)\n        activation = Activation(activation)\n        super().__init__(pool, flatten, dropout, linear, activation)\n'"
segmentation_models_pytorch/base/initialization.py,1,"b'import torch.nn as nn\n\n\ndef initialize_decoder(module):\n    for m in module.modules():\n\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_uniform_(m.weight, mode=""fan_in"", nonlinearity=""relu"")\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n\n        elif isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n\ndef initialize_head(module):\n    for m in module.modules():\n        if isinstance(m, (nn.Linear, nn.Conv2d)):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n'"
segmentation_models_pytorch/base/model.py,3,"b'import torch\nfrom . import initialization as init\n\n\nclass SegmentationModel(torch.nn.Module):\n\n    def initialize(self):\n        init.initialize_decoder(self.decoder)\n        init.initialize_head(self.segmentation_head)\n        if self.classification_head is not None:\n            init.initialize_head(self.classification_head)\n\n    def forward(self, x):\n        """"""Sequentially pass `x` trough model`s encoder, decoder and heads""""""\n        features = self.encoder(x)\n        decoder_output = self.decoder(*features)\n\n        masks = self.segmentation_head(decoder_output)\n\n        if self.classification_head is not None:\n            labels = self.classification_head(features[-1])\n            return masks, labels\n\n        return masks\n\n    def predict(self, x):\n        """"""Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n\n        Args:\n            x: 4D torch tensor with shape (batch_size, channels, height, width)\n\n        Return:\n            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n\n        """"""\n        if self.training:\n            self.eval()\n\n        with torch.no_grad():\n            x = self.forward(x)\n\n        return x\n'"
segmentation_models_pytorch/base/modules.py,2,"b'import torch\nimport torch.nn as nn\n\ntry:\n    from inplace_abn import InPlaceABN\nexcept ImportError:\n    InPlaceABN = None\n\n\nclass Conv2dReLU(nn.Sequential):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=0,\n            stride=1,\n            use_batchnorm=True,\n    ):\n\n        if use_batchnorm == ""inplace"" and InPlaceABN is None:\n            raise RuntimeError(\n                ""In order to use `use_batchnorm=\'inplace\'` inplace_abn package must be installed. ""\n                + ""To install see: https://github.com/mapillary/inplace_abn""\n            )\n\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        if use_batchnorm == ""inplace"":\n            bn = InPlaceABN(out_channels, activation=""leaky_relu"", activation_param=0.0)\n            relu = nn.Identity()\n\n        elif use_batchnorm and use_batchnorm != ""inplace"":\n            bn = nn.BatchNorm2d(out_channels)\n\n        else:\n            bn = nn.Identity()\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\n\nclass SCSEModule(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\n\nclass ArgMax(nn.Module):\n\n    def __init__(self, dim=None):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.argmax(x, dim=dim)\n\n\nclass Activation(nn.Module):\n\n    def __init__(self, name, **params):\n\n        super().__init__()\n\n        if name is None or name == \'identity\':\n            self.activation = nn.Identity(**params)\n        elif name == \'sigmoid\':\n            self.activation = nn.Sigmoid()\n        elif name == \'softmax2d\':\n            self.activation = nn.Softmax(dim=1, **params)\n        elif name == \'softmax\':\n            self.activation = nn.Softmax(**params)\n        elif name == \'logsoftmax\':\n            self.activation = nn.LogSoftmax(**params)\n        elif name == \'argmax\':\n            self.activation = ArgMax(**params)\n        elif name == \'argmax2d\':\n            self.activation = ArgMax(dim=1, **params)\n        elif callable(name):\n            self.activation = name(**params)\n        else:\n            raise ValueError(\'Activation should be callable/sigmoid/softmax/logsoftmax/None; got {}\'.format(name))\n\n    def forward(self, x):\n        return self.activation(x)\n\n\nclass Attention(nn.Module):\n\n    def __init__(self, name, **params):\n        super().__init__()\n\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == \'scse\':\n            self.attention = SCSEModule(**params)\n        else:\n            raise ValueError(""Attention {} is not implemented"".format(name))\n\n    def forward(self, x):\n        return self.attention(x)\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n'"
segmentation_models_pytorch/deeplabv3/__init__.py,0,"b'from .model import DeepLabV3, DeepLabV3Plus'"
segmentation_models_pytorch/deeplabv3/decoder.py,3,"b'""""""\nBSD 3-Clause License\n\nCopyright (c) Soumith Chintala 2016,\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n__all__ = [""DeepLabV3Decoder""]\n\n\nclass DeepLabV3Decoder(nn.Sequential):\n    def __init__(self, in_channels, out_channels=256, atrous_rates=(12, 24, 36)):\n        super().__init__(\n            ASPP(in_channels, out_channels, atrous_rates),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.out_channels = out_channels\n\n    def forward(self, *features):\n        return super().forward(features[-1])\n\n\nclass DeepLabV3PlusDecoder(nn.Module):\n    def __init__(\n        self,\n        encoder_channels,\n        out_channels=256,\n        atrous_rates=(12, 24, 36),\n        output_stride=16,\n    ):\n        super().__init__()\n        if output_stride not in {8, 16}:\n            raise ValueError(""Output stride should be 8 or 16, got {}."".format(output_stride))\n\n        self.out_channels = out_channels\n        self.output_stride = output_stride\n\n        self.aspp = nn.Sequential(\n            ASPP(encoder_channels[-1], out_channels, atrous_rates, separable=True),\n            SeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        scale_factor = 2 if output_stride == 8 else 4\n        self.up = nn.UpsamplingBilinear2d(scale_factor=scale_factor)\n\n        highres_in_channels = encoder_channels[-4]\n        highres_out_channels = 48   # proposed by authors of paper\n        self.block1 = nn.Sequential(\n            nn.Conv2d(highres_in_channels, highres_out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(highres_out_channels),\n            nn.ReLU(),\n        )\n        self.block2 = nn.Sequential(\n            SeparableConv2d(\n                highres_out_channels + out_channels,\n                out_channels,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n    def forward(self, *features):\n        aspp_features = self.aspp(features[-1])\n        aspp_features = self.up(aspp_features)\n        high_res_features = self.block1(features[-4])\n        concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n        fused_features = self.block2(concat_features)\n        return fused_features\n\n\nclass ASPPConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        super().__init__(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=3,\n                padding=dilation,\n                dilation=dilation,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n\nclass ASPPSeparableConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        super().__init__(\n            SeparableConv2d(\n                in_channels,\n                out_channels,\n                kernel_size=3,\n                padding=dilation,\n                dilation=dilation,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n\nclass ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        for mod in self:\n            x = mod(x)\n        return F.interpolate(x, size=size, mode=\'bilinear\', align_corners=False)\n\n\nclass ASPP(nn.Module):\n    def __init__(self, in_channels, out_channels, atrous_rates, separable=False):\n        super(ASPP, self).__init__()\n        modules = []\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(),\n            )\n        )\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        ASPPConvModule = ASPPConv if not separable else ASPPSeparableConv\n\n        modules.append(ASPPConvModule(in_channels, out_channels, rate1))\n        modules.append(ASPPConvModule(in_channels, out_channels, rate2))\n        modules.append(ASPPConvModule(in_channels, out_channels, rate3))\n        modules.append(ASPPPooling(in_channels, out_channels))\n\n        self.convs = nn.ModuleList(modules)\n\n        self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n        )\n\n    def forward(self, x):\n        res = []\n        for conv in self.convs:\n            res.append(conv(x))\n        res = torch.cat(res, dim=1)\n        return self.project(res)\n\n\nclass SeparableConv2d(nn.Sequential):\n\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1,\n            bias=True,\n    ):\n        dephtwise_conv = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=in_channels,\n            bias=False,\n        )\n        pointwise_conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            bias=bias,\n        )\n        super().__init__(dephtwise_conv, pointwise_conv)\n'"
segmentation_models_pytorch/deeplabv3/model.py,3,"b'import torch.nn as nn\n\nfrom typing import Optional\nfrom .decoder import DeepLabV3Decoder, DeepLabV3PlusDecoder\nfrom ..base import SegmentationModel, SegmentationHead, ClassificationHead\nfrom ..encoders import get_encoder\n\n\nclass DeepLabV3(SegmentationModel):\n    """"""DeepLabV3_ implemetation from ""Rethinking Atrous Convolution for Semantic Image Segmentation""\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n                extractor to build segmentation model.\n        encoder_depth: number of stages used in decoder, larger depth - more features are generated.\n            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have\n            spatial resolution (H/(2^depth), W/(2^depth)]\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        decoder_channels: a number of convolution filters in ASPP module (default 256).\n        in_channels: number of input channels for model, default is 3.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation (str, callable): activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax2d``, callable, None]\n        upsampling: optional, final upsampling factor\n            (default is 8 to preserve input -> output spatial shape identity)\n        aux_params: if specified model will have additional classification auxiliary output\n            build on top of encoder, supported params:\n                - classes (int): number of classes\n                - pooling (str): one of \'max\', \'avg\'. Default is \'avg\'.\n                - dropout (float): dropout factor in [0, 1)\n                - activation (str): activation function to apply ""sigmoid""/""softmax"" (could be None to return logits)\n    Returns:\n        ``torch.nn.Module``: **DeepLabV3**\n    .. _DeeplabV3:\n        https://arxiv.org/abs/1706.05587\n    """"""\n        \n    def __init__(\n            self,\n            encoder_name: str = ""resnet34"",\n            encoder_depth: int = 5,\n            encoder_weights: Optional[str] = ""imagenet"",\n            decoder_channels: int = 256,\n            in_channels: int = 3,\n            classes: int = 1,\n            activation: Optional[str] = None,\n            upsampling: int = 8,\n            aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n        self.encoder.make_dilated(\n            stage_list=[4, 5],\n            dilation_list=[2, 4]\n        )\n\n        self.decoder = DeepLabV3Decoder(\n            in_channels=self.encoder.out_channels[-1],\n            out_channels=decoder_channels,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=self.decoder.out_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n            upsampling=upsampling,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n\nclass DeepLabV3Plus(SegmentationModel):\n    """"""DeepLabV3Plus_ implemetation from ""Encoder-Decoder with Atrous Separable\nConvolution for Semantic Image Segmentation""\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n                extractor to build segmentation model.\n        encoder_depth: number of stages used in decoder, larger depth - more features are generated.\n            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have\n            spatial resolution (H/(2^depth), W/(2^depth)]\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        encoder_output_stride: downsampling factor for deepest encoder features (see original paper for explanation)\n        decoder_atrous_rates: dilation rates for ASPP module (should be a tuple of 3 integer values)\n        decoder_channels: a number of convolution filters in ASPP module (default 256).\n        in_channels: number of input channels for model, default is 3.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation (str, callable): activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax2d``, callable, None]\n        upsampling: optional, final upsampling factor\n            (default is 8 to preserve input -> output spatial shape identity)\n        aux_params: if specified model will have additional classification auxiliary output\n            build on top of encoder, supported params:\n                - classes (int): number of classes\n                - pooling (str): one of \'max\', \'avg\'. Default is \'avg\'.\n                - dropout (float): dropout factor in [0, 1)\n                - activation (str): activation function to apply ""sigmoid""/""softmax"" (could be None to return logits)\n    Returns:\n        ``torch.nn.Module``: **DeepLabV3Plus**\n    .. _DeeplabV3Plus:\n        https://arxiv.org/abs/1802.02611v3\n    """"""\n    def __init__(\n            self,\n            encoder_name: str = ""resnet34"",\n            encoder_depth: int = 5,\n            encoder_weights: Optional[str] = ""imagenet"",\n            encoder_output_stride: int = 16,\n            decoder_channels: int = 256,\n            decoder_atrous_rates: tuple = (12, 24, 36),\n            in_channels: int = 3,\n            classes: int = 1,\n            activation: Optional[str] = None,\n            upsampling: int = 4,\n            aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        if encoder_output_stride == 8:\n            self.encoder.make_dilated(\n                stage_list=[4, 5],\n                dilation_list=[2, 4]\n            )\n\n        elif encoder_output_stride == 16:\n            self.encoder.make_dilated(\n                stage_list=[5],\n                dilation_list=[2]\n            )\n        else:\n            raise ValueError(\n                ""Encoder output stride should be 8 or 16, got {}"".format(encoder_output_stride)\n            )\n\n        self.decoder = DeepLabV3PlusDecoder(\n            encoder_channels=self.encoder.out_channels,\n            out_channels=decoder_channels,\n            atrous_rates=decoder_atrous_rates,\n            output_stride=encoder_output_stride,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=self.decoder.out_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n            upsampling=upsampling,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n'"
segmentation_models_pytorch/encoders/__init__.py,1,"b'import functools\nimport torch.utils.model_zoo as model_zoo\n\nfrom .resnet import resnet_encoders\nfrom .dpn import dpn_encoders\nfrom .vgg import vgg_encoders\nfrom .senet import senet_encoders\nfrom .densenet import densenet_encoders\nfrom .inceptionresnetv2 import inceptionresnetv2_encoders\nfrom .inceptionv4 import inceptionv4_encoders\nfrom .efficientnet import efficient_net_encoders\nfrom .mobilenet import mobilenet_encoders\nfrom .xception import xception_encoders\nfrom .timm_efficientnet import timm_efficientnet_encoders\n\nfrom ._preprocessing import preprocess_input\n\nencoders = {}\nencoders.update(resnet_encoders)\nencoders.update(dpn_encoders)\nencoders.update(vgg_encoders)\nencoders.update(senet_encoders)\nencoders.update(densenet_encoders)\nencoders.update(inceptionresnetv2_encoders)\nencoders.update(inceptionv4_encoders)\nencoders.update(efficient_net_encoders)\nencoders.update(mobilenet_encoders)\nencoders.update(xception_encoders)\nencoders.update(timm_efficientnet_encoders)\n\n\ndef get_encoder(name, in_channels=3, depth=5, weights=None):\n    Encoder = encoders[name][""encoder""]\n    params = encoders[name][""params""]\n    params.update(depth=depth)\n    encoder = Encoder(**params)\n\n    if weights is not None:\n        settings = encoders[name][""pretrained_settings""][weights]\n        encoder.load_state_dict(model_zoo.load_url(settings[""url""]))\n\n    encoder.set_in_channels(in_channels)\n\n    return encoder\n\n\ndef get_encoder_names():\n    return list(encoders.keys())\n\n\ndef get_preprocessing_params(encoder_name, pretrained=""imagenet""):\n    settings = encoders[encoder_name][""pretrained_settings""]\n\n    if pretrained not in settings.keys():\n        raise ValueError(""Avaliable pretrained options {}"".format(settings.keys()))\n\n    formatted_settings = {}\n    formatted_settings[""input_space""] = settings[pretrained].get(""input_space"")\n    formatted_settings[""input_range""] = settings[pretrained].get(""input_range"")\n    formatted_settings[""mean""] = settings[pretrained].get(""mean"")\n    formatted_settings[""std""] = settings[pretrained].get(""std"")\n    return formatted_settings\n\n\ndef get_preprocessing_fn(encoder_name, pretrained=""imagenet""):\n    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n    return functools.partial(preprocess_input, **params)\n'"
segmentation_models_pytorch/encoders/_base.py,1,"b'import torch\nimport torch.nn as nn\nfrom typing import List\nfrom collections import OrderedDict\n\nfrom . import _utils as utils\n\n\nclass EncoderMixin:\n    """"""Add encoder functionality such as:\n        - output channels specification of feature tensors (produced by encoder)\n        - patching first convolution for arbitrary input channels\n    """"""\n\n    @property\n    def out_channels(self):\n        """"""Return channels dimensions for each tensor of forward output of encoder""""""\n        return self._out_channels[: self._depth + 1]\n\n    def set_in_channels(self, in_channels):\n        """"""Change first convolution chennels""""""\n        if in_channels == 3:\n            return\n\n        self._in_channels = in_channels\n        if self._out_channels[0] == 3:\n            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n\n        utils.patch_first_conv(model=self, in_channels=in_channels)\n\n    def get_stages(self):\n        """"""Method should be overridden in encoder""""""\n        raise NotImplementedError\n\n    def make_dilated(self, stage_list, dilation_list):\n        stages = self.get_stages()\n        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n            utils.replace_strides_with_dilation(\n                module=stages[stage_indx],\n                dilation_rate=dilation_rate,\n            )\n'"
segmentation_models_pytorch/encoders/_preprocessing.py,0,"b'import numpy as np\n\n\ndef preprocess_input(\n    x, mean=None, std=None, input_space=""RGB"", input_range=None, **kwargs\n):\n\n    if input_space == ""BGR"":\n        x = x[..., ::-1].copy()\n\n    if input_range is not None:\n        if x.max() > 1 and input_range[1] == 1:\n            x = x / 255.0\n\n    if mean is not None:\n        mean = np.array(mean)\n        x = x - mean\n\n    if std is not None:\n        std = np.array(std)\n        x = x / std\n\n    return x\n'"
segmentation_models_pytorch/encoders/_utils.py,2,"b'import torch\nimport torch.nn as nn\n\n\ndef patch_first_conv(model, in_channels):\n    """"""Change first convolution layer input channels.\n    In case:\n        in_channels == 1 or in_channels == 2 -> reuse original weights\n        in_channels > 3 -> make random kaiming normal initialization\n    """"""\n\n    # get first conv\n    for module in model.modules():\n        if isinstance(module, nn.Conv2d):\n            break\n\n    # change input channels for first conv\n    module.in_channels = in_channels\n    weight = module.weight.detach()\n    reset = False\n\n    if in_channels == 1:\n        weight = weight.sum(1, keepdim=True)\n    elif in_channels == 2:\n        weight = weight[:, :2] * (3.0 / 2.0)\n    else:\n        reset = True\n        weight = torch.Tensor(\n            module.out_channels,\n            module.in_channels // module.groups,\n            *module.kernel_size\n        )\n\n    module.weight = nn.parameter.Parameter(weight)\n    if reset:\n        module.reset_parameters()\n\n\ndef replace_strides_with_dilation(module, dilation_rate):\n    """"""Patch Conv2d modules replacing strides with dilation""""""\n    for mod in module.modules():\n        if isinstance(mod, nn.Conv2d):\n            mod.stride = (1, 1)\n            mod.dilation = (dilation_rate, dilation_rate)\n            kh, kw = mod.kernel_size\n            mod.padding = ((kh // 2) * dilation_rate, (kh // 2) * dilation_rate)\n\n            # Kostyl for EfficientNet\n            if hasattr(mod, ""static_padding""):\n                mod.static_padding = nn.Identity()\n'"
segmentation_models_pytorch/encoders/densenet.py,3,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\n\nimport re\nimport torch.nn as nn\n\nfrom pretrainedmodels.models.torchvision_models import pretrained_settings\nfrom torchvision.models.densenet import DenseNet\n\nfrom ._base import EncoderMixin\n\n\nclass TransitionWithSkip(nn.Module):\n\n    def __init__(self, module):\n        super().__init__()\n        self.module = module\n\n    def forward(self, x):\n        for module in self.module:\n            x = module(x)\n            if isinstance(module, nn.ReLU):\n                skip = x\n        return x, skip\n\n\nclass DenseNetEncoder(DenseNet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n        del self.classifier\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(""DenseNet encoders do not support dilated mode ""\n                         ""due to pooling operation for downsampling!"")\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.features.conv0, self.features.norm0, self.features.relu0),\n            nn.Sequential(self.features.pool0, self.features.denseblock1,\n                          TransitionWithSkip(self.features.transition1)),\n            nn.Sequential(self.features.denseblock2, TransitionWithSkip(self.features.transition2)),\n            nn.Sequential(self.features.denseblock3, TransitionWithSkip(self.features.transition3)),\n            nn.Sequential(self.features.denseblock4, self.features.norm5)\n        ]\n\n    def forward(self, x):\n\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            if isinstance(x, (list, tuple)):\n                x, skip = x\n                features.append(skip)\n            else:\n                features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict):\n        pattern = re.compile(\n            r""^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$""\n        )\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n\n        # remove linear\n        state_dict.pop(""classifier.bias"")\n        state_dict.pop(""classifier.weight"")\n\n        super().load_state_dict(state_dict)\n\n\ndensenet_encoders = {\n    ""densenet121"": {\n        ""encoder"": DenseNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""densenet121""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 1024),\n            ""num_init_features"": 64,\n            ""growth_rate"": 32,\n            ""block_config"": (6, 12, 24, 16),\n        },\n    },\n    ""densenet169"": {\n        ""encoder"": DenseNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""densenet169""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1280, 1664),\n            ""num_init_features"": 64,\n            ""growth_rate"": 32,\n            ""block_config"": (6, 12, 32, 32),\n        },\n    },\n    ""densenet201"": {\n        ""encoder"": DenseNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""densenet201""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1792, 1920),\n            ""num_init_features"": 64,\n            ""growth_rate"": 32,\n            ""block_config"": (6, 12, 48, 32),\n        },\n    },\n    ""densenet161"": {\n        ""encoder"": DenseNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""densenet161""],\n        ""params"": {\n            ""out_channels"": (3, 96, 384, 768, 2112, 2208),\n            ""num_init_features"": 96,\n            ""growth_rate"": 48,\n            ""block_config"": (6, 12, 36, 24),\n        },\n    },\n}\n'"
segmentation_models_pytorch/encoders/dpn.py,5,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom pretrainedmodels.models.dpn import DPN\nfrom pretrainedmodels.models.dpn import pretrained_settings\n\nfrom ._base import EncoderMixin\n\n\nclass DPNEncorder(DPN, EncoderMixin):\n    def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._stage_idxs = stage_idxs\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n\n        del self.last_linear\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.features[0].conv, self.features[0].bn, self.features[0].act),\n            nn.Sequential(self.features[0].pool, self.features[1 : self._stage_idxs[0]]),\n            self.features[self._stage_idxs[0] : self._stage_idxs[1]],\n            self.features[self._stage_idxs[1] : self._stage_idxs[2]],\n            self.features[self._stage_idxs[2] : self._stage_idxs[3]],\n        ]\n\n    def forward(self, x):\n\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            if isinstance(x, (list, tuple)):\n                features.append(F.relu(torch.cat(x, dim=1), inplace=True))\n            else:\n                features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(""last_linear.bias"")\n        state_dict.pop(""last_linear.weight"")\n        super().load_state_dict(state_dict, **kwargs)\n\n\ndpn_encoders = {\n    ""dpn68"": {\n        ""encoder"": DPNEncorder,\n        ""pretrained_settings"": pretrained_settings[""dpn68""],\n        ""params"": {\n            ""stage_idxs"": (4, 8, 20, 24),\n            ""out_channels"": (3, 10, 144, 320, 704, 832),\n            ""groups"": 32,\n            ""inc_sec"": (16, 32, 32, 64),\n            ""k_r"": 128,\n            ""k_sec"": (3, 4, 12, 3),\n            ""num_classes"": 1000,\n            ""num_init_features"": 10,\n            ""small"": True,\n            ""test_time_pool"": True,\n        },\n    },\n    ""dpn68b"": {\n        ""encoder"": DPNEncorder,\n        ""pretrained_settings"": pretrained_settings[""dpn68b""],\n        ""params"": {\n            ""stage_idxs"": (4, 8, 20, 24),\n            ""out_channels"": (3, 10, 144, 320, 704, 832),\n            ""b"": True,\n            ""groups"": 32,\n            ""inc_sec"": (16, 32, 32, 64),\n            ""k_r"": 128,\n            ""k_sec"": (3, 4, 12, 3),\n            ""num_classes"": 1000,\n            ""num_init_features"": 10,\n            ""small"": True,\n            ""test_time_pool"": True,\n        },\n    },\n    ""dpn92"": {\n        ""encoder"": DPNEncorder,\n        ""pretrained_settings"": pretrained_settings[""dpn92""],\n        ""params"": {\n            ""stage_idxs"": (4, 8, 28, 32),\n            ""out_channels"": (3, 64, 336, 704, 1552, 2688),\n            ""groups"": 32,\n            ""inc_sec"": (16, 32, 24, 128),\n            ""k_r"": 96,\n            ""k_sec"": (3, 4, 20, 3),\n            ""num_classes"": 1000,\n            ""num_init_features"": 64,\n            ""test_time_pool"": True,\n        },\n    },\n    ""dpn98"": {\n        ""encoder"": DPNEncorder,\n        ""pretrained_settings"": pretrained_settings[""dpn98""],\n        ""params"": {\n            ""stage_idxs"": (4, 10, 30, 34),\n            ""out_channels"": (3, 96, 336, 768, 1728, 2688),\n            ""groups"": 40,\n            ""inc_sec"": (16, 32, 32, 128),\n            ""k_r"": 160,\n            ""k_sec"": (3, 6, 20, 3),\n            ""num_classes"": 1000,\n            ""num_init_features"": 96,\n            ""test_time_pool"": True,\n        },\n    },\n    ""dpn107"": {\n        ""encoder"": DPNEncorder,\n        ""pretrained_settings"": pretrained_settings[""dpn107""],\n        ""params"": {\n            ""stage_idxs"": (5, 13, 33, 37),\n            ""out_channels"": (3, 128, 376, 1152, 2432, 2688),\n            ""groups"": 50,\n            ""inc_sec"": (20, 64, 64, 128),\n            ""k_r"": 200,\n            ""k_sec"": (4, 8, 20, 3),\n            ""num_classes"": 1000,\n            ""num_init_features"": 128,\n            ""test_time_pool"": True,\n        },\n    },\n    ""dpn131"": {\n        ""encoder"": DPNEncorder,\n        ""pretrained_settings"": pretrained_settings[""dpn131""],\n        ""params"": {\n            ""stage_idxs"": (5, 13, 41, 45),\n            ""out_channels"": (3, 128, 352, 832, 1984, 2688),\n            ""groups"": 40,\n            ""inc_sec"": (16, 32, 32, 128),\n            ""k_r"": 160,\n            ""k_sec"": (4, 8, 28, 3),\n            ""num_classes"": 1000,\n            ""num_init_features"": 128,\n            ""test_time_pool"": True,\n        },\n    },\n}\n'"
segmentation_models_pytorch/encoders/efficientnet.py,4,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\nimport torch.nn as nn\nfrom efficientnet_pytorch import EfficientNet\nfrom efficientnet_pytorch.utils import url_map, url_map_advprop, get_model_params\n\nfrom ._base import EncoderMixin\n\n\nclass EfficientNetEncoder(EfficientNet, EncoderMixin):\n    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n\n        blocks_args, global_params = get_model_params(model_name, override_params=None)\n        super().__init__(blocks_args, global_params)\n\n        self._stage_idxs = stage_idxs\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        del self._fc\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self._conv_stem, self._bn0, self._swish),\n            self._blocks[:self._stage_idxs[0]],\n            self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n            self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n            self._blocks[self._stage_idxs[2]:],\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        block_number = 0.\n        drop_connect_rate = self._global_params.drop_connect_rate\n\n        features = []\n        for i in range(self._depth + 1):\n\n            # Identity and Sequential stages\n            if i < 2:\n                x = stages[i](x)\n\n            # Block stages need drop_connect rate\n            else:\n                for module in stages[i]:\n                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n                    block_number += 1.\n                    x = module(x, drop_connect)\n\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(""_fc.bias"")\n        state_dict.pop(""_fc.weight"")\n        super().load_state_dict(state_dict, **kwargs)\n\n\ndef _get_pretrained_settings(encoder):\n    pretrained_settings = {\n        ""imagenet"": {\n            ""mean"": [0.485, 0.456, 0.406],\n            ""std"": [0.229, 0.224, 0.225],\n            ""url"": url_map[encoder],\n            ""input_space"": ""RGB"",\n            ""input_range"": [0, 1],\n        },\n        ""advprop"": {\n            ""mean"": [0.5, 0.5, 0.5],\n            ""std"": [0.5, 0.5, 0.5],\n            ""url"": url_map_advprop[encoder],\n            ""input_space"": ""RGB"",\n            ""input_range"": [0, 1],\n        }\n    }\n    return pretrained_settings\n\n\nefficient_net_encoders = {\n    ""efficientnet-b0"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": _get_pretrained_settings(""efficientnet-b0""),\n        ""params"": {\n            ""out_channels"": (3, 32, 24, 40, 112, 320),\n            ""stage_idxs"": (3, 5, 9, 16),\n            ""model_name"": ""efficientnet-b0"",\n        },\n    },\n    ""efficientnet-b1"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": _get_pretrained_settings(""efficientnet-b1""),\n        ""params"": {\n            ""out_channels"": (3, 32, 24, 40, 112, 320),\n            ""stage_idxs"": (5, 8, 16, 23),\n            ""model_name"": ""efficientnet-b1"",\n        },\n    },\n    ""efficientnet-b2"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": _get_pretrained_settings(""efficientnet-b2""),\n        ""params"": {\n            ""out_channels"": (3, 32, 24, 48, 120, 352),\n            ""stage_idxs"": (5, 8, 16, 23),\n            ""model_name"": ""efficientnet-b2"",\n        },\n    },\n    ""efficientnet-b3"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": _get_pretrained_settings(""efficientnet-b3""),\n        ""params"": {\n            ""out_channels"": (3, 40, 32, 48, 136, 384),\n            ""stage_idxs"": (5, 8, 18, 26),\n            ""model_name"": ""efficientnet-b3"",\n        },\n    },\n    ""efficientnet-b4"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": _get_pretrained_settings(""efficientnet-b4""),\n        ""params"": {\n            ""out_channels"": (3, 48, 32, 56, 160, 448),\n            ""stage_idxs"": (6, 10, 22, 32),\n            ""model_name"": ""efficientnet-b4"",\n        },\n    },\n    ""efficientnet-b5"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": _get_pretrained_settings(""efficientnet-b5""),\n        ""params"": {\n            ""out_channels"": (3, 48, 40, 64, 176, 512),\n            ""stage_idxs"": (8, 13, 27, 39),\n            ""model_name"": ""efficientnet-b5"",\n        },\n    },\n    ""efficientnet-b6"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": _get_pretrained_settings(""efficientnet-b6""),\n        ""params"": {\n            ""out_channels"": (3, 56, 40, 72, 200, 576),\n            ""stage_idxs"": (9, 15, 31, 45),\n            ""model_name"": ""efficientnet-b6"",\n        },\n    },\n    ""efficientnet-b7"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": _get_pretrained_settings(""efficientnet-b7""),\n        ""params"": {\n            ""out_channels"": (3, 64, 48, 80, 224, 640),\n            ""stage_idxs"": (11, 18, 38, 55),\n            ""model_name"": ""efficientnet-b7"",\n        },\n    },\n}\n'"
segmentation_models_pytorch/encoders/inceptionresnetv2.py,3,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\n\nimport torch.nn as nn\nfrom pretrainedmodels.models.inceptionresnetv2 import InceptionResNetV2\nfrom pretrainedmodels.models.inceptionresnetv2 import pretrained_settings\n\nfrom ._base import EncoderMixin\n\n\nclass InceptionResNetV2Encoder(InceptionResNetV2, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        # correct paddings\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.kernel_size == (3, 3):\n                    m.padding = (1, 1)\n            if isinstance(m, nn.MaxPool2d):\n                m.padding = (1, 1)\n\n        # remove linear layers\n        del self.avgpool_1a\n        del self.last_linear\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(""InceptionResnetV2 encoder does not support dilated mode ""\n                         ""due to pooling operation for downsampling!"")\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv2d_1a, self.conv2d_2a, self.conv2d_2b),\n            nn.Sequential(self.maxpool_3a, self.conv2d_3b, self.conv2d_4a),\n            nn.Sequential(self.maxpool_5a, self.mixed_5b, self.repeat),\n            nn.Sequential(self.mixed_6a, self.repeat_1),\n            nn.Sequential(self.mixed_7a, self.repeat_2, self.block8, self.conv2d_7b),\n        ]\n\n    def forward(self, x):\n\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(""last_linear.bias"")\n        state_dict.pop(""last_linear.weight"")\n        super().load_state_dict(state_dict, **kwargs)\n\n\ninceptionresnetv2_encoders = {\n    ""inceptionresnetv2"": {\n        ""encoder"": InceptionResNetV2Encoder,\n        ""pretrained_settings"": pretrained_settings[""inceptionresnetv2""],\n        ""params"": {""out_channels"": (3, 64, 192, 320, 1088, 1536), ""num_classes"": 1000},\n    }\n}\n'"
segmentation_models_pytorch/encoders/inceptionv4.py,3,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\n\nimport torch.nn as nn\nfrom pretrainedmodels.models.inceptionv4 import InceptionV4, BasicConv2d\nfrom pretrainedmodels.models.inceptionv4 import pretrained_settings\n\nfrom ._base import EncoderMixin\n\n\nclass InceptionV4Encoder(InceptionV4, EncoderMixin):\n    def __init__(self, stage_idxs, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._stage_idxs = stage_idxs\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        # correct paddings\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.kernel_size == (3, 3):\n                    m.padding = (1, 1)\n            if isinstance(m, nn.MaxPool2d):\n                m.padding = (1, 1)\n\n        # remove linear layers\n        del self.last_linear\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(""InceptionV4 encoder does not support dilated mode ""\n                         ""due to pooling operation for downsampling!"")\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            self.features[: self._stage_idxs[0]],\n            self.features[self._stage_idxs[0]: self._stage_idxs[1]],\n            self.features[self._stage_idxs[1]: self._stage_idxs[2]],\n            self.features[self._stage_idxs[2]: self._stage_idxs[3]],\n            self.features[self._stage_idxs[3]:],\n        ]\n\n    def forward(self, x):\n\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(""last_linear.bias"")\n        state_dict.pop(""last_linear.weight"")\n        super().load_state_dict(state_dict, **kwargs)\n\n\ninceptionv4_encoders = {\n    ""inceptionv4"": {\n        ""encoder"": InceptionV4Encoder,\n        ""pretrained_settings"": pretrained_settings[""inceptionv4""],\n        ""params"": {\n            ""stage_idxs"": (3, 5, 9, 15),\n            ""out_channels"": (3, 64, 192, 384, 1024, 1536),\n            ""num_classes"": 1001,\n        },\n    }\n}\n'"
segmentation_models_pytorch/encoders/mobilenet.py,4,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\n\nimport torchvision\nimport torch.nn as nn\n\nfrom ._base import EncoderMixin\n\n\nclass MobileNetV2Encoder(torchvision.models.MobileNetV2, EncoderMixin):\n\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n        del self.classifier\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            self.features[:2],\n            self.features[2:4],\n            self.features[4:7],\n            self.features[7:14],\n            self.features[14:],\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(""classifier.1.bias"")\n        state_dict.pop(""classifier.1.weight"")\n        super().load_state_dict(state_dict, **kwargs)\n\n\nmobilenet_encoders = {\n    ""mobilenet_v2"": {\n        ""encoder"": MobileNetV2Encoder,\n        ""pretrained_settings"": {\n            ""imagenet"": {\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225],\n                ""url"": ""https://download.pytorch.org/models/mobilenet_v2-b0353104.pth"",\n                ""input_space"": ""RGB"",\n                ""input_range"": [0, 1],\n            },\n        },\n        ""params"": {\n            ""out_channels"": (3, 16, 24, 32, 96, 1280),\n        },\n    },\n}\n'"
segmentation_models_pytorch/encoders/resnet.py,9,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\n\nimport torch.nn as nn\n\nfrom torchvision.models.resnet import ResNet\nfrom torchvision.models.resnet import BasicBlock\nfrom torchvision.models.resnet import Bottleneck\nfrom pretrainedmodels.models.torchvision_models import pretrained_settings\n\nfrom ._base import EncoderMixin\n\n\nclass ResNetEncoder(ResNet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n\n        del self.fc\n        del self.avgpool\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.relu),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(""fc.bias"")\n        state_dict.pop(""fc.weight"")\n        super().load_state_dict(state_dict, **kwargs)\n\n\nresnet_encoders = {\n    ""resnet18"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""resnet18""],\n        ""params"": {\n            ""out_channels"": (3, 64, 64, 128, 256, 512),\n            ""block"": BasicBlock,\n            ""layers"": [2, 2, 2, 2],\n        },\n    },\n    ""resnet34"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""resnet34""],\n        ""params"": {\n            ""out_channels"": (3, 64, 64, 128, 256, 512),\n            ""block"": BasicBlock,\n            ""layers"": [3, 4, 6, 3],\n        },\n    },\n    ""resnet50"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""resnet50""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": Bottleneck,\n            ""layers"": [3, 4, 6, 3],\n        },\n    },\n    ""resnet101"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""resnet101""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": Bottleneck,\n            ""layers"": [3, 4, 23, 3],\n        },\n    },\n    ""resnet152"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": pretrained_settings[""resnet152""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": Bottleneck,\n            ""layers"": [3, 8, 36, 3],\n        },\n    },\n    ""resnext50_32x4d"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": {\n                ""url"": ""https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth"",\n                ""input_space"": ""RGB"",\n                ""input_size"": [3, 224, 224],\n                ""input_range"": [0, 1],\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225],\n                ""num_classes"": 1000,\n            }\n        },\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": Bottleneck,\n            ""layers"": [3, 4, 6, 3],\n            ""groups"": 32,\n            ""width_per_group"": 4,\n        },\n    },\n    ""resnext101_32x8d"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": {\n                ""url"": ""https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth"",\n                ""input_space"": ""RGB"",\n                ""input_size"": [3, 224, 224],\n                ""input_range"": [0, 1],\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225],\n                ""num_classes"": 1000,\n            },\n            ""instagram"": {\n                ""url"": ""https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth"",\n                ""input_space"": ""RGB"",\n                ""input_size"": [3, 224, 224],\n                ""input_range"": [0, 1],\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225],\n                ""num_classes"": 1000,\n            },\n        },\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": Bottleneck,\n            ""layers"": [3, 4, 23, 3],\n            ""groups"": 32,\n            ""width_per_group"": 8,\n        },\n    },\n    ""resnext101_32x16d"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": {\n            ""instagram"": {\n                ""url"": ""https://download.pytorch.org/models/ig_resnext101_32x16-c6f796b0.pth"",\n                ""input_space"": ""RGB"",\n                ""input_size"": [3, 224, 224],\n                ""input_range"": [0, 1],\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225],\n                ""num_classes"": 1000,\n            }\n        },\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": Bottleneck,\n            ""layers"": [3, 4, 23, 3],\n            ""groups"": 32,\n            ""width_per_group"": 16,\n        },\n    },\n    ""resnext101_32x32d"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": {\n            ""instagram"": {\n                ""url"": ""https://download.pytorch.org/models/ig_resnext101_32x32-e4b90b00.pth"",\n                ""input_space"": ""RGB"",\n                ""input_size"": [3, 224, 224],\n                ""input_range"": [0, 1],\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225],\n                ""num_classes"": 1000,\n            }\n        },\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": Bottleneck,\n            ""layers"": [3, 4, 23, 3],\n            ""groups"": 32,\n            ""width_per_group"": 32,\n        },\n    },\n    ""resnext101_32x48d"": {\n        ""encoder"": ResNetEncoder,\n        ""pretrained_settings"": {\n            ""instagram"": {\n                ""url"": ""https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth"",\n                ""input_space"": ""RGB"",\n                ""input_size"": [3, 224, 224],\n                ""input_range"": [0, 1],\n                ""mean"": [0.485, 0.456, 0.406],\n                ""std"": [0.229, 0.224, 0.225],\n                ""num_classes"": 1000,\n            }\n        },\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": Bottleneck,\n            ""layers"": [3, 4, 23, 3],\n            ""groups"": 32,\n            ""width_per_group"": 48,\n        },\n    },\n}\n'"
segmentation_models_pytorch/encoders/senet.py,3,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\n\nimport torch.nn as nn\n\nfrom pretrainedmodels.models.senet import (\n    SENet,\n    SEBottleneck,\n    SEResNetBottleneck,\n    SEResNeXtBottleneck,\n    pretrained_settings,\n)\nfrom ._base import EncoderMixin\n\n\nclass SENetEncoder(SENet, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        del self.last_linear\n        del self.avg_pool\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            self.layer0[:-1],\n            nn.Sequential(self.layer0[-1], self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(""last_linear.bias"")\n        state_dict.pop(""last_linear.weight"")\n        super().load_state_dict(state_dict, **kwargs)\n\n\nsenet_encoders = {\n    ""senet154"": {\n        ""encoder"": SENetEncoder,\n        ""pretrained_settings"": pretrained_settings[""senet154""],\n        ""params"": {\n            ""out_channels"": (3, 128, 256, 512, 1024, 2048),\n            ""block"": SEBottleneck,\n            ""dropout_p"": 0.2,\n            ""groups"": 64,\n            ""layers"": [3, 8, 36, 3],\n            ""num_classes"": 1000,\n            ""reduction"": 16,\n        },\n    },\n    ""se_resnet50"": {\n        ""encoder"": SENetEncoder,\n        ""pretrained_settings"": pretrained_settings[""se_resnet50""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": SEResNetBottleneck,\n            ""layers"": [3, 4, 6, 3],\n            ""downsample_kernel_size"": 1,\n            ""downsample_padding"": 0,\n            ""dropout_p"": None,\n            ""groups"": 1,\n            ""inplanes"": 64,\n            ""input_3x3"": False,\n            ""num_classes"": 1000,\n            ""reduction"": 16,\n        },\n    },\n    ""se_resnet101"": {\n        ""encoder"": SENetEncoder,\n        ""pretrained_settings"": pretrained_settings[""se_resnet101""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": SEResNetBottleneck,\n            ""layers"": [3, 4, 23, 3],\n            ""downsample_kernel_size"": 1,\n            ""downsample_padding"": 0,\n            ""dropout_p"": None,\n            ""groups"": 1,\n            ""inplanes"": 64,\n            ""input_3x3"": False,\n            ""num_classes"": 1000,\n            ""reduction"": 16,\n        },\n    },\n    ""se_resnet152"": {\n        ""encoder"": SENetEncoder,\n        ""pretrained_settings"": pretrained_settings[""se_resnet152""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": SEResNetBottleneck,\n            ""layers"": [3, 8, 36, 3],\n            ""downsample_kernel_size"": 1,\n            ""downsample_padding"": 0,\n            ""dropout_p"": None,\n            ""groups"": 1,\n            ""inplanes"": 64,\n            ""input_3x3"": False,\n            ""num_classes"": 1000,\n            ""reduction"": 16,\n        },\n    },\n    ""se_resnext50_32x4d"": {\n        ""encoder"": SENetEncoder,\n        ""pretrained_settings"": pretrained_settings[""se_resnext50_32x4d""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": SEResNeXtBottleneck,\n            ""layers"": [3, 4, 6, 3],\n            ""downsample_kernel_size"": 1,\n            ""downsample_padding"": 0,\n            ""dropout_p"": None,\n            ""groups"": 32,\n            ""inplanes"": 64,\n            ""input_3x3"": False,\n            ""num_classes"": 1000,\n            ""reduction"": 16,\n        },\n    },\n    ""se_resnext101_32x4d"": {\n        ""encoder"": SENetEncoder,\n        ""pretrained_settings"": pretrained_settings[""se_resnext101_32x4d""],\n        ""params"": {\n            ""out_channels"": (3, 64, 256, 512, 1024, 2048),\n            ""block"": SEResNeXtBottleneck,\n            ""layers"": [3, 4, 23, 3],\n            ""downsample_kernel_size"": 1,\n            ""downsample_padding"": 0,\n            ""dropout_p"": None,\n            ""groups"": 32,\n            ""inplanes"": 64,\n            ""input_3x3"": False,\n            ""num_classes"": 1000,\n            ""reduction"": 16,\n        },\n    },\n}\n'"
segmentation_models_pytorch/encoders/timm_efficientnet.py,1,"b'import torch\nimport torch.nn as nn\n\nfrom timm.models.efficientnet import EfficientNet, Swish\nfrom timm.models.efficientnet import decode_arch_def, round_channels, default_cfgs\n\nfrom ._base import EncoderMixin\n\n\ndef get_efficientnet_kwargs(channel_multiplier=1.0, depth_multiplier=1.0):\n    """"""Creates an EfficientNet model.\n    Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    Paper: https://arxiv.org/abs/1905.11946\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n    \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n    \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n    \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n    \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n    \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n    \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n    \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n    \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n    \'efficientnet-b8\': (2.2, 3.6, 672, 0.5),\n    \'efficientnet-l2\': (4.3, 5.3, 800, 0.5),\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_e1_c16_se0.25\'],\n        [\'ir_r2_k3_s2_e6_c24_se0.25\'],\n        [\'ir_r2_k5_s2_e6_c40_se0.25\'],\n        [\'ir_r3_k3_s2_e6_c80_se0.25\'],\n        [\'ir_r3_k5_s1_e6_c112_se0.25\'],\n        [\'ir_r4_k5_s2_e6_c192_se0.25\'],\n        [\'ir_r1_k3_s1_e6_c320_se0.25\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=Swish,\n        norm_kwargs={},  # TODO: check\n        drop_rate=0.2,\n        drop_path_rate=0.2,\n    )\n    return model_kwargs\n\n\nclass EfficientNetEncoder(EfficientNet, EncoderMixin):\n\n    def __init__(self, stage_idxs, out_channels, depth=5, channel_multiplier=1.0, depth_multiplier=1.0):\n            kwargs = get_efficientnet_kwargs(channel_multiplier, depth_multiplier)\n            super().__init__(**kwargs)\n\n            self._stage_idxs = stage_idxs\n            self._out_channels = out_channels\n            self._depth = depth\n            self._in_channels = 3\n\n            del self.classifier\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv_stem, self.bn1, self.act1),\n            self.blocks[:self._stage_idxs[0]],\n            self.blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n            self.blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n            self.blocks[self._stage_idxs[2]:],\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(""classifier.bias"")\n        state_dict.pop(""classifier.weight"")\n        super().load_state_dict(state_dict, **kwargs)\n\n\ndef prepare_settings(settings):\n    return {\n        ""mean"": settings[""mean""],\n        ""std"": settings[""std""],\n        ""url"": settings[""url""],\n        ""input_range"": (0, 1),\n        ""input_space"": ""RGB"",\n    }\n\n\ntimm_efficientnet_encoders = {\n\n    ""timm-efficientnet-b0"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b0""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b0_ap""]),\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_b0_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 32, 24, 40, 112, 320),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 1.0,\n            ""depth_multiplier"": 1.0,\n        },\n    },\n\n    ""timm-efficientnet-b1"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b1""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b1_ap""]),\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_b1_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 32, 24, 40, 112, 320),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 1.0,\n            ""depth_multiplier"": 1.1,\n        },\n    },\n\n    ""timm-efficientnet-b2"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b2""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b2_ap""]),\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_b2_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 32, 24, 48, 120, 352),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 1.1,\n            ""depth_multiplier"": 1.2,\n        },\n    },\n\n    ""timm-efficientnet-b3"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b3""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b3_ap""]),\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_b3_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 40, 32, 48, 136, 384),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 1.2,\n            ""depth_multiplier"": 1.4,\n        },\n    },\n\n    ""timm-efficientnet-b4"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b4""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b4_ap""]),\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_b4_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 48, 32, 56, 160, 448),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 1.4,\n            ""depth_multiplier"": 1.8,\n        },\n    },\n\n    ""timm-efficientnet-b5"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b5""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b5_ap""]),\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_b5_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 48, 40, 64, 176, 512),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 1.6,\n            ""depth_multiplier"": 2.2,\n        },\n    },\n\n    ""timm-efficientnet-b6"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b6""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b6_ap""]),\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_b6_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 56, 40, 72, 200, 576),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 1.8,\n            ""depth_multiplier"": 2.6,\n        },\n    },\n\n    ""timm-efficientnet-b7"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b7""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b7_ap""]),\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_b7_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 64, 48, 80, 224, 640),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 2.0,\n            ""depth_multiplier"": 3.1,\n        },\n    },\n\n    ""timm-efficientnet-b8"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""imagenet"": prepare_settings(default_cfgs[""tf_efficientnet_b8""]),\n            ""advprop"": prepare_settings(default_cfgs[""tf_efficientnet_b8_ap""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 72, 56, 88, 248, 704),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 2.2,\n            ""depth_multiplier"": 3.6,\n        },\n    },\n\n    ""timm-efficientnet-l2"": {\n        ""encoder"": EfficientNetEncoder,\n        ""pretrained_settings"": {\n            ""noisy-student"": prepare_settings(default_cfgs[""tf_efficientnet_l2_ns""]),\n        },\n        ""params"": {\n            ""out_channels"": (3, 136, 104, 176, 480, 1376),\n            ""stage_idxs"": (2, 3, 5),\n            ""channel_multiplier"": 4.3,\n            ""depth_multiplier"": 5.3,\n        },\n    },\n}\n'"
segmentation_models_pytorch/encoders/vgg.py,3,"b'"""""" Each encoder should have following attributes and methods and be inherited from `_base.EncoderMixin`\n\nAttributes:\n\n    _out_channels (list of int): specify number of channels for each encoder feature tensor\n    _depth (int): specify number of stages in decoder (in other words number of downsampling operations)\n    _in_channels (int): default number of input channels in first Conv2d layer for encoder (usually 3)\n\nMethods:\n\n    forward(self, x: torch.Tensor)\n        produce list of features of different spatial resolutions, each feature is a 4D torch.tensor of\n        shape NCHW (features should be sorted in descending order according to spatial resolution, starting\n        with resolution same as input `x` tensor).\n\n        Input: `x` with shape (1, 3, 64, 64)\n        Output: [f0, f1, f2, f3, f4, f5] - features with corresponding shapes\n                [(1, 3, 64, 64), (1, 64, 32, 32), (1, 128, 16, 16), (1, 256, 8, 8),\n                (1, 512, 4, 4), (1, 1024, 2, 2)] (C - dim may differ)\n\n        also should support number of features according to specified depth, e.g. if depth = 5,\n        number of feature tensors = 6 (one with same resolution as input and 5 downsampled),\n        depth = 3 -> number of feature tensors = 4 (one with same resolution as input and 3 downsampled).\n""""""\n\nimport torch.nn as nn\nfrom torchvision.models.vgg import VGG\nfrom torchvision.models.vgg import make_layers\nfrom pretrainedmodels.models.torchvision_models import pretrained_settings\n\nfrom ._base import EncoderMixin\n\n# fmt: off\ncfg = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n# fmt: on\n\n\nclass VGGEncoder(VGG, EncoderMixin):\n    def __init__(self, out_channels, config, batch_norm=False, depth=5, **kwargs):\n        super().__init__(make_layers(config, batch_norm=batch_norm), **kwargs)\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n        del self.classifier\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(""\'VGG\' models do not support dilated mode due to Max Pooling""\n                         "" operations for downsampling!"")\n\n    def get_stages(self):\n        stages = []\n        stage_modules = []\n        for module in self.features:\n            if isinstance(module, nn.MaxPool2d):\n                stages.append(nn.Sequential(*stage_modules))\n                stage_modules = []\n            stage_modules.append(module)\n        stages.append(nn.Sequential(*stage_modules))\n        return stages\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        keys = list(state_dict.keys())\n        for k in keys:\n            if k.startswith(""classifier""):\n                state_dict.pop(k)\n        super().load_state_dict(state_dict, **kwargs)\n\n\nvgg_encoders = {\n    ""vgg11"": {\n        ""encoder"": VGGEncoder,\n        ""pretrained_settings"": pretrained_settings[""vgg11""],\n        ""params"": {\n            ""out_channels"": (64, 128, 256, 512, 512, 512),\n            ""config"": cfg[""A""],\n            ""batch_norm"": False,\n        },\n    },\n    ""vgg11_bn"": {\n        ""encoder"": VGGEncoder,\n        ""pretrained_settings"": pretrained_settings[""vgg11_bn""],\n        ""params"": {\n            ""out_channels"": (64, 128, 256, 512, 512, 512),\n            ""config"": cfg[""A""],\n            ""batch_norm"": True,\n        },\n    },\n    ""vgg13"": {\n        ""encoder"": VGGEncoder,\n        ""pretrained_settings"": pretrained_settings[""vgg13""],\n        ""params"": {\n            ""out_channels"": (64, 128, 256, 512, 512, 512),\n            ""config"": cfg[""B""],\n            ""batch_norm"": False,\n        },\n    },\n    ""vgg13_bn"": {\n        ""encoder"": VGGEncoder,\n        ""pretrained_settings"": pretrained_settings[""vgg13_bn""],\n        ""params"": {\n            ""out_channels"": (64, 128, 256, 512, 512, 512),\n            ""config"": cfg[""B""],\n            ""batch_norm"": True,\n        },\n    },\n    ""vgg16"": {\n        ""encoder"": VGGEncoder,\n        ""pretrained_settings"": pretrained_settings[""vgg16""],\n        ""params"": {\n            ""out_channels"": (64, 128, 256, 512, 512, 512),\n            ""config"": cfg[""D""],\n            ""batch_norm"": False,\n        },\n    },\n    ""vgg16_bn"": {\n        ""encoder"": VGGEncoder,\n        ""pretrained_settings"": pretrained_settings[""vgg16_bn""],\n        ""params"": {\n            ""out_channels"": (64, 128, 256, 512, 512, 512),\n            ""config"": cfg[""D""],\n            ""batch_norm"": True,\n        },\n    },\n    ""vgg19"": {\n        ""encoder"": VGGEncoder,\n        ""pretrained_settings"": pretrained_settings[""vgg19""],\n        ""params"": {\n            ""out_channels"": (64, 128, 256, 512, 512, 512),\n            ""config"": cfg[""E""],\n            ""batch_norm"": False,\n        },\n    },\n    ""vgg19_bn"": {\n        ""encoder"": VGGEncoder,\n        ""pretrained_settings"": pretrained_settings[""vgg19_bn""],\n        ""params"": {\n            ""out_channels"": (64, 128, 256, 512, 512, 512),\n            ""config"": cfg[""E""],\n            ""batch_norm"": True,\n        },\n    },\n}\n'"
segmentation_models_pytorch/encoders/xception.py,1,"b'import re\nimport torch.nn as nn\n\nfrom pretrainedmodels.models.xception import pretrained_settings\nfrom pretrainedmodels.models.xception import Xception\n\nfrom ._base import EncoderMixin\n\n\nclass XceptionEncoder(Xception, EncoderMixin):\n\n    def __init__(self, out_channels, *args, depth=5, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._out_channels = out_channels\n        self._depth = depth\n        self._in_channels = 3\n\n        # modify padding to maintain output shape\n        self.conv1.padding = (1, 1)\n        self.conv2.padding = (1, 1)\n\n        del self.fc\n\n    def make_dilated(self, stage_list, dilation_list):\n        raise ValueError(""Xception encoder does not support dilated mode ""\n                         ""due to pooling operation for downsampling!"")\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.relu, self.conv2, self.bn2, self.relu),\n            self.block1,\n            self.block2,\n            nn.Sequential(self.block3, self.block4, self.block5, self.block6, self.block7,\n                          self.block8, self.block9, self.block10, self.block11),\n            nn.Sequential(self.block12, self.conv3, self.bn3, self.relu, self.conv4, self.bn4),\n        ]\n\n    def forward(self, x):\n        stages = self.get_stages()\n\n        features = []\n        for i in range(self._depth + 1):\n            x = stages[i](x)\n            features.append(x)\n\n        return features\n\n    def load_state_dict(self, state_dict):\n        # remove linear\n        state_dict.pop(\'fc.bias\')\n        state_dict.pop(\'fc.weight\')\n\n        super().load_state_dict(state_dict)\n\n\nxception_encoders = {\n    \'xception\': {\n        \'encoder\': XceptionEncoder,\n        \'pretrained_settings\': pretrained_settings[\'xception\'],\n        \'params\': {\n            \'out_channels\': (3, 64, 128, 256, 728, 2048),\n        }\n    },\n}\n'"
segmentation_models_pytorch/fpn/__init__.py,0,b'from .model import FPN'
segmentation_models_pytorch/fpn/decoder.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Conv3x3GNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.block = nn.Sequential(\n            nn.Conv2d(\n                in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False\n            ),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2, mode=""bilinear"", align_corners=True)\n        return x\n\n\nclass FPNBlock(nn.Module):\n    def __init__(self, pyramid_channels, skip_channels):\n        super().__init__()\n        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        skip = self.skip_conv(skip)\n        x = x + skip\n        return x\n\n\nclass SegmentationBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, n_upsamples=0):\n        super().__init__()\n\n        blocks = [Conv3x3GNReLU(in_channels, out_channels, upsample=bool(n_upsamples))]\n\n        if n_upsamples > 1:\n            for _ in range(1, n_upsamples):\n                blocks.append(Conv3x3GNReLU(out_channels, out_channels, upsample=True))\n\n        self.block = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass MergeBlock(nn.Module):\n    def __init__(self, policy):\n        super().__init__()\n        if policy not in [""add"", ""cat""]:\n            raise ValueError(\n                ""`merge_policy` must be one of: [\'add\', \'cat\'], got {}"".format(\n                    policy\n                )\n            )\n        self.policy = policy\n\n    def forward(self, x):\n        if self.policy == \'add\':\n            return sum(x)\n        elif self.policy == \'cat\':\n            return torch.cat(x, dim=1)\n        else:\n            raise ValueError(\n                ""`merge_policy` must be one of: [\'add\', \'cat\'], got {}"".format(self.policy)\n            )\n\n\nclass FPNDecoder(nn.Module):\n    def __init__(\n            self,\n            encoder_channels,\n            encoder_depth=5,\n            pyramid_channels=256,\n            segmentation_channels=128,\n            dropout=0.2,\n            merge_policy=""add"",\n    ):\n        super().__init__()\n\n        self.out_channels = segmentation_channels if merge_policy == ""add"" else segmentation_channels * 4\n        if encoder_depth < 3:\n            raise ValueError(""Encoder depth for FPN decoder cannot be less than 3, got {}."".format(encoder_depth))\n\n        encoder_channels = encoder_channels[::-1]\n        encoder_channels = encoder_channels[:encoder_depth + 1]\n\n        self.p5 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)\n        self.p4 = FPNBlock(pyramid_channels, encoder_channels[1])\n        self.p3 = FPNBlock(pyramid_channels, encoder_channels[2])\n        self.p2 = FPNBlock(pyramid_channels, encoder_channels[3])\n\n        self.seg_blocks = nn.ModuleList([\n            SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)\n            for n_upsamples in [3, 2, 1, 0]\n        ])\n\n        self.merge = MergeBlock(merge_policy)\n        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n\n    def forward(self, *features):\n        c2, c3, c4, c5 = features[-4:]\n\n        p5 = self.p5(c5)\n        p4 = self.p4(p5, c4)\n        p3 = self.p3(p4, c3)\n        p2 = self.p2(p3, c2)\n\n        feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p5, p4, p3, p2])]\n        x = self.merge(feature_pyramid)\n        x = self.dropout(x)\n\n        return x\n'"
segmentation_models_pytorch/fpn/model.py,1,"b'from typing import Optional, Union\nfrom .decoder import FPNDecoder\nfrom ..base import SegmentationModel, SegmentationHead, ClassificationHead\nfrom ..encoders import get_encoder\n\n\nclass FPN(SegmentationModel):\n    """"""FPN_ is a fully convolution neural network for image semantic segmentation\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n                extractor to build segmentation model.\n        encoder_depth: number of stages used in decoder, larger depth - more features are generated.\n            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have\n            spatial resolution (H/(2^depth), W/(2^depth)]\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        decoder_pyramid_channels: a number of convolution filters in Feature Pyramid of FPN_.\n        decoder_segmentation_channels: a number of convolution filters in segmentation head of FPN_.\n        decoder_merge_policy: determines how to merge outputs inside FPN.\n            One of [``add``, ``cat``]\n        decoder_dropout: spatial dropout rate in range (0, 1).\n        in_channels: number of input channels for model, default is 3.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation (str, callable): activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax2d``, callable, None]\n        upsampling: optional, final upsampling factor\n            (default is 4 to preserve input -> output spatial shape identity)\n        aux_params: if specified model will have additional classification auxiliary output\n            build on top of encoder, supported params:\n                - classes (int): number of classes\n                - pooling (str): one of \'max\', \'avg\'. Default is \'avg\'.\n                - dropout (float): dropout factor in [0, 1)\n                - activation (str): activation function to apply ""sigmoid""/""softmax"" (could be None to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **FPN**\n\n    .. _FPN:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    """"""\n\n    def __init__(\n        self,\n        encoder_name: str = ""resnet34"",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = ""imagenet"",\n        decoder_pyramid_channels: int = 256,\n        decoder_segmentation_channels: int = 128,\n        decoder_merge_policy: str = ""add"",\n        decoder_dropout: float = 0.2,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[str] = None,\n        upsampling: int = 4,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = FPNDecoder(\n            encoder_channels=self.encoder.out_channels,\n            encoder_depth=encoder_depth,\n            pyramid_channels=decoder_pyramid_channels,\n            segmentation_channels=decoder_segmentation_channels,\n            dropout=decoder_dropout,\n            merge_policy=decoder_merge_policy,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=self.decoder.out_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=1,\n            upsampling=upsampling,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = ""fpn-{}"".format(encoder_name)\n        self.initialize()\n'"
segmentation_models_pytorch/linknet/__init__.py,0,b'from .model import Linknet'
segmentation_models_pytorch/linknet/decoder.py,1,"b'import torch.nn as nn\n\nfrom ..base import modules\n\n\nclass TransposeX2(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        super().__init__()\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(inplace=True)\n        ]\n\n        if use_batchnorm:\n            layers.insert(1, nn.BatchNorm2d(out_channels))\n\n        super().__init__(*layers)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            modules.Conv2dReLU(in_channels, in_channels // 4, kernel_size=1, use_batchnorm=use_batchnorm),\n            TransposeX2(in_channels // 4, in_channels // 4, use_batchnorm=use_batchnorm),\n            modules.Conv2dReLU(in_channels // 4, out_channels, kernel_size=1, use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x, skip=None):\n        x = self.block(x)\n        if skip is not None:\n            x = x + skip\n        return x\n\n\nclass LinknetDecoder(nn.Module):\n\n    def __init__(\n            self,\n            encoder_channels,\n            prefinal_channels=32,\n            n_blocks=5,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n\n        encoder_channels = encoder_channels[1:]  # remove first skip\n        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n\n        channels = list(encoder_channels) + [prefinal_channels]\n\n        self.blocks = nn.ModuleList([\n            DecoderBlock(channels[i], channels[i + 1], use_batchnorm=use_batchnorm)\n            for i in range(n_blocks)\n        ])\n\n    def forward(self, *features):\n        features = features[1:]  # remove first skip\n        features = features[::-1]  # reverse channels to start from head of encoder\n\n        x = features[0]\n        skips = features[1:]\n\n        for i, decoder_block in enumerate(self.blocks):\n            skip = skips[i] if i < len(skips) else None\n            x = decoder_block(x, skip)\n\n        return x\n'"
segmentation_models_pytorch/linknet/model.py,1,"b'from typing import Optional, Union\nfrom .decoder import LinknetDecoder\nfrom ..base import SegmentationHead, SegmentationModel, ClassificationHead\nfrom ..encoders import get_encoder\n\n\nclass Linknet(SegmentationModel):\n    """"""Linknet_ is a fully convolution neural network for fast image semantic segmentation\n\n    Note:\n        This implementation by default has 4 skip connections (original - 3).\n\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n            extractor to build segmentation model.\n        encoder_depth (int): number of stages used in decoder, larger depth - more features are generated.\n            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have\n            spatial resolution (H/(2^depth), W/(2^depth)]\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n            is used. If \'inplace\' InplaceABN will be used, allows to decrease memory consumption.\n            One of [True, False, \'inplace\']\n        in_channels: number of input channels for model, default is 3.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation: activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n        aux_params: if specified model will have additional classification auxiliary output\n            build on top of encoder, supported params:\n                - classes (int): number of classes\n                - pooling (str): one of \'max\', \'avg\'. Default is \'avg\'.\n                - dropout (float): dropout factor in [0, 1)\n                - activation (str): activation function to apply ""sigmoid""/""softmax"" (could be None to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **Linknet**\n\n    .. _Linknet:\n        https://arxiv.org/pdf/1707.03718.pdf\n    """"""\n\n    def __init__(\n        self,\n        encoder_name: str = ""resnet34"",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = ""imagenet"",\n        decoder_use_batchnorm: bool = True,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = LinknetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            n_blocks=encoder_depth,\n            prefinal_channels=32,\n            use_batchnorm=decoder_use_batchnorm,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=32, out_channels=classes, activation=activation, kernel_size=1\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = ""link-{}"".format(encoder_name)\n        self.initialize()\n'"
segmentation_models_pytorch/pan/__init__.py,0,b'from .model import PAN'
segmentation_models_pytorch/pan/decoder.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ConvBnRelu(nn.Module):\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            kernel_size: int,\n            stride: int = 1,\n            padding: int = 0,\n            dilation: int = 1,\n            groups: int = 1,\n            bias: bool = True,\n            add_relu: bool = True,\n            interpolate: bool = False\n    ):\n        super(ConvBnRelu, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n            stride=stride, padding=padding, dilation=dilation, bias=bias, groups=groups\n        )\n        self.add_relu = add_relu\n        self.interpolate = interpolate\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.add_relu:\n            x = self.activation(x)\n        if self.interpolate:\n            x = F.interpolate(x, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        return x\n\n\nclass FPABlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            upscale_mode=\'bilinear\'\n    ):\n        super(FPABlock, self).__init__()\n\n        self.upscale_mode = upscale_mode\n        if self.upscale_mode == \'bilinear\':\n            self.align_corners = True\n        else:\n            self.align_corners = False\n\n        # global pooling branch\n        self.branch1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n        )\n\n        # midddle branch\n        self.mid = nn.Sequential(\n            ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n        )\n        self.down1 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            ConvBnRelu(in_channels=in_channels, out_channels=1, kernel_size=7, stride=1, padding=3)\n        )\n        self.down2 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            ConvBnRelu(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=2)\n        )\n        self.down3 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            ConvBnRelu(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1),\n            ConvBnRelu(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1),\n        )\n        self.conv2 = ConvBnRelu(in_channels=1, out_channels=1, kernel_size=5, stride=1, padding=2)\n        self.conv1 = ConvBnRelu(in_channels=1, out_channels=1, kernel_size=7, stride=1, padding=3)\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n        b1 = self.branch1(x)\n        upscale_parameters = dict(\n            mode=self.upscale_mode,\n            align_corners=self.align_corners\n        )\n        b1 = F.interpolate(b1, size=(h, w), **upscale_parameters)\n\n        mid = self.mid(x)\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x3 = F.interpolate(x3, size=(h // 4, w // 4), **upscale_parameters)\n\n        x2 = self.conv2(x2)\n        x = x2 + x3\n        x = F.interpolate(x, size=(h // 2, w // 2), **upscale_parameters)\n\n        x1 = self.conv1(x1)\n        x = x + x1\n        x = F.interpolate(x, size=(h, w), **upscale_parameters)\n\n        x = torch.mul(x, mid)\n        x = x + b1\n        return x\n\n\nclass GAUBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            upscale_mode: str = \'bilinear\'\n    ):\n        super(GAUBlock, self).__init__()\n\n        self.upscale_mode = upscale_mode\n        self.align_corners = True if upscale_mode == \'bilinear\' else None\n\n        self.conv1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            ConvBnRelu(in_channels=out_channels, out_channels=out_channels, kernel_size=1, add_relu=False),\n            nn.Sigmoid()\n        )\n        self.conv2 = ConvBnRelu(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n\n    def forward(self, x, y):\n        """"""\n        Args:\n            x: low level feature\n            y: high level feature\n        """"""\n        h, w = x.size(2), x.size(3)\n        y_up = F.interpolate(\n            y, size=(h, w), mode=self.upscale_mode, align_corners=self.align_corners\n        )\n        x = self.conv2(x)\n        y = self.conv1(y)\n        z = torch.mul(x, y)\n        return y_up + z\n\n\nclass PANDecoder(nn.Module):\n\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels,\n            upscale_mode: str = \'bilinear\'\n    ):\n        super().__init__()\n\n        self.fpa = FPABlock(in_channels=encoder_channels[-1], out_channels=decoder_channels)\n        self.gau3 = GAUBlock(in_channels=encoder_channels[-2], out_channels=decoder_channels, upscale_mode=upscale_mode)\n        self.gau2 = GAUBlock(in_channels=encoder_channels[-3], out_channels=decoder_channels, upscale_mode=upscale_mode)\n        self.gau1 = GAUBlock(in_channels=encoder_channels[-4], out_channels=decoder_channels, upscale_mode=upscale_mode)\n\n    def forward(self, *features):\n        bottleneck = features[-1]\n        x5 = self.fpa(bottleneck)         # 1/32\n        x4 = self.gau3(features[-2], x5)  # 1/16\n        x3 = self.gau2(features[-3], x4)  # 1/8\n        x2 = self.gau1(features[-4], x3)  # 1/4\n\n        return x2\n'"
segmentation_models_pytorch/pan/model.py,1,"b'from typing import Optional, Union\nfrom .decoder import PANDecoder\nfrom ..encoders import get_encoder\nfrom ..base import SegmentationModel\nfrom ..base import SegmentationHead, ClassificationHead\n\n\nclass PAN(SegmentationModel):\n    """""" Implementation of _PAN (Pyramid Attention Network).\n    Currently works with shape of input tensor >= [B x C x 128 x 128] for pytorch <= 1.1.0\n    and with shape of input tensor >= [B x C x 256 x 256] for pytorch == 1.3.1\n\n\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n            extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        encoder_dilation: Flag to use dilation in encoder last layer.\n            Doesn\'t work with [``*ception*``, ``vgg*``, ``densenet*``] backbones, default is True.\n        decoder_channels: Number of ``Conv2D`` layer filters in decoder blocks\n        in_channels: number of input channels for model, default is 3.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation: activation function to apply after final convolution;\n            One of [``sigmoid``, ``softmax``, ``logsoftmax``, ``identity``, callable, None]\n        upsampling: optional, final upsampling factor\n            (default is 4 to preserve input -> output spatial shape identity)\n\n        aux_params: if specified model will have additional classification auxiliary output\n            build on top of encoder, supported params:\n                - classes (int): number of classes\n                - pooling (str): one of \'max\', \'avg\'. Default is \'avg\'.\n                - dropout (float): dropout factor in [0, 1)\n                - activation (str): activation function to apply ""sigmoid""/""softmax"" (could be None to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **PAN**\n\n    .. _PAN:\n        https://arxiv.org/abs/1805.10180\n\n    """"""\n\n    def __init__(\n            self,\n            encoder_name: str = ""resnet34"",\n            encoder_weights: str = ""imagenet"",\n            encoder_dilation: bool = True,\n            decoder_channels: int = 32,\n            in_channels: int = 3,\n            classes: int = 1,\n            activation: Optional[Union[str, callable]] = None,\n            upsampling: int = 4,\n            aux_params: Optional[dict] = None\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=5,\n            weights=encoder_weights,\n        )\n\n        if encoder_dilation:\n            self.encoder.make_dilated(\n                stage_list=[5],\n                dilation_list=[2]\n            )\n\n        self.decoder = PANDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels,\n            out_channels=classes,\n            activation=activation,\n            kernel_size=3,\n            upsampling=upsampling\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = ""pan-{}"".format(encoder_name)\n        self.initialize()\n'"
segmentation_models_pytorch/pspnet/__init__.py,0,b'from .model import PSPNet'
segmentation_models_pytorch/pspnet/decoder.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..base import modules\n\n\nclass PSPBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, pool_size, use_bathcnorm=True):\n        super().__init__()\n        if pool_size == 1:\n            use_bathcnorm = False  # PyTorch does not support BatchNorm for 1x1 shape\n        self.pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)),\n            modules.Conv2dReLU(in_channels, out_channels, (1, 1), use_batchnorm=use_bathcnorm)\n        )\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n        x = self.pool(x)\n        x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n        return x\n\n\nclass PSPModule(nn.Module):\n    def __init__(self, in_channels, sizes=(1, 2, 3, 6), use_bathcnorm=True):\n        super().__init__()\n\n        self.blocks = nn.ModuleList([\n            PSPBlock(in_channels, in_channels // len(sizes), size, use_bathcnorm=use_bathcnorm) for size in sizes\n        ])\n\n    def forward(self, x):\n        xs = [block(x) for block in self.blocks] + [x]\n        x = torch.cat(xs, dim=1)\n        return x\n\n\nclass PSPDecoder(nn.Module):\n\n    def __init__(\n            self,\n            encoder_channels,\n            use_batchnorm=True,\n            out_channels=512,\n            dropout=0.2,\n    ):\n        super().__init__()\n\n        self.psp = PSPModule(\n            in_channels=encoder_channels[-1],\n            sizes=(1, 2, 3, 6),\n            use_bathcnorm=use_batchnorm,\n        )\n\n        self.conv = modules.Conv2dReLU(\n            in_channels=encoder_channels[-1] * 2,\n            out_channels=out_channels,\n            kernel_size=1,\n            use_batchnorm=use_batchnorm,\n        )\n\n        self.dropout = nn.Dropout2d(p=dropout)\n\n    def forward(self, *features):\n        x = features[-1]\n        x = self.psp(x)\n        x = self.conv(x)\n        x = self.dropout(x)\n\n        return x\n"""
segmentation_models_pytorch/pspnet/model.py,1,"b'from typing import Optional, Union\n\nfrom .decoder import PSPDecoder\nfrom ..encoders import get_encoder\n\nfrom ..base import SegmentationModel\nfrom ..base import SegmentationHead, ClassificationHead\n\n\nclass PSPNet(SegmentationModel):\n    """"""PSPNet_ is a fully convolution neural network for image semantic segmentation\n\n    Args:\n        encoder_name: name of classification model used as feature\n                extractor to build segmentation model.\n        encoder_depth: number of stages used in decoder, larger depth - more features are generated.\n            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have\n            spatial resolution (H/(2^depth), W/(2^depth)]\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        psp_out_channels: number of filters in PSP block.\n        psp_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n            is used. If \'inplace\' InplaceABN will be used, allows to decrease memory consumption.\n            One of [True, False, \'inplace\']\n        psp_dropout: spatial dropout rate between 0 and 1.\n        in_channels: number of input channels for model, default is 3.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation: activation function used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n        upsampling: optional, final upsampling factor\n            (default is 8 for depth=3 to preserve input -> output spatial shape identity)\n        aux_params: if specified model will have additional classification auxiliary output\n            build on top of encoder, supported params:\n                - classes (int): number of classes\n                - pooling (str): one of \'max\', \'avg\'. Default is \'avg\'.\n                - dropout (float): dropout factor in [0, 1)\n                - activation (str): activation function to apply ""sigmoid""/""softmax"" (could be None to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **PSPNet**\n\n    .. _PSPNet:\n        https://arxiv.org/pdf/1612.01105.pdf\n    """"""\n\n    def __init__(\n        self,\n        encoder_name: str = ""resnet34"",\n        encoder_weights: Optional[str] = ""imagenet"",\n        encoder_depth: int = 3,\n        psp_out_channels: int = 512,\n        psp_use_batchnorm: bool = True,\n        psp_dropout: float = 0.2,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        upsampling: int = 8,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = PSPDecoder(\n            encoder_channels=self.encoder.out_channels,\n            use_batchnorm=psp_use_batchnorm,\n            out_channels=psp_out_channels,\n            dropout=psp_dropout,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=psp_out_channels,\n            out_channels=classes,\n            kernel_size=3,\n            activation=activation,\n            upsampling=upsampling,\n        )\n\n        if aux_params:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = ""psp-{}"".format(encoder_name)\n        self.initialize()\n'"
segmentation_models_pytorch/unet/__init__.py,0,b'from .model import Unet'
segmentation_models_pytorch/unet/decoder.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..base import modules as md\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n            self,\n            in_channels,\n            skip_channels,\n            out_channels,\n            use_batchnorm=True,\n            attention_type=None,\n    ):\n        super().__init__()\n        self.conv1 = md.Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n        self.conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=""nearest"")\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\n\nclass CenterBlock(nn.Sequential):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        conv1 = md.Conv2dReLU(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        conv2 = md.Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        super().__init__(conv1, conv2)\n\n\nclass UnetDecoder(nn.Module):\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels,\n            n_blocks=5,\n            use_batchnorm=True,\n            attention_type=None,\n            center=False,\n    ):\n        super().__init__()\n\n        if n_blocks != len(decoder_channels):\n            raise ValueError(\n                ""Model depth is {}, but you provide `decoder_channels` for {} blocks."".format(\n                    n_blocks, len(decoder_channels)\n                )\n            )\n\n        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        skip_channels = list(encoder_channels[1:]) + [0]\n        out_channels = decoder_channels\n\n        if center:\n            self.center = CenterBlock(\n                head_channels, head_channels, use_batchnorm=use_batchnorm\n            )\n        else:\n            self.center = nn.Identity()\n\n        # combine decoder keyword arguments\n        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n        blocks = [\n            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, *features):\n\n        features = features[1:]    # remove first skip with same spatial resolution\n        features = features[::-1]  # reverse channels to start from head of encoder\n\n        head = features[0]\n        skips = features[1:]\n\n        x = self.center(head)\n        for i, decoder_block in enumerate(self.blocks):\n            skip = skips[i] if i < len(skips) else None\n            x = decoder_block(x, skip)\n\n        return x\n'"
segmentation_models_pytorch/unet/model.py,1,"b'from typing import Optional, Union, List\nfrom .decoder import UnetDecoder\nfrom ..encoders import get_encoder\nfrom ..base import SegmentationModel\nfrom ..base import SegmentationHead, ClassificationHead\n\n\nclass Unet(SegmentationModel):\n    """"""Unet_ is a fully convolution neural network for image semantic segmentation\n\n    Args:\n        encoder_name: name of classification model (without last dense layers) used as feature\n            extractor to build segmentation model.\n        encoder_depth (int): number of stages used in decoder, larger depth - more features are generated.\n            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature tensor will have\n            spatial resolution (H/(2^depth), W/(2^depth)]\n        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n            is used. If \'inplace\' InplaceABN will be used, allows to decrease memory consumption.\n            One of [True, False, \'inplace\']\n        decoder_attention_type: attention module used in decoder of the model\n            One of [``None``, ``scse``]\n        in_channels: number of input channels for model, default is 3.\n        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n        activation: activation function to apply after final convolution;\n            One of [``sigmoid``, ``softmax``, ``logsoftmax``, ``identity``, callable, None]\n        aux_params: if specified model will have additional classification auxiliary output\n            build on top of encoder, supported params:\n                - classes (int): number of classes\n                - pooling (str): one of \'max\', \'avg\'. Default is \'avg\'.\n                - dropout (float): dropout factor in [0, 1)\n                - activation (str): activation function to apply ""sigmoid""/""softmax"" (could be None to return logits)\n\n    Returns:\n        ``torch.nn.Module``: **Unet**\n\n    .. _Unet:\n        https://arxiv.org/pdf/1505.04597\n\n    """"""\n\n    def __init__(\n        self,\n        encoder_name: str = ""resnet34"",\n        encoder_depth: int = 5,\n        encoder_weights: str = ""imagenet"",\n        decoder_use_batchnorm: bool = True,\n        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n        decoder_attention_type: Optional[str] = None,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = UnetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            center=True if encoder_name.startswith(""vgg"") else False,\n            attention_type=decoder_attention_type,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=classes,\n            activation=activation,\n            kernel_size=3,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = ""u-{}"".format(encoder_name)\n        self.initialize()\n'"
segmentation_models_pytorch/utils/__init__.py,0,b'from . import train\nfrom . import losses\nfrom . import metrics'
segmentation_models_pytorch/utils/base.py,3,"b""import re\nimport functools\nimport torch\nimport torch.nn as nn\n\n\nclass Activation(nn.Module):\n    def __init__(self, activation):\n        super().__init__()\n        if activation == None or activation == 'identity':\n            self.activation = nn.Identity()\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n        elif activation == 'softmax2d':\n            self.activation = functools.partial(torch.softmax, dim=1)\n        elif callable(activation):\n            self.activation = activation\n        else:\n            raise ValueError\n\n    def forward(self, x):\n        return self.activation(x)\n\nclass BaseObject(nn.Module):\n\n    def __init__(self, name=None):\n        super().__init__()\n        self._name = name\n\n    @property\n    def __name__(self):\n        if self._name is None:\n            name = self.__class__.__name__\n            s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n            return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n        else:\n            return self._name\n\n\nclass Metric(BaseObject):\n    pass\n\n\nclass Loss(BaseObject):\n\n    def __add__(self, other):\n        if isinstance(other, Loss):\n            return SumOfLosses(self, other)\n        else:\n            raise ValueError('Loss should be inherited from `Loss` class')\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __mul__(self, value):\n        if isinstance(value, (int, float)):\n            return MultipliedLoss(self, value)\n        else:\n            raise ValueError('Loss should be inherited from `BaseLoss` class')\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n\nclass SumOfLosses(Loss):\n\n    def __init__(self, l1, l2):\n        name = '{} + {}'.format(l1.__name__, l2.__name__)\n        super().__init__(name=name)\n        self.l1 = l1\n        self.l2 = l2\n\n    def __call__(self, *inputs):\n        return self.l1.forward(*inputs) + self.l2.forward(*inputs)\n\n\nclass MultipliedLoss(Loss):\n\n    def __init__(self, loss, multiplier):\n\n        # resolve name\n        if len(loss.__name__.split('+')) > 1:\n            name = '{} * ({})'.format(multiplier, loss.__name__)\n        else:\n            name = '{} * {}'.format(multiplier, loss.__name__)\n        super().__init__(name=name)\n        self.loss = loss\n        self.multiplier = multiplier\n\n    def __call__(self, *inputs):\n        return self.multiplier * self.loss.forward(*inputs)\n"""
segmentation_models_pytorch/utils/functional.py,21,"b'import torch\n\n\ndef _take_channels(*xs, ignore_channels=None):\n    if ignore_channels is None:\n        return xs\n    else:\n        channels = [channel for channel in range(xs[0].shape[1]) if channel not in ignore_channels]\n        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]\n        return xs\n\n\ndef _threshold(x, threshold=None):\n    if threshold is not None:\n        return (x > threshold).type(x.dtype)\n    else:\n        return x\n\n\ndef iou(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n    """"""Calculate Intersection over Union between ground truth and prediction\n    Args:\n        pr (torch.Tensor): predicted tensor\n        gt (torch.Tensor):  ground truth tensor\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: IoU (Jaccard) score\n    """"""\n\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    intersection = torch.sum(gt * pr)\n    union = torch.sum(gt) + torch.sum(pr) - intersection + eps\n    return (intersection + eps) / union\n\n\njaccard = iou\n\n\ndef f_score(pr, gt, beta=1, eps=1e-7, threshold=None, ignore_channels=None):\n    """"""Calculate F-score between ground truth and prediction\n    Args:\n        pr (torch.Tensor): predicted tensor\n        gt (torch.Tensor):  ground truth tensor\n        beta (float): positive constant\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: F score\n    """"""\n\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    tp = torch.sum(gt * pr)\n    fp = torch.sum(pr) - tp\n    fn = torch.sum(gt) - tp\n\n    score = ((1 + beta ** 2) * tp + eps) \\\n            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n\n    return score\n\n\ndef accuracy(pr, gt, threshold=0.5, ignore_channels=None):\n    """"""Calculate accuracy score between ground truth and prediction\n    Args:\n        pr (torch.Tensor): predicted tensor\n        gt (torch.Tensor):  ground truth tensor\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: precision score\n    """"""\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    tp = torch.sum(gt == pr, dtype=pr.dtype)\n    score = tp / gt.view(-1).shape[0]\n    return score\n\n\ndef precision(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n    """"""Calculate precision score between ground truth and prediction\n    Args:\n        pr (torch.Tensor): predicted tensor\n        gt (torch.Tensor):  ground truth tensor\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: precision score\n    """"""\n\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    tp = torch.sum(gt * pr)\n    fp = torch.sum(pr) - tp\n\n    score = (tp + eps) / (tp + fp + eps)\n\n    return score\n\n\ndef recall(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n    """"""Calculate Recall between ground truth and prediction\n    Args:\n        pr (torch.Tensor): A list of predicted elements\n        gt (torch.Tensor):  A list of elements that are to be predicted\n        eps (float): epsilon to avoid zero division\n        threshold: threshold for outputs binarization\n    Returns:\n        float: recall score\n    """"""\n\n    pr = _threshold(pr, threshold=threshold)\n    pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n    tp = torch.sum(gt * pr)\n    fn = torch.sum(gt) - tp\n\n    score = (tp + eps) / (tp + fn + eps)\n\n    return score\n'"
segmentation_models_pytorch/utils/losses.py,1,"b'import torch.nn as nn\n\nfrom . import base\nfrom . import functional as F\nfrom  .base import Activation\n\n\nclass JaccardLoss(base.Loss):\n\n    def __init__(self, eps=1., activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return 1 - F.jaccard(\n            y_pr, y_gt,\n            eps=self.eps,\n            threshold=None,\n            ignore_channels=self.ignore_channels,\n        )\n\n\nclass DiceLoss(base.Loss):\n\n    def __init__(self, eps=1., beta=1., activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.beta = beta\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return 1 - F.f_score(\n            y_pr, y_gt,\n            beta=self.beta,\n            eps=self.eps,\n            threshold=None,\n            ignore_channels=self.ignore_channels,\n        )\n\n\nclass L1Loss(nn.L1Loss, base.Loss):\n    pass\n\n\nclass MSELoss(nn.MSELoss, base.Loss):\n    pass\n\n\nclass CrossEntropyLoss(nn.CrossEntropyLoss, base.Loss):\n    pass\n\n\nclass NLLLoss(nn.NLLLoss, base.Loss):\n    pass\n\n\nclass BCELoss(nn.BCELoss, base.Loss):\n    pass\n\n\nclass BCEWithLogitsLoss(nn.BCEWithLogitsLoss, base.Loss):\n    pass\n'"
segmentation_models_pytorch/utils/meter.py,0,"b""import numpy as np\n\n\nclass Meter(object):\n    '''Meters provide a way to keep track of important statistics in an online manner.\n    This class is abstract, but provides a standard interface for all meters to follow.\n    '''\n\n    def reset(self):\n        '''Resets the meter to default settings.'''\n        pass\n\n    def add(self, value):\n        '''Log a new value to the meter\n        Args:\n            value: Next restult to include.\n        '''\n        pass\n\n    def value(self):\n        '''Get the value of the meter in the current state.'''\n        pass\n\n\nclass AverageValueMeter(Meter):\n    def __init__(self):\n        super(AverageValueMeter, self).__init__()\n        self.reset()\n        self.val = 0\n\n    def add(self, value, n=1):\n        self.val = value\n        self.sum += value\n        self.var += value * value\n        self.n += n\n\n        if self.n == 0:\n            self.mean, self.std = np.nan, np.nan\n        elif self.n == 1:\n            self.mean = 0.0 + self.sum  # This is to force a copy in torch/numpy\n            self.std = np.inf\n            self.mean_old = self.mean\n            self.m_s = 0.0\n        else:\n            self.mean = self.mean_old + (value - n * self.mean_old) / float(self.n)\n            self.m_s += (value - self.mean_old) * (value - self.mean)\n            self.mean_old = self.mean\n            self.std = np.sqrt(self.m_s / (self.n - 1.0))\n\n    def value(self):\n        return self.mean, self.std\n\n    def reset(self):\n        self.n = 0\n        self.sum = 0.0\n        self.var = 0.0\n        self.val = 0.0\n        self.mean = np.nan\n        self.mean_old = 0.0\n        self.m_s = 0.0\n        self.std = np.nan\n"""
segmentation_models_pytorch/utils/metrics.py,0,"b""from . import base\nfrom . import functional as F\nfrom .base import Activation\n\n\nclass IoU(base.Metric):\n    __name__ = 'iou_score'\n\n    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.iou(\n            y_pr, y_gt,\n            eps=self.eps,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n\n\nclass Fscore(base.Metric):\n\n    def __init__(self, beta=1, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.beta = beta\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.f_score(\n            y_pr, y_gt,\n            eps=self.eps,\n            beta=self.beta,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n\n\nclass Accuracy(base.Metric):\n\n    def __init__(self, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.accuracy(\n            y_pr, y_gt,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n\n\nclass Recall(base.Metric):\n\n    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.recall(\n            y_pr, y_gt,\n            eps=self.eps,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n\n\nclass Precision(base.Metric):\n\n    def __init__(self, eps=1e-7, threshold=0.5, activation=None, ignore_channels=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.threshold = threshold\n        self.activation = Activation(activation)\n        self.ignore_channels = ignore_channels\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return F.precision(\n            y_pr, y_gt,\n            eps=self.eps,\n            threshold=self.threshold,\n            ignore_channels=self.ignore_channels,\n        )\n"""
segmentation_models_pytorch/utils/train.py,1,"b""import sys\nimport torch\nfrom tqdm import tqdm as tqdm\nfrom .meter import AverageValueMeter\n\n\nclass Epoch:\n\n    def __init__(self, model, loss, metrics, stage_name, device='cpu', verbose=True):\n        self.model = model\n        self.loss = loss\n        self.metrics = metrics\n        self.stage_name = stage_name\n        self.verbose = verbose\n        self.device = device\n\n        self._to_device()\n\n    def _to_device(self):\n        self.model.to(self.device)\n        self.loss.to(self.device)\n        for metric in self.metrics:\n            metric.to(self.device)\n\n    def _format_logs(self, logs):\n        str_logs = ['{} - {:.4}'.format(k, v) for k, v in logs.items()]\n        s = ', '.join(str_logs)\n        return s\n\n    def batch_update(self, x, y):\n        raise NotImplementedError\n\n    def on_epoch_start(self):\n        pass\n\n    def run(self, dataloader):\n\n        self.on_epoch_start()\n\n        logs = {}\n        loss_meter = AverageValueMeter()\n        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}\n\n        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n            for x, y in iterator:\n                x, y = x.to(self.device), y.to(self.device)\n                loss, y_pred = self.batch_update(x, y)\n\n                # update loss logs\n                loss_value = loss.cpu().detach().numpy()\n                loss_meter.add(loss_value)\n                loss_logs = {self.loss.__name__: loss_meter.mean}\n                logs.update(loss_logs)\n\n                # update metrics logs\n                for metric_fn in self.metrics:\n                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()\n                    metrics_meters[metric_fn.__name__].add(metric_value)\n                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n                logs.update(metrics_logs)\n\n                if self.verbose:\n                    s = self._format_logs(logs)\n                    iterator.set_postfix_str(s)\n\n        return logs\n\n\nclass TrainEpoch(Epoch):\n\n    def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True):\n        super().__init__(\n            model=model,\n            loss=loss,\n            metrics=metrics,\n            stage_name='train',\n            device=device,\n            verbose=verbose,\n        )\n        self.optimizer = optimizer\n\n    def on_epoch_start(self):\n        self.model.train()\n\n    def batch_update(self, x, y):\n        self.optimizer.zero_grad()\n        prediction = self.model.forward(x)\n        loss = self.loss(prediction, y)\n        loss.backward()\n        self.optimizer.step()\n        return loss, prediction\n\n\nclass ValidEpoch(Epoch):\n\n    def __init__(self, model, loss, metrics, device='cpu', verbose=True):\n        super().__init__(\n            model=model,\n            loss=loss,\n            metrics=metrics,\n            stage_name='valid',\n            device=device,\n            verbose=verbose,\n        )\n\n    def on_epoch_start(self):\n        self.model.eval()\n\n    def batch_update(self, x, y):\n        with torch.no_grad():\n            prediction = self.model.forward(x)\n            loss = self.loss(prediction, y)\n        return loss, prediction\n"""
