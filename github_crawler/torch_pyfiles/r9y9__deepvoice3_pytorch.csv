file_path,api_count,code
audio.py,0,"b'import librosa\nimport librosa.filters\nimport math\nimport numpy as np\nfrom scipy import signal\nfrom hparams import hparams\nfrom scipy.io import wavfile\n\nimport lws\n\n\ndef load_wav(path):\n    return librosa.core.load(path, sr=hparams.sample_rate)[0]\n\n\ndef save_wav(wav, path):\n    wav = wav * 32767 / max(0.01, np.max(np.abs(wav)))\n    wavfile.write(path, hparams.sample_rate, wav.astype(np.int16))\n\n\ndef preemphasis(x):\n    from nnmnkwii.preprocessing import preemphasis\n    return preemphasis(x, hparams.preemphasis)\n\n\ndef inv_preemphasis(x):\n    from nnmnkwii.preprocessing import inv_preemphasis\n    return inv_preemphasis(x, hparams.preemphasis)\n\n\ndef spectrogram(y):\n    D = _lws_processor().stft(preemphasis(y)).T\n    S = _amp_to_db(np.abs(D)) - hparams.ref_level_db\n    return _normalize(S)\n\n\ndef inv_spectrogram(spectrogram):\n    \'\'\'Converts spectrogram to waveform using librosa\'\'\'\n    S = _db_to_amp(_denormalize(spectrogram) + hparams.ref_level_db)  # Convert back to linear\n    processor = _lws_processor()\n    D = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n    y = processor.istft(D).astype(np.float32)\n    return inv_preemphasis(y)\n\n\ndef melspectrogram(y):\n    D = _lws_processor().stft(preemphasis(y)).T\n    S = _amp_to_db(_linear_to_mel(np.abs(D))) - hparams.ref_level_db\n    if not hparams.allow_clipping_in_normalization:\n        assert S.max() <= 0 and S.min() - hparams.min_level_db >= 0\n    return _normalize(S)\n\n\ndef _lws_processor():\n    return lws.lws(hparams.fft_size, hparams.hop_size, mode=""speech"")\n\n\n# Conversions:\n\n\n_mel_basis = None\n\n\ndef _linear_to_mel(spectrogram):\n    global _mel_basis\n    if _mel_basis is None:\n        _mel_basis = _build_mel_basis()\n    return np.dot(_mel_basis, spectrogram)\n\n\ndef _build_mel_basis():\n    if hparams.fmax is not None:\n        assert hparams.fmax <= hparams.sample_rate // 2\n    return librosa.filters.mel(hparams.sample_rate, hparams.fft_size,\n                               fmin=hparams.fmin, fmax=hparams.fmax,\n                               n_mels=hparams.num_mels)\n\n\ndef _amp_to_db(x):\n    min_level = np.exp(hparams.min_level_db / 20 * np.log(10))\n    return 20 * np.log10(np.maximum(min_level, x))\n\n\ndef _db_to_amp(x):\n    return np.power(10.0, x * 0.05)\n\n\ndef _normalize(S):\n    return np.clip((S - hparams.min_level_db) / -hparams.min_level_db, 0, 1)\n\n\ndef _denormalize(S):\n    return (np.clip(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db\n'"
compute_timestamp_ratio.py,0,"b'""""""Compute output/input timestamp ratio.\n\nusage: compute_timestamp_ratio.py [options] <data_root>\n\noptions:\n    --hparams=<parmas>        Hyper parameters [default: ].\n    --preset=<json>           Path of preset parameters (json).\n    -h, --help                Show this help message and exit\n""""""\nfrom docopt import docopt\nimport sys\nimport numpy as np\nfrom hparams import hparams, hparams_debug_string\nimport train\nfrom train import TextDataSource, MelSpecDataSource\nfrom nnmnkwii.datasets import FileSourceDataset\nfrom tqdm import trange\nfrom deepvoice3_pytorch import frontend\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    data_root = args[""<data_root>""]\n    preset = args[""--preset""]\n\n    # Load preset if specified\n    if preset is not None:\n        with open(preset) as f:\n            hparams.parse_json(f.read())\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n    assert hparams.name == ""deepvoice3""\n\n    train._frontend = getattr(frontend, hparams.frontend)\n\n    # Code below\n    X = FileSourceDataset(TextDataSource(data_root))\n    Mel = FileSourceDataset(MelSpecDataSource(data_root))\n\n    in_sizes = []\n    out_sizes = []\n    for i in trange(len(X)):\n        x, m = X[i], Mel[i]\n        if X.file_data_source.multi_speaker:\n            x = x[0]\n        in_sizes.append(x.shape[0])\n        out_sizes.append(m.shape[0])\n\n    in_sizes = np.array(in_sizes)\n    out_sizes = np.array(out_sizes)\n\n    input_timestamps = np.sum(in_sizes)\n    output_timestamps = np.sum(out_sizes) / hparams.outputs_per_step / hparams.downsample_step\n\n    print(input_timestamps, output_timestamps, output_timestamps / input_timestamps)\n    sys.exit(0)\n'"
dump_hparams_to_json.py,0,"b'# coding: utf-8\n""""""\nDump hyper parameters to json file.\n\nusage: dump_hparams_to_json.py [options] <output_json_path>\n\noptions:\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\n\nimport sys\nimport os\nfrom os.path import dirname, join, basename, splitext\n\nimport audio\n\n# The deepvoice3 model\nfrom deepvoice3_pytorch import frontend\nfrom hparams import hparams\nimport json\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    output_json_path = args[""<output_json_path>""]\n\n    j = hparams.values()\n\n    # for compat legacy\n    for k in [""preset"", ""presets""]:\n        if k in j:\n            del j[k]\n\n    with open(output_json_path, ""w"") as f:\n        json.dump(j, f, indent=2)\n    sys.exit(0)\n'"
gentle_web_align.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Apr 21 09:06:37 2018\nPhoneme alignment and conversion in HTK-style label file using Web-served Gentle\nThis works on any type of english dataset.\nUnlike prepare_htk_alignments_vctk.py, this is Python3 and Windows(with Docker) compatible.\nPreliminary results show that gentle has better performance with noisy dataset\n(e.g. movie extracted audioclips)\n*This work was derived from vctk_preprocess/prepare_htk_alignments_vctk.py\n@author: engiecat(github)\n\nusage:\n    gentle_web_align.py (-w wav_pattern) (-t text_pattern) [options]\n    gentle_web_align.py (--nested-directories=<main_directory>) [options]\n\noptions:\n    -w <wav_pattern> --wav_pattern=<wav_pattern> Pattern of wav files to be aligned\n    -t <txt_pattern> --txt_pattern=<txt_pattern> Pattern of txt transcript files to be aligned (same name required)\n    --nested-directories=<main_directory>        Process every wav/txt file in the subfolders of the given folder\n    --server_addr=<server_addr>                  Server address that serves gentle. [default: localhost]\n    --port=<port>                                Server port that serves gentle. [default: 8567]\n    --max_unalign=<max_unalign>                  Maximum threshold for unalignment occurence (0.0 ~ 1.0) [default: 0.3] \n    --skip-already-done                          Skips if there are preexisting .lab file\n    -h --help                                    show this help message and exit\n""""""\n\nfrom docopt import docopt\nfrom glob import glob\nfrom tqdm import tqdm\nimport os.path\nimport requests\nimport numpy as np\n\ndef write_hts_label(labels, lab_path):\n    lab = """"\n    for s, e, l in labels:\n        s, e = float(s) * 1e7, float(e) * 1e7\n        s, e = int(s), int(e)\n        lab += ""{} {} {}\\n"".format(s, e, l)\n    print(lab)\n    with open(lab_path, ""w"", encoding=\'utf-8\') as f:\n        f.write(lab)\n\n\ndef json2hts(data):\n    emit_bos = False\n    emit_eos = False\n\n    phone_start = 0\n    phone_end = None\n    labels = []\n    failure_count = 0\n    \n    for word in data[""words""]:\n        case = word[""case""]\n        if case != ""success"":\n            failure_count += 1 # instead of failing everything, \n            #raise RuntimeError(""Alignment failed"")\n            continue\n        start = float(word[""start""])\n        word_end = float(word[""end""])\n\n        if not emit_bos:\n            labels.append((phone_start, start, ""silB""))\n            emit_bos = True\n\n        phone_start = start\n        phone_end = None\n        for phone in word[""phones""]:\n            ph = str(phone[""phone""][:-2])\n            duration = float(phone[""duration""])\n            phone_end = phone_start + duration\n            labels.append((phone_start, phone_end, ph))\n            phone_start += duration\n        assert np.allclose(phone_end, word_end)\n    if not emit_eos:\n        labels.append((phone_start, phone_end, ""silE""))\n        emit_eos = True\n    unalign_ratio = float(failure_count) / len(data[\'words\'])\n    return unalign_ratio, labels\n\n\ndef gentle_request(wav_path,txt_path, server_addr, port, debug=False):\n    print(\'\\n\')\n    response = None\n    wav_name = os.path.basename(wav_path)\n    txt_name = os.path.basename(txt_path)\n    if os.path.splitext(wav_name)[0] != os.path.splitext(txt_name)[0]:\n        print(\' [!] wav name and transcript name does not match - exiting...\')\n        return response\n    with open(txt_path, \'r\', encoding=\'utf-8-sig\') as txt_file:\n        print(\'Transcript - \'+\'\'.join(txt_file.readlines()))\n    with open(wav_path,\'rb\') as wav_file, open(txt_path, \'rb\') as txt_file:\n        params = ((\'async\',\'false\'),)\n        files={\'audio\':(wav_name,wav_file),\n               \'transcript\':(txt_name,txt_file),\n               }\n        server_path = \'http://\'+server_addr+\':\'+str(port)+\'/transcriptions\'\n        response = requests.post(server_path, params=params,files=files)\n        if response.status_code != 200:\n            print(\' [!] External server({}) returned bad response({})\'.format(server_path, response.status_code))\n    if debug:\n        print(\'Response\')\n        print(response.json())\n    return response\n\nif __name__ == \'__main__\':\n    arguments = docopt(__doc__)    \n    server_addr = arguments[\'--server_addr\']\n    port = int(arguments[\'--port\'])\n    max_unalign  = float(arguments[\'--max_unalign\'])\n    if arguments[\'--nested-directories\'] is None:\n        wav_paths = sorted(glob(arguments[\'--wav_pattern\']))\n        txt_paths = sorted(glob(arguments[\'--txt_pattern\']))    \n    else:\n        # if this is multi-foldered environment\n        # (e.g. DATASET/speaker1/blahblah.wav)\n        wav_paths=[]\n        txt_paths=[]\n        topdir = arguments[\'--nested-directories\']\n        subdirs = [f for f in os.listdir(topdir) if os.path.isdir(os.path.join(topdir, f))]\n        for subdir in subdirs:\n            wav_pattern_subdir = os.path.join(topdir, subdir, \'*.wav\')\n            txt_pattern_subdir = os.path.join(topdir, subdir, \'*.txt\')\n            wav_paths.extend(sorted(glob(wav_pattern_subdir)))\n            txt_paths.extend(sorted(glob(txt_pattern_subdir)))\n        \n    t = tqdm(range(len(wav_paths)))\n    for idx in t:\n        try:\n            t.set_description(""Align via Gentle"")\n            wav_path = wav_paths[idx]\n            txt_path = txt_paths[idx]\n            lab_path = os.path.splitext(wav_path)[0]+\'.lab\'\n            if os.path.exists(lab_path) and arguments[\'--skip-already-done\']:\n                print(\'[!] skipping because of pre-existing .lab file - {}\'.format(lab_path))\n                continue\n            res=gentle_request(wav_path,txt_path, server_addr, port)\n            unalign_ratio, lab = json2hts(res.json())\n            print(\'[*] Unaligned Ratio - {}\'.format(unalign_ratio))\n            if unalign_ratio > max_unalign:\n                print(\'[!] skipping this due to bad alignment\')\n                continue\n            write_hts_label(lab, lab_path)\n        except:\n            # if sth happens, skip it\n            import traceback\n            tb = traceback.format_exc()\n            print(\'[!] ERROR while processing {}\'.format(wav_paths[idx]))\n            print(\'[!] StackTrace - \')\n            print(tb)\n\n    '"
hparams.py,1,"b'from deepvoice3_pytorch.tfcompat.hparam import HParams\n\n# NOTE: If you want full control for model architecture. please take a look\n# at the code and change whatever you want. Some hyper parameters are hardcoded.\n\n# Default hyperparameters:\nhparams = HParams(\n    name=""deepvoice3"",\n\n    # Text:\n    # [en, jp]\n    frontend=\'en\',\n\n    # Replace words to its pronunciation with fixed probability.\n    # e.g., \'hello\' to \'HH AH0 L OW1\'\n    # [en, jp]\n    # en: Word -> pronunciation using CMUDict\n    # jp: Word -> pronounciation usnig MeCab\n    # [0 ~ 1.0]: 0 means no replacement happens.\n    replace_pronunciation_prob=0.5,\n\n    # Convenient model builder\n    # [deepvoice3, deepvoice3_multispeaker, nyanko]\n    # Definitions can be found at deepvoice3_pytorch/builder.py\n    # deepvoice3: DeepVoice3 https://arxiv.org/abs/1710.07654\n    # deepvoice3_multispeaker: Multi-speaker version of DeepVoice3\n    # nyanko: https://arxiv.org/abs/1710.08969\n    builder=""deepvoice3"",\n\n    # Must be configured depends on the dataset and model you use\n    n_speakers=1,\n    speaker_embed_dim=16,\n\n    # Audio:\n    num_mels=80,\n    fmin=125,\n    fmax=7600,\n    fft_size=1024,\n    hop_size=256,\n    sample_rate=22050,\n    preemphasis=0.97,\n    min_level_db=-100,\n    ref_level_db=20,\n    # whether to rescale waveform or not.\n    # Let x is an input waveform, rescaled waveform y is given by:\n    # y = x / np.abs(x).max() * rescaling_max\n    rescaling=False,\n    rescaling_max=0.999,\n    # mel-spectrogram is normalized to [0, 1] for each utterance and clipping may\n    # happen depends on min_level_db and ref_level_db, causing clipping noise.\n    # If False, assertion is added to ensure no clipping happens.\n    allow_clipping_in_normalization=True,\n\n    # Model:\n    downsample_step=4,  # must be 4 when builder=""nyanko""\n    outputs_per_step=1,  # must be 1 when builder=""nyanko""\n    embedding_weight_std=0.1,\n    speaker_embedding_weight_std=0.01,\n    padding_idx=0,\n    # Maximum number of input text length\n    # try setting larger value if you want to give very long text input\n    max_positions=512,\n    dropout=1 - 0.95,\n    kernel_size=3,\n    text_embed_dim=128,\n    encoder_channels=256,\n    decoder_channels=256,\n    # Note: large converter channels requires significant computational cost\n    converter_channels=256,\n    query_position_rate=1.0,\n    # can be computed by `compute_timestamp_ratio.py`.\n    key_position_rate=1.385,  # 2.37 for jsut\n    key_projection=False,\n    value_projection=False,\n    use_memory_mask=True,\n    trainable_positional_encodings=False,\n    freeze_embedding=False,\n    # If True, use decoder\'s internal representation for postnet inputs,\n    # otherwise use mel-spectrogram.\n    use_decoder_state_for_postnet_input=True,\n\n    # Data loader\n    pin_memory=True,\n    num_workers=2,  # Set it to 1 when in Windows (MemoryError, THAllocator.c 0x5)\n\n    # Loss\n    masked_loss_weight=0.5,  # (1-w)*loss + w * masked_loss\n    priority_freq=3000,  # heuristic: priotrize [0 ~ priotiry_freq] for linear loss\n    priority_freq_weight=0.0,  # (1-w)*linear_loss + w*priority_linear_loss\n    # https://arxiv.org/pdf/1710.08969.pdf\n    # Adding the divergence to the loss stabilizes training, expecially for\n    # very deep (> 10 layers) networks.\n    # Binary div loss seems has approx 10x scale compared to L1 loss, so I choose 0.1.\n    binary_divergence_weight=0.1,  # set 0 to disable\n    use_guided_attention=True,\n    guided_attention_sigma=0.2,\n\n    # Training:\n    batch_size=16,\n    adam_beta1=0.5,\n    adam_beta2=0.9,\n    adam_eps=1e-6,\n    amsgrad=False,\n    initial_learning_rate=5e-4,  # 0.001,\n    lr_schedule=""noam_learning_rate_decay"",\n    lr_schedule_kwargs={},\n    nepochs=2000,\n    weight_decay=0.0,\n    clip_thresh=0.1,\n\n    # Save\n    checkpoint_interval=10000,\n    eval_interval=10000,\n    save_optimizer_state=True,\n\n    # Eval:\n    # this can be list for multple layers of attention\n    # e.g., [True, False, False, False, True]\n    force_monotonic_attention=True,\n    # Attention constraint for incremental decoding\n    window_ahead=3,\n    # 0 tends to prevent word repretetion, but sometime causes skip words\n    window_backward=1,\n    power=1.4,  # Power to raise magnitudes to prior to phase retrieval\n\n    # GC:\n    # Forced garbage collection probability\n    # Use only when MemoryError continues in Windows (Disabled by default)\n    #gc_probability = 0.001,\n\n    # json_meta mode only\n    # 0: ""use all"",\n    # 1: ""ignore only unmatched_alignment"",\n    # 2: ""fully ignore recognition"",\n    ignore_recognition_level=2,\n    # when dealing with non-dedicated speech dataset(e.g. movie excerpts), setting min_text above 15 is desirable. Can be adjusted by dataset.\n    min_text=20,\n    # if true, data without phoneme alignment file(.lab) will be ignored\n    process_only_htk_aligned=False,\n)\n\n\ndef hparams_debug_string():\n    values = hparams.values()\n    hp = [\'  %s: %s\' % (name, values[name]) for name in sorted(values)]\n    return \'Hyperparameters:\\n\' + \'\\n\'.join(hp)\n'"
json_meta.py,0,"b'\'\'\'\nStarted in 1945h, Mar 10, 2018\nFirst done in 2103h, Mar 11, 2018\nTest done in 2324h, Mar 11, 2018\nModified for HTK labeling in 1426h, Apr 21, 2018\nby engiecat(github)\n\nThis makes r9y9/deepvoice3_pytorch compatible with json format of carpedm20/multi-speaker-tacotron-tensorflow and keithito/tacotron.\nThe json file is given per speaker, generated in the format of \n\t(if completely aligned)\n\t\t(path-to-the-audio):aligned text\n\n\t(if partially aligned)\n\t\t(path-to-the-audio):[candidate sentence - not aligned,recognized words]\n\n\t(if non-aligned)\n\t\t(path-to-the-audio):[recognized words]\nis given per speaker.\n\n(e.g. python preprocess.py json_meta ""./datasets/LJSpeech_1_0/alignment.json,./datasets/GoTBookRev/alignment.json"" ""./datasets/LJ+GoTBookRev"" --preset=./presets/deepvoice3_vctk.json )\n\nusage: \n    python preprocess.py [option] <json_paths> <output_data_path>\n\n\noptions:\n    --preset     Path of preset parameters (json).\n    -h --help    show this help message and exit\n\n\n\'\'\'\n\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport audio\nfrom nnmnkwii.io import hts\nfrom hparams import hparams\nfrom os.path import exists\nimport librosa\nimport json\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n    \n    json_paths = in_dir.split(\',\')\n    json_paths = [json_path.replace(""\'"", """").replace(\'""\',"""") for json_path in json_paths]\n    num_speakers = len(json_paths)\n    is_aligned = {}\n    \n    speaker_id=0\n    for json_path in json_paths:\n        # Loads json metadata info\n        if json_path.endswith(""json""):\n            with open(json_path, encoding=\'utf8\') as f:\n                content = f.read()\n            info = json.loads(content)\n        elif json_path.endswith(""csv""):\n            with open(json_path) as f:\n                info = {}\n                for line in f:\n                    path, text = line.strip().split(\'|\')\n                    info[path] = text\n        else:\n            raise Exception("" [!] Unknown metadata format: {}"".format(json_path))\n\n        print("" [*] Loaded - {}"".format(json_path))\n        # check audio file existence\n        base_dir = os.path.dirname(json_path)\n        new_info = {}\n        for path in info.keys():\n            if not os.path.exists(path):\n                new_path = os.path.join(base_dir, path)\n                if not os.path.exists(new_path):\n                    print("" [!] Audio not found: {}"".format([path, new_path]))\n                    continue\n            else:\n                new_path = path\n            \n            new_info[new_path] = info[path]\n        \n        info = new_info\n        \n        # ignore_recognition_level check\n        for path in info.keys():\n            is_aligned[path] = True\n            if isinstance(info[path], list):\n                if hparams.ignore_recognition_level == 1 and len(info[path]) == 1 or \\\n                        hparams.ignore_recognition_level == 2:\n                    # flag the path to be \'non-aligned\' text\n                    is_aligned[path] = False\n                info[path] = info[path][0]\n        \n        # Reserve for future processing\n        queue_count = 0\n        for audio_path, text in info.items():\n            if isinstance(text, list):\n                if hparams.ignore_recognition_level == 0:\n                    text = text[-1]\n                else:\n                    text = text[0]\n            if hparams.ignore_recognition_level > 0 and not is_aligned[audio_path]:\n                continue\n            if hparams.min_text > len(text):\n                continue\n            if num_speakers == 1:\n                # Single-speaker\n                futures.append(executor.submit(\n                    partial(_process_utterance_single, out_dir, text, audio_path)))\n            else:\n                # Multi-speaker\n                futures.append(executor.submit(\n                    partial(_process_utterance, out_dir, text, audio_path, speaker_id)))\n            queue_count += 1\n        print("" [*] Appended {} entries in the queue"".format(queue_count))\n        \n        # increase speaker_id\n        speaker_id += 1\n    \n    # Show ignore_recognition_level description\n    ignore_description = {\n        0: ""use all"",\n        1: ""ignore only unmatched_alignment"",\n        2: ""fully ignore recognition"",\n    }\n    print("" [!] Skip recognition level: {} ({})"". \\\n            format(hparams.ignore_recognition_level,\n                   ignore_description[hparams.ignore_recognition_level]))\n    \n    if num_speakers == 1:\n        print("" [!] Single-speaker mode activated!"")\n    else:\n        print("" [!] Multi-speaker({}) mode activated!"".format(num_speakers))\n    \n    # Now, Do the job!\n    results = [future.result() for future in tqdm(futures)]\n    # Remove entries with None (That has been filtered due to bad htk alginment (if process_only_htk_aligned is enabled in hparams)\n    results = [result for result in results if result != None]\n    return results\n    \n\ndef start_at(labels):\n    has_silence = labels[0][-1] == ""pau""\n    if not has_silence:\n        return labels[0][0]\n    for i in range(1, len(labels)):\n        if labels[i][-1] != ""pau"":\n            return labels[i][0]\n    assert False\n\n\ndef end_at(labels):\n    has_silence = labels[-1][-1] == ""pau""\n    if not has_silence:\n        return labels[-1][1]\n    for i in range(len(labels) - 2, 0, -1):\n        if labels[i][-1] != ""pau"":\n            return labels[i][1]\n    assert False\n\n\ndef _process_utterance(out_dir, text, wav_path, speaker_id=None):\n\n    # check whether singlespeaker_mode\n    if speaker_id is None:\n        return _process_utterance_single(out_dir,text,wav_path)\n    # modified version of VCTK _process_utterance\n    sr = hparams.sample_rate\n\n    # Load the audio to a numpy array:\n    wav = audio.load_wav(wav_path)\n\n    lab_path = wav_path.replace(""wav48/"", ""lab/"").replace("".wav"", "".lab"")\n    if not exists(lab_path):\n        lab_path = os.path.splitext(wav_path)[0]+\'.lab\'\n\n    # Trim silence from hts labels if available\n    if exists(lab_path):\n        labels = hts.load(lab_path)\n        b = int(start_at(labels) * 1e-7 * sr)\n        e = int(end_at(labels) * 1e-7 * sr)\n        wav = wav[b:e]\n        wav, _ = librosa.effects.trim(wav, top_db=25)\n    else:\n        if hparams.process_only_htk_aligned:\n            return None\n        wav, _ = librosa.effects.trim(wav, top_db=15)\n\n    if hparams.rescaling:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    # Compute the linear-scale spectrogram from the wav:\n    spectrogram = audio.spectrogram(wav).astype(np.float32)\n    n_frames = spectrogram.shape[1]\n\n    # Compute a mel-scale spectrogram from the wav:\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n    # Write the spectrograms to disk: \n    # Get filename from wav_path\n    wav_name = os.path.basename(wav_path)\n    wav_name = os.path.splitext(wav_name)[0]\n    \n    # case if wave files across different speakers have the same naming format.\n    # e.g. Recording0.wav\n    spectrogram_filename = \'spec-{}-{}.npy\'.format(speaker_id, wav_name)\n    mel_filename = \'mel-{}-{}.npy\'.format(speaker_id, wav_name)\n    np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n    # Return a tuple describing this training example:\n    return (spectrogram_filename, mel_filename, n_frames, text, speaker_id)\n    \ndef _process_utterance_single(out_dir, text, wav_path):\n    # modified version of LJSpeech _process_utterance\n\n    # Load the audio to a numpy array:\n    wav = audio.load_wav(wav_path)\n    sr = hparams.sample_rate\n    # Added from the multispeaker version\n    lab_path = wav_path.replace(""wav48/"", ""lab/"").replace("".wav"", "".lab"")\n    if not exists(lab_path):\n        lab_path = os.path.splitext(wav_path)[0]+\'.lab\'\n\n    # Trim silence from hts labels if available\n    if exists(lab_path):\n        labels = hts.load(lab_path)\n        b = int(start_at(labels) * 1e-7 * sr)\n        e = int(end_at(labels) * 1e-7 * sr)\n        wav = wav[b:e]\n        wav, _ = librosa.effects.trim(wav, top_db=25)\n    else:\n        if hparams.process_only_htk_aligned:\n            return None\n        wav, _ = librosa.effects.trim(wav, top_db=15)\n    # End added from the multispeaker version\n    \n    if hparams.rescaling:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    # Compute the linear-scale spectrogram from the wav:\n    spectrogram = audio.spectrogram(wav).astype(np.float32)\n    n_frames = spectrogram.shape[1]\n\n    # Compute a mel-scale spectrogram from the wav:\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n    # Write the spectrograms to disk: \n    # Get filename from wav_path\n    wav_name = os.path.basename(wav_path)\n    wav_name = os.path.splitext(wav_name)[0]\n    spectrogram_filename = \'spec-{}.npy\'.format(wav_name)\n    mel_filename = \'mel-{}.npy\'.format(wav_name)\n    np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return (spectrogram_filename, mel_filename, n_frames, text)\n\n'"
jsut.py,0,"b'from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport audio\nfrom nnmnkwii.datasets import jsut\nfrom nnmnkwii.io import hts\nfrom hparams import hparams\nfrom os.path import exists\nimport librosa\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n\n    transcriptions = jsut.TranscriptionDataSource(\n        in_dir, subsets=jsut.available_subsets).collect_files()\n    wav_paths = jsut.WavFileDataSource(\n        in_dir, subsets=jsut.available_subsets).collect_files()\n\n    for index, (text, wav_path) in enumerate(zip(transcriptions, wav_paths)):\n        futures.append(executor.submit(\n            partial(_process_utterance, out_dir, index + 1, wav_path, text)))\n    return [future.result() for future in tqdm(futures)]\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n    sr = hparams.sample_rate\n\n    # Load the audio to a numpy array:\n    wav = audio.load_wav(wav_path)\n\n    lab_path = wav_path.replace(""wav/"", ""lab/"").replace("".wav"", "".lab"")\n\n    # Trim silence from hts labels if available\n    if exists(lab_path):\n        labels = hts.load(lab_path)\n        assert labels[0][-1] == ""silB""\n        assert labels[-1][-1] == ""silE""\n        b = int(labels[0][1] * 1e-7 * sr)\n        e = int(labels[-1][0] * 1e-7 * sr)\n        wav = wav[b:e]\n    else:\n        wav, _ = librosa.effects.trim(wav, top_db=30)\n\n    if hparams.rescaling:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    # Compute the linear-scale spectrogram from the wav:\n    spectrogram = audio.spectrogram(wav).astype(np.float32)\n    n_frames = spectrogram.shape[1]\n\n    # Compute a mel-scale spectrogram from the wav:\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n    # Write the spectrograms to disk:\n    spectrogram_filename = \'jsut-spec-%05d.npy\' % index\n    mel_filename = \'jsut-mel-%05d.npy\' % index\n    np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return (spectrogram_filename, mel_filename, n_frames, text)\n'"
ljspeech.py,0,"b""from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport audio\nfrom hparams import hparams\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n    '''Preprocesses the LJ Speech dataset from a given input path into a given output directory.\n\n      Args:\n        in_dir: The directory where you have downloaded the LJ Speech dataset\n        out_dir: The directory to write the output into\n        num_workers: Optional number of worker processes to parallelize across\n        tqdm: You can optionally pass tqdm to get a nice progress bar\n\n      Returns:\n        A list of tuples describing the training examples. This should be written to train.txt\n    '''\n\n    # We use ProcessPoolExecutor to parallize across processes. This is just an optimization and you\n    # can omit it and just call _process_utterance on each input if you want.\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n    index = 1\n    with open(os.path.join(in_dir, 'metadata.csv'), encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split('|')\n            wav_path = os.path.join(in_dir, 'wavs', '%s.wav' % parts[0])\n            text = parts[2]\n            if len(text) < hparams.min_text:\n                continue\n            futures.append(executor.submit(\n                partial(_process_utterance, out_dir, index, wav_path, text)))\n            index += 1\n    return [future.result() for future in tqdm(futures)]\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n    '''Preprocesses a single utterance audio/text pair.\n\n    This writes the mel and linear scale spectrograms to disk and returns a tuple to write\n    to the train.txt file.\n\n    Args:\n      out_dir: The directory to write the spectrograms into\n      index: The numeric index to use in the spectrogram filenames.\n      wav_path: Path to the audio file containing the speech input\n      text: The text spoken in the input audio file\n\n    Returns:\n      A (spectrogram_filename, mel_filename, n_frames, text) tuple to write to train.txt\n    '''\n\n    # Load the audio to a numpy array:\n    wav = audio.load_wav(wav_path)\n\n    if hparams.rescaling:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    # Compute the linear-scale spectrogram from the wav:\n    spectrogram = audio.spectrogram(wav).astype(np.float32)\n    n_frames = spectrogram.shape[1]\n\n    # Compute a mel-scale spectrogram from the wav:\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n    # Write the spectrograms to disk:\n    spectrogram_filename = 'ljspeech-spec-%05d.npy' % index\n    mel_filename = 'ljspeech-mel-%05d.npy' % index\n    np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return (spectrogram_filename, mel_filename, n_frames, text)\n"""
lrschedule.py,0,"b'import numpy as np\n\n\n# https://github.com/tensorflow/tensor2tensor/issues/280#issuecomment-339110329\ndef noam_learning_rate_decay(init_lr, global_step, warmup_steps=4000):\n     # Noam scheme from tensor2tensor:\n    warmup_steps = float(warmup_steps)\n    step = global_step + 1.\n    lr = init_lr * warmup_steps**0.5 * np.minimum(\n        step * warmup_steps**-1.5, step**-0.5)\n    return lr\n\n\ndef step_learning_rate_decay(init_lr, global_step,\n                             anneal_rate=0.98,\n                             anneal_interval=30000):\n    return init_lr * anneal_rate ** (global_step // anneal_interval)\n\n\ndef cyclic_cosine_annealing(init_lr, global_step, T, M):\n    """"""Cyclic cosine annealing\n\n    https://arxiv.org/pdf/1704.00109.pdf\n\n    Args:\n        init_lr (float): Initial learning rate\n        global_step (int): Current iteration number\n        T (int): Total iteration number (i,e. nepoch)\n        M (int): Number of ensembles we want\n\n    Returns:\n        float: Annealed learning rate\n    """"""\n    TdivM = T // M\n    return init_lr / 2.0 * (np.cos(np.pi * ((global_step - 1) % TdivM) / TdivM) + 1.0)\n'"
nikl_m.py,1,"b""from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport audio\nimport re\n\nfrom hparams import hparams\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n    '''Preprocesses the LJ Speech dataset from a given input path into a given output directory.\n\n      Args:\n        in_dir: The directory where you have downloaded the LJ Speech dataset\n        out_dir: The directory to write the output into\n        num_workers: Optional number of worker processes to parallelize across\n        tqdm: You can optionally pass tqdm to get a nice progress bar\n\n      Returns:\n        A list of tuples describing the training examples. This should be written to train.txt\n    '''\n\n    # We use ProcessPoolExecutor to parallize across processes. This is just an optimization and you\n    # can omit it and just call _process_utterance on each input if you want.\n\n    # You will need to modify and format NIKL transcrption file will UTF-8 format\n    # please check https://github.com/homink/deepspeech.pytorch.ko/blob/master/data/local/clean_corpus.sh\n\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n\n    spk_id = {}\n    with open(in_dir + '/speaker.mid', encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            spk_id[line.rstrip()] = i\n\n    index = 1\n    with open(in_dir + '/metadata.txt', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split('|')\n            wav_path = parts[0]\n            text = parts[1]\n            uid = re.search(r'([a-z][a-z][0-9][0-9]_t)', wav_path)\n            uid = uid.group(1).replace('_t', '')\n            futures.append(executor.submit(\n                partial(_process_utterance, out_dir, index + 1, spk_id[uid], wav_path, text)))\n            index += 1\n    return [future.result() for future in tqdm(futures)]\n\n\ndef _process_utterance(out_dir, index, speaker_id, wav_path, text):\n    '''Preprocesses a single utterance audio/text pair.\n\n    This writes the mel and linear scale spectrograms to disk and returns a tuple to write\n    to the train.txt file.\n\n    Args:\n      out_dir: The directory to write the spectrograms into\n      index: The numeric index to use in the spectrogram filenames.\n      wav_path: Path to the audio file containing the speech input\n      text: The text spoken in the input audio file\n\n    Returns:\n      A (spectrogram_filename, mel_filename, n_frames, text) tuple to write to train.txt\n    '''\n\n    # Load the audio to a numpy array:\n    wav = audio.load_wav(wav_path)\n\n    if hparams.rescaling:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    # Compute the linear-scale spectrogram from the wav:\n    spectrogram = audio.spectrogram(wav).astype(np.float32)\n    n_frames = spectrogram.shape[1]\n\n    # Compute a mel-scale spectrogram from the wav:\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n    # Write the spectrograms to disk:\n    spectrogram_filename = 'nikl-multi-spec-%05d.npy' % index\n    mel_filename = 'nikl-multi-mel-%05d.npy' % index\n    np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return (spectrogram_filename, mel_filename, n_frames, text, speaker_id)\n"""
nikl_s.py,1,"b""from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport audio\nimport re\n\nfrom hparams import hparams\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n    '''Preprocesses the LJ Speech dataset from a given input path into a given output directory.\n\n      Args:\n        in_dir: The directory where you have downloaded the LJ Speech dataset\n        out_dir: The directory to write the output into\n        num_workers: Optional number of worker processes to parallelize across\n        tqdm: You can optionally pass tqdm to get a nice progress bar\n\n      Returns:\n        A list of tuples describing the training examples. This should be written to train.txt\n    '''\n\n    # We use ProcessPoolExecutor to parallize across processes. This is just an optimization and you\n    # can omit it and just call _process_utterance on each input if you want.\n\n    # You will need to modify and format NIKL transcrption file will UTF-8 format\n    # please check https://github.com/homink/deepspeech.pytorch.ko/blob/master/data/local/clean_corpus.sh\n\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n\n    with open(in_dir + '/speaker.sid', encoding='utf-8') as f:\n        spk_id = f.readline().rstrip()\n\n    index = 1\n    with open(in_dir + '/metadata.txt', encoding='utf-8') as f:\n        for line in f:\n            if spk_id in line:\n                parts = line.strip().split('|')\n                wav_path = parts[0]\n                text = parts[1]\n                futures.append(executor.submit(\n                    partial(_process_utterance, out_dir, index + 1, wav_path, text)))\n            index += 1\n    return [future.result() for future in tqdm(futures)]\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n    '''Preprocesses a single utterance audio/text pair.\n\n    This writes the mel and linear scale spectrograms to disk and returns a tuple to write\n    to the train.txt file.\n\n    Args:\n      out_dir: The directory to write the spectrograms into\n      index: The numeric index to use in the spectrogram filenames.\n      wav_path: Path to the audio file containing the speech input\n      text: The text spoken in the input audio file\n\n    Returns:\n      A (spectrogram_filename, mel_filename, n_frames, text) tuple to write to train.txt\n    '''\n\n    # Load the audio to a numpy array:\n    wav = audio.load_wav(wav_path)\n\n    if hparams.rescaling:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    # Compute the linear-scale spectrogram from the wav:\n    spectrogram = audio.spectrogram(wav).astype(np.float32)\n    n_frames = spectrogram.shape[1]\n\n    # Compute a mel-scale spectrogram from the wav:\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n    # Write the spectrograms to disk:\n    spectrogram_filename = 'nikl-single-spec-%05d.npy' % index\n    mel_filename = 'nikl-single-mel-%05d.npy' % index\n    np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return (spectrogram_filename, mel_filename, n_frames, text)\n"""
preprocess.py,0,"b'# coding: utf-8\n""""""\nPreprocess dataset\n\nusage: preprocess.py [options] <name> <in_dir> <out_dir>\n\noptions:\n    --num_workers=<n>        Num workers.\n    --hparams=<parmas>       Hyper parameters [default: ].\n    --preset=<json>          Path of preset parameters (json).\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\nimport os\nfrom multiprocessing import cpu_count\nfrom tqdm import tqdm\nimport importlib\nfrom hparams import hparams, hparams_debug_string\n\n\ndef preprocess(mod, in_dir, out_root, num_workers):\n    os.makedirs(out_dir, exist_ok=True)\n    metadata = mod.build_from_path(in_dir, out_dir, num_workers, tqdm=tqdm)\n    write_metadata(metadata, out_dir)\n\n\ndef write_metadata(metadata, out_dir):\n    with open(os.path.join(out_dir, \'train.txt\'), \'w\', encoding=\'utf-8\') as f:\n        for m in metadata:\n            f.write(\'|\'.join([str(x) for x in m]) + \'\\n\')\n    frames = sum([m[2] for m in metadata])\n    frame_shift_ms = hparams.hop_size / hparams.sample_rate * 1000\n    hours = frames * frame_shift_ms / (3600 * 1000)\n    print(\'Wrote %d utterances, %d frames (%.2f hours)\' % (len(metadata), frames, hours))\n    print(\'Max input length:  %d\' % max(len(m[3]) for m in metadata))\n    print(\'Max output length: %d\' % max(m[2] for m in metadata))\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    name = args[""<name>""]\n    in_dir = args[""<in_dir>""]\n    out_dir = args[""<out_dir>""]\n    num_workers = args[""--num_workers""]\n    num_workers = cpu_count() if num_workers is None else int(num_workers)\n    preset = args[""--preset""]\n\n    # Load preset if specified\n    if preset is not None:\n        with open(preset) as f:\n            hparams.parse_json(f.read())\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n    assert hparams.name == ""deepvoice3""\n    print(hparams_debug_string())\n\n    assert name in [""jsut"", ""ljspeech"", ""vctk"", ""nikl_m"", ""nikl_s"", ""json_meta""]\n    mod = importlib.import_module(name)\n    preprocess(mod, in_dir, out_dir, num_workers)\n'"
setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\nimport setuptools.command.develop\nimport setuptools.command.build_py\nimport os\nimport subprocess\nfrom os.path import exists\n\nversion = \'0.1.1\'\n\n# Adapted from https://github.com/pytorch/pytorch\ncwd = os.path.dirname(os.path.abspath(__file__))\nif os.getenv(\'DEEPVOICE3_PYTORCH_BUILD_VERSION\'):\n    version = os.getenv(\'DEEPVOICE3_PYTORCH_BUILD_VERSION\')\nelse:\n    try:\n        sha = subprocess.check_output(\n            [\'git\', \'rev-parse\', \'HEAD\'], cwd=cwd).decode(\'ascii\').strip()\n        version += \'+\' + sha[:7]\n    except subprocess.CalledProcessError:\n        pass\n    except IOError:  # FileNotFoundError for python 3\n        pass\n\n\nclass build_py(setuptools.command.build_py.build_py):\n\n    def run(self):\n        self.create_version_file()\n        setuptools.command.build_py.build_py.run(self)\n\n    @staticmethod\n    def create_version_file():\n        global version, cwd\n        print(\'-- Building version \' + version)\n        version_path = os.path.join(cwd, \'deepvoice3_pytorch\', \'version.py\')\n        with open(version_path, \'w\') as f:\n            f.write(""__version__ = \'{}\'\\n"".format(version))\n\n\nclass develop(setuptools.command.develop.develop):\n\n    def run(self):\n        build_py.create_version_file()\n        setuptools.command.develop.develop.run(self)\n\n\ndef create_readme_rst():\n    global cwd\n    try:\n        subprocess.check_call(\n            [""pandoc"", ""--from=markdown"", ""--to=rst"", ""--output=README.rst"",\n             ""README.md""], cwd=cwd)\n        print(""Generated README.rst from README.md using pandoc."")\n    except subprocess.CalledProcessError:\n        pass\n    except OSError:\n        pass\n\n\nif not exists(\'README.rst\'):\n    create_readme_rst()\n\nif exists(\'README.rst\'):\n    README = open(\'README.rst\', \'rb\').read().decode(""utf-8"")\nelse:\n    README = \'\'\n\nsetup(name=\'deepvoice3_pytorch\',\n      version=version,\n      description=\'PyTorch implementation of convolutional networks-based text-to-speech synthesis models.\',\n      long_description=README,\n      packages=find_packages(),\n      cmdclass={\n          \'build_py\': build_py,\n          \'develop\': develop,\n      },\n      install_requires=[\n          ""numpy"",\n          ""scipy"",\n          ""torch >= 1.0.0"",\n          ""unidecode"",\n          ""inflect"",\n          ""librosa"",\n          ""numba"",\n          ""lws"",\n          ""nltk"",\n      ],\n      extras_require={\n          ""bin"": [\n              ""docopt"",\n              ""tqdm"",\n              ""tensorboardX <= 1.2"",\n              ""nnmnkwii >= 0.0.19"",\n              ""requests"",\n              ""matplotlib"",\n          ],\n          ""test"": [\n              ""nose"",\n          ],\n          ""jp"": [\n              ""jaconv"",\n              ""mecab-python3"",\n          ],\n      })\n'"
synthesis.py,8,"b'# coding: utf-8\n""""""\nSynthesis waveform from trained model.\n\nusage: synthesis.py [options] <checkpoint> <text_list_file> <dst_dir>\n\noptions:\n    --hparams=<parmas>                Hyper parameters [default: ].\n    --preset=<json>                   Path of preset parameters (json).\n    --checkpoint-seq2seq=<path>       Load seq2seq model from checkpoint path.\n    --checkpoint-postnet=<path>       Load postnet model from checkpoint path.\n    --file-name-suffix=<s>            File name suffix [default: ].\n    --max-decoder-steps=<N>           Max decoder steps [default: 500].\n    --replace_pronunciation_prob=<N>  Prob [default: 0.0].\n    --speaker_id=<id>                 Speaker ID (for multi-speaker model).\n    --output-html                     Output html for blog post.\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\n\nimport sys\nimport os\nfrom os.path import dirname, join, basename, splitext\n\nimport audio\n\nimport torch\nimport numpy as np\nimport nltk\n\n# The deepvoice3 model\nfrom deepvoice3_pytorch import frontend\nfrom hparams import hparams, hparams_debug_string\n\nfrom tqdm import tqdm\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n_frontend = None  # to be set later\n\n\ndef tts(model, text, p=0, speaker_id=None, fast=False):\n    """"""Convert text to speech waveform given a deepvoice3 model.\n\n    Args:\n        text (str) : Input text to be synthesized\n        p (float) : Replace word to pronounciation if p > 0. Default is 0.\n    """"""\n    model = model.to(device)\n    model.eval()\n    if fast:\n        model.make_generation_fast_()\n\n    sequence = np.array(_frontend.text_to_sequence(text, p=p))\n    sequence = torch.from_numpy(sequence).unsqueeze(0).long().to(device)\n    text_positions = torch.arange(1, sequence.size(-1) + 1).unsqueeze(0).long().to(device)\n    speaker_ids = None if speaker_id is None else torch.LongTensor([speaker_id]).to(device)\n\n    # Greedy decoding\n    with torch.no_grad():\n        mel_outputs, linear_outputs, alignments, done = model(\n            sequence, text_positions=text_positions, speaker_ids=speaker_ids)\n\n    linear_output = linear_outputs[0].cpu().data.numpy()\n    spectrogram = audio._denormalize(linear_output)\n    alignment = alignments[0].cpu().data.numpy()\n    mel = mel_outputs[0].cpu().data.numpy()\n    mel = audio._denormalize(mel)\n\n    # Predicted audio signal\n    waveform = audio.inv_spectrogram(linear_output.T)\n\n    return waveform, alignment, spectrogram, mel\n\n\ndef _load(checkpoint_path):\n    if use_cuda:\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path,\n                                map_location=lambda storage, loc: storage)\n    return checkpoint\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    print(""Command line args:\\n"", args)\n    checkpoint_path = args[""<checkpoint>""]\n    text_list_file_path = args[""<text_list_file>""]\n    dst_dir = args[""<dst_dir>""]\n    checkpoint_seq2seq_path = args[""--checkpoint-seq2seq""]\n    checkpoint_postnet_path = args[""--checkpoint-postnet""]\n    max_decoder_steps = int(args[""--max-decoder-steps""])\n    file_name_suffix = args[""--file-name-suffix""]\n    replace_pronunciation_prob = float(args[""--replace_pronunciation_prob""])\n    output_html = args[""--output-html""]\n    speaker_id = args[""--speaker_id""]\n    if speaker_id is not None:\n        speaker_id = int(speaker_id)\n    preset = args[""--preset""]\n\n    # Load preset if specified\n    if preset is not None:\n        with open(preset) as f:\n            hparams.parse_json(f.read())\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n    assert hparams.name == ""deepvoice3""\n\n    _frontend = getattr(frontend, hparams.frontend)\n    import train\n    train._frontend = _frontend\n    from train import plot_alignment, build_model\n\n    # Model\n    model = build_model()\n\n    # Load checkpoints separately\n    if checkpoint_postnet_path is not None and checkpoint_seq2seq_path is not None:\n        checkpoint = _load(checkpoint_seq2seq_path)\n        model.seq2seq.load_state_dict(checkpoint[""state_dict""])\n        checkpoint = _load(checkpoint_postnet_path)\n        model.postnet.load_state_dict(checkpoint[""state_dict""])\n        checkpoint_name = splitext(basename(checkpoint_seq2seq_path))[0]\n    else:\n        checkpoint = _load(checkpoint_path)\n        model.load_state_dict(checkpoint[""state_dict""])\n        checkpoint_name = splitext(basename(checkpoint_path))[0]\n\n    model.seq2seq.decoder.max_decoder_steps = max_decoder_steps\n\n    os.makedirs(dst_dir, exist_ok=True)\n    with open(text_list_file_path, ""rb"") as f:\n        lines = f.readlines()\n        for idx, line in enumerate(lines):\n            text = line.decode(""utf-8"")[:-1]\n            words = nltk.word_tokenize(text)\n            waveform, alignment, _, _ = tts(\n                model, text, p=replace_pronunciation_prob, speaker_id=speaker_id, fast=True)\n            dst_wav_path = join(dst_dir, ""{}_{}{}.wav"".format(\n                idx, checkpoint_name, file_name_suffix))\n            dst_alignment_path = join(\n                dst_dir, ""{}_{}{}_alignment.png"".format(idx, checkpoint_name,\n                                                        file_name_suffix))\n            plot_alignment(alignment.T, dst_alignment_path,\n                           info=""{}, {}"".format(hparams.builder, basename(checkpoint_path)))\n            audio.save_wav(waveform, dst_wav_path)\n            name = splitext(basename(text_list_file_path))[0]\n            if output_html:\n                print(""""""\n{}\n\n({} chars, {} words)\n\n<audio controls=""controls"" >\n<source src=""/audio/{}/{}/{}"" autoplay/>\nYour browser does not support the audio element.\n</audio>\n\n<div align=""center""><img src=""/audio/{}/{}/{}"" /></div>\n                  """""".format(text, len(text), len(words),\n                             hparams.builder, name, basename(dst_wav_path),\n                             hparams.builder, name, basename(dst_alignment_path)))\n            else:\n                print(idx, "": {}\\n ({} chars, {} words)"".format(text, len(text), len(words)))\n\n    print(""Finished! Check out {} for generated audio samples."".format(dst_dir))\n    sys.exit(0)\n'"
train.py,24,"b'""""""Trainining script for seq2seq text-to-speech synthesis model.\n\nusage: train.py [options]\n\noptions:\n    --data-root=<dir>            Directory contains preprocessed features.\n    --checkpoint-dir=<dir>       Directory where to save model checkpoints [default: checkpoints].\n    --hparams=<parmas>           Hyper parameters [default: ].\n    --preset=<json>              Path of preset parameters (json).\n    --checkpoint=<path>          Restore model from checkpoint path if given.\n    --checkpoint-seq2seq=<path>  Restore seq2seq model from checkpoint path.\n    --checkpoint-postnet=<path>  Restore postnet model from checkpoint path.\n    --train-seq2seq-only         Train only seq2seq model.\n    --train-postnet-only         Train only postnet model.\n    --restore-parts=<path>       Restore part of the model.\n    --log-event-path=<name>      Log event path.\n    --reset-optimizer            Reset optimizer.\n    --load-embedding=<path>      Load embedding from checkpoint.\n    --speaker-id=<N>             Use specific speaker of data in case for multi-speaker datasets.\n    -h, --help                   Show this help message and exit\n""""""\nfrom docopt import docopt\n\nimport sys\nimport gc\nimport platform\nfrom os.path import dirname, join\nfrom tqdm import tqdm, trange\nfrom datetime import datetime\n\n# The deepvoice3 model\nfrom deepvoice3_pytorch import frontend, builder\nimport audio\nimport lrschedule\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils import data as data_utils\nfrom torch.utils.data.sampler import Sampler\nimport numpy as np\nfrom numba import jit\n\nfrom nnmnkwii.datasets import FileSourceDataset, FileDataSource\nfrom os.path import join, expanduser\nimport random\n\nimport librosa.display\nfrom matplotlib import pyplot as plt\nimport sys\nimport os\nfrom tensorboardX import SummaryWriter\nfrom matplotlib import cm\nfrom warnings import warn\nfrom hparams import hparams, hparams_debug_string\n\nglobal_step = 0\nglobal_epoch = 0\nuse_cuda = torch.cuda.is_available()\nif use_cuda:\n    cudnn.benchmark = False\n\n_frontend = None  # to be set later\n\n\ndef _pad(seq, max_len, constant_values=0):\n    return np.pad(seq, (0, max_len - len(seq)),\n                  mode=\'constant\', constant_values=constant_values)\n\n\ndef _pad_2d(x, max_len, b_pad=0):\n    x = np.pad(x, [(b_pad, max_len - len(x) - b_pad), (0, 0)],\n               mode=""constant"", constant_values=0)\n    return x\n\n\ndef plot_alignment(alignment, path, info=None):\n    fig, ax = plt.subplots()\n    im = ax.imshow(\n        alignment,\n        aspect=\'auto\',\n        origin=\'lower\',\n        interpolation=\'none\')\n    fig.colorbar(im, ax=ax)\n    xlabel = \'Decoder timestep\'\n    if info is not None:\n        xlabel += \'\\n\\n\' + info\n    plt.xlabel(xlabel)\n    plt.ylabel(\'Encoder timestep\')\n    plt.tight_layout()\n    plt.savefig(path, format=\'png\')\n    plt.close()\n\n\nclass TextDataSource(FileDataSource):\n    def __init__(self, data_root, speaker_id=None):\n        self.data_root = data_root\n        self.speaker_ids = None\n        self.multi_speaker = False\n        # If not None, filter by speaker_id\n        self.speaker_id = speaker_id\n\n    def collect_files(self):\n        meta = join(self.data_root, ""train.txt"")\n        with open(meta, ""rb"") as f:\n            lines = f.readlines()\n        l = lines[0].decode(""utf-8"").split(""|"")\n        assert len(l) == 4 or len(l) == 5\n        self.multi_speaker = len(l) == 5\n        texts = list(map(lambda l: l.decode(""utf-8"").split(""|"")[3], lines))\n        if self.multi_speaker:\n            speaker_ids = list(map(lambda l: int(l.decode(""utf-8"").split(""|"")[-1]), lines))\n            # Filter by speaker_id\n            # using multi-speaker dataset as a single speaker dataset\n            if self.speaker_id is not None:\n                indices = np.array(speaker_ids) == self.speaker_id\n                texts = list(np.array(texts)[indices])\n                self.multi_speaker = False\n                return texts\n\n            return texts, speaker_ids\n        else:\n            return texts\n\n    def collect_features(self, *args):\n        if self.multi_speaker:\n            text, speaker_id = args\n        else:\n            text = args[0]\n        global _frontend\n        if _frontend is None:\n            _frontend = getattr(frontend, hparams.frontend)\n        seq = _frontend.text_to_sequence(text, p=hparams.replace_pronunciation_prob)\n\n        if platform.system() == ""Windows"":\n            if hasattr(hparams, \'gc_probability\'):\n                _frontend = None  # memory leaking prevention in Windows\n                if np.random.rand() < hparams.gc_probability:\n                    gc.collect()  # garbage collection enforced\n                    print(""GC done"")\n\n        if self.multi_speaker:\n            return np.asarray(seq, dtype=np.int32), int(speaker_id)\n        else:\n            return np.asarray(seq, dtype=np.int32)\n\n\nclass _NPYDataSource(FileDataSource):\n    def __init__(self, data_root, col, speaker_id=None):\n        self.data_root = data_root\n        self.col = col\n        self.frame_lengths = []\n        self.speaker_id = speaker_id\n\n    def collect_files(self):\n        meta = join(self.data_root, ""train.txt"")\n        with open(meta, ""rb"") as f:\n            lines = f.readlines()\n        l = lines[0].decode(""utf-8"").split(""|"")\n        assert len(l) == 4 or len(l) == 5\n        multi_speaker = len(l) == 5\n        self.frame_lengths = list(\n            map(lambda l: int(l.decode(""utf-8"").split(""|"")[2]), lines))\n\n        paths = list(map(lambda l: l.decode(""utf-8"").split(""|"")[self.col], lines))\n        paths = list(map(lambda f: join(self.data_root, f), paths))\n\n        if multi_speaker and self.speaker_id is not None:\n            speaker_ids = list(map(lambda l: int(l.decode(""utf-8"").split(""|"")[-1]), lines))\n            # Filter by speaker_id\n            # using multi-speaker dataset as a single speaker dataset\n            indices = np.array(speaker_ids) == self.speaker_id\n            paths = list(np.array(paths)[indices])\n            self.frame_lengths = list(np.array(self.frame_lengths)[indices])\n            # aha, need to cast numpy.int64 to int\n            self.frame_lengths = list(map(int, self.frame_lengths))\n\n        return paths\n\n    def collect_features(self, path):\n        return np.load(path)\n\n\nclass MelSpecDataSource(_NPYDataSource):\n    def __init__(self, data_root, speaker_id=None):\n        super(MelSpecDataSource, self).__init__(data_root, 1, speaker_id)\n\n\nclass LinearSpecDataSource(_NPYDataSource):\n    def __init__(self, data_root, speaker_id=None):\n        super(LinearSpecDataSource, self).__init__(data_root, 0, speaker_id)\n\n\nclass PartialyRandomizedSimilarTimeLengthSampler(Sampler):\n    """"""Partially randmoized sampler\n\n    1. Sort by lengths\n    2. Pick a small patch and randomize it\n    3. Permutate mini-batchs\n    """"""\n\n    def __init__(self, lengths, batch_size=16, batch_group_size=None,\n                 permutate=True):\n        self.lengths, self.sorted_indices = torch.sort(torch.LongTensor(lengths))\n        self.batch_size = batch_size\n        if batch_group_size is None:\n            batch_group_size = min(batch_size * 32, len(self.lengths))\n            if batch_group_size % batch_size != 0:\n                batch_group_size -= batch_group_size % batch_size\n\n        self.batch_group_size = batch_group_size\n        assert batch_group_size % batch_size == 0\n        self.permutate = permutate\n\n    def __iter__(self):\n        indices = self.sorted_indices.clone()\n        batch_group_size = self.batch_group_size\n        s, e = 0, 0\n        for i in range(len(indices) // batch_group_size):\n            s = i * batch_group_size\n            e = s + batch_group_size\n            random.shuffle(indices[s:e])\n\n        # Permutate batches\n        if self.permutate:\n            perm = np.arange(len(indices[:e]) // self.batch_size)\n            random.shuffle(perm)\n            indices[:e] = indices[:e].view(-1, self.batch_size)[perm, :].view(-1)\n\n        # Handle last elements\n        s += batch_group_size\n        if s < len(indices):\n            random.shuffle(indices[s:])\n\n        return iter(indices)\n\n    def __len__(self):\n        return len(self.sorted_indices)\n\n\nclass PyTorchDataset(object):\n    def __init__(self, X, Mel, Y):\n        self.X = X\n        self.Mel = Mel\n        self.Y = Y\n        # alias\n        self.multi_speaker = X.file_data_source.multi_speaker\n\n    def __getitem__(self, idx):\n        if self.multi_speaker:\n            text, speaker_id = self.X[idx]\n            return text, self.Mel[idx], self.Y[idx], speaker_id\n        else:\n            return self.X[idx], self.Mel[idx], self.Y[idx]\n\n    def __len__(self):\n        return len(self.X)\n\n\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.data.max()\n    batch_size = sequence_length.size(0)\n    seq_range = torch.arange(0, max_len).long()\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    if sequence_length.is_cuda:\n        seq_range_expand = seq_range_expand.cuda()\n    seq_length_expand = sequence_length.unsqueeze(1) \\\n        .expand_as(seq_range_expand)\n    return (seq_range_expand < seq_length_expand).float()\n\n\nclass MaskedL1Loss(nn.Module):\n    def __init__(self):\n        super(MaskedL1Loss, self).__init__()\n        self.criterion = nn.L1Loss(reduction=""sum"")\n\n    def forward(self, input, target, lengths=None, mask=None, max_len=None):\n        if lengths is None and mask is None:\n            raise RuntimeError(""Should provide either lengths or mask"")\n\n        # (B, T, 1)\n        if mask is None:\n            mask = sequence_mask(lengths, max_len).unsqueeze(-1)\n\n        # (B, T, D)\n        mask_ = mask.expand_as(input)\n        loss = self.criterion(input * mask_, target * mask_)\n        return loss / mask_.sum()\n\n\ndef collate_fn(batch):\n    """"""Create batch""""""\n    r = hparams.outputs_per_step\n    downsample_step = hparams.downsample_step\n    multi_speaker = len(batch[0]) == 4\n\n    # Lengths\n    input_lengths = [len(x[0]) for x in batch]\n    max_input_len = max(input_lengths)\n\n    target_lengths = [len(x[1]) for x in batch]\n\n    max_target_len = max(target_lengths)\n    if max_target_len % r != 0:\n        max_target_len += r - max_target_len % r\n        assert max_target_len % r == 0\n    if max_target_len % downsample_step != 0:\n        max_target_len += downsample_step - max_target_len % downsample_step\n        assert max_target_len % downsample_step == 0\n\n    # Set 0 for zero beginning padding\n    # imitates initial decoder states\n    b_pad = r\n    max_target_len += b_pad * downsample_step\n\n    a = np.array([_pad(x[0], max_input_len) for x in batch], dtype=np.int)\n    x_batch = torch.LongTensor(a)\n\n    input_lengths = torch.LongTensor(input_lengths)\n    target_lengths = torch.LongTensor(target_lengths)\n\n    b = np.array([_pad_2d(x[1], max_target_len, b_pad=b_pad) for x in batch],\n                 dtype=np.float32)\n    mel_batch = torch.FloatTensor(b)\n\n    c = np.array([_pad_2d(x[2], max_target_len, b_pad=b_pad) for x in batch],\n                 dtype=np.float32)\n    y_batch = torch.FloatTensor(c)\n\n    # text positions\n    text_positions = np.array([_pad(np.arange(1, len(x[0]) + 1), max_input_len)\n                               for x in batch], dtype=np.int)\n    text_positions = torch.LongTensor(text_positions)\n\n    max_decoder_target_len = max_target_len // r // downsample_step\n\n    # frame positions\n    s, e = 1, max_decoder_target_len + 1\n    # if b_pad > 0:\n    #    s, e = s - 1, e - 1\n    # NOTE: needs clone to supress RuntimeError in dataloarder...\n    # ref: https://github.com/pytorch/pytorch/issues/10756\n    frame_positions = torch.arange(s, e).long().unsqueeze(0).expand(\n        len(batch), max_decoder_target_len).clone()\n\n    # done flags\n    done = np.array([_pad(np.zeros(len(x[1]) // r // downsample_step - 1),\n                          max_decoder_target_len, constant_values=1)\n                     for x in batch])\n    done = torch.FloatTensor(done).unsqueeze(-1)\n\n    if multi_speaker:\n        speaker_ids = torch.LongTensor([x[3] for x in batch])\n    else:\n        speaker_ids = None\n\n    return x_batch, input_lengths, mel_batch, y_batch, \\\n        (text_positions, frame_positions), done, target_lengths, speaker_ids\n\n\ndef time_string():\n    return datetime.now().strftime(\'%Y-%m-%d %H:%M\')\n\n\ndef save_alignment(path, attn):\n    plot_alignment(attn.T, path, info=""{}, {}, step={}"".format(\n        hparams.builder, time_string(), global_step))\n\n\ndef prepare_spec_image(spectrogram):\n    # [0, 1]\n    spectrogram = (spectrogram - np.min(spectrogram)) / (np.max(spectrogram) - np.min(spectrogram))\n    spectrogram = np.flip(spectrogram, axis=1)  # flip against freq axis\n    return np.uint8(cm.magma(spectrogram.T) * 255)\n\n\ndef eval_model(global_step, writer, device, model, checkpoint_dir, ismultispeaker):\n    # harded coded\n    texts = [\n        ""Scientists at the CERN laboratory say they have discovered a new particle."",\n        ""There\'s a way to measure the acute emotional intelligence that has never gone out of style."",\n        ""President Trump met with other leaders at the Group of 20 conference."",\n        ""Generative adversarial network or variational auto-encoder."",\n        ""Please call Stella."",\n        ""Some have accepted this as a miracle without any physical explanation."",\n    ]\n    import synthesis\n    synthesis._frontend = _frontend\n\n    eval_output_dir = join(checkpoint_dir, ""eval"")\n    os.makedirs(eval_output_dir, exist_ok=True)\n\n    # Prepare model for evaluation\n    model_eval = build_model().to(device)\n    model_eval.load_state_dict(model.state_dict())\n\n    # hard coded\n    speaker_ids = [0, 1, 10] if ismultispeaker else [None]\n    for speaker_id in speaker_ids:\n        speaker_str = ""multispeaker{}"".format(speaker_id) if speaker_id is not None else ""single""\n\n        for idx, text in enumerate(texts):\n            signal, alignment, _, mel = synthesis.tts(\n                model_eval, text, p=0, speaker_id=speaker_id, fast=True)\n            signal /= np.max(np.abs(signal))\n\n            # Alignment\n            path = join(eval_output_dir, ""step{:09d}_text{}_{}_alignment.png"".format(\n                global_step, idx, speaker_str))\n            save_alignment(path, alignment)\n            tag = ""eval_averaged_alignment_{}_{}"".format(idx, speaker_str)\n            try:\n                writer.add_image(tag, np.uint8(cm.viridis(np.flip(alignment, 1).T) * 255), global_step)\n            except Exception as e:\n                warn(str(e))\n\n            # Mel\n            try:\n                writer.add_image(""(Eval) Predicted mel spectrogram text{}_{}"".format(idx, speaker_str),\n                                 prepare_spec_image(mel), global_step)\n            except Exception as e:\n                warn(str(e))\n\n            # Audio\n            path = join(eval_output_dir, ""step{:09d}_text{}_{}_predicted.wav"".format(\n                global_step, idx, speaker_str))\n            audio.save_wav(signal, path)\n\n            try:\n                writer.add_audio(""(Eval) Predicted audio signal {}_{}"".format(idx, speaker_str),\n                                 signal, global_step, sample_rate=hparams.sample_rate)\n            except Exception as e:\n                warn(str(e))\n                pass\n\n\ndef save_states(global_step, writer, mel_outputs, linear_outputs, attn, mel, y,\n                input_lengths, checkpoint_dir=None):\n    print(""Save intermediate states at step {}"".format(global_step))\n\n    # idx = np.random.randint(0, len(input_lengths))\n    idx = min(1, len(input_lengths) - 1)\n    input_length = input_lengths[idx]\n\n    # Alignment\n    # Multi-hop attention\n    if attn is not None and attn.dim() == 4:\n        for i, alignment in enumerate(attn):\n            alignment = alignment[idx].cpu().data.numpy()\n            tag = ""alignment_layer{}"".format(i + 1)\n            try:\n                writer.add_image(tag, np.uint8(cm.viridis(\n                    np.flip(alignment, 1).T) * 255), global_step)\n                # save files as well for now\n                alignment_dir = join(\n                    checkpoint_dir, ""alignment_layer{}"".format(i + 1))\n                os.makedirs(alignment_dir, exist_ok=True)\n                path = join(alignment_dir, ""step{:09d}_layer_{}_alignment.png"".format(\n                    global_step, i + 1))\n                save_alignment(path, alignment)\n            except Exception as e:\n                warn(str(e))\n\n        # Save averaged alignment\n        alignment_dir = join(checkpoint_dir, ""alignment_ave"")\n        os.makedirs(alignment_dir, exist_ok=True)\n        path = join(alignment_dir, ""step{:09d}_layer_alignment.png"".format(global_step))\n        alignment = attn.mean(0)[idx].cpu().data.numpy()\n        save_alignment(path, alignment)\n        tag = ""averaged_alignment""\n\n        try:\n            writer.add_image(tag, np.uint8(cm.viridis(\n                np.flip(alignment, 1).T) * 255), global_step)\n        except Exception as e:\n            warn(str(e))\n\n    # Predicted mel spectrogram\n    if mel_outputs is not None:\n        mel_output = mel_outputs[idx].cpu().data.numpy()\n        mel_output = prepare_spec_image(audio._denormalize(mel_output))\n        try:\n            writer.add_image(""Predicted mel spectrogram"",\n                             mel_output, global_step)\n        except Exception as e:\n            warn(str(e))\n            pass\n\n    # Predicted spectrogram\n    if linear_outputs is not None:\n        linear_output = linear_outputs[idx].cpu().data.numpy()\n        spectrogram = prepare_spec_image(audio._denormalize(linear_output))\n        try:\n            writer.add_image(""Predicted linear spectrogram"",\n                             spectrogram, global_step)\n        except Exception as e:\n            warn(str(e))\n            pass\n\n        # Predicted audio signal\n        signal = audio.inv_spectrogram(linear_output.T)\n        signal /= np.max(np.abs(signal))\n        path = join(checkpoint_dir, ""step{:09d}_predicted.wav"".format(\n            global_step))\n        try:\n            writer.add_audio(""Predicted audio signal"", signal,\n                             global_step, sample_rate=hparams.sample_rate)\n        except Exception as e:\n            warn(str(e))\n            pass\n        audio.save_wav(signal, path)\n\n    # Target mel spectrogram\n    if mel_outputs is not None:\n        mel_output = mel[idx].cpu().data.numpy()\n        mel_output = prepare_spec_image(audio._denormalize(mel_output))\n        try:\n            writer.add_image(""Target mel spectrogram"", mel_output, global_step)\n        except Exception as e:\n            warn(str(e))\n            pass\n\n    # Target spectrogram\n    if linear_outputs is not None:\n        linear_output = y[idx].cpu().data.numpy()\n        spectrogram = prepare_spec_image(audio._denormalize(linear_output))\n        try:\n            writer.add_image(""Target linear spectrogram"",\n                             spectrogram, global_step)\n        except Exception as e:\n            warn(str(e))\n            pass\n\n\ndef logit(x, eps=1e-8):\n    return torch.log(x + eps) - torch.log(1 - x + eps)\n\n\ndef masked_mean(y, mask):\n    # (B, T, D)\n    mask_ = mask.expand_as(y)\n    return (y * mask_).sum() / mask_.sum()\n\n\ndef spec_loss(y_hat, y, mask, priority_bin=None, priority_w=0):\n    masked_l1 = MaskedL1Loss()\n    l1 = nn.L1Loss()\n\n    w = hparams.masked_loss_weight\n\n    # L1 loss\n    if w > 0:\n        assert mask is not None\n        l1_loss = w * masked_l1(y_hat, y, mask=mask) + (1 - w) * l1(y_hat, y)\n    else:\n        assert mask is None\n        l1_loss = l1(y_hat, y)\n\n    # Priority L1 loss\n    if priority_bin is not None and priority_w > 0:\n        if w > 0:\n            priority_loss = w * masked_l1(\n                y_hat[:, :, :priority_bin], y[:, :, :priority_bin], mask=mask) \\\n                + (1 - w) * l1(y_hat[:, :, :priority_bin], y[:, :, :priority_bin])\n        else:\n            priority_loss = l1(y_hat[:, :, :priority_bin], y[:, :, :priority_bin])\n        l1_loss = (1 - priority_w) * l1_loss + priority_w * priority_loss\n\n    # Binary divergence loss\n    if hparams.binary_divergence_weight <= 0:\n        binary_div = y.data.new(1).zero_()\n    else:\n        y_hat_logits = logit(y_hat)\n        z = -y * y_hat_logits + torch.log1p(torch.exp(y_hat_logits))\n        if w > 0:\n            binary_div = w * masked_mean(z, mask) + (1 - w) * z.mean()\n        else:\n            binary_div = z.mean()\n\n    return l1_loss, binary_div\n\n\n@jit(nopython=True)\ndef guided_attention(N, max_N, T, max_T, g):\n    W = np.zeros((max_N, max_T), dtype=np.float32)\n    for n in range(N):\n        for t in range(T):\n            W[n, t] = 1 - np.exp(-(n / N - t / T)**2 / (2 * g * g))\n    return W\n\n\ndef guided_attentions(input_lengths, target_lengths, max_target_len, g=0.2):\n    B = len(input_lengths)\n    max_input_len = input_lengths.max()\n    W = np.zeros((B, max_target_len, max_input_len), dtype=np.float32)\n    for b in range(B):\n        W[b] = guided_attention(input_lengths[b], max_input_len,\n                                target_lengths[b], max_target_len, g).T\n    return W\n\n\ndef train(device, model, data_loader, optimizer, writer,\n          init_lr=0.002,\n          checkpoint_dir=None, checkpoint_interval=None, nepochs=None,\n          clip_thresh=1.0,\n          train_seq2seq=True, train_postnet=True):\n    linear_dim = model.linear_dim\n    r = hparams.outputs_per_step\n    downsample_step = hparams.downsample_step\n    current_lr = init_lr\n\n    binary_criterion = nn.BCELoss()\n\n    assert train_seq2seq or train_postnet\n\n    global global_step, global_epoch\n    while global_epoch < nepochs:\n        running_loss = 0.\n        for step, (x, input_lengths, mel, y, positions, done, target_lengths,\n                   speaker_ids) \\\n                in tqdm(enumerate(data_loader)):\n            model.train()\n            ismultispeaker = speaker_ids is not None\n            # Learning rate schedule\n            if hparams.lr_schedule is not None:\n                lr_schedule_f = getattr(lrschedule, hparams.lr_schedule)\n                current_lr = lr_schedule_f(\n                    init_lr, global_step, **hparams.lr_schedule_kwargs)\n                for param_group in optimizer.param_groups:\n                    param_group[\'lr\'] = current_lr\n            optimizer.zero_grad()\n\n            # Used for Position encoding\n            text_positions, frame_positions = positions\n\n            # Downsample mel spectrogram\n            if downsample_step > 1:\n                mel = mel[:, 0::downsample_step, :].contiguous()\n\n            # Lengths\n            input_lengths = input_lengths.long().numpy()\n            decoder_lengths = target_lengths.long().numpy() // r // downsample_step\n\n            max_seq_len = max(input_lengths.max(), decoder_lengths.max())\n            if max_seq_len >= hparams.max_positions:\n                raise RuntimeError(\n                    """"""max_seq_len ({}) >= max_posision ({})\nInput text or decoder targget length exceeded the maximum length.\nPlease set a larger value for ``max_position`` in hyper parameters."""""".format(\n                        max_seq_len, hparams.max_positions))\n\n            # Transform data to CUDA device\n            if train_seq2seq:\n                x = x.to(device)\n                text_positions = text_positions.to(device)\n                frame_positions = frame_positions.to(device)\n            if train_postnet:\n                y = y.to(device)\n            mel, done = mel.to(device), done.to(device)\n            target_lengths = target_lengths.to(device)\n            speaker_ids = speaker_ids.to(device) if ismultispeaker else None\n\n            # Create mask if we use masked loss\n            if hparams.masked_loss_weight > 0:\n                # decoder output domain mask\n                decoder_target_mask = sequence_mask(\n                    target_lengths / (r * downsample_step),\n                    max_len=mel.size(1)).unsqueeze(-1)\n                if downsample_step > 1:\n                    # spectrogram-domain mask\n                    target_mask = sequence_mask(\n                        target_lengths, max_len=y.size(1)).unsqueeze(-1)\n                else:\n                    target_mask = decoder_target_mask\n                # shift mask\n                decoder_target_mask = decoder_target_mask[:, r:, :]\n                target_mask = target_mask[:, r:, :]\n            else:\n                decoder_target_mask, target_mask = None, None\n\n            # Apply model\n            if train_seq2seq and train_postnet:\n                mel_outputs, linear_outputs, attn, done_hat = model(\n                    x, mel, speaker_ids=speaker_ids,\n                    text_positions=text_positions, frame_positions=frame_positions,\n                    input_lengths=input_lengths)\n            elif train_seq2seq:\n                assert speaker_ids is None\n                mel_outputs, attn, done_hat, _ = model.seq2seq(\n                    x, mel,\n                    text_positions=text_positions, frame_positions=frame_positions,\n                    input_lengths=input_lengths)\n                # reshape\n                mel_outputs = mel_outputs.view(len(mel), -1, mel.size(-1))\n                linear_outputs = None\n            elif train_postnet:\n                assert speaker_ids is None\n                linear_outputs = model.postnet(mel)\n                mel_outputs, attn, done_hat = None, None, None\n\n            # Losses\n            w = hparams.binary_divergence_weight\n\n            # mel:\n            if train_seq2seq:\n                mel_l1_loss, mel_binary_div = spec_loss(\n                    mel_outputs[:, :-r, :], mel[:, r:, :], decoder_target_mask)\n                mel_loss = (1 - w) * mel_l1_loss + w * mel_binary_div\n\n            # done:\n            if train_seq2seq:\n                done_loss = binary_criterion(done_hat, done)\n\n            # linear:\n            if train_postnet:\n                n_priority_freq = int(hparams.priority_freq / (hparams.sample_rate * 0.5) * linear_dim)\n                linear_l1_loss, linear_binary_div = spec_loss(\n                    linear_outputs[:, :-r, :], y[:, r:, :], target_mask,\n                    priority_bin=n_priority_freq,\n                    priority_w=hparams.priority_freq_weight)\n                linear_loss = (1 - w) * linear_l1_loss + w * linear_binary_div\n\n            # Combine losses\n            if train_seq2seq and train_postnet:\n                loss = mel_loss + linear_loss + done_loss\n            elif train_seq2seq:\n                loss = mel_loss + done_loss\n            elif train_postnet:\n                loss = linear_loss\n\n            # attention\n            if train_seq2seq and hparams.use_guided_attention:\n                soft_mask = guided_attentions(input_lengths, decoder_lengths,\n                                              attn.size(-2),\n                                              g=hparams.guided_attention_sigma)\n                soft_mask = torch.from_numpy(soft_mask).to(device)\n                attn_loss = (attn * soft_mask).mean()\n                loss += attn_loss\n\n            if global_step > 0 and global_step % checkpoint_interval == 0:\n                save_states(\n                    global_step, writer, mel_outputs, linear_outputs, attn,\n                    mel, y, input_lengths, checkpoint_dir)\n                save_checkpoint(\n                    model, optimizer, global_step, checkpoint_dir, global_epoch,\n                    train_seq2seq, train_postnet)\n\n            if global_step > 0 and global_step % hparams.eval_interval == 0:\n                eval_model(global_step, writer, device, model,\n                           checkpoint_dir, ismultispeaker)\n\n            # Update\n            loss.backward()\n            if clip_thresh > 0:\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    model.get_trainable_parameters(), clip_thresh)\n            optimizer.step()\n\n            # Logs\n            writer.add_scalar(""loss"", float(loss.item()), global_step)\n            if train_seq2seq:\n                writer.add_scalar(""done_loss"", float(done_loss.item()), global_step)\n                writer.add_scalar(""mel loss"", float(mel_loss.item()), global_step)\n                writer.add_scalar(""mel_l1_loss"", float(mel_l1_loss.item()), global_step)\n                writer.add_scalar(""mel_binary_div_loss"", float(mel_binary_div.item()), global_step)\n            if train_postnet:\n                writer.add_scalar(""linear_loss"", float(linear_loss.item()), global_step)\n                writer.add_scalar(""linear_l1_loss"", float(linear_l1_loss.item()), global_step)\n                writer.add_scalar(""linear_binary_div_loss"", float(linear_binary_div.item()), global_step)\n            if train_seq2seq and hparams.use_guided_attention:\n                writer.add_scalar(""attn_loss"", float(attn_loss.item()), global_step)\n            if clip_thresh > 0:\n                writer.add_scalar(""gradient norm"", grad_norm, global_step)\n            writer.add_scalar(""learning rate"", current_lr, global_step)\n\n            global_step += 1\n            running_loss += loss.item()\n\n        averaged_loss = running_loss / (len(data_loader))\n        writer.add_scalar(""loss (per epoch)"", averaged_loss, global_epoch)\n        print(""Loss: {}"".format(running_loss / (len(data_loader))))\n\n        global_epoch += 1\n\n\ndef save_checkpoint(model, optimizer, step, checkpoint_dir, epoch,\n                    train_seq2seq, train_postnet):\n    if train_seq2seq and train_postnet:\n        suffix = """"\n        m = model\n    elif train_seq2seq:\n        suffix = ""_seq2seq""\n        m = model.seq2seq\n    elif train_postnet:\n        suffix = ""_postnet""\n        m = model.postnet\n\n    checkpoint_path = join(\n        checkpoint_dir, ""checkpoint_step{:09d}{}.pth"".format(global_step, suffix))\n    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n    torch.save({\n        ""state_dict"": m.state_dict(),\n        ""optimizer"": optimizer_state,\n        ""global_step"": step,\n        ""global_epoch"": epoch,\n    }, checkpoint_path)\n    print(""Saved checkpoint:"", checkpoint_path)\n\n\ndef build_model():\n    model = getattr(builder, hparams.builder)(\n        n_speakers=hparams.n_speakers,\n        speaker_embed_dim=hparams.speaker_embed_dim,\n        n_vocab=_frontend.n_vocab,\n        embed_dim=hparams.text_embed_dim,\n        mel_dim=hparams.num_mels,\n        linear_dim=hparams.fft_size // 2 + 1,\n        r=hparams.outputs_per_step,\n        downsample_step=hparams.downsample_step,\n        padding_idx=hparams.padding_idx,\n        dropout=hparams.dropout,\n        kernel_size=hparams.kernel_size,\n        encoder_channels=hparams.encoder_channels,\n        decoder_channels=hparams.decoder_channels,\n        converter_channels=hparams.converter_channels,\n        use_memory_mask=hparams.use_memory_mask,\n        trainable_positional_encodings=hparams.trainable_positional_encodings,\n        force_monotonic_attention=hparams.force_monotonic_attention,\n        use_decoder_state_for_postnet_input=hparams.use_decoder_state_for_postnet_input,\n        max_positions=hparams.max_positions,\n        speaker_embedding_weight_std=hparams.speaker_embedding_weight_std,\n        freeze_embedding=hparams.freeze_embedding,\n        window_ahead=hparams.window_ahead,\n        window_backward=hparams.window_backward,\n        key_projection=hparams.key_projection,\n        value_projection=hparams.value_projection,\n    )\n    return model\n\n\ndef _load(checkpoint_path):\n    if use_cuda:\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path,\n                                map_location=lambda storage, loc: storage)\n    return checkpoint\n\n\ndef load_checkpoint(path, model, optimizer, reset_optimizer):\n    global global_step\n    global global_epoch\n\n    print(""Load checkpoint from: {}"".format(path))\n    checkpoint = _load(path)\n    model.load_state_dict(checkpoint[""state_dict""])\n    if not reset_optimizer:\n        optimizer_state = checkpoint[""optimizer""]\n        if optimizer_state is not None:\n            print(""Load optimizer state from {}"".format(path))\n            optimizer.load_state_dict(checkpoint[""optimizer""])\n    global_step = checkpoint[""global_step""]\n    global_epoch = checkpoint[""global_epoch""]\n\n    return model\n\n\ndef _load_embedding(path, model):\n    state = _load(path)[""state_dict""]\n    key = ""seq2seq.encoder.embed_tokens.weight""\n    model.seq2seq.encoder.embed_tokens.weight.data = state[key]\n\n# https://discuss.pytorch.org/t/how-to-load-part-of-pre-trained-model/1113/3\n\n\ndef restore_parts(path, model):\n    print(""Restore part of the model from: {}"".format(path))\n    state = _load(path)[""state_dict""]\n    model_dict = model.state_dict()\n    valid_state_dict = {k: v for k, v in state.items() if k in model_dict}\n\n    try:\n        model_dict.update(valid_state_dict)\n        model.load_state_dict(model_dict)\n    except RuntimeError as e:\n        # there should be invalid size of weight(s), so load them per parameter\n        print(str(e))\n        model_dict = model.state_dict()\n        for k, v in valid_state_dict.items():\n            model_dict[k] = v\n            try:\n                model.load_state_dict(model_dict)\n            except RuntimeError as e:\n                print(str(e))\n                warn(""{}: may contain invalid size of weight. skipping..."".format(k))\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    print(""Command line args:\\n"", args)\n    checkpoint_dir = args[""--checkpoint-dir""]\n    checkpoint_path = args[""--checkpoint""]\n    checkpoint_seq2seq_path = args[""--checkpoint-seq2seq""]\n    checkpoint_postnet_path = args[""--checkpoint-postnet""]\n    load_embedding = args[""--load-embedding""]\n    checkpoint_restore_parts = args[""--restore-parts""]\n    speaker_id = args[""--speaker-id""]\n    speaker_id = int(speaker_id) if speaker_id is not None else None\n    preset = args[""--preset""]\n\n    data_root = args[""--data-root""]\n    if data_root is None:\n        data_root = join(dirname(__file__), ""data"", ""ljspeech"")\n\n    log_event_path = args[""--log-event-path""]\n    reset_optimizer = args[""--reset-optimizer""]\n\n    # Which model to be trained\n    train_seq2seq = args[""--train-seq2seq-only""]\n    train_postnet = args[""--train-postnet-only""]\n    # train both if not specified\n    if not train_seq2seq and not train_postnet:\n        print(""Training whole model"")\n        train_seq2seq, train_postnet = True, True\n    if train_seq2seq:\n        print(""Training seq2seq model"")\n    elif train_postnet:\n        print(""Training postnet model"")\n    else:\n        assert False, ""must be specified wrong args""\n\n    # Load preset if specified\n    if preset is not None:\n        with open(preset) as f:\n            hparams.parse_json(f.read())\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n\n    # Preventing Windows specific error such as MemoryError\n    # Also reduces the occurrence of THAllocator.c 0x05 error in Widows build of PyTorch\n    if platform.system() == ""Windows"":\n        print("" [!] Windows Detected - IF THAllocator.c 0x05 error occurs SET num_workers to 1"")\n\n    assert hparams.name == ""deepvoice3""\n    print(hparams_debug_string())\n\n    _frontend = getattr(frontend, hparams.frontend)\n\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    # Input dataset definitions\n    X = FileSourceDataset(TextDataSource(data_root, speaker_id))\n    Mel = FileSourceDataset(MelSpecDataSource(data_root, speaker_id))\n    Y = FileSourceDataset(LinearSpecDataSource(data_root, speaker_id))\n\n    # Prepare sampler\n    frame_lengths = Mel.file_data_source.frame_lengths\n    sampler = PartialyRandomizedSimilarTimeLengthSampler(\n        frame_lengths, batch_size=hparams.batch_size)\n\n    # Dataset and Dataloader setup\n    dataset = PyTorchDataset(X, Mel, Y)\n    data_loader = data_utils.DataLoader(\n        dataset, batch_size=hparams.batch_size,\n        num_workers=hparams.num_workers, sampler=sampler,\n        collate_fn=collate_fn, pin_memory=hparams.pin_memory, drop_last=True)\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    # Model\n    model = build_model().to(device)\n\n    optimizer = optim.Adam(model.get_trainable_parameters(),\n                           lr=hparams.initial_learning_rate, betas=(\n        hparams.adam_beta1, hparams.adam_beta2),\n        eps=hparams.adam_eps, weight_decay=hparams.weight_decay,\n        amsgrad=hparams.amsgrad)\n\n    if checkpoint_restore_parts is not None:\n        restore_parts(checkpoint_restore_parts, model)\n\n    # Load checkpoints\n    if checkpoint_postnet_path is not None:\n        load_checkpoint(checkpoint_postnet_path, model.postnet, optimizer, reset_optimizer)\n\n    if checkpoint_seq2seq_path is not None:\n        load_checkpoint(checkpoint_seq2seq_path, model.seq2seq, optimizer, reset_optimizer)\n\n    if checkpoint_path is not None:\n        load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer)\n\n    # Load embedding\n    if load_embedding is not None:\n        print(""Loading embedding from {}"".format(load_embedding))\n        _load_embedding(load_embedding, model)\n\n    # Setup summary writer for tensorboard\n    if log_event_path is None:\n        if platform.system() == ""Windows"":\n            log_event_path = ""log/run-test"" + str(datetime.now()).replace("" "", ""_"").replace("":"", ""_"")\n        else:\n            log_event_path = ""log/run-test"" + str(datetime.now()).replace("" "", ""_"")\n    print(""Log event path: {}"".format(log_event_path))\n    writer = SummaryWriter(log_event_path)\n\n    # Train!\n    try:\n        train(device, model, data_loader, optimizer, writer,\n              init_lr=hparams.initial_learning_rate,\n              checkpoint_dir=checkpoint_dir,\n              checkpoint_interval=hparams.checkpoint_interval,\n              nepochs=hparams.nepochs,\n              clip_thresh=hparams.clip_thresh,\n              train_seq2seq=train_seq2seq, train_postnet=train_postnet)\n    except KeyboardInterrupt:\n        save_checkpoint(\n            model, optimizer, global_step, checkpoint_dir, global_epoch,\n            train_seq2seq, train_postnet)\n\n    print(""Finished"")\n    sys.exit(0)\n'"
vctk.py,0,"b'from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport audio\nfrom nnmnkwii.datasets import vctk\nfrom nnmnkwii.io import hts\nfrom hparams import hparams\nfrom os.path import exists\nimport librosa\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n\n    speakers = vctk.available_speakers\n\n    td = vctk.TranscriptionDataSource(in_dir, speakers=speakers)\n    transcriptions = td.collect_files()\n    speaker_ids = td.labels\n    wav_paths = vctk.WavFileDataSource(\n        in_dir, speakers=speakers).collect_files()\n\n    for index, (speaker_id, text, wav_path) in enumerate(\n            zip(speaker_ids, transcriptions, wav_paths)):\n        futures.append(executor.submit(\n            partial(_process_utterance, out_dir, index + 1, speaker_id, wav_path, text)))\n    return [future.result() for future in tqdm(futures)]\n\n\ndef start_at(labels):\n    has_silence = labels[0][-1] == ""pau""\n    if not has_silence:\n        return labels[0][0]\n    for i in range(1, len(labels)):\n        if labels[i][-1] != ""pau"":\n            return labels[i][0]\n    assert False\n\n\ndef end_at(labels):\n    has_silence = labels[-1][-1] == ""pau""\n    if not has_silence:\n        return labels[-1][1]\n    for i in range(len(labels) - 2, 0, -1):\n        if labels[i][-1] != ""pau"":\n            return labels[i][1]\n    assert False\n\n\ndef _process_utterance(out_dir, index, speaker_id, wav_path, text):\n    sr = hparams.sample_rate\n\n    # Load the audio to a numpy array:\n    wav = audio.load_wav(wav_path)\n\n    lab_path = wav_path.replace(""wav48/"", ""lab/"").replace("".wav"", "".lab"")\n\n    # Trim silence from hts labels if available\n    if exists(lab_path):\n        labels = hts.load(lab_path)\n        b = int(start_at(labels) * 1e-7 * sr)\n        e = int(end_at(labels) * 1e-7 * sr)\n        wav = wav[b:e]\n        wav, _ = librosa.effects.trim(wav, top_db=25)\n    else:\n        wav, _ = librosa.effects.trim(wav, top_db=15)\n\n    if hparams.rescaling:\n        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n\n    # Compute the linear-scale spectrogram from the wav:\n    spectrogram = audio.spectrogram(wav).astype(np.float32)\n    n_frames = spectrogram.shape[1]\n\n    # Compute a mel-scale spectrogram from the wav:\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32)\n\n    # Write the spectrograms to disk:\n    spectrogram_filename = \'vctk-spec-%05d.npy\' % index\n    mel_filename = \'vctk-mel-%05d.npy\' % index\n    np.save(os.path.join(out_dir, spectrogram_filename), spectrogram.T, allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return (spectrogram_filename, mel_filename, n_frames, text, speaker_id)\n'"
deepvoice3_pytorch/__init__.py,0,"b'# coding: utf-8\n\nfrom .version import __version__\n\nimport torch\nfrom torch import nn\n\nfrom .modules import Embedding\n\n\nclass MultiSpeakerTTSModel(nn.Module):\n    """"""Attention seq2seq model + post processing network\n    """"""\n\n    def __init__(self, seq2seq, postnet,\n                 mel_dim=80, linear_dim=513,\n                 n_speakers=1, speaker_embed_dim=16, padding_idx=None,\n                 trainable_positional_encodings=False,\n                 use_decoder_state_for_postnet_input=False,\n                 speaker_embedding_weight_std=0.01,\n                 freeze_embedding=False):\n        super(MultiSpeakerTTSModel, self).__init__()\n        self.seq2seq = seq2seq\n        self.postnet = postnet  # referred as ""Converter"" in DeepVoice3\n        self.mel_dim = mel_dim\n        self.linear_dim = linear_dim\n        self.trainable_positional_encodings = trainable_positional_encodings\n        self.use_decoder_state_for_postnet_input = use_decoder_state_for_postnet_input\n        self.freeze_embedding = freeze_embedding\n\n        # Speaker embedding\n        if n_speakers > 1:\n            self.embed_speakers = Embedding(\n                n_speakers, speaker_embed_dim, padding_idx=None,\n                std=speaker_embedding_weight_std)\n        self.n_speakers = n_speakers\n        self.speaker_embed_dim = speaker_embed_dim\n\n    def make_generation_fast_(self):\n\n        def remove_weight_norm(m):\n            try:\n                nn.utils.remove_weight_norm(m)\n            except ValueError:  # this module didn\'t have weight norm\n                return\n        self.apply(remove_weight_norm)\n\n    def get_trainable_parameters(self):\n        freezed_param_ids = set()\n\n        encoder, decoder = self.seq2seq.encoder, self.seq2seq.decoder\n\n        # Avoid updating the position encoding\n        if not self.trainable_positional_encodings:\n            pe_query_param_ids = set(map(id, decoder.embed_query_positions.parameters()))\n            pe_keys_param_ids = set(map(id, decoder.embed_keys_positions.parameters()))\n            freezed_param_ids |= (pe_query_param_ids | pe_keys_param_ids)\n        # Avoid updating the text embedding\n        if self.freeze_embedding:\n            embed_param_ids = set(map(id, encoder.embed_tokens.parameters()))\n            freezed_param_ids |= embed_param_ids\n\n        return (p for p in self.parameters() if id(p) not in freezed_param_ids)\n\n    def forward(self, text_sequences, mel_targets=None, speaker_ids=None,\n                text_positions=None, frame_positions=None, input_lengths=None):\n        B = text_sequences.size(0)\n\n        if speaker_ids is not None:\n            assert self.n_speakers > 1\n            speaker_embed = self.embed_speakers(speaker_ids)\n        else:\n            speaker_embed = None\n\n        # Apply seq2seq\n        # (B, T//r, mel_dim*r)\n        mel_outputs, alignments, done, decoder_states = self.seq2seq(\n            text_sequences, mel_targets, speaker_embed,\n            text_positions, frame_positions, input_lengths)\n\n        # Reshape\n        # (B, T, mel_dim)\n        mel_outputs = mel_outputs.view(B, -1, self.mel_dim)\n\n        # Prepare postnet inputs\n        if self.use_decoder_state_for_postnet_input:\n            postnet_inputs = decoder_states.view(B, mel_outputs.size(1), -1)\n        else:\n            postnet_inputs = mel_outputs\n\n        # (B, T, linear_dim)\n        # Convert coarse mel-spectrogram (or decoder hidden states) to\n        # high resolution spectrogram\n        linear_outputs = self.postnet(postnet_inputs, speaker_embed)\n        assert linear_outputs.size(-1) == self.linear_dim\n\n        return mel_outputs, linear_outputs, alignments, done\n\n\nclass AttentionSeq2Seq(nn.Module):\n    """"""Encoder + Decoder with attention\n    """"""\n\n    def __init__(self, encoder, decoder):\n        super(AttentionSeq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        if isinstance(self.decoder.attention, nn.ModuleList):\n            self.encoder.num_attention_layers = sum(\n                [layer is not None for layer in decoder.attention])\n\n    def forward(self, text_sequences, mel_targets=None, speaker_embed=None,\n                text_positions=None, frame_positions=None, input_lengths=None):\n        # (B, T, text_embed_dim)\n        encoder_outputs = self.encoder(\n            text_sequences, lengths=input_lengths, speaker_embed=speaker_embed)\n\n        # Mel: (B, T//r, mel_dim*r)\n        # Alignments: (N, B, T_target, T_input)\n        # Done: (B, T//r, 1)\n        mel_outputs, alignments, done, decoder_states = self.decoder(\n            encoder_outputs, mel_targets,\n            text_positions=text_positions, frame_positions=frame_positions,\n            speaker_embed=speaker_embed, lengths=input_lengths)\n\n        return mel_outputs, alignments, done, decoder_states\n'"
deepvoice3_pytorch/builder.py,3,"b'import torch\nfrom torch import nn\n\nfrom deepvoice3_pytorch import MultiSpeakerTTSModel, AttentionSeq2Seq\n\n\ndef deepvoice3(n_vocab, embed_dim=256, mel_dim=80, linear_dim=513, r=4,\n               downsample_step=1,\n               n_speakers=1, speaker_embed_dim=16, padding_idx=0,\n               dropout=(1 - 0.95), kernel_size=5,\n               encoder_channels=128,\n               decoder_channels=256,\n               converter_channels=256,\n               query_position_rate=1.0,\n               key_position_rate=1.29,\n               use_memory_mask=False,\n               trainable_positional_encodings=False,\n               force_monotonic_attention=True,\n               use_decoder_state_for_postnet_input=True,\n               max_positions=512,\n               embedding_weight_std=0.1,\n               speaker_embedding_weight_std=0.01,\n               freeze_embedding=False,\n               window_ahead=3,\n               window_backward=1,\n               key_projection=False,\n               value_projection=False,\n               ):\n    """"""Build deepvoice3\n    """"""\n    from deepvoice3_pytorch.deepvoice3 import Encoder, Decoder, Converter\n\n    time_upsampling = max(downsample_step // r, 1)\n\n    # Seq2seq\n    h = encoder_channels  # hidden dim (channels)\n    k = kernel_size   # kernel size\n    encoder = Encoder(\n        n_vocab, embed_dim, padding_idx=padding_idx,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        dropout=dropout, max_positions=max_positions,\n        embedding_weight_std=embedding_weight_std,\n        # (channels, kernel_size, dilation)\n        convolutions=[(h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n                      (h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n                      (h, k, 1), (h, k, 3)],\n    )\n\n    h = decoder_channels\n    decoder = Decoder(\n        embed_dim, in_dim=mel_dim, r=r, padding_idx=padding_idx,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        dropout=dropout, max_positions=max_positions,\n        preattention=[(h, k, 1), (h, k, 3)],\n        convolutions=[(h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n                      (h, k, 1)],\n        attention=[True, False, False, False, True],\n        force_monotonic_attention=force_monotonic_attention,\n        query_position_rate=query_position_rate,\n        key_position_rate=key_position_rate,\n        use_memory_mask=use_memory_mask,\n        window_ahead=window_ahead,\n        window_backward=window_backward,\n        key_projection=key_projection,\n        value_projection=value_projection,\n    )\n\n    seq2seq = AttentionSeq2Seq(encoder, decoder)\n\n    # Post net\n    if use_decoder_state_for_postnet_input:\n        in_dim = h // r\n    else:\n        in_dim = mel_dim\n    h = converter_channels\n    converter = Converter(\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        in_dim=in_dim, out_dim=linear_dim, dropout=dropout,\n        time_upsampling=time_upsampling,\n        convolutions=[(h, k, 1), (h, k, 3), (2 * h, k, 1), (2 * h, k, 3)],\n    )\n\n    # Seq2seq + post net\n    model = MultiSpeakerTTSModel(\n        seq2seq, converter, padding_idx=padding_idx,\n        mel_dim=mel_dim, linear_dim=linear_dim,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        trainable_positional_encodings=trainable_positional_encodings,\n        use_decoder_state_for_postnet_input=use_decoder_state_for_postnet_input,\n        speaker_embedding_weight_std=speaker_embedding_weight_std,\n        freeze_embedding=freeze_embedding)\n\n    return model\n\n\ndef nyanko(n_vocab, embed_dim=128, mel_dim=80, linear_dim=513, r=1,\n           downsample_step=4,\n           n_speakers=1, speaker_embed_dim=16, padding_idx=0,\n           dropout=(1 - 0.95), kernel_size=3,\n           encoder_channels=256,\n           decoder_channels=256,\n           converter_channels=512,\n           query_position_rate=1.0,\n           key_position_rate=1.29,\n           use_memory_mask=False,\n           trainable_positional_encodings=False,\n           force_monotonic_attention=True,\n           use_decoder_state_for_postnet_input=False,\n           max_positions=512, embedding_weight_std=0.01,\n           speaker_embedding_weight_std=0.01,\n           freeze_embedding=False,\n           window_ahead=3,\n           window_backward=1,\n           key_projection=False,\n           value_projection=False,\n           ):\n    from deepvoice3_pytorch.nyanko import Encoder, Decoder, Converter\n    assert encoder_channels == decoder_channels\n\n    if n_speakers != 1:\n        raise ValueError(""Multi-speaker is not supported"")\n    if not (downsample_step == 4 and r == 1):\n        raise ValueError(""Not supported. You need to change hardcoded parameters"")\n\n    # Seq2seq\n    encoder = Encoder(\n        n_vocab, embed_dim, channels=encoder_channels, kernel_size=kernel_size,\n        padding_idx=padding_idx,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        dropout=dropout, embedding_weight_std=embedding_weight_std,\n    )\n\n    decoder = Decoder(\n        embed_dim, in_dim=mel_dim, r=r, channels=decoder_channels,\n        kernel_size=kernel_size, padding_idx=padding_idx,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        dropout=dropout, max_positions=max_positions,\n        force_monotonic_attention=force_monotonic_attention,\n        query_position_rate=query_position_rate,\n        key_position_rate=key_position_rate,\n        use_memory_mask=use_memory_mask,\n        window_ahead=window_ahead,\n        window_backward=window_backward,\n        key_projection=key_projection,\n        value_projection=value_projection,\n    )\n\n    seq2seq = AttentionSeq2Seq(encoder, decoder)\n\n    if use_decoder_state_for_postnet_input:\n        in_dim = decoder_channels // r\n    else:\n        in_dim = mel_dim\n\n    converter = Converter(\n        in_dim=in_dim, out_dim=linear_dim, channels=converter_channels,\n        kernel_size=kernel_size, dropout=dropout)\n\n    # Seq2seq + post net\n    model = MultiSpeakerTTSModel(\n        seq2seq, converter, padding_idx=padding_idx,\n        mel_dim=mel_dim, linear_dim=linear_dim,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        trainable_positional_encodings=trainable_positional_encodings,\n        use_decoder_state_for_postnet_input=use_decoder_state_for_postnet_input,\n        speaker_embedding_weight_std=speaker_embedding_weight_std,\n        freeze_embedding=freeze_embedding)\n\n    return model\n\n\ndef deepvoice3_multispeaker(n_vocab, embed_dim=256, mel_dim=80, linear_dim=513, r=4,\n                            downsample_step=1,\n                            n_speakers=1, speaker_embed_dim=16, padding_idx=0,\n                            dropout=(1 - 0.95), kernel_size=5,\n                            encoder_channels=128,\n                            decoder_channels=256,\n                            converter_channels=256,\n                            query_position_rate=1.0,\n                            key_position_rate=1.29,\n                            use_memory_mask=False,\n                            trainable_positional_encodings=False,\n                            force_monotonic_attention=True,\n                            use_decoder_state_for_postnet_input=True,\n                            max_positions=512,\n                            embedding_weight_std=0.1,\n                            speaker_embedding_weight_std=0.01,\n                            freeze_embedding=False,\n                            window_ahead=3,\n                            window_backward=1,\n                            key_projection=True,\n                            value_projection=True,\n                            ):\n    """"""Build multi-speaker deepvoice3\n    """"""\n    from deepvoice3_pytorch.deepvoice3 import Encoder, Decoder, Converter\n\n    time_upsampling = max(downsample_step // r, 1)\n\n    # Seq2seq\n    h = encoder_channels  # hidden dim (channels)\n    k = kernel_size   # kernel size\n    encoder = Encoder(\n        n_vocab, embed_dim, padding_idx=padding_idx,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        dropout=dropout, max_positions=max_positions,\n        embedding_weight_std=embedding_weight_std,\n        # (channels, kernel_size, dilation)\n        convolutions=[(h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n                      (h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n                      (h, k, 1), (h, k, 3)],\n    )\n\n    h = decoder_channels\n    decoder = Decoder(\n        embed_dim, in_dim=mel_dim, r=r, padding_idx=padding_idx,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        dropout=dropout, max_positions=max_positions,\n        preattention=[(h, k, 1)],\n        convolutions=[(h, k, 1), (h, k, 3), (h, k, 9), (h, k, 27),\n                      (h, k, 1)],\n        attention=[True, False, False, False, False],\n        force_monotonic_attention=force_monotonic_attention,\n        query_position_rate=query_position_rate,\n        key_position_rate=key_position_rate,\n        use_memory_mask=use_memory_mask,\n        window_ahead=window_ahead,\n        window_backward=window_backward,\n        key_projection=key_projection,\n        value_projection=value_projection,\n    )\n\n    seq2seq = AttentionSeq2Seq(encoder, decoder)\n\n    # Post net\n    if use_decoder_state_for_postnet_input:\n        in_dim = h // r\n    else:\n        in_dim = mel_dim\n    h = converter_channels\n    converter = Converter(\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        in_dim=in_dim, out_dim=linear_dim, dropout=dropout,\n        time_upsampling=time_upsampling,\n        convolutions=[(h, k, 1), (h, k, 3), (2 * h, k, 1), (2 * h, k, 3)],\n    )\n\n    # Seq2seq + post net\n    model = MultiSpeakerTTSModel(\n        seq2seq, converter, padding_idx=padding_idx,\n        mel_dim=mel_dim, linear_dim=linear_dim,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        trainable_positional_encodings=trainable_positional_encodings,\n        use_decoder_state_for_postnet_input=use_decoder_state_for_postnet_input,\n        speaker_embedding_weight_std=speaker_embedding_weight_std,\n        freeze_embedding=freeze_embedding)\n\n    return model\n'"
deepvoice3_pytorch/conv.py,1,"b'# coding: utf-8\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Conv1d(nn.Conv1d):\n    """"""Extended nn.Conv1d for incremental dilated convolutions\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clear_buffer()\n        self._linearized_weight = None\n        self.register_backward_hook(self._clear_linearized_weight)\n\n    def incremental_forward(self, input):\n        # input: (B, T, C)\n        if self.training:\n            raise RuntimeError(\'incremental_forward only supports eval mode\')\n\n        # run forward pre hooks (e.g., weight norm)\n        for hook in self._forward_pre_hooks.values():\n            hook(self, input)\n\n        # reshape weight\n        weight = self._get_linearized_weight()\n        kw = self.kernel_size[0]\n        dilation = self.dilation[0]\n\n        bsz = input.size(0)  # input: bsz x len x dim\n        if kw > 1:\n            input = input.data\n            if self.input_buffer is None:\n                self.input_buffer = input.new(bsz, kw + (kw - 1) * (dilation - 1), input.size(2))\n                self.input_buffer.zero_()\n            else:\n                # shift buffer\n                self.input_buffer[:, :-1, :] = self.input_buffer[:, 1:, :].clone()\n            # append next input\n            self.input_buffer[:, -1, :] = input[:, -1, :]\n            input = self.input_buffer\n            if dilation > 1:\n                input = input[:, 0::dilation, :].contiguous()\n        output = F.linear(input.view(bsz, -1), weight, self.bias)\n        return output.view(bsz, 1, -1)\n\n    def clear_buffer(self):\n        self.input_buffer = None\n\n    def _get_linearized_weight(self):\n        if self._linearized_weight is None:\n            kw = self.kernel_size[0]\n            # nn.Conv1d\n            if self.weight.size() == (self.out_channels, self.in_channels, kw):\n                weight = self.weight.transpose(1, 2).contiguous()\n            else:\n                # fairseq.modules.conv_tbc.ConvTBC\n                weight = self.weight.transpose(2, 1).transpose(1, 0).contiguous()\n            assert weight.size() == (self.out_channels, kw, self.in_channels)\n            self._linearized_weight = weight.view(self.out_channels, -1)\n        return self._linearized_weight\n\n    def _clear_linearized_weight(self, *args):\n        self._linearized_weight = None\n'"
deepvoice3_pytorch/deepvoice3.py,16,"b'# coding: utf-8\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\nimport numpy as np\n\nfrom .modules import Conv1d, ConvTranspose1d, Embedding, Linear, GradMultiply\nfrom .modules import get_mask_from_lengths, SinusoidalEncoding, Conv1dGLU\n\n\ndef expand_speaker_embed(inputs_btc, speaker_embed=None, tdim=1):\n    if speaker_embed is None:\n        return None\n    # expand speaker embedding for all time steps\n    # (B, N) -> (B, T, N)\n    ss = speaker_embed.size()\n    speaker_embed_btc = speaker_embed.unsqueeze(1).expand(\n        ss[0], inputs_btc.size(tdim), ss[-1])\n    return speaker_embed_btc\n\n\nclass Encoder(nn.Module):\n    def __init__(self, n_vocab, embed_dim, n_speakers, speaker_embed_dim,\n                 padding_idx=None, embedding_weight_std=0.1,\n                 convolutions=((64, 5, .1),) * 7,\n                 max_positions=512, dropout=0.1, apply_grad_scaling=False):\n        super(Encoder, self).__init__()\n        self.dropout = dropout\n        self.num_attention_layers = None\n        self.apply_grad_scaling = apply_grad_scaling\n\n        # Text input embeddings\n        self.embed_tokens = Embedding(\n            n_vocab, embed_dim, padding_idx, embedding_weight_std)\n\n        # Speaker embedding\n        if n_speakers > 1:\n            self.speaker_fc1 = Linear(speaker_embed_dim, embed_dim, dropout=dropout)\n            self.speaker_fc2 = Linear(speaker_embed_dim, embed_dim, dropout=dropout)\n        self.n_speakers = n_speakers\n\n        # Non causual convolution blocks\n        in_channels = embed_dim\n        self.convolutions = nn.ModuleList()\n        std_mul = 1.0\n        for (out_channels, kernel_size, dilation) in convolutions:\n            if in_channels != out_channels:\n                # Conv1d + ReLU\n                self.convolutions.append(\n                    Conv1d(in_channels, out_channels, kernel_size=1, padding=0,\n                           dilation=1, std_mul=std_mul))\n                self.convolutions.append(nn.ReLU(inplace=True))\n                in_channels = out_channels\n                std_mul = 2.0\n            self.convolutions.append(\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, out_channels, kernel_size, causal=False,\n                          dilation=dilation, dropout=dropout, std_mul=std_mul,\n                          residual=True))\n            in_channels = out_channels\n            std_mul = 4.0\n        # Last 1x1 convolution\n        self.convolutions.append(Conv1d(in_channels, embed_dim, kernel_size=1,\n                                        padding=0, dilation=1, std_mul=std_mul,\n                                        dropout=dropout))\n\n    def forward(self, text_sequences, text_positions=None, lengths=None,\n                speaker_embed=None):\n        assert self.n_speakers == 1 or speaker_embed is not None\n\n        # embed text_sequences\n        x = self.embed_tokens(text_sequences.long())\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # expand speaker embedding for all time steps\n        speaker_embed_btc = expand_speaker_embed(x, speaker_embed)\n        if speaker_embed_btc is not None:\n            speaker_embed_btc = F.dropout(speaker_embed_btc, p=self.dropout, training=self.training)\n            x = x + F.softsign(self.speaker_fc1(speaker_embed_btc))\n\n        input_embedding = x\n\n        # B x T x C -> B x C x T\n        x = x.transpose(1, 2)\n\n        # \xef\xbc\x91D conv blocks\n        for f in self.convolutions:\n            x = f(x, speaker_embed_btc) if isinstance(f, Conv1dGLU) else f(x)\n\n        # Back to B x T x C\n        keys = x.transpose(1, 2)\n\n        if speaker_embed_btc is not None:\n            keys = keys + F.softsign(self.speaker_fc2(speaker_embed_btc))\n\n        # scale gradients (this only affects backward, not forward)\n        if self.apply_grad_scaling and self.num_attention_layers is not None:\n            keys = GradMultiply.apply(keys, 1.0 / (2.0 * self.num_attention_layers))\n\n        # add output to input embedding for attention\n        values = (keys + input_embedding) * math.sqrt(0.5)\n\n        return keys, values\n\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, conv_channels, embed_dim, dropout=0.1,\n                 window_ahead=3, window_backward=1,\n                 key_projection=True, value_projection=True):\n        super(AttentionLayer, self).__init__()\n        self.query_projection = Linear(conv_channels, embed_dim)\n        if key_projection:\n            self.key_projection = Linear(embed_dim, embed_dim)\n            # According to the DeepVoice3 paper, intiailize weights to same values\n            # TODO: Does this really work well? not sure..\n            if conv_channels == embed_dim:\n                self.key_projection.weight.data = self.query_projection.weight.data.clone()\n        else:\n            self.key_projection = None\n        if value_projection:\n            self.value_projection = Linear(embed_dim, embed_dim)\n        else:\n            self.value_projection = None\n\n        self.out_projection = Linear(embed_dim, conv_channels)\n        self.dropout = dropout\n        self.window_ahead = window_ahead\n        self.window_backward = window_backward\n\n    def forward(self, query, encoder_out, mask=None, last_attended=None):\n        keys, values = encoder_out\n        residual = query\n        if self.value_projection is not None:\n            values = self.value_projection(values)\n        # TODO: yes, this is inefficient\n        if self.key_projection is not None:\n            keys = self.key_projection(keys.transpose(1, 2)).transpose(1, 2)\n\n        # attention\n        x = self.query_projection(query)\n        x = torch.bmm(x, keys)\n\n        mask_value = -float(""inf"")\n        if mask is not None:\n            mask = mask.view(query.size(0), 1, -1)\n            x.data.masked_fill_(mask, mask_value)\n\n        if last_attended is not None:\n            backward = last_attended - self.window_backward\n            if backward > 0:\n                x[:, :, :backward] = mask_value\n            ahead = last_attended + self.window_ahead\n            if ahead < x.size(-1):\n                x[:, :, ahead:] = mask_value\n\n        # softmax over last dim\n        # (B, tgt_len, src_len)\n        sz = x.size()\n        x = F.softmax(x.view(sz[0] * sz[1], sz[2]), dim=1)\n        x = x.view(sz)\n        attn_scores = x\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        x = torch.bmm(x, values)\n\n        # scale attention output\n        s = values.size(1)\n        x = x * (s * math.sqrt(1.0 / s))\n\n        # project back\n        x = self.out_projection(x)\n        x = (x + residual) * math.sqrt(0.5)\n        return x, attn_scores\n\n\nclass Decoder(nn.Module):\n    def __init__(self, embed_dim, n_speakers, speaker_embed_dim,\n                 in_dim=80, r=5,\n                 max_positions=512, padding_idx=None,\n                 preattention=((128, 5, 1),) * 4,\n                 convolutions=((128, 5, 1),) * 4,\n                 attention=True, dropout=0.1,\n                 use_memory_mask=False,\n                 force_monotonic_attention=False,\n                 query_position_rate=1.0,\n                 key_position_rate=1.29,\n                 window_ahead=3,\n                 window_backward=1,\n                 key_projection=True,\n                 value_projection=True,\n                 ):\n        super(Decoder, self).__init__()\n        self.dropout = dropout\n        self.in_dim = in_dim\n        self.r = r\n        self.query_position_rate = query_position_rate\n        self.key_position_rate = key_position_rate\n\n        in_channels = in_dim * r\n        if isinstance(attention, bool):\n            # expand True into [True, True, ...] and do the same with False\n            attention = [attention] * len(convolutions)\n\n        # Position encodings for query (decoder states) and keys (encoder states)\n        self.embed_query_positions = SinusoidalEncoding(\n            max_positions, convolutions[0][0])\n        self.embed_keys_positions = SinusoidalEncoding(\n            max_positions, embed_dim)\n        # Used for compute multiplier for positional encodings\n        if n_speakers > 1:\n            self.speaker_proj1 = Linear(speaker_embed_dim, 1, dropout=dropout)\n            self.speaker_proj2 = Linear(speaker_embed_dim, 1, dropout=dropout)\n        else:\n            self.speaker_proj1, self.speaker_proj2 = None, None\n\n        # Prenet: causal convolution blocks\n        self.preattention = nn.ModuleList()\n        in_channels = in_dim * r\n        std_mul = 1.0\n        for out_channels, kernel_size, dilation in preattention:\n            if in_channels != out_channels:\n                # Conv1d + ReLU\n                self.preattention.append(\n                    Conv1d(in_channels, out_channels, kernel_size=1, padding=0,\n                           dilation=1, std_mul=std_mul))\n                self.preattention.append(nn.ReLU(inplace=True))\n                in_channels = out_channels\n                std_mul = 2.0\n            self.preattention.append(\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, out_channels, kernel_size, causal=True,\n                          dilation=dilation, dropout=dropout, std_mul=std_mul,\n                          residual=True))\n            in_channels = out_channels\n            std_mul = 4.0\n\n        # Causal convolution blocks + attention layers\n        self.convolutions = nn.ModuleList()\n        self.attention = nn.ModuleList()\n\n        for i, (out_channels, kernel_size, dilation) in enumerate(convolutions):\n            assert in_channels == out_channels\n            self.convolutions.append(\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, out_channels, kernel_size, causal=True,\n                          dilation=dilation, dropout=dropout, std_mul=std_mul,\n                          residual=False))\n            self.attention.append(\n                AttentionLayer(out_channels, embed_dim,\n                               dropout=dropout,\n                               window_ahead=window_ahead,\n                               window_backward=window_backward,\n                               key_projection=key_projection,\n                               value_projection=value_projection)\n                if attention[i] else None)\n            in_channels = out_channels\n            std_mul = 4.0\n        # Last 1x1 convolution\n        self.last_conv = Conv1d(in_channels, in_dim * r, kernel_size=1,\n                                padding=0, dilation=1, std_mul=std_mul,\n                                dropout=dropout)\n\n        # Mel-spectrogram (before sigmoid) -> Done binary flag\n        self.fc = Linear(in_dim * r, 1)\n\n        self.max_decoder_steps = 200\n        self.min_decoder_steps = 10\n        self.use_memory_mask = use_memory_mask\n        if isinstance(force_monotonic_attention, bool):\n            self.force_monotonic_attention = [force_monotonic_attention] * len(convolutions)\n        else:\n            self.force_monotonic_attention = force_monotonic_attention\n\n    def forward(self, encoder_out, inputs=None,\n                text_positions=None, frame_positions=None,\n                speaker_embed=None, lengths=None):\n        if inputs is None:\n            assert text_positions is not None\n            self.start_fresh_sequence()\n            outputs = self.incremental_forward(encoder_out, text_positions, speaker_embed)\n            return outputs\n\n        # Grouping multiple frames if necessary\n        if inputs.size(-1) == self.in_dim:\n            inputs = inputs.view(inputs.size(0), inputs.size(1) // self.r, -1)\n        assert inputs.size(-1) == self.in_dim * self.r\n\n        # expand speaker embedding for all time steps\n        speaker_embed_btc = expand_speaker_embed(inputs, speaker_embed)\n        if speaker_embed_btc is not None:\n            speaker_embed_btc = F.dropout(speaker_embed_btc, p=self.dropout, training=self.training)\n\n        keys, values = encoder_out\n\n        if self.use_memory_mask and lengths is not None:\n            mask = get_mask_from_lengths(keys, lengths)\n        else:\n            mask = None\n\n        # position encodings\n        if text_positions is not None:\n            w = self.key_position_rate\n            # TODO: may be useful to have projection per attention layer\n            if self.speaker_proj1 is not None:\n                w = w * torch.sigmoid(self.speaker_proj1(speaker_embed)).view(-1)\n            text_pos_embed = self.embed_keys_positions(text_positions, w)\n            keys = keys + text_pos_embed\n        if frame_positions is not None:\n            w = self.query_position_rate\n            if self.speaker_proj2 is not None:\n                w = w * torch.sigmoid(self.speaker_proj2(speaker_embed)).view(-1)\n            frame_pos_embed = self.embed_query_positions(frame_positions, w)\n\n        # transpose only once to speed up attention layers\n        keys = keys.transpose(1, 2).contiguous()\n\n        x = inputs\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Generic case: B x T x C -> B x C x T\n        x = x.transpose(1, 2)\n\n        # Prenet\n        for f in self.preattention:\n            x = f(x, speaker_embed_btc) if isinstance(f, Conv1dGLU) else f(x)\n\n        # Casual convolutions + Multi-hop attentions\n        alignments = []\n        for f, attention in zip(self.convolutions, self.attention):\n            residual = x\n\n            x = f(x, speaker_embed_btc) if isinstance(f, Conv1dGLU) else f(x)\n\n            # Feed conv output to attention layer as query\n            if attention is not None:\n                assert isinstance(f, Conv1dGLU)\n                # (B x T x C)\n                x = x.transpose(1, 2)\n                x = x if frame_positions is None else x + frame_pos_embed\n                x, alignment = attention(x, (keys, values), mask=mask)\n                # (T x B x C)\n                x = x.transpose(1, 2)\n                alignments += [alignment]\n\n            if isinstance(f, Conv1dGLU):\n                x = (x + residual) * math.sqrt(0.5)\n\n        # decoder state (B x T x C):\n        # internal representation before compressed to output dimention\n        decoder_states = x.transpose(1, 2).contiguous()\n        x = self.last_conv(x)\n\n        # Back to B x T x C\n        x = x.transpose(1, 2)\n\n        # project to mel-spectorgram\n        outputs = torch.sigmoid(x)\n\n        # Done flag\n        done = torch.sigmoid(self.fc(x))\n\n        return outputs, torch.stack(alignments), done, decoder_states\n\n    def incremental_forward(self, encoder_out, text_positions, speaker_embed=None,\n                            initial_input=None, test_inputs=None):\n        keys, values = encoder_out\n        B = keys.size(0)\n\n        # position encodings\n        w = self.key_position_rate\n        # TODO: may be useful to have projection per attention layer\n        if self.speaker_proj1 is not None:\n            w = w * torch.sigmoid(self.speaker_proj1(speaker_embed)).view(-1)\n        text_pos_embed = self.embed_keys_positions(text_positions, w)\n        keys = keys + text_pos_embed\n\n        # transpose only once to speed up attention layers\n        keys = keys.transpose(1, 2).contiguous()\n\n        decoder_states = []\n        outputs = []\n        alignments = []\n        dones = []\n        # intially set to zeros\n        last_attended = [None] * len(self.attention)\n        for idx, v in enumerate(self.force_monotonic_attention):\n            last_attended[idx] = 0 if v else None\n\n        num_attention_layers = sum([layer is not None for layer in self.attention])\n        t = 0\n        if initial_input is None:\n            initial_input = keys.data.new(B, 1, self.in_dim * self.r).zero_()\n        current_input = initial_input\n        while True:\n            # frame pos start with 1.\n            frame_pos = keys.data.new(B, 1).fill_(t + 1).long()\n            w = self.query_position_rate\n            if self.speaker_proj2 is not None:\n                w = w * torch.sigmoid(self.speaker_proj2(speaker_embed)).view(-1)\n            frame_pos_embed = self.embed_query_positions(frame_pos, w)\n\n            if test_inputs is not None:\n                if t >= test_inputs.size(1):\n                    break\n                current_input = test_inputs[:, t, :].unsqueeze(1)\n            else:\n                if t > 0:\n                    current_input = outputs[-1]\n            x = current_input\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n            # Prenet\n            for f in self.preattention:\n                if isinstance(f, Conv1dGLU):\n                    x = f.incremental_forward(x, speaker_embed)\n                else:\n                    try:\n                        x = f.incremental_forward(x)\n                    except AttributeError as e:\n                        x = f(x)\n\n            # Casual convolutions + Multi-hop attentions\n            ave_alignment = None\n            for idx, (f, attention) in enumerate(zip(self.convolutions,\n                                                     self.attention)):\n                residual = x\n                if isinstance(f, Conv1dGLU):\n                    x = f.incremental_forward(x, speaker_embed)\n                else:\n                    try:\n                        x = f.incremental_forward(x)\n                    except AttributeError as e:\n                        x = f(x)\n\n                # attention\n                if attention is not None:\n                    assert isinstance(f, Conv1dGLU)\n                    x = x + frame_pos_embed\n                    x, alignment = attention(x, (keys, values),\n                                             last_attended=last_attended[idx])\n                    if self.force_monotonic_attention[idx]:\n                        last_attended[idx] = alignment.max(-1)[1].view(-1).data[0]\n                    if ave_alignment is None:\n                        ave_alignment = alignment\n                    else:\n                        ave_alignment = ave_alignment + ave_alignment\n\n                # residual\n                if isinstance(f, Conv1dGLU):\n                    x = (x + residual) * math.sqrt(0.5)\n\n            decoder_state = x\n            x = self.last_conv.incremental_forward(x)\n            ave_alignment = ave_alignment.div_(num_attention_layers)\n\n            # Ooutput & done flag predictions\n            output = torch.sigmoid(x)\n            done = torch.sigmoid(self.fc(x))\n\n            decoder_states += [decoder_state]\n            outputs += [output]\n            alignments += [ave_alignment]\n            dones += [done]\n\n            t += 1\n            if test_inputs is None:\n                if (done > 0.5).all() and t > self.min_decoder_steps:\n                    break\n                elif t > self.max_decoder_steps:\n                    break\n\n        # Remove 1-element time axis\n        alignments = list(map(lambda x: x.squeeze(1), alignments))\n        decoder_states = list(map(lambda x: x.squeeze(1), decoder_states))\n        outputs = list(map(lambda x: x.squeeze(1), outputs))\n\n        # Combine outputs for all time steps\n        alignments = torch.stack(alignments).transpose(0, 1)\n        decoder_states = torch.stack(decoder_states).transpose(0, 1).contiguous()\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n\n        return outputs, alignments, dones, decoder_states\n\n    def start_fresh_sequence(self):\n        _clear_modules(self.preattention)\n        _clear_modules(self.convolutions)\n        self.last_conv.clear_buffer()\n\n\ndef _clear_modules(modules):\n    for m in modules:\n        try:\n            m.clear_buffer()\n        except AttributeError as e:\n            pass\n\n\nclass Converter(nn.Module):\n    def __init__(self, n_speakers, speaker_embed_dim,\n                 in_dim, out_dim, convolutions=((256, 5, 1),) * 4,\n                 time_upsampling=1,\n                 dropout=0.1):\n        super(Converter, self).__init__()\n        self.dropout = dropout\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.n_speakers = n_speakers\n\n        # Non causual convolution blocks\n        in_channels = convolutions[0][0]\n        # Idea from nyanko\n        if time_upsampling == 4:\n            self.convolutions = nn.ModuleList([\n                Conv1d(in_dim, in_channels, kernel_size=1, padding=0, dilation=1,\n                       std_mul=1.0),\n                ConvTranspose1d(in_channels, in_channels, kernel_size=2,\n                                padding=0, stride=2, std_mul=1.0),\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, in_channels, kernel_size=3, causal=False,\n                          dilation=1, dropout=dropout, std_mul=1.0, residual=True),\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, in_channels, kernel_size=3, causal=False,\n                          dilation=3, dropout=dropout, std_mul=4.0, residual=True),\n                ConvTranspose1d(in_channels, in_channels, kernel_size=2,\n                                padding=0, stride=2, std_mul=4.0),\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, in_channels, kernel_size=3, causal=False,\n                          dilation=1, dropout=dropout, std_mul=1.0, residual=True),\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, in_channels, kernel_size=3, causal=False,\n                          dilation=3, dropout=dropout, std_mul=4.0, residual=True),\n            ])\n        elif time_upsampling == 2:\n            self.convolutions = nn.ModuleList([\n                Conv1d(in_dim, in_channels, kernel_size=1, padding=0, dilation=1,\n                       std_mul=1.0),\n                ConvTranspose1d(in_channels, in_channels, kernel_size=2,\n                                padding=0, stride=2, std_mul=1.0),\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, in_channels, kernel_size=3, causal=False,\n                          dilation=1, dropout=dropout, std_mul=1.0, residual=True),\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, in_channels, kernel_size=3, causal=False,\n                          dilation=3, dropout=dropout, std_mul=4.0, residual=True),\n            ])\n        elif time_upsampling == 1:\n            self.convolutions = nn.ModuleList([\n                # 1x1 convolution first\n                Conv1d(in_dim, in_channels, kernel_size=1, padding=0, dilation=1,\n                       std_mul=1.0),\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, in_channels, kernel_size=3, causal=False,\n                          dilation=3, dropout=dropout, std_mul=4.0, residual=True),\n            ])\n        else:\n            raise ValueError(""Not supported"")\n\n        std_mul = 4.0\n        for (out_channels, kernel_size, dilation) in convolutions:\n            if in_channels != out_channels:\n                self.convolutions.append(\n                    Conv1d(in_channels, out_channels, kernel_size=1, padding=0,\n                           dilation=1, std_mul=std_mul))\n                self.convolutions.append(nn.ReLU(inplace=True))\n                in_channels = out_channels\n                std_mul = 2.0\n            self.convolutions.append(\n                Conv1dGLU(n_speakers, speaker_embed_dim,\n                          in_channels, out_channels, kernel_size, causal=False,\n                          dilation=dilation, dropout=dropout, std_mul=std_mul,\n                          residual=True))\n            in_channels = out_channels\n            std_mul = 4.0\n        # Last 1x1 convolution\n        self.convolutions.append(Conv1d(in_channels, out_dim, kernel_size=1,\n                                        padding=0, dilation=1, std_mul=std_mul,\n                                        dropout=dropout))\n\n    def forward(self, x, speaker_embed=None):\n        assert self.n_speakers == 1 or speaker_embed is not None\n\n        # expand speaker embedding for all time steps\n        speaker_embed_btc = expand_speaker_embed(x, speaker_embed)\n        if speaker_embed_btc is not None:\n            speaker_embed_btc = F.dropout(speaker_embed_btc, p=self.dropout, training=self.training)\n\n        # Generic case: B x T x C -> B x C x T\n        x = x.transpose(1, 2)\n\n        for f in self.convolutions:\n            # Case for upsampling\n            if speaker_embed_btc is not None and speaker_embed_btc.size(1) != x.size(-1):\n                speaker_embed_btc = expand_speaker_embed(x, speaker_embed, tdim=-1)\n                speaker_embed_btc = F.dropout(\n                    speaker_embed_btc, p=self.dropout, training=self.training)\n            x = f(x, speaker_embed_btc) if isinstance(f, Conv1dGLU) else f(x)\n\n        # Back to B x T x C\n        x = x.transpose(1, 2)\n\n        return torch.sigmoid(x)\n'"
deepvoice3_pytorch/modules.py,11,"b'# coding: utf-8\n\nimport torch\nfrom torch import nn\nimport math\nimport numpy as np\nfrom torch.nn import functional as F\n\n\ndef position_encoding_init(n_position, d_pos_vec, position_rate=1.0,\n                           sinusoidal=True):\n    \'\'\' Init the sinusoid position encoding table \'\'\'\n\n    # keep dim 0 for padding token position encoding zero vector\n    position_enc = np.array([\n        [position_rate * pos / np.power(10000, 2 * (i // 2) / d_pos_vec) for i in range(d_pos_vec)]\n        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n\n    position_enc = torch.from_numpy(position_enc).float()\n    if sinusoidal:\n        position_enc[1:, 0::2] = torch.sin(position_enc[1:, 0::2])  # dim 2i\n        position_enc[1:, 1::2] = torch.cos(position_enc[1:, 1::2])  # dim 2i+1\n\n    return position_enc\n\n\ndef sinusoidal_encode(x, w):\n    y = w * x\n    y[1:, 0::2] = torch.sin(y[1:, 0::2].clone())\n    y[1:, 1::2] = torch.cos(y[1:, 1::2].clone())\n    return y\n\n\nclass SinusoidalEncoding(nn.Embedding):\n\n    def __init__(self, num_embeddings, embedding_dim,\n                 *args, **kwargs):\n        super(SinusoidalEncoding, self).__init__(num_embeddings, embedding_dim,\n                                                 padding_idx=0,\n                                                 *args, **kwargs)\n        self.weight.data = position_encoding_init(num_embeddings, embedding_dim,\n                                                  position_rate=1.0,\n                                                  sinusoidal=False)\n\n    def forward(self, x, w=1.0):\n        isscaler = np.isscalar(w)\n        assert self.padding_idx is not None\n\n        if isscaler or w.size(0) == 1:\n            weight = sinusoidal_encode(self.weight, w)\n            return F.embedding(\n                x, weight, self.padding_idx, self.max_norm,\n                self.norm_type, self.scale_grad_by_freq, self.sparse)\n        else:\n            # TODO: cannot simply apply for batch\n            # better to implement efficient function\n            pe = []\n            for batch_idx, we in enumerate(w):\n                weight = sinusoidal_encode(self.weight, we)\n                pe.append(F.embedding(\n                    x[batch_idx], weight, self.padding_idx, self.max_norm,\n                    self.norm_type, self.scale_grad_by_freq, self.sparse))\n            pe = torch.stack(pe)\n            return pe\n\n\nclass GradMultiply(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, scale):\n        ctx.scale = scale\n        res = x.new(x)\n        ctx.mark_shared_storage((x, res))\n        return res\n\n    @staticmethod\n    def backward(ctx, grad):\n        return grad * ctx.scale, None\n\n\ndef Linear(in_features, out_features, dropout=0):\n    """"""Weight-normalized Linear layer (input: N x T x C)""""""\n    m = nn.Linear(in_features, out_features)\n    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n    m.bias.data.zero_()\n    return nn.utils.weight_norm(m)\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx, std=0.01):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    m.weight.data.normal_(0, std)\n    return m\n\n\ndef Conv1d(in_channels, out_channels, kernel_size, dropout=0, std_mul=4.0, **kwargs):\n    from .conv import Conv1d\n    m = Conv1d(in_channels, out_channels, kernel_size, **kwargs)\n    std = math.sqrt((std_mul * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n    m.weight.data.normal_(mean=0, std=std)\n    m.bias.data.zero_()\n    return nn.utils.weight_norm(m)\n\n\ndef ConvTranspose1d(in_channels, out_channels, kernel_size, dropout=0,\n                    std_mul=1.0, **kwargs):\n    m = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, **kwargs)\n    std = math.sqrt((std_mul * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))\n    m.weight.data.normal_(mean=0, std=std)\n    m.bias.data.zero_()\n    return nn.utils.weight_norm(m)\n\n\nclass Conv1dGLU(nn.Module):\n    """"""(Dilated) Conv1d + Gated linear unit + (optionally) speaker embedding\n    """"""\n\n    def __init__(self, n_speakers, speaker_embed_dim,\n                 in_channels, out_channels, kernel_size,\n                 dropout, padding=None, dilation=1, causal=False, residual=False,\n                 *args, **kwargs):\n        super(Conv1dGLU, self).__init__()\n        self.dropout = dropout\n        self.residual = residual\n        if padding is None:\n            # no future time stamps available\n            if causal:\n                padding = (kernel_size - 1) * dilation\n            else:\n                padding = (kernel_size - 1) // 2 * dilation\n        self.causal = causal\n\n        self.conv = Conv1d(in_channels, 2 * out_channels, kernel_size,\n                           dropout=dropout, padding=padding, dilation=dilation,\n                           *args, **kwargs)\n        if n_speakers > 1:\n            self.speaker_proj = Linear(speaker_embed_dim, out_channels)\n        else:\n            self.speaker_proj = None\n\n    def forward(self, x, speaker_embed=None):\n        return self._forward(x, speaker_embed, False)\n\n    def incremental_forward(self, x, speaker_embed=None):\n        return self._forward(x, speaker_embed, True)\n\n    def _forward(self, x, speaker_embed, is_incremental):\n        residual = x\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        if is_incremental:\n            splitdim = -1\n            x = self.conv.incremental_forward(x)\n        else:\n            splitdim = 1\n            x = self.conv(x)\n            # remove future time steps\n            x = x[:, :, :residual.size(-1)] if self.causal else x\n\n        a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n        if self.speaker_proj is not None:\n            softsign = F.softsign(self.speaker_proj(speaker_embed))\n            # Since conv layer assumes BCT, we need to transpose\n            softsign = softsign if is_incremental else softsign.transpose(1, 2)\n            a = a + softsign\n        x = a * torch.sigmoid(b)\n        return (x + residual) * math.sqrt(0.5) if self.residual else x\n\n    def clear_buffer(self):\n        self.conv.clear_buffer()\n\n\nclass HighwayConv1d(nn.Module):\n    """"""Weight normzlized Conv1d + Highway network (support incremental forward)\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size=1, padding=None,\n                 dilation=1, causal=False, dropout=0, std_mul=None, glu=False):\n        super(HighwayConv1d, self).__init__()\n        if std_mul is None:\n            std_mul = 4.0 if glu else 1.0\n        if padding is None:\n            # no future time stamps available\n            if causal:\n                padding = (kernel_size - 1) * dilation\n            else:\n                padding = (kernel_size - 1) // 2 * dilation\n        self.causal = causal\n        self.dropout = dropout\n        self.glu = glu\n\n        self.conv = Conv1d(in_channels, 2 * out_channels,\n                           kernel_size=kernel_size, padding=padding,\n                           dilation=dilation, dropout=dropout,\n                           std_mul=std_mul)\n\n    def forward(self, x):\n        return self._forward(x, False)\n\n    def incremental_forward(self, x):\n        return self._forward(x, True)\n\n    def _forward(self, x, is_incremental):\n        """"""Forward\n\n        Args:\n            x: (B, in_channels, T)\n        returns:\n            (B, out_channels, T)\n        """"""\n\n        residual = x\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        if is_incremental:\n            splitdim = -1\n            x = self.conv.incremental_forward(x)\n        else:\n            splitdim = 1\n            x = self.conv(x)\n            # remove future time steps\n            x = x[:, :, :residual.size(-1)] if self.causal else x\n\n        if self.glu:\n            x = F.glu(x, dim=splitdim)\n            return (x + residual) * math.sqrt(0.5)\n        else:\n            a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n            T = torch.sigmoid(b)\n            return (T * a + (1 - T) * residual)\n\n    def clear_buffer(self):\n        self.conv.clear_buffer()\n\n\ndef get_mask_from_lengths(memory, memory_lengths):\n    """"""Get mask tensor from list of length\n    Args:\n        memory: (batch, max_time, dim)\n        memory_lengths: array like\n    """"""\n    max_len = max(memory_lengths)\n    mask = torch.arange(max_len).expand(memory.size(0), max_len) < torch.tensor(memory_lengths).unsqueeze(-1)\n    mask = mask.to(memory.device)\n    return ~mask\n'"
deepvoice3_pytorch/nyanko.py,10,"b'# coding: utf-8\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\nimport numpy as np\n\nfrom .modules import Embedding, Linear, Conv1d, ConvTranspose1d\nfrom .modules import HighwayConv1d, get_mask_from_lengths\nfrom .modules import position_encoding_init\nfrom .deepvoice3 import AttentionLayer\n\n\nclass Encoder(nn.Module):\n    def __init__(self, n_vocab, embed_dim, channels, kernel_size=3,\n                 n_speakers=1, speaker_embed_dim=16, embedding_weight_std=0.01,\n                 padding_idx=None, dropout=0.1):\n        super(Encoder, self).__init__()\n        self.dropout = dropout\n\n        # Text input embeddings\n        self.embed_tokens = Embedding(\n            n_vocab, embed_dim, padding_idx, embedding_weight_std)\n\n        E = embed_dim\n        D = channels\n        self.convnet = nn.Sequential(\n            Conv1d(E, 2 * D, kernel_size=1, padding=0, dilation=1, std_mul=1.0),\n            nn.ReLU(inplace=True),\n            Conv1d(2 * D, 2 * D, kernel_size=1, padding=0, dilation=1, std_mul=2.0),\n\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=3, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=9, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=27, std_mul=1.0, dropout=dropout),\n\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=3, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=9, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=27, std_mul=1.0, dropout=dropout),\n\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(2 * D, 2 * D, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n\n            HighwayConv1d(2 * D, 2 * D, kernel_size=1, padding=0,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n        )\n\n    def forward(self, text_sequences, text_positions=None, lengths=None,\n                speaker_embed=None):\n        # embed text_sequences\n        # (B, T, E)\n        x = self.embed_tokens(text_sequences)\n\n        x = self.convnet(x.transpose(1, 2)).transpose(1, 2)\n\n        # (B, T, D) and (B, T, D)\n        keys, values = x.split(x.size(-1) // 2, dim=-1)\n\n        return keys, values\n\n\nclass Decoder(nn.Module):\n    def __init__(self, embed_dim, in_dim=80, r=5, channels=256, kernel_size=3,\n                 n_speakers=1, speaker_embed_dim=16,\n                 max_positions=512, padding_idx=None,\n                 dropout=0.1,\n                 use_memory_mask=False,\n                 force_monotonic_attention=False,\n                 query_position_rate=1.0,\n                 key_position_rate=1.29,\n                 window_ahead=3,\n                 window_backward=1,\n                 key_projection=False,\n                 value_projection=False,\n                 ):\n        super(Decoder, self).__init__()\n        self.dropout = dropout\n        self.in_dim = in_dim\n        self.r = r\n\n        D = channels\n        F = in_dim * r  # should be r = 1 to replicate\n        self.audio_encoder_modules = nn.ModuleList([\n            Conv1d(F, D, kernel_size=1, padding=0, dilation=1, std_mul=1.0),\n            nn.ReLU(inplace=True),\n            Conv1d(D, D, kernel_size=1, padding=0, dilation=1, std_mul=2.0),\n            nn.ReLU(inplace=True),\n            Conv1d(D, D, kernel_size=1, padding=0, dilation=1, std_mul=2.0),\n\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=1, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=3, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=9, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=27, causal=True, std_mul=1.0, dropout=dropout),\n\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=1, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=3, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=9, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=27, causal=True, std_mul=1.0, dropout=dropout),\n\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=3, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=3, causal=True, std_mul=1.0, dropout=dropout),\n        ])\n\n        self.attention = AttentionLayer(D, D, dropout=dropout,\n                                        window_ahead=window_ahead,\n                                        window_backward=window_backward,\n                                        key_projection=key_projection,\n                                        value_projection=value_projection)\n\n        self.audio_decoder_modules = nn.ModuleList([\n            Conv1d(2 * D, D, kernel_size=1, padding=0, dilation=1, std_mul=1.0),\n\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=1, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=3, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=9, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=27, causal=True, std_mul=1.0, dropout=dropout),\n\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=1, causal=True, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(D, D, kernel_size=kernel_size, padding=None,\n                          dilation=1, causal=True, std_mul=1.0, dropout=dropout),\n\n            Conv1d(D, D, kernel_size=1, padding=0, dilation=1, std_mul=1.0),\n            nn.ReLU(inplace=True),\n            Conv1d(D, D, kernel_size=1, padding=0, dilation=1, std_mul=2.0),\n            nn.ReLU(inplace=True),\n            Conv1d(D, D, kernel_size=1, padding=0, dilation=1, std_mul=2.0),\n            nn.ReLU(inplace=True),\n        ])\n        self.last_conv = Conv1d(D, F, kernel_size=1, padding=0, dilation=1, std_mul=2.0)\n\n        # Done prediction\n        self.fc = Linear(F, 1)\n\n        # Position encodings for query (decoder states) and keys (encoder states)\n        self.embed_query_positions = Embedding(\n            max_positions, D, padding_idx)\n        self.embed_query_positions.weight.data = position_encoding_init(\n            max_positions, D, position_rate=query_position_rate, sinusoidal=True)\n        self.embed_keys_positions = Embedding(\n            max_positions, D, padding_idx)\n        self.embed_keys_positions.weight.data = position_encoding_init(\n            max_positions, D, position_rate=key_position_rate, sinusoidal=True)\n\n        # options\n        self.max_decoder_steps = 200\n        self.min_decoder_steps = 10\n        self.use_memory_mask = use_memory_mask\n        self.force_monotonic_attention = force_monotonic_attention\n\n    def forward(self, encoder_out, inputs=None,\n                text_positions=None, frame_positions=None,\n                speaker_embed=None, lengths=None):\n\n        if inputs is None:\n            assert text_positions is not None\n            self.start_fresh_sequence()\n            outputs = self.incremental_forward(encoder_out, text_positions)\n            return outputs\n\n        # Grouping multiple frames if necessary\n        if inputs.size(-1) == self.in_dim:\n            inputs = inputs.view(inputs.size(0), inputs.size(1) // self.r, -1)\n        assert inputs.size(-1) == self.in_dim * self.r\n\n        keys, values = encoder_out\n\n        if self.use_memory_mask and lengths is not None:\n            mask = get_mask_from_lengths(keys, lengths)\n        else:\n            mask = None\n\n        # position encodings\n        if text_positions is not None:\n            text_pos_embed = self.embed_keys_positions(text_positions)\n            keys = keys + text_pos_embed\n        if frame_positions is not None:\n            frame_pos_embed = self.embed_query_positions(frame_positions)\n\n        # transpose only once to speed up attention layers\n        keys = keys.transpose(1, 2).contiguous()\n\n        # (B, T, C)\n        x = inputs\n\n        # (B, C, T)\n        x = x.transpose(1, 2)\n\n        # Apply audio encoder\n        for f in self.audio_encoder_modules:\n            x = f(x)\n        Q = x\n\n        # Attention modules assume query as (B, T, C)\n        x = x.transpose(1, 2)\n        x = x if frame_positions is None else x + frame_pos_embed\n        R, alignments = self.attention(x, (keys, values), mask=mask)\n        R = R.transpose(1, 2)\n\n        # (B, C*2, T)\n        Rd = torch.cat((R, Q), dim=1)\n        x = Rd\n\n        # Apply audio decoder\n        for f in self.audio_decoder_modules:\n            x = f(x)\n        decoder_states = x.transpose(1, 2).contiguous()\n        x = self.last_conv(x)\n\n        # (B, T, C)\n        x = x.transpose(1, 2)\n\n        # Mel\n        outputs = torch.sigmoid(x)\n\n        # Done prediction\n        done = torch.sigmoid(self.fc(x))\n\n        # Adding extra dim for convenient\n        alignments = alignments.unsqueeze(0)\n\n        return outputs, alignments, done, decoder_states\n\n    def incremental_forward(self, encoder_out, text_positions,\n                            initial_input=None, test_inputs=None):\n        keys, values = encoder_out\n        B = keys.size(0)\n\n        # position encodings\n        if text_positions is not None:\n            text_pos_embed = self.embed_keys_positions(text_positions)\n            keys = keys + text_pos_embed\n\n        # transpose only once to speed up attention layers\n        keys = keys.transpose(1, 2).contiguous()\n\n        decoder_states = []\n        outputs = []\n        alignments = []\n        dones = []\n        # intially set to zeros\n        last_attended = 0 if self.force_monotonic_attention else None\n\n        t = 0\n        if initial_input is None:\n            initial_input = keys.data.new(B, 1, self.in_dim * self.r).zero_()\n        current_input = initial_input\n        while True:\n            # frame pos start with 1.\n            frame_pos = keys.data.new(B, 1).fill_(t + 1).long()\n            frame_pos_embed = self.embed_query_positions(frame_pos)\n\n            if test_inputs is not None:\n                if t >= test_inputs.size(1):\n                    break\n                current_input = test_inputs[:, t, :].unsqueeze(1)\n            else:\n                if t > 0:\n                    current_input = outputs[-1]\n\n            # (B, 1, C)\n            x = current_input\n\n            for f in self.audio_encoder_modules:\n                try:\n                    x = f.incremental_forward(x)\n                except AttributeError as e:\n                    x = f(x)\n            Q = x\n\n            R, alignment = self.attention(\n                x + frame_pos_embed, (keys, values), last_attended=last_attended)\n            if self.force_monotonic_attention:\n                last_attended = alignment.max(-1)[1].view(-1).data[0]\n\n            Rd = torch.cat((R, Q), dim=-1)\n            x = Rd\n            for f in self.audio_decoder_modules:\n                try:\n                    x = f.incremental_forward(x)\n                except AttributeError as e:\n                    x = f(x)\n            decoder_state = x\n            x = self.last_conv.incremental_forward(x)\n\n            # Ooutput & done flag predictions\n            output = torch.sigmoid(x)\n            done = torch.sigmoid(self.fc(x))\n\n            decoder_states += [decoder_state]\n            outputs += [output]\n            alignments += [alignment]\n            dones += [done]\n\n            t += 1\n            if test_inputs is None:\n                if (done > 0.5).all() and t > self.min_decoder_steps:\n                    break\n                elif t > self.max_decoder_steps:\n                    break\n\n        # Remove 1-element time axis\n        alignments = list(map(lambda x: x.squeeze(1), alignments))\n        decoder_states = list(map(lambda x: x.squeeze(1), decoder_states))\n        outputs = list(map(lambda x: x.squeeze(1), outputs))\n\n        # Combine outputs for all time steps\n        alignments = torch.stack(alignments).transpose(0, 1)\n        decoder_states = torch.stack(decoder_states).transpose(0, 1).contiguous()\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n\n        return outputs, alignments, dones, decoder_states\n\n    def start_fresh_sequence(self):\n        _clear_modules(self.audio_encoder_modules)\n        _clear_modules(self.audio_decoder_modules)\n        self.last_conv.clear_buffer()\n\n\ndef _clear_modules(modules):\n    for m in modules:\n        try:\n            m.clear_buffer()\n        except AttributeError as e:\n            pass\n\n\nclass Converter(nn.Module):\n    def __init__(self, in_dim, out_dim, channels=512,  kernel_size=3, dropout=0.1):\n        super(Converter, self).__init__()\n        self.dropout = dropout\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n\n        F = in_dim\n        Fd = out_dim\n        C = channels\n        self.convnet = nn.Sequential(\n            Conv1d(F, C, kernel_size=1, padding=0, dilation=1, std_mul=1.0),\n\n            HighwayConv1d(C, C, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(C, C, kernel_size=kernel_size, padding=None,\n                          dilation=3, std_mul=1.0, dropout=dropout),\n\n            ConvTranspose1d(C, C, kernel_size=2, padding=0, stride=2, std_mul=1.0),\n            HighwayConv1d(C, C, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(C, C, kernel_size=kernel_size, padding=None,\n                          dilation=3, std_mul=1.0, dropout=dropout),\n            ConvTranspose1d(C, C, kernel_size=2, padding=0, stride=2, std_mul=1.0),\n            HighwayConv1d(C, C, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(C, C, kernel_size=kernel_size, padding=None,\n                          dilation=3, std_mul=1.0, dropout=dropout),\n\n            Conv1d(C, 2 * C, kernel_size=1, padding=0, dilation=1, std_mul=1.0),\n\n            HighwayConv1d(2 * C, 2 * C, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n            HighwayConv1d(2 * C, 2 * C, kernel_size=kernel_size, padding=None,\n                          dilation=1, std_mul=1.0, dropout=dropout),\n\n            Conv1d(2 * C, Fd, kernel_size=1, padding=0, dilation=1, std_mul=1.0),\n\n            Conv1d(Fd, Fd, kernel_size=1, padding=0, dilation=1, std_mul=1.0),\n            nn.ReLU(inplace=True),\n            Conv1d(Fd, Fd, kernel_size=1, padding=0, dilation=1, std_mul=2.0),\n            nn.ReLU(inplace=True),\n\n            Conv1d(Fd, Fd, kernel_size=1, padding=0, dilation=1, std_mul=2.0),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, speaker_embed=None):\n        return self.convnet(x.transpose(1, 2)).transpose(1, 2)\n'"
nikl_preprocess/prepare_metafile.py,0,"b'from __future__ import print_function\nimport subprocess,os,re\n\ndef pwrap(args, shell=False):\n    p = subprocess.Popen(args, shell=shell, stdout=subprocess.PIPE,\n                         stdin=subprocess.PIPE, stderr=subprocess.PIPE,\n                         universal_newlines=True)\n    return p\n\ndef execute(cmd, shell=False):\n    popen = pwrap(cmd, shell=shell)\n    for stdout_line in iter(popen.stdout.readline, """"):\n        yield stdout_line\n\n    popen.stdout.close()\n    return_code = popen.wait()\n    if return_code:\n        raise subprocess.CalledProcessError(return_code, cmd)\n\ndef pe(cmd, shell=False):\n    """"""\n    Print and execute command on system\n    """"""\n    ret = []\n    for line in execute(cmd, shell=shell):\n        ret.append(line)\n        print(line, end="""")\n    return ret\n\n\nif __name__ == ""__main__"":\n  import argparse\n  parser = argparse.ArgumentParser(description=""Produce metafile where wav file path and its transcription are aligned"",\n                                   epilog=""Example usage: python preprea_metadata $HOME/copora/NIKL"")\n  parser.add_argument(""--corpus_dir"", ""-c"",\n                      help=""filepath for the root directory of corpus"",\n                      required=True)\n\n  parser.add_argument(""--trans_file"", ""-t"",\n                      help=""Extracted transcription file obatained from extract_trans.py"",\n                      required=True)\n\n  parser.add_argument(""--spk_id"", ""-sid"",\n                      help=""Speaker ID for single speaker such as fv01"",\n                      required=False)\n  args = parser.parse_args()\n\n  print(""Prepare metadata file for all speakers"")\n  pe(""find %s -name %s | grep -v \'Bad\\|Non\\|Invalid\' > %s/wav.lst"" % (args.corpus_dir,""*.wav"",args.corpus_dir),shell=True)\n\n  trans={}\n  with open(args.trans_file,""r"") as f:\n    for line in f:\n      line = line.rstrip()\n      line_split = line.split("" "")\n      trans[line_split[0]] = "" "".join(line_split[1:])\n\n  with open(args.corpus_dir+""/wav.lst"", ""r"") as f:\n    wavfiles = f.readlines()\n\n  pe(""rm -f %s/metadata.txt"" % (args.corpus_dir),shell=True)\n  for w in wavfiles:\n    w = w.rstrip()\n    tid = re.search(r\'(t[0-9][0-9]_s[0-9][0-9])\',w)\n    if tid:\n      tid_found = tid.group(1)\n      pe(\'echo %s""|""%s >> %s/metadata.txt\' % (w,trans.get(tid_found),args.corpus_dir),shell=True)\n\n  print(""Metadata files is created in %s/metadata.txt"" % (args.corpus_dir))\n  pe(""ls -d -- %s/*/ | grep -v \'Bad\\|Non\\|Invalid\' | rev | cut -d\'/\' -f2 | rev > %s/speaker.mid"" % (args.corpus_dir,args.corpus_dir),shell=True)\n  pe(""head -n 1 %s/speaker.mid > %s/speaker.sid"" % (args.corpus_dir,args.corpus_dir),shell=True)\n'"
tests/test_audio.py,0,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport sys\nfrom os.path import dirname, join\nsys.path.insert(0, join(dirname(__file__), ""..""))\n\nimport numpy as np\nfrom nose.plugins.attrib import attr\n\nimport logging\nlogging.getLogger(\'tensorflow\').disabled = True\n\n\n@attr(""local_only"")\ndef test_amp_to_db():\n    import audio\n    x = np.random.rand(10)\n    x_hat = audio._db_to_amp(audio._amp_to_db(x))\n    assert np.allclose(x, x_hat)\n'"
tests/test_conv.py,4,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom deepvoice3_pytorch.conv import Conv1d\n\n\ndef test_conv1d_incremental():\n    def __test(kernel_size, dilation, T, B, C, causual=True):\n        dilation = (dilation,)\n\n        # dilation = (4,)\n        # causual\n        assert causual\n        if causual:\n            padding = (kernel_size - 1) * dilation[0]\n        else:\n            padding = (kernel_size - 1) // 2 * dilation[0]\n\n        # weight: (Cout, Cin, K)\n        conv = nn.Conv1d(\n            C, C * 2, kernel_size=kernel_size, padding=padding,\n            dilation=dilation).eval()\n        conv.weight.data.fill_(1.0)\n        conv.bias.data.zero_()\n\n        # weight: (K, Cin, Cout)\n        # weight (linearized): (Cout*K, Cin)\n        conv_online = Conv1d(\n            C, C * 2, kernel_size=kernel_size, padding=padding,\n            dilation=dilation).eval()\n        conv_online.weight.data.fill_(1.0)\n        conv_online.bias.data.zero_()\n\n        # (B, C, T)\n        bct = torch.zeros(B, C, T) + torch.arange(0, T).float()\n        output_conv = conv(bct)\n\n        # Remove future time stamps\n        output_conv = output_conv[:, :, :T]\n\n        output_conv_online = []\n\n        # B, T, C\n        btc = bct.transpose(1, 2).contiguous()\n        for t in range(btc.size(1)):\n            input = btc[:, t, :].contiguous().view(B, -1, C)\n            output = conv_online.incremental_forward(input)\n            output_conv_online += [output]\n\n        output_conv_online = torch.stack(output_conv_online).squeeze(2)\n        output_conv_online = output_conv_online.transpose(0, 1).transpose(1, 2)\n\n        assert (output_conv == output_conv_online).all()\n\n    for B in [1, 4]:\n        for T in [5, 10]:\n            for C in [1, 2, 4]:\n                for kernel_size in [2, 3]:\n                    for dilation in [1, 2, 3, 4, 5, 9, 27]:\n                        yield __test, kernel_size, dilation, T, B, C\n'"
tests/test_deepvoice3.py,24,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport sys\nfrom os.path import dirname, join, exists\n\nfrom deepvoice3_pytorch.frontend.en import text_to_sequence, n_vocab\n\nimport torch\nfrom torch import nn\nimport numpy as np\n\nfrom nose.plugins.attrib import attr\n\nfrom deepvoice3_pytorch.builder import deepvoice3\nfrom deepvoice3_pytorch import MultiSpeakerTTSModel, AttentionSeq2Seq\n\n\nuse_cuda = torch.cuda.is_available() and False\ntorch.backends.cudnn.deterministic = True\nnum_mels = 80\nnum_freq = 513\noutputs_per_step = 4\npadding_idx = 0\n\n\ndef _get_model(n_speakers=1, speaker_embed_dim=None,\n               force_monotonic_attention=False,\n               use_decoder_state_for_postnet_input=False, use_memory_mask=False):\n    model = deepvoice3(n_vocab=n_vocab,\n                       embed_dim=32,\n                       mel_dim=num_mels,\n                       linear_dim=num_freq,\n                       r=outputs_per_step,\n                       padding_idx=padding_idx,\n                       n_speakers=n_speakers,\n                       speaker_embed_dim=speaker_embed_dim,\n                       dropout=1 - 0.95,\n                       kernel_size=5,\n                       encoder_channels=16,\n                       decoder_channels=32,\n                       converter_channels=32,\n                       force_monotonic_attention=force_monotonic_attention,\n                       use_decoder_state_for_postnet_input=use_decoder_state_for_postnet_input,\n                       use_memory_mask=use_memory_mask,\n                       )\n    return model\n\n\ndef _pad(seq, max_len):\n    return np.pad(seq, (0, max_len - len(seq)),\n                  mode=\'constant\', constant_values=0)\n\n\ndef _test_data():\n    texts = [""Thank you very much."", ""Hello."", ""Deep voice 3.""]\n    seqs = [np.array(text_to_sequence(t), dtype=np.int) for t in texts]\n    input_lengths = np.array([len(s) for s in seqs])\n    max_len = np.max(input_lengths)\n    seqs = np.array([_pad(s, max_len) for s in seqs])\n\n    # Test encoder\n    x = torch.LongTensor(seqs)\n    y = torch.rand(x.size(0), 12, 80)\n\n    return x, y, input_lengths\n\n\ndef _deepvoice3(n_vocab, embed_dim=256, mel_dim=80,\n                linear_dim=4096, r=5,\n                n_speakers=1, speaker_embed_dim=16,\n                padding_idx=None,\n                dropout=(1 - 0.95), dilation=1):\n\n    from deepvoice3_pytorch.deepvoice3 import Encoder, Decoder, Converter\n    h = 128\n    encoder = Encoder(\n        n_vocab, embed_dim, padding_idx=padding_idx,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        dropout=dropout,\n        convolutions=[(h, 3, dilation), (h, 3, dilation), (h, 3, dilation),\n                      (h, 3, dilation), (h, 3, dilation)],\n    )\n\n    h = 256\n    decoder = Decoder(\n        embed_dim, in_dim=mel_dim, r=r, padding_idx=padding_idx,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n        dropout=dropout,\n        preattention=[(h, 3, 1)],\n        convolutions=[(h, 3, dilation), (h, 3, dilation), (h, 3, dilation),\n                      (h, 3, dilation), (h, 3, dilation)],\n        attention=[True, False, False, False, True],\n        force_monotonic_attention=False)\n\n    seq2seq = AttentionSeq2Seq(encoder, decoder)\n\n    in_dim = mel_dim\n    h = 256\n    converter = Converter(n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n                          in_dim=in_dim, out_dim=linear_dim, dropout=dropout,\n                          convolutions=[(h, 3, dilation), (h, 3, dilation), (h, 3, dilation),\n                                        (h, 3, dilation), (h, 3, dilation)])\n\n    model = MultiSpeakerTTSModel(\n        seq2seq, converter, padding_idx=padding_idx,\n        mel_dim=mel_dim, linear_dim=linear_dim,\n        n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim)\n\n    return model\n\n\ndef test_single_speaker_deepvoice3():\n    x, y, lengths = _test_data()\n\n    for v in [False, True]:\n        model = _get_model(use_decoder_state_for_postnet_input=v)\n        mel_outputs, linear_outputs, alignments, done = model(x, y, input_lengths=lengths)\n\n    model = _get_model(use_memory_mask=True)\n    mel_outputs, linear_outputs, alignments, done = model(x, y, input_lengths=lengths)\n\n\ndef _pad_2d(x, max_len, b_pad=0):\n    x = np.pad(x, [(b_pad, max_len - len(x) - b_pad), (0, 0)],\n               mode=""constant"", constant_values=0)\n    return x\n\n\ndef test_multi_speaker_deepvoice3():\n    texts = [""Thank you very much."", ""Hello."", ""Deep voice 3.""]\n    seqs = [np.array(text_to_sequence(t), dtype=np.int) for t in texts]\n    input_lengths = np.array([len(s) for s in seqs])\n    max_len = np.max(input_lengths)\n    seqs = np.array([_pad(s, max_len) for s in seqs])\n\n    # Test encoder\n    x = torch.LongTensor(seqs)\n    y = torch.rand(x.size(0), 4 * 33, 80)\n    model = _get_model(n_speakers=32, speaker_embed_dim=16)\n    speaker_ids = torch.LongTensor([1, 2, 3])\n\n    mel_outputs, linear_outputs, alignments, done = model(x, y, speaker_ids=speaker_ids)\n    print(""Input text:"", x.size())\n    print(""Input mel:"", y.size())\n    print(""Mel:"", mel_outputs.size())\n    print(""Linear:"", linear_outputs.size())\n    print(""Alignments:"", alignments.size())\n    print(""Done:"", done.size())\n\n\n@attr(""issue38"")\ndef test_incremental_path_multiple_times():\n    texts = [""they discarded this for a more completely Roman and far less beautiful letter.""]\n    seqs = np.array([text_to_sequence(t) for t in texts])\n    text_positions = np.arange(1, len(seqs[0]) + 1).reshape(1, len(seqs[0]))\n\n    r = 4\n    mel_dim = 80\n    sequence = torch.LongTensor(seqs)\n    text_positions = torch.LongTensor(text_positions)\n\n    for model, speaker_ids in [\n            (_get_model(force_monotonic_attention=False), None),\n            (_get_model(force_monotonic_attention=False, n_speakers=32, speaker_embed_dim=16), torch.LongTensor([1]))]:\n        model.eval()\n\n        # first call\n        mel_outputs, linear_outputs, alignments, done = model(\n            sequence, text_positions=text_positions, speaker_ids=speaker_ids)\n\n        # second call\n        mel_outputs2, linear_outputs2, alignments2, done2 = model(\n            sequence, text_positions=text_positions, speaker_ids=speaker_ids)\n\n        # Should get same result\n        c = (mel_outputs - mel_outputs2).abs()\n        print(c.mean(), c.max())\n\n        assert np.allclose(mel_outputs.cpu().data.numpy(),\n                           mel_outputs2.cpu().data.numpy(), atol=1e-5)\n\n\ndef test_incremental_correctness():\n    texts = [""they discarded this for a more completely Roman and far less beautiful letter.""]\n    seqs = np.array([text_to_sequence(t) for t in texts])\n    text_positions = np.arange(1, len(seqs[0]) + 1).reshape(1, len(seqs[0]))\n\n    mel_path = join(dirname(__file__), ""data"", ""ljspeech-mel-00001.npy"")\n    mel = np.load(mel_path)\n    max_target_len = mel.shape[0]\n    r = 4\n    mel_dim = 80\n    if max_target_len % r != 0:\n        max_target_len += r - max_target_len % r\n        assert max_target_len % r == 0\n    mel = _pad_2d(mel, max_target_len)\n    mel = torch.from_numpy(mel)\n    mel_reshaped = mel.contiguous().view(1, -1, mel_dim * r)\n    frame_positions = np.arange(1, mel_reshaped.size(1) + 1).reshape(1, mel_reshaped.size(1))\n\n    x = torch.LongTensor(seqs)\n    text_positions = torch.LongTensor(text_positions)\n    frame_positions = torch.LongTensor(frame_positions)\n\n    for model, speaker_ids in [\n            (_get_model(force_monotonic_attention=False), None),\n            (_get_model(force_monotonic_attention=False, n_speakers=32, speaker_embed_dim=16), torch.LongTensor([1]))]:\n        model.eval()\n\n        if speaker_ids is not None:\n            speaker_embed = model.embed_speakers(speaker_ids)\n        else:\n            speaker_embed = None\n\n        # Encoder\n        encoder_outs = model.seq2seq.encoder(x, speaker_embed=speaker_embed)\n\n        # Off line decoding\n        mel_outputs_offline, alignments_offline, done, _ = model.seq2seq.decoder(\n            encoder_outs, mel_reshaped, speaker_embed=speaker_embed,\n            text_positions=text_positions, frame_positions=frame_positions)\n\n        # Online decoding with test inputs\n        model.seq2seq.decoder.start_fresh_sequence()\n        mel_outputs_online, alignments, dones_online, _ = model.seq2seq.decoder.incremental_forward(\n            encoder_outs, text_positions, speaker_embed=speaker_embed,\n            test_inputs=mel_reshaped)\n\n        # Should get same result\n        c = (mel_outputs_offline - mel_outputs_online).abs()\n        print(c.mean(), c.max())\n\n        assert np.allclose(mel_outputs_offline.cpu().data.numpy(),\n                           mel_outputs_online.cpu().data.numpy(), atol=1e-5)\n\n\n@attr(""local_only"")\ndef test_incremental_forward():\n    checkpoint_path = join(dirname(__file__), ""../test_whole/checkpoint_step000265000.pth"")\n    if not exists(checkpoint_path):\n        return\n    model = _get_model()\n\n    use_cuda = False\n\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[""state_dict""])\n    model.make_generation_fast_()\n    model = model.cuda() if use_cuda else model\n\n    texts = [""they discarded this for a more completely Roman and far less beautiful letter.""]\n    seqs = np.array([text_to_sequence(t) for t in texts])\n    input_lengths = [len(s) for s in seqs]\n\n    use_manual_padding = False\n    if use_manual_padding:\n        max_input_len = np.max(input_lengths) + 10  # manuall padding\n        seqs = np.array([_pad(x, max_input_len) for x in seqs], dtype=np.int)\n        input_lengths = torch.LongTensor(input_lengths)\n        input_lengths = input_lengths.cuda() if use_cuda else input_lengths\n    else:\n        input_lengths = None\n\n    text_positions = np.arange(1, len(seqs[0]) + 1).reshape(1, len(seqs[0]))\n\n    mel = np.load(""/home/ryuichi/Dropbox/sp/deepvoice3_pytorch/data/ljspeech/ljspeech-mel-00035.npy"")\n    max_target_len = mel.shape[0]\n    r = 4\n    mel_dim = 80\n    if max_target_len % r != 0:\n        max_target_len += r - max_target_len % r\n        assert max_target_len % r == 0\n    mel = _pad_2d(mel, max_target_len)\n    mel = torch.from_numpy(mel)\n    mel_reshaped = mel.contiguous().view(1, -1, mel_dim * r)\n\n    frame_positions = np.arange(1, mel_reshaped.size(1) + 1).reshape(1, mel_reshaped.size(1))\n\n    x = torch.LongTensor(seqs)\n    text_positions = torch.LongTensor(text_positions)\n    frame_positions = torch.LongTensor(frame_positions)\n\n    if use_cuda:\n        x = x.cuda()\n        text_positions = text_positions.cuda()\n        frame_positions = frame_positions.cuda()\n        mel_reshaped = mel_reshaped.cuda()\n\n    model.eval()\n\n    def _plot(mel, mel_predicted, alignments):\n        from matplotlib import pylab as plt\n        plt.figure(figsize=(16, 10))\n        plt.subplot(3, 1, 1)\n        plt.imshow(mel.data.cpu().numpy().T, origin=""lower bottom"",\n                   aspect=""auto"", cmap=""magma"")\n        plt.colorbar()\n\n        plt.subplot(3, 1, 2)\n        plt.imshow(mel_predicted.view(-1, mel_dim).data.cpu().numpy().T,\n                   origin=""lower bottom"", aspect=""auto"", cmap=""magma"")\n        plt.colorbar()\n\n        plt.subplot(3, 1, 3)\n        if alignments.dim() == 4:\n            alignments = alignments.mean(0)\n        plt.imshow(alignments[0].data.cpu(\n        ).numpy().T, origin=""lower bottom"", aspect=""auto"")\n        plt.colorbar()\n        plt.show()\n\n    # Encoder\n    encoder_outs = model.seq2seq.encoder(x, lengths=input_lengths)\n\n    # Off line decoding\n    mel_output_offline, alignments_offline, done = model.seq2seq.decoder(\n        encoder_outs, mel_reshaped,\n        text_positions=text_positions, frame_positions=frame_positions,\n        lengths=input_lengths)\n\n    _plot(mel, mel_output_offline, alignments_offline)\n\n    # Online decoding\n    test_inputs = None\n    # test_inputs = mel_reshaped\n    model.seq2seq.decoder.start_fresh_sequence()\n    mel_outputs, alignments, dones_online = model.seq2seq.decoder.incremental_forward(\n        encoder_outs, text_positions,\n        # initial_input=mel_reshaped[:, :1, :],\n        test_inputs=test_inputs)\n\n    if test_inputs is not None:\n        c = (mel_output_offline - mel_outputs).abs()\n        print(c.mean(), c.max())\n        _plot(mel, c, alignments)\n\n    _plot(mel, mel_outputs, alignments)\n'"
tests/test_embedding.py,2,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport torch\nfrom torch import nn\nfrom deepvoice3_pytorch.modules import SinusoidalEncoding, position_encoding_init\nimport numpy as np\n\n\ndef test_sinusoidal():\n    num_embedding = 512\n    embedding_dim = 128\n\n    for w in [1.0, 0.5, 2.0, 10.0, 20.0]:\n        a = nn.Embedding(num_embedding, embedding_dim, padding_idx=0)\n        a.weight.data = position_encoding_init(\n            num_embedding, embedding_dim, position_rate=w)\n\n        b = SinusoidalEncoding(num_embedding, embedding_dim)\n\n        x = torch.arange(0, 128).long()\n        ax = a(x).data.numpy()\n        bx = b(x, w).data.numpy()\n\n        print(w, np.abs(ax - bx).mean())\n        try:\n            assert np.allclose(ax, bx)\n        except:\n            print(""TODO: has little numerical errors?"")\n            assert np.abs(ax - bx).mean() < 1e-5\n'"
tests/test_frontend.py,0,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nfrom deepvoice3_pytorch import frontend\nfrom nose.plugins.attrib import attr\n\neos = 1\n\n\ndef test_en():\n    f = getattr(frontend, ""en"")\n    seq = f.text_to_sequence(""hello world."")\n    assert seq[-1] == eos\n    t = f.sequence_to_text(seq)\n    assert t == ""hello world.~""\n\n\n@attr(""local_only"")\ndef test_ja():\n    f = getattr(frontend, ""jp"")\n    seq = f.text_to_sequence(""\xe3\x81\x93\xe3\x82\x93\xe3\x81\xab\xe3\x81\xa1\xe3\x82\x8f"")\n    assert seq[-1] == eos\n    t = f.sequence_to_text(seq)\n    assert t[:-1] == ""\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x8b\xe3\x83\x81\xe3\x83\xaf\xe3\x80\x82""\n\n\n@attr(""local_only"")\ndef test_en_lj():\n    f = getattr(frontend, ""en"")\n    from nnmnkwii.datasets import ljspeech\n    from tqdm import trange\n    import jaconv\n\n    d = ljspeech.TranscriptionDataSource(""/home/ryuichi/data/LJSpeech-1.0"")\n    texts = d.collect_files()\n\n    for p in [0.0, 0.5, 1.0]:\n        for idx in trange(len(texts)):\n            text = texts[idx]\n            seq = f.text_to_sequence(text, p=p)\n            assert seq[-1] == eos\n            t = f.sequence_to_text(seq)\n\n            if idx < 10:\n                print(""""""{0}: {1}\\n{0}: {2}\\n"""""".format(idx, text, t))\n\n\n@attr(""local_only"")\ndef test_ja_jsut():\n    f = getattr(frontend, ""jp"")\n    from nnmnkwii.datasets import jsut\n    from tqdm import trange\n    import jaconv\n\n    d = jsut.TranscriptionDataSource(""/home/ryuichi/data/jsut_ver1.1/"",\n                                     subsets=jsut.available_subsets)\n    texts = d.collect_files()\n\n    for p in [0.0, 0.5, 1.0]:\n        for idx in trange(len(texts)):\n            text = texts[idx]\n            seq = f.text_to_sequence(text, p=p)\n            assert seq[-1] == eos\n            t = f.sequence_to_text(seq)\n\n            if idx < 10:\n                print(""""""{0}: {1}\\n{0}: {2}\\n"""""".format(idx, text, t))\n'"
tests/test_nyanko.py,15,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport sys\nfrom os.path import dirname, join, exists\n\nfrom deepvoice3_pytorch.frontend.en import text_to_sequence, n_vocab\n\nimport torch\nfrom torch import nn\nimport numpy as np\n\nfrom nose.plugins.attrib import attr\n\nfrom deepvoice3_pytorch.builder import nyanko\nfrom deepvoice3_pytorch import MultiSpeakerTTSModel, AttentionSeq2Seq\n\nuse_cuda = torch.cuda.is_available() and False\nnum_mels = 80\nnum_freq = 513\noutputs_per_step = 4\npadding_idx = 0\n\n\ndef _pad(seq, max_len):\n    return np.pad(seq, (0, max_len - len(seq)),\n                  mode=\'constant\', constant_values=0)\n\n\ndef _test_data():\n    texts = [""Thank you very much."", ""Hello."", ""Deep voice 3.""]\n    seqs = [np.array(text_to_sequence(t), dtype=np.int) for t in texts]\n    input_lengths = np.array([len(s) for s in seqs])\n    max_len = np.max(input_lengths)\n    seqs = np.array([_pad(s, max_len) for s in seqs])\n\n    # Test encoder\n    x = torch.LongTensor(seqs)\n    y = torch.rand(x.size(0), 12, 80)\n\n    return x, y\n\n\ndef _pad_2d(x, max_len, b_pad=0):\n    x = np.pad(x, [(b_pad, max_len - len(x) - b_pad), (0, 0)],\n               mode=""constant"", constant_values=0)\n    return x\n\n\ndef test_nyanko_basics():\n    x, y = _test_data()\n\n    for v in [False, True]:\n        model = nyanko(n_vocab, mel_dim=num_mels, linear_dim=num_freq, r=1, downsample_step=4,\n                       use_decoder_state_for_postnet_input=v)\n        mel_outputs, linear_outputs, alignments, done = model(x, y)\n\n\n@attr(""issue38"")\ndef test_incremental_path_multiple_times():\n    texts = [""they discarded this for a more completely Roman and far less beautiful letter.""]\n    seqs = np.array([text_to_sequence(t) for t in texts])\n    text_positions = np.arange(1, len(seqs[0]) + 1).reshape(1, len(seqs[0]))\n\n    r = 1\n    mel_dim = 80\n\n    sequence = torch.LongTensor(seqs)\n    text_positions = torch.LongTensor(text_positions)\n\n    model = nyanko(n_vocab, mel_dim=mel_dim, linear_dim=513, downsample_step=4,\n                   r=r, force_monotonic_attention=False)\n    model.eval()\n\n    # first call\n    mel_outputs, linear_outputs, alignments, done = model(\n        sequence, text_positions=text_positions, speaker_ids=None)\n\n    # second call\n    mel_outputs2, linear_outputs2, alignments2, done2 = model(\n        sequence, text_positions=text_positions, speaker_ids=None)\n\n    # Should get same result\n    c = (mel_outputs - mel_outputs2).abs()\n    print(c.mean(), c.max())\n\n    assert np.allclose(mel_outputs.cpu().data.numpy(),\n                       mel_outputs2.cpu().data.numpy(), atol=1e-5)\n\n\ndef test_incremental_correctness():\n    texts = [""they discarded this for a more completely Roman and far less beautiful letter.""]\n    seqs = np.array([text_to_sequence(t) for t in texts])\n    text_positions = np.arange(1, len(seqs[0]) + 1).reshape(1, len(seqs[0]))\n\n    mel_path = join(dirname(__file__), ""data"", ""ljspeech-mel-00001.npy"")\n    mel = np.load(mel_path)[::4]\n    max_target_len = mel.shape[0]\n    r = 1\n    mel_dim = 80\n    if max_target_len % r != 0:\n        max_target_len += r - max_target_len % r\n        assert max_target_len % r == 0\n    mel = _pad_2d(mel, max_target_len)\n    mel = torch.from_numpy(mel)\n    mel_reshaped = mel.view(1, -1, mel_dim * r)\n    frame_positions = np.arange(1, mel_reshaped.size(1) + 1).reshape(1, mel_reshaped.size(1))\n\n    x = torch.LongTensor(seqs)\n    text_positions = torch.LongTensor(text_positions)\n    frame_positions = torch.LongTensor(frame_positions)\n\n    model = nyanko(n_vocab, mel_dim=mel_dim, linear_dim=513, downsample_step=4,\n                   r=r, force_monotonic_attention=False)\n    model.eval()\n\n    # Encoder\n    encoder_outs = model.seq2seq.encoder(x)\n\n    # Off line decoding\n    mel_outputs_offline, alignments_offline, done, _ = model.seq2seq.decoder(\n        encoder_outs, mel_reshaped,\n        text_positions=text_positions, frame_positions=frame_positions)\n\n    # Online decoding with test inputs\n    model.seq2seq.decoder.start_fresh_sequence()\n    mel_outputs_online, alignments, dones_online, _ = model.seq2seq.decoder.incremental_forward(\n        encoder_outs, text_positions,\n        test_inputs=mel_reshaped)\n\n    # Should get same result\n    assert np.allclose(mel_outputs_offline.cpu().data.numpy(),\n                       mel_outputs_online.cpu().data.numpy())\n\n\n@attr(""local_only"")\ndef test_nyanko():\n    texts = [""they discarded this for a more completely Roman and far less beautiful letter.""]\n    seqs = np.array([text_to_sequence(t) for t in texts])\n    text_positions = np.arange(1, len(seqs[0]) + 1).reshape(1, len(seqs[0]))\n\n    mel = np.load(""/home/ryuichi/Dropbox/sp/deepvoice3_pytorch/data/ljspeech/ljspeech-mel-00035.npy"")\n    max_target_len = mel.shape[0]\n    r = 1\n    mel_dim = 80\n    if max_target_len % r != 0:\n        max_target_len += r - max_target_len % r\n        assert max_target_len % r == 0\n    mel = _pad_2d(mel, max_target_len)\n    mel = torch.from_numpy(mel)\n    mel_reshaped = mel.view(1, -1, mel_dim * r)\n    frame_positions = np.arange(1, mel_reshaped.size(1) + 1).reshape(1, mel_reshaped.size(1))\n\n    x = torch.LongTensor(seqs)\n    text_positions = torch.LongTensor(text_positions)\n    frame_positions = torch.LongTensor(frame_positions)\n\n    model = nyanko(n_vocab, mel_dim=mel_dim, linear_dim=513, downsample_step=4,\n                   r=r, force_monotonic_attention=False)\n    model.eval()\n\n    def _plot(mel, mel_predicted, alignments):\n        from matplotlib import pylab as plt\n        plt.figure(figsize=(16, 10))\n        plt.subplot(3, 1, 1)\n        plt.imshow(mel.data.cpu().numpy().T, origin=""lower bottom"", aspect=""auto"", cmap=""magma"")\n        plt.colorbar()\n\n        plt.subplot(3, 1, 2)\n        plt.imshow(mel_predicted.view(-1, mel_dim).data.cpu().numpy().T,\n                   origin=""lower bottom"", aspect=""auto"", cmap=""magma"")\n        plt.colorbar()\n\n        plt.subplot(3, 1, 3)\n        if alignments.dim() == 4:\n            alignments = alignments.mean(0)\n        plt.imshow(alignments[0].data.cpu(\n        ).numpy().T, origin=""lower bottom"", aspect=""auto"")\n        plt.colorbar()\n        plt.show()\n\n    seq2seq = model.seq2seq\n\n    # Encoder\n    encoder_outs = seq2seq.encoder(x)\n\n    # Off line decoding\n    print(""Offline decoding"")\n    mel_outputs_offline, alignments_offline, done, _ = seq2seq.decoder(\n        encoder_outs, mel_reshaped,\n        text_positions=text_positions, frame_positions=frame_positions)\n\n    _plot(mel, mel_outputs_offline, alignments_offline)\n\n    # Online decoding with test inputs\n    print(""Online decoding"")\n    seq2seq.decoder.start_fresh_sequence()\n    mel_outputs_online, alignments, dones_online, _ = seq2seq.decoder.incremental_forward(\n        encoder_outs, text_positions,\n        test_inputs=mel_reshaped)\n\n    a = mel_outputs_offline.cpu().data.numpy()\n    b = mel_outputs_online.cpu().data.numpy()\n    c = (mel_outputs_offline - mel_outputs_online).abs()\n    print(c.mean(), c.max())\n\n    _plot(mel, mel_outputs_offline, alignments_offline)\n    _plot(mel, mel_outputs_online, alignments)\n    _plot(mel, c, alignments)\n\n    # Should get same result\n    assert np.allclose(a, b)\n\n    postnet = model.postnet\n\n    linear_outputs = postnet(mel_outputs_offline)\n    print(linear_outputs.size())\n'"
vctk_preprocess/extract_feats.py,0,"b'from __future__ import print_function\nimport os\nimport shutil\nimport stat\nimport subprocess\nimport time\nimport numpy as np\nfrom scipy.io import wavfile\nimport re\nimport glob\n\n# File to extract features (mostly) automatically using the merlin speech\n# pipeline\n# example tts_env.sh file , written out by installer script install_tts.py\n# https://gist.github.com/kastnerkyle/001a58a58d090658ee5350cb6129f857\n""""""\nexport ESTDIR=/Tmp/kastner/speech_synthesis/speech_tools/\nexport FESTDIR=/Tmp/kastner/speech_synthesis/festival/\nexport FESTVOXDIR=/Tmp/kastner/speech_synthesis/festvox/\nexport VCTKDIR=/Tmp/kastner/vctk/VCTK-Corpus/\nexport HTKDIR=/Tmp/kastner/speech_synthesis/htk/\nexport SPTKDIR=/Tmp/kastner/speech_synthesis/SPTK-3.9/\nexport HTSENGINEDIR=/Tmp/kastner/speech_synthesis/hts_engine_API-1.10/\nexport HTSDEMODIR=/Tmp/kastner/speech_synthesis/HTS-demo_CMU-ARCTIC-SLT/\nexport HTSPATCHDIR=/Tmp/kastner/speech_synthesis/HTS-2.3_for_HTL-3.4.1/\nexport MERLINDIR=/Tmp/kastner/speech_synthesis/latest_features/merlin/\n""""""\n\n# Not currently needed...\n\n\ndef subfolder_select(subfolders):\n    r = [sf for sf in subfolders if sf == ""p294""]\n    if len(r) == 0:\n        raise ValueError(""Error: subfolder_select failed"")\n    return r\n\n# Need to edit the conf...\n\n\ndef replace_conflines(conf, match, sub, replace_line=""%s: %s\\n""):\n    replace = None\n    for n, l in enumerate(conf):\n        if l[:len(match)] == match:\n            replace = n\n            break\n    conf[replace] = replace_line % (match, sub)\n    return conf\n\n\ndef replace_write(fpath, match, sub, replace_line=""%s: %s\\n""):\n    with open(fpath, ""r"") as f:\n        conf = f.readlines()\n    conf = replace_conflines(conf, match, sub, replace_line=replace_line)\n\n    with open(fpath, ""w"") as f:\n        f.writelines(conf)\n\n\ndef copytree(src, dst, symlinks=False, ignore=None):\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n        shutil.copystat(src, dst)\n    lst = os.listdir(src)\n    if ignore:\n        excl = ignore(src, lst)\n        lst = [x for x in lst if x not in excl]\n    for item in lst:\n        s = os.path.join(src, item)\n        d = os.path.join(dst, item)\n        if symlinks and os.path.islink(s):\n            if os.path.lexists(d):\n                os.remove(d)\n            os.symlink(os.readlink(s), d)\n            try:\n                st = os.lstat(s)\n                mode = stat.S_IMODE(st.st_mode)\n                os.lchmod(d, mode)\n            except:\n                pass  # lchmod not available\n        elif os.path.isdir(s):\n            copytree(s, d, symlinks, ignore)\n        else:\n            shutil.copy2(s, d)\n\n# Convenience function to reuse the defined env\n\n\ndef pwrap(args, shell=False):\n    p = subprocess.Popen(args, shell=shell, stdout=subprocess.PIPE,\n                         stdin=subprocess.PIPE, stderr=subprocess.PIPE,\n                         universal_newlines=True)\n    return p\n\n# Print output\n# http://stackoverflow.com/questions/4417546/constantly-print-subprocess-output-while-process-is-running\n\n\ndef execute(cmd, shell=False):\n    popen = pwrap(cmd, shell=shell)\n    for stdout_line in iter(popen.stdout.readline, """"):\n        yield stdout_line\n\n    popen.stdout.close()\n    return_code = popen.wait()\n    if return_code:\n        raise subprocess.CalledProcessError(return_code, cmd)\n\n\ndef pe(cmd, shell=False):\n    """"""\n    Print and execute command on system\n    """"""\n    ret = []\n    for line in execute(cmd, shell=shell):\n        ret.append(line)\n        print(line, end="""")\n    return ret\n\n\n# from merlin\ndef load_binary_file(file_name, dimension):\n    fid_lab = open(file_name, \'rb\')\n    features = np.fromfile(fid_lab, dtype=np.float32)\n    fid_lab.close()\n    assert features.size % float(\n        dimension) == 0.0, \'specified dimension %s not compatible with data\' % (dimension)\n    features = features[:(dimension * (features.size / dimension))]\n    features = features.reshape((-1, dimension))\n    return features\n\n\ndef array_to_binary_file(data, output_file_name):\n    data = np.array(data, \'float32\')\n    fid = open(output_file_name, \'wb\')\n    data.tofile(fid)\n    fid.close()\n\n\ndef load_binary_file_frame(file_name, dimension):\n    fid_lab = open(file_name, \'rb\')\n    features = np.fromfile(fid_lab, dtype=np.float32)\n    fid_lab.close()\n    assert features.size % float(\n        dimension) == 0.0, \'specified dimension %s not compatible with data\' % (dimension)\n    frame_number = features.size / dimension\n    features = features[:(dimension * frame_number)]\n    features = features.reshape((-1, dimension))\n    return features, frame_number\n\n\n# Source the tts_env_script\nenv_script = ""tts_env.sh""\nif os.path.isfile(env_script):\n    command = \'env -i bash -c ""source %s && env""\' % env_script\n    for line in execute(command, shell=True):\n        key, value = line.split(""="")\n        # remove newline\n        value = value.strip()\n        os.environ[key] = value\nelse:\n    raise IOError(""Cannot find file %s"" % env_script)\n\nfestdir = os.environ[""FESTDIR""]\nfestvoxdir = os.environ[""FESTVOXDIR""]\nestdir = os.environ[""ESTDIR""]\nsptkdir = os.environ[""SPTKDIR""]\n# generalize to more than VCTK when this is done...\n\nvctkdir = os.environ[""VCTKDIR""]\nhtkdir = os.environ[""HTKDIR""]\nmerlindir = os.environ[""MERLINDIR""]\n\n\ndef extract_intermediate_features(wav_path, txt_path, keep_silences=False,\n                                  full_features=False, ehmm_max_n_itr=1):\n    basedir = os.getcwd()\n    latest_feature_dir = ""latest_features""\n    if not os.path.exists(latest_feature_dir):\n        os.mkdir(latest_feature_dir)\n\n    os.chdir(latest_feature_dir)\n    latest_feature_dir = os.getcwd()\n\n    if not os.path.exists(""merlin""):\n        clone_cmd = ""git clone https://github.com/kastnerkyle/merlin""\n        pe(clone_cmd, shell=True)\n\n    if keep_silences:\n        # REMOVE SILENCES TO MATCH JOSE PREPROC\n        os.chdir(""merlin/src"")\n        pe(""sed -i.bak -e \'708,712d;\' run_merlin.py"", shell=True)\n        pe(""sed -i.bak -e \'695,706d;\' run_merlin.py"", shell=True)\n        os.chdir(latest_feature_dir)\n\n    os.chdir(""merlin"")\n    merlin_dir = os.getcwd()\n    os.chdir(""egs/build_your_own_voice/s1"")\n    experiment_dir = os.getcwd()\n\n    if not os.path.exists(""database""):\n        print(""Creating database and copying in files"")\n        pe(""bash -x 01_setup.sh my_new_voice 2>&1"", shell=True)\n\n        # Copy in wav files\n        wav_partial_path = wav_path  # vctkdir + ""wav48/""\n        """"""\n        subfolders = sorted(os.listdir(wav_partial_path))\n        # only p294 for now...\n        subfolders = subfolder_select(subfolders)\n        os.chdir(""database/wav"")\n        for sf in subfolders:\n            wav_path = wav_partial_path + sf + ""/*.wav""\n            pe(""cp %s ."" % wav_path, shell=True)\n        """"""\n        to_copy = os.listdir(wav_partial_path)\n        if len([tc for tc in to_copy if tc[-4:] == "".wav""]) == 0:\n            raise IOError(\n                ""Unable to find any wav files in %s, make sure the filenames end in .wav!"" % wav_partial_path)\n        os.chdir(""database/wav"")\n        if wav_partial_path[-1] != ""/"":\n            wav_partial_path = wav_partial_path + ""/""\n        wav_match_path = wav_partial_path + ""*.wav""\n        for fi in glob.glob(wav_match_path):\n            pe(""echo %s; cp %s ."" % (fi, fi), shell=True)\n        # THIS MAY FAIL IF TOO MANY WAV FILES\n        # pe(""cp %s ."" % wav_match_path, shell=True)\n        for f in os.listdir("".""):\n            # This is only necessary because of corrupted files...\n            fs, d = wavfile.read(f)\n            wavfile.write(f, fs, d)\n\n        # downsample the files\n        get_sr_cmd = \'file `ls *.wav | head -n 1` | cut -d "" "" -f 12\'\n        sr = pe(get_sr_cmd, shell=True)\n        sr_int = int(sr[0].strip())\n        print(""Got samplerate {}, converting to 16000"".format(sr_int))\n        # was assuming all were 48000\n        convert = estdir + \\\n            ""bin/ch_wave $i -o tmp_$i -itype wav -otype wav -F 16000 -f {}"".format(sr_int)\n        pe(""for i in *.wav; do echo %s; %s; mv tmp_$i $i; done"" % (convert, convert), shell=True)\n\n        os.chdir(experiment_dir)\n        txt_partial_path = txt_path  # vctkdir + ""txt/""\n        """"""\n        subfolders = sorted(os.listdir(txt_partial_path))\n        # only p294 for now...\n        subfolders = subfolder_select(subfolders)\n        os.chdir(""database/txt"")\n        for sf in subfolders:\n            txt_path = txt_partial_path + sf + ""/*.txt""\n            pe(""cp %s ."" % txt_path, shell=True)\n        """"""\n        os.chdir(""database/txt"")\n        to_copy = os.listdir(txt_partial_path)\n        if len([tc for tc in to_copy if tc[-4:] == "".txt""]) == 0:\n            raise IOError(\n                ""Unable to find any txt files in %s. Be sure the filenames end in .txt!"" % txt_partial_path)\n        txt_match_path = txt_partial_path + ""/*.txt""\n        for fi in glob.glob(txt_match_path):\n            # escape string...\n            fi = re.escape(fi)\n            try:\n                pe(""echo %s; cp %s ."" % (fi, fi), shell=True)\n            except:\n                from IPython import embed\n                embed()\n                raise ValueError()\n\n        #pe(""cp %s ."" % txt_match_path, shell=True)\n\n    do_state_align = False\n    if do_state_align:\n        raise ValueError(""Replace these lies with something that points at the right place"")\n        os.chdir(merlin_dir)\n        os.chdir(""misc/scripts/alignment/state_align"")\n        pe(""bash -x setup.sh 2>&1"", shell=True)\n\n        with open(""config.cfg"", ""r"") as f:\n            config_lines = f.readlines()\n\n        # replace FESTDIR with the correct path\n        festdir_replace_line = None\n        for n, l in enumerate(config_lines):\n            if ""FESTDIR="" in l:\n                festdir_replace_line = n\n                break\n\n        config_lines[festdir_replace_line] = ""FESTDIR=%s\\n"" % festdir\n\n        # replace HTKDIR with the correct path\n        htkdir_replace_line = None\n        for n, l in enumerate(config_lines):\n            if ""HTKDIR="" in l:\n                htkdir_replace_line = n\n                break\n\n        config_lines[htkdir_replace_line] = ""HTKDIR=%s\\n"" % htkdir\n\n        with open(""config.cfg"", ""w"") as f:\n            f.writelines(config_lines)\n\n        pe(""bash -x run_aligner.sh config.cfg 2>&1"", shell=True)\n    else:\n        os.chdir(merlin_dir)\n        if not os.path.exists(""misc/scripts/alignment/phone_align/full-context-labels/full""):\n            os.chdir(""misc/scripts/alignment/phone_align"")\n            pe(""bash -x setup.sh 2>&1"", shell=True)\n\n            with open(""config.cfg"", ""r"") as f:\n                config_lines = f.readlines()\n\n            # replace ESTDIR with the correct path\n            estdir_replace_line = None\n            for n, l in enumerate(config_lines):\n                if ""ESTDIR="" in l and l[0] == ""E"":\n                    estdir_replace_line = n\n                    break\n\n            config_lines[estdir_replace_line] = ""ESTDIR=%s\\n"" % estdir\n\n            # replace FESTDIR with the correct path\n            festdir_replace_line = None\n            for n, l in enumerate(config_lines):\n                # EST/FEST\n                if ""FESTDIR="" in l and l[0] == ""F"":\n                    festdir_replace_line = n\n                    break\n\n            config_lines[festdir_replace_line] = ""FESTDIR=%s\\n"" % festdir\n\n            # replace FESTVOXDIR with the correct path\n            festvoxdir_replace_line = None\n            for n, l in enumerate(config_lines):\n                if ""FESTVOXDIR="" in l:\n                    festvoxdir_replace_line = n\n                    break\n\n            config_lines[festvoxdir_replace_line] = ""FESTVOXDIR=%s\\n"" % festvoxdir\n\n            with open(""config.cfg"", ""w"") as f:\n                f.writelines(config_lines)\n\n            with open(""run_aligner.sh"", ""r"") as f:\n                run_aligner_lines = f.readlines()\n\n            replace_line = None\n            for n, l in enumerate(run_aligner_lines):\n                if ""cp ../cmuarctic.data"" in l:\n                    replace_line = n\n                    break\n\n            run_aligner_lines[replace_line] = ""cp ../txt.done.data etc/txt.done.data\\n""\n\n            # Make the txt.done.data file\n            def format_info_tup(info_tup):\n                return ""( "" + str(info_tup[0]) + \' ""\' + info_tup[1] + \'"" )\\n\'\n\n            # Now we need to get the text info\n            txt_partial_path = txt_path  # vctkdir + ""txt/""\n            cwd = os.getcwd()\n            out_path = ""txt.done.data""\n            out_file = open(out_path, ""w"")\n            """"""\n            subfolders = sorted(os.listdir(txt_partial_path))\n            # TODO: Avoid this truncation and have an option to select subfolder(s)...\n            subfolders = subfolder_select(subfolders)\n\n            txt_ids = []\n            for sf in subfolders:\n                print(""Processing subfolder %s"" % sf)\n                txt_sf_path = txt_partial_path + sf + ""/""\n                for txtpath in os.listdir(txt_sf_path):\n                    full_txtpath = txt_sf_path + txtpath\n                    with open(full_txtpath, \'r\') as f:\n                        r = f.readlines()\n                        assert len(r) == 1\n                        # remove txt extension\n                        name = txtpath.split(""."")[0]\n                        text = r[0].strip()\n                        info_tup = (name, text)\n                        txt_ids.append(name)\n                        out_file.writelines(format_info_tup(info_tup))\n            """"""\n            txt_ids = []\n            txt_l_path = txt_partial_path\n            for txtpath in os.listdir(txt_l_path):\n                print(""Processing %s"" % txtpath)\n                full_txtpath = txt_l_path + txtpath\n                name = txtpath.split(""."")[0]\n                wavpath_matches = [fname.split(""."")[0] for fname in os.listdir(wav_partial_path)\n                                   if name in fname]\n                for name in wavpath_matches:\n                    # Need an extra level here for pavoque :/\n                    with open(full_txtpath, \'r\') as f:\n                        r = f.readlines()\n                    if len(r) == 0:\n                        continue\n                    if len(r) != 1:\n                        new_r = []\n                        for ri in r:\n                            if ri != ""\\n"":\n                                new_r.append(ri)\n                        r = new_r\n                    if len(r) != 1:\n                        print(""Something wrong in text extraction, cowardly bailing to IPython"")\n                        from IPython import embed\n                        embed()\n                        raise ValueError()\n                    assert len(r) == 1\n                    # remove txt extension\n                    text = r[0].strip()\n                    info_tup = (name, text)\n                    txt_ids.append(name)\n                    out_file.writelines(format_info_tup(info_tup))\n            out_file.close()\n            pe(""cp %s %s/txt.done.data"" % (out_path, latest_feature_dir),\n               shell=True)\n            os.chdir(cwd)\n\n            replace_line = None\n            for n, l in enumerate(run_aligner_lines):\n                if ""cp ../slt_wav/*.wav"" in l:\n                    replace_line = n\n                    break\n\n            run_aligner_lines[replace_line] = ""cp ../wav/*.wav wav\\n""\n\n            # Put wav file in the correct place\n            wav_partial_path = experiment_dir + ""/database/wav""\n            """"""\n            subfolders = sorted(os.listdir(wav_partial_path))\n            """"""\n            if not os.path.exists(""wav""):\n                os.mkdir(""wav"")\n            cwd = os.getcwd()\n            os.chdir(""wav"")\n            """"""\n            for sf in subfolders:\n                wav_path = wav_partial_path + ""/*.wav""\n                pe(""cp %s ."" % wav_path, shell=True)\n            """"""\n            wav_match_path = wav_partial_path + ""/*.wav""\n            for fi in glob.glob(wav_match_path):\n                fi = re.escape(fi)\n                try:\n                    pe(""echo %s; cp %s ."" % (fi, fi), shell=True)\n                except:\n                    from IPython import embed\n                    embed()\n                    raise ValueError()\n                #pe(""echo %s; cp %s ."" % (fi, fi), shell=True)\n            #pe(""cp %s ."" % wav_match_path, shell=True)\n            os.chdir(cwd)\n\n            replace_line = None\n            for n, l in enumerate(run_aligner_lines):\n                if ""cat cmuarctic.data |"" in l:\n                    replace_line = n\n                    break\n\n            run_aligner_lines[replace_line] = \'cat txt.done.data | cut -d "" "" -f 2 > file_id_list.scp\\n\'\n\n            # FIXME\n            # Hackaround to avoid harcoded 30 in festivox do_ehmm\n            if not full_features:\n                bdir = os.getcwd()\n\n                # need to hack up run_aligner more..\n                # do setup manually\n                pe(""mkdir cmu_us_slt_arctic"", shell=True)\n                os.chdir(""cmu_us_slt_arctic"")\n\n                pe(""%s/src/clustergen/setup_cg cmu us slt_arctic"" % festvoxdir, shell=True)\n\n                pe(""cp ../txt.done.data etc/txt.done.data"", shell=True)\n                wmp = ""../wav/*.wav""\n                for fi in glob.glob(wmp):\n                    fi = re.escape(fi)\n                    try:\n                        pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)\n                    except:\n                        from IPython import embed\n                        embed()\n                        raise ValueError()\n                    #pe(""echo %s; cp %s wav/"" % (fi, fi), shell=True)\n                #pe(""cp ../wav/*.wav wav/"", shell=True)\n\n                # remove top part but keep cd call\n                run_aligner_lines = run_aligner_lines[:13] + \\\n                    [""cd cmu_us_slt_arctic\\n""] + run_aligner_lines[35:]\n\n                \'\'\'\n                # need to change do_build\n                # NO LONGER NECESSARY DUE TO FESTIVAL DEPENDENCE ON FILENAME\n\n                os.chdir(""bin"")\n                with open(""do_build"", ""r"") as f:\n                    do_build_lines = f.readlines()\n\n                replace_line = None\n                for n, l in enumerate(do_build_lines):\n                    if ""$FESTVOXDIR/src/ehmm/bin/do_ehmm"" in l:\n                        replace_line = n\n                        break\n\n                do_build_lines[replace_line] = ""   $FESTVOXDIR/src/ehmm/bin/do_ehmm\\n""\n\n                # FIXME Why does this hang when not overwritten???\n                with open(""edit_do_build"", ""w"") as f:\n                    f.writelines(do_build_lines)\n                \'\'\'\n\n                # need to change do_ehmm\n                os.chdir(festvoxdir)\n                os.chdir(""src/ehmm/bin/"")\n\n                # this is to fix festival if we somehow kill in the middle of training :(\n                # all due to festival\'s apparent dependence on name of script!\n                # really, really, REALLY weird\n                if os.path.exists(""do_ehmm.bak""):\n                    with open(""do_ehmm.bak"", ""r"") as f:\n                        fix = f.readlines()\n\n                    with open(""do_ehmm"", ""w"") as f:\n                        f.writelines(fix)\n\n                with open(""do_ehmm"", ""r"") as f:\n                    do_ehmm_lines = f.readlines()\n\n                with open(""do_ehmm.bak"", ""w"") as f:\n                    f.writelines(do_ehmm_lines)\n\n                replace_line = None\n                for n, l in enumerate(do_ehmm_lines):\n                    if ""$EHMMDIR/bin/ehmm ehmm/etc/ph_list.int"" in l:\n                        replace_line = n\n                        break\n\n                max_n_itr = ehmm_max_n_itr\n                do_ehmm_lines[replace_line] = ""    $EHMMDIR/bin/ehmm ehmm/etc/ph_list.int ehmm/etc/txt.phseq.data.int 1 0 ehmm/binfeat scaledft ehmm/mod 0 0 0 %s $num_cpus\\n"" % str(\n                    max_n_itr)\n\n                # depends on *name* of the script?????????\n                with open(""do_ehmm"", ""w"") as f:\n                    f.writelines(do_ehmm_lines)\n\n                # need to edit run_aligner....\n                dbn = ""do_build""\n                # FIXME\n                # WHY DOES IT DEPEND ON FILENAME????!!!!!??????\n                # should be able to call only edit_do_build label\n                # but hangs indefinitely...\n                replace_line = None\n                for n, l in enumerate(run_aligner_lines):\n                    if ""./bin/do_build build_prompts"" in l:\n                        replace_line = n\n                        break\n                run_aligner_lines[replace_line] = ""./bin/%s build_prompts\\n"" % dbn\n\n                replace_line = None\n                for n, l in enumerate(run_aligner_lines):\n                    if ""./bin/do_build label"" in l:\n                        replace_line = n\n                        break\n                run_aligner_lines[replace_line] = ""./bin/%s label\\n"" % dbn\n\n                replace_line = None\n                for n, l in enumerate(run_aligner_lines):\n                    if ""./bin/do_build build_utts"" in l:\n                        replace_line = n\n                        break\n                run_aligner_lines[replace_line] = ""./bin/%s build_utts\\n"" % dbn\n                os.chdir(bdir)\n\n            with open(""edit_run_aligner.sh"", ""w"") as f:\n                f.writelines(run_aligner_lines)\n\n            # 2>&1 needed to make it work?? really sketchy\n            pe(""bash -x edit_run_aligner.sh config.cfg 2>&1"", shell=True)\n\n    # compile vocoder\n    os.chdir(merlin_dir)\n    # set it to run on cpu\n    pe(""sed -i.bak -e s/MERLIN_THEANO_FLAGS=.*/MERLIN_THEANO_FLAGS=\'device=cpu,floatX=float32,on_unused_input=ignore\'/g src/setup_env.sh"", shell=True)\n    os.chdir(""tools"")\n    if not os.path.exists(""SPTK-3.9""):\n        pe(""bash -x compile_tools.sh 2>&1"", shell=True)\n\n    # slt_arctic stuff\n    os.chdir(merlin_dir)\n    os.chdir(""egs/slt_arctic/s1"")\n\n    # This madness due to autogen configs...\n    pe(""bash -x scripts/setup.sh slt_arctic_full 2>&1"", shell=True)\n\n    global_config_file = ""conf/global_settings.cfg""\n    replace_write(global_config_file, ""Labels"", ""phone_align"", replace_line=""%s=%s\\n"")\n    replace_write(global_config_file, ""Train"", ""1132"", replace_line=""%s=%s\\n"")\n    replace_write(global_config_file, ""Valid"", ""0"", replace_line=""%s=%s\\n"")\n    replace_write(global_config_file, ""Test"", ""0"", replace_line=""%s=%s\\n"")\n\n    pe(""bash -x scripts/prepare_config_files.sh %s 2>&1"" % global_config_file, shell=True)\n    pe(""bash -x scripts/prepare_config_files_for_synthesis.sh %s 2>&1"" % global_config_file, shell=True)\n    # delete the setup lines from run_full_voice.sh\n    pe(""sed -i.bak -e \'11d;12d;13d\' run_full_voice.sh"", shell=True)\n\n    pushd = os.getcwd()\n    os.chdir(""conf"")\n\n    acoustic_conf = ""acoustic_slt_arctic_full.conf""\n    replace_write(acoustic_conf, ""train_file_number"", ""1132"")\n    replace_write(acoustic_conf, ""valid_file_number"", ""0"")\n    replace_write(acoustic_conf, ""test_file_number"", ""0"")\n\n    replace_write(acoustic_conf, ""label_type"", ""phone_align"")\n    replace_write(acoustic_conf, ""subphone_feats"", ""coarse_coding"")\n    replace_write(acoustic_conf, ""dmgc"", ""60"")\n    replace_write(acoustic_conf, ""dbap"", ""1"")\n    # hack this to add an extra line in the config\n    replace_write(acoustic_conf, ""dlf0"", ""1\\ndo_MLPG: False"")\n\n    if not full_features:\n        replace_write(acoustic_conf, ""warmup_epoch"", ""1"")\n        replace_write(acoustic_conf, ""training_epochs"", ""1"")\n    replace_write(acoustic_conf, ""TRAINDNN"", ""False"")\n    replace_write(acoustic_conf, ""DNNGEN"", ""False"")\n    replace_write(acoustic_conf, ""GENWAV"", ""False"")\n    replace_write(acoustic_conf, ""CALMCD"", ""False"")\n\n    duration_conf = ""duration_slt_arctic_full.conf""\n    replace_write(duration_conf, ""train_file_number"", ""1132"")\n    replace_write(duration_conf, ""valid_file_number"", ""0"")\n    replace_write(duration_conf, ""test_file_number"", ""0"")\n    replace_write(duration_conf, ""label_type"", ""phone_align"")\n    replace_write(duration_conf, ""dur"", ""1"")\n    if not full_features:\n        replace_write(duration_conf, ""warmup_epoch"", ""1"")\n        replace_write(duration_conf, ""training_epochs"", ""1"")\n\n    replace_write(duration_conf, ""TRAINDNN"", ""False"")\n    replace_write(duration_conf, ""DNNGEN"", ""False"")\n    replace_write(duration_conf, ""CALMCD"", ""False"")\n\n    os.chdir(pushd)\n    if not os.path.exists(""slt_arctic_full_data""):\n        pe(""bash -x run_full_voice.sh 2>&1"", shell=True)\n\n    pe(""mv run_full_voice.sh.bak run_full_voice.sh"", shell=True)\n\n    os.chdir(merlin_dir)\n    os.chdir(""misc/scripts/vocoder/world"")\n\n    with open(""extract_features_for_merlin.sh"", ""r"") as f:\n        ex_lines = f.readlines()\n\n    ex_line_replace = None\n    for n, l in enumerate(ex_lines):\n        if ""merlin_dir="" in l:\n            ex_line_replace = n\n            break\n\n    ex_lines[ex_line_replace] = \'merlin_dir=""%s""\' % merlin_dir\n\n    ex_line_replace = None\n    for n, l in enumerate(ex_lines):\n        if ""wav_dir="" in l:\n            ex_line_replace = n\n            break\n\n    ex_lines[ex_line_replace] = \'wav_dir=""%s""\' % (experiment_dir + ""/database/wav"")\n\n    with open(""edit_extract_features_for_merlin.sh"", ""w"") as f:\n        f.writelines(ex_lines)\n\n    pe(""bash -x edit_extract_features_for_merlin.sh 2>&1"", shell=True)\n\n    os.chdir(basedir)\n    os.chdir(""latest_features"")\n    os.symlink(merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat"", ""audio_feat"")\n    os.symlink(merlin_dir + ""/misc/scripts/alignment/phone_align/full-context-labels/full"", ""text_feat"")\n\n    print(""Audio features in %s (and %s)"" % (os.getcwd() + ""/audio_feat"",\n                                             merlin_dir + ""/egs/slt_arctic/s1/slt_arctic_full_data/feat""))\n    print(""Text features in %s (and %s)"" % (os.getcwd() + ""/text_feat"", merlin_dir +\n                                            ""/misc/scripts/alignment/phone_align/full-context-labels/full""))\n    os.chdir(basedir)\n\n\ndef extract_final_features():\n    launchdir = os.getcwd()\n    os.chdir(""latest_features"")\n    basedir = os.path.abspath(os.getcwd()) + ""/""\n    text_files = os.listdir(""text_feat"")\n    audio_files = os.listdir(""audio_feat/bap"")\n    os.chdir(""merlin/egs/build_your_own_voice/s1"")\n    expdir = os.getcwd()\n\n    # make the file list\n    file_list_base = ""experiments/my_new_voice/duration_model/data/""\n    if not os.path.exists(file_list_base):\n        os.mkdir(file_list_base)\n\n    file_list_path = file_list_base + ""file_id_list_full.scp""\n    with open(file_list_path, ""w"") as f:\n        f.writelines([tef.split(""."")[0] + ""\\n"" for tef in text_files])\n\n    if not os.path.exists(basedir + ""file_id_list_full.scp""):\n        os.symlink(os.path.abspath(file_list_path),\n                   os.path.abspath(basedir + ""file_id_list_full.scp""))\n\n    # make the file list\n    file_list_base = ""experiments/my_new_voice/acoustic_model/data/""\n    if not os.path.exists(file_list_base):\n        os.mkdir(file_list_base)\n\n    file_list_path = file_list_base + ""file_id_list_full.scp""\n    with open(file_list_path, ""w"") as f:\n        f.writelines([tef.split(""."")[0] + ""\\n"" for tef in text_files])\n\n    if not os.path.exists(basedir + ""file_id_list_full.scp""):\n        os.symlink(os.path.abspath(file_list_path),\n                   os.path.abspath(basedir + ""file_id_list_full.scp""))\n\n    file_list_base = ""experiments/my_new_voice/test_synthesis/""\n    if not os.path.exists(file_list_base):\n        os.mkdir(file_list_base)\n\n    file_list_path = file_list_base + ""test_id_list.scp""\n    # debug with no test utterances\n    with open(file_list_path, ""w"") as f:\n        # f.writelines([""\\n"",])\n        f.writelines([tef.split(""."")[0] + ""\\n"" for tef in text_files[:20]])\n\n    if not os.path.exists(basedir + ""test_id_list.scp""):\n        os.symlink(os.path.abspath(file_list_path), os.path.abspath(basedir + ""test_id_list.scp""))\n\n    # now copy in the data - don\'t symlink due to possibilities of inplace\n    # modification\n    os.chdir(expdir)\n    basedatadir = ""experiments/my_new_voice/""\n    os.chdir(basedatadir)\n\n    labeldatadir = ""duration_model/data/label_phone_align""\n    if not os.path.exists(labeldatadir):\n        os.mkdir(labeldatadir)\n\n    # IT USES HTS STYLE LABELS\n    copytree(basedir + ""text_feat"", labeldatadir)\n\n    labeldatadir = ""acoustic_model/data/label_phone_align""\n    if not os.path.exists(labeldatadir):\n        os.mkdir(labeldatadir)\n\n    bapdatadir = ""acoustic_model/data/bap""\n    if not os.path.exists(bapdatadir):\n        os.mkdir(bapdatadir)\n\n    lf0datadir = ""acoustic_model/data/lf0""\n    if not os.path.exists(lf0datadir):\n        os.mkdir(lf0datadir)\n\n    mgcdatadir = ""acoustic_model/data/mgc""\n    if not os.path.exists(mgcdatadir):\n        os.mkdir(mgcdatadir)\n\n    # IT USES HTS STYLE LABELS\n    copytree(basedir + ""text_feat"", labeldatadir)\n    copytree(basedir + ""audio_feat/bap"", bapdatadir)\n    copytree(basedir + ""audio_feat/lf0"", lf0datadir)\n    copytree(basedir + ""audio_feat/mgc"", mgcdatadir)\n    #pe(""cp %s acoustic_model/data"" % ""label_norm_HTS_420.dat"")\n\n    while len(os.listdir(mgcdatadir)) < len(os.listdir(basedir + ""audio_feat/mgc"")):\n        print(""waiting for mgc file copy to complete..."")\n        time.sleep(3)\n\n    while len(os.listdir(lf0datadir)) < len(os.listdir(basedir + ""audio_feat/lf0"")):\n        print(""waiting for lf0 file copy to complete..."")\n        time.sleep(3)\n\n    while len(os.listdir(bapdatadir)) < len(os.listdir(basedir + ""audio_feat/bap"")):\n        print(""waiting for bap file copy to complete..."")\n        time.sleep(3)\n\n    num_audio_files = len(os.listdir(mgcdatadir))\n    num_label_files = len(os.listdir(labeldatadir))\n    num_files = min([num_audio_files, num_label_files])\n\n    os.chdir(expdir)\n\n    global_config_file = ""conf/global_settings.cfg""\n    pe(""bash -x scripts/prepare_config_files.sh %s 2>&1"" % global_config_file, shell=True)\n    pe(""bash -x scripts/prepare_config_files_for_synthesis.sh %s 2>&1"" % global_config_file, shell=True)\n\n    # this actally won\'t matter I don\'t think...\n    replace_write(global_config_file, ""Train"", str(num_files), replace_line=""%s=%s\\n"")\n    replace_write(global_config_file, ""Valid"", ""0"", replace_line=""%s=%s\\n"")\n    replace_write(global_config_file, ""Test"", ""0"", replace_line=""%s=%s\\n"")\n\n    acoustic_conf = ""conf/acoustic_my_new_voice.conf""\n    replace_write(acoustic_conf, ""train_file_number"", str(num_files))\n    replace_write(acoustic_conf, ""valid_file_number"", ""0"")\n    replace_write(acoustic_conf, ""test_file_number"", ""0"")\n\n    replace_write(acoustic_conf, ""label_type"", ""phone_align"")\n    replace_write(acoustic_conf, ""subphone_feats"", ""coarse_coding"")\n    replace_write(acoustic_conf, ""dmgc"", ""60"")\n    replace_write(acoustic_conf, ""dbap"", ""1"")\n    # hack this to add an extra line in the config\n    replace_write(acoustic_conf, ""dlf0"", ""1\\ndo_MLPG: False"")\n\n    if not full_features:\n        replace_write(acoustic_conf, ""warmup_epoch"", ""1"")\n        replace_write(acoustic_conf, ""training_epochs"", ""1"")\n    replace_write(acoustic_conf, ""TRAINDNN"", ""False"")\n    replace_write(acoustic_conf, ""DNNGEN"", ""False"")\n    replace_write(acoustic_conf, ""GENWAV"", ""False"")\n    replace_write(acoustic_conf, ""CALMCD"", ""False"")\n\n    duration_conf = ""conf/duration_my_new_voice.conf""\n    replace_write(duration_conf, ""train_file_number"", str(num_files))\n    replace_write(duration_conf, ""valid_file_number"", ""0"")\n    replace_write(duration_conf, ""test_file_number"", ""0"")\n    replace_write(duration_conf, ""label_type"", ""phone_align"")\n    replace_write(duration_conf, ""dur"", ""1"")\n    if not full_features:\n        replace_write(duration_conf, ""warmup_epoch"", ""1"")\n        replace_write(duration_conf, ""training_epochs"", ""1"")\n\n    \'\'\'\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""train_file_number"", str(num_files))\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""valid_file_number"", ""0"")\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""test_file_number"", ""0"")\n\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""dmgc"", ""60"")\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""dbap"", ""1"")\n    # hack this to add an extra line in the config\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""dlf0"", ""1\\ndo_MLPG: False"")\n\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""TRAINDNN"", ""False"")\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""DNNGEN"", ""False"")\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""GENWAV"", ""False"")\n    replace_write(""conf/acoustic_my_new_voice.conf"", ""CALMCD"", ""False"")\n\n\n    replace_write(""conf/duration_my_new_voice.conf"", ""train_file_number"", str(num_files))\n    replace_write(""conf/duration_my_new_voice.conf"", ""valid_file_number"", ""0"")\n    replace_write(""conf/duration_my_new_voice.conf"", ""test_file_number"", ""0"")\n\n    replace_write(""conf/duration_my_new_voice.conf"", ""TRAINDNN"", ""False"")\n    replace_write(""conf/duration_my_new_voice.conf"", ""DNNGEN"", ""False"")\n    replace_write(""conf/duration_my_new_voice.conf"", ""CALMCD"", ""False"")\n    \'\'\'\n\n    pe(""sed -i.bak -e \'19,20d;30,39d\' 03_run_merlin.sh"", shell=True)\n    pe(""bash -x 03_run_merlin.sh 2>&1"", shell=True)\n    pe(""mv 03_run_merlin.sh.bak 03_run_merlin.sh"", shell=True)\n    if not os.path.exists(basedir + ""final_acoustic_data""):\n        os.symlink(os.path.abspath(""experiments/my_new_voice/acoustic_model/data""),\n                   basedir + ""final_acoustic_data"")\n    if not os.path.exists(basedir + ""final_duration_data""):\n        os.symlink(os.path.abspath(""experiments/my_new_voice/duration_model/data""),\n                   basedir + ""final_duration_data"")\n    os.chdir(launchdir)\n\n\ndef save_numpy_features():\n    n_ins = 420\n    n_outs = 63  # 187\n\n    feature_dir = ""latest_features/""\n    with open(feature_dir + ""file_id_list_full.scp"") as f:\n        file_list = [l.strip() for l in f.readlines()]\n\n    norm_info_dir = os.path.abspath(""latest_features/norm_info/"") + ""/""\n    if not os.path.exists(norm_info_dir):\n        os.mkdir(norm_info_dir)\n\n    acoustic_dir = os.path.abspath(feature_dir + ""final_acoustic_data/"") + ""/""\n    audio_norm_file = ""norm_info_mgc_lf0_vuv_bap_%s_MVN.dat"" % str(n_outs)\n    audio_norm_source = acoustic_dir + audio_norm_file\n    audio_norm_dest = norm_info_dir + audio_norm_file\n    shutil.copy2(audio_norm_source, audio_norm_dest)\n\n    with open(audio_norm_source) as fid:\n        cmp_info = np.fromfile(fid, dtype=np.float32)\n        cmp_info = cmp_info.reshape((2, -1))\n    audio_norm = cmp_info\n\n    label_norm_file = ""label_norm_HTS_%s.dat"" % n_ins\n    label_norm_source = acoustic_dir + label_norm_file\n    label_norm_dest = norm_info_dir + label_norm_file\n    shutil.copy2(label_norm_source, label_norm_dest)\n\n    with open(label_norm_source) as fid:\n        cmp_info = np.fromfile(fid, dtype=np.float32)\n        cmp_info = cmp_info.reshape((2, -1))\n    label_norm = cmp_info\n\n    text_file = feature_dir + \'txt.done.data\'\n\n    with open(text_file) as f:\n        text_data = [l.strip() for l in f.readlines()]\n\n    monophone_path = os.path.abspath(""latest_features/monophones"") + ""/""\n    if not os.path.exists(monophone_path):\n        # Trailing ""/"" causes issues\n        os.symlink(os.path.abspath(\n            ""latest_features/merlin/misc/scripts/alignment/phone_align/cmu_us_slt_arctic/lab""), monophone_path[:-1])\n\n    launchdir = os.getcwd()\n    phone_files = {gl[:-4]: monophone_path + gl for gl in os.listdir(monophone_path)\n                   if gl[-4:] == "".lab""}\n\n    text_ids = [td.split("" "")[1] for td in text_data]\n\n    label_files_path = os.path.abspath(\n        ""latest_features/final_acoustic_data/nn_no_silence_lab_420"") + ""/""\n    # still has silence in it?\n    #audio_files_path = os.path.abspath(""latest_features/final_acoustic_data/nn_mgc_lf0_vuv_bap_63"") + ""/""\n    audio_files_path = os.path.abspath(\n        ""latest_features/final_acoustic_data/nn_norm_mgc_lf0_vuv_bap_63"") + ""/""\n    label_files = {lf[:-4]: label_files_path +\n                   lf for lf in os.listdir(label_files_path) if lf[-4:] == "".lab""}\n    audio_files = {af[:-4]: audio_files_path +\n                   af for af in os.listdir(audio_files_path) if af[-4:] == "".cmp""}\n\n    error_files = [\n        (i, x) for i, x in enumerate(text_ids) if x not in file_list]\n\n    # Solve corrupted files issues\n    for i, x in error_files:\n        try:\n            text_ids.remove(x)\n        except ValueError:\n            pass\n        try:\n            file_list.remove(x)\n        except ValueError:\n            pass\n        text_data = [td for td in text_data if td.split("" "")[1] != x]\n\n    text_utts = [td.split(\'""\')[1] for td in text_data]\n    text_tups = list(zip(text_ids, text_utts))\n    text_lu = {k: v for k, v in text_tups}\n    text_rlu = {v: k for k, v in text_lu.items()}\n\n    # take only valid subset.... ?\n    new_file_list = []\n    text_tup_fnames = [tt[0] for tt in text_tups]\n    for n, fname in enumerate(file_list):\n        if fname in text_tup_fnames:\n            new_file_list.append(fname)\n\n    file_list = new_file_list\n\n    new_text_tups = []\n    for n, ttup in enumerate(text_tups):\n        if ttup[0] in file_list:\n            new_text_tups.append(ttup)\n\n    text_tups = new_text_tups\n\n    # why on earth should this fail\n    #assert len(text_tups) == len(file_list)\n    assert sum([ti not in file_list for ti in text_ids]) == 0\n\n    char_set = sorted(list(set(\'\'.join(text_utts).lower())))\n    char2code = {x: i for i, x in enumerate(char_set)}\n    code2char = {v: k for k, v in char2code.items()}\n\n    phone_set = tuple(\'sil\',)\n    for fid in file_list:\n        with open(phone_files[fid]) as f:\n            phonemes = [p.strip() for p in f.readlines()]\n        # FIXME: Bug here that allows filenames in\n        phonemes = [x.strip().split(\' \') for x in phonemes[1:]]\n        durations, phonemes = zip(*[[float(x), z] for x, y, z in phonemes])\n        phone_set = tuple(sorted(list(set(phone_set + phonemes))))\n    phone2code = {x: i for i, x in enumerate(phone_set)}\n    code2phone = {v: k for k, v in phone2code.items()}\n    order = range(len(file_list))\n    np.random.seed(1)\n    np.random.shuffle(order)\n\n    all_in_features = []\n    all_out_features = []\n    all_phonemes = []\n    all_durations = []\n    all_text = []\n    all_ids = []\n    for i, idx in enumerate(order):\n        fid = file_list[idx]\n        # if i % 100 == 0:\n        #    print(i)\n        in_features, lab_frame_number = load_binary_file_frame(\n            label_files[fid], n_ins)\n        out_features, out_frame_number = load_binary_file_frame(\n            audio_files[fid], n_outs)\n\n        # print(lab_frame_number)\n        # print(out_frame_number)\n        if lab_frame_number != out_frame_number:\n            print(""WARNING: misaligned frame size for %s, using min"" % fid)\n            mf = min(lab_frame_number, out_frame_number)\n            in_features = in_features[:mf]\n            out_features = out_features[:mf]\n\n        with open(phone_files[fid]) as f:\n            phonemes = f.readlines()\n\n        phonemes = [x.strip().split(\' \') for x in phonemes[1:]]\n        durations, phonemes = zip(*[[float(x), z] for x, y, z in phonemes])\n\n        # first non pause phoneme\n        first_phoneme = next(\n            k - 1 for k, x in enumerate(phonemes) if x != \'pau\')\n\n        last_phoneme = len(phonemes) - next(\n            k - 1 for k, x in enumerate(phonemes[::-1]) if x != \'pau\')\n\n        phonemes = phonemes[first_phoneme:last_phoneme]\n        durations = durations[first_phoneme:last_phoneme]\n\n        assert phonemes[0] == \'pau\'\n        assert phonemes[-1] == \'pau\'\n        # assert \'pau\' not in phonemes[1:-1]\n        phonemes = phonemes[1:-1]\n\n        durations = np.array(durations)\n        durations = durations * 200\n        durations = durations - durations[0]\n        durations = durations[1:] - durations[:-1]\n        durations = durations[:-1]\n        durations = np.round(durations, 0).astype(\'int32\')\n        phonemes = np.array([phone2code[x] for x in phonemes], dtype=\'int32\')\n        all_in_features.append(in_features)\n        all_out_features.append(out_features)\n        all_phonemes.append(phonemes)\n        all_durations.append(durations)\n        all_text.append(text_lu[fid])\n        all_ids.append(fid)\n\n    assert len(all_in_features) == len(all_out_features)\n    assert len(all_in_features) == len(all_phonemes)\n    assert len(all_in_features) == len(all_durations)\n    assert len(all_in_features) == len(all_text)\n    assert len(all_in_features) == len(all_ids)\n\n    if not os.path.exists(""latest_features/numpy_features""):\n        os.mkdir(""latest_features/numpy_features"")\n\n    def oa(s_dict):\n        a = []\n        for i in range(max([int(k) for k in s_dict.keys()])):\n            a.append(s_dict[i])\n        return arr(a)\n\n    def arr(s):\n        return np.array(s)\n\n    for i in range(len(all_ids)):\n        print(""Saving %s"" % all_ids[i])\n        save_dict = {""file_id"": arr(all_ids[i]),\n                     ""phonemes"": arr(all_phonemes[i]),\n                     ""durations"": arr(all_durations[i]),\n                     ""text"": arr(all_text[i]),\n                     #""text_features"": arr(all_in_features[i]),\n                     #""text_norminfo"": label_norm,\n                     ""audio_features"": arr(all_out_features[i]),\n                     #""audio_norminfo"": audio_norm,\n                     ""mgc_extent"": arr(60),\n                     ""lf0_idx"": arr(60),\n                     ""vuv_idx"": arr(61),\n                     ""bap_idx"": arr(62),\n                     #""code2phone"": oa(code2phone),\n                     #""code2char"": oa(code2char),\n                     #""code2speaker"": oa(code2speaker),\n                     }\n\n        np.savez_compressed(""latest_features/numpy_features/%s.npz"" % all_ids[i],\n                            **save_dict)\n\n\ndef generate_merlin_wav(\n        data, gen_dir, file_basename=None,  # norm_info_file,\n        do_post_filtering=True, mgc_dim=60, fl=1024, sr=16000):\n    # Made from Jose\'s code and Merlin\n    gen_dir = os.path.abspath(gen_dir) + ""/""\n    if file_basename is None:\n        base = ""tmp_gen_wav""\n    else:\n        base = file_basename\n    if not os.path.exists(gen_dir):\n        os.mkdir(gen_dir)\n\n    file_name = os.path.join(gen_dir, base + "".cmp"")\n    """"""\n    fid = open(norm_info_file, \'rb\')\n    cmp_info = numpy.fromfile(fid, dtype=numpy.float32)\n    fid.close()\n    cmp_info = cmp_info.reshape((2, -1))\n    cmp_mean = cmp_info[0, ]\n    cmp_std = cmp_info[1, ]\n\n    data = data * cmp_std + cmp_mean\n    """"""\n\n    array_to_binary_file(data, file_name)\n    # This code was adapted from Merlin. All licenses apply\n\n    out_dimension_dict = {\'bap\': 1, \'lf0\': 1, \'mgc\': 60, \'vuv\': 1}\n    stream_start_index = {}\n    file_extension_dict = {\n        \'mgc\': \'.mgc\', \'bap\': \'.bap\', \'lf0\': \'.lf0\',\n        \'dur\': \'.dur\', \'cmp\': \'.cmp\'}\n    gen_wav_features = [\'mgc\', \'lf0\', \'bap\']\n\n    dimension_index = 0\n    for feature_name in out_dimension_dict.keys():\n        stream_start_index[feature_name] = dimension_index\n        dimension_index += out_dimension_dict[feature_name]\n\n    dir_name = os.path.dirname(file_name)\n    file_id = os.path.splitext(os.path.basename(file_name))[0]\n    features, frame_number = load_binary_file_frame(file_name, 63)\n\n    for feature_name in gen_wav_features:\n\n        current_features = features[\n            :, stream_start_index[feature_name]:\n            stream_start_index[feature_name] +\n            out_dimension_dict[feature_name]]\n\n        gen_features = current_features\n\n        if feature_name in [\'lf0\', \'F0\']:\n            if \'vuv\' in stream_start_index.keys():\n                vuv_feature = features[\n                    :, stream_start_index[\'vuv\']:stream_start_index[\'vuv\'] + 1]\n\n                for i in range(frame_number):\n                    if vuv_feature[i, 0] < 0.5:\n                        gen_features[i, 0] = -1.0e+10  # self.inf_float\n\n        new_file_name = os.path.join(\n            dir_name, file_id + file_extension_dict[feature_name])\n\n        array_to_binary_file(gen_features, new_file_name)\n\n    pf_coef = 1.4\n    fw_alpha = 0.58\n    co_coef = 511\n\n    sptkdir = merlindir + ""tools/bin/SPTK-3.9/""\n    #sptkdir = os.path.abspath(""latest_features/merlin/tools/bin/SPTK-3.9"") + ""/""\n    sptk_path = {\n        \'SOPR\': sptkdir + \'sopr\',\n        \'FREQT\': sptkdir + \'freqt\',\n        \'VSTAT\': sptkdir + \'vstat\',\n        \'MGC2SP\': sptkdir + \'mgc2sp\',\n        \'MERGE\': sptkdir + \'merge\',\n        \'BCP\': sptkdir + \'bcp\',\n        \'MC2B\': sptkdir + \'mc2b\',\n        \'C2ACR\': sptkdir + \'c2acr\',\n        \'MLPG\': sptkdir + \'mlpg\',\n        \'VOPR\': sptkdir + \'vopr\',\n        \'B2MC\': sptkdir + \'b2mc\',\n        \'X2X\': sptkdir + \'x2x\',\n        \'VSUM\': sptkdir + \'vsum\'}\n\n    #worlddir = os.path.abspath(""latest_features/merlin/tools/bin/WORLD"") + ""/""\n    worlddir = merlindir + ""tools/bin/WORLD/""\n    world_path = {\n        \'ANALYSIS\': worlddir + \'analysis\',\n        \'SYNTHESIS\': worlddir + \'synth\'}\n\n    fw_coef = fw_alpha\n    fl_coef = fl\n\n    files = {\'sp\': base + \'.sp\',\n             \'mgc\': base + \'.mgc\',\n             \'f0\': base + \'.f0\',\n             \'lf0\': base + \'.lf0\',\n             \'ap\': base + \'.ap\',\n             \'bap\': base + \'.bap\',\n             \'wav\': base + \'.wav\'}\n\n    mgc_file_name = files[\'mgc\']\n    cur_dir = os.getcwd()\n    os.chdir(gen_dir)\n\n    #  post-filtering\n    if do_post_filtering:\n        line = ""echo 1 1 ""\n        for i in range(2, mgc_dim):\n            line = line + str(pf_coef) + "" ""\n\n        pe(\n            \'{line} | {x2x} +af > {weight}\'\n            .format(\n                line=line, x2x=sptk_path[\'X2X\'],\n                weight=os.path.join(gen_dir, \'weight\')), shell=True)\n\n        pe(\n            \'{freqt} -m {order} -a {fw} -M {co} -A 0 < {mgc} | \'\n            \'{c2acr} -m {co} -M 0 -l {fl} > {base_r0}\'\n            .format(\n                freqt=sptk_path[\'FREQT\'], order=mgc_dim - 1,\n                fw=fw_coef, co=co_coef, mgc=files[\'mgc\'],\n                c2acr=sptk_path[\'C2ACR\'], fl=fl_coef,\n                base_r0=files[\'mgc\'] + \'_r0\'), shell=True)\n\n        pe(\n            \'{vopr} -m -n {order} < {mgc} {weight} | \'\n            \'{freqt} -m {order} -a {fw} -M {co} -A 0 | \'\n            \'{c2acr} -m {co} -M 0 -l {fl} > {base_p_r0}\'\n            .format(\n                vopr=sptk_path[\'VOPR\'], order=mgc_dim - 1,\n                mgc=files[\'mgc\'],\n                weight=os.path.join(gen_dir, \'weight\'),\n                freqt=sptk_path[\'FREQT\'], fw=fw_coef, co=co_coef,\n                c2acr=sptk_path[\'C2ACR\'], fl=fl_coef,\n                base_p_r0=files[\'mgc\'] + \'_p_r0\'), shell=True)\n\n        pe(\n            \'{vopr} -m -n {order} < {mgc} {weight} | \'\n            \'{mc2b} -m {order} -a {fw} | \'\n            \'{bcp} -n {order} -s 0 -e 0 > {base_b0}\'\n            .format(\n                vopr=sptk_path[\'VOPR\'], order=mgc_dim - 1,\n                mgc=files[\'mgc\'],\n                weight=os.path.join(gen_dir, \'weight\'),\n                mc2b=sptk_path[\'MC2B\'], fw=fw_coef,\n                bcp=sptk_path[\'BCP\'], base_b0=files[\'mgc\'] + \'_b0\'), shell=True)\n\n        pe(\n            \'{vopr} -d < {base_r0} {base_p_r0} | \'\n            \'{sopr} -LN -d 2 | {vopr} -a {base_b0} > {base_p_b0}\'\n            .format(\n                vopr=sptk_path[\'VOPR\'],\n                base_r0=files[\'mgc\'] + \'_r0\',\n                base_p_r0=files[\'mgc\'] + \'_p_r0\',\n                sopr=sptk_path[\'SOPR\'],\n                base_b0=files[\'mgc\'] + \'_b0\',\n                base_p_b0=files[\'mgc\'] + \'_p_b0\'), shell=True)\n\n        pe(\n            \'{vopr} -m -n {order} < {mgc} {weight} | \'\n            \'{mc2b} -m {order} -a {fw} | \'\n            \'{bcp} -n {order} -s 1 -e {order} | \'\n            \'{merge} -n {order2} -s 0 -N 0 {base_p_b0} | \'\n            \'{b2mc} -m {order} -a {fw} > {base_p_mgc}\'\n            .format(\n                vopr=sptk_path[\'VOPR\'], order=mgc_dim - 1,\n                mgc=files[\'mgc\'],\n                weight=os.path.join(gen_dir, \'weight\'),\n                mc2b=sptk_path[\'MC2B\'], fw=fw_coef,\n                bcp=sptk_path[\'BCP\'],\n                merge=sptk_path[\'MERGE\'], order2=mgc_dim - 2,\n                base_p_b0=files[\'mgc\'] + \'_p_b0\',\n                b2mc=sptk_path[\'B2MC\'],\n                base_p_mgc=files[\'mgc\'] + \'_p_mgc\'), shell=True)\n\n        mgc_file_name = files[\'mgc\'] + \'_p_mgc\'\n\n    # Vocoder WORLD\n\n    pe(\n        \'{sopr} -magic -1.0E+10 -EXP -MAGIC 0.0 {lf0} | \'\n        \'{x2x} +fd > {f0}\'\n        .format(\n            sopr=sptk_path[\'SOPR\'], lf0=files[\'lf0\'],\n            x2x=sptk_path[\'X2X\'], f0=files[\'f0\']), shell=True)\n\n    pe(\n        \'{sopr} -c 0 {bap} | {x2x} +fd > {ap}\'.format(\n            sopr=sptk_path[\'SOPR\'], bap=files[\'bap\'],\n            x2x=sptk_path[\'X2X\'], ap=files[\'ap\']), shell=True)\n\n    pe(\n        \'{mgc2sp} -a {alpha} -g 0 -m {order} -l {fl} -o 2 {mgc} | \'\n        \'{sopr} -d 32768.0 -P | {x2x} +fd > {sp}\'.format(\n            mgc2sp=sptk_path[\'MGC2SP\'], alpha=fw_alpha,\n            order=mgc_dim - 1, fl=fl, mgc=mgc_file_name,\n            sopr=sptk_path[\'SOPR\'], x2x=sptk_path[\'X2X\'], sp=files[\'sp\']),\n        shell=True)\n\n    pe(\n        \'{synworld} {fl} {sr} {f0} {sp} {ap} {wav}\'.format(\n            synworld=world_path[\'SYNTHESIS\'], fl=fl, sr=sr,\n            f0=files[\'f0\'], sp=files[\'sp\'], ap=files[\'ap\'],\n            wav=files[\'wav\']),\n        shell=True)\n\n    pe(\n        \'rm -f {ap} {sp} {f0} {bap} {lf0} {mgc} {mgc}_b0 {mgc}_p_b0 \'\n        \'{mgc}_p_mgc {mgc}_p_r0 {mgc}_r0 {cmp} weight\'.format(\n            ap=files[\'ap\'], sp=files[\'sp\'], f0=files[\'f0\'],\n            bap=files[\'bap\'], lf0=files[\'lf0\'], mgc=files[\'mgc\'],\n            cmp=base + \'.cmp\'),\n        shell=True)\n    os.chdir(cur_dir)\n\n\ndef get_reconstructions():\n    features_dir = ""latest_features/numpy_features/""\n    norm_info_file = ""latest_features/norm_info/norm_info_mgc_lf0_vuv_bap_63_MVN.dat""\n    with open(norm_info_file, ""rb"") as f:\n        cmp_info = np.fromfile(f, dtype=np.float32)\n    cmp_info = cmp_info.reshape((2, -1))\n    cmp_mean = cmp_info[0]\n    cmp_std = cmp_info[1]\n    for fp in os.listdir(features_dir)[:5]:\n        print(""Reconstructing %s"" % fp)\n        a = np.load(features_dir + fp)\n        af = a[""audio_features""]\n        r = af * cmp_std + cmp_mean\n        generate_merlin_wav(r, ""latest_features/gen"",\n                            file_basename=fp.split(""."")[0],\n                            do_post_filtering=False)\n\n\nif __name__ == ""__main__"":\n    launchdir = os.getcwd()\n    import argparse\n    parser = argparse.ArgumentParser(description=""Extract audio and text features using speech synthesis toolkits including SPTK, HTS, HTK, and Merlin. Special thanks to Jose Sotelo and the Edinburgh Speech Synthesis team. The text to use must not contain any parenthesis characters e.g. \'(\' or \')\' ."",\n                                     epilog=""Example usage: python extract_features.py -w wav48/p294 -t txt/p294"")\n    parser.add_argument(""--wav_dir"", ""-w"",\n                        help=""filepath for directory of wav files"",\n                        required=True)\n    parser.add_argument(""--txt_dir"", ""-t"",\n                        help=""filepath for directory of txt files"",\n                        required=True)\n    parser.add_argument(""--keep_silences"", ""-k"",\n                        help=""keep silences in audio, may be necessary for certain languages or datasets"",\n                        action=""store_true"", default=False)\n    parser.add_argument(""--full_features"", ""-f"",\n                        help=""Extract all label features, rather than focusing only on audio"",\n                        action=""store_true"", default=False)\n    args = parser.parse_args()\n\n    wav_dir = os.path.abspath(args.wav_dir)\n    txt_dir = os.path.abspath(args.txt_dir)\n    keep_silences = args.keep_silences\n    full_features = args.full_features\n    if wav_dir[-1] != ""/"":\n        wav_dir += ""/""\n    if txt_dir[-1] != ""/"":\n        txt_dir += ""/""\n\n    """"""\n    # handle .data files?\n    import os\n\n    with open(""cmuarctic.data"", ""r"") as f:\n        lines = f.readlines()\n\n    if not os.path.exists(""txt""):\n        os.mkdir(""txt"")\n\n    for l in lines:\n        ls = l.split(\'""\')\n        base = ls[0].split("" "")[1]\n        txt = ls[-2].strip()\n        with open(""txt/%s.txt"" % base, ""w"") as f:\n            f.write(""%s\\n"" % txt)\n    """"""\n    n_split = 5000\n    total_wav = sorted(os.listdir(wav_dir))\n    total_txt = sorted(os.listdir(txt_dir))\n    n_total_wav = len(total_wav)\n    n_total_txt = len(total_txt)\n\n    if n_total_wav <= n_split:\n        multifolder = False\n        itr = [0]\n        cur_wav_dir = wav_dir\n        cur_txt_dir = txt_dir\n    else:\n        multifolder = True\n        print(""Large fileset found"")\n        print(""Performing temporary splits"")\n        n_splits = n_total_wav // n_split + 1\n        itr = range(n_splits)\n        s = 0\n        for i in itr:\n            e = s + n_split\n            sub_wav = [wav_dir + str(os.sep) + tw for tw in total_wav[s:e]]\n            sub_txt = []\n            for sw in sub_wav:\n                fn = sw.split(os.sep)[-1].split(""."")[0]\n                txt_i = [t for t in total_txt if fn in t]\n                if len(txt_i) != 1:\n                    # exact match\n                    txt_i = [t for t in txt_i if t.split(""."")[0] == fn]\n                    if len(txt_i) != 1:\n                        raise ValueError(""Multiple/no match found for wav file {}"".format(fn))\n                        #from IPython import embed; embed(); raise ValueError()\n                txt_i = txt_i[0]\n                sub_txt.append(txt_dir + str(os.sep) + txt_i)\n            tmp_wav_dir = ""tmp_wav_%i"" % i\n            tmp_txt_dir = ""tmp_txt_%i"" % i\n            if os.path.exists(tmp_wav_dir):\n                shutil.rmtree(tmp_wav_dir)\n            if os.path.exists(tmp_txt_dir):\n                shutil.rmtree(tmp_txt_dir)\n            os.mkdir(tmp_wav_dir)\n            os.mkdir(tmp_txt_dir)\n            assert len(sub_wav) == len(sub_txt)\n            print(""Copying subset to tmp_*_%i"" % i)\n            for wf, tf in zip(sub_wav, sub_txt):\n                shutil.copy2(wf, tmp_wav_dir)\n                shutil.copy2(tf, tmp_txt_dir)\n            s = e\n\n    for i in itr:\n        if multifolder:\n            cur_wav_dir = os.getcwd() + str(os.sep) + ""tmp_wav_%i"" % i + str(os.sep)\n            cur_txt_dir = os.getcwd() + str(os.sep) + ""tmp_txt_%i"" % i + str(os.sep)\n            if os.path.exists(""latest_features""):\n                shutil.rmtree(""latest_features"")\n        if not os.path.exists(""latest_features""):\n            extract_intermediate_features(cur_wav_dir, cur_txt_dir, keep_silences, full_features)\n        elif os.path.exists(""latest_features""):\n            if not os.path.exists(""latest_features/text_feat"") and not os.path.exists(""latest_features/audio_feat""):\n                print(""Redoing feature extraction"")\n                pdir = os.getcwd()\n                os.chdir(""latest_features"")\n                if os.path.exists(""merlin""):\n                    shutil.rmtree(""merlin"")\n                if os.path.exists(""text_feat""):\n                    os.remove(""text_feat"")\n                if os.path.exists(""audio_feat""):\n                    os.remove(""audio_feat"")\n                os.chdir(pdir)\n                extract_intermediate_features(cur_wav_dir, cur_txt_dir,\n                                              keep_silences, full_features)\n        if not os.path.exists(""latest_features/final_duration_data"") or not os.path.exists(""latest_features/final_acoustic_data""):\n            extract_final_features()\n            print(""Feature extraction complete!"")\n        if not os.path.exists(""latest_features/numpy_features""):\n            save_numpy_features()\n        # if not os.path.exists(""latest_features/gen""):\n        #    get_reconstructions()\n        # TODO: Add -clean argument\n        if multifolder:\n            tmp_results = ""tmp_results_%i"" % i\n            if os.path.exists(tmp_results):\n                shutil.rmtree(tmp_results)\n            shutil.copytree(""latest_features"" + str(os.sep) + ""numpy_features"",\n                            tmp_results)\n    if multifolder:\n        for i in itr:\n            for f in os.listdir(""tmp_results_%i"" % i):\n                try:\n                    shutil.move(""tmp_results_%i"" % i + str(os.sep) + f,\n                                ""latest_features"" + str(os.sep) + ""numpy_features"")\n                except shutil.Error:\n                    continue\n    print(""All files generated, remove the directories to rerun"")\n'"
vctk_preprocess/prepare_htk_alignments_vctk.py,0,"b'# coding: utf-8\n""""""\nScript for do force alignment by gentle for VCTK. This script takes approx\n~40 hours to finish. It processes all utterances in VCTK.\n\nNOTE: Must be run with Python2, since gentle doesn\'t work with Python3.\n\nUsage:\n    1. Install https://github.com/lowerquality/gentle\n    2. Download VCTK http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\n\nand then run the script by:\n\n    python2 prepare_htk_alignments_vctk.py ${your_vctk_data_path}\n\nAfter running the script, you will see alignment files in `lab` directory as\nfollows:\n\n    > tree ~/data/VCTK-Corpus/ -d -L\n\n    /home/ryuichi/data/VCTK-Corpus/\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 lab\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 txt\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 wav48\n""""""\nimport argparse\nimport logging\nimport multiprocessing\nimport os\nimport sys\nfrom tqdm import tqdm\nimport json\nfrom os.path import join, basename, dirname, exists\nimport numpy as np\n\nimport gentle\nimport librosa\nfrom nnmnkwii.datasets import vctk\n\n\ndef on_progress(p):\n    for k, v in p.items():\n        logging.debug(""%s: %s"" % (k, v))\n\n\ndef write_hts_label(labels, lab_path):\n    lab = """"\n    for s, e, l in labels:\n        s, e = float(s) * 1e7, float(e) * 1e7\n        s, e = int(s), int(e)\n        lab += ""{} {} {}\\n"".format(s, e, l)\n    print(lab)\n    with open(lab_path, ""w"") as f:\n        f.write(lab)\n\n\ndef json2hts(data):\n    emit_bos = False\n    emit_eos = False\n\n    phone_start = 0\n    phone_end = None\n    labels = []\n\n    for word in data[""words""]:\n        case = word[""case""]\n        if case != ""success"":\n            raise RuntimeError(""Alignment failed"")\n        start = float(word[""start""])\n        word_end = float(word[""end""])\n\n        if not emit_bos:\n            labels.append((phone_start, start, ""silB""))\n            emit_bos = True\n\n        phone_start = start\n        phone_end = None\n        for phone in word[""phones""]:\n            ph = str(phone[""phone""][:-2])\n            duration = float(phone[""duration""])\n            phone_end = phone_start + duration\n            labels.append((phone_start, phone_end, ph))\n            phone_start += duration\n        assert np.allclose(phone_end, word_end)\n    if not emit_eos:\n        labels.append((phone_start, phone_end, ""silE""))\n        emit_eos = True\n\n    return labels\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=\'Do force alignment for VCTK and save HTK-style alignments\')\n    parser.add_argument(\n        \'--nthreads\', default=multiprocessing.cpu_count(), type=int,\n        help=\'number of alignment threads\')\n    parser.add_argument(\n        \'--conservative\', dest=\'conservative\', action=\'store_true\',\n        help=\'conservative alignment\')\n    parser.set_defaults(conservative=False)\n    parser.add_argument(\n        \'--disfluency\', dest=\'disfluency\', action=\'store_true\',\n        help=\'include disfluencies (uh, um) in alignment\')\n    parser.set_defaults(disfluency=False)\n    parser.add_argument(\n        \'--log\', default=""INFO"",\n        help=\'the log level (DEBUG, INFO, WARNING, ERROR, or CRITICAL)\')\n    parser.add_argument(\'data_root\', type=str, help=\'Data root\')\n\n    args = parser.parse_args()\n\n    log_level = args.log.upper()\n    logging.getLogger().setLevel(log_level)\n    disfluencies = set([\'uh\', \'um\'])\n\n    data_root = args.data_root\n\n    # Do for all speakers\n    speakers = vctk.available_speakers\n\n    # Collect all transcripts/wav files\n    td = vctk.TranscriptionDataSource(data_root, speakers=speakers)\n    transcriptions = td.collect_files()\n    wav_paths = vctk.WavFileDataSource(\n        data_root, speakers=speakers).collect_files()\n\n    # Save dir\n    save_dir = join(data_root, ""lab"")\n    if not exists(save_dir):\n        os.makedirs(save_dir)\n\n    resources = gentle.Resources()\n\n    for idx in tqdm(range(len(wav_paths))):\n        transcript = transcriptions[idx]\n        audiofile = wav_paths[idx]\n        lab_path = audiofile.replace(""wav48/"", ""lab/"").replace("".wav"", "".lab"")\n        print(transcript)\n        print(audiofile)\n        print(lab_path)\n        lab_dir = dirname(lab_path)\n        if not exists(lab_dir):\n            os.makedirs(lab_dir)\n\n        logging.info(""converting audio to 8K sampled wav"")\n        with gentle.resampled(audiofile) as wavfile:\n            logging.info(""starting alignment"")\n            aligner = gentle.ForcedAligner(resources, transcript,\n                                           nthreads=args.nthreads,\n                                           disfluency=args.disfluency,\n                                           conservative=args.conservative,\n                                           disfluencies=disfluencies)\n            result = aligner.transcribe(\n                wavfile, progress_cb=on_progress, logging=logging)\n\n            # convert to htk format\n            a = json.loads(result.to_json())\n            try:\n                labels = json2hts(a)\n            except RuntimeError as e:\n                from warnings import warn\n                warn(str(e))\n                continue\n\n            # Insert end time\n            x, sr = librosa.load(wavfile, sr=8000)\n            endtime = float(len(x)) / sr\n            labels[-1] = (labels[-1][0], endtime, labels[-1][-1])\n\n            # write to file\n            write_hts_label(labels, lab_path)\n'"
vctk_preprocess/prepare_vctk_labels.py,0,"b'# coding: utf-8\n""""""\nPrepare HTS alignments for VCTK.\n\nusage: prepare_vctk_labels.py [options] <data_root> <out_dir>\n\noptions:\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\nimport os\nfrom nnmnkwii.datasets import vctk\nfrom os.path import join, exists, splitext, basename\nimport sys\nfrom glob import glob\n\nfrom subprocess import Popen, PIPE\nfrom tqdm import tqdm\n\n\ndef do(cmd):\n    print(cmd)\n    p = Popen(cmd, shell=True)\n    p.wait()\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    data_root = args[""<data_root>""]\n    out_dir = args[""<out_dir>""]\n\n    for idx in tqdm(range(len(vctk.available_speakers))):\n        speaker = vctk.available_speakers[idx]\n\n        wav_root = join(data_root, ""wav48/p{}"".format(speaker))\n        txt_root = join(data_root, ""txt/p{}"".format(speaker))\n        assert exists(wav_root)\n        assert exists(txt_root)\n        print(wav_root, txt_root)\n\n        # Do alignments\n        cmd = ""python ./extract_feats.py -w {} -t {}"".format(wav_root, txt_root)\n        do(cmd)\n\n        # Copy\n        lab_dir = join(out_dir, ""p{}"".format(speaker))\n        if not exists(lab_dir):\n            os.makedirs(lab_dir)\n        cmd = ""cp ./latest_features/merlin/misc/scripts/alignment/phone_align/full-context-labels/mono/*.lab {}"".format(\n            lab_dir)\n        do(cmd)\n\n        # Remove\n        do(""rm -rf ./latest_features"")\n\n    sys.exit(0)\n'"
deepvoice3_pytorch/frontend/__init__.py,4,"b'# coding: utf-8\n\n""""""Text processing frontend\n\nAll frontend module should have the following functions:\n\n- text_to_sequence(text, p)\n- sequence_to_text(sequence)\n\nand the property:\n\n- n_vocab\n\n""""""\nfrom deepvoice3_pytorch.frontend import en\n\n# optinoal Japanese frontend\ntry:\n    from deepvoice3_pytorch.frontend import jp\nexcept ImportError:\n    jp = None\n\ntry:\n    from deepvoice3_pytorch.frontend import ko\nexcept ImportError:\n    ko = None\n\n# if you are going to use the frontend, you need to modify _characters in symbol.py:\n# _characters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'(),-.:;? \' + \'\xc2\xa1\xc2\xbf\xc3\xb1\xc3\xa1\xc3\xa9\xc3\xad\xc3\xb3\xc3\xba\xc3\x81\xc3\x89\xc3\x8d\xc3\x93\xc3\x9a\xc3\x91\'\ntry:\n    from deepvoice3_pytorch.frontend import es\nexcept ImportError:\n    es = None\n'"
deepvoice3_pytorch/tfcompat/__init__.py,0,b''
deepvoice3_pytorch/tfcompat/hparam.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Hyperparameter values.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport numbers\nimport re\n\nimport six\n\n## from tensorflow.contrib.training.python.training import hparam_pb2\n## from tensorflow.python.framework import ops\n## from tensorflow.python.util import compat\n## from tensorflow.python.util import deprecation\n\n# Define the regular expression for parsing a single clause of the input\n# (delimited by commas).  A legal clause looks like:\n#   <variable name>[<index>]? = <rhs>\n# where <rhs> is either a single token or [] enclosed list of tokens.\n# For example:  ""var[1] = a"" or ""x = [1,2,3]""\nPARAM_RE = re.compile(r""""""\n  (?P<name>[a-zA-Z][\\w\\.]*)      # variable name: ""var"" or ""x""\n  (\\[\\s*(?P<index>\\d+)\\s*\\])?  # (optional) index: ""1"" or None\n  \\s*=\\s*\n  ((?P<val>[^,\\[]*)            # single value: ""a"" or None\n   |\n   \\[(?P<vals>[^\\]]*)\\])       # list of values: None or ""1,2,3""\n  ($|,\\s*)"""""", re.VERBOSE)\n\n\ndef _parse_fail(name, var_type, value, values):\n  """"""Helper function for raising a value error for bad assignment.""""""\n  raise ValueError(\n      \'Could not parse hparam \\\'%s\\\' of type \\\'%s\\\' with value \\\'%s\\\' in %s\' %\n      (name, var_type.__name__, value, values))\n\n\ndef _reuse_fail(name, values):\n  """"""Helper function for raising a value error for reuse of name.""""""\n  raise ValueError(\'Multiple assignments to variable \\\'%s\\\' in %s\' % (name,\n                                                                      values))\n\n\ndef _process_scalar_value(name, parse_fn, var_type, m_dict, values,\n                          results_dictionary):\n  """"""Update results_dictionary with a scalar value.\n\n  Used to update the results_dictionary to be returned by parse_values when\n  encountering a clause with a scalar RHS (e.g.  ""s=5"" or ""arr[0]=5"".)\n\n  Mutates results_dictionary.\n\n  Args:\n    name: Name of variable in assignment (""s"" or ""arr"").\n    parse_fn: Function for parsing the actual value.\n    var_type: Type of named variable.\n    m_dict: Dictionary constructed from regex parsing.\n      m_dict[\'val\']: RHS value (scalar)\n      m_dict[\'index\']: List index value (or None)\n    values: Full expression being parsed\n    results_dictionary: The dictionary being updated for return by the parsing\n      function.\n\n  Raises:\n    ValueError: If the name has already been used.\n  """"""\n  try:\n    parsed_value = parse_fn(m_dict[\'val\'])\n  except ValueError:\n    _parse_fail(name, var_type, m_dict[\'val\'], values)\n\n  # If no index is provided\n  if not m_dict[\'index\']:\n    if name in results_dictionary:\n      _reuse_fail(name, values)\n    results_dictionary[name] = parsed_value\n  else:\n    if name in results_dictionary:\n      # The name has already been used as a scalar, then it\n      # will be in this dictionary and map to a non-dictionary.\n      if not isinstance(results_dictionary.get(name), dict):\n        _reuse_fail(name, values)\n    else:\n      results_dictionary[name] = {}\n\n    index = int(m_dict[\'index\'])\n    # Make sure the index position hasn\'t already been assigned a value.\n    if index in results_dictionary[name]:\n      _reuse_fail(\'{}[{}]\'.format(name, index), values)\n    results_dictionary[name][index] = parsed_value\n\n\ndef _process_list_value(name, parse_fn, var_type, m_dict, values,\n                        results_dictionary):\n  """"""Update results_dictionary from a list of values.\n\n  Used to update results_dictionary to be returned by parse_values when\n  encountering a clause with a list RHS (e.g.  ""arr=[1,2,3]"".)\n\n  Mutates results_dictionary.\n\n  Args:\n    name: Name of variable in assignment (""arr"").\n    parse_fn: Function for parsing individual values.\n    var_type: Type of named variable.\n    m_dict: Dictionary constructed from regex parsing.\n      m_dict[\'val\']: RHS value (scalar)\n    values: Full expression being parsed\n    results_dictionary: The dictionary being updated for return by the parsing\n      function.\n\n  Raises:\n    ValueError: If the name has an index or the values cannot be parsed.\n  """"""\n  if m_dict[\'index\'] is not None:\n    raise ValueError(\'Assignment of a list to a list index.\')\n  elements = filter(None, re.split(\'[ ,]\', m_dict[\'vals\']))\n  # Make sure the name hasn\'t already been assigned a value\n  if name in results_dictionary:\n    raise _reuse_fail(name, values)\n  try:\n    results_dictionary[name] = [parse_fn(e) for e in elements]\n  except ValueError:\n    _parse_fail(name, var_type, m_dict[\'vals\'], values)\n\n\ndef _cast_to_type_if_compatible(name, param_type, value):\n  """"""Cast hparam to the provided type, if compatible.\n\n  Args:\n    name: Name of the hparam to be cast.\n    param_type: The type of the hparam.\n    value: The value to be cast, if compatible.\n\n  Returns:\n    The result of casting `value` to `param_type`.\n\n  Raises:\n    ValueError: If the type of `value` is not compatible with param_type.\n      * If `param_type` is a string type, but `value` is not.\n      * If `param_type` is a boolean, but `value` is not, or vice versa.\n      * If `param_type` is an integer type, but `value` is not.\n      * If `param_type` is a float type, but `value` is not a numeric type.\n  """"""\n  fail_msg = (\n      ""Could not cast hparam \'%s\' of type \'%s\' from value %r"" %\n      (name, param_type, value))\n\n  # Some callers use None, for which we can\'t do any casting/checking. :(\n  if issubclass(param_type, type(None)):\n    return value\n\n  # Avoid converting a non-string type to a string.\n  if (issubclass(param_type, (six.string_types, six.binary_type)) and\n      not isinstance(value, (six.string_types, six.binary_type))):\n    raise ValueError(fail_msg)\n\n  # Avoid converting a number or string type to a boolean or vice versa.\n  if issubclass(param_type, bool) != isinstance(value, bool):\n    raise ValueError(fail_msg)\n\n  # Avoid converting float to an integer (the reverse is fine).\n  if (issubclass(param_type, numbers.Integral) and\n      not isinstance(value, numbers.Integral)):\n    raise ValueError(fail_msg)\n\n  # Avoid converting a non-numeric type to a numeric type.\n  if (issubclass(param_type, numbers.Number) and\n      not isinstance(value, numbers.Number)):\n    raise ValueError(fail_msg)\n\n  return param_type(value)\n\n\ndef parse_values(values, type_map):\n  """"""Parses hyperparameter values from a string into a python map.\n\n  `values` is a string containing comma-separated `name=value` pairs.\n  For each pair, the value of the hyperparameter named `name` is set to\n  `value`.\n\n  If a hyperparameter name appears multiple times in `values`, a ValueError\n  is raised (e.g. \'a=1,a=2\', \'a[1]=1,a[1]=2\').\n\n  If a hyperparameter name in both an index assignment and scalar assignment,\n  a ValueError is raised.  (e.g. \'a=[1,2,3],a[0] = 1\').\n\n  The hyperparameter name may contain \'.\' symbols, which will result in an\n  attribute name that is only accessible through the getattr and setattr\n  functions.  (And must be first explicit added through add_hparam.)\n\n  WARNING: Use of \'.\' in your variable names is allowed, but is not well\n  supported and not recommended.\n\n  The `value` in `name=value` must follows the syntax according to the\n  type of the parameter:\n\n  *  Scalar integer: A Python-parsable integer point value.  E.g.: 1,\n     100, -12.\n  *  Scalar float: A Python-parsable floating point value.  E.g.: 1.0,\n     -.54e89.\n  *  Boolean: Either true or false.\n  *  Scalar string: A non-empty sequence of characters, excluding comma,\n     spaces, and square brackets.  E.g.: foo, bar_1.\n  *  List: A comma separated list of scalar values of the parameter type\n     enclosed in square brackets.  E.g.: [1,2,3], [1.0,1e-12], [high,low].\n\n  When index assignment is used, the corresponding type_map key should be the\n  list name.  E.g. for ""arr[1]=0"" the type_map must have the key ""arr"" (not\n  ""arr[1]"").\n\n  Args:\n    values: String.  Comma separated list of `name=value` pairs where\n      \'value\' must follow the syntax described above.\n    type_map: A dictionary mapping hyperparameter names to types.  Note every\n      parameter name in values must be a key in type_map.  The values must\n      conform to the types indicated, where a value V is said to conform to a\n      type T if either V has type T, or V is a list of elements of type T.\n      Hence, for a multidimensional parameter \'x\' taking float values,\n      \'x=[0.1,0.2]\' will parse successfully if type_map[\'x\'] = float.\n\n  Returns:\n    A python map mapping each name to either:\n    * A scalar value.\n    * A list of scalar values.\n    * A dictionary mapping index numbers to scalar values.\n    (e.g. ""x=5,L=[1,2],arr[1]=3"" results in {\'x\':5,\'L\':[1,2],\'arr\':{1:3}}"")\n\n  Raises:\n    ValueError: If there is a problem with input.\n    * If `values` cannot be parsed.\n    * If a list is assigned to a list index (e.g. \'a[1] = [1,2,3]\').\n    * If the same rvalue is assigned two different values (e.g. \'a=1,a=2\',\n      \'a[1]=1,a[1]=2\', or \'a=1,a=[1]\')\n  """"""\n  results_dictionary = {}\n  pos = 0\n  while pos < len(values):\n    m = PARAM_RE.match(values, pos)\n    if not m:\n      raise ValueError(\'Malformed hyperparameter value: %s\' % values[pos:])\n    # Check that there is a comma between parameters and move past it.\n    pos = m.end()\n    # Parse the values.\n    m_dict = m.groupdict()\n    name = m_dict[\'name\']\n    if name not in type_map:\n      raise ValueError(\'Unknown hyperparameter type for %s\' % name)\n    type_ = type_map[name]\n\n    # Set up correct parsing function (depending on whether type_ is a bool)\n    if type_ == bool:\n\n      def parse_bool(value):\n        if value in [\'true\', \'True\']:\n          return True\n        elif value in [\'false\', \'False\']:\n          return False\n        else:\n          try:\n            return bool(int(value))\n          except ValueError:\n            _parse_fail(name, type_, value, values)\n\n      parse = parse_bool\n    else:\n      parse = type_\n\n    # If a singe value is provided\n    if m_dict[\'val\'] is not None:\n      _process_scalar_value(name, parse, type_, m_dict, values,\n                            results_dictionary)\n\n    # If the assigned value is a list:\n    elif m_dict[\'vals\'] is not None:\n      _process_list_value(name, parse, type_, m_dict, values,\n                          results_dictionary)\n\n    else:  # Not assigned a list or value\n      _parse_fail(name, type_, \'\', values)\n\n  return results_dictionary\n\n\nclass HParams(object):\n  """"""Class to hold a set of hyperparameters as name-value pairs.\n\n  A `HParams` object holds hyperparameters used to build and train a model,\n  such as the number of hidden units in a neural net layer or the learning rate\n  to use when training.\n\n  You first create a `HParams` object by specifying the names and values of the\n  hyperparameters.\n\n  To make them easily accessible the parameter names are added as direct\n  attributes of the class.  A typical usage is as follows:\n\n  ```python\n  # Create a HParams object specifying names and values of the model\n  # hyperparameters:\n  hparams = HParams(learning_rate=0.1, num_hidden_units=100)\n\n  # The hyperparameter are available as attributes of the HParams object:\n  hparams.learning_rate ==> 0.1\n  hparams.num_hidden_units ==> 100\n  ```\n\n  Hyperparameters have type, which is inferred from the type of their value\n  passed at construction type.   The currently supported types are: integer,\n  float, boolean, string, and list of integer, float, boolean, or string.\n\n  You can override hyperparameter values by calling the\n  [`parse()`](#HParams.parse) method, passing a string of comma separated\n  `name=value` pairs.  This is intended to make it possible to override\n  any hyperparameter values from a single command-line flag to which\n  the user passes \'hyper-param=value\' pairs.  It avoids having to define\n  one flag for each hyperparameter.\n\n  The syntax expected for each value depends on the type of the parameter.\n  See `parse()` for a description of the syntax.\n\n  Example:\n\n  ```python\n  # Define a command line flag to pass name=value pairs.\n  # For example using argparse:\n  import argparse\n  parser = argparse.ArgumentParser(description=\'Train my model.\')\n  parser.add_argument(\'--hparams\', type=str,\n                      help=\'Comma separated list of ""name=value"" pairs.\')\n  args = parser.parse_args()\n  ...\n  def my_program():\n    # Create a HParams object specifying the names and values of the\n    # model hyperparameters:\n    hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,\n                         activations=[\'relu\', \'tanh\'])\n\n    # Override hyperparameters values by parsing the command line\n    hparams.parse(args.hparams)\n\n    # If the user passed `--hparams=learning_rate=0.3` on the command line\n    # then \'hparams\' has the following attributes:\n    hparams.learning_rate ==> 0.3\n    hparams.num_hidden_units ==> 100\n    hparams.activations ==> [\'relu\', \'tanh\']\n\n    # If the hyperparameters are in json format use parse_json:\n    hparams.parse_json(\'{""learning_rate"": 0.3, ""activations"": ""relu""}\')\n  ```\n  """"""\n\n  _HAS_DYNAMIC_ATTRIBUTES = True  # Required for pytype checks.\n\n  def __init__(self, hparam_def=None, model_structure=None, **kwargs):\n    """"""Create an instance of `HParams` from keyword arguments.\n\n    The keyword arguments specify name-values pairs for the hyperparameters.\n    The parameter types are inferred from the type of the values passed.\n\n    The parameter names are added as attributes of `HParams` object, so they\n    can be accessed directly with the dot notation `hparams._name_`.\n\n    Example:\n\n    ```python\n    # Define 3 hyperparameters: \'learning_rate\' is a float parameter,\n    # \'num_hidden_units\' an integer parameter, and \'activation\' a string\n    # parameter.\n    hparams = tf.HParams(\n        learning_rate=0.1, num_hidden_units=100, activation=\'relu\')\n\n    hparams.activation ==> \'relu\'\n    ```\n\n    Note that a few names are reserved and cannot be used as hyperparameter\n    names.  If you use one of the reserved name the constructor raises a\n    `ValueError`.\n\n    Args:\n      hparam_def: Serialized hyperparameters, encoded as a hparam_pb2.HParamDef\n        protocol buffer. If provided, this object is initialized by\n        deserializing hparam_def.  Otherwise **kwargs is used.\n      model_structure: An instance of ModelStructure, defining the feature\n        crosses to be used in the Trial.\n      **kwargs: Key-value pairs where the key is the hyperparameter name and\n        the value is the value for the parameter.\n\n    Raises:\n      ValueError: If both `hparam_def` and initialization values are provided,\n        or if one of the arguments is invalid.\n\n    """"""\n    # Register the hyperparameters and their type in _hparam_types.\n    # This simplifies the implementation of parse().\n    # _hparam_types maps the parameter name to a tuple (type, bool).\n    # The type value is the type of the parameter for scalar hyperparameters,\n    # or the type of the list elements for multidimensional hyperparameters.\n    # The bool value is True if the value is a list, False otherwise.\n    self._hparam_types = {}\n    self._model_structure = model_structure\n    if hparam_def:\n##       self._init_from_proto(hparam_def)\n##       if kwargs:\n##         raise ValueError(\'hparam_def and initialization values are \'\n##                          \'mutually exclusive\')\n      raise ValueError(\'hparam_def has been disabled in this version\')\n    else:\n      for name, value in six.iteritems(kwargs):\n        self.add_hparam(name, value)\n\n##   def _init_from_proto(self, hparam_def):\n##     """"""Creates a new HParams from `HParamDef` protocol buffer.\n## \n##     Args:\n##       hparam_def: `HParamDef` protocol buffer.\n##     """"""\n##     assert isinstance(hparam_def, hparam_pb2.HParamDef)\n##     for name, value in hparam_def.hparam.items():\n##       kind = value.WhichOneof(\'kind\')\n##       if kind.endswith(\'_value\'):\n##         # Single value.\n##         if kind.startswith(\'int64\'):\n##           # Setting attribute value to be \'int\' to ensure the type is compatible\n##           # with both Python2 and Python3.\n##           self.add_hparam(name, int(getattr(value, kind)))\n##         elif kind.startswith(\'bytes\'):\n##           # Setting attribute value to be \'str\' to ensure the type is compatible\n##           # with both Python2 and Python3. UTF-8 encoding is assumed.\n##           self.add_hparam(name, compat.as_str(getattr(value, kind)))\n##         else:\n##           self.add_hparam(name, getattr(value, kind))\n##       else:\n##         # List of values.\n##         if kind.startswith(\'int64\'):\n##           # Setting attribute value to be \'int\' to ensure the type is compatible\n##           # with both Python2 and Python3.\n##           self.add_hparam(name, [int(v) for v in getattr(value, kind).value])\n##         elif kind.startswith(\'bytes\'):\n##           # Setting attribute value to be \'str\' to ensure the type is compatible\n##           # with both Python2 and Python3. UTF-8 encoding is assumed.\n##           self.add_hparam(\n##               name, [compat.as_str(v) for v in getattr(value, kind).value])\n##         else:\n##           self.add_hparam(name, [v for v in getattr(value, kind).value])\n\n  def add_hparam(self, name, value):\n    """"""Adds {name, value} pair to hyperparameters.\n\n    Args:\n      name: Name of the hyperparameter.\n      value: Value of the hyperparameter. Can be one of the following types:\n        int, float, string, int list, float list, or string list.\n\n    Raises:\n      ValueError: if one of the arguments is invalid.\n    """"""\n    # Keys in kwargs are unique, but \'name\' could the name of a pre-existing\n    # attribute of this object.  In that case we refuse to use it as a\n    # hyperparameter name.\n    if getattr(self, name, None) is not None:\n      raise ValueError(\'Hyperparameter name is reserved: %s\' % name)\n    if isinstance(value, (list, tuple)):\n      if not value:\n        raise ValueError(\n            \'Multi-valued hyperparameters cannot be empty: %s\' % name)\n      self._hparam_types[name] = (type(value[0]), True)\n    else:\n      self._hparam_types[name] = (type(value), False)\n    setattr(self, name, value)\n\n  def set_hparam(self, name, value):\n    """"""Set the value of an existing hyperparameter.\n\n    This function verifies that the type of the value matches the type of the\n    existing hyperparameter.\n\n    Args:\n      name: Name of the hyperparameter.\n      value: New value of the hyperparameter.\n\n    Raises:\n      ValueError: If there is a type mismatch.\n    """"""\n    param_type, is_list = self._hparam_types[name]\n    if isinstance(value, list):\n      if not is_list:\n        raise ValueError(\n            \'Must not pass a list for single-valued parameter: %s\' % name)\n      setattr(self, name, [\n          _cast_to_type_if_compatible(name, param_type, v) for v in value])\n    else:\n      if is_list:\n        raise ValueError(\n            \'Must pass a list for multi-valued parameter: %s.\' % name)\n      setattr(self, name, _cast_to_type_if_compatible(name, param_type, value))\n\n  def del_hparam(self, name):\n    """"""Removes the hyperparameter with key \'name\'.\n\n    Args:\n      name: Name of the hyperparameter.\n    """"""\n    if hasattr(self, name):\n      delattr(self, name)\n      del self._hparam_types[name]\n\n  def parse(self, values):\n    """"""Override hyperparameter values, parsing new values from a string.\n\n    See parse_values for more detail on the allowed format for values.\n\n    Args:\n      values: String.  Comma separated list of `name=value` pairs where\n        \'value\' must follow the syntax described above.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values` cannot be parsed.\n    """"""\n    type_map = dict()\n    for name, t in self._hparam_types.items():\n      param_type, _ = t\n      type_map[name] = param_type\n\n    values_map = parse_values(values, type_map)\n    return self.override_from_dict(values_map)\n\n  def override_from_dict(self, values_dict):\n    """"""Override hyperparameter values, parsing new values from a dictionary.\n\n    Args:\n      values_dict: Dictionary of name:value pairs.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values_dict` cannot be parsed.\n    """"""\n    for name, value in values_dict.items():\n      self.set_hparam(name, value)\n    return self\n\n##   @deprecation.deprecated(None, \'Use `override_from_dict`.\')\n  def set_from_map(self, values_map):\n    """"""DEPRECATED. Use override_from_dict.""""""\n    return self.override_from_dict(values_dict=values_map)\n\n  def set_model_structure(self, model_structure):\n    self._model_structure = model_structure\n\n  def get_model_structure(self):\n    return self._model_structure\n\n  def to_json(self, indent=None, separators=None, sort_keys=False):\n    """"""Serializes the hyperparameters into JSON.\n\n    Args:\n      indent: If a non-negative integer, JSON array elements and object members\n        will be pretty-printed with that indent level. An indent level of 0, or\n        negative, will only insert newlines. `None` (the default) selects the\n        most compact representation.\n      separators: Optional `(item_separator, key_separator)` tuple. Default is\n        `(\', \', \': \')`.\n      sort_keys: If `True`, the output dictionaries will be sorted by key.\n\n    Returns:\n      A JSON string.\n    """"""\n    return json.dumps(\n        self.values(),\n        indent=indent,\n        separators=separators,\n        sort_keys=sort_keys)\n\n  def parse_json(self, values_json):\n    """"""Override hyperparameter values, parsing new values from a json object.\n\n    Args:\n      values_json: String containing a json object of name:value pairs.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values_json` cannot be parsed.\n    """"""\n    values_map = json.loads(values_json)\n    return self.override_from_dict(values_map)\n\n  def values(self):\n    """"""Return the hyperparameter values as a Python dictionary.\n\n    Returns:\n      A dictionary with hyperparameter names as keys.  The values are the\n      hyperparameter values.\n    """"""\n    return {n: getattr(self, n) for n in self._hparam_types.keys()}\n\n  def get(self, key, default=None):\n    """"""Returns the value of `key` if it exists, else `default`.""""""\n    if key in self._hparam_types:\n      # Ensure that default is compatible with the parameter type.\n      if default is not None:\n        param_type, is_param_list = self._hparam_types[key]\n        type_str = \'list<%s>\' % param_type if is_param_list else str(param_type)\n        fail_msg = (""Hparam \'%s\' of type \'%s\' is incompatible with ""\n                    \'default=%s\' % (key, type_str, default))\n\n        is_default_list = isinstance(default, list)\n        if is_param_list != is_default_list:\n          raise ValueError(fail_msg)\n\n        try:\n          if is_default_list:\n            for value in default:\n              _cast_to_type_if_compatible(key, param_type, value)\n          else:\n            _cast_to_type_if_compatible(key, param_type, default)\n        except ValueError as e:\n          raise ValueError(\'%s. %s\' % (fail_msg, e))\n\n      return getattr(self, key)\n\n    return default\n\n  def __contains__(self, key):\n    return key in self._hparam_types\n\n  def __str__(self):\n    return str(sorted(self.values().items()))\n\n  def __repr__(self):\n    return \'%s(%s)\' % (type(self).__name__, self.__str__())\n\n  @staticmethod\n  def _get_kind_name(param_type, is_list):\n    """"""Returns the field name given parameter type and is_list.\n\n    Args:\n      param_type: Data type of the hparam.\n      is_list: Whether this is a list.\n\n    Returns:\n      A string representation of the field name.\n\n    Raises:\n      ValueError: If parameter type is not recognized.\n    """"""\n    if issubclass(param_type, bool):\n      # This check must happen before issubclass(param_type, six.integer_types),\n      # since Python considers bool to be a subclass of int.\n      typename = \'bool\'\n    elif issubclass(param_type, six.integer_types):\n      # Setting \'int\' and \'long\' types to be \'int64\' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = \'int64\'\n    elif issubclass(param_type, (six.string_types, six.binary_type)):\n      # Setting \'string\' and \'bytes\' types to be \'bytes\' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = \'bytes\'\n    elif issubclass(param_type, float):\n      typename = \'float\'\n    else:\n      raise ValueError(\'Unsupported parameter type: %s\' % str(param_type))\n\n    suffix = \'list\' if is_list else \'value\'\n    return \'_\'.join([typename, suffix])\n\n##   def to_proto(self, export_scope=None):  # pylint: disable=unused-argument\n##     """"""Converts a `HParams` object to a `HParamDef` protocol buffer.\n## \n##     Args:\n##       export_scope: Optional `string`. Name scope to remove.\n## \n##     Returns:\n##       A `HParamDef` protocol buffer.\n##     """"""\n##     hparam_proto = hparam_pb2.HParamDef()\n##     for name in self._hparam_types:\n##       # Parse the values.\n##       param_type, is_list = self._hparam_types.get(name, (None, None))\n##       kind = HParams._get_kind_name(param_type, is_list)\n## \n##       if is_list:\n##         if kind.startswith(\'bytes\'):\n##           v_list = [compat.as_bytes(v) for v in getattr(self, name)]\n##         else:\n##           v_list = [v for v in getattr(self, name)]\n##         getattr(hparam_proto.hparam[name], kind).value.extend(v_list)\n##       else:\n##         v = getattr(self, name)\n##         if kind.startswith(\'bytes\'):\n##           v = compat.as_bytes(getattr(self, name))\n##         setattr(hparam_proto.hparam[name], kind, v)\n## \n##     return hparam_proto\n\n##   @staticmethod\n##   def from_proto(hparam_def, import_scope=None):  # pylint: disable=unused-argument\n##     return HParams(hparam_def=hparam_def)\n\n\n## ops.register_proto_function(\n##     \'hparams\',\n##     proto_type=hparam_pb2.HParamDef,\n##     to_proto=HParams.to_proto,\n##     from_proto=HParams.from_proto)\n'"
deepvoice3_pytorch/frontend/en/__init__.py,3,"b'# coding: utf-8\nfrom deepvoice3_pytorch.frontend.text.symbols import symbols\n\nimport nltk\nfrom random import random\n\nn_vocab = len(symbols)\n\n_arpabet = nltk.corpus.cmudict.dict()\n\n\ndef _maybe_get_arpabet(word, p):\n    try:\n        phonemes = _arpabet[word][0]\n        phonemes = "" "".join(phonemes)\n    except KeyError:\n        return word\n\n    return \'{%s}\' % phonemes if random() < p else word\n\n\ndef mix_pronunciation(text, p):\n    text = \' \'.join(_maybe_get_arpabet(word, p) for word in text.split(\' \'))\n    return text\n\n\ndef text_to_sequence(text, p=0.0):\n    if p >= 0:\n        text = mix_pronunciation(text, p)\n    from deepvoice3_pytorch.frontend.text import text_to_sequence\n    text = text_to_sequence(text, [""english_cleaners""])\n    return text\n\n\nfrom deepvoice3_pytorch.frontend.text import sequence_to_text\n'"
deepvoice3_pytorch/frontend/es/__init__.py,3,"b'# coding: utf-8\nfrom deepvoice3_pytorch.frontend.text.symbols import symbols\n\nimport nltk\nfrom random import random\n\nn_vocab = len(symbols)\n\n\ndef text_to_sequence(text, p=0.0):\n    from deepvoice3_pytorch.frontend.text import text_to_sequence\n    text = text_to_sequence(text, [""basic_cleaners""])\n    return text\n\n\nfrom deepvoice3_pytorch.frontend.text import sequence_to_text\n'"
deepvoice3_pytorch/frontend/jp/__init__.py,0,"b'# coding: utf-8\n\n\nimport MeCab\nimport jaconv\nfrom random import random\n\nn_vocab = 0xffff\n\n_eos = 1\n_pad = 0\n_tagger = None\n\n\ndef _yomi(mecab_result):\n    tokens = []\n    yomis = []\n    for line in mecab_result.split(""\\n"")[:-1]:\n        s = line.split(""\\t"")\n        if len(s) == 1:\n            break\n        token, rest = s\n        rest = rest.split("","")\n        tokens.append(token)\n        yomi = rest[7] if len(rest) > 7 else None\n        yomi = None if yomi == ""*"" else yomi\n        yomis.append(yomi)\n\n    return tokens, yomis\n\n\ndef _mix_pronunciation(tokens, yomis, p):\n    return """".join(\n        yomis[idx] if yomis[idx] is not None and random() < p else tokens[idx]\n        for idx in range(len(tokens)))\n\n\ndef mix_pronunciation(text, p):\n    global _tagger\n    if _tagger is None:\n        _tagger = MeCab.Tagger("""")\n    tokens, yomis = _yomi(_tagger.parse(text))\n    return _mix_pronunciation(tokens, yomis, p)\n\n\ndef add_punctuation(text):\n    last = text[-1]\n    if last not in [""."", "","", ""\xe3\x80\x81"", ""\xe3\x80\x82"", ""\xef\xbc\x81"", ""\xef\xbc\x9f"", ""!"", ""?""]:\n        text = text + ""\xe3\x80\x82""\n    return text\n\n\ndef normalize_delimitor(text):\n    text = text.replace("","", ""\xe3\x80\x81"")\n    text = text.replace(""."", ""\xe3\x80\x82"")\n    text = text.replace(""\xef\xbc\x8c"", ""\xe3\x80\x81"")\n    text = text.replace(""\xef\xbc\x8e"", ""\xe3\x80\x82"")\n    return text\n\n\ndef text_to_sequence(text, p=0.0):\n    for c in ["" "", ""\xe3\x80\x80"", ""\xe3\x80\x8c"", ""\xe3\x80\x8d"", ""\xe3\x80\x8e"", ""\xe3\x80\x8f"", ""\xe3\x83\xbb"", ""\xe3\x80\x90"", ""\xe3\x80\x91"",\n              ""\xef\xbc\x88"", ""\xef\xbc\x89"", ""("", "")""]:\n        text = text.replace(c, """")\n    text = text.replace(""!"", ""\xef\xbc\x81"")\n    text = text.replace(""?"", ""\xef\xbc\x9f"")\n\n    text = normalize_delimitor(text)\n    text = jaconv.normalize(text)\n    if p > 0:\n        text = mix_pronunciation(text, p)\n    text = jaconv.hira2kata(text)\n    text = add_punctuation(text)\n\n    return [ord(c) for c in text] + [_eos]  # EOS\n\n\ndef sequence_to_text(seq):\n    return """".join(chr(n) for n in seq)\n'"
deepvoice3_pytorch/frontend/ko/__init__.py,0,"b'# coding: utf-8\n\n\nfrom random import random\n\nn_vocab = 0xffff\n\n_eos = 1\n_pad = 0\n_tagger = None\n\n\ndef text_to_sequence(text, p=0.0):\n    return [ord(c) for c in text] + [_eos]  # EOS\n\ndef sequence_to_text(seq):\n    return """".join(chr(n) for n in seq)\n'"
deepvoice3_pytorch/frontend/text/__init__.py,2,"b'import re\nfrom deepvoice3_pytorch.frontend.text import cleaners\nfrom deepvoice3_pytorch.frontend.text.symbols import symbols\n\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n\ndef text_to_sequence(text, cleaner_names):\n    \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n      The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n      in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n      Args:\n        text: string to convert to a sequence\n        cleaner_names: names of the cleaner functions to run the text through\n\n      Returns:\n        List of integers corresponding to the symbols in the text\n    \'\'\'\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[\'~\'])\n    return sequence\n\n\ndef sequence_to_text(sequence):\n    \'\'\'Converts a sequence of IDs back to a string\'\'\'\n    result = \'\'\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \'@\':\n                s = \'{%s}\' % s[1:]\n            result += s\n    return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n    for name in cleaner_names:\n        cleaner = getattr(cleaners, name)\n        if not cleaner:\n            raise Exception(\'Unknown cleaner: %s\' % name)\n        text = cleaner(text)\n    return text\n\n\ndef _symbols_to_sequence(symbols):\n    return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n    return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n    return s in _symbol_to_id and s is not \'_\' and s is not \'~\'\n'"
deepvoice3_pytorch/frontend/text/cleaners.py,0,"b'\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1]) for x in [\n    (\'mrs\', \'misess\'),\n    (\'mr\', \'mister\'),\n    (\'dr\', \'doctor\'),\n    (\'st\', \'saint\'),\n    (\'co\', \'company\'),\n    (\'jr\', \'junior\'),\n    (\'maj\', \'major\'),\n    (\'gen\', \'general\'),\n    (\'drs\', \'doctors\'),\n    (\'rev\', \'reverend\'),\n    (\'lt\', \'lieutenant\'),\n    (\'hon\', \'honorable\'),\n    (\'sgt\', \'sergeant\'),\n    (\'capt\', \'captain\'),\n    (\'esq\', \'esquire\'),\n    (\'ltd\', \'limited\'),\n    (\'col\', \'colonel\'),\n    (\'ft\', \'fort\'),\n]]\n\n\ndef expand_abbreviations(text):\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef expand_numbers(text):\n    return normalize_numbers(text)\n\n\ndef lowercase(text):\n    return text.lower()\n\n\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \' \', text)\n\n\ndef convert_to_ascii(text):\n    return unidecode(text)\n\n\ndef add_punctuation(text):\n    if len(text) == 0:\n        return text\n    if text[-1] not in \'!,.:;?\':\n        text = text + \'.\'  # without this decoder is confused when to output EOS\n    return text\n\n\ndef basic_cleaners(text):\n    \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef transliteration_cleaners(text):\n    \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef english_cleaners(text):\n    \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n    text = convert_to_ascii(text)\n    text = add_punctuation(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n'"
deepvoice3_pytorch/frontend/text/cmudict.py,0,"b'import re\n\n\nvalid_symbols = [\n    \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\', \'AH2\',\n    \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\', \'AY1\', \'AY2\',\n    \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\', \'ER1\', \'ER2\', \'EY\',\n    \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\', \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\',\n    \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\', \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\',\n    \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\', \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\',\n    \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\', \'Y\', \'Z\', \'ZH\'\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n    \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n\n    def __init__(self, file_or_path, keep_ambiguous=True):\n        if isinstance(file_or_path, str):\n            with open(file_or_path, encoding=\'latin-1\') as f:\n                entries = _parse_cmudict(f)\n        else:\n            entries = _parse_cmudict(file_or_path)\n        if not keep_ambiguous:\n            entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n        self._entries = entries\n\n    def __len__(self):\n        return len(self._entries)\n\n    def lookup(self, word):\n        \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n        return self._entries.get(word.upper())\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n    cmudict = {}\n    for line in file:\n        if len(line) and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n            parts = line.split(\'  \')\n            word = re.sub(_alt_re, \'\', parts[0])\n            pronunciation = _get_pronunciation(parts[1])\n            if pronunciation:\n                if word in cmudict:\n                    cmudict[word].append(pronunciation)\n                else:\n                    cmudict[word] = [pronunciation]\n    return cmudict\n\n\ndef _get_pronunciation(s):\n    parts = s.strip().split(\' \')\n    for part in parts:\n        if part not in _valid_symbol_set:\n            return None\n    return \' \'.join(parts)\n'"
deepvoice3_pytorch/frontend/text/numbers.py,0,"b""# -*- coding: utf-8 -*-\n\nimport inflect\nimport re\n\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n_pounds_re = re.compile(r'\xc2\xa3([0-9\\,]*[0-9]+)')\n_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n_number_re = re.compile(r'[0-9]+')\n\n\ndef _remove_commas(m):\n    return m.group(1).replace(',', '')\n\n\ndef _expand_decimal_point(m):\n    return m.group(1).replace('.', ' point ')\n\n\ndef _expand_dollars(m):\n    match = m.group(1)\n    parts = match.split('.')\n    if len(parts) > 2:\n        return match + ' dollars'  # Unexpected format\n    dollars = int(parts[0]) if parts[0] else 0\n    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n    if dollars and cents:\n        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n        cent_unit = 'cent' if cents == 1 else 'cents'\n        return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n    elif dollars:\n        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n        return '%s %s' % (dollars, dollar_unit)\n    elif cents:\n        cent_unit = 'cent' if cents == 1 else 'cents'\n        return '%s %s' % (cents, cent_unit)\n    else:\n        return 'zero dollars'\n\n\ndef _expand_ordinal(m):\n    return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n    num = int(m.group(0))\n    if num > 1000 and num < 3000:\n        if num == 2000:\n            return 'two thousand'\n        elif num > 2000 and num < 2010:\n            return 'two thousand ' + _inflect.number_to_words(num % 100)\n        elif num % 100 == 0:\n            return _inflect.number_to_words(num // 100) + ' hundred'\n        else:\n            return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n    else:\n        return _inflect.number_to_words(num, andword='')\n\n\ndef normalize_numbers(text):\n    text = re.sub(_comma_number_re, _remove_commas, text)\n    text = re.sub(_pounds_re, r'\\1 pounds', text)\n    text = re.sub(_dollars_re, _expand_dollars, text)\n    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n    text = re.sub(_ordinal_re, _expand_ordinal, text)\n    text = re.sub(_number_re, _expand_number, text)\n    return text\n"""
deepvoice3_pytorch/frontend/text/symbols.py,0,"b'\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n\'\'\'\nfrom .cmudict import valid_symbols\n\n_pad = \'_\'\n_eos = \'~\'\n_characters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'(),-.:;? \'\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\'@\' + s for s in valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad, _eos] + list(_characters) + _arpabet\n'"
