file_path,api_count,code
hparams.py,0,"b'import tensorflow as tf\n\n\n# Default hyperparameters:\nhparams = tf.contrib.training.HParams(\n    # Comma-separated list of cleaners to run on text prior to training and eval. For non-English\n    # text, you may want to use ""basic_cleaners"" or ""transliteration_cleaners"" See TRAINING_DATA.md.\n    cleaners=\'english_cleaners\',\n    use_cmudict=False,  # Use CMUDict during training to learn pronunciation of ARPAbet phonemes\n\n    # Audio:\n    num_mels=80,\n    num_freq=1025,\n    sample_rate=20000,\n    frame_length_ms=50,\n    frame_shift_ms=12.5,\n    preemphasis=0.97,\n    min_level_db=-100,\n    ref_level_db=20,\n\n    # Model:\n    # TODO: add more configurable hparams\n    outputs_per_step=5,\n    padding_idx=None,\n    use_memory_mask=False,\n\n    # Data loader\n    pin_memory=True,\n    num_workers=2,\n\n    # Training:\n    batch_size=32,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    initial_learning_rate=0.002,\n    decay_learning_rate=True,\n    nepochs=1000,\n    weight_decay=0.0,\n    clip_thresh=1.0,\n\n    # Save\n    checkpoint_interval=5000,\n\n    # Eval:\n    max_iters=200,\n    griffin_lim_iters=60,\n    power=1.5,              # Power to raise magnitudes to prior to Griffin-Lim\n)\n\n\ndef hparams_debug_string():\n    values = hparams.values()\n    hp = [\'  %s: %s\' % (name, values[name]) for name in sorted(values)]\n    return \'Hyperparameters:\\n\' + \'\\n\'.join(hp)\n'"
setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\nimport setuptools.command.develop\nimport setuptools.command.build_py\nimport os\nimport subprocess\n\nversion = \'0.0.1\'\n\n# Adapted from https://github.com/pytorch/pytorch\ncwd = os.path.dirname(os.path.abspath(__file__))\nif os.getenv(\'TACOTRON_BUILD_VERSION\'):\n    version = os.getenv(\'TACOTRON_BUILD_VERSION\')\nelse:\n    try:\n        sha = subprocess.check_output(\n            [\'git\', \'rev-parse\', \'HEAD\'], cwd=cwd).decode(\'ascii\').strip()\n        version += \'+\' + sha[:7]\n    except subprocess.CalledProcessError:\n        pass\n\n\nclass build_py(setuptools.command.build_py.build_py):\n\n    def run(self):\n        self.create_version_file()\n        setuptools.command.build_py.build_py.run(self)\n\n    @staticmethod\n    def create_version_file():\n        global version, cwd\n        print(\'-- Building version \' + version)\n        version_path = os.path.join(cwd, \'tacotron_pytorch\', \'version.py\')\n        with open(version_path, \'w\') as f:\n            f.write(""__version__ = \'{}\'\\n"".format(version))\n\n\nclass develop(setuptools.command.develop.develop):\n\n    def run(self):\n        build_py.create_version_file()\n        setuptools.command.develop.develop.run(self)\n\n\nsetup(name=\'tacotron_pytorch\',\n      version=version,\n      description=\'PyTorch implementation of Tacotron speech synthesis model.\',\n      packages=find_packages(),\n      cmdclass={\n          \'build_py\': build_py,\n          \'develop\': develop,\n      },\n      install_requires=[\n          ""numpy"",\n          ""scipy"",\n      ],\n      extras_require={\n          ""train"": [\n              ""librosa"",\n              ""unidecode"",\n              ""inflect"",\n              ""docopt"",\n              ""tqdm"",\n              ""tensorboard_logger"",\n              ""nnmnkwii"",\n              ""nltk"",\n          ],\n          ""test"": [\n              ""nose"",\n              ""unidecode"",\n              ""inflect"",\n          ],\n      })\n'"
synthesis.py,4,"b'# coding: utf-8\n""""""\nSynthesis waveform from trained model.\n\nusage: tts.py [options] <checkpoint> <text_list_file> <dst_dir>\n\noptions:\n    --file-name-suffix=<s>   File name suffix [default: ].\n    --max-decoder-steps=<N>  Max decoder steps [default: 500].\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\n\n# Use text & audio modules from existing Tacotron implementation.\nimport sys\nimport os\nfrom os.path import dirname, join\ntacotron_lib_dir = join(dirname(__file__), ""lib"", ""tacotron"")\nsys.path.append(tacotron_lib_dir)\nfrom text import text_to_sequence, symbols\nfrom util import audio\nfrom util.plot import plot_alignment\n\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport nltk\n\nfrom tacotron_pytorch import Tacotron\nfrom hparams import hparams\n\nfrom tqdm import tqdm\n\nuse_cuda = torch.cuda.is_available()\n\n\ndef tts(model, text):\n    """"""Convert text to speech waveform given a Tacotron model.\n    """"""\n    if use_cuda:\n        model = model.cuda()\n    # TODO: Turning off dropout of decoder\'s prenet causes serious performance\n    # regression, not sure why.\n    # model.decoder.eval()\n    model.encoder.eval()\n    model.postnet.eval()\n\n    sequence = np.array(text_to_sequence(text, [hparams.cleaners]))\n    sequence = Variable(torch.from_numpy(sequence)).unsqueeze(0)\n    if use_cuda:\n        sequence = sequence.cuda()\n\n    # Greedy decoding\n    mel_outputs, linear_outputs, alignments = model(sequence)\n\n    linear_output = linear_outputs[0].cpu().data.numpy()\n    spectrogram = audio._denormalize(linear_output)\n    alignment = alignments[0].cpu().data.numpy()\n\n    # Predicted audio signal\n    waveform = audio.inv_spectrogram(linear_output.T)\n\n    return waveform, alignment, spectrogram\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    print(""Command line args:\\n"", args)\n    checkpoint_path = args[""<checkpoint>""]\n    text_list_file_path = args[""<text_list_file>""]\n    dst_dir = args[""<dst_dir>""]\n    max_decoder_steps = int(args[""--max-decoder-steps""])\n    file_name_suffix = args[""--file-name-suffix""]\n\n    model = Tacotron(n_vocab=len(symbols),\n                     embedding_dim=256,\n                     mel_dim=hparams.num_mels,\n                     linear_dim=hparams.num_freq,\n                     r=hparams.outputs_per_step,\n                     padding_idx=hparams.padding_idx,\n                     use_memory_mask=hparams.use_memory_mask,\n                     )\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[""state_dict""])\n    model.decoder.max_decoder_steps = max_decoder_steps\n\n    os.makedirs(dst_dir, exist_ok=True)\n\n    with open(text_list_file_path, ""rb"") as f:\n        lines = f.readlines()\n        for idx, line in enumerate(lines):\n            text = line.decode(""utf-8"")[:-1]\n            words = nltk.word_tokenize(text)\n            print(""{}: {} ({} chars, {} words)"".format(idx, text, len(text), len(words)))\n            waveform, alignment, _ = tts(model, text)\n            dst_wav_path = join(dst_dir, ""{}{}.wav"".format(idx, file_name_suffix))\n            dst_alignment_path = join(dst_dir, ""{}_alignment.png"".format(idx))\n            plot_alignment(alignment.T, dst_alignment_path,\n                           info=""tacotron, {}"".format(checkpoint_path))\n            audio.save_wav(waveform, dst_wav_path)\n\n    print(""Finished! Check out {} for generated audio samples."".format(dst_dir))\n    sys.exit(0)\n'"
train.py,13,"b'""""""Trainining script for Tacotron speech synthesis model.\n\nusage: train.py [options]\n\noptions:\n    --data-root=<dir>         Directory contains preprocessed features.\n    --checkpoint-dir=<dir>    Directory where to save model checkpoints [default: checkpoints].\n    --checkpoint-path=<name>  Restore model from checkpoint path if given.\n    --hparams=<parmas>        Hyper parameters [default: ].\n    -h, --help                Show this help message and exit\n""""""\nfrom docopt import docopt\n\n# Use text & audio modules from existing Tacotron implementation.\nimport sys\nfrom os.path import dirname, join\ntacotron_lib_dir = join(dirname(__file__), ""lib"", ""tacotron"")\nsys.path.append(tacotron_lib_dir)\nfrom text import text_to_sequence, symbols\nfrom util import audio\nfrom util.plot import plot_alignment\nfrom tqdm import tqdm, trange\n\n# The tacotron model\nfrom tacotron_pytorch import Tacotron\n\nimport torch\nfrom torch.utils import data as data_utils\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch import optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils import data as data_utils\nimport numpy as np\n\nfrom nnmnkwii.datasets import FileSourceDataset, FileDataSource\nfrom os.path import join, expanduser\n\nimport librosa.display\nfrom matplotlib import pyplot as plt\nimport sys\nimport os\nimport tensorboard_logger\nfrom tensorboard_logger import log_value\nfrom hparams import hparams, hparams_debug_string\n\n# Default DATA_ROOT\nDATA_ROOT = join(expanduser(""~""), ""tacotron"", ""training"")\n\nfs = hparams.sample_rate\n\nglobal_step = 0\nglobal_epoch = 0\nuse_cuda = torch.cuda.is_available()\nif use_cuda:\n    cudnn.benchmark = False\n\n\ndef _pad(seq, max_len):\n    return np.pad(seq, (0, max_len - len(seq)),\n                  mode=\'constant\', constant_values=0)\n\n\ndef _pad_2d(x, max_len):\n    x = np.pad(x, [(0, max_len - len(x)), (0, 0)],\n               mode=""constant"", constant_values=0)\n    return x\n\n\nclass TextDataSource(FileDataSource):\n    def __init__(self):\n        self._cleaner_names = [x.strip() for x in hparams.cleaners.split(\',\')]\n\n    def collect_files(self):\n        meta = join(DATA_ROOT, ""train.txt"")\n        with open(meta, ""rb"") as f:\n            lines = f.readlines()\n        lines = list(map(lambda l: l.decode(""utf-8"").split(""|"")[-1], lines))\n        return lines\n\n    def collect_features(self, text):\n        return np.asarray(text_to_sequence(text, self._cleaner_names),\n                          dtype=np.int32)\n\n\nclass _NPYDataSource(FileDataSource):\n    def __init__(self, col):\n        self.col = col\n\n    def collect_files(self):\n        meta = join(DATA_ROOT, ""train.txt"")\n        with open(meta, ""rb"") as f:\n            lines = f.readlines()\n        lines = list(map(lambda l: l.decode(""utf-8"").split(""|"")[self.col], lines))\n        paths = list(map(lambda f: join(DATA_ROOT, f), lines))\n        return paths\n\n    def collect_features(self, path):\n        return np.load(path)\n\n\nclass MelSpecDataSource(_NPYDataSource):\n    def __init__(self):\n        super(MelSpecDataSource, self).__init__(1)\n\n\nclass LinearSpecDataSource(_NPYDataSource):\n    def __init__(self):\n        super(LinearSpecDataSource, self).__init__(0)\n\n\nclass PyTorchDataset(object):\n    def __init__(self, X, Mel, Y):\n        self.X = X\n        self.Mel = Mel\n        self.Y = Y\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.Mel[idx], self.Y[idx]\n\n    def __len__(self):\n        return len(self.X)\n\n\ndef collate_fn(batch):\n    """"""Create batch""""""\n    r = hparams.outputs_per_step\n    input_lengths = [len(x[0]) for x in batch]\n    max_input_len = np.max(input_lengths)\n    # Add single zeros frame at least, so plus 1\n    max_target_len = np.max([len(x[1]) for x in batch]) + 1\n    if max_target_len % r != 0:\n        max_target_len += r - max_target_len % r\n        assert max_target_len % r == 0\n\n    a = np.array([_pad(x[0], max_input_len) for x in batch], dtype=np.int)\n    x_batch = torch.LongTensor(a)\n\n    input_lengths = torch.LongTensor(input_lengths)\n\n    b = np.array([_pad_2d(x[1], max_target_len) for x in batch],\n                 dtype=np.float32)\n    mel_batch = torch.FloatTensor(b)\n\n    c = np.array([_pad_2d(x[2], max_target_len) for x in batch],\n                 dtype=np.float32)\n    y_batch = torch.FloatTensor(c)\n    return x_batch, input_lengths, mel_batch, y_batch\n\n\ndef save_alignment(path, attn):\n    plot_alignment(attn.T, path, info=""tacotron, step={}"".format(global_step))\n\n\ndef save_spectrogram(path, linear_output):\n    spectrogram = audio._denormalize(linear_output)\n    plt.figure(figsize=(16, 10))\n    plt.imshow(spectrogram.T, aspect=""auto"", origin=""lower"")\n    plt.colorbar()\n    plt.tight_layout()\n    plt.savefig(path, format=""png"")\n    plt.close()\n\n\ndef _learning_rate_decay(init_lr, global_step):\n    warmup_steps = 4000.0\n    step = global_step + 1.\n    lr = init_lr * warmup_steps**0.5 * np.minimum(\n        step * warmup_steps**-1.5, step**-0.5)\n    return lr\n\n\ndef save_states(global_step, mel_outputs, linear_outputs, attn, y,\n                input_lengths, checkpoint_dir=None):\n    print(""Save intermediate states at step {}"".format(global_step))\n\n    # idx = np.random.randint(0, len(input_lengths))\n    idx = min(1, len(input_lengths) - 1)\n    input_length = input_lengths[idx]\n\n    # Alignment\n    path = join(checkpoint_dir, ""step{}_alignment.png"".format(\n        global_step))\n    # alignment = attn[idx].cpu().data.numpy()[:, :input_length]\n    alignment = attn[idx].cpu().data.numpy()\n    save_alignment(path, alignment)\n\n    # Predicted spectrogram\n    path = join(checkpoint_dir, ""step{}_predicted_spectrogram.png"".format(\n        global_step))\n    linear_output = linear_outputs[idx].cpu().data.numpy()\n    save_spectrogram(path, linear_output)\n\n    # Predicted audio signal\n    signal = audio.inv_spectrogram(linear_output.T)\n    path = join(checkpoint_dir, ""step{}_predicted.wav"".format(\n        global_step))\n    audio.save_wav(signal, path)\n\n    # Target spectrogram\n    path = join(checkpoint_dir, ""step{}_target_spectrogram.png"".format(\n        global_step))\n    linear_output = y[idx].cpu().data.numpy()\n    save_spectrogram(path, linear_output)\n\n\ndef train(model, data_loader, optimizer,\n          init_lr=0.002,\n          checkpoint_dir=None, checkpoint_interval=None, nepochs=None,\n          clip_thresh=1.0):\n    model.train()\n    if use_cuda:\n        model = model.cuda()\n    linear_dim = model.linear_dim\n\n    criterion = nn.L1Loss()\n\n    global global_step, global_epoch\n    while global_epoch < nepochs:\n        running_loss = 0.\n        for step, (x, input_lengths, mel, y) in tqdm(enumerate(data_loader)):\n            # Decay learning rate\n            current_lr = _learning_rate_decay(init_lr, global_step)\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = current_lr\n\n            optimizer.zero_grad()\n\n            # Sort by length\n            sorted_lengths, indices = torch.sort(\n                input_lengths.view(-1), dim=0, descending=True)\n            sorted_lengths = sorted_lengths.long().numpy()\n\n            x, mel, y = x[indices], mel[indices], y[indices]\n\n            # Feed data\n            x, mel, y = Variable(x), Variable(mel), Variable(y)\n            if use_cuda:\n                x, mel, y = x.cuda(), mel.cuda(), y.cuda()\n            mel_outputs, linear_outputs, attn = model(\n                x, mel, input_lengths=sorted_lengths)\n\n            # Loss\n            mel_loss = criterion(mel_outputs, mel)\n            n_priority_freq = int(3000 / (fs * 0.5) * linear_dim)\n            linear_loss = 0.5 * criterion(linear_outputs, y) \\\n                + 0.5 * criterion(linear_outputs[:, :, :n_priority_freq],\n                                  y[:, :, :n_priority_freq])\n            loss = mel_loss + linear_loss\n\n            if global_step > 0 and global_step % checkpoint_interval == 0:\n                save_states(\n                    global_step, mel_outputs, linear_outputs, attn, y,\n                    sorted_lengths, checkpoint_dir)\n                save_checkpoint(\n                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n\n            # Update\n            loss.backward()\n            grad_norm = torch.nn.utils.clip_grad_norm(\n                model.parameters(), clip_thresh)\n            optimizer.step()\n\n            # Logs\n            log_value(""loss"", float(loss.data[0]), global_step)\n            log_value(""mel loss"", float(mel_loss.data[0]), global_step)\n            log_value(""linear loss"", float(linear_loss.data[0]), global_step)\n            log_value(""gradient norm"", grad_norm, global_step)\n            log_value(""learning rate"", current_lr, global_step)\n\n            global_step += 1\n            running_loss += loss.data[0]\n\n        averaged_loss = running_loss / (len(data_loader))\n        log_value(""loss (per epoch)"", averaged_loss, global_epoch)\n        print(""Loss: {}"".format(running_loss / (len(data_loader))))\n\n        global_epoch += 1\n\n\ndef save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n    checkpoint_path = join(\n        checkpoint_dir, ""checkpoint_step{}.pth"".format(global_step))\n    torch.save({\n        ""state_dict"": model.state_dict(),\n        ""optimizer"": optimizer.state_dict(),\n        ""global_step"": step,\n        ""global_epoch"": epoch,\n    }, checkpoint_path)\n    print(""Saved checkpoint:"", checkpoint_path)\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    print(""Command line args:\\n"", args)\n    checkpoint_dir = args[""--checkpoint-dir""]\n    checkpoint_path = args[""--checkpoint-path""]\n    data_root = args[""--data-root""]\n    if data_root:\n        DATA_ROOT = data_root\n\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    # Input dataset definitions\n    X = FileSourceDataset(TextDataSource())\n    Mel = FileSourceDataset(MelSpecDataSource())\n    Y = FileSourceDataset(LinearSpecDataSource())\n\n    # Dataset and Dataloader setup\n    dataset = PyTorchDataset(X, Mel, Y)\n    data_loader = data_utils.DataLoader(\n        dataset, batch_size=hparams.batch_size,\n        num_workers=hparams.num_workers, shuffle=True,\n        collate_fn=collate_fn, pin_memory=hparams.pin_memory)\n\n    # Model\n    model = Tacotron(n_vocab=len(symbols),\n                     embedding_dim=256,\n                     mel_dim=hparams.num_mels,\n                     linear_dim=hparams.num_freq,\n                     r=hparams.outputs_per_step,\n                     padding_idx=hparams.padding_idx,\n                     use_memory_mask=hparams.use_memory_mask,\n                     )\n    optimizer = optim.Adam(model.parameters(),\n                           lr=hparams.initial_learning_rate, betas=(\n                               hparams.adam_beta1, hparams.adam_beta2),\n                           weight_decay=hparams.weight_decay)\n\n    # Load checkpoint\n    if checkpoint_path:\n        print(""Load checkpoint from: {}"".format(checkpoint_path))\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint[""state_dict""])\n        optimizer.load_state_dict(checkpoint[""optimizer""])\n        try:\n            global_step = checkpoint[""global_step""]\n            global_epoch = checkpoint[""global_epoch""]\n        except:\n            # TODO\n            pass\n\n    # Setup tensorboard logger\n    tensorboard_logger.configure(""log/run-test"")\n\n    print(hparams_debug_string())\n\n    # Train!\n    try:\n        train(model, data_loader, optimizer,\n              init_lr=hparams.initial_learning_rate,\n              checkpoint_dir=checkpoint_dir,\n              checkpoint_interval=hparams.checkpoint_interval,\n              nepochs=hparams.nepochs,\n              clip_thresh=hparams.clip_thresh)\n    except KeyboardInterrupt:\n        save_checkpoint(\n            model, optimizer, global_step, checkpoint_dir, global_epoch)\n\n    print(""Finished"")\n    sys.exit(0)\n'"
tacotron_pytorch/__init__.py,0,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nfrom .tacotron import Tacotron\nfrom .version import __version__\n'"
tacotron_pytorch/attention.py,4,"b'import torch\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, dim):\n        super(BahdanauAttention, self).__init__()\n        self.query_layer = nn.Linear(dim, dim, bias=False)\n        self.tanh = nn.Tanh()\n        self.v = nn.Linear(dim, 1, bias=False)\n\n    def forward(self, query, processed_memory):\n        """"""\n        Args:\n            query: (batch, 1, dim) or (batch, dim)\n            processed_memory: (batch, max_time, dim)\n        """"""\n        if query.dim() == 2:\n            # insert time-axis for broadcasting\n            query = query.unsqueeze(1)\n        # (batch, 1, dim)\n        processed_query = self.query_layer(query)\n\n        # (batch, max_time, 1)\n        alignment = self.v(self.tanh(processed_query + processed_memory))\n\n        # (batch, max_time)\n        return alignment.squeeze(-1)\n\n\ndef get_mask_from_lengths(memory, memory_lengths):\n    """"""Get mask tensor from list of length\n\n    Args:\n        memory: (batch, max_time, dim)\n        memory_lengths: array like\n    """"""\n    mask = memory.data.new(memory.size(0), memory.size(1)).byte().zero_()\n    for idx, l in enumerate(memory_lengths):\n        mask[idx][:l] = 1\n    return ~mask\n\n\nclass AttentionWrapper(nn.Module):\n    def __init__(self, rnn_cell, attention_mechanism,\n                 score_mask_value=-float(""inf"")):\n        super(AttentionWrapper, self).__init__()\n        self.rnn_cell = rnn_cell\n        self.attention_mechanism = attention_mechanism\n        self.score_mask_value = score_mask_value\n\n    def forward(self, query, attention, cell_state, memory,\n                processed_memory=None, mask=None, memory_lengths=None):\n        if processed_memory is None:\n            processed_memory = memory\n        if memory_lengths is not None and mask is None:\n            mask = get_mask_from_lengths(memory, memory_lengths)\n\n        # Concat input query and previous attention context\n        cell_input = torch.cat((query, attention), -1)\n\n        # Feed it to RNN\n        cell_output = self.rnn_cell(cell_input, cell_state)\n\n        # Alignment\n        # (batch, max_time)\n        alignment = self.attention_mechanism(cell_output, processed_memory)\n\n        if mask is not None:\n            mask = mask.view(query.size(0), -1)\n            alignment.data.masked_fill_(mask, self.score_mask_value)\n\n        # Normalize attention weight\n        alignment = F.softmax(alignment)\n\n        # Attention context vector\n        # (batch, 1, dim)\n        attention = torch.bmm(alignment.unsqueeze(1), memory)\n\n        # (batch, dim)\n        attention = attention.squeeze(1)\n\n        return cell_output, attention, alignment\n'"
tacotron_pytorch/tacotron.py,5,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\n\nfrom .attention import BahdanauAttention, AttentionWrapper\nfrom .attention import get_mask_from_lengths\n\n\nclass Prenet(nn.Module):\n    def __init__(self, in_dim, sizes=[256, 128]):\n        super(Prenet, self).__init__()\n        in_sizes = [in_dim] + sizes[:-1]\n        self.layers = nn.ModuleList(\n            [nn.Linear(in_size, out_size)\n             for (in_size, out_size) in zip(in_sizes, sizes)])\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, inputs):\n        for linear in self.layers:\n            inputs = self.dropout(self.relu(linear(inputs)))\n        return inputs\n\n\nclass BatchNormConv1d(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size, stride, padding,\n                 activation=None):\n        super(BatchNormConv1d, self).__init__()\n        self.conv1d = nn.Conv1d(in_dim, out_dim,\n                                kernel_size=kernel_size,\n                                stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm1d(out_dim)\n        self.activation = activation\n\n    def forward(self, x):\n        x = self.conv1d(x)\n        if self.activation is not None:\n            x = self.activation(x)\n        return self.bn(x)\n\n\nclass Highway(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(Highway, self).__init__()\n        self.H = nn.Linear(in_size, out_size)\n        self.H.bias.data.zero_()\n        self.T = nn.Linear(in_size, out_size)\n        self.T.bias.data.fill_(-1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, inputs):\n        H = self.relu(self.H(inputs))\n        T = self.sigmoid(self.T(inputs))\n        return H * T + inputs * (1.0 - T)\n\n\nclass CBHG(nn.Module):\n    """"""CBHG module: a recurrent neural network composed of:\n        - 1-d convolution banks\n        - Highway networks + residual connections\n        - Bidirectional gated recurrent units\n    """"""\n\n    def __init__(self, in_dim, K=16, projections=[128, 128]):\n        super(CBHG, self).__init__()\n        self.in_dim = in_dim\n        self.relu = nn.ReLU()\n        self.conv1d_banks = nn.ModuleList(\n            [BatchNormConv1d(in_dim, in_dim, kernel_size=k, stride=1,\n                             padding=k // 2, activation=self.relu)\n             for k in range(1, K + 1)])\n        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n\n        in_sizes = [K * in_dim] + projections[:-1]\n        activations = [self.relu] * (len(projections) - 1) + [None]\n        self.conv1d_projections = nn.ModuleList(\n            [BatchNormConv1d(in_size, out_size, kernel_size=3, stride=1,\n                             padding=1, activation=ac)\n             for (in_size, out_size, ac) in zip(\n                 in_sizes, projections, activations)])\n\n        self.pre_highway = nn.Linear(projections[-1], in_dim, bias=False)\n        self.highways = nn.ModuleList(\n            [Highway(in_dim, in_dim) for _ in range(4)])\n\n        self.gru = nn.GRU(\n            in_dim, in_dim, 1, batch_first=True, bidirectional=True)\n\n    def forward(self, inputs, input_lengths=None):\n        # (B, T_in, in_dim)\n        x = inputs\n\n        # Needed to perform conv1d on time-axis\n        # (B, in_dim, T_in)\n        if x.size(-1) == self.in_dim:\n            x = x.transpose(1, 2)\n\n        T = x.size(-1)\n\n        # (B, in_dim*K, T_in)\n        # Concat conv1d bank outputs\n        x = torch.cat([conv1d(x)[:, :, :T] for conv1d in self.conv1d_banks], dim=1)\n        assert x.size(1) == self.in_dim * len(self.conv1d_banks)\n        x = self.max_pool1d(x)[:, :, :T]\n\n        for conv1d in self.conv1d_projections:\n            x = conv1d(x)\n\n        # (B, T_in, in_dim)\n        # Back to the original shape\n        x = x.transpose(1, 2)\n\n        if x.size(-1) != self.in_dim:\n            x = self.pre_highway(x)\n\n        # Residual connection\n        x += inputs\n        for highway in self.highways:\n            x = highway(x)\n\n        if input_lengths is not None:\n            x = nn.utils.rnn.pack_padded_sequence(\n                x, input_lengths, batch_first=True)\n\n        # (B, T_in, in_dim*2)\n        outputs, _ = self.gru(x)\n\n        if input_lengths is not None:\n            outputs, _ = nn.utils.rnn.pad_packed_sequence(\n                outputs, batch_first=True)\n\n        return outputs\n\n\nclass Encoder(nn.Module):\n    def __init__(self, in_dim):\n        super(Encoder, self).__init__()\n        self.prenet = Prenet(in_dim, sizes=[256, 128])\n        self.cbhg = CBHG(128, K=16, projections=[128, 128])\n\n    def forward(self, inputs, input_lengths=None):\n        inputs = self.prenet(inputs)\n        return self.cbhg(inputs, input_lengths)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, in_dim, r):\n        super(Decoder, self).__init__()\n        self.in_dim = in_dim\n        self.r = r\n        self.prenet = Prenet(in_dim * r, sizes=[256, 128])\n        # (prenet_out + attention context) -> output\n        self.attention_rnn = AttentionWrapper(\n            nn.GRUCell(256 + 128, 256),\n            BahdanauAttention(256)\n        )\n        self.memory_layer = nn.Linear(256, 256, bias=False)\n        self.project_to_decoder_in = nn.Linear(512, 256)\n\n        self.decoder_rnns = nn.ModuleList(\n            [nn.GRUCell(256, 256) for _ in range(2)])\n\n        self.proj_to_mel = nn.Linear(256, in_dim * r)\n        self.max_decoder_steps = 200\n\n    def forward(self, encoder_outputs, inputs=None, memory_lengths=None):\n        """"""\n        Decoder forward step.\n\n        If decoder inputs are not given (e.g., at testing time), as noted in\n        Tacotron paper, greedy decoding is adapted.\n\n        Args:\n            encoder_outputs: Encoder outputs. (B, T_encoder, dim)\n            inputs: Decoder inputs. i.e., mel-spectrogram. If None (at eval-time),\n              decoder outputs are used as decoder inputs.\n            memory_lengths: Encoder output (memory) lengths. If not None, used for\n              attention masking.\n        """"""\n        B = encoder_outputs.size(0)\n\n        processed_memory = self.memory_layer(encoder_outputs)\n        if memory_lengths is not None:\n            mask = get_mask_from_lengths(processed_memory, memory_lengths)\n        else:\n            mask = None\n\n        # Run greedy decoding if inputs is None\n        greedy = inputs is None\n\n        if inputs is not None:\n            # Grouping multiple frames if necessary\n            if inputs.size(-1) == self.in_dim:\n                inputs = inputs.view(B, inputs.size(1) // self.r, -1)\n            assert inputs.size(-1) == self.in_dim * self.r\n            T_decoder = inputs.size(1)\n\n        # go frames\n        initial_input = Variable(\n            encoder_outputs.data.new(B, self.in_dim * self.r).zero_())\n\n        # Init decoder states\n        attention_rnn_hidden = Variable(\n            encoder_outputs.data.new(B, 256).zero_())\n        decoder_rnn_hiddens = [Variable(\n            encoder_outputs.data.new(B, 256).zero_())\n            for _ in range(len(self.decoder_rnns))]\n        current_attention = Variable(\n            encoder_outputs.data.new(B, 256).zero_())\n\n        # Time first (T_decoder, B, in_dim)\n        if inputs is not None:\n            inputs = inputs.transpose(0, 1)\n\n        outputs = []\n        alignments = []\n\n        t = 0\n        current_input = initial_input\n        while True:\n            if t > 0:\n                current_input = outputs[-1] if greedy else inputs[t - 1]\n            # Prenet\n            current_input = self.prenet(current_input)\n\n            # Attention RNN\n            attention_rnn_hidden, current_attention, alignment = self.attention_rnn(\n                current_input, current_attention, attention_rnn_hidden,\n                encoder_outputs, processed_memory=processed_memory, mask=mask)\n\n            # Concat RNN output and attention context vector\n            decoder_input = self.project_to_decoder_in(\n                torch.cat((attention_rnn_hidden, current_attention), -1))\n\n            # Pass through the decoder RNNs\n            for idx in range(len(self.decoder_rnns)):\n                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                    decoder_input, decoder_rnn_hiddens[idx])\n                # Residual connectinon\n                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n\n            output = decoder_input\n            output = self.proj_to_mel(output)\n\n            outputs += [output]\n            alignments += [alignment]\n\n            t += 1\n\n            if greedy:\n                if t > 1 and is_end_of_frames(output):\n                    break\n                elif t > self.max_decoder_steps:\n                    print(""Warning! doesn\'t seems to be converged"")\n                    break\n            else:\n                if t >= T_decoder:\n                    break\n\n        assert greedy or len(outputs) == T_decoder\n\n        # Back to batch first\n        alignments = torch.stack(alignments).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n\n        return outputs, alignments\n\n\ndef is_end_of_frames(output, eps=0.2):\n    return (output.data <= eps).all()\n\n\nclass Tacotron(nn.Module):\n    def __init__(self, n_vocab, embedding_dim=256, mel_dim=80, linear_dim=1025,\n                 r=5, padding_idx=None, use_memory_mask=False):\n        super(Tacotron, self).__init__()\n        self.mel_dim = mel_dim\n        self.linear_dim = linear_dim\n        self.use_memory_mask = use_memory_mask\n        self.embedding = nn.Embedding(n_vocab, embedding_dim,\n                                      padding_idx=padding_idx)\n        # Trying smaller std\n        self.embedding.weight.data.normal_(0, 0.3)\n        self.encoder = Encoder(embedding_dim)\n        self.decoder = Decoder(mel_dim, r)\n\n        self.postnet = CBHG(mel_dim, K=8, projections=[256, mel_dim])\n        self.last_linear = nn.Linear(mel_dim * 2, linear_dim)\n\n    def forward(self, inputs, targets=None, input_lengths=None):\n        B = inputs.size(0)\n\n        inputs = self.embedding(inputs)\n        # (B, T\', in_dim)\n        encoder_outputs = self.encoder(inputs, input_lengths)\n\n        if self.use_memory_mask:\n            memory_lengths = input_lengths\n        else:\n            memory_lengths = None\n        # (B, T\', mel_dim*r)\n        mel_outputs, alignments = self.decoder(\n            encoder_outputs, targets, memory_lengths=memory_lengths)\n\n        # Post net processing below\n\n        # Reshape\n        # (B, T, mel_dim)\n        mel_outputs = mel_outputs.view(B, -1, self.mel_dim)\n\n        linear_outputs = self.postnet(mel_outputs)\n        linear_outputs = self.last_linear(linear_outputs)\n\n        return mel_outputs, linear_outputs, alignments\n'"
tests/test_attention.py,7,"b'import torch\nfrom torch.autograd import Variable\nfrom torch import nn\n\nfrom tacotron_pytorch.attention import BahdanauAttention, AttentionWrapper\nfrom tacotron_pytorch.attention import get_mask_from_lengths\n\n\ndef test_attention_wrapper():\n    B = 2\n\n    encoder_outputs = Variable(torch.rand(B, 100, 256))\n    memory_lengths = [100, 50]\n\n    mask = get_mask_from_lengths(encoder_outputs, memory_lengths)\n    print(""Mask size:"", mask.size())\n\n    memory_layer = nn.Linear(256, 256)\n    query = Variable(torch.rand(B, 128))\n\n    attention_mechanism = BahdanauAttention(256)\n\n    # Attention context + input\n    rnn = nn.GRUCell(256 + 128, 256)\n\n    attention_rnn = AttentionWrapper(rnn, attention_mechanism)\n    initial_attention = Variable(torch.zeros(B, 256))\n    cell_state = Variable(torch.zeros(B, 256))\n\n    processed_memory = memory_layer(encoder_outputs)\n\n    cell_output, attention, alignment = attention_rnn(\n        query, initial_attention, cell_state, encoder_outputs,\n        processed_memory=processed_memory,\n        mask=None, memory_lengths=memory_lengths)\n\n    print(""Cell output size:"", cell_output.size())\n    print(""Attention output size:"", attention.size())\n    print(""Alignment size:"", alignment.size())\n\n    assert (alignment.sum(-1) == 1).data.all()\n\n\ntest_attention_wrapper()\n'"
tests/test_tacotron.py,3,"b'# coding: utf-8\nimport sys\nfrom os.path import dirname, join\ntacotron_lib_dir = join(dirname(__file__), "".."", ""lib"", ""tacotron"")\nsys.path.append(tacotron_lib_dir)\nfrom text import text_to_sequence, symbols\nimport torch\nfrom torch.autograd import Variable\nfrom tacotron_pytorch import Tacotron\nimport numpy as np\n\n\ndef _pad(seq, max_len):\n    return np.pad(seq, (0, max_len - len(seq)),\n                  mode=\'constant\', constant_values=0)\n\n\ndef test_taco():\n    B, T_out, D_out = 2, 400, 80\n    r = 5\n    T_encoder = T_out // r\n\n    texts = [""Thank you very much."", ""Hello""]\n    seqs = [np.array(text_to_sequence(\n        t, [""english_cleaners""]), dtype=np.int) for t in texts]\n    input_lengths = np.array([len(s) for s in seqs])\n    max_len = np.max(input_lengths)\n    seqs = np.array([_pad(s, max_len) for s in seqs])\n\n    x = torch.LongTensor(seqs)\n    y = torch.rand(B, T_out, D_out)\n    x = Variable(x)\n    y = Variable(y)\n\n    model = Tacotron(n_vocab=len(symbols), r=r)\n\n    print(""Encoder input shape: "", x.size())\n    print(""Decoder input shape: "", y.size())\n    a, b, c = model(x, y, input_lengths=input_lengths)\n    print(""Mel shape:"", a.size())\n    print(""Linear shape:"", b.size())\n    print(""Attention shape:"", c.size())\n\n    assert c.size() == (B, T_encoder, max_len)\n\n    # Test greddy decoding\n    a, b, c = model(x, input_lengths=input_lengths)\n'"
