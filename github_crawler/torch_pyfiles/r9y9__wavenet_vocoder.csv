file_path,api_count,code
audio.py,0,"b'import librosa\nimport librosa.filters\nimport numpy as np\nfrom hparams import hparams\nfrom scipy.io import wavfile\nfrom nnmnkwii import preprocessing as P\n\n\ndef low_cut_filter(x, fs, cutoff=70):\n    """"""APPLY LOW CUT FILTER.\n\n    https://github.com/kan-bayashi/PytorchWaveNetVocoder\n\n    Args:\n        x (ndarray): Waveform sequence.\n        fs (int): Sampling frequency.\n        cutoff (float): Cutoff frequency of low cut filter.\n    Return:\n        ndarray: Low cut filtered waveform sequence.\n    """"""\n    nyquist = fs // 2\n    norm_cutoff = cutoff / nyquist\n    from scipy.signal import firwin, lfilter\n\n    # low cut filter\n    fil = firwin(255, norm_cutoff, pass_zero=False)\n    lcf_x = lfilter(fil, 1, x)\n\n    return lcf_x\n\n\ndef load_wav(path):\n    sr, x = wavfile.read(path)\n    signed_int16_max = 2**15\n    if x.dtype == np.int16:\n        x = x.astype(np.float32) / signed_int16_max\n    if sr != hparams.sample_rate:\n        x = librosa.resample(x, sr, hparams.sample_rate)\n    x = np.clip(x, -1.0, 1.0)\n    return x\n\n\ndef save_wav(wav, path):\n    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n    wavfile.write(path, hparams.sample_rate, wav.astype(np.int16))\n\n\ndef trim(quantized):\n    start, end = start_and_end_indices(quantized, hparams.silence_threshold)\n    return quantized[start:end]\n\n\ndef preemphasis(x, coef=0.85):\n    return P.preemphasis(x, coef)\n\n\ndef inv_preemphasis(x, coef=0.85):\n    return P.inv_preemphasis(x, coef)\n\n\ndef adjust_time_resolution(quantized, mel):\n    """"""Adjust time resolution by repeating features\n\n    Args:\n        quantized (ndarray): (T,)\n        mel (ndarray): (N, D)\n\n    Returns:\n        tuple: Tuple of (T,) and (T, D)\n    """"""\n    assert len(quantized.shape) == 1\n    assert len(mel.shape) == 2\n\n    upsample_factor = quantized.size // mel.shape[0]\n    mel = np.repeat(mel, upsample_factor, axis=0)\n    n_pad = quantized.size - mel.shape[0]\n    if n_pad != 0:\n        assert n_pad > 0\n        mel = np.pad(mel, [(0, n_pad), (0, 0)], mode=""constant"", constant_values=0)\n\n    # trim\n    start, end = start_and_end_indices(quantized, hparams.silence_threshold)\n\n    return quantized[start:end], mel[start:end, :]\n\n\ndef start_and_end_indices(quantized, silence_threshold=2):\n    for start in range(quantized.size):\n        if abs(quantized[start] - 127) > silence_threshold:\n            break\n    for end in range(quantized.size - 1, 1, -1):\n        if abs(quantized[end] - 127) > silence_threshold:\n            break\n\n    assert abs(quantized[start] - 127) > silence_threshold\n    assert abs(quantized[end] - 127) > silence_threshold\n\n    return start, end\n\n\ndef logmelspectrogram(y, pad_mode=""reflect""):\n    """"""Same log-melspectrogram computation as espnet\n    https://github.com/espnet/espnet\n    from espnet.transform.spectrogram import logmelspectrogram\n    """"""\n    D = _stft(y, pad_mode=pad_mode)\n    S = _linear_to_mel(np.abs(D))\n    S = np.log10(np.maximum(S, 1e-10))\n    return S\n\n\ndef get_hop_size():\n    hop_size = hparams.hop_size\n    if hop_size is None:\n        assert hparams.frame_shift_ms is not None\n        hop_size = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n    return hop_size\n\n\ndef get_win_length():\n    win_length = hparams.win_length\n    if win_length < 0:\n        assert hparams.win_length_ms > 0\n        win_length = int(hparams.win_length_ms / 1000 * hparams.sample_rate)\n    return win_length\n\n\ndef _stft(y, pad_mode=""constant""):\n    # use constant padding (defaults to zeros) instead of reflection padding\n    return librosa.stft(y=y, n_fft=hparams.fft_size, hop_length=get_hop_size(),\n                        win_length=get_win_length(), window=hparams.window,\n                        pad_mode=pad_mode)\n\n\ndef pad_lr(x, fsize, fshift):\n    return (0, fsize)\n\n# Conversions:\n\n\n_mel_basis = None\n\n\ndef _linear_to_mel(spectrogram):\n    global _mel_basis\n    if _mel_basis is None:\n        _mel_basis = _build_mel_basis()\n    return np.dot(_mel_basis, spectrogram)\n\n\ndef _build_mel_basis():\n    if hparams.fmax is not None:\n        assert hparams.fmax <= hparams.sample_rate // 2\n    return librosa.filters.mel(hparams.sample_rate, hparams.fft_size,\n                               fmin=hparams.fmin, fmax=hparams.fmax,\n                               n_mels=hparams.num_mels)\n\n\ndef _amp_to_db(x):\n    min_level = np.exp(hparams.min_level_db / 20 * np.log(10))\n    return 20 * np.log10(np.maximum(min_level, x))\n\n\ndef _db_to_amp(x):\n    return np.power(10.0, x * 0.05)\n\n\ndef _normalize(S):\n    return np.clip((S - hparams.min_level_db) / -hparams.min_level_db, 0, 1)\n\n\ndef _denormalize(S):\n    return (np.clip(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db\n'"
compute-meanvar-stats.py,0,"b'# coding: utf-8\n""""""Compute mean-variance normalization stats.\n\nusage: compute_meanvar_stats.py [options] <list_file> <out_path>\n\noptions:\n    -h, --help               Show help message.\n    --verbose=<n>            Verbosity [default: 0].\n""""""\nfrom docopt import docopt\nimport sys\nfrom tqdm import tqdm\nimport numpy as np\nimport json\n\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    list_file = args[""<list_file>""]\n    out_path = args[""<out_path>""]\n    verbose = int(args[""--verbose""])\n\n    scaler = StandardScaler()\n    with open(list_file) as f:\n        lines = f.readlines()\n    assert len(lines) > 0\n    for path in tqdm(lines):\n        c = np.load(path.strip())\n        scaler.partial_fit(c)\n    joblib.dump(scaler, out_path)\n\n    if verbose > 0:\n        print(""mean:\\n{}"".format(scaler.mean_))\n        print(""var:\\n{}"".format(scaler.var_))\n\n    sys.exit(0)\n'"
evaluate.py,8,"b'# coding: utf-8\n""""""\nSynthesis waveform for testset\n\nusage: evaluate.py [options] <dump-root> <checkpoint> <dst_dir>\n\noptions:\n    --hparams=<parmas>          Hyper parameters [default: ].\n    --preset=<json>             Path of preset parameters (json).\n    --length=<T>                Steps to generate [default: 32000].\n    --speaker-id=<N>            Use specific speaker of data in case for multi-speaker datasets.\n    --initial-value=<n>         Initial value for the WaveNet decoder.\n    --output-html               Output html for blog post.\n    --num-utterances=N>         Generate N utterenaces per speaker [default: -1].\n    --verbose=<level>           Verbosity level [default: 0].\n    -h, --help                  Show help message.\n""""""\nfrom docopt import docopt\n\nimport sys\nfrom glob import glob\nimport os\nfrom os.path import dirname, join, basename, splitext, exists\nimport torch\nimport numpy as np\nfrom nnmnkwii import preprocessing as P\nfrom tqdm import tqdm\nfrom scipy.io import wavfile\nfrom torch.utils import data as data_utils\nfrom torch.nn import functional as F\n\nfrom wavenet_vocoder.util import is_mulaw_quantize, is_mulaw, is_raw\n\nimport audio\nfrom hparams import hparams\nfrom train import RawAudioDataSource, MelSpecDataSource, PyTorchDataset, _pad_2d\nfrom nnmnkwii.datasets import FileSourceDataset\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n\ndef to_int16(x):\n    if x.dtype == np.int16:\n        return x\n    assert x.dtype == np.float32\n    assert x.min() >= -1 and x.max() <= 1.0\n    return (x * 32767).astype(np.int16)\n\n\ndef dummy_collate(batch):\n    N = len(batch)\n    input_lengths = [(len(x) - hparams.cin_pad * 2) * audio.get_hop_size() for x in batch]\n    input_lengths = torch.LongTensor(input_lengths)\n    max_len = max([len(x) for x in batch])\n    c_batch = np.array([_pad_2d(x, max_len) for x in batch], dtype=np.float32)\n    c_batch = torch.FloatTensor(c_batch).transpose(1, 2).contiguous()\n    return [None]*N, [None]*N, c_batch, None, input_lengths\n\n\ndef get_data_loader(data_dir, collate_fn):\n    wav_paths = glob(join(data_dir, ""*-wave.npy""))\n    if len(wav_paths) != 0:\n        X = FileSourceDataset(RawAudioDataSource(data_dir,\n                                                 hop_size=audio.get_hop_size(),\n                                                 max_steps=None, cin_pad=hparams.cin_pad))\n    else:\n        X = None\n    C = FileSourceDataset(MelSpecDataSource(data_dir,\n                                            hop_size=audio.get_hop_size(),\n                                            max_steps=None, cin_pad=hparams.cin_pad))\n    # No audio found:\n    if X is None:\n        assert len(C) > 0\n        data_loader = data_utils.DataLoader(\n            C, batch_size=hparams.batch_size, drop_last=False,\n            num_workers=hparams.num_workers, sampler=None, shuffle=False,\n            collate_fn=dummy_collate, pin_memory=hparams.pin_memory)\n    else:\n        assert len(X) == len(C)\n        if C[0].shape[-1] != hparams.cin_channels:\n            raise RuntimeError(\n                """"""Invalid cin_channnels {}. Expectd to be {}."""""".format(\n                    hparams.cin_channels, C[0].shape[-1]))\n        dataset = PyTorchDataset(X, C)\n\n        data_loader = data_utils.DataLoader(\n            dataset, batch_size=hparams.batch_size, drop_last=False,\n            num_workers=hparams.num_workers, sampler=None, shuffle=False,\n            collate_fn=collate_fn, pin_memory=hparams.pin_memory)\n\n    return data_loader\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    verbose = int(args[""--verbose""])\n    if verbose > 0:\n        print(""Command line args:\\n"", args)\n    data_root = args[""<dump-root>""]\n    checkpoint_path = args[""<checkpoint>""]\n    dst_dir = args[""<dst_dir>""]\n\n    length = int(args[""--length""])\n    # Note that speaker-id is used for filtering out unrelated-speaker from\n    # multi-speaker dataset.\n    speaker_id = args[""--speaker-id""]\n    speaker_id = int(speaker_id) if speaker_id is not None else None\n    initial_value = args[""--initial-value""]\n    initial_value = None if initial_value is None else float(initial_value)\n    output_html = args[""--output-html""]\n    num_utterances = int(args[""--num-utterances""])\n    preset = args[""--preset""]\n\n    # Load preset if specified\n    if preset is not None:\n        with open(preset) as f:\n            hparams.parse_json(f.read())\n    else:\n        hparams_json = join(dirname(checkpoint_path), ""hparams.json"")\n        if exists(hparams_json):\n            print(""Loading hparams from {}"".format(hparams_json))\n            with open(hparams_json) as f:\n                hparams.parse_json(f.read())\n\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n    assert hparams.name == ""wavenet_vocoder""\n\n    hparams.max_time_sec = None\n    hparams.max_time_steps = None\n\n    from train import build_model, get_data_loaders\n    from synthesis import batch_wavegen\n\n    # Data\n    # Use exactly same testset used in training script\n    # disable shuffle for convenience\n    # test_data_loader = get_data_loaders(data_root, speaker_id, test_shuffle=False)[""test""]\n    from train import collate_fn\n    test_data_loader = get_data_loader(data_root, collate_fn)\n    test_dataset = test_data_loader.dataset\n\n    # Model\n    model = build_model().to(device)\n\n    # Load checkpoint\n    print(""Load checkpoint from {}"".format(checkpoint_path))\n    if use_cuda:\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n    model.load_state_dict(checkpoint[""state_dict""])\n    checkpoint_name = splitext(basename(checkpoint_path))[0]\n\n    os.makedirs(dst_dir, exist_ok=True)\n    dst_dir_name = basename(os.path.normpath(dst_dir))\n\n    generated_utterances = {}\n    cin_pad = hparams.cin_pad\n    file_idx = 0\n    for idx, (x, y, c, g, input_lengths) in enumerate(test_data_loader):\n        if cin_pad > 0:\n            c = F.pad(c, pad=(cin_pad, cin_pad), mode=""replicate"")\n\n        # B x 1 x T\n        if x[0] is not None:\n            B, _, T = x.shape\n        else:\n            B, _, Tn = c.shape\n            T = Tn * audio.get_hop_size()\n\n        if g is None and num_utterances > 0 and B * idx >= num_utterances:\n            break\n\n        ref_files = []\n        ref_feats = []\n        for i in range(B):\n            # Yes this is ugly...\n            if hasattr(test_data_loader.dataset, ""X""):\n                ref_files.append(test_data_loader.dataset.X.collected_files[file_idx][0])\n            else:\n                pass\n            if hasattr(test_data_loader.dataset, ""Mel""):\n                ref_feats.append(test_data_loader.dataset.Mel.collected_files[file_idx][0])\n            else:\n                ref_feats.append(test_data_loader.dataset.collected_files[file_idx][0])\n            file_idx += 1\n\n        if num_utterances > 0 and g is not None:\n            try:\n                generated_utterances[g] += 1\n                if generated_utterances[g] > num_utterances:\n                    continue\n            except KeyError:\n                generated_utterances[g] = 1\n\n        if output_html:\n            def _tqdm(x): return x\n        else:\n            _tqdm = tqdm\n\n        # Generate\n        y_hats = batch_wavegen(model, c=c, g=g, fast=True, tqdm=_tqdm)\n\n        # Save each utt.\n        has_ref_file = len(ref_files) > 0\n        for i, (ref, gen, length) in enumerate(zip(x, y_hats, input_lengths)):\n            if has_ref_file:\n                if is_mulaw_quantize(hparams.input_type):\n                    # needs to be float since mulaw_inv returns in range of [-1, 1]\n                    ref = ref.max(0)[1].view(-1).float().cpu().numpy()[:length]\n                else:\n                    ref = ref.view(-1).cpu().numpy()[:length]\n            gen = gen[:length]\n            if has_ref_file:\n                target_audio_path = ref_files[i]\n                name = splitext(basename(target_audio_path))[0].replace(""-wave"", """")\n            else:\n                target_feat_path = ref_feats[i]\n                name = splitext(basename(target_feat_path))[0].replace(""-feats"", """")\n\n            # Paths\n            if g is None:\n                dst_wav_path = join(dst_dir, ""{}_gen.wav"".format(\n                    name))\n                target_wav_path = join(dst_dir, ""{}_ref.wav"".format(\n                    name))\n            else:\n                dst_wav_path = join(dst_dir, ""speaker{}_{}_gen.wav"".format(\n                    g, name))\n                target_wav_path = join(dst_dir, ""speaker{}_{}_ref.wav"".format(\n                    g, name))\n\n            # save\n            if has_ref_file:\n                if is_mulaw_quantize(hparams.input_type):\n                    ref = P.inv_mulaw_quantize(ref, hparams.quantize_channels - 1)\n                elif is_mulaw(hparams.input_type):\n                    ref = P.inv_mulaw(ref, hparams.quantize_channels - 1)\n                if hparams.postprocess is not None and hparams.postprocess not in ["""", ""none""]:\n                    ref = getattr(audio, hparams.postprocess)(ref)\n                if hparams.global_gain_scale > 0:\n                    ref /= hparams.global_gain_scale\n\n            # clip (just in case)\n            gen = np.clip(gen, -1.0, 1.0)\n            if has_ref_file:\n                ref = np.clip(ref, -1.0, 1.0)\n\n            wavfile.write(dst_wav_path, hparams.sample_rate, to_int16(gen))\n            if has_ref_file:\n                wavfile.write(target_wav_path, hparams.sample_rate, to_int16(ref))\n\n            # log (TODO)\n            if output_html and False:\n                print(""""""\n    <audio controls=""controls"" >\n    <source src=""/{}/audio/{}/{}"" autoplay/>\n    Your browser does not support the audio element.\n    </audio>\n    """""".format(hparams.name, dst_dir_name, basename(dst_wav_path)))\n\n    print(""Finished! Check out {} for generated audio samples."".format(dst_dir))\n    sys.exit(0)\n'"
hparams.py,0,"b'from wavenet_vocoder.tfcompat.hparam import HParams\nimport numpy as np\n\n# NOTE: If you want full control for model architecture. please take a look\n# at the code and change whatever you want. Some hyper parameters are hardcoded.\n\n# Default hyperparameters:\nhparams = HParams(\n    name=""wavenet_vocoder"",\n\n    # Input type:\n    # 1. raw [-1, 1]\n    # 2. mulaw [-1, 1]\n    # 3. mulaw-quantize [0, mu]\n    # If input_type is raw or mulaw, network assumes scalar input and\n    # discretized mixture of logistic distributions output, otherwise one-hot\n    # input and softmax output are assumed.\n    # **NOTE**: if you change the one of the two parameters below, you need to\n    # re-run preprocessing before training.\n    input_type=""raw"",\n    quantize_channels=65536,  # 65536 or 256\n\n    # Audio:\n    # time-domain pre/post-processing\n    # e.g., preemphasis/inv_preemphasis\n    # ref: LPCNet https://arxiv.org/abs/1810.11846\n    preprocess="""",\n    postprocess="""",\n    # waveform domain scaling\n    global_gain_scale=1.0,\n\n    sample_rate=22050,\n    # this is only valid for mulaw is True\n    silence_threshold=2,\n    num_mels=80,\n    fmin=125,\n    fmax=7600,\n    fft_size=1024,\n    # shift can be specified by either hop_size or frame_shift_ms\n    hop_size=256,\n    frame_shift_ms=None,\n    win_length=1024,\n    win_length_ms=-1.0,\n    window=""hann"",\n\n    # DC removal\n    highpass_cutoff=70.0,\n\n    # Parametric output distribution type for scalar input\n    # 1) Logistic or 2) Normal\n    output_distribution=""Logistic"",\n    log_scale_min=-16.0,\n\n    # Model:\n    # This should equal to `quantize_channels` if mu-law quantize enabled\n    # otherwise num_mixture * 3 (pi, mean, log_scale)\n    # single mixture case: 2\n    out_channels=10 * 3,\n    layers=24,\n    stacks=4,\n    residual_channels=128,\n    gate_channels=256,  # split into 2 gropus internally for gated activation\n    skip_out_channels=128,\n    dropout=0.0,\n    kernel_size=3,\n\n    # Local conditioning (set negative value to disable))\n    cin_channels=80,\n    cin_pad=2,\n    # If True, use transposed convolutions to upsample conditional features,\n    # otherwise repeat features to adjust time resolution\n    upsample_conditional_features=True,\n    upsample_net=""ConvInUpsampleNetwork"",\n    upsample_params={\n        ""upsample_scales"": [4, 4, 4, 4],  # should np.prod(upsample_scales) == hop_size\n    },\n\n    # Global conditioning (set negative value to disable)\n    # currently limited for speaker embedding\n    # this should only be enabled for multi-speaker dataset\n    gin_channels=-1,  # i.e., speaker embedding dim\n    n_speakers=7,  # 7 for CMU ARCTIC\n\n    # Data loader\n    pin_memory=True,\n    num_workers=2,\n\n    # Loss\n\n    # Training:\n    batch_size=8,\n    optimizer=""Adam"",\n    optimizer_params={\n        ""lr"": 1e-3,\n        ""eps"": 1e-8,\n        ""weight_decay"": 0.0,\n    },\n\n    # see lrschedule.py for available lr_schedule\n    lr_schedule=""step_learning_rate_decay"",\n    lr_schedule_kwargs={""anneal_rate"": 0.5, ""anneal_interval"": 200000},\n\n    max_train_steps=1000000,\n    nepochs=2000,\n\n    clip_thresh=-1,\n\n    # max time steps can either be specified as sec or steps\n    # if both are None, then full audio samples are used in a batch\n    max_time_sec=None,\n    max_time_steps=10240,  # 256 * 40\n\n    # Hold moving averaged parameters and use them for evaluation\n    exponential_moving_average=True,\n    # averaged = decay * averaged + (1 - decay) * x\n    ema_decay=0.9999,\n\n    # Save\n    # per-step intervals\n    checkpoint_interval=100000,\n    train_eval_interval=100000,\n    # per-epoch interval\n    test_eval_epoch_interval=50,\n    save_optimizer_state=True,\n\n    # Eval:\n)\n\n\ndef hparams_debug_string():\n    values = hparams.values()\n    hp = [\'  %s: %s\' % (name, values[name]) for name in sorted(values)]\n    return \'Hyperparameters:\\n\' + \'\\n\'.join(hp)\n'"
lrschedule.py,0,"b'import numpy as np\n\n\n# https://github.com/tensorflow/tensor2tensor/issues/280#issuecomment-339110329\ndef noam_learning_rate_decay(init_lr, global_step, warmup_steps=4000):\n    # Noam scheme from tensor2tensor:\n    warmup_steps = float(warmup_steps)\n    step = global_step + 1.\n    lr = init_lr * warmup_steps**0.5 * np.minimum(\n        step * warmup_steps**-1.5, step**-0.5)\n    return lr\n\n\ndef step_learning_rate_decay(init_lr, global_step,\n                             anneal_rate=0.98,\n                             anneal_interval=30000):\n    return init_lr * anneal_rate ** (global_step // anneal_interval)\n\n\ndef cyclic_cosine_annealing(init_lr, global_step, T, M):\n    """"""Cyclic cosine annealing\n\n    https://arxiv.org/pdf/1704.00109.pdf\n\n    Args:\n        init_lr (float): Initial learning rate\n        global_step (int): Current iteration number\n        T (int): Total iteration number (i,e. nepoch)\n        M (int): Number of ensembles we want\n\n    Returns:\n        float: Annealed learning rate\n    """"""\n    TdivM = T // M\n    return init_lr / 2.0 * (np.cos(np.pi * ((global_step - 1) % TdivM) / TdivM) + 1.0)\n'"
mksubset.py,0,"b'# coding: utf-8\n""""""\nMake subset of dataset\n\nusage: mksubset.py [options] <in_dir> <out_dir>\n\noptions:\n    -h, --help               Show help message.\n    --limit=<N>              Limit dataset size by N-hours [default: 10000].\n    --train-dev-test-split   Train/test split.\n    --dev-size=<N>           Development size or rate [default: 0.1].\n    --test-size=<N>          Test size or rate [default: 0.1].\n    --target-sr=<N>          Resampling.\n    --random-state=<N>       Random seed [default: 1234].\n""""""\nfrom docopt import docopt\nimport librosa\nfrom glob import glob\nfrom os.path import join, basename, exists, splitext\nfrom tqdm import tqdm\nimport sys\nimport os\nfrom shutil import copy2\nfrom scipy.io import wavfile\nimport numpy as np\n\n\ndef read_wav_or_raw(src_file, is_raw):\n    if is_raw:\n        sr = 24000  # hard coded for now\n        x = np.fromfile(src_file, dtype=np.int16)\n    else:\n        sr, x = wavfile.read(src_file)\n    return sr, x\n\n\ndef write_wav_or_raw(dst_path, sr, x, is_raw):\n    if is_raw:\n        x.tofile(dst_path)\n    else:\n        wavfile.write(dst_path, sr, x)\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    in_dir = args[""<in_dir>""]\n    out_dir = args[""<out_dir>""]\n    limit = float(args[""--limit""])\n    train_dev_test_split = args[""--train-dev-test-split""]\n    dev_size = float(args[""--dev-size""])\n    test_size = float(args[""--test-size""])\n    target_sr = args[""--target-sr""]\n    target_sr = int(target_sr) if target_sr is not None else None\n    random_state = int(args[""--random-state""])\n\n    src_files = sorted(glob(join(in_dir, ""*.wav"")))\n    raw_files = sorted(glob(join(in_dir, ""*.raw"")))\n    is_raw = len(src_files) == 0 and len(raw_files) > 0\n    if is_raw:\n        print(""Assuming 24kHz /16bit audio data"")\n        src_files = raw_files\n    if len(src_files) == 0:\n        raise RuntimeError(""No files found in {}"".format(in_dir))\n\n    total_samples = 0\n    indices = []\n    signed_int16_max = 2**15\n\n    os.makedirs(out_dir, exist_ok=True)\n    if train_dev_test_split:\n        os.makedirs(join(out_dir, ""train_no_dev""), exist_ok=True)\n        os.makedirs(join(out_dir, ""dev""), exist_ok=True)\n        os.makedirs(join(out_dir, ""eval""), exist_ok=True)\n\n    print(""Total number of utterances: {}"".format(len(src_files)))\n    for idx, src_file in tqdm(enumerate(src_files)):\n        sr, x = read_wav_or_raw(src_file, is_raw)\n        if x.dtype == np.int16:\n            x = x.astype(np.float32) / signed_int16_max\n        total_samples += len(x)\n        total_hours = float(total_samples) / sr / 3600.0\n        indices.append(idx)\n\n        if total_hours > limit:\n            print(""Total hours {:.3f} exceeded limit ({} hours)."".format(total_hours, limit))\n            break\n    print(""Total number of collected utterances: {}"".format(len(indices)))\n\n    if train_dev_test_split:\n        from sklearn.model_selection import train_test_split as split\n        # Get test and dev set from last\n        if test_size > 1 and dev_size > 1:\n            test_size = int(test_size)\n            dev_size = int(dev_size)\n            testdev_size = test_size + dev_size\n            train_indices = indices[:-testdev_size]\n            dev_indices = indices[-testdev_size:-testdev_size + dev_size]\n            test_indices = indices[-test_size:]\n        else:\n            train_indices, dev_test_indices = split(\n                indices, test_size=test_size + dev_size, random_state=random_state)\n            dev_indices, test_indices = split(\n                dev_test_indices, test_size=test_size / (test_size + dev_size),\n                random_state=random_state)\n        sets = [\n            (sorted(train_indices), join(out_dir, ""train_no_dev"")),\n            (sorted(dev_indices), join(out_dir, ""dev"")),\n            (sorted(test_indices), join(out_dir, ""eval"")),\n        ]\n    else:\n        sets = [(indices, out_dir)]\n\n    from sklearn.preprocessing import MinMaxScaler\n    scaler = MinMaxScaler()\n\n    total_samples = {}\n    sr = 0\n    for indices, d in sets:\n        set_name = basename(d)\n        total_samples[set_name] = 0\n        for idx in tqdm(indices):\n            src_file = src_files[idx]\n            dst_path = join(d, basename(src_file))\n            if target_sr is not None:\n                sr, x = read_wav_or_raw(src_file, is_raw)\n                is_int16 = x.dtype == np.int16\n                if is_int16:\n                    x = x.astype(np.float32) / signed_int16_max\n                if target_sr is not None and target_sr != sr:\n                    x = librosa.resample(x, sr, target_sr)\n                    sr = target_sr\n                scaler.partial_fit(x.astype(np.float64).reshape(-1, 1))\n                if is_int16:\n                    x = (x * signed_int16_max).astype(np.int16)\n                write_wav_or_raw(dst_path, sr, x, is_raw)\n                total_samples[set_name] += len(x)\n            else:\n                sr, x = read_wav_or_raw(src_file, is_raw)\n                is_int16 = x.dtype == np.int16\n                if is_int16:\n                    x = x.astype(np.float32) / signed_int16_max\n                scaler.partial_fit(x.astype(np.float64).reshape(-1, 1))\n                total_samples[set_name] += len(x)\n                copy2(src_file, dst_path)\n\n    print(""Waveform min: {}"".format(scaler.data_min_))\n    print(""Waveform max: {}"".format(scaler.data_max_))\n    absmax = max(np.abs(scaler.data_min_[0]), np.abs(scaler.data_max_[0]))\n    print(""Waveform absolute max: {}"".format(absmax))\n    if absmax > 1.0:\n        print(""There were clipping(s) in your dataset."")\n    print(""Global scaling factor would be around {}"".format(1.0 / absmax))\n\n    if train_dev_test_split:\n        print(""Train/dev/test split:"")\n        for n, s in zip([""train_no_dev"", ""dev"", ""eval""], sets):\n            hours = total_samples[n] / sr / 3600.0\n            print(""{}: {:.2f} hours ({} utt)"".format(n, hours, len(s[0])))\n\n    sys.exit(0)\n'"
preprocess.py,0,"b'# coding: utf-8\n""""""\nPreprocess dataset\n\nusage: preprocess.py [options] <name> <in_dir> <out_dir>\n\noptions:\n    --num_workers=<n>        Num workers.\n    --hparams=<parmas>       Hyper parameters [default: ].\n    --preset=<json>          Path of preset parameters (json).\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\nimport os\nfrom os.path import join\nfrom multiprocessing import cpu_count\nfrom tqdm import tqdm\nimport importlib\nfrom hparams import hparams\n\n\ndef preprocess(mod, in_dir, out_root, num_workers):\n    os.makedirs(out_dir, exist_ok=True)\n    metadata = mod.build_from_path(in_dir, out_dir, num_workers, tqdm=tqdm)\n    write_metadata(metadata, out_dir)\n\n\ndef write_metadata(metadata, out_dir):\n    with open(os.path.join(out_dir, \'train.txt\'), \'w\', encoding=\'utf-8\') as f:\n        for m in metadata:\n            f.write(\'|\'.join([str(x) for x in m]) + \'\\n\')\n    frames = sum([m[2] for m in metadata])\n    sr = hparams.sample_rate\n    hours = frames / sr / 3600\n    print(\'Wrote %d utterances, %d time steps (%.2f hours)\' % (len(metadata), frames, hours))\n    print(\'Min frame length: %d\' % min(m[2] for m in metadata))\n    print(\'Max frame length: %d\' % max(m[2] for m in metadata))\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    name = args[""<name>""]\n    in_dir = args[""<in_dir>""]\n    out_dir = args[""<out_dir>""]\n    num_workers = args[""--num_workers""]\n    num_workers = cpu_count() // 2 if num_workers is None else int(num_workers)\n    preset = args[""--preset""]\n\n    # Load preset if specified\n    if preset is not None:\n        with open(preset) as f:\n            hparams.parse_json(f.read())\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n    assert hparams.name == ""wavenet_vocoder""\n\n    print(""Sampling frequency: {}"".format(hparams.sample_rate))\n    if name in [""cmu_arctic"", ""jsut"", ""librivox""]:\n        print(""""""warn!: {} is no longer explicitly supported!\n\nPlease use a generic dataest \'wavallin\' instead.\nAll you need to do is to put all wav files in a single directory."""""".format(name))\n        sys.exit(1)\n\n    if name == ""ljspeech"":\n        print(""""""warn: ljspeech is deprecated!\nPlease use a generic dataset \'wavallin\' instead."""""")\n        sys.exit(1)\n\n    mod = importlib.import_module(""datasets."" + name)\n    preprocess(mod, in_dir, out_dir, num_workers)\n'"
preprocess_normalize.py,0,"b'# coding: utf-8\n""""""Perform meanvar normalization to preprocessed features.\n\nusage: preprocess_normalize.py [options] <in_dir> <out_dir> <scaler>\n\noptions:\n    --inverse                Inverse transform.\n    --num_workers=<n>        Num workers.\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\nimport os\nfrom os.path import join, exists, basename, splitext\nfrom multiprocessing import cpu_count\nfrom tqdm import tqdm\nfrom nnmnkwii import preprocessing as P\nimport numpy as np\nimport json\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nfrom shutil import copyfile\n\nimport joblib\nfrom glob import glob\nfrom itertools import zip_longest\n\n\ndef get_paths_by_glob(in_dir, filt):\n    return glob(join(in_dir, filt))\n\n\ndef _process_utterance(out_dir, audio_path, feat_path, scaler, inverse):\n    # [Optional] copy audio with the same name if exists\n    if audio_path is not None and exists(audio_path):\n        name = splitext(basename(audio_path))[0]\n        np.save(join(out_dir, name), np.load(audio_path), allow_pickle=False)\n\n    # [Required] apply normalization for features\n    assert exists(feat_path)\n    x = np.load(feat_path)\n    if inverse:\n        y = scaler.inverse_transform(x)\n    else:\n        y = scaler.transform(x)\n    assert x.dtype == y.dtype\n    name = splitext(basename(feat_path))[0]\n    np.save(join(out_dir, name), y, allow_pickle=False)\n\n\ndef apply_normalization_dir2dir(in_dir, out_dir, scaler, inverse, num_workers):\n    # NOTE: at this point, audio_paths can be empty\n    audio_paths = get_paths_by_glob(in_dir, ""*-wave.npy"")\n    feature_paths = get_paths_by_glob(in_dir, ""*-feats.npy"")\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n    for audio_path, feature_path in zip_longest(audio_paths, feature_paths):\n        futures.append(executor.submit(\n            partial(_process_utterance, out_dir, audio_path, feature_path, scaler, inverse)))\n    for future in tqdm(futures):\n        future.result()\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    in_dir = args[""<in_dir>""]\n    out_dir = args[""<out_dir>""]\n    scaler_path = args[""<scaler>""]\n    scaler = joblib.load(scaler_path)\n    inverse = args[""--inverse""]\n    num_workers = args[""--num_workers""]\n    num_workers = cpu_count() // 2 if num_workers is None else int(num_workers)\n\n    os.makedirs(out_dir, exist_ok=True)\n    apply_normalization_dir2dir(in_dir, out_dir, scaler, inverse, num_workers)\n\n    # Copy meta information if exists\n    traintxt = join(in_dir, ""train.txt"")\n    if exists(traintxt):\n        copyfile(join(in_dir, ""train.txt""), join(out_dir, ""train.txt""))\n'"
setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\n\nfrom importlib.machinery import SourceFileLoader\n\nversion = SourceFileLoader(\'wavenet_vocoder.version\',\n                           \'wavenet_vocoder/version.py\').load_module().version\n\n\nsetup(name=\'wavenet_vocoder\',\n      version=version,\n      description=\'PyTorch implementation of WaveNet vocoder\',\n      packages=find_packages(),\n      install_requires=[\n          ""numpy"",\n          ""scipy"",\n          ""torch >= 0.4.1"",\n          ""docopt"",\n          ""joblib"",\n          ""tqdm"",\n          ""tensorboardX"",\n          ""nnmnkwii >= 0.0.11"",\n          ""scikit-learn"",\n          ""librosa"",\n      ],\n      extras_require={\n          ""test"": [\n              ""nose"",\n              ""pysptk >= 0.1.9"",\n              ""matplotlib"",\n          ],\n      })\n'"
synthesis.py,11,"b'# coding: utf-8\n""""""\nSynthesis waveform from trained WaveNet.\n\nusage: synthesis.py [options] <checkpoint> <dst_dir>\n\noptions:\n    --hparams=<parmas>                Hyper parameters [default: ].\n    --preset=<json>                   Path of preset parameters (json).\n    --length=<T>                      Steps to generate [default: 32000].\n    --initial-value=<n>               Initial value for the WaveNet decoder.\n    --conditional=<p>                 Conditional features path.\n    --file-name-suffix=<s>            File name suffix [default: ].\n    --speaker-id=<id>                 Speaker ID (for multi-speaker model).\n    --output-html                     Output html for blog post.\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\n\nimport sys\nimport os\nfrom os.path import dirname, join, basename, splitext\nimport torch\nimport numpy as np\nfrom nnmnkwii import preprocessing as P\nfrom tqdm import tqdm\nimport librosa\n\nfrom wavenet_vocoder.util import is_mulaw_quantize, is_mulaw, is_raw\n\nimport audio\nfrom hparams import hparams\n\nfrom train import to_categorical\n\n\ntorch.set_num_threads(4)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n\ndef batch_wavegen(model, c=None, g=None, fast=True, tqdm=tqdm):\n    from train import sanity_check\n    sanity_check(model, c, g)\n    assert c is not None\n    B = c.shape[0]\n    model.eval()\n    if fast:\n        model.make_generation_fast_()\n\n    # Transform data to GPU\n    g = None if g is None else g.to(device)\n    c = None if c is None else c.to(device)\n\n    if hparams.upsample_conditional_features:\n        length = (c.shape[-1] - hparams.cin_pad * 2) * audio.get_hop_size()\n    else:\n        # already dupulicated\n        length = c.shape[-1]\n\n    with torch.no_grad():\n        y_hat = model.incremental_forward(\n            c=c, g=g, T=length, tqdm=tqdm, softmax=True, quantize=True,\n            log_scale_min=hparams.log_scale_min)\n\n    if is_mulaw_quantize(hparams.input_type):\n        # needs to be float since mulaw_inv returns in range of [-1, 1]\n        y_hat = y_hat.max(1)[1].view(B, -1).float().cpu().data.numpy()\n        for i in range(B):\n            y_hat[i] = P.inv_mulaw_quantize(y_hat[i], hparams.quantize_channels - 1)\n    elif is_mulaw(hparams.input_type):\n        y_hat = y_hat.view(B, -1).cpu().data.numpy()\n        for i in range(B):\n            y_hat[i] = P.inv_mulaw(y_hat[i], hparams.quantize_channels - 1)\n    else:\n        y_hat = y_hat.view(B, -1).cpu().data.numpy()\n\n    if hparams.postprocess is not None and hparams.postprocess not in ["""", ""none""]:\n        for i in range(B):\n            y_hat[i] = getattr(audio, hparams.postprocess)(y_hat[i])\n\n    if hparams.global_gain_scale > 0:\n        for i in range(B):\n            y_hat[i] /= hparams.global_gain_scale\n\n    return y_hat\n\n\ndef _to_numpy(x):\n    # this is ugly\n    if x is None:\n        return None\n    if isinstance(x, np.ndarray) or np.isscalar(x):\n        return x\n    # remove batch axis\n    if x.dim() == 3:\n        x = x.squeeze(0)\n    return x.numpy()\n\n\ndef wavegen(model, length=None, c=None, g=None, initial_value=None,\n            fast=False, tqdm=tqdm):\n    """"""Generate waveform samples by WaveNet.\n\n    Args:\n        model (nn.Module) : WaveNet decoder\n        length (int): Time steps to generate. If conditinlal features are given,\n          then this is determined by the feature size.\n        c (numpy.ndarray): Conditional features, of shape T x C\n        g (scaler): Speaker ID\n        initial_value (int) : initial_value for the WaveNet decoder.\n        fast (Bool): Whether to remove weight normalization or not.\n        tqdm (lambda): tqdm\n\n    Returns:\n        numpy.ndarray : Generated waveform samples\n    """"""\n    from train import sanity_check\n    sanity_check(model, c, g)\n\n    c = _to_numpy(c)\n    g = _to_numpy(g)\n\n    model.eval()\n    if fast:\n        model.make_generation_fast_()\n\n    if c is None:\n        assert length is not None\n    else:\n        # (Tc, D)\n        if c.ndim != 2:\n            raise RuntimeError(\n                ""Expected 2-dim shape (T, {}) for the conditional feature, but {} was actually given."".format(hparams.cin_channels, c.shape))\n            assert c.ndim == 2\n        Tc = c.shape[0]\n        upsample_factor = audio.get_hop_size()\n        # Overwrite length according to feature size\n        length = Tc * upsample_factor\n        # (Tc, D) -> (Tc\', D)\n        # Repeat features before feeding it to the network\n        if not hparams.upsample_conditional_features:\n            c = np.repeat(c, upsample_factor, axis=0)\n\n        # B x C x T\n        c = torch.FloatTensor(c.T).unsqueeze(0)\n\n    if initial_value is None:\n        if is_mulaw_quantize(hparams.input_type):\n            initial_value = P.mulaw_quantize(0, hparams.quantize_channels - 1)\n        else:\n            initial_value = 0.0\n\n    if is_mulaw_quantize(hparams.input_type):\n        assert initial_value >= 0 and initial_value < hparams.quantize_channels\n        initial_input = np_utils.to_categorical(\n            initial_value, num_classes=hparams.quantize_channels).astype(np.float32)\n        initial_input = torch.from_numpy(initial_input).view(\n            1, 1, hparams.quantize_channels)\n    else:\n        initial_input = torch.zeros(1, 1, 1).fill_(initial_value)\n\n    g = None if g is None else torch.LongTensor([g])\n\n    # Transform data to GPU\n    initial_input = initial_input.to(device)\n    g = None if g is None else g.to(device)\n    c = None if c is None else c.to(device)\n\n    with torch.no_grad():\n        y_hat = model.incremental_forward(\n            initial_input, c=c, g=g, T=length, tqdm=tqdm, softmax=True, quantize=True,\n            log_scale_min=hparams.log_scale_min)\n\n    if is_mulaw_quantize(hparams.input_type):\n        y_hat = y_hat.max(1)[1].view(-1).long().cpu().data.numpy()\n        y_hat = P.inv_mulaw_quantize(y_hat, hparams.quantize_channels)\n    elif is_mulaw(hparams.input_type):\n        y_hat = P.inv_mulaw(y_hat.view(-1).cpu().data.numpy(), hparams.quantize_channels)\n    else:\n        y_hat = y_hat.view(-1).cpu().data.numpy()\n\n    if hparams.postprocess is not None and hparams.postprocess not in ["""", ""none""]:\n        y_hat = getattr(audio, hparams.postprocess)(y_hat)\n\n    if hparams.global_gain_scale > 0:\n        y_hat /= hparams.global_gain_scale\n\n    return y_hat\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    print(""Command line args:\\n"", args)\n    checkpoint_path = args[""<checkpoint>""]\n    dst_dir = args[""<dst_dir>""]\n\n    length = int(args[""--length""])\n    initial_value = args[""--initial-value""]\n    initial_value = None if initial_value is None else float(initial_value)\n    conditional_path = args[""--conditional""]\n\n    file_name_suffix = args[""--file-name-suffix""]\n    output_html = args[""--output-html""]\n    speaker_id = args[""--speaker-id""]\n    speaker_id = None if speaker_id is None else int(speaker_id)\n    preset = args[""--preset""]\n\n    # Load preset if specified\n    if preset is not None:\n        with open(preset) as f:\n            hparams.parse_json(f.read())\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n    assert hparams.name == ""wavenet_vocoder""\n\n    # Load conditional features\n    if conditional_path is not None:\n        c = np.load(conditional_path)\n        if c.shape[1] != hparams.num_mels:\n            c = np.swapaxes(c, 0, 1)\n    else:\n        c = None\n\n    from train import build_model\n\n    # Model\n    model = build_model().to(device)\n\n    # Load checkpoint\n    print(""Load checkpoint from {}"".format(checkpoint_path))\n    if use_cuda:\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n    model.load_state_dict(checkpoint[""state_dict""])\n    checkpoint_name = splitext(basename(checkpoint_path))[0]\n\n    os.makedirs(dst_dir, exist_ok=True)\n    dst_wav_path = join(dst_dir, ""{}{}.wav"".format(checkpoint_name, file_name_suffix))\n\n    # DO generate\n    waveform = batch_wavegen(model, length, c=c, g=speaker_id, initial_value=initial_value, fast=True)\n\n    # save\n    librosa.output.write_wav(dst_wav_path, waveform, sr=hparams.sample_rate)\n\n    print(""Finished! Check out {} for generated audio samples."".format(dst_dir))\n    sys.exit(0)\n'"
tojson.py,0,"b'# coding: utf-8\n""""""\nDump hyper parameters to json file.\n\nusage: tojson.py [options] <output_json_path>\n\noptions:\n    --hparams=<parmas>       Hyper parameters [default: ].\n    -h, --help               Show help message.\n""""""\nfrom docopt import docopt\n\nimport sys\nimport os\nfrom os.path import dirname, join, basename, splitext\nimport json\n\nfrom hparams import hparams\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    output_json_path = args[""<output_json_path>""]\n\n    hparams.parse(args[""--hparams""])\n    j = hparams.values()\n    with open(output_json_path, ""w"") as f:\n        json.dump(j, f, indent=2)\n    sys.exit(0)\n'"
train.py,30,"b'""""""Trainining script for WaveNet vocoder\n\nusage: train.py [options]\n\noptions:\n    --dump-root=<dir>            Directory contains preprocessed features.\n    --checkpoint-dir=<dir>       Directory where to save model checkpoints [default: checkpoints].\n    --hparams=<parmas>           Hyper parameters [default: ].\n    --preset=<json>              Path of preset parameters (json).\n    --checkpoint=<path>          Restore model from checkpoint path if given.\n    --restore-parts=<path>       Restore part of the model.\n    --log-event-path=<name>      Log event path.\n    --reset-optimizer            Reset optimizer.\n    --speaker-id=<N>             Use specific speaker of data in case for multi-speaker datasets.\n    -h, --help                   Show this help message and exit\n""""""\nfrom docopt import docopt\n\nimport sys\n\nimport os\nfrom os.path import dirname, join, expanduser, exists\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport random\nimport json\nfrom glob import glob\n\nimport numpy as np\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nimport lrschedule\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch import optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils import data as data_utils\nfrom torch.utils.data.sampler import Sampler\n\nfrom nnmnkwii import preprocessing as P\nfrom nnmnkwii.datasets import FileSourceDataset, FileDataSource\n\nimport librosa.display\n\nfrom tensorboardX import SummaryWriter\nfrom matplotlib import cm\nfrom warnings import warn\n\nfrom wavenet_vocoder import WaveNet\nfrom wavenet_vocoder.util import is_mulaw_quantize, is_mulaw, is_raw, is_scalar_input\nfrom wavenet_vocoder.mixture import discretized_mix_logistic_loss\nfrom wavenet_vocoder.mixture import sample_from_discretized_mix_logistic\nfrom wavenet_vocoder.mixture import mix_gaussian_loss\nfrom wavenet_vocoder.mixture import sample_from_mix_gaussian\n\nimport audio\nfrom hparams import hparams, hparams_debug_string\n\nglobal_step = 0\nglobal_test_step = 0\nglobal_epoch = 0\nuse_cuda = torch.cuda.is_available()\nif use_cuda:\n    cudnn.benchmark = True\n\n\ndef sanity_check(model, c, g):\n    if model.has_speaker_embedding():\n        if g is None:\n            raise RuntimeError(\n                ""WaveNet expects speaker embedding, but speaker-id is not provided"")\n    else:\n        if g is not None:\n            raise RuntimeError(\n                ""WaveNet expects no speaker embedding, but speaker-id is provided"")\n\n    if model.local_conditioning_enabled():\n        if c is None:\n            raise RuntimeError(""WaveNet expects conditional features, but not given"")\n    else:\n        if c is not None:\n            raise RuntimeError(""WaveNet expects no conditional features, but given"")\n\n\ndef maybe_set_epochs_based_on_max_steps(hp, steps_per_epoch):\n    nepochs = hp.nepochs\n    max_train_steps = hp.max_train_steps\n    if max_train_steps is not None:\n        epochs = int(np.ceil(max_train_steps / steps_per_epoch))\n        hp.nepochs = epochs\n        print(""info; Number of epochs is set based on max_train_steps: {}"".format(epochs))\n\n\ndef _pad(seq, max_len, constant_values=0):\n    return np.pad(seq, (0, max_len - len(seq)),\n                  mode=\'constant\', constant_values=constant_values)\n\n\ndef _pad_2d(x, max_len, b_pad=0, constant_values=0):\n    x = np.pad(x, [(b_pad, max_len - len(x) - b_pad), (0, 0)],\n               mode=""constant"", constant_values=constant_values)\n    return x\n\n# from: https://github.com/keras-team/keras/blob/master/keras/utils/np_utils.py\n# to avoid keras dependency\n\n\ndef to_categorical(y, num_classes=None, dtype=\'float32\'):\n    """"""Converts a class vector (integers) to binary class matrix.\n    E.g. for use with categorical_crossentropy.\n    # Arguments\n        y: class vector to be converted into a matrix\n            (integers from 0 to num_classes).\n        num_classes: total number of classes.\n        dtype: The data type expected by the input, as a string\n            (`float32`, `float64`, `int32`...)\n    # Returns\n        A binary matrix representation of the input. The classes axis\n        is placed last.\n    # Example\n    ```python\n    # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n    > labels\n    array([0, 2, 1, 2, 0])\n    # `to_categorical` converts this into a matrix with as many\n    # columns as there are classes. The number of rows\n    # stays the same.\n    > to_categorical(labels)\n    array([[ 1.,  0.,  0.],\n           [ 0.,  0.,  1.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  1.],\n           [ 1.,  0.,  0.]], dtype=float32)\n    ```\n    """"""\n\n    y = np.array(y, dtype=\'int\')\n    input_shape = y.shape\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n        input_shape = tuple(input_shape[:-1])\n    y = y.ravel()\n    if not num_classes:\n        num_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, num_classes), dtype=dtype)\n    categorical[np.arange(n), y] = 1\n    output_shape = input_shape + (num_classes,)\n    categorical = np.reshape(categorical, output_shape)\n    return categorical\n\n\n# TODO: I know this is too ugly...\nclass _NPYDataSource(FileDataSource):\n    def __init__(self, dump_root, col, typ="""", speaker_id=None, max_steps=8000,\n                 cin_pad=0, hop_size=256):\n        self.dump_root = dump_root\n        self.col = col\n        self.lengths = []\n        self.speaker_id = speaker_id\n        self.multi_speaker = False\n        self.speaker_ids = None\n        self.max_steps = max_steps\n        self.cin_pad = cin_pad\n        self.hop_size = hop_size\n        self.typ = typ\n\n    def collect_files(self):\n        meta = join(self.dump_root, ""train.txt"")\n        if not exists(meta):\n            paths = sorted(glob(join(self.dump_root, ""*-{}.npy"".format(self.typ))))\n            return paths\n\n        with open(meta, ""rb"") as f:\n            lines = f.readlines()\n        l = lines[0].decode(""utf-8"").split(""|"")\n        assert len(l) == 4 or len(l) == 5\n        self.multi_speaker = len(l) == 5\n        self.lengths = list(\n            map(lambda l: int(l.decode(""utf-8"").split(""|"")[2]), lines))\n\n        paths_relative = list(map(lambda l: l.decode(""utf-8"").split(""|"")[self.col], lines))\n        paths = list(map(lambda f: join(self.dump_root, f), paths_relative))\n\n        # Exclude small files (assuming lenghts are in frame unit)\n        # TODO: consider this for multi-speaker\n        if self.max_steps is not None:\n            idx = np.array(self.lengths) * self.hop_size > self.max_steps + 2 * self.cin_pad * self.hop_size\n            if idx.sum() != len(self.lengths):\n                print(""{} short samples are omitted for training."".format(len(self.lengths) - idx.sum()))\n            self.lengths = list(np.array(self.lengths)[idx])\n            paths = list(np.array(paths)[idx])\n\n        if self.multi_speaker:\n            speaker_ids = list(map(lambda l: int(l.decode(""utf-8"").split(""|"")[-1]), lines))\n            self.speaker_ids = speaker_ids\n            if self.speaker_id is not None:\n                # Filter by speaker_id\n                # using multi-speaker dataset as a single speaker dataset\n                indices = np.array(speaker_ids) == self.speaker_id\n                paths = list(np.array(paths)[indices])\n                self.lengths = list(np.array(self.lengths)[indices])\n                # aha, need to cast numpy.int64 to int\n                self.lengths = list(map(int, self.lengths))\n                self.multi_speaker = False\n\n        if self.multi_speaker:\n            speaker_ids_np = list(np.array(self.speaker_ids)[indices])\n            self.speaker_ids = list(map(int, speaker_ids_np))\n            assert len(paths) == len(self.speaker_ids)\n\n        return paths\n\n    def collect_features(self, path):\n        return np.load(path)\n\n\nclass RawAudioDataSource(_NPYDataSource):\n    def __init__(self, dump_root, **kwargs):\n        super(RawAudioDataSource, self).__init__(dump_root, 0, ""wave"", **kwargs)\n\n\nclass MelSpecDataSource(_NPYDataSource):\n    def __init__(self, dump_root, **kwargs):\n        super(MelSpecDataSource, self).__init__(dump_root, 1, ""feats"", **kwargs)\n\n\nclass PartialyRandomizedSimilarTimeLengthSampler(Sampler):\n    """"""Partially randomized sampler\n\n    1. Sort by lengths\n    2. Pick a small patch and randomize it\n    3. Permutate mini-batches\n    """"""\n\n    def __init__(self, lengths, batch_size=8, batch_group_size=None):\n        self.lengths, self.sorted_indices = torch.sort(torch.LongTensor(lengths))\n\n        self.batch_size = batch_size\n        if batch_group_size is None:\n            batch_group_size = min(batch_size * 8, len(self.lengths))\n            if batch_group_size % batch_size != 0:\n                batch_group_size -= batch_group_size % batch_size\n\n        self.batch_group_size = batch_group_size\n        assert batch_group_size % batch_size == 0\n\n    def __iter__(self):\n        indices = self.sorted_indices.numpy()\n        batch_group_size = self.batch_group_size\n        s, e = 0, 0\n        bins = []\n        for i in range(len(indices) // batch_group_size):\n            s = i * batch_group_size\n            e = s + batch_group_size\n            group = indices[s:e]\n            random.shuffle(group)\n            bins += [group]\n\n        # Permutate batches\n        random.shuffle(bins)\n        binned_idx = np.stack(bins).reshape(-1)\n\n        # Handle last elements\n        s += batch_group_size\n        if s < len(indices):\n            last_bin = indices[len(binned_idx):]\n            random.shuffle(last_bin)\n            binned_idx = np.concatenate([binned_idx, last_bin])\n\n        return iter(torch.tensor(binned_idx).long())\n\n    def __len__(self):\n        return len(self.sorted_indices)\n\n\nclass PyTorchDataset(object):\n    def __init__(self, X, Mel):\n        self.X = X\n        self.Mel = Mel\n        # alias\n        self.multi_speaker = X.file_data_source.multi_speaker\n\n    def __getitem__(self, idx):\n        if self.Mel is None:\n            mel = None\n        else:\n            mel = self.Mel[idx]\n\n        raw_audio = self.X[idx]\n        if self.multi_speaker:\n            speaker_id = self.X.file_data_source.speaker_ids[idx]\n        else:\n            speaker_id = None\n\n        # (x,c,g)\n        return raw_audio, mel, speaker_id\n\n    def __len__(self):\n        return len(self.X)\n\n\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.data.max()\n    batch_size = sequence_length.size(0)\n    seq_range = torch.arange(0, max_len).long()\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    if sequence_length.is_cuda:\n        seq_range_expand = seq_range_expand.cuda()\n    seq_length_expand = sequence_length.unsqueeze(1) \\\n        .expand_as(seq_range_expand)\n    return (seq_range_expand < seq_length_expand).float()\n\n\n# https://discuss.pytorch.org/t/how-to-apply-exponential-moving-average-decay-for-variables/10856/4\n# https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\nclass ExponentialMovingAverage(object):\n    def __init__(self, decay):\n        self.decay = decay\n        self.shadow = {}\n\n    def register(self, name, val):\n        self.shadow[name] = val.clone()\n\n    def update(self, name, x):\n        assert name in self.shadow\n        update_delta = self.shadow[name] - x\n        self.shadow[name] -= (1.0 - self.decay) * update_delta\n\n\ndef clone_as_averaged_model(device, model, ema):\n    assert ema is not None\n    averaged_model = build_model().to(device)\n    averaged_model.load_state_dict(model.state_dict())\n    for name, param in averaged_model.named_parameters():\n        if name in ema.shadow:\n            param.data = ema.shadow[name].clone()\n    return averaged_model\n\n\nclass MaskedCrossEntropyLoss(nn.Module):\n    def __init__(self):\n        super(MaskedCrossEntropyLoss, self).__init__()\n        self.criterion = nn.CrossEntropyLoss(reduction=\'none\')\n\n    def forward(self, input, target, lengths=None, mask=None, max_len=None):\n        if lengths is None and mask is None:\n            raise RuntimeError(""Should provide either lengths or mask"")\n\n        # (B, T, 1)\n        if mask is None:\n            mask = sequence_mask(lengths, max_len).unsqueeze(-1)\n\n        # (B, T, D)\n        mask_ = mask.expand_as(target)\n        losses = self.criterion(input, target)\n        return ((losses * mask_).sum()) / mask_.sum()\n\n\nclass DiscretizedMixturelogisticLoss(nn.Module):\n    def __init__(self):\n        super(DiscretizedMixturelogisticLoss, self).__init__()\n\n    def forward(self, input, target, lengths=None, mask=None, max_len=None):\n        if lengths is None and mask is None:\n            raise RuntimeError(""Should provide either lengths or mask"")\n\n        # (B, T, 1)\n        if mask is None:\n            mask = sequence_mask(lengths, max_len).unsqueeze(-1)\n\n        # (B, T, 1)\n        mask_ = mask.expand_as(target)\n\n        losses = discretized_mix_logistic_loss(\n            input, target, num_classes=hparams.quantize_channels,\n            log_scale_min=hparams.log_scale_min, reduce=False)\n        assert losses.size() == target.size()\n        return ((losses * mask_).sum()) / mask_.sum()\n\n\nclass MixtureGaussianLoss(nn.Module):\n    def __init__(self):\n        super(MixtureGaussianLoss, self).__init__()\n\n    def forward(self, input, target, lengths=None, mask=None, max_len=None):\n        if lengths is None and mask is None:\n            raise RuntimeError(""Should provide either lengths or mask"")\n\n        # (B, T, 1)\n        if mask is None:\n            mask = sequence_mask(lengths, max_len).unsqueeze(-1)\n\n        # (B, T, 1)\n        mask_ = mask.expand_as(target)\n\n        losses = mix_gaussian_loss(\n            input, target, log_scale_min=hparams.log_scale_min, reduce=False)\n        assert losses.size() == target.size()\n        return ((losses * mask_).sum()) / mask_.sum()\n\n\ndef ensure_divisible(length, divisible_by=256, lower=True):\n    if length % divisible_by == 0:\n        return length\n    if lower:\n        return length - length % divisible_by\n    else:\n        return length + (divisible_by - length % divisible_by)\n\n\ndef assert_ready_for_upsampling(x, c, cin_pad):\n    assert len(x) == (len(c) - 2 * cin_pad) * audio.get_hop_size()\n\n\ndef collate_fn(batch):\n    """"""Create batch\n\n    Args:\n        batch(tuple): List of tuples\n            - x[0] (ndarray,int) : list of (T,)\n            - x[1] (ndarray,int) : list of (T, D)\n            - x[2] (ndarray,int) : list of (1,), speaker id\n    Returns:\n        tuple: Tuple of batch\n            - x (FloatTensor) : Network inputs (B, C, T)\n            - y (LongTensor)  : Network targets (B, T, 1)\n    """"""\n\n    local_conditioning = len(batch[0]) >= 2 and hparams.cin_channels > 0\n    global_conditioning = len(batch[0]) >= 3 and hparams.gin_channels > 0\n\n    if hparams.max_time_sec is not None:\n        max_time_steps = int(hparams.max_time_sec * hparams.sample_rate)\n    elif hparams.max_time_steps is not None:\n        max_time_steps = hparams.max_time_steps\n    else:\n        max_time_steps = None\n\n    # Time resolution adjustment\n    cin_pad = hparams.cin_pad\n    if local_conditioning:\n        new_batch = []\n        for idx in range(len(batch)):\n            x, c, g = batch[idx]\n            if hparams.upsample_conditional_features:\n                assert_ready_for_upsampling(x, c, cin_pad=0)\n                if max_time_steps is not None:\n                    max_steps = ensure_divisible(max_time_steps, audio.get_hop_size(), True)\n                    if len(x) > max_steps:\n                        max_time_frames = max_steps // audio.get_hop_size()\n                        s = np.random.randint(cin_pad, len(c) - max_time_frames - cin_pad)\n                        ts = s * audio.get_hop_size()\n                        x = x[ts:ts + audio.get_hop_size() * max_time_frames]\n                        c = c[s - cin_pad:s + max_time_frames + cin_pad, :]\n                        assert_ready_for_upsampling(x, c, cin_pad=cin_pad)\n            else:\n                x, c = audio.adjust_time_resolution(x, c)\n                if max_time_steps is not None and len(x) > max_time_steps:\n                    s = np.random.randint(cin_pad, len(x) - max_time_steps - cin_pad)\n                    x = x[s:s + max_time_steps]\n                    c = c[s - cin_pad:s + max_time_steps + cin_pad, :]\n                assert len(x) == len(c)\n            new_batch.append((x, c, g))\n        batch = new_batch\n    else:\n        new_batch = []\n        for idx in range(len(batch)):\n            x, c, g = batch[idx]\n            x = audio.trim(x)\n            if max_time_steps is not None and len(x) > max_time_steps:\n                s = np.random.randint(0, len(x) - max_time_steps)\n                if local_conditioning:\n                    x, c = x[s:s + max_time_steps], c[s:s + max_time_steps, :]\n                else:\n                    x = x[s:s + max_time_steps]\n            new_batch.append((x, c, g))\n        batch = new_batch\n\n    # Lengths\n    input_lengths = [len(x[0]) for x in batch]\n    max_input_len = max(input_lengths)\n\n    # (B, T, C)\n    # pad for time-axis\n    if is_mulaw_quantize(hparams.input_type):\n        padding_value = P.mulaw_quantize(0, mu=hparams.quantize_channels - 1)\n        x_batch = np.array([_pad_2d(to_categorical(\n            x[0], num_classes=hparams.quantize_channels),\n            max_input_len, 0, padding_value) for x in batch], dtype=np.float32)\n    else:\n        x_batch = np.array([_pad_2d(x[0].reshape(-1, 1), max_input_len)\n                            for x in batch], dtype=np.float32)\n    assert len(x_batch.shape) == 3\n\n    # (B, T)\n    if is_mulaw_quantize(hparams.input_type):\n        padding_value = P.mulaw_quantize(0, mu=hparams.quantize_channels - 1)\n        y_batch = np.array([_pad(x[0], max_input_len, constant_values=padding_value)\n                            for x in batch], dtype=np.int)\n    else:\n        y_batch = np.array([_pad(x[0], max_input_len) for x in batch], dtype=np.float32)\n    assert len(y_batch.shape) == 2\n\n    # (B, T, D)\n    if local_conditioning:\n        max_len = max([len(x[1]) for x in batch])\n        c_batch = np.array([_pad_2d(x[1], max_len) for x in batch], dtype=np.float32)\n        assert len(c_batch.shape) == 3\n        # (B x C x T)\n        c_batch = torch.FloatTensor(c_batch).transpose(1, 2).contiguous()\n    else:\n        c_batch = None\n\n    if global_conditioning:\n        g_batch = torch.LongTensor([x[2] for x in batch])\n    else:\n        g_batch = None\n\n    # Covnert to channel first i.e., (B, C, T)\n    x_batch = torch.FloatTensor(x_batch).transpose(1, 2).contiguous()\n    # Add extra axis\n    if is_mulaw_quantize(hparams.input_type):\n        y_batch = torch.LongTensor(y_batch).unsqueeze(-1).contiguous()\n    else:\n        y_batch = torch.FloatTensor(y_batch).unsqueeze(-1).contiguous()\n\n    input_lengths = torch.LongTensor(input_lengths)\n\n    return x_batch, y_batch, c_batch, g_batch, input_lengths\n\n\ndef time_string():\n    return datetime.now().strftime(\'%Y-%m-%d %H:%M\')\n\n\ndef save_waveplot(path, y_hat, y_target):\n    sr = hparams.sample_rate\n\n    plt.figure(figsize=(16, 6))\n    plt.subplot(2, 1, 1)\n    librosa.display.waveplot(y_target, sr=sr)\n    plt.subplot(2, 1, 2)\n    librosa.display.waveplot(y_hat, sr=sr)\n    plt.tight_layout()\n    plt.savefig(path, format=""png"")\n    plt.close()\n\n\ndef eval_model(global_step, writer, device, model, y, c, g, input_lengths, eval_dir, ema=None):\n    if ema is not None:\n        print(""Using averaged model for evaluation"")\n        model = clone_as_averaged_model(device, model, ema)\n        model.make_generation_fast_()\n\n    model.eval()\n    idx = np.random.randint(0, len(y))\n    length = input_lengths[idx].data.cpu().item()\n\n    # (T,)\n    y_target = y[idx].view(-1).data.cpu().numpy()[:length]\n\n    if c is not None:\n        if hparams.upsample_conditional_features:\n            c = c[idx, :, :length // audio.get_hop_size() + hparams.cin_pad * 2].unsqueeze(0)\n        else:\n            c = c[idx, :, :length].unsqueeze(0)\n        assert c.dim() == 3\n        print(""Shape of local conditioning features: {}"".format(c.size()))\n    if g is not None:\n        # TODO: test\n        g = g[idx]\n        print(""Shape of global conditioning features: {}"".format(g.size()))\n\n    # Dummy silence\n    if is_mulaw_quantize(hparams.input_type):\n        initial_value = P.mulaw_quantize(0, hparams.quantize_channels - 1)\n    elif is_mulaw(hparams.input_type):\n        initial_value = P.mulaw(0.0, hparams.quantize_channels)\n    else:\n        initial_value = 0.0\n\n    # (C,)\n    if is_mulaw_quantize(hparams.input_type):\n        initial_input = to_categorical(\n            initial_value, num_classes=hparams.quantize_channels).astype(np.float32)\n        initial_input = torch.from_numpy(initial_input).view(\n            1, 1, hparams.quantize_channels)\n    else:\n        initial_input = torch.zeros(1, 1, 1).fill_(initial_value)\n    initial_input = initial_input.to(device)\n\n    # Run the model in fast eval mode\n    with torch.no_grad():\n        y_hat = model.incremental_forward(\n            initial_input, c=c, g=g, T=length, softmax=True, quantize=True, tqdm=tqdm,\n            log_scale_min=hparams.log_scale_min)\n\n    if is_mulaw_quantize(hparams.input_type):\n        y_hat = y_hat.max(1)[1].view(-1).long().cpu().data.numpy()\n        y_hat = P.inv_mulaw_quantize(y_hat, hparams.quantize_channels - 1)\n        y_target = P.inv_mulaw_quantize(y_target, hparams.quantize_channels - 1)\n    elif is_mulaw(hparams.input_type):\n        y_hat = P.inv_mulaw(y_hat.view(-1).cpu().data.numpy(), hparams.quantize_channels)\n        y_target = P.inv_mulaw(y_target, hparams.quantize_channels)\n    else:\n        y_hat = y_hat.view(-1).cpu().data.numpy()\n\n    # Save audio\n    os.makedirs(eval_dir, exist_ok=True)\n    path = join(eval_dir, ""step{:09d}_predicted.wav"".format(global_step))\n    librosa.output.write_wav(path, y_hat, sr=hparams.sample_rate)\n    path = join(eval_dir, ""step{:09d}_target.wav"".format(global_step))\n    librosa.output.write_wav(path, y_target, sr=hparams.sample_rate)\n\n    # save figure\n    path = join(eval_dir, ""step{:09d}_waveplots.png"".format(global_step))\n    save_waveplot(path, y_hat, y_target)\n\n\ndef save_states(global_step, writer, y_hat, y, input_lengths, checkpoint_dir=None):\n    print(""Save intermediate states at step {}"".format(global_step))\n    idx = np.random.randint(0, len(y_hat))\n    length = input_lengths[idx].data.cpu().item()\n\n    # (B, C, T)\n    if y_hat.dim() == 4:\n        y_hat = y_hat.squeeze(-1)\n\n    if is_mulaw_quantize(hparams.input_type):\n        # (B, T)\n        y_hat = F.softmax(y_hat, dim=1).max(1)[1]\n\n        # (T,)\n        y_hat = y_hat[idx].data.cpu().long().numpy()\n        y = y[idx].view(-1).data.cpu().long().numpy()\n\n        y_hat = P.inv_mulaw_quantize(y_hat, hparams.quantize_channels - 1)\n        y = P.inv_mulaw_quantize(y, hparams.quantize_channels - 1)\n    else:\n        # (B, T)\n        if hparams.output_distribution == ""Logistic"":\n            y_hat = sample_from_discretized_mix_logistic(\n                y_hat, log_scale_min=hparams.log_scale_min)\n        elif hparams.output_distribution == ""Normal"":\n            y_hat = sample_from_mix_gaussian(\n                y_hat, log_scale_min=hparams.log_scale_min)\n        else:\n            assert False\n\n        # (T,)\n        y_hat = y_hat[idx].view(-1).data.cpu().numpy()\n        y = y[idx].view(-1).data.cpu().numpy()\n\n        if is_mulaw(hparams.input_type):\n            y_hat = P.inv_mulaw(y_hat, hparams.quantize_channels)\n            y = P.inv_mulaw(y, hparams.quantize_channels)\n\n    # Mask by length\n    y_hat[length:] = 0\n    y[length:] = 0\n\n    # Save audio\n    audio_dir = join(checkpoint_dir, ""intermediate"", ""audio"")\n    os.makedirs(audio_dir, exist_ok=True)\n    path = join(audio_dir, ""step{:09d}_predicted.wav"".format(global_step))\n    librosa.output.write_wav(path, y_hat, sr=hparams.sample_rate)\n    path = join(audio_dir, ""step{:09d}_target.wav"".format(global_step))\n    librosa.output.write_wav(path, y, sr=hparams.sample_rate)\n\n# workaround for https://github.com/pytorch/pytorch/issues/15716\n# the idea is to return outputs and replicas explicitly, so that making pytorch\n# not to release the nodes (this is a pytorch bug though)\n\n\ndef data_parallel_workaround(model, input):\n    device_ids = list(range(torch.cuda.device_count()))\n    output_device = device_ids[0]\n    replicas = torch.nn.parallel.replicate(model, device_ids)\n    inputs = torch.nn.parallel.scatter(input, device_ids)\n    replicas = replicas[:len(inputs)]\n    outputs = torch.nn.parallel.parallel_apply(replicas, inputs)\n    y_hat = torch.nn.parallel.gather(outputs, output_device)\n    return y_hat, outputs, replicas\n\n\ndef __train_step(device, phase, epoch, global_step, global_test_step,\n                 model, optimizer, writer, criterion,\n                 x, y, c, g, input_lengths,\n                 checkpoint_dir, eval_dir=None, do_eval=False, ema=None):\n    sanity_check(model, c, g)\n\n    # x : (B, C, T)\n    # y : (B, T, 1)\n    # c : (B, C, T)\n    # g : (B,)\n    train = (phase == ""train_no_dev"")\n    clip_thresh = hparams.clip_thresh\n    if train:\n        model.train()\n        step = global_step\n    else:\n        model.eval()\n        step = global_test_step\n\n    # Learning rate schedule\n    current_lr = hparams.optimizer_params[""lr""]\n    if train and hparams.lr_schedule is not None:\n        lr_schedule_f = getattr(lrschedule, hparams.lr_schedule)\n        current_lr = lr_schedule_f(\n            hparams.optimizer_params[""lr""], step, **hparams.lr_schedule_kwargs)\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = current_lr\n    optimizer.zero_grad()\n\n    # Prepare data\n    x, y = x.to(device), y.to(device)\n    input_lengths = input_lengths.to(device)\n    c = c.to(device) if c is not None else None\n    g = g.to(device) if g is not None else None\n\n    # (B, T, 1)\n    mask = sequence_mask(input_lengths, max_len=x.size(-1)).unsqueeze(-1)\n    mask = mask[:, 1:, :]\n\n    # Apply model: Run the model in regular eval mode\n    # NOTE: softmax is handled in F.cross_entrypy_loss\n    # y_hat: (B x C x T)\n\n    if use_cuda:\n        # multi gpu support\n        # you must make sure that batch size % num gpu == 0\n        y_hat, _outputs, _replicas = data_parallel_workaround(model, (x, c, g, False))\n    else:\n        y_hat = model(x, c, g, False)\n\n    if is_mulaw_quantize(hparams.input_type):\n        # wee need 4d inputs for spatial cross entropy loss\n        # (B, C, T, 1)\n        y_hat = y_hat.unsqueeze(-1)\n        loss = criterion(y_hat[:, :, :-1, :], y[:, 1:, :], mask=mask)\n    else:\n        loss = criterion(y_hat[:, :, :-1], y[:, 1:, :], mask=mask)\n\n    if train and step > 0 and step % hparams.checkpoint_interval == 0:\n        save_states(step, writer, y_hat, y, input_lengths, checkpoint_dir)\n        save_checkpoint(device, model, optimizer, step, checkpoint_dir, epoch, ema)\n\n    if do_eval:\n        # NOTE: use train step (i.e., global_step) for filename\n        eval_model(global_step, writer, device, model, y, c, g, input_lengths, eval_dir, ema)\n\n    # Update\n    if train:\n        loss.backward()\n        if clip_thresh > 0:\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_thresh)\n        optimizer.step()\n        # update moving average\n        if ema is not None:\n            for name, param in model.named_parameters():\n                if name in ema.shadow:\n                    ema.update(name, param.data)\n\n    # Logs\n    writer.add_scalar(""{} loss"".format(phase), float(loss.item()), step)\n    if train:\n        if clip_thresh > 0:\n            writer.add_scalar(""gradient norm"", grad_norm, step)\n        writer.add_scalar(""learning rate"", current_lr, step)\n\n    return loss.item()\n\n\ndef train_loop(device, model, data_loaders, optimizer, writer, checkpoint_dir=None):\n    if is_mulaw_quantize(hparams.input_type):\n        criterion = MaskedCrossEntropyLoss()\n    else:\n        if hparams.output_distribution == ""Logistic"":\n            criterion = DiscretizedMixturelogisticLoss()\n        elif hparams.output_distribution == ""Normal"":\n            criterion = MixtureGaussianLoss()\n        else:\n            raise RuntimeError(\n                ""Not supported output distribution type: {}"".format(\n                    hparams.output_distribution))\n\n    if hparams.exponential_moving_average:\n        ema = ExponentialMovingAverage(hparams.ema_decay)\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                ema.register(name, param.data)\n    else:\n        ema = None\n\n    global global_step, global_epoch, global_test_step\n    while global_epoch < hparams.nepochs:\n        for phase, data_loader in data_loaders.items():\n            train = (phase == ""train_no_dev"")\n            running_loss = 0.\n            test_evaluated = False\n            for step, (x, y, c, g, input_lengths) in tqdm(enumerate(data_loader)):\n                # Whether to save eval (i.e., online decoding) result\n                do_eval = False\n                eval_dir = join(checkpoint_dir, ""intermediate"", ""{}_eval"".format(phase))\n                # Do eval per eval_interval for train\n                if train and global_step > 0 \\\n                        and global_step % hparams.train_eval_interval == 0:\n                    do_eval = True\n                # Do eval for test\n                # NOTE: Decoding WaveNet is quite time consuming, so\n                # do only once in a single epoch for testset\n                if not train and not test_evaluated \\\n                        and global_epoch % hparams.test_eval_epoch_interval == 0:\n                    do_eval = True\n                    test_evaluated = True\n                if do_eval:\n                    print(""[{}] Eval at train step {}"".format(phase, global_step))\n\n                # Do step\n                running_loss += __train_step(device,\n                                             phase, global_epoch, global_step, global_test_step, model,\n                                             optimizer, writer, criterion, x, y, c, g, input_lengths,\n                                             checkpoint_dir, eval_dir, do_eval, ema)\n\n                # update global state\n                if train:\n                    global_step += 1\n                else:\n                    global_test_step += 1\n\n                if global_step >= hparams.max_train_steps:\n                    print(""Training reached max train steps ({}). will exit"".format(hparams.max_train_steps))\n                    return ema\n\n            # log per epoch\n            averaged_loss = running_loss / len(data_loader)\n            writer.add_scalar(""{} loss (per epoch)"".format(phase),\n                              averaged_loss, global_epoch)\n            print(""Step {} [{}] Loss: {}"".format(\n                global_step, phase, running_loss / len(data_loader)))\n\n        global_epoch += 1\n    return ema\n\n\ndef save_checkpoint(device, model, optimizer, step, checkpoint_dir, epoch, ema=None):\n    checkpoint_path = join(\n        checkpoint_dir, ""checkpoint_step{:09d}.pth"".format(global_step))\n    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n    global global_test_step\n    torch.save({\n        ""state_dict"": model.state_dict(),\n        ""optimizer"": optimizer_state,\n        ""global_step"": step,\n        ""global_epoch"": epoch,\n        ""global_test_step"": global_test_step,\n    }, checkpoint_path)\n    print(""Saved checkpoint:"", checkpoint_path)\n\n    import shutil\n    latest_pth = join(checkpoint_dir, ""checkpoint_latest.pth"")\n    shutil.copyfile(checkpoint_path, latest_pth)\n\n    if ema is not None:\n        averaged_model = clone_as_averaged_model(device, model, ema)\n        checkpoint_path = join(\n            checkpoint_dir, ""checkpoint_step{:09d}_ema.pth"".format(global_step))\n        torch.save({\n            ""state_dict"": averaged_model.state_dict(),\n            ""optimizer"": optimizer_state,\n            ""global_step"": step,\n            ""global_epoch"": epoch,\n            ""global_test_step"": global_test_step,\n        }, checkpoint_path)\n        print(""Saved averaged checkpoint:"", checkpoint_path)\n\n        latest_pth = join(checkpoint_dir, ""checkpoint_latest_ema.pth"")\n        shutil.copyfile(checkpoint_path, latest_pth)\n\n\ndef build_model():\n    if is_mulaw_quantize(hparams.input_type):\n        if hparams.out_channels != hparams.quantize_channels:\n            raise RuntimeError(\n                ""out_channels must equal to quantize_chennels if input_type is \'mulaw-quantize\'"")\n    if hparams.upsample_conditional_features and hparams.cin_channels < 0:\n        s = ""Upsample conv layers were specified while local conditioning disabled. ""\n        s += ""Notice that upsample conv layers will never be used.""\n        warn(s)\n\n    upsample_params = hparams.upsample_params\n    upsample_params[""cin_channels""] = hparams.cin_channels\n    upsample_params[""cin_pad""] = hparams.cin_pad\n    model = WaveNet(\n        out_channels=hparams.out_channels,\n        layers=hparams.layers,\n        stacks=hparams.stacks,\n        residual_channels=hparams.residual_channels,\n        gate_channels=hparams.gate_channels,\n        skip_out_channels=hparams.skip_out_channels,\n        cin_channels=hparams.cin_channels,\n        gin_channels=hparams.gin_channels,\n        n_speakers=hparams.n_speakers,\n        dropout=hparams.dropout,\n        kernel_size=hparams.kernel_size,\n        cin_pad=hparams.cin_pad,\n        upsample_conditional_features=hparams.upsample_conditional_features,\n        upsample_params=upsample_params,\n        scalar_input=is_scalar_input(hparams.input_type),\n        output_distribution=hparams.output_distribution,\n    )\n    return model\n\n\ndef _load(checkpoint_path):\n    if use_cuda:\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path,\n                                map_location=lambda storage, loc: storage)\n    return checkpoint\n\n\ndef load_checkpoint(path, model, optimizer, reset_optimizer):\n    global global_step\n    global global_epoch\n    global global_test_step\n\n    print(""Load checkpoint from: {}"".format(path))\n    checkpoint = _load(path)\n    model.load_state_dict(checkpoint[""state_dict""])\n    if not reset_optimizer:\n        optimizer_state = checkpoint[""optimizer""]\n        if optimizer_state is not None:\n            print(""Load optimizer state from {}"".format(path))\n            optimizer.load_state_dict(checkpoint[""optimizer""])\n    global_step = checkpoint[""global_step""]\n    global_epoch = checkpoint[""global_epoch""]\n    global_test_step = checkpoint.get(""global_test_step"", 0)\n\n    return model\n\n\n# https://discuss.pytorch.org/t/how-to-load-part-of-pre-trained-model/1113/3\ndef restore_parts(path, model):\n    print(""Restore part of the model from: {}"".format(path))\n    state = _load(path)[""state_dict""]\n    model_dict = model.state_dict()\n    valid_state_dict = {k: v for k, v in state.items() if k in model_dict}\n\n    try:\n        model_dict.update(valid_state_dict)\n        model.load_state_dict(model_dict)\n    except RuntimeError as e:\n        # there should be invalid size of weight(s), so load them per parameter\n        print(str(e))\n        model_dict = model.state_dict()\n        for k, v in valid_state_dict.items():\n            model_dict[k] = v\n            try:\n                model.load_state_dict(model_dict)\n            except RuntimeError as e:\n                print(str(e))\n                warn(""{}: may contain invalid size of weight. skipping..."".format(k))\n\n\ndef get_data_loaders(dump_root, speaker_id, test_shuffle=True):\n    data_loaders = {}\n    local_conditioning = hparams.cin_channels > 0\n\n    if hparams.max_time_steps is not None:\n        max_steps = ensure_divisible(hparams.max_time_steps, audio.get_hop_size(), True)\n    else:\n        max_steps = None\n\n    for phase in [""train_no_dev"", ""dev""]:\n        train = phase == ""train_no_dev""\n        X = FileSourceDataset(\n            RawAudioDataSource(join(dump_root, phase), speaker_id=speaker_id,\n                               max_steps=max_steps, cin_pad=hparams.cin_pad,\n                               hop_size=audio.get_hop_size()))\n        if local_conditioning:\n            Mel = FileSourceDataset(\n                MelSpecDataSource(join(dump_root, phase), speaker_id=speaker_id,\n                                  max_steps=max_steps, cin_pad=hparams.cin_pad,\n                                  hop_size=audio.get_hop_size()))\n            assert len(X) == len(Mel)\n            print(""Local conditioning enabled. Shape of a sample: {}."".format(\n                Mel[0].shape))\n        else:\n            Mel = None\n        print(""[{}]: length of the dataset is {}"".format(phase, len(X)))\n\n        if train:\n            lengths = np.array(X.file_data_source.lengths)\n            # Prepare sampler\n            sampler = PartialyRandomizedSimilarTimeLengthSampler(\n                lengths, batch_size=hparams.batch_size)\n            shuffle = False\n            # make sure that there\'s no sorting bugs for https://github.com/r9y9/wavenet_vocoder/issues/130\n            sampler_idx = np.asarray(sorted(list(map(lambda s: int(s), sampler))))\n            assert (sampler_idx == np.arange(len(sampler_idx), dtype=np.int)).all()\n        else:\n            sampler = None\n            shuffle = test_shuffle\n\n        dataset = PyTorchDataset(X, Mel)\n        data_loader = data_utils.DataLoader(\n            dataset, batch_size=hparams.batch_size, drop_last=True,\n            num_workers=hparams.num_workers, sampler=sampler, shuffle=shuffle,\n            collate_fn=collate_fn, pin_memory=hparams.pin_memory)\n\n        speaker_ids = {}\n        if X.file_data_source.multi_speaker:\n            for idx, (x, c, g) in enumerate(dataset):\n                if g is not None:\n                    try:\n                        speaker_ids[g] += 1\n                    except KeyError:\n                        speaker_ids[g] = 1\n            if len(speaker_ids) > 0:\n                print(""Speaker stats:"", speaker_ids)\n\n        data_loaders[phase] = data_loader\n\n    return data_loaders\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    print(""Command line args:\\n"", args)\n    checkpoint_dir = args[""--checkpoint-dir""]\n    checkpoint_path = args[""--checkpoint""]\n    checkpoint_restore_parts = args[""--restore-parts""]\n    speaker_id = args[""--speaker-id""]\n    speaker_id = int(speaker_id) if speaker_id is not None else None\n    preset = args[""--preset""]\n\n    dump_root = args[""--dump-root""]\n    if dump_root is None:\n        dump_root = join(dirname(__file__), ""data"", ""ljspeech"")\n\n    log_event_path = args[""--log-event-path""]\n    reset_optimizer = args[""--reset-optimizer""]\n\n    # Load preset if specified\n    if preset is not None:\n        with open(preset) as f:\n            hparams.parse_json(f.read())\n    # Override hyper parameters\n    hparams.parse(args[""--hparams""])\n    assert hparams.name == ""wavenet_vocoder""\n    print(hparams_debug_string())\n\n    fs = hparams.sample_rate\n\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    output_json_path = join(checkpoint_dir, ""hparams.json"")\n    with open(output_json_path, ""w"") as f:\n        json.dump(hparams.values(), f, indent=2)\n\n    # Dataloader setup\n    data_loaders = get_data_loaders(dump_root, speaker_id, test_shuffle=True)\n\n    maybe_set_epochs_based_on_max_steps(hparams, len(data_loaders[""train_no_dev""]))\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    # Model\n    model = build_model().to(device)\n\n    receptive_field = model.receptive_field\n    print(""Receptive field (samples / ms): {} / {}"".format(\n        receptive_field, receptive_field / fs * 1000))\n\n    from torch import optim\n    Optimizer = getattr(optim, hparams.optimizer)\n    optimizer = Optimizer(model.parameters(), **hparams.optimizer_params)\n\n    if checkpoint_restore_parts is not None:\n        restore_parts(checkpoint_restore_parts, model)\n\n    # Load checkpoints\n    if checkpoint_path is not None:\n        load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer)\n\n    # Setup summary writer for tensorboard\n    if log_event_path is None:\n        log_event_path = ""log/run-test"" + str(datetime.now()).replace("" "", ""_"")\n    print(""TensorBoard event log path: {}"".format(log_event_path))\n    writer = SummaryWriter(log_dir=log_event_path)\n\n    # Train!\n    ema = None\n    try:\n        ema = train_loop(device, model, data_loaders, optimizer, writer,\n                         checkpoint_dir=checkpoint_dir)\n    except KeyboardInterrupt:\n        print(""Interrupted!"")\n        pass\n    finally:\n        save_checkpoint(\n            device, model, optimizer, global_step, checkpoint_dir, global_epoch, ema)\n\n    print(""Finished"")\n\n    sys.exit(0)\n'"
datasets/wavallin.py,0,"b'from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport audio\n\nfrom nnmnkwii import preprocessing as P\nfrom hparams import hparams\nfrom os.path import exists, basename, splitext\nimport librosa\nfrom glob import glob\nfrom os.path import join\n\nfrom wavenet_vocoder.util import is_mulaw_quantize, is_mulaw, is_raw\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1, tqdm=lambda x: x):\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n    index = 1\n    src_files = sorted(glob(join(in_dir, ""*.wav"")))\n    for wav_path in src_files:\n        futures.append(executor.submit(\n            partial(_process_utterance, out_dir, index, wav_path, ""dummy"")))\n        index += 1\n    return [future.result() for future in tqdm(futures)]\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n    # Load the audio to a numpy array:\n    wav = audio.load_wav(wav_path)\n\n    # Trim begin/end silences\n    # NOTE: the threshold was chosen for clean signals\n    wav, _ = librosa.effects.trim(wav, top_db=60, frame_length=2048, hop_length=512)\n\n    if hparams.highpass_cutoff > 0.0:\n        wav = audio.low_cut_filter(wav, hparams.sample_rate, hparams.highpass_cutoff)\n\n    # Mu-law quantize\n    if is_mulaw_quantize(hparams.input_type):\n        # Trim silences in mul-aw quantized domain\n        silence_threshold = 0\n        if silence_threshold > 0:\n            # [0, quantize_channels)\n            out = P.mulaw_quantize(wav, hparams.quantize_channels - 1)\n            start, end = audio.start_and_end_indices(out, silence_threshold)\n            wav = wav[start:end]\n        constant_values = P.mulaw_quantize(0, hparams.quantize_channels - 1)\n        out_dtype = np.int16\n    elif is_mulaw(hparams.input_type):\n        # [-1, 1]\n        constant_values = P.mulaw(0.0, hparams.quantize_channels - 1)\n        out_dtype = np.float32\n    else:\n        # [-1, 1]\n        constant_values = 0.0\n        out_dtype = np.float32\n\n    # Compute a mel-scale spectrogram from the trimmed wav:\n    # (N, D)\n    mel_spectrogram = audio.logmelspectrogram(wav).astype(np.float32).T\n\n    if hparams.global_gain_scale > 0:\n        wav *= hparams.global_gain_scale\n\n    # Time domain preprocessing\n    if hparams.preprocess is not None and hparams.preprocess not in ["""", ""none""]:\n        f = getattr(audio, hparams.preprocess)\n        wav = f(wav)\n\n    # Clip\n    if np.abs(wav).max() > 1.0:\n        print(""""""Warning: abs max value exceeds 1.0: {}"""""".format(np.abs(wav).max()))\n        # ignore this sample\n        return (""dummy"", ""dummy"", -1, ""dummy"")\n\n    wav = np.clip(wav, -1.0, 1.0)\n\n    # Set waveform target (out)\n    if is_mulaw_quantize(hparams.input_type):\n        out = P.mulaw_quantize(wav, hparams.quantize_channels - 1)\n    elif is_mulaw(hparams.input_type):\n        out = P.mulaw(wav, hparams.quantize_channels - 1)\n    else:\n        out = wav\n\n    # zero pad\n    # this is needed to adjust time resolution between audio and mel-spectrogram\n    l, r = audio.pad_lr(out, hparams.fft_size, audio.get_hop_size())\n    if l > 0 or r > 0:\n        out = np.pad(out, (l, r), mode=""constant"", constant_values=constant_values)\n    N = mel_spectrogram.shape[0]\n    assert len(out) >= N * audio.get_hop_size()\n\n    # time resolution adjustment\n    # ensure length of raw audio is multiple of hop_size so that we can use\n    # transposed convolution to upsample\n    out = out[:N * audio.get_hop_size()]\n    assert len(out) % audio.get_hop_size() == 0\n\n    # Write the spectrograms to disk:\n    name = splitext(basename(wav_path))[0]\n    audio_filename = \'%s-wave.npy\' % (name)\n    mel_filename = \'%s-feats.npy\' % (name)\n    np.save(os.path.join(out_dir, audio_filename),\n            out.astype(out_dtype), allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename),\n            mel_spectrogram.astype(np.float32), allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return (audio_filename, mel_filename, N, text)\n'"
tests/test_audio.py,0,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport sys\nfrom os.path import dirname, join\nsys.path.insert(0, join(dirname(__file__), ""..""))\n\nimport numpy as np\nfrom nose.plugins.attrib import attr\n\nimport logging\nlogging.getLogger(\'tensorflow\').disabled = True\n\n\n@attr(""local_only"")\ndef test_amp_to_db():\n    import audio\n    x = np.random.rand(10)\n    x_hat = audio._db_to_amp(audio._amp_to_db(x))\n    assert np.allclose(x, x_hat)\n'"
tests/test_misc.py,0,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nfrom wavenet_vocoder import receptive_field_size\n\n\ndef test_receptive_field_size():\n    # Table 4 in https://arxiv.org/abs/1711.10433\n    assert receptive_field_size(total_layers=30, num_cycles=3, kernel_size=3) == 6139\n    assert receptive_field_size(total_layers=24, num_cycles=4, kernel_size=3) == 505\n    assert receptive_field_size(total_layers=12, num_cycles=2, kernel_size=3) == 253\n    assert receptive_field_size(total_layers=30, num_cycles=1,\n                                kernel_size=3, dilation=lambda x: 1) == 61\n'"
tests/test_mixture.py,14,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport librosa\nimport pysptk\nfrom nose.plugins.attrib import attr\n\n\nfrom wavenet_vocoder.mixture import discretized_mix_logistic_loss\nfrom wavenet_vocoder.mixture import sample_from_discretized_mix_logistic\nfrom wavenet_vocoder.mixture import mix_gaussian_loss\nfrom wavenet_vocoder.mixture import sample_from_mix_gaussian\n\n\ndef log_prob_from_logits(x):\n    """""" numerically stable log_softmax implementation that prevents overflow """"""\n    # TF ordering\n    axis = len(x.size()) - 1\n    m, _ = torch.max(x, dim=-1, keepdim=True)\n    return x - m - torch.log(torch.sum(torch.exp(x - m), dim=axis, keepdim=True))\n\n\n@attr(""mixture"")\ndef test_log_softmax():\n    x = torch.rand(2, 16000, 30)\n    y = log_prob_from_logits(x)\n    y_hat = F.log_softmax(x, -1)\n\n    y = y.data.cpu().numpy()\n    y_hat = y_hat.data.cpu().numpy()\n    assert np.allclose(y, y_hat)\n\n\n@attr(""mixture"")\ndef test_logistic_mixture():\n    np.random.seed(1234)\n\n    x, sr = librosa.load(pysptk.util.example_audio_file(), sr=None)\n    assert sr == 16000\n\n    T = len(x)\n    x = x.reshape(1, T, 1)\n    y = torch.from_numpy(x).float()\n    y_hat = torch.rand(1, 30, T).float()\n\n    print(y.shape, y_hat.shape)\n\n    loss = discretized_mix_logistic_loss(y_hat, y)\n    print(loss)\n\n    loss = discretized_mix_logistic_loss(y_hat, y, reduce=False)\n    print(loss.size(), y.size())\n    assert loss.size() == y.size()\n\n    y = sample_from_discretized_mix_logistic(y_hat)\n    print(y.shape)\n\n\n@attr(""mixture"")\ndef test_gaussian_mixture():\n    np.random.seed(1234)\n\n    x, sr = librosa.load(pysptk.util.example_audio_file(), sr=None)\n    assert sr == 16000\n\n    T = len(x)\n    x = x.reshape(1, T, 1)\n    y = torch.from_numpy(x).float()\n    y_hat = torch.rand(1, 30, T).float()\n\n    print(y.shape, y_hat.shape)\n\n    loss = mix_gaussian_loss(y_hat, y)\n    print(loss)\n\n    loss = mix_gaussian_loss(y_hat, y, reduce=False)\n    print(loss.size(), y.size())\n    assert loss.size() == y.size()\n\n    y = sample_from_mix_gaussian(y_hat)\n    print(y.shape)\n\n\n@attr(""mixture"")\ndef test_misc():\n    # https://en.wikipedia.org/wiki/Logistic_distribution\n    # what i have learned\n    # m = (x - mu) / s\n    m = torch.rand(10, 10)\n    log_pdf_mid1 = -2 * torch.log(torch.exp(m / 2) + torch.exp(-m / 2))\n    log_pdf_mid2 = m - 2 * F.softplus(m)\n    assert np.allclose(log_pdf_mid1.data.numpy(), log_pdf_mid2.data.numpy())\n\n    # Edge case for 0\n    plus_in = torch.rand(10, 10)\n    log_cdf_plus1 = torch.sigmoid(m).log()\n    log_cdf_plus2 = m - F.softplus(m)\n    assert np.allclose(log_cdf_plus1.data.numpy(), log_cdf_plus2.data.numpy())\n\n    # Edge case for 255\n    min_in = torch.rand(10, 10)\n    log_one_minus_cdf_min1 = (1 - torch.sigmoid(min_in)).log()\n    log_one_minus_cdf_min2 = -F.softplus(min_in)\n    assert np.allclose(log_one_minus_cdf_min1.data.numpy(), log_one_minus_cdf_min2.data.numpy())\n'"
tests/test_model.py,19,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom nnmnkwii import preprocessing as P\nfrom pysptk.util import example_audio_file\nimport librosa\nimport numpy as np\nfrom tqdm import tqdm\nfrom os.path import join, dirname, exists\nfrom functools import partial\nfrom nose.plugins.attrib import attr\n\nfrom wavenet_vocoder.modules import ResidualConv1dGLU\nfrom wavenet_vocoder import WaveNet\n\nuse_cuda = False\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n# For test\nbuild_compact_model = partial(WaveNet, layers=4, stacks=2, residual_channels=32,\n                              gate_channels=32, skip_out_channels=32,\n                              scalar_input=False)\n\n# https://github.com/keras-team/keras/blob/master/keras/utils/np_utils.py\n# copied to avoid keras dependency in tests\n\n\ndef to_categorical(y, num_classes=None):\n    """"""Converts a class vector (integers) to binary class matrix.\n    E.g. for use with categorical_crossentropy.\n    # Arguments\n        y: class vector to be converted into a matrix\n            (integers from 0 to num_classes).\n        num_classes: total number of classes.\n    # Returns\n        A binary matrix representation of the input.\n    """"""\n    y = np.array(y, dtype=\'int\')\n    input_shape = y.shape\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n        input_shape = tuple(input_shape[:-1])\n    y = y.ravel()\n    if not num_classes:\n        num_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, num_classes))\n    categorical[np.arange(n), y] = 1\n    output_shape = input_shape + (num_classes,)\n    categorical = np.reshape(categorical, output_shape)\n    return categorical\n\n\ndef test_conv_block():\n    conv = ResidualConv1dGLU(30, 30, kernel_size=3, dropout=1 - 0.95)\n    print(conv)\n    x = torch.zeros(16, 30, 16000)\n    y, h = conv(x)\n    print(y.size(), h.size())\n\n\ndef test_wavenet():\n    model = build_compact_model()\n    print(model)\n    x = torch.zeros(16, 256, 1000)\n    y = model(x)\n    print(y.size())\n\n\ndef _test_data(sr=4000, N=3000, returns_power=False, mulaw=True):\n    x, _ = librosa.load(example_audio_file(), sr=sr)\n    x, _ = librosa.effects.trim(x, top_db=15)\n\n    # To save computational cost\n    x = x[:N]\n\n    # For power conditioning wavenet\n    if returns_power:\n        # (1 x N\')\n        p = librosa.feature.rms(x, frame_length=256, hop_length=128)\n        upsample_factor = x.size // p.size\n        # (1 x N)\n        p = np.repeat(p, upsample_factor, axis=-1)\n        if p.size < x.size:\n            # pad against time axis\n            p = np.pad(p, [(0, 0), (0, x.size - p.size)], mode=""constant"", constant_values=0)\n\n        # shape adajst\n        p = p.reshape(1, 1, -1)\n\n    # (T,)\n    if mulaw:\n        x = P.mulaw_quantize(x)\n        x_org = P.inv_mulaw_quantize(x)\n        # (C, T)\n        x = to_categorical(x, num_classes=256).T\n        # (1, C, T)\n        x = x.reshape(1, 256, -1).astype(np.float32)\n    else:\n        x_org = x\n        x = x.reshape(1, 1, -1)\n\n    if returns_power:\n        return x, x_org, p\n\n    return x, x_org\n\n\n@attr(""mixture"")\ndef test_mixture_wavenet():\n    x, x_org, c = _test_data(returns_power=True, mulaw=False)\n    # 10 mixtures\n    model = build_compact_model(out_channels=3 * 10, cin_channels=1,\n                                scalar_input=True)\n    T = x.shape[-1]\n    print(model.first_conv)\n\n    # scalar input, not one-hot\n    assert x.shape[1] == 1\n\n    x = torch.from_numpy(x).contiguous().to(device)\n    c = torch.from_numpy(c).contiguous().to(device)\n\n    # make batch\n    x = x.expand((x.shape[0] * 2, x.shape[1], x.shape[2]))\n    c = c.expand((c.shape[0] * 2, c.shape[1], c.shape[2]))\n\n    print(c.size())\n\n    model.eval()\n\n    # Incremental forward with forced teaching\n    y_online = model.incremental_forward(\n        test_inputs=x, c=c, T=None, tqdm=tqdm)\n\n    assert y_online.size() == x.size()\n\n    y_online2 = model.incremental_forward(\n        test_inputs=None, c=c, T=T, tqdm=tqdm)\n\n    assert y_online2.size() == x.size()\n    print(x.size())\n\n\n@attr(""local_conditioning"")\ndef test_local_conditioning_correctness():\n    # condition by power\n    x, x_org, c = _test_data(returns_power=True)\n    model = build_compact_model(cin_channels=1)\n    assert model.local_conditioning_enabled()\n    assert not model.has_speaker_embedding()\n\n    x = torch.from_numpy(x).contiguous().to(device)\n\n    c = torch.from_numpy(c).contiguous().to(device)\n    print(x.size(), c.size())\n\n    model.eval()\n\n    y_offline = model(x, c=c, softmax=True)\n\n    # Incremental forward with forced teaching\n    y_online = model.incremental_forward(\n        test_inputs=x, c=c, T=None, tqdm=tqdm, softmax=True, quantize=False)\n\n    # (1 x C x T)\n    c = (y_offline - y_online).abs()\n    print(c.mean(), c.max())\n\n    try:\n        assert np.allclose(y_offline.cpu().data.numpy(),\n                           y_online.cpu().data.numpy(), atol=1e-4)\n    except Exception:\n        from warnings import warn\n        warn(""oops! must be a bug!"")\n\n\n@attr(""local_conditioning"")\ndef test_local_conditioning_upsample_correctness():\n    # condition by power\n    x, x_org, c = _test_data(returns_power=True)\n\n    # downsample by 4\n    assert c.shape[-1] % 4 == 0\n    c = c[:, :, 0::4]\n\n    model = build_compact_model(\n        cin_channels=1, upsample_conditional_features=True,\n        upsample_params={""upsample_scales"": [2, 2], ""cin_channels"": 1})\n    assert model.local_conditioning_enabled()\n    assert not model.has_speaker_embedding()\n\n    x = torch.from_numpy(x).contiguous().to(device)\n\n    c = torch.from_numpy(c).contiguous().to(device)\n    print(x.size(), c.size())\n\n    model.eval()\n\n    y_offline = model(x, c=c, softmax=True)\n\n    # Incremental forward with forced teaching\n    y_online = model.incremental_forward(\n        test_inputs=x, c=c, T=None, tqdm=tqdm, softmax=True, quantize=False)\n\n    # (1 x C x T)\n    c = (y_offline - y_online).abs()\n    print(c.mean(), c.max())\n\n    try:\n        assert np.allclose(y_offline.cpu().data.numpy(),\n                           y_online.cpu().data.numpy(), atol=1e-4)\n    except Exception:\n        from warnings import warn\n        warn(""oops! must be a bug!"")\n\n\n@attr(""global_conditioning"")\ndef test_global_conditioning_with_embedding_correctness():\n    # condition by mean power\n    x, x_org, c = _test_data(returns_power=True)\n    g = c.mean(axis=-1, keepdims=True).astype(np.int)\n    model = build_compact_model(gin_channels=16, n_speakers=256,\n                                use_speaker_embedding=True)\n    assert not model.local_conditioning_enabled()\n    assert model.has_speaker_embedding()\n\n    x = torch.from_numpy(x).contiguous().to(device)\n\n    g = torch.from_numpy(g).long().contiguous().to(device)\n    print(g.size())\n\n    model.eval()\n\n    y_offline = model(x, g=g, softmax=True)\n\n    # Incremental forward with forced teaching\n    y_online = model.incremental_forward(\n        test_inputs=x, g=g, T=None, tqdm=tqdm, softmax=True, quantize=False)\n\n    # (1 x C x T)\n    c = (y_offline - y_online).abs()\n    print(c.mean(), c.max())\n\n    try:\n        assert np.allclose(y_offline.cpu().data.numpy(),\n                           y_online.cpu().data.numpy(), atol=1e-4)\n    except Exception:\n        from warnings import warn\n        warn(""oops! must be a bug!"")\n\n\n@attr(""global_conditioning"")\ndef test_global_conditioning_correctness():\n    # condition by mean power\n    x, x_org, c = _test_data(returns_power=True)\n    # must be floating-point type\n    g = c.mean(axis=-1, keepdims=True).astype(np.float32)\n    model = build_compact_model(gin_channels=1, use_speaker_embedding=False)\n    assert not model.local_conditioning_enabled()\n    # `use_speaker_embedding` False should diable embedding layer\n    assert not model.has_speaker_embedding()\n\n    x = torch.from_numpy(x).contiguous().to(device)\n\n    g = torch.from_numpy(g).contiguous().to(device)\n    print(g.size())\n\n    model.eval()\n    y_offline = model(x, g=g, softmax=True)\n\n    # Incremental forward with forced teaching\n    y_online = model.incremental_forward(\n        test_inputs=x, g=g, T=None, tqdm=tqdm, softmax=True, quantize=False)\n\n    # (1 x C x T)\n    c = (y_offline - y_online).abs()\n    print(c.mean(), c.max())\n\n    try:\n        assert np.allclose(y_offline.cpu().data.numpy(),\n                           y_online.cpu().data.numpy(), atol=1e-4)\n    except Exception:\n        from warnings import warn\n        warn(""oops! must be a bug!"")\n\n\n@attr(""local_and_global_conditioning"")\ndef test_global_and_local_conditioning_correctness():\n    x, x_org, c = _test_data(returns_power=True)\n    g = c.mean(axis=-1, keepdims=True).astype(np.int)\n    model = build_compact_model(\n        cin_channels=1, gin_channels=16, use_speaker_embedding=True, n_speakers=256)\n    assert model.local_conditioning_enabled()\n    assert model.has_speaker_embedding()\n\n    x = torch.from_numpy(x).contiguous().to(device)\n\n    # per-sample power\n    c = torch.from_numpy(c).contiguous().to(device)\n\n    # mean power\n    g = torch.from_numpy(g).long().contiguous().to(device)\n\n    print(c.size(), g.size())\n\n    model.eval()\n\n    y_offline = model(x, c=c, g=g, softmax=True)\n\n    # Incremental forward with forced teaching\n    y_online = model.incremental_forward(\n        test_inputs=x, c=c, g=g, T=None, tqdm=tqdm, softmax=True, quantize=False)\n    # (1 x C x T)\n\n    c = (y_offline - y_online).abs()\n    print(c.mean(), c.max())\n\n    try:\n        assert np.allclose(y_offline.cpu().data.numpy(),\n                           y_online.cpu().data.numpy(), atol=1e-4)\n    except Exception:\n        from warnings import warn\n        warn(""oops! must be a bug!"")\n\n\n@attr(""local_only"")\ndef test_incremental_forward_correctness():\n    import librosa.display\n    from matplotlib import pyplot as plt\n\n    model = build_compact_model().to(device)\n\n    checkpoint_path = join(dirname(__file__), "".."", ""foobar/checkpoint_step000058000.pth"")\n    if exists(checkpoint_path):\n        print(""Loading from:"", checkpoint_path)\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint[""state_dict""])\n\n    sr = 4000\n    x, x_org = _test_data(sr=sr, N=3000)\n    x = torch.from_numpy(x).contiguous().to(device)\n\n    model.eval()\n\n    # Batch forward\n    y_offline = model(x, softmax=True)\n\n    # Test from zero start\n    y_online = model.incremental_forward(initial_input=None, T=100, tqdm=tqdm, softmax=True)\n\n    # Incremental forward with forced teaching\n    y_online = model.incremental_forward(test_inputs=x, tqdm=tqdm, softmax=True, quantize=False)\n\n    # (1 x C x T)\n    c = (y_offline - y_online).abs()\n    print(c.mean(), c.max())\n\n    try:\n        assert np.allclose(y_offline.cpu().data.numpy(),\n                           y_online.cpu().data.numpy(), atol=1e-4)\n    except Exception:\n        from warnings import warn\n        warn(""oops! must be a bug!"")\n\n    # (1, T, C)\n    xt = x.transpose(1, 2).contiguous()\n\n    initial_input = xt[:, 0, :].unsqueeze(1).contiguous()\n    print(initial_input.size())\n    print(""Inital value:"", initial_input.view(-1).max(0)[1])\n\n    # With zero start\n    zerostart = True\n    if zerostart:\n        y_inference = model.incremental_forward(\n            initial_input=initial_input, T=xt.size(1), tqdm=tqdm, softmax=True, quantize=True)\n    else:\n        # Feed a few samples as test_inputs and then generate auto-regressively\n        N = 1000\n        y_inference = model.incremental_forward(\n            initial_input=None, test_inputs=xt[:, :N, :],\n            T=xt.size(1), tqdm=tqdm, softmax=True, quantize=True)\n\n    # Waveforms\n    # (T,)\n    y_offline = y_offline.max(1)[1].view(-1)\n    y_online = y_online.max(1)[1].view(-1)\n    y_inference = y_inference.max(1)[1].view(-1)\n\n    y_offline = P.inv_mulaw_quantize(y_offline.cpu().data.long().numpy())\n    y_online = P.inv_mulaw_quantize(y_online.cpu().data.long().numpy())\n    y_inference = P.inv_mulaw_quantize(y_inference.cpu().data.long().numpy())\n\n    plt.figure(figsize=(16, 10))\n    plt.subplot(4, 1, 1)\n    librosa.display.waveplot(x_org, sr=sr)\n    plt.subplot(4, 1, 2)\n    librosa.display.waveplot(y_offline, sr=sr)\n    plt.subplot(4, 1, 3)\n    librosa.display.waveplot(y_online, sr=sr)\n    plt.subplot(4, 1, 4)\n    librosa.display.waveplot(y_inference, sr=sr)\n    plt.show()\n\n    save_audio = False\n    if save_audio:\n        librosa.output.write_wav(""target.wav"", x_org, sr=sr)\n        librosa.output.write_wav(""online.wav"", y_online, sr=sr)\n        librosa.output.write_wav(""inference.wav"", y_inference, sr=sr)\n'"
wavenet_vocoder/__init__.py,0,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nfrom .version import version as __version__\n\nfrom .wavenet import receptive_field_size, WaveNet\n'"
wavenet_vocoder/conv.py,1,"b'# coding: utf-8\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Conv1d(nn.Conv1d):\n    """"""Extended nn.Conv1d for incremental dilated convolutions\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clear_buffer()\n        self._linearized_weight = None\n        self.register_backward_hook(self._clear_linearized_weight)\n\n    def incremental_forward(self, input):\n        # input: (B, T, C)\n        if self.training:\n            raise RuntimeError(\'incremental_forward only supports eval mode\')\n\n        # run forward pre hooks (e.g., weight norm)\n        for hook in self._forward_pre_hooks.values():\n            hook(self, input)\n\n        # reshape weight\n        weight = self._get_linearized_weight()\n        kw = self.kernel_size[0]\n        dilation = self.dilation[0]\n\n        bsz = input.size(0)  # input: bsz x len x dim\n        if kw > 1:\n            input = input.data\n            if self.input_buffer is None:\n                self.input_buffer = input.new(bsz, kw + (kw - 1) * (dilation - 1), input.size(2))\n                self.input_buffer.zero_()\n            else:\n                # shift buffer\n                self.input_buffer[:, :-1, :] = self.input_buffer[:, 1:, :].clone()\n            # append next input\n            self.input_buffer[:, -1, :] = input[:, -1, :]\n            input = self.input_buffer\n            if dilation > 1:\n                input = input[:, 0::dilation, :].contiguous()\n        output = F.linear(input.view(bsz, -1), weight, self.bias)\n        return output.view(bsz, 1, -1)\n\n    def clear_buffer(self):\n        self.input_buffer = None\n\n    def _get_linearized_weight(self):\n        if self._linearized_weight is None:\n            kw = self.kernel_size[0]\n            # nn.Conv1d\n            if self.weight.size() == (self.out_channels, self.in_channels, kw):\n                weight = self.weight.transpose(1, 2).contiguous()\n            else:\n                # fairseq.modules.conv_tbc.ConvTBC\n                weight = self.weight.transpose(2, 1).transpose(1, 0).contiguous()\n            assert weight.size() == (self.out_channels, kw, self.in_channels)\n            self._linearized_weight = weight.view(self.out_channels, -1)\n        return self._linearized_weight\n\n    def _clear_linearized_weight(self, *args):\n        self._linearized_weight = None\n'"
wavenet_vocoder/mixture.py,30,"b'# coding: utf-8\n# Code is adapted from:\n# https://github.com/pclucas14/pixel-cnn-pp\n# https://github.com/openai/pixel-cnn\n\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport math\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.distributions import Normal\n\n\ndef log_sum_exp(x):\n    """""" numerically stable log_sum_exp implementation that prevents overflow """"""\n    # TF ordering\n    axis = len(x.size()) - 1\n    m, _ = torch.max(x, dim=axis)\n    m2, _ = torch.max(x, dim=axis, keepdim=True)\n    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n\n\ndef discretized_mix_logistic_loss(y_hat, y, num_classes=256,\n                                  log_scale_min=-7.0, reduce=True):\n    """"""Discretized mixture of logistic distributions loss\n\n    Note that it is assumed that input is scaled to [-1, 1].\n\n    Args:\n        y_hat (Tensor): Predicted output (B x C x T)\n        y (Tensor): Target (B x T x 1).\n        num_classes (int): Number of classes\n        log_scale_min (float): Log scale minimum value\n        reduce (bool): If True, the losses are averaged or summed for each\n          minibatch.\n\n    Returns\n        Tensor: loss\n    """"""\n    assert y_hat.dim() == 3\n    assert y_hat.size(1) % 3 == 0\n    nr_mix = y_hat.size(1) // 3\n\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n\n    # unpack parameters. (B, T, num_mixtures) x 3\n    logit_probs = y_hat[:, :, :nr_mix]\n    means = y_hat[:, :, nr_mix:2 * nr_mix]\n    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n\n    # B x T x 1 -> B x T x num_mixtures\n    y = y.expand_as(means)\n\n    centered_y = y - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n    cdf_plus = torch.sigmoid(plus_in)\n    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n    cdf_min = torch.sigmoid(min_in)\n\n    # log probability for edge case of 0 (before scaling)\n    # equivalent: torch.log(torch.sigmoid(plus_in))\n    log_cdf_plus = plus_in - F.softplus(plus_in)\n\n    # log probability for edge case of 255 (before scaling)\n    # equivalent: (1 - torch.sigmoid(min_in)).log()\n    log_one_minus_cdf_min = -F.softplus(min_in)\n\n    # probability for all other cases\n    cdf_delta = cdf_plus - cdf_min\n\n    mid_in = inv_stdv * centered_y\n    # log probability in the center of the bin, to be used in extreme cases\n    # (not actually used in our code)\n    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n\n    # tf equivalent\n    """"""\n    log_probs = tf.where(x < -0.999, log_cdf_plus,\n                         tf.where(x > 0.999, log_one_minus_cdf_min,\n                                  tf.where(cdf_delta > 1e-5,\n                                           tf.log(tf.maximum(cdf_delta, 1e-12)),\n                                           log_pdf_mid - np.log(127.5))))\n    """"""\n    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n    # for num_classes=65536 case? 1e-7? not sure..\n    inner_inner_cond = (cdf_delta > 1e-5).float()\n\n    inner_inner_out = inner_inner_cond * \\\n        torch.log(torch.clamp(cdf_delta, min=1e-12)) + \\\n        (1. - inner_inner_cond) * (log_pdf_mid - np.log((num_classes - 1) / 2))\n    inner_cond = (y > 0.999).float()\n    inner_out = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n    cond = (y < -0.999).float()\n    log_probs = cond * log_cdf_plus + (1. - cond) * inner_out\n\n    log_probs = log_probs + F.log_softmax(logit_probs, -1)\n\n    if reduce:\n        return -torch.sum(log_sum_exp(log_probs))\n    else:\n        return -log_sum_exp(log_probs).unsqueeze(-1)\n\n\ndef to_one_hot(tensor, n, fill_with=1.):\n    # we perform one hot encore with respect to the last axis\n    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_()\n    if tensor.is_cuda:\n        one_hot = one_hot.cuda()\n    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n    return one_hot\n\n\ndef sample_from_discretized_mix_logistic(y, log_scale_min=-7.0,\n                                         clamp_log_scale=False):\n    """"""\n    Sample from discretized mixture of logistic distributions\n\n    Args:\n        y (Tensor): B x C x T\n        log_scale_min (float): Log scale minimum value\n\n    Returns:\n        Tensor: sample in range of [-1, 1].\n    """"""\n    assert y.size(1) % 3 == 0\n    nr_mix = y.size(1) // 3\n\n    # B x T x C\n    y = y.transpose(1, 2)\n    logit_probs = y[:, :, :nr_mix]\n\n    # sample mixture indicator from softmax\n    temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n    temp = logit_probs.data - torch.log(- torch.log(temp))\n    _, argmax = temp.max(dim=-1)\n\n    # (B, T) -> (B, T, nr_mix)\n    one_hot = to_one_hot(argmax, nr_mix)\n    # select logistic parameters\n    means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n    log_scales = torch.sum(y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, dim=-1)\n    if clamp_log_scale:\n        log_scales = torch.clamp(log_scales, min=log_scale_min)\n    # sample from logistic & clip to interval\n    # we don\'t actually round to the nearest 8bit value when sampling\n    u = means.data.new(means.size()).uniform_(1e-5, 1.0 - 1e-5)\n    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n\n    x = torch.clamp(torch.clamp(x, min=-1.), max=1.)\n\n    return x\n\n\n# we can easily define discretized version of the gaussian loss, however,\n# use continuous version as same as the https://clarinet-demo.github.io/\ndef mix_gaussian_loss(y_hat, y, log_scale_min=-7.0, reduce=True):\n    """"""Mixture of continuous gaussian distributions loss\n\n    Note that it is assumed that input is scaled to [-1, 1].\n\n    Args:\n        y_hat (Tensor): Predicted output (B x C x T)\n        y (Tensor): Target (B x T x 1).\n        log_scale_min (float): Log scale minimum value\n        reduce (bool): If True, the losses are averaged or summed for each\n          minibatch.\n    Returns\n        Tensor: loss\n    """"""\n    assert y_hat.dim() == 3\n    C = y_hat.size(1)\n    if C == 2:\n        nr_mix = 1\n    else:\n        assert y_hat.size(1) % 3 == 0\n        nr_mix = y_hat.size(1) // 3\n\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n\n    # unpack parameters.\n    if C == 2:\n        # special case for C == 2, just for compatibility\n        logit_probs = None\n        means = y_hat[:, :, 0:1]\n        log_scales = torch.clamp(y_hat[:, :, 1:2], min=log_scale_min)\n    else:\n        #  (B, T, num_mixtures) x 3\n        logit_probs = y_hat[:, :, :nr_mix]\n        means = y_hat[:, :, nr_mix:2 * nr_mix]\n        log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n\n    # B x T x 1 -> B x T x num_mixtures\n    y = y.expand_as(means)\n\n    centered_y = y - means\n    dist = Normal(loc=0., scale=torch.exp(log_scales))\n    # do we need to add a trick to avoid log(0)?\n    log_probs = dist.log_prob(centered_y)\n\n    if nr_mix > 1:\n        log_probs = log_probs + F.log_softmax(logit_probs, -1)\n\n    if reduce:\n        if nr_mix == 1:\n            return -torch.sum(log_probs)\n        else:\n            return -torch.sum(log_sum_exp(log_probs))\n    else:\n        if nr_mix == 1:\n            return -log_probs\n        else:\n            return -log_sum_exp(log_probs).unsqueeze(-1)\n\n\ndef sample_from_mix_gaussian(y, log_scale_min=-7.0):\n    """"""\n    Sample from (discretized) mixture of gaussian distributions\n    Args:\n        y (Tensor): B x C x T\n        log_scale_min (float): Log scale minimum value\n    Returns:\n        Tensor: sample in range of [-1, 1].\n    """"""\n    C = y.size(1)\n    if C == 2:\n        nr_mix = 1\n    else:\n        assert y.size(1) % 3 == 0\n        nr_mix = y.size(1) // 3\n\n    # B x T x C\n    y = y.transpose(1, 2)\n\n    if C == 2:\n        logit_probs = None\n    else:\n        logit_probs = y[:, :, :nr_mix]\n\n    if nr_mix > 1:\n        # sample mixture indicator from softmax\n        temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n        temp = logit_probs.data - torch.log(- torch.log(temp))\n        _, argmax = temp.max(dim=-1)\n\n        # (B, T) -> (B, T, nr_mix)\n        one_hot = to_one_hot(argmax, nr_mix)\n\n        # Select means and log scales\n        means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n        log_scales = torch.sum(y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, dim=-1)\n    else:\n        if C == 2:\n            means, log_scales = y[:, :, 0], y[:, :, 1]\n        elif C == 3:\n            means, log_scales = y[:, :, 1], y[:, :, 2]\n        else:\n            assert False, ""shouldn\'t happen""\n\n    scales = torch.exp(log_scales)\n    dist = Normal(loc=means, scale=scales)\n    x = dist.sample()\n\n    x = torch.clamp(x, min=-1.0, max=1.0)\n    return x\n'"
wavenet_vocoder/modules.py,2,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport math\nimport numpy as np\n\nimport torch\nfrom wavenet_vocoder import conv\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ndef Conv1d(in_channels, out_channels, kernel_size, dropout=0, **kwargs):\n    m = conv.Conv1d(in_channels, out_channels, kernel_size, **kwargs)\n    nn.init.kaiming_normal_(m.weight, nonlinearity=""relu"")\n    if m.bias is not None:\n        nn.init.constant_(m.bias, 0)\n    return nn.utils.weight_norm(m)\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx, std=0.01):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    m.weight.data.normal_(0, std)\n    return m\n\n\ndef ConvTranspose2d(in_channels, out_channels, kernel_size, **kwargs):\n    freq_axis_kernel_size = kernel_size[0]\n    m = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, **kwargs)\n    m.weight.data.fill_(1.0 / freq_axis_kernel_size)\n    m.bias.data.zero_()\n    return nn.utils.weight_norm(m)\n\n\ndef Conv1d1x1(in_channels, out_channels, bias=True):\n    """"""1-by-1 convolution layer\n    """"""\n    return Conv1d(in_channels, out_channels, kernel_size=1, padding=0,\n                  dilation=1, bias=bias)\n\n\ndef _conv1x1_forward(conv, x, is_incremental):\n    """"""Conv1x1 forward\n    """"""\n    if is_incremental:\n        x = conv.incremental_forward(x)\n    else:\n        x = conv(x)\n    return x\n\n\nclass ResidualConv1dGLU(nn.Module):\n    """"""Residual dilated conv1d + Gated linear unit\n\n    Args:\n        residual_channels (int): Residual input / output channels\n        gate_channels (int): Gated activation channels.\n        kernel_size (int): Kernel size of convolution layers.\n        skip_out_channels (int): Skip connection channels. If None, set to same\n          as ``residual_channels``.\n        cin_channels (int): Local conditioning channels. If negative value is\n          set, local conditioning is disabled.\n        gin_channels (int): Global conditioning channels. If negative value is\n          set, global conditioning is disabled.\n        dropout (float): Dropout probability.\n        padding (int): Padding for convolution layers. If None, proper padding\n          is computed depends on dilation and kernel_size.\n        dilation (int): Dilation factor.\n    """"""\n\n    def __init__(self, residual_channels, gate_channels, kernel_size,\n                 skip_out_channels=None,\n                 cin_channels=-1, gin_channels=-1,\n                 dropout=1 - 0.95, padding=None, dilation=1, causal=True,\n                 bias=True, *args, **kwargs):\n        super(ResidualConv1dGLU, self).__init__()\n        self.dropout = dropout\n        if skip_out_channels is None:\n            skip_out_channels = residual_channels\n        if padding is None:\n            # no future time stamps available\n            if causal:\n                padding = (kernel_size - 1) * dilation\n            else:\n                padding = (kernel_size - 1) // 2 * dilation\n        self.causal = causal\n\n        self.conv = Conv1d(residual_channels, gate_channels, kernel_size,\n                           padding=padding, dilation=dilation,\n                           bias=bias, *args, **kwargs)\n\n        # local conditioning\n        if cin_channels > 0:\n            self.conv1x1c = Conv1d1x1(cin_channels, gate_channels, bias=False)\n        else:\n            self.conv1x1c = None\n\n        # global conditioning\n        if gin_channels > 0:\n            self.conv1x1g = Conv1d1x1(gin_channels, gate_channels, bias=False)\n        else:\n            self.conv1x1g = None\n\n        # conv output is split into two groups\n        gate_out_channels = gate_channels // 2\n        self.conv1x1_out = Conv1d1x1(gate_out_channels, residual_channels, bias=bias)\n        self.conv1x1_skip = Conv1d1x1(gate_out_channels, skip_out_channels, bias=bias)\n\n    def forward(self, x, c=None, g=None):\n        return self._forward(x, c, g, False)\n\n    def incremental_forward(self, x, c=None, g=None):\n        return self._forward(x, c, g, True)\n\n    def _forward(self, x, c, g, is_incremental):\n        """"""Forward\n\n        Args:\n            x (Tensor): B x C x T\n            c (Tensor): B x C x T, Local conditioning features\n            g (Tensor): B x C x T, Expanded global conditioning features\n            is_incremental (Bool) : Whether incremental mode or not\n\n        Returns:\n            Tensor: output\n        """"""\n        residual = x\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        if is_incremental:\n            splitdim = -1\n            x = self.conv.incremental_forward(x)\n        else:\n            splitdim = 1\n            x = self.conv(x)\n            # remove future time steps\n            x = x[:, :, :residual.size(-1)] if self.causal else x\n\n        a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n\n        # local conditioning\n        if c is not None:\n            assert self.conv1x1c is not None\n            c = _conv1x1_forward(self.conv1x1c, c, is_incremental)\n            ca, cb = c.split(c.size(splitdim) // 2, dim=splitdim)\n            a, b = a + ca, b + cb\n\n        # global conditioning\n        if g is not None:\n            assert self.conv1x1g is not None\n            g = _conv1x1_forward(self.conv1x1g, g, is_incremental)\n            ga, gb = g.split(g.size(splitdim) // 2, dim=splitdim)\n            a, b = a + ga, b + gb\n\n        x = torch.tanh(a) * torch.sigmoid(b)\n\n        # For skip connection\n        s = _conv1x1_forward(self.conv1x1_skip, x, is_incremental)\n\n        # For residual connection\n        x = _conv1x1_forward(self.conv1x1_out, x, is_incremental)\n\n        x = (x + residual) * math.sqrt(0.5)\n        return x, s\n\n    def clear_buffer(self):\n        for c in [self.conv, self.conv1x1_out, self.conv1x1_skip,\n                  self.conv1x1c, self.conv1x1g]:\n            if c is not None:\n                c.clear_buffer()\n'"
wavenet_vocoder/upsample.py,1,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport math\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Stretch2d(nn.Module):\n    def __init__(self, x_scale, y_scale, mode=""nearest""):\n        super(Stretch2d, self).__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n        self.mode = mode\n\n    def forward(self, x):\n        return F.interpolate(\n            x, scale_factor=(self.y_scale, self.x_scale), mode=self.mode)\n\n\ndef _get_activation(upsample_activation):\n    nonlinear = getattr(nn, upsample_activation)\n    return nonlinear\n\n\nclass UpsampleNetwork(nn.Module):\n    def __init__(self, upsample_scales, upsample_activation=""none"",\n                 upsample_activation_params={}, mode=""nearest"",\n                 freq_axis_kernel_size=1, cin_pad=0, cin_channels=80):\n        super(UpsampleNetwork, self).__init__()\n        self.up_layers = nn.ModuleList()\n        total_scale = np.prod(upsample_scales)\n        self.indent = cin_pad * total_scale\n        for scale in upsample_scales:\n            freq_axis_padding = (freq_axis_kernel_size - 1) // 2\n            k_size = (freq_axis_kernel_size, scale * 2 + 1)\n            padding = (freq_axis_padding, scale)\n            stretch = Stretch2d(scale, 1, mode)\n            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n            conv.weight.data.fill_(1. / np.prod(k_size))\n            conv = nn.utils.weight_norm(conv)\n            self.up_layers.append(stretch)\n            self.up_layers.append(conv)\n            if upsample_activation != ""none"":\n                nonlinear = _get_activation(upsample_activation)\n                self.up_layers.append(nonlinear(**upsample_activation_params))\n\n    def forward(self, c):\n        """"""\n        Args:\n            c : B x C x T\n        """"""\n\n        # B x 1 x C x T\n        c = c.unsqueeze(1)\n        for f in self.up_layers:\n            c = f(c)\n        # B x C x T\n        c = c.squeeze(1)\n\n        if self.indent > 0:\n            c = c[:, :, self.indent:-self.indent]\n        return c\n\n\nclass ConvInUpsampleNetwork(nn.Module):\n    def __init__(self, upsample_scales, upsample_activation=""none"",\n                 upsample_activation_params={}, mode=""nearest"",\n                 freq_axis_kernel_size=1, cin_pad=0,\n                 cin_channels=80):\n        super(ConvInUpsampleNetwork, self).__init__()\n        # To capture wide-context information in conditional features\n        # meaningless if cin_pad == 0\n        ks = 2 * cin_pad + 1\n        self.conv_in = nn.Conv1d(cin_channels, cin_channels, kernel_size=ks, bias=False)\n        self.upsample = UpsampleNetwork(\n            upsample_scales, upsample_activation, upsample_activation_params,\n            mode, freq_axis_kernel_size, cin_pad=0, cin_channels=cin_channels)\n\n    def forward(self, c):\n        c_up = self.upsample(self.conv_in(c))\n        return c_up\n'"
wavenet_vocoder/util.py,0,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\n\ndef _assert_valid_input_type(s):\n    assert s == ""mulaw-quantize"" or s == ""mulaw"" or s == ""raw""\n\n\ndef is_mulaw_quantize(s):\n    _assert_valid_input_type(s)\n    return s == ""mulaw-quantize""\n\n\ndef is_mulaw(s):\n    _assert_valid_input_type(s)\n    return s == ""mulaw""\n\n\ndef is_raw(s):\n    _assert_valid_input_type(s)\n    return s == ""raw""\n\n\ndef is_scalar_input(s):\n    return is_raw(s) or is_mulaw(s)\n'"
wavenet_vocoder/version.py,0,"b""version = '0.2.0'\n"""
wavenet_vocoder/wavenet.py,5,"b'# coding: utf-8\nfrom __future__ import with_statement, print_function, absolute_import\n\nimport math\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .modules import Embedding\n\nfrom .modules import Conv1d1x1, ResidualConv1dGLU, ConvTranspose2d\nfrom .mixture import sample_from_discretized_mix_logistic\nfrom .mixture import sample_from_mix_gaussian\nfrom wavenet_vocoder import upsample\n\n\ndef _expand_global_features(B, T, g, bct=True):\n    """"""Expand global conditioning features to all time steps\n\n    Args:\n        B (int): Batch size.\n        T (int): Time length.\n        g (Tensor): Global features, (B x C) or (B x C x 1).\n        bct (bool) : returns (B x C x T) if True, otherwise (B x T x C)\n\n    Returns:\n        Tensor: B x C x T or B x T x C or None\n    """"""\n    if g is None:\n        return None\n    g = g.unsqueeze(-1) if g.dim() == 2 else g\n    if bct:\n        g_bct = g.expand(B, -1, T)\n        return g_bct.contiguous()\n    else:\n        g_btc = g.expand(B, -1, T).transpose(1, 2)\n        return g_btc.contiguous()\n\n\ndef receptive_field_size(total_layers, num_cycles, kernel_size,\n                         dilation=lambda x: 2**x):\n    """"""Compute receptive field size\n\n    Args:\n        total_layers (int): total layers\n        num_cycles (int): cycles\n        kernel_size (int): kernel size\n        dilation (lambda): lambda to compute dilation factor. ``lambda x : 1``\n          to disable dilated convolution.\n\n    Returns:\n        int: receptive field size in sample\n\n    """"""\n    assert total_layers % num_cycles == 0\n    layers_per_cycle = total_layers // num_cycles\n    dilations = [dilation(i % layers_per_cycle) for i in range(total_layers)]\n    return (kernel_size - 1) * sum(dilations) + 1\n\n\nclass WaveNet(nn.Module):\n    """"""The WaveNet model that supports local and global conditioning.\n\n    Args:\n        out_channels (int): Output channels. If input_type is mu-law quantized\n          one-hot vecror. this must equal to the quantize channels. Other wise\n          num_mixtures x 3 (pi, mu, log_scale).\n        layers (int): Number of total layers\n        stacks (int): Number of dilation cycles\n        residual_channels (int): Residual input / output channels\n        gate_channels (int): Gated activation channels.\n        skip_out_channels (int): Skip connection channels.\n        kernel_size (int): Kernel size of convolution layers.\n        dropout (float): Dropout probability.\n        cin_channels (int): Local conditioning channels. If negative value is\n          set, local conditioning is disabled.\n        gin_channels (int): Global conditioning channels. If negative value is\n          set, global conditioning is disabled.\n        n_speakers (int): Number of speakers. Used only if global conditioning\n          is enabled.\n        upsample_conditional_features (bool): Whether upsampling local\n          conditioning features by transposed convolution layers or not.\n        upsample_scales (list): List of upsample scale.\n          ``np.prod(upsample_scales)`` must equal to hop size. Used only if\n          upsample_conditional_features is enabled.\n        freq_axis_kernel_size (int): Freq-axis kernel_size for transposed\n          convolution layers for upsampling. If you only care about time-axis\n          upsampling, set this to 1.\n        scalar_input (Bool): If True, scalar input ([-1, 1]) is expected, otherwise\n          quantized one-hot vector is expected.\n        use_speaker_embedding (Bool): Use speaker embedding or Not. Set to False\n          if you want to disable embedding layer and use external features\n          directly.\n    """"""\n\n    def __init__(self, out_channels=256, layers=20, stacks=2,\n                 residual_channels=512,\n                 gate_channels=512,\n                 skip_out_channels=512,\n                 kernel_size=3, dropout=1 - 0.95,\n                 cin_channels=-1, gin_channels=-1, n_speakers=None,\n                 upsample_conditional_features=False,\n                 upsample_net=""ConvInUpsampleNetwork"",\n                 upsample_params={""upsample_scales"": [4, 4, 4, 4]},\n                 scalar_input=False,\n                 use_speaker_embedding=False,\n                 output_distribution=""Logistic"",\n                 cin_pad=0,\n                 ):\n        super(WaveNet, self).__init__()\n        self.scalar_input = scalar_input\n        self.out_channels = out_channels\n        self.cin_channels = cin_channels\n        self.output_distribution = output_distribution\n        assert layers % stacks == 0\n        layers_per_stack = layers // stacks\n        if scalar_input:\n            self.first_conv = Conv1d1x1(1, residual_channels)\n        else:\n            self.first_conv = Conv1d1x1(out_channels, residual_channels)\n\n        self.conv_layers = nn.ModuleList()\n        for layer in range(layers):\n            dilation = 2**(layer % layers_per_stack)\n            conv = ResidualConv1dGLU(\n                residual_channels, gate_channels,\n                kernel_size=kernel_size,\n                skip_out_channels=skip_out_channels,\n                bias=True,  # magenda uses bias, but musyoku doesn\'t\n                dilation=dilation, dropout=dropout,\n                cin_channels=cin_channels,\n                gin_channels=gin_channels)\n            self.conv_layers.append(conv)\n        self.last_conv_layers = nn.ModuleList([\n            nn.ReLU(inplace=True),\n            Conv1d1x1(skip_out_channels, skip_out_channels),\n            nn.ReLU(inplace=True),\n            Conv1d1x1(skip_out_channels, out_channels),\n        ])\n\n        if gin_channels > 0 and use_speaker_embedding:\n            assert n_speakers is not None\n            self.embed_speakers = Embedding(\n                n_speakers, gin_channels, padding_idx=None, std=0.1)\n        else:\n            self.embed_speakers = None\n\n        # Upsample conv net\n        if upsample_conditional_features:\n            self.upsample_net = getattr(upsample, upsample_net)(**upsample_params)\n        else:\n            self.upsample_net = None\n\n        self.receptive_field = receptive_field_size(layers, stacks, kernel_size)\n\n    def has_speaker_embedding(self):\n        return self.embed_speakers is not None\n\n    def local_conditioning_enabled(self):\n        return self.cin_channels > 0\n\n    def forward(self, x, c=None, g=None, softmax=False):\n        """"""Forward step\n\n        Args:\n            x (Tensor): One-hot encoded audio signal, shape (B x C x T)\n            c (Tensor): Local conditioning features,\n              shape (B x cin_channels x T)\n            g (Tensor): Global conditioning features,\n              shape (B x gin_channels x 1) or speaker Ids of shape (B x 1).\n              Note that ``self.use_speaker_embedding`` must be False when you\n              want to disable embedding layer and use external features\n              directly (e.g., one-hot vector).\n              Also type of input tensor must be FloatTensor, not LongTensor\n              in case of ``self.use_speaker_embedding`` equals False.\n            softmax (bool): Whether applies softmax or not.\n\n        Returns:\n            Tensor: output, shape B x out_channels x T\n        """"""\n        B, _, T = x.size()\n\n        if g is not None:\n            if self.embed_speakers is not None:\n                # (B x 1) -> (B x 1 x gin_channels)\n                g = self.embed_speakers(g.view(B, -1))\n                # (B x gin_channels x 1)\n                g = g.transpose(1, 2)\n                assert g.dim() == 3\n        # Expand global conditioning features to all time steps\n        g_bct = _expand_global_features(B, T, g, bct=True)\n\n        if c is not None and self.upsample_net is not None:\n            c = self.upsample_net(c)\n            assert c.size(-1) == x.size(-1)\n\n        # Feed data to network\n        x = self.first_conv(x)\n        skips = 0\n        for f in self.conv_layers:\n            x, h = f(x, c, g_bct)\n            skips += h\n        skips *= math.sqrt(1.0 / len(self.conv_layers))\n\n        x = skips\n        for f in self.last_conv_layers:\n            x = f(x)\n\n        x = F.softmax(x, dim=1) if softmax else x\n\n        return x\n\n    def incremental_forward(self, initial_input=None, c=None, g=None,\n                            T=100, test_inputs=None,\n                            tqdm=lambda x: x, softmax=True, quantize=True,\n                            log_scale_min=-50.0):\n        """"""Incremental forward step\n\n        Due to linearized convolutions, inputs of shape (B x C x T) are reshaped\n        to (B x T x C) internally and fed to the network for each time step.\n        Input of each time step will be of shape (B x 1 x C).\n\n        Args:\n            initial_input (Tensor): Initial decoder input, (B x C x 1)\n            c (Tensor): Local conditioning features, shape (B x C\' x T)\n            g (Tensor): Global conditioning features, shape (B x C\'\' or B x C\'\'x 1)\n            T (int): Number of time steps to generate.\n            test_inputs (Tensor): Teacher forcing inputs (for debugging)\n            tqdm (lamda) : tqdm\n            softmax (bool) : Whether applies softmax or not\n            quantize (bool): Whether quantize softmax output before feeding the\n              network output to input for the next time step. TODO: rename\n            log_scale_min (float):  Log scale minimum value.\n\n        Returns:\n            Tensor: Generated one-hot encoded samples. B x C x T\xe3\x80\x80\n              or scaler vector B x 1 x T\n        """"""\n        self.clear_buffer()\n        B = 1\n\n        # Note: shape should be **(B x T x C)**, not (B x C x T) opposed to\n        # batch forward due to linealized convolution\n        if test_inputs is not None:\n            if self.scalar_input:\n                if test_inputs.size(1) == 1:\n                    test_inputs = test_inputs.transpose(1, 2).contiguous()\n            else:\n                if test_inputs.size(1) == self.out_channels:\n                    test_inputs = test_inputs.transpose(1, 2).contiguous()\n\n            B = test_inputs.size(0)\n            if T is None:\n                T = test_inputs.size(1)\n            else:\n                T = max(T, test_inputs.size(1))\n        # cast to int in case of numpy.int64...\n        T = int(T)\n\n        # Global conditioning\n        if g is not None:\n            if self.embed_speakers is not None:\n                g = self.embed_speakers(g.view(B, -1))\n                # (B x gin_channels, 1)\n                g = g.transpose(1, 2)\n                assert g.dim() == 3\n        g_btc = _expand_global_features(B, T, g, bct=False)\n\n        # Local conditioning\n        if c is not None:\n            B = c.shape[0]\n            if self.upsample_net is not None:\n                c = self.upsample_net(c)\n                assert c.size(-1) == T\n            if c.size(-1) == T:\n                c = c.transpose(1, 2).contiguous()\n\n        outputs = []\n        if initial_input is None:\n            if self.scalar_input:\n                initial_input = torch.zeros(B, 1, 1)\n            else:\n                initial_input = torch.zeros(B, 1, self.out_channels)\n                initial_input[:, :, 127] = 1  # TODO: is this ok?\n            # https://github.com/pytorch/pytorch/issues/584#issuecomment-275169567\n            if next(self.parameters()).is_cuda:\n                initial_input = initial_input.cuda()\n        else:\n            if initial_input.size(1) == self.out_channels:\n                initial_input = initial_input.transpose(1, 2).contiguous()\n\n        current_input = initial_input\n\n        for t in tqdm(range(T)):\n            if test_inputs is not None and t < test_inputs.size(1):\n                current_input = test_inputs[:, t, :].unsqueeze(1)\n            else:\n                if t > 0:\n                    current_input = outputs[-1]\n\n            # Conditioning features for single time step\n            ct = None if c is None else c[:, t, :].unsqueeze(1)\n            gt = None if g is None else g_btc[:, t, :].unsqueeze(1)\n\n            x = current_input\n            x = self.first_conv.incremental_forward(x)\n            skips = 0\n            for f in self.conv_layers:\n                x, h = f.incremental_forward(x, ct, gt)\n                skips += h\n            skips *= math.sqrt(1.0 / len(self.conv_layers))\n            x = skips\n            for f in self.last_conv_layers:\n                try:\n                    x = f.incremental_forward(x)\n                except AttributeError:\n                    x = f(x)\n\n            # Generate next input by sampling\n            if self.scalar_input:\n                if self.output_distribution == ""Logistic"":\n                    x = sample_from_discretized_mix_logistic(\n                        x.view(B, -1, 1), log_scale_min=log_scale_min)\n                elif self.output_distribution == ""Normal"":\n                    x = sample_from_mix_gaussian(\n                        x.view(B, -1, 1), log_scale_min=log_scale_min)\n                else:\n                    assert False\n            else:\n                x = F.softmax(x.view(B, -1), dim=1) if softmax else x.view(B, -1)\n                if quantize:\n                    dist = torch.distributions.OneHotCategorical(x)\n                    x = dist.sample()\n            outputs += [x.data]\n        # T x B x C\n        outputs = torch.stack(outputs)\n        # B x C x T\n        outputs = outputs.transpose(0, 1).transpose(1, 2).contiguous()\n\n        self.clear_buffer()\n        return outputs\n\n    def clear_buffer(self):\n        self.first_conv.clear_buffer()\n        for f in self.conv_layers:\n            f.clear_buffer()\n        for f in self.last_conv_layers:\n            try:\n                f.clear_buffer()\n            except AttributeError:\n                pass\n\n    def make_generation_fast_(self):\n        def remove_weight_norm(m):\n            try:\n                nn.utils.remove_weight_norm(m)\n            except ValueError:  # this module didn\'t have weight norm\n                return\n        self.apply(remove_weight_norm)\n'"
wavenet_vocoder/tfcompat/__init__.py,0,b''
wavenet_vocoder/tfcompat/hparam.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Hyperparameter values.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport numbers\nimport re\n\nimport six\n\n## from tensorflow.contrib.training.python.training import hparam_pb2\n## from tensorflow.python.framework import ops\n## from tensorflow.python.util import compat\n## from tensorflow.python.util import deprecation\n\n# Define the regular expression for parsing a single clause of the input\n# (delimited by commas).  A legal clause looks like:\n#   <variable name>[<index>]? = <rhs>\n# where <rhs> is either a single token or [] enclosed list of tokens.\n# For example:  ""var[1] = a"" or ""x = [1,2,3]""\nPARAM_RE = re.compile(r""""""\n  (?P<name>[a-zA-Z][\\w\\.]*)      # variable name: ""var"" or ""x""\n  (\\[\\s*(?P<index>\\d+)\\s*\\])?  # (optional) index: ""1"" or None\n  \\s*=\\s*\n  ((?P<val>[^,\\[]*)            # single value: ""a"" or None\n   |\n   \\[(?P<vals>[^\\]]*)\\])       # list of values: None or ""1,2,3""\n  ($|,\\s*)"""""", re.VERBOSE)\n\n\ndef _parse_fail(name, var_type, value, values):\n  """"""Helper function for raising a value error for bad assignment.""""""\n  raise ValueError(\n      \'Could not parse hparam \\\'%s\\\' of type \\\'%s\\\' with value \\\'%s\\\' in %s\' %\n      (name, var_type.__name__, value, values))\n\n\ndef _reuse_fail(name, values):\n  """"""Helper function for raising a value error for reuse of name.""""""\n  raise ValueError(\'Multiple assignments to variable \\\'%s\\\' in %s\' % (name,\n                                                                      values))\n\n\ndef _process_scalar_value(name, parse_fn, var_type, m_dict, values,\n                          results_dictionary):\n  """"""Update results_dictionary with a scalar value.\n\n  Used to update the results_dictionary to be returned by parse_values when\n  encountering a clause with a scalar RHS (e.g.  ""s=5"" or ""arr[0]=5"".)\n\n  Mutates results_dictionary.\n\n  Args:\n    name: Name of variable in assignment (""s"" or ""arr"").\n    parse_fn: Function for parsing the actual value.\n    var_type: Type of named variable.\n    m_dict: Dictionary constructed from regex parsing.\n      m_dict[\'val\']: RHS value (scalar)\n      m_dict[\'index\']: List index value (or None)\n    values: Full expression being parsed\n    results_dictionary: The dictionary being updated for return by the parsing\n      function.\n\n  Raises:\n    ValueError: If the name has already been used.\n  """"""\n  try:\n    parsed_value = parse_fn(m_dict[\'val\'])\n  except ValueError:\n    _parse_fail(name, var_type, m_dict[\'val\'], values)\n\n  # If no index is provided\n  if not m_dict[\'index\']:\n    if name in results_dictionary:\n      _reuse_fail(name, values)\n    results_dictionary[name] = parsed_value\n  else:\n    if name in results_dictionary:\n      # The name has already been used as a scalar, then it\n      # will be in this dictionary and map to a non-dictionary.\n      if not isinstance(results_dictionary.get(name), dict):\n        _reuse_fail(name, values)\n    else:\n      results_dictionary[name] = {}\n\n    index = int(m_dict[\'index\'])\n    # Make sure the index position hasn\'t already been assigned a value.\n    if index in results_dictionary[name]:\n      _reuse_fail(\'{}[{}]\'.format(name, index), values)\n    results_dictionary[name][index] = parsed_value\n\n\ndef _process_list_value(name, parse_fn, var_type, m_dict, values,\n                        results_dictionary):\n  """"""Update results_dictionary from a list of values.\n\n  Used to update results_dictionary to be returned by parse_values when\n  encountering a clause with a list RHS (e.g.  ""arr=[1,2,3]"".)\n\n  Mutates results_dictionary.\n\n  Args:\n    name: Name of variable in assignment (""arr"").\n    parse_fn: Function for parsing individual values.\n    var_type: Type of named variable.\n    m_dict: Dictionary constructed from regex parsing.\n      m_dict[\'val\']: RHS value (scalar)\n    values: Full expression being parsed\n    results_dictionary: The dictionary being updated for return by the parsing\n      function.\n\n  Raises:\n    ValueError: If the name has an index or the values cannot be parsed.\n  """"""\n  if m_dict[\'index\'] is not None:\n    raise ValueError(\'Assignment of a list to a list index.\')\n  elements = filter(None, re.split(\'[ ,]\', m_dict[\'vals\']))\n  # Make sure the name hasn\'t already been assigned a value\n  if name in results_dictionary:\n    raise _reuse_fail(name, values)\n  try:\n    results_dictionary[name] = [parse_fn(e) for e in elements]\n  except ValueError:\n    _parse_fail(name, var_type, m_dict[\'vals\'], values)\n\n\ndef _cast_to_type_if_compatible(name, param_type, value):\n  """"""Cast hparam to the provided type, if compatible.\n\n  Args:\n    name: Name of the hparam to be cast.\n    param_type: The type of the hparam.\n    value: The value to be cast, if compatible.\n\n  Returns:\n    The result of casting `value` to `param_type`.\n\n  Raises:\n    ValueError: If the type of `value` is not compatible with param_type.\n      * If `param_type` is a string type, but `value` is not.\n      * If `param_type` is a boolean, but `value` is not, or vice versa.\n      * If `param_type` is an integer type, but `value` is not.\n      * If `param_type` is a float type, but `value` is not a numeric type.\n  """"""\n  fail_msg = (\n      ""Could not cast hparam \'%s\' of type \'%s\' from value %r"" %\n      (name, param_type, value))\n\n  # Some callers use None, for which we can\'t do any casting/checking. :(\n  if issubclass(param_type, type(None)):\n    return value\n\n  # Avoid converting a non-string type to a string.\n  if (issubclass(param_type, (six.string_types, six.binary_type)) and\n      not isinstance(value, (six.string_types, six.binary_type))):\n    raise ValueError(fail_msg)\n\n  # Avoid converting a number or string type to a boolean or vice versa.\n  if issubclass(param_type, bool) != isinstance(value, bool):\n    raise ValueError(fail_msg)\n\n  # Avoid converting float to an integer (the reverse is fine).\n  if (issubclass(param_type, numbers.Integral) and\n      not isinstance(value, numbers.Integral)):\n    raise ValueError(fail_msg)\n\n  # Avoid converting a non-numeric type to a numeric type.\n  if (issubclass(param_type, numbers.Number) and\n      not isinstance(value, numbers.Number)):\n    raise ValueError(fail_msg)\n\n  return param_type(value)\n\n\ndef parse_values(values, type_map):\n  """"""Parses hyperparameter values from a string into a python map.\n\n  `values` is a string containing comma-separated `name=value` pairs.\n  For each pair, the value of the hyperparameter named `name` is set to\n  `value`.\n\n  If a hyperparameter name appears multiple times in `values`, a ValueError\n  is raised (e.g. \'a=1,a=2\', \'a[1]=1,a[1]=2\').\n\n  If a hyperparameter name in both an index assignment and scalar assignment,\n  a ValueError is raised.  (e.g. \'a=[1,2,3],a[0] = 1\').\n\n  The hyperparameter name may contain \'.\' symbols, which will result in an\n  attribute name that is only accessible through the getattr and setattr\n  functions.  (And must be first explicit added through add_hparam.)\n\n  WARNING: Use of \'.\' in your variable names is allowed, but is not well\n  supported and not recommended.\n\n  The `value` in `name=value` must follows the syntax according to the\n  type of the parameter:\n\n  *  Scalar integer: A Python-parsable integer point value.  E.g.: 1,\n     100, -12.\n  *  Scalar float: A Python-parsable floating point value.  E.g.: 1.0,\n     -.54e89.\n  *  Boolean: Either true or false.\n  *  Scalar string: A non-empty sequence of characters, excluding comma,\n     spaces, and square brackets.  E.g.: foo, bar_1.\n  *  List: A comma separated list of scalar values of the parameter type\n     enclosed in square brackets.  E.g.: [1,2,3], [1.0,1e-12], [high,low].\n\n  When index assignment is used, the corresponding type_map key should be the\n  list name.  E.g. for ""arr[1]=0"" the type_map must have the key ""arr"" (not\n  ""arr[1]"").\n\n  Args:\n    values: String.  Comma separated list of `name=value` pairs where\n      \'value\' must follow the syntax described above.\n    type_map: A dictionary mapping hyperparameter names to types.  Note every\n      parameter name in values must be a key in type_map.  The values must\n      conform to the types indicated, where a value V is said to conform to a\n      type T if either V has type T, or V is a list of elements of type T.\n      Hence, for a multidimensional parameter \'x\' taking float values,\n      \'x=[0.1,0.2]\' will parse successfully if type_map[\'x\'] = float.\n\n  Returns:\n    A python map mapping each name to either:\n    * A scalar value.\n    * A list of scalar values.\n    * A dictionary mapping index numbers to scalar values.\n    (e.g. ""x=5,L=[1,2],arr[1]=3"" results in {\'x\':5,\'L\':[1,2],\'arr\':{1:3}}"")\n\n  Raises:\n    ValueError: If there is a problem with input.\n    * If `values` cannot be parsed.\n    * If a list is assigned to a list index (e.g. \'a[1] = [1,2,3]\').\n    * If the same rvalue is assigned two different values (e.g. \'a=1,a=2\',\n      \'a[1]=1,a[1]=2\', or \'a=1,a=[1]\')\n  """"""\n  results_dictionary = {}\n  pos = 0\n  while pos < len(values):\n    m = PARAM_RE.match(values, pos)\n    if not m:\n      raise ValueError(\'Malformed hyperparameter value: %s\' % values[pos:])\n    # Check that there is a comma between parameters and move past it.\n    pos = m.end()\n    # Parse the values.\n    m_dict = m.groupdict()\n    name = m_dict[\'name\']\n    if name not in type_map:\n      raise ValueError(\'Unknown hyperparameter type for %s\' % name)\n    type_ = type_map[name]\n\n    # Set up correct parsing function (depending on whether type_ is a bool)\n    if type_ == bool:\n\n      def parse_bool(value):\n        if value in [\'true\', \'True\']:\n          return True\n        elif value in [\'false\', \'False\']:\n          return False\n        else:\n          try:\n            return bool(int(value))\n          except ValueError:\n            _parse_fail(name, type_, value, values)\n\n      parse = parse_bool\n    else:\n      parse = type_\n\n    # If a singe value is provided\n    if m_dict[\'val\'] is not None:\n      _process_scalar_value(name, parse, type_, m_dict, values,\n                            results_dictionary)\n\n    # If the assigned value is a list:\n    elif m_dict[\'vals\'] is not None:\n      _process_list_value(name, parse, type_, m_dict, values,\n                          results_dictionary)\n\n    else:  # Not assigned a list or value\n      _parse_fail(name, type_, \'\', values)\n\n  return results_dictionary\n\n\nclass HParams(object):\n  """"""Class to hold a set of hyperparameters as name-value pairs.\n\n  A `HParams` object holds hyperparameters used to build and train a model,\n  such as the number of hidden units in a neural net layer or the learning rate\n  to use when training.\n\n  You first create a `HParams` object by specifying the names and values of the\n  hyperparameters.\n\n  To make them easily accessible the parameter names are added as direct\n  attributes of the class.  A typical usage is as follows:\n\n  ```python\n  # Create a HParams object specifying names and values of the model\n  # hyperparameters:\n  hparams = HParams(learning_rate=0.1, num_hidden_units=100)\n\n  # The hyperparameter are available as attributes of the HParams object:\n  hparams.learning_rate ==> 0.1\n  hparams.num_hidden_units ==> 100\n  ```\n\n  Hyperparameters have type, which is inferred from the type of their value\n  passed at construction type.   The currently supported types are: integer,\n  float, boolean, string, and list of integer, float, boolean, or string.\n\n  You can override hyperparameter values by calling the\n  [`parse()`](#HParams.parse) method, passing a string of comma separated\n  `name=value` pairs.  This is intended to make it possible to override\n  any hyperparameter values from a single command-line flag to which\n  the user passes \'hyper-param=value\' pairs.  It avoids having to define\n  one flag for each hyperparameter.\n\n  The syntax expected for each value depends on the type of the parameter.\n  See `parse()` for a description of the syntax.\n\n  Example:\n\n  ```python\n  # Define a command line flag to pass name=value pairs.\n  # For example using argparse:\n  import argparse\n  parser = argparse.ArgumentParser(description=\'Train my model.\')\n  parser.add_argument(\'--hparams\', type=str,\n                      help=\'Comma separated list of ""name=value"" pairs.\')\n  args = parser.parse_args()\n  ...\n  def my_program():\n    # Create a HParams object specifying the names and values of the\n    # model hyperparameters:\n    hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,\n                         activations=[\'relu\', \'tanh\'])\n\n    # Override hyperparameters values by parsing the command line\n    hparams.parse(args.hparams)\n\n    # If the user passed `--hparams=learning_rate=0.3` on the command line\n    # then \'hparams\' has the following attributes:\n    hparams.learning_rate ==> 0.3\n    hparams.num_hidden_units ==> 100\n    hparams.activations ==> [\'relu\', \'tanh\']\n\n    # If the hyperparameters are in json format use parse_json:\n    hparams.parse_json(\'{""learning_rate"": 0.3, ""activations"": ""relu""}\')\n  ```\n  """"""\n\n  _HAS_DYNAMIC_ATTRIBUTES = True  # Required for pytype checks.\n\n  def __init__(self, hparam_def=None, model_structure=None, **kwargs):\n    """"""Create an instance of `HParams` from keyword arguments.\n\n    The keyword arguments specify name-values pairs for the hyperparameters.\n    The parameter types are inferred from the type of the values passed.\n\n    The parameter names are added as attributes of `HParams` object, so they\n    can be accessed directly with the dot notation `hparams._name_`.\n\n    Example:\n\n    ```python\n    # Define 3 hyperparameters: \'learning_rate\' is a float parameter,\n    # \'num_hidden_units\' an integer parameter, and \'activation\' a string\n    # parameter.\n    hparams = tf.HParams(\n        learning_rate=0.1, num_hidden_units=100, activation=\'relu\')\n\n    hparams.activation ==> \'relu\'\n    ```\n\n    Note that a few names are reserved and cannot be used as hyperparameter\n    names.  If you use one of the reserved name the constructor raises a\n    `ValueError`.\n\n    Args:\n      hparam_def: Serialized hyperparameters, encoded as a hparam_pb2.HParamDef\n        protocol buffer. If provided, this object is initialized by\n        deserializing hparam_def.  Otherwise **kwargs is used.\n      model_structure: An instance of ModelStructure, defining the feature\n        crosses to be used in the Trial.\n      **kwargs: Key-value pairs where the key is the hyperparameter name and\n        the value is the value for the parameter.\n\n    Raises:\n      ValueError: If both `hparam_def` and initialization values are provided,\n        or if one of the arguments is invalid.\n\n    """"""\n    # Register the hyperparameters and their type in _hparam_types.\n    # This simplifies the implementation of parse().\n    # _hparam_types maps the parameter name to a tuple (type, bool).\n    # The type value is the type of the parameter for scalar hyperparameters,\n    # or the type of the list elements for multidimensional hyperparameters.\n    # The bool value is True if the value is a list, False otherwise.\n    self._hparam_types = {}\n    self._model_structure = model_structure\n    if hparam_def:\n##       self._init_from_proto(hparam_def)\n##       if kwargs:\n##         raise ValueError(\'hparam_def and initialization values are \'\n##                          \'mutually exclusive\')\n      raise ValueError(\'hparam_def has been disabled in this version\')\n    else:\n      for name, value in six.iteritems(kwargs):\n        self.add_hparam(name, value)\n\n##   def _init_from_proto(self, hparam_def):\n##     """"""Creates a new HParams from `HParamDef` protocol buffer.\n## \n##     Args:\n##       hparam_def: `HParamDef` protocol buffer.\n##     """"""\n##     assert isinstance(hparam_def, hparam_pb2.HParamDef)\n##     for name, value in hparam_def.hparam.items():\n##       kind = value.WhichOneof(\'kind\')\n##       if kind.endswith(\'_value\'):\n##         # Single value.\n##         if kind.startswith(\'int64\'):\n##           # Setting attribute value to be \'int\' to ensure the type is compatible\n##           # with both Python2 and Python3.\n##           self.add_hparam(name, int(getattr(value, kind)))\n##         elif kind.startswith(\'bytes\'):\n##           # Setting attribute value to be \'str\' to ensure the type is compatible\n##           # with both Python2 and Python3. UTF-8 encoding is assumed.\n##           self.add_hparam(name, compat.as_str(getattr(value, kind)))\n##         else:\n##           self.add_hparam(name, getattr(value, kind))\n##       else:\n##         # List of values.\n##         if kind.startswith(\'int64\'):\n##           # Setting attribute value to be \'int\' to ensure the type is compatible\n##           # with both Python2 and Python3.\n##           self.add_hparam(name, [int(v) for v in getattr(value, kind).value])\n##         elif kind.startswith(\'bytes\'):\n##           # Setting attribute value to be \'str\' to ensure the type is compatible\n##           # with both Python2 and Python3. UTF-8 encoding is assumed.\n##           self.add_hparam(\n##               name, [compat.as_str(v) for v in getattr(value, kind).value])\n##         else:\n##           self.add_hparam(name, [v for v in getattr(value, kind).value])\n\n  def add_hparam(self, name, value):\n    """"""Adds {name, value} pair to hyperparameters.\n\n    Args:\n      name: Name of the hyperparameter.\n      value: Value of the hyperparameter. Can be one of the following types:\n        int, float, string, int list, float list, or string list.\n\n    Raises:\n      ValueError: if one of the arguments is invalid.\n    """"""\n    # Keys in kwargs are unique, but \'name\' could the name of a pre-existing\n    # attribute of this object.  In that case we refuse to use it as a\n    # hyperparameter name.\n    if getattr(self, name, None) is not None:\n      raise ValueError(\'Hyperparameter name is reserved: %s\' % name)\n    if isinstance(value, (list, tuple)):\n      if not value:\n        raise ValueError(\n            \'Multi-valued hyperparameters cannot be empty: %s\' % name)\n      self._hparam_types[name] = (type(value[0]), True)\n    else:\n      self._hparam_types[name] = (type(value), False)\n    setattr(self, name, value)\n\n  def set_hparam(self, name, value):\n    """"""Set the value of an existing hyperparameter.\n\n    This function verifies that the type of the value matches the type of the\n    existing hyperparameter.\n\n    Args:\n      name: Name of the hyperparameter.\n      value: New value of the hyperparameter.\n\n    Raises:\n      ValueError: If there is a type mismatch.\n    """"""\n    param_type, is_list = self._hparam_types[name]\n    if isinstance(value, list):\n      if not is_list:\n        raise ValueError(\n            \'Must not pass a list for single-valued parameter: %s\' % name)\n      setattr(self, name, [\n          _cast_to_type_if_compatible(name, param_type, v) for v in value])\n    else:\n      if is_list:\n        raise ValueError(\n            \'Must pass a list for multi-valued parameter: %s.\' % name)\n      setattr(self, name, _cast_to_type_if_compatible(name, param_type, value))\n\n  def del_hparam(self, name):\n    """"""Removes the hyperparameter with key \'name\'.\n\n    Args:\n      name: Name of the hyperparameter.\n    """"""\n    if hasattr(self, name):\n      delattr(self, name)\n      del self._hparam_types[name]\n\n  def parse(self, values):\n    """"""Override hyperparameter values, parsing new values from a string.\n\n    See parse_values for more detail on the allowed format for values.\n\n    Args:\n      values: String.  Comma separated list of `name=value` pairs where\n        \'value\' must follow the syntax described above.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values` cannot be parsed.\n    """"""\n    type_map = dict()\n    for name, t in self._hparam_types.items():\n      param_type, _ = t\n      type_map[name] = param_type\n\n    values_map = parse_values(values, type_map)\n    return self.override_from_dict(values_map)\n\n  def override_from_dict(self, values_dict):\n    """"""Override hyperparameter values, parsing new values from a dictionary.\n\n    Args:\n      values_dict: Dictionary of name:value pairs.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values_dict` cannot be parsed.\n    """"""\n    for name, value in values_dict.items():\n      self.set_hparam(name, value)\n    return self\n\n##   @deprecation.deprecated(None, \'Use `override_from_dict`.\')\n  def set_from_map(self, values_map):\n    """"""DEPRECATED. Use override_from_dict.""""""\n    return self.override_from_dict(values_dict=values_map)\n\n  def set_model_structure(self, model_structure):\n    self._model_structure = model_structure\n\n  def get_model_structure(self):\n    return self._model_structure\n\n  def to_json(self, indent=None, separators=None, sort_keys=False):\n    """"""Serializes the hyperparameters into JSON.\n\n    Args:\n      indent: If a non-negative integer, JSON array elements and object members\n        will be pretty-printed with that indent level. An indent level of 0, or\n        negative, will only insert newlines. `None` (the default) selects the\n        most compact representation.\n      separators: Optional `(item_separator, key_separator)` tuple. Default is\n        `(\', \', \': \')`.\n      sort_keys: If `True`, the output dictionaries will be sorted by key.\n\n    Returns:\n      A JSON string.\n    """"""\n    return json.dumps(\n        self.values(),\n        indent=indent,\n        separators=separators,\n        sort_keys=sort_keys)\n\n  def parse_json(self, values_json):\n    """"""Override hyperparameter values, parsing new values from a json object.\n\n    Args:\n      values_json: String containing a json object of name:value pairs.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values_json` cannot be parsed.\n    """"""\n    values_map = json.loads(values_json)\n    return self.override_from_dict(values_map)\n\n  def values(self):\n    """"""Return the hyperparameter values as a Python dictionary.\n\n    Returns:\n      A dictionary with hyperparameter names as keys.  The values are the\n      hyperparameter values.\n    """"""\n    return {n: getattr(self, n) for n in self._hparam_types.keys()}\n\n  def get(self, key, default=None):\n    """"""Returns the value of `key` if it exists, else `default`.""""""\n    if key in self._hparam_types:\n      # Ensure that default is compatible with the parameter type.\n      if default is not None:\n        param_type, is_param_list = self._hparam_types[key]\n        type_str = \'list<%s>\' % param_type if is_param_list else str(param_type)\n        fail_msg = (""Hparam \'%s\' of type \'%s\' is incompatible with ""\n                    \'default=%s\' % (key, type_str, default))\n\n        is_default_list = isinstance(default, list)\n        if is_param_list != is_default_list:\n          raise ValueError(fail_msg)\n\n        try:\n          if is_default_list:\n            for value in default:\n              _cast_to_type_if_compatible(key, param_type, value)\n          else:\n            _cast_to_type_if_compatible(key, param_type, default)\n        except ValueError as e:\n          raise ValueError(\'%s. %s\' % (fail_msg, e))\n\n      return getattr(self, key)\n\n    return default\n\n  def __contains__(self, key):\n    return key in self._hparam_types\n\n  def __str__(self):\n    return str(sorted(self.values().items()))\n\n  def __repr__(self):\n    return \'%s(%s)\' % (type(self).__name__, self.__str__())\n\n  @staticmethod\n  def _get_kind_name(param_type, is_list):\n    """"""Returns the field name given parameter type and is_list.\n\n    Args:\n      param_type: Data type of the hparam.\n      is_list: Whether this is a list.\n\n    Returns:\n      A string representation of the field name.\n\n    Raises:\n      ValueError: If parameter type is not recognized.\n    """"""\n    if issubclass(param_type, bool):\n      # This check must happen before issubclass(param_type, six.integer_types),\n      # since Python considers bool to be a subclass of int.\n      typename = \'bool\'\n    elif issubclass(param_type, six.integer_types):\n      # Setting \'int\' and \'long\' types to be \'int64\' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = \'int64\'\n    elif issubclass(param_type, (six.string_types, six.binary_type)):\n      # Setting \'string\' and \'bytes\' types to be \'bytes\' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = \'bytes\'\n    elif issubclass(param_type, float):\n      typename = \'float\'\n    else:\n      raise ValueError(\'Unsupported parameter type: %s\' % str(param_type))\n\n    suffix = \'list\' if is_list else \'value\'\n    return \'_\'.join([typename, suffix])\n\n##   def to_proto(self, export_scope=None):  # pylint: disable=unused-argument\n##     """"""Converts a `HParams` object to a `HParamDef` protocol buffer.\n## \n##     Args:\n##       export_scope: Optional `string`. Name scope to remove.\n## \n##     Returns:\n##       A `HParamDef` protocol buffer.\n##     """"""\n##     hparam_proto = hparam_pb2.HParamDef()\n##     for name in self._hparam_types:\n##       # Parse the values.\n##       param_type, is_list = self._hparam_types.get(name, (None, None))\n##       kind = HParams._get_kind_name(param_type, is_list)\n## \n##       if is_list:\n##         if kind.startswith(\'bytes\'):\n##           v_list = [compat.as_bytes(v) for v in getattr(self, name)]\n##         else:\n##           v_list = [v for v in getattr(self, name)]\n##         getattr(hparam_proto.hparam[name], kind).value.extend(v_list)\n##       else:\n##         v = getattr(self, name)\n##         if kind.startswith(\'bytes\'):\n##           v = compat.as_bytes(getattr(self, name))\n##         setattr(hparam_proto.hparam[name], kind, v)\n## \n##     return hparam_proto\n\n##   @staticmethod\n##   def from_proto(hparam_def, import_scope=None):  # pylint: disable=unused-argument\n##     return HParams(hparam_def=hparam_def)\n\n\n## ops.register_proto_function(\n##     \'hparams\',\n##     proto_type=hparam_pb2.HParamDef,\n##     to_proto=HParams.to_proto,\n##     from_proto=HParams.from_proto)\n'"
