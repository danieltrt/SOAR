file_path,api_count,code
train.py,15,"b'#!/usr/bin/env python3\nimport argparse\nimport logging\nfrom os.path import dirname, abspath, join, isfile\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport training.optim as optimz\nfrom training.summary_utils import SummaryMaker\nfrom training import train_utils\nfrom training.datasets import ImageNetVID, ImageNetVID_val\nfrom training.labels import create_BCELogit_loss_label\nimport training.models as mdl\nimport training.losses as losses\nimport training.metrics as met\nfrom training.train_utils import RunningAverage\nfrom utils.profiling import Timer\nfrom utils.exceptions import IncompleteArgument\nimport utils.image_utils as imutils\n\ndevice = torch.device(""cuda"") if torch.cuda.is_available() \\\n    else torch.device(""cpu"")\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description=""Training script"")\n    parser.add_argument(\'-m\', \'--mode\', default=\'train\', choices=[\'train\', \'eval\'],\n                        help=""The mode of execution of the script. Options are ""\n                        ""\'train\' to train a model, and \'eval\' to evaluate a model ""\n                        ""on the ImageNet eval dataset."")\n    parser.add_argument(\'-d\', \'--data_dir\', default=\'/home/ml2/workspace_rafael/dummy_Imagenet\',\n                        help=""Full path to the directory containing the dataset"")\n    parser.add_argument(\'-e\', \'--exp_name\', default=\'default\',\n                        help=""The name of the experiment folder that contains the ""\n                             ""parameters, checkpoints and logs. Must be in ""\n                             ""training/experiments"")\n    parser.add_argument(\'-r\', \'--restore_file\', default=None,\n                        help=""Optional, name of file to restore from (without its""\n                             ""extension .pth.tar)"")\n    parser.add_argument(""-t"", ""--timer"", action=""store_true"", dest=""timer"",\n                        default=False, help=""Writes the elapsed time for some ""\n                                            ""sections of code on the log"")\n    parser.add_argument(""-j"", ""--num_workers"", dest=""num_workers"", type=int,\n                        default=4, help=""The number of workers for the dataloaders""\n                                        "" i.e. the number of additional""\n                                        "" dedicated threads to dataloading."")\n    parser.add_argument(\'-f\', \'--imutils_flag\', default=\'fast\', type=str,\n                        choices=imutils.VALID_FLAGS,\n                        help=""Optional, the flag of the image_utils defining ""\n                        ""the image processing tools."")\n    parser.add_argument(\'-s\', \'--summary_samples\', default=5, type=int,\n                        help=""Optional, the number of pairs the TensorboardX ""\n                        ""samples during validation to write in the summary. ""\n                        ""For each epoch it saves the ref and the search ""\n                        ""embeddings as well as the final correlation map."")\n    args = parser.parse_args()\n    return args\n\n\ndef main(args):\n    root_dir = dirname(abspath(__file__))\n    # Load the parameters from json file\n    imagenet_dir = args.data_dir\n    exp_dir = join(root_dir, \'training\', \'experiments\', args.exp_name)\n    json_path = join(exp_dir, \'parameters.json\')\n    assert isfile(json_path), (""No json configuration file found at {}""\n                               .format(json_path))\n    params = train_utils.Params(json_path)\n    # Add the timer option to the parameters\n    params.update_with_dict({\'timer\': args.timer})\n    params.update_with_dict({\'num_workers\': args.num_workers})\n\n    train_utils.set_logger(join(exp_dir, \'{}.log\'.format(args.mode)))\n    logging.info(""----Starting train script in mode: {}----"".format(args.mode))\n\n    setup_timer = Timer(convert=True)\n    setup_timer.reset()\n    logging.info(""Loading datasets..."")\n\n    # Get the correct model\n    if params.model == \'BaselineEmbeddingNet\':\n        model = mdl.SiameseNet(mdl.BaselineEmbeddingNet(), upscale=params.upscale,\n                               corr_map_size=33, stride=4)\n    elif params.model == \'VGG11EmbeddingNet_5c\':\n        model = mdl.SiameseNet(mdl.VGG11EmbeddingNet_5c(), upscale=params.upscale,\n                               corr_map_size=33, stride=4)\n    elif params.model == \'VGG16EmbeddingNet_8c\':\n        model = mdl.SiameseNet(mdl.VGG16EmbeddingNet_8c(), upscale=params.upscale,\n                               corr_map_size=33, stride=4)\n\n    # Freeze all the indicated parameters\n    for i, (name, parameter) in enumerate(model.named_parameters()):\n        if i in params.parameter_freeze:\n            logging.info(""Freezing parameter {}"".format(name))\n            parameter.requires_grad = False\n\n    model = model.to(device)\n    # Set the tensorboard summary maker\n    summ_maker = SummaryMaker(join(exp_dir, \'tensorboard\'),\n                              params,\n                              model.upscale_factor)\n\n    label_function = create_BCELogit_loss_label\n    img_read_fcn = imutils.get_decode_jpeg_fcn(flag=args.imutils_flag)\n    img_resize_fcn = imutils.get_resize_fcn(flag=args.imutils_flag)\n\n    logging.info(""Validation dataset..."")\n\n    metadata_val_file = join(exp_dir, ""metadata.val"")\n    val_set = ImageNetVID_val(imagenet_dir,\n                              label_fcn=label_function,\n                              pos_thr=params.pos_thr,\n                              neg_thr=params.neg_thr,\n                              upscale_factor=model.upscale_factor,\n                              cxt_margin=params.context_margin,\n                              reference_size=params.reference_sz,\n                              search_size=params.search_sz,\n                              img_read_fcn=img_read_fcn,\n                              resize_fcn=img_resize_fcn,\n                              metadata_file=metadata_val_file,\n                              save_metadata=metadata_val_file,\n                              max_frame_sep=params.max_frame_sep)\n    val_loader = DataLoader(val_set, batch_size=params.batch_size,\n                            shuffle=False, num_workers=params.num_workers,\n                            pin_memory=True)\n    if params.eval_epoch_size > len(val_loader):\n        logging.info(\'The user set eval_epoch_size ({}) is bigger than the \'\n                     \'size of the eval set ({}). \\n Setting \'\n                     \'eval_epoch_size to the eval set size.\'\n                     .format(params.eval_epoch_size, len(val_loader)))\n        params.eval_epoch_size = len(val_loader)\n\n    # Define the model and optimizer\n\n    # fetch loss function and metrics\n    loss_fn = losses.BCELogit_Loss\n    metrics = met.METRICS\n    # Set the optional keyword arguments for the functions that need it\n    metrics[\'center_error\'][\'kwargs\'][\'upscale_factor\'] = model.upscale_factor\n\n    try:\n        if args.mode == \'train\':\n\n            logging.info(""Training dataset..."")\n\n            metadata_train_file = join(exp_dir, ""metadata.train"")\n            train_set = ImageNetVID(imagenet_dir,\n                                    label_fcn=label_function,\n                                    pos_thr=params.pos_thr,\n                                    neg_thr=params.neg_thr,\n                                    upscale_factor=model.upscale_factor,\n                                    cxt_margin=params.context_margin,\n                                    reference_size=params.reference_sz,\n                                    search_size=params.search_sz,\n                                    img_read_fcn=img_read_fcn,\n                                    resize_fcn=img_resize_fcn,\n                                    metadata_file=metadata_train_file,\n                                    save_metadata=metadata_train_file,\n                                    max_frame_sep=params.max_frame_sep)\n            train_loader = DataLoader(train_set, batch_size=params.batch_size,\n                                      shuffle=True, num_workers=params.num_workers,\n                                      pin_memory=True)\n\n            # Though I\'m not a big fan of changing the value of a parameter\n            # variable after it has been read, at least I let the user know I\'m\n            # changing it.\n            if params.train_epoch_size > len(train_loader):\n                logging.info(\'The user set train_epoch_size ({}) is bigger than the \'\n                             \'size of the train set ({}). \\n Setting \'\n                             \'train_epoch_size to the train set size.\'\n                             .format(params.train_epoch_size, len(train_loader)))\n                params.train_epoch_size = len(train_loader)\n\n            logging.info(""Done"")\n            logging.info(""Setup time: {}"".format(setup_timer.elapsed))\n            parameters = filter(lambda p: p.requires_grad,model.parameters())\n            optimizer = optimz.OPTIMIZERS[params.optim](parameters, **params.optim_kwargs)\n            # Set the scheduler, that updates the learning rate using a exponential\n            # decay. If you don\'t want lr decay set it to 1.\n            logging.info(""Using Exponential Learning Rate Decay of {}"".format(params.lr_decay))\n            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, params.lr_decay)\n\n            logging.info(""Epoch sizes: {} in train and {} in eval""\n                         .format(params.train_epoch_size, params.eval_epoch_size))\n\n            logging.info(""Starting training for {} epoch(s)"".format(params.num_epochs))\n            with Timer(convert=True) as t:\n                train_and_evaluate(model, train_loader, val_loader, optimizer,\n                                   scheduler, loss_fn, metrics, params, exp_dir,\n                                   args, summ_maker=summ_maker)\n            if params.timer:\n                logging.info(""[profiling] Total time to train {} epochs, with {}""\n                             "" elements on training dataset and {} ""\n                             ""on val dataset: {}""\n                             .format(params.num_epochs, len(train_loader),\n                                     len(val_loader), t.elapsed))\n\n        elif args.mode == \'eval\':\n            logging.info(""Done"")\n            with Timer(convert=True) as total:\n                logging.info(""Starting evaluation"")\n                # TODO write a decent Exception\n                if args.restore_file is None:\n                    raise IncompleteArgument(""In eval mode you have to specify""\n                                             "" a model checkpoint to be loaded""\n                                             "" and evaluated.""\n                                             "" E.g: --restore_file best"")\n                checkpoint_path = join(exp_dir, args.restore_file + \'.pth.tar\')\n                train_utils.load_checkpoint(checkpoint_path, model)\n                # Evaluate\n                summ_maker.epoch = 0\n                test_metrics = evaluate(model, loss_fn, val_loader, metrics,\n                                        params, args, summ_maker=summ_maker)\n                save_path = join(exp_dir,\n                                 ""metrics_test_{}.json"".format(args.restore_file))\n                train_utils.save_dict_to_json(test_metrics, save_path)\n            if params.timer:\n                logging.info(""[profiling] Total evaluation time: {}""\n                             .format(total.elapsed))\n\n    except KeyboardInterrupt:\n        logging.info(""=== User interrupted execution ==="")\n        raise\n    except Exception as e:\n        logging.exception(""Fatal error in main loop"")\n        logging.info(""=== Execution Terminated with error ==="")\n    else:\n        logging.info(""=== Execution exited normally ==="")\n\n\ndef train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, scheduler,\n                       loss_fn, metrics, params, exp_dir, args, summ_maker=None):\n    """"""Train the model and evaluate every epoch.\n    Args:\n        model: (torch.nn.Module) the neural network\n        train_dataloader: (DataLoader) a torch.utils.data.DataLoader object\n            that fetches training data\n        val_dataloader: (DataLoader) a torch.utils.data.DataLoader object that\n            fetches validation data\n        optimizer: (torch.optim) optimizer for parameters of model\n        scheduler: (torch.optim.lr_scheduler.ExponentialLR) The exponential\n            learning rate scheduler.\n        loss_fn: a function that takes batch_output and batch_labels and\n            computes the loss for the batch\n        metrics: (dict) a dictionary of functions that compute a metric using\n            the output and labels of each batch\n        params: (Params) hyperparameters\n        exp_dir: (string) directory containing the parameters, weights and\n            logs for the current experiment. The full path.\n        args: The parser object containing the user informed arguments\n        summ_maker: The SummaryMaker object that writes the training information\n        to a tensorboard-readable file.\n    """"""\n    # reload weights from restore_file if specified\n    # TODO load and set best validation error\n    if args.restore_file is not None:\n        restore_path = join(exp_dir, (args.restore_file + \'.pth.tar\'))\n        logging.info(""Restoring parameters from {}"".format(restore_path))\n        train_utils.load_checkpoint(restore_path, model)\n\n    # best_val_c_error = float(""inf"")\n    best_val_auc = 0\n    # Before starting the first epoch do the eval\n    logging.info(\'Pretraining evaluation...\')\n    # Epoch 0 is the validation epoch before the learning starts.\n    summ_maker.epoch = 0\n    val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params, args,\n                           summ_maker=summ_maker)\n\n    for epoch in range(params.num_epochs):\n        # The first epoch after training is 1 not 0\n        summ_maker.epoch = epoch + 1\n        # Run one epoch\n        logging.info(""Epoch {}/{}"".format(epoch + 1, params.num_epochs))\n\n        # compute number of batches in one epoch (one full pass over the training set)\n        train(model, optimizer, loss_fn, train_dataloader, metrics, params,\n              summ_maker=summ_maker)\n\n        # Update the Learning rate\n        scheduler.step()\n\n        # Evaluate for one epoch on validation set\n        val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params,\n                               args, summ_maker=summ_maker)\n\n        val_auc = val_metrics[\'AUC\']\n        is_best = val_auc >= best_val_auc\n\n        # Save weights\n        train_utils.save_checkpoint({\'epoch\': epoch + 1,\n                                     \'state_dict\': model.state_dict(),\n                                     \'optim_dict\': optimizer.state_dict()},\n                                    is_best=is_best,\n                                    checkpoint=exp_dir)\n\n        # If best_eval, best_save_path\n        if is_best:\n            logging.info(""- Found new best auc"")\n            best_val_auc = val_auc\n\n            # Save best val metrics in a json file in the model directory\n            best_json_path = join(exp_dir, ""metrics_val_best_weights.json"")\n            train_utils.save_dict_to_json(val_metrics, best_json_path)\n            pass\n\n        # Save latest val metrics in a json file in the model directory\n        last_json_path = join(exp_dir, ""metrics_val_last_weights.json"")\n        train_utils.save_dict_to_json(val_metrics, last_json_path)\n\n\ndef train(model, optimizer, loss_fn, dataloader, metrics, params,\n          summ_maker=None):\n    """"""Train the model\n    Args:\n        model: (torch.nn.Module) the neural network\n        optimizer: (torch.optim) optimizer for parameters of model\n        loss_fn: a function that takes batch_output and batch_labels and\n            computes the loss for the batch\n        dataloader: (DataLoader) a torch.utils.data.DataLoader object that\n            fetches training data\n        metrics: (dict) a dictionary of functions that compute a metric using\n            the output and labels of each batch\n        params: (Params) hyperparameters\n        summ_maker: The SummaryMaker object that writes the training information\n        to a tensorboard-readable file.\n    """"""\n    # set model to training mode\n    model.train()\n\n    # summary for current training loop and a running average object for loss\n    summ = {metric:RunningAverage() for metric in metrics}\n    loss_avg = RunningAverage()\n    profiled_values = [\'load_data\', \'batch\']\n    profil_summ = {name: RunningAverage() for name in profiled_values}\n    timer = Timer()\n    # Use tqdm for progress bar\n    logging.info(""Training on train set"")\n    with tqdm(total=params.train_epoch_size) as progbar:\n        timer.reset()\n        for i, sample in enumerate(dataloader):\n            ref_img_batch = sample[\'ref_frame\'].to(device)\n            search_batch = sample[\'srch_frame\'].to(device)\n            labels_batch = sample[\'label\'].to(device)\n            # move to GPU if available\n            profil_summ[\'load_data\'].update(timer.elapsed)\n            timer.reset()\n\n            # compute model output and loss\n            output_batch = model(ref_img_batch, search_batch)\n            loss = loss_fn(output_batch, labels_batch)\n\n            # clear previous gradients, compute gradients of all variables wrt loss\n            optimizer.zero_grad()\n            loss.backward()\n\n            # performs updates using calculated gradients\n            optimizer.step()\n\n            # Evaluate summaries only once in a while\n            if i % params.save_summary_steps == 0:\n                # extract data from torch Variable, move to cpu, convert to numpy arrays\n                output_batch = output_batch.detach().cpu().numpy()\n                labels_batch = labels_batch.detach().cpu().numpy()\n\n                # compute all metrics on this batch\n                for (metric_name, metric_dict) in metrics.items():\n                    metric_fcn = metric_dict[\'fcn\']\n                    kwargs = metric_dict[\'kwargs\']\n                    metric_value = metric_fcn(output_batch, labels_batch, **kwargs)\n                    summ[metric_name].update(metric_value)\n\n            # update the average loss\n            loss_avg.update(loss.item())\n            profil_summ[\'batch\'].update(timer.elapsed)\n            progbar.set_postfix(loss=\'{:05.3f}\'.format(loss_avg()))\n            progbar.update()\n            timer.reset()\n\n            if i >= params.train_epoch_size - 1:\n                break\n\n    # compute mean of all metrics in summary\n    metrics_mean = {metric: values() for (metric, values) in summ.items()}\n    metrics_mean[\'loss\'] = loss_avg()\n    if summ_maker:\n        for (m_name, m_value) in metrics_mean.items():\n            summ_maker.add_epochwise_scalar(\'train\', m_name, m_value)\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v) for k, v in metrics_mean.items())\n    logging.info(""- Train metrics: "" + metrics_string)\n    if params.timer:\n        logging.info(""[profiling][train] Mean load_data time: {}"".format(profil_summ[\'load_data\']()))\n        logging.info(""[profiling][train] Mean batch time: {}"".format(profil_summ[\'batch\']()))\n\n\n@torch.no_grad()\ndef evaluate(model, loss_fn, dataloader, metrics, params, args, summ_maker=None):\n    """"""Evaluate the model\n    Args:\n        model: (torch.nn.Module) the neural network\n        loss_fn: a function that takes batch_output and batch_labels and\n            computes the loss for the batch\n        dataloader: (DataLoader) a torch.utils.data.DataLoader object that\n            fetches data\n        metrics: (dict) a dictionary of functions that compute a metric using\n            the output and labels of each batch\n        params: (Params) hyperparameters\n        args: The parser object containing the user informed arguments\n        summ_maker: The SummaryMaker object that writes the training information\n        to a tensorboard-readable file.\n    """"""\n\n    # set model to evaluation mode\n    model.eval()\n\n    # summary for current eval loop\n    summ = []\n    loss_avg = RunningAverage()\n    profiled_values = [\'load_data\', \'batch\', \'metrics\']\n    profil_summ = {name: RunningAverage() for name in profiled_values}\n    timer = Timer()\n    # compute metrics over the dataset\n    logging.info(""Validation on val set"")\n    with tqdm(total=params.eval_epoch_size) as progbar:\n        timer.reset()\n        # The TensorBoardX summary index, used to keep track of the number of\n        # summaries already written.\n        tbx_index = 0\n        for i, sample in enumerate(dataloader):\n            ref_img_batch = sample[\'ref_frame\'].to(device)\n            search_batch = sample[\'srch_frame\'].to(device)\n            labels_batch = sample[\'label\'].to(device)\n            # move to GPU if available\n            profil_summ[\'load_data\'].update(timer.elapsed)\n            timer.reset()\n\n            # compute model output\n            embed_ref = model.get_embedding(ref_img_batch)\n            embed_srch = model.get_embedding(search_batch)\n            output_batch = model.match_corr(embed_ref, embed_srch)\n\n            loss = loss_fn(output_batch, labels_batch)\n            # Make a TensorBoardX summary for the number of pairs informed by\n            # user in args.summary_samples. It takes the first n pairs, so it\n            # it is guaranteed to save the results for the same pairs in each\n            # execution, independently on the batch size.\n            if (tbx_index < args.summary_samples) and (summ_maker is not None):\n                # The batch_index selects an element of the batch. We get the\n                # batch size every time instead of using the user informed batch\n                # size to make sure no out of bounds exception raised for\n                # the last batch which might contain less elements.\n                batch_index = 0\n                batch_size = embed_ref.shape[0]\n                while (tbx_index < args.summary_samples) and (batch_index < batch_size):\n                    # Since the val dataloader does not shuffle, we can use the\n                    # tbx_index to get the information about the pairs in the\n                    # list_pairs metadata.\n                    seq, first_frame, second_frame = dataloader.dataset.list_pairs[tbx_index]\n                    seq_name = dataloader.dataset.get_seq_name(seq)\n                    index_string = ""{}_{}_{}"".format(tbx_index,\n                                                     seq_name,\n                                                     first_frame)\n\n                    summ_maker.add_overlay(""Ref_image_{}"".format(index_string),\n                                           embed_ref[batch_index],\n                                           ref_img_batch[batch_index],\n                                           cmap=\'inferno\')\n                    summ_maker.add_overlay(""Search_image_{}"".format(index_string),\n                                           embed_srch[batch_index],\n                                           search_batch[batch_index],\n                                           cmap=\'inferno\')\n                    summ_maker.add_overlay(""Correlation_map_{}-{}"".format(index_string,\n                                                                          second_frame),\n                                           output_batch[batch_index],\n                                           search_batch[batch_index],\n                                           cmap=\'inferno\',\n                                           add_ref=ref_img_batch[batch_index])\n                    logging.info(""Saving embeddings for summary {}"".format(tbx_index))\n                    tbx_index += 1\n                    batch_index += 1\n\n            # extract data from torch Variable, move to cpu, convert to numpy arrays\n            output_batch = output_batch.cpu().numpy()\n            labels_batch = labels_batch.cpu().numpy()\n\n            profil_summ[\'batch\'].update(timer.elapsed)\n            timer.reset()\n\n            # compute all metrics on this batch\n            summary_batch = {metric_name: metric_dict[\'fcn\'](output_batch,\n                                                             labels_batch,\n                                                             **(metric_dict[\'kwargs\']))\n                             for metric_name, metric_dict in metrics.items()}\n            summary_batch[\'loss\'] = loss.item()\n            loss_avg.update(loss.item())\n            summ.append(summary_batch)\n            profil_summ[\'metrics\'].update(timer.elapsed)\n            progbar.set_postfix(loss=\'{:05.3f}\'.format(loss_avg()))\n            progbar.update()\n            timer.reset()\n\n            if i >= params.eval_epoch_size - 1:\n                break\n\n    # compute mean of all metrics in summary\n    metrics_mean = {metric: np.mean([x[metric] for x in summ])\n                    for metric in summ[0]}\n    if summ_maker:\n        for (m_name, m_value) in metrics_mean.items():\n            summ_maker.add_epochwise_scalar(\'val\', m_name, m_value)\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v)\n                                for k, v in metrics_mean.items())\n    logging.info(""- Eval metrics : "" + metrics_string)\n    if params.timer:\n        logging.info(""[profiling][eval] Mean load_data time: {}"".format(profil_summ[\'load_data\']()))\n        logging.info(""[profiling][eval] Mean batch time: {}"".format(profil_summ[\'batch\']()))\n        logging.info(""[profiling][eval] Mean metrics computation time: {}"".format(profil_summ[\'metrics\']()))\n    return metrics_mean\n\n\nif __name__ == \'__main__\':\n    args = parse_arguments()\n    main(args)\n'"
vis_app.py,0,"b'""""""\nTo execute the app first install pyqtgraph, then execute main.py\nwith the following flags:\n   -d: The full path to the dataset folder, the root of the imagenet\nfolder structure.\n   -n: The path to the network to be used, that is, the .pth.tar\nfile containing the networks weights.\n   -t: The type of subset of the dataset, that is, \'train\' or \'val\'\n   -s: The sequence you want to visualize, ranging from 0-3861 for\n\'train\' and 0-554 for \'val\'.\n   -f: The maximum framerate in fps. The actual framerate will\ndepend on the processing speed of the computer.\n\nThe app is divided in 4 files:\n   -main.py: The main script, it parses the user\'s flags, initializes\nthe objects, calls the producer thread, which does all the neural\nnetwork processing, and execute the display application in the main\nthread.\n   -display.py: defines the behaviour of the display application,\non what to display the images, how to organize the windows, how to\ndisplay the information, etc.\n   -producer.py: The app is organized in a threaded producer-consumer\nparadigm, so this file defines the behaviour of the producer thread,\nwhich reads the data from the file system and processes it through\nthe network, then feeding the results to the consumer thread (the\nmain thread) using a buffer queue.\n   -app_utils.py: Defines general functions used in the app.\n""""""\n\nimport argparse\nfrom queue import Queue\n\nfrom PyQt5 import QtGui, QtCore\nimport pyqtgraph as pg\n\nfrom appSiamFC.display import MainUI\nfrom appSiamFC.producer import ProducerThread\n\n\npg.setConfigOptions(imageAxisOrder=\'row-major\')\n\nIMAGENET = \'ILSVRC2015\'\nDEFAULT_MODEL = \'best.pth.tar\'\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description=""SiamFC app"")\n    parser.add_argument(\'-f\', \'--fps\', default=25, dest=""fps"", type=int,\n                        help=""The frame per second rate you wish to display the""\n                             ""video. The true fps rate is displayed FYI."")\n    parser.add_argument(\'-d\', \'--data_dir\', default=IMAGENET,\n                        help=""Full path to the directory containing the dataset"")\n    parser.add_argument(\'-s\', \'--seq\', default=0, dest=""seq"", type=int,\n                        help=""The number of the sequence to be displayed.""\n                             ""according to alphabetic order"")\n    parser.add_argument(\'-t\', \'--type\', default=\'train\', choices=[\'train\', \'val\'],\n                        help=""The subset of the Imagenet, can be \'train\' or""\n                        ""\'val\'"")\n    parser.add_argument(\'-n\', \'--net\', default=DEFAULT_MODEL,\n                        help=""Full path the .pth.tar file containing the network\'s""\n                             "" weights"")\n    parser.add_argument(\'-p\', \'--prior_width\', default=None, type=int,\n                        help=""The standard deviation of the gaussian displacement""\n                        ""prior probability that will be applied to the score map,""\n                        ""centered in the last peak position. The value is given""\n                        ""in pixels, so the given width corresponds to a contour""\n                        ""of approximately 0.6 probability. 63 pixels is an overall""\n                        ""good value, as it represents half of the initial ref""\n                        ""context_region"")\n    parser.add_argument(\'-e\', \'--exit_on_end\', action=\'store_true\', default=False,\n                        help=""When True this flag exits the program once it has""\n                             ""finished displaying the frames."")\n    parser.add_argument(\'-b\', \'--branch\', default=\'alexnet\',\n                        choices=[\'alexnet\', \'vgg11_5c\', \'vgg16_8c\'],\n                        help=""The branch architecture of the siamese net. Should""\n                        ""correspond to the informed net."")\n    parser.add_argument(\'-c\', \'--ctx_mode\', default=\'max\', choices=[\'max\', \'mean\'],\n                        help=""The strategy used to define the context region around""\n                        ""the target, using the bounding box dimensions. The \'max\'""\n                        ""mode uses the biggest dimension, while the \'mean\' mode""\n                        ""uses the mean of the dimensions."")\n    parser.add_argument(\'-a\', \'--alpha\', default=0.7, type=restricted_float,\n                        help=""The alpha value is the proportion of intensity ""\n                        ""of the score map in the mixture with the frame pixels. ""\n                        ""Set alpha to 1 to have only the score map intensity ""\n                        ""displayed, and 0 to have only the sequence\'s frames ""\n                        ""displayed. Alpha must be between 0 and 1."")\n    \n    args = parser.parse_args()\n    return args\n\n\ndef restricted_float(x):\n    x = float(x)\n    if x < 0.0 or x > 1.0:\n        raise argparse.ArgumentTypeError(""%r not in range [0.0, 1.0]""%(x,))\n    return x\n\n\ndef update():\n    if display.alive:\n        display.update()\n        app.processEvents()  # force complete redraw for every plot\n    else:\n        pass\n\n\nif __name__ == \'__main__\':\n    args = parse_arguments()\n    BUFFER = Queue(maxsize=32)\n    # Always start by initializing Qt (only once per application)\n    app = QtGui.QApplication([])\n    # win = QtGui.QMainWindow()\n    # win = pg.GraphicsLayoutWidget(show=True, size=(800,800), border=True)\n    win = pg.GraphicsLayoutWidget(border=True)\n\n    display = MainUI(win, BUFFER, disp_prior=args.prior_width,\n                     exit_on_end=args.exit_on_end, alpha=args.alpha)\n    producer = ProducerThread(args.seq, BUFFER, args.data_dir, args.net,\n                              set_type=args.type, branch_arch=args.branch,\n                              ctx_mode=args.ctx_mode)\n    timer = QtCore.QTimer()\n    timer.timeout.connect(update)\n    timer.start(1000/args.fps)\n\n    # Start the Producer Thread\n    producer.start()\n    # Start the application\n    QtGui.QApplication.instance().exec_()\n'"
appSiamFC/app_utils.py,0,"b'import glob\nimport os\nfrom os.path import join, basename, dirname\n\nimport numpy as np\n\nfrom training.train_utils import get_annotations\n\n\ndef get_sequence(seq_num, imagenet_dir, set_type=\'train\'):\n    """"""\n    """"""\n    dir_data = join(imagenet_dir, \'Data\', \'VID\', set_type)\n    dir_annot = join(imagenet_dir, \'Annotations\', \'VID\', set_type)\n    if set_type == \'val\':\n        glob_expression = join(dir_data, \'*\')\n    elif set_type == \'train\':\n        glob_expression = join(dir_data, \'*\', \'*\')\n\n    sequence_path = sorted(glob.glob(glob_expression))[seq_num]\n    frames = []\n    bboxes_norm = []\n    valid_frame = []\n\n    for frame in sorted(os.listdir(sequence_path)):\n        frame_path = join(dir_annot, sequence_path, frame)\n        # Get the relative path to the sequence form the annotations directory\n        if set_type == \'val\':\n            sequence_rel_path = basename(sequence_path)\n        elif set_type == \'train\':\n            # Yes, I\'m aware this line is a monstrosity. \n            sequence_rel_path = join(basename(dirname(sequence_path)), basename(sequence_path))\n        annot, w, h, valid = get_annotations(dir_annot, sequence_rel_path, frame)\n        # The bounding box is written in terms of the x and y coordinate of the\n        # top left corner in proportional terms of the sides of the whole picture.\n        # So to get the pixel values for it one must multiply the x dimensions\n        # 0 and 2 by the width of the image and the y dimensions by the height.\n        # We use this notation because it is invariant to any scaling of the\n        # image.\n        if valid:\n            bbox_norm = [annot[\'xmin\']/w,\n                         annot[\'ymin\']/h,\n                         (annot[\'xmax\']-annot[\'xmin\'])/w,\n                         (annot[\'ymax\']-annot[\'ymin\'])/h]\n        else:\n            bbox_norm = None\n        frames.append(frame_path)\n        bboxes_norm.append(bbox_norm)\n        valid_frame.append(valid)\n        dims = (h, w)\n\n    return frames, bboxes_norm, valid_frame, dims\n\n\ndef make_gaussian_map(dims, center, sig):\n    """"""\n    """"""\n    var = sig**2\n    x = np.arange(dims[1])[None].astype(np.float)\n    y = np.arange(dims[0])[None].astype(np.float).T\n    gauss = np.exp(-(y-center[0])**2/(2*var))*(np.exp(-(x-center[1])**2/(2*var)))\n\n    return gauss\n\n\ndef rgb2gray(rgb):\n    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n    gray = np.stack([gray, gray, gray], axis=2)\n\n    return gray'"
appSiamFC/display.py,0,"b'import sys\n\nimport numpy as np\nfrom PyQt5 import QtGui, QtWidgets\nimport pyqtgraph as pg\nimport pyqtgraph.ptime as ptime\nfrom matplotlib import cm\n\nfrom appSiamFC.app_utils import make_gaussian_map, rgb2gray\n\n\nclass MainUI(object):\n\n    def __init__(self, MainWindow, buffer, disp_prior=None, exit_on_end=False,\n                 alpha=0.7):\n        """"""\n        """"""\n        self.alive = True\n        self.disp_prior = disp_prior\n        self.exit_on_end = exit_on_end\n\n        MainWindow.setWindowTitle(\'SiamFC - Demo\')\n        MainWindow.resize(1000, 900)\n\n        # Define the ViewBoxes for each of the images to be displayed\n        self.score_box = MainWindow.addViewBox(1, 0, colspan=3)\n        self.gt_box = MainWindow.addViewBox(3, 0, colspan=2)\n        self.ref_box = MainWindow.addViewBox(3, 2)\n        self.score_box.invertY(True)  # Images usually have their Y-axis pointing downward\n        self.gt_box.invertY(True)\n        self.ref_box.invertY(True)\n        self.score_box.setAspectLocked(True)\n        self.gt_box.setAspectLocked(True)\n        self.ref_box.setAspectLocked(True)\n\n        self.fpsLabel = pg.LabelItem(justify=\'left\')\n        MainWindow.addItem(self.fpsLabel, 0, 0)\n        self.visibleLabel = MainWindow.addLabel(\'\', 0, 1)\n        self.bufferLabel = MainWindow.addLabel(\'\', 0, 2)\n        self.nameLabel = MainWindow.addLabel(\'\', 4, 0, colspan=3)\n        font = QtGui.QFont()\n        font.setPointSize(4)\n        self.nameLabel.setFont(font)\n\n        self.score_img = pg.ImageItem()\n        self.gt_img = pg.ImageItem()\n        self.ref_img = pg.ImageItem()\n        self.score_box.addItem(self.score_img)\n        self.gt_box.addItem(self.gt_img)\n        self.ref_box.addItem(self.ref_img)\n        # self.view_box.setRange(QtCore.QRectF(0, 0, 512, 512))\n        self.bounding_box = QtWidgets.QGraphicsRectItem()\n        self.bounding_box.setPen(QtGui.QColor(255, 0, 0))\n        self.bounding_box.setParentItem(self.gt_img)\n        self.gt_box.addItem(self.bounding_box)\n        brush = QtGui.QBrush(QtGui.QColor(0, 255, 0))\n        self.peak = pg.GraphItem(size=30, symbol=\'+\', pxMode=True,\n                                 symbolBrush=brush,\n                                 symbolPen=None)\n        self.peak.setParentItem(self.score_img)\n        self.score_box.addItem(self.peak)\n        self.peak_pos = None\n        brush = QtGui.QBrush(QtGui.QColor(0, 0, 255, alpha=0))\n        self.prior_radius = pg.GraphItem(size=0, symbol=\'o\', pxMode=True,\n                                         symbolBrush=brush, symbolPen=\'b\')\n        self.prior_radius.setParentItem(self.score_img)\n        self.score_box.addItem(self.prior_radius)\n\n        # Add the Labels to the images\n        param_dict = {\'color\':(255,255,255),\n                      \'anchor\':(0,1)}\n        label_score = pg.TextItem(text=\'Score Map\', **param_dict)\n        label_gt = pg.TextItem(text=\'Ground Truth\', **param_dict)\n        label_ref = pg.TextItem(text=\'Reference Image\', **param_dict)\n        font.setPointSize(16)\n        label_score.setFont(font)\n        label_gt.setFont(font)\n        label_ref.setFont(font)\n        label_score.setParentItem(self.score_img)\n        label_gt.setParentItem(self.gt_img)\n        label_ref.setParentItem(self.ref_img)\n        self.score_box.addItem(label_score)\n        self.gt_box.addItem(label_gt)\n        self.ref_box.addItem(label_ref)\n\n        # The alpha parameter is used to overlay the score map with the image,\n        # where alpha=1 corresponds to the score_map alone and alpha=0 is\n        # the image alone.\n        self.alpha = alpha\n\n        self.error_plot = MainWindow.addPlot(5, 0, colspan=3,\n                                             title=\'Center Error (pixels)\')\n        self.curve = self.error_plot.plot(pen=\'y\')\n        # Sets a line indicating the 63 pixel error corresponding to half of the\n        # initial reference bounding box, and a possible measure of tracking\n        # failure.\n        half_ref = pg.InfiniteLine(movable=False, angle=0, pen=(0, 0, 200),\n                                   label=\'ctr_error={value:0.2f}px\',\n                                   labelOpts={\'color\': (200,200,200),\n                                              \'movable\': True,\n                                              \'fill\': (0, 0, 200, 100)})\n        half_ref.setPos([63, 63])\n        self.error_plot.addItem(half_ref)\n        self.center_errors = []\n\n        self.index = 0\n\n        MainWindow.show()\n\n        self.lastTime = ptime.time()\n        self.fps = None\n        self.buffer = buffer\n\n    def exit(self):\n        sys.exit()\n\n    def update(self):\n        """"""\n        """"""\n        # Gets an element from the buffer and free the buffer\n        buffer_element = self.buffer.get()\n        self.buffer.task_done()\n        # When the Producer Thread finishes publishing the data it sends a None\n        # through the buffer to sinalize it has finished.\n        if buffer_element is not None:\n            score_map = buffer_element.score_map\n            gt_img = buffer_element.img\n            ref_img = buffer_element.ref_img\n            visible = buffer_element.visible\n            name = buffer_element.name\n            bbox = buffer_element.bbox\n\n            if self.peak_pos is not None and self.disp_prior is not None:\n                h, w = score_map.shape\n                disp_prior = make_gaussian_map((h,w), self.peak_pos, self.disp_prior)\n                np.expand_dims(disp_prior, axis=2)\n                score_map = score_map*disp_prior\n                self.prior_radius.setData(pos=[(self.peak_pos[1], self.peak_pos[0])], size=self.disp_prior)\n\n            # Find peak of the score map. You must incorporate all priors before\n            # taking the max.\n            peak = np.unravel_index(score_map.argmax(), score_map.shape)\n            self.peak_pos = peak\n            # Apply the inferno color map to the score map.\n            # The output of cm.inferno has 4 channels, 3 color channels and a\n            # transparency channel\n            score_img = cm.inferno(score_map)[:, :, 0:3]\n            # Overlay the score_img with a grayscale version of the original\n            # frame\n            img_gray = rgb2gray(gt_img)\n            score_img = score_img[0:img_gray.shape[0], 0:img_gray.shape[1], :]\n            score_img = score_img*self.alpha + (1-self.alpha)*img_gray/255\n\n            vis_color = \'g\' if visible else \'r\'\n            self.score_img.setImage(score_img, autoDownsample=False)\n            # Set the marker in the peak. The pyqtgraph GraphItem takes the\n            # position in terms of the x and y coordinates.\n            self.peak.setData(pos=[(peak[1], peak[0])])\n            self.gt_img.setImage(gt_img, autoDownsample=False)\n            if bbox is not None:\n                self.bounding_box.setRect(*bbox)\n                center_error = np.linalg.norm([bbox[0]+bbox[2]/2-peak[1], bbox[1]+bbox[3]/2-peak[0]])\n                self.center_errors.append(center_error)\n                self.curve.setData(self.center_errors)\n            else:\n                self.bounding_box.setRect(0, 0, 0, 0)\n            self.ref_img.setImage(ref_img, autoDownsample=False)\n            # Calculate the fps rate.\n            now = ptime.time()\n            dt = now - self.lastTime\n            self.lastTime = now\n            if self.fps is None:\n                self.fps = 1.0/dt\n            else:\n                s = np.clip(dt*3., 0, 1)\n                self.fps = self.fps * (1-s) + (1.0/dt) * s\n            self.fpsLabel.setText(\'{:.2f} fps\'.format(self.fps), color=\'w\')\n            self.visibleLabel.setText(\'Visible: {}\'.format(visible), color=vis_color)\n            self.bufferLabel.setText(\'{} in Buffer\'.format(self.buffer.qsize()))\n            self.nameLabel.setText(name, size=\'10pt\', color=\'w\')\n\n            self.index += 1\n        else:\n            # Set alive attribute to False to indicate the end of the program\n            self.alive = False\n            if self.exit_on_end:\n                self.exit()\n'"
appSiamFC/producer.py,6,"b'from math import floor\n\nimport numpy as np\nfrom threading import Thread\nfrom collections import namedtuple\nimport torch\nimport torch.nn.functional as F\nfrom torch import sigmoid\n\nimport training.models as mdl\nfrom appSiamFC.app_utils import get_sequence, make_gaussian_map\nimport utils.image_utils as imutils\nfrom utils.tensor_conv import numpy_to_torch_var, torch_var_to_numpy\n\ndevice = torch.device(""cuda"") if torch.cuda.is_available() \\\n    else torch.device(""cpu"")\nFLAG = \'safe\'\nif imutils.LIBJPEG_TURBO_PRESENT:\n    FLAG = \'fast\'\nimg_read_fcn = imutils.get_decode_jpeg_fcn(flag=FLAG)\nimg_resize_fcn = imutils.get_resize_fcn(flag=\'fast\')\n\nBufferElement = namedtuple(\'BufferElement\', [\'score_map\', \'img\', \'ref_img\',\n                                             \'visible\', \'name\', \'bbox\'])\n\n\nclass ProducerThread(Thread):\n    """"""\n    """"""\n\n    def __init__(self, seq, buffer, dataset_path, model_path, set_type=\'train\',\n                 max_res=800, branch_arch=\'alexnet\', ctx_mode=\'max\'):\n        """"""\n        Args:\n            seq: (int) The number of the sequence according to the get_sequence\n                function, which mirrors the indexing of the ImageNetVID class.\n            buffer: (queue.Queue) The data buffer between the producerThread and\n                the consumer application (the display). The elements stored in\n                this buffer are defined by the BufferElement namedtuple.\n            dataset_path: (string) The path to the root of the ImageNet dataset.\n            model_path: (string) The path to the models .pth.tar file containing\n                the model\'s weights.\n            set_type: (string) The subset of the ImageNet VID dataset, can be\n                \'train\' or \'val\'.\n            max_res: (int) The maximum resolution in pixels. If any dimension\n                of the image exceeds this value, the final image published by\n                the producer is resized (keeping the aspect ratio). Used to\n                balance the load between the consumer (main) thread and the\n                producer.\n            branch_arch: (string) The architecture of the branch of the siamese\n                net. Might be: \'alexnet\', \'vgg11_5c\'.\n            ctx_mode: (string) The strategy used to define the context region\n                around the target, using the bounding box dimensions. The \'max\'\n                mode uses the biggest dimension, while the \'mean\' mode uses the\n                mean of the dimensions.\n        """"""\n        super(ProducerThread, self).__init__(daemon=True)\n        self.frames, self.bboxes_norm, self.valid_frames, self.vid_dims = (\n            get_sequence(seq, dataset_path, set_type=set_type))\n        self.idx = 0\n        self.seq_size = len(self.frames)\n        self.buffer = buffer\n        # TODO put the model info inside the checkpoint file.\n        if branch_arch == \'alexnet\':\n            self.net = mdl.SiameseNet(mdl.BaselineEmbeddingNet(), stride=4)\n        elif branch_arch == \'vgg11_5c\':\n            self.net = mdl.SiameseNet(mdl.VGG11EmbeddingNet_5c(), stride=4)\n        elif branch_arch == ""vgg16_8c"":\n            self.net = mdl.SiameseNet(mdl.VGG16EmbeddingNet_8c(), stride=4)\n        checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n        self.net.load_state_dict(checkpoint[\'state_dict\'])\n        # Tuple of (H, w), the dimensions to which the image will be resized.\n        self.resize_dims = None\n        self.net = self.net.to(device)\n        self.net.eval()\n        self.ref, self.ref_emb = self.make_ref(ctx_mode=ctx_mode)\n\n    @torch.no_grad()\n    def run(self):\n        """""" The main loop of the Thread. It processes sequentially each frame\n        of the specified sequence and publishes the results to the main thread\n        through their shared buffer. When it finishes all the frames it sends\n        a signal to the buffer indicating it is done and waits for the main\n        thread to finish (because daemon=True it dies along with the main thread).\n        """"""\n        while self.idx < self.seq_size:\n            dims = self.vid_dims\n            if self.resize_dims is not None:\n                img = img_read_fcn(self.frames[self.idx])\n                img = img_resize_fcn(img, self.resize_dims, interp=\'bilinear\')\n                dims = self.resize_dims\n            if self.valid_frames[self.idx]:\n                bbox = self.denorm_bbox(self.bboxes_norm[self.idx], dims)\n            else:\n                bbox = None\n            score_map = self.make_score_map(img)\n            data = BufferElement(score_map,\n                                 img,\n                                 self.ref,\n                                 self.valid_frames[self.idx],\n                                 self.frames[self.idx],\n                                 bbox)\n            self.buffer.put(data)\n            self.idx += 1\n        print(""ProducerThread finished publishing the data"")\n        # Publish a None to sinalize to the consumer that the stream has finished\n        self.buffer.put(None)\n\n    def denorm_bbox(self, bbox_norm, img_dims):\n        """""" Denormalizes the bounding box, taking it from its relative values to\n        the pixel values in the full image with dimension img_dims.\n\n        Args:\n            bbox_norm: (list) The normalized bounding boxes, with 4 values that\n                represent respectively, the x and y dimensions of the upper-left\n                corner, and the width and height of the bounding boxes. All values\n                are normalized by the full image\'s dimensions, so they are\n                invariant to resizes of the image.\n            img_dims: (tuple) The dimensions of the current image, in the form\n                (Height, Width).\n        Returns:\n            bbox: (tuple) The bounding box in pixel terms, corresponding to the\n                correct dimensions for an image with the given dimensions.\n        """"""\n        bbox = bbox_norm[:]\n        bbox[0] = int(bbox[0]*img_dims[1])\n        bbox[1] = int(bbox[1]*img_dims[0])\n        bbox[2] = int(floor(bbox[2]*img_dims[1]))\n        bbox[3] = int(floor(bbox[3]*img_dims[0]))\n        return tuple(bbox)\n\n    @torch.no_grad()\n    def make_ref(self, ctx_mode=\'max\'):\n        """""" Extracts the reference image and its embedding.\n\n        Args:\n            ctx_mode: (str) The method used to define the context region around\n                the target, options are [\'max\', \'mean\'], where \'max\' simply takes\n                the largest of the two dimensions of the bounding box and mean\n                takes the mean.\n        """"""\n        # Get the first valid frame index\n        ref_idx = self.valid_frames.index(True)\n        ref_frame = img_read_fcn(self.frames[ref_idx])\n        bbox = self.denorm_bbox(self.bboxes_norm[ref_idx], self.vid_dims)\n        if ctx_mode == \'max\':\n            ctx_size = max(bbox[2], bbox[3])\n        elif ctx_mode == \'mean\':\n            ctx_size = int((bbox[2] + bbox[3])/2)\n        # It resizes the image so that the reference image has dimensions 127x127\n        if ctx_size != 127:\n            new_H = int(self.vid_dims[0]*127/ctx_size)\n            new_W = int(self.vid_dims[1]*127/ctx_size)\n            self.resize_dims = (new_H, new_W)\n            ref_frame = img_resize_fcn(ref_frame, self.resize_dims, interp=\'bilinear\')\n            bbox = self.denorm_bbox(self.bboxes_norm[ref_idx], self.resize_dims)\n            ctx_size = 127\n        # Set image values to the range 0-1 before feeding to the network\n        ref_frame = ref_frame/255\n        ref_center = (int((bbox[1] + bbox[3]/2)), int((bbox[0] + bbox[2]/2)))\n        ref_img = self.extract_ref(ref_frame, ref_center, ctx_size)\n\n        ref_tensor = numpy_to_torch_var(ref_img, device)\n        ref_embed = self.net.get_embedding(ref_tensor)\n\n        return ref_img, ref_embed\n\n    def extract_ref(self, full_img, center, ctx_size, apply_gauss=False, gauss_sig=30):\n        """""" Extracts the reference img from the reference frame by cropping a\n        square region around the center of the bounding box with the given size.\n        If the region exceeds the boundaries of the image, it pads the reference\n        image with the mean value of the image.\n\n        Args:\n            full_img: (numpy.ndarray) The full reference frame.\n            center: (tuple) The (y, x) coordinates of the center of the bounding\n                box.\n            ctx_size: (int) The side of the square region. In the current\n                implementation it should be an odd integer, otherwise it would\n                output an image with an excess of 1 pixel in each dimension.\n\n        Return:\n            ref_img: (numpy.ndarray) The (ctx_size, ctx_size) reference image.\n        """"""\n        H, W, _ = full_img.shape\n        y_min = max(0, center[0]-ctx_size//2)\n        y_max = min(H-1, center[0] + ctx_size//2)\n        x_min = max(0, center[1]-ctx_size//2)\n        x_max = min(W-1, center[1] + ctx_size//2)\n        offset_top = max(0, ctx_size//2 - center[0])\n        offset_bot = max(0, center[0] + ctx_size//2 - H + 1)\n        offset_left = max(0, ctx_size//2 - center[1])\n        offset_right = max(0, center[1] + ctx_size//2 - W + 1)\n        img_mean = full_img.mean()\n        ref_img = np.ones([ctx_size, ctx_size, 3])*img_mean\n        ref_img[offset_top:(ctx_size-offset_bot),\n                offset_left:(ctx_size-offset_right)] = (\n                    full_img[y_min:(y_max+1), x_min:(x_max+1)])\n        if apply_gauss:\n            h, w, _ = ref_img.shape\n            gauss = make_gaussian_map((h, w), (h//2, w//2), sig=gauss_sig)\n            gauss = np.expand_dims(gauss, axis=2)\n            ref_img = ref_img*gauss\n        return ref_img\n\n    def make_score_map(self, img, mode=\'sigmoid\'):\n        """"""\n        """"""\n        img = img/255\n        # The offset is inserted so that the final size of the score map matches\n        # the search image. To know more see ""How to overlay the search img with\n        # the score map"" in Trello/Report. It is half of the dimension of the\n        # Smallest Class Equivalent of the Ref image.\n        offset = (((self.ref.shape[0] + 1)//4)*4 - 1)//2\n        img_mean = img.mean()\n        img_padded = np.pad(img, ((offset, offset), (offset, offset), (0, 0)),\n                            mode=\'constant\', constant_values=img_mean)\n        img_padded = numpy_to_torch_var(img_padded, device)\n        srch_emb = self.net.get_embedding(img_padded)\n        score_map = self.net.match_corr(self.ref_emb, srch_emb)\n        dimx = score_map.shape[-1]\n        dimy = score_map.shape[-2]\n        score_map = score_map.view(-1, dimy, dimx)\n        if mode == \'sigmoid\':\n            score_map = sigmoid(score_map)\n        elif mode == \'norm\':\n            score_map = score_map - score_map.min()\n            score_map = score_map/score_map.max()\n        score_map = score_map.unsqueeze(0)\n        # We upscale 4 times, because the total stride of the network is 4\n        score_map = F.interpolate(score_map, scale_factor=4, mode=\'bilinear\',\n                                  align_corners=False)\n\n        score_map = score_map.cpu()\n        score_map = torch_var_to_numpy(score_map)\n\n        return score_map\n'"
training/crops_train.py,0,"b'from math import floor\n\nimport numpy as np\nfrom scipy.misc import imresize\n\n# REMEMBER, numpy\'s image notation is always (Y,X,CH)\n\n\ndef Pads():\n    """""" Factory function to generate pads dictionaries with the placeholders for\n    the amount of padding in each direction of an image. Note: the \'up\' direction\n    is up in relation to the displayed image, thus it represent the negative\n    direction of the y indices.\n\n    Return:\n        pads: (dictionary) The padding in each direction.\n    """"""\n    pads = {\'up\': 0, \'down\': 0, \'left\': 0, \'right\': 0}\n    return pads\n\n\ndef crop_img(img, cy, cx, reg_s):\n    """""" Crops a an image given its center and the side of the crop region.\n    If the crop exceeds the size of the image, it calculates how much padding\n    it should have in each one of the four sides. In the case that the bounding\n    box has an even number of pixels in either dimension we approach the center\n    of the bounding box as the floor in each dimension, which may displace the\n    actual center half a pixel in each dimension. This way we always deal with\n    bounding boxes of odd size in each dimension. Furthermore, we add the same\n    amount of context region in every direction, which makes the effective\n    region size always an odd integer as well, so to enforce this approach we\n    require the region size to be an odd integer.\n\n\n    Args:\n        img: (numpy.ndarray) The image to be cropped. Must be of dimensions\n            [height, width, channels]\n        cy: (int) The y coordinate of the center of the target.\n        cx: (int) The x coordinate of the center of the target.\n        reg_s: The side of the square region around the target, in pixels. Must\n            be an odd integer.\n\n    Returns:\n        cropped_img: (numpy.ndarray) The cropped image.\n        pads: (dictionary) A dictionary with the amount of padding in pixels in\n            each side. The order is: up, down, left, right, or it can be\n            accessed by their names, e.g.: pads[\'up\']\n    """"""\n    assert reg_s % 2 != 0, ""The region side must be an odd integer.""\n    pads = Pads()\n    h, w, _ = img.shape\n    context = (reg_s-1)/2  # The amount added in each direction\n    xcrop_min = int(floor(cx) - context)\n    xcrop_max = int(floor(cx) + context)\n    ycrop_min = int(floor(cy) - context)\n    ycrop_max = int(floor(cy) + context)\n    # Check if any of the corners exceeds the boundaries of the image.\n    if xcrop_min < 0:\n        pads[\'left\'] = -(xcrop_min)\n        xcrop_min = 0\n    if ycrop_min < 0:\n        pads[\'up\'] = -(ycrop_min)\n        ycrop_min = 0\n    if xcrop_max >= w:\n        pads[\'right\'] = xcrop_max - w + 1\n        xcrop_max = w - 1\n    if ycrop_max >= h:\n        pads[\'down\'] = ycrop_max - h + 1\n        ycrop_max = h - 1\n    cropped_img = img[ycrop_min:(ycrop_max+1), xcrop_min:(xcrop_max+1)]\n\n    return cropped_img, pads\n\n\ndef resize_and_pad(cropped_img, out_sz, pads, reg_s=None, use_avg=True, resize_fcn=imresize):\n    """""" Resizes and pads the cropped image.\n\n    Args:\n        cropped_img: (numpy.ndarray) The cropped image.\n        out_sz: (int) Output size, the desired size of the output.\n        pads: (dictionary) A dictionary of the amount of pad in each side of the\n            cropped image. They can be accessed by their names, e.g.: pads[\'up\']\n        reg_s: (int) Optional: The region side, used to check that the crop size\n            plus the padding amount is equal to the region side in each axis.\n        use_avg: (bool) Indicates the mode of padding employed. If True, the\n            image is padded with the mean value, else it is padded with zeroes.\n\n    Returns:\n        out_img: (numpy.ndarray) The output image, resized and padded. Has size\n            (out_sz, out_sz).\n    """"""\n    # crop height and width\n    cr_h, cr_w, _ = cropped_img.shape\n    if reg_s:\n        assert ((cr_h+pads[\'up\']+pads[\'down\'] == reg_s) and\n                (cr_w+pads[\'left\']+pads[\'right\'] == reg_s)), (\n            \'The informed crop dimensions and pad amounts are not consistent \'\n            \'with the informed region side. Cropped img shape: {}, Pads: {}, \'\n            \'Region size: {}.\'\n            .format(cropped_img.shape, pads, reg_s))\n    # Resize ratio. Here we assume the region is always a square. Obs: The sum\n    # below is equal to reg_s.\n    rz_ratio = out_sz/(cr_h + pads[\'up\'] + pads[\'down\'])\n    rz_cr_h = round(rz_ratio*cr_h)\n    rz_cr_w = round(rz_ratio*cr_w)\n    # Resizes the paddings as well, always guaranteeing that the sum of the\n    # padding amounts with the crop sizes is equal to the output size in each\n    # dimension.\n    pads[\'up\'] = round(rz_ratio*pads[\'up\'])\n    pads[\'down\'] = out_sz - (rz_cr_h + pads[\'up\'])\n    pads[\'left\'] = round(rz_ratio*pads[\'left\'])\n    pads[\'right\'] = out_sz - (rz_cr_w + pads[\'left\'])\n    # Notice that this resized crop is not necessarily a square.\n    rz_crop = resize_fcn(cropped_img, (rz_cr_h, rz_cr_w), interp=\'bilinear\')\n    # Differently from the paper here we are using the mean of all channels\n    # not on each channel. It might be a problem, but the solution might add a lot\n    # of overhead\n    if use_avg:\n        const = np.mean(cropped_img)\n    else:\n        const = 0\n    # Pads only if necessary, i.e., checks if all pad amounts are zero.\n    if not all(p == 0 for p in pads.values()):\n        out_img = np.pad(rz_crop, ((pads[\'up\'], pads[\'down\']),\n                                   (pads[\'left\'], pads[\'right\']),\n                                   (0, 0)),\n                         mode=\'constant\',\n                         constant_values=const)\n    else:\n        out_img = rz_crop\n    return out_img\n'"
training/datasets.py,7,"b'import os\nfrom os.path import join, relpath, isfile\nfrom math import sqrt\nimport random\nimport glob\nimport json\n\nimport numpy as np\nfrom imageio import imread\nfrom scipy.misc import imresize\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import Dataset\n\nfrom training.crops_train import crop_img, resize_and_pad\nfrom utils.exceptions import IncompatibleImagenetStructure\nfrom training.train_utils import get_annotations, check_folder_tree\nfrom training.labels import create_BCELogit_loss_label as BCELoss\n\n\nclass ImageNetVID(Dataset):\n    """""" Dataset subclass representing the ImageNet VID train dataset. It was\n    designed to work with the out-of-the box folder structure of the dataset,\n    so the folder organization should have the following structure:\n    Imagenet/\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Annotations\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 VID\n    \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n    \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 val\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 Data\n     \xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 VID\n     \xc2\xa0\xc2\xa0     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n     \xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 val\n    The other folders present in the ImageNet VID (ImageSets, test, snippets)\n    are ignored and can be safely removed if needed.\n    OBS: the validation dataset inherits from this class.\n    """"""\n\n    def __init__(self, imagenet_dir, transforms=ToTensor(),\n                 reference_size=127, search_size=255, final_size=33,\n                 label_fcn=BCELoss, upscale_factor=4,\n                 max_frame_sep=50, pos_thr=25, neg_thr=50,\n                 cxt_margin=0.5, single_label=True, img_read_fcn=imread,\n                 resize_fcn=imresize, metadata_file=None, save_metadata=None):\n        """""" Initializes the ImageNet VID dataset object.\n\n        Args:\n            imagenet_dir: (str) The full path to the ImageNet VID dataset root\n                folder.\n            transforms: (torchvision.transforms.transforms) A composition of\n                transformations using torchvision.transforms.Compose (or a\n                single transformation) to be applied to the data.\n            reference_size: The total size of the side of the reference region,\n                in pixels.\n            search_size: The total size of the side of the search region,\n                in pixels.\n            final_size: The size of the output of the network.\n            label_fcn: The function that generates the labels for the output of\n                the network. Should match your definition of loss function.\n            upscale_factor: The mean displacement of the receptive field in the\n                input images as we move the output of one pixel. Typically, it\n                is equal to the sum of the network\'s strides when there is no\n                upscaling, and 1 when there is a full upscaling.\n            max_frame_sep: The reference/search pairs are chosen (pseudo)-randomly\n                from frames that are separated by at most \'max_frame_sep\' frames.\n                This parameter influences the capacity of the model to represent\n                long-term model appearance changes.\n            single_label: (bool) Indicates if we are always using the same label,\n                so that we might create it once and for all, instead of calling\n                the label function to create the same label at each iteration.\n            img_read_fcn: (function) The function used to read the images, by\n                default it is the Scikit-image imread function.\n            resize_fcn: (function) The function used to resize the images of the\n                dataset. Default function is scipy\'s imresize.\n            metadata_file: (str) The path to the file containing the\n                .metadata file to be loaded.\n            save_metadata: (str) The path to the file where you want to\n                save the new .metadata file in the case that no metadata file\n                could be loaded. Set to none if you don\'t want to save the metadata.\n        """"""\n        # Do a basic check on the structure of the dataset file-tree\n        if not check_folder_tree(imagenet_dir):\n            raise IncompatibleImagenetStructure\n        self.set_root_dirs(imagenet_dir)\n        self.max_frame_sep = max_frame_sep\n        self.reference_size = reference_size\n        self.search_size = search_size\n        self.upscale_factor = upscale_factor\n        self.cxt_margin = cxt_margin\n        self.final_size = final_size\n        self.pos_thr = pos_thr\n        self.neg_thr = neg_thr\n        self.transforms = transforms\n        self.label_fcn = label_fcn\n        if single_label:\n            self.label = self.label_fcn(self.final_size, self.pos_thr,\n                                        self.neg_thr,\n                                        upscale_factor=self.upscale_factor)\n        else:\n            self.label = None\n        self.img_read = img_read_fcn\n        self.resize_fcn = resize_fcn\n        # Create Metadata. This section is specific to the train version\n        self.get_metadata(metadata_file, save_metadata)\n\n    def set_root_dirs(self, root):\n        self.dir_data = join(root, \'Data\', \'VID\', ""train"")\n        self.dir_annot = join(root, \'Annotations\', \'VID\', ""train"")\n\n    def get_scenes_dirs(self):\n        glob_expression = join(self.dir_data, \'*\', \'*\')\n        relative_paths = [relpath(p, self.dir_data) for p in sorted(glob.glob(glob_expression))]\n        return relative_paths\n\n    def get_metadata(self, metadata_file, save_metadata):\n        """""" Gets the metadata, either by loading it from a json file or by\n        building it from scratch. If the metadata is built, we might save it\n        for later use as a json metadata file.\n\n        Args:\n            metadata_dir: (str or None) The path to the metadata file.\n                Set to None to build the metadata from scratch, based on the\n                imagenet directory attribute of the object.\n            save_metadata: (str or None) The path of the file you want the\n                metadata to be saved on in the case the program has to build it\n                from scratch. If None, the program will not save the metadata\n                in any case.\n                Note: even if save_metadata specifies a valid path, no metadata\n                files will be saved if the program is able to get the metadata\n                from metadata file.\n        """"""\n        # Check if not None\n        if metadata_file and isfile(metadata_file):\n            with open(metadata_file) as json_file:\n                mdata = json.load(json_file)\n            # Check the metadata file.\n            if self.check_metadata(mdata):\n                print(""Metadata file found. Loading its content."")\n                for key, value in mdata.items():\n                    setattr(self, key, value)\n                return\n        # If couldn\'t get metadata from file, build it from scratch\n        mdata = self.build_metadata()\n        if save_metadata is not None:\n            with open(save_metadata, \'w\') as outfile:\n                json.dump(mdata, outfile)\n\n    def check_metadata(self, metadata):\n        """""" Checks if the metadata loaded from file is correct. Doesn\'t make\n        a thorough verification, the main case we are trying to avoid is where\n        the metadata was built on a different machine or the data have been\n        moved around.\n\n        Args:\n            metadata: (dictionary) The dictionary loaded from the json metadata\n                file. It shoud contain the keys \'frames\', \'annotations\' and\n                \'list_idx\'.\n        Returns:\n            Bool: Indicates whether the dictionary corresponds to our expectations,\n                if not it simply returns False, and the program should build\n                the metadata from scratch.\n        """"""\n        if not all(key in metadata for key in (\'frames\', \'annotations\', \'list_idx\')):\n            return False\n        # Check if first and last frames exist in indicated paths.\n        if not (isfile(metadata[\'frames\'][0][0]) and isfile(metadata[\'frames\'][-1][-1])):\n            return False\n        return True\n\n    def get_seq_name(self, seq_idx):\n        """""" Returns the name of the video sequence corresponding to seq_idx, which\n        allows the user to identify the video in the ImageNet folder.\n\n        Args:\n            seq_idx: (int) The index of the sequence according to self.frames\n        Returns:\n            The name of the sequence\'s folder. E.g. \'ILSVRC2015_val_00007029\'\n        """"""\n        return self.frames[seq_idx][0].split(os.sep)[-2]\n\n    def build_metadata(self):\n        """""" Builds the metadata from scratch. Gets the\n        paths to each of the frames as well as their annotations.\n\n        Returns:\n            frames: (list) A list with all the paths to the frames in the\n                concerned set. Organized as a list of lists, where the first\n                dimension indicates the sequence and the second, the frame number\n                inside that sequence.\n            annotations: (list) A list of list of dictionaries containing the\n                annotations for each frame. The list is organized the same way\n                as frames, so frames[seq][frame] and annotations[seq][frame]\n                correspond to the same frame. The annotation information can be\n                accessed using the keywords \'xmax\', \'xmin\', \'ymax\', \'ymin\':\n                Example: del_x =  annotation[\'xmax\'] - annotation[\'xmin\']\n            list_idx: (list) A flat list with lenght equal to the total number\n                of valid frames in the set. Each element is the sequence number\n                of the corresponding frame, so that when we\'re using the\n                __getitem__ method, we can know the sequence of a frame of index\n                idx as list_idx[idx]. For example, if the first sequence has 3\n                frames and the second one has 4, the list would be:\n                [0,0,0,1,1,1,1,2,...]\n        """"""\n        frames = []\n        annotations = []\n        list_idx = []\n\n        # In the training folder the tree depth is one level deeper, so that\n        # a sequence folder in training would be:\n        # \'ILSVRC2015_VID_train_0000/ILSVRC2015_train_00030000\', while in\n        # the val set it would be: \'ILSVRC2015_val_00000000\'\n        scenes_dirs = self.get_scenes_dirs()\n\n        for i, sequence in enumerate(scenes_dirs):\n            seq_frames = []\n            seq_annots = []\n            for frame in sorted(os.listdir(join(self.dir_data, sequence))):\n                # So far we are ignoring the frame dimensions (h, w).\n                annot, h, w, valid = get_annotations(self.dir_annot, sequence, frame)\n                if valid:\n                    seq_frames.append(join(self.dir_data, sequence, frame))\n                    seq_annots.append(annot)\n                    list_idx.append(i)\n            frames.append(seq_frames)\n            annotations.append(seq_annots)\n\n        metadata = {\'frames\': frames,\n                    \'annotations\': annotations,\n                    \'list_idx\': list_idx}\n\n        for key, value in metadata.items():\n            setattr(self, key, value)\n\n        return metadata\n\n    def get_pair(self, seq_idx, frame_idx=None):\n        """""" Gets two frames separated by at most self.max_frame_sep frames (in\n        both directions), both contained in the sequence corresponding to the\n        given index.\n\n        Args:\n            seq_idx: (int) The index of the sequence from which sequence we\n                randomly choose one or two frames.\n            frame_idx: (int) Optional, the index of the first frame of the\n                pair we are constructing\n\n        Returns:\n            first_frame_idx: The index of the reference frame inside self.frames\n            second_frame_idx: The index of the search frame inside self.frames\n        """"""\n        size = len(self.frames[seq_idx])\n        if frame_idx is None:\n            first_frame_idx = random.randint(0, size-1)\n        else:\n            first_frame_idx = frame_idx\n\n        min_frame_idx = max(0, (first_frame_idx - self.max_frame_sep))\n        max_frame_idx = min(size - 1, (first_frame_idx + self.max_frame_sep))\n        second_frame_idx = random.randint(min_frame_idx, max_frame_idx)\n        # # Guarantees that the first and second frames are not the same though\n        # # it wouldn\'t be that much of a problem\n        # if second_frame_idx == first_frame_idx:\n        #     if first_frame_idx == 0:\n        #         second_frame_idx += 1\n        #     else:\n        #         second_frame_idx -= 1\n        return first_frame_idx, second_frame_idx\n\n    def ref_context_size(self, h, w):\n        """""" This function defines the size of the reference region around the\n        target as a function of its bounding box dimensions when the context\n        margin is added.\n\n        Args:\n            h: (int) The height of the bounding box in pixels.\n            w: (int) The width of the bounding box in pixels.\n        Returns:\n            ref_size: (int) The side of the square reference region in pixels.\n                Note that it is always an odd integer.\n        """"""\n        margin_size = self.cxt_margin*(w + h)\n        ref_size = sqrt((w + margin_size) * (h + margin_size))\n        # make sur ref_size is an odd number\n        ref_size = (ref_size//2)*2 + 1\n        return int(ref_size)\n\n    def preprocess_sample(self, seq_idx, first_idx, second_idx):\n        """""" Loads and preprocesses the inputs and output for the learning problem.\n        The input consists of an reference image tensor and a search image tensor,\n        the output is the corresponding label tensor. It also outputs the index of\n        the sequence from which the pair was chosen as well as the ref and search\n        frame indexes in said sequences.\n\n        Args:\n            seq_idx: (int) The index of a sequence inside the whole dataset\n            first_idx: (int) The index of the reference frame inside\n                the sequence.\n            second_idx: (int) The index of the search frame inside\n                the sequence.\n\n        Returns:\n            out_dict: (dict) Dictionary containing all the output information\n                stored with the following keys:\n\n                \'ref_frame\': (torch.Tensor) The reference frame with the\n                    specified size.\n                \'srch_frame\': (torch.Tensor) The search frame with the\n                    specified size.\n                \'label\': (torch.Tensor) The label created with the specified\n                    function in self.label_fcn.\n                \'seq_idx\': (int) The index of the sequence in terms of the self.frames\n                    list.\n                \'ref_idx\': (int) The index of the reference image inside the given\n                    sequence, also in terms of self.frames.\n                \'srch_idx\': (int) The index of the search image inside the given\n                    sequence, also in terms of self.frames.\n        """"""\n        reference_frame_path = self.frames[seq_idx][first_idx]\n        search_frame_path = self.frames[seq_idx][second_idx]\n        ref_annot = self.annotations[seq_idx][first_idx]\n        srch_annot = self.annotations[seq_idx][second_idx]\n\n        # Get size of context region for the reference image, as the geometric\n        # mean of the dimensions added with a context margin\n        ref_w = (ref_annot[\'xmax\'] - ref_annot[\'xmin\'])/2\n        ref_h = (ref_annot[\'ymax\'] - ref_annot[\'ymin\'])/2\n        ref_ctx_size = self.ref_context_size(ref_h, ref_w)\n        ref_cx = (ref_annot[\'xmax\'] + ref_annot[\'xmin\'])/2\n        ref_cy = (ref_annot[\'ymax\'] + ref_annot[\'ymin\'])/2\n\n        ref_frame = self.img_read(reference_frame_path)\n        ref_frame = np.float32(ref_frame)\n\n        ref_frame, pad_amounts_ref = crop_img(ref_frame, ref_cy, ref_cx, ref_ctx_size)\n        try:\n            ref_frame = resize_and_pad(ref_frame, self.reference_size, pad_amounts_ref,\n                                       reg_s=ref_ctx_size, use_avg=True,\n                                       resize_fcn=self.resize_fcn)\n        # If any error occurs during the resize_and_pad function we print to\n        # the terminal the path to the frame that raised such error.\n        except AssertionError:\n            print(\'Fail Ref: \', reference_frame_path)\n            raise\n\n        srch_ctx_size = ref_ctx_size * self.search_size / self.reference_size\n        srch_ctx_size = (srch_ctx_size//2)*2 + 1\n\n        srch_cx = (srch_annot[\'xmax\'] + srch_annot[\'xmin\'])/2\n        srch_cy = (srch_annot[\'ymax\'] + srch_annot[\'ymin\'])/2\n\n        srch_frame = self.img_read(search_frame_path)\n        srch_frame = np.float32(srch_frame)\n        srch_frame, pad_amounts_srch = crop_img(srch_frame, srch_cy, srch_cx, srch_ctx_size)\n        try:\n            srch_frame = resize_and_pad(srch_frame, self.search_size, pad_amounts_srch,\n                                        reg_s=srch_ctx_size, use_avg=True,\n                                        resize_fcn=self.resize_fcn)\n        except AssertionError:\n            print(\'Fail Search: \', search_frame_path)\n            raise\n\n        if self.label is not None:\n            label = self.label\n        else:\n            label = self.label_fcn(self.final_size, self.pos_thr, self.neg_thr,\n                                   upscale_factor=self.upscale_factor)\n        # OBS: The ToTensor transform converts the images from a 0-255 range\n        # to a 0-1 range\n        ref_frame = self.transforms(ref_frame)\n        srch_frame = self.transforms(srch_frame)\n\n        out_dict = {\'ref_frame\': ref_frame, \'srch_frame\': srch_frame,\n                    \'label\': label, \'seq_idx\': seq_idx, \'ref_idx\': first_idx,\n                    \'srch_idx\': second_idx }\n        return out_dict\n\n    def __getitem__(self, idx):\n        """""" Returns the inputs and output for the learning problem. The input\n        consists of an reference image tensor and a search image tensor, the\n        output is the corresponding label tensor.\n\n        Args:\n            idx: (int) The index of a sequence inside the whole dataset, from\n                which the function will choose the reference and search frames.\n\n        Returns:\n            ref_frame (torch.Tensor): The reference frame with the\n                specified size.\n            srch_frame (torch.Tensor): The search frame with the\n                specified size.\n            label (torch.Tensor): The label created with the specified\n                function in self.label_fcn.\n        """"""\n        seq_idx = self.list_idx[idx]\n        first_idx, second_idx = self.get_pair(seq_idx)\n        return self.preprocess_sample(seq_idx, first_idx, second_idx)\n\n    def __len__(self):\n        return len(self.list_idx)\n\n\nclass ImageNetVID_val(ImageNetVID):\n    """""" The validation version of the ImageNetVID dataset inherits from the\n    train version. The main difference between the two is the way they sample\n    the images: while the train version samples in train time, choosing a pair\n    of images for the given sequence, the val version chooses all pairs in\n    initialization time and stores them internally. The val sampling follows a\n    pseudo-random sequence, but since it seeds the random sequence generators,\n    the choice is deterministic, so the choice is saved in a file called\n    val.metadata that can be charged at initialization time.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        """""" For the complete argument description see the Parent class. The val\n        version uses the original initialization but it gets a list of pairs of\n        indexes consistent with the list of frames and annotations.\n        """"""\n        super().__init__(*args, **kwargs)\n\n    def set_root_dirs(self, root):\n        self.dir_data = join(root, \'Data\', \'VID\', ""val"")\n        self.dir_annot = join(root, \'Annotations\', \'VID\', ""val"")\n\n    def get_scenes_dirs(self):\n        """"""Apply the right glob function to get the scene directories, relative\n        to the data directory\n        """"""\n        glob_expression = join(self.dir_data, \'*\')\n        relative_paths = [relpath(p, self.dir_data) for p in sorted(glob.glob(glob_expression))]\n        return relative_paths\n\n    def check_metadata(self, metadata):\n        """""" Checks the val metadata, first by doing the check defined in the\n        parent class, but for the val dataset, since the \'frames\', \'annotations\'\n        and \'list_idx\' have the same structure for both types.Then it checks\n        the specific val elements of the metadata.\n\n        Args:\n            metadata: (dict) The metadata dictionary, derived from the json\n                metadata file.\n        """"""\n        if not super().check_metadata(metadata):\n            return False\n        if not all(key in metadata for key in (\'list_pairs\', \'max_frame_sep\')):\n            return False\n        # Check if the maximum frame separation is the same as the metadata one,\n        # since the choice of pairs depends on this separation parameter.\n        if metadata[\'max_frame_sep\'] != self.max_frame_sep:\n            return False\n        return True\n\n    def build_metadata(self):\n        metadata = super().build_metadata()\n        self.list_pairs = self.build_test_pairs()\n        metadata[\'list_pairs\'] = self.list_pairs\n        metadata[\'max_frame_sep\'] = self.max_frame_sep\n        return metadata\n\n    def build_test_pairs(self):\n        """""" Creates the list of pairs of reference and search images in the\n        val dataset. It follows the same general rules for choosing pairs as the\n        train dataset, but it does it in initialization time instead of\n        train/val time. Moreover it seeds the random module with a fixed seed to\n        get the same choice of pairs across different executions.\n        Returns:\n            list_pairs: (list) A list with all the choosen pairs. It consists\n                of a list of tuples with 3 elements each: the index inside the\n                sequence of the reference and of the search frame and their\n                sequence index. It has one tuple for each frame in the dataset,\n                though the function could be easily extended to any number of\n                pairs.\n        """"""\n        # Seed random lib to get consistent values\n        random.seed(100)\n        list_pairs = []\n        for seq_idx, seq in enumerate(self.frames):\n            for frame_idx in range(len(seq)):\n                list_pairs.append([seq_idx, *super().get_pair(seq_idx, frame_idx)])\n        random.shuffle(list_pairs)\n        # Reseed the random module to avoid disrupting any external use of it.\n        random.seed()\n        return list_pairs\n\n    def __getitem__(self, idx):\n        """""" Returns the reference and search frame indexes along with the sequence\n        index corresponding to the given index \'idx\'. The sequence itself is fixed\n        from the initialization, so the function simply returns the idx\'th element\n        of the list_pairs.\n\n        Args:\n            idx: (int) The index of the current iteration of validation.\n        """"""\n        list_idx, first_idx, second_idx = self.list_pairs[idx]\n        return self.preprocess_sample(list_idx, first_idx, second_idx)\n'"
training/labels.py,0,"b'import numpy as np\n\n\ndef create_BCELogit_loss_label(label_size, pos_thr, neg_thr, upscale_factor=4):\n    """""" Creates the label for the output of a training pair. Because both crops\n    are always centered, the label doesn\'t depend on the particular positions\n    of the target on their respective images.\n    This label has a equivalent effect than logistic_loss_label, when used with\n    the Binary Cross Entropy Logit loss, but saves processing time and is more\n    in line with common ML practices. To model the behavior of the 3-level\n    labels of the logistic loss it has two dimensions for each pixel, one that\n    encodes the positive (1) and negative (0) pixels and a second one that\n    encodes if the pixel is positive/negative (1) or neutral (0). The neutral\n    values do not contribute to the gradient, so they are simply ignored when\n    calculating the loss, and their value in the first dimension can be set\n    arbitrarily.\n    The thresholds are defined in terms of pixel distances in the search image,\n    so, if the correlation map is not upscaled (i.e. it has a stride > 1) then\n    we must convert the thresholds to the corresponding distances in the\n    correlation map (by simply dividing the thresholds by the stride).\n    The correlation map corresponds to a centered subregion of the search image\n    (typically, for a search image of 255 pixels, and ref image of 127, this\n    subregion has 129 pixels) so there are practical limits to the thresholds:\n    any value larger than the half diagonal of this subregion (typically, 91.2\n    pixels) will encompass the whole correlation map.\n\n    The tests are dist <= pos_thr and dist > neg_thr.\n\n    Args:\n        label_size: (int) The size in pixels of the output of the network.\n        pos_thr: (int) The positive values threshold, in terms of the pixels of\n            the search image.\n        neg_thr: (int) The negative values threshold, in terms of the pixels of\n            the search image.\n        upscale_factor: (int) How much we have to upscale the output feature map\n            to match the input images. It represents the mean displacement of\n            the receptive field of the network in the input images for a\n            one-pixel movement in the correlation map. Typically it tells how\n            much the network up or downscales the image.\n\n    Returns:\n        label (numpy.ndarray): The label for each pair with dimension\n            label_size * label_size * 2\n\n    OBS: Set both pos_thr and neg_thr to zero to get the label for an empty\n        frame\n    """"""\n    # Convert the thresholds if the stride is bigger than 1, else it stays the same.\n    pos_thr = pos_thr / upscale_factor\n    neg_thr = neg_thr / upscale_factor\n    # Note that the center might be between pixels if the label size is even.\n    center = (label_size - 1) / 2\n    line = np.arange(0, label_size)\n    line = line - center\n    line = line**2\n    line = np.expand_dims(line, axis=0)\n    dist_map = line + line.transpose()\n\n    label = np.zeros([label_size, label_size, 2]).astype(np.float32)\n    label[:, :, 0] = dist_map <= pos_thr**2\n    label[:, :, 1] = (dist_map <= pos_thr**2) | (dist_map > neg_thr**2)\n\n    return label\n'"
training/losses.py,4,"b'import torch.nn.functional as F\n\n\ndef BCELogit_Loss(score_map, labels):\n    """""" The Binary Cross-Correlation with Logits Loss.\n    Args:\n        score_map (torch.Tensor): The score map tensor of shape [B,1,H,W]\n        labels (torch.Tensor): The label tensor of shape [B,H,W,2] where the\n            fourth dimension is separated in two maps, the first indicates whether\n            the pixel is negative (0) or positive (1) and the second one whether\n            the pixel is positive/negative (1) or neutral (0) in which case it\n            will simply be ignored.\n    Return:\n        loss (scalar torch.Tensor): The BCE Loss with Logits for the score map and labels.\n    """"""\n    labels = labels.unsqueeze(1)\n    loss = F.binary_cross_entropy_with_logits(score_map, labels[:, :, :, :, 0],\n                                              weight=labels[:, :, :, :, 1],\n                                              reduction=\'mean\')\n    return loss\n'"
training/metrics.py,0,"b'import numpy as np\nfrom sklearn.metrics import roc_auc_score\n\n\ndef center_error(output, label, upscale_factor=4):\n    """""" The center error is a metric that measures the displacement between the\n    estimated center of the target and the \'ground-truth\'. One must note that\n    since the dataset is not annotated in terms of target center, there is a\n    intrinsic noise in the center ground-truth.\n    The center_error is calculated in terms of the displacement in the search\n    image in pixels, so when no upscaling is applied to the correlation map,\n    the center error uses the stride of the network to calculate it.\n    Networks with different strides can be directly compared using the center\n    error, but networks with different reference or search image sizes are more\n    tricky to be compared, since the maximum value (and the probability\n    distribution) of the center error are different for different final score\n    map sizes (i.e. a smaller score map is more likely to have a smaller\n    center error even by random choice).\n\n    THIS VERSION IS IMPLEMENTED WITH NUMPY, NOT PYTORCH TENSORS\n\n    Args:\n        output: (np.ndarray) The output of the network with dimension [Bx1xHxW]\n        label: (np.ndarray) The labels with dimension [BxHxWx2] (Not used, kept\n            for constistency with the metric_fcn(output,label) calls)\n        upscale_factor: (int) Indicates the how much we must upscale the output\n            feature map to match it to the input images. When we use an upscaling\n            layer the upscale_factor is 1.\n\n    Returns:\n        c_error: (int) The center displacement in pixels.\n    """"""\n    b = output.shape[0]\n    s = output.shape[-1]\n    out_flat = output.reshape(b, -1)\n    max_idx = np.argmax(out_flat, axis=1)\n    estim_center = np.stack([max_idx//s, max_idx % s], axis=1)\n    dist = np.linalg.norm(estim_center - s//2, axis=1)\n    c_error = dist.mean()\n    c_error = c_error * upscale_factor\n    return c_error\n\n\ndef AUC(output, label):\n    """""" Calculates the area under the ROC curve of the given outputs. All of the\n    outputs in the neutral region of the label (see labels.py) are ignored when\n    calculating the AUC.\n    Why I\'m using SkLearn\'s AUC: https://github.com/pytorch/tnt/issues/54\n\n    Args:\n        output: (np.ndarray) The output of the network with dimension [Bx1xHxW]\n        label: (np.ndarray) The labels with dimension [BxHxWx2]\n    """"""\n    b = output.shape[0]\n    output = output.reshape(b, -1)\n    mask = label[:, :, :, 1].reshape(b, -1)\n    label = label[:, :, :, 0].reshape(b, -1)\n    total_auc = 0\n    for i in range(b):\n        total_auc += roc_auc_score(label[i], output[i], sample_weight=mask[i])\n    return total_auc/b\n\n\n# def center_error_normalized_by_target_size():\n#     pass\n\n\n# The dictionary containing all the available metrics.\nMETRICS = {\n    \'AUC\': {\'fcn\': AUC, \'kwargs\': {}},\n    \'center_error\': {\'fcn\': center_error,\n                     \'kwargs\': {\'upscale_factor\': 4}}\n}\n'"
training/models.py,9,"b'"""""" Module containing the definitions of the networks used in training. Inspired\nby https://github.com/adambielski/siamese-triplet/blob/master/networks.py , it\ndefines embedding networks, that represent the branch phase of the architectures,\nand joining networks (siamese or triplet) that take as argument the embedding\nnetworks.\n""""""\nimport math\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.init import xavier_uniform_, constant_, zeros_, normal_\n\n\nclass BaselineEmbeddingNet(nn.Module):\n    """""" Definition of the embedding network used in the baseline experiment of\n    Bertinetto et al in https://arxiv.org/pdf/1704.06036.pdf.\n    It basically corresponds to the convolutional stage of AlexNet, with some\n    of its hyperparameters changed.\n    """"""\n\n    def __init__(self):\n        super(BaselineEmbeddingNet, self).__init__()\n        self.fully_conv = nn.Sequential(nn.Conv2d(3, 96, kernel_size=11,\n                                                  stride=2, bias=True),\n                                        nn.BatchNorm2d(96),\n                                        nn.ReLU(),\n                                        nn.MaxPool2d(3, stride=2),\n\n                                        nn.Conv2d(96, 256, kernel_size=5,\n                                                  stride=1, groups=2,\n                                                  bias=True),\n                                        nn.BatchNorm2d(256),\n                                        nn.ReLU(),\n                                        nn.MaxPool2d(3, stride=1),\n                                        nn.Conv2d(256, 384, kernel_size=3,\n                                                  stride=1, groups=1,\n                                                  bias=True),\n                                        nn.BatchNorm2d(384),\n                                        nn.ReLU(),\n                                        nn.Conv2d(384, 384, kernel_size=3,\n                                                  stride=1, groups=2,\n                                                  bias=True),\n                                        nn.BatchNorm2d(384),\n                                        nn.ReLU(),\n                                        nn.Conv2d(384, 32, kernel_size=3,\n                                                  stride=1, groups=2,\n                                                  bias=True))\n\n    def forward(self, x):\n        output = self.fully_conv(x)\n        return output\n\n    def get_embedding(self, x):\n        return self.forward(x)\n\n\nclass VGG11EmbeddingNet_5c(nn.Module):\n    """""" Embedding branch based on Pytorch\'s VGG11 with Batchnorm (https://pytor\n    ch.org/docs/stable/torchvision/models.html). This is version 5c, meaning\n    that it has 5 convolutional layers, it follows the original model up until\n    the 13th layer (The ReLU after the 4th convolution), in order to keep the\n    total stride equal to 4. It adds the 5th convolutional layer which acts as\n    a bottleck a feature bottleneck reducing the features from 256 to 32 and\n    must always be trained. The layers 0 to 13 can be loaded from\n    torchvision.models.vgg11_bn(pretrained=True)\n    """"""\n\n    def __init__(self):\n        super(VGG11EmbeddingNet_5c, self).__init__()\n        self.fully_conv = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(64),\n                                        nn.ReLU(),\n                                        nn.MaxPool2d(2, stride=2),\n\n                                        nn.Conv2d(64, 128, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(128),\n                                        nn.ReLU(),\n                                        nn.MaxPool2d(2, stride=2),\n                                        nn.Conv2d(128, 256, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(256),\n                                        nn.ReLU(),\n                                        nn.Conv2d(256, 256, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(256),\n                                        nn.ReLU(),\n                                        # Added ConvLayer, not in original model\n                                        nn.Conv2d(256, 32, kernel_size=3,\n                                                  stride=1, bias=True))\n\n    def forward(self, x):\n        output = self.fully_conv(x)\n        return output\n\n    def get_embedding(self, x):\n        return self.forward(x)\n\n\nclass VGG16EmbeddingNet_8c(nn.Module):\n    """""" Embedding branch based on Pytorch\'s VGG16 with Batchnorm (https://pytor\n    ch.org/docs/stable/torchvision/models.html). This is version 8c, meaning\n    that it has 8 convolutional layers, it follows the original model up until\n    the 22th layer (The ReLU after the 7th convolution), in order to keep the\n    total stride equal to 4. It adds the 8th convolutional layer which acts as\n    a bottleck a feature bottleneck reducing the features from 256 to 32 and\n    must always be trained. The layers 0 to 22 can be loaded from\n    torchvision.models.vgg16_bn(pretrained=True)\n    """"""\n\n    def __init__(self):\n        super(VGG16EmbeddingNet_8c, self).__init__()\n        self.fully_conv = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(64),\n                                        nn.ReLU(),\n                                        nn.Conv2d(64, 64, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(64),\n                                        nn.ReLU(),\n                                        nn.MaxPool2d(2, stride=2),\n                                        nn.Conv2d(64, 128, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(128),\n                                        nn.ReLU(),\n                                        nn.Conv2d(128, 128, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(128),\n                                        nn.ReLU(),\n                                        nn.MaxPool2d(2, stride=2),\n                                        nn.Conv2d(128, 256, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(256),\n                                        nn.ReLU(),\n                                        nn.Conv2d(256, 256, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(256),\n                                        nn.ReLU(),\n                                        nn.Conv2d(256, 256, kernel_size=3,\n                                                  stride=1, bias=True),\n                                        nn.BatchNorm2d(256),\n                                        nn.ReLU(),\n                                        # Added ConvLayer, not in original model\n                                        nn.Conv2d(256, 32, kernel_size=3,\n                                                  stride=1, bias=True))\n\n    def forward(self, x):\n        output = self.fully_conv(x)\n        return output\n\n    def get_embedding(self, x):\n        return self.forward(x)\n\n\nclass SiameseNet(nn.Module):\n    """""" The basic siamese network joining network, that takes the outputs of\n    two embedding branches and joins them applying a correlation operation.\n    Should always be used with tensors of the form [B x C x H x W], i.e.\n    you must always include the batch dimension.\n    """"""\n\n    def __init__(self, embedding_net, upscale=False, corr_map_size=33, stride=4):\n        super(SiameseNet, self).__init__()\n        self.embedding_net = embedding_net\n        self.match_batchnorm = nn.BatchNorm2d(1)\n\n        self.upscale = upscale\n        # TODO calculate automatically the final size and stride from the\n        # parameters of the branch\n        self.corr_map_size = corr_map_size\n        self.stride = stride\n        # Calculates the upscale size based on the correlation map size and\n        # the total stride of the network, so as to align the corners of the\n        # original and the upscaled one, which also aligns the centers.\n        self.upsc_size = (self.corr_map_size-1)*self.stride + 1\n        # The upscale_factor is the correspondence between a movement in the output\n        # feature map and the input images. So if a network has a total stride of 4\n        # and no deconvolutional or upscaling layers, a single pixel displacement\n        # in the output corresponds to a 4 pixels displacement in the input\n        # image. The easiest way to compensate this effect is to do a bilinear\n        # or bicubic upscaling.\n        if upscale:\n            self.upscale_factor = 1\n        else:\n            self.upscale_factor = self.stride\n\n    def forward(self, x1, x2):\n        """"""\n        Args:\n            x1 (torch.Tensor): The reference patch of dimensions [B, C, H, W].\n                Usually the shape is [8, 3, 127, 127].\n            x2 (torch.Tensor): The search region image of dimensions\n                [B, C, H\', W\']. Usually the shape is [8, 3, 255, 255].\n        Returns:\n            match_map (torch.Tensor): The score map for the pair. For the usual\n                input shapes, the output shape is [8, 1, 33, 33].\n        """"""\n        embedding_reference = self.embedding_net(x1)\n        embedding_search = self.embedding_net(x2)\n        match_map = self.match_corr(embedding_reference, embedding_search)\n        return match_map\n\n    def get_embedding(self, x):\n        return self.embedding_net(x)\n\n    def match_corr(self, embed_ref, embed_srch):\n        """""" Matches the two embeddings using the correlation layer. As per usual\n        it expects input tensors of the form [B, C, H, W].\n\n        Args:\n            embed_ref: (torch.Tensor) The embedding of the reference image, or\n                the template of reference (the average of many embeddings for\n                example).\n            embed_srch: (torch.Tensor) The embedding of the search image.\n\n        Returns:\n            match_map: (torch.Tensor) The correlation between\n        """"""\n        b, c, h, w = embed_srch.shape\n        # Here the correlation layer is implemented using a trick with the\n        # conv2d function using groups in order to do the correlation with\n        # batch dimension. Basically we concatenate each element of the batch\n        # in the channel dimension for the search image (making it\n        # [1 x (B.C) x H\' x W\']) and setting the number of groups to the size of\n        # the batch. This grouped convolution/correlation is equivalent to a\n        # correlation between the two images, though it is not obvious.\n        match_map = F.conv2d(embed_srch.view(1, b * c, h, w),\n                             embed_ref, groups=b)\n        # Here we reorder the dimensions to get back the batch dimension.\n        match_map = match_map.permute(1, 0, 2, 3)\n        match_map = self.match_batchnorm(match_map)\n        if self.upscale:\n            match_map = F.interpolate(match_map, self.upsc_size, mode=\'bilinear\',\n                                      align_corners=False)\n\n        return match_map\n\n\ndef weights_init(model):\n    """""" Initializes the weights of the CNN model using the Xavier\n    initialization.\n    """"""\n    if isinstance(model, nn.Conv2d):\n        xavier_uniform_(model.weight, gain=math.sqrt(2.0))\n        constant_(model.bias, 0.1)\n    elif isinstance(model, nn.BatchNorm2d):\n        normal_(model.weight, 1.0, 0.02)\n        zeros_(model.bias)\n'"
training/optim.py,1,"b'import torch.optim as optim\n\n\ndef get_SGD(model_params, **kwargs):\n    return optim.SGD(model_params, **kwargs)\n\n\ndef get_Adam(model_params, **kwargs):\n    return optim.Adam(model_params, **kwargs)\n\n\nOPTIMIZERS = {""SGD"": get_SGD,\n              ""Adam"": get_Adam}\n'"
training/summary_utils.py,6,"b'"""""" This module defines the classes and functions that write the summaries\nof the training using TensorboardX.\n""""""\nfrom os.path import join\n\nimport torch\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid\nfrom tensorboardX import SummaryWriter\n\nimport utils.colormaps as cm\n\n\nclass SummaryMaker():\n    """"""This class is mostly a wrapper around the tensorboardX SummaryWriter,\n    intended to carry the current epoch index, and implement the processing of\n    the outputs for more elaborate summaries.\n    """"""\n\n    def __init__(self, log_dir, params, up_factor):\n        """"""\n        Args:\n            log_dir: (str) The path to the folder where the summary files are\n                going to be written. The summary object creates a train and a\n                val folders to store the summary files.\n            params: (train.utils.Params) The parameters loaded from the\n                parameters.json file.\n            up_factor: (int) The upscale factor that indicates how much the\n                scores maps need to be upscaled to match the original scale\n                (used when superposing the embeddings and score maps to the\n                input images).\n\n        Attributes:\n            writer_train: (tensorboardX.writer.SummaryWriter) The tensorboardX\n                writer that writes the training informations.\n            writer_val: (tensorboardX.writer.SummaryWriter) The tensorboardX\n                writer that writes the validation informations.\n            epoch: (int) Stores the current epoch.\n            ref_sz: (int) The size in pixels of the reference image.\n            srch_sz: (int) The size in pixels of the search image.\n            up_factor: (int) The upscale factor. See Args.\n\n        """"""\n        # We use two different summary writers so we can plot both curves in\n        # the same plot, as suggested in https://www.quora.com/How-do-you-plot-training-and-validation-loss-on-the-same-graph-using-TensorFlow%E2%80%99s-TensorBoard\n        self.writer_train = SummaryWriter(join(log_dir, \'train\'))\n        self.writer_val = SummaryWriter(join(log_dir, \'val\'))\n        self.epoch = None\n        self.ref_sz = params.reference_sz\n        self.srch_sz = params.search_sz\n        self.up_factor = up_factor\n\n    def add_epochwise_scalar(self, mode, tag, scalar_value):\n        """""" Wraps the writer.add_scalar function, using the attribute epoch as\n        global_step.\n\n        Args:\n            mode: (str) Indicates wheter \'train\' or \'val\'\n            tag: (str) The tag identifying the scalar.\n            scalar_value: (scalar) The value of the scalar.\n        """"""\n        if mode == \'train\':\n            self.writer_train.add_scalar(tag, scalar_value, self.epoch)\n        elif mode == \'val\':\n            self.writer_val.add_scalar(tag, scalar_value, self.epoch)\n\n    def add_overlay(self, tag, embed, img, alpha=0.8, cmap=\'inferno\', add_ref=None):\n        """""" Adds to the summary the images of the input image (ref or search)\n        overlayed with the corresponding embedding or correlation map. It expect\n        tensors of with dimensions [C x H x W] or [B x C x H x W] if the tensor\n        has a batch dimension it takes the FIRST ELEMENT of the batch. The image\n        is displayed as fusion of the input image in grayscale and the overlay\n        in the chosen color_map, this fusion is controlled by the alpha factor.\n        In the case of the embeddings, since there are multiple feature\n        channels, we show each of them individually in a grid.\n        OBS: The colors represent relative values, where the peak color corresponds\n        to the maximum value in any given channel, so no direct value comparisons\n        can be made between epochs, only the relative distribution of neighboring\n        pixel values, (which should be enough, since we are mosly interested\n        in finding the maximum of a given correlation map)\n\n        Args:\n            tag: (str) The string identifying the image in tensorboard, images\n                with the same tag are grouped together with a slider, and are\n                indexed by epoch.\n            embed: (torch.Tensor) The tensor containing the embedding of an\n                input (ref or search image) or a correlation map (the final\n                output). The shape should be [B, C, H, W] or [B, H, W] for the\n                case of the correlation map.\n            img: (torch.Tensor) The image on top of which the embed is going\n                to be overlaid. Reference image embeddings should be overlaid\n                on top of reference images and search image embeddings as well\n                as the correlation maps should be overlaid on top of the search\n                images.\n            alpha: (float) A mixing variable, it controls how much of the final\n                embedding corresponds to the grayscale input image and how much\n                corresponds to the overlay. Alpha = 0, means there is no\n                overlay in the final image, only the input image. Conversely,\n                Alpha = 1 means there is only overlay. Adjust this value so\n                you can distinctly see the overlay details while still seeing\n                where it is in relation to the orignal image.\n            cmap: (str) The name of the colormap to be used with the overlay.\n                The colormaps are defined in the colormaps.py module, but values\n                include \'viridis\' (greenish blue) and \'inferno\' (yellowish red).\n            add_ref: (torch.Tensor) Optional. An additional reference image that\n                will be plotted to the side of the other images. Useful when\n                plotting correlation maps, because it lets the user see both\n                the search image and the reference that is used as the target.\n\n        ``Example``\n            >>> summ_maker = SummaryMaker(os.path.join(exp_dir, \'tensorboard\'), params,\n                                           model.upscale_factor)\n            ...\n            >>> embed_ref = model.get_embedding(ref_img_batch)\n            >>> embed_srch = model.get_embedding(search_batch)\n            >>> output_batch = model.match_corr(embed_ref, embed_srch)\n            >>> batch_index = 0\n            >>> summ_maker.add_overlay(""Ref_image_{}"".format(tbx_index), embed_ref[batch_index], ref_img_batch[batch_index], cmap=\'inferno\')\n            >>> summ_maker.add_overlay(""Search_image_{}"".format(tbx_index), embed_srch[batch_index], search_batch[batch_index], cmap=\'inferno\')\n            >>> summ_maker.add_overlay(""Correlation_map_{}"".format(tbx_index), output_batch[batch_index], search_batch[batch_index], cmap=\'inferno\')\n        """"""\n        # TODO Add numbers in the final image to the feature channels.\n        # TODO Add the color bar showing the progression of values.\n        # If minibatch is given, take only the first image\n        # TODO let the user select the image? Loop on all images?\n        if len(embed.shape) == 4:\n            embed = embed[0]\n        if len(img.shape) == 4:\n            img = img[0]\n        # Normalize the image.\n        img = img/255\n        embed = cm.apply_cmap(embed, cmap=cmap)\n        # Get grayscale version of image by taking the weighted average of the channels\n        # as described in https://www.cs.virginia.edu/~vicente/recognition/notebooks/image_processing_lab.html#2.-Converting-to-Grayscale\n        R,G,B = img\n        img_gray = 0.21 * R + 0.72 * G + 0.07 * B\n        # Get the upscaled size of the embedding, so as to take into account\n        # the network\'s downscale caused by the stride.\n        upsc_size = (embed.shape[-1] - 1) * self.up_factor + 1\n        embed = F.interpolate(embed, upsc_size, mode=\'bilinear\',\n                              align_corners=False)\n        # Pad the embedding with zeros to match the image dimensions. We pad\n        # all 4 corners equally to keep the embedding centered.\n        tot_pad = img.shape[-1] - upsc_size\n        # Sanity check 1. The amount of padding must be equal on all sides, so\n        # the total padding on any dimension must be an even integer.\n        assert tot_pad % 2 == 0, ""The embed or image dimensions are incorrect.""\n        pad = int(tot_pad/2)\n        embed = F.pad(embed, (pad, pad, pad, pad), \'constant\', 0)\n        # Sanity check 2, the size of the embedding in the (H, w) dimensions\n        # matches the size of the image.\n        assert embed.shape[-2:] == img.shape[-2:], (""The embedding overlay ""\n                                                    ""and image dimensions ""\n                                                    ""do not agree."")\n        final_imgs = alpha * embed + (1-alpha) * img_gray\n        # The embedding_channel (or feature channel) dimension is treated like\n        # a batch dimension, so the grid shows each individual embeding\n        # overlayed with the input image. Plus the original image is also shown.\n        # If add_ref is used the ref image is the first to be shown.\n        img = img.unsqueeze(0)\n        final_imgs = torch.cat((img, final_imgs))\n        if add_ref is not None:\n            # Pads the image if necessary\n            add_ref = add_ref/255\n            pad = int((img.shape[-1] - add_ref.shape[-1])//2)\n            add_ref = F.pad(add_ref, (pad, pad, pad, pad), \'constant\', 0)\n            add_ref = add_ref.unsqueeze(0)\n            final_imgs = torch.cat((add_ref, final_imgs))\n        final_imgs = make_grid(final_imgs, nrow=6)\n        self.writer_val.add_image(tag, final_imgs, self.epoch)\n'"
training/train_utils.py,5,"b'import xml.etree.ElementTree as ET\nimport os\nfrom os.path import join, isdir, isfile, splitext\nimport json\nimport logging\nimport shutil\n\nimport torch\n\n\ndef get_annotations(annot_dir, sequence_dir, frame_file):\n    """""" Gets the annotations contained in the xml file of the current frame.\n\n    Args:\n        annot_dir (str): The root dir of annotation folders\n        sequence_dir(str): The directory in which frame is located, relative to\n        root directory\n        frame_file (str): The frame filename\n        i.e., sequence_identifier/frame_number.JPEG, or the full path.\n    Return:\n        annotation: (dictionary) A dictionary containing the four values of the\n            the bounding box stored in the following keywords:\n            \'xmax\' (int): The rightmost x coordinate of the bounding box.\n            \'xmin\' (int): The leftmost x coordinate of the bounding box.\n            \'ymax\' (int): The uppermost y coordinate of the bounding box.\n            \'ymin\' (int): The lowest y coordinate of the bounding box.\n\n        width (int): The width of the jpeg image in pixels.\n        height (int): The height of the jpeg image in pixels.\n        valid_frame (bool): False if the target is present in the frame,\n            and True otherwise.\n        OBS: If the target is not in the frame the bounding box information\n        are all None.\n    """"""\n    # Separate the sequence from the frame id using the OS path separator\n    \n    frame_number = splitext(frame_file)[0]\n    annot_path = join(annot_dir, sequence_dir, frame_number + \'.xml\')\n    if isfile(annot_path):\n        tree = ET.parse(annot_path)\n        root = tree.getroot()\n        size = root.find(\'size\')\n        width = int(size.find(\'width\').text)\n        height = int(size.find(\'height\').text)\n        if root.find(\'object\') is None:\n            annotation = {\'xmax\': None, \'xmin\': None, \'ymax\': None, \'ymin\': None}\n            valid_frame = False\n            return annotation, width, height, valid_frame\n        # TODO Decide what to do with multi-object sequences. For now I\'m\n        # just going to take the one with track_id = 0\n        track_id_zero_present = False\n        for obj in root.findall(\'object\'):\n            if obj.find(\'trackid\').text == \'0\':\n                bbox = obj.find(\'bndbox\')\n                xmax = int(bbox.find(\'xmax\').text)\n                xmin = int(bbox.find(\'xmin\').text)\n                ymax = int(bbox.find(\'ymax\').text)\n                ymin = int(bbox.find(\'ymin\').text)\n                track_id_zero_present = True\n        if not track_id_zero_present:\n            annotation = {\'xmax\': None, \'xmin\': None, \'ymax\': None, \'ymin\': None}\n            valid_frame = False\n            return annotation, width, height, valid_frame\n    else:\n        raise FileNotFoundError(""The file {} could not be found""\n                                .format(annot_path))\n\n    annotation = {\'xmax\': xmax, \'xmin\': xmin, \'ymax\': ymax, \'ymin\': ymin}\n    valid_frame = True\n    return annotation, width, height, valid_frame\n\n\ndef check_folder_tree(root_dir):\n    """""" Checks if the folder structure is compatible with the expected one.\n    Args:\n        root_dir: (str) The path to the root directory of the dataset\n\n    Return:\n        bool: True if the structure is compatible, False otherwise.\n    """"""\n    data_type = [\'Annotations\', \'Data\']\n    dataset_type = [\'train\', \'val\']\n    necessary_folders = [join(root_dir, data, \'VID\', dataset) for data in data_type for dataset in dataset_type]\n    return all(isdir(path) for path in necessary_folders)\n\n\nclass Params():\n    """"""Class that loads hyperparameters from a json file.\n    Example:\n    ```\n    params = Params(json_path)\n    print(params.learning_rate)\n    params.learning_rate = 0.5  # change the value of learning_rate in params\n    ```\n    """"""\n\n    def __init__(self, json_path):\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    def save(self, json_path):\n        with open(json_path, \'w\') as f:\n            json.dump(self.__dict__, f, indent=4)\n\n    def update(self, json_path):\n        """"""Loads parameters from json file""""""\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    def update_with_dict(self, dictio):\n        """""" Updates the parameters with the keys and values of a dictionary.""""""\n        self.__dict__.update(dictio)\n\n    @property\n    def dict(self):\n        """"""Gives dict-like access to Params instance by `params.dict[\'learning_rate\']""""""\n        return self.__dict__\n\n\nclass RunningAverage():\n    """"""A simple class that maintains the running average of a quantity\n\n    Example:\n    ```\n    loss_avg = RunningAverage()\n    loss_avg.update(2)\n    loss_avg.update(4)\n    loss_avg() = 3\n    ```\n    """"""\n\n    def __init__(self):\n        self.steps = 0\n        self.total = 0\n\n    def update(self, val):\n        self.total += val\n        self.steps += 1\n\n    def __call__(self):\n        return self.total/float(self.steps)\n\n\ndef set_logger(log_path):\n    """"""Set the logger to log info in terminal and file `log_path`.\n    In general, it is useful to have a logger so that every output to the\n    terminal is saved in a permanent file.\n    Example:\n    ```\n    logging.info(""Starting training..."")\n    ```\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s:\'\n                                                    \' %(message)s\'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        logger.addHandler(stream_handler)\n\n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict of floats in json file\n    Args:\n        d: (dict) of float-castable values (np.float, int, float, etc.)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        # We need to convert the values to float for json (it doesn\'t accept\n        # np.array, np.float, )\n        d = {k: float(v) for k, v in d.items()}\n        json.dump(d, f, indent=4)\n\n\ndef save_checkpoint(state, is_best, checkpoint):\n    """"""Saves model and training parameters at checkpoint + \'last.pth.tar\'.\n    If is_best==True, also saves checkpoint + \'best.pth.tar\'\n    Args:\n        state: (dict) contains model\'s state_dict, may contain other keys such\n            as epoch, optimizer state_dict \n        is_best: (bool) True if it is the best\n            model seen till now \n        checkpoint: (string) folder where parameters are\n            to be saved\n    """"""\n    filepath = os.path.join(checkpoint, \'last.pth.tar\')\n    if not os.path.exists(checkpoint):\n        print(""Checkpoint Directory does not exist! Making directory {}""\n              .format(checkpoint))\n        os.mkdir(checkpoint)\n    else:\n        print(""Checkpoint Directory exists! "")\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'best.pth.tar\'))\n\n\ndef load_checkpoint(checkpoint, model, optimizer=None):\n    """"""Loads model parameters (state_dict) from file_path. If optimizer is\n    provided, loads state_dict of optimizer assuming it is present in\n    checkpoint.\n    Args:\n        checkpoint: (string) filename which needs to be loaded\n        model: (torch.nn.Module) model for which the parameters are loaded\n        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n    """"""\n    if not os.path.exists(checkpoint):\n        raise FileNotFoundError(""File doesn\'t exist {}"".format(checkpoint))\n    checkpoint = torch.load(checkpoint) if torch.cuda.is_available() \\\n        else torch.load(checkpoint, map_location=\'cpu\')\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n    if optimizer:\n        optimizer.load_state_dict(checkpoint[\'optim_dict\'])\n\n    return checkpoint\n'"
utils/bbox_transforms.py,0,"b'import numpy as np\n\n\ndef region_to_bbox(region, center=True):\n    """"""\n    Transforms the ground-truth annotation to the convenient format. The\n    annotations come in different formats depending on the dataset of origin\n    (see README, --Dataset--, for details), some use 4 numbers and some use 8\n    numbers to describe the bounding boxes.\n    """"""\n    n = len(region)\n    assert n in [4, 8], (\'GT region format is invalid, should have 4 or 8 entries.\')\n\n    if n == 4:\n        return _rect(region, center)\n    else:\n        return _poly(region, center)\n\n\ndef _rect(region, center):\n    """"""\n    Calculate the center if center=True, otherwise the 4 number annotations\n    used in the TempleColor and VOT13 datasets are already in the correct\n    format.\n    (cx, cy) is the center and w, h are the width and height of the target\n    When center is False it returns region, which is a 4 tuple containing the\n    (x, y) coordinates of the LowerLeft corner and its width and height.\n    """"""\n    if center:\n        x = region[0]\n        y = region[1]\n        w = region[2]\n        h = region[3]\n        cx = x+w/2\n        cy = y+h/2\n        return cx, cy, w, h\n    else:\n        return region\n\n\ndef _poly(region, center):\n    """"""\n    Calculates the center, width and height of the bounding box when the\n    annotations are 8 number rotated bounding boxes (used in VOT14 and VOT16).\n    Since the Tracker does not try to estimate the rotation of the target, this\n    function returns a upright bounding box with the same center width and\n    height of the original one.\n    The 8 numbers correspond to the (x,y) coordinates of the each of the 4\n    corner points of the bounding box.\n    """"""\n    cx = np.mean(region[::2])\n    cy = np.mean(region[1::2])\n    x1 = np.min(region[::2])\n    x2 = np.max(region[::2])\n    y1 = np.min(region[1::2])\n    y2 = np.max(region[1::2])\n    A1 = np.linalg.norm(region[0:2] - region[2:4]) * np.linalg.norm(region[2:4] - region[4:6])\n    A2 = (x2 - x1) * (y2 - y1)\n    s = np.sqrt(A1/A2)\n    w = s * (x2 - x1) + 1\n    h = s * (y2 - y1) + 1\n\n    if center:\n        return cx, cy, w, h\n    else:\n        return cx-w/2, cy-h/2, w, h\n'"
utils/colormaps.py,2,"b'"""""" This file defines the functions that convert tensors of 2D single channel\nchannel images (2D tensors, or some collection of 2D tensors) into a collection\nof colored 2D RGB maps (3D tensors), for better visualization. Its main use case\nis to transform a score map, where the score is a 1 dimensional number for each\npixel of the image, into a colored image.\nIt defines the values of such mappings, based on https://bids.github.io/colormap/\nwhich defines colorblind-friendly 1D -> [RGB] mappings. This file was written\nto be used with tensorboardX, thus only uses pytorch operations, and can be run\non a gpu.\n""""""\nfrom .color_tables import VIRIDIS_cm as VIRIDIS\nfrom .color_tables import INFERNO_cm as INFERNO\n\n\ndef apply_cmap(tensor, dim=-3, cmap=\'inferno\', norm=\'per_channel\'):\n    """""" Applies a colormap to a collection of 2D score maps. The tensor of score\n    maps might be a single score map of shape [H, W], a collection of score maps\n    with multiple feature channels of shape [C, H, W] or a batch of collections\n    of shape [B, C, H, W]. The transformation consists of mapping the values\n    of each pixel to 256 different RGB values defined in one of COLOR_MAPS\n    color maps. The available colormaps are listed in the COLOR_MAPS dictionary.\n    The mapping takes into account the range of the data inside the tensor,\n    if you set the normalization to \'per_channel\' t\n\n    Args:\n        tensor: (torch.Tensor) The tensor containing the score maps, can have\n            different shapes, but the two last dimensions must be H and W.\n        dim: (int) The dimension of the final tensor where you wish to put\n            the RGB channels, by default it is the third dimension from the end,\n            making the final tensor something of shape [..., RGB_Ch, H, W]. It\n            supports python\'s negative indexing.\n        cmap: (str) The name of the particular colormap to be used. Default\n            value is \'inferno\', which is a blueish black to yellowish white\n            colormap.\n        norm: (str) The type of normalization to be used. \'per_channel\'\n            indicates that the min and max values are calculated in each individual\n            channel, while \'per_batch\' calculates the min and max values per batch.\n\n    Returns:\n        tensor: (torch.Tensor) The tensor containing the input score maps,\n            converted to colored maps based on their pixel values. It adds\n            one dimension of size 3 to the tensor (the 3 RGB channels), the\n            position of such dimension is defined by the user, but by default it\n            is the third to last dimension.\n    """"""\n    # Normalize\n    if norm == \'per_channel\' or len(tensor.shape) == 2:\n        # Performs channel-wise normalization\n        # First we have to squash the H and W dimensions to get the min and max\n        # in each 2D map. Then we subtract the min of each channel, and then\n        # we divide by the max of each channel (of the subtracted tensor).\n        *v_dims, H, W = tensor.shape\n        tensor = tensor - tensor.view(*v_dims, -1).min(-1)[0].view(*v_dims, 1, 1)\n        tensor = tensor/tensor.view(*v_dims, -1).max(-1)[0].view(*v_dims, 1, 1)\n    elif norm == \'per_batch\':\n        # Performs batch-wise normalization\n        *v_dims, C, H, W = tensor.shape\n        tensor = tensor - tensor.view(*v_dims, -1).min(-1)[0].view(*v_dims, 1, 1, 1)\n        tensor = tensor/tensor.view(*v_dims, -1).max(-1)[0].view(*v_dims, 1, 1, 1)\n    # Converts all the values to 0-255 integers to be used as indexes to the color map\n    tensor = 255*tensor\n    tensor.floor_()\n    tensor = tensor.long()\n    # Get the [256, 3] color map tensor\n    color_map = COLOR_MAPS[cmap]\n    if tensor.is_cuda:\n        color_map = color_map.cuda()\n    # Transform the tensor by using its values as indexes to the color map.\n    # This constructions might seem confusing, but essentially it accesses each\n    # element of tensor and uses its value as index to the color_map, returning\n    # a 3D vector (a RGB pixel) for each element of tensor. Thus the right side\n    # tensor has a size 3 dimension added to the original tensor shape.\n    tensor = color_map[tensor]\n    # The RGB channels dimension is added in the end of the tensor, so we must\n    # permute the dimensions to put the RGB dimension in the requested position.\n\n    # First off, in case the user uses a negative indexing we convert it to\n    # the positive equivalent\n    if dim < 0:\n        dim += len(tensor.shape)\n    # Then we generate a list of the original dimensions\n    permutation = list(range(len(tensor.shape)-1))\n    # And insert the last dimension (the RGB dim) in the requested position\n    permutation.insert(dim, -1)\n    # And we permute the tensors dimensions using this list. Note that the non-\n    # RGB dimensions are left unchanged.\n    tensor = tensor.permute(permutation)\n    return tensor\n\n\nCOLOR_MAPS = {\'viridis\': VIRIDIS, \'inferno\': INFERNO}\n'"
utils/crops.py,0,"b'import numpy as np\nfrom scipy.misc import imresize\n\n\ndef pad_frame(im, frame_sz, pos_x, pos_y, patch_sz, use_avg=True):\n    """""" Pads a frame equally on all sides, enough to fit a region of size\n    patch_sz centered in (pos_x, pos_y). If the region is already inside the\n    frame, it doesn\'t do anything.\n\n    Args:\n        im: (numpy.ndarray) The image to be padded.\n        frame_sz: (tuple) The width and height of the frame in pixels.\n        pos_x: (int) The x coordinate of the center of the target in the frame.\n        pos_y: (int) The y coordinate of the center of the target in the frame.\n        path_sz: (int) The size of the patch corresponding to the context\n            region around the bounding box.\n        use_avg: (bool) Indicates if we should pad with the mean value of the\n            pixels in the image (True) or zero (False).\n\n    Returns:\n        im_padded: (numpy.ndarray) The image after the padding.\n        npad: (int) the amount of padding applied\n\n    """"""\n    c = patch_sz / 2\n    xleft_pad = np.maximum(0, - np.round(pos_x - c))\n    ytop_pad = np.maximum(0, - np.round(pos_y - c))\n    xright_pad = np.maximum(0, np.round(pos_x + c) - frame_sz[1])\n    ybottom_pad = np.maximum(0, np.round(pos_y + c) - frame_sz[0])\n    npad = np.amax(np.asarray([xleft_pad, ytop_pad, xright_pad, ybottom_pad]))\n    npad = np.int32(np.round(npad))\n    paddings = ((npad, npad), (npad,npad), (0,0))\n    if use_avg:\n        im0 = np.pad(im[:, :, 0], paddings[0:2], mode=\'constant\',\n                     constant_values=im[:, :, 0].mean())\n        im1 = np.pad(im[:, :, 1], paddings[0:2], mode=\'constant\',\n                     constant_values=im[:, :, 1].mean())\n        im2 = np.pad(im[:, :, 2], paddings[0:2], mode=\'constant\',\n                     constant_values=im[:, :, 2].mean())\n        im_padded = np.stack([im0,im1,im2], axis=2)\n    else:\n        im_padded = np.pad(im, paddings, mode=\'constant\')\n    return im_padded, npad\n\n\ndef extract_crops_z(im, npad, pos_x, pos_y, sz_src, sz_dst):\n    """""" Extracts the reference patch from the image.\n\n    Args:\n        im: (numpy.ndarray) The padded image.\n        npad: (int) The amount of padding added to each side.\n        pos_x: (int) The x coordinate of the center of the reference in the\n            original frame, not considering the padding.\n\n        pos_y: (int) The y coordinate of the center of the reference in the\n            original frame, not considering the padding.\n        sz_src: (int) The original size of the reference patch.\n        sz_dst: (int) The final size of the patch (usually 127)\n    Returns:\n        crop: (numpy.ndarray) The cropped image containing the reference with its\n            context region.\n    """"""\n    dist_to_side = sz_src / 2\n    # get top-left corner of bbox and consider padding\n    tf_x = np.int32(npad + np.round(pos_x - dist_to_side))\n    # Compute size from rounded co-ords to ensure rectangle lies inside padding.\n    tf_y = np.int32(npad + np.round(pos_y - dist_to_side))\n    width = np.int32(np.round(pos_x + dist_to_side) - np.round(pos_x - dist_to_side))\n    height = np.int32(np.round(pos_y + dist_to_side) - np.round(pos_y - dist_to_side))\n    crop = im[tf_y:(tf_y + height), tf_x:(tf_x + width), :]\n    crop = imresize(crop, (sz_dst, sz_dst), interp=\'bilinear\')\n\n    # TODO Add Batch dimension\n    # crops = np.stack([crop, crop, crop])\n    return crop\n\n\ndef extract_crops_x(im, npad, pos_x, pos_y, sz_src0, sz_src1, sz_src2, sz_dst):\n    """""" Extracts the 3 scaled crops of the search patch from the image.\n\n    Args:\n        im: (numpy.ndarray) The padded image.\n        npad: (int) The amount of padding added to each side.\n        pos_x: (int) The x coordinate of the center of the bounding box in the\n            original frame, not considering the padding.\n\n        pos_y: (int) The y coordinate of the center of the bounding box in the\n            original frame, not considering the padding.\n        sz_src0: (int) The downscaled size of the search region.\n        sz_src1: (int) The original size of the search region.\n        sz_src2: (int) The upscaled size of the search region.\n        sz_dst: (int) The final size for each crop (usually 255)\n    Returns:\n        crops: (numpy.ndarray) The 3 cropped images containing the search region\n        in 3 different scales.\n    """"""\n    # take center of the biggest scaled source patch\n    dist_to_side = sz_src2 / 2\n    # get top-left corner of bbox and consider padding\n    tf_x = npad + np.int32(np.round(pos_x - dist_to_side))\n    tf_y = npad + np.int32(np.round(pos_y - dist_to_side))\n    # Compute size from rounded co-ords to ensure rectangle lies inside padding.\n    width = np.int32(np.round(pos_x + dist_to_side) - np.round(pos_x - dist_to_side))\n    height = np.int32(np.round(pos_y + dist_to_side) - np.round(pos_y - dist_to_side))\n    search_area = im[tf_y:(tf_y + height), tf_x:(tf_x + width), :]\n\n    # TODO: Use computed width and height here?\n    offset_s0 = (sz_src2 - sz_src0) / 2\n    offset_s1 = (sz_src2 - sz_src1) / 2\n\n    crop_s0 = search_area[np.int32(offset_s0):np.int32(offset_s0 + sz_src0),\n                          np.int32(offset_s0):np.int32(offset_s0 + sz_src0), :]\n    crop_s0 = imresize(crop_s0, (sz_dst, sz_dst), interp=\'bilinear\')\n\n    crop_s1 = search_area[np.int32(offset_s1):np.int32(offset_s1 + sz_src1),\n                          np.int32(offset_s1):np.int32(offset_s1 + sz_src1), :]\n    crop_s1 = imresize(crop_s1, (sz_dst, sz_dst), interp=\'bilinear\')\n\n    crop_s2 = imresize(search_area, (sz_dst, sz_dst), interp=\'bilinear\')\n\n    crops = np.stack([crop_s0, crop_s1, crop_s2])\n    return crops\n'"
utils/exceptions.py,0,"b'class IncompatibleFolderStructure(Exception):\n    pass\n\n\nclass IncompatibleImagenetStructure(IncompatibleFolderStructure):\n    def __init__(self, msg=None):\n        info = (""\\n""\n                ""The given root directory does not conform with the expected ""\n                ""folder structure. It should have the following structure:\\n""\n                ""Imagenet/\\n""\n                ""\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Annotations\\n""\n                ""\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 VID\\n""\n                ""\xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\\n""\n                ""\xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 val\\n""\n                ""\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 Data\\n""\n                "" \xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 VID\\n""\n                "" \xc2\xa0\xc2\xa0     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\\n""\n                "" \xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 val\\n"")\n        if msg is not None:\n            info = info + msg\n        super().__init__(info)\n\n\nclass InvalidOption(Exception):\n    pass\n\n\nclass IncompleteArgument(Exception):\n    pass'"
utils/image_utils.py,0,"b'"""""" This module exposes the image processing methods as module level global\nvariables, that are loaded based on user defined flags.\n\nSince there are multiple different implementations and libraries available for each\nof the image tasks we want perform (e.g \'load jpeg\' and \'resize\'), we give the\nuser the choice of using different versions of each function without changing\nthe rest of the code. One example in which one would use a \'safe\' flag is if\nthere are non jpeg images in the dataset, as imageio.imread can read them,\nbut not jpeg4py.\n\nThe basic changes introduced here are the libjpeg-turbo which is a instruction\nlevel parallel (SSE, AVX, etc) implementation of the default C++ jpeg lib\nlibjpeg. Morover we also switched the default Pillow for a fork called PIL-SIMD,\nwhich is identical in terms of the API but uses vectorized instructions as well\nas heavy loop unrolling to optimize some of Pillows functions (particularly the\nresize function). Since the PIL-SIMD is a drop-in replacement for Pillow, even\nif the SIMD version is not installed the program should work just as fine with\nthe default Pillow (without the massive speed gains, though).\n""""""\nimport numpy as np\nfrom scipy.misc import imresize\nimport PIL\nfrom PIL import Image\ntry:\n    import jpeg4py as jpeg\nexcept (KeyboardInterrupt, EOFError):\n    raise\nexcept Exception as e:\n    print(\'[IMAGE-UTILS] package jpeg4py not available. Continuing...\')\n    LIBJPEG_TURBO_PRESENT = False\nelse:\n    LIBJPEG_TURBO_PRESENT = True\n\n\nfrom utils.exceptions import InvalidOption\n\nVALID_FLAGS = [\'fast\', \'safe\']\nPIL_FLAGS = {\'bilinear\': PIL.Image.BILINEAR, \'bicubic\': PIL.Image.BICUBIC,\n             \'nearest\': PIL.Image.NEAREST}\n\n\ndef decode_jpeg_fast(img_path):\n    """""" Jpeg decoding method implemented by jpeg4py, available in\n    https://github.com/ajkxyz/jpeg4py . This library binds the libjpeg-turbo\n    C++ library (available in\n    https://github.com/libjpeg-turbo/libjpeg-turbo/blob/master/BUILDING.md),\n    and can be up to 9 times faster than the non SIMD implementation.\n    Requires libjpeg-turbo, built and installed correctly.\n    """"""\n    return jpeg.JPEG(img_path).decode()\n\n\ndef get_decode_jpeg_fcn(flag=\'fast\'):\n    """""" Yields the function demanded by the user based on the flags given and\n    the the system responses to imports. If the demanded function is not\n    available an exception is raised and the user is informed it should try\n    using another flag.\n    """"""\n    if flag == \'fast\':\n        assert LIBJPEG_TURBO_PRESENT, (\'[IMAGE-UTILS] Error: It seems that the \'\n                                       \'used image utils flag is not available,\'\n                                       \' try setting the flag to \\\'safe\\\'.\')\n        decode_jpeg_fcn = decode_jpeg_fast\n    elif flag == \'safe\':\n        from imageio import imread\n        decode_jpeg_fcn = imread\n    else:\n        raise InvalidOption(\'The informed flag: {}, is not valid. Valid flags \'\n                            ""include: {}"".format(flag, VALID_FLAGS[\'decode_jpeg\']))\n\n    return decode_jpeg_fcn\n\n\ndef resize_fast(img, size_tup, interp=\'bilinear\'):\n    """""" Implements the PIL resize method from a numpy image input, using the\n    same interface as the scipy imresize method.\n    OBS: if concerned with the resizing of the correlation score map, the\n    default behavior of the PIL resize is to align the corners, so we can\n    simply use resize(img, (129,129)) without any problems. The center pixel\n    is kept in the center (at least for the 4x upscale) and the corner pixels\n    aligned.\n\n    Args:\n        img: (numpy.ndarray) A numpy RGB image.\n        size_tup: (tuple) A 2D tuple containing the height and weight of the\n            resized image.\n        interp: (str) The flag indicating the interpolation method. Available\n            methods include \'bilinear\', \'bicubic\' and \'nearest\'.\n    Returns:\n        img_res: (numpy.ndarray) The resized image\n    """"""\n    # The order of the size tuple is inverted in PIL compared to scipy\n    size_tup = (size_tup[1], size_tup[0])\n    original_type = img.dtype\n    img_res = Image.fromarray(img.astype(\'uint8\', copy=False), \'RGB\')\n    img_res = img_res.resize(size_tup, PIL_FLAGS[interp])\n    img_res = np.asarray(img_res)\n    img_res = img_res.astype(original_type)\n    return img_res\n\n\ndef get_resize_fcn(flag=\'fast\'):\n    """"""Yields the resize function demanded by the user based on the flags given\n    and the the system responses to imports. If the demanded function is not\n    available an exception is raised and the user is informed it should try\n    using another flag.\n    """"""\n    if flag == \'fast\':\n        return resize_fast\n    elif flag == \'safe\':\n        return imresize\n    else:\n        raise InvalidOption(\'The informed flag: {}, is not valid. Valid flags \'\n                            ""include: {}"".format(flag, VALID_FLAGS[\'resize\']))\n'"
utils/load_baseline.py,15,"b'"""""" This script creates a pytorch weight file based on the file containing the\nparameters of the matconvnet implementation. It has been written with the file\nbaseline-conv5_e55.mat (available in the author\'s page\nhttp://www.robots.ox.ac.uk/~luca/cfnet.html) in mind, and has some of its\nspecific parameters hard coded, such as the number of layers and the composition\nof each layer.\n""""""\nimport argparse\nimport os\n\nimport torch\nimport torch.optim as optim\nfrom scipy import io\nimport numpy as np\n\nimport training.models as mdl\n\n# These dictionaries store the correspondence between the layers of the matlab\n# net and the pytorch one. The parameters in the .mat file are saved with names\n# corresponding to the layers, e.g.: \'br_conv1f\' and \'br_conv1b\' contains the\n# the weights and biases of the first convolutional layer.\nconvs = {\n    \'br_conv1\': 0,\n    \'br_conv2\': 4,\n    \'br_conv3\': 8,\n    \'br_conv4\': 11,\n    \'br_conv5\': 14\n}\nbnorms = {\n    \'br_bn1\': 1,\n    \'br_bn2\': 5,\n    \'br_bn3\': 9,\n    \'br_bn4\': 12,\n    \'fin_adjust_bn\': \'match\'\n}\n\n\ndef _import_from_matconvnet(net_path):\n    """""" Imports the data from the .mat file containing the parameters of the\n    network. The organization of the imported .mat file is based on the\n    definition of the original matlab object, thus the data is nested in various\n    dictionaries and arrays, and is not very pleasing to look at.\n    The basic nesting organization of the file is:\n        mat_file\n        \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 net\n            \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 vars\n            \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 params\n            |   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 (useless [1 1] numpy nesting)\n            |       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 (useless [1 1] numpy nesting)\n            |           \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 name\n            |           |    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 (useless [1] numpy nesting)\n            |           |        \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 The actual name\n            |           \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 value\n            |                \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 (useless [1] numpy nesting)\n            |                    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 The actual value\n            \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 layers\n            \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 meta\n    Args:\n        net_path: (string) The path to the .mat parameter file.\n\n    Returns:\n        params_names_list: (list) The list containing the names of all the\n            networks layers.\n        params_values_list: (list) The list containing the parameters of all the\n            networks layers.\n    """"""\n    mat = io.loadmat(net_path)\n    net_dot_mat = mat.get(\'net\')\n    # organize parameters to import\n    params = net_dot_mat[\'params\']\n    params = params[0][0]\n    params_names = params[\'name\'][0]\n    params_names_list = [params_names[p][0] for p in range(params_names.size)]\n    params_values = params[\'value\'][0]\n    params_values_list = [params_values[p] for p in range(params_values.size)]\n    return params_names_list, params_values_list\n\n\ndef mat2pyt_conv(W, b):\n    """""" Converts the parameters of a convolutional layer from the MatConvNet\n    format to PyTorch Tensors with the appropriate order. The weights of the\n    convolutional layers are stored in the following order:\n        MatConvNet = [H, W, in_ch, out_ch]\n        PyTorch = [out_ch, in_ch, H, W]\n    Moreover the biases are saved in a nested array, so the dimensions when\n    reading are [n 1] instead of [n].\n\n    Args:\n        W: (numpy.ndarray) The weights of a convolutional layer.\n        b: (numpy.ndarray) The biases of the same layer.\n\n    Returns:\n        W: (torch.nn.Parameter) The weights as a torch parameter.\n        b: (torch.nn.Parameter) The biases as a torch parameter.\n\n    """"""\n    W = np.transpose(W, (3, 2, 0, 1))\n    W = torch.from_numpy(W).float()\n    b = torch.from_numpy(b).float()\n    b = b.view(torch.numel(b))\n    W = torch.nn.Parameter(W)\n    b = torch.nn.Parameter(b)\n    return W, b\n\n\ndef mat2pyt_bn(param):\n    """""" Converts the batchnorm parameters (one at a time) into pytorch format.\n\n    Args:\n        param: (numpy.ndarray) One of the batchnorm parameters.\n\n    Returns:\n        param: (torch.nn.Parameter) The corresponding parameter in pytorch format.\n    """"""\n    param = torch.from_numpy(param).float()\n    param = torch.nn.Parameter(param)\n    param = param.view(torch.numel(param))\n    return param\n\n\ndef load_baseline(net_path, pyt_net):\n    """"""Loads the baseline weights into a pytorch SiamFC object.\n\n    Args:\n        net_path: (str) The path to the baseline file. So far it only works for\n            the baseline-conv5_e55.mat file.\n        pyt_net: (torch.nn.Module) The pytorch module object of the SiamFC net,\n            normally created as models.SiameseNet(models.BaselineEmbeddingNet())\n    Returns:\n        pyt_net: (torch.nn.Module) The network with the baseline weights loaded.\n    """"""\n    # read mat file from net_path and start TF Siamese graph from placeholders X and Z\n    params_names_list, params_values_list = _import_from_matconvnet(net_path)\n    for conv_name, pyt_layer in convs.items():\n        conv_W_name = conv_name + \'f\'\n        conv_b_name = conv_name + \'b\'\n        conv_W = params_values_list[params_names_list.index(conv_W_name)]\n        conv_b = params_values_list[params_names_list.index(conv_b_name)]\n        W, b = mat2pyt_conv(conv_W, conv_b)\n        # The original Network has been trained with RGB images with pixels\n        # in the range 0-255, in order to use this network with our 0-1 range\n        # images we need to multiply the first layer weights by 255.\n        if conv_W_name == \'br_conv1f\':\n            W = 255 * W\n            converted_weights = True\n        conv_dict = {\'weight\': W, \'bias\': b}\n        pyt_net.embedding_net.fully_conv[pyt_layer].load_state_dict(conv_dict)\n\n    # Sanity-check\n    assert converted_weights, (""The weights of the first layer have not been ""\n                               ""multiplied by 255. 0-1 range images won\'t work"")\n\n    for bn_name, pyt_layer in bnorms.items():\n        bn_beta_name = bn_name + \'b\'\n        bn_gamma_name = bn_name + \'m\'\n        bn_moments_name = bn_name + \'x\'\n        bn_beta = params_values_list[params_names_list.index(bn_beta_name)]\n        bn_gamma = params_values_list[params_names_list.index(bn_gamma_name)]\n        bn_moments = params_values_list[params_names_list.index(bn_moments_name)]\n        bn_moving_mean = bn_moments[:, 0]\n        bn_moving_variance = bn_moments[:, 1]**2  # saved as std in matconvnet\n        beta = mat2pyt_bn(bn_beta)\n        gamma = mat2pyt_bn(bn_gamma)\n        mean = mat2pyt_bn(bn_moving_mean)\n        var = mat2pyt_bn(bn_moving_variance)\n        bn_dict = {\'bias\': beta, \'weight\': gamma, \'running_mean\': mean,\n                   \'running_var\': var}\n        if pyt_layer != \'match\':\n            pyt_net.embedding_net.fully_conv[pyt_layer].load_state_dict(bn_dict)\n        else:\n            pyt_net.match_batchnorm.load_state_dict(bn_dict)\n\n    return pyt_net\n\n\ndef export_to_checkpoint(state, checkpoint):\n    """""" Saves the network as a checkpoint file in the given path.\n    Args:\n        state: (dict) contains model\'s state_dict, may contain other keys such\n        as epoch, optimizer state_dict\n        checkpoint: (string) folder where parameters are to be saved\n    """"""\n    filepath = os.path.join(checkpoint, \'baseline_pretrained.pth.tar\')\n    torch.save(state, filepath)\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description=""Load Baseline script"")\n    parser.add_argument(\'-n\', \'--net_file_path\',\n                        default=\'baseline-conv5_e55.mat\',\n                        help=""The path to the .mat file containing ""\n                             ""the networks weights."")\n    parser.add_argument(\'-d\', \'--dst_path\',\n                        default=\'training/experiments/default\',\n                        help=""The destination path on which ""\n                             ""the pretrained model will be saved."")\n    args = parser.parse_args()\n    return args\n\n\ndef main(args):\n    """""" Execute this file as a script and it will save the baseline model in\n    the default experiment folder.\n    """"""\n    net_path = args.net_file_path\n    net = mdl.SiameseNet(mdl.BaselineEmbeddingNet())\n    net = load_baseline(net_path, net)\n    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n    export_to_checkpoint({\'epoch\': 0,\n                          \'state_dict\': net.state_dict(),\n                          \'optim_dict\': optimizer.state_dict()},\n                         checkpoint=args.dst_path)\n\n\nif __name__ == \'__main__\':\n    args_ = parse_arguments()\n    main(args_)\n'"
utils/profiling.py,0,"b'import time\n\n\n# TODO have a option that converts time into minutes and hours as needed.\nclass Timer(object):\n    def __init__(self, print_at_exit=False, convert=False):\n        self.print_at_exit = print_at_exit\n        self.convert = convert\n        self.exited = False\n        self.start_time = None\n        self.end_time = None\n\n    def __enter__(self):\n        self.start_time = time.time()\n        return self\n\n    def __exit__(self, *args):\n        self.end_time = time.time()\n        if self.print_at_exit:\n            print(self)\n\n    def reset(self):\n        self.start_time = time.time()\n\n    def __repr__(self):\n        return ""<{} elapsed={}>"".format(self.__class__.__name__,\n                                        self.elapsed)\n\n    @property\n    def elapsed(self):\n        # If called inside the context manager it calculates the partial elapsed\n        # time as well.\n        if not self.exited:\n            self.end_time = time.time()\n        elap = self.end_time - self.start_time\n        if self.convert:\n            sec = (elap) % 60\n            minut = (elap // 60) % 60\n            hour = elap // 3600\n            return ""{:.0f}h{:.0f}m{:.2f}s"".format(hour, minut, sec)\n        else:\n            return elap\n\n\ndef _main():\n    with Timer() as tim:\n        for i in range(3):\n            print(tim.elapsed)\n            time.sleep(1)\n            for j in range(3):\n                print(tim.elapsed)\n                time.sleep(0.5)\n\n\nif __name__ == ""__main__"":\n    _main()\n'"
utils/tensor_conv.py,4,"b'"""""" This module contains the functions used to convert tensors from Pytorch to\nnumpy objects and vice-versa.\n""""""\nimport numpy as np\nimport torch\n\n\ndef torch_var_to_numpy(var):\n    """""" Converts Tensor from a Pytorch tensor to a numpy array.\n\n    Args:\n        var: (torch.Tensor) A Pytorch tensor.\n\n    Returns:\n        np_tensor: (numpy.ndarray) The same tensor as a numpy array.\n    """"""\n    np_tensor = var.detach().cpu().numpy()\n    np_tensor = np.transpose(np_tensor, (0, 2, 3, 1))\n    np_tensor = np_tensor.squeeze()\n    return np_tensor\n\n\ndef numpy_to_torch_var(np_tensor, device):\n    """""" Converts Tensor from a numpy array to a Pytorch tensor.\n\n    Args:\n        np_tensor: (numpy.ndarray) The same tensor as a numpy array.\n\n    Returns:\n        var: (torch.Tensor) A Pytorch tensor.\n    """"""\n    if len(np_tensor.shape) == 3:\n        np_tensor = np.expand_dims(np_tensor, axis=0)\n    var = np.transpose(np_tensor, (0, 3, 1, 2))\n    var = torch.from_numpy(var).float()\n    var = var.to(device)\n    return var\n\n\nclass ToTensorWithoutScaling(object):\n    """""" This function implements the ToTensor class, without scaling the pixel\n    values to the [0,1] range.\n    H x W x C -> C x H x W\n    """"""\n\n    def __call__(self, picture):\n        return torch.FloatTensor(np.array(picture)).permute(2, 0, 1)'"
utils/visualization.py,0,"b'import os\nimport numpy as np\nimport matplotlib\nif os.name == \'posix\' and ""DISPLAY"" not in os.environ:\n    matplotlib.use(\'Agg\')\n    import matplotlib.pyplot as plt\nelse:\n    import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\ndef show_frame(frame, bbox, fig_n, pause=2):\n    plt.ion()\n    plt.clf()\n    fig = plt.figure(fig_n)\n    ax = fig.gca()\n    r = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor=\'r\', fill=False)\n    ax.imshow(np.uint8(frame))\n    ax.add_patch(r)\n    fig.show()\n    fig.canvas.draw()\n    plt.pause(pause)\n\n\ndef show_frame_and_response_map(frame, bbox, fig_n, crop_x, score, pause=2):\n    fig = plt.figure(fig_n)\n    ax = fig.add_subplot(131)\n    ax.set_title(\'Tracked sequence\')\n    r = patches.Rectangle((bbox[0],bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor=\'r\', fill=False)\n    ax.imshow(np.uint8(frame))\n    ax.add_patch(r)\n    ax2 = fig.add_subplot(132)\n    ax2.set_title(\'Context region\')\n    ax2.imshow(np.uint8(crop_x))\n    ax2.spines[\'left\'].set_position(\'center\')\n    ax2.spines[\'right\'].set_color(\'none\')\n    ax2.spines[\'bottom\'].set_position(\'center\')\n    ax2.spines[\'top\'].set_color(\'none\')\n    ax2.set_yticklabels([])\n    ax2.set_xticklabels([])\n    ax3 = fig.add_subplot(133)\n    ax3.set_title(\'Response map\')\n    ax3.spines[\'left\'].set_position(\'center\')\n    ax3.spines[\'right\'].set_color(\'none\')\n    ax3.spines[\'bottom\'].set_position(\'center\')\n    ax3.spines[\'top\'].set_color(\'none\')\n    ax3.set_yticklabels([])\n    ax3.set_xticklabels([])\n    ax3.imshow(np.uint8(score))\n\n    plt.ion()\n    plt.show()\n    plt.pause(pause)\n    plt.clf()\n\n\ndef save_frame_and_response_map(frame, bbox, fig_n, crop_x, score, writer, fig):\n    # fig = plt.figure(fig_n)\n    plt.clf()\n    ax = fig.add_subplot(131)\n    ax.set_title(\'Tracked sequence\')\n    r = patches.Rectangle((bbox[0],bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor=\'r\', fill=False)\n    ax.imshow(np.uint8(frame))\n    ax.add_patch(r)\n    ax2 = fig.add_subplot(132)\n    ax2.set_title(\'Context region\')\n    ax2.imshow(np.uint8(crop_x))\n    ax2.spines[\'left\'].set_position(\'center\')\n    ax2.spines[\'right\'].set_color(\'none\')\n    ax2.spines[\'bottom\'].set_position(\'center\')\n    ax2.spines[\'top\'].set_color(\'none\')\n    ax2.set_yticklabels([])\n    ax2.set_xticklabels([])\n    ax3 = fig.add_subplot(133)\n    ax3.set_title(\'Response map\')\n    ax3.spines[\'left\'].set_position(\'center\')\n    ax3.spines[\'right\'].set_color(\'none\')\n    ax3.spines[\'bottom\'].set_position(\'center\')\n    ax3.spines[\'top\'].set_color(\'none\')\n    ax3.set_yticklabels([])\n    ax3.set_xticklabels([])\n    ax3.imshow(np.uint8(score))\n\n    # ax3.grid()\n    writer.grab_frame()\n\n\ndef show_crops(crops, fig_n):\n    fig = plt.figure(fig_n)\n    ax1 = fig.add_subplot(131)\n    ax2 = fig.add_subplot(132)\n    ax3 = fig.add_subplot(133)\n    ax1.imshow(np.uint8(crops[0,:,:,:]))\n    ax2.imshow(np.uint8(crops[1,:,:,:]))\n    ax3.imshow(np.uint8(crops[2,:,:,:]))\n    plt.ion()\n    plt.show()\n    plt.pause(0.001)\n\n\ndef show_scores(scores, fig_n):\n    fig = plt.figure(fig_n)\n    ax1 = fig.add_subplot(131)\n    ax2 = fig.add_subplot(132)\n    ax3 = fig.add_subplot(133)\n    ax1.imshow(scores[0,:,:], interpolation=\'none\', cmap=\'hot\')\n    ax2.imshow(scores[1,:,:], interpolation=\'none\', cmap=\'hot\')\n    ax3.imshow(scores[2,:,:], interpolation=\'none\', cmap=\'hot\')\n    plt.ion()\n    plt.show()\n    plt.pause(0.001)'"
utils/color_tables/__init__.py,0,b'from .viridis import VIRIDIS_cm\nfrom .inferno import INFERNO_cm'
utils/color_tables/inferno.py,1,"b'import torch\n\n# Inferno is a blueish black to yellowish white color map.\nINFERNO_cm = torch.Tensor(\n    [[1.46159096e-03, 4.66127766e-04, 1.38655200e-02],\n     [2.26726368e-03, 1.26992553e-03, 1.85703520e-02],\n     [3.29899092e-03, 2.24934863e-03, 2.42390508e-02],\n     [4.54690615e-03, 3.39180156e-03, 3.09092475e-02],\n     [6.00552565e-03, 4.69194561e-03, 3.85578980e-02],\n     [7.67578856e-03, 6.13611626e-03, 4.68360336e-02],\n     [9.56051094e-03, 7.71344131e-03, 5.51430756e-02],\n     [1.16634769e-02, 9.41675403e-03, 6.34598080e-02],\n     [1.39950388e-02, 1.12247138e-02, 7.18616890e-02],\n     [1.65605595e-02, 1.31362262e-02, 8.02817951e-02],\n     [1.93732295e-02, 1.51325789e-02, 8.87668094e-02],\n     [2.24468865e-02, 1.71991484e-02, 9.73274383e-02],\n     [2.57927373e-02, 1.93306298e-02, 1.05929835e-01],\n     [2.94324251e-02, 2.15030771e-02, 1.14621328e-01],\n     [3.33852235e-02, 2.37024271e-02, 1.23397286e-01],\n     [3.76684211e-02, 2.59207864e-02, 1.32232108e-01],\n     [4.22525554e-02, 2.81385015e-02, 1.41140519e-01],\n     [4.69146287e-02, 3.03236129e-02, 1.50163867e-01],\n     [5.16437624e-02, 3.24736172e-02, 1.59254277e-01],\n     [5.64491009e-02, 3.45691867e-02, 1.68413539e-01],\n     [6.13397200e-02, 3.65900213e-02, 1.77642172e-01],\n     [6.63312620e-02, 3.85036268e-02, 1.86961588e-01],\n     [7.14289181e-02, 4.02939095e-02, 1.96353558e-01],\n     [7.66367560e-02, 4.19053329e-02, 2.05798788e-01],\n     [8.19620773e-02, 4.33278666e-02, 2.15289113e-01],\n     [8.74113897e-02, 4.45561662e-02, 2.24813479e-01],\n     [9.29901526e-02, 4.55829503e-02, 2.34357604e-01],\n     [9.87024972e-02, 4.64018731e-02, 2.43903700e-01],\n     [1.04550936e-01, 4.70080541e-02, 2.53430300e-01],\n     [1.10536084e-01, 4.73986708e-02, 2.62912235e-01],\n     [1.16656423e-01, 4.75735920e-02, 2.72320803e-01],\n     [1.22908126e-01, 4.75360183e-02, 2.81624170e-01],\n     [1.29284984e-01, 4.72930838e-02, 2.90788012e-01],\n     [1.35778450e-01, 4.68563678e-02, 2.99776404e-01],\n     [1.42377819e-01, 4.62422566e-02, 3.08552910e-01],\n     [1.49072957e-01, 4.54676444e-02, 3.17085139e-01],\n     [1.55849711e-01, 4.45588056e-02, 3.25338414e-01],\n     [1.62688939e-01, 4.35542881e-02, 3.33276678e-01],\n     [1.69575148e-01, 4.24893149e-02, 3.40874188e-01],\n     [1.76493202e-01, 4.14017089e-02, 3.48110606e-01],\n     [1.83428775e-01, 4.03288858e-02, 3.54971391e-01],\n     [1.90367453e-01, 3.93088888e-02, 3.61446945e-01],\n     [1.97297425e-01, 3.84001825e-02, 3.67534629e-01],\n     [2.04209298e-01, 3.76322609e-02, 3.73237557e-01],\n     [2.11095463e-01, 3.70296488e-02, 3.78563264e-01],\n     [2.17948648e-01, 3.66146049e-02, 3.83522415e-01],\n     [2.24762908e-01, 3.64049901e-02, 3.88128944e-01],\n     [2.31538148e-01, 3.64052511e-02, 3.92400150e-01],\n     [2.38272961e-01, 3.66209949e-02, 3.96353388e-01],\n     [2.44966911e-01, 3.70545017e-02, 4.00006615e-01],\n     [2.51620354e-01, 3.77052832e-02, 4.03377897e-01],\n     [2.58234265e-01, 3.85706153e-02, 4.06485031e-01],\n     [2.64809649e-01, 3.96468666e-02, 4.09345373e-01],\n     [2.71346664e-01, 4.09215821e-02, 4.11976086e-01],\n     [2.77849829e-01, 4.23528741e-02, 4.14392106e-01],\n     [2.84321318e-01, 4.39325787e-02, 4.16607861e-01],\n     [2.90763373e-01, 4.56437598e-02, 4.18636756e-01],\n     [2.97178251e-01, 4.74700293e-02, 4.20491164e-01],\n     [3.03568182e-01, 4.93958927e-02, 4.22182449e-01],\n     [3.09935342e-01, 5.14069729e-02, 4.23720999e-01],\n     [3.16281835e-01, 5.34901321e-02, 4.25116277e-01],\n     [3.22609671e-01, 5.56335178e-02, 4.26376869e-01],\n     [3.28920763e-01, 5.78265505e-02, 4.27510546e-01],\n     [3.35216916e-01, 6.00598734e-02, 4.28524320e-01],\n     [3.41499828e-01, 6.23252772e-02, 4.29424503e-01],\n     [3.47771086e-01, 6.46156100e-02, 4.30216765e-01],\n     [3.54032169e-01, 6.69246832e-02, 4.30906186e-01],\n     [3.60284449e-01, 6.92471753e-02, 4.31497309e-01],\n     [3.66529195e-01, 7.15785403e-02, 4.31994185e-01],\n     [3.72767575e-01, 7.39149211e-02, 4.32400419e-01],\n     [3.79000659e-01, 7.62530701e-02, 4.32719214e-01],\n     [3.85228383e-01, 7.85914864e-02, 4.32954973e-01],\n     [3.91452659e-01, 8.09267058e-02, 4.33108763e-01],\n     [3.97674379e-01, 8.32568129e-02, 4.33182647e-01],\n     [4.03894278e-01, 8.55803445e-02, 4.33178526e-01],\n     [4.10113015e-01, 8.78961593e-02, 4.33098056e-01],\n     [4.16331169e-01, 9.02033992e-02, 4.32942678e-01],\n     [4.22549249e-01, 9.25014543e-02, 4.32713635e-01],\n     [4.28767696e-01, 9.47899342e-02, 4.32411996e-01],\n     [4.34986885e-01, 9.70686417e-02, 4.32038673e-01],\n     [4.41207124e-01, 9.93375510e-02, 4.31594438e-01],\n     [4.47428382e-01, 1.01597079e-01, 4.31080497e-01],\n     [4.53650614e-01, 1.03847716e-01, 4.30497898e-01],\n     [4.59874623e-01, 1.06089165e-01, 4.29845789e-01],\n     [4.66100494e-01, 1.08321923e-01, 4.29124507e-01],\n     [4.72328255e-01, 1.10546584e-01, 4.28334320e-01],\n     [4.78557889e-01, 1.12763831e-01, 4.27475431e-01],\n     [4.84789325e-01, 1.14974430e-01, 4.26547991e-01],\n     [4.91022448e-01, 1.17179219e-01, 4.25552106e-01],\n     [4.97257069e-01, 1.19379132e-01, 4.24487908e-01],\n     [5.03492698e-01, 1.21575414e-01, 4.23356110e-01],\n     [5.09729541e-01, 1.23768654e-01, 4.22155676e-01],\n     [5.15967304e-01, 1.25959947e-01, 4.20886594e-01],\n     [5.22205646e-01, 1.28150439e-01, 4.19548848e-01],\n     [5.28444192e-01, 1.30341324e-01, 4.18142411e-01],\n     [5.34682523e-01, 1.32533845e-01, 4.16667258e-01],\n     [5.40920186e-01, 1.34729286e-01, 4.15123366e-01],\n     [5.47156706e-01, 1.36928959e-01, 4.13510662e-01],\n     [5.53391649e-01, 1.39134147e-01, 4.11828882e-01],\n     [5.59624442e-01, 1.41346265e-01, 4.10078028e-01],\n     [5.65854477e-01, 1.43566769e-01, 4.08258132e-01],\n     [5.72081108e-01, 1.45797150e-01, 4.06369246e-01],\n     [5.78303656e-01, 1.48038934e-01, 4.04411444e-01],\n     [5.84521407e-01, 1.50293679e-01, 4.02384829e-01],\n     [5.90733615e-01, 1.52562977e-01, 4.00289528e-01],\n     [5.96939751e-01, 1.54848232e-01, 3.98124897e-01],\n     [6.03138930e-01, 1.57151161e-01, 3.95891308e-01],\n     [6.09330184e-01, 1.59473549e-01, 3.93589349e-01],\n     [6.15512627e-01, 1.61817111e-01, 3.91219295e-01],\n     [6.21685340e-01, 1.64183582e-01, 3.88781456e-01],\n     [6.27847374e-01, 1.66574724e-01, 3.86276180e-01],\n     [6.33997746e-01, 1.68992314e-01, 3.83703854e-01],\n     [6.40135447e-01, 1.71438150e-01, 3.81064906e-01],\n     [6.46259648e-01, 1.73913876e-01, 3.78358969e-01],\n     [6.52369348e-01, 1.76421271e-01, 3.75586209e-01],\n     [6.58463166e-01, 1.78962399e-01, 3.72748214e-01],\n     [6.64539964e-01, 1.81539111e-01, 3.69845599e-01],\n     [6.70598572e-01, 1.84153268e-01, 3.66879025e-01],\n     [6.76637795e-01, 1.86806728e-01, 3.63849195e-01],\n     [6.82656407e-01, 1.89501352e-01, 3.60756856e-01],\n     [6.88653158e-01, 1.92238994e-01, 3.57602797e-01],\n     [6.94626769e-01, 1.95021500e-01, 3.54387853e-01],\n     [7.00575937e-01, 1.97850703e-01, 3.51112900e-01],\n     [7.06499709e-01, 2.00728196e-01, 3.47776863e-01],\n     [7.12396345e-01, 2.03656029e-01, 3.44382594e-01],\n     [7.18264447e-01, 2.06635993e-01, 3.40931208e-01],\n     [7.24102613e-01, 2.09669834e-01, 3.37423766e-01],\n     [7.29909422e-01, 2.12759270e-01, 3.33861367e-01],\n     [7.35683432e-01, 2.15905976e-01, 3.30245147e-01],\n     [7.41423185e-01, 2.19111589e-01, 3.26576275e-01],\n     [7.47127207e-01, 2.22377697e-01, 3.22855952e-01],\n     [7.52794009e-01, 2.25705837e-01, 3.19085410e-01],\n     [7.58422090e-01, 2.29097492e-01, 3.15265910e-01],\n     [7.64009940e-01, 2.32554083e-01, 3.11398734e-01],\n     [7.69556038e-01, 2.36076967e-01, 3.07485188e-01],\n     [7.75058888e-01, 2.39667435e-01, 3.03526312e-01],\n     [7.80517023e-01, 2.43326720e-01, 2.99522665e-01],\n     [7.85928794e-01, 2.47055968e-01, 2.95476756e-01],\n     [7.91292674e-01, 2.50856232e-01, 2.91389943e-01],\n     [7.96607144e-01, 2.54728485e-01, 2.87263585e-01],\n     [8.01870689e-01, 2.58673610e-01, 2.83099033e-01],\n     [8.07081807e-01, 2.62692401e-01, 2.78897629e-01],\n     [8.12239008e-01, 2.66785558e-01, 2.74660698e-01],\n     [8.17340818e-01, 2.70953688e-01, 2.70389545e-01],\n     [8.22385784e-01, 2.75197300e-01, 2.66085445e-01],\n     [8.27372474e-01, 2.79516805e-01, 2.61749643e-01],\n     [8.32299481e-01, 2.83912516e-01, 2.57383341e-01],\n     [8.37165425e-01, 2.88384647e-01, 2.52987700e-01],\n     [8.41968959e-01, 2.92933312e-01, 2.48563825e-01],\n     [8.46708768e-01, 2.97558528e-01, 2.44112767e-01],\n     [8.51383572e-01, 3.02260213e-01, 2.39635512e-01],\n     [8.55992130e-01, 3.07038188e-01, 2.35132978e-01],\n     [8.60533241e-01, 3.11892183e-01, 2.30606009e-01],\n     [8.65005747e-01, 3.16821833e-01, 2.26055368e-01],\n     [8.69408534e-01, 3.21826685e-01, 2.21481734e-01],\n     [8.73740530e-01, 3.26906201e-01, 2.16885699e-01],\n     [8.78000715e-01, 3.32059760e-01, 2.12267762e-01],\n     [8.82188112e-01, 3.37286663e-01, 2.07628326e-01],\n     [8.86301795e-01, 3.42586137e-01, 2.02967696e-01],\n     [8.90340885e-01, 3.47957340e-01, 1.98286080e-01],\n     [8.94304553e-01, 3.53399363e-01, 1.93583583e-01],\n     [8.98192017e-01, 3.58911240e-01, 1.88860212e-01],\n     [9.02002544e-01, 3.64491949e-01, 1.84115876e-01],\n     [9.05735448e-01, 3.70140419e-01, 1.79350388e-01],\n     [9.09390090e-01, 3.75855533e-01, 1.74563472e-01],\n     [9.12965874e-01, 3.81636138e-01, 1.69754764e-01],\n     [9.16462251e-01, 3.87481044e-01, 1.64923826e-01],\n     [9.19878710e-01, 3.93389034e-01, 1.60070152e-01],\n     [9.23214783e-01, 3.99358867e-01, 1.55193185e-01],\n     [9.26470039e-01, 4.05389282e-01, 1.50292329e-01],\n     [9.29644083e-01, 4.11479007e-01, 1.45366973e-01],\n     [9.32736555e-01, 4.17626756e-01, 1.40416519e-01],\n     [9.35747126e-01, 4.23831237e-01, 1.35440416e-01],\n     [9.38675494e-01, 4.30091162e-01, 1.30438175e-01],\n     [9.41521384e-01, 4.36405243e-01, 1.25409440e-01],\n     [9.44284543e-01, 4.42772199e-01, 1.20354038e-01],\n     [9.46964741e-01, 4.49190757e-01, 1.15272059e-01],\n     [9.49561766e-01, 4.55659658e-01, 1.10163947e-01],\n     [9.52075421e-01, 4.62177656e-01, 1.05030614e-01],\n     [9.54505523e-01, 4.68743522e-01, 9.98735931e-02],\n     [9.56851903e-01, 4.75356048e-01, 9.46952268e-02],\n     [9.59114397e-01, 4.82014044e-01, 8.94989073e-02],\n     [9.61292850e-01, 4.88716345e-01, 8.42893891e-02],\n     [9.63387110e-01, 4.95461806e-01, 7.90731907e-02],\n     [9.65397031e-01, 5.02249309e-01, 7.38591143e-02],\n     [9.67322465e-01, 5.09077761e-01, 6.86589199e-02],\n     [9.69163264e-01, 5.15946092e-01, 6.34881971e-02],\n     [9.70919277e-01, 5.22853259e-01, 5.83674890e-02],\n     [9.72590351e-01, 5.29798246e-01, 5.33237243e-02],\n     [9.74176327e-01, 5.36780059e-01, 4.83920090e-02],\n     [9.75677038e-01, 5.43797733e-01, 4.36177922e-02],\n     [9.77092313e-01, 5.50850323e-01, 3.90500131e-02],\n     [9.78421971e-01, 5.57936911e-01, 3.49306227e-02],\n     [9.79665824e-01, 5.65056600e-01, 3.14091591e-02],\n     [9.80823673e-01, 5.72208516e-01, 2.85075931e-02],\n     [9.81895311e-01, 5.79391803e-01, 2.62497353e-02],\n     [9.82880522e-01, 5.86605627e-01, 2.46613416e-02],\n     [9.83779081e-01, 5.93849168e-01, 2.37702263e-02],\n     [9.84590755e-01, 6.01121626e-01, 2.36063833e-02],\n     [9.85315301e-01, 6.08422211e-01, 2.42021174e-02],\n     [9.85952471e-01, 6.15750147e-01, 2.55921853e-02],\n     [9.86502013e-01, 6.23104667e-01, 2.78139496e-02],\n     [9.86963670e-01, 6.30485011e-01, 3.09075459e-02],\n     [9.87337182e-01, 6.37890424e-01, 3.49160639e-02],\n     [9.87622296e-01, 6.45320152e-01, 3.98857472e-02],\n     [9.87818759e-01, 6.52773439e-01, 4.55808037e-02],\n     [9.87926330e-01, 6.60249526e-01, 5.17503867e-02],\n     [9.87944783e-01, 6.67747641e-01, 5.83286889e-02],\n     [9.87873910e-01, 6.75267000e-01, 6.52570167e-02],\n     [9.87713535e-01, 6.82806802e-01, 7.24892330e-02],\n     [9.87463516e-01, 6.90366218e-01, 7.99897176e-02],\n     [9.87123759e-01, 6.97944391e-01, 8.77314215e-02],\n     [9.86694229e-01, 7.05540424e-01, 9.56941797e-02],\n     [9.86174970e-01, 7.13153375e-01, 1.03863324e-01],\n     [9.85565739e-01, 7.20782460e-01, 1.12228756e-01],\n     [9.84865203e-01, 7.28427497e-01, 1.20784651e-01],\n     [9.84075129e-01, 7.36086521e-01, 1.29526579e-01],\n     [9.83195992e-01, 7.43758326e-01, 1.38453063e-01],\n     [9.82228463e-01, 7.51441596e-01, 1.47564573e-01],\n     [9.81173457e-01, 7.59134892e-01, 1.56863224e-01],\n     [9.80032178e-01, 7.66836624e-01, 1.66352544e-01],\n     [9.78806183e-01, 7.74545028e-01, 1.76037298e-01],\n     [9.77497453e-01, 7.82258138e-01, 1.85923357e-01],\n     [9.76108474e-01, 7.89973753e-01, 1.96017589e-01],\n     [9.74637842e-01, 7.97691563e-01, 2.06331925e-01],\n     [9.73087939e-01, 8.05409333e-01, 2.16876839e-01],\n     [9.71467822e-01, 8.13121725e-01, 2.27658046e-01],\n     [9.69783146e-01, 8.20825143e-01, 2.38685942e-01],\n     [9.68040817e-01, 8.28515491e-01, 2.49971582e-01],\n     [9.66242589e-01, 8.36190976e-01, 2.61533898e-01],\n     [9.64393924e-01, 8.43848069e-01, 2.73391112e-01],\n     [9.62516656e-01, 8.51476340e-01, 2.85545675e-01],\n     [9.60625545e-01, 8.59068716e-01, 2.98010219e-01],\n     [9.58720088e-01, 8.66624355e-01, 3.10820466e-01],\n     [9.56834075e-01, 8.74128569e-01, 3.23973947e-01],\n     [9.54997177e-01, 8.81568926e-01, 3.37475479e-01],\n     [9.53215092e-01, 8.88942277e-01, 3.51368713e-01],\n     [9.51546225e-01, 8.96225909e-01, 3.65627005e-01],\n     [9.50018481e-01, 9.03409063e-01, 3.80271225e-01],\n     [9.48683391e-01, 9.10472964e-01, 3.95289169e-01],\n     [9.47594362e-01, 9.17399053e-01, 4.10665194e-01],\n     [9.46809163e-01, 9.24168246e-01, 4.26373236e-01],\n     [9.46391536e-01, 9.30760752e-01, 4.42367495e-01],\n     [9.46402951e-01, 9.37158971e-01, 4.58591507e-01],\n     [9.46902568e-01, 9.43347775e-01, 4.74969778e-01],\n     [9.47936825e-01, 9.49317522e-01, 4.91426053e-01],\n     [9.49544830e-01, 9.55062900e-01, 5.07859649e-01],\n     [9.51740304e-01, 9.60586693e-01, 5.24203026e-01],\n     [9.54529281e-01, 9.65895868e-01, 5.40360752e-01],\n     [9.57896053e-01, 9.71003330e-01, 5.56275090e-01],\n     [9.61812020e-01, 9.75924241e-01, 5.71925382e-01],\n     [9.66248822e-01, 9.80678193e-01, 5.87205773e-01],\n     [9.71161622e-01, 9.85282161e-01, 6.02154330e-01],\n     [9.76510983e-01, 9.89753437e-01, 6.16760413e-01],\n     [9.82257307e-01, 9.94108844e-01, 6.31017009e-01],\n     [9.88362068e-01, 9.98364143e-01, 6.44924005e-01]])'"
utils/color_tables/viridis.py,1,"b'import torch\n\n# Viridis is a blueish black to greenish yellow color map.\nVIRIDIS_cm = torch.Tensor(\n    [[0.26700401,  0.00487433,  0.32941519],\n     [0.26851048, 0.00960483, 0.33542652],\n     [0.26994384, 0.01462494, 0.34137895],\n     [0.27130489, 0.01994186, 0.34726862],\n     [0.27259384, 0.02556309, 0.35309303],\n     [0.27380934, 0.03149748, 0.35885256],\n     [0.27495242, 0.03775181, 0.36454323],\n     [0.27602238, 0.04416723, 0.37016418],\n     [0.27701840, 0.05034437, 0.37571452],\n     [0.27794143, 0.05632444, 0.38119074],\n     [0.27879067, 0.06214536, 0.38659204],\n     [0.27956550, 0.06783587, 0.39191723],\n     [0.28026658, 0.07341724, 0.39716349],\n     [0.28089358, 0.07890703, 0.40232944],\n     [0.28144581, 0.08431970, 0.40741404],\n     [0.28192358, 0.08966622, 0.41241521],\n     [0.28232739, 0.09495545, 0.41733086],\n     [0.28265633, 0.10019576, 0.42216032],\n     [0.28291049, 0.10539345, 0.42690202],\n     [0.28309095, 0.11055307, 0.43155375],\n     [0.28319704, 0.11567966, 0.43611482],\n     [0.28322882, 0.12077701, 0.44058404],\n     [0.28318684, 0.12584799, 0.44496000],\n     [0.28307200, 0.13089477, 0.44924127],\n     [0.28288389, 0.13592005, 0.45342734],\n     [0.28262297, 0.14092556, 0.45751726],\n     [0.28229037, 0.14591233, 0.46150995],\n     [0.28188676, 0.15088147, 0.46540474],\n     [0.28141228, 0.15583425, 0.46920128],\n     [0.28086773, 0.16077132, 0.47289909],\n     [0.28025468, 0.16569272, 0.47649762],\n     [0.27957399, 0.17059884, 0.47999675],\n     [0.27882618, 0.17549020, 0.48339654],\n     [0.27801236, 0.18036684, 0.48669702],\n     [0.27713437, 0.18522836, 0.48989831],\n     [0.27619376, 0.19007447, 0.49300074],\n     [0.27519116, 0.19490540, 0.49600488],\n     [0.27412802, 0.19972086, 0.49891131],\n     [0.27300596, 0.20452049, 0.50172076],\n     [0.27182812, 0.20930306, 0.50443413],\n     [0.27059473, 0.21406899, 0.50705243],\n     [0.26930756, 0.21881782, 0.50957678],\n     [0.26796846, 0.22354911, 0.51200840],\n     [0.26657984, 0.22826210, 0.51434870],\n     [0.26514450, 0.23295593, 0.51659930],\n     [0.26366320, 0.23763078, 0.51876163],\n     [0.26213801, 0.24228619, 0.52083736],\n     [0.26057103, 0.24692170, 0.52282822],\n     [0.25896451, 0.25153685, 0.52473609],\n     [0.25732244, 0.25613040, 0.52656332],\n     [0.25564519, 0.26070284, 0.52831152],\n     [0.25393498, 0.26525384, 0.52998273],\n     [0.25219404, 0.26978306, 0.53157905],\n     [0.25042462, 0.27429024, 0.53310261],\n     [0.24862899, 0.27877509, 0.53455561],\n     [0.24681140, 0.28323662, 0.53594093],\n     [0.24497208, 0.28767547, 0.53726018],\n     [0.24311324, 0.29209154, 0.53851561],\n     [0.24123708, 0.29648471, 0.53970946],\n     [0.23934575, 0.30085494, 0.54084398],\n     [0.23744138, 0.30520222, 0.54192140],\n     [0.23552606, 0.30952657, 0.54294396],\n     [0.23360277, 0.31382773, 0.54391424],\n     [0.23167350, 0.31810580, 0.54483444],\n     [0.22973926, 0.32236127, 0.54570633],\n     [0.22780192, 0.32659432, 0.54653200],\n     [0.22586330, 0.33080515, 0.54731353],\n     [0.22392515, 0.33499400, 0.54805291],\n     [0.22198915, 0.33916114, 0.54875211],\n     [0.22005691, 0.34330688, 0.54941304],\n     [0.21812995, 0.34743154, 0.55003755],\n     [0.21620971, 0.35153548, 0.55062743],\n     [0.21429757, 0.35561907, 0.55118440],\n     [0.21239477, 0.35968273, 0.55171011],\n     [0.21050310, 0.36372671, 0.55220646],\n     [0.20862342, 0.36775151, 0.55267486],\n     [0.20675628, 0.37175775, 0.55311653],\n     [0.20490257, 0.37574589, 0.55353282],\n     [0.20306309, 0.37971644, 0.55392505],\n     [0.20123854, 0.38366989, 0.55429441],\n     [0.19942950, 0.38760678, 0.55464205],\n     [0.19763650, 0.39152762, 0.55496905],\n     [0.19585993, 0.39543297, 0.55527637],\n     [0.19410009, 0.39932336, 0.55556494],\n     [0.19235719, 0.40319934, 0.55583559],\n     [0.19063135, 0.40706148, 0.55608907],\n     [0.18892259, 0.41091033, 0.55632606],\n     [0.18723083, 0.41474645, 0.55654717],\n     [0.18555593, 0.41857040, 0.55675292],\n     [0.18389763, 0.42238275, 0.55694377],\n     [0.18225561, 0.42618405, 0.55712010],\n     [0.18062949, 0.42997486, 0.55728221],\n     [0.17901879, 0.43375572, 0.55743035],\n     [0.17742298, 0.43752720, 0.55756466],\n     [0.17584148, 0.44128981, 0.55768526],\n     [0.17427363, 0.44504410, 0.55779216],\n     [0.17271876, 0.44879060, 0.55788532],\n     [0.17117615, 0.45252980, 0.55796464],\n     [0.16964573, 0.45626209, 0.55803034],\n     [0.16812641, 0.45998802, 0.55808199],\n     [0.16661710, 0.46370813, 0.55811913],\n     [0.16511703, 0.46742290, 0.55814141],\n     [0.16362543, 0.47113278, 0.55814842],\n     [0.16214155, 0.47483821, 0.55813967],\n     [0.16066467, 0.47853961, 0.55811466],\n     [0.15919413, 0.48223740, 0.55807280],\n     [0.15772933, 0.48593197, 0.55801347],\n     [0.15626973, 0.48962370, 0.55793600],\n     [0.15481488, 0.49331293, 0.55783967],\n     [0.15336445, 0.49700003, 0.55772371],\n     [0.15191820, 0.50068529, 0.55758733],\n     [0.15047605, 0.50436904, 0.55742968],\n     [0.14903918, 0.50805136, 0.55725050],\n     [0.14760731, 0.51173263, 0.55704861],\n     [0.14618026, 0.51541316, 0.55682271],\n     [0.14475863, 0.51909319, 0.55657181],\n     [0.14334327, 0.52277292, 0.55629491],\n     [0.14193527, 0.52645254, 0.55599097],\n     [0.14053599, 0.53013219, 0.55565893],\n     [0.13914708, 0.53381201, 0.55529773],\n     [0.13777048, 0.53749213, 0.55490625],\n     [0.13640850, 0.54117264, 0.55448339],\n     [0.13506561, 0.54485335, 0.55402906],\n     [0.13374299, 0.54853458, 0.55354108],\n     [0.13244401, 0.55221637, 0.55301828],\n     [0.13117249, 0.55589872, 0.55245948],\n     [0.12993270, 0.55958162, 0.55186354],\n     [0.12872938, 0.56326503, 0.55122927],\n     [0.12756771, 0.56694891, 0.55055551],\n     [0.12645338, 0.57063316, 0.54984110],\n     [0.12539383, 0.57431754, 0.54908564],\n     [0.12439474, 0.57800205, 0.54828740],\n     [0.12346281, 0.58168661, 0.54744498],\n     [0.12260562, 0.58537105, 0.54655722],\n     [0.12183122, 0.58905521, 0.54562298],\n     [0.12114807, 0.59273889, 0.54464114],\n     [0.12056501, 0.59642187, 0.54361058],\n     [0.12009154, 0.60010387, 0.54253043],\n     [0.11973756, 0.60378459, 0.54139999],\n     [0.11951163, 0.60746388, 0.54021751],\n     [0.11942341, 0.61114146, 0.53898192],\n     [0.11948255, 0.61481702, 0.53769219],\n     [0.11969858, 0.61849025, 0.53634733],\n     [0.12008079, 0.62216081, 0.53494633],\n     [0.12063824, 0.62582833, 0.53348834],\n     [0.12137972, 0.62949242, 0.53197275],\n     [0.12231244, 0.63315277, 0.53039808],\n     [0.12344358, 0.63680899, 0.52876343],\n     [0.12477953, 0.64046069, 0.52706792],\n     [0.12632581, 0.64410744, 0.52531069],\n     [0.12808703, 0.64774881, 0.52349092],\n     [0.13006688, 0.65138436, 0.52160791],\n     [0.13226797, 0.65501363, 0.51966086],\n     [0.13469183, 0.65863619, 0.51764880],\n     [0.13733921, 0.66225157, 0.51557101],\n     [0.14020991, 0.66585927, 0.51342680],\n     [0.14330291, 0.66945881, 0.51121549],\n     [0.14661640, 0.67304968, 0.50893644],\n     [0.15014782, 0.67663139, 0.50658890],\n     [0.15389405, 0.68020343, 0.50417217],\n     [0.15785146, 0.68376525, 0.50168574],\n     [0.16201598, 0.68731632, 0.49912906],\n     [0.16638320, 0.69085611, 0.49650163],\n     [0.17094840, 0.69438405, 0.49380294],\n     [0.17570671, 0.69789960, 0.49103252],\n     [0.18065314, 0.70140222, 0.48818938],\n     [0.18578266, 0.70489133, 0.48527326],\n     [0.19109018, 0.70836635, 0.48228395],\n     [0.19657063, 0.71182668, 0.47922108],\n     [0.20221902, 0.71527175, 0.47608431],\n     [0.20803045, 0.71870095, 0.47287330],\n     [0.21400015, 0.72211371, 0.46958774],\n     [0.22012381, 0.72550945, 0.46622638],\n     [0.22639690, 0.72888753, 0.46278934],\n     [0.23281498, 0.73224735, 0.45927675],\n     [0.23937390, 0.73558828, 0.45568838],\n     [0.24606968, 0.73890972, 0.45202405],\n     [0.25289851, 0.74221104, 0.44828355],\n     [0.25985676, 0.74549162, 0.44446673],\n     [0.26694127, 0.74875084, 0.44057284],\n     [0.27414922, 0.75198807, 0.43660090],\n     [0.28147681, 0.75520266, 0.43255207],\n     [0.28892102, 0.75839399, 0.42842626],\n     [0.29647899, 0.76156142, 0.42422341],\n     [0.30414796, 0.76470433, 0.41994346],\n     [0.31192534, 0.76782207, 0.41558638],\n     [0.31980860, 0.77091403, 0.41115215],\n     [0.32779580, 0.77397953, 0.40664011],\n     [0.33588539, 0.77701790, 0.40204917],\n     [0.34407411, 0.78002855, 0.39738103],\n     [0.35235985, 0.78301086, 0.39263579],\n     [0.36074053, 0.78596419, 0.38781353],\n     [0.36921420, 0.78888793, 0.38291438],\n     [0.37777892, 0.79178146, 0.37793850],\n     [0.38643282, 0.79464415, 0.37288606],\n     [0.39517408, 0.79747541, 0.36775726],\n     [0.40400101, 0.80027461, 0.36255223],\n     [0.41291350, 0.80304099, 0.35726893],\n     [0.42190813, 0.80577412, 0.35191009],\n     [0.43098317, 0.80847343, 0.34647607],\n     [0.44013691, 0.81113836, 0.34096730],\n     [0.44936763, 0.81376835, 0.33538426],\n     [0.45867362, 0.81636288, 0.32972749],\n     [0.46805314, 0.81892143, 0.32399761],\n     [0.47750446, 0.82144351, 0.31819529],\n     [0.48702580, 0.82392862, 0.31232133],\n     [0.49661536, 0.82637633, 0.30637661],\n     [0.50627130, 0.82878621, 0.30036211],\n     [0.51599182, 0.83115784, 0.29427888],\n     [0.52577622, 0.83349064, 0.28812650],\n     [0.53562110, 0.83578452, 0.28190832],\n     [0.54552440, 0.83803918, 0.27562602],\n     [0.55548397, 0.84025437, 0.26928147],\n     [0.56549760, 0.84242990, 0.26287683],\n     [0.57556297, 0.84456561, 0.25641457],\n     [0.58567772, 0.84666139, 0.24989748],\n     [0.59583934, 0.84871722, 0.24332878],\n     [0.60604528, 0.85073310, 0.23671214],\n     [0.61629283, 0.85270912, 0.23005179],\n     [0.62657923, 0.85464543, 0.22335258],\n     [0.63690157, 0.85654226, 0.21662012],\n     [0.64725685, 0.85839991, 0.20986086],\n     [0.65764197, 0.86021878, 0.20308229],\n     [0.66805369, 0.86199932, 0.19629307],\n     [0.67848868, 0.86374211, 0.18950326],\n     [0.68894351, 0.86544779, 0.18272455],\n     [0.69941463, 0.86711711, 0.17597055],\n     [0.70989842, 0.86875092, 0.16925712],\n     [0.72039115, 0.87035015, 0.16260273],\n     [0.73088902, 0.87191584, 0.15602894],\n     [0.74138803, 0.87344918, 0.14956101],\n     [0.75188414, 0.87495143, 0.14322828],\n     [0.76237342, 0.87642392, 0.13706449],\n     [0.77285183, 0.87786808, 0.13110864],\n     [0.78331535, 0.87928545, 0.12540538],\n     [0.79375994, 0.88067763, 0.12000532],\n     [0.80418159, 0.88204632, 0.11496505],\n     [0.81457634, 0.88339329, 0.11034678],\n     [0.82494028, 0.88472036, 0.10621724],\n     [0.83526959, 0.88602943, 0.10264590],\n     [0.84556056, 0.88732243, 0.09970219],\n     [0.85580960, 0.88860134, 0.09745186],\n     [0.86601325, 0.88986815, 0.09595277],\n     [0.87616824, 0.89112487, 0.09525046],\n     [0.88627146, 0.89237353, 0.09537439],\n     [0.89632002, 0.89361614, 0.09633538],\n     [0.90631121, 0.89485467, 0.09812496],\n     [0.91624212, 0.89609127, 0.10071680],\n     [0.92610579, 0.89732977, 0.10407067],\n     [0.93590444, 0.89857040, 0.10813094],\n     [0.94563626, 0.89981500, 0.11283773],\n     [0.95529972, 0.90106534, 0.11812832],\n     [0.96489353, 0.90232311, 0.12394051],\n     [0.97441665, 0.90358991, 0.13021494],\n     [0.98386829, 0.90486726, 0.13689671],\n     [0.99324789, 0.90615657, 0.14393620]])'"
