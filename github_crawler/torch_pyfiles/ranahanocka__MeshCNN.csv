file_path,api_count,code
test.py,0,"b""from options.test_options import TestOptions\nfrom data import DataLoader\nfrom models import create_model\nfrom util.writer import Writer\n\n\ndef run_test(epoch=-1):\n    print('Running Test')\n    opt = TestOptions().parse()\n    opt.serial_batches = True  # no shuffle\n    dataset = DataLoader(opt)\n    model = create_model(opt)\n    writer = Writer(opt)\n    # test\n    writer.reset_counter()\n    for i, data in enumerate(dataset):\n        model.set_input(data)\n        ncorrect, nexamples = model.test()\n        writer.update_counter(ncorrect, nexamples)\n    writer.print_acc(epoch, writer.acc)\n    return writer.acc\n\n\nif __name__ == '__main__':\n    run_test()\n"""
train.py,0,"b""import time\nfrom options.train_options import TrainOptions\nfrom data import DataLoader\nfrom models import create_model\nfrom util.writer import Writer\nfrom test import run_test\n\nif __name__ == '__main__':\n    opt = TrainOptions().parse()\n    dataset = DataLoader(opt)\n    dataset_size = len(dataset)\n    print('#training meshes = %d' % dataset_size)\n\n    model = create_model(opt)\n    writer = Writer(opt)\n    total_steps = 0\n\n    for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n        epoch_start_time = time.time()\n        iter_data_time = time.time()\n        epoch_iter = 0\n\n        for i, data in enumerate(dataset):\n            iter_start_time = time.time()\n            if total_steps % opt.print_freq == 0:\n                t_data = iter_start_time - iter_data_time\n            total_steps += opt.batch_size\n            epoch_iter += opt.batch_size\n            model.set_input(data)\n            model.optimize_parameters()\n\n            if total_steps % opt.print_freq == 0:\n                loss = model.loss\n                t = (time.time() - iter_start_time) / opt.batch_size\n                writer.print_current_losses(epoch, epoch_iter, loss, t, t_data)\n                writer.plot_loss(loss, epoch, epoch_iter, dataset_size)\n\n            if i % opt.save_latest_freq == 0:\n                print('saving the latest model (epoch %d, total_steps %d)' %\n                      (epoch, total_steps))\n                model.save_network('latest')\n\n            iter_data_time = time.time()\n        if epoch % opt.save_epoch_freq == 0:\n            print('saving the model at the end of epoch %d, iters %d' %\n                  (epoch, total_steps))\n            model.save_network('latest')\n            model.save_network(epoch)\n\n        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n        model.update_learning_rate()\n        if opt.verbose_plot:\n            writer.plot_model_wts(model, epoch)\n\n        if epoch % opt.run_test_freq == 0:\n            acc = run_test(epoch)\n            writer.plot_acc(acc, epoch)\n\n    writer.close()\n"""
data/__init__.py,2,"b'import torch.utils.data\nfrom data.base_dataset import collate_fn\n\ndef CreateDataset(opt):\n    """"""loads dataset class""""""\n\n    if opt.dataset_mode == \'segmentation\':\n        from data.segmentation_data import SegmentationData\n        dataset = SegmentationData(opt)\n    elif opt.dataset_mode == \'classification\':\n        from data.classification_data import ClassificationData\n        dataset = ClassificationData(opt)\n    return dataset\n\n\nclass DataLoader:\n    """"""multi-threaded data loading""""""\n\n    def __init__(self, opt):\n        self.opt = opt\n        self.dataset = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batch_size,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.num_threads),\n            collate_fn=collate_fn)\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n                break\n            yield data\n'"
data/base_dataset.py,1,"b'import torch.utils.data as data\nimport numpy as np\nimport pickle\nimport os\n\nclass BaseDataset(data.Dataset):\n\n    def __init__(self, opt):\n        self.opt = opt\n        self.mean = 0\n        self.std = 1\n        self.ninput_channels = None\n        super(BaseDataset, self).__init__()\n\n    def get_mean_std(self):\n        """""" Computes Mean and Standard Deviation from Training Data\n        If mean/std file doesn\'t exist, will compute one\n        :returns\n        mean: N-dimensional mean\n        std: N-dimensional standard deviation\n        ninput_channels: N\n        (here N=5)\n        """"""\n\n        mean_std_cache = os.path.join(self.root, \'mean_std_cache.p\')\n        if not os.path.isfile(mean_std_cache):\n            print(\'computing mean std from train data...\')\n            # doesn\'t run augmentation during m/std computation\n            num_aug = self.opt.num_aug\n            self.opt.num_aug = 1\n            mean, std = np.array(0), np.array(0)\n            for i, data in enumerate(self):\n                if i % 500 == 0:\n                    print(\'{} of {}\'.format(i, self.size))\n                features = data[\'edge_features\']\n                mean = mean + features.mean(axis=1)\n                std = std + features.std(axis=1)\n            mean = mean / (i + 1)\n            std = std / (i + 1)\n            transform_dict = {\'mean\': mean[:, np.newaxis], \'std\': std[:, np.newaxis],\n                              \'ninput_channels\': len(mean)}\n            with open(mean_std_cache, \'wb\') as f:\n                pickle.dump(transform_dict, f)\n            print(\'saved: \', mean_std_cache)\n            self.opt.num_aug = num_aug\n        # open mean / std from file\n        with open(mean_std_cache, \'rb\') as f:\n            transform_dict = pickle.load(f)\n            print(\'loaded mean / std from cache\')\n            self.mean = transform_dict[\'mean\']\n            self.std = transform_dict[\'std\']\n            self.ninput_channels = transform_dict[\'ninput_channels\']\n\n\ndef collate_fn(batch):\n    """"""Creates mini-batch tensors\n    We should build custom collate_fn rather than using default collate_fn\n    """"""\n    meta = {}\n    keys = batch[0].keys()\n    for key in keys:\n        meta.update({key: np.array([d[key] for d in batch])})\n    return meta'"
data/classification_data.py,1,"b""import os\nimport torch\nfrom data.base_dataset import BaseDataset\nfrom util.util import is_mesh_file, pad\nfrom models.layers.mesh import Mesh\n\nclass ClassificationData(BaseDataset):\n\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n        self.opt = opt\n        self.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')\n        self.root = opt.dataroot\n        self.dir = os.path.join(opt.dataroot)\n        self.classes, self.class_to_idx = self.find_classes(self.dir)\n        self.paths = self.make_dataset_by_class(self.dir, self.class_to_idx, opt.phase)\n        self.nclasses = len(self.classes)\n        self.size = len(self.paths)\n        self.get_mean_std()\n        # modify for network later.\n        opt.nclasses = self.nclasses\n        opt.input_nc = self.ninput_channels\n\n    def __getitem__(self, index):\n        path = self.paths[index][0]\n        label = self.paths[index][1]\n        mesh = Mesh(file=path, opt=self.opt, hold_history=False, export_folder=self.opt.export_folder)\n        meta = {'mesh': mesh, 'label': label}\n        # get edge features\n        edge_features = mesh.extract_features()\n        edge_features = pad(edge_features, self.opt.ninput_edges)\n        meta['edge_features'] = (edge_features - self.mean) / self.std\n        return meta\n\n    def __len__(self):\n        return self.size\n\n    # this is when the folders are organized by class...\n    @staticmethod\n    def find_classes(dir):\n        classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n    @staticmethod\n    def make_dataset_by_class(dir, class_to_idx, phase):\n        meshes = []\n        dir = os.path.expanduser(dir)\n        for target in sorted(os.listdir(dir)):\n            d = os.path.join(dir, target)\n            if not os.path.isdir(d):\n                continue\n            for root, _, fnames in sorted(os.walk(d)):\n                for fname in sorted(fnames):\n                    if is_mesh_file(fname) and (root.count(phase)==1):\n                        path = os.path.join(root, fname)\n                        item = (path, class_to_idx[target])\n                        meshes.append(item)\n        return meshes\n"""
data/segmentation_data.py,1,"b""import os\nimport torch\nfrom data.base_dataset import BaseDataset\nfrom util.util import is_mesh_file, pad\nimport numpy as np\nfrom models.layers.mesh import Mesh\n\nclass SegmentationData(BaseDataset):\n\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n        self.opt = opt\n        self.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')\n        self.root = opt.dataroot\n        self.dir = os.path.join(opt.dataroot, opt.phase)\n        self.paths = self.make_dataset(self.dir)\n        self.seg_paths = self.get_seg_files(self.paths, os.path.join(self.root, 'seg'), seg_ext='.eseg')\n        self.sseg_paths = self.get_seg_files(self.paths, os.path.join(self.root, 'sseg'), seg_ext='.seseg')\n        self.classes, self.offset = self.get_n_segs(os.path.join(self.root, 'classes.txt'), self.seg_paths)\n        self.nclasses = len(self.classes)\n        self.size = len(self.paths)\n        self.get_mean_std()\n        # # modify for network later.\n        opt.nclasses = self.nclasses\n        opt.input_nc = self.ninput_channels\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        mesh = Mesh(file=path, opt=self.opt, hold_history=True, export_folder=self.opt.export_folder)\n        meta = {}\n        meta['mesh'] = mesh\n        label = read_seg(self.seg_paths[index]) - self.offset\n        label = pad(label, self.opt.ninput_edges, val=-1, dim=0)\n        meta['label'] = label\n        soft_label = read_sseg(self.sseg_paths[index])\n        meta['soft_label'] = pad(soft_label, self.opt.ninput_edges, val=-1, dim=0)\n        # get edge features\n        edge_features = mesh.extract_features()\n        edge_features = pad(edge_features, self.opt.ninput_edges)\n        meta['edge_features'] = (edge_features - self.mean) / self.std\n        return meta\n\n    def __len__(self):\n        return self.size\n\n    @staticmethod\n    def get_seg_files(paths, seg_dir, seg_ext='.seg'):\n        segs = []\n        for path in paths:\n            segfile = os.path.join(seg_dir, os.path.splitext(os.path.basename(path))[0] + seg_ext)\n            assert(os.path.isfile(segfile))\n            segs.append(segfile)\n        return segs\n\n    @staticmethod\n    def get_n_segs(classes_file, seg_files):\n        if not os.path.isfile(classes_file):\n            all_segs = np.array([], dtype='float64')\n            for seg in seg_files:\n                all_segs = np.concatenate((all_segs, read_seg(seg)))\n            segnames = np.unique(all_segs)\n            np.savetxt(classes_file, segnames, fmt='%d')\n        classes = np.loadtxt(classes_file)\n        offset = classes[0]\n        classes = classes - offset\n        return classes, offset\n\n    @staticmethod\n    def make_dataset(path):\n        meshes = []\n        assert os.path.isdir(path), '%s is not a valid directory' % path\n\n        for root, _, fnames in sorted(os.walk(path)):\n            for fname in fnames:\n                if is_mesh_file(fname):\n                    path = os.path.join(root, fname)\n                    meshes.append(path)\n\n        return meshes\n\n\ndef read_seg(seg):\n    seg_labels = np.loadtxt(open(seg, 'r'), dtype='float64')\n    return seg_labels\n\n\ndef read_sseg(sseg_file):\n    sseg_labels = read_seg(sseg_file)\n    sseg_labels = np.array(sseg_labels > 0, dtype=np.int32)\n    return sseg_labels"""
models/__init__.py,0,b'def create_model(opt):\n    from .mesh_classifier import ClassifierModel # todo - get rid of this ?\n    model = ClassifierModel(opt)\n    return model\n'
models/mesh_classifier.py,11,"b'import torch\nfrom . import networks\nfrom os.path import join\nfrom util.util import seg_accuracy, print_network\n\n\nclass ClassifierModel:\n    """""" Class for training Model weights\n\n    :args opt: structure containing configuration params\n    e.g.,\n    --dataset_mode -> classification / segmentation)\n    --arch -> network type\n    """"""\n    def __init__(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.is_train = opt.is_train\n        self.device = torch.device(\'cuda:{}\'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device(\'cpu\')\n        self.save_dir = join(opt.checkpoints_dir, opt.name)\n        self.optimizer = None\n        self.edge_features = None\n        self.labels = None\n        self.mesh = None\n        self.soft_label = None\n        self.loss = None\n\n        #\n        self.nclasses = opt.nclasses\n\n        # load/define networks\n        self.net = networks.define_classifier(opt.input_nc, opt.ncf, opt.ninput_edges, opt.nclasses, opt,\n                                              self.gpu_ids, opt.arch, opt.init_type, opt.init_gain)\n        self.net.train(self.is_train)\n        self.criterion = networks.define_loss(opt).to(self.device)\n\n        if self.is_train:\n            self.optimizer = torch.optim.Adam(self.net.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.scheduler = networks.get_scheduler(self.optimizer, opt)\n            print_network(self.net)\n\n        if not self.is_train or opt.continue_train:\n            self.load_network(opt.which_epoch)\n\n    def set_input(self, data):\n        input_edge_features = torch.from_numpy(data[\'edge_features\']).float()\n        labels = torch.from_numpy(data[\'label\']).long()\n        # set inputs\n        self.edge_features = input_edge_features.to(self.device).requires_grad_(self.is_train)\n        self.labels = labels.to(self.device)\n        self.mesh = data[\'mesh\']\n        if self.opt.dataset_mode == \'segmentation\' and not self.is_train:\n            self.soft_label = torch.from_numpy(data[\'soft_label\'])\n\n\n    def forward(self):\n        out = self.net(self.edge_features, self.mesh)\n        return out\n\n    def backward(self, out):\n        self.loss = self.criterion(out, self.labels)\n        self.loss.backward()\n\n    def optimize_parameters(self):\n        self.optimizer.zero_grad()\n        out = self.forward()\n        self.backward(out)\n        self.optimizer.step()\n\n\n##################\n\n    def load_network(self, which_epoch):\n        """"""load model from disk""""""\n        save_filename = \'%s_net.pth\' % which_epoch\n        load_path = join(self.save_dir, save_filename)\n        net = self.net\n        if isinstance(net, torch.nn.DataParallel):\n            net = net.module\n        print(\'loading the model from %s\' % load_path)\n        # PyTorch newer than 0.4 (e.g., built from\n        # GitHub source), you can remove str() on self.device\n        state_dict = torch.load(load_path, map_location=str(self.device))\n        if hasattr(state_dict, \'_metadata\'):\n            del state_dict._metadata\n        net.load_state_dict(state_dict)\n\n\n    def save_network(self, which_epoch):\n        """"""save model to disk""""""\n        save_filename = \'%s_net.pth\' % (which_epoch)\n        save_path = join(self.save_dir, save_filename)\n        if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n            torch.save(self.net.module.cpu().state_dict(), save_path)\n            self.net.cuda(self.gpu_ids[0])\n        else:\n            torch.save(self.net.cpu().state_dict(), save_path)\n\n    def update_learning_rate(self):\n        """"""update learning rate (called once every epoch)""""""\n        self.scheduler.step()\n        lr = self.optimizer.param_groups[0][\'lr\']\n        print(\'learning rate = %.7f\' % lr)\n\n    def test(self):\n        """"""tests model\n        returns: number correct and total number\n        """"""\n        with torch.no_grad():\n            out = self.forward()\n            # compute number of correct\n            pred_class = out.data.max(1)[1]\n            label_class = self.labels\n            self.export_segmentation(pred_class.cpu())\n            correct = self.get_accuracy(pred_class, label_class)\n        return correct, len(label_class)\n\n    def get_accuracy(self, pred, labels):\n        """"""computes accuracy for classification / segmentation """"""\n        if self.opt.dataset_mode == \'classification\':\n            correct = pred.eq(labels).sum()\n        elif self.opt.dataset_mode == \'segmentation\':\n            correct = seg_accuracy(pred, self.soft_label, self.mesh)\n        return correct\n\n    def export_segmentation(self, pred_seg):\n        if self.opt.dataset_mode == \'segmentation\':\n            for meshi, mesh in enumerate(self.mesh):\n                mesh.export_segments(pred_seg[meshi, :])\n'"
models/networks.py,11,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\nfrom models.layers.mesh_conv import MeshConv\nimport torch.nn.functional as F\nfrom models.layers.mesh_pool import MeshPool\nfrom models.layers.mesh_unpool import MeshUnpool\n\n\n###############################################################################\n# Helper Functions\n###############################################################################\n\n\ndef get_norm_layer(norm_type=\'instance\', num_groups=1):\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n    elif norm_type == \'group\':\n        norm_layer = functools.partial(nn.GroupNorm, affine=True, num_groups=num_groups)\n    elif norm_type == \'none\':\n        norm_layer = NoNorm\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n    return norm_layer\n\ndef get_norm_args(norm_layer, nfeats_list):\n    if hasattr(norm_layer, \'__name__\') and norm_layer.__name__ == \'NoNorm\':\n        norm_args = [{\'fake\': True} for f in nfeats_list]\n    elif norm_layer.func.__name__ == \'GroupNorm\':\n        norm_args = [{\'num_channels\': f} for f in nfeats_list]\n    elif norm_layer.func.__name__ == \'BatchNorm\':\n        norm_args = [{\'num_features\': f} for f in nfeats_list]\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_layer.func.__name__)\n    return norm_args\n\nclass NoNorm(nn.Module): #todo with abstractclass and pass\n    def __init__(self, fake=True):\n        self.fake = fake\n        super(NoNorm, self).__init__()\n    def forward(self, x):\n        return x\n    def __call__(self, x):\n        return self.forward(x)\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == \'lambda\':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == \'step\':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == \'plateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.2, threshold=0.01, patience=5)\n    else:\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', opt.lr_policy)\n    return scheduler\n\n\ndef init_weights(net, init_type, init_gain):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1):\n            if init_type == \'normal\':\n                init.normal_(m.weight.data, 0.0, init_gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=init_gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=init_gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n        elif classname.find(\'BatchNorm2d\') != -1:\n            init.normal_(m.weight.data, 1.0, init_gain)\n            init.constant_(m.bias.data, 0.0)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type, init_gain, gpu_ids):\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.cuda(gpu_ids[0])\n        net = net.cuda()\n        net = torch.nn.DataParallel(net, gpu_ids)\n    if init_type != \'none\':\n        init_weights(net, init_type, init_gain)\n    return net\n\n\ndef define_classifier(input_nc, ncf, ninput_edges, nclasses, opt, gpu_ids, arch, init_type, init_gain):\n    net = None\n    norm_layer = get_norm_layer(norm_type=opt.norm, num_groups=opt.num_groups)\n\n    if arch == \'mconvnet\':\n        net = MeshConvNet(norm_layer, input_nc, ncf, nclasses, ninput_edges, opt.pool_res, opt.fc_n,\n                          opt.resblocks)\n    elif arch == \'meshunet\':\n        down_convs = [input_nc] + ncf\n        up_convs = ncf[::-1] + [nclasses]\n        pool_res = [ninput_edges] + opt.pool_res\n        net = MeshEncoderDecoder(pool_res, down_convs, up_convs, blocks=opt.resblocks,\n                                 transfer_data=True)\n    else:\n        raise NotImplementedError(\'Encoder model name [%s] is not recognized\' % arch)\n    return init_net(net, init_type, init_gain, gpu_ids)\n\ndef define_loss(opt):\n    if opt.dataset_mode == \'classification\':\n        loss = torch.nn.CrossEntropyLoss()\n    elif opt.dataset_mode == \'segmentation\':\n        loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n    return loss\n\n##############################################################################\n# Classes For Classification / Segmentation Networks\n##############################################################################\n\nclass MeshConvNet(nn.Module):\n    """"""Network for learning a global shape descriptor (classification)\n    """"""\n    def __init__(self, norm_layer, nf0, conv_res, nclasses, input_res, pool_res, fc_n,\n                 nresblocks=3):\n        super(MeshConvNet, self).__init__()\n        self.k = [nf0] + conv_res\n        self.res = [input_res] + pool_res\n        norm_args = get_norm_args(norm_layer, self.k[1:])\n\n        for i, ki in enumerate(self.k[:-1]):\n            setattr(self, \'conv{}\'.format(i), MResConv(ki, self.k[i + 1], nresblocks))\n            setattr(self, \'norm{}\'.format(i), norm_layer(**norm_args[i]))\n            setattr(self, \'pool{}\'.format(i), MeshPool(self.res[i + 1]))\n\n\n        self.gp = torch.nn.AvgPool1d(self.res[-1])\n        # self.gp = torch.nn.MaxPool1d(self.res[-1])\n        self.fc1 = nn.Linear(self.k[-1], fc_n)\n        self.fc2 = nn.Linear(fc_n, nclasses)\n\n    def forward(self, x, mesh):\n\n        for i in range(len(self.k) - 1):\n            x = getattr(self, \'conv{}\'.format(i))(x, mesh)\n            x = F.relu(getattr(self, \'norm{}\'.format(i))(x))\n            x = getattr(self, \'pool{}\'.format(i))(x, mesh)\n\n        x = self.gp(x)\n        x = x.view(-1, self.k[-1])\n\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nclass MResConv(nn.Module):\n    def __init__(self, in_channels, out_channels, skips=1):\n        super(MResConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.skips = skips\n        self.conv0 = MeshConv(self.in_channels, self.out_channels, bias=False)\n        for i in range(self.skips):\n            setattr(self, \'bn{}\'.format(i + 1), nn.BatchNorm2d(self.out_channels))\n            setattr(self, \'conv{}\'.format(i + 1),\n                    MeshConv(self.out_channels, self.out_channels, bias=False))\n\n    def forward(self, x, mesh):\n        x = self.conv0(x, mesh)\n        x1 = x\n        for i in range(self.skips):\n            x = getattr(self, \'bn{}\'.format(i + 1))(F.relu(x))\n            x = getattr(self, \'conv{}\'.format(i + 1))(x, mesh)\n        x += x1\n        x = F.relu(x)\n        return x\n\n\nclass MeshEncoderDecoder(nn.Module):\n    """"""Network for fully-convolutional tasks (segmentation)\n    """"""\n    def __init__(self, pools, down_convs, up_convs, blocks=0, transfer_data=True):\n        super(MeshEncoderDecoder, self).__init__()\n        self.transfer_data = transfer_data\n        self.encoder = MeshEncoder(pools, down_convs, blocks=blocks)\n        unrolls = pools[:-1].copy()\n        unrolls.reverse()\n        self.decoder = MeshDecoder(unrolls, up_convs, blocks=blocks, transfer_data=transfer_data)\n\n    def forward(self, x, meshes):\n        fe, before_pool = self.encoder((x, meshes))\n        fe = self.decoder((fe, meshes), before_pool)\n        return fe\n\n    def __call__(self, x, meshes):\n        return self.forward(x, meshes)\n\nclass DownConv(nn.Module):\n    def __init__(self, in_channels, out_channels, blocks=0, pool=0):\n        super(DownConv, self).__init__()\n        self.bn = []\n        self.pool = None\n        self.conv1 = MeshConv(in_channels, out_channels)\n        self.conv2 = []\n        for _ in range(blocks):\n            self.conv2.append(MeshConv(out_channels, out_channels))\n            self.conv2 = nn.ModuleList(self.conv2)\n        for _ in range(blocks + 1):\n            self.bn.append(nn.InstanceNorm2d(out_channels))\n            self.bn = nn.ModuleList(self.bn)\n        if pool:\n            self.pool = MeshPool(pool)\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def forward(self, x):\n        fe, meshes = x\n        x1 = self.conv1(fe, meshes)\n        if self.bn:\n            x1 = self.bn[0](x1)\n        x1 = F.relu(x1)\n        x2 = x1\n        for idx, conv in enumerate(self.conv2):\n            x2 = conv(x1, meshes)\n            if self.bn:\n                x2 = self.bn[idx + 1](x2)\n            x2 = x2 + x1\n            x2 = F.relu(x2)\n            x1 = x2\n        x2 = x2.squeeze(3)\n        before_pool = None\n        if self.pool:\n            before_pool = x2\n            x2 = self.pool(x2, meshes)\n        return x2, before_pool\n\n\nclass UpConv(nn.Module):\n    def __init__(self, in_channels, out_channels, blocks=0, unroll=0, residual=True,\n                 batch_norm=True, transfer_data=True):\n        super(UpConv, self).__init__()\n        self.residual = residual\n        self.bn = []\n        self.unroll = None\n        self.transfer_data = transfer_data\n        self.up_conv = MeshConv(in_channels, out_channels)\n        if transfer_data:\n            self.conv1 = MeshConv(2 * out_channels, out_channels)\n        else:\n            self.conv1 = MeshConv(out_channels, out_channels)\n        self.conv2 = []\n        for _ in range(blocks):\n            self.conv2.append(MeshConv(out_channels, out_channels))\n            self.conv2 = nn.ModuleList(self.conv2)\n        if batch_norm:\n            for _ in range(blocks + 1):\n                self.bn.append(nn.InstanceNorm2d(out_channels))\n            self.bn = nn.ModuleList(self.bn)\n        if unroll:\n            self.unroll = MeshUnpool(unroll)\n\n    def __call__(self, x, from_down=None):\n        return self.forward(x, from_down)\n\n    def forward(self, x, from_down):\n        from_up, meshes = x\n        x1 = self.up_conv(from_up, meshes).squeeze(3)\n        if self.unroll:\n            x1 = self.unroll(x1, meshes)\n        if self.transfer_data:\n            x1 = torch.cat((x1, from_down), 1)\n        x1 = self.conv1(x1, meshes)\n        if self.bn:\n            x1 = self.bn[0](x1)\n        x1 = F.relu(x1)\n        x2 = x1\n        for idx, conv in enumerate(self.conv2):\n            x2 = conv(x1, meshes)\n            if self.bn:\n                x2 = self.bn[idx + 1](x2)\n            if self.residual:\n                x2 = x2 + x1\n            x2 = F.relu(x2)\n            x1 = x2\n        x2 = x2.squeeze(3)\n        return x2\n\n\nclass MeshEncoder(nn.Module):\n    def __init__(self, pools, convs, fcs=None, blocks=0, global_pool=None):\n        super(MeshEncoder, self).__init__()\n        self.fcs = None\n        self.convs = []\n        for i in range(len(convs) - 1):\n            if i + 1 < len(pools):\n                pool = pools[i + 1]\n            else:\n                pool = 0\n            self.convs.append(DownConv(convs[i], convs[i + 1], blocks=blocks, pool=pool))\n        self.global_pool = None\n        if fcs is not None:\n            self.fcs = []\n            self.fcs_bn = []\n            last_length = convs[-1]\n            if global_pool is not None:\n                if global_pool == \'max\':\n                    self.global_pool = nn.MaxPool1d(pools[-1])\n                elif global_pool == \'avg\':\n                    self.global_pool = nn.AvgPool1d(pools[-1])\n                else:\n                    assert False, \'global_pool %s is not defined\' % global_pool\n            else:\n                last_length *= pools[-1]\n            if fcs[0] == last_length:\n                fcs = fcs[1:]\n            for length in fcs:\n                self.fcs.append(nn.Linear(last_length, length))\n                self.fcs_bn.append(nn.InstanceNorm1d(length))\n                last_length = length\n            self.fcs = nn.ModuleList(self.fcs)\n            self.fcs_bn = nn.ModuleList(self.fcs_bn)\n        self.convs = nn.ModuleList(self.convs)\n        reset_params(self)\n\n    def forward(self, x):\n        fe, meshes = x\n        encoder_outs = []\n        for conv in self.convs:\n            fe, before_pool = conv((fe, meshes))\n            encoder_outs.append(before_pool)\n        if self.fcs is not None:\n            if self.global_pool is not None:\n                fe = self.global_pool(fe)\n            fe = fe.contiguous().view(fe.size()[0], -1)\n            for i in range(len(self.fcs)):\n                fe = self.fcs[i](fe)\n                if self.fcs_bn:\n                    x = fe.unsqueeze(1)\n                    fe = self.fcs_bn[i](x).squeeze(1)\n                if i < len(self.fcs) - 1:\n                    fe = F.relu(fe)\n        return fe, encoder_outs\n\n    def __call__(self, x):\n        return self.forward(x)\n\n\nclass MeshDecoder(nn.Module):\n    def __init__(self, unrolls, convs, blocks=0, batch_norm=True, transfer_data=True):\n        super(MeshDecoder, self).__init__()\n        self.up_convs = []\n        for i in range(len(convs) - 2):\n            if i < len(unrolls):\n                unroll = unrolls[i]\n            else:\n                unroll = 0\n            self.up_convs.append(UpConv(convs[i], convs[i + 1], blocks=blocks, unroll=unroll,\n                                        batch_norm=batch_norm, transfer_data=transfer_data))\n        self.final_conv = UpConv(convs[-2], convs[-1], blocks=blocks, unroll=False,\n                                 batch_norm=batch_norm, transfer_data=False)\n        self.up_convs = nn.ModuleList(self.up_convs)\n        reset_params(self)\n\n    def forward(self, x, encoder_outs=None):\n        fe, meshes = x\n        for i, up_conv in enumerate(self.up_convs):\n            before_pool = None\n            if encoder_outs is not None:\n                before_pool = encoder_outs[-(i+2)]\n            fe = up_conv((fe, meshes), before_pool)\n        fe = self.final_conv((fe, meshes))\n        return fe\n\n    def __call__(self, x, encoder_outs=None):\n        return self.forward(x, encoder_outs)\n\ndef reset_params(model): # todo replace with my init\n    for i, m in enumerate(model.modules()):\n        weight_init(m)\n\ndef weight_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.xavier_normal_(m.weight)\n        nn.init.constant_(m.bias, 0)\n'"
options/__init__.py,0,b''
options/base_options.py,2,"b'import argparse\nimport os\nfrom util import util\nimport torch\n\n\nclass BaseOptions:\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        self.initialized = False\n\n    def initialize(self):\n        # data params\n        self.parser.add_argument(\'--dataroot\', required=True, help=\'path to meshes (should have subfolders train, test)\')\n        self.parser.add_argument(\'--dataset_mode\', choices={""classification"", ""segmentation""}, default=\'classification\')\n        self.parser.add_argument(\'--ninput_edges\', type=int, default=750, help=\'# of input edges (will include dummy edges)\')\n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples per epoch\')\n        # network params\n        self.parser.add_argument(\'--batch_size\', type=int, default=16, help=\'input batch size\')\n        self.parser.add_argument(\'--arch\', type=str, default=\'mconvnet\', help=\'selects network to use\') #todo add choices\n        self.parser.add_argument(\'--resblocks\', type=int, default=0, help=\'# of res blocks\')\n        self.parser.add_argument(\'--fc_n\', type=int, default=100, help=\'# between fc and nclasses\') #todo make generic\n        self.parser.add_argument(\'--ncf\', nargs=\'+\', default=[16, 32, 32], type=int, help=\'conv filters\')\n        self.parser.add_argument(\'--pool_res\', nargs=\'+\', default=[1140, 780, 580], type=int, help=\'pooling res\')\n        self.parser.add_argument(\'--norm\', type=str, default=\'batch\',help=\'instance normalization or batch normalization or group normalization\')\n        self.parser.add_argument(\'--num_groups\', type=int, default=16, help=\'# of groups for groupnorm\')\n        self.parser.add_argument(\'--init_type\', type=str, default=\'normal\', help=\'network initialization [normal|xavier|kaiming|orthogonal]\')\n        self.parser.add_argument(\'--init_gain\', type=float, default=0.02, help=\'scaling factor for normal, xavier and orthogonal.\')\n        # general params\n        self.parser.add_argument(\'--num_threads\', default=3, type=int, help=\'# threads for loading data\')\n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--name\', type=str, default=\'debug\', help=\'name of the experiment. It decides where to store samples and models\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes meshes in order, otherwise takes them randomly\')\n        self.parser.add_argument(\'--seed\', type=int, help=\'if specified, uses seed\')\n        # visualization params\n        self.parser.add_argument(\'--export_folder\', type=str, default=\'\', help=\'exports intermediate collapses to this folder\')\n        #\n        self.initialized = True\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt, unknown = self.parser.parse_known_args()\n        self.opt.is_train = self.is_train   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        if self.opt.seed is not None:\n            import numpy as np\n            import random\n            torch.manual_seed(self.opt.seed)\n            np.random.seed(self.opt.seed)\n            random.seed(self.opt.seed)\n\n        if self.opt.export_folder:\n            self.opt.export_folder = os.path.join(self.opt.checkpoints_dir, self.opt.name, self.opt.export_folder)\n            util.mkdir(self.opt.export_folder)\n\n        if self.is_train:\n            print(\'------------ Options -------------\')\n            for k, v in sorted(args.items()):\n                print(\'%s: %s\' % (str(k), str(v)))\n            print(\'-------------- End ----------------\')\n\n            # save to the disk\n            expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n            util.mkdir(expr_dir)\n\n            file_name = os.path.join(expr_dir, \'opt.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(\'------------ Options -------------\\n\')\n                for k, v in sorted(args.items()):\n                    opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n                opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
options/test_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n        self.parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc') #todo delete.\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--num_aug', type=int, default=1, help='# of augmentation files')\n        self.is_train = False"""
options/train_options.py,0,"b""from .base_options import BaseOptions\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument('--print_freq', type=int, default=10, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=250, help='frequency of saving the latest results')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency of saving checkpoints at the end of epochs')\n        self.parser.add_argument('--run_test_freq', type=int, default=1, help='frequency of running test in training script')\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=500, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.9, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        self.parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n        self.parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n        # data augmentation stuff\n        self.parser.add_argument('--num_aug', type=int, default=10, help='# of augmentation files')\n        self.parser.add_argument('--scale_verts', action='store_true', help='non-uniformly scale the mesh e.g., in x, y or z')\n        self.parser.add_argument('--slide_verts', type=float, default=0, help='percent vertices which will be shifted along the mesh surface')\n        self.parser.add_argument('--flip_edges', type=float, default=0, help='percent of edges to randomly flip')\n        # tensorboard visualization\n        self.parser.add_argument('--no_vis', action='store_true', help='will not use tensorboard')\n        self.parser.add_argument('--verbose_plot', action='store_true', help='plots network weights, etc.')\n        self.is_train = True\n"""
scripts/test_general.py,0,"b'import pytest\nimport os\nimport shutil\nimport glob\nimport subprocess\n\'\'\'\nscripts for unit testing\n\'\'\'\n\n\ndef get_data(dset):\n    dpaths = glob.glob(\'./datasets/{}*\'.format(dset))\n    [shutil.rmtree(d) for d in dpaths]\n    cmd = \'./scripts/{}/get_data.sh > /dev/null 2>&1\'.format(dset)\n    os.system(cmd)\n\ndef add_args(file, temp_file, new_args):\n    with open(file) as f:\n        tokens = f.readlines()\n    # now make the config so it only trains for one iteration\n    tokens[-1] = tokens[-1] + \'\\n\'\n    for arg in new_args:\n        tokens.append(arg)\n    with open(temp_file, \'w\') as f:\n        f.writelines(tokens)\n\ndef run_train(dset):\n    train_file = \'./scripts/{}/train.sh\'.format(dset)\n    temp_train_file = \'./scripts/{}/train_temp.sh\'.format(dset)\n    p = subprocess.run([\'cp\', \'-p\', \'--preserve\', train_file, temp_train_file])\n    add_args(train_file, temp_train_file, [\'--niter_decay 0 \\\\\\n\', \'--niter 1 \\\\\\n\', \'--max_dataset_size 2 \\\\\\n\', \'--gpu_ids -1 \\\\\'])\n    cmd = ""bash -c \'source ~/anaconda3/bin/activate ~/anaconda3/envs/meshcnn && {} > /dev/null 2>&1\'"".format(temp_train_file)\n    os.system(cmd)\n    os.remove(temp_train_file)\n\ndef get_pretrained(dset):\n    cmd = \'./scripts/{}/get_pretrained.sh > /dev/null 2>&1\'.format(dset)\n    os.system(cmd)\n\ndef run_test(dset):\n    test_file = \'./scripts/{}/test.sh\'.format(dset)\n    temp_test_file = \'./scripts/{}/test_temp.sh\'.format(dset)\n    p = subprocess.run([\'cp\', \'-p\', \'--preserve\', test_file, temp_test_file])\n    add_args(test_file, temp_test_file, [\'--gpu_ids -1 \\\\\'])\n    # now run inference\n    cmd = ""bash -c \'source ~/anaconda3/bin/activate ~/anaconda3/envs/meshcnn && {}\'"".format(temp_test_file)\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n    (_out, err) = proc.communicate()\n    out = str(_out)\n    idf0 = \'TEST ACC: [\'\n    token = out[out.find(idf0) + len(idf0):]\n    idf1 = \'%]\'\n    accs = token[:token.find(idf1)]\n    acc = float(accs)\n    if dset == \'shrec\':\n        assert acc == 99.167, ""shrec accuracy was {} and not 99.167"".format(acc)\n    if dset == \'human_seg\':\n        assert acc == 92.554, ""human_seg accuracy was {} and not 92.554"".format(acc)\n    os.remove(temp_test_file)\n\ndef run_dataset(dset):\n    get_data(dset)\n    run_train(dset)\n    get_pretrained(dset)\n    run_test(dset)\n\ndef test_shrec():\n    run_dataset(\'shrec\')\n\ndef test_human_seg():\n    run_dataset(\'human_seg\')'"
util/__init__.py,0,b''
util/mesh_viewer.py,0,"b'import mpl_toolkits.mplot3d as a3\r\nimport matplotlib.colors as colors\r\nimport pylab as pl\r\nimport numpy as np\r\n\r\nV = np.array\r\nr2h = lambda x: colors.rgb2hex(tuple(map(lambda y: y / 255., x)))\r\nsurface_color = r2h((255, 230, 205))\r\nedge_color = r2h((90, 90, 90))\r\nedge_colors = (r2h((15, 167, 175)), r2h((230, 81, 81)), r2h((142, 105, 252)), r2h((248, 235, 57)),\r\n               r2h((51, 159, 255)), r2h((225, 117, 231)), r2h((97, 243, 185)), r2h((161, 183, 196)))\r\n\r\n\r\n\r\n\r\ndef init_plot():\r\n    ax = pl.figure().add_subplot(111, projection=\'3d\')\r\n    # hide axis, thank to\r\n    # https://stackoverflow.com/questions/29041326/3d-plot-with-matplotlib-hide-axes-but-keep-axis-labels/\r\n    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\r\n    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\r\n    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\r\n    # Get rid of the spines\r\n    ax.w_xaxis.line.set_color((1.0, 1.0, 1.0, 0.0))\r\n    ax.w_yaxis.line.set_color((1.0, 1.0, 1.0, 0.0))\r\n    ax.w_zaxis.line.set_color((1.0, 1.0, 1.0, 0.0))\r\n    # Get rid of the ticks\r\n    ax.set_xticks([])\r\n    ax.set_yticks([])\r\n    ax.set_zticks([])\r\n    return (ax, [np.inf, -np.inf, np.inf, -np.inf, np.inf, -np.inf])\r\n\r\n\r\ndef update_lim(mesh, plot):\r\n    vs = mesh[0]\r\n    for i in range(3):\r\n        plot[1][2 * i] = min(plot[1][2 * i], vs[:, i].min())\r\n        plot[1][2 * i + 1] = max(plot[1][2 * i], vs[:, i].max())\r\n    return plot\r\n\r\n\r\ndef update_plot(mesh, plot):\r\n    if plot is None:\r\n        plot = init_plot()\r\n    return update_lim(mesh, plot)\r\n\r\n\r\ndef surfaces(mesh, plot):\r\n    vs, faces, edges = mesh\r\n    vtx = vs[faces]\r\n    edgecolor = edge_color if not len(edges) else \'none\'\r\n    tri = a3.art3d.Poly3DCollection(vtx, facecolors=surface_color +\'55\', edgecolors=edgecolor,\r\n                                    linewidths=.5, linestyles=\'dashdot\')\r\n    plot[0].add_collection3d(tri)\r\n    return plot\r\n\r\n\r\ndef segments(mesh, plot):\r\n    vs, _, edges = mesh\r\n    for edge_c, edge_group in enumerate(edges):\r\n        for edge_idx in edge_group:\r\n            edge = vs[edge_idx]\r\n            line = a3.art3d.Line3DCollection([edge],  linewidths=.5, linestyles=\'dashdot\')\r\n            line.set_color(edge_colors[edge_c % len(edge_colors)])\r\n            plot[0].add_collection3d(line)\r\n    return plot\r\n\r\n\r\ndef plot_mesh(mesh, *whats, show=True, plot=None):\r\n    for what in [update_plot] + list(whats):\r\n        plot = what(mesh, plot)\r\n    if show:\r\n        li = max(plot[1][1], plot[1][3], plot[1][5])\r\n        plot[0].auto_scale_xyz([0, li], [0, li], [0, li])\r\n        pl.tight_layout()\r\n        pl.show()\r\n    return plot\r\n\r\n\r\ndef parse_obje(obj_file, scale_by):\r\n    vs = []\r\n    faces = []\r\n    edges = []\r\n\r\n    def add_to_edges():\r\n        if edge_c >= len(edges):\r\n            for _ in range(len(edges), edge_c + 1):\r\n                edges.append([])\r\n        edges[edge_c].append(edge_v)\r\n\r\n    def fix_vertices():\r\n        nonlocal vs, scale_by\r\n        vs = V(vs)\r\n        z = vs[:, 2].copy()\r\n        vs[:, 2] = vs[:, 1]\r\n        vs[:, 1] = z\r\n        max_range = 0\r\n        for i in range(3):\r\n            min_value = np.min(vs[:, i])\r\n            max_value = np.max(vs[:, i])\r\n            max_range = max(max_range, max_value - min_value)\r\n            vs[:, i] -= min_value\r\n        if not scale_by:\r\n            scale_by = max_range\r\n        vs /= scale_by\r\n\r\n    with open(obj_file) as f:\r\n        for line in f:\r\n            line = line.strip()\r\n            splitted_line = line.split()\r\n            if not splitted_line:\r\n                continue\r\n            elif splitted_line[0] == \'v\':\r\n                vs.append([float(v) for v in splitted_line[1:]])\r\n            elif splitted_line[0] == \'f\':\r\n                faces.append([int(c) - 1 for c in splitted_line[1:]])\r\n            elif splitted_line[0] == \'e\':\r\n                if len(splitted_line) >= 4:\r\n                    edge_v = [int(c) - 1 for c in splitted_line[1:-1]]\r\n                    edge_c = int(splitted_line[-1])\r\n                    add_to_edges()\r\n\r\n    vs = V(vs)\r\n    fix_vertices()\r\n    faces = V(faces, dtype=int)\r\n    edges = [V(c, dtype=int) for c in edges]\r\n    return (vs, faces, edges), scale_by\r\n\r\n\r\ndef view_meshes(*files, offset=.2):\r\n    plot = None\r\n    max_x = 0\r\n    scale = 0\r\n    for file in files:\r\n        mesh, scale = parse_obje(file, scale)\r\n        max_x_current = mesh[0][:, 0].max()\r\n        mesh[0][:, 0] += max_x + offset\r\n        plot = plot_mesh(mesh, surfaces, segments, plot=plot, show=file == files[-1])\r\n        max_x += max_x_current + offset\r\n\r\n\r\nif __name__==\'__main__\':\r\n    import argparse\r\n    parser = argparse.ArgumentParser(""view meshes"")\r\n    parser.add_argument(\'--files\', nargs=\'+\', default=[\'checkpoints/human_seg/meshes/shrec__14_0.obj\',\r\n                                                       \'checkpoints/human_seg/meshes/shrec__14_3.obj\'], type=str,\r\n                        help=""list of 1 or more .obj files"")\r\n    args = parser.parse_args()\r\n\r\n    # view meshes\r\n    view_meshes(*args.files)\r\n\r\n'"
util/util.py,1,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nimport os\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\nMESH_EXTENSIONS = [\n    \'.obj\',\n]\n\ndef is_mesh_file(filename):\n    return any(filename.endswith(extension) for extension in MESH_EXTENSIONS)\n\ndef pad(input_arr, target_length, val=0, dim=1):\n    shp = input_arr.shape\n    npad = [(0, 0) for _ in range(len(shp))]\n    npad[dim] = (0, target_length - shp[dim])\n    return np.pad(input_arr, pad_width=npad, mode=\'constant\', constant_values=val)\n\ndef seg_accuracy(predicted, ssegs, meshes):\n    correct = 0\n    ssegs = ssegs.squeeze(-1)\n    correct_mat = ssegs.gather(2, predicted.cpu().unsqueeze(dim=2))\n    for mesh_id, mesh in enumerate(meshes):\n        correct_vec = correct_mat[mesh_id, :mesh.edges_count, 0]\n        edge_areas = torch.from_numpy(mesh.get_edge_areas())\n        correct += (correct_vec.float() * edge_areas).sum()\n    return correct\n\ndef print_network(net):\n    """"""Print the total number of parameters in the network\n    Parameters:\n        network\n    """"""\n    print(\'---------- Network initialized -------------\')\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(\'[Network] Total number of parameters : %.3f M\' % (num_params / 1e6))\n    print(\'-----------------------------------------------\')\n\ndef get_heatmap_color(value, minimum=0, maximum=1):\n    minimum, maximum = float(minimum), float(maximum)\n    ratio = 2 * (value-minimum) / (maximum - minimum)\n    b = int(max(0, 255*(1 - ratio)))\n    r = int(max(0, 255*(ratio - 1)))\n    g = 255 - b - r\n    return r, g, b\n\n\ndef normalize_np_array(np_array):\n    min_value = np.min(np_array)\n    max_value = np.max(np_array)\n    return (np_array - min_value) / (max_value - min_value)\n\n\ndef calculate_entropy(np_array):\n    entropy = 0\n    np_array /= np.sum(np_array)\n    for a in np_array:\n        if a != 0:\n            entropy -= a * np.log(a)\n    entropy /= np.log(np_array.shape[0])\n    return entropy\n'"
util/writer.py,0,"b'import os\nimport time\n\ntry:\n    from tensorboardX import SummaryWriter\nexcept ImportError as error:\n    print(\'tensorboard X not installed, visualizing wont be available\')\n    SummaryWriter = None\n\nclass Writer:\n    def __init__(self, opt):\n        self.name = opt.name\n        self.opt = opt\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        self.log_name = os.path.join(self.save_dir, \'loss_log.txt\')\n        self.testacc_log = os.path.join(self.save_dir, \'testacc_log.txt\')\n        self.start_logs()\n        self.nexamples = 0\n        self.ncorrect = 0\n        #\n        if opt.is_train and not opt.no_vis and SummaryWriter is not None:\n            self.display = SummaryWriter(comment=opt.name)\n        else:\n            self.display = None\n\n    def start_logs(self):\n        """""" creates test / train log files """"""\n        if self.opt.is_train:\n            with open(self.log_name, ""a"") as log_file:\n                now = time.strftime(""%c"")\n                log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n        else:\n            with open(self.testacc_log, ""a"") as log_file:\n                now = time.strftime(""%c"")\n                log_file.write(\'================ Testing Acc (%s) ================\\n\' % now)\n\n    def print_current_losses(self, epoch, i, losses, t, t_data):\n        """""" prints train loss to terminal / file """"""\n        message = \'(epoch: %d, iters: %d, time: %.3f, data: %.3f) loss: %.3f \' \\\n                  % (epoch, i, t, t_data, losses.item())\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    def plot_loss(self, loss, epoch, i, n):\n        iters = i + (epoch - 1) * n\n        if self.display:\n            self.display.add_scalar(\'data/train_loss\', loss, iters)\n\n    def plot_model_wts(self, model, epoch):\n        if self.opt.is_train and self.display:\n            for name, param in model.net.named_parameters():\n                self.display.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n\n    def print_acc(self, epoch, acc):\n        """""" prints test accuracy to terminal / file """"""\n        message = \'epoch: {}, TEST ACC: [{:.5} %]\\n\' \\\n            .format(epoch, acc * 100)\n        print(message)\n        with open(self.testacc_log, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    def plot_acc(self, acc, epoch):\n        if self.display:\n            self.display.add_scalar(\'data/test_acc\', acc, epoch)\n\n    def reset_counter(self):\n        """"""\n        counts # of correct examples\n        """"""\n        self.ncorrect = 0\n        self.nexamples = 0\n\n    def update_counter(self, ncorrect, nexamples):\n        self.ncorrect += ncorrect\n        self.nexamples += nexamples\n\n    @property\n    def acc(self):\n        return float(self.ncorrect) / self.nexamples\n\n    def close(self):\n        if self.display is not None:\n            self.display.close()\n'"
models/layers/__init__.py,0,b''
models/layers/mesh.py,2,"b'from tempfile import mkstemp\nfrom shutil import move\nimport torch\nimport numpy as np\nimport os\nfrom models.layers.mesh_union import MeshUnion\nfrom models.layers.mesh_prepare import fill_mesh\n\n\nclass Mesh:\n\n    def __init__(self, file=None, opt=None, hold_history=False, export_folder=\'\'):\n        self.vs = self.v_mask = self.filename = self.features = self.edge_areas = None\n        self.edges = self.gemm_edges = self.sides = None\n        self.pool_count = 0\n        fill_mesh(self, file, opt)\n        self.export_folder = export_folder\n        self.history_data = None\n        if hold_history:\n            self.init_history()\n        self.export()\n\n    def extract_features(self):\n        return self.features\n\n    def merge_vertices(self, edge_id):\n        self.remove_edge(edge_id)\n        edge = self.edges[edge_id]\n        v_a = self.vs[edge[0]]\n        v_b = self.vs[edge[1]]\n        # update pA\n        v_a.__iadd__(v_b)\n        v_a.__itruediv__(2)\n        self.v_mask[edge[1]] = False\n        mask = self.edges == edge[1]\n        self.ve[edge[0]].extend(self.ve[edge[1]])\n        self.edges[mask] = edge[0]\n\n    def remove_vertex(self, v):\n        self.v_mask[v] = False\n\n    def remove_edge(self, edge_id):\n        vs = self.edges[edge_id]\n        for v in vs:\n            if edge_id not in self.ve[v]:\n                print(self.ve[v])\n                print(self.filename)\n            self.ve[v].remove(edge_id)\n\n    def clean(self, edges_mask, groups):\n        edges_mask = edges_mask.astype(bool)\n        torch_mask = torch.from_numpy(edges_mask.copy())\n        self.gemm_edges = self.gemm_edges[edges_mask]\n        self.edges = self.edges[edges_mask]\n        self.sides = self.sides[edges_mask]\n        new_ve = []\n        edges_mask = np.concatenate([edges_mask, [False]])\n        new_indices = np.zeros(edges_mask.shape[0], dtype=np.int32)\n        new_indices[-1] = -1\n        new_indices[edges_mask] = np.arange(0, np.ma.where(edges_mask)[0].shape[0])\n        self.gemm_edges[:, :] = new_indices[self.gemm_edges[:, :]]\n        for v_index, ve in enumerate(self.ve):\n            update_ve = []\n            # if self.v_mask[v_index]:\n            for e in ve:\n                update_ve.append(new_indices[e])\n            new_ve.append(update_ve)\n        self.ve = new_ve\n        self.__clean_history(groups, torch_mask)\n        self.pool_count += 1\n        self.export()\n\n\n    def export(self, file=None, vcolor=None):\n        if file is None:\n            if self.export_folder:\n                filename, file_extension = os.path.splitext(self.filename)\n                file = \'%s/%s_%d%s\' % (self.export_folder, filename, self.pool_count, file_extension)\n            else:\n                return\n        faces = []\n        vs = self.vs[self.v_mask]\n        gemm = np.array(self.gemm_edges)\n        new_indices = np.zeros(self.v_mask.shape[0], dtype=np.int32)\n        new_indices[self.v_mask] = np.arange(0, np.ma.where(self.v_mask)[0].shape[0])\n        for edge_index in range(len(gemm)):\n            cycles = self.__get_cycle(gemm, edge_index)\n            for cycle in cycles:\n                faces.append(self.__cycle_to_face(cycle, new_indices))\n        with open(file, \'w+\') as f:\n            for vi, v in enumerate(vs):\n                vcol = \' %f %f %f\' % (vcolor[vi, 0], vcolor[vi, 1], vcolor[vi, 2]) if vcolor is not None else \'\'\n                f.write(""v %f %f %f%s\\n"" % (v[0], v[1], v[2], vcol))\n            for face_id in range(len(faces) - 1):\n                f.write(""f %d %d %d\\n"" % (faces[face_id][0] + 1, faces[face_id][1] + 1, faces[face_id][2] + 1))\n            f.write(""f %d %d %d"" % (faces[-1][0] + 1, faces[-1][1] + 1, faces[-1][2] + 1))\n            for edge in self.edges:\n                f.write(""\\ne %d %d"" % (new_indices[edge[0]] + 1, new_indices[edge[1]] + 1))\n\n    def export_segments(self, segments):\n        if not self.export_folder:\n            return\n        cur_segments = segments\n        for i in range(self.pool_count + 1):\n            filename, file_extension = os.path.splitext(self.filename)\n            file = \'%s/%s_%d%s\' % (self.export_folder, filename, i, file_extension)\n            fh, abs_path = mkstemp()\n            edge_key = 0\n            with os.fdopen(fh, \'w\') as new_file:\n                with open(file) as old_file:\n                    for line in old_file:\n                        if line[0] == \'e\':\n                            new_file.write(\'%s %d\' % (line.strip(), cur_segments[edge_key]))\n                            if edge_key < len(cur_segments):\n                                edge_key += 1\n                                new_file.write(\'\\n\')\n                        else:\n                            new_file.write(line)\n            os.remove(file)\n            move(abs_path, file)\n            if i < len(self.history_data[\'edges_mask\']):\n                cur_segments = segments[:len(self.history_data[\'edges_mask\'][i])]\n                cur_segments = cur_segments[self.history_data[\'edges_mask\'][i]]\n\n    def __get_cycle(self, gemm, edge_id):\n        cycles = []\n        for j in range(2):\n            next_side = start_point = j * 2\n            next_key = edge_id\n            if gemm[edge_id, start_point] == -1:\n                continue\n            cycles.append([])\n            for i in range(3):\n                tmp_next_key = gemm[next_key, next_side]\n                tmp_next_side = self.sides[next_key, next_side]\n                tmp_next_side = tmp_next_side + 1 - 2 * (tmp_next_side % 2)\n                gemm[next_key, next_side] = -1\n                gemm[next_key, next_side + 1 - 2 * (next_side % 2)] = -1\n                next_key = tmp_next_key\n                next_side = tmp_next_side\n                cycles[-1].append(next_key)\n        return cycles\n\n    def __cycle_to_face(self, cycle, v_indices):\n        face = []\n        for i in range(3):\n            v = list(set(self.edges[cycle[i]]) & set(self.edges[cycle[(i + 1) % 3]]))[0]\n            face.append(v_indices[v])\n        return face\n\n    def init_history(self):\n        self.history_data = {\n                               \'groups\': [],\n                               \'gemm_edges\': [self.gemm_edges.copy()],\n                               \'occurrences\': [],\n                               \'old2current\': np.arange(self.edges_count, dtype=np.int32),\n                               \'current2old\': np.arange(self.edges_count, dtype=np.int32),\n                               \'edges_mask\': [torch.ones(self.edges_count,dtype=torch.bool)],\n                               \'edges_count\': [self.edges_count],\n                              }\n        if self.export_folder:\n            self.history_data[\'collapses\'] = MeshUnion(self.edges_count)\n\n    def union_groups(self, source, target):\n        if self.export_folder and self.history_data:\n            self.history_data[\'collapses\'].union(self.history_data[\'current2old\'][source], self.history_data[\'current2old\'][target])\n        return\n\n    def remove_group(self, index):\n        if self.history_data is not None:\n            self.history_data[\'edges_mask\'][-1][self.history_data[\'current2old\'][index]] = 0\n            self.history_data[\'old2current\'][self.history_data[\'current2old\'][index]] = -1\n            if self.export_folder:\n                self.history_data[\'collapses\'].remove_group(self.history_data[\'current2old\'][index])\n\n    def get_groups(self):\n        return self.history_data[\'groups\'].pop()\n\n    def get_occurrences(self):\n        return self.history_data[\'occurrences\'].pop()\n    \n    def __clean_history(self, groups, pool_mask):\n        if self.history_data is not None:\n            mask = self.history_data[\'old2current\'] != -1\n            self.history_data[\'old2current\'][mask] = np.arange(self.edges_count, dtype=np.int32)\n            self.history_data[\'current2old\'][0: self.edges_count] = np.ma.where(mask)[0]\n            if self.export_folder != \'\':\n                self.history_data[\'edges_mask\'].append(self.history_data[\'edges_mask\'][-1].clone())\n            self.history_data[\'occurrences\'].append(groups.get_occurrences())\n            self.history_data[\'groups\'].append(groups.get_groups(pool_mask))\n            self.history_data[\'gemm_edges\'].append(self.gemm_edges.copy())\n            self.history_data[\'edges_count\'].append(self.edges_count)\n    \n    def unroll_gemm(self):\n        self.history_data[\'gemm_edges\'].pop()\n        self.gemm_edges = self.history_data[\'gemm_edges\'][-1]\n        self.history_data[\'edges_count\'].pop()\n        self.edges_count = self.history_data[\'edges_count\'][-1]\n\n    def get_edge_areas(self):\n        return self.edge_areas\n'"
models/layers/mesh_conv.py,12,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MeshConv(nn.Module):\n    """""" Computes convolution between edges and 4 incident (1-ring) edge neighbors\n    in the forward pass takes:\n    x: edge features (Batch x Features x Edges)\n    mesh: list of mesh data-structure (len(mesh) == Batch)\n    and applies convolution\n    """"""\n    def __init__(self, in_channels, out_channels, k=5, bias=True):\n        super(MeshConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, k), bias=bias)\n        self.k = k\n\n    def __call__(self, edge_f, mesh):\n        return self.forward(edge_f, mesh)\n\n    def forward(self, x, mesh):\n        x = x.squeeze(-1)\n        G = torch.cat([self.pad_gemm(i, x.shape[2], x.device) for i in mesh], 0)\n        # build \'neighborhood image\' and apply convolution\n        G = self.create_GeMM(x, G)\n        x = self.conv(G)\n        return x\n\n    def flatten_gemm_inds(self, Gi):\n        (b, ne, nn) = Gi.shape\n        ne += 1\n        batch_n = torch.floor(torch.arange(b * ne, device=Gi.device).float() / ne).view(b, ne)\n        add_fac = batch_n * ne\n        add_fac = add_fac.view(b, ne, 1)\n        add_fac = add_fac.repeat(1, 1, nn)\n        # flatten Gi\n        Gi = Gi.float() + add_fac[:, 1:, :]\n        return Gi\n\n    def create_GeMM(self, x, Gi):\n        """""" gathers the edge features (x) with from the 1-ring indices (Gi)\n        applys symmetric functions to handle order invariance\n        returns a \'fake image\' which can use 2d convolution on\n        output dimensions: Batch x Channels x Edges x 5\n        """"""\n        Gishape = Gi.shape\n        # pad the first row of  every sample in batch with zeros\n        padding = torch.zeros((x.shape[0], x.shape[1], 1), requires_grad=True, device=x.device)\n        # padding = padding.to(x.device)\n        x = torch.cat((padding, x), dim=2)\n        Gi = Gi + 1 #shift\n\n        # first flatten indices\n        Gi_flat = self.flatten_gemm_inds(Gi)\n        Gi_flat = Gi_flat.view(-1).long()\n        #\n        odim = x.shape\n        x = x.permute(0, 2, 1).contiguous()\n        x = x.view(odim[0] * odim[2], odim[1])\n\n        f = torch.index_select(x, dim=0, index=Gi_flat)\n        f = f.view(Gishape[0], Gishape[1], Gishape[2], -1)\n        f = f.permute(0, 3, 1, 2)\n\n        # apply the symmetric functions for an equivariant conv\n        x_1 = f[:, :, :, 1] + f[:, :, :, 3]\n        x_2 = f[:, :, :, 2] + f[:, :, :, 4]\n        x_3 = torch.abs(f[:, :, :, 1] - f[:, :, :, 3])\n        x_4 = torch.abs(f[:, :, :, 2] - f[:, :, :, 4])\n        f = torch.stack([f[:, :, :, 0], x_1, x_2, x_3, x_4], dim=3)\n        return f\n\n    def pad_gemm(self, m, xsz, device):\n        """""" extracts one-ring neighbors (4x) -> m.gemm_edges\n        which is of size #edges x 4\n        add the edge_id itself to make #edges x 5\n        then pad to desired size e.g., xsz x 5\n        """"""\n        padded_gemm = torch.tensor(m.gemm_edges, device=device).float()\n        padded_gemm = padded_gemm.requires_grad_()\n        padded_gemm = torch.cat((torch.arange(m.edges_count, device=device).float().unsqueeze(1), padded_gemm), dim=1)\n        # pad using F\n        padded_gemm = F.pad(padded_gemm, (0, 0, 0, xsz - m.edges_count), ""constant"", 0)\n        padded_gemm = padded_gemm.unsqueeze(0)\n        return padded_gemm\n'"
models/layers/mesh_pool.py,5,"b'import torch\nimport torch.nn as nn\nfrom threading import Thread\nfrom models.layers.mesh_union import MeshUnion\nimport numpy as np\nfrom heapq import heappop, heapify\n\n\nclass MeshPool(nn.Module):\n    \n    def __init__(self, target, multi_thread=False):\n        super(MeshPool, self).__init__()\n        self.__out_target = target\n        self.__multi_thread = multi_thread\n        self.__fe = None\n        self.__updated_fe = None\n        self.__meshes = None\n        self.__merge_edges = [-1, -1]\n\n    def __call__(self, fe, meshes):\n        return self.forward(fe, meshes)\n\n    def forward(self, fe, meshes):\n        self.__updated_fe = [[] for _ in range(len(meshes))]\n        pool_threads = []\n        self.__fe = fe\n        self.__meshes = meshes\n        # iterate over batch\n        for mesh_index in range(len(meshes)):\n            if self.__multi_thread:\n                pool_threads.append(Thread(target=self.__pool_main, args=(mesh_index,)))\n                pool_threads[-1].start()\n            else:\n                self.__pool_main(mesh_index)\n        if self.__multi_thread:\n            for mesh_index in range(len(meshes)):\n                pool_threads[mesh_index].join()\n        out_features = torch.cat(self.__updated_fe).view(len(meshes), -1, self.__out_target)\n        return out_features\n\n    def __pool_main(self, mesh_index):\n        mesh = self.__meshes[mesh_index]\n        queue = self.__build_queue(self.__fe[mesh_index, :, :mesh.edges_count], mesh.edges_count)\n        # recycle = []\n        # last_queue_len = len(queue)\n        last_count = mesh.edges_count + 1\n        mask = np.ones(mesh.edges_count, dtype=np.bool)\n        edge_groups = MeshUnion(mesh.edges_count, self.__fe.device)\n        while mesh.edges_count > self.__out_target:\n            value, edge_id = heappop(queue)\n            edge_id = int(edge_id)\n            if mask[edge_id]:\n                self.__pool_edge(mesh, edge_id, mask, edge_groups)\n        mesh.clean(mask, edge_groups)\n        fe = edge_groups.rebuild_features(self.__fe[mesh_index], mask, self.__out_target)\n        self.__updated_fe[mesh_index] = fe\n\n    def __pool_edge(self, mesh, edge_id, mask, edge_groups):\n        if self.has_boundaries(mesh, edge_id):\n            return False\n        elif self.__clean_side(mesh, edge_id, mask, edge_groups, 0)\\\n            and self.__clean_side(mesh, edge_id, mask, edge_groups, 2) \\\n            and self.__is_one_ring_valid(mesh, edge_id):\n            self.__merge_edges[0] = self.__pool_side(mesh, edge_id, mask, edge_groups, 0)\n            self.__merge_edges[1] = self.__pool_side(mesh, edge_id, mask, edge_groups, 2)\n            mesh.merge_vertices(edge_id)\n            mask[edge_id] = False\n            MeshPool.__remove_group(mesh, edge_groups, edge_id)\n            mesh.edges_count -= 1\n            return True\n        else:\n            return False\n\n    def __clean_side(self, mesh, edge_id, mask, edge_groups, side):\n        if mesh.edges_count <= self.__out_target:\n            return False\n        invalid_edges = MeshPool.__get_invalids(mesh, edge_id, edge_groups, side)\n        while len(invalid_edges) != 0 and mesh.edges_count > self.__out_target:\n            self.__remove_triplete(mesh, mask, edge_groups, invalid_edges)\n            if mesh.edges_count <= self.__out_target:\n                return False\n            if self.has_boundaries(mesh, edge_id):\n                return False\n            invalid_edges = self.__get_invalids(mesh, edge_id, edge_groups, side)\n        return True\n\n    @staticmethod\n    def has_boundaries(mesh, edge_id):\n        for edge in mesh.gemm_edges[edge_id]:\n            if edge == -1 or -1 in mesh.gemm_edges[edge]:\n                return True\n        return False\n\n\n    @staticmethod\n    def __is_one_ring_valid(mesh, edge_id):\n        v_a = set(mesh.edges[mesh.ve[mesh.edges[edge_id, 0]]].reshape(-1))\n        v_b = set(mesh.edges[mesh.ve[mesh.edges[edge_id, 1]]].reshape(-1))\n        shared = v_a & v_b - set(mesh.edges[edge_id])\n        return len(shared) == 2\n\n    def __pool_side(self, mesh, edge_id, mask, edge_groups, side):\n        info = MeshPool.__get_face_info(mesh, edge_id, side)\n        key_a, key_b, side_a, side_b, _, other_side_b, _, other_keys_b = info\n        self.__redirect_edges(mesh, key_a, side_a - side_a % 2, other_keys_b[0], mesh.sides[key_b, other_side_b])\n        self.__redirect_edges(mesh, key_a, side_a - side_a % 2 + 1, other_keys_b[1], mesh.sides[key_b, other_side_b + 1])\n        MeshPool.__union_groups(mesh, edge_groups, key_b, key_a)\n        MeshPool.__union_groups(mesh, edge_groups, edge_id, key_a)\n        mask[key_b] = False\n        MeshPool.__remove_group(mesh, edge_groups, key_b)\n        mesh.remove_edge(key_b)\n        mesh.edges_count -= 1\n        return key_a\n\n    @staticmethod\n    def __get_invalids(mesh, edge_id, edge_groups, side):\n        info = MeshPool.__get_face_info(mesh, edge_id, side)\n        key_a, key_b, side_a, side_b, other_side_a, other_side_b, other_keys_a, other_keys_b = info\n        shared_items = MeshPool.__get_shared_items(other_keys_a, other_keys_b)\n        if len(shared_items) == 0:\n            return []\n        else:\n            assert (len(shared_items) == 2)\n            middle_edge = other_keys_a[shared_items[0]]\n            update_key_a = other_keys_a[1 - shared_items[0]]\n            update_key_b = other_keys_b[1 - shared_items[1]]\n            update_side_a = mesh.sides[key_a, other_side_a + 1 - shared_items[0]]\n            update_side_b = mesh.sides[key_b, other_side_b + 1 - shared_items[1]]\n            MeshPool.__redirect_edges(mesh, edge_id, side, update_key_a, update_side_a)\n            MeshPool.__redirect_edges(mesh, edge_id, side + 1, update_key_b, update_side_b)\n            MeshPool.__redirect_edges(mesh, update_key_a, MeshPool.__get_other_side(update_side_a), update_key_b, MeshPool.__get_other_side(update_side_b))\n            MeshPool.__union_groups(mesh, edge_groups, key_a, edge_id)\n            MeshPool.__union_groups(mesh, edge_groups, key_b, edge_id)\n            MeshPool.__union_groups(mesh, edge_groups, key_a, update_key_a)\n            MeshPool.__union_groups(mesh, edge_groups, middle_edge, update_key_a)\n            MeshPool.__union_groups(mesh, edge_groups, key_b, update_key_b)\n            MeshPool.__union_groups(mesh, edge_groups, middle_edge, update_key_b)\n            return [key_a, key_b, middle_edge]\n\n    @staticmethod\n    def __redirect_edges(mesh, edge_a_key, side_a, edge_b_key, side_b):\n        mesh.gemm_edges[edge_a_key, side_a] = edge_b_key\n        mesh.gemm_edges[edge_b_key, side_b] = edge_a_key\n        mesh.sides[edge_a_key, side_a] = side_b\n        mesh.sides[edge_b_key, side_b] = side_a\n\n    @staticmethod\n    def __get_shared_items(list_a, list_b):\n        shared_items = []\n        for i in range(len(list_a)):\n            for j in range(len(list_b)):\n                if list_a[i] == list_b[j]:\n                    shared_items.extend([i, j])\n        return shared_items\n\n    @staticmethod\n    def __get_other_side(side):\n        return side + 1 - 2 * (side % 2)\n\n    @staticmethod\n    def __get_face_info(mesh, edge_id, side):\n        key_a = mesh.gemm_edges[edge_id, side]\n        key_b = mesh.gemm_edges[edge_id, side + 1]\n        side_a = mesh.sides[edge_id, side]\n        side_b = mesh.sides[edge_id, side + 1]\n        other_side_a = (side_a - (side_a % 2) + 2) % 4\n        other_side_b = (side_b - (side_b % 2) + 2) % 4\n        other_keys_a = [mesh.gemm_edges[key_a, other_side_a], mesh.gemm_edges[key_a, other_side_a + 1]]\n        other_keys_b = [mesh.gemm_edges[key_b, other_side_b], mesh.gemm_edges[key_b, other_side_b + 1]]\n        return key_a, key_b, side_a, side_b, other_side_a, other_side_b, other_keys_a, other_keys_b\n\n    @staticmethod\n    def __remove_triplete(mesh, mask, edge_groups, invalid_edges):\n        vertex = set(mesh.edges[invalid_edges[0]])\n        for edge_key in invalid_edges:\n            vertex &= set(mesh.edges[edge_key])\n            mask[edge_key] = False\n            MeshPool.__remove_group(mesh, edge_groups, edge_key)\n        mesh.edges_count -= 3\n        vertex = list(vertex)\n        assert(len(vertex) == 1)\n        mesh.remove_vertex(vertex[0])\n\n    def __build_queue(self, features, edges_count):\n        # delete edges with smallest norm\n        squared_magnitude = torch.sum(features * features, 0)\n        if squared_magnitude.shape[-1] != 1:\n            squared_magnitude = squared_magnitude.unsqueeze(-1)\n        edge_ids = torch.arange(edges_count, device=squared_magnitude.device, dtype=torch.float32).unsqueeze(-1)\n        heap = torch.cat((squared_magnitude, edge_ids), dim=-1).tolist()\n        heapify(heap)\n        return heap\n\n    @staticmethod\n    def __union_groups(mesh, edge_groups, source, target):\n        edge_groups.union(source, target)\n        mesh.union_groups(source, target)\n\n    @staticmethod\n    def __remove_group(mesh, edge_groups, index):\n        edge_groups.remove_group(index)\n        mesh.remove_group(index)\n\n'"
models/layers/mesh_prepare.py,0,"b'import numpy as np\nimport os\nimport ntpath\n\n\ndef fill_mesh(mesh2fill, file: str, opt):\n    load_path = get_mesh_path(file, opt.num_aug)\n    if os.path.exists(load_path):\n        mesh_data = np.load(load_path, encoding=\'latin1\', allow_pickle=True)\n    else:\n        mesh_data = from_scratch(file, opt)\n        np.savez_compressed(load_path, gemm_edges=mesh_data.gemm_edges, vs=mesh_data.vs, edges=mesh_data.edges,\n                            edges_count=mesh_data.edges_count, ve=mesh_data.ve, v_mask=mesh_data.v_mask,\n                            filename=mesh_data.filename, sides=mesh_data.sides,\n                            edge_lengths=mesh_data.edge_lengths, edge_areas=mesh_data.edge_areas,\n                            features=mesh_data.features)\n    mesh2fill.vs = mesh_data[\'vs\']\n    mesh2fill.edges = mesh_data[\'edges\']\n    mesh2fill.gemm_edges = mesh_data[\'gemm_edges\']\n    mesh2fill.edges_count = int(mesh_data[\'edges_count\'])\n    mesh2fill.ve = mesh_data[\'ve\']\n    mesh2fill.v_mask = mesh_data[\'v_mask\']\n    mesh2fill.filename = str(mesh_data[\'filename\'])\n    mesh2fill.edge_lengths = mesh_data[\'edge_lengths\']\n    mesh2fill.edge_areas = mesh_data[\'edge_areas\']\n    mesh2fill.features = mesh_data[\'features\']\n    mesh2fill.sides = mesh_data[\'sides\']\n\ndef get_mesh_path(file: str, num_aug: int):\n    filename, _ = os.path.splitext(file)\n    dir_name = os.path.dirname(filename)\n    prefix = os.path.basename(filename)\n    load_dir = os.path.join(dir_name, \'cache\')\n    load_file = os.path.join(load_dir, \'%s_%03d.npz\' % (prefix, np.random.randint(0, num_aug)))\n    if not os.path.isdir(load_dir):\n        os.makedirs(load_dir, exist_ok=True)\n    return load_file\n\ndef from_scratch(file, opt):\n\n    class MeshPrep:\n        def __getitem__(self, item):\n            return eval(\'self.\' + item)\n\n    mesh_data = MeshPrep()\n    mesh_data.vs = mesh_data.edges = None\n    mesh_data.gemm_edges = mesh_data.sides = None\n    mesh_data.edges_count = None\n    mesh_data.ve = None\n    mesh_data.v_mask = None\n    mesh_data.filename = \'unknown\'\n    mesh_data.edge_lengths = None\n    mesh_data.edge_areas = []\n    mesh_data.vs, faces = fill_from_file(mesh_data, file)\n    mesh_data.v_mask = np.ones(len(mesh_data.vs), dtype=bool)\n    faces, face_areas = remove_non_manifolds(mesh_data, faces)\n    if opt.num_aug > 1:\n        faces = augmentation(mesh_data, opt, faces)\n    build_gemm(mesh_data, faces, face_areas)\n    if opt.num_aug > 1:\n        post_augmentation(mesh_data, opt)\n    mesh_data.features = extract_features(mesh_data)\n    return mesh_data\n\ndef fill_from_file(mesh, file):\n    mesh.filename = ntpath.split(file)[1]\n    mesh.fullfilename = file\n    vs, faces = [], []\n    f = open(file)\n    for line in f:\n        line = line.strip()\n        splitted_line = line.split()\n        if not splitted_line:\n            continue\n        elif splitted_line[0] == \'v\':\n            vs.append([float(v) for v in splitted_line[1:4]])\n        elif splitted_line[0] == \'f\':\n            face_vertex_ids = [int(c.split(\'/\')[0]) for c in splitted_line[1:]]\n            assert len(face_vertex_ids) == 3\n            face_vertex_ids = [(ind - 1) if (ind >= 0) else (len(vs) + ind)\n                               for ind in face_vertex_ids]\n            faces.append(face_vertex_ids)\n    f.close()\n    vs = np.asarray(vs)\n    faces = np.asarray(faces, dtype=int)\n    assert np.logical_and(faces >= 0, faces < len(vs)).all()\n    return vs, faces\n\n\ndef remove_non_manifolds(mesh, faces):\n    mesh.ve = [[] for _ in mesh.vs]\n    edges_set = set()\n    mask = np.ones(len(faces), dtype=bool)\n    _, face_areas = compute_face_normals_and_areas(mesh, faces)\n    for face_id, face in enumerate(faces):\n        if face_areas[face_id] == 0:\n            mask[face_id] = False\n            continue\n        faces_edges = []\n        is_manifold = False\n        for i in range(3):\n            cur_edge = (face[i], face[(i + 1) % 3])\n            if cur_edge in edges_set:\n                is_manifold = True\n                break\n            else:\n                faces_edges.append(cur_edge)\n        if is_manifold:\n            mask[face_id] = False\n        else:\n            for idx, edge in enumerate(faces_edges):\n                edges_set.add(edge)\n    return faces[mask], face_areas[mask]\n\n\ndef build_gemm(mesh, faces, face_areas):\n    """"""\n    gemm_edges: array (#E x 4) of the 4 one-ring neighbors for each edge\n    sides: array (#E x 4) indices (values of: 0,1,2,3) indicating where an edge is in the gemm_edge entry of the 4 neighboring edges\n    for example edge i -> gemm_edges[gemm_edges[i], sides[i]] == [i, i, i, i]\n    """"""\n    mesh.ve = [[] for _ in mesh.vs]\n    edge_nb = []\n    sides = []\n    edge2key = dict()\n    edges = []\n    edges_count = 0\n    nb_count = []\n    for face_id, face in enumerate(faces):\n        faces_edges = []\n        for i in range(3):\n            cur_edge = (face[i], face[(i + 1) % 3])\n            faces_edges.append(cur_edge)\n        for idx, edge in enumerate(faces_edges):\n            edge = tuple(sorted(list(edge)))\n            faces_edges[idx] = edge\n            if edge not in edge2key:\n                edge2key[edge] = edges_count\n                edges.append(list(edge))\n                edge_nb.append([-1, -1, -1, -1])\n                sides.append([-1, -1, -1, -1])\n                mesh.ve[edge[0]].append(edges_count)\n                mesh.ve[edge[1]].append(edges_count)\n                mesh.edge_areas.append(0)\n                nb_count.append(0)\n                edges_count += 1\n            mesh.edge_areas[edge2key[edge]] += face_areas[face_id] / 3\n        for idx, edge in enumerate(faces_edges):\n            edge_key = edge2key[edge]\n            edge_nb[edge_key][nb_count[edge_key]] = edge2key[faces_edges[(idx + 1) % 3]]\n            edge_nb[edge_key][nb_count[edge_key] + 1] = edge2key[faces_edges[(idx + 2) % 3]]\n            nb_count[edge_key] += 2\n        for idx, edge in enumerate(faces_edges):\n            edge_key = edge2key[edge]\n            sides[edge_key][nb_count[edge_key] - 2] = nb_count[edge2key[faces_edges[(idx + 1) % 3]]] - 1\n            sides[edge_key][nb_count[edge_key] - 1] = nb_count[edge2key[faces_edges[(idx + 2) % 3]]] - 2\n    mesh.edges = np.array(edges, dtype=np.int32)\n    mesh.gemm_edges = np.array(edge_nb, dtype=np.int64)\n    mesh.sides = np.array(sides, dtype=np.int64)\n    mesh.edges_count = edges_count\n    mesh.edge_areas = np.array(mesh.edge_areas, dtype=np.float32) / np.sum(face_areas) #todo whats the difference between edge_areas and edge_lenghts?\n\n\ndef compute_face_normals_and_areas(mesh, faces):\n    face_normals = np.cross(mesh.vs[faces[:, 1]] - mesh.vs[faces[:, 0]],\n                            mesh.vs[faces[:, 2]] - mesh.vs[faces[:, 1]])\n    face_areas = np.sqrt((face_normals ** 2).sum(axis=1))\n    face_normals /= face_areas[:, np.newaxis]\n    assert (not np.any(face_areas[:, np.newaxis] == 0)), \'has zero area face: %s\' % mesh.filename\n    face_areas *= 0.5\n    return face_normals, face_areas\n\n\n# Data augmentation methods\ndef augmentation(mesh, opt, faces=None):\n    if hasattr(opt, \'scale_verts\') and opt.scale_verts:\n        scale_verts(mesh)\n    if hasattr(opt, \'flip_edges\') and opt.flip_edges:\n        faces = flip_edges(mesh, opt.flip_edges, faces)\n    return faces\n\n\ndef post_augmentation(mesh, opt):\n    if hasattr(opt, \'slide_verts\') and opt.slide_verts:\n        slide_verts(mesh, opt.slide_verts)\n\n\ndef slide_verts(mesh, prct):\n    edge_points = get_edge_points(mesh)\n    dihedral = dihedral_angle(mesh, edge_points).squeeze() #todo make fixed_division epsilon=0\n    thr = np.mean(dihedral) + np.std(dihedral)\n    vids = np.random.permutation(len(mesh.ve))\n    target = int(prct * len(vids))\n    shifted = 0\n    for vi in vids:\n        if shifted < target:\n            edges = mesh.ve[vi]\n            if min(dihedral[edges]) > 2.65:\n                edge = mesh.edges[np.random.choice(edges)]\n                vi_t = edge[1] if vi == edge[0] else edge[0]\n                nv = mesh.vs[vi] + np.random.uniform(0.2, 0.5) * (mesh.vs[vi_t] - mesh.vs[vi])\n                mesh.vs[vi] = nv\n                shifted += 1\n        else:\n            break\n    mesh.shifted = shifted / len(mesh.ve)\n\n\ndef scale_verts(mesh, mean=1, var=0.1):\n    for i in range(mesh.vs.shape[1]):\n        mesh.vs[:, i] = mesh.vs[:, i] * np.random.normal(mean, var)\n\n\ndef angles_from_faces(mesh, edge_faces, faces):\n    normals = [None, None]\n    for i in range(2):\n        edge_a = mesh.vs[faces[edge_faces[:, i], 2]] - mesh.vs[faces[edge_faces[:, i], 1]]\n        edge_b = mesh.vs[faces[edge_faces[:, i], 1]] - mesh.vs[faces[edge_faces[:, i], 0]]\n        normals[i] = np.cross(edge_a, edge_b)\n        div = fixed_division(np.linalg.norm(normals[i], ord=2, axis=1), epsilon=0)\n        normals[i] /= div[:, np.newaxis]\n    dot = np.sum(normals[0] * normals[1], axis=1).clip(-1, 1)\n    angles = np.pi - np.arccos(dot)\n    return angles\n\n\ndef flip_edges(mesh, prct, faces):\n    edge_count, edge_faces, edges_dict = get_edge_faces(faces)\n    dihedral = angles_from_faces(mesh, edge_faces[:, 2:], faces)\n    edges2flip = np.random.permutation(edge_count)\n    # print(dihedral.min())\n    # print(dihedral.max())\n    target = int(prct * edge_count)\n    flipped = 0\n    for edge_key in edges2flip:\n        if flipped == target:\n            break\n        if dihedral[edge_key] > 2.7:\n            edge_info = edge_faces[edge_key]\n            if edge_info[3] == -1:\n                continue\n            new_edge = tuple(sorted(list(set(faces[edge_info[2]]) ^ set(faces[edge_info[3]]))))\n            if new_edge in edges_dict:\n                continue\n            new_faces = np.array(\n                [[edge_info[1], new_edge[0], new_edge[1]], [edge_info[0], new_edge[0], new_edge[1]]])\n            if check_area(mesh, new_faces):\n                del edges_dict[(edge_info[0], edge_info[1])]\n                edge_info[:2] = [new_edge[0], new_edge[1]]\n                edges_dict[new_edge] = edge_key\n                rebuild_face(faces[edge_info[2]], new_faces[0])\n                rebuild_face(faces[edge_info[3]], new_faces[1])\n                for i, face_id in enumerate([edge_info[2], edge_info[3]]):\n                    cur_face = faces[face_id]\n                    for j in range(3):\n                        cur_edge = tuple(sorted((cur_face[j], cur_face[(j + 1) % 3])))\n                        if cur_edge != new_edge:\n                            cur_edge_key = edges_dict[cur_edge]\n                            for idx, face_nb in enumerate(\n                                    [edge_faces[cur_edge_key, 2], edge_faces[cur_edge_key, 3]]):\n                                if face_nb == edge_info[2 + (i + 1) % 2]:\n                                    edge_faces[cur_edge_key, 2 + idx] = face_id\n                flipped += 1\n    # print(flipped)\n    return faces\n\n\ndef rebuild_face(face, new_face):\n    new_point = list(set(new_face) - set(face))[0]\n    for i in range(3):\n        if face[i] not in new_face:\n            face[i] = new_point\n            break\n    return face\n\ndef check_area(mesh, faces):\n    face_normals = np.cross(mesh.vs[faces[:, 1]] - mesh.vs[faces[:, 0]],\n                            mesh.vs[faces[:, 2]] - mesh.vs[faces[:, 1]])\n    face_areas = np.sqrt((face_normals ** 2).sum(axis=1))\n    face_areas *= 0.5\n    return face_areas[0] > 0 and face_areas[1] > 0\n\n\ndef get_edge_faces(faces):\n    edge_count = 0\n    edge_faces = []\n    edge2keys = dict()\n    for face_id, face in enumerate(faces):\n        for i in range(3):\n            cur_edge = tuple(sorted((face[i], face[(i + 1) % 3])))\n            if cur_edge not in edge2keys:\n                edge2keys[cur_edge] = edge_count\n                edge_count += 1\n                edge_faces.append(np.array([cur_edge[0], cur_edge[1], -1, -1]))\n            edge_key = edge2keys[cur_edge]\n            if edge_faces[edge_key][2] == -1:\n                edge_faces[edge_key][2] = face_id\n            else:\n                edge_faces[edge_key][3] = face_id\n    return edge_count, np.array(edge_faces), edge2keys\n\n\ndef set_edge_lengths(mesh, edge_points=None):\n    if edge_points is not None:\n        edge_points = get_edge_points(mesh)\n    edge_lengths = np.linalg.norm(mesh.vs[edge_points[:, 0]] - mesh.vs[edge_points[:, 1]], ord=2, axis=1)\n    mesh.edge_lengths = edge_lengths\n\n\ndef extract_features(mesh):\n    features = []\n    edge_points = get_edge_points(mesh)\n    set_edge_lengths(mesh, edge_points)\n    with np.errstate(divide=\'raise\'):\n        try:\n            for extractor in [dihedral_angle, symmetric_opposite_angles, symmetric_ratios]:\n                feature = extractor(mesh, edge_points)\n                features.append(feature)\n            return np.concatenate(features, axis=0)\n        except Exception as e:\n            print(e)\n            raise ValueError(mesh.filename, \'bad features\')\n\n\ndef dihedral_angle(mesh, edge_points):\n    normals_a = get_normals(mesh, edge_points, 0)\n    normals_b = get_normals(mesh, edge_points, 3)\n    dot = np.sum(normals_a * normals_b, axis=1).clip(-1, 1)\n    angles = np.expand_dims(np.pi - np.arccos(dot), axis=0)\n    return angles\n\n\ndef symmetric_opposite_angles(mesh, edge_points):\n    """""" computes two angles: one for each face shared between the edge\n        the angle is in each face opposite the edge\n        sort handles order ambiguity\n    """"""\n    angles_a = get_opposite_angles(mesh, edge_points, 0)\n    angles_b = get_opposite_angles(mesh, edge_points, 3)\n    angles = np.concatenate((np.expand_dims(angles_a, 0), np.expand_dims(angles_b, 0)), axis=0)\n    angles = np.sort(angles, axis=0)\n    return angles\n\n\ndef symmetric_ratios(mesh, edge_points):\n    """""" computes two ratios: one for each face shared between the edge\n        the ratio is between the height / base (edge) of each triangle\n        sort handles order ambiguity\n    """"""\n    ratios_a = get_ratios(mesh, edge_points, 0)\n    ratios_b = get_ratios(mesh, edge_points, 3)\n    ratios = np.concatenate((np.expand_dims(ratios_a, 0), np.expand_dims(ratios_b, 0)), axis=0)\n    return np.sort(ratios, axis=0)\n\n\ndef get_edge_points(mesh):\n    """""" returns: edge_points (#E x 4) tensor, with four vertex ids per edge\n        for example: edge_points[edge_id, 0] and edge_points[edge_id, 1] are the two vertices which define edge_id \n        each adjacent face to edge_id has another vertex, which is edge_points[edge_id, 2] or edge_points[edge_id, 3]\n    """"""\n    edge_points = np.zeros([mesh.edges_count, 4], dtype=np.int32)\n    for edge_id, edge in enumerate(mesh.edges):\n        edge_points[edge_id] = get_side_points(mesh, edge_id)\n        # edge_points[edge_id, 3:] = mesh.get_side_points(edge_id, 2)\n    return edge_points\n\n\ndef get_side_points(mesh, edge_id):\n    # if mesh.gemm_edges[edge_id, side] == -1:\n    #     return mesh.get_side_points(edge_id, ((side + 2) % 4))\n    # else:\n    edge_a = mesh.edges[edge_id]\n\n    if mesh.gemm_edges[edge_id, 0] == -1:\n        edge_b = mesh.edges[mesh.gemm_edges[edge_id, 2]]\n        edge_c = mesh.edges[mesh.gemm_edges[edge_id, 3]]\n    else:\n        edge_b = mesh.edges[mesh.gemm_edges[edge_id, 0]]\n        edge_c = mesh.edges[mesh.gemm_edges[edge_id, 1]]\n    if mesh.gemm_edges[edge_id, 2] == -1:\n        edge_d = mesh.edges[mesh.gemm_edges[edge_id, 0]]\n        edge_e = mesh.edges[mesh.gemm_edges[edge_id, 1]]\n    else:\n        edge_d = mesh.edges[mesh.gemm_edges[edge_id, 2]]\n        edge_e = mesh.edges[mesh.gemm_edges[edge_id, 3]]\n    first_vertex = 0\n    second_vertex = 0\n    third_vertex = 0\n    if edge_a[1] in edge_b:\n        first_vertex = 1\n    if edge_b[1] in edge_c:\n        second_vertex = 1\n    if edge_d[1] in edge_e:\n        third_vertex = 1\n    return [edge_a[first_vertex], edge_a[1 - first_vertex], edge_b[second_vertex], edge_d[third_vertex]]\n\n\ndef get_normals(mesh, edge_points, side):\n    edge_a = mesh.vs[edge_points[:, side // 2 + 2]] - mesh.vs[edge_points[:, side // 2]]\n    edge_b = mesh.vs[edge_points[:, 1 - side // 2]] - mesh.vs[edge_points[:, side // 2]]\n    normals = np.cross(edge_a, edge_b)\n    div = fixed_division(np.linalg.norm(normals, ord=2, axis=1), epsilon=0.1)\n    normals /= div[:, np.newaxis]\n    return normals\n\ndef get_opposite_angles(mesh, edge_points, side):\n    edges_a = mesh.vs[edge_points[:, side // 2]] - mesh.vs[edge_points[:, side // 2 + 2]]\n    edges_b = mesh.vs[edge_points[:, 1 - side // 2]] - mesh.vs[edge_points[:, side // 2 + 2]]\n\n    edges_a /= fixed_division(np.linalg.norm(edges_a, ord=2, axis=1), epsilon=0.1)[:, np.newaxis]\n    edges_b /= fixed_division(np.linalg.norm(edges_b, ord=2, axis=1), epsilon=0.1)[:, np.newaxis]\n    dot = np.sum(edges_a * edges_b, axis=1).clip(-1, 1)\n    return np.arccos(dot)\n\n\ndef get_ratios(mesh, edge_points, side):\n    edges_lengths = np.linalg.norm(mesh.vs[edge_points[:, side // 2]] - mesh.vs[edge_points[:, 1 - side // 2]],\n                                   ord=2, axis=1)\n    point_o = mesh.vs[edge_points[:, side // 2 + 2]]\n    point_a = mesh.vs[edge_points[:, side // 2]]\n    point_b = mesh.vs[edge_points[:, 1 - side // 2]]\n    line_ab = point_b - point_a\n    projection_length = np.sum(line_ab * (point_o - point_a), axis=1) / fixed_division(\n        np.linalg.norm(line_ab, ord=2, axis=1), epsilon=0.1)\n    closest_point = point_a + (projection_length / edges_lengths)[:, np.newaxis] * line_ab\n    d = np.linalg.norm(point_o - closest_point, ord=2, axis=1)\n    return d / edges_lengths\n\ndef fixed_division(to_div, epsilon):\n    if epsilon == 0:\n        to_div[to_div == 0] = 0.1\n    else:\n        to_div += epsilon\n    return to_div\n'"
models/layers/mesh_union.py,9,"b""import torch\nfrom torch.nn import ConstantPad2d\n\n\nclass MeshUnion:\n    def __init__(self, n, device=torch.device('cpu')):\n        self.__size = n\n        self.rebuild_features = self.rebuild_features_average\n        self.groups = torch.eye(n, device=device)\n\n    def union(self, source, target):\n        self.groups[target, :] += self.groups[source, :]\n\n    def remove_group(self, index):\n        return\n\n    def get_group(self, edge_key):\n        return self.groups[edge_key, :]\n\n    def get_occurrences(self):\n        return torch.sum(self.groups, 0)\n\n    def get_groups(self, tensor_mask):\n        self.groups = torch.clamp(self.groups, 0, 1)\n        return self.groups[tensor_mask, :]\n\n    def rebuild_features_average(self, features, mask, target_edges):\n        self.prepare_groups(features, mask)\n        fe = torch.matmul(features.squeeze(-1), self.groups)\n        occurrences = torch.sum(self.groups, 0).expand(fe.shape)\n        fe = fe / occurrences\n        padding_b = target_edges - fe.shape[1]\n        if padding_b > 0:\n            padding_b = ConstantPad2d((0, padding_b, 0, 0), 0)\n            fe = padding_b(fe)\n        return fe\n\n    def prepare_groups(self, features, mask):\n        tensor_mask = torch.from_numpy(mask)\n        self.groups = torch.clamp(self.groups[tensor_mask, :], 0, 1).transpose_(1, 0)\n        padding_a = features.shape[1] - self.groups.shape[0]\n        if padding_a > 0:\n            padding_a = ConstantPad2d((0, 0, 0, padding_a), 0)\n            self.groups = padding_a(self.groups)\n"""
models/layers/mesh_unpool.py,4,"b'import torch\nimport torch.nn as nn\n\n\n\nclass MeshUnpool(nn.Module):\n    def __init__(self, unroll_target):\n        super(MeshUnpool, self).__init__()\n        self.unroll_target = unroll_target\n\n    def __call__(self, features, meshes):\n        return self.forward(features, meshes)\n\n    def pad_groups(self, group, unroll_start):\n        start, end = group.shape\n        padding_rows =  unroll_start - start\n        padding_cols = self.unroll_target - end\n        if padding_rows != 0 or padding_cols !=0:\n            padding = nn.ConstantPad2d((0, padding_cols, 0, padding_rows), 0)\n            group = padding(group)\n        return group\n\n    def pad_occurrences(self, occurrences):\n        padding = self.unroll_target - occurrences.shape[0]\n        if padding != 0:\n            padding = nn.ConstantPad1d((0, padding), 1)\n            occurrences = padding(occurrences)\n        return occurrences\n\n    def forward(self, features, meshes):\n        batch_size, nf, edges = features.shape\n        groups = [self.pad_groups(mesh.get_groups(), edges) for mesh in meshes]\n        unroll_mat = torch.cat(groups, dim=0).view(batch_size, edges, -1)\n        occurrences = [self.pad_occurrences(mesh.get_occurrences()) for mesh in meshes]\n        occurrences = torch.cat(occurrences, dim=0).view(batch_size, 1, -1)\n        occurrences = occurrences.expand(unroll_mat.shape)\n        unroll_mat = unroll_mat / occurrences\n        unroll_mat = unroll_mat.to(features.device)\n        for mesh in meshes:\n            mesh.unroll_gemm()\n        return torch.matmul(features, unroll_mat)\n'"
scripts/dataprep/blender_process.py,0,"b'import bpy\nimport os\nimport sys\n\n\n\'\'\'\nSimplifies mesh to target number of faces\nRequires Blender 2.8\nAuthor: Rana Hanocka\n\n@input: \n    <obj_file>\n    <target_faces> number of target faces\n    <outfile> name of simplified .obj file\n\n@output:\n    simplified mesh .obj\n    to run it from cmd line:\n    /opt/blender/blender --background --python blender_process.py /home/rana/koala.obj 1000 /home/rana/koala_1000.obj\n\'\'\'\n\nclass Process:\n    def __init__(self, obj_file, target_faces, export_name):\n        mesh = self.load_obj(obj_file)\n        self.simplify(mesh, target_faces)\n        self.export_obj(mesh, export_name)\n\n    def load_obj(self, obj_file):\n        bpy.ops.import_scene.obj(filepath=obj_file, axis_forward=\'-Z\', axis_up=\'Y\', filter_glob=""*.obj;*.mtl"", use_edges=True,\n                                 use_smooth_groups=True, use_split_objects=False, use_split_groups=False,\n                                 use_groups_as_vgroups=False, use_image_search=True, split_mode=\'ON\')\n        ob = bpy.context.selected_objects[0]\n        return ob\n\n    def subsurf(self, mesh):\n        # subdivide mesh\n        bpy.context.view_layer.objects.active = mesh\n        mod = mesh.modifiers.new(name=\'Subsurf\', type=\'SUBSURF\')\n        mod.subdivision_type = \'SIMPLE\'\n        bpy.ops.object.modifier_apply(modifier=mod.name)\n        # now triangulate\n        mod = mesh.modifiers.new(name=\'Triangluate\', type=\'TRIANGULATE\')\n        bpy.ops.object.modifier_apply(modifier=mod.name)\n\n    def simplify(self, mesh, target_faces):\n        bpy.context.view_layer.objects.active = mesh\n        mod = mesh.modifiers.new(name=\'Decimate\', type=\'DECIMATE\')\n        bpy.context.object.modifiers[\'Decimate\'].use_collapse_triangulate = True\n        #\n        nfaces = len(mesh.data.polygons)\n        if nfaces < target_faces:\n            self.subsurf(mesh)\n            nfaces = len(mesh.data.polygons)\n        ratio = target_faces / float(nfaces)\n        mod.ratio = float(\'%s\' % (\'%.6g\' % (ratio)))\n        print(\'faces: \', mod.face_count, mod.ratio)\n        bpy.ops.object.modifier_apply(modifier=mod.name)\n\n\n    def export_obj(self, mesh, export_name):\n        outpath = os.path.dirname(export_name)\n        if not os.path.isdir(outpath): os.makedirs(outpath)\n        print(\'EXPORTING\', export_name)\n        bpy.ops.object.select_all(action=\'DESELECT\')\n        mesh.select_set(state=True)\n        bpy.ops.export_scene.obj(filepath=export_name, check_existing=False, filter_glob=""*.obj;*.mtl"",\n                                 use_selection=True, use_animation=False, use_mesh_modifiers=True, use_edges=True,\n                                 use_smooth_groups=False, use_smooth_groups_bitflags=False, use_normals=True,\n                                 use_uvs=False, use_materials=False, use_triangles=True, use_nurbs=False,\n                                 use_vertex_groups=False, use_blen_objects=True, group_by_object=False,\n                                 group_by_material=False, keep_vertex_order=True, global_scale=1, path_mode=\'AUTO\',\n                                 axis_forward=\'-Z\', axis_up=\'Y\')\n\nobj_file = sys.argv[-3]\ntarget_faces = int(sys.argv[-2])\nexport_name = sys.argv[-1]\n\n\nprint(\'args: \', obj_file, target_faces, export_name)\nblender = Process(obj_file, target_faces, export_name)\n'"
