file_path,api_count,code
data_loader.py,1,"b'#coding=utf-8\n\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset,DataLoader\nimport copy\n\nimport matplotlib.pyplot as plt\n\n# from train import config\n\n\ndef plot_sample(x, y, axis):\n    """"""\n\n    :param x: (9216,)\n    :param y: (15,2)\n    :param axis:\n    :return:\n    """"""\n    img = x.reshape(96, 96)\n    axis.imshow(img, cmap=\'gray\')\n    axis.scatter(y[:,0], y[:,1], marker=\'x\', s=10)\n\ndef plot_demo(X,y):\n    fig = plt.figure(figsize=(6, 6))\n    fig.subplots_adjust(\n        left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\n    for i in range(16):\n        ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n        plot_sample(X[i], y[i], ax)\n\n    plt.show()\n\n\nclass KFDataset(Dataset):\n    def __init__(self,config,X=None,gts=None):\n        """"""\n\n        :param X: (N,96*96)\n        :param gts: (N,15,2)\n        """"""\n        self.__X = X\n        self.__gts = gts\n        self.__sigma = config[\'sigma\']\n        self.__debug_vis = config[\'debug_vis\']\n        self.__fname = config[\'fname\']\n        self.__is_test = config[\'is_test\']\n        # self.__ftrain = config[\'ftrain\']\n        # self.load(self.__ftrain)\n\n    def load(self,cols=None):\n        """"""\n\n        :param fname:\n        :param test:\n        :param cols:\n        :return: X (N,96*96) Y (N,15,2)\n        """"""\n        test = self.__is_test\n        fname = self.__fname\n        df = pd.read_csv(fname)\n\n        # The Image column has pixel values separated by space; convert\n        # the values to numpy arrays:\n        df[\'Image\'] = df[\'Image\'].apply(lambda im: np.fromstring(im, sep=\' \'))\n\n        if cols:  # get a subset of columns\n            df = df[list(cols) + [\'Image\']]\n\n        # print(df.count())  # prints the number of values for each column\n        if test == False:\n            # df = df.dropna()  # drop all rows that have missing values in them\n            pass\n        # \xe9\x80\x89\xe5\x87\xba\xe5\xad\x98\xe5\x9c\xa8\xe7\xbc\xba\xe5\xa4\xb1\xe5\x80\xbc\xe7\x9a\x84iamge\n        # df = df[df.isnull().any(axis=1)]\n        # print(df.count())  # prints the number of values for each column\n        df_np = df.as_matrix()\n\n        X = df_np[:,-1]\n        x_list = X.tolist()\n        x_list = [item.tolist() for item in x_list]\n        X = np.array(x_list)\n        # X = X.astype(np.float32)\n        # X = X / 255.  # scale pixel values to [0, 1]\n\n        if not test:  # only FTRAIN has any target columns\n            gts = df_np[:, :-1]\n            gts = gts.reshape((gts.shape[0], -1, 2))\n            gts = gts.astype(np.float32)\n        else:\n            gts = df[\'ImageId\'].as_matrix()\n\n        self.__X = X\n        self.__gts = gts\n\n        return X, gts\n\n    def __len__(self):\n        return len(self.__X)\n\n    def __getitem__(self, item):\n        H,W = 96,96\n        x = self.__X[item]\n        gt = self.__gts[item]\n\n        if self.__is_test:\n            x = x.reshape((1, 96, 96)).astype(np.float32)\n            x = x / 255.\n            return x,gt #\xe8\xbf\x94\xe5\x9b\x9e\xe5\x9b\xbe\xe5\x83\x8f\xe4\xbb\xa5\xe5\x8f\x8a\xe5\x85\xb6id\n\n\n        heatmaps = self._putGaussianMaps(gt,H,W,1,self.__sigma)\n\n        if self.__debug_vis == True:\n            for i in range(heatmaps.shape[0]):\n                img = copy.deepcopy(x).astype(np.uint8).reshape((H,W))\n                self.visualize_heatmap_target(img,copy.deepcopy(heatmaps[i]),1)\n\n        x = x.reshape((1,96,96)).astype(np.float32)\n        x = x / 255.\n        heatmaps = heatmaps.astype(np.float32)\n        return x,heatmaps,gt\n\n    def _putGaussianMap(self, center, visible_flag, crop_size_y, crop_size_x, stride, sigma):\n        """"""\n        \xe6\xa0\xb9\xe6\x8d\xae\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9,\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaaheatmap\n        :param center:\n        :return:\n        """"""\n        grid_y = crop_size_y / stride\n        grid_x = crop_size_x / stride\n        if visible_flag == False:\n            return np.zeros((grid_y,grid_x))\n        start = stride / 2.0 - 0.5\n        y_range = [i for i in range(grid_y)]\n        x_range = [i for i in range(grid_x)]\n        xx, yy = np.meshgrid(x_range, y_range)\n        xx = xx * stride + start\n        yy = yy * stride + start\n        d2 = (xx - center[0]) ** 2 + (yy - center[1]) ** 2\n        exponent = d2 / 2.0 / sigma / sigma\n        heatmap = np.exp(-exponent)\n        return heatmap\n\n    def _putGaussianMaps(self,keypoints,crop_size_y, crop_size_x, stride, sigma):\n        """"""\n\n        :param keypoints: (15,2)\n        :param crop_size_y: int\n        :param crop_size_x: int\n        :param stride: int\n        :param sigma: float\n        :return:\n        """"""\n        all_keypoints = keypoints\n        point_num = all_keypoints.shape[0]\n        heatmaps_this_img = []\n        for k in range(point_num):\n            flag = ~np.isnan(all_keypoints[k,0])\n            heatmap = self._putGaussianMap(all_keypoints[k],flag,crop_size_y,crop_size_x,stride,sigma)\n            heatmap = heatmap[np.newaxis,...]\n            heatmaps_this_img.append(heatmap)\n        heatmaps_this_img = np.concatenate(heatmaps_this_img,axis=0) # (num_joint,crop_size_y/stride,crop_size_x/stride)\n        return heatmaps_this_img\n\n    def visualize_heatmap_target(self,oriImg,heatmap,stride):\n\n        plt.imshow(oriImg)\n        plt.imshow(heatmap, alpha=.5)\n        plt.show()\n\nif __name__ == \'__main__\':\n    from train import config\n    dataset = KFDataset(config)\n    dataset.load()\n    dataLoader = DataLoader(dataset=dataset,batch_size=64,shuffle=False)\n    for i, (x, y ,gt) in enumerate(dataLoader):\n        print(x.size())\n        print(y.size())\n        print(gt.size())\n'"
evaluation.py,3,"b'#coding=utf-8\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n\nfrom data_loader import KFDataset\nfrom models import KFSGNet\nfrom train import config,get_peak_points,get_mse\n\n\ndef demo(img,heatmaps):\n    """"""\n\n    :param img: (96,96)\n    :param heatmaps: ()\n    :return:\n    """"""\n    # img = img.reshape(96, 96)\n    # axis.imshow(img, cmap=\'gray\')\n    # axis.scatter(y[:, 0], y[:, 1], marker=\'x\', s=10)\n    pass\n\ndef evaluate():\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\n    net = KFSGNet()\n    net.float().cuda()\n    net.eval()\n    if (config[\'checkout\'] != \'\'):\n        net.load_state_dict(torch.load(config[\'checkout\']))\n\n    dataset = KFDataset(config)\n    dataset.load()\n    dataLoader = DataLoader(dataset,1)\n    for i,(images,_,gts) in enumerate(dataLoader):\n        images = Variable(images).float().cuda()\n\n        pred_heatmaps = net.forward(images)\n        demo_img = images[0].cpu().data.numpy()[0]\n        demo_img = (demo_img * 255.).astype(np.uint8)\n        demo_heatmaps = pred_heatmaps[0].cpu().data.numpy()[np.newaxis,...]\n        demo_pred_poins = get_peak_points(demo_heatmaps)[0] # (15,2)\n        plt.imshow(demo_img,cmap=\'gray\')\n        plt.scatter(demo_pred_poins[:,0],demo_pred_poins[:,1])\n        plt.show()\n\n        # loss = get_mse(demo_pred_poins[np.newaxis,...],gts)\n\nif __name__ == \'__main__\':\n    evaluate()'"
hg.py,6,"b'#coding=utf-8\n"""""" pytorch\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x9astacked hourglass network architecture""""""\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import UpsamplingNearest2d,Upsample\nfrom torch.autograd import Variable\n\nclass StackedHourGlass(nn.Module):\n    def __init__(self,nFeats=256,nStack=8,nJoints=18):\n        """"""\n        \xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x9a 256^2\n        """"""\n        super(StackedHourGlass,self).__init__()\n        self._nFeats = nFeats\n        self._nStack = nStack\n        self._nJoints = nJoints\n        self.conv1 = nn.Conv2d(3,64,7,2,3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.res1 = Residual(64,128)\n        self.pool1 = nn.MaxPool2d(2,2)\n        self.res2 = Residual(128,128)\n        self.res3 = Residual(128,self._nFeats)           #cmd:option(\'-nFeats\',            256, \'Number of features in the hourglass\')\n        self._init_stacked_hourglass()\n\n    def _init_stacked_hourglass(self):\n        for i in range(self._nStack):\n            setattr(self,\'hg\'+str(i),HourGlass(4,self._nFeats))\n            setattr(self,\'hg\'+str(i)+\'_res1\',Residual(self._nFeats,self._nFeats))\n            setattr(self,\'hg\'+str(i)+\'_lin1\',Lin(self._nFeats,self._nFeats))\n            setattr(self,\'hg\'+str(i)+\'_conv_pred\',nn.Conv2d(self._nFeats,self._nJoints,1))\n            if i < self._nStack - 1:\n                setattr(self,\'hg\'+str(i)+\'_conv1\',nn.Conv2d(self._nFeats,self._nFeats,1))\n                setattr(self,\'hg\'+str(i)+\'_conv2\',nn.Conv2d(self._nJoints,self._nFeats,1))\n\n    def forward(self,x):\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\x84\xe7\x90\x86\n        x = self.relu1(self.bn1(self.conv1(x))) #(n,64,128,128)\n        x = self.res1(x)                        #(n,128,128,128)\n        x = self.pool1(x)                       #(n,128,64,64)\n        x = self.res2(x)                        #(n,128,64,64)\n        x = self.res3(x)                        #(n,256,64,64)\n\n        out = []\n        inter = x\n\n        for i in range(self._nStack):                      #cmd:option(\'-nStack\',              8, \'Number of hourglasses to stack\')\n            hg = eval(\'self.hg\'+str(i))(inter)\n            # Residual layers at output resolution\n            ll = hg\n            ll = eval(\'self.hg\'+str(i)+\'_res1\')(ll)\n            # Linear layer to produce first set of predictions\n            ll = eval(\'self.hg\'+str(i)+\'_lin1\')(ll)\n            # Predicted heatmaps\n            tmpOut = eval(\'self.hg\'+str(i)+\'_conv_pred\')(ll)\n            out.append(tmpOut)\n            # Add predictions back\n            if i < self._nStack - 1:\n                ll_ = eval(\'self.hg\'+str(i)+\'_conv1\')(ll)\n                tmpOut_ = eval(\'self.hg\'+str(i)+\'_conv2\')(tmpOut)\n                inter = inter + ll_ + tmpOut_\n        return out\n\nclass HourGlass(nn.Module):\n    """"""\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe9\xab\x98\xe5\xae\xbd""""""\n    def __init__(self,n=4,f=256):\n        """"""\n        :param n: hourglass\xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe5\xb1\x82\xe7\xba\xa7\xe6\x95\xb0\xe7\x9b\xae\n        :param f: hourglass\xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\xad\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe6\x95\xb0\xe9\x87\x8f\n        :return:\n        """"""\n        super(HourGlass,self).__init__()\n        self._n = n\n        self._f = f\n        self._init_layers(self._n,self._f)\n\n    def _init_layers(self,n,f):\n        # \xe4\xb8\x8a\xe5\x88\x86\xe6\x94\xaf\n        setattr(self,\'res\'+str(n)+\'_1\',Residual(f,f))\n        # \xe4\xb8\x8b\xe5\x88\x86\xe6\x94\xaf\n        setattr(self,\'pool\'+str(n)+\'_1\',nn.MaxPool2d(2,2))\n        setattr(self,\'res\'+str(n)+\'_2\',Residual(f,f))\n        if n > 1:\n            self._init_layers(n-1,f)\n        else:\n            self.res_center = Residual(f,f)\n        setattr(self,\'res\'+str(n)+\'_3\',Residual(f,f))\n        # setattr(self,\'SUSN\'+str(n),UpsamplingNearest2d(scale_factor=2))\n        setattr(self,\'SUSN\'+str(n),Upsample(scale_factor=2))\n\n\n    def _forward(self,x,n,f):\n        # \xe4\xb8\x8a\xe5\x88\x86\xe6\x94\xaf\n        up1 = x\n        up1 = eval(\'self.res\'+str(n)+\'_1\')(up1)\n        # \xe4\xb8\x8b\xe5\x88\x86\xe6\x94\xaf\n        low1 = eval(\'self.pool\'+str(n)+\'_1\')(x)\n        low1 = eval(\'self.res\'+str(n)+\'_2\')(low1)\n        if n > 1:\n            low2 = self._forward(low1,n-1,f)\n        else:\n            low2 = self.res_center(low1)\n        low3 = low2\n        low3 = eval(\'self.\'+\'res\'+str(n)+\'_3\')(low3)\n        up2 = eval(\'self.\'+\'SUSN\'+str(n)).forward(low3)\n\n        return up1+up2\n\n    def forward(self,x):\n        return self._forward(x,self._n,self._f)\n\nclass Residual(nn.Module):\n    """"""\n    \xe6\xae\x8b\xe5\xb7\xae\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe5\xae\xbd\xe9\xab\x98\n    """"""\n    def __init__(self,ins,outs):\n        super(Residual,self).__init__()\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\n        self.convBlock = nn.Sequential(\n            nn.BatchNorm2d(ins),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(ins,outs/2,1),\n            nn.BatchNorm2d(outs/2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(outs/2,outs/2,3,1,1),\n            nn.BatchNorm2d(outs/2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(outs/2,outs,1)\n        )\n        # \xe8\xb7\xb3\xe5\xb1\x82\n        if ins != outs:\n            self.skipConv = nn.Conv2d(ins,outs,1)\n        self.ins = ins\n        self.outs = outs\n    def forward(self,x):\n        residual = x\n        x = self.convBlock(x)\n        if self.ins != self.outs:\n            residual = self.skipConv(residual)\n        x += residual\n        return x\n\nclass Lin(nn.Module):\n    def __init__(self,numIn,numout):\n        super(Lin,self).__init__()\n        self.conv = nn.Conv2d(numIn,numout,1)\n        self.bn = nn.BatchNorm2d(numout)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self,x):\n        return self.relu(self.bn(self.conv(x)))\n\n\nfrom torch.utils.data import Dataset,DataLoader\nimport numpy as np\nimport torch.optim as optim\n\nclass tempDataset(Dataset):\n    def __init__(self):\n        self.X = np.random.randn(100,3,512,512)\n        self.Y = np.random.randn(100,18,128,128)\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, item):\n        # \xe8\xbf\x99\xe9\x87\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x8d\xe8\xa6\x81\xe8\xae\xbe\xe7\xbd\xaebatch_size\n        return self.X[item],self.Y[item]\n\nif __name__ == \'__main__\':\n    from torch.nn import MSELoss\n    critical = MSELoss()\n\n    dataset = tempDataset()\n    dataLoader = DataLoader(dataset=dataset)\n    shg = StackedHourGlass()\n    optimizer = optim.SGD(shg.parameters(), lr=0.01, momentum=0.9,weight_decay=1e-4)\n    for i,(x,y) in enumerate(dataLoader):\n        x = Variable(x,requires_grad=True).float()\n        y = Variable(y).float()\n        y_pred = shg.forward(x)\n        loss = critical(y_pred[0],y[0])\n        print(\'loss : {}\'.format(loss))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()'"
models.py,6,"b'#coding=utf-8\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Upsample\nfrom torch.autograd import Variable\n\nclass HourGlass(nn.Module):\n    """"""\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe9\xab\x98\xe5\xae\xbd""""""\n    def __init__(self,n=4,f=128):\n        """"""\n        :param n: hourglass\xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe5\xb1\x82\xe7\xba\xa7\xe6\x95\xb0\xe7\x9b\xae\n        :param f: hourglass\xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\xad\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe6\x95\xb0\xe9\x87\x8f\n        :return:\n        """"""\n        super(HourGlass,self).__init__()\n        self._n = n\n        self._f = f\n        self._init_layers(self._n,self._f)\n\n    def _init_layers(self,n,f):\n        # \xe4\xb8\x8a\xe5\x88\x86\xe6\x94\xaf\n        setattr(self,\'res\'+str(n)+\'_1\',Residual(f,f))\n        # \xe4\xb8\x8b\xe5\x88\x86\xe6\x94\xaf\n        setattr(self,\'pool\'+str(n)+\'_1\',nn.MaxPool2d(2,2))\n        setattr(self,\'res\'+str(n)+\'_2\',Residual(f,f))\n        if n > 1:\n            self._init_layers(n-1,f)\n        else:\n            self.res_center = Residual(f,f)\n        setattr(self,\'res\'+str(n)+\'_3\',Residual(f,f))\n        setattr(self,\'unsample\'+str(n),Upsample(scale_factor=2))\n\n\n    def _forward(self,x,n,f):\n        # \xe4\xb8\x8a\xe5\x88\x86\xe6\x94\xaf\n        up1 = x\n        up1 = eval(\'self.res\'+str(n)+\'_1\')(up1)\n        # \xe4\xb8\x8b\xe5\x88\x86\xe6\x94\xaf\n        low1 = eval(\'self.pool\'+str(n)+\'_1\')(x)\n        low1 = eval(\'self.res\'+str(n)+\'_2\')(low1)\n        if n > 1:\n            low2 = self._forward(low1,n-1,f)\n        else:\n            low2 = self.res_center(low1)\n        low3 = low2\n        low3 = eval(\'self.\'+\'res\'+str(n)+\'_3\')(low3)\n        up2 = eval(\'self.\'+\'unsample\'+str(n)).forward(low3)\n\n        return up1+up2\n\n    def forward(self,x):\n        return self._forward(x,self._n,self._f)\n\nclass Residual(nn.Module):\n    """"""\n    \xe6\xae\x8b\xe5\xb7\xae\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe5\xae\xbd\xe9\xab\x98\n    """"""\n    def __init__(self,ins,outs):\n        super(Residual,self).__init__()\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\n        self.convBlock = nn.Sequential(\n            nn.BatchNorm2d(ins),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(ins,outs/2,1),\n            nn.BatchNorm2d(outs/2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(outs/2,outs/2,3,1,1),\n            nn.BatchNorm2d(outs/2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(outs/2,outs,1)\n        )\n        # \xe8\xb7\xb3\xe5\xb1\x82\n        if ins != outs:\n            self.skipConv = nn.Conv2d(ins,outs,1)\n        self.ins = ins\n        self.outs = outs\n    def forward(self,x):\n        residual = x\n        x = self.convBlock(x)\n        if self.ins != self.outs:\n            residual = self.skipConv(residual)\n        x += residual\n        return x\n\nclass Lin(nn.Module):\n    def __init__(self,numIn=128,numout=15):\n        super(Lin,self).__init__()\n        self.conv = nn.Conv2d(numIn,numout,1)\n        self.bn = nn.BatchNorm2d(numout)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self,x):\n        return self.relu(self.bn(self.conv(x)))\n\n\nclass KFSGNet(nn.Module):\n\n    def __init__(self):\n        super(KFSGNet,self).__init__()\n        self.__conv1 = nn.Conv2d(1,64,1)\n        self.__relu1 = nn.ReLU(inplace=True)\n        self.__conv2 = nn.Conv2d(64,128,1)\n        self.__relu2 = nn.ReLU(inplace=True)\n        self.__hg = HourGlass()\n        self.__lin = Lin()\n    def forward(self,x):\n        x = self.__relu1(self.__conv1(x))\n        x = self.__relu2(self.__conv2(x))\n        x = self.__hg(x)\n        x = self.__lin(x)\n        return x\n\n\nfrom torch.utils.data import Dataset,DataLoader\nimport numpy as np\nimport torch.optim as optim\n\nclass tempDataset(Dataset):\n    def __init__(self):\n        self.X = np.random.randn(100,1,96,96)\n        self.Y = np.random.randn(100,30,96,96)\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, item):\n        # \xe8\xbf\x99\xe9\x87\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x8d\xe8\xa6\x81\xe8\xae\xbe\xe7\xbd\xaebatch_size\n        return self.X[item],self.Y[item]\n\nif __name__ == \'__main__\':\n    from torch.nn import MSELoss\n    critical = MSELoss()\n\n    dataset = tempDataset()\n    dataLoader = DataLoader(dataset=dataset,batch_size=64)\n    shg = KFSGNet().cuda()\n    optimizer = optim.SGD(shg.parameters(), lr=0.001, momentum=0.9,weight_decay=1e-4)\n\n    for e in range(200):\n        for i,(x,y) in enumerate(dataLoader):\n            x = Variable(x,requires_grad=True).float().cuda()\n            y = Variable(y).float().cuda()\n            y_pred = shg.forward(x)\n            loss = critical(y_pred[0],y[0])\n            print(\'loss : {}\'.format(loss.data))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()'"
test.py,3,"b'#coding=utf-8\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom data_loader import KFDataset\nfrom models import KFSGNet\nfrom train import config,get_peak_points\n\n\ndef test():\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\n    net = KFSGNet()\n    net.float().cuda()\n    net.eval()\n    if (config[\'checkout\'] != \'\'):\n        net.load_state_dict(torch.load(config[\'checkout\']))\n\n    dataset = KFDataset(config)\n    dataset.load()\n    dataLoader = DataLoader(dataset,32)\n    all_result = []\n    lookup_df = pd.read_csv(config[\'lookup\'])\n    num = len(dataset)\n    for i,(images,ids) in enumerate(dataLoader):\n        print(\'{} / {}\'.format(i,num))\n        images = Variable(images).float().cuda()\n        ids = ids.numpy()\n        pred_heatmaps = net.forward(images)\n\n        """"""\n        \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n        demo_img = images[0].cpu().data.numpy()[0]\n        demo_img = (demo_img * 255.).astype(np.uint8)\n        demo_heatmaps = pred_heatmaps[0].cpu().data.numpy()[np.newaxis,...]\n        demo_pred_poins = get_peak_points(demo_heatmaps)[0] # (15,2)\n        plt.imshow(demo_img,cmap=\'gray\')\n        plt.scatter(demo_pred_poins[:,0],demo_pred_poins[:,1])\n        plt.show()\n        """"""\n\n        pred_points = get_peak_points(pred_heatmaps.cpu().data.numpy()) #(N,15,2)\n        pred_points = pred_points.reshape((pred_points.shape[0],-1)) #(N,30)\n\n        # \xe7\xad\x9b\xe9\x80\x89\xe5\x87\xba\xe8\xa6\x81\xe6\x9f\xa5\xe8\xaf\xa2\xe7\x9a\x84features\n        for idx,img_id in enumerate(ids):\n            result_img = lookup_df[lookup_df[\'ImageId\'] == img_id]\n            # \xe6\x98\xa0\xe5\xb0\x84feature names to ids\n            fea_names = result_img[\'FeatureName\'].as_matrix()\n            fea_ids = [config[\'featurename2id\'][name] for name in fea_names]\n            pred_values = pred_points[idx][fea_ids]\n            result_img[\'Location\'] = pred_values\n            all_result.append(result_img)\n\n\n        # loss = get_mse(demo_pred_poins[np.newaxis,...],gts)\n    result_df = pd.concat(all_result)\n    result_df = result_df.drop(columns=[\'ImageId\',\'FeatureName\'])\n    result_df.to_csv(\'data/result_909.csv\',index=False)\n\nif __name__ == \'__main__\':\n    test()'"
train.py,13,"b'#coding=utf-8\nimport torch\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pprint\n\nfrom data_loader import KFDataset\nfrom models import KFSGNet\n\nconfig = dict()\nconfig[\'lr\'] = 0.000001\nconfig[\'momentum\'] = 0.9\nconfig[\'weight_decay\'] = 1e-4\nconfig[\'epoch_num\'] = 400\nconfig[\'batch_size\'] = 72\nconfig[\'sigma\'] = 5.\nconfig[\'debug_vis\'] = False         # \xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96heatmaps\nconfig[\'fname\'] = \'data/test.csv\'\n# config[\'fname\'] = \'data/training.csv\'\n# config[\'is_test\'] = False\nconfig[\'is_test\'] = True\nconfig[\'save_freq\'] = 10\nconfig[\'checkout\'] = \'data/weight/kd_epoch_909_model.ckpt\'\nconfig[\'start_epoch\'] = 850\nconfig[\'eval_freq\'] = 5\nconfig[\'debug\'] = False\nconfig[\'lookup\'] = \'data/IdLookupTable.csv\'\nconfig[\'featurename2id\'] = {\n    \'left_eye_center_x\':0,\n    \'left_eye_center_y\':1,\n    \'right_eye_center_x\':2,\n    \'right_eye_center_y\':3,\n    \'left_eye_inner_corner_x\':4,\n    \'left_eye_inner_corner_y\':5,\n    \'left_eye_outer_corner_x\':6,\n    \'left_eye_outer_corner_y\':7,\n    \'right_eye_inner_corner_x\':8,\n    \'right_eye_inner_corner_y\':9,\n    \'right_eye_outer_corner_x\':10,\n    \'right_eye_outer_corner_y\':11,\n    \'left_eyebrow_inner_end_x\':12,\n    \'left_eyebrow_inner_end_y\':13,\n    \'left_eyebrow_outer_end_x\':14,\n    \'left_eyebrow_outer_end_y\':15,\n    \'right_eyebrow_inner_end_x\':16,\n    \'right_eyebrow_inner_end_y\':17,\n    \'right_eyebrow_outer_end_x\':18,\n    \'right_eyebrow_outer_end_y\':19,\n    \'nose_tip_x\':20,\n    \'nose_tip_y\':21,\n    \'mouth_left_corner_x\':22,\n    \'mouth_left_corner_y\':23,\n    \'mouth_right_corner_x\':24,\n    \'mouth_right_corner_y\':25,\n    \'mouth_center_top_lip_x\':26,\n    \'mouth_center_top_lip_y\':27,\n    \'mouth_center_bottom_lip_x\':28,\n    \'mouth_center_bottom_lip_y\':29,\n}\n\ndef get_peak_points(heatmaps):\n    """"""\n\n    :param heatmaps: numpy array (N,15,96,96)\n    :return:numpy array (N,15,2)\n    """"""\n    N,C,H,W = heatmaps.shape\n    all_peak_points = []\n    for i in range(N):\n        peak_points = []\n        for j in range(C):\n            yy,xx = np.where(heatmaps[i,j] == heatmaps[i,j].max())\n            y = yy[0]\n            x = xx[0]\n            peak_points.append([x,y])\n        all_peak_points.append(peak_points)\n    all_peak_points = np.array(all_peak_points)\n    return all_peak_points\n\ndef get_mse(pred_points,gts,indices_valid=None):\n    """"""\n\n    :param pred_points: numpy (N,15,2)\n    :param gts: numpy (N,15,2)\n    :return:\n    """"""\n    pred_points = pred_points[indices_valid[0],indices_valid[1],:]\n    gts = gts[indices_valid[0],indices_valid[1],:]\n    pred_points = Variable(torch.from_numpy(pred_points).float(),requires_grad=False)\n    gts = Variable(torch.from_numpy(gts).float(),requires_grad=False)\n    criterion = nn.MSELoss()\n    loss = criterion(pred_points,gts)\n    return loss\n\ndef calculate_mask(heatmaps_target):\n    """"""\n\n    :param heatmaps_target: Variable (N,15,96,96)\n    :return: Variable (N,15,96,96)\n    """"""\n    N,C,_,_ = heatmaps_targets.size()\n    N_idx = []\n    C_idx = []\n    for n in range(N):\n        for c in range(C):\n            max_v = heatmaps_targets[n,c,:,:].max().data[0]\n            if max_v != 0.0:\n                N_idx.append(n)\n                C_idx.append(c)\n    mask = Variable(torch.zeros(heatmaps_targets.size()))\n    mask[N_idx,C_idx,:,:] = 1.\n    mask = mask.float().cuda()\n    return mask,[N_idx,C_idx]\n\nif __name__ == \'__main__\':\n    pprint.pprint(config)\n    torch.manual_seed(0)\n    cudnn.benchmark = True\n    net = KFSGNet()\n    net.float().cuda()\n    net.train()\n    criterion = nn.MSELoss()\n    # optimizer = optim.SGD(net.parameters(), lr=config[\'lr\'], momentum=config[\'momentum\'] , weight_decay=config[\'weight_decay\'])\n    optimizer = optim.Adam(net.parameters(),lr=config[\'lr\'])\n    trainDataset = KFDataset(config)\n    trainDataset.load()\n    trainDataLoader = DataLoader(trainDataset,config[\'batch_size\'],True)\n    sample_num = len(trainDataset)\n\n    if (config[\'checkout\'] != \'\'):\n        net.load_state_dict(torch.load(config[\'checkout\']))\n\n    for epoch in range(config[\'start_epoch\'],config[\'epoch_num\']+config[\'start_epoch\']):\n        running_loss = 0.0\n        for i, (inputs, heatmaps_targets, gts) in enumerate(trainDataLoader):\n            inputs = Variable(inputs).cuda()\n            heatmaps_targets = Variable(heatmaps_targets).cuda()\n            mask,indices_valid = calculate_mask(heatmaps_targets)\n\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            outputs = outputs * mask\n            heatmaps_targets = heatmaps_targets * mask\n            loss = criterion(outputs, heatmaps_targets)\n            loss.backward()\n            optimizer.step()\n\n            # \xe7\xbb\x9f\xe8\xae\xa1\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe4\xb8\x8e\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\n            v_max = torch.max(outputs)\n            v_min = torch.min(outputs)\n\n            # \xe8\xaf\x84\xe4\xbc\xb0\n            all_peak_points = get_peak_points(heatmaps_targets.cpu().data.numpy())\n            loss_coor = get_mse(all_peak_points, gts.numpy(),indices_valid)\n\n            print(\'[ Epoch {:005d} -> {:005d} / {} ] loss : {:15} loss_coor : {:15} max : {:10} min : {}\'.format(\n                epoch, i * config[\'batch_size\'],\n                sample_num, loss.data[0],loss_coor.data[0],v_max.data[0],v_min.data[0]))\n\n\n\n        if (epoch+1) % config[\'save_freq\'] == 0 or epoch == config[\'epoch_num\'] - 1:\n            torch.save(net.state_dict(),\'kd_epoch_{}_model.ckpt\'.format(epoch))\n\n'"
visualize.py,5,"b'from graphviz import Digraph\nimport torch\nfrom torch.autograd import Variable\n\n\ndef make_dot(var, params=None):\n    """""" Produces Graphviz representation of PyTorch autograd graph\n\n    Blue nodes are the Variables that require grad, orange are Tensors\n    saved for backward in torch.autograd.Function\n\n    Args:\n        var: output Variable\n        params: dict of (name, Variable) to add names to node that\n            require grad (TODO: make optional)\n    """"""\n    if params is not None:\n        assert isinstance(params.values()[0], Variable)\n        param_map = {id(v): k for k, v in params.items()}\n\n    node_attr = dict(style=\'filled\',\n                     shape=\'box\',\n                     align=\'left\',\n                     fontsize=\'12\',\n                     ranksep=\'0.1\',\n                     height=\'0.2\')\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=""12,12""))\n    seen = set()\n\n    def size_to_str(size):\n        return \'(\'+(\', \').join([\'%d\' % v for v in size])+\')\'\n\n    def add_nodes(var):\n        if var not in seen:\n            if torch.is_tensor(var):\n                dot.node(str(id(var)), size_to_str(var.size()), fillcolor=\'orange\')\n            elif hasattr(var, \'variable\'):\n                u = var.variable\n                name = param_map[id(u)] if params is not None else \'\'\n                node_name = \'%s\\n %s\' % (name, size_to_str(u.size()))\n                dot.node(str(id(var)), node_name, fillcolor=\'lightblue\')\n            else:\n                dot.node(str(id(var)), str(type(var).__name__))\n            seen.add(var)\n            if hasattr(var, \'next_functions\'):\n                for u in var.next_functions:\n                    if u[0] is not None:\n                        dot.edge(str(id(u[0])), str(id(var)))\n                        add_nodes(u[0])\n            if hasattr(var, \'saved_tensors\'):\n                for t in var.saved_tensors:\n                    dot.edge(str(id(t)), str(id(var)))\n                    add_nodes(t)\n    add_nodes(var.grad_fn)\n    return dot\n\nif __name__ == \'__main__\':\n    from models import KFSGNet\n    from torch.autograd import Variable\n    import torch\n\n    net = KFSGNet()\n    x = Variable(torch.randn((1,1,96,96)))\n    y = net(x)\n    g = make_dot(y)\n    g.view()\n    pass'"
