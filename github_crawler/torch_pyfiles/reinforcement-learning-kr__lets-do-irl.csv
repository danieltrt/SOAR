file_path,api_count,code
mountaincar/app/app.py,0,"b'import numpy as np\r\nimport cvxpy as cp\r\nfrom train import idx_state\r\n\r\nclass FeatureEstimate:\r\n    def __init__(self, feature_num, env):\r\n        self.env = env\r\n        self.feature_num = feature_num\r\n        self.feature = np.ones(self.feature_num)\r\n\r\n    def gaussian_function(self, x, mu):\r\n        return np.exp(-np.power(x - mu, 2.) / (2 * np.power(1., 2.)))\r\n\r\n    def get_features(self, state):\r\n        env_low = self.env.observation_space.low\r\n        env_high = self.env.observation_space.high\r\n        env_distance = (env_high - env_low) / (self.feature_num - 1)\r\n\r\n        for i in range(int(self.feature_num/2)):\r\n            # position\r\n            self.feature[i] = self.gaussian_function(state[0], \r\n                                    env_low[0] + i * env_distance[0])\r\n            # velocity\r\n            self.feature[i+int(self.feature_num/2)] = self.gaussian_function(state[1], \r\n                                                        env_low[1] + i * env_distance[1])\r\n\r\n        return self.feature\r\n\r\n\r\ndef calc_feature_expectation(feature_num, gamma, q_table, demonstrations, env):\r\n    feature_estimate = FeatureEstimate(feature_num, env)\r\n    feature_expectations = np.zeros(feature_num)\r\n    demo_num = len(demonstrations)\r\n    \r\n    for _ in range(demo_num):\r\n        state = env.reset()\r\n        demo_length = 0\r\n        done = False\r\n        \r\n        while not done:\r\n            demo_length += 1\r\n\r\n            state_idx = idx_state(env, state)\r\n            action = np.argmax(q_table[state_idx])\r\n            next_state, reward, done, _ = env.step(action)\r\n            \r\n            features = feature_estimate.get_features(next_state)\r\n            feature_expectations += (gamma**(demo_length)) * np.array(features)\r\n\r\n            state = next_state\r\n    \r\n    feature_expectations = feature_expectations/ demo_num\r\n\r\n    return feature_expectations\r\n\r\ndef expert_feature_expectation(feature_num, gamma, demonstrations, env):\r\n    feature_estimate = FeatureEstimate(feature_num, env)\r\n    feature_expectations = np.zeros(feature_num)\r\n    \r\n    for demo_num in range(len(demonstrations)):\r\n        for demo_length in range(len(demonstrations[0])):\r\n            state = demonstrations[demo_num][demo_length]\r\n            features = feature_estimate.get_features(state)\r\n            feature_expectations += (gamma**(demo_length)) * np.array(features)\r\n    \r\n    feature_expectations = feature_expectations / len(demonstrations)\r\n    \r\n    return feature_expectations\r\n\r\n\r\ndef QP_optimizer(feature_num, learner, expert):\r\n    w = cp.Variable(feature_num)\r\n    \r\n    obj_func = cp.Minimize(cp.norm(w))\r\n    constraints = [(expert-learner) * w >= 2] \r\n\r\n    prob = cp.Problem(obj_func, constraints)\r\n    prob.solve()\r\n\r\n    if prob.status == ""optimal"":\r\n        print(""status:"", prob.status)\r\n        print(""optimal value"", prob.value)\r\n    \r\n        weights = np.squeeze(np.asarray(w.value))\r\n        return weights, prob.status\r\n    else:\r\n        print(""status:"", prob.status)\r\n        \r\n        weights = np.zeros(feature_num)\r\n        return weights, prob.status\r\n\r\n\r\ndef add_feature_expectation(learner, temp_learner):\r\n    # save new feature expectation to list after RL step\r\n    learner = np.vstack([learner, temp_learner])\r\n    return learner\r\n\r\ndef subtract_feature_expectation(learner):\r\n    # if status is infeasible, subtract first feature expectation\r\n    learner = learner[1:][:]\r\n    return learner'"
mountaincar/app/test.py,0,"b'import numpy as np\r\nimport gym\r\nimport random\r\nimport sys\r\nimport cvxpy as cp\r\n\r\nN_idx = 20\r\nF_idx = 4\r\nGAMMA = 0.99\r\n\r\ndef idx_to_state(env, state):\r\n    env_low = env.observation_space.low\r\n    env_high = env.observation_space.high\r\n    env_distance = (env_high - env_low) / N_idx\r\n    position_idx = int((state[0] - env_low[0]) / env_distance[0])\r\n    velocity_idx = int((state[1] - env_low[1]) / env_distance[1])\r\n    state_idx = position_idx + velocity_idx * N_idx\r\n    return state_idx\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    print("":: Testing APP-learning.\\n"")\r\n    \r\n    # Load the agent\r\n    n_states = N_idx**2  # position - 20, velocity - 20\r\n    n_actions = 3\r\n    q_table = np.load(file=""results/app_q_table.npy"")\r\n\r\n    # Create a new game instance.\r\n    env = gym.make(\'MountainCar-v0\')\r\n    n_episode = 10 # test the agent 10times\r\n    scores = []\r\n\r\n    for ep in range(n_episode):\r\n        state = env.reset()\r\n        score = 0\r\n\r\n        while True:\r\n            # Render the play\r\n            env.render()\r\n\r\n            state_idx = idx_to_state(env, state)\r\n\r\n            action = np.argmax(q_table[state_idx])\r\n\r\n            next_state, reward, done, _ = env.step(action)\r\n            next_state_idx = idx_to_state(env, next_state)\r\n\r\n            score += reward\r\n            state = next_state\r\n\r\n            if done:\r\n                print(\'{} episode | score: {:.1f}\'.format(ep + 1, score))\r\n                \r\n                break\r\n\r\n    env.close()\r\n    sys.exit()'"
mountaincar/app/train.py,0,"b'import sys\r\nimport gym\r\nimport pylab\r\nimport numpy as np\r\n\r\nfrom app import *\r\n\r\nn_states = 400 # position - 20, velocity - 20\r\nn_actions = 3\r\none_feature = 20 # number of state per one feature\r\nfeature_num = 4\r\nq_table = np.zeros((n_states, n_actions))  # (400, 3)\r\n\r\ngamma = 0.99\r\nq_learning_rate = 0.03\r\n\r\ndef idx_state(env, state):\r\n    env_low = env.observation_space.low\r\n    env_high = env.observation_space.high\r\n    env_distance = (env_high - env_low) / one_feature\r\n    positioone_feature = int((state[0] - env_low[0]) / env_distance[0])\r\n    velocity_idx = int((state[1] - env_low[1]) / env_distance[1])\r\n    state_idx = positioone_feature + velocity_idx * one_feature\r\n    return state_idx\r\n\r\ndef update_q_table(state, action, reward, next_state):\r\n    q_1 = q_table[state][action]\r\n    q_2 = reward + gamma * max(q_table[next_state])\r\n    q_table[state][action] += q_learning_rate * (q_2 - q_1)\r\n\r\n\r\ndef main():\r\n    env = gym.make(\'MountainCar-v0\')\r\n    demonstrations = np.load(file=""expert_demo/expert_demo.npy"")\r\n    \r\n    feature_estimate = FeatureEstimate(feature_num, env)\r\n    \r\n    learner = calc_feature_expectation(feature_num, gamma, q_table, demonstrations, env)\r\n    learner = np.matrix([learner])\r\n    \r\n    expert = expert_feature_expectation(feature_num, gamma, demonstrations, env)\r\n    expert = np.matrix([expert])\r\n    \r\n    w, status = QP_optimizer(feature_num, learner, expert)\r\n    \r\n    \r\n    episodes, scores = [], []\r\n    \r\n    for episode in range(60000):\r\n        state = env.reset()\r\n        score = 0\r\n\r\n        while True:\r\n            state_idx = idx_state(env, state)\r\n            action = np.argmax(q_table[state_idx])\r\n            next_state, reward, done, _ = env.step(action)\r\n            \r\n            features = feature_estimate.get_features(state)\r\n            irl_reward = np.dot(w, features)\r\n            \r\n            next_state_idx = idx_state(env, next_state)\r\n            update_q_table(state_idx, action, irl_reward, next_state_idx)\r\n\r\n            score += reward\r\n            state = next_state\r\n\r\n            if done:\r\n                scores.append(score)\r\n                episodes.append(episode)\r\n                break\r\n\r\n        if episode % 1000 == 0:\r\n            score_avg = np.mean(scores)\r\n            print(\'{} episode score is {:.2f}\'.format(episode, score_avg))\r\n            pylab.plot(episodes, scores, \'b\')\r\n            pylab.savefig(""./learning_curves/app_eps_60000.png"")\r\n            np.save(""./results/app_q_table"", arr=q_table)\r\n\r\n        if episode % 5000 == 0:\r\n            # optimize weight per 5000 episode\r\n            status = ""infeasible""\r\n            temp_learner = calc_feature_expectation(feature_num, gamma, q_table, demonstrations, env)\r\n            learner = add_feature_expectation(learner, temp_learner)\r\n            \r\n            while status==""infeasible"":\r\n                w, status = QP_optimizer(feature_num, learner, expert)\r\n                if status==""infeasible"":\r\n                    learner = subtract_feature_expectation(learner)\r\n\r\nif __name__ == \'__main__\':\r\n    main()'"
mountaincar/maxent/maxent.py,0,"b'import numpy as np\n\ndef get_reward(feature_matrix, theta, n_states, state_idx):\n    irl_rewards = feature_matrix.dot(theta).reshape((n_states,))\n    return irl_rewards[state_idx]\n\ndef expert_feature_expectations(feature_matrix, demonstrations):\n    feature_expectations = np.zeros(feature_matrix.shape[0])\n    \n    for demonstration in demonstrations:\n        for state_idx, _, _ in demonstration:\n            feature_expectations += feature_matrix[int(state_idx)]\n\n    feature_expectations /= demonstrations.shape[0]\n    return feature_expectations\n\ndef maxent_irl(expert, learner, theta, learning_rate):\n    gradient = expert - learner\n    theta += learning_rate * gradient\n\n    # Clip theta\n    for j in range(len(theta)):\n        if theta[j] > 0:\n            theta[j] = 0'"
mountaincar/maxent/test.py,0,"b'import gym\nimport pylab\nimport numpy as np\n\nq_table = np.load(file=""results/maxent_20_epoch_100000_epi_test.npy"") # (400, 3)\none_feature = 20 # number of state per one feature\n\ndef idx_to_state(env, state):\n    """""" Convert pos and vel about mounting car environment to the integer value""""""\n    env_low = env.observation_space.low\n    env_high = env.observation_space.high \n    env_distance = (env_high - env_low) / one_feature \n    position_idx = int((state[0] - env_low[0]) / env_distance[0])\n    velocity_idx = int((state[1] - env_low[1]) / env_distance[1])\n    state_idx = position_idx + velocity_idx * one_feature\n    return state_idx\n    \ndef main():\n    env = gym.make(\'MountainCar-v0\')\n    \n    episodes, scores = [], []\n\n    for episode in range(10):\n        state = env.reset()\n        score = 0\n\n        while True:\n            env.render()\n            state_idx = idx_to_state(env, state)\n            action = np.argmax(q_table[state_idx])\n            next_state, reward, done, _ = env.step(action)\n            \n            score += reward\n            state = next_state\n            \n            if done:\n                scores.append(score)\n                episodes.append(episode)\n                pylab.plot(episodes, scores, \'b\')\n                pylab.savefig(""./learning_curves/maxent_test.png"")\n                break\n\n        if episode % 1 == 0:\n            print(\'{} episode score is {:.2f}\'.format(episode, score))\n\nif __name__ == \'__main__\':\n    main()\n    '"
mountaincar/maxent/train.py,0,"b'import gym\nimport pylab\nimport numpy as np\n\nfrom maxent import *\n\nn_states = 400 # position - 20, velocity - 20\nn_actions = 3\none_feature = 20 # number of state per one feature\nq_table = np.zeros((n_states, n_actions)) # (400, 3)\nfeature_matrix = np.eye((n_states)) # (400, 400)\n\ngamma = 0.99\nq_learning_rate = 0.03\ntheta_learning_rate = 0.05\n\nnp.random.seed(1)\n\ndef idx_demo(env, one_feature):\n    env_low = env.observation_space.low     \n    env_high = env.observation_space.high   \n    env_distance = (env_high - env_low) / one_feature  \n\n    raw_demo = np.load(file=""expert_demo/expert_demo.npy"")\n    demonstrations = np.zeros((len(raw_demo), len(raw_demo[0]), 3))\n\n    for x in range(len(raw_demo)):\n        for y in range(len(raw_demo[0])):\n            position_idx = int((raw_demo[x][y][0] - env_low[0]) / env_distance[0])\n            velocity_idx = int((raw_demo[x][y][1] - env_low[1]) / env_distance[1])\n            state_idx = position_idx + velocity_idx * one_feature\n\n            demonstrations[x][y][0] = state_idx\n            demonstrations[x][y][1] = raw_demo[x][y][2] \n            \n    return demonstrations\n\ndef idx_state(env, state):\n    env_low = env.observation_space.low\n    env_high = env.observation_space.high \n    env_distance = (env_high - env_low) / one_feature \n    position_idx = int((state[0] - env_low[0]) / env_distance[0])\n    velocity_idx = int((state[1] - env_low[1]) / env_distance[1])\n    state_idx = position_idx + velocity_idx * one_feature\n    return state_idx\n\ndef update_q_table(state, action, reward, next_state):\n    q_1 = q_table[state][action]\n    q_2 = reward + gamma * max(q_table[next_state])\n    q_table[state][action] += q_learning_rate * (q_2 - q_1)\n\n\ndef main():\n    env = gym.make(\'MountainCar-v0\')\n    demonstrations = idx_demo(env, one_feature)\n\n    expert = expert_feature_expectations(feature_matrix, demonstrations)\n    learner_feature_expectations = np.zeros(n_states)\n\n    theta = -(np.random.uniform(size=(n_states,)))\n\n    episodes, scores = [], []\n\n    for episode in range(30000):\n        state = env.reset()\n        score = 0\n\n        if (episode != 0 and episode == 10000) or (episode > 10000 and episode % 5000 == 0):\n            learner = learner_feature_expectations / episode\n            maxent_irl(expert, learner, theta, theta_learning_rate)\n                \n        while True:\n            state_idx = idx_state(env, state)\n            action = np.argmax(q_table[state_idx])\n            next_state, reward, done, _ = env.step(action)\n            \n            irl_reward = get_reward(feature_matrix, theta, n_states, state_idx)\n            next_state_idx = idx_state(env, next_state)\n            update_q_table(state_idx, action, irl_reward, next_state_idx)\n            \n            learner_feature_expectations += feature_matrix[int(state_idx)]\n\n            score += reward\n            state = next_state\n            \n            if done:\n                scores.append(score)\n                episodes.append(episode)\n                break\n\n        if episode % 1000 == 0:\n            score_avg = np.mean(scores)\n            print(\'{} episode score is {:.2f}\'.format(episode, score_avg))\n            pylab.plot(episodes, scores, \'b\')\n            pylab.savefig(""./learning_curves/maxent_30000.png"")\n            np.save(""./results/maxent_q_table"", arr=q_table)\n\nif __name__ == \'__main__\':\n    main()'"
mujoco/gail/main.py,4,"b'import os\nimport gym\nimport pickle\nimport argparse\nimport numpy as np\nfrom collections import deque\n\nimport torch\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter \n\nfrom utils.utils import *\nfrom utils.zfilter import ZFilter\nfrom model import Actor, Critic, Discriminator\nfrom train_model import train_actor_critic, train_discrim\n\nparser = argparse.ArgumentParser(description=\'PyTorch GAIL\')\nparser.add_argument(\'--env_name\', type=str, default=""Hopper-v2"", \n                    help=\'name of the environment to run\')\nparser.add_argument(\'--load_model\', type=str, default=None, \n                    help=\'path to load the saved model\')\nparser.add_argument(\'--render\', action=""store_true"", default=False, \n                    help=\'if you dont want to render, set this to False\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, \n                    help=\'discounted factor (default: 0.99)\')\nparser.add_argument(\'--lamda\', type=float, default=0.98, \n                    help=\'GAE hyper-parameter (default: 0.98)\')\nparser.add_argument(\'--hidden_size\', type=int, default=100, \n                    help=\'hidden unit size of actor, critic and discrim networks (default: 100)\')\nparser.add_argument(\'--learning_rate\', type=float, default=3e-4, \n                    help=\'learning rate of models (default: 3e-4)\')\nparser.add_argument(\'--l2_rate\', type=float, default=1e-3, \n                    help=\'l2 regularizer coefficient (default: 1e-3)\')\nparser.add_argument(\'--clip_param\', type=float, default=0.2, \n                    help=\'clipping parameter for PPO (default: 0.2)\')\nparser.add_argument(\'--discrim_update_num\', type=int, default=2, \n                    help=\'update number of discriminator (default: 2)\')\nparser.add_argument(\'--actor_critic_update_num\', type=int, default=10, \n                    help=\'update number of actor-critic (default: 10)\')\nparser.add_argument(\'--total_sample_size\', type=int, default=2048, \n                    help=\'total sample size to collect before PPO update (default: 2048)\')\nparser.add_argument(\'--batch_size\', type=int, default=64, \n                    help=\'batch size to update (default: 64)\')\nparser.add_argument(\'--suspend_accu_exp\', type=float, default=0.8,\n                    help=\'accuracy for suspending discriminator about expert data (default: 0.8)\')\nparser.add_argument(\'--suspend_accu_gen\', type=float, default=0.8,\n                    help=\'accuracy for suspending discriminator about generated data (default: 0.8)\')\nparser.add_argument(\'--max_iter_num\', type=int, default=4000,\n                    help=\'maximal number of main iterations (default: 4000)\')\nparser.add_argument(\'--seed\', type=int, default=500,\n                    help=\'random seed (default: 500)\')\nparser.add_argument(\'--logdir\', type=str, default=\'logs\',\n                    help=\'tensorboardx logs directory\')\nargs = parser.parse_args()\n\n\ndef main():\n    env = gym.make(args.env_name)\n    env.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    num_inputs = env.observation_space.shape[0]\n    num_actions = env.action_space.shape[0]\n    running_state = ZFilter((num_inputs,), clip=5)\n\n    print(\'state size:\', num_inputs) \n    print(\'action size:\', num_actions)\n\n    actor = Actor(num_inputs, num_actions, args)\n    critic = Critic(num_inputs, args)\n    discrim = Discriminator(num_inputs + num_actions, args)\n\n    actor_optim = optim.Adam(actor.parameters(), lr=args.learning_rate)\n    critic_optim = optim.Adam(critic.parameters(), lr=args.learning_rate, \n                              weight_decay=args.l2_rate) \n    discrim_optim = optim.Adam(discrim.parameters(), lr=args.learning_rate)\n    \n    # load demonstrations\n    expert_demo, _ = pickle.load(open(\'./expert_demo/expert_demo.p\', ""rb""))\n    demonstrations = np.array(expert_demo)\n    print(""demonstrations.shape"", demonstrations.shape)\n    \n    writer = SummaryWriter(args.logdir)\n\n    if args.load_model is not None:\n        saved_ckpt_path = os.path.join(os.getcwd(), \'save_model\', str(args.load_model))\n        ckpt = torch.load(saved_ckpt_path)\n\n        actor.load_state_dict(ckpt[\'actor\'])\n        critic.load_state_dict(ckpt[\'critic\'])\n        discrim.load_state_dict(ckpt[\'discrim\'])\n\n        running_state.rs.n = ckpt[\'z_filter_n\']\n        running_state.rs.mean = ckpt[\'z_filter_m\']\n        running_state.rs.sum_square = ckpt[\'z_filter_s\']\n\n        print(""Loaded OK ex. Zfilter N {}"".format(running_state.rs.n))\n\n    \n    episodes = 0\n    train_discrim_flag = True\n\n    for iter in range(args.max_iter_num):\n        actor.eval(), critic.eval()\n        memory = deque()\n\n        steps = 0\n        scores = []\n\n        while steps < args.total_sample_size: \n            state = env.reset()\n            score = 0\n\n            state = running_state(state)\n            \n            for _ in range(10000): \n                if args.render:\n                    env.render()\n\n                steps += 1\n\n                mu, std = actor(torch.Tensor(state).unsqueeze(0))\n                action = get_action(mu, std)[0]\n                next_state, reward, done, _ = env.step(action)\n                irl_reward = get_reward(discrim, state, action)\n\n                if done:\n                    mask = 0\n                else:\n                    mask = 1\n\n                memory.append([state, action, irl_reward, mask])\n\n                next_state = running_state(next_state)\n                state = next_state\n\n                score += reward\n\n                if done:\n                    break\n            \n            episodes += 1\n            scores.append(score)\n        \n        score_avg = np.mean(scores)\n        print(\'{}:: {} episode score is {:.2f}\'.format(iter, episodes, score_avg))\n        writer.add_scalar(\'log/score\', float(score_avg), iter)\n\n        actor.train(), critic.train(), discrim.train()\n        if train_discrim_flag:\n            expert_acc, learner_acc = train_discrim(discrim, memory, discrim_optim, demonstrations, args)\n            print(""Expert: %.2f%% | Learner: %.2f%%"" % (expert_acc * 100, learner_acc * 100))\n            if expert_acc > args.suspend_accu_exp and learner_acc > args.suspend_accu_gen:\n                train_discrim_flag = False\n        train_actor_critic(actor, critic, memory, actor_optim, critic_optim, args)\n\n        if iter % 100:\n            score_avg = int(score_avg)\n\n            model_path = os.path.join(os.getcwd(),\'save_model\')\n            if not os.path.isdir(model_path):\n                os.makedirs(model_path)\n\n            ckpt_path = os.path.join(model_path, \'ckpt_\'+ str(score_avg)+\'.pth.tar\')\n\n            save_checkpoint({\n                \'actor\': actor.state_dict(),\n                \'critic\': critic.state_dict(),\n                \'discrim\': discrim.state_dict(),\n                \'z_filter_n\':running_state.rs.n,\n                \'z_filter_m\': running_state.rs.mean,\n                \'z_filter_s\': running_state.rs.sum_square,\n                \'args\': args,\n                \'score\': score_avg\n            }, filename=ckpt_path)\n\nif __name__==""__main__"":\n    main()'"
mujoco/gail/model.py,10,"b'import torch\nimport torch.nn as nn\n\nclass Actor(nn.Module):\n    def __init__(self, num_inputs, num_outputs, args):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, num_outputs)\n        \n        self.fc3.weight.data.mul_(0.1)\n        self.fc3.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        mu = self.fc3(x)\n        logstd = torch.zeros_like(mu)\n        std = torch.exp(logstd)\n        return mu, std\n\n\nclass Critic(nn.Module):\n    def __init__(self, num_inputs, args):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, 1)\n        \n        self.fc3.weight.data.mul_(0.1)\n        self.fc3.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        v = self.fc3(x)\n        return v\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_inputs, args):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, 1)\n        \n        self.fc3.weight.data.mul_(0.1)\n        self.fc3.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        prob = torch.sigmoid(self.fc3(x))\n        return prob'"
mujoco/gail/test.py,3,"b'import os\nimport gym\nimport torch\nimport argparse\n\nfrom model import Actor, Critic\nfrom utils.utils import get_action\nfrom utils.running_state import ZFilter\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--env\', type=str, default=""Hopper-v2"",\n                    help=\'name of Mujoco environement\')\nparser.add_argument(\'--iter\', type=int, default=5,\n                    help=\'number of episodes to play\')\nparser.add_argument(""--load_model"", type=str, default=\'ppo_max.tar\',\n                     help=""if you test pretrained file, write filename in save_model folder"")\n\nargs = parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    env = gym.make(args.env)\n    env.seed(500)\n    torch.manual_seed(500)\n\n    num_inputs = env.observation_space.shape[0]\n    num_actions = env.action_space.shape[0]\n\n    print(""state size: "", num_inputs)\n    print(""action size: "", num_actions)\n\n    actor = Actor(num_inputs, num_actions)\n    critic = Critic(num_inputs)\n\n    running_state = ZFilter((num_inputs,), clip=5)\n    \n    if args.load_model is not None:\n        pretrained_model_path = os.path.join(os.getcwd(), \'save_model\', str(args.load_model))\n\n        pretrained_model = torch.load(pretrained_model_path)\n\n        actor.load_state_dict(pretrained_model[\'actor\'])\n        critic.load_state_dict(pretrained_model[\'critic\'])\n\n        running_state.rs.n = pretrained_model[\'z_filter_n\']\n        running_state.rs.mean = pretrained_model[\'z_filter_m\']\n        running_state.rs.sum_square = pretrained_model[\'z_filter_s\']\n\n        print(""Loaded OK ex. ZFilter N {}"".format(running_state.rs.n))\n\n    else:\n        assert(""Should write pretrained filename in save_model folder. ex) python3 test_algo.py --load_model ppo_max.tar"")\n\n\n    actor.eval(), critic.eval()\n    for episode in range(args.iter):\n        state = env.reset()\n        steps = 0\n        score = 0\n        for _ in range(10000):\n            env.render()\n            mu, std, _ = actor(torch.Tensor(state).unsqueeze(0))\n            action = get_action(mu, std)[0]\n\n            next_state, reward, done, _ = env.step(action)\n            next_state = running_state(next_state)\n            \n            state = next_state\n            score += reward\n            \n            if done:\n                print(""{} cumulative reward: {}"".format(episode, score))\n                break\n'"
mujoco/gail/train_model.py,24,"b'import torch\nimport numpy as np\nfrom utils.utils import get_entropy, log_prob_density\n\ndef train_discrim(discrim, memory, discrim_optim, demonstrations, args):\n    memory = np.array(memory) \n    states = np.vstack(memory[:, 0]) \n    actions = list(memory[:, 1]) \n\n    states = torch.Tensor(states)\n    actions = torch.Tensor(actions)\n        \n    criterion = torch.nn.BCELoss()\n\n    for _ in range(args.discrim_update_num):\n        learner = discrim(torch.cat([states, actions], dim=1))\n        demonstrations = torch.Tensor(demonstrations)\n        expert = discrim(demonstrations)\n\n        discrim_loss = criterion(learner, torch.ones((states.shape[0], 1))) + \\\n                        criterion(expert, torch.zeros((demonstrations.shape[0], 1)))\n                \n        discrim_optim.zero_grad()\n        discrim_loss.backward()\n        discrim_optim.step()\n\n    expert_acc = ((discrim(demonstrations) < 0.5).float()).mean()\n    learner_acc = ((discrim(torch.cat([states, actions], dim=1)) > 0.5).float()).mean()\n\n    return expert_acc, learner_acc\n\n\ndef train_actor_critic(actor, critic, memory, actor_optim, critic_optim, args):\n    memory = np.array(memory) \n    states = np.vstack(memory[:, 0]) \n    actions = list(memory[:, 1]) \n    rewards = list(memory[:, 2]) \n    masks = list(memory[:, 3]) \n\n    old_values = critic(torch.Tensor(states))\n    returns, advants = get_gae(rewards, masks, old_values, args)\n    \n    mu, std = actor(torch.Tensor(states))\n    old_policy = log_prob_density(torch.Tensor(actions), mu, std)\n\n    criterion = torch.nn.MSELoss()\n    n = len(states)\n    arr = np.arange(n)\n\n    for _ in range(args.actor_critic_update_num):\n        np.random.shuffle(arr)\n\n        for i in range(n // args.batch_size): \n            batch_index = arr[args.batch_size * i : args.batch_size * (i + 1)]\n            batch_index = torch.LongTensor(batch_index)\n            \n            inputs = torch.Tensor(states)[batch_index]\n            actions_samples = torch.Tensor(actions)[batch_index]\n            returns_samples = returns.unsqueeze(1)[batch_index]\n            advants_samples = advants.unsqueeze(1)[batch_index]\n            oldvalue_samples = old_values[batch_index].detach()\n            \n            values = critic(inputs)\n            clipped_values = oldvalue_samples + \\\n                             torch.clamp(values - oldvalue_samples,\n                                         -args.clip_param, \n                                         args.clip_param)\n            critic_loss1 = criterion(clipped_values, returns_samples)\n            critic_loss2 = criterion(values, returns_samples)\n            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n\n            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n                                         old_policy.detach(), actions_samples,\n                                         batch_index)\n            clipped_ratio = torch.clamp(ratio,\n                                        1.0 - args.clip_param,\n                                        1.0 + args.clip_param)\n            clipped_loss = clipped_ratio * advants_samples\n            actor_loss = -torch.min(loss, clipped_loss).mean()\n\n            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n\n            critic_optim.zero_grad()\n            loss.backward(retain_graph=True) \n            critic_optim.step()\n\n            actor_optim.zero_grad()\n            loss.backward()\n            actor_optim.step()\n\ndef get_gae(rewards, masks, values, args):\n    rewards = torch.Tensor(rewards)\n    masks = torch.Tensor(masks)\n    returns = torch.zeros_like(rewards)\n    advants = torch.zeros_like(rewards)\n    \n    running_returns = 0\n    previous_value = 0\n    running_advants = 0\n\n    for t in reversed(range(0, len(rewards))):\n        running_returns = rewards[t] + (args.gamma * running_returns * masks[t])\n        returns[t] = running_returns\n\n        running_delta = rewards[t] + (args.gamma * previous_value * masks[t]) - \\\n                                        values.data[t]\n        previous_value = values.data[t]\n        \n        running_advants = running_delta + (args.gamma * args.lamda * \\\n                                            running_advants * masks[t])\n        advants[t] = running_advants\n\n    advants = (advants - advants.mean()) / advants.std()\n    return returns, advants\n\ndef surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n    mu, std = actor(states)\n    new_policy = log_prob_density(actions, mu, std)\n    old_policy = old_policy[batch_index]\n\n    ratio = torch.exp(new_policy - old_policy)\n    surrogate_loss = ratio * advants\n    entropy = get_entropy(mu, std)\n\n    return surrogate_loss, ratio, entropy'"
mujoco/ppo/main.py,4,"b'import os\nimport gym\nimport argparse\nimport numpy as np\nfrom collections import deque\n\nimport torch\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter \n\nfrom ppo import train_model\nfrom model import Actor, Critic\nfrom utils.zfilter import ZFilter\nfrom utils.utils import get_action, save_checkpoint\n\nparser = argparse.ArgumentParser(description=\'PyTorch PPO\')\nparser.add_argument(\'--env_name\', type=str, default=""Hopper-v2"", \n                    help=\'name of the environment to run\')\nparser.add_argument(\'--load_model\', type=str, default=None, \n                    help=\'path to load the saved model\')\nparser.add_argument(\'--render\', action=""store_true"", default=False, \n                    help=\'if you dont want to render, set this to False\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, \n                    help=\'discounted factor (default: 0.99)\')\nparser.add_argument(\'--lamda\', type=float, default=0.98, \n                    help=\'GAE hyper-parameter (default: 0.98)\')\nparser.add_argument(\'--hidden_size\', type=int, default=64, \n                    help=\'hidden unit size of actor, critic networks (default: 64)\')\nparser.add_argument(\'--learning_rate\', type=float, default=3e-4, \n                    help=\'learning rate of models (default: 3e-4)\')\nparser.add_argument(\'--l2_rate\', type=float, default=1e-3, \n                    help=\'l2 regularizer coefficient (default: 1e-3)\')\nparser.add_argument(\'--clip_param\', type=float, default=0.2, \n                    help=\'clipping parameter for PPO (default: 0.2)\')\nparser.add_argument(\'--model_update_num\', type=int, default=10, \n                    help=\'update number of actor-critic (default: 10)\')\nparser.add_argument(\'--total_sample_size\', type=int, default=2048, \n                    help=\'total sample size to collect before PPO update (default: 2048)\')\nparser.add_argument(\'--batch_size\', type=int, default=64, \n                    help=\'batch size to update (default: 64)\')\nparser.add_argument(\'--max_iter_num\', type=int, default=12000,\n                    help=\'maximal number of main iterations (default: 12000)\')\nparser.add_argument(\'--seed\', type=int, default=500,\n                    help=\'random seed (default: 500)\')\nparser.add_argument(\'--logdir\', type=str, default=\'logs\',\n                    help=\'tensorboardx logs directory\')\nargs = parser.parse_args()\n\ndef main():\n    env = gym.make(args.env_name)\n    env.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    num_inputs = env.observation_space.shape[0]\n    num_actions = env.action_space.shape[0]\n    running_state = ZFilter((num_inputs,), clip=5)\n\n    print(\'state size:\', num_inputs) \n    print(\'action size:\', num_actions)\n\n    actor = Actor(num_inputs, num_actions, args)\n    critic = Critic(num_inputs, args)\n\n    actor_optim = optim.Adam(actor.parameters(), lr=args.learning_rate)\n    critic_optim = optim.Adam(critic.parameters(), lr=args.learning_rate, \n                              weight_decay=args.l2_rate)\n\n    writer = SummaryWriter(comment=""-ppo_iter-"" + str(args.max_iter_num))\n    \n    if args.load_model is not None:\n        saved_ckpt_path = os.path.join(os.getcwd(), \'save_model\', str(args.load_model))\n        ckpt = torch.load(saved_ckpt_path)\n\n        actor.load_state_dict(ckpt[\'actor\'])\n        critic.load_state_dict(ckpt[\'critic\'])\n\n        running_state.rs.n = ckpt[\'z_filter_n\']\n        running_state.rs.mean = ckpt[\'z_filter_m\']\n        running_state.rs.sum_square = ckpt[\'z_filter_s\']\n\n        print(""Loaded OK ex. Zfilter N {}"".format(running_state.rs.n))\n\n    \n    episodes = 0    \n\n    for iter in range(args.max_iter_num):\n        actor.eval(), critic.eval()\n        memory = deque()\n\n        steps = 0\n        scores = []\n\n        while steps < args.total_sample_size: \n            state = env.reset()\n            score = 0\n\n            state = running_state(state)\n            \n            for _ in range(10000): \n                if args.render:\n                    env.render()\n\n                steps += 1\n\n                mu, std = actor(torch.Tensor(state).unsqueeze(0))\n                action = get_action(mu, std)[0]\n                next_state, reward, done, _ = env.step(action)\n\n                if done:\n                    mask = 0\n                else:\n                    mask = 1\n\n                memory.append([state, action, reward, mask])\n\n                next_state = running_state(next_state)\n                state = next_state\n\n                score += reward\n\n                if done:\n                    break\n            \n            episodes += 1\n            scores.append(score)\n        \n        score_avg = np.mean(scores)\n        print(\'{}:: {} episode score is {:.2f}\'.format(iter, episodes, score_avg))\n        writer.add_scalar(\'log/score\', float(score_avg), iter)\n\n        actor.train(), critic.train()\n        train_model(actor, critic, memory, actor_optim, critic_optim, args)\n\n        if iter % 100:\n            score_avg = int(score_avg)\n\n            model_path = os.path.join(os.getcwd(),\'save_model\')\n            if not os.path.isdir(model_path):\n                os.makedirs(model_path)\n\n            ckpt_path = os.path.join(model_path, \'ckpt_\'+ str(score_avg)+\'.pth.tar\')\n\n            save_checkpoint({\n                \'actor\': actor.state_dict(),\n                \'critic\': critic.state_dict(),\n                \'z_filter_n\':running_state.rs.n,\n                \'z_filter_m\': running_state.rs.mean,\n                \'z_filter_s\': running_state.rs.sum_square,\n                \'args\': args,\n                \'score\': score_avg\n            }, filename=ckpt_path)\n\nif __name__==""__main__"":\n    main()'"
mujoco/ppo/model.py,7,"b'import torch\nimport torch.nn as nn\n\nclass Actor(nn.Module):\n    def __init__(self, num_inputs, num_outputs, args):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, num_outputs)\n        \n        self.fc3.weight.data.mul_(0.1)\n        self.fc3.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        mu = self.fc3(x)\n        logstd = torch.zeros_like(mu)\n        std = torch.exp(logstd)\n        return mu, std\n\n\nclass Critic(nn.Module):\n    def __init__(self, num_inputs, args):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, 1)\n        \n        self.fc3.weight.data.mul_(0.1)\n        self.fc3.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        v = self.fc3(x)\n        return v'"
mujoco/ppo/ppo.py,16,"b'import torch\nimport numpy as np\nfrom utils.utils import log_prob_density\n\ndef train_model(actor, critic, memory, actor_optim, critic_optim, args):\n    memory = np.array(memory) \n    states = np.vstack(memory[:, 0]) \n    actions = list(memory[:, 1]) \n    rewards = list(memory[:, 2]) \n    masks = list(memory[:, 3]) \n\n    old_values = critic(torch.Tensor(states))\n    returns, advants = get_gae(rewards, masks, old_values, args)\n    \n    mu, std = actor(torch.Tensor(states))\n    old_policy = log_prob_density(torch.Tensor(actions), mu, std)\n\n    criterion = torch.nn.MSELoss()\n    n = len(states)\n    arr = np.arange(n)\n\n    for _ in range(args.model_update_num):\n        np.random.shuffle(arr)\n\n        for i in range(n // args.batch_size): \n            batch_index = arr[args.batch_size * i : args.batch_size * (i + 1)]\n            batch_index = torch.LongTensor(batch_index)\n            \n            inputs = torch.Tensor(states)[batch_index]\n            actions_samples = torch.Tensor(actions)[batch_index]\n            returns_samples = returns.unsqueeze(1)[batch_index]\n            advants_samples = advants.unsqueeze(1)[batch_index]\n            oldvalue_samples = old_values[batch_index].detach()\n            \n            values = critic(inputs)\n            clipped_values = oldvalue_samples + \\\n                             torch.clamp(values - oldvalue_samples,\n                                         -args.clip_param, \n                                         args.clip_param)\n            critic_loss1 = criterion(clipped_values, returns_samples)\n            critic_loss2 = criterion(values, returns_samples)\n            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n\n            loss, ratio = surrogate_loss(actor, advants_samples, inputs,\n                                         old_policy.detach(), actions_samples,\n                                         batch_index)\n            clipped_ratio = torch.clamp(ratio,\n                                        1.0 - args.clip_param,\n                                        1.0 + args.clip_param)\n            clipped_loss = clipped_ratio * advants_samples\n            actor_loss = -torch.min(loss, clipped_loss).mean()\n\n            loss = actor_loss + 0.5 * critic_loss\n\n            critic_optim.zero_grad()\n            loss.backward(retain_graph=True) \n            critic_optim.step()\n\n            actor_optim.zero_grad()\n            loss.backward()\n            actor_optim.step()\n\ndef get_gae(rewards, masks, values, args):\n    rewards = torch.Tensor(rewards)\n    masks = torch.Tensor(masks)\n    returns = torch.zeros_like(rewards)\n    advants = torch.zeros_like(rewards)\n    \n    running_returns = 0\n    previous_value = 0\n    running_advants = 0\n\n    for t in reversed(range(0, len(rewards))):\n        running_returns = rewards[t] + (args.gamma * running_returns * masks[t])\n        returns[t] = running_returns\n\n        running_delta = rewards[t] + (args.gamma * previous_value * masks[t]) - \\\n                                        values.data[t]\n        previous_value = values.data[t]\n        \n        running_advants = running_delta + (args.gamma * args.lamda * \\\n                                            running_advants * masks[t])\n        advants[t] = running_advants\n\n    advants = (advants - advants.mean()) / advants.std()\n    return returns, advants\n\ndef surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n    mu, std = actor(states)\n    new_policy = log_prob_density(actions, mu, std)\n    old_policy = old_policy[batch_index]\n\n    ratio = torch.exp(new_policy - old_policy)\n    surrogate_loss = ratio * advants\n\n    return surrogate_loss, ratio'"
mujoco/ppo/test.py,3,"b'import os\nimport gym\nimport torch\nimport argparse\n\nfrom model import Actor, Critic\nfrom utils.utils import get_action\nfrom utils.running_state import ZFilter\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--env\', type=str, default=""Hopper-v2"",\n                    help=\'name of Mujoco environement\')\nparser.add_argument(\'--iter\', type=int, default=5,\n                    help=\'number of episodes to play\')\nparser.add_argument(""--load_model"", type=str, default=\'ppo_max.tar\',\n                     help=""if you test pretrained file, write filename in save_model folder"")\n\nargs = parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    env = gym.make(args.env)\n    env.seed(500)\n    torch.manual_seed(500)\n\n    num_inputs = env.observation_space.shape[0]\n    num_actions = env.action_space.shape[0]\n\n    print(""state size: "", num_inputs)\n    print(""action size: "", num_actions)\n\n    actor = Actor(num_inputs, num_actions)\n    critic = Critic(num_inputs)\n\n    running_state = ZFilter((num_inputs,), clip=5)\n    \n    if args.load_model is not None:\n        pretrained_model_path = os.path.join(os.getcwd(), \'save_model\', str(args.load_model))\n\n        pretrained_model = torch.load(pretrained_model_path)\n\n        actor.load_state_dict(pretrained_model[\'actor\'])\n        critic.load_state_dict(pretrained_model[\'critic\'])\n\n        running_state.rs.n = pretrained_model[\'z_filter_n\']\n        running_state.rs.mean = pretrained_model[\'z_filter_m\']\n        running_state.rs.sum_square = pretrained_model[\'z_filter_s\']\n\n        print(""Loaded OK ex. ZFilter N {}"".format(running_state.rs.n))\n\n    else:\n        assert(""Should write pretrained filename in save_model folder. ex) python3 test_algo.py --load_model ppo_max.tar"")\n\n\n    actor.eval(), critic.eval()\n    for episode in range(args.iter):\n        state = env.reset()\n        steps = 0\n        score = 0\n        for _ in range(10000):\n            env.render()\n            mu, std, _ = actor(torch.Tensor(state).unsqueeze(0))\n            action = get_action(mu, std)[0]\n\n            next_state, reward, done, _ = env.step(action)\n            next_state = running_state(next_state)\n            \n            state = next_state\n            score += reward\n            \n            if done:\n                print(""{} cumulative reward: {}"".format(episode, score))\n                break\n'"
mujoco/vail/main.py,4,"b'import os\nimport gym\nimport pickle\nimport argparse\nimport numpy as np\nfrom collections import deque\n\nimport torch\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter \n\nfrom utils.utils import *\nfrom utils.zfilter import ZFilter\nfrom model import Actor, Critic, VDB\nfrom train_model import train_actor_critic, train_vdb\n\nparser = argparse.ArgumentParser(description=\'PyTorch VAIL\')\nparser.add_argument(\'--env_name\', type=str, default=""Hopper-v2"", \n                    help=\'name of the environment to run\')\nparser.add_argument(\'--load_model\', type=str, default=None, \n                    help=\'path to load the saved model\')\nparser.add_argument(\'--render\', action=""store_true"", default=False, \n                    help=\'if you dont want to render, set this to False\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, \n                    help=\'discounted factor (default: 0.99)\')\nparser.add_argument(\'--lamda\', type=float, default=0.98, \n                    help=\'GAE hyper-parameter (default: 0.98)\')\nparser.add_argument(\'--hidden_size\', type=int, default=100, \n                    help=\'hidden unit size of actor, critic and vdb networks (default: 100)\')\nparser.add_argument(\'--z_size\', type=int, default=4,\n                    help=\'latent vector z unit size of vdb networks (default: 4)\')\nparser.add_argument(\'--learning_rate\', type=float, default=3e-4, \n                    help=\'learning rate of models (default: 3e-4)\')\nparser.add_argument(\'--l2_rate\', type=float, default=1e-3, \n                    help=\'l2 regularizer coefficient (default: 1e-3)\')\nparser.add_argument(\'--clip_param\', type=float, default=0.2, \n                    help=\'clipping parameter for PPO (default: 0.2)\')\nparser.add_argument(\'--alpha_beta\', type=float, default=1e-4,\n                    help=\'step size to be used in beta term (default: 1e-4)\')\nparser.add_argument(\'--i_c\', type=float, default=0.5, \n                    help=\'constraint for KL-Divergence upper bound (default: 0.5)\')\nparser.add_argument(\'--vdb_update_num\', type=int, default=3, \n                    help=\'update number of variational discriminator bottleneck (default: 3)\')\nparser.add_argument(\'--ppo_update_num\', type=int, default=10, \n                    help=\'update number of actor-critic (default: 10)\')\nparser.add_argument(\'--total_sample_size\', type=int, default=2048, \n                    help=\'total sample size to collect before PPO update (default: 2048)\')\nparser.add_argument(\'--batch_size\', type=int, default=64, \n                    help=\'batch size to update (default: 64)\')\nparser.add_argument(\'--suspend_accu_exp\', type=float, default=0.8,\n                    help=\'accuracy for suspending discriminator about expert data (default: 0.8)\')\nparser.add_argument(\'--suspend_accu_gen\', type=float, default=0.8,\n                    help=\'accuracy for suspending discriminator about generated data (default: 0.8)\')\nparser.add_argument(\'--max_iter_num\', type=int, default=4000,\n                    help=\'maximal number of main iterations (default: 4000)\')\nparser.add_argument(\'--seed\', type=int, default=500,\n                    help=\'random seed (default: 500)\')\nparser.add_argument(\'--logdir\', type=str, default=\'logs\',\n                    help=\'tensorboardx logs directory\')\nargs = parser.parse_args()\n\n\ndef main():\n    env = gym.make(args.env_name)\n    env.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    num_inputs = env.observation_space.shape[0]\n    num_actions = env.action_space.shape[0]\n    running_state = ZFilter((num_inputs,), clip=5)\n\n    print(\'state size:\', num_inputs) \n    print(\'action size:\', num_actions)\n\n    actor = Actor(num_inputs, num_actions, args)\n    critic = Critic(num_inputs, args)\n    vdb = VDB(num_inputs + num_actions, args)\n\n    actor_optim = optim.Adam(actor.parameters(), lr=args.learning_rate)\n    critic_optim = optim.Adam(critic.parameters(), lr=args.learning_rate, \n                              weight_decay=args.l2_rate) \n    vdb_optim = optim.Adam(vdb.parameters(), lr=args.learning_rate)\n    \n    # load demonstrations\n    expert_demo, _ = pickle.load(open(\'./expert_demo/expert_demo.p\', ""rb""))\n    demonstrations = np.array(expert_demo)\n    print(""demonstrations.shape"", demonstrations.shape)\n\n    writer = SummaryWriter(args.logdir)\n\n    if args.load_model is not None:\n        saved_ckpt_path = os.path.join(os.getcwd(), \'save_model\', str(args.load_model))\n        ckpt = torch.load(saved_ckpt_path)\n\n        actor.load_state_dict(ckpt[\'actor\'])\n        critic.load_state_dict(ckpt[\'critic\'])\n        vdb.load_state_dict(ckpt[\'vdb\'])\n\n        running_state.rs.n = ckpt[\'z_filter_n\']\n        running_state.rs.mean = ckpt[\'z_filter_m\']\n        running_state.rs.sum_square = ckpt[\'z_filter_s\']\n\n        print(""Loaded OK ex. Zfilter N {}"".format(running_state.rs.n))\n\n    \n    episodes = 0\n    train_discrim_flag = True\n\n    for iter in range(args.max_iter_num):\n        actor.eval(), critic.eval()\n        memory = deque()\n\n        steps = 0\n        scores = []\n\n        while steps < args.total_sample_size: \n            state = env.reset()\n            score = 0\n\n            state = running_state(state)\n            \n            for _ in range(10000): \n                if args.render:\n                    env.render()\n\n                steps += 1\n\n                mu, std = actor(torch.Tensor(state).unsqueeze(0))\n                action = get_action(mu, std)[0]\n                next_state, reward, done, _ = env.step(action)\n                irl_reward = get_reward(vdb, state, action)\n\n                if done:\n                    mask = 0\n                else:\n                    mask = 1\n\n                memory.append([state, action, irl_reward, mask])\n\n                next_state = running_state(next_state)\n                state = next_state\n\n                score += reward\n\n                if done:\n                    break\n            \n            episodes += 1\n            scores.append(score)\n        \n        score_avg = np.mean(scores)\n        print(\'{}:: {} episode score is {:.2f}\'.format(iter, episodes, score_avg))\n        writer.add_scalar(\'log/score\', float(score_avg), iter)\n\n        actor.train(), critic.train(), vdb.train()\n        if train_discrim_flag:\n            expert_acc, learner_acc = train_vdb(vdb, memory, vdb_optim, demonstrations, 0, args)\n            print(""Expert: %.2f%% | Learner: %.2f%%"" % (expert_acc * 100, learner_acc * 100))\n            if expert_acc > args.suspend_accu_exp and learner_acc > args.suspend_accu_gen:\n                train_discrim_flag = False\n        train_actor_critic(actor, critic, memory, actor_optim, critic_optim, args)\n\n        if iter % 100:\n            score_avg = int(score_avg)\n\n            model_path = os.path.join(os.getcwd(),\'save_model\')\n            if not os.path.isdir(model_path):\n                os.makedirs(model_path)\n\n            ckpt_path = os.path.join(model_path, \'ckpt_\'+ str(score_avg)+\'.pth.tar\')\n\n            save_checkpoint({\n                \'actor\': actor.state_dict(),\n                \'critic\': critic.state_dict(),\n                \'vdb\': vdb.state_dict(),\n                \'z_filter_n\':running_state.rs.n,\n                \'z_filter_m\': running_state.rs.mean,\n                \'z_filter_s\': running_state.rs.sum_square,\n                \'args\': args,\n                \'score\': score_avg\n            }, filename=ckpt_path)\n\nif __name__==""__main__"":\n    main()'"
mujoco/vail/model.py,12,"b'import torch\nimport torch.nn as nn\n\nclass Actor(nn.Module):\n    def __init__(self, num_inputs, num_outputs, args):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, num_outputs)\n        \n        self.fc3.weight.data.mul_(0.1)\n        self.fc3.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        mu = self.fc3(x)\n        logstd = torch.zeros_like(mu)\n        std = torch.exp(logstd)\n        return mu, std\n\n\nclass Critic(nn.Module):\n    def __init__(self, num_inputs, args):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.fc3 = nn.Linear(args.hidden_size, 1)\n        \n        self.fc3.weight.data.mul_(0.1)\n        self.fc3.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = torch.tanh(self.fc2(x))\n        v = self.fc3(x)\n        return v\n\n\nclass VDB(nn.Module):\n    def __init__(self, num_inputs, args):\n        super(VDB, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, args.hidden_size)\n        self.fc2 = nn.Linear(args.hidden_size, args.z_size)\n        self.fc3 = nn.Linear(args.hidden_size, args.z_size)\n        self.fc4 = nn.Linear(args.z_size, args.hidden_size)\n        self.fc5 = nn.Linear(args.hidden_size, 1)\n        \n        self.fc5.weight.data.mul_(0.1)\n        self.fc5.bias.data.mul_(0.0)\n\n    def encoder(self, x):\n        h = torch.tanh(self.fc1(x))\n        return self.fc2(h), self.fc3(h)\n    \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(logvar/2)\n        eps = torch.randn_like(std)\n        return mu + std * eps\n\n    def discriminator(self, z):\n        h = torch.tanh(self.fc4(z))\n        return torch.sigmoid(self.fc5(h))\n    \n    def forward(self, x):\n        mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        prob = self.discriminator(z)\n        return prob, mu, logvar'"
mujoco/vail/test.py,3,"b'import os\nimport gym\nimport torch\nimport argparse\n\nfrom model import Actor, Critic\nfrom utils.utils import get_action\nfrom utils.running_state import ZFilter\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--env\', type=str, default=""Hopper-v2"",\n                    help=\'name of Mujoco environement\')\nparser.add_argument(\'--iter\', type=int, default=5,\n                    help=\'number of episodes to play\')\nparser.add_argument(""--load_model"", type=str, default=\'ppo_max.tar\',\n                     help=""if you test pretrained file, write filename in save_model folder"")\n\nargs = parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    env = gym.make(args.env)\n    env.seed(500)\n    torch.manual_seed(500)\n\n    num_inputs = env.observation_space.shape[0]\n    num_actions = env.action_space.shape[0]\n\n    print(""state size: "", num_inputs)\n    print(""action size: "", num_actions)\n\n    actor = Actor(num_inputs, num_actions)\n    critic = Critic(num_inputs)\n\n    running_state = ZFilter((num_inputs,), clip=5)\n    \n    if args.load_model is not None:\n        pretrained_model_path = os.path.join(os.getcwd(), \'save_model\', str(args.load_model))\n\n        pretrained_model = torch.load(pretrained_model_path)\n\n        actor.load_state_dict(pretrained_model[\'actor\'])\n        critic.load_state_dict(pretrained_model[\'critic\'])\n\n        running_state.rs.n = pretrained_model[\'z_filter_n\']\n        running_state.rs.mean = pretrained_model[\'z_filter_m\']\n        running_state.rs.sum_square = pretrained_model[\'z_filter_s\']\n\n        print(""Loaded OK ex. ZFilter N {}"".format(running_state.rs.n))\n\n    else:\n        assert(""Should write pretrained filename in save_model folder. ex) python3 test_algo.py --load_model ppo_max.tar"")\n\n\n    actor.eval(), critic.eval()\n    for episode in range(args.iter):\n        state = env.reset()\n        steps = 0\n        score = 0\n        for _ in range(10000):\n            env.render()\n            mu, std, _ = actor(torch.Tensor(state).unsqueeze(0))\n            action = get_action(mu, std)[0]\n\n            next_state, reward, done, _ = env.step(action)\n            next_state = running_state(next_state)\n            \n            state = next_state\n            score += reward\n            \n            if done:\n                print(""{} cumulative reward: {}"".format(episode, score))\n                break\n'"
mujoco/vail/train_model.py,24,"b'import torch\nimport numpy as np\nfrom utils.utils import *\n\ndef train_vdb(vdb, memory, vdb_optim, demonstrations, beta, args):\n    memory = np.array(memory) \n    states = np.vstack(memory[:, 0]) \n    actions = list(memory[:, 1]) \n\n    states = torch.Tensor(states)\n    actions = torch.Tensor(actions)\n\n    criterion = torch.nn.BCELoss()\n\n    for _ in range(args.vdb_update_num):\n        learner, l_mu, l_logvar = vdb(torch.cat([states, actions], dim=1))\n        demonstrations = torch.Tensor(demonstrations)\n        expert, e_mu, e_logvar = vdb(demonstrations)\n\n        l_kld = kl_divergence(l_mu, l_logvar)\n        l_kld = l_kld.mean()\n        \n        e_kld = kl_divergence(e_mu, e_logvar)\n        e_kld = e_kld.mean()\n        \n        kld = 0.5 * (l_kld + e_kld)\n        bottleneck_loss = kld - args.i_c\n\n        beta = max(0, beta + args.alpha_beta * bottleneck_loss)\n\n        vdb_loss = criterion(learner, torch.ones((states.shape[0], 1))) + \\\n                    criterion(expert, torch.zeros((demonstrations.shape[0], 1))) + \\\n                    beta * bottleneck_loss\n                \n        vdb_optim.zero_grad()\n        vdb_loss.backward(retain_graph=True)\n        vdb_optim.step()\n\n    expert_acc = ((vdb(demonstrations)[0] < 0.5).float()).mean()\n    learner_acc = ((vdb(torch.cat([states, actions], dim=1))[0] > 0.5).float()).mean()\n\n    return expert_acc, learner_acc\n    \n\ndef train_actor_critic(actor, critic, memory, actor_optim, critic_optim, args):\n    memory = np.array(memory) \n    states = np.vstack(memory[:, 0]) \n    actions = list(memory[:, 1]) \n    rewards = list(memory[:, 2]) \n    masks = list(memory[:, 3]) \n\n    old_values = critic(torch.Tensor(states))\n    returns, advants = get_gae(rewards, masks, old_values, args)\n    \n    mu, std = actor(torch.Tensor(states))\n    old_policy = log_prob_density(torch.Tensor(actions), mu, std)\n\n    criterion = torch.nn.MSELoss()\n    n = len(states)\n    arr = np.arange(n)\n\n    for _ in range(args.ppo_update_num):\n        np.random.shuffle(arr)\n\n        for i in range(n // args.batch_size): \n            batch_index = arr[args.batch_size * i : args.batch_size * (i + 1)]\n            batch_index = torch.LongTensor(batch_index)\n            \n            inputs = torch.Tensor(states)[batch_index]\n            actions_samples = torch.Tensor(actions)[batch_index]\n            returns_samples = returns.unsqueeze(1)[batch_index]\n            advants_samples = advants.unsqueeze(1)[batch_index]\n            oldvalue_samples = old_values[batch_index].detach()\n            \n            values = critic(inputs)\n            clipped_values = oldvalue_samples + \\\n                             torch.clamp(values - oldvalue_samples,\n                                         -args.clip_param, \n                                         args.clip_param)\n            critic_loss1 = criterion(clipped_values, returns_samples)\n            critic_loss2 = criterion(values, returns_samples)\n            critic_loss = torch.max(critic_loss1, critic_loss2).mean()\n\n            loss, ratio, entropy = surrogate_loss(actor, advants_samples, inputs,\n                                         old_policy.detach(), actions_samples,\n                                         batch_index)\n            clipped_ratio = torch.clamp(ratio,\n                                        1.0 - args.clip_param,\n                                        1.0 + args.clip_param)\n            clipped_loss = clipped_ratio * advants_samples\n            actor_loss = -torch.min(loss, clipped_loss).mean()\n\n            loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n\n            critic_optim.zero_grad()\n            loss.backward(retain_graph=True) \n            critic_optim.step()\n\n            actor_optim.zero_grad()\n            loss.backward()\n            actor_optim.step()\n\ndef get_gae(rewards, masks, values, args):\n    rewards = torch.Tensor(rewards)\n    masks = torch.Tensor(masks)\n    returns = torch.zeros_like(rewards)\n    advants = torch.zeros_like(rewards)\n    \n    running_returns = 0\n    previous_value = 0\n    running_advants = 0\n\n    for t in reversed(range(0, len(rewards))):\n        running_returns = rewards[t] + (args.gamma * running_returns * masks[t])\n        returns[t] = running_returns\n\n        running_delta = rewards[t] + (args.gamma * previous_value * masks[t]) - \\\n                                        values.data[t]\n        previous_value = values.data[t]\n        \n        running_advants = running_delta + (args.gamma * args.lamda * \\\n                                            running_advants * masks[t])\n        advants[t] = running_advants\n\n    advants = (advants - advants.mean()) / advants.std()\n    return returns, advants\n\ndef surrogate_loss(actor, advants, states, old_policy, actions, batch_index):\n    mu, std = actor(states)\n    new_policy = log_prob_density(actions, mu, std)\n    old_policy = old_policy[batch_index]\n\n    ratio = torch.exp(new_policy - old_policy)\n    surrogate_loss = ratio * advants\n    entropy = get_entropy(mu, std)\n\n    return surrogate_loss, ratio, entropy'"
mountaincar/app/expert_demo/make_expert.py,0,"b'import gym\nimport readchar\nimport numpy as np\n\n# MACROS\nPush_Left = 0\nNo_Push = 1\nPush_Right = 2\n\n# Key mapping\narrow_keys = {\n    \'\\x1b[D\': Push_Left,\n    \'\\x1b[B\': No_Push,\n    \'\\x1b[C\': Push_Right}\n\nenv = gym.make(\'MountainCar-v0\')\n\ntrajectories = []\nepisode_step = 0\n\nfor episode in range(20): # n_trajectories : 20\n    trajectory = []\n    step = 0\n\n    env.reset()\n    print(""episode_step"", episode_step)\n\n    while True: \n        env.render()\n        print(""step"", step)\n\n        key = readchar.readkey()\n        if key not in arrow_keys.keys():\n            break\n\n        action = arrow_keys[key]\n        state, reward, done, _ = env.step(action)\n\n        if state[0] >= env.env.goal_position and step > 129: # trajectory_length : 130\n            break\n\n        trajectory.append((state[0], state[1], action))\n        step += 1\n\n    trajectory_numpy = np.array(trajectory, float)\n    print(""trajectory_numpy.shape"", trajectory_numpy.shape)\n    episode_step += 1\n    trajectories.append(trajectory)\n\nnp_trajectories = np.array(trajectories, float)\nprint(""np_trajectories.shape"", np_trajectories.shape)\n\nnp.save(""expert_demo"", arr=np_trajectories)'"
mountaincar/maxent/expert_demo/make_expert.py,0,"b'import gym\nimport readchar\nimport numpy as np\n\n# # MACROS\nPush_Left = 0\nNo_Push = 1\nPush_Right = 2\n\n# Key mapping\narrow_keys = {\n    \'\\x1b[D\': Push_Left,\n    \'\\x1b[B\': No_Push,\n    \'\\x1b[C\': Push_Right}\n\nenv = gym.make(\'MountainCar-v0\')\n\ntrajectories = []\nepisode_step = 0\n\nfor episode in range(20): # n_trajectories : 20\n    trajectory = []\n    step = 0\n\n    env.reset()\n    print(""episode_step"", episode_step)\n\n    while True: \n        env.render()\n        print(""step"", step)\n\n        key = readchar.readkey()\n        if key not in arrow_keys.keys():\n            break\n\n        action = arrow_keys[key]\n        state, reward, done, _ = env.step(action)\n\n        if state[0] >= env.env.goal_position and step > 129: # trajectory_length : 130\n            break\n\n        trajectory.append((state[0], state[1], action))\n        step += 1\n\n    # trajectory_numpy = np.array(trajectory, float)\n    # print(""trajectory_numpy.shape"", trajectory_numpy.shape)\n    # episode_step += 1\n    # trajectories.append(trajectory)\n\n# np_trajectories = np.array(trajectories, float)\n# print(""np_trajectories.shape"", np_trajectories.shape)\n\n# np.save(""expert_trajectories"", arr=np_trajectories)'"
mujoco/gail/utils/utils.py,7,"b'import math\nimport torch\nfrom torch.distributions import Normal\n\ndef get_action(mu, std):\n    action = torch.normal(mu, std)\n    action = action.data.numpy()\n    return action\n\ndef get_entropy(mu, std):\n    dist = Normal(mu, std)\n    entropy = dist.entropy().mean()\n    return entropy\n\ndef log_prob_density(x, mu, std):\n    log_prob_density = -(x - mu).pow(2) / (2 * std.pow(2)) \\\n                     - 0.5 * math.log(2 * math.pi)\n    return log_prob_density.sum(1, keepdim=True)\n\ndef get_reward(discrim, state, action):\n    state = torch.Tensor(state)\n    action = torch.Tensor(action)\n    state_action = torch.cat([state, action])\n    with torch.no_grad():\n        return -math.log(discrim(state_action)[0].item())\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)'"
mujoco/gail/utils/zfilter.py,0,"b'import numpy as np\n\n# from https://github.com/joschu/modular_rl\n# http://www.johndcook.com/blog/standard_deviation/\n\nclass RunningStat(object):\n    def __init__(self, shape): \n        self._n = 0\n        self._M = np.zeros(shape)\n        self._S = np.zeros(shape)\n\n    def push(self, x):\n        x = np.asarray(x)\n        assert x.shape == self._M.shape\n        self._n += 1\n        if self._n == 1: \n            self._M[...] = x\n        else: \n            oldM = self._M.copy()\n            self._M[...] = oldM + (x - oldM) / self._n\n            self._S[...] = self._S + (x - oldM) * (x - self._M)\n\n    @property\n    def n(self):\n        return self._n\n\n    @n.setter\n    def n(self, n):\n        self._n = n\n\n    @property\n    def mean(self):\n        return self._M\n\n    @mean.setter\n    def mean(self, M):\n        self._M = M\n\n    @property\n    def sum_square(self):\n        return self._S\n\n    @sum_square.setter\n    def sum_square(self, S):\n        self._S = S\n\n    @property\n    def var(self):\n        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n\n    @property\n    def std(self):\n        return np.sqrt(self.var)\n\n    @property\n    def shape(self):\n        return self._M.shape\n\n\nclass ZFilter:\n    """"""\n    y = (x-mean)/std\n    using running estimates of mean,std\n    """"""\n\n    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n        self.demean = demean\n        self.destd = destd\n        self.clip = clip\n\n        self.rs = RunningStat(shape)\n\n    def __call__(self, x, update=True):\n        if update: self.rs.push(x)\n            \n        if self.demean:\n            x = x - self.rs.mean\n            \n        if self.destd:\n            x = x / (self.rs.std + 1e-8)\n            \n        if self.clip:\n            x = np.clip(x, -self.clip, self.clip)\n            \n        return x\n'"
mujoco/ppo/utils/utils.py,2,"b'import math\nimport torch\n\ndef get_action(mu, std):\n    action = torch.normal(mu, std)\n    action = action.data.numpy()\n    return action\n\ndef log_prob_density(x, mu, std):\n    log_prob_density = -(x - mu).pow(2) / (2 * std.pow(2)) \\\n                     - 0.5 * math.log(2 * math.pi)\n    return log_prob_density.sum(1, keepdim=True)\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)'"
mujoco/ppo/utils/zfilter.py,0,"b'import numpy as np\n\n# from https://github.com/joschu/modular_rl\n# http://www.johndcook.com/blog/standard_deviation/\n\nclass RunningStat(object):\n    def __init__(self, shape): \n        self._n = 0\n        self._M = np.zeros(shape)\n        self._S = np.zeros(shape)\n\n    def push(self, x):\n        x = np.asarray(x)\n        assert x.shape == self._M.shape\n        self._n += 1\n        if self._n == 1: \n            self._M[...] = x\n        else: \n            oldM = self._M.copy()\n            self._M[...] = oldM + (x - oldM) / self._n\n            self._S[...] = self._S + (x - oldM) * (x - self._M)\n\n    @property\n    def n(self):\n        return self._n\n\n    @n.setter\n    def n(self, n):\n        self._n = n\n\n    @property\n    def mean(self):\n        return self._M\n\n    @mean.setter\n    def mean(self, M):\n        self._M = M\n\n    @property\n    def sum_square(self):\n        return self._S\n\n    @sum_square.setter\n    def sum_square(self, S):\n        self._S = S\n\n    @property\n    def var(self):\n        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n\n    @property\n    def std(self):\n        return np.sqrt(self.var)\n\n    @property\n    def shape(self):\n        return self._M.shape\n\n\nclass ZFilter:\n    """"""\n    y = (x-mean)/std\n    using running estimates of mean,std\n    """"""\n\n    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n        self.demean = demean\n        self.destd = destd\n        self.clip = clip\n\n        self.rs = RunningStat(shape)\n\n    def __call__(self, x, update=True):\n        if update: self.rs.push(x)\n            \n        if self.demean:\n            x = x - self.rs.mean\n            \n        if self.destd:\n            x = x / (self.rs.std + 1e-8)\n            \n        if self.clip:\n            x = np.clip(x, -self.clip, self.clip)\n            \n        return x\n'"
mujoco/vail/utils/utils.py,8,"b'import math\nimport torch\nfrom torch.distributions import Normal\n\ndef get_action(mu, std):\n    action = torch.normal(mu, std)\n    action = action.data.numpy()\n    return action\n\ndef get_entropy(mu, std):\n    dist = Normal(mu, std)\n    entropy = dist.entropy().mean()\n    return entropy\n\ndef log_prob_density(x, mu, std):\n    log_prob_density = -(x - mu).pow(2) / (2 * std.pow(2)) \\\n                     - 0.5 * math.log(2 * math.pi)\n    return log_prob_density.sum(1, keepdim=True)\n\ndef get_reward(vdb, state, action):\n    state = torch.Tensor(state)\n    action = torch.Tensor(action)\n    state_action = torch.cat([state, action])\n    with torch.no_grad():\n        return -math.log(vdb(state_action)[0].item())\n\ndef kl_divergence(mu, logvar):\n    kl_div = 0.5 * torch.sum(mu.pow(2) + logvar.exp() - logvar - 1, dim=1)\n    return kl_div\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)'"
mujoco/vail/utils/zfilter.py,0,"b'import numpy as np\n\n# from https://github.com/joschu/modular_rl\n# http://www.johndcook.com/blog/standard_deviation/\n\nclass RunningStat(object):\n    def __init__(self, shape): \n        self._n = 0\n        self._M = np.zeros(shape)\n        self._S = np.zeros(shape)\n\n    def push(self, x):\n        x = np.asarray(x)\n        assert x.shape == self._M.shape\n        self._n += 1\n        if self._n == 1: \n            self._M[...] = x\n        else: \n            oldM = self._M.copy()\n            self._M[...] = oldM + (x - oldM) / self._n\n            self._S[...] = self._S + (x - oldM) * (x - self._M)\n\n    @property\n    def n(self):\n        return self._n\n\n    @n.setter\n    def n(self, n):\n        self._n = n\n\n    @property\n    def mean(self):\n        return self._M\n\n    @mean.setter\n    def mean(self, M):\n        self._M = M\n\n    @property\n    def sum_square(self):\n        return self._S\n\n    @sum_square.setter\n    def sum_square(self, S):\n        self._S = S\n\n    @property\n    def var(self):\n        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n\n    @property\n    def std(self):\n        return np.sqrt(self.var)\n\n    @property\n    def shape(self):\n        return self._M.shape\n\n\nclass ZFilter:\n    """"""\n    y = (x-mean)/std\n    using running estimates of mean,std\n    """"""\n\n    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n        self.demean = demean\n        self.destd = destd\n        self.clip = clip\n\n        self.rs = RunningStat(shape)\n\n    def __call__(self, x, update=True):\n        if update: self.rs.push(x)\n            \n        if self.demean:\n            x = x - self.rs.mean\n            \n        if self.destd:\n            x = x / (self.rs.std + 1e-8)\n            \n        if self.clip:\n            x = np.clip(x, -self.clip, self.clip)\n            \n        return x\n'"
